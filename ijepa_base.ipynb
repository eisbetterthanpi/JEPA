{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO3ndAlMyHHvNxbVbtGfiLt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/ijepa_base.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title data\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/transforms.py\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.RandomResizedCrop((32,32), scale=(.3,1.)), transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/datasets/imagenet1k.py\n",
        "import torch\n",
        "import torchvision\n",
        "# import torchvision.transforms as transforms\n",
        "# # transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# transform = transforms.Compose([transforms.ToTensor(),])\n",
        "\n",
        "# CIFAR10: 60000 32x32 color images in 10 classes, with 6000 images per class\n",
        "train_data = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
        "test_data = torchvision.datasets.CIFAR10(root='data', train=False, download=True, transform=transform)\n",
        "batch_size = 64 # 4\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "# train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=mask_collator)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "# classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4EuXaPSYV8Lj"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title masks\n",
        "import torch\n",
        "\n",
        "def multiblock(seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "    return target_mask\n",
        "\n",
        "\n",
        "def multiblock2d(hw=(8,8), scale=(.15,.2), aspect_ratio=(.75,1.5), M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_aspect = torch.rand(1) * (aspect_ratio[1] - aspect_ratio[0]) + aspect_ratio[0] # in (min_s, max_s) # all blocks same size\n",
        "    mask_scale = torch.rand(1) * (scale[1] - scale[0]) + scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    h = (mask_scale/mask_aspect)**.5# h*(h*aspect) = scale\n",
        "    w = h * mask_aspect\n",
        "    h_pos, w_pos = torch.rand(M)*(1-w), torch.rand(M)*(1-h) # in (0, 1 - mask_len)\n",
        "    h_len, h_pos = (h*hw[0]).int(), h_pos*hw[0]\n",
        "    w_len, w_pos = (w*hw[1]).int(), w_pos*hw[1]\n",
        "    h_ind, w_ind = torch.arange(hw[0]).unsqueeze(0), torch.arange(hw[1]).unsqueeze(0) # [1, seq]\n",
        "    h_mask = (h_ind>=h_pos.unsqueeze(-1)) & (h_ind<(h_pos+h_len).unsqueeze(-1)) # [M, seq]\n",
        "    w_mask = (w_ind>=w_pos.unsqueeze(-1)) & (w_ind<(w_pos+w_len).unsqueeze(-1)) # [M, seq]\n",
        "    target_mask = h_mask.unsqueeze(-1) & w_mask.unsqueeze(-2) # [M, seq, seq]\n",
        "    return target_mask\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "5W-Z3badFElA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title tensors.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/utils/tensors.py#L65\n",
        "import math\n",
        "import torch\n",
        "\n",
        "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
        "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n",
        "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
        "    def norm_cdf(x):\n",
        "        # Computes standard normal cumulative distribution function\n",
        "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Values are generated by using a truncated uniform distribution and\n",
        "        # then using the inverse CDF for the normal distribution.\n",
        "        # Get upper and lower cdf values\n",
        "        l = norm_cdf((a - mean) / std)\n",
        "        u = norm_cdf((b - mean) / std)\n",
        "\n",
        "        # Uniformly fill tensor with values from [l, u], then translate to\n",
        "        # [2l-1, 2u-1].\n",
        "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
        "\n",
        "        # Use inverse cdf transform for normal distribution to get truncated\n",
        "        # standard normal\n",
        "        tensor.erfinv_()\n",
        "\n",
        "        # Transform to proper mean, std\n",
        "        tensor.mul_(std * math.sqrt(2.))\n",
        "        tensor.add_(mean)\n",
        "\n",
        "        # Clamp to ensure it's in the proper range\n",
        "        tensor.clamp_(min=a, max=b)\n",
        "        return tensor\n",
        "\n",
        "\n",
        "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
        "    # type: (Tensor, float, float, float, float) -> Tensor\n",
        "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n",
        "\n",
        "\n",
        "def apply_masks(x, masks):\n",
        "    \"\"\"\n",
        "    :param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
        "    :param masks: list of tensors containing indices of patches in [N] to keep\n",
        "    \"\"\"\n",
        "    all_x = []\n",
        "    for m in masks:\n",
        "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1))\n",
        "        all_x += [torch.gather(x, dim=1, index=mask_keep)]\n",
        "    return torch.cat(all_x, dim=0)\n",
        "\n",
        "\n",
        "def repeat_interleave_batch(x, B, repeat):\n",
        "    N = len(x) // B\n",
        "    x = torch.cat([\n",
        "        torch.cat([x[i*B:(i+1)*B] for _ in range(repeat)], dim=0)\n",
        "        for i in range(N)\n",
        "    ], dim=0)\n",
        "    return x\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "sUGC2cYb9gI1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title schedulers.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/utils/schedulers.py#L11\n",
        "import math\n",
        "\n",
        "class WarmupCosineSchedule(object):\n",
        "    def __init__(self, optimizer, warmup_steps, start_lr, ref_lr, T_max, last_epoch=-1, final_lr=0.):\n",
        "        self.optimizer = optimizer\n",
        "        self.start_lr = start_lr\n",
        "        self.ref_lr = ref_lr\n",
        "        self.final_lr = final_lr\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.T_max = T_max - warmup_steps\n",
        "        self._step = 0.\n",
        "\n",
        "    def step(self):\n",
        "        self._step += 1\n",
        "        if self._step < self.warmup_steps:\n",
        "            progress = float(self._step) / float(max(1, self.warmup_steps))\n",
        "            new_lr = self.start_lr + progress * (self.ref_lr - self.start_lr)\n",
        "        else:\n",
        "            # -- progress after warmup\n",
        "            progress = float(self._step - self.warmup_steps) / float(max(1, self.T_max))\n",
        "            new_lr = max(self.final_lr, self.final_lr + (self.ref_lr - self.final_lr) * 0.5 * (1. + math.cos(math.pi * progress)))\n",
        "        for group in self.optimizer.param_groups:\n",
        "            group['lr'] = new_lr\n",
        "        return new_lr\n",
        "\n",
        "\n",
        "class CosineWDSchedule(object):\n",
        "    def __init__(self, optimizer, ref_wd, T_max, final_wd=0.):\n",
        "        self.optimizer = optimizer\n",
        "        self.ref_wd = ref_wd\n",
        "        self.final_wd = final_wd\n",
        "        self.T_max = T_max\n",
        "        self._step = 0.\n",
        "\n",
        "    def step(self):\n",
        "        self._step += 1\n",
        "        progress = self._step / self.T_max\n",
        "        new_wd = self.final_wd + (self.ref_wd - self.final_wd) * 0.5 * (1. + math.cos(math.pi * progress))\n",
        "        if self.final_wd <= self.ref_wd: new_wd = max(self.final_wd, new_wd)\n",
        "        else: new_wd = min(self.final_wd, new_wd)\n",
        "        for group in self.optimizer.param_groups:\n",
        "            if ('WD_exclude' not in group) or not group['WD_exclude']: group['weight_decay'] = new_wd\n",
        "        return new_wd\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "tPevTbyMvYE4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "pUst83Ei78Gl",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title vision_transformer.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/models/vision_transformer.py\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid height and width\n",
        "    return:\n",
        "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid_h = np.arange(grid_size, dtype=float)\n",
        "    grid_w = np.arange(grid_size, dtype=float)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    assert embed_dim % 2 == 0\n",
        "\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid length\n",
        "    return:\n",
        "    pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid = np.arange(grid_size, dtype=float)\n",
        "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=float)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega   # (D/2,)\n",
        "\n",
        "    pos = pos.reshape(-1)   # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
        "\n",
        "    emb_sin = np.sin(out)  # (M, D/2)\n",
        "    emb_cos = np.cos(out)  # (M, D/2)\n",
        "\n",
        "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    if drop_prob == 0. or not training: return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(in_features, hidden_features), nn.GELU(), nn.Dropout(drop),\n",
        "            nn.Linear(hidden_features, out_features), nn.Dropout(drop),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mlp(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        # attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        # attn = attn.softmax(dim=-1)\n",
        "        # attn = self.attn_drop(attn)\n",
        "        # x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "\n",
        "        q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2)\n",
        "        context = k.transpose(-2,-1) @ v # [batch, n_heads, d_head, d_head]\n",
        "        x = q @ context # [batch, n_heads, T/num_tok, d_head]\n",
        "        x = x.transpose(1, 2).reshape(B, N, C)\n",
        "\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        # return x, attn\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0., drop_path=0):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, drop=drop)\n",
        "\n",
        "    def forward(self, x, return_attention=False):\n",
        "        # y, attn = self.attn(self.norm1(x))\n",
        "        y = self.attn(self.norm1(x))\n",
        "        # if return_attention:\n",
        "        #     return attn\n",
        "        x = x + self.drop_path(y)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VisionTransformerPredictor(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, num_patches, embed_dim=768, predictor_embed_dim=384, depth=6,\n",
        "        num_heads=12, mlp_ratio=4, qkv_bias=True, qk_scale=None,\n",
        "        drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0, **kwargs):\n",
        "        super().__init__()\n",
        "        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        # --\n",
        "        self.predictor_pos_embed = nn.Parameter(torch.zeros(1, num_patches, predictor_embed_dim), requires_grad=False)\n",
        "        predictor_pos_embed = get_2d_sincos_pos_embed(self.predictor_pos_embed.shape[-1], int(num_patches**.5), cls_token=False)\n",
        "        self.predictor_pos_embed.data.copy_(torch.from_numpy(predictor_pos_embed).float().unsqueeze(0))\n",
        "        # --\n",
        "        self.predictor_blocks = nn.ModuleList([Block(dim=predictor_embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i]) for i in range(depth)])\n",
        "        self.predictor_norm = nn.LayerNorm(predictor_embed_dim)\n",
        "        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n",
        "        # ------\n",
        "        trunc_normal_(self.mask_token, std=0.02)\n",
        "        self.apply(self._init_weights)\n",
        "        self.fix_init_weight()\n",
        "\n",
        "    def fix_init_weight(self):\n",
        "        def rescale(param, layer_id):\n",
        "            param.div_(math.sqrt(2.0 * layer_id))\n",
        "\n",
        "        for layer_id, layer in enumerate(self.predictor_blocks):\n",
        "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
        "            # rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
        "            rescale(layer.mlp.mlp[-2].weight.data, layer_id + 1)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
        "            trunc_normal_(m.weight, std=0.02)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    def forward(self, x, masks_x, masks):\n",
        "        assert (masks is not None) and (masks_x is not None), 'Cannot run predictor without mask indices'\n",
        "        if not isinstance(masks_x, list): masks_x = [masks_x]\n",
        "        if not isinstance(masks, list): masks = [masks]\n",
        "\n",
        "        # -- Batch Size\n",
        "        B = len(x) // len(masks_x)\n",
        "\n",
        "        # -- map from encoder-dim to pedictor-dim\n",
        "        x = self.predictor_embed(x)\n",
        "\n",
        "        # -- add positional embedding to x tokens\n",
        "        x_pos_embed = self.predictor_pos_embed.repeat(B, 1, 1)\n",
        "        x += apply_masks(x_pos_embed, masks_x)\n",
        "\n",
        "        _, N_ctxt, D = x.shape\n",
        "\n",
        "        # -- concat mask tokens to x\n",
        "        pos_embs = self.predictor_pos_embed.repeat(B, 1, 1)\n",
        "        pos_embs = apply_masks(pos_embs, masks)\n",
        "        pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x))\n",
        "        # --\n",
        "        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1)\n",
        "        # --\n",
        "        pred_tokens += pos_embs\n",
        "        x = x.repeat(len(masks), 1, 1)\n",
        "        x = torch.cat([x, pred_tokens], dim=1)\n",
        "\n",
        "        for blk in self.predictor_blocks: x = blk(x)\n",
        "        x = self.predictor_norm(x)\n",
        "        x = x[:, N_ctxt:]\n",
        "        x = self.predictor_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, img_size=[224], patch_size=16, in_chans=3, embed_dim=768, predictor_embed_dim=384,\n",
        "        depth=12, predictor_depth=12, num_heads=12, mlp_ratio=4.0,\n",
        "        qkv_bias=True, qk_scale=None,\n",
        "        drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0, **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_features = self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        # --\n",
        "        self.patch_embed = PatchEmbed(img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        # --\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=False)\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "        # --\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i])\n",
        "            for i in range(depth)])\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        # ------\n",
        "        self.apply(self._init_weights)\n",
        "        self.fix_init_weight()\n",
        "\n",
        "    def fix_init_weight(self):\n",
        "        def rescale(param, layer_id):\n",
        "            param.div_(math.sqrt(2.0 * layer_id))\n",
        "\n",
        "        for layer_id, layer in enumerate(self.blocks):\n",
        "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
        "            # rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
        "            rescale(layer.mlp.mlp[-2].weight.data, layer_id + 1)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
        "            trunc_normal_(m.weight, std=0.02)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    def forward(self, x, masks=None):\n",
        "        if masks is not None:\n",
        "            if not isinstance(masks, list): masks = [masks]\n",
        "\n",
        "        # -- patchify x\n",
        "        x = self.patch_embed(x)\n",
        "        B, N, D = x.shape\n",
        "\n",
        "        # -- add positional embedding to x\n",
        "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
        "        x = x + pos_embed\n",
        "\n",
        "        # -- mask x\n",
        "        if masks is not None: x = apply_masks(x, masks)\n",
        "\n",
        "        for i, blk in enumerate(self.blocks): x = blk(x)\n",
        "        if self.norm is not None: x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "    def interpolate_pos_encoding(self, x, pos_embed):\n",
        "        npatch = x.shape[1] - 1\n",
        "        N = pos_embed.shape[1] - 1\n",
        "        if npatch == N:\n",
        "            return pos_embed\n",
        "        class_emb = pos_embed[:, 0]\n",
        "        pos_embed = pos_embed[:, 1:]\n",
        "        dim = x.shape[-1]\n",
        "        pos_embed = nn.functional.interpolate(\n",
        "            pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n",
        "            scale_factor=math.sqrt(npatch / N), mode='bicubic')\n",
        "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
        "\n",
        "\n",
        "# def vit_predictor(**kwargs): VisionTransformerPredictor(mlp_ratio=4, qkv_bias=True, **kwargs)\n",
        "# def vit_tiny(patch_size=16, **kwargs): VisionTransformer(patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, qkv_bias=True, **kwargs)\n",
        "# # d_head 64\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title model mask scheduler\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/train.py\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "patch_size= 1# 16\n",
        "crop_size=32 # 224\n",
        "\n",
        "encoder = VisionTransformer(img_size=[crop_size], patch_size=patch_size, embed_dim=32, depth=4, num_heads=4).to(device)\n",
        "predictor = VisionTransformerPredictor(num_patches=encoder.patch_embed.num_patches, embed_dim=encoder.embed_dim, predictor_embed_dim=12, depth=1, num_heads=3).to(device)\n",
        "import copy\n",
        "target_encoder = copy.deepcopy(encoder)\n",
        "for p in target_encoder.parameters(): p.requires_grad = False\n",
        "\n",
        "\n",
        "# mask_collator = MaskCollator(input_size=crop_size, patch_size=patch_size,\n",
        "#     pred_mask_scale=(.15,.2), enc_mask_scale=(.85,1.), aspect_ratio=(.75,1.5),\n",
        "#     nenc=1, npred=1, # 1,4\n",
        "#     allow_overlap=True, # paper:True, config:False\n",
        "#     min_keep=10)\n",
        "\n",
        "\n",
        "param_groups = [\n",
        "    {'params': (p for n, p in encoder.named_parameters() if ('bias' not in n) and (len(p.shape) != 1))},\n",
        "    {'params': (p for n, p in predictor.named_parameters() if ('bias' not in n) and (len(p.shape) != 1))},\n",
        "    {'params': (p for n, p in encoder.named_parameters() if ('bias' in n) or (len(p.shape) == 1)),\n",
        "        'WD_exclude': True, 'weight_decay': 0},\n",
        "    {'params': (p for n, p in predictor.named_parameters() if ('bias' in n) or (len(p.shape) == 1)),\n",
        "        'WD_exclude': True, 'weight_decay': 0}\n",
        "]\n",
        "optimizer = torch.optim.AdamW(param_groups)\n",
        "\n",
        "\n",
        "\n",
        "ipe = len(train_loader)*30\n",
        "ipe_scale = 1.  # scheduler scale factor (def: 1.0)\n",
        "num_epochs = 1 # 300\n",
        "# scheduler = WarmupCosineSchedule(optimizer, warmup_steps=int(40*ipe), start_lr=2e-4, ref_lr=1e-3, final_lr=1e-6, T_max=int(ipe_scale*num_epochs*ipe))\n",
        "# wd_scheduler = CosineWDSchedule(optimizer, ref_wd=0.04, final_wd=0.4, T_max=int(ipe_scale*num_epochs*ipe))\n",
        "# ema = (.996,1.)\n",
        "# momentum_scheduler = (ema[0] + i*(ema[1]-ema[0])/(ipe*num_epochs*ipe_scale) for i in range(int(ipe*num_epochs*ipe_scale)+1))\n",
        "\n",
        "tt_steps = num_epochs*ipe\n",
        "scheduler = WarmupCosineSchedule(optimizer, warmup_steps=40, start_lr=2e-4, ref_lr=1e-3, final_lr=1e-6, T_max=tt_steps)\n",
        "wd_scheduler = CosineWDSchedule(optimizer, ref_wd=0.04, final_wd=0.4, T_max=tt_steps)\n",
        "ema = (.996,1.)\n",
        "momentum_scheduler = (ema[0] + i*(ema[1]-ema[0])/tt_steps for i in range(tt_steps+1))\n",
        "\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(32).to(device)\n",
        "# classifier = Classifier(32, 18).to(device)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jWFzr3Z0a7wi"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title train\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/train.py\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def train(target_encoder, encoder, predictor, dataloader, optim, scheduler=None, wd_scheduler=None, momentum_scheduler=None):\n",
        "    target_encoder.train()\n",
        "    predictor.train()\n",
        "    # for i, (udata, masks_enc, masks_pred) in enumerate(dataloader):\n",
        "    for i, (imgs, y) in enumerate(dataloader):\n",
        "        # imgs = udata[0].to(device, non_blocking=True)\n",
        "        # masks_enc = [u.to(device, non_blocking=True) for u in masks_enc]\n",
        "        # masks_pred = [u.to(device, non_blocking=True) for u in masks_pred]\n",
        "\n",
        "        batch = imgs.shape[0]\n",
        "        imgs = imgs.to(device)#.to(torch.bfloat16)\n",
        "        masks_enc = multiblock2d((32,32), scale=(.85,1), aspect_ratio=(1,1), M=1).flatten() # [1,h,w], True->Mask\n",
        "        masks_pred = multiblock2d((32,32), scale=(.15,.2), aspect_ratio=(.75,1.5), M=4).any(0).flatten() # [1,h,w], True->Mask\n",
        "\n",
        "        masks_enc, masks_pred = masks_enc.nonzero().squeeze(-1).expand(batch,-1), masks_pred.nonzero().squeeze(-1).expand(batch,-1) # int idx [num_trg_toks] , idx of targets that are masked\n",
        "        masks_enc, masks_pred = [masks_enc.to(device)], [masks_pred.to(device)]\n",
        "\n",
        "        _new_lr = scheduler.step()\n",
        "        _new_wd = wd_scheduler.step()\n",
        "\n",
        "        # Step 1. Forward\n",
        "        with torch.cuda.amp.autocast(dtype=torch.bfloat16, enabled=True):\n",
        "            with torch.no_grad():\n",
        "                h = target_encoder(imgs)\n",
        "                h = F.layer_norm(h, (h.size(-1),))  # normalize over feature-dim\n",
        "                B = len(h)\n",
        "                # -- create targets (masked regions of h)\n",
        "                h = apply_masks(h, masks_pred)\n",
        "                h = repeat_interleave_batch(h, B, repeat=len(masks_enc))\n",
        "\n",
        "            z = encoder(imgs, masks_enc)\n",
        "            z = predictor(z, masks_enc, masks_pred)\n",
        "\n",
        "            loss = F.smooth_l1_loss(z, h)\n",
        "            # loss = F.mse_loss(z, h)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "        # loss.backward()\n",
        "        # optim.step()\n",
        "        optim.zero_grad()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            m = next(momentum_scheduler)\n",
        "            for param_q, param_k in zip(encoder.parameters(), target_encoder.parameters()):\n",
        "                param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "        print(\"strain\",loss.item())\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=50: break\n",
        "\n",
        "\n",
        "def ctrain(model, classifier, dataloader, coptim, scheduler=None):\n",
        "    model.eval()\n",
        "    classifier.train()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
        "            with torch.no_grad():\n",
        "                sx = model(x).detach().mean(1)\n",
        "            y_ = classifier(sx)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "\n",
        "        coptim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(coptim)\n",
        "        scaler.update()\n",
        "        print(\"classify\",loss.item())\n",
        "        try: wandb.log({\"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        # x = x.flatten(2).transpose(-2,-1)#.to(torch.bfloat16)\n",
        "        # x, y = x[...,1:].to(device).to(torch.float), y[1].to(device) # [batch, ] # (id, activity)\n",
        "        with torch.no_grad():\n",
        "            sx = model(x).mean(1)\n",
        "            y_ = classifier(sx)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        print(correct/len(y))\n",
        "        try: wandb.log({\"correct\": correct/len(y)})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "# for i in range(1):\n",
        "for i in range(1000):\n",
        "    # train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=mask_collator)\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "    train(target_encoder, encoder, predictor, train_loader, optimizer, scheduler, wd_scheduler, momentum_scheduler)\n",
        "    ctrain(target_encoder, classifier, train_loader, coptim)\n",
        "    test(target_encoder, classifier, test_loader)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "peselyVSYhgC",
        "outputId": "c37e4c3b-2d0c-40e3-a857-6c45c0fa70d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-002e30a5fe0f>:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "<ipython-input-34-002e30a5fe0f>:30: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.bfloat16, enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "0.15625\n",
            "0.15625\n",
            "0.140625\n",
            "0.15625\n",
            "strain 0.05917774513363838\n",
            "strain 0.0512780137360096\n",
            "strain 0.06702923774719238\n",
            "strain 0.07694786041975021\n",
            "strain 0.05473554879426956\n",
            "strain 0.05688923969864845\n",
            "strain 0.0549122616648674\n",
            "strain 0.0652696043252945\n",
            "strain 0.061489708721637726\n",
            "strain 0.05721943825483322\n",
            "strain 0.0527515783905983\n",
            "strain 0.05756057798862457\n",
            "strain 0.058595649898052216\n",
            "strain 0.0632408857345581\n",
            "strain 0.0550558902323246\n",
            "strain 0.06751485913991928\n",
            "strain 0.05967104062438011\n",
            "strain 0.059411682188510895\n",
            "strain 0.06520182639360428\n",
            "strain 0.07718334347009659\n",
            "strain 0.05774755775928497\n",
            "strain 0.06257352977991104\n",
            "strain 0.05866948515176773\n",
            "strain 0.05591804161667824\n",
            "strain 0.05392888933420181\n",
            "strain 0.07157948613166809\n",
            "strain 0.0692027360200882\n",
            "strain 0.07414358109235764\n",
            "strain 0.06527040153741837\n",
            "strain 0.05965424329042435\n",
            "strain 0.06073557585477829\n",
            "strain 0.057460904121398926\n",
            "strain 0.056713782250881195\n",
            "strain 0.05853532999753952\n",
            "strain 0.05697012320160866\n",
            "strain 0.06691924482584\n",
            "strain 0.06002776697278023\n",
            "strain 0.06302154809236526\n",
            "strain 0.07515040040016174\n",
            "strain 0.050609100610017776\n",
            "strain 0.0643419697880745\n",
            "strain 0.0647064819931984\n",
            "strain 0.05718859285116196\n",
            "strain 0.06044978275895119\n",
            "strain 0.062498655170202255\n",
            "strain 0.054902199655771255\n",
            "strain 0.05726335197687149\n",
            "strain 0.056398455053567886\n",
            "strain 0.06430604308843613\n",
            "strain 0.05164340138435364\n",
            "strain 0.0649631917476654\n",
            "classify 2.2623291015625\n",
            "classify 2.2554931640625\n",
            "classify 2.238037109375\n",
            "classify 2.26611328125\n",
            "classify 2.25\n",
            "classify 2.2562255859375\n",
            "classify 2.2969970703125\n",
            "classify 2.244384765625\n",
            "classify 2.2547607421875\n",
            "classify 2.254150390625\n",
            "classify 2.28173828125\n",
            "0.21875\n",
            "0.140625\n",
            "0.15625\n",
            "0.15625\n",
            "0.15625\n",
            "0.109375\n",
            "0.125\n",
            "0.109375\n",
            "0.125\n",
            "0.140625\n",
            "0.171875\n",
            "strain 0.06336387246847153\n",
            "strain 0.06103288009762764\n",
            "strain 0.06612355262041092\n",
            "strain 0.05717203766107559\n",
            "strain 0.06221548095345497\n",
            "strain 0.06108209118247032\n",
            "strain 0.0646384209394455\n",
            "strain 0.06195266172289848\n",
            "strain 0.06895969063043594\n",
            "strain 0.056045807898044586\n",
            "strain 0.05660218745470047\n",
            "strain 0.05468759685754776\n",
            "strain 0.05484090745449066\n",
            "strain 0.05860535055398941\n",
            "strain 0.0609108991920948\n",
            "strain 0.05548609420657158\n",
            "strain 0.0688534528017044\n",
            "strain 0.05202444642782211\n",
            "strain 0.06443987786769867\n",
            "strain 0.0594196543097496\n",
            "strain 0.05367318168282509\n",
            "strain 0.06065531075000763\n",
            "strain 0.06064172461628914\n",
            "strain 0.062057383358478546\n",
            "strain 0.06966657936573029\n",
            "strain 0.05843048170208931\n",
            "strain 0.058778081089258194\n",
            "strain 0.05110714212059975\n",
            "strain 0.05215759947896004\n",
            "strain 0.06422574818134308\n",
            "strain 0.05977378785610199\n",
            "strain 0.05833671987056732\n",
            "strain 0.0628616139292717\n",
            "strain 0.0677158385515213\n",
            "strain 0.05947102978825569\n",
            "strain 0.06502583622932434\n",
            "strain 0.058354999870061874\n",
            "strain 0.05626380071043968\n",
            "strain 0.0620972216129303\n",
            "strain 0.05380510538816452\n",
            "strain 0.057587385177612305\n",
            "strain 0.07238839566707611\n",
            "strain 0.06383220106363297\n",
            "strain 0.06356080621480942\n",
            "strain 0.046990517526865005\n",
            "strain 0.05744063854217529\n",
            "strain 0.06699308007955551\n",
            "strain 0.05918284133076668\n",
            "strain 0.05730421468615532\n",
            "strain 0.06640928983688354\n",
            "strain 0.06323406100273132\n",
            "classify 2.2239990234375\n",
            "classify 2.29736328125\n",
            "classify 2.2972412109375\n",
            "classify 2.305908203125\n",
            "classify 2.239990234375\n",
            "classify 2.2333984375\n",
            "classify 2.2674560546875\n",
            "classify 2.247314453125\n",
            "classify 2.2596435546875\n",
            "classify 2.2569580078125\n",
            "classify 2.2220458984375\n",
            "0.171875\n",
            "0.171875\n",
            "0.1875\n",
            "0.171875\n",
            "0.09375\n",
            "0.140625\n",
            "0.21875\n",
            "0.171875\n",
            "0.15625\n",
            "0.171875\n",
            "0.171875\n",
            "strain 0.058740582317113876\n",
            "strain 0.06701676547527313\n",
            "strain 0.06380138546228409\n",
            "strain 0.05093579366803169\n",
            "strain 0.06598645448684692\n",
            "strain 0.059974320232868195\n",
            "strain 0.06208888813853264\n",
            "strain 0.058503203094005585\n",
            "strain 0.06391861289739609\n",
            "strain 0.059733837842941284\n",
            "strain 0.05765322968363762\n",
            "strain 0.06525866687297821\n",
            "strain 0.06312689185142517\n",
            "strain 0.057462383061647415\n",
            "strain 0.06416528671979904\n",
            "strain 0.07209757715463638\n",
            "strain 0.06896514445543289\n",
            "strain 0.06776610016822815\n",
            "strain 0.059505511075258255\n",
            "strain 0.05324991047382355\n",
            "strain 0.0646376758813858\n",
            "strain 0.07252441346645355\n",
            "strain 0.05371534079313278\n",
            "strain 0.06981399655342102\n",
            "strain 0.057846229523420334\n",
            "strain 0.05918525159358978\n",
            "strain 0.06582340598106384\n",
            "strain 0.06160268187522888\n",
            "strain 0.059133972972631454\n",
            "strain 0.0718049556016922\n",
            "strain 0.05690450593829155\n",
            "strain 0.06984534859657288\n",
            "strain 0.0674322172999382\n",
            "strain 0.061590492725372314\n",
            "strain 0.0659857913851738\n",
            "strain 0.057630326598882675\n",
            "strain 0.05284786969423294\n",
            "strain 0.05773776024580002\n",
            "strain 0.06449724733829498\n",
            "strain 0.0605754517018795\n",
            "strain 0.06178669258952141\n",
            "strain 0.0616583526134491\n",
            "strain 0.05453205108642578\n",
            "strain 0.0590999573469162\n",
            "strain 0.0619368776679039\n",
            "strain 0.050381168723106384\n",
            "strain 0.06155787780880928\n",
            "strain 0.061485547572374344\n",
            "strain 0.05450358986854553\n",
            "strain 0.06244485825300217\n",
            "strain 0.058989182114601135\n",
            "classify 2.2646484375\n",
            "classify 2.255126953125\n",
            "classify 2.2330322265625\n",
            "classify 2.23876953125\n",
            "classify 2.2703857421875\n",
            "classify 2.275390625\n",
            "classify 2.2891845703125\n",
            "classify 2.2457275390625\n",
            "classify 2.2506103515625\n",
            "classify 2.256591796875\n",
            "classify 2.2496337890625\n",
            "0.140625\n",
            "0.140625\n",
            "0.21875\n",
            "0.140625\n",
            "0.125\n",
            "0.171875\n",
            "0.15625\n",
            "0.171875\n",
            "0.140625\n",
            "0.15625\n",
            "0.171875\n",
            "strain 0.06109309569001198\n",
            "strain 0.061373792588710785\n",
            "strain 0.06184207275509834\n",
            "strain 0.06336864829063416\n",
            "strain 0.06258925050497055\n",
            "strain 0.06208081543445587\n",
            "strain 0.06370464712381363\n",
            "strain 0.0684153139591217\n",
            "strain 0.05636105686426163\n",
            "strain 0.060705386102199554\n",
            "strain 0.05160211771726608\n",
            "strain 0.06363759189844131\n",
            "strain 0.06266110390424728\n",
            "strain 0.07244725525379181\n",
            "strain 0.06824442744255066\n",
            "strain 0.057308729737997055\n",
            "strain 0.05115135759115219\n",
            "strain 0.05633377656340599\n",
            "strain 0.061655815690755844\n",
            "strain 0.06395462155342102\n",
            "strain 0.05886917561292648\n",
            "strain 0.05782525986433029\n",
            "strain 0.06606545299291611\n",
            "strain 0.060003940016031265\n",
            "strain 0.06592176854610443\n",
            "strain 0.06155039370059967\n",
            "strain 0.056710194796323776\n",
            "strain 0.04836677759885788\n",
            "strain 0.06798861175775528\n",
            "strain 0.06299984455108643\n",
            "strain 0.057470329105854034\n",
            "strain 0.0600397065281868\n",
            "strain 0.05783230811357498\n",
            "strain 0.06502053141593933\n",
            "strain 0.06770186126232147\n",
            "strain 0.060892555862665176\n",
            "strain 0.055777717381715775\n",
            "strain 0.05998367816209793\n",
            "strain 0.06125845015048981\n",
            "strain 0.05260203778743744\n",
            "strain 0.061987850815057755\n",
            "strain 0.06251141428947449\n",
            "strain 0.059733159840106964\n",
            "strain 0.061836082488298416\n",
            "strain 0.04804352670907974\n",
            "strain 0.06405787914991379\n",
            "strain 0.06251157075166702\n",
            "strain 0.05674241855740547\n",
            "strain 0.05833597853779793\n",
            "strain 0.0639851838350296\n",
            "strain 0.06042797490954399\n",
            "classify 2.273193359375\n",
            "classify 2.2462158203125\n",
            "classify 2.2816162109375\n",
            "classify 2.291748046875\n",
            "classify 2.3134765625\n",
            "classify 2.2620849609375\n",
            "classify 2.263671875\n",
            "classify 2.2564697265625\n",
            "classify 2.279052734375\n",
            "classify 2.249755859375\n",
            "classify 2.245361328125\n",
            "0.15625\n",
            "0.046875\n",
            "0.1875\n",
            "0.171875\n",
            "0.078125\n",
            "0.203125\n",
            "0.140625\n",
            "0.1875\n",
            "0.171875\n",
            "0.09375\n",
            "0.140625\n",
            "strain 0.06315404176712036\n",
            "strain 0.07139741629362106\n",
            "strain 0.0668303593993187\n",
            "strain 0.05229488015174866\n",
            "strain 0.05992060527205467\n",
            "strain 0.06727895885705948\n",
            "strain 0.058595605194568634\n",
            "strain 0.0629744902253151\n",
            "strain 0.06715252995491028\n",
            "strain 0.06285550445318222\n",
            "strain 0.055722083896398544\n",
            "strain 0.0620080791413784\n",
            "strain 0.054037656635046005\n",
            "strain 0.05417570471763611\n",
            "strain 0.05592675507068634\n",
            "strain 0.06574428826570511\n",
            "strain 0.06430976092815399\n",
            "strain 0.052911460399627686\n",
            "strain 0.062073517590761185\n",
            "strain 0.0508432537317276\n",
            "strain 0.06770198047161102\n",
            "strain 0.06676416844129562\n",
            "strain 0.05746995657682419\n",
            "strain 0.061636101454496384\n",
            "strain 0.06453349441289902\n",
            "strain 0.05600046366453171\n",
            "strain 0.06598304212093353\n",
            "strain 0.05160479247570038\n",
            "strain 0.06722994893789291\n",
            "strain 0.05774417519569397\n",
            "strain 0.06792569160461426\n",
            "strain 0.06603662669658661\n",
            "strain 0.060344282537698746\n",
            "strain 0.0587034597992897\n",
            "strain 0.05442697927355766\n",
            "strain 0.058468643575906754\n",
            "strain 0.054610345512628555\n",
            "strain 0.06258610635995865\n",
            "strain 0.06856263428926468\n",
            "strain 0.06015162542462349\n",
            "strain 0.06455978006124496\n",
            "strain 0.05731359124183655\n",
            "strain 0.06780800223350525\n",
            "strain 0.05891432985663414\n",
            "strain 0.05670058727264404\n",
            "strain 0.06832925975322723\n",
            "strain 0.06940972059965134\n",
            "strain 0.0651162788271904\n",
            "strain 0.06448487937450409\n",
            "strain 0.0609266571700573\n",
            "strain 0.0662989541888237\n",
            "classify 2.2657470703125\n",
            "classify 2.27294921875\n",
            "classify 2.2708740234375\n",
            "classify 2.295654296875\n",
            "classify 2.2525634765625\n",
            "classify 2.3206787109375\n",
            "classify 2.248779296875\n",
            "classify 2.2615966796875\n",
            "classify 2.2421875\n",
            "classify 2.3004150390625\n",
            "classify 2.2659912109375\n",
            "0.171875\n",
            "0.15625\n",
            "0.140625\n",
            "0.140625\n",
            "0.125\n",
            "0.171875\n",
            "0.15625\n",
            "0.171875\n",
            "0.21875\n",
            "0.171875\n",
            "0.125\n",
            "strain 0.05538395419716835\n",
            "strain 0.07330767810344696\n",
            "strain 0.058073338121175766\n",
            "strain 0.05639679357409477\n",
            "strain 0.055107343941926956\n",
            "strain 0.0617845319211483\n",
            "strain 0.06727287173271179\n",
            "strain 0.060349155217409134\n",
            "strain 0.06345505267381668\n",
            "strain 0.05924965813755989\n",
            "strain 0.05571822449564934\n",
            "strain 0.060832999646663666\n",
            "strain 0.06259140372276306\n",
            "strain 0.05493234097957611\n",
            "strain 0.06973055750131607\n",
            "strain 0.06620292365550995\n",
            "strain 0.061593953520059586\n",
            "strain 0.06264548748731613\n",
            "strain 0.06102758273482323\n",
            "strain 0.05904553458094597\n",
            "strain 0.0670032873749733\n",
            "strain 0.061042871326208115\n",
            "strain 0.05884648486971855\n",
            "strain 0.05466119199991226\n",
            "strain 0.06547833234071732\n",
            "strain 0.06933324784040451\n",
            "strain 0.06172361969947815\n",
            "strain 0.05770673602819443\n",
            "strain 0.06381742656230927\n",
            "strain 0.05381624773144722\n",
            "strain 0.05977662280201912\n",
            "strain 0.06346557289361954\n",
            "strain 0.05589553713798523\n",
            "strain 0.05214186757802963\n",
            "strain 0.0665387436747551\n",
            "strain 0.05886564776301384\n",
            "strain 0.0504148006439209\n",
            "strain 0.0662354826927185\n",
            "strain 0.06304405629634857\n",
            "strain 0.053337644785642624\n",
            "strain 0.07888051867485046\n",
            "strain 0.05251793563365936\n",
            "strain 0.07944532483816147\n",
            "strain 0.05266807600855827\n",
            "strain 0.06190382316708565\n",
            "strain 0.06067618727684021\n",
            "strain 0.05996000021696091\n",
            "strain 0.06276698410511017\n",
            "strain 0.060408663004636765\n",
            "strain 0.056339241564273834\n",
            "strain 0.05664714053273201\n",
            "classify 2.27490234375\n",
            "classify 2.2818603515625\n",
            "classify 2.249267578125\n",
            "classify 2.2642822265625\n",
            "classify 2.2518310546875\n",
            "classify 2.2811279296875\n",
            "classify 2.256591796875\n",
            "classify 2.2662353515625\n",
            "classify 2.2685546875\n",
            "classify 2.26513671875\n",
            "classify 2.243896484375\n",
            "0.171875\n",
            "0.15625\n",
            "0.21875\n",
            "0.15625\n",
            "0.109375\n",
            "0.15625\n",
            "0.203125\n",
            "0.171875\n",
            "0.1875\n",
            "0.171875\n",
            "0.171875\n",
            "strain 0.06289124488830566\n",
            "strain 0.055013690143823624\n",
            "strain 0.06165749579668045\n",
            "strain 0.05683417618274689\n",
            "strain 0.05731258913874626\n",
            "strain 0.057867348194122314\n",
            "strain 0.06231531873345375\n",
            "strain 0.05951064079999924\n",
            "strain 0.059856582432985306\n",
            "strain 0.060253389179706573\n",
            "strain 0.05847180634737015\n",
            "strain 0.051385506987571716\n",
            "strain 0.05964132025837898\n",
            "strain 0.06275366246700287\n",
            "strain 0.06634742766618729\n",
            "strain 0.06513737142086029\n",
            "strain 0.057589802891016006\n",
            "strain 0.0642089918255806\n",
            "strain 0.05648306757211685\n",
            "strain 0.06473974138498306\n",
            "strain 0.05766459181904793\n",
            "strain 0.05612665042281151\n",
            "strain 0.06503894180059433\n",
            "strain 0.06778760254383087\n",
            "strain 0.06210147216916084\n",
            "strain 0.055377207696437836\n",
            "strain 0.07050848007202148\n",
            "strain 0.060137927532196045\n",
            "strain 0.06682471930980682\n",
            "strain 0.05334394797682762\n",
            "strain 0.06058275327086449\n",
            "strain 0.06678128242492676\n",
            "strain 0.06524012237787247\n",
            "strain 0.053606513887643814\n",
            "strain 0.05730777233839035\n",
            "strain 0.06344587355852127\n",
            "strain 0.07337887585163116\n",
            "strain 0.053346093744039536\n",
            "strain 0.06587719917297363\n",
            "strain 0.05960245430469513\n",
            "strain 0.05369017273187637\n",
            "strain 0.05771530419588089\n",
            "strain 0.061532653868198395\n",
            "strain 0.054406341165304184\n",
            "strain 0.05365432798862457\n",
            "strain 0.06801652908325195\n",
            "strain 0.056216686964035034\n",
            "strain 0.05583649128675461\n",
            "strain 0.05780281871557236\n",
            "strain 0.07483617216348648\n",
            "strain 0.05988415330648422\n",
            "classify 2.298828125\n",
            "classify 2.331787109375\n",
            "classify 2.277587890625\n",
            "classify 2.2537841796875\n",
            "classify 2.2664794921875\n",
            "classify 2.2962646484375\n",
            "classify 2.2374267578125\n",
            "classify 2.3048095703125\n",
            "classify 2.2703857421875\n",
            "classify 2.251220703125\n",
            "classify 2.257080078125\n",
            "0.171875\n",
            "0.078125\n",
            "0.234375\n",
            "0.1875\n",
            "0.140625\n",
            "0.140625\n",
            "0.140625\n",
            "0.1875\n",
            "0.203125\n",
            "0.1875\n",
            "0.140625\n",
            "strain 0.05490879341959953\n",
            "strain 0.06379249691963196\n",
            "strain 0.06774817407131195\n",
            "strain 0.06766377389431\n",
            "strain 0.0531163327395916\n",
            "strain 0.06897995620965958\n",
            "strain 0.06408701837062836\n",
            "strain 0.05932978168129921\n",
            "strain 0.061961155384778976\n",
            "strain 0.05453038588166237\n",
            "strain 0.0600772500038147\n",
            "strain 0.0851447731256485\n",
            "strain 0.07031399756669998\n",
            "strain 0.05424464866518974\n",
            "strain 0.06531108915805817\n",
            "strain 0.06725995987653732\n",
            "strain 0.058733440935611725\n",
            "strain 0.06134326383471489\n",
            "strain 0.06274935603141785\n",
            "strain 0.06345662474632263\n",
            "strain 0.06228095665574074\n",
            "strain 0.07344711571931839\n",
            "strain 0.05746603384613991\n",
            "strain 0.07103684544563293\n",
            "strain 0.06524217128753662\n",
            "strain 0.05801986902952194\n",
            "strain 0.06373005360364914\n",
            "strain 0.06295690685510635\n",
            "strain 0.053501661866903305\n",
            "strain 0.05107063055038452\n",
            "strain 0.051481157541275024\n",
            "strain 0.06026017665863037\n",
            "strain 0.05822541564702988\n",
            "strain 0.05784023553133011\n",
            "strain 0.05434078350663185\n",
            "strain 0.07082828134298325\n",
            "strain 0.05894919112324715\n",
            "strain 0.06517744064331055\n",
            "strain 0.06039459630846977\n",
            "strain 0.06357065588235855\n",
            "strain 0.06168704107403755\n",
            "strain 0.06252884119749069\n",
            "strain 0.06532344222068787\n",
            "strain 0.05495508387684822\n",
            "strain 0.0844699889421463\n",
            "strain 0.06782367825508118\n",
            "strain 0.0578908696770668\n",
            "strain 0.06502047181129456\n",
            "strain 0.06267876923084259\n",
            "strain 0.053907159715890884\n",
            "strain 0.068895623087883\n",
            "classify 2.282958984375\n",
            "classify 2.2764892578125\n",
            "classify 2.2542724609375\n",
            "classify 2.294921875\n",
            "classify 2.2802734375\n",
            "classify 2.2576904296875\n",
            "classify 2.280029296875\n",
            "classify 2.278076171875\n",
            "classify 2.2724609375\n",
            "classify 2.270263671875\n",
            "classify 2.2579345703125\n",
            "0.125\n",
            "0.078125\n",
            "0.265625\n",
            "0.125\n",
            "0.109375\n",
            "0.109375\n",
            "0.1875\n",
            "0.265625\n",
            "0.171875\n",
            "0.15625\n",
            "0.09375\n",
            "strain 0.06738140434026718\n",
            "strain 0.05455814301967621\n",
            "strain 0.06836388260126114\n",
            "strain 0.06095202639698982\n",
            "strain 0.05928976088762283\n",
            "strain 0.07663552463054657\n",
            "strain 0.05723758414387703\n",
            "strain 0.06373576074838638\n",
            "strain 0.07426200807094574\n",
            "strain 0.05836015194654465\n",
            "strain 0.0611160546541214\n",
            "strain 0.06398165225982666\n",
            "strain 0.05922810733318329\n",
            "strain 0.05653231218457222\n",
            "strain 0.05868629366159439\n",
            "strain 0.057699307799339294\n",
            "strain 0.06440041214227676\n",
            "strain 0.05553658306598663\n",
            "strain 0.07926683872938156\n",
            "strain 0.05507843196392059\n",
            "strain 0.055121421813964844\n",
            "strain 0.059493288397789\n",
            "strain 0.06147681921720505\n",
            "strain 0.06013529375195503\n",
            "strain 0.05349927395582199\n",
            "strain 0.06453898549079895\n",
            "strain 0.05592985078692436\n",
            "strain 0.07284307479858398\n",
            "strain 0.054472632706165314\n",
            "strain 0.06680137664079666\n",
            "strain 0.08530408143997192\n",
            "strain 0.05791637673974037\n",
            "strain 0.06406770646572113\n",
            "strain 0.05587808042764664\n",
            "strain 0.05793042480945587\n",
            "strain 0.06570055335760117\n",
            "strain 0.05621785297989845\n",
            "strain 0.0483429953455925\n",
            "strain 0.051142506301403046\n",
            "strain 0.06324903666973114\n",
            "strain 0.06761986017227173\n",
            "strain 0.059931911528110504\n",
            "strain 0.05944150313735008\n",
            "strain 0.06316562741994858\n",
            "strain 0.06423724442720413\n",
            "strain 0.05695948749780655\n",
            "strain 0.04943156987428665\n",
            "strain 0.05424046143889427\n",
            "strain 0.06652065366506577\n",
            "strain 0.06304728984832764\n",
            "strain 0.06198235973715782\n",
            "classify 2.26220703125\n",
            "classify 2.230712890625\n",
            "classify 2.29443359375\n",
            "classify 2.2513427734375\n",
            "classify 2.2567138671875\n",
            "classify 2.2467041015625\n",
            "classify 2.26318359375\n",
            "classify 2.2684326171875\n",
            "classify 2.2703857421875\n",
            "classify 2.2584228515625\n",
            "classify 2.2747802734375\n",
            "0.171875\n",
            "0.125\n",
            "0.203125\n",
            "0.171875\n",
            "0.140625\n",
            "0.1875\n",
            "0.15625\n",
            "0.171875\n",
            "0.171875\n",
            "0.125\n",
            "0.171875\n",
            "strain 0.06686249375343323\n",
            "strain 0.06958810240030289\n",
            "strain 0.07152984291315079\n",
            "strain 0.058948468416929245\n",
            "strain 0.05975668504834175\n",
            "strain 0.062126338481903076\n",
            "strain 0.06174873188138008\n",
            "strain 0.0621400885283947\n",
            "strain 0.058130308985710144\n",
            "strain 0.05673929676413536\n",
            "strain 0.06423967331647873\n",
            "strain 0.0641234889626503\n",
            "strain 0.06750454008579254\n",
            "strain 0.061184920370578766\n",
            "strain 0.05438341945409775\n",
            "strain 0.05752915143966675\n",
            "strain 0.0536922849714756\n",
            "strain 0.07542487233877182\n",
            "strain 0.06080832704901695\n",
            "strain 0.061255693435668945\n",
            "strain 0.06563960015773773\n",
            "strain 0.0530775710940361\n",
            "strain 0.059575144201517105\n",
            "strain 0.051844093948602676\n",
            "strain 0.05356848984956741\n",
            "strain 0.05549962446093559\n",
            "strain 0.061535026878118515\n",
            "strain 0.05678802356123924\n",
            "strain 0.05262519791722298\n",
            "strain 0.058198850601911545\n",
            "strain 0.055025938898324966\n",
            "strain 0.062329504638910294\n",
            "strain 0.056210972368717194\n",
            "strain 0.06400707364082336\n",
            "strain 0.066722072660923\n",
            "strain 0.059196408838033676\n",
            "strain 0.05774728208780289\n",
            "strain 0.06457151472568512\n",
            "strain 0.05955246090888977\n",
            "strain 0.06434166431427002\n",
            "strain 0.05462811142206192\n",
            "strain 0.06720665097236633\n",
            "strain 0.06200781464576721\n",
            "strain 0.05918819084763527\n",
            "strain 0.05773528292775154\n",
            "strain 0.0650220662355423\n",
            "strain 0.05359866842627525\n",
            "strain 0.0673564150929451\n",
            "strain 0.06513583660125732\n",
            "strain 0.054422974586486816\n",
            "strain 0.0594494454562664\n",
            "classify 2.25\n",
            "classify 2.2762451171875\n",
            "classify 2.2569580078125\n",
            "classify 2.2764892578125\n",
            "classify 2.25390625\n",
            "classify 2.280029296875\n",
            "classify 2.26318359375\n",
            "classify 2.26416015625\n",
            "classify 2.254638671875\n",
            "classify 2.26123046875\n",
            "classify 2.2369384765625\n",
            "0.140625\n",
            "0.0625\n",
            "0.203125\n",
            "0.15625\n",
            "0.125\n",
            "0.140625\n",
            "0.171875\n",
            "0.203125\n",
            "0.1875\n",
            "0.171875\n",
            "0.109375\n",
            "strain 0.05762288719415665\n",
            "strain 0.04871910437941551\n",
            "strain 0.05399306118488312\n",
            "strain 0.063064806163311\n",
            "strain 0.06656496971845627\n",
            "strain 0.06453189253807068\n",
            "strain 0.05319296568632126\n",
            "strain 0.05542316660284996\n",
            "strain 0.06871532648801804\n",
            "strain 0.06703977286815643\n",
            "strain 0.06515353173017502\n",
            "strain 0.05746561661362648\n",
            "strain 0.06095178797841072\n",
            "strain 0.06420200318098068\n",
            "strain 0.06334025412797928\n",
            "strain 0.0609116330742836\n",
            "strain 0.0918193981051445\n",
            "strain 0.05671461299061775\n",
            "strain 0.05927563086152077\n",
            "strain 0.06491877138614655\n",
            "strain 0.06048739701509476\n",
            "strain 0.061164747923612595\n",
            "strain 0.06362777203321457\n",
            "strain 0.05749820917844772\n",
            "strain 0.05854307860136032\n",
            "strain 0.05693136528134346\n",
            "strain 0.06643123924732208\n",
            "strain 0.06273534148931503\n",
            "strain 0.054844729602336884\n",
            "strain 0.05886399373412132\n",
            "strain 0.0594894215464592\n",
            "strain 0.053982991725206375\n",
            "strain 0.06491313129663467\n",
            "strain 0.07228013873100281\n",
            "strain 0.062106918543577194\n",
            "strain 0.06738319993019104\n",
            "strain 0.05819961428642273\n",
            "strain 0.04916071519255638\n",
            "strain 0.0658411979675293\n",
            "strain 0.055741116404533386\n",
            "strain 0.060010798275470734\n",
            "strain 0.05652332305908203\n",
            "strain 0.0686434954404831\n",
            "strain 0.06455957144498825\n",
            "strain 0.05828002840280533\n",
            "strain 0.06631109118461609\n",
            "strain 0.05680954083800316\n",
            "strain 0.048762015998363495\n",
            "strain 0.060770582407712936\n",
            "strain 0.062250036746263504\n",
            "strain 0.05891919136047363\n",
            "classify 2.2589111328125\n",
            "classify 2.277099609375\n",
            "classify 2.278076171875\n",
            "classify 2.251708984375\n",
            "classify 2.240478515625\n",
            "classify 2.2451171875\n",
            "classify 2.27490234375\n",
            "classify 2.267578125\n",
            "classify 2.2550048828125\n",
            "classify 2.2822265625\n",
            "classify 2.249755859375\n",
            "0.203125\n",
            "0.109375\n",
            "0.234375\n",
            "0.15625\n",
            "0.109375\n",
            "0.15625\n",
            "0.171875\n",
            "0.203125\n",
            "0.171875\n",
            "0.15625\n",
            "0.09375\n",
            "strain 0.05466986075043678\n",
            "strain 0.06366297602653503\n",
            "strain 0.05367272347211838\n",
            "strain 0.057667069137096405\n",
            "strain 0.05561824515461922\n",
            "strain 0.059745389968156815\n",
            "strain 0.06227792799472809\n",
            "strain 0.06194614619016647\n",
            "strain 0.05638729780912399\n",
            "strain 0.05783364176750183\n",
            "strain 0.05647917464375496\n",
            "strain 0.05629851669073105\n",
            "strain 0.05557912960648537\n",
            "strain 0.053452298045158386\n",
            "strain 0.06000682711601257\n",
            "strain 0.060786884278059006\n",
            "strain 0.0625438466668129\n",
            "strain 0.05371034890413284\n",
            "strain 0.06983722001314163\n",
            "strain 0.05357956886291504\n",
            "strain 0.0681481584906578\n",
            "strain 0.05465395003557205\n",
            "strain 0.05022135749459267\n",
            "strain 0.07005329430103302\n",
            "strain 0.06316651403903961\n",
            "strain 0.06318103522062302\n",
            "strain 0.06094388663768768\n",
            "strain 0.052483316510915756\n",
            "strain 0.06330427527427673\n",
            "strain 0.0636037141084671\n",
            "strain 0.06462155282497406\n",
            "strain 0.06119119003415108\n",
            "strain 0.06082678586244583\n",
            "strain 0.06205737590789795\n",
            "strain 0.0593641921877861\n",
            "strain 0.06613703817129135\n",
            "strain 0.08444703370332718\n",
            "strain 0.05917774885892868\n",
            "strain 0.04734731465578079\n",
            "strain 0.05442316085100174\n",
            "strain 0.052622467279434204\n",
            "strain 0.06045655161142349\n",
            "strain 0.05930681899189949\n",
            "strain 0.05849786475300789\n",
            "strain 0.07657221704721451\n",
            "strain 0.062043752521276474\n",
            "strain 0.06079543009400368\n",
            "strain 0.059996847063302994\n",
            "strain 0.0596405528485775\n",
            "strain 0.051814667880535126\n",
            "strain 0.058562759310007095\n",
            "classify 2.2808837890625\n",
            "classify 2.2650146484375\n",
            "classify 2.2508544921875\n",
            "classify 2.2789306640625\n",
            "classify 2.2086181640625\n",
            "classify 2.2535400390625\n",
            "classify 2.2886962890625\n",
            "classify 2.28271484375\n",
            "classify 2.267333984375\n",
            "classify 2.30908203125\n",
            "classify 2.2354736328125\n",
            "0.1875\n",
            "0.125\n",
            "0.203125\n",
            "0.171875\n",
            "0.125\n",
            "0.109375\n",
            "0.1875\n",
            "0.15625\n",
            "0.15625\n",
            "0.1875\n",
            "0.140625\n",
            "strain 0.05967672914266586\n",
            "strain 0.05235036090016365\n",
            "strain 0.046043191105127335\n",
            "strain 0.05204314365983009\n",
            "strain 0.05444328114390373\n",
            "strain 0.052108872681856155\n",
            "strain 0.058645714074373245\n",
            "strain 0.06506592035293579\n",
            "strain 0.05792136862874031\n",
            "strain 0.05847897008061409\n",
            "strain 0.05788031220436096\n",
            "strain 0.05909755453467369\n",
            "strain 0.05705687403678894\n",
            "strain 0.054537925869226456\n",
            "strain 0.057637784630060196\n",
            "strain 0.05941823124885559\n",
            "strain 0.0566796213388443\n",
            "strain 0.060950201004743576\n",
            "strain 0.0520426481962204\n",
            "strain 0.058937832713127136\n",
            "strain 0.06614658981561661\n",
            "strain 0.05825093016028404\n",
            "strain 0.062430109828710556\n",
            "strain 0.052297674119472504\n",
            "strain 0.06214118003845215\n",
            "strain 0.06137645244598389\n",
            "strain 0.06468142569065094\n",
            "strain 0.07581856101751328\n",
            "strain 0.06158558279275894\n",
            "strain 0.056861985474824905\n",
            "strain 0.06202240288257599\n",
            "strain 0.062411144375801086\n",
            "strain 0.06422250717878342\n",
            "strain 0.05601831153035164\n",
            "strain 0.05398733913898468\n",
            "strain 0.05401165038347244\n",
            "strain 0.06452609598636627\n",
            "strain 0.05820384621620178\n",
            "strain 0.06434958428144455\n",
            "strain 0.06707362085580826\n",
            "strain 0.05542726814746857\n",
            "strain 0.05698159337043762\n",
            "strain 0.06326466053724289\n",
            "strain 0.05465834587812424\n",
            "strain 0.05705171823501587\n",
            "strain 0.05918149650096893\n",
            "strain 0.05765342339873314\n",
            "strain 0.05541859567165375\n",
            "strain 0.056092921644449234\n",
            "strain 0.05950799956917763\n",
            "strain 0.05540255457162857\n",
            "classify 2.2930908203125\n",
            "classify 2.2894287109375\n",
            "classify 2.2969970703125\n",
            "classify 2.2969970703125\n",
            "classify 2.2720947265625\n",
            "classify 2.2406005859375\n",
            "classify 2.2490234375\n",
            "classify 2.234130859375\n",
            "classify 2.2783203125\n",
            "classify 2.2740478515625\n",
            "classify 2.286376953125\n",
            "0.203125\n",
            "0.078125\n",
            "0.1875\n",
            "0.1875\n",
            "0.125\n",
            "0.140625\n",
            "0.140625\n",
            "0.1875\n",
            "0.171875\n",
            "0.15625\n",
            "0.15625\n",
            "strain 0.05610742047429085\n",
            "strain 0.060408059507608414\n",
            "strain 0.06062481924891472\n",
            "strain 0.058729138225317\n",
            "strain 0.06000852212309837\n",
            "strain 0.05462925508618355\n",
            "strain 0.05866276100277901\n",
            "strain 0.06745723634958267\n",
            "strain 0.05213337019085884\n",
            "strain 0.06581082195043564\n",
            "strain 0.05951622128486633\n",
            "strain 0.07063088566064835\n",
            "strain 0.05262143909931183\n",
            "strain 0.08370626717805862\n",
            "strain 0.06662140041589737\n",
            "strain 0.053952548652887344\n",
            "strain 0.05757034569978714\n",
            "strain 0.06528784334659576\n",
            "strain 0.07187511771917343\n",
            "strain 0.060651298612356186\n",
            "strain 0.06267976015806198\n",
            "strain 0.060241084545850754\n",
            "strain 0.057081982493400574\n",
            "strain 0.05848679319024086\n",
            "strain 0.06261718273162842\n",
            "strain 0.06989212334156036\n",
            "strain 0.056193456053733826\n",
            "strain 0.05476461350917816\n",
            "strain 0.06259529292583466\n",
            "strain 0.06180667132139206\n",
            "strain 0.06523986160755157\n",
            "strain 0.06076579913496971\n",
            "strain 0.054194942116737366\n",
            "strain 0.0668099895119667\n",
            "strain 0.05686299875378609\n",
            "strain 0.05755208432674408\n",
            "strain 0.061828285455703735\n",
            "strain 0.06102331355214119\n",
            "strain 0.0662803128361702\n",
            "strain 0.05696837976574898\n",
            "strain 0.0641503557562828\n",
            "strain 0.060307879000902176\n",
            "strain 0.0723104178905487\n",
            "strain 0.06673183292150497\n",
            "strain 0.06216559559106827\n",
            "strain 0.06529072672128677\n",
            "strain 0.07032659649848938\n",
            "strain 0.05934811756014824\n",
            "strain 0.06404118984937668\n",
            "strain 0.05410067364573479\n",
            "strain 0.0670076385140419\n",
            "classify 2.2918701171875\n",
            "classify 2.26904296875\n",
            "classify 2.257568359375\n",
            "classify 2.3267822265625\n",
            "classify 2.2659912109375\n",
            "classify 2.24853515625\n",
            "classify 2.2646484375\n",
            "classify 2.2625732421875\n",
            "classify 2.29443359375\n",
            "classify 2.245849609375\n",
            "classify 2.284912109375\n",
            "0.15625\n",
            "0.109375\n",
            "0.171875\n",
            "0.140625\n",
            "0.15625\n",
            "0.171875\n",
            "0.15625\n",
            "0.1875\n",
            "0.140625\n",
            "0.1875\n",
            "0.140625\n",
            "strain 0.06525461375713348\n",
            "strain 0.06694836914539337\n",
            "strain 0.05145678296685219\n",
            "strain 0.06109516695141792\n",
            "strain 0.05977848544716835\n",
            "strain 0.053585465997457504\n",
            "strain 0.0637316033244133\n",
            "strain 0.07061100751161575\n",
            "strain 0.057352714240550995\n",
            "strain 0.07836781442165375\n",
            "strain 0.0596834160387516\n",
            "strain 0.060417648404836655\n",
            "strain 0.05434579774737358\n",
            "strain 0.052324675023555756\n",
            "strain 0.0684223547577858\n",
            "strain 0.06371759623289108\n",
            "strain 0.05886691436171532\n",
            "strain 0.05455409362912178\n",
            "strain 0.055906374007463455\n",
            "strain 0.06045547500252724\n",
            "strain 0.05206475406885147\n",
            "strain 0.05073721334338188\n",
            "strain 0.05939825624227524\n",
            "strain 0.05604170635342598\n",
            "strain 0.06117529422044754\n",
            "strain 0.058309365063905716\n",
            "strain 0.06009966507554054\n",
            "strain 0.06490933895111084\n",
            "strain 0.06542050838470459\n",
            "strain 0.05849968269467354\n",
            "strain 0.06539386510848999\n",
            "strain 0.0605756975710392\n",
            "strain 0.054348915815353394\n",
            "strain 0.05657082051038742\n",
            "strain 0.057410866022109985\n",
            "strain 0.054933637380599976\n",
            "strain 0.061753708869218826\n",
            "strain 0.05632399395108223\n",
            "strain 0.0689707100391388\n",
            "strain 0.05821424350142479\n",
            "strain 0.060209568589925766\n",
            "strain 0.05588184669613838\n",
            "strain 0.06746268272399902\n",
            "strain 0.05925016850233078\n",
            "strain 0.061665263026952744\n",
            "strain 0.0559072308242321\n",
            "strain 0.05541067570447922\n",
            "strain 0.05701765418052673\n",
            "strain 0.06787212938070297\n",
            "strain 0.06806769967079163\n",
            "strain 0.06387373805046082\n",
            "classify 2.2967529296875\n",
            "classify 2.2987060546875\n",
            "classify 2.25048828125\n",
            "classify 2.306396484375\n",
            "classify 2.2869873046875\n",
            "classify 2.2647705078125\n",
            "classify 2.2864990234375\n",
            "classify 2.246337890625\n",
            "classify 2.2880859375\n",
            "classify 2.275390625\n",
            "classify 2.2474365234375\n",
            "0.140625\n",
            "0.140625\n",
            "0.21875\n",
            "0.125\n",
            "0.140625\n",
            "0.140625\n",
            "0.15625\n",
            "0.1875\n",
            "0.203125\n",
            "0.15625\n",
            "0.125\n",
            "strain 0.060558658093214035\n",
            "strain 0.06333064287900925\n",
            "strain 0.05039617419242859\n",
            "strain 0.06130300834774971\n",
            "strain 0.0523906871676445\n",
            "strain 0.0618971586227417\n",
            "strain 0.06119295954704285\n",
            "strain 0.05165070295333862\n",
            "strain 0.05003642663359642\n",
            "strain 0.05875037610530853\n",
            "strain 0.054141223430633545\n",
            "strain 0.058694902807474136\n",
            "strain 0.05964692309498787\n",
            "strain 0.05678120255470276\n",
            "strain 0.06346111744642258\n",
            "strain 0.05562986806035042\n",
            "strain 0.05354289710521698\n",
            "strain 0.06975652277469635\n",
            "strain 0.05799517035484314\n",
            "strain 0.053898029029369354\n",
            "strain 0.06150589510798454\n",
            "strain 0.052982162684202194\n",
            "strain 0.05454198643565178\n",
            "strain 0.06645051389932632\n",
            "strain 0.06482356041669846\n",
            "strain 0.06525659561157227\n",
            "strain 0.057324305176734924\n",
            "strain 0.04998088255524635\n",
            "strain 0.06534560769796371\n",
            "strain 0.06510988622903824\n",
            "strain 0.05049103498458862\n",
            "strain 0.06423301994800568\n",
            "strain 0.06496638059616089\n",
            "strain 0.06928176432847977\n",
            "strain 0.0525747612118721\n",
            "strain 0.06955714523792267\n",
            "strain 0.05615398660302162\n",
            "strain 0.06311167031526566\n",
            "strain 0.06804310530424118\n",
            "strain 0.05542823672294617\n",
            "strain 0.057958945631980896\n",
            "strain 0.06343883275985718\n",
            "strain 0.062238629907369614\n",
            "strain 0.07115538418292999\n",
            "strain 0.057141393423080444\n",
            "strain 0.05779246240854263\n",
            "strain 0.07598955929279327\n",
            "strain 0.05769507959485054\n",
            "strain 0.05346866697072983\n",
            "strain 0.05971091613173485\n",
            "strain 0.05571381002664566\n",
            "classify 2.2708740234375\n",
            "classify 2.2838134765625\n",
            "classify 2.2762451171875\n",
            "classify 2.2562255859375\n",
            "classify 2.267333984375\n",
            "classify 2.2786865234375\n",
            "classify 2.27734375\n",
            "classify 2.23974609375\n",
            "classify 2.280517578125\n",
            "classify 2.25146484375\n",
            "classify 2.28271484375\n",
            "0.203125\n",
            "0.078125\n",
            "0.1875\n",
            "0.15625\n",
            "0.140625\n",
            "0.1875\n",
            "0.1875\n",
            "0.203125\n",
            "0.171875\n",
            "0.171875\n",
            "0.15625\n",
            "strain 0.06259243190288544\n",
            "strain 0.060803718864917755\n",
            "strain 0.06385036557912827\n",
            "strain 0.08304179459810257\n",
            "strain 0.06577424705028534\n",
            "strain 0.058120738714933395\n",
            "strain 0.05446182191371918\n",
            "strain 0.06742897629737854\n",
            "strain 0.075679711997509\n",
            "strain 0.062251873314380646\n",
            "strain 0.06280666589736938\n",
            "strain 0.06558313220739365\n",
            "strain 0.07705298811197281\n",
            "strain 0.05933449789881706\n",
            "strain 0.07002229243516922\n",
            "strain 0.050835106521844864\n",
            "strain 0.05687455087900162\n",
            "strain 0.056350674480199814\n",
            "strain 0.059848885983228683\n",
            "strain 0.06618840992450714\n",
            "strain 0.0716484859585762\n",
            "strain 0.06681433320045471\n",
            "strain 0.0526006706058979\n",
            "strain 0.06807732582092285\n",
            "strain 0.06529700756072998\n",
            "strain 0.05902858451008797\n",
            "strain 0.05752637982368469\n",
            "strain 0.062301844358444214\n",
            "strain 0.06516262888908386\n",
            "strain 0.0606066919863224\n",
            "strain 0.052766792476177216\n",
            "strain 0.055837489664554596\n",
            "strain 0.05707485228776932\n",
            "strain 0.06063040718436241\n",
            "strain 0.060585275292396545\n",
            "strain 0.05740899220108986\n",
            "strain 0.05501561239361763\n",
            "strain 0.061502061784267426\n",
            "strain 0.06978897005319595\n",
            "strain 0.05175492912530899\n",
            "strain 0.062357690185308456\n",
            "strain 0.04412457346916199\n",
            "strain 0.05773463845252991\n",
            "strain 0.0654589980840683\n",
            "strain 0.06517861038446426\n",
            "strain 0.04545583575963974\n",
            "strain 0.06254971772432327\n",
            "strain 0.060366302728652954\n",
            "strain 0.052662476897239685\n",
            "strain 0.06485321372747421\n",
            "strain 0.06260307878255844\n",
            "classify 2.2806396484375\n",
            "classify 2.296142578125\n",
            "classify 2.2750244140625\n",
            "classify 2.2586669921875\n",
            "classify 2.2669677734375\n",
            "classify 2.2509765625\n",
            "classify 2.266845703125\n",
            "classify 2.2525634765625\n",
            "classify 2.2860107421875\n",
            "classify 2.25048828125\n",
            "classify 2.23291015625\n",
            "0.109375\n",
            "0.078125\n",
            "0.21875\n",
            "0.125\n",
            "0.125\n",
            "0.171875\n",
            "0.140625\n",
            "0.15625\n",
            "0.171875\n",
            "0.171875\n",
            "0.125\n",
            "strain 0.056109342724084854\n",
            "strain 0.0569852851331234\n",
            "strain 0.055843107402324677\n",
            "strain 0.05589834973216057\n",
            "strain 0.05069206655025482\n",
            "strain 0.07471568882465363\n",
            "strain 0.06540300697088242\n",
            "strain 0.05947119742631912\n",
            "strain 0.05898742377758026\n",
            "strain 0.06093752384185791\n",
            "strain 0.054051630198955536\n",
            "strain 0.06843388825654984\n",
            "strain 0.058875344693660736\n",
            "strain 0.06294289976358414\n",
            "strain 0.0600619874894619\n",
            "strain 0.06759407371282578\n",
            "strain 0.05604957416653633\n",
            "strain 0.059194717556238174\n",
            "strain 0.06242794916033745\n",
            "strain 0.05843811109662056\n",
            "strain 0.06364654749631882\n",
            "strain 0.06123831868171692\n",
            "strain 0.06974520534276962\n",
            "strain 0.05978834256529808\n",
            "strain 0.06412553042173386\n",
            "strain 0.06271396577358246\n",
            "strain 0.04857187718153\n",
            "strain 0.057524070143699646\n",
            "strain 0.05597437918186188\n",
            "strain 0.05340928956866264\n",
            "strain 0.05392564833164215\n",
            "strain 0.07590238749980927\n",
            "strain 0.06306049972772598\n",
            "strain 0.06478405743837357\n",
            "strain 0.07224828749895096\n",
            "strain 0.06930067390203476\n",
            "strain 0.056260328739881516\n",
            "strain 0.05356030538678169\n",
            "strain 0.05806894972920418\n",
            "strain 0.06081240996718407\n",
            "strain 0.057371895760297775\n",
            "strain 0.049982860684394836\n",
            "strain 0.05234842747449875\n",
            "strain 0.055722933262586594\n",
            "strain 0.06039945036172867\n",
            "strain 0.06348278373479843\n",
            "strain 0.05240679159760475\n",
            "strain 0.06585739552974701\n",
            "strain 0.05933982506394386\n",
            "strain 0.06097210571169853\n",
            "strain 0.0506216399371624\n",
            "classify 2.2962646484375\n",
            "classify 2.30078125\n",
            "classify 2.2886962890625\n",
            "classify 2.2545166015625\n",
            "classify 2.281982421875\n",
            "classify 2.274658203125\n",
            "classify 2.290283203125\n",
            "classify 2.2850341796875\n",
            "classify 2.25634765625\n",
            "classify 2.2528076171875\n",
            "classify 2.307861328125\n",
            "0.140625\n",
            "0.109375\n",
            "0.203125\n",
            "0.125\n",
            "0.125\n",
            "0.140625\n",
            "0.171875\n",
            "0.15625\n",
            "0.15625\n",
            "0.203125\n",
            "0.171875\n",
            "strain 0.05411536246538162\n",
            "strain 0.05665578320622444\n",
            "strain 0.0648091584444046\n",
            "strain 0.06650885194540024\n",
            "strain 0.05747057497501373\n",
            "strain 0.059315040707588196\n",
            "strain 0.06414345651865005\n",
            "strain 0.06689242273569107\n",
            "strain 0.06132521852850914\n",
            "strain 0.05118400603532791\n",
            "strain 0.06600246578454971\n",
            "strain 0.05688861384987831\n",
            "strain 0.0571117028594017\n",
            "strain 0.0574645921587944\n",
            "strain 0.05761696398258209\n",
            "strain 0.05506736412644386\n",
            "strain 0.065557099878788\n",
            "strain 0.06127393618226051\n",
            "strain 0.05758395791053772\n",
            "strain 0.05334099754691124\n",
            "strain 0.060244254767894745\n",
            "strain 0.0736900344491005\n",
            "strain 0.06287835538387299\n",
            "strain 0.05496940389275551\n",
            "strain 0.05401105806231499\n",
            "strain 0.06829565763473511\n",
            "strain 0.05698588490486145\n",
            "strain 0.058255068957805634\n",
            "strain 0.0581970140337944\n",
            "strain 0.06240008398890495\n",
            "strain 0.06122906878590584\n",
            "strain 0.054972462356090546\n",
            "strain 0.05890318378806114\n",
            "strain 0.05007588863372803\n",
            "strain 0.05767226964235306\n",
            "strain 0.06404135376214981\n",
            "strain 0.06057150661945343\n",
            "strain 0.07545330375432968\n",
            "strain 0.07212787866592407\n",
            "strain 0.06487677991390228\n",
            "strain 0.06182434782385826\n",
            "strain 0.054534271359443665\n",
            "strain 0.06225316971540451\n",
            "strain 0.05677913501858711\n",
            "strain 0.05591621994972229\n",
            "strain 0.0627855435013771\n",
            "strain 0.0632924810051918\n",
            "strain 0.0586882010102272\n",
            "strain 0.05879451334476471\n",
            "strain 0.06790033727884293\n",
            "strain 0.05653950199484825\n",
            "classify 2.247314453125\n",
            "classify 2.2745361328125\n",
            "classify 2.265380859375\n",
            "classify 2.2535400390625\n",
            "classify 2.2469482421875\n",
            "classify 2.2403564453125\n",
            "classify 2.287841796875\n",
            "classify 2.2078857421875\n",
            "classify 2.312255859375\n",
            "classify 2.239990234375\n",
            "classify 2.2833251953125\n",
            "0.1875\n",
            "0.078125\n",
            "0.1875\n",
            "0.140625\n",
            "0.109375\n",
            "0.109375\n",
            "0.15625\n",
            "0.203125\n",
            "0.1875\n",
            "0.15625\n",
            "0.15625\n",
            "strain 0.053074609488248825\n",
            "strain 0.06267928332090378\n",
            "strain 0.06535550206899643\n",
            "strain 0.04659842699766159\n",
            "strain 0.06456176936626434\n",
            "strain 0.06030280888080597\n",
            "strain 0.055245980620384216\n",
            "strain 0.06263308972120285\n",
            "strain 0.05756322294473648\n",
            "strain 0.06138051301240921\n",
            "strain 0.05481700971722603\n",
            "strain 0.05553659051656723\n",
            "strain 0.06988979876041412\n",
            "strain 0.05451486259698868\n",
            "strain 0.07144124805927277\n",
            "strain 0.06456120312213898\n",
            "strain 0.05646097660064697\n",
            "strain 0.060864951461553574\n",
            "strain 0.057100810110569\n",
            "strain 0.06684087961912155\n",
            "strain 0.05948320031166077\n",
            "strain 0.059872232377529144\n",
            "strain 0.06387356668710709\n",
            "strain 0.05379827693104744\n",
            "strain 0.06310272216796875\n",
            "strain 0.05906730517745018\n",
            "strain 0.061917033046483994\n",
            "strain 0.05544546991586685\n",
            "strain 0.05775560438632965\n",
            "strain 0.06115288287401199\n",
            "strain 0.07096488028764725\n",
            "strain 0.05529022961854935\n",
            "strain 0.06467359513044357\n",
            "strain 0.0646025538444519\n",
            "strain 0.056928012520074844\n",
            "strain 0.062122851610183716\n",
            "strain 0.06263700127601624\n",
            "strain 0.06103071942925453\n",
            "strain 0.06365042924880981\n",
            "strain 0.06752731651067734\n",
            "strain 0.06237368658185005\n",
            "strain 0.06195695325732231\n",
            "strain 0.06812717765569687\n",
            "strain 0.0645265206694603\n",
            "strain 0.07035882771015167\n",
            "strain 0.051162298768758774\n",
            "strain 0.05152323842048645\n",
            "strain 0.04929081350564957\n",
            "strain 0.0544724203646183\n",
            "strain 0.06025282293558121\n",
            "strain 0.06127743795514107\n",
            "classify 2.2696533203125\n",
            "classify 2.27783203125\n",
            "classify 2.242919921875\n",
            "classify 2.2269287109375\n",
            "classify 2.289794921875\n",
            "classify 2.24560546875\n",
            "classify 2.2578125\n",
            "classify 2.29296875\n",
            "classify 2.28564453125\n",
            "classify 2.25537109375\n",
            "classify 2.2860107421875\n",
            "0.171875\n",
            "0.109375\n",
            "0.171875\n",
            "0.140625\n",
            "0.125\n",
            "0.109375\n",
            "0.171875\n",
            "0.21875\n",
            "0.125\n",
            "0.203125\n",
            "0.15625\n",
            "strain 0.05511189624667168\n",
            "strain 0.05322062596678734\n",
            "strain 0.061230823397636414\n",
            "strain 0.06372030824422836\n",
            "strain 0.05890310928225517\n",
            "strain 0.05517398938536644\n",
            "strain 0.0649556815624237\n",
            "strain 0.07067494839429855\n",
            "strain 0.05221707746386528\n",
            "strain 0.06002551317214966\n",
            "strain 0.05644683167338371\n",
            "strain 0.055665016174316406\n",
            "strain 0.06677456945180893\n",
            "strain 0.05916015803813934\n",
            "strain 0.06355129182338715\n",
            "strain 0.06634104251861572\n",
            "strain 0.061562538146972656\n",
            "strain 0.06288406997919083\n",
            "strain 0.044215068221092224\n",
            "strain 0.058846693485975266\n",
            "strain 0.06271763145923615\n",
            "strain 0.06445919722318649\n",
            "strain 0.05960673838853836\n",
            "strain 0.06221886724233627\n",
            "strain 0.07046669721603394\n",
            "strain 0.057043369859457016\n",
            "strain 0.06838488578796387\n",
            "strain 0.05389496684074402\n",
            "strain 0.06299103796482086\n",
            "strain 0.059916675090789795\n",
            "strain 0.05679994076490402\n",
            "strain 0.07729264348745346\n",
            "strain 0.058188389986753464\n",
            "strain 0.04928331822156906\n",
            "strain 0.05159042030572891\n",
            "strain 0.05476851388812065\n",
            "strain 0.060216132551431656\n",
            "strain 0.05292010307312012\n",
            "strain 0.05308198556303978\n",
            "strain 0.06144203245639801\n",
            "strain 0.059009965509176254\n",
            "strain 0.05798598751425743\n",
            "strain 0.07003368437290192\n",
            "strain 0.059183984994888306\n",
            "strain 0.06005199998617172\n",
            "strain 0.06579786539077759\n",
            "strain 0.05775311216711998\n",
            "strain 0.05290229246020317\n",
            "strain 0.0548340380191803\n",
            "strain 0.06117289140820503\n",
            "strain 0.06335262954235077\n",
            "classify 2.2886962890625\n",
            "classify 2.2579345703125\n",
            "classify 2.2669677734375\n",
            "classify 2.2750244140625\n",
            "classify 2.2733154296875\n",
            "classify 2.2606201171875\n",
            "classify 2.243408203125\n",
            "classify 2.25634765625\n",
            "classify 2.25146484375\n",
            "classify 2.2783203125\n",
            "classify 2.2978515625\n",
            "0.15625\n",
            "0.09375\n",
            "0.203125\n",
            "0.15625\n",
            "0.15625\n",
            "0.171875\n",
            "0.15625\n",
            "0.21875\n",
            "0.15625\n",
            "0.1875\n",
            "0.140625\n",
            "strain 0.05417469143867493\n",
            "strain 0.06342639029026031\n",
            "strain 0.05791598558425903\n",
            "strain 0.059510238468647\n",
            "strain 0.06150154024362564\n",
            "strain 0.06183987855911255\n",
            "strain 0.06764591485261917\n",
            "strain 0.05095876008272171\n",
            "strain 0.04561511427164078\n",
            "strain 0.04859409108757973\n",
            "strain 0.0667254775762558\n",
            "strain 0.06281275302171707\n",
            "strain 0.0661686509847641\n",
            "strain 0.06116999313235283\n",
            "strain 0.06128988042473793\n",
            "strain 0.06042766943573952\n",
            "strain 0.06313052028417587\n",
            "strain 0.06347452849149704\n",
            "strain 0.05534609034657478\n",
            "strain 0.07184429466724396\n",
            "strain 0.05770985409617424\n",
            "strain 0.06182553991675377\n",
            "strain 0.06096596643328667\n",
            "strain 0.05974268913269043\n",
            "strain 0.06121144816279411\n",
            "strain 0.06423281878232956\n",
            "strain 0.06586608290672302\n",
            "strain 0.05364925041794777\n",
            "strain 0.06564716249704361\n",
            "strain 0.077182337641716\n",
            "strain 0.057942572981119156\n",
            "strain 0.05612470582127571\n",
            "strain 0.0607781782746315\n",
            "strain 0.07306481152772903\n",
            "strain 0.053812239319086075\n",
            "strain 0.06511041522026062\n",
            "strain 0.06304476410150528\n",
            "strain 0.056290723383426666\n",
            "strain 0.059966545552015305\n",
            "strain 0.051599569618701935\n",
            "strain 0.05998307839035988\n",
            "strain 0.05706203356385231\n",
            "strain 0.059472259134054184\n",
            "strain 0.060816071927547455\n",
            "strain 0.05980297550559044\n",
            "strain 0.06283706426620483\n",
            "strain 0.0626135766506195\n",
            "strain 0.06198253855109215\n",
            "strain 0.05442750081419945\n",
            "strain 0.0616815946996212\n",
            "strain 0.06267335265874863\n",
            "classify 2.275634765625\n",
            "classify 2.29931640625\n",
            "classify 2.2257080078125\n",
            "classify 2.283447265625\n",
            "classify 2.3218994140625\n",
            "classify 2.2347412109375\n",
            "classify 2.2607421875\n",
            "classify 2.2957763671875\n",
            "classify 2.2625732421875\n",
            "classify 2.267822265625\n",
            "classify 2.2862548828125\n",
            "0.1875\n",
            "0.078125\n",
            "0.234375\n",
            "0.15625\n",
            "0.15625\n",
            "0.1875\n",
            "0.171875\n",
            "0.203125\n",
            "0.140625\n",
            "0.171875\n",
            "0.125\n",
            "strain 0.054203376173973083\n",
            "strain 0.06798578053712845\n",
            "strain 0.06292493641376495\n",
            "strain 0.05215272679924965\n",
            "strain 0.0583619549870491\n",
            "strain 0.06333781033754349\n",
            "strain 0.06145794317126274\n",
            "strain 0.06296335905790329\n",
            "strain 0.06178911030292511\n",
            "strain 0.057032715529203415\n",
            "strain 0.06537558138370514\n",
            "strain 0.06380875408649445\n",
            "strain 0.06616469472646713\n",
            "strain 0.06265359371900558\n",
            "strain 0.061498481780290604\n",
            "strain 0.05758817121386528\n",
            "strain 0.05761789530515671\n",
            "strain 0.05724833905696869\n",
            "strain 0.07324078679084778\n",
            "strain 0.057921525090932846\n",
            "strain 0.06043929606676102\n",
            "strain 0.06698034703731537\n",
            "strain 0.055446553975343704\n",
            "strain 0.06469619274139404\n",
            "strain 0.06030969321727753\n",
            "strain 0.05995616316795349\n",
            "strain 0.05119801312685013\n",
            "strain 0.05856936424970627\n",
            "strain 0.06298580765724182\n",
            "strain 0.06752569228410721\n",
            "strain 0.05397336184978485\n",
            "strain 0.061490681022405624\n",
            "strain 0.05781515687704086\n",
            "strain 0.06296002119779587\n",
            "strain 0.05443085730075836\n",
            "strain 0.050382643938064575\n",
            "strain 0.057977981865406036\n",
            "strain 0.060522567480802536\n",
            "strain 0.06030360609292984\n",
            "strain 0.05785223841667175\n",
            "strain 0.05994884669780731\n",
            "strain 0.053784459829330444\n",
            "strain 0.05908157676458359\n",
            "strain 0.05679306015372276\n",
            "strain 0.059949081391096115\n",
            "strain 0.052404358983039856\n",
            "strain 0.05030602589249611\n",
            "strain 0.06883640587329865\n",
            "strain 0.060705769807100296\n",
            "strain 0.06587367504835129\n",
            "strain 0.051549121737480164\n",
            "classify 2.31640625\n",
            "classify 2.2861328125\n",
            "classify 2.2939453125\n",
            "classify 2.2886962890625\n",
            "classify 2.27880859375\n",
            "classify 2.2760009765625\n",
            "classify 2.2803955078125\n",
            "classify 2.2950439453125\n",
            "classify 2.2950439453125\n",
            "classify 2.27685546875\n",
            "classify 2.2886962890625\n",
            "0.1875\n",
            "0.109375\n",
            "0.1875\n",
            "0.15625\n",
            "0.140625\n",
            "0.15625\n",
            "0.140625\n",
            "0.1875\n",
            "0.15625\n",
            "0.15625\n",
            "0.15625\n",
            "strain 0.07447775453329086\n",
            "strain 0.0611247718334198\n",
            "strain 0.05916778743267059\n",
            "strain 0.055286955088377\n",
            "strain 0.05040309578180313\n",
            "strain 0.04970027133822441\n",
            "strain 0.059205062687397\n",
            "strain 0.0561184361577034\n",
            "strain 0.05399365723133087\n",
            "strain 0.06281206011772156\n",
            "strain 0.051429301500320435\n",
            "strain 0.05211950093507767\n",
            "strain 0.05823281779885292\n",
            "strain 0.06864004582166672\n",
            "strain 0.05313622206449509\n",
            "strain 0.06224856525659561\n",
            "strain 0.06690739840269089\n",
            "strain 0.05085195600986481\n",
            "strain 0.04793021082878113\n",
            "strain 0.05468413606286049\n",
            "strain 0.061840370297431946\n",
            "strain 0.05913817510008812\n",
            "strain 0.0584302619099617\n",
            "strain 0.06459618359804153\n",
            "strain 0.04915153980255127\n",
            "strain 0.06207902729511261\n",
            "strain 0.057791274040937424\n",
            "strain 0.05831257998943329\n",
            "strain 0.060892872512340546\n",
            "strain 0.07367289811372757\n",
            "strain 0.04608888179063797\n",
            "strain 0.059072256088256836\n",
            "strain 0.060156144201755524\n",
            "strain 0.06152905523777008\n",
            "strain 0.051558587700128555\n",
            "strain 0.05021382495760918\n",
            "strain 0.060772597789764404\n",
            "strain 0.0571075938642025\n",
            "strain 0.06631925702095032\n",
            "strain 0.04812145605683327\n",
            "strain 0.06691889464855194\n",
            "strain 0.04757697135210037\n",
            "strain 0.06408575177192688\n",
            "strain 0.055853649973869324\n",
            "strain 0.055690035223960876\n",
            "strain 0.06313559412956238\n",
            "strain 0.06381329149007797\n",
            "strain 0.06317642331123352\n",
            "strain 0.055476196110248566\n",
            "strain 0.052555158734321594\n",
            "strain 0.055707234889268875\n",
            "classify 2.289306640625\n",
            "classify 2.2593994140625\n",
            "classify 2.3109130859375\n",
            "classify 2.297607421875\n",
            "classify 2.267578125\n",
            "classify 2.2501220703125\n",
            "classify 2.2613525390625\n",
            "classify 2.2613525390625\n",
            "classify 2.28955078125\n",
            "classify 2.246826171875\n",
            "classify 2.28857421875\n",
            "0.171875\n",
            "0.046875\n",
            "0.1875\n",
            "0.171875\n",
            "0.125\n",
            "0.109375\n",
            "0.15625\n",
            "0.21875\n",
            "0.203125\n",
            "0.171875\n",
            "0.109375\n",
            "strain 0.06330791115760803\n",
            "strain 0.06697273254394531\n",
            "strain 0.06243886053562164\n",
            "strain 0.05409350246191025\n",
            "strain 0.052067652344703674\n",
            "strain 0.05578717216849327\n",
            "strain 0.05449507758021355\n",
            "strain 0.06328140944242477\n",
            "strain 0.07073900103569031\n",
            "strain 0.05982770398259163\n",
            "strain 0.06610016524791718\n",
            "strain 0.05473572760820389\n",
            "strain 0.06465952098369598\n",
            "strain 0.061024121940135956\n",
            "strain 0.07027317583560944\n",
            "strain 0.0548771433532238\n",
            "strain 0.05474041402339935\n",
            "strain 0.0680990144610405\n",
            "strain 0.06747142225503922\n",
            "strain 0.060942016541957855\n",
            "strain 0.0522225983440876\n",
            "strain 0.05812589451670647\n",
            "strain 0.0628022626042366\n",
            "strain 0.06406004726886749\n",
            "strain 0.05875268951058388\n",
            "strain 0.05127504840493202\n",
            "strain 0.062137071043252945\n",
            "strain 0.059073712676763535\n",
            "strain 0.05246831104159355\n",
            "strain 0.0596141591668129\n",
            "strain 0.06301973760128021\n",
            "strain 0.05793589726090431\n",
            "strain 0.0682905912399292\n",
            "strain 0.05293566733598709\n",
            "strain 0.05451136827468872\n",
            "strain 0.05449209362268448\n",
            "strain 0.07537438720464706\n",
            "strain 0.06806688755750656\n",
            "strain 0.06327664852142334\n",
            "strain 0.06070763245224953\n",
            "strain 0.059311527758836746\n",
            "strain 0.06489863246679306\n",
            "strain 0.053186412900686264\n",
            "strain 0.06010052189230919\n",
            "strain 0.049835383892059326\n",
            "strain 0.06041356921195984\n",
            "strain 0.06320645660161972\n",
            "strain 0.0656890720129013\n",
            "strain 0.05724381282925606\n",
            "strain 0.0597122423350811\n",
            "strain 0.06481639295816422\n",
            "classify 2.2252197265625\n",
            "classify 2.236083984375\n",
            "classify 2.27978515625\n",
            "classify 2.313720703125\n",
            "classify 2.3060302734375\n",
            "classify 2.28759765625\n",
            "classify 2.270263671875\n",
            "classify 2.2108154296875\n",
            "classify 2.288330078125\n",
            "classify 2.291015625\n",
            "classify 2.23828125\n",
            "0.15625\n",
            "0.0625\n",
            "0.1875\n",
            "0.15625\n",
            "0.15625\n",
            "0.171875\n",
            "0.203125\n",
            "0.203125\n",
            "0.171875\n",
            "0.171875\n",
            "0.109375\n",
            "strain 0.04988519847393036\n",
            "strain 0.05831701681017876\n",
            "strain 0.04960797354578972\n",
            "strain 0.05930182710289955\n",
            "strain 0.05676457658410072\n",
            "strain 0.058260221034288406\n",
            "strain 0.06135815754532814\n",
            "strain 0.06182501092553139\n",
            "strain 0.054097797721624374\n",
            "strain 0.06110086664557457\n",
            "strain 0.058917440474033356\n",
            "strain 0.055641040205955505\n",
            "strain 0.05421938747167587\n",
            "strain 0.06364540010690689\n",
            "strain 0.058478403836488724\n",
            "strain 0.06452307850122452\n",
            "strain 0.06187004968523979\n",
            "strain 0.06627766788005829\n",
            "strain 0.05490565672516823\n",
            "strain 0.0644119456410408\n",
            "strain 0.05622830241918564\n",
            "strain 0.07254105806350708\n",
            "strain 0.06808710843324661\n",
            "strain 0.058469172567129135\n",
            "strain 0.055697761476039886\n",
            "strain 0.05852215364575386\n",
            "strain 0.06985792517662048\n",
            "strain 0.06316516548395157\n",
            "strain 0.06519699841737747\n",
            "strain 0.06371571123600006\n",
            "strain 0.05090198665857315\n",
            "strain 0.07041656225919724\n",
            "strain 0.08098123222589493\n",
            "strain 0.058909881860017776\n",
            "strain 0.05553761124610901\n",
            "strain 0.060302890837192535\n",
            "strain 0.05641816183924675\n",
            "strain 0.07130036503076553\n",
            "strain 0.057261306792497635\n",
            "strain 0.06026635318994522\n",
            "strain 0.05924520641565323\n",
            "strain 0.06002771109342575\n",
            "strain 0.05868334695696831\n",
            "strain 0.05192772299051285\n",
            "strain 0.05664549022912979\n",
            "strain 0.05946706607937813\n",
            "strain 0.05629171058535576\n",
            "strain 0.059658635407686234\n",
            "strain 0.05169080197811127\n",
            "strain 0.05951113998889923\n",
            "strain 0.05306355655193329\n",
            "classify 2.2886962890625\n",
            "classify 2.31396484375\n",
            "classify 2.2357177734375\n",
            "classify 2.258056640625\n",
            "classify 2.2623291015625\n",
            "classify 2.23779296875\n",
            "classify 2.2322998046875\n",
            "classify 2.27978515625\n",
            "classify 2.28369140625\n",
            "classify 2.262939453125\n",
            "classify 2.283447265625\n",
            "0.171875\n",
            "0.078125\n",
            "0.1875\n",
            "0.15625\n",
            "0.125\n",
            "0.171875\n",
            "0.1875\n",
            "0.171875\n",
            "0.140625\n",
            "0.171875\n",
            "0.140625\n",
            "strain 0.06210381165146828\n",
            "strain 0.06695889681577682\n",
            "strain 0.0522163100540638\n",
            "strain 0.05131618306040764\n",
            "strain 0.05905252695083618\n",
            "strain 0.07155124098062515\n",
            "strain 0.06486567854881287\n",
            "strain 0.054265305399894714\n",
            "strain 0.06067502126097679\n",
            "strain 0.05498364195227623\n",
            "strain 0.06368806958198547\n",
            "strain 0.062367189675569534\n",
            "strain 0.06838592141866684\n",
            "strain 0.06833342462778091\n",
            "strain 0.05418610945343971\n",
            "strain 0.05865105241537094\n",
            "strain 0.06095530837774277\n",
            "strain 0.06088580563664436\n",
            "strain 0.07156539708375931\n",
            "strain 0.06593012064695358\n",
            "strain 0.060022275894880295\n",
            "strain 0.06011471897363663\n",
            "strain 0.05784803256392479\n",
            "strain 0.05838171765208244\n",
            "strain 0.05499738082289696\n",
            "strain 0.061491236090660095\n",
            "strain 0.06658010929822922\n",
            "strain 0.06145571544766426\n",
            "strain 0.05814847722649574\n",
            "strain 0.05565927177667618\n",
            "strain 0.06820521503686905\n",
            "strain 0.06348829716444016\n",
            "strain 0.0609247125685215\n",
            "strain 0.051498789340257645\n",
            "strain 0.06102745607495308\n",
            "strain 0.059225041419267654\n",
            "strain 0.06247209757566452\n",
            "strain 0.06767997145652771\n",
            "strain 0.06323815882205963\n",
            "strain 0.043192289769649506\n",
            "strain 0.05871768295764923\n",
            "strain 0.05559539794921875\n",
            "strain 0.053580936044454575\n",
            "strain 0.060477424412965775\n",
            "strain 0.0581284835934639\n",
            "strain 0.06272850930690765\n",
            "strain 0.05312255024909973\n",
            "strain 0.07417616993188858\n",
            "strain 0.060745708644390106\n",
            "strain 0.057481929659843445\n",
            "strain 0.0637543722987175\n",
            "classify 2.26318359375\n",
            "classify 2.264404296875\n",
            "classify 2.26025390625\n",
            "classify 2.28271484375\n",
            "classify 2.2410888671875\n",
            "classify 2.24951171875\n",
            "classify 2.2445068359375\n",
            "classify 2.2545166015625\n",
            "classify 2.2529296875\n",
            "classify 2.2408447265625\n",
            "classify 2.273681640625\n",
            "0.140625\n",
            "0.078125\n",
            "0.1875\n",
            "0.171875\n",
            "0.125\n",
            "0.140625\n",
            "0.125\n",
            "0.203125\n",
            "0.28125\n",
            "0.140625\n",
            "0.15625\n",
            "strain 0.04917206987738609\n",
            "strain 0.05947672948241234\n",
            "strain 0.06608790159225464\n",
            "strain 0.06201724708080292\n",
            "strain 0.05637430027127266\n",
            "strain 0.051289282739162445\n",
            "strain 0.05230177938938141\n",
            "strain 0.06958473473787308\n",
            "strain 0.06427890807390213\n",
            "strain 0.058873552829027176\n",
            "strain 0.0631675198674202\n",
            "strain 0.05385003611445427\n",
            "strain 0.060359083116054535\n",
            "strain 0.0575166717171669\n",
            "strain 0.057877685874700546\n",
            "strain 0.05126781016588211\n",
            "strain 0.059253159910440445\n",
            "strain 0.060670606791973114\n",
            "strain 0.05528682842850685\n",
            "strain 0.05753115564584732\n",
            "strain 0.05307754874229431\n",
            "strain 0.06345009058713913\n",
            "strain 0.053576696664094925\n",
            "strain 0.061206843703985214\n",
            "strain 0.060172658413648605\n",
            "strain 0.06311451643705368\n",
            "strain 0.06654603779315948\n",
            "strain 0.06874064356088638\n",
            "strain 0.055620089173316956\n",
            "strain 0.05919434502720833\n",
            "strain 0.057651765644550323\n",
            "strain 0.05565488338470459\n",
            "strain 0.06446459144353867\n",
            "strain 0.06432750076055527\n",
            "strain 0.05725899338722229\n",
            "strain 0.06712061166763306\n",
            "strain 0.05234408751130104\n",
            "strain 0.07489616423845291\n",
            "strain 0.06321528553962708\n",
            "strain 0.05723697319626808\n",
            "strain 0.06044101342558861\n",
            "strain 0.056828487664461136\n",
            "strain 0.057216089218854904\n",
            "strain 0.058827612549066544\n",
            "strain 0.04490762576460838\n",
            "strain 0.06254890561103821\n",
            "strain 0.05371050909161568\n",
            "strain 0.06600995361804962\n",
            "strain 0.05393286049365997\n",
            "strain 0.062268517911434174\n",
            "strain 0.05368485301733017\n",
            "classify 2.27294921875\n",
            "classify 2.2657470703125\n",
            "classify 2.3114013671875\n",
            "classify 2.290283203125\n",
            "classify 2.3052978515625\n",
            "classify 2.2899169921875\n",
            "classify 2.306396484375\n",
            "classify 2.21142578125\n",
            "classify 2.22802734375\n",
            "classify 2.258056640625\n",
            "classify 2.2591552734375\n",
            "0.171875\n",
            "0.078125\n",
            "0.21875\n",
            "0.171875\n",
            "0.15625\n",
            "0.140625\n",
            "0.1875\n",
            "0.1875\n",
            "0.171875\n",
            "0.1875\n",
            "0.09375\n",
            "strain 0.0600752979516983\n",
            "strain 0.05248717591166496\n",
            "strain 0.0670754536986351\n",
            "strain 0.05835216864943504\n",
            "strain 0.0629013329744339\n",
            "strain 0.05946798622608185\n",
            "strain 0.06073874235153198\n",
            "strain 0.07217276096343994\n",
            "strain 0.05871431156992912\n",
            "strain 0.04740627110004425\n",
            "strain 0.052251189947128296\n",
            "strain 0.06155966594815254\n",
            "strain 0.06106545776128769\n",
            "strain 0.07426314800977707\n",
            "strain 0.05002375319600105\n",
            "strain 0.05663711950182915\n",
            "strain 0.06003866344690323\n",
            "strain 0.06305006891489029\n",
            "strain 0.05970895662903786\n",
            "strain 0.06390088051557541\n",
            "strain 0.0642438679933548\n",
            "strain 0.056455716490745544\n",
            "strain 0.05757419392466545\n",
            "strain 0.0557347796857357\n",
            "strain 0.0617394857108593\n",
            "strain 0.05765832960605621\n",
            "strain 0.06101732328534126\n",
            "strain 0.06613240391016006\n",
            "strain 0.05544513836503029\n",
            "strain 0.062398478388786316\n",
            "strain 0.06057457998394966\n",
            "strain 0.05778980627655983\n",
            "strain 0.055389001965522766\n",
            "strain 0.058176230639219284\n",
            "strain 0.05565829947590828\n",
            "strain 0.06954220682382584\n",
            "strain 0.06166238337755203\n",
            "strain 0.0679524689912796\n",
            "strain 0.05783463269472122\n",
            "strain 0.07413014024496078\n",
            "strain 0.05702003836631775\n",
            "strain 0.05182931572198868\n",
            "strain 0.05888856574892998\n",
            "strain 0.05563250929117203\n",
            "strain 0.05730793625116348\n",
            "strain 0.0615835003554821\n",
            "strain 0.05548964440822601\n",
            "strain 0.06490341573953629\n",
            "strain 0.05818033590912819\n",
            "strain 0.053097669035196304\n",
            "strain 0.06403204053640366\n",
            "classify 2.2568359375\n",
            "classify 2.2327880859375\n",
            "classify 2.295654296875\n",
            "classify 2.2945556640625\n",
            "classify 2.2490234375\n",
            "classify 2.312744140625\n",
            "classify 2.2803955078125\n",
            "classify 2.271728515625\n",
            "classify 2.264892578125\n",
            "classify 2.26220703125\n",
            "classify 2.2784423828125\n",
            "0.140625\n",
            "0.109375\n",
            "0.1875\n",
            "0.1875\n",
            "0.15625\n",
            "0.140625\n",
            "0.21875\n",
            "0.125\n",
            "0.1875\n",
            "0.1875\n",
            "0.109375\n",
            "strain 0.05555297061800957\n",
            "strain 0.060372862964868546\n",
            "strain 0.057020656764507294\n",
            "strain 0.05706104263663292\n",
            "strain 0.057180244475603104\n",
            "strain 0.06915763020515442\n",
            "strain 0.06711607426404953\n",
            "strain 0.05725199356675148\n",
            "strain 0.05852586776018143\n",
            "strain 0.05401894450187683\n",
            "strain 0.06275051087141037\n",
            "strain 0.06516434252262115\n",
            "strain 0.0615764856338501\n",
            "strain 0.05521190166473389\n",
            "strain 0.05103933811187744\n",
            "strain 0.05659836158156395\n",
            "strain 0.060645394027233124\n",
            "strain 0.058578480035066605\n",
            "strain 0.05566328018903732\n",
            "strain 0.05862024053931236\n",
            "strain 0.06138615310192108\n",
            "strain 0.06766387075185776\n",
            "strain 0.05084932968020439\n",
            "strain 0.063397116959095\n",
            "strain 0.054250381886959076\n",
            "strain 0.06599196046590805\n",
            "strain 0.06752375513315201\n",
            "strain 0.06173790618777275\n",
            "strain 0.05944930016994476\n",
            "strain 0.05812136456370354\n",
            "strain 0.06255142390727997\n",
            "strain 0.05431969091296196\n",
            "strain 0.052108705043792725\n",
            "strain 0.054150767624378204\n",
            "strain 0.07077100872993469\n",
            "strain 0.055789243429899216\n",
            "strain 0.05891270190477371\n",
            "strain 0.06384555995464325\n",
            "strain 0.06030169874429703\n",
            "strain 0.055726855993270874\n",
            "strain 0.05716708302497864\n",
            "strain 0.05418829619884491\n",
            "strain 0.05946468189358711\n",
            "strain 0.06701934337615967\n",
            "strain 0.05289796367287636\n",
            "strain 0.06810490041971207\n",
            "strain 0.05589683726429939\n",
            "strain 0.05555648356676102\n",
            "strain 0.06070050224661827\n",
            "strain 0.05500580370426178\n",
            "strain 0.060341380536556244\n",
            "classify 2.271728515625\n",
            "classify 2.273681640625\n",
            "classify 2.3226318359375\n",
            "classify 2.297607421875\n",
            "classify 2.254638671875\n",
            "classify 2.261474609375\n",
            "classify 2.259521484375\n",
            "classify 2.235595703125\n",
            "classify 2.2431640625\n",
            "classify 2.243896484375\n",
            "classify 2.2802734375\n",
            "0.1875\n",
            "0.078125\n",
            "0.1875\n",
            "0.140625\n",
            "0.109375\n",
            "0.21875\n",
            "0.15625\n",
            "0.15625\n",
            "0.15625\n",
            "0.1875\n",
            "0.109375\n",
            "strain 0.0737634152173996\n",
            "strain 0.057168345898389816\n",
            "strain 0.06777369230985641\n",
            "strain 0.0604839101433754\n",
            "strain 0.05806159973144531\n",
            "strain 0.053781598806381226\n",
            "strain 0.06991253048181534\n",
            "strain 0.056959934532642365\n",
            "strain 0.06521394848823547\n",
            "strain 0.059371963143348694\n",
            "strain 0.048902563750743866\n",
            "strain 0.05411021411418915\n",
            "strain 0.0634845718741417\n",
            "strain 0.051911860704422\n",
            "strain 0.05653341859579086\n",
            "strain 0.05865779146552086\n",
            "strain 0.06212993711233139\n",
            "strain 0.05383598059415817\n",
            "strain 0.05491336062550545\n",
            "strain 0.057436175644397736\n",
            "strain 0.05922047421336174\n",
            "strain 0.06785020232200623\n",
            "strain 0.05537411570549011\n",
            "strain 0.05917799845337868\n",
            "strain 0.06046435236930847\n",
            "strain 0.05351853370666504\n",
            "strain 0.055370014160871506\n",
            "strain 0.07258867472410202\n",
            "strain 0.051863931119441986\n",
            "strain 0.056639380753040314\n",
            "strain 0.06401391327381134\n",
            "strain 0.059619251638650894\n",
            "strain 0.059377722442150116\n",
            "strain 0.06124257296323776\n",
            "strain 0.0702173039317131\n",
            "strain 0.060816455632448196\n",
            "strain 0.06001540645956993\n",
            "strain 0.05452564358711243\n",
            "strain 0.05927027761936188\n",
            "strain 0.05338871479034424\n",
            "strain 0.05769065022468567\n",
            "strain 0.059063613414764404\n",
            "strain 0.053174182772636414\n",
            "strain 0.05849049240350723\n",
            "strain 0.057158201932907104\n",
            "strain 0.06255904585123062\n",
            "strain 0.057659100741147995\n",
            "strain 0.06434500962495804\n",
            "strain 0.06554101407527924\n",
            "strain 0.05855116620659828\n",
            "strain 0.05717582255601883\n",
            "classify 2.270263671875\n",
            "classify 2.261962890625\n",
            "classify 2.27197265625\n",
            "classify 2.266845703125\n",
            "classify 2.27490234375\n",
            "classify 2.28955078125\n",
            "classify 2.29248046875\n",
            "classify 2.304931640625\n",
            "classify 2.2677001953125\n",
            "classify 2.244384765625\n",
            "classify 2.294677734375\n",
            "0.171875\n",
            "0.109375\n",
            "0.234375\n",
            "0.171875\n",
            "0.140625\n",
            "0.15625\n",
            "0.171875\n",
            "0.171875\n",
            "0.21875\n",
            "0.15625\n",
            "0.125\n",
            "strain 0.05271364375948906\n",
            "strain 0.0568307600915432\n",
            "strain 0.05565430223941803\n",
            "strain 0.06377916783094406\n",
            "strain 0.06720122694969177\n",
            "strain 0.05978824943304062\n",
            "strain 0.05982842296361923\n",
            "strain 0.061615560203790665\n",
            "strain 0.05078980326652527\n",
            "strain 0.0608636774122715\n",
            "strain 0.04951705038547516\n",
            "strain 0.058782756328582764\n",
            "strain 0.05758637189865112\n",
            "strain 0.05181600898504257\n",
            "strain 0.04694562405347824\n",
            "strain 0.0603913776576519\n",
            "strain 0.05604608356952667\n",
            "strain 0.05341143161058426\n",
            "strain 0.06495624035596848\n",
            "strain 0.057802606374025345\n",
            "strain 0.05310185253620148\n",
            "strain 0.05788130685687065\n",
            "strain 0.06001118943095207\n",
            "strain 0.05510601028800011\n",
            "strain 0.06183851137757301\n",
            "strain 0.05367860943078995\n",
            "strain 0.0649491623044014\n",
            "strain 0.05659952387213707\n",
            "strain 0.06003917008638382\n",
            "strain 0.058834489434957504\n",
            "strain 0.05772487074136734\n",
            "strain 0.058439403772354126\n",
            "strain 0.061835553497076035\n",
            "strain 0.057610876858234406\n",
            "strain 0.0591624416410923\n",
            "strain 0.06893233209848404\n",
            "strain 0.059776000678539276\n",
            "strain 0.06006302684545517\n",
            "strain 0.06812059879302979\n",
            "strain 0.051173996180295944\n",
            "strain 0.05647912621498108\n",
            "strain 0.04817003384232521\n",
            "strain 0.060043517500162125\n",
            "strain 0.07180625200271606\n",
            "strain 0.06601886451244354\n",
            "strain 0.05639911815524101\n",
            "strain 0.05680505931377411\n",
            "strain 0.06013744696974754\n",
            "strain 0.048551205545663834\n",
            "strain 0.06202762946486473\n",
            "strain 0.05723663792014122\n",
            "classify 2.24755859375\n",
            "classify 2.278564453125\n",
            "classify 2.273681640625\n",
            "classify 2.2960205078125\n",
            "classify 2.2958984375\n",
            "classify 2.2823486328125\n",
            "classify 2.2757568359375\n",
            "classify 2.2821044921875\n",
            "classify 2.2548828125\n",
            "classify 2.2796630859375\n",
            "classify 2.22900390625\n",
            "0.15625\n",
            "0.125\n",
            "0.25\n",
            "0.171875\n",
            "0.15625\n",
            "0.109375\n",
            "0.09375\n",
            "0.1875\n",
            "0.203125\n",
            "0.140625\n",
            "0.140625\n",
            "strain 0.06883585453033447\n",
            "strain 0.061612192541360855\n",
            "strain 0.06231380254030228\n",
            "strain 0.06350358575582504\n",
            "strain 0.05328501760959625\n",
            "strain 0.05547768995165825\n",
            "strain 0.059627026319503784\n",
            "strain 0.05318257212638855\n",
            "strain 0.05315140634775162\n",
            "strain 0.0678149163722992\n",
            "strain 0.05398853123188019\n",
            "strain 0.04929223656654358\n",
            "strain 0.05360256880521774\n",
            "strain 0.048136886209249496\n",
            "strain 0.06029849126935005\n",
            "strain 0.060344524681568146\n",
            "strain 0.06149089336395264\n",
            "strain 0.06201523169875145\n",
            "strain 0.05262075737118721\n",
            "strain 0.05255262553691864\n",
            "strain 0.05033595860004425\n",
            "strain 0.059483662247657776\n",
            "strain 0.05367323383688927\n",
            "strain 0.05518775060772896\n",
            "strain 0.050372589379549026\n",
            "strain 0.0547446571290493\n",
            "strain 0.05708155781030655\n",
            "strain 0.05754154920578003\n",
            "strain 0.05403345823287964\n",
            "strain 0.06499751657247543\n",
            "strain 0.05709757283329964\n",
            "strain 0.06311453133821487\n",
            "strain 0.05115528777241707\n",
            "strain 0.05461062490940094\n",
            "strain 0.05745810642838478\n",
            "strain 0.054404743015766144\n",
            "strain 0.05952974408864975\n",
            "strain 0.059429336339235306\n",
            "strain 0.053433265537023544\n",
            "strain 0.05325653403997421\n",
            "strain 0.06226807087659836\n",
            "strain 0.054845090955495834\n",
            "strain 0.06328665465116501\n",
            "strain 0.060336045920848846\n",
            "strain 0.05463772639632225\n",
            "strain 0.0501595102250576\n",
            "strain 0.06376446038484573\n",
            "strain 0.056831471621990204\n",
            "strain 0.05357085168361664\n",
            "strain 0.057231705635786057\n",
            "strain 0.0583297461271286\n",
            "classify 2.263671875\n",
            "classify 2.309326171875\n",
            "classify 2.2933349609375\n",
            "classify 2.2493896484375\n",
            "classify 2.2757568359375\n",
            "classify 2.2635498046875\n",
            "classify 2.2567138671875\n",
            "classify 2.2598876953125\n",
            "classify 2.275390625\n",
            "classify 2.2337646484375\n",
            "classify 2.2791748046875\n",
            "0.203125\n",
            "0.109375\n",
            "0.140625\n",
            "0.109375\n",
            "0.125\n",
            "0.1875\n",
            "0.1875\n",
            "0.21875\n",
            "0.15625\n",
            "0.125\n",
            "0.109375\n",
            "strain 0.06414631009101868\n",
            "strain 0.06046152859926224\n",
            "strain 0.059126969426870346\n",
            "strain 0.07626896351575851\n",
            "strain 0.05850989371538162\n",
            "strain 0.056415900588035583\n",
            "strain 0.06693295389413834\n",
            "strain 0.05692046880722046\n",
            "strain 0.06169484928250313\n",
            "strain 0.05610012263059616\n",
            "strain 0.06263706833124161\n",
            "strain 0.05668690800666809\n",
            "strain 0.07297107577323914\n",
            "strain 0.05775657296180725\n",
            "strain 0.06244971603155136\n",
            "strain 0.04949790984392166\n",
            "strain 0.06097077578306198\n",
            "strain 0.06286279857158661\n",
            "strain 0.056144945323467255\n",
            "strain 0.0509963221848011\n",
            "strain 0.06577864289283752\n",
            "strain 0.054547619074583054\n",
            "strain 0.05995585024356842\n",
            "strain 0.055956024676561356\n",
            "strain 0.05062412843108177\n",
            "strain 0.06424590945243835\n",
            "strain 0.06088517606258392\n",
            "strain 0.06439146399497986\n",
            "strain 0.05478211119771004\n",
            "strain 0.060592956840991974\n",
            "strain 0.07846759259700775\n",
            "strain 0.052964191883802414\n",
            "strain 0.05398973077535629\n",
            "strain 0.053778208792209625\n",
            "strain 0.07479485869407654\n",
            "strain 0.05855492129921913\n",
            "strain 0.057278893887996674\n",
            "strain 0.05576850101351738\n",
            "strain 0.0623156800866127\n",
            "strain 0.0525016225874424\n",
            "strain 0.05688809975981712\n",
            "strain 0.05853193998336792\n",
            "strain 0.055699244141578674\n",
            "strain 0.05042222887277603\n",
            "strain 0.050500866025686264\n",
            "strain 0.054222557693719864\n",
            "strain 0.060608766973018646\n",
            "strain 0.0493583008646965\n",
            "strain 0.0609917975962162\n",
            "strain 0.06552207469940186\n",
            "strain 0.05522336810827255\n",
            "classify 2.294189453125\n",
            "classify 2.234130859375\n",
            "classify 2.259765625\n",
            "classify 2.281005859375\n",
            "classify 2.3179931640625\n",
            "classify 2.3275146484375\n",
            "classify 2.2996826171875\n",
            "classify 2.303466796875\n",
            "classify 2.27197265625\n",
            "classify 2.2847900390625\n",
            "classify 2.2916259765625\n",
            "0.15625\n",
            "0.09375\n",
            "0.15625\n",
            "0.171875\n",
            "0.15625\n",
            "0.125\n",
            "0.1875\n",
            "0.15625\n",
            "0.1875\n",
            "0.1875\n",
            "0.09375\n",
            "strain 0.06516275554895401\n",
            "strain 0.054849207401275635\n",
            "strain 0.05580531805753708\n",
            "strain 0.06078552082180977\n",
            "strain 0.06087179109454155\n",
            "strain 0.05554492771625519\n",
            "strain 0.05757516250014305\n",
            "strain 0.06289081275463104\n",
            "strain 0.05084294453263283\n",
            "strain 0.06395167112350464\n",
            "strain 0.05639449134469032\n",
            "strain 0.06399227678775787\n",
            "strain 0.06118219718337059\n",
            "strain 0.06815724074840546\n",
            "strain 0.07099097967147827\n",
            "strain 0.05748128518462181\n",
            "strain 0.05496618151664734\n",
            "strain 0.07214105129241943\n",
            "strain 0.04861848056316376\n",
            "strain 0.05849887430667877\n",
            "strain 0.05570044741034508\n",
            "strain 0.061964258551597595\n",
            "strain 0.05562680959701538\n",
            "strain 0.06120237708091736\n",
            "strain 0.05092210695147514\n",
            "strain 0.060168731957674026\n",
            "strain 0.05601703003048897\n",
            "strain 0.06032946705818176\n",
            "strain 0.06100386008620262\n",
            "strain 0.05761037766933441\n",
            "strain 0.050581011921167374\n",
            "strain 0.04682748764753342\n",
            "strain 0.06008618697524071\n",
            "strain 0.060899097472429276\n",
            "strain 0.05492447316646576\n",
            "strain 0.05049876868724823\n",
            "strain 0.05256498605012894\n",
            "strain 0.05615077167749405\n",
            "strain 0.057286445051431656\n",
            "strain 0.06779008358716965\n",
            "strain 0.057642657309770584\n",
            "strain 0.05540693923830986\n",
            "strain 0.05201362818479538\n",
            "strain 0.06499634683132172\n",
            "strain 0.0584334172308445\n",
            "strain 0.06222589313983917\n",
            "strain 0.06255026161670685\n",
            "strain 0.06687042117118835\n",
            "strain 0.049407366663217545\n",
            "strain 0.06879059225320816\n",
            "strain 0.055143777281045914\n",
            "classify 2.2569580078125\n",
            "classify 2.260009765625\n",
            "classify 2.273193359375\n",
            "classify 2.230224609375\n",
            "classify 2.2940673828125\n",
            "classify 2.23876953125\n",
            "classify 2.285888671875\n",
            "classify 2.2750244140625\n",
            "classify 2.3121337890625\n",
            "classify 2.23876953125\n",
            "classify 2.267822265625\n",
            "0.203125\n",
            "0.0625\n",
            "0.171875\n",
            "0.15625\n",
            "0.171875\n",
            "0.140625\n",
            "0.1875\n",
            "0.21875\n",
            "0.15625\n",
            "0.15625\n",
            "0.15625\n",
            "strain 0.05924065411090851\n",
            "strain 0.07070254534482956\n",
            "strain 0.05812320113182068\n",
            "strain 0.06674085557460785\n",
            "strain 0.04983137547969818\n",
            "strain 0.05333302542567253\n",
            "strain 0.05511719733476639\n",
            "strain 0.06044178083539009\n",
            "strain 0.06672948598861694\n",
            "strain 0.06774047017097473\n",
            "strain 0.06950777769088745\n",
            "strain 0.05264503136277199\n",
            "strain 0.057995785027742386\n",
            "strain 0.04732983931899071\n",
            "strain 0.05566670373082161\n",
            "strain 0.06198043376207352\n",
            "strain 0.061179954558610916\n",
            "strain 0.05929822474718094\n",
            "strain 0.0521680973470211\n",
            "strain 0.058951038867235184\n",
            "strain 0.05844694748520851\n",
            "strain 0.06643593311309814\n",
            "strain 0.05471616983413696\n",
            "strain 0.05352828651666641\n",
            "strain 0.059221867471933365\n",
            "strain 0.06115537881851196\n",
            "strain 0.0615963377058506\n",
            "strain 0.05755496770143509\n",
            "strain 0.062304817140102386\n",
            "strain 0.056148260831832886\n",
            "strain 0.057178791612386703\n",
            "strain 0.05861378833651543\n",
            "strain 0.05767732113599777\n",
            "strain 0.053323324769735336\n",
            "strain 0.0602237842977047\n",
            "strain 0.05400334298610687\n",
            "strain 0.058679673820734024\n",
            "strain 0.05614906921982765\n",
            "strain 0.0565042719244957\n",
            "strain 0.04958300665020943\n",
            "strain 0.05875450000166893\n",
            "strain 0.07904475927352905\n",
            "strain 0.06029908359050751\n",
            "strain 0.06544111669063568\n",
            "strain 0.05980287864804268\n",
            "strain 0.057031773030757904\n",
            "strain 0.06260035932064056\n",
            "strain 0.05855557322502136\n",
            "strain 0.06413209438323975\n",
            "strain 0.05537433177232742\n",
            "strain 0.05075085535645485\n",
            "classify 2.303955078125\n",
            "classify 2.297607421875\n",
            "classify 2.281494140625\n",
            "classify 2.2567138671875\n",
            "classify 2.31005859375\n",
            "classify 2.247802734375\n",
            "classify 2.2467041015625\n",
            "classify 2.217041015625\n",
            "classify 2.2674560546875\n",
            "classify 2.2679443359375\n",
            "classify 2.2515869140625\n",
            "0.21875\n",
            "0.078125\n",
            "0.1875\n",
            "0.15625\n",
            "0.171875\n",
            "0.125\n",
            "0.140625\n",
            "0.15625\n",
            "0.15625\n",
            "0.203125\n",
            "0.15625\n",
            "strain 0.057867784053087234\n",
            "strain 0.07114553451538086\n",
            "strain 0.056019701063632965\n",
            "strain 0.055429622530937195\n",
            "strain 0.05800851806998253\n",
            "strain 0.049735285341739655\n",
            "strain 0.05716351792216301\n",
            "strain 0.053133368492126465\n",
            "strain 0.07157452404499054\n",
            "strain 0.05511707067489624\n",
            "strain 0.05744142830371857\n",
            "strain 0.05352257564663887\n",
            "strain 0.060867492109537125\n",
            "strain 0.05814295634627342\n",
            "strain 0.06245335564017296\n",
            "strain 0.05862100049853325\n",
            "strain 0.05613042414188385\n",
            "strain 0.04996010288596153\n",
            "strain 0.05219614878296852\n",
            "strain 0.05626314878463745\n",
            "strain 0.0544050894677639\n",
            "strain 0.05955531820654869\n",
            "strain 0.04714711382985115\n",
            "strain 0.06844575703144073\n",
            "strain 0.05097886174917221\n",
            "strain 0.06878259778022766\n",
            "strain 0.05588344484567642\n",
            "strain 0.06601417809724808\n",
            "strain 0.06257574260234833\n",
            "strain 0.05992640182375908\n",
            "strain 0.06036572530865669\n",
            "strain 0.05404466763138771\n",
            "strain 0.06379584223031998\n",
            "strain 0.06277142465114594\n",
            "strain 0.058779701590538025\n",
            "strain 0.060385383665561676\n",
            "strain 0.059309814125299454\n",
            "strain 0.06466647982597351\n",
            "strain 0.06184474378824234\n",
            "strain 0.06784458458423615\n",
            "strain 0.05635019391775131\n",
            "strain 0.059842176735401154\n",
            "strain 0.0600898377597332\n",
            "strain 0.0598779022693634\n",
            "strain 0.06306211650371552\n",
            "strain 0.05753495171666145\n",
            "strain 0.05408206582069397\n",
            "strain 0.0639175996184349\n",
            "strain 0.057964541018009186\n",
            "strain 0.06984554976224899\n",
            "strain 0.04704291373491287\n",
            "classify 2.26806640625\n",
            "classify 2.2554931640625\n",
            "classify 2.2418212890625\n",
            "classify 2.290771484375\n",
            "classify 2.2603759765625\n",
            "classify 2.263427734375\n",
            "classify 2.250732421875\n",
            "classify 2.2679443359375\n",
            "classify 2.2581787109375\n",
            "classify 2.297607421875\n",
            "classify 2.2723388671875\n",
            "0.15625\n",
            "0.078125\n",
            "0.234375\n",
            "0.171875\n",
            "0.140625\n",
            "0.09375\n",
            "0.203125\n",
            "0.15625\n",
            "0.21875\n",
            "0.1875\n",
            "0.140625\n",
            "strain 0.053926803171634674\n",
            "strain 0.06094157323241234\n",
            "strain 0.049990713596343994\n",
            "strain 0.05575025454163551\n",
            "strain 0.06166878342628479\n",
            "strain 0.06478241831064224\n",
            "strain 0.052121710032224655\n",
            "strain 0.05536476522684097\n",
            "strain 0.05447767674922943\n",
            "strain 0.05443562567234039\n",
            "strain 0.05567695200443268\n",
            "strain 0.056189052760601044\n",
            "strain 0.055388305336236954\n",
            "strain 0.0537903718650341\n",
            "strain 0.05073944479227066\n",
            "strain 0.05923554301261902\n",
            "strain 0.05312340706586838\n",
            "strain 0.053883619606494904\n",
            "strain 0.06383001804351807\n",
            "strain 0.05214390158653259\n",
            "strain 0.062182243913412094\n",
            "strain 0.05890709161758423\n",
            "strain 0.06281600147485733\n",
            "strain 0.060340795665979385\n",
            "strain 0.06518319994211197\n",
            "strain 0.06629370898008347\n",
            "strain 0.06825456768274307\n",
            "strain 0.0554620586335659\n",
            "strain 0.05653909593820572\n",
            "strain 0.04852921515703201\n",
            "strain 0.05658034235239029\n",
            "strain 0.06361092627048492\n",
            "strain 0.06296850740909576\n",
            "strain 0.07221562415361404\n",
            "strain 0.07361685484647751\n",
            "strain 0.060102831572294235\n",
            "strain 0.05337446928024292\n",
            "strain 0.05699316784739494\n",
            "strain 0.06999222189188004\n",
            "strain 0.05599221959710121\n",
            "strain 0.05201520025730133\n",
            "strain 0.059130679816007614\n",
            "strain 0.052774861454963684\n",
            "strain 0.05004537105560303\n",
            "strain 0.05999487638473511\n",
            "strain 0.05750541761517525\n",
            "strain 0.06417955458164215\n",
            "strain 0.055305931717157364\n",
            "strain 0.0569942370057106\n",
            "strain 0.05543047562241554\n",
            "strain 0.06718888878822327\n",
            "classify 2.3052978515625\n",
            "classify 2.22265625\n",
            "classify 2.241455078125\n",
            "classify 2.2569580078125\n",
            "classify 2.263916015625\n",
            "classify 2.2718505859375\n",
            "classify 2.3038330078125\n",
            "classify 2.30859375\n",
            "classify 2.259033203125\n",
            "classify 2.3046875\n",
            "classify 2.277099609375\n",
            "0.171875\n",
            "0.109375\n",
            "0.203125\n",
            "0.140625\n",
            "0.125\n",
            "0.15625\n",
            "0.171875\n",
            "0.15625\n",
            "0.234375\n",
            "0.140625\n",
            "0.125\n",
            "strain 0.056740082800388336\n",
            "strain 0.059946127235889435\n",
            "strain 0.060832392424345016\n",
            "strain 0.05154994875192642\n",
            "strain 0.061078961938619614\n",
            "strain 0.05408031493425369\n",
            "strain 0.059608012437820435\n",
            "strain 0.05494754761457443\n",
            "strain 0.05853256955742836\n",
            "strain 0.0477311797440052\n",
            "strain 0.051105350255966187\n",
            "strain 0.05835000053048134\n",
            "strain 0.05614115670323372\n",
            "strain 0.05875678360462189\n",
            "strain 0.05551133677363396\n",
            "strain 0.06581398844718933\n",
            "strain 0.059131793677806854\n",
            "strain 0.058133311569690704\n",
            "strain 0.058448269963264465\n",
            "strain 0.055698417127132416\n",
            "strain 0.06407953053712845\n",
            "strain 0.052461523562669754\n",
            "strain 0.0577586404979229\n",
            "strain 0.05456219241023064\n",
            "strain 0.058574385941028595\n",
            "strain 0.06304892152547836\n",
            "strain 0.05638463795185089\n",
            "strain 0.05368207395076752\n",
            "strain 0.052145808935165405\n",
            "strain 0.0544060617685318\n",
            "strain 0.05508042871952057\n",
            "strain 0.05519908666610718\n",
            "strain 0.056879378855228424\n",
            "strain 0.06871788203716278\n",
            "strain 0.05198026821017265\n",
            "strain 0.061066143214702606\n",
            "strain 0.05791790038347244\n",
            "strain 0.0598478764295578\n",
            "strain 0.055383287370204926\n",
            "strain 0.05348611995577812\n",
            "strain 0.05222233757376671\n",
            "strain 0.06974004209041595\n",
            "strain 0.05777067318558693\n",
            "strain 0.058330949395895004\n",
            "strain 0.059019189327955246\n",
            "strain 0.0585651621222496\n",
            "strain 0.05968242511153221\n",
            "strain 0.056120626628398895\n",
            "strain 0.058097243309020996\n",
            "strain 0.051191918551921844\n",
            "strain 0.05311831086874008\n",
            "classify 2.2545166015625\n",
            "classify 2.2666015625\n",
            "classify 2.2852783203125\n",
            "classify 2.2325439453125\n",
            "classify 2.239501953125\n",
            "classify 2.2781982421875\n",
            "classify 2.2418212890625\n",
            "classify 2.2154541015625\n",
            "classify 2.258544921875\n",
            "classify 2.2950439453125\n",
            "classify 2.296142578125\n",
            "0.171875\n",
            "0.078125\n",
            "0.203125\n",
            "0.171875\n",
            "0.15625\n",
            "0.171875\n",
            "0.21875\n",
            "0.1875\n",
            "0.1875\n",
            "0.171875\n",
            "0.109375\n",
            "strain 0.05973963439464569\n",
            "strain 0.057323720306158066\n",
            "strain 0.06027824431657791\n",
            "strain 0.056458644568920135\n",
            "strain 0.0629049614071846\n",
            "strain 0.05893981084227562\n",
            "strain 0.06077657639980316\n",
            "strain 0.07684937864542007\n",
            "strain 0.061357930302619934\n",
            "strain 0.054139502346515656\n",
            "strain 0.07181060314178467\n",
            "strain 0.06080631911754608\n",
            "strain 0.056459229439496994\n",
            "strain 0.05414310097694397\n",
            "strain 0.06848469376564026\n",
            "strain 0.05653921142220497\n",
            "strain 0.060367897152900696\n",
            "strain 0.058099254965782166\n",
            "strain 0.04899033159017563\n",
            "strain 0.05882696062326431\n",
            "strain 0.06517967581748962\n",
            "strain 0.058024484664201736\n",
            "strain 0.06786711513996124\n",
            "strain 0.05818632245063782\n",
            "strain 0.06728223711252213\n",
            "strain 0.05614687129855156\n",
            "strain 0.05850106105208397\n",
            "strain 0.0632489025592804\n",
            "strain 0.05648941546678543\n",
            "strain 0.06894400715827942\n",
            "strain 0.05832471698522568\n",
            "strain 0.06139001250267029\n",
            "strain 0.06293469667434692\n",
            "strain 0.057238172739744186\n",
            "strain 0.06245340406894684\n",
            "strain 0.05172192305326462\n",
            "strain 0.05595611035823822\n",
            "strain 0.057296283543109894\n",
            "strain 0.060702402144670486\n",
            "strain 0.06398706138134003\n",
            "strain 0.06714221835136414\n",
            "strain 0.05540042743086815\n",
            "strain 0.056610580533742905\n",
            "strain 0.05256091058254242\n",
            "strain 0.05716223269701004\n",
            "strain 0.058894410729408264\n",
            "strain 0.06076743081212044\n",
            "strain 0.06956750899553299\n",
            "strain 0.05970627814531326\n",
            "strain 0.05513710528612137\n",
            "strain 0.05687085911631584\n",
            "classify 2.2586669921875\n",
            "classify 2.2393798828125\n",
            "classify 2.2998046875\n",
            "classify 2.2838134765625\n",
            "classify 2.2528076171875\n",
            "classify 2.266357421875\n",
            "classify 2.302734375\n",
            "classify 2.288818359375\n",
            "classify 2.249755859375\n",
            "classify 2.258544921875\n",
            "classify 2.28564453125\n",
            "0.203125\n",
            "0.078125\n",
            "0.171875\n",
            "0.1875\n",
            "0.140625\n",
            "0.140625\n",
            "0.234375\n",
            "0.234375\n",
            "0.1875\n",
            "0.15625\n",
            "0.140625\n",
            "strain 0.05674893781542778\n",
            "strain 0.05355898663401604\n",
            "strain 0.0653325766324997\n",
            "strain 0.06553688645362854\n",
            "strain 0.07200313359498978\n",
            "strain 0.05637717619538307\n",
            "strain 0.06027509644627571\n",
            "strain 0.0598096065223217\n",
            "strain 0.060273632407188416\n",
            "strain 0.05717388913035393\n",
            "strain 0.061867859214544296\n",
            "strain 0.055758655071258545\n",
            "strain 0.05434153228998184\n",
            "strain 0.05001465231180191\n",
            "strain 0.05791814252734184\n",
            "strain 0.06310351192951202\n",
            "strain 0.05269322916865349\n",
            "strain 0.0608212985098362\n",
            "strain 0.0698322057723999\n",
            "strain 0.06467024981975555\n",
            "strain 0.06110363081097603\n",
            "strain 0.05414807051420212\n",
            "strain 0.05710878595709801\n",
            "strain 0.05586686730384827\n",
            "strain 0.07213743031024933\n",
            "strain 0.05575829744338989\n",
            "strain 0.059310540556907654\n",
            "strain 0.05654940381646156\n",
            "strain 0.059981077909469604\n",
            "strain 0.06523317098617554\n",
            "strain 0.059200942516326904\n",
            "strain 0.08229353278875351\n",
            "strain 0.06520778685808182\n",
            "strain 0.049055010080337524\n",
            "strain 0.06417996436357498\n",
            "strain 0.04667123407125473\n",
            "strain 0.052200447767972946\n",
            "strain 0.05009625107049942\n",
            "strain 0.06915149092674255\n",
            "strain 0.05955291539430618\n",
            "strain 0.05964362621307373\n",
            "strain 0.04596918821334839\n",
            "strain 0.06603767722845078\n",
            "strain 0.06293707340955734\n",
            "strain 0.072332002222538\n",
            "strain 0.0582582913339138\n",
            "strain 0.05527925118803978\n",
            "strain 0.06187897175550461\n",
            "strain 0.05551798269152641\n",
            "strain 0.06389668583869934\n",
            "strain 0.056834712624549866\n",
            "classify 2.2769775390625\n",
            "classify 2.251953125\n",
            "classify 2.233154296875\n",
            "classify 2.2718505859375\n",
            "classify 2.260009765625\n",
            "classify 2.2528076171875\n",
            "classify 2.269287109375\n",
            "classify 2.2314453125\n",
            "classify 2.2586669921875\n",
            "classify 2.2593994140625\n",
            "classify 2.251708984375\n",
            "0.171875\n",
            "0.09375\n",
            "0.1875\n",
            "0.171875\n",
            "0.140625\n",
            "0.21875\n",
            "0.1875\n",
            "0.1875\n",
            "0.203125\n",
            "0.25\n",
            "0.109375\n",
            "strain 0.05556902289390564\n",
            "strain 0.05415206030011177\n",
            "strain 0.06933984905481339\n",
            "strain 0.07171998172998428\n",
            "strain 0.055058836936950684\n",
            "strain 0.060513488948345184\n",
            "strain 0.06438358873128891\n",
            "strain 0.05949510633945465\n",
            "strain 0.06184437498450279\n",
            "strain 0.06645359843969345\n",
            "strain 0.06449376046657562\n",
            "strain 0.05455715209245682\n",
            "strain 0.055085066705942154\n",
            "strain 0.0488143265247345\n",
            "strain 0.059916578233242035\n",
            "strain 0.06240880861878395\n",
            "strain 0.051743313670158386\n",
            "strain 0.05493148788809776\n",
            "strain 0.057343099266290665\n",
            "strain 0.05150337517261505\n",
            "strain 0.05744902044534683\n",
            "strain 0.06431867927312851\n",
            "strain 0.050775956362485886\n",
            "strain 0.06159676983952522\n",
            "strain 0.06159372255206108\n",
            "strain 0.05712641403079033\n",
            "strain 0.06247634440660477\n",
            "strain 0.05825604498386383\n",
            "strain 0.060782771557569504\n",
            "strain 0.06377767771482468\n",
            "strain 0.0565461665391922\n",
            "strain 0.05115494877099991\n",
            "strain 0.05341316759586334\n",
            "strain 0.05905940383672714\n",
            "strain 0.05532444268465042\n",
            "strain 0.0571122020483017\n",
            "strain 0.05013355612754822\n",
            "strain 0.06260579824447632\n",
            "strain 0.06264765560626984\n",
            "strain 0.054372385144233704\n",
            "strain 0.04931268468499184\n",
            "strain 0.05784596502780914\n",
            "strain 0.054847486317157745\n",
            "strain 0.061155106872320175\n",
            "strain 0.05849626660346985\n",
            "strain 0.05150816589593887\n",
            "strain 0.07517384737730026\n",
            "strain 0.0651579201221466\n",
            "strain 0.05383198335766792\n",
            "strain 0.06819839030504227\n",
            "strain 0.05327849090099335\n",
            "classify 2.263427734375\n",
            "classify 2.2344970703125\n",
            "classify 2.2423095703125\n",
            "classify 2.26611328125\n",
            "classify 2.26708984375\n",
            "classify 2.3209228515625\n",
            "classify 2.296630859375\n",
            "classify 2.2728271484375\n",
            "classify 2.26318359375\n",
            "classify 2.2916259765625\n",
            "classify 2.2506103515625\n",
            "0.171875\n",
            "0.0625\n",
            "0.25\n",
            "0.15625\n",
            "0.125\n",
            "0.15625\n",
            "0.140625\n",
            "0.1875\n",
            "0.171875\n",
            "0.125\n",
            "0.078125\n",
            "strain 0.060649193823337555\n",
            "strain 0.060499608516693115\n",
            "strain 0.06149207055568695\n",
            "strain 0.07011464238166809\n",
            "strain 0.060768403112888336\n",
            "strain 0.05940588563680649\n",
            "strain 0.06226962059736252\n",
            "strain 0.056550588458776474\n",
            "strain 0.05383335426449776\n",
            "strain 0.052298251539468765\n",
            "strain 0.07167096436023712\n",
            "strain 0.059406641870737076\n",
            "strain 0.06158732622861862\n",
            "strain 0.0647018626332283\n",
            "strain 0.05386880785226822\n",
            "strain 0.057235751301050186\n",
            "strain 0.058702051639556885\n",
            "strain 0.04772474989295006\n",
            "strain 0.06372708082199097\n",
            "strain 0.060521431267261505\n",
            "strain 0.049535512924194336\n",
            "strain 0.06240193918347359\n",
            "strain 0.05802283436059952\n",
            "strain 0.06299819052219391\n",
            "strain 0.056851573288440704\n",
            "strain 0.05060796067118645\n",
            "strain 0.06230834126472473\n",
            "strain 0.05332212522625923\n",
            "strain 0.05563276633620262\n",
            "strain 0.0606854185461998\n",
            "strain 0.05513886734843254\n",
            "strain 0.05664816498756409\n",
            "strain 0.06266134977340698\n",
            "strain 0.05953342840075493\n",
            "strain 0.05439981445670128\n",
            "strain 0.06438682228326797\n",
            "strain 0.059880420565605164\n",
            "strain 0.057681623846292496\n",
            "strain 0.054956529289484024\n",
            "strain 0.0591433160007\n",
            "strain 0.06207531318068504\n",
            "strain 0.05148465931415558\n",
            "strain 0.05748765543103218\n",
            "strain 0.05491156131029129\n",
            "strain 0.06134489178657532\n",
            "strain 0.04869784042239189\n",
            "strain 0.05028856545686722\n",
            "strain 0.06016603857278824\n",
            "strain 0.06621435284614563\n",
            "strain 0.05390480160713196\n",
            "strain 0.06345253437757492\n",
            "classify 2.212158203125\n",
            "classify 2.25048828125\n",
            "classify 2.2769775390625\n",
            "classify 2.27734375\n",
            "classify 2.2867431640625\n",
            "classify 2.2353515625\n",
            "classify 2.2364501953125\n",
            "classify 2.2884521484375\n",
            "classify 2.2735595703125\n",
            "classify 2.24609375\n",
            "classify 2.2738037109375\n",
            "0.171875\n",
            "0.109375\n",
            "0.203125\n",
            "0.140625\n",
            "0.15625\n",
            "0.125\n",
            "0.1875\n",
            "0.234375\n",
            "0.140625\n",
            "0.15625\n",
            "0.09375\n",
            "strain 0.05087253451347351\n",
            "strain 0.06387713551521301\n",
            "strain 0.05770478397607803\n",
            "strain 0.06074024364352226\n",
            "strain 0.0636436715722084\n",
            "strain 0.052355822175741196\n",
            "strain 0.07147340476512909\n",
            "strain 0.06185665726661682\n",
            "strain 0.056313175708055496\n",
            "strain 0.051855411380529404\n",
            "strain 0.06194472685456276\n",
            "strain 0.06738355755805969\n",
            "strain 0.06869799643754959\n",
            "strain 0.06074463948607445\n",
            "strain 0.05366356670856476\n",
            "strain 0.05664116144180298\n",
            "strain 0.06654366105794907\n",
            "strain 0.05614360421895981\n",
            "strain 0.05218559503555298\n",
            "strain 0.05848352983593941\n",
            "strain 0.05561129003763199\n",
            "strain 0.04857805371284485\n",
            "strain 0.06041276082396507\n",
            "strain 0.052817732095718384\n",
            "strain 0.04763539880514145\n",
            "strain 0.05225246772170067\n",
            "strain 0.056297410279512405\n",
            "strain 0.05929245054721832\n",
            "strain 0.055396128445863724\n",
            "strain 0.05494772642850876\n",
            "strain 0.050666775554418564\n",
            "strain 0.059346798807382584\n",
            "strain 0.055015549063682556\n",
            "strain 0.052204832434654236\n",
            "strain 0.053157467395067215\n",
            "strain 0.058097049593925476\n",
            "strain 0.06005116552114487\n",
            "strain 0.0628996416926384\n",
            "strain 0.057829272001981735\n",
            "strain 0.054885879158973694\n",
            "strain 0.059723760932683945\n",
            "strain 0.050245948135852814\n",
            "strain 0.059724900871515274\n",
            "strain 0.055138517171144485\n",
            "strain 0.06760169565677643\n",
            "strain 0.05038461089134216\n",
            "strain 0.05246035382151604\n",
            "strain 0.06104739010334015\n",
            "strain 0.07480231672525406\n",
            "strain 0.07028486579656601\n",
            "strain 0.05739781633019447\n",
            "classify 2.3162841796875\n",
            "classify 2.29052734375\n",
            "classify 2.24365234375\n",
            "classify 2.2423095703125\n",
            "classify 2.2584228515625\n",
            "classify 2.3040771484375\n",
            "classify 2.2734375\n",
            "classify 2.260498046875\n",
            "classify 2.24462890625\n",
            "classify 2.27587890625\n",
            "classify 2.251220703125\n",
            "0.1875\n",
            "0.09375\n",
            "0.25\n",
            "0.171875\n",
            "0.109375\n",
            "0.125\n",
            "0.203125\n",
            "0.203125\n",
            "0.25\n",
            "0.203125\n",
            "0.09375\n",
            "strain 0.05946296826004982\n",
            "strain 0.057052478194236755\n",
            "strain 0.05050719901919365\n",
            "strain 0.05271901562809944\n",
            "strain 0.05677151307463646\n",
            "strain 0.06258993595838547\n",
            "strain 0.053137846291065216\n",
            "strain 0.05511750280857086\n",
            "strain 0.05334613099694252\n",
            "strain 0.049659613519907\n",
            "strain 0.057004984468221664\n",
            "strain 0.06023364141583443\n",
            "strain 0.06456469744443893\n",
            "strain 0.0543878972530365\n",
            "strain 0.06827639788389206\n",
            "strain 0.06638442724943161\n",
            "strain 0.05928358435630798\n",
            "strain 0.057578667998313904\n",
            "strain 0.057943567633628845\n",
            "strain 0.053061798214912415\n",
            "strain 0.05280199646949768\n",
            "strain 0.053731657564640045\n",
            "strain 0.06365740299224854\n",
            "strain 0.05318419262766838\n",
            "strain 0.059862397611141205\n",
            "strain 0.04818308353424072\n",
            "strain 0.04390566051006317\n",
            "strain 0.051585447043180466\n",
            "strain 0.0504046306014061\n",
            "strain 0.05326501652598381\n",
            "strain 0.06284162402153015\n",
            "strain 0.06479757279157639\n",
            "strain 0.06132063269615173\n",
            "strain 0.06676843017339706\n",
            "strain 0.057898834347724915\n",
            "strain 0.06310124695301056\n",
            "strain 0.06136541813611984\n",
            "strain 0.057323992252349854\n",
            "strain 0.05646520480513573\n",
            "strain 0.053257182240486145\n",
            "strain 0.06387759745121002\n",
            "strain 0.06256022304296494\n",
            "strain 0.053394515067338943\n",
            "strain 0.05984441936016083\n",
            "strain 0.05233706533908844\n",
            "strain 0.05285460501909256\n",
            "strain 0.05217243731021881\n",
            "strain 0.055695585906505585\n",
            "strain 0.06538908928632736\n",
            "strain 0.06607729941606522\n",
            "strain 0.05444546788930893\n",
            "classify 2.2646484375\n",
            "classify 2.2867431640625\n",
            "classify 2.261474609375\n",
            "classify 2.28173828125\n",
            "classify 2.26953125\n",
            "classify 2.2735595703125\n",
            "classify 2.2960205078125\n",
            "classify 2.2967529296875\n",
            "classify 2.2689208984375\n",
            "classify 2.2669677734375\n",
            "classify 2.28125\n",
            "0.171875\n",
            "0.078125\n",
            "0.203125\n",
            "0.15625\n",
            "0.15625\n",
            "0.140625\n",
            "0.1875\n",
            "0.203125\n",
            "0.15625\n",
            "0.140625\n",
            "0.09375\n",
            "strain 0.05704284831881523\n",
            "strain 0.054331690073013306\n",
            "strain 0.06276195496320724\n",
            "strain 0.055298738181591034\n",
            "strain 0.05536791309714317\n",
            "strain 0.06168410927057266\n",
            "strain 0.06146353855729103\n",
            "strain 0.05299442633986473\n",
            "strain 0.051177818328142166\n",
            "strain 0.05393414944410324\n",
            "strain 0.060635872185230255\n",
            "strain 0.06484731286764145\n",
            "strain 0.05488095432519913\n",
            "strain 0.05657457932829857\n",
            "strain 0.05229265242815018\n",
            "strain 0.05161777511239052\n",
            "strain 0.056577444076538086\n",
            "strain 0.0572659932076931\n",
            "strain 0.04906579107046127\n",
            "strain 0.05786538124084473\n",
            "strain 0.06985669583082199\n",
            "strain 0.060710955411195755\n",
            "strain 0.056630875915288925\n",
            "strain 0.05801708623766899\n",
            "strain 0.05683840438723564\n",
            "strain 0.0630267933011055\n",
            "strain 0.05755724012851715\n",
            "strain 0.06735365837812424\n",
            "strain 0.0588667057454586\n",
            "strain 0.06292808055877686\n",
            "strain 0.055783189833164215\n",
            "strain 0.06059817597270012\n",
            "strain 0.04955127090215683\n",
            "strain 0.05941944569349289\n",
            "strain 0.058214038610458374\n",
            "strain 0.07176057249307632\n",
            "strain 0.05860656127333641\n",
            "strain 0.05228571221232414\n",
            "strain 0.06539754569530487\n",
            "strain 0.05910462141036987\n",
            "strain 0.05306540057063103\n",
            "strain 0.05832383409142494\n",
            "strain 0.05418219044804573\n",
            "strain 0.06998391449451447\n",
            "strain 0.05559961870312691\n",
            "strain 0.06494888663291931\n",
            "strain 0.054442375898361206\n",
            "strain 0.05221626162528992\n",
            "strain 0.05345806106925011\n",
            "strain 0.04890220984816551\n",
            "strain 0.07104811072349548\n",
            "classify 2.2911376953125\n",
            "classify 2.25390625\n",
            "classify 2.2615966796875\n",
            "classify 2.30126953125\n",
            "classify 2.2672119140625\n",
            "classify 2.2650146484375\n",
            "classify 2.2850341796875\n",
            "classify 2.3046875\n",
            "classify 2.2452392578125\n",
            "classify 2.2552490234375\n",
            "classify 2.3133544921875\n",
            "0.171875\n",
            "0.078125\n",
            "0.234375\n",
            "0.109375\n",
            "0.125\n",
            "0.125\n",
            "0.25\n",
            "0.171875\n",
            "0.21875\n",
            "0.21875\n",
            "0.125\n",
            "strain 0.05593986436724663\n",
            "strain 0.060014352202415466\n",
            "strain 0.05853259563446045\n",
            "strain 0.06248554214835167\n",
            "strain 0.060501620173454285\n",
            "strain 0.052825894206762314\n",
            "strain 0.056014250963926315\n",
            "strain 0.05502728372812271\n",
            "strain 0.060411471873521805\n",
            "strain 0.052639905363321304\n",
            "strain 0.06490641087293625\n",
            "strain 0.05653782933950424\n",
            "strain 0.05981147289276123\n",
            "strain 0.05164991319179535\n",
            "strain 0.05896135792136192\n",
            "strain 0.057925909757614136\n",
            "strain 0.05789868161082268\n",
            "strain 0.0407225638628006\n",
            "strain 0.06315198540687561\n",
            "strain 0.057077664881944656\n",
            "strain 0.0713769793510437\n",
            "strain 0.05948963388800621\n",
            "strain 0.0544244684278965\n",
            "strain 0.05928600952029228\n",
            "strain 0.05684373155236244\n",
            "strain 0.052240848541259766\n",
            "strain 0.06571724265813828\n",
            "strain 0.05661148950457573\n",
            "strain 0.05584387853741646\n",
            "strain 0.058717116713523865\n",
            "strain 0.06797445565462112\n",
            "strain 0.06042135879397392\n",
            "strain 0.05456340312957764\n",
            "strain 0.05543357506394386\n",
            "strain 0.05711713060736656\n",
            "strain 0.0589015856385231\n",
            "strain 0.05652555450797081\n",
            "strain 0.06378107517957687\n",
            "strain 0.05825118348002434\n",
            "strain 0.0576888732612133\n",
            "strain 0.06745637953281403\n",
            "strain 0.06081385537981987\n",
            "strain 0.05749456584453583\n",
            "strain 0.058173876255750656\n",
            "strain 0.0596008226275444\n",
            "strain 0.056922879070043564\n",
            "strain 0.0601278580725193\n",
            "strain 0.053119052201509476\n",
            "strain 0.052227746695280075\n",
            "strain 0.05864261835813522\n",
            "strain 0.060675203800201416\n",
            "classify 2.2672119140625\n",
            "classify 2.291015625\n",
            "classify 2.285400390625\n",
            "classify 2.27197265625\n",
            "classify 2.2939453125\n",
            "classify 2.2685546875\n",
            "classify 2.2271728515625\n",
            "classify 2.25390625\n",
            "classify 2.2630615234375\n",
            "classify 2.289306640625\n",
            "classify 2.2479248046875\n",
            "0.171875\n",
            "0.125\n",
            "0.234375\n",
            "0.15625\n",
            "0.140625\n",
            "0.15625\n",
            "0.171875\n",
            "0.203125\n",
            "0.15625\n",
            "0.1875\n",
            "0.109375\n",
            "strain 0.07080144435167313\n",
            "strain 0.05710722133517265\n",
            "strain 0.05111079663038254\n",
            "strain 0.05567462369799614\n",
            "strain 0.05313709005713463\n",
            "strain 0.05777726694941521\n",
            "strain 0.05459297075867653\n",
            "strain 0.055574215948581696\n",
            "strain 0.04943959042429924\n",
            "strain 0.0596652552485466\n",
            "strain 0.06185319647192955\n",
            "strain 0.06915122270584106\n",
            "strain 0.0520138256251812\n",
            "strain 0.054445113986730576\n",
            "strain 0.06435791403055191\n",
            "strain 0.05633857101202011\n",
            "strain 0.048065297305583954\n",
            "strain 0.05350704863667488\n",
            "strain 0.06130148470401764\n",
            "strain 0.05484071001410484\n",
            "strain 0.04821782931685448\n",
            "strain 0.06062434986233711\n",
            "strain 0.05311023071408272\n",
            "strain 0.05936623737215996\n",
            "strain 0.053935304284095764\n",
            "strain 0.06018935889005661\n",
            "strain 0.05970362201333046\n",
            "strain 0.057564809918403625\n",
            "strain 0.05169985070824623\n",
            "strain 0.05486418679356575\n",
            "strain 0.0545194037258625\n",
            "strain 0.04949355125427246\n",
            "strain 0.05281347036361694\n",
            "strain 0.0621190145611763\n",
            "strain 0.05601348727941513\n",
            "strain 0.07814838737249374\n",
            "strain 0.05514303222298622\n",
            "strain 0.06108323857188225\n",
            "strain 0.0570247583091259\n",
            "strain 0.0545942485332489\n",
            "strain 0.06218848004937172\n",
            "strain 0.058822549879550934\n",
            "strain 0.06242910772562027\n",
            "strain 0.055285364389419556\n",
            "strain 0.05349385365843773\n",
            "strain 0.05654832348227501\n",
            "strain 0.05883078649640083\n",
            "strain 0.06389197707176208\n",
            "strain 0.06619668006896973\n",
            "strain 0.067245252430439\n",
            "strain 0.06720002740621567\n",
            "classify 2.28173828125\n",
            "classify 2.251220703125\n",
            "classify 2.2376708984375\n",
            "classify 2.252197265625\n",
            "classify 2.2640380859375\n",
            "classify 2.2412109375\n",
            "classify 2.281005859375\n",
            "classify 2.2781982421875\n",
            "classify 2.287841796875\n",
            "classify 2.27099609375\n",
            "classify 2.26220703125\n",
            "0.125\n",
            "0.078125\n",
            "0.25\n",
            "0.140625\n",
            "0.171875\n",
            "0.140625\n",
            "0.25\n",
            "0.21875\n",
            "0.203125\n",
            "0.15625\n",
            "0.125\n",
            "strain 0.0631500631570816\n",
            "strain 0.06477127224206924\n",
            "strain 0.06430791318416595\n",
            "strain 0.06036551669239998\n",
            "strain 0.06354472041130066\n",
            "strain 0.05400431156158447\n",
            "strain 0.058453429490327835\n",
            "strain 0.05740252882242203\n",
            "strain 0.055484622716903687\n",
            "strain 0.05565016344189644\n",
            "strain 0.06510090082883835\n",
            "strain 0.05419694632291794\n",
            "strain 0.0621907077729702\n",
            "strain 0.05924272537231445\n",
            "strain 0.06342270970344543\n",
            "strain 0.05758514255285263\n",
            "strain 0.05950966477394104\n",
            "strain 0.06288120895624161\n",
            "strain 0.06776004284620285\n",
            "strain 0.06525041908025742\n",
            "strain 0.05053793266415596\n",
            "strain 0.05174868926405907\n",
            "strain 0.050087153911590576\n",
            "strain 0.07229255884885788\n",
            "strain 0.06060408055782318\n",
            "strain 0.06611097604036331\n",
            "strain 0.058138567954301834\n",
            "strain 0.05534479394555092\n",
            "strain 0.055762555450201035\n",
            "strain 0.06302206218242645\n",
            "strain 0.04988710209727287\n",
            "strain 0.06483086198568344\n",
            "strain 0.07735594362020493\n",
            "strain 0.05370210111141205\n",
            "strain 0.05108801648020744\n",
            "strain 0.05968082323670387\n",
            "strain 0.05103962495923042\n",
            "strain 0.048447318375110626\n",
            "strain 0.06185923144221306\n",
            "strain 0.05426245927810669\n",
            "strain 0.05406498908996582\n",
            "strain 0.05901582911610603\n",
            "strain 0.057779278606176376\n",
            "strain 0.05829340219497681\n",
            "strain 0.05697702616453171\n",
            "strain 0.07458849996328354\n",
            "strain 0.05113065987825394\n",
            "strain 0.06847406923770905\n",
            "strain 0.0797060951590538\n",
            "strain 0.07389793545007706\n",
            "strain 0.055286649614572525\n",
            "classify 2.2652587890625\n",
            "classify 2.274658203125\n",
            "classify 2.24462890625\n",
            "classify 2.260009765625\n",
            "classify 2.28369140625\n",
            "classify 2.2774658203125\n",
            "classify 2.2313232421875\n",
            "classify 2.2530517578125\n",
            "classify 2.246337890625\n",
            "classify 2.3026123046875\n",
            "classify 2.2503662109375\n",
            "0.15625\n",
            "0.109375\n",
            "0.21875\n",
            "0.140625\n",
            "0.171875\n",
            "0.140625\n",
            "0.15625\n",
            "0.203125\n",
            "0.1875\n",
            "0.171875\n",
            "0.140625\n",
            "strain 0.06278251856565475\n",
            "strain 0.04886125773191452\n",
            "strain 0.053211770951747894\n",
            "strain 0.05341597646474838\n",
            "strain 0.05746033415198326\n",
            "strain 0.068270742893219\n",
            "strain 0.05076267942786217\n",
            "strain 0.04972202703356743\n",
            "strain 0.05835796520113945\n",
            "strain 0.05986102670431137\n",
            "strain 0.06311487406492233\n",
            "strain 0.057307712733745575\n",
            "strain 0.058029770851135254\n",
            "strain 0.05843424052000046\n",
            "strain 0.05675703287124634\n",
            "strain 0.0513690821826458\n",
            "strain 0.05145745351910591\n",
            "strain 0.061913974583148956\n",
            "strain 0.059177324175834656\n",
            "strain 0.06847067922353745\n",
            "strain 0.05397028848528862\n",
            "strain 0.05271139368414879\n",
            "strain 0.050320833921432495\n",
            "strain 0.0563054084777832\n",
            "strain 0.0640895888209343\n",
            "strain 0.05542750656604767\n",
            "strain 0.08312714844942093\n",
            "strain 0.05355876311659813\n",
            "strain 0.05383925512433052\n",
            "strain 0.05262799933552742\n",
            "strain 0.05859815701842308\n",
            "strain 0.06328956037759781\n",
            "strain 0.05324476584792137\n",
            "strain 0.055518463253974915\n",
            "strain 0.0663103461265564\n",
            "strain 0.05436060205101967\n",
            "strain 0.05191645398736\n",
            "strain 0.06360716372728348\n",
            "strain 0.06577645242214203\n",
            "strain 0.05856483429670334\n",
            "strain 0.05462087690830231\n",
            "strain 0.058627478778362274\n",
            "strain 0.05425984412431717\n",
            "strain 0.052522215992212296\n",
            "strain 0.05316111072897911\n",
            "strain 0.05994360148906708\n",
            "strain 0.058769214898347855\n",
            "strain 0.05681733787059784\n",
            "strain 0.05296734720468521\n",
            "strain 0.05116985738277435\n",
            "strain 0.051217321306467056\n",
            "classify 2.2879638671875\n",
            "classify 2.283935546875\n",
            "classify 2.284423828125\n",
            "classify 2.2305908203125\n",
            "classify 2.2447509765625\n",
            "classify 2.28564453125\n",
            "classify 2.28564453125\n",
            "classify 2.273193359375\n",
            "classify 2.296630859375\n",
            "classify 2.278076171875\n",
            "classify 2.295166015625\n",
            "0.15625\n",
            "0.078125\n",
            "0.15625\n",
            "0.171875\n",
            "0.125\n",
            "0.15625\n",
            "0.15625\n",
            "0.203125\n",
            "0.25\n",
            "0.1875\n",
            "0.109375\n",
            "strain 0.05695084482431412\n",
            "strain 0.06187933683395386\n",
            "strain 0.05106617882847786\n",
            "strain 0.05949913337826729\n",
            "strain 0.05614309757947922\n",
            "strain 0.06422973424196243\n",
            "strain 0.06330975145101547\n",
            "strain 0.055820949375629425\n",
            "strain 0.07256018370389938\n",
            "strain 0.0552377849817276\n",
            "strain 0.05551405996084213\n",
            "strain 0.056402385234832764\n",
            "strain 0.05642913281917572\n",
            "strain 0.053176820278167725\n",
            "strain 0.05501696839928627\n",
            "strain 0.05097565799951553\n",
            "strain 0.06079763546586037\n",
            "strain 0.06631918996572495\n",
            "strain 0.056630101054906845\n",
            "strain 0.06917795538902283\n",
            "strain 0.05452495068311691\n",
            "strain 0.054574087262153625\n",
            "strain 0.05795952305197716\n",
            "strain 0.05965312942862511\n",
            "strain 0.057935893535614014\n",
            "strain 0.06347624212503433\n",
            "strain 0.05556716397404671\n",
            "strain 0.056203097105026245\n",
            "strain 0.05867402255535126\n",
            "strain 0.061069127172231674\n",
            "strain 0.055237673223018646\n",
            "strain 0.05577268823981285\n",
            "strain 0.0611177459359169\n",
            "strain 0.06225709989666939\n",
            "strain 0.07117505371570587\n",
            "strain 0.05373738333582878\n",
            "strain 0.06585510820150375\n",
            "strain 0.05519895628094673\n",
            "strain 0.0500282347202301\n",
            "strain 0.059583310037851334\n",
            "strain 0.05374809354543686\n",
            "strain 0.055691637098789215\n",
            "strain 0.050531916320323944\n",
            "strain 0.05465926602482796\n",
            "strain 0.05372532084584236\n",
            "strain 0.05733460560441017\n",
            "strain 0.0598452165722847\n",
            "strain 0.07150733470916748\n",
            "strain 0.053128354251384735\n",
            "strain 0.0565045140683651\n",
            "strain 0.06054692342877388\n",
            "classify 2.2674560546875\n",
            "classify 2.2750244140625\n",
            "classify 2.2962646484375\n",
            "classify 2.2598876953125\n",
            "classify 2.25732421875\n",
            "classify 2.279296875\n",
            "classify 2.27294921875\n",
            "classify 2.251708984375\n",
            "classify 2.29150390625\n",
            "classify 2.2537841796875\n",
            "classify 2.2821044921875\n",
            "0.15625\n",
            "0.09375\n",
            "0.203125\n",
            "0.171875\n",
            "0.15625\n",
            "0.140625\n",
            "0.15625\n",
            "0.203125\n",
            "0.09375\n",
            "0.140625\n",
            "0.140625\n",
            "strain 0.05060850456357002\n",
            "strain 0.06001899763941765\n",
            "strain 0.05403243750333786\n",
            "strain 0.06160561740398407\n",
            "strain 0.05664043128490448\n",
            "strain 0.054534900933504105\n",
            "strain 0.06483543664216995\n",
            "strain 0.057542331516742706\n",
            "strain 0.05256453901529312\n",
            "strain 0.057963188737630844\n",
            "strain 0.055786456912755966\n",
            "strain 0.049810752272605896\n",
            "strain 0.05653449147939682\n",
            "strain 0.06503038853406906\n",
            "strain 0.05592476949095726\n",
            "strain 0.06039445102214813\n",
            "strain 0.05558791011571884\n",
            "strain 0.06918880343437195\n",
            "strain 0.05766230449080467\n",
            "strain 0.057328853756189346\n",
            "strain 0.05172969028353691\n",
            "strain 0.05459402874112129\n",
            "strain 0.061136599630117416\n",
            "strain 0.0626642256975174\n",
            "strain 0.06003596633672714\n",
            "strain 0.05691724643111229\n",
            "strain 0.05884537473320961\n",
            "strain 0.06376376748085022\n",
            "strain 0.05358206853270531\n",
            "strain 0.06222331151366234\n",
            "strain 0.06853637844324112\n",
            "strain 0.057972460985183716\n",
            "strain 0.05679553374648094\n",
            "strain 0.05475322902202606\n",
            "strain 0.055824317038059235\n",
            "strain 0.062455564737319946\n",
            "strain 0.05118079110980034\n",
            "strain 0.05702463909983635\n",
            "strain 0.058502182364463806\n",
            "strain 0.059870392084121704\n",
            "strain 0.054031845182180405\n",
            "strain 0.05764149874448776\n",
            "strain 0.057315029203891754\n",
            "strain 0.05276309698820114\n",
            "strain 0.05976560339331627\n",
            "strain 0.05945419520139694\n",
            "strain 0.0660155862569809\n",
            "strain 0.05599208548665047\n",
            "strain 0.05775543302297592\n",
            "strain 0.05344640463590622\n",
            "strain 0.06785617023706436\n",
            "classify 2.227294921875\n",
            "classify 2.2413330078125\n",
            "classify 2.2835693359375\n",
            "classify 2.2664794921875\n",
            "classify 2.2843017578125\n",
            "classify 2.251953125\n",
            "classify 2.3006591796875\n",
            "classify 2.2764892578125\n",
            "classify 2.280517578125\n",
            "classify 2.2691650390625\n",
            "classify 2.239013671875\n",
            "0.1875\n",
            "0.078125\n",
            "0.203125\n",
            "0.171875\n",
            "0.15625\n",
            "0.140625\n",
            "0.1875\n",
            "0.1875\n",
            "0.21875\n",
            "0.171875\n",
            "0.125\n",
            "strain 0.058294475078582764\n",
            "strain 0.05578766390681267\n",
            "strain 0.057078856974840164\n",
            "strain 0.051645100116729736\n",
            "strain 0.06004386395215988\n",
            "strain 0.05194835737347603\n",
            "strain 0.05570521205663681\n",
            "strain 0.058493416756391525\n",
            "strain 0.05585922673344612\n",
            "strain 0.05599996820092201\n",
            "strain 0.056321755051612854\n",
            "strain 0.05438645929098129\n",
            "strain 0.07206064462661743\n",
            "strain 0.059438541531562805\n",
            "strain 0.0701892077922821\n",
            "strain 0.050483088940382004\n",
            "strain 0.049335990101099014\n",
            "strain 0.07306192070245743\n",
            "strain 0.05509861558675766\n",
            "strain 0.053820669651031494\n",
            "strain 0.05646674707531929\n",
            "strain 0.05531662702560425\n",
            "strain 0.05181051418185234\n",
            "strain 0.062011491507291794\n",
            "strain 0.060857903212308884\n",
            "strain 0.052520688623189926\n",
            "strain 0.05573059991002083\n",
            "strain 0.06122048571705818\n",
            "strain 0.05631794407963753\n",
            "strain 0.05078883096575737\n",
            "strain 0.05628766119480133\n",
            "strain 0.057655688375234604\n",
            "strain 0.052824124693870544\n",
            "strain 0.054679062217473984\n",
            "strain 0.048560116440057755\n",
            "strain 0.06350179761648178\n",
            "strain 0.0630195215344429\n",
            "strain 0.05991242825984955\n",
            "strain 0.05988297984004021\n",
            "strain 0.051267899572849274\n",
            "strain 0.06500411033630371\n",
            "strain 0.05642225965857506\n",
            "strain 0.057887379080057144\n",
            "strain 0.0622086264193058\n",
            "strain 0.05489684268832207\n",
            "strain 0.05223356932401657\n",
            "strain 0.05977226421236992\n",
            "strain 0.050326209515333176\n",
            "strain 0.06744302064180374\n",
            "strain 0.05191420018672943\n",
            "strain 0.0692959576845169\n",
            "classify 2.2728271484375\n",
            "classify 2.28662109375\n",
            "classify 2.318115234375\n",
            "classify 2.2628173828125\n",
            "classify 2.260986328125\n",
            "classify 2.23681640625\n",
            "classify 2.298095703125\n",
            "classify 2.2891845703125\n",
            "classify 2.2435302734375\n",
            "classify 2.25048828125\n",
            "classify 2.3035888671875\n",
            "0.21875\n",
            "0.109375\n",
            "0.25\n",
            "0.15625\n",
            "0.15625\n",
            "0.171875\n",
            "0.234375\n",
            "0.203125\n",
            "0.171875\n",
            "0.1875\n",
            "0.125\n",
            "strain 0.05337079241871834\n",
            "strain 0.06339050084352493\n",
            "strain 0.05058921128511429\n",
            "strain 0.058736518025398254\n",
            "strain 0.05173991993069649\n",
            "strain 0.06573419272899628\n",
            "strain 0.05217569321393967\n",
            "strain 0.05366072058677673\n",
            "strain 0.0525115430355072\n",
            "strain 0.04919048771262169\n",
            "strain 0.05493119731545448\n",
            "strain 0.05928364396095276\n",
            "strain 0.06765005737543106\n",
            "strain 0.057070542126894\n",
            "strain 0.07717189192771912\n",
            "strain 0.049996498972177505\n",
            "strain 0.05631206929683685\n",
            "strain 0.05365867540240288\n",
            "strain 0.05795004963874817\n",
            "strain 0.05819110944867134\n",
            "strain 0.05629405006766319\n",
            "strain 0.06252609938383102\n",
            "strain 0.05887080729007721\n",
            "strain 0.058083418756723404\n",
            "strain 0.060706429183483124\n",
            "strain 0.05865310877561569\n",
            "strain 0.05782390758395195\n",
            "strain 0.05494885891675949\n",
            "strain 0.06397923082113266\n",
            "strain 0.05406152084469795\n",
            "strain 0.060046736150979996\n",
            "strain 0.049558427184820175\n",
            "strain 0.051437731832265854\n",
            "strain 0.06491385400295258\n",
            "strain 0.05887144058942795\n",
            "strain 0.056636743247509\n",
            "strain 0.049692969769239426\n",
            "strain 0.05825107917189598\n",
            "strain 0.058305609971284866\n",
            "strain 0.06024131923913956\n",
            "strain 0.06757476925849915\n",
            "strain 0.055274687707424164\n",
            "strain 0.05920283496379852\n",
            "strain 0.048606354743242264\n",
            "strain 0.06995262950658798\n",
            "strain 0.050402481108903885\n",
            "strain 0.07195953279733658\n",
            "strain 0.052383627742528915\n",
            "strain 0.05824761837720871\n",
            "strain 0.06942898035049438\n",
            "strain 0.067221499979496\n",
            "classify 2.26953125\n",
            "classify 2.2935791015625\n",
            "classify 2.29150390625\n",
            "classify 2.262451171875\n",
            "classify 2.2886962890625\n",
            "classify 2.24609375\n",
            "classify 2.281494140625\n",
            "classify 2.2440185546875\n",
            "classify 2.2650146484375\n",
            "classify 2.2791748046875\n",
            "classify 2.2479248046875\n",
            "0.203125\n",
            "0.109375\n",
            "0.234375\n",
            "0.15625\n",
            "0.140625\n",
            "0.140625\n",
            "0.203125\n",
            "0.15625\n",
            "0.1875\n",
            "0.203125\n",
            "0.109375\n",
            "strain 0.05453943461179733\n",
            "strain 0.05859054625034332\n",
            "strain 0.057448577135801315\n",
            "strain 0.051378969103097916\n",
            "strain 0.06676993519067764\n",
            "strain 0.056232523173093796\n",
            "strain 0.05098229646682739\n",
            "strain 0.05863650143146515\n",
            "strain 0.06687458604574203\n",
            "strain 0.05805151164531708\n",
            "strain 0.06548755615949631\n",
            "strain 0.058439530432224274\n",
            "strain 0.05674248933792114\n",
            "strain 0.05789240449666977\n",
            "strain 0.06040496379137039\n",
            "strain 0.05621059611439705\n",
            "strain 0.059439487755298615\n",
            "strain 0.053632237017154694\n",
            "strain 0.048179786652326584\n",
            "strain 0.05892070755362511\n",
            "strain 0.0615871399641037\n",
            "strain 0.06160243600606918\n",
            "strain 0.05384300276637077\n",
            "strain 0.052129294723272324\n",
            "strain 0.061787571758031845\n",
            "strain 0.06056683883070946\n",
            "strain 0.05218906328082085\n",
            "strain 0.05988196283578873\n",
            "strain 0.057133425027132034\n",
            "strain 0.06775899976491928\n",
            "strain 0.04821067675948143\n",
            "strain 0.05656825006008148\n",
            "strain 0.06605882197618484\n",
            "strain 0.05333038046956062\n",
            "strain 0.05016721785068512\n",
            "strain 0.057110704481601715\n",
            "strain 0.06311039626598358\n",
            "strain 0.05571453645825386\n",
            "strain 0.04838041216135025\n",
            "strain 0.05990012735128403\n",
            "strain 0.05969903618097305\n",
            "strain 0.05530424043536186\n",
            "strain 0.05836193636059761\n",
            "strain 0.05845816433429718\n",
            "strain 0.06343192607164383\n",
            "strain 0.05232252925634384\n",
            "strain 0.06856296211481094\n",
            "strain 0.06366072595119476\n",
            "strain 0.060003407299518585\n",
            "strain 0.060340121388435364\n",
            "strain 0.06397800147533417\n",
            "classify 2.2548828125\n",
            "classify 2.3123779296875\n",
            "classify 2.26220703125\n",
            "classify 2.25341796875\n",
            "classify 2.2523193359375\n",
            "classify 2.30419921875\n",
            "classify 2.2115478515625\n",
            "classify 2.244873046875\n",
            "classify 2.2852783203125\n",
            "classify 2.28076171875\n",
            "classify 2.26708984375\n",
            "0.15625\n",
            "0.09375\n",
            "0.1875\n",
            "0.140625\n",
            "0.109375\n",
            "0.203125\n",
            "0.1875\n",
            "0.1875\n",
            "0.15625\n",
            "0.171875\n",
            "0.109375\n",
            "strain 0.06193426623940468\n",
            "strain 0.06077467277646065\n",
            "strain 0.05777403712272644\n",
            "strain 0.051845483481884\n",
            "strain 0.0618295781314373\n",
            "strain 0.05475260317325592\n",
            "strain 0.05816971883177757\n",
            "strain 0.061002280563116074\n",
            "strain 0.06423918902873993\n",
            "strain 0.053087230771780014\n",
            "strain 0.052005719393491745\n",
            "strain 0.07745847851037979\n",
            "strain 0.06048397719860077\n",
            "strain 0.05652884021401405\n",
            "strain 0.05166134983301163\n",
            "strain 0.06251116842031479\n",
            "strain 0.06009473279118538\n",
            "strain 0.061678446829319\n",
            "strain 0.0567157045006752\n",
            "strain 0.05633091554045677\n",
            "strain 0.061876751482486725\n",
            "strain 0.062362853437662125\n",
            "strain 0.056684333831071854\n",
            "strain 0.06298394501209259\n",
            "strain 0.05135612562298775\n",
            "strain 0.061959635466337204\n",
            "strain 0.054399240761995316\n",
            "strain 0.06178919970989227\n",
            "strain 0.05168154090642929\n",
            "strain 0.06207139790058136\n",
            "strain 0.051530882716178894\n",
            "strain 0.05177891254425049\n",
            "strain 0.05044248327612877\n",
            "strain 0.05131835117936134\n",
            "strain 0.05964037775993347\n",
            "strain 0.04970294237136841\n",
            "strain 0.06183869391679764\n",
            "strain 0.050431154668331146\n",
            "strain 0.05574299767613411\n",
            "strain 0.0516495481133461\n",
            "strain 0.05149877443909645\n",
            "strain 0.053696490824222565\n",
            "strain 0.05964187532663345\n",
            "strain 0.06716509163379669\n",
            "strain 0.06734342128038406\n",
            "strain 0.06087575480341911\n",
            "strain 0.05453253537416458\n",
            "strain 0.07373560965061188\n",
            "strain 0.06594663858413696\n",
            "strain 0.05479389801621437\n",
            "strain 0.07013600319623947\n",
            "classify 2.2646484375\n",
            "classify 2.2791748046875\n",
            "classify 2.2333984375\n",
            "classify 2.2733154296875\n",
            "classify 2.253173828125\n",
            "classify 2.2830810546875\n",
            "classify 2.258544921875\n",
            "classify 2.271240234375\n",
            "classify 2.2928466796875\n",
            "classify 2.2779541015625\n",
            "classify 2.2369384765625\n",
            "0.203125\n",
            "0.09375\n",
            "0.203125\n",
            "0.15625\n",
            "0.171875\n",
            "0.140625\n",
            "0.140625\n",
            "0.21875\n",
            "0.171875\n",
            "0.1875\n",
            "0.140625\n",
            "strain 0.05357614532113075\n",
            "strain 0.059669531881809235\n",
            "strain 0.04692576825618744\n",
            "strain 0.05943508818745613\n",
            "strain 0.054104819893836975\n",
            "strain 0.06353426724672318\n",
            "strain 0.054941486567258835\n",
            "strain 0.06666712462902069\n",
            "strain 0.05445100739598274\n",
            "strain 0.06260517239570618\n",
            "strain 0.05445273593068123\n",
            "strain 0.06258166581392288\n",
            "strain 0.058031730353832245\n",
            "strain 0.057289037853479385\n",
            "strain 0.05275826156139374\n",
            "strain 0.057174667716026306\n",
            "strain 0.055151548236608505\n",
            "strain 0.06223621591925621\n",
            "strain 0.05258575826883316\n",
            "strain 0.05374060198664665\n",
            "strain 0.060718830674886703\n",
            "strain 0.06626702845096588\n",
            "strain 0.06008775904774666\n",
            "strain 0.06015418842434883\n",
            "strain 0.06356342881917953\n",
            "strain 0.06077822297811508\n",
            "strain 0.06924643367528915\n",
            "strain 0.055848777294158936\n",
            "strain 0.04471695423126221\n",
            "strain 0.05209605023264885\n",
            "strain 0.06048888713121414\n",
            "strain 0.06699806451797485\n",
            "strain 0.0725761279463768\n",
            "strain 0.055301230400800705\n",
            "strain 0.0676790252327919\n",
            "strain 0.04578658938407898\n",
            "strain 0.06058914586901665\n",
            "strain 0.054766714572906494\n",
            "strain 0.054377175867557526\n",
            "strain 0.054961688816547394\n",
            "strain 0.06642501801252365\n",
            "strain 0.056581784039735794\n",
            "strain 0.05372638255357742\n",
            "strain 0.05864843353629112\n",
            "strain 0.06492658704519272\n",
            "strain 0.05368855223059654\n",
            "strain 0.06139247491955757\n",
            "strain 0.06901723891496658\n",
            "strain 0.052261870354413986\n",
            "strain 0.06571699678897858\n",
            "strain 0.05100119858980179\n",
            "classify 2.2904052734375\n",
            "classify 2.2525634765625\n",
            "classify 2.2440185546875\n",
            "classify 2.289306640625\n",
            "classify 2.292724609375\n",
            "classify 2.2745361328125\n",
            "classify 2.289794921875\n",
            "classify 2.249267578125\n",
            "classify 2.247314453125\n",
            "classify 2.2237548828125\n",
            "classify 2.2259521484375\n",
            "0.15625\n",
            "0.078125\n",
            "0.203125\n",
            "0.140625\n",
            "0.125\n",
            "0.140625\n",
            "0.234375\n",
            "0.171875\n",
            "0.203125\n",
            "0.15625\n",
            "0.125\n",
            "strain 0.05540570244193077\n",
            "strain 0.06441059708595276\n",
            "strain 0.0582752600312233\n",
            "strain 0.054072242230176926\n",
            "strain 0.06046372652053833\n",
            "strain 0.05056072771549225\n",
            "strain 0.05174620449542999\n",
            "strain 0.05785365402698517\n",
            "strain 0.05431554839015007\n",
            "strain 0.05898177623748779\n",
            "strain 0.061943795531988144\n",
            "strain 0.05942818522453308\n",
            "strain 0.057598210871219635\n",
            "strain 0.05405870079994202\n",
            "strain 0.05626627057790756\n",
            "strain 0.05318351835012436\n",
            "strain 0.06980720162391663\n",
            "strain 0.0554041750729084\n",
            "strain 0.048374127596616745\n",
            "strain 0.0550917312502861\n",
            "strain 0.04706345871090889\n",
            "strain 0.06153200939297676\n",
            "strain 0.05314425751566887\n",
            "strain 0.061459608376026154\n",
            "strain 0.06553192436695099\n",
            "strain 0.06498695909976959\n",
            "strain 0.05610891059041023\n",
            "strain 0.054409049451351166\n",
            "strain 0.048835985362529755\n",
            "strain 0.05590985715389252\n",
            "strain 0.050877854228019714\n",
            "strain 0.060201700776815414\n",
            "strain 0.0648355782032013\n",
            "strain 0.05497366562485695\n",
            "strain 0.05826430395245552\n",
            "strain 0.061168570071458817\n",
            "strain 0.05753658339381218\n",
            "strain 0.057376962155103683\n",
            "strain 0.05831645056605339\n",
            "strain 0.053877249360084534\n",
            "strain 0.06148790940642357\n",
            "strain 0.055263955146074295\n",
            "strain 0.06335476040840149\n",
            "strain 0.052225854247808456\n",
            "strain 0.055331818759441376\n",
            "strain 0.06273702532052994\n",
            "strain 0.05644729360938072\n",
            "strain 0.062193047255277634\n",
            "strain 0.05570228770375252\n",
            "strain 0.05198473483324051\n",
            "strain 0.06406696885824203\n",
            "classify 2.265869140625\n",
            "classify 2.3204345703125\n",
            "classify 2.275634765625\n",
            "classify 2.23095703125\n",
            "classify 2.2861328125\n",
            "classify 2.2567138671875\n",
            "classify 2.23291015625\n",
            "classify 2.2724609375\n",
            "classify 2.333251953125\n",
            "classify 2.2978515625\n",
            "classify 2.2698974609375\n",
            "0.140625\n",
            "0.09375\n",
            "0.234375\n",
            "0.140625\n",
            "0.140625\n",
            "0.09375\n",
            "0.1875\n",
            "0.234375\n",
            "0.1875\n",
            "0.203125\n",
            "0.15625\n",
            "strain 0.05540609359741211\n",
            "strain 0.053501781076192856\n",
            "strain 0.05865362659096718\n",
            "strain 0.05954409018158913\n",
            "strain 0.051509350538253784\n",
            "strain 0.049913354218006134\n",
            "strain 0.061515726149082184\n",
            "strain 0.059478603303432465\n",
            "strain 0.06498242169618607\n",
            "strain 0.051340632140636444\n",
            "strain 0.06314115226268768\n",
            "strain 0.06252309679985046\n",
            "strain 0.055478569120168686\n",
            "strain 0.06105353683233261\n",
            "strain 0.053038761019706726\n",
            "strain 0.052759718149900436\n",
            "strain 0.05851409584283829\n",
            "strain 0.04990973323583603\n",
            "strain 0.06631249189376831\n",
            "strain 0.05429248511791229\n",
            "strain 0.05108314007520676\n",
            "strain 0.05546297878026962\n",
            "strain 0.0544818677008152\n",
            "strain 0.05138801783323288\n",
            "strain 0.06381608545780182\n",
            "strain 0.07360178232192993\n",
            "strain 0.052243921905756\n",
            "strain 0.061916936188936234\n",
            "strain 0.051270004361867905\n",
            "strain 0.05164046213030815\n",
            "strain 0.06958194822072983\n",
            "strain 0.05914691090583801\n",
            "strain 0.05795084685087204\n",
            "strain 0.059177935123443604\n",
            "strain 0.05678858980536461\n",
            "strain 0.0607021227478981\n",
            "strain 0.0536884181201458\n",
            "strain 0.05146518722176552\n",
            "strain 0.05234399437904358\n",
            "strain 0.060797229409217834\n",
            "strain 0.0683460608124733\n",
            "strain 0.04959172010421753\n",
            "strain 0.050332266837358475\n",
            "strain 0.06352245807647705\n",
            "strain 0.052046775817871094\n",
            "strain 0.06470481306314468\n",
            "strain 0.0581243559718132\n",
            "strain 0.07219510525465012\n",
            "strain 0.06448069959878922\n",
            "strain 0.05743227154016495\n",
            "strain 0.058315131813287735\n",
            "classify 2.2701416015625\n",
            "classify 2.233642578125\n",
            "classify 2.29443359375\n",
            "classify 2.314453125\n",
            "classify 2.2938232421875\n",
            "classify 2.2279052734375\n",
            "classify 2.30615234375\n",
            "classify 2.228271484375\n",
            "classify 2.2291259765625\n",
            "classify 2.2718505859375\n",
            "classify 2.261474609375\n",
            "0.171875\n",
            "0.09375\n",
            "0.1875\n",
            "0.15625\n",
            "0.125\n",
            "0.15625\n",
            "0.1875\n",
            "0.203125\n",
            "0.15625\n",
            "0.1875\n",
            "0.140625\n",
            "strain 0.06475923210382462\n",
            "strain 0.0638081282377243\n",
            "strain 0.05892513692378998\n",
            "strain 0.0773114487528801\n",
            "strain 0.06007777899503708\n",
            "strain 0.05030148848891258\n",
            "strain 0.05220312997698784\n",
            "strain 0.05717270448803902\n",
            "strain 0.061656028032302856\n",
            "strain 0.058201197534799576\n",
            "strain 0.05463268607854843\n",
            "strain 0.06384529173374176\n",
            "strain 0.0656835287809372\n",
            "strain 0.054333869367837906\n",
            "strain 0.0595456063747406\n",
            "strain 0.06137039512395859\n",
            "strain 0.06006117910146713\n",
            "strain 0.046886809170246124\n",
            "strain 0.06220932677388191\n",
            "strain 0.05504154786467552\n",
            "strain 0.053734827786684036\n",
            "strain 0.05854456499218941\n",
            "strain 0.05800413712859154\n",
            "strain 0.053087711334228516\n",
            "strain 0.059328265488147736\n",
            "strain 0.054928187280893326\n",
            "strain 0.05830145254731178\n",
            "strain 0.06450848281383514\n",
            "strain 0.0523567721247673\n",
            "strain 0.06248731166124344\n",
            "strain 0.05670361593365669\n",
            "strain 0.057741839438676834\n",
            "strain 0.06430211663246155\n",
            "strain 0.05689285323023796\n",
            "strain 0.04888959228992462\n",
            "strain 0.051800403743982315\n",
            "strain 0.06156524643301964\n",
            "strain 0.06738722324371338\n",
            "strain 0.058842357248067856\n",
            "strain 0.05481180176138878\n",
            "strain 0.056826408952474594\n",
            "strain 0.054498981684446335\n",
            "strain 0.0511527843773365\n",
            "strain 0.05711536854505539\n",
            "strain 0.05244497209787369\n",
            "strain 0.05087940767407417\n",
            "strain 0.056886546313762665\n",
            "strain 0.06003393977880478\n",
            "strain 0.05739380046725273\n",
            "strain 0.058887649327516556\n",
            "strain 0.061474692076444626\n",
            "classify 2.24609375\n",
            "classify 2.308837890625\n",
            "classify 2.3232421875\n",
            "classify 2.2757568359375\n",
            "classify 2.263916015625\n",
            "classify 2.2535400390625\n",
            "classify 2.2685546875\n",
            "classify 2.2708740234375\n",
            "classify 2.2777099609375\n",
            "classify 2.2799072265625\n",
            "classify 2.3076171875\n",
            "0.21875\n",
            "0.09375\n",
            "0.203125\n",
            "0.15625\n",
            "0.140625\n",
            "0.15625\n",
            "0.203125\n",
            "0.15625\n",
            "0.1875\n",
            "0.171875\n",
            "0.140625\n",
            "strain 0.055641114711761475\n",
            "strain 0.056226782500743866\n",
            "strain 0.05017451196908951\n",
            "strain 0.050573792308568954\n",
            "strain 0.054602835327386856\n",
            "strain 0.04713062942028046\n",
            "strain 0.061168767511844635\n",
            "strain 0.07086256891489029\n",
            "strain 0.0611102432012558\n",
            "strain 0.059421684592962265\n",
            "strain 0.06297417730093002\n",
            "strain 0.06395349651575089\n",
            "strain 0.0524163655936718\n",
            "strain 0.064361073076725\n",
            "strain 0.06383258104324341\n",
            "strain 0.05257629603147507\n",
            "strain 0.06334517151117325\n",
            "strain 0.04985751956701279\n",
            "strain 0.052455268800258636\n",
            "strain 0.059868209064006805\n",
            "strain 0.054232049733400345\n",
            "strain 0.05627195164561272\n",
            "strain 0.05519196391105652\n",
            "strain 0.05549323186278343\n",
            "strain 0.054476406425237656\n",
            "strain 0.051002807915210724\n",
            "strain 0.06027226895093918\n",
            "strain 0.056157082319259644\n",
            "strain 0.08345520496368408\n",
            "strain 0.06059886887669563\n",
            "strain 0.0591551810503006\n",
            "strain 0.05612817034125328\n",
            "strain 0.056756392121315\n",
            "strain 0.053017109632492065\n",
            "strain 0.07576145976781845\n",
            "strain 0.05333820730447769\n",
            "strain 0.0645366758108139\n",
            "strain 0.05266224592924118\n",
            "strain 0.058387208729982376\n",
            "strain 0.05136650428175926\n",
            "strain 0.05354750528931618\n",
            "strain 0.058239176869392395\n",
            "strain 0.05257672816514969\n",
            "strain 0.06218663603067398\n",
            "strain 0.059376586228609085\n",
            "strain 0.05475733429193497\n",
            "strain 0.05386741831898689\n",
            "strain 0.06046679988503456\n",
            "strain 0.06553489714860916\n",
            "strain 0.06265868246555328\n",
            "strain 0.05891624465584755\n",
            "classify 2.2266845703125\n",
            "classify 2.2952880859375\n",
            "classify 2.2674560546875\n",
            "classify 2.25439453125\n",
            "classify 2.2928466796875\n",
            "classify 2.253173828125\n",
            "classify 2.2828369140625\n",
            "classify 2.2449951171875\n",
            "classify 2.2548828125\n",
            "classify 2.32080078125\n",
            "classify 2.255615234375\n",
            "0.203125\n",
            "0.046875\n",
            "0.21875\n",
            "0.1875\n",
            "0.140625\n",
            "0.125\n",
            "0.1875\n",
            "0.1875\n",
            "0.1875\n",
            "0.1875\n",
            "0.09375\n",
            "strain 0.06793399900197983\n",
            "strain 0.050814878195524216\n",
            "strain 0.06548222154378891\n",
            "strain 0.053693391382694244\n",
            "strain 0.06144128367304802\n",
            "strain 0.06870177388191223\n",
            "strain 0.06553032249212265\n",
            "strain 0.050583869218826294\n",
            "strain 0.04886772111058235\n",
            "strain 0.04811399430036545\n",
            "strain 0.05728892609477043\n",
            "strain 0.061273351311683655\n",
            "strain 0.06603284925222397\n",
            "strain 0.056894801557064056\n",
            "strain 0.051533304154872894\n",
            "strain 0.05668945237994194\n",
            "strain 0.05297376215457916\n",
            "strain 0.05430537089705467\n",
            "strain 0.06007423251867294\n",
            "strain 0.059233974665403366\n",
            "strain 0.05697829648852348\n",
            "strain 0.05627821758389473\n",
            "strain 0.05554656684398651\n",
            "strain 0.055614370852708817\n",
            "strain 0.061814822256565094\n",
            "strain 0.06540345400571823\n",
            "strain 0.06458046287298203\n",
            "strain 0.06094958633184433\n",
            "strain 0.052857525646686554\n",
            "strain 0.05385122075676918\n",
            "strain 0.051306333392858505\n",
            "strain 0.06279486417770386\n",
            "strain 0.06300656497478485\n",
            "strain 0.06175745651125908\n",
            "strain 0.0571938157081604\n",
            "strain 0.06500113755464554\n",
            "strain 0.05380060151219368\n",
            "strain 0.06221078708767891\n",
            "strain 0.06101101636886597\n",
            "strain 0.07153912633657455\n",
            "strain 0.061044614762067795\n",
            "strain 0.05443263053894043\n",
            "strain 0.05336132273077965\n",
            "strain 0.056511204689741135\n",
            "strain 0.04933848977088928\n",
            "strain 0.0573631152510643\n",
            "strain 0.05487941578030586\n",
            "strain 0.0601787306368351\n",
            "strain 0.06375430524349213\n",
            "strain 0.06736408174037933\n",
            "strain 0.05446288734674454\n",
            "classify 2.2705078125\n",
            "classify 2.22998046875\n",
            "classify 2.263427734375\n",
            "classify 2.2615966796875\n",
            "classify 2.302978515625\n",
            "classify 2.2388916015625\n",
            "classify 2.2742919921875\n",
            "classify 2.26220703125\n",
            "classify 2.261962890625\n",
            "classify 2.2734375\n",
            "classify 2.275146484375\n",
            "0.171875\n",
            "0.0625\n",
            "0.21875\n",
            "0.125\n",
            "0.203125\n",
            "0.125\n",
            "0.1875\n",
            "0.125\n",
            "0.21875\n",
            "0.25\n",
            "0.09375\n",
            "strain 0.052520349621772766\n",
            "strain 0.05940399691462517\n",
            "strain 0.05503268167376518\n",
            "strain 0.05419418588280678\n",
            "strain 0.0501069650053978\n",
            "strain 0.05752753093838692\n",
            "strain 0.06166307255625725\n",
            "strain 0.05581779405474663\n",
            "strain 0.06114562228322029\n",
            "strain 0.062131959944963455\n",
            "strain 0.06063799187541008\n",
            "strain 0.05739014595746994\n",
            "strain 0.05037233978509903\n",
            "strain 0.0656493604183197\n",
            "strain 0.06471459567546844\n",
            "strain 0.062345605343580246\n",
            "strain 0.055740516632795334\n",
            "strain 0.061974573880434036\n",
            "strain 0.053133685141801834\n",
            "strain 0.05545387044548988\n",
            "strain 0.055777229368686676\n",
            "strain 0.056869376450777054\n",
            "strain 0.055629197508096695\n",
            "strain 0.05709585174918175\n",
            "strain 0.05264725163578987\n",
            "strain 0.05708736926317215\n",
            "strain 0.05757845193147659\n",
            "strain 0.05596258491277695\n",
            "strain 0.06466889381408691\n",
            "strain 0.05579669401049614\n",
            "strain 0.06286896020174026\n",
            "strain 0.06525847315788269\n",
            "strain 0.06011063978075981\n",
            "strain 0.055945441126823425\n",
            "strain 0.05713105946779251\n",
            "strain 0.06005268543958664\n",
            "strain 0.054726533591747284\n",
            "strain 0.05277128145098686\n",
            "strain 0.05059986934065819\n",
            "strain 0.05192207545042038\n",
            "strain 0.05315832793712616\n",
            "strain 0.05870228260755539\n",
            "strain 0.05913173034787178\n",
            "strain 0.06994644552469254\n",
            "strain 0.05749911069869995\n",
            "strain 0.06353570520877838\n",
            "strain 0.05464475601911545\n",
            "strain 0.04896507412195206\n",
            "strain 0.06101546064019203\n",
            "strain 0.06081831082701683\n",
            "strain 0.059134215116500854\n",
            "classify 2.256103515625\n",
            "classify 2.2646484375\n",
            "classify 2.30224609375\n",
            "classify 2.224853515625\n",
            "classify 2.2540283203125\n",
            "classify 2.2552490234375\n",
            "classify 2.294677734375\n",
            "classify 2.266845703125\n",
            "classify 2.2830810546875\n",
            "classify 2.306640625\n",
            "classify 2.2530517578125\n",
            "0.15625\n",
            "0.078125\n",
            "0.203125\n",
            "0.171875\n",
            "0.109375\n",
            "0.140625\n",
            "0.1875\n",
            "0.1875\n",
            "0.171875\n",
            "0.09375\n",
            "0.109375\n",
            "strain 0.058488525450229645\n",
            "strain 0.06643244624137878\n",
            "strain 0.057478681206703186\n",
            "strain 0.05819127336144447\n",
            "strain 0.0606595017015934\n",
            "strain 0.0647808387875557\n",
            "strain 0.0562170185148716\n",
            "strain 0.050549302250146866\n",
            "strain 0.06458760797977448\n",
            "strain 0.05461404100060463\n",
            "strain 0.05902683362364769\n",
            "strain 0.05986428260803223\n",
            "strain 0.05910282954573631\n",
            "strain 0.06198163703083992\n",
            "strain 0.060789935290813446\n",
            "strain 0.05878595635294914\n",
            "strain 0.0657755583524704\n",
            "strain 0.05073774605989456\n",
            "strain 0.04760891571640968\n",
            "strain 0.05060727894306183\n",
            "strain 0.05972728505730629\n",
            "strain 0.0599164180457592\n",
            "strain 0.04807865247130394\n",
            "strain 0.048878926783800125\n",
            "strain 0.05471928045153618\n",
            "strain 0.062128495424985886\n",
            "strain 0.05563841387629509\n",
            "strain 0.0628913938999176\n",
            "strain 0.06036602705717087\n",
            "strain 0.061174750328063965\n",
            "strain 0.0565766803920269\n",
            "strain 0.05495636910200119\n",
            "strain 0.05381820350885391\n",
            "strain 0.05056769773364067\n",
            "strain 0.05836325138807297\n",
            "strain 0.05878244340419769\n",
            "strain 0.05304377153515816\n",
            "strain 0.05092078447341919\n",
            "strain 0.05582049489021301\n",
            "strain 0.05269214138388634\n",
            "strain 0.0549854040145874\n",
            "strain 0.060513775795698166\n",
            "strain 0.04958829656243324\n",
            "strain 0.05521268770098686\n",
            "strain 0.05329250916838646\n",
            "strain 0.06036701053380966\n",
            "strain 0.05798511579632759\n",
            "strain 0.06079910323023796\n",
            "strain 0.05517224594950676\n",
            "strain 0.05786607041954994\n",
            "strain 0.06152956560254097\n",
            "classify 2.2713623046875\n",
            "classify 2.261474609375\n",
            "classify 2.24658203125\n",
            "classify 2.26171875\n",
            "classify 2.2958984375\n",
            "classify 2.3028564453125\n",
            "classify 2.2947998046875\n",
            "classify 2.332763671875\n",
            "classify 2.2630615234375\n",
            "classify 2.2474365234375\n",
            "classify 2.315673828125\n",
            "0.203125\n",
            "0.109375\n",
            "0.171875\n",
            "0.15625\n",
            "0.140625\n",
            "0.203125\n",
            "0.1875\n",
            "0.15625\n",
            "0.171875\n",
            "0.109375\n",
            "0.109375\n",
            "strain 0.0501147136092186\n",
            "strain 0.057762060314416885\n",
            "strain 0.05270210653543472\n",
            "strain 0.05075053498148918\n",
            "strain 0.04583066329360008\n",
            "strain 0.05023175850510597\n",
            "strain 0.05296528711915016\n",
            "strain 0.059446584433317184\n",
            "strain 0.057006705552339554\n",
            "strain 0.0547027550637722\n",
            "strain 0.060796938836574554\n",
            "strain 0.0723031610250473\n",
            "strain 0.05608998239040375\n",
            "strain 0.05724640563130379\n",
            "strain 0.05861277878284454\n",
            "strain 0.07178144156932831\n",
            "strain 0.0657709538936615\n",
            "strain 0.061057791113853455\n",
            "strain 0.05853617936372757\n",
            "strain 0.05468239635229111\n",
            "strain 0.05298088863492012\n",
            "strain 0.055256567895412445\n",
            "strain 0.04297184944152832\n",
            "strain 0.050097785890102386\n",
            "strain 0.06150321662425995\n",
            "strain 0.06086244806647301\n",
            "strain 0.05993644893169403\n",
            "strain 0.055417317897081375\n",
            "strain 0.05590585619211197\n",
            "strain 0.05497559532523155\n",
            "strain 0.05824621394276619\n",
            "strain 0.06175531446933746\n",
            "strain 0.06428732722997665\n",
            "strain 0.05502147972583771\n",
            "strain 0.0563431978225708\n",
            "strain 0.056906118988990784\n",
            "strain 0.07107386738061905\n",
            "strain 0.048637572675943375\n",
            "strain 0.051435209810733795\n",
            "strain 0.061290595680475235\n",
            "strain 0.06099904328584671\n",
            "strain 0.05708969756960869\n",
            "strain 0.04795137420296669\n",
            "strain 0.05663637816905975\n",
            "strain 0.05783157795667648\n",
            "strain 0.05370297282934189\n",
            "strain 0.05759013444185257\n",
            "strain 0.05390758812427521\n",
            "strain 0.06038140133023262\n",
            "strain 0.05507887527346611\n",
            "strain 0.06381034851074219\n",
            "classify 2.2403564453125\n",
            "classify 2.267333984375\n",
            "classify 2.2567138671875\n",
            "classify 2.3118896484375\n",
            "classify 2.268798828125\n",
            "classify 2.232666015625\n",
            "classify 2.2677001953125\n",
            "classify 2.2518310546875\n",
            "classify 2.2679443359375\n",
            "classify 2.2843017578125\n",
            "classify 2.2908935546875\n",
            "0.1875\n",
            "0.078125\n",
            "0.234375\n",
            "0.140625\n",
            "0.15625\n",
            "0.171875\n",
            "0.15625\n",
            "0.21875\n",
            "0.171875\n",
            "0.171875\n",
            "0.09375\n",
            "strain 0.04970721900463104\n",
            "strain 0.048314277082681656\n",
            "strain 0.04946776479482651\n",
            "strain 0.04838358610868454\n",
            "strain 0.04680073633790016\n",
            "strain 0.06702381372451782\n",
            "strain 0.057273298501968384\n",
            "strain 0.04794938489794731\n",
            "strain 0.056988637894392014\n",
            "strain 0.06360632926225662\n",
            "strain 0.058040302246809006\n",
            "strain 0.05317254737019539\n",
            "strain 0.05167170986533165\n",
            "strain 0.05779481679201126\n",
            "strain 0.0674281194806099\n",
            "strain 0.05275624245405197\n",
            "strain 0.05014243721961975\n",
            "strain 0.05736800655722618\n",
            "strain 0.058315593749284744\n",
            "strain 0.06331003457307816\n",
            "strain 0.05806289240717888\n",
            "strain 0.06210609897971153\n",
            "strain 0.05381365492939949\n",
            "strain 0.06267117708921432\n",
            "strain 0.05254318192601204\n",
            "strain 0.0702713206410408\n",
            "strain 0.05305025726556778\n",
            "strain 0.052570946514606476\n",
            "strain 0.05557892844080925\n",
            "strain 0.0654141902923584\n",
            "strain 0.05694785714149475\n",
            "strain 0.062066420912742615\n",
            "strain 0.059187859296798706\n",
            "strain 0.06361288577318192\n",
            "strain 0.06859665364027023\n",
            "strain 0.0547194704413414\n",
            "strain 0.04487544298171997\n",
            "strain 0.05304408818483353\n",
            "strain 0.05989396944642067\n",
            "strain 0.06857261061668396\n",
            "strain 0.06617297232151031\n",
            "strain 0.06775180250406265\n",
            "strain 0.052558936178684235\n",
            "strain 0.053334761410951614\n",
            "strain 0.05150410905480385\n",
            "strain 0.05859363079071045\n",
            "strain 0.05807601660490036\n",
            "strain 0.055105846375226974\n",
            "strain 0.05585654452443123\n",
            "strain 0.05254456773400307\n",
            "strain 0.04957759007811546\n",
            "classify 2.266845703125\n",
            "classify 2.2166748046875\n",
            "classify 2.2510986328125\n",
            "classify 2.2471923828125\n",
            "classify 2.265625\n",
            "classify 2.2607421875\n",
            "classify 2.260986328125\n",
            "classify 2.2767333984375\n",
            "classify 2.245849609375\n",
            "classify 2.270751953125\n",
            "classify 2.2396240234375\n",
            "0.140625\n",
            "0.09375\n",
            "0.1875\n",
            "0.125\n",
            "0.140625\n",
            "0.1875\n",
            "0.15625\n",
            "0.203125\n",
            "0.171875\n",
            "0.1875\n",
            "0.140625\n",
            "strain 0.05773952603340149\n",
            "strain 0.05188332498073578\n",
            "strain 0.05945770815014839\n",
            "strain 0.05504712462425232\n",
            "strain 0.06099223345518112\n",
            "strain 0.06654824316501617\n",
            "strain 0.056357674300670624\n",
            "strain 0.0495406799018383\n",
            "strain 0.06726102530956268\n",
            "strain 0.06311646848917007\n",
            "strain 0.051801975816488266\n",
            "strain 0.06354334950447083\n",
            "strain 0.05257192254066467\n",
            "strain 0.05124003812670708\n",
            "strain 0.05137170851230621\n",
            "strain 0.05216784402728081\n",
            "strain 0.05833883956074715\n",
            "strain 0.056755028665065765\n",
            "strain 0.06091058999300003\n",
            "strain 0.05498955026268959\n",
            "strain 0.053276177495718\n",
            "strain 0.05175677314400673\n",
            "strain 0.05685761943459511\n",
            "strain 0.061227865517139435\n",
            "strain 0.0552796870470047\n",
            "strain 0.05022299662232399\n",
            "strain 0.048328083008527756\n",
            "strain 0.05061827972531319\n",
            "strain 0.06411568075418472\n",
            "strain 0.05851626396179199\n",
            "strain 0.04788827896118164\n",
            "strain 0.05393708497285843\n",
            "strain 0.05378318205475807\n",
            "strain 0.06494758278131485\n",
            "strain 0.043835919350385666\n",
            "strain 0.05244087427854538\n",
            "strain 0.04511464014649391\n",
            "strain 0.06743525713682175\n",
            "strain 0.05235004425048828\n",
            "strain 0.056858211755752563\n",
            "strain 0.06243858486413956\n",
            "strain 0.052710216492414474\n",
            "strain 0.04935847595334053\n",
            "strain 0.05199192836880684\n",
            "strain 0.06667416542768478\n",
            "strain 0.05594787001609802\n",
            "strain 0.0451480895280838\n",
            "strain 0.05479860305786133\n",
            "strain 0.06316324323415756\n",
            "strain 0.057858072221279144\n",
            "strain 0.05478418990969658\n",
            "classify 2.3013916015625\n",
            "classify 2.292724609375\n",
            "classify 2.23388671875\n",
            "classify 2.306884765625\n",
            "classify 2.2454833984375\n",
            "classify 2.2545166015625\n",
            "classify 2.2747802734375\n",
            "classify 2.26513671875\n",
            "classify 2.301513671875\n",
            "classify 2.2708740234375\n",
            "classify 2.255126953125\n",
            "0.15625\n",
            "0.140625\n",
            "0.234375\n",
            "0.15625\n",
            "0.140625\n",
            "0.1875\n",
            "0.234375\n",
            "0.203125\n",
            "0.21875\n",
            "0.203125\n",
            "0.171875\n",
            "strain 0.06228763982653618\n",
            "strain 0.06564135104417801\n",
            "strain 0.06104690581560135\n",
            "strain 0.052282627671957016\n",
            "strain 0.048464104533195496\n",
            "strain 0.057759251445531845\n",
            "strain 0.05535542964935303\n",
            "strain 0.0659337118268013\n",
            "strain 0.05483822524547577\n",
            "strain 0.05324365943670273\n",
            "strain 0.05815470963716507\n",
            "strain 0.06433612108230591\n",
            "strain 0.05685878172516823\n",
            "strain 0.06865818053483963\n",
            "strain 0.05729514732956886\n",
            "strain 0.05747758597135544\n",
            "strain 0.05638103559613228\n",
            "strain 0.06837397068738937\n",
            "strain 0.053094249218702316\n",
            "strain 0.05487237498164177\n",
            "strain 0.05518588423728943\n",
            "strain 0.05447028577327728\n",
            "strain 0.053439557552337646\n",
            "strain 0.05308469012379646\n",
            "strain 0.05393084138631821\n",
            "strain 0.04734957963228226\n",
            "strain 0.04856880381703377\n",
            "strain 0.05483495444059372\n",
            "strain 0.055754922330379486\n",
            "strain 0.046022266149520874\n",
            "strain 0.05757525935769081\n",
            "strain 0.06456483155488968\n",
            "strain 0.06192914769053459\n",
            "strain 0.05121220275759697\n",
            "strain 0.057901881635189056\n",
            "strain 0.06180400028824806\n",
            "strain 0.05596869811415672\n",
            "strain 0.06075415760278702\n",
            "strain 0.05859217420220375\n",
            "strain 0.057018592953681946\n",
            "strain 0.059786196798086166\n",
            "strain 0.07647537440061569\n",
            "strain 0.05107847601175308\n",
            "strain 0.06025678664445877\n",
            "strain 0.05129637569189072\n",
            "strain 0.06775892525911331\n",
            "strain 0.05463889613747597\n",
            "strain 0.05955838784575462\n",
            "strain 0.0675831064581871\n",
            "strain 0.059093184769153595\n",
            "strain 0.06889549642801285\n",
            "classify 2.244384765625\n",
            "classify 2.263916015625\n",
            "classify 2.276611328125\n",
            "classify 2.298828125\n",
            "classify 2.294189453125\n",
            "classify 2.27783203125\n",
            "classify 2.3033447265625\n",
            "classify 2.3084716796875\n",
            "classify 2.261962890625\n",
            "classify 2.2412109375\n",
            "classify 2.2459716796875\n",
            "0.203125\n",
            "0.09375\n",
            "0.125\n",
            "0.125\n",
            "0.171875\n",
            "0.1875\n",
            "0.1875\n",
            "0.203125\n",
            "0.125\n",
            "0.125\n",
            "0.125\n",
            "strain 0.06608130782842636\n",
            "strain 0.06079932674765587\n",
            "strain 0.04789746180176735\n",
            "strain 0.05769997462630272\n",
            "strain 0.06427405029535294\n",
            "strain 0.048573192209005356\n",
            "strain 0.053524844348430634\n",
            "strain 0.0528409443795681\n",
            "strain 0.07211323082447052\n",
            "strain 0.05678959935903549\n",
            "strain 0.0605403333902359\n",
            "strain 0.0582892969250679\n",
            "strain 0.05420027673244476\n",
            "strain 0.0726827085018158\n",
            "strain 0.06163026764988899\n",
            "strain 0.055496055632829666\n",
            "strain 0.06068674847483635\n",
            "strain 0.05569726228713989\n",
            "strain 0.04316255450248718\n",
            "strain 0.04959644749760628\n",
            "strain 0.05317463353276253\n",
            "strain 0.05586893483996391\n",
            "strain 0.06605144590139389\n",
            "strain 0.060677677392959595\n",
            "strain 0.06514258682727814\n",
            "strain 0.05686339735984802\n",
            "strain 0.06436819583177567\n",
            "strain 0.051194313913583755\n",
            "strain 0.053966034203767776\n",
            "strain 0.05348145589232445\n",
            "strain 0.04966310039162636\n",
            "strain 0.06359744817018509\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Nd-sGe6Ku4S",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title wandb\n",
        "!pip install -q wandb\n",
        "import wandb # https://docs.wandb.ai/quickstart\n",
        "wandb.login(key='487a2109e55dce4e13fc70681781de9f50f27be7')\n",
        "try: run.finish()\n",
        "except NameError: pass\n",
        "run = wandb.init(project=\"ijepa\", config={\"model\": \"res18\",})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## drawer"
      ],
      "metadata": {
        "id": "c1-oKWjZ8Vxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title train.py works\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/train.py\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "patch_size= 1# 16\n",
        "crop_size=32 # 224\n",
        "\n",
        "encoder = VisionTransformer(img_size=[crop_size], patch_size=patch_size, embed_dim=32, depth=4, num_heads=4)\n",
        "predictor = VisionTransformerPredictor(num_patches=encoder.patch_embed.num_patches, embed_dim=encoder.embed_dim, predictor_embed_dim=12, depth=1, num_heads=3)\n",
        "import copy\n",
        "target_encoder = copy.deepcopy(encoder)\n",
        "for p in target_encoder.parameters(): p.requires_grad = False\n",
        "\n",
        "\n",
        "mask_collator = MaskCollator(input_size=crop_size, patch_size=patch_size,\n",
        "    pred_mask_scale=(.15,.2), enc_mask_scale=(.85,1.), aspect_ratio=(.75,1.5),\n",
        "    nenc=1, npred=1, # 1,4\n",
        "    allow_overlap=True, # paper:True, config:False\n",
        "    min_keep=10)\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/transforms.py\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.RandomResizedCrop((32,32), scale=(.3,1.)), transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/datasets/imagenet1k.py\n",
        "import torch\n",
        "import torchvision\n",
        "# import torchvision.transforms as transforms\n",
        "# # transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# transform = transforms.Compose([transforms.ToTensor(),])\n",
        "\n",
        "# CIFAR10: 60000 32x32 color images in 10 classes, with 6000 images per class\n",
        "train_data = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
        "# test_data = torchvision.datasets.CIFAR10(root='data', train=False, download=True, transform=transform)\n",
        "batch_size = 64 # 4\n",
        "# train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "unsupervised_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=mask_collator)\n",
        "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "# classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "ipe = len(unsupervised_loader)\n",
        "\n",
        "\n",
        "\n",
        "param_groups = [\n",
        "    {'params': (p for n, p in encoder.named_parameters() if ('bias' not in n) and (len(p.shape) != 1))},\n",
        "    {'params': (p for n, p in predictor.named_parameters() if ('bias' not in n) and (len(p.shape) != 1))},\n",
        "    {'params': (p for n, p in encoder.named_parameters() if ('bias' in n) or (len(p.shape) == 1)),\n",
        "        'WD_exclude': True, 'weight_decay': 0},\n",
        "    {'params': (p for n, p in predictor.named_parameters() if ('bias' in n) or (len(p.shape) == 1)),\n",
        "        'WD_exclude': True, 'weight_decay': 0}\n",
        "]\n",
        "optimizer = torch.optim.AdamW(param_groups)\n",
        "\n",
        "\n",
        "\n",
        "ipe_scale = 1.  # scheduler scale factor (def: 1.0)\n",
        "num_epochs = 300\n",
        "scheduler = WarmupCosineSchedule(optimizer, warmup_steps=int(40*ipe), start_lr=2e-4, ref_lr=1e-3, final_lr=1e-6, T_max=int(ipe_scale*num_epochs*ipe))\n",
        "wd_scheduler = CosineWDSchedule(optimizer, ref_wd=0.04, final_wd=0.4, T_max=int(ipe_scale*num_epochs*ipe))\n",
        "ema = (.996,1.)\n",
        "momentum_scheduler = (ema[0] + i*(ema[1]-ema[0])/(ipe*num_epochs*ipe_scale) for i in range(int(ipe*num_epochs*ipe_scale)+1))\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "for epoch in range(num_epochs):\n",
        "    for itr, (udata, masks_enc, masks_pred) in enumerate(unsupervised_loader):\n",
        "        imgs = udata[0].to(device, non_blocking=True)\n",
        "        masks_enc = [u.to(device, non_blocking=True) for u in masks_enc]\n",
        "        masks_pred = [u.to(device, non_blocking=True) for u in masks_pred]\n",
        "\n",
        "        _new_lr = scheduler.step()\n",
        "        _new_wd = wd_scheduler.step()\n",
        "\n",
        "        # Step 1. Forward\n",
        "        with torch.cuda.amp.autocast(dtype=torch.bfloat16, enabled=True):\n",
        "            with torch.no_grad():\n",
        "                h = target_encoder(imgs)\n",
        "                h = F.layer_norm(h, (h.size(-1),))  # normalize over feature-dim\n",
        "                B = len(h)\n",
        "                # -- create targets (masked regions of h)\n",
        "                h = apply_masks(h, masks_pred)\n",
        "                h = repeat_interleave_batch(h, B, repeat=len(masks_enc))\n",
        "\n",
        "            z = encoder(imgs, masks_enc)\n",
        "            z = predictor(z, masks_enc, masks_pred)\n",
        "\n",
        "            loss = F.smooth_l1_loss(z, h)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        # loss.backward()\n",
        "        # optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            m = next(momentum_scheduler)\n",
        "            for param_q, param_k in zip(encoder.parameters(), target_encoder.parameters()):\n",
        "                param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "        print('loss',loss.item())\n",
        "        assert not np.isnan(loss.detach().numpy()), 'loss is nan'\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Ve3e3MiJTO8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title src/helper.py init_model init_opt\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/helper.py\n",
        "def init_model(\n",
        "    device,\n",
        "    patch_size=16,\n",
        "    model_name='vit_base',\n",
        "    crop_size=224,\n",
        "    pred_depth=6,\n",
        "    pred_emb_dim=384\n",
        "):\n",
        "    encoder = vit.__dict__[model_name](\n",
        "        img_size=[crop_size],\n",
        "        patch_size=patch_size)\n",
        "\n",
        "    predictor = vit.__dict__['vit_predictor'](\n",
        "        num_patches=encoder.patch_embed.num_patches,\n",
        "        embed_dim=encoder.embed_dim,\n",
        "        predictor_embed_dim=pred_emb_dim,\n",
        "        depth=pred_depth,\n",
        "        num_heads=encoder.num_heads)\n",
        "\n",
        "    def init_weights(m):\n",
        "        if isinstance(m, torch.nn.Linear):\n",
        "            trunc_normal_(m.weight, std=0.02)\n",
        "            if m.bias is not None:\n",
        "                torch.nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, torch.nn.LayerNorm):\n",
        "            torch.nn.init.constant_(m.bias, 0)\n",
        "            torch.nn.init.constant_(m.weight, 1.0)\n",
        "    for m in encoder.modules(): init_weights(m)\n",
        "    for m in predictor.modules(): init_weights(m)\n",
        "    encoder.to(device)\n",
        "    predictor.to(device)\n",
        "    return encoder, predictor\n",
        "\n",
        "\n",
        "def init_opt(\n",
        "    encoder,\n",
        "    predictor,\n",
        "    iterations_per_epoch,\n",
        "    start_lr,\n",
        "    ref_lr,\n",
        "    warmup,\n",
        "    num_epochs,\n",
        "    wd=1e-6,\n",
        "    final_wd=1e-6,\n",
        "    final_lr=0.0,\n",
        "    use_bfloat16=False,\n",
        "    ipe_scale=1.25\n",
        "):\n",
        "    param_groups = [\n",
        "        {'params': (p for n, p in encoder.named_parameters() if ('bias' not in n) and (len(p.shape) != 1))},\n",
        "        {'params': (p for n, p in predictor.named_parameters() if ('bias' not in n) and (len(p.shape) != 1))},\n",
        "        {'params': (p for n, p in encoder.named_parameters() if ('bias' in n) or (len(p.shape) == 1)),\n",
        "            'WD_exclude': True, 'weight_decay': 0},\n",
        "        {'params': (p for n, p in predictor.named_parameters() if ('bias' in n) or (len(p.shape) == 1)),\n",
        "            'WD_exclude': True, 'weight_decay': 0}\n",
        "    ]\n",
        "\n",
        "    optimizer = torch.optim.AdamW(param_groups)\n",
        "    scheduler = WarmupCosineSchedule(optimizer, warmup_steps=int(warmup*iterations_per_epoch),\n",
        "        start_lr=start_lr, ref_lr=ref_lr, final_lr=final_lr, T_max=int(ipe_scale*num_epochs*iterations_per_epoch))\n",
        "    wd_scheduler = CosineWDSchedule(optimizer, ref_wd=wd, final_wd=final_wd,\n",
        "        T_max=int(ipe_scale*num_epochs*iterations_per_epoch))\n",
        "    scaler = torch.cuda.amp.GradScaler() if use_bfloat16 else None\n",
        "    return optimizer, scaler, scheduler, wd_scheduler\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "HfjzczR-c7pN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title multiblock.py MaskCollator\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py#L20\n",
        "import math\n",
        "from multiprocessing import Value\n",
        "import torch\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(self, input_size=(224, 224), patch_size=16,\n",
        "        enc_mask_scale=(0.2, 0.8), pred_mask_scale=(0.2, 0.8), aspect_ratio=(0.3, 3.0),\n",
        "        nenc=1, npred=2, min_keep=4, allow_overlap=False):\n",
        "        super().__init__()\n",
        "        if not isinstance(input_size, tuple): input_size = (input_size,) * 2\n",
        "        self.patch_size = patch_size\n",
        "        self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.aspect_ratio = aspect_ratio\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "        self._itr_counter = Value('i', -1)  # collator is shared across worker processes\n",
        "\n",
        "    def step(self):\n",
        "        i = self._itr_counter\n",
        "        with i.get_lock():\n",
        "            i.value += 1\n",
        "            v = i.value\n",
        "        return v\n",
        "\n",
        "    def _sample_block_size(self, generator, scale, aspect_ratio_scale):\n",
        "        _rand = torch.rand(1, generator=generator).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.height * self.width * mask_scale)\n",
        "        # -- Sample block aspect-ratio\n",
        "        min_ar, max_ar = aspect_ratio_scale\n",
        "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
        "        while h >= self.height: h -= 1\n",
        "        while w >= self.width: w -= 1\n",
        "        return (h, w)\n",
        "\n",
        "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
        "        h, w = b_size\n",
        "\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # --\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            top = torch.randint(0, self.height - h, (1,))\n",
        "            left = torch.randint(0, self.width - w, (1,))\n",
        "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
        "            mask[top:top+h, left:left+w] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "                    print(f'Mask generator says: \"Valid mask not found, decreasing acceptable-regions [{tries}]\"')\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
        "        mask_complement[top:top+h, left:left+w] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        B = len(batch)\n",
        "\n",
        "        collated_batch = torch.utils.data.default_collate(batch)\n",
        "\n",
        "        seed = self.step()\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(seed)\n",
        "        p_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.pred_mask_scale,\n",
        "            aspect_ratio_scale=self.aspect_ratio)\n",
        "        e_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.enc_mask_scale,\n",
        "            aspect_ratio_scale=(1., 1.))\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.height * self.width\n",
        "        min_keep_enc = self.height * self.width\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                print(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "\n",
        "        return collated_batch, collated_masks_enc, collated_masks_pred\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "B6oEDkyl9eE6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test MaskCollator\n",
        "\n",
        "\n",
        "masks_enc = multiblock2d((32,32), scale=(.85,1), aspect_ratio=(1,1), M=1).flatten()#.repeat(4,1) # [1,h,w], True->Mask\n",
        "idx = masks_enc.nonzero().squeeze(-1) # int idx [num_trg_toks] , idx of targets that are masked\n",
        "# idx = masks_enc.nonzero()\n",
        "# print(masks_enc.dtype)\n",
        "\n",
        "print(masks_enc.shape)\n",
        "print(idx.dtype)\n",
        "print(idx.shape)\n",
        "idx=idx.expand(4,-1)\n",
        "print(idx.shape)\n",
        "\n",
        "\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=mask_collator)\n",
        "\n",
        "# for i, (udata, masks_enc, masks_pred) in enumerate(train_loader):\n",
        "#     imgs = udata[0].to(device, non_blocking=True)\n",
        "#     masks_enc = [u.to(device, non_blocking=True) for u in masks_enc]\n",
        "#     masks_pred = [u.to(device, non_blocking=True) for u in masks_pred]\n",
        "#     break\n",
        "\n",
        "print(imgs.shape)\n",
        "print(len(masks_enc), len(masks_pred)) # 1,1\n",
        "print(masks_enc[0].shape, masks_pred[0].shape)\n",
        "display_mask(masks_enc[0][0]) # True big square\n",
        "display_mask(masks_pred[0][0]) # True small square\n",
        "display_mask(masks_enc[0][-1]) # True big square\n",
        "display_mask(masks_pred[0][-1]) # True small square\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "YcHd1MxIg-g4",
        "outputId": "86d8c253-327b-47cf-abf7-fdd27a76aa93"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1024])\n",
            "torch.int64\n",
            "torch.Size([900])\n",
            "torch.Size([4, 900])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title train.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/train.py\n",
        "\n",
        "import os\n",
        "# -- FOR DISTRIBUTED TRAINING ENSURE ONLY 1 DEVICE VISIBLE PER PROCESS\n",
        "try:\n",
        "    # -- WARNING: IF DOING DISTRIBUTED TRAINING ON A NON-SLURM CLUSTER, MAKE\n",
        "    # --          SURE TO UPDATE THIS TO GET LOCAL-RANK ON NODE, OR ENSURE\n",
        "    # --          THAT YOUR JOBS ARE LAUNCHED WITH ONLY 1 DEVICE VISIBLE\n",
        "    # --          TO EACH PROCESS\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = os.environ['SLURM_LOCALID']\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "import copy\n",
        "# import logging\n",
        "# import sys\n",
        "# import yaml\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "# import torch.multiprocessing as mp\n",
        "import torch.nn.functional as F\n",
        "# from torch.nn.parallel import DistributedDataParallel\n",
        "\n",
        "from src.masks.multiblock import MaskCollator as MBMaskCollator\n",
        "from src.masks.utils import apply_masks\n",
        "from src.utils.distributed import (\n",
        "    init_distributed,\n",
        "    # AllReduce\n",
        ")\n",
        "# from src.utils.logging import (\n",
        "#     # CSVLogger,\n",
        "#     # gpu_timer,\n",
        "#     # grad_logger,\n",
        "#     # AverageMeter)\n",
        "from src.utils.tensors import repeat_interleave_batch\n",
        "from src.datasets.imagenet1k import make_imagenet1k\n",
        "\n",
        "from src.helper import load_checkpoint, init_model, init_opt\n",
        "from src.transforms import make_transforms\n",
        "\n",
        "# --\n",
        "log_timings = True\n",
        "log_freq = 10\n",
        "checkpoint_freq = 50\n",
        "# --\n",
        "\n",
        "_GLOBAL_SEED = 0\n",
        "np.random.seed(_GLOBAL_SEED)\n",
        "torch.manual_seed(_GLOBAL_SEED)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "# logger = logging.getLogger()\n",
        "\n",
        "\n",
        "def main(args, resume_preempt=False):\n",
        "\n",
        "    # ----------------------------------------------------------------------- #\n",
        "    #  PASSED IN PARAMS FROM CONFIG FILE\n",
        "    # ----------------------------------------------------------------------- #\n",
        "\n",
        "    # -- META\n",
        "    use_bfloat16 = args['meta']['use_bfloat16']\n",
        "    model_name = args['meta']['model_name']\n",
        "    load_model = args['meta']['load_checkpoint'] or resume_preempt\n",
        "    r_file = args['meta']['read_checkpoint']\n",
        "    copy_data = args['meta']['copy_data']\n",
        "    pred_depth = args['meta']['pred_depth']\n",
        "    pred_emb_dim = args['meta']['pred_emb_dim']\n",
        "    if not torch.cuda.is_available():\n",
        "        device = torch.device('cpu')\n",
        "    else:\n",
        "        device = torch.device('cuda:0')\n",
        "        torch.cuda.set_device(device)\n",
        "\n",
        "    # # -- DATA\n",
        "    # use_gaussian_blur = args['data']['use_gaussian_blur']\n",
        "    # use_horizontal_flip = args['data']['use_horizontal_flip']\n",
        "    # use_color_distortion = args['data']['use_color_distortion']\n",
        "    # color_jitter = args['data']['color_jitter_strength']\n",
        "    # --\n",
        "    batch_size = args['data']['batch_size']\n",
        "    pin_mem = args['data']['pin_mem']\n",
        "    num_workers = args['data']['num_workers']\n",
        "    root_path = args['data']['root_path']\n",
        "    image_folder = args['data']['image_folder']\n",
        "    crop_size = args['data']['crop_size']\n",
        "    crop_scale = args['data']['crop_scale']\n",
        "    # --\n",
        "\n",
        "    # -- MASK\n",
        "    # allow_overlap = args['mask']['allow_overlap']  # whether to allow overlap b/w context and target blocks\n",
        "    patch_size = args['mask']['patch_size']  # patch-size for model training\n",
        "    num_enc_masks = args['mask']['num_enc_masks']  # number of context blocks\n",
        "    min_keep = args['mask']['min_keep']  # min number of patches in context block\n",
        "    enc_mask_scale = args['mask']['enc_mask_scale']  # scale of context blocks\n",
        "    num_pred_masks = args['mask']['num_pred_masks']  # number of target blocks\n",
        "    pred_mask_scale = args['mask']['pred_mask_scale']  # scale of target blocks\n",
        "    aspect_ratio = args['mask']['aspect_ratio']  # aspect ratio of target blocks\n",
        "    # --\n",
        "\n",
        "    # -- OPTIMIZATION\n",
        "    ema = args['optimization']['ema']\n",
        "    ipe_scale = args['optimization']['ipe_scale']  # scheduler scale factor (def: 1.0)\n",
        "    wd = float(args['optimization']['weight_decay'])\n",
        "    final_wd = float(args['optimization']['final_weight_decay'])\n",
        "    num_epochs = args['optimization']['epochs']\n",
        "    warmup = args['optimization']['warmup']\n",
        "    start_lr = args['optimization']['start_lr']\n",
        "    lr = args['optimization']['lr']\n",
        "    final_lr = args['optimization']['final_lr']\n",
        "\n",
        "    # # -- LOGGING\n",
        "    # folder = args['logging']['folder']\n",
        "    # tag = args['logging']['write_tag']\n",
        "\n",
        "    # dump = os.path.join(folder, 'params-ijepa.yaml')\n",
        "    # with open(dump, 'w') as f:\n",
        "    #     yaml.dump(args, f)\n",
        "    # ----------------------------------------------------------------------- #\n",
        "\n",
        "    # try:\n",
        "    #     mp.set_start_method('spawn')\n",
        "    # except Exception:\n",
        "    #     pass\n",
        "\n",
        "    # -- init torch distributed backend\n",
        "    # world_size, rank = init_distributed()\n",
        "    # logger.info(f'Initialized (rank/world-size) {rank}/{world_size}')\n",
        "    # if rank > 0:\n",
        "    #     logger.setLevel(logging.ERROR)\n",
        "\n",
        "    # # -- log/checkpointing paths\n",
        "    # log_file = os.path.join(folder, f'{tag}_r{rank}.csv')\n",
        "    # save_path = os.path.join(folder, f'{tag}' + '-ep{epoch}.pth.tar')\n",
        "    # latest_path = os.path.join(folder, f'{tag}-latest.pth.tar')\n",
        "    # load_path = None\n",
        "    # if load_model:\n",
        "    #     load_path = os.path.join(folder, r_file) if r_file is not None else latest_path\n",
        "\n",
        "    # -- make csv_logger\n",
        "    # csv_logger = CSVLogger(log_file,\n",
        "    #                        ('%d', 'epoch'),\n",
        "    #                        ('%d', 'itr'),\n",
        "    #                        ('%.5f', 'loss'),\n",
        "    #                        ('%.5f', 'mask-A'),\n",
        "    #                        ('%.5f', 'mask-B'),\n",
        "    #                        ('%d', 'time (ms)'))\n",
        "\n",
        "    # -- init model\n",
        "    encoder, predictor = init_model(\n",
        "        device=device,\n",
        "        patch_size=patch_size,\n",
        "        crop_size=crop_size,\n",
        "        pred_depth=pred_depth,\n",
        "        pred_emb_dim=pred_emb_dim,\n",
        "        model_name=model_name)\n",
        "    target_encoder = copy.deepcopy(encoder)\n",
        "\n",
        "    # -- make data transforms\n",
        "    mask_collator = MBMaskCollator(\n",
        "        input_size=crop_size,\n",
        "        patch_size=patch_size,\n",
        "        pred_mask_scale=pred_mask_scale,\n",
        "        enc_mask_scale=enc_mask_scale,\n",
        "        aspect_ratio=aspect_ratio,\n",
        "        nenc=num_enc_masks,\n",
        "        npred=num_pred_masks,\n",
        "        allow_overlap=allow_overlap,\n",
        "        min_keep=min_keep)\n",
        "\n",
        "    transform = make_transforms(\n",
        "        crop_size=crop_size,\n",
        "        crop_scale=crop_scale,\n",
        "        # gaussian_blur=use_gaussian_blur,\n",
        "        # horizontal_flip=use_horizontal_flip,\n",
        "        # color_distortion=use_color_distortion,\n",
        "        # color_jitter=color_jitter\n",
        "        )\n",
        "\n",
        "    # -- init data-loaders/samplers\n",
        "    _, unsupervised_loader, unsupervised_sampler = make_imagenet1k(\n",
        "            transform=transform,\n",
        "            batch_size=batch_size,\n",
        "            collator=mask_collator,\n",
        "            pin_mem=pin_mem,\n",
        "            training=True,\n",
        "            num_workers=num_workers,\n",
        "            # world_size=world_size,\n",
        "            # rank=rank,\n",
        "            root_path=root_path,\n",
        "            image_folder=image_folder,\n",
        "            copy_data=copy_data,\n",
        "            drop_last=True)\n",
        "    ipe = len(unsupervised_loader)\n",
        "\n",
        "    # -- init optimizer and scheduler\n",
        "    optimizer, scaler, scheduler, wd_scheduler = init_opt(\n",
        "        encoder=encoder,\n",
        "        predictor=predictor,\n",
        "        wd=wd,\n",
        "        final_wd=final_wd,\n",
        "        start_lr=start_lr,\n",
        "        ref_lr=lr,\n",
        "        final_lr=final_lr,\n",
        "        iterations_per_epoch=ipe,\n",
        "        warmup=warmup,\n",
        "        num_epochs=num_epochs,\n",
        "        ipe_scale=ipe_scale,\n",
        "        use_bfloat16=use_bfloat16)\n",
        "    # encoder = DistributedDataParallel(encoder, static_graph=True)\n",
        "    # predictor = DistributedDataParallel(predictor, static_graph=True)\n",
        "    # target_encoder = DistributedDataParallel(target_encoder)\n",
        "    for p in target_encoder.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    # -- momentum schedule\n",
        "    momentum_scheduler = (ema[0] + i*(ema[1]-ema[0])/(ipe*num_epochs*ipe_scale) for i in range(int(ipe*num_epochs*ipe_scale)+1))\n",
        "\n",
        "    start_epoch = 0\n",
        "    # # -- load training checkpoint\n",
        "    # if load_model:\n",
        "    #     encoder, predictor, target_encoder, optimizer, scaler, start_epoch = load_checkpoint(\n",
        "    #         device=device,\n",
        "    #         r_path=load_path,\n",
        "    #         encoder=encoder,\n",
        "    #         predictor=predictor,\n",
        "    #         target_encoder=target_encoder,\n",
        "    #         opt=optimizer,\n",
        "    #         scaler=scaler)\n",
        "    #     for _ in range(start_epoch*ipe):\n",
        "    #         scheduler.step()\n",
        "    #         wd_scheduler.step()\n",
        "    #         next(momentum_scheduler)\n",
        "    #         mask_collator.step()\n",
        "\n",
        "    # def save_checkpoint(epoch):\n",
        "    #     save_dict = {\n",
        "    #         'encoder': encoder.state_dict(),\n",
        "    #         'predictor': predictor.state_dict(),\n",
        "    #         'target_encoder': target_encoder.state_dict(),\n",
        "    #         'opt': optimizer.state_dict(),\n",
        "    #         'scaler': None if scaler is None else scaler.state_dict(),\n",
        "    #         'epoch': epoch,\n",
        "    #         'loss': loss_meter.avg,\n",
        "    #         'batch_size': batch_size,\n",
        "    #         'world_size': world_size,\n",
        "    #         'lr': lr\n",
        "    #     }\n",
        "    #     if rank == 0:\n",
        "    #         torch.save(save_dict, latest_path)\n",
        "    #         if (epoch + 1) % checkpoint_freq == 0:\n",
        "    #             torch.save(save_dict, save_path.format(epoch=f'{epoch + 1}'))\n",
        "\n",
        "    # -- TRAINING LOOP\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        logger.info('Epoch %d' % (epoch + 1))\n",
        "\n",
        "        # -- update distributed-data-loader epoch\n",
        "        # unsupervised_sampler.set_epoch(epoch)\n",
        "\n",
        "        # loss_meter = AverageMeter()\n",
        "        # maskA_meter = AverageMeter()\n",
        "        # maskB_meter = AverageMeter()\n",
        "        # time_meter = AverageMeter()\n",
        "\n",
        "        for itr, (udata, masks_enc, masks_pred) in enumerate(unsupervised_loader):\n",
        "\n",
        "            def load_imgs():\n",
        "                # -- unsupervised imgs\n",
        "                imgs = udata[0].to(device, non_blocking=True)\n",
        "                masks_1 = [u.to(device, non_blocking=True) for u in masks_enc]\n",
        "                masks_2 = [u.to(device, non_blocking=True) for u in masks_pred]\n",
        "                return (imgs, masks_1, masks_2)\n",
        "            imgs, masks_enc, masks_pred = load_imgs()\n",
        "            # maskA_meter.update(len(masks_enc[0][0]))\n",
        "            # maskB_meter.update(len(masks_pred[0][0]))\n",
        "\n",
        "            def train_step():\n",
        "                _new_lr = scheduler.step()\n",
        "                _new_wd = wd_scheduler.step()\n",
        "                # --\n",
        "\n",
        "                def forward_target():\n",
        "                    with torch.no_grad():\n",
        "                        h = target_encoder(imgs)\n",
        "                        h = F.layer_norm(h, (h.size(-1),))  # normalize over feature-dim\n",
        "                        B = len(h)\n",
        "                        # -- create targets (masked regions of h)\n",
        "                        h = apply_masks(h, masks_pred)\n",
        "                        h = repeat_interleave_batch(h, B, repeat=len(masks_enc))\n",
        "                        return h\n",
        "\n",
        "                def forward_context():\n",
        "                    z = encoder(imgs, masks_enc)\n",
        "                    z = predictor(z, masks_enc, masks_pred)\n",
        "                    return z\n",
        "\n",
        "                def loss_fn(z, h):\n",
        "                    loss = F.smooth_l1_loss(z, h)\n",
        "                    # loss = AllReduce.apply(loss)\n",
        "                    return loss\n",
        "\n",
        "                # Step 1. Forward\n",
        "                with torch.cuda.amp.autocast(dtype=torch.bfloat16, enabled=use_bfloat16):\n",
        "                    h = forward_target()\n",
        "                    z = forward_context()\n",
        "                    loss = loss_fn(z, h)\n",
        "\n",
        "                #  Step 2. Backward & step\n",
        "                if use_bfloat16:\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                else:\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                # grad_stats = grad_logger(encoder.named_parameters())\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Step 3. momentum update of target encoder\n",
        "                with torch.no_grad():\n",
        "                    m = next(momentum_scheduler)\n",
        "                    for param_q, param_k in zip(encoder.parameters(), target_encoder.parameters()):\n",
        "                        param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "\n",
        "                # return (float(loss), _new_lr, _new_wd, grad_stats)\n",
        "                return float(loss), _new_lr, _new_wd\n",
        "            # (loss, _new_lr, _new_wd, grad_stats), etime = gpu_timer(train_step)\n",
        "            loss, _new_lr, _new_wd = train_step()\n",
        "            # loss_meter.update(loss)\n",
        "            # time_meter.update(etime)\n",
        "\n",
        "            # # -- Logging\n",
        "            # def log_stats():\n",
        "            #     csv_logger.log(epoch + 1, itr, loss, maskA_meter.val, maskB_meter.val, etime)\n",
        "            #     if (itr % log_freq == 0) or np.isnan(loss) or np.isinf(loss):\n",
        "            #         logger.info('[%d, %5d] loss: %.3f '\n",
        "            #                     'masks: %.1f %.1f '\n",
        "            #                     '[wd: %.2e] [lr: %.2e] '\n",
        "            #                     '[mem: %.2e] '\n",
        "            #                     '(%.1f ms)'\n",
        "            #                     % (epoch + 1, itr,\n",
        "            #                        loss_meter.avg,\n",
        "            #                        maskA_meter.avg,\n",
        "            #                        maskB_meter.avg,\n",
        "            #                        _new_wd,\n",
        "            #                        _new_lr,\n",
        "            #                        torch.cuda.max_memory_allocated() / 1024.**2,\n",
        "            #                        time_meter.avg))\n",
        "\n",
        "                    # if grad_stats is not None:\n",
        "                    #     logger.info('[%d, %5d] grad_stats: [%.2e %.2e] (%.2e, %.2e)'\n",
        "                    #                 % (epoch + 1, itr,\n",
        "                    #                    grad_stats.first_layer,\n",
        "                    #                    grad_stats.last_layer,\n",
        "                    #                    grad_stats.min,\n",
        "                    #                    grad_stats.max))\n",
        "\n",
        "            # log_stats()\n",
        "\n",
        "            assert not np.isnan(loss), 'loss is nan'\n",
        "\n",
        "        # -- Save Checkpoint after every epoch\n",
        "        # logger.info('avg. loss %.3f' % loss_meter.avg)\n",
        "        # save_checkpoint(epoch+1)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "vsNUzrme8VSn",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test multiblock params\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "dataiter = iter(train_data)\n",
        "img,y = next(dataiter)\n",
        "img = img.unsqueeze(0)\n",
        "# img = F.interpolate(img, (8,8))\n",
        "b,c,h,w = img.shape\n",
        "# img = img.flatten(2).transpose(-2,-1).to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "print(img.shape)\n",
        "# batch, seq,_ = img.shape\n",
        "\n",
        "\n",
        "# # target_mask = randpatch(seq, mask_size=8, gamma=.9) # 8.9 [seq]\n",
        "# target_mask = multiblock(seq, min_s=0.2, max_s=0.3, M=4).any(0) # best.2.3M4 og.15.2M4# mask out targets to be predicted # [M, seq]\n",
        "# target_mask = multiblock(seq, min_s=0.2, max_s=0.3, M=4).any(0)\n",
        "\n",
        "# target_mask = multiblock2d((8,8), M=4).any(0).unsqueeze(0)\n",
        "\n",
        "\n",
        "# # context_mask = ~multiblock(seq, min_s=0.85, max_s=1., M=1)|target_mask # [1, seq], True->Mask\n",
        "# context_mask = ~multiblock2d((8,8), scale=(.85,.85), aspect_ratio=(.9,1.1), M=1)|target_mask # [1, seq], True->Mask\n",
        "\n",
        "mask_collator = MaskCollator(input_size=(32, 32), patch_size=1,\n",
        "        enc_mask_scale=(0.8, 1.), pred_mask_scale=(0.2, 0.3),\n",
        "        aspect_ratio=(0.75, 1.25),\n",
        "        nenc=1, npred=4,\n",
        "        min_keep=4,\n",
        "        allow_overlap=True)\n",
        "        # allow_overlap=False)\n",
        "collated_batch, collated_masks_enc, collated_masks_pred = mask_collator([0,1,2,3])\n",
        "\n",
        "# print(collated_batch) # input to mask_collator\n",
        "# print(len(collated_masks_enc), len(collated_masks_pred)) # nenc, npred\n",
        "print([m.shape for m in collated_masks_enc], [m.shape for m in collated_masks_pred]) # [batch, num_context_patches = 121 or 11], [batch, num_target_patches]\n",
        "# (224/16)^2 = 196\n",
        "\n",
        "target_mask, context_mask = collated_masks_enc[0], collated_masks_pred[0]\n",
        "\n",
        "print(target_mask, context_mask)\n",
        "\n",
        "# print(target_mask.shape, context_mask.shape)\n",
        "# target_img, context_img = img*target_mask.unsqueeze(-1), img*context_mask.unsqueeze(-1)\n",
        "# target_img, context_img = img*~target_mask.unsqueeze(-1), img*~context_mask.unsqueeze(-1)\n",
        "# target_img, context_img = img*~target_mask.unsqueeze(0), img*~context_mask.unsqueeze(0)\n",
        "img = img.flatten(2)\n",
        "# target_img, context_img = img.clone(), img.clone()\n",
        "# target_img[...,target_mask] = 0\n",
        "# context_img[...,context_mask] = 0\n",
        "# target_img, context_img = target_img.reshape(b,c,h,w), context_img.reshape(b,c,h,w)\n",
        "# # context_img\n",
        "# print(target_img.shape, context_img.shape)\n",
        "# # target_img, context_img = target_img.transpose(-2,-1).reshape(b,c,h,w), context_img.transpose(-2,-1).reshape(b,c,h,w)\n",
        "# target_img, context_img = target_img.float(), context_img.float()\n",
        "\n",
        "# # imshow(out.detach().cpu())\n",
        "# imshow(target_img[0])\n",
        "# imshow(context_img[0])\n",
        "\n",
        "def display_mask(mask, img=img):\n",
        "    _img = img.clone()\n",
        "    _img[...,mask] = 0\n",
        "    _img = _img.reshape(b,c,h,w).float()\n",
        "    imshow(_img[0])\n",
        "\n",
        "display_mask(target_mask)\n",
        "for m in context_mask: display_mask(m)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "3bpiybH7M0O1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## raw"
      ],
      "metadata": {
        "id": "uZWqHyLHs3nU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title multiblock.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py#L20\n",
        "\n",
        "import math\n",
        "from multiprocessing import Value\n",
        "from logging import getLogger\n",
        "import torch\n",
        "\n",
        "_GLOBAL_SEED = 0\n",
        "logger = getLogger()\n",
        "\n",
        "\n",
        "class MaskCollator(object):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=(224, 224),\n",
        "        patch_size=16,\n",
        "        enc_mask_scale=(0.2, 0.8),\n",
        "        pred_mask_scale=(0.2, 0.8),\n",
        "        aspect_ratio=(0.3, 3.0),\n",
        "        nenc=1,\n",
        "        npred=2,\n",
        "        min_keep=4,\n",
        "        allow_overlap=False\n",
        "    ):\n",
        "        super(MaskCollator, self).__init__()\n",
        "        if not isinstance(input_size, tuple):\n",
        "            input_size = (input_size, ) * 2\n",
        "        self.patch_size = patch_size\n",
        "        self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.aspect_ratio = aspect_ratio\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "        self._itr_counter = Value('i', -1)  # collator is shared across worker processes\n",
        "\n",
        "    def step(self):\n",
        "        i = self._itr_counter\n",
        "        with i.get_lock():\n",
        "            i.value += 1\n",
        "            v = i.value\n",
        "        return v\n",
        "\n",
        "    def _sample_block_size(self, generator, scale, aspect_ratio_scale):\n",
        "        _rand = torch.rand(1, generator=generator).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.height * self.width * mask_scale)\n",
        "        # -- Sample block aspect-ratio\n",
        "        min_ar, max_ar = aspect_ratio_scale\n",
        "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
        "        while h >= self.height:\n",
        "            h -= 1\n",
        "        while w >= self.width:\n",
        "            w -= 1\n",
        "\n",
        "        return (h, w)\n",
        "\n",
        "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
        "        h, w = b_size\n",
        "\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # --\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            top = torch.randint(0, self.height - h, (1,))\n",
        "            left = torch.randint(0, self.width - w, (1,))\n",
        "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
        "            mask[top:top+h, left:left+w] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "                    logger.warning(f'Mask generator says: \"Valid mask not found, decreasing acceptable-regions [{tries}]\"')\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
        "        mask_complement[top:top+h, left:left+w] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        B = len(batch)\n",
        "\n",
        "        collated_batch = torch.utils.data.default_collate(batch)\n",
        "\n",
        "        seed = self.step()\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(seed)\n",
        "        p_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.pred_mask_scale,\n",
        "            aspect_ratio_scale=self.aspect_ratio)\n",
        "        e_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.enc_mask_scale,\n",
        "            aspect_ratio_scale=(1., 1.))\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.height * self.width\n",
        "        min_keep_enc = self.height * self.width\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                logger.warning(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "\n",
        "        return collated_batch, collated_masks_enc, collated_masks_pred\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "KMq2k9iV88AI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title tensors.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/utils/tensors.py#L65\n",
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "#\n",
        "\n",
        "import math\n",
        "import torch\n",
        "from logging import getLogger\n",
        "logger = getLogger()\n",
        "\n",
        "\n",
        "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
        "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n",
        "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
        "    def norm_cdf(x):\n",
        "        # Computes standard normal cumulative distribution function\n",
        "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Values are generated by using a truncated uniform distribution and\n",
        "        # then using the inverse CDF for the normal distribution.\n",
        "        # Get upper and lower cdf values\n",
        "        l = norm_cdf((a - mean) / std)\n",
        "        u = norm_cdf((b - mean) / std)\n",
        "\n",
        "        # Uniformly fill tensor with values from [l, u], then translate to\n",
        "        # [2l-1, 2u-1].\n",
        "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
        "\n",
        "        # Use inverse cdf transform for normal distribution to get truncated\n",
        "        # standard normal\n",
        "        tensor.erfinv_()\n",
        "\n",
        "        # Transform to proper mean, std\n",
        "        tensor.mul_(std * math.sqrt(2.))\n",
        "        tensor.add_(mean)\n",
        "\n",
        "        # Clamp to ensure it's in the proper range\n",
        "        tensor.clamp_(min=a, max=b)\n",
        "        return tensor\n",
        "\n",
        "\n",
        "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
        "    # type: (Tensor, float, float, float, float) -> Tensor\n",
        "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n",
        "\n",
        "\n",
        "def apply_masks(x, masks):\n",
        "    \"\"\"\n",
        "    :param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
        "    :param masks: list of tensors containing indices of patches in [N] to keep\n",
        "    \"\"\"\n",
        "    all_x = []\n",
        "    for m in masks:\n",
        "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1))\n",
        "        all_x += [torch.gather(x, dim=1, index=mask_keep)]\n",
        "    return torch.cat(all_x, dim=0)\n",
        "\n",
        "\n",
        "def repeat_interleave_batch(x, B, repeat):\n",
        "    N = len(x) // B\n",
        "    x = torch.cat([\n",
        "        torch.cat([x[i*B:(i+1)*B] for _ in range(repeat)], dim=0)\n",
        "        for i in range(N)\n",
        "    ], dim=0)\n",
        "    return x\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "8pqAsLvI86E_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_ySduzfq8TsQ"
      },
      "outputs": [],
      "source": [
        "# @title vision_transformer.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/models/vision_transformer.py\n",
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "#\n",
        "\n",
        "import math\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from src.utils.tensors import (\n",
        "    trunc_normal_,\n",
        "    repeat_interleave_batch\n",
        ")\n",
        "from src.masks.utils import apply_masks\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid height and width\n",
        "    return:\n",
        "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid_h = np.arange(grid_size, dtype=float)\n",
        "    grid_w = np.arange(grid_size, dtype=float)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    assert embed_dim % 2 == 0\n",
        "\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid length\n",
        "    return:\n",
        "    pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid = np.arange(grid_size, dtype=float)\n",
        "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=float)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega   # (D/2,)\n",
        "\n",
        "    pos = pos.reshape(-1)   # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
        "\n",
        "    emb_sin = np.sin(out)  # (M, D/2)\n",
        "    emb_cos = np.cos(out)  # (M, D/2)\n",
        "\n",
        "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x, attn\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x, return_attention=False):\n",
        "        y, attn = self.attn(self.norm1(x))\n",
        "        if return_attention:\n",
        "            return attn\n",
        "        x = x + self.drop_path(y)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvEmbed(nn.Module):\n",
        "    \"\"\"\n",
        "    3x3 Convolution stems for ViT following ViTC models\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels, strides, img_size=224, in_chans=3, batch_norm=True):\n",
        "        super().__init__()\n",
        "        # Build the stems\n",
        "        stem = []\n",
        "        channels = [in_chans] + channels\n",
        "        for i in range(len(channels) - 2):\n",
        "            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3,\n",
        "                               stride=strides[i], padding=1, bias=(not batch_norm))]\n",
        "            if batch_norm:\n",
        "                stem += [nn.BatchNorm2d(channels[i+1])]\n",
        "            stem += [nn.ReLU(inplace=True)]\n",
        "        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n",
        "        self.stem = nn.Sequential(*stem)\n",
        "\n",
        "        # Comptute the number of patches\n",
        "        stride_prod = int(np.prod(strides))\n",
        "        self.num_patches = (img_size[0] // stride_prod)**2\n",
        "\n",
        "    def forward(self, x):\n",
        "        p = self.stem(x)\n",
        "        return p.flatten(2).transpose(1, 2)\n",
        "\n",
        "\n",
        "class VisionTransformerPredictor(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_patches,\n",
        "        embed_dim=768,\n",
        "        predictor_embed_dim=384,\n",
        "        depth=6,\n",
        "        num_heads=12,\n",
        "        mlp_ratio=4.0,\n",
        "        qkv_bias=True,\n",
        "        qk_scale=None,\n",
        "        drop_rate=0.0,\n",
        "        attn_drop_rate=0.0,\n",
        "        drop_path_rate=0.0,\n",
        "        norm_layer=nn.LayerNorm,\n",
        "        init_std=0.02,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        # --\n",
        "        self.predictor_pos_embed = nn.Parameter(torch.zeros(1, num_patches, predictor_embed_dim),\n",
        "                                                requires_grad=False)\n",
        "        predictor_pos_embed = get_2d_sincos_pos_embed(self.predictor_pos_embed.shape[-1],\n",
        "                                                      int(num_patches**.5),\n",
        "                                                      cls_token=False)\n",
        "        self.predictor_pos_embed.data.copy_(torch.from_numpy(predictor_pos_embed).float().unsqueeze(0))\n",
        "        # --\n",
        "        self.predictor_blocks = nn.ModuleList([\n",
        "            Block(\n",
        "                dim=predictor_embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.predictor_norm = norm_layer(predictor_embed_dim)\n",
        "        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n",
        "        # ------\n",
        "        self.init_std = init_std\n",
        "        trunc_normal_(self.mask_token, std=self.init_std)\n",
        "        self.apply(self._init_weights)\n",
        "        self.fix_init_weight()\n",
        "\n",
        "    def fix_init_weight(self):\n",
        "        def rescale(param, layer_id):\n",
        "            param.div_(math.sqrt(2.0 * layer_id))\n",
        "\n",
        "        for layer_id, layer in enumerate(self.predictor_blocks):\n",
        "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
        "            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=self.init_std)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "        elif isinstance(m, nn.Conv2d):\n",
        "            trunc_normal_(m.weight, std=self.init_std)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x, masks_x, masks):\n",
        "        assert (masks is not None) and (masks_x is not None), 'Cannot run predictor without mask indices'\n",
        "\n",
        "        if not isinstance(masks_x, list):\n",
        "            masks_x = [masks_x]\n",
        "\n",
        "        if not isinstance(masks, list):\n",
        "            masks = [masks]\n",
        "\n",
        "        # -- Batch Size\n",
        "        B = len(x) // len(masks_x)\n",
        "\n",
        "        # -- map from encoder-dim to pedictor-dim\n",
        "        x = self.predictor_embed(x)\n",
        "\n",
        "        # -- add positional embedding to x tokens\n",
        "        x_pos_embed = self.predictor_pos_embed.repeat(B, 1, 1)\n",
        "        x += apply_masks(x_pos_embed, masks_x)\n",
        "\n",
        "        _, N_ctxt, D = x.shape\n",
        "\n",
        "        # -- concat mask tokens to x\n",
        "        pos_embs = self.predictor_pos_embed.repeat(B, 1, 1)\n",
        "        pos_embs = apply_masks(pos_embs, masks)\n",
        "        pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x))\n",
        "        # --\n",
        "        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1)\n",
        "        # --\n",
        "        pred_tokens += pos_embs\n",
        "        x = x.repeat(len(masks), 1, 1)\n",
        "        x = torch.cat([x, pred_tokens], dim=1)\n",
        "\n",
        "        # -- fwd prop\n",
        "        for blk in self.predictor_blocks:\n",
        "            x = blk(x)\n",
        "        x = self.predictor_norm(x)\n",
        "\n",
        "        # -- return preds for mask tokens\n",
        "        x = x[:, N_ctxt:]\n",
        "        x = self.predictor_proj(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size=[224],\n",
        "        patch_size=16,\n",
        "        in_chans=3,\n",
        "        embed_dim=768,\n",
        "        predictor_embed_dim=384,\n",
        "        depth=12,\n",
        "        predictor_depth=12,\n",
        "        num_heads=12,\n",
        "        mlp_ratio=4.0,\n",
        "        qkv_bias=True,\n",
        "        qk_scale=None,\n",
        "        drop_rate=0.0,\n",
        "        attn_drop_rate=0.0,\n",
        "        drop_path_rate=0.0,\n",
        "        norm_layer=nn.LayerNorm,\n",
        "        init_std=0.02,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.num_features = self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        # --\n",
        "        self.patch_embed = PatchEmbed(\n",
        "            img_size=img_size[0],\n",
        "            patch_size=patch_size,\n",
        "            in_chans=in_chans,\n",
        "            embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        # --\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1],\n",
        "                                            int(self.patch_embed.num_patches**.5),\n",
        "                                            cls_token=False)\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "        # --\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(\n",
        "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "        # ------\n",
        "        self.init_std = init_std\n",
        "        self.apply(self._init_weights)\n",
        "        self.fix_init_weight()\n",
        "\n",
        "    def fix_init_weight(self):\n",
        "        def rescale(param, layer_id):\n",
        "            param.div_(math.sqrt(2.0 * layer_id))\n",
        "\n",
        "        for layer_id, layer in enumerate(self.blocks):\n",
        "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
        "            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=self.init_std)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "        elif isinstance(m, nn.Conv2d):\n",
        "            trunc_normal_(m.weight, std=self.init_std)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x, masks=None):\n",
        "        if masks is not None:\n",
        "            if not isinstance(masks, list):\n",
        "                masks = [masks]\n",
        "\n",
        "        # -- patchify x\n",
        "        x = self.patch_embed(x)\n",
        "        B, N, D = x.shape\n",
        "\n",
        "        # -- add positional embedding to x\n",
        "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
        "        x = x + pos_embed\n",
        "\n",
        "        # -- mask x\n",
        "        if masks is not None:\n",
        "            x = apply_masks(x, masks)\n",
        "\n",
        "        # -- fwd prop\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            x = blk(x)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            x = self.norm(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def interpolate_pos_encoding(self, x, pos_embed):\n",
        "        npatch = x.shape[1] - 1\n",
        "        N = pos_embed.shape[1] - 1\n",
        "        if npatch == N:\n",
        "            return pos_embed\n",
        "        class_emb = pos_embed[:, 0]\n",
        "        pos_embed = pos_embed[:, 1:]\n",
        "        dim = x.shape[-1]\n",
        "        pos_embed = nn.functional.interpolate(\n",
        "            pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n",
        "            scale_factor=math.sqrt(npatch / N),\n",
        "            mode='bicubic',\n",
        "        )\n",
        "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
        "\n",
        "\n",
        "def vit_predictor(**kwargs):\n",
        "    model = VisionTransformerPredictor(\n",
        "        mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
        "        **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def vit_tiny(patch_size=16, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4,\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def vit_small(patch_size=16, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4,\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def vit_base(patch_size=16, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4,\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def vit_large(patch_size=16, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4,\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def vit_huge(patch_size=16, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=1280, depth=32, num_heads=16, mlp_ratio=4,\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def vit_giant(patch_size=16, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=1408, depth=40, num_heads=16, mlp_ratio=48/11,\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "VIT_EMBED_DIMS = {\n",
        "    'vit_tiny': 192,\n",
        "    'vit_small': 384,\n",
        "    'vit_base': 768,\n",
        "    'vit_large': 1024,\n",
        "    'vit_huge': 1280,\n",
        "    'vit_giant': 1408,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title train.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/train.py\n",
        "\n",
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "#\n",
        "\n",
        "import os\n",
        "\n",
        "# -- FOR DISTRIBUTED TRAINING ENSURE ONLY 1 DEVICE VISIBLE PER PROCESS\n",
        "try:\n",
        "    # -- WARNING: IF DOING DISTRIBUTED TRAINING ON A NON-SLURM CLUSTER, MAKE\n",
        "    # --          SURE TO UPDATE THIS TO GET LOCAL-RANK ON NODE, OR ENSURE\n",
        "    # --          THAT YOUR JOBS ARE LAUNCHED WITH ONLY 1 DEVICE VISIBLE\n",
        "    # --          TO EACH PROCESS\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = os.environ['SLURM_LOCALID']\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "import copy\n",
        "import logging\n",
        "import sys\n",
        "import yaml\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.multiprocessing as mp\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "\n",
        "from src.masks.multiblock import MaskCollator as MBMaskCollator\n",
        "from src.masks.utils import apply_masks\n",
        "from src.utils.distributed import (\n",
        "    init_distributed,\n",
        "    AllReduce\n",
        ")\n",
        "from src.utils.logging import (\n",
        "    CSVLogger,\n",
        "    gpu_timer,\n",
        "    grad_logger,\n",
        "    AverageMeter)\n",
        "from src.utils.tensors import repeat_interleave_batch\n",
        "from src.datasets.imagenet1k import make_imagenet1k\n",
        "\n",
        "from src.helper import (\n",
        "    load_checkpoint,\n",
        "    init_model,\n",
        "    init_opt)\n",
        "from src.transforms import make_transforms\n",
        "\n",
        "# --\n",
        "log_timings = True\n",
        "log_freq = 10\n",
        "checkpoint_freq = 50\n",
        "# --\n",
        "\n",
        "_GLOBAL_SEED = 0\n",
        "np.random.seed(_GLOBAL_SEED)\n",
        "torch.manual_seed(_GLOBAL_SEED)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logger = logging.getLogger()\n",
        "\n",
        "\n",
        "def main(args, resume_preempt=False):\n",
        "\n",
        "    # ----------------------------------------------------------------------- #\n",
        "    #  PASSED IN PARAMS FROM CONFIG FILE\n",
        "    # ----------------------------------------------------------------------- #\n",
        "\n",
        "    # -- META\n",
        "    use_bfloat16 = args['meta']['use_bfloat16']\n",
        "    model_name = args['meta']['model_name']\n",
        "    load_model = args['meta']['load_checkpoint'] or resume_preempt\n",
        "    r_file = args['meta']['read_checkpoint']\n",
        "    copy_data = args['meta']['copy_data']\n",
        "    pred_depth = args['meta']['pred_depth']\n",
        "    pred_emb_dim = args['meta']['pred_emb_dim']\n",
        "    if not torch.cuda.is_available():\n",
        "        device = torch.device('cpu')\n",
        "    else:\n",
        "        device = torch.device('cuda:0')\n",
        "        torch.cuda.set_device(device)\n",
        "\n",
        "    # -- DATA\n",
        "    use_gaussian_blur = args['data']['use_gaussian_blur']\n",
        "    use_horizontal_flip = args['data']['use_horizontal_flip']\n",
        "    use_color_distortion = args['data']['use_color_distortion']\n",
        "    color_jitter = args['data']['color_jitter_strength']\n",
        "    # --\n",
        "    batch_size = args['data']['batch_size']\n",
        "    pin_mem = args['data']['pin_mem']\n",
        "    num_workers = args['data']['num_workers']\n",
        "    root_path = args['data']['root_path']\n",
        "    image_folder = args['data']['image_folder']\n",
        "    crop_size = args['data']['crop_size']\n",
        "    crop_scale = args['data']['crop_scale']\n",
        "    # --\n",
        "\n",
        "    # -- MASK\n",
        "    allow_overlap = args['mask']['allow_overlap']  # whether to allow overlap b/w context and target blocks\n",
        "    patch_size = args['mask']['patch_size']  # patch-size for model training\n",
        "    num_enc_masks = args['mask']['num_enc_masks']  # number of context blocks\n",
        "    min_keep = args['mask']['min_keep']  # min number of patches in context block\n",
        "    enc_mask_scale = args['mask']['enc_mask_scale']  # scale of context blocks\n",
        "    num_pred_masks = args['mask']['num_pred_masks']  # number of target blocks\n",
        "    pred_mask_scale = args['mask']['pred_mask_scale']  # scale of target blocks\n",
        "    aspect_ratio = args['mask']['aspect_ratio']  # aspect ratio of target blocks\n",
        "    # --\n",
        "\n",
        "    # -- OPTIMIZATION\n",
        "    ema = args['optimization']['ema']\n",
        "    ipe_scale = args['optimization']['ipe_scale']  # scheduler scale factor (def: 1.0)\n",
        "    wd = float(args['optimization']['weight_decay'])\n",
        "    final_wd = float(args['optimization']['final_weight_decay'])\n",
        "    num_epochs = args['optimization']['epochs']\n",
        "    warmup = args['optimization']['warmup']\n",
        "    start_lr = args['optimization']['start_lr']\n",
        "    lr = args['optimization']['lr']\n",
        "    final_lr = args['optimization']['final_lr']\n",
        "\n",
        "    # -- LOGGING\n",
        "    folder = args['logging']['folder']\n",
        "    tag = args['logging']['write_tag']\n",
        "\n",
        "    dump = os.path.join(folder, 'params-ijepa.yaml')\n",
        "    with open(dump, 'w') as f:\n",
        "        yaml.dump(args, f)\n",
        "    # ----------------------------------------------------------------------- #\n",
        "\n",
        "    try:\n",
        "        mp.set_start_method('spawn')\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # -- init torch distributed backend\n",
        "    world_size, rank = init_distributed()\n",
        "    logger.info(f'Initialized (rank/world-size) {rank}/{world_size}')\n",
        "    if rank > 0:\n",
        "        logger.setLevel(logging.ERROR)\n",
        "\n",
        "    # -- log/checkpointing paths\n",
        "    log_file = os.path.join(folder, f'{tag}_r{rank}.csv')\n",
        "    save_path = os.path.join(folder, f'{tag}' + '-ep{epoch}.pth.tar')\n",
        "    latest_path = os.path.join(folder, f'{tag}-latest.pth.tar')\n",
        "    load_path = None\n",
        "    if load_model:\n",
        "        load_path = os.path.join(folder, r_file) if r_file is not None else latest_path\n",
        "\n",
        "    # -- make csv_logger\n",
        "    csv_logger = CSVLogger(log_file,\n",
        "                           ('%d', 'epoch'),\n",
        "                           ('%d', 'itr'),\n",
        "                           ('%.5f', 'loss'),\n",
        "                           ('%.5f', 'mask-A'),\n",
        "                           ('%.5f', 'mask-B'),\n",
        "                           ('%d', 'time (ms)'))\n",
        "\n",
        "    # -- init model\n",
        "    encoder, predictor = init_model(\n",
        "        device=device,\n",
        "        patch_size=patch_size,\n",
        "        crop_size=crop_size,\n",
        "        pred_depth=pred_depth,\n",
        "        pred_emb_dim=pred_emb_dim,\n",
        "        model_name=model_name)\n",
        "    target_encoder = copy.deepcopy(encoder)\n",
        "\n",
        "    # -- make data transforms\n",
        "    mask_collator = MBMaskCollator(\n",
        "        input_size=crop_size,\n",
        "        patch_size=patch_size,\n",
        "        pred_mask_scale=pred_mask_scale,\n",
        "        enc_mask_scale=enc_mask_scale,\n",
        "        aspect_ratio=aspect_ratio,\n",
        "        nenc=num_enc_masks,\n",
        "        npred=num_pred_masks,\n",
        "        allow_overlap=allow_overlap,\n",
        "        min_keep=min_keep)\n",
        "\n",
        "    transform = make_transforms(\n",
        "        crop_size=crop_size,\n",
        "        crop_scale=crop_scale,\n",
        "        gaussian_blur=use_gaussian_blur,\n",
        "        horizontal_flip=use_horizontal_flip,\n",
        "        color_distortion=use_color_distortion,\n",
        "        color_jitter=color_jitter)\n",
        "\n",
        "    # -- init data-loaders/samplers\n",
        "    _, unsupervised_loader, unsupervised_sampler = make_imagenet1k(\n",
        "            transform=transform,\n",
        "            batch_size=batch_size,\n",
        "            collator=mask_collator,\n",
        "            pin_mem=pin_mem,\n",
        "            training=True,\n",
        "            num_workers=num_workers,\n",
        "            world_size=world_size,\n",
        "            rank=rank,\n",
        "            root_path=root_path,\n",
        "            image_folder=image_folder,\n",
        "            copy_data=copy_data,\n",
        "            drop_last=True)\n",
        "    ipe = len(unsupervised_loader)\n",
        "\n",
        "    # -- init optimizer and scheduler\n",
        "    optimizer, scaler, scheduler, wd_scheduler = init_opt(\n",
        "        encoder=encoder,\n",
        "        predictor=predictor,\n",
        "        wd=wd,\n",
        "        final_wd=final_wd,\n",
        "        start_lr=start_lr,\n",
        "        ref_lr=lr,\n",
        "        final_lr=final_lr,\n",
        "        iterations_per_epoch=ipe,\n",
        "        warmup=warmup,\n",
        "        num_epochs=num_epochs,\n",
        "        ipe_scale=ipe_scale,\n",
        "        use_bfloat16=use_bfloat16)\n",
        "    encoder = DistributedDataParallel(encoder, static_graph=True)\n",
        "    predictor = DistributedDataParallel(predictor, static_graph=True)\n",
        "    target_encoder = DistributedDataParallel(target_encoder)\n",
        "    for p in target_encoder.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    # -- momentum schedule\n",
        "    momentum_scheduler = (ema[0] + i*(ema[1]-ema[0])/(ipe*num_epochs*ipe_scale)\n",
        "                          for i in range(int(ipe*num_epochs*ipe_scale)+1))\n",
        "\n",
        "    start_epoch = 0\n",
        "    # -- load training checkpoint\n",
        "    if load_model:\n",
        "        encoder, predictor, target_encoder, optimizer, scaler, start_epoch = load_checkpoint(\n",
        "            device=device,\n",
        "            r_path=load_path,\n",
        "            encoder=encoder,\n",
        "            predictor=predictor,\n",
        "            target_encoder=target_encoder,\n",
        "            opt=optimizer,\n",
        "            scaler=scaler)\n",
        "        for _ in range(start_epoch*ipe):\n",
        "            scheduler.step()\n",
        "            wd_scheduler.step()\n",
        "            next(momentum_scheduler)\n",
        "            mask_collator.step()\n",
        "\n",
        "    def save_checkpoint(epoch):\n",
        "        save_dict = {\n",
        "            'encoder': encoder.state_dict(),\n",
        "            'predictor': predictor.state_dict(),\n",
        "            'target_encoder': target_encoder.state_dict(),\n",
        "            'opt': optimizer.state_dict(),\n",
        "            'scaler': None if scaler is None else scaler.state_dict(),\n",
        "            'epoch': epoch,\n",
        "            'loss': loss_meter.avg,\n",
        "            'batch_size': batch_size,\n",
        "            'world_size': world_size,\n",
        "            'lr': lr\n",
        "        }\n",
        "        if rank == 0:\n",
        "            torch.save(save_dict, latest_path)\n",
        "            if (epoch + 1) % checkpoint_freq == 0:\n",
        "                torch.save(save_dict, save_path.format(epoch=f'{epoch + 1}'))\n",
        "\n",
        "    # -- TRAINING LOOP\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        logger.info('Epoch %d' % (epoch + 1))\n",
        "\n",
        "        # -- update distributed-data-loader epoch\n",
        "        unsupervised_sampler.set_epoch(epoch)\n",
        "\n",
        "        loss_meter = AverageMeter()\n",
        "        maskA_meter = AverageMeter()\n",
        "        maskB_meter = AverageMeter()\n",
        "        time_meter = AverageMeter()\n",
        "\n",
        "        for itr, (udata, masks_enc, masks_pred) in enumerate(unsupervised_loader):\n",
        "\n",
        "            def load_imgs():\n",
        "                # -- unsupervised imgs\n",
        "                imgs = udata[0].to(device, non_blocking=True)\n",
        "                masks_1 = [u.to(device, non_blocking=True) for u in masks_enc]\n",
        "                masks_2 = [u.to(device, non_blocking=True) for u in masks_pred]\n",
        "                return (imgs, masks_1, masks_2)\n",
        "            imgs, masks_enc, masks_pred = load_imgs()\n",
        "            maskA_meter.update(len(masks_enc[0][0]))\n",
        "            maskB_meter.update(len(masks_pred[0][0]))\n",
        "\n",
        "            def train_step():\n",
        "                _new_lr = scheduler.step()\n",
        "                _new_wd = wd_scheduler.step()\n",
        "                # --\n",
        "\n",
        "                def forward_target():\n",
        "                    with torch.no_grad():\n",
        "                        h = target_encoder(imgs)\n",
        "                        h = F.layer_norm(h, (h.size(-1),))  # normalize over feature-dim\n",
        "                        B = len(h)\n",
        "                        # -- create targets (masked regions of h)\n",
        "                        h = apply_masks(h, masks_pred)\n",
        "                        h = repeat_interleave_batch(h, B, repeat=len(masks_enc))\n",
        "                        return h\n",
        "\n",
        "                def forward_context():\n",
        "                    z = encoder(imgs, masks_enc)\n",
        "                    z = predictor(z, masks_enc, masks_pred)\n",
        "                    return z\n",
        "\n",
        "                def loss_fn(z, h):\n",
        "                    loss = F.smooth_l1_loss(z, h)\n",
        "                    loss = AllReduce.apply(loss)\n",
        "                    return loss\n",
        "\n",
        "                # Step 1. Forward\n",
        "                with torch.cuda.amp.autocast(dtype=torch.bfloat16, enabled=use_bfloat16):\n",
        "                    h = forward_target()\n",
        "                    z = forward_context()\n",
        "                    loss = loss_fn(z, h)\n",
        "\n",
        "                #  Step 2. Backward & step\n",
        "                if use_bfloat16:\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                else:\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                grad_stats = grad_logger(encoder.named_parameters())\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Step 3. momentum update of target encoder\n",
        "                with torch.no_grad():\n",
        "                    m = next(momentum_scheduler)\n",
        "                    for param_q, param_k in zip(encoder.parameters(), target_encoder.parameters()):\n",
        "                        param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "\n",
        "                return (float(loss), _new_lr, _new_wd, grad_stats)\n",
        "            (loss, _new_lr, _new_wd, grad_stats), etime = gpu_timer(train_step)\n",
        "            loss_meter.update(loss)\n",
        "            time_meter.update(etime)\n",
        "\n",
        "            # -- Logging\n",
        "            def log_stats():\n",
        "                csv_logger.log(epoch + 1, itr, loss, maskA_meter.val, maskB_meter.val, etime)\n",
        "                if (itr % log_freq == 0) or np.isnan(loss) or np.isinf(loss):\n",
        "                    logger.info('[%d, %5d] loss: %.3f '\n",
        "                                'masks: %.1f %.1f '\n",
        "                                '[wd: %.2e] [lr: %.2e] '\n",
        "                                '[mem: %.2e] '\n",
        "                                '(%.1f ms)'\n",
        "                                % (epoch + 1, itr,\n",
        "                                   loss_meter.avg,\n",
        "                                   maskA_meter.avg,\n",
        "                                   maskB_meter.avg,\n",
        "                                   _new_wd,\n",
        "                                   _new_lr,\n",
        "                                   torch.cuda.max_memory_allocated() / 1024.**2,\n",
        "                                   time_meter.avg))\n",
        "\n",
        "                    if grad_stats is not None:\n",
        "                        logger.info('[%d, %5d] grad_stats: [%.2e %.2e] (%.2e, %.2e)'\n",
        "                                    % (epoch + 1, itr,\n",
        "                                       grad_stats.first_layer,\n",
        "                                       grad_stats.last_layer,\n",
        "                                       grad_stats.min,\n",
        "                                       grad_stats.max))\n",
        "\n",
        "            log_stats()\n",
        "\n",
        "            assert not np.isnan(loss), 'loss is nan'\n",
        "\n",
        "        # -- Save Checkpoint after every epoch\n",
        "        logger.info('avg. loss %.3f' % loss_meter.avg)\n",
        "        save_checkpoint(epoch+1)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "YwYVTVKu8My0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}