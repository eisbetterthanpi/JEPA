{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/locuslab_mpc_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsXCi49JNzX-"
      },
      "source": [
        "## mpc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxrfE91Cy820"
      },
      "source": [
        "### locuslab_mpc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jf-o1_yAjBsN"
      },
      "outputs": [],
      "source": [
        "# https://github.com/locuslab/mpc.pytorch\n",
        "# https://locuslab.github.io/mpc.pytorch/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhvqvtdaj8E-",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title util\n",
        "# https://github.com/locuslab/mpc.pytorch/blob/master/mpc/util.py\n",
        "import torch\n",
        "from torch.autograd import Function, Variable\n",
        "import operator\n",
        "\n",
        "def jacobian(f, x, eps):\n",
        "    if x.ndimension() == 2:\n",
        "        assert x.size(0) == 1\n",
        "        x = x.squeeze()\n",
        "    e = Variable(torch.eye(len(x)).type_as(get_data_maybe(x)))\n",
        "    J = []\n",
        "    for i in range(len(x)):\n",
        "        J.append((f(x + eps*e[i]) - f(x - eps*e[i]))/(2.*eps))\n",
        "    J = torch.stack(J).transpose(0,1)\n",
        "    return J\n",
        "\n",
        "# def expandParam(X, n_batch, nDim):\n",
        "#     if X.ndimension() in (0, nDim):\n",
        "#         return X, False\n",
        "#     elif X.ndimension() == nDim - 1:\n",
        "#         return X.unsqueeze(0).expand(*([n_batch] + list(X.size()))), True\n",
        "#     else:\n",
        "#         raise RuntimeError(\"Unexpected number of dimensions.\")\n",
        "\n",
        "def bdiag(d):\n",
        "    assert d.ndimension() == 2\n",
        "    nBatch, sz = d.size()\n",
        "    dtype = d.type() if not isinstance(d, Variable) else d.data.type()\n",
        "    D = torch.zeros(nBatch, sz, sz).type(dtype)\n",
        "    I = torch.eye(sz).repeat(nBatch, 1, 1).type(dtype).byte()\n",
        "    D[I] = d.view(-1)\n",
        "    return D\n",
        "\n",
        "def bger(x, y):\n",
        "    return x.unsqueeze(2).bmm(y.unsqueeze(1))\n",
        "\n",
        "def bmv(X, y):\n",
        "    return X.bmm(y.unsqueeze(2)).squeeze(2)\n",
        "\n",
        "def bquad(x, Q):\n",
        "    return x.unsqueeze(1).bmm(Q).bmm(x.unsqueeze(2)).squeeze(1).squeeze(1)\n",
        "\n",
        "def bdot(x, y):\n",
        "    return torch.bmm(x.unsqueeze(1), y.unsqueeze(2)).squeeze(1).squeeze(1)\n",
        "\n",
        "def eclamp(x, lower, upper):\n",
        "    # In-place!!\n",
        "    if type(lower) == type(x):\n",
        "        assert x.size() == lower.size()\n",
        "    if type(upper) == type(x):\n",
        "        assert x.size() == upper.size()\n",
        "    I = x < lower\n",
        "    x[I] = lower[I] if not isinstance(lower, float) else lower\n",
        "    I = x > upper\n",
        "    x[I] = upper[I] if not isinstance(upper, float) else upper\n",
        "    return x\n",
        "\n",
        "def get_data_maybe(x):\n",
        "    return x if not isinstance(x, Variable) else x.data\n",
        "\n",
        "# _seen_tables = []\n",
        "# def table_log(tag, d):\n",
        "#     # TODO: There's probably a better way to handle formatting here, or a better way altogether to replace this quick hack.\n",
        "#     global _seen_tables\n",
        "#     def print_row(r):\n",
        "#         print('| ' + ' | '.join(r) + ' |')\n",
        "#     if tag not in _seen_tables:\n",
        "#         print_row(map(operator.itemgetter(0), d))\n",
        "#         _seen_tables.append(tag)\n",
        "#     s = []\n",
        "#     for di in d:\n",
        "#         assert len(di) in [2,3]\n",
        "#         if len(di) == 3:\n",
        "#             e, fmt = di[1:]\n",
        "#             s.append(fmt.format(e))\n",
        "#         else:\n",
        "#             e = di[1]\n",
        "#             s.append(str(e))\n",
        "#     print_row(s)\n",
        "\n",
        "def get_traj(T, u, x_init, dynamics):\n",
        "    if isinstance(dynamics, LinDx):\n",
        "        F = get_data_maybe(dynamics.F)\n",
        "        f = get_data_maybe(dynamics.f)\n",
        "        if f is not None:\n",
        "            assert f.shape == F.shape[:3]\n",
        "    x = [get_data_maybe(x_init)]\n",
        "    for t in range(T):\n",
        "        xt = x[t]\n",
        "        ut = get_data_maybe(u[t])\n",
        "        if t < T-1:\n",
        "            # new_x = f(Variable(xt), Variable(ut)).data\n",
        "            if isinstance(dynamics, LinDx):\n",
        "                xut = torch.cat((xt, ut), 1)\n",
        "                new_x = bmv(F[t], xut)\n",
        "                if f is not None:\n",
        "                    new_x += f[t]\n",
        "            else:\n",
        "                # print(\"in get_traj xt: \",xt.requires_grad) # 2 not here\n",
        "                new_x = dynamics(Variable(xt), Variable(ut)).data\n",
        "                # print(\"in get_traj new_x: \",new_x.requires_grad)\n",
        "            x.append(new_x)\n",
        "    x = torch.stack(x, dim=0)\n",
        "    return x\n",
        "\n",
        "def get_cost(T, u, cost, dynamics=None, x_init=None, x=None):\n",
        "    assert x_init is not None or x is not None\n",
        "    if isinstance(cost, QuadCost):\n",
        "        C = get_data_maybe(cost.C)\n",
        "        c = get_data_maybe(cost.c)\n",
        "    if x is None:\n",
        "        x = get_traj(T, u, x_init, dynamics)\n",
        "    objs = []\n",
        "    for t in range(T):\n",
        "        xt = x[t]\n",
        "        ut = u[t]\n",
        "        xut = torch.cat((xt, ut), 1)\n",
        "        if isinstance(cost, QuadCost):\n",
        "            obj = 0.5*bquad(xut, C[t]) + bdot(xut, c[t])\n",
        "        else:\n",
        "            obj = cost(xut)\n",
        "        objs.append(obj)\n",
        "    objs = torch.stack(objs, dim=0)\n",
        "    total_obj = torch.sum(objs, dim=0)\n",
        "    return total_obj\n",
        "\n",
        "def detach_maybe(x):\n",
        "    if x is None:\n",
        "        return None\n",
        "    return x if not x.requires_grad else x.detach()\n",
        "\n",
        "def data_maybe(x):\n",
        "    if x is None:\n",
        "        return None\n",
        "    return x.data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDr01NA8jxQ8",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title pnqp\n",
        "# https://github.com/locuslab/mpc.pytorch/blob/master/mpc/pnqp.py\n",
        "import torch\n",
        "\n",
        "def pnqp(H, q, lower, upper, x_init=None, n_iter=20):\n",
        "    GAMMA = 0.1\n",
        "    n_batch, n, _ = H.size()\n",
        "    pnqp_I = 1e-11*torch.eye(n).type_as(H).expand_as(H)\n",
        "    def obj(x):\n",
        "        return 0.5*bquad(x, H) + bdot(q, x)\n",
        "    if x_init is None:\n",
        "        if n == 1:\n",
        "            x_init = -(1./H.squeeze(2))*q\n",
        "        else:\n",
        "            # H_lu = H.lu()\n",
        "            # x_init = -q.unsqueeze(2).lu_solve(*H_lu).squeeze(2) # Clamped in the x assignment.\n",
        "            H_lu = torch.lu(H.squeeze(0))\n",
        "            x_init = torch.linalg.lu_solve(*H_lu, -q.unsqueeze(2)).squeeze(2)\n",
        "    else:\n",
        "        x_init = x_init.clone() # Don't over-write the original x_init.\n",
        "    x = eclamp(x_init, lower, upper)\n",
        "    # Active examples in the batch.\n",
        "    J = torch.ones(n_batch).type_as(x).byte()\n",
        "    for i in range(n_iter):\n",
        "        g = bmv(H, x) + q\n",
        "        # TODO: Could clean up the types here.\n",
        "        Ic = (((x == lower) & (g > 0)) | ((x == upper) & (g < 0))).float()\n",
        "        If = 1-Ic\n",
        "        if If.is_cuda:\n",
        "            Hff_I = bger(If.float(), If.float()).type_as(If)\n",
        "            not_Hff_I = 1-Hff_I\n",
        "            Hfc_I = bger(If.float(), Ic.float()).type_as(If)\n",
        "        else:\n",
        "            Hff_I = bger(If, If)\n",
        "            not_Hff_I = 1-Hff_I\n",
        "            Hfc_I = bger(If, Ic)\n",
        "        g_ = g.clone()\n",
        "        g_[Ic.bool()] = 0.\n",
        "        H_ = H.clone()\n",
        "        H_[not_Hff_I.bool()] = 0.0\n",
        "        H_ += pnqp_I\n",
        "        if n == 1:\n",
        "            dx = -(1./H_.squeeze(2))*g_\n",
        "        else:\n",
        "            # H_lu_ = H_.lu()\n",
        "            H_lu_ = torch.lu(H_)\n",
        "            dx = -g_.unsqueeze(2).lu_solve(*H_lu_).squeeze(2)\n",
        "        J = torch.norm(dx, 2, 1) >= 1e-4\n",
        "        m = J.sum().item() # Number of active examples in the batch.\n",
        "        if m == 0:\n",
        "            return x, H_ if n == 1 else H_lu_, If, i\n",
        "        alpha = torch.ones(n_batch).type_as(x)\n",
        "        decay = 0.1\n",
        "        max_armijo = GAMMA\n",
        "        count = 0\n",
        "        while max_armijo <= GAMMA and count < 10:\n",
        "            # Crude way of making sure too much time isn't being spent doing the line search.\n",
        "            # assert count < 10\n",
        "            maybe_x = eclamp(x+torch.diag(alpha).mm(dx), lower, upper)\n",
        "            armijos = (GAMMA+1e-6)*torch.ones(n_batch).type_as(x)\n",
        "            armijos[J] = (obj(x)-obj(maybe_x))[J]/bdot(g, x-maybe_x)[J]\n",
        "            I = armijos <= GAMMA\n",
        "            alpha[I] *= decay\n",
        "            max_armijo = torch.max(armijos)\n",
        "            count += 1\n",
        "        x = maybe_x\n",
        "    print(\"[WARNING] pnqp warning: Did not converge\") # TODO: Maybe change this to a warning.\n",
        "    return x, H_ if n == 1 else H_lu_, If, i\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0_0b9x8jmpa",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title dynamics\n",
        "# https://github.com/locuslab/mpc.pytorch/blob/master/mpc/dynamics.py\n",
        "import torch\n",
        "from torch.autograd import Function, Variable\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "# ACTS = {'sigmoid': torch.sigmoid, 'relu': F.relu, 'elu': F.elu,}\n",
        "\n",
        "# class NNDynamics(nn.Module):\n",
        "#     def __init__(self, n_state, n_ctrl, hidden_sizes=[100], activation='sigmoid', passthrough=True):\n",
        "#         super().__init__()\n",
        "#         self.passthrough = passthrough\n",
        "#         self.fcs = []\n",
        "#         in_sz = n_state+n_ctrl\n",
        "#         for out_sz in hidden_sizes + [n_state]:\n",
        "#             fc = nn.Linear(in_sz, out_sz)\n",
        "#             self.fcs.append(fc)\n",
        "#             in_sz = out_sz\n",
        "#         self.fcs = nn.ModuleList(self.fcs)\n",
        "#         assert activation in ACTS.keys()\n",
        "#         act_f = ACTS[activation]\n",
        "#         self.activation = activation\n",
        "#         self.acts = [act_f]*(len(self.fcs)-1)+[lambda x:x] # Activation functions.\n",
        "#         self.Ws = [y.weight for y in self.fcs]\n",
        "#         self.zs = [] # Activations.\n",
        "\n",
        "#     def __getstate__(self):\n",
        "#         return (self.fcs, self.activation, self.passthrough)\n",
        "\n",
        "#     def __setstate__(self, state):\n",
        "#         super().__init__()\n",
        "#         if len(state) == 2:\n",
        "#             # TODO: Remove this soon, keeping for some old models.\n",
        "#             self.fcs, self.activation = state\n",
        "#             self.passthrough = True\n",
        "#         else:\n",
        "#             self.fcs, self.activation, self.passthrough = state\n",
        "#         act_f = ACTS[self.activation]\n",
        "#         self.acts = [act_f]*(len(self.fcs)-1)+[lambda x:x] # Activation functions.\n",
        "#         self.Ws = [y.weight for y in self.fcs]\n",
        "\n",
        "#     def forward(self, x, u):\n",
        "#         x_dim, u_dim = x.ndimension(), u.ndimension()\n",
        "#         if x_dim == 1:\n",
        "#             x = x.unsqueeze(0)\n",
        "#         if u_dim == 1:\n",
        "#             u = u.unsqueeze(0)\n",
        "#         self.zs = []\n",
        "#         z = torch.cat((x, u), 1)\n",
        "#         for act, fc in zip(self.acts, self.fcs):\n",
        "#             z = act(fc(z))\n",
        "#             self.zs.append(z)\n",
        "#         # Hack: Don't include the output.\n",
        "#         self.zs = self.zs[:-1]\n",
        "#         if self.passthrough:\n",
        "#             z += x\n",
        "#         if x_dim == 1:\n",
        "#             z = z.squeeze(0)\n",
        "#         return z\n",
        "\n",
        "#     def grad_input(self, x, u):\n",
        "#         assert isinstance(x, Variable) == isinstance(u, Variable)\n",
        "#         diff = isinstance(x, Variable)\n",
        "#         x_dim, u_dim = x.ndimension(), u.ndimension()\n",
        "#         n_batch, n_state = x.size()\n",
        "#         _, n_ctrl = u.size()\n",
        "#         if not diff:\n",
        "#             Ws = [W.data for W in self.Ws]\n",
        "#             zs = [z.data for z in self.zs]\n",
        "#         else:\n",
        "#             Ws = self.Ws\n",
        "#             zs = self.zs\n",
        "#         assert len(zs) == len(Ws)-1\n",
        "#         grad = Ws[-1].repeat(n_batch,1,1)\n",
        "#         for i in range(len(zs)-1, 0-1, -1):\n",
        "#             n_out, n_in = Ws[i].size()\n",
        "#             if self.activation == 'relu':\n",
        "#                 I = get_data_maybe(zs[i] <= 0.).unsqueeze(2).repeat(1,1,n_in)\n",
        "#                 Wi_grad = Ws[i].repeat(n_batch,1,1)\n",
        "#                 Wi_grad[I] = 0.\n",
        "#             elif self.activation == 'sigmoid':\n",
        "#                 d = zs[i]*(1.-zs[i])\n",
        "#                 d = d.unsqueeze(2).expand(n_batch, n_out, n_in)\n",
        "#                 Wi_grad = Ws[i].repeat(n_batch,1,1)*d\n",
        "#             else:\n",
        "#                 assert False\n",
        "#             grad = grad.bmm(Wi_grad)\n",
        "#         R = grad[:,:,:n_state]\n",
        "#         S = grad[:,:,n_state:]\n",
        "#         if self.passthrough:\n",
        "#             I = torch.eye(n_state).type_as(get_data_maybe(R)).unsqueeze(0).repeat(n_batch, 1, 1)\n",
        "#             if diff:\n",
        "#                 I = Variable(I)\n",
        "#             R = R + I\n",
        "#         if x_dim == 1:\n",
        "#             R = R.squeeze(0)\n",
        "#             S = S.squeeze(0)\n",
        "#         return R, S\n",
        "\n",
        "class CtrlPassthroughDynamics(nn.Module):\n",
        "    def __init__(self, dynamics):\n",
        "        super().__init__()\n",
        "        self.dynamics = dynamics\n",
        "\n",
        "    def forward(self, tilde_x, u):\n",
        "        tilde_x_dim, u_dim = tilde_x.ndimension(), u.ndimension()\n",
        "        if tilde_x_dim == 1:\n",
        "            tilde_x = tilde_x.unsqueeze(0)\n",
        "        if u_dim == 1:\n",
        "            u = u.unsqueeze(0)\n",
        "        n_ctrl = u.size(1)\n",
        "        x = tilde_x[:,n_ctrl:]\n",
        "        # print(\"in cpd x: \",x.requires_grad)\n",
        "        xtp1 = self.dynamics(x, u)\n",
        "        # print(\"in cpd xtp1: \",xtp1.requires_grad)\n",
        "        tilde_xtp1 = torch.cat((u, xtp1), dim=1)\n",
        "        if tilde_x_dim == 1:\n",
        "            tilde_xtp1 = tilde_xtp1.squeeze()\n",
        "        return tilde_xtp1\n",
        "\n",
        "    def grad_input(self, x, u):\n",
        "        assert False, \"Unimplemented\"\n",
        "\n",
        "# class AffineDynamics(nn.Module):\n",
        "#     def __init__(self, A, B, c=None):\n",
        "#         super().__init__()\n",
        "#         assert A.ndimension() == 2\n",
        "#         assert B.ndimension() == 2\n",
        "#         if c is not None:\n",
        "#             assert c.ndimension() == 1\n",
        "#         self.A = A\n",
        "#         self.B = B\n",
        "#         self.c = c\n",
        "\n",
        "#     def forward(self, x, u):\n",
        "#         if not isinstance(x, Variable) and isinstance(self.A, Variable):\n",
        "#             A = self.A.data\n",
        "#             B = self.B.data\n",
        "#             c = self.c.data if self.c is not None else 0.\n",
        "#         else:\n",
        "#             A = self.A\n",
        "#             B = self.B\n",
        "#             c = self.c if self.c is not None else 0.\n",
        "#         x_dim, u_dim = x.ndimension(), u.ndimension()\n",
        "#         if x_dim == 1:\n",
        "#             x = x.unsqueeze(0)\n",
        "#         if u_dim == 1:\n",
        "#             u = u.unsqueeze(0)\n",
        "#         z = x.mm(A.t()) + u.mm(B.t()) + c\n",
        "#         if x_dim == 1:\n",
        "#             z = z.squeeze(0)\n",
        "#         return z\n",
        "\n",
        "#     def grad_input(self, x, u):\n",
        "#         n_batch = x.size(0)\n",
        "#         A, B = self.A, self.B\n",
        "#         A = A.unsqueeze(0).repeat(n_batch, 1, 1)\n",
        "#         B = B.unsqueeze(0).repeat(n_batch, 1, 1)\n",
        "#         if not isinstance(x, Variable) and isinstance(A, Variable):\n",
        "#             A, B = A.data, B.data\n",
        "#         return A, B\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dtt2mx2qjKdy",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title lqr_step\n",
        "# https://github.com/locuslab/mpc.pytorch/blob/master/mpc/lqr_step.py\n",
        "# time-varying linear control (LQR) problem\n",
        "import torch\n",
        "from torch.autograd import Function, Variable\n",
        "import numpy as np\n",
        "import numpy.random as npr\n",
        "from collections import namedtuple\n",
        "import time\n",
        "\n",
        "LqrBackOut = namedtuple('lqrBackOut', 'n_total_qp_iter')\n",
        "LqrForOut = namedtuple('lqrForOut', 'objs full_du_norm alpha_du_norm mean_alphas costs')\n",
        "\n",
        "def LQRStep(n_state, n_ctrl, T,\n",
        "            u_lower=None, u_upper=None,\n",
        "            u_zero_I=None,\n",
        "            delta_u=None,\n",
        "            linesearch_decay=0.2,\n",
        "            max_linesearch_iter=10,\n",
        "            true_cost=None,\n",
        "            true_dynamics=None,\n",
        "            delta_space=True,\n",
        "            current_x=None, current_u=None,\n",
        "            verbose=0,\n",
        "            back_eps=1e-3,\n",
        "            no_op_forward=False):\n",
        "    \"\"\"A single step of the box-constrained iLQR solver.\n",
        "        Required Args:\n",
        "            n_state, n_ctrl, T\n",
        "            x_init: The initial state [n_batch, n_state]\n",
        "        Optional Args:\n",
        "            u_lower, u_upper: The lower- and upper-bounds on the controls.\n",
        "                These can either be floats or shaped as [T, n_batch, n_ctrl]\n",
        "                TODO: Better support automatic expansion of these.\n",
        "            TODO\"\"\"\n",
        "    def lqr_backward(ctx, C, c, F, f):\n",
        "        n_batch = C.size(1)\n",
        "        u = ctx.current_u\n",
        "        Ks = []\n",
        "        ks = []\n",
        "        prev_kt = None\n",
        "        n_total_qp_iter = 0\n",
        "        Vtp1 = vtp1 = None\n",
        "        for t in range(T-1, -1, -1):\n",
        "            if t == T-1:\n",
        "                Qt = C[t]\n",
        "                qt = c[t]\n",
        "            else:\n",
        "                Ft = F[t]\n",
        "                Ft_T = Ft.transpose(1,2)\n",
        "                Qt = C[t] + Ft_T.bmm(Vtp1).bmm(Ft)\n",
        "                if f is None or f.nelement() == 0:\n",
        "                    qt = c[t] + Ft_T.bmm(vtp1.unsqueeze(2)).squeeze(2)\n",
        "                else:\n",
        "                    ft = f[t]\n",
        "                    qt = c[t] + Ft_T.bmm(Vtp1).bmm(ft.unsqueeze(2)).squeeze(2) + Ft_T.bmm(vtp1.unsqueeze(2)).squeeze(2)\n",
        "            Qt_xx = Qt[:, :n_state, :n_state]\n",
        "            Qt_xu = Qt[:, :n_state, n_state:]\n",
        "            Qt_ux = Qt[:, n_state:, :n_state]\n",
        "            Qt_uu = Qt[:, n_state:, n_state:]\n",
        "            qt_x = qt[:, :n_state]\n",
        "            qt_u = qt[:, n_state:]\n",
        "            if u_lower is None:\n",
        "                if n_ctrl == 1 and u_zero_I is None:\n",
        "                    Kt = -(1./Qt_uu)*Qt_ux\n",
        "                    kt = -(1./Qt_uu.squeeze(2))*qt_u\n",
        "                else:\n",
        "                    if u_zero_I is None:\n",
        "                        Qt_uu_inv = [torch.pinverse(Qt_uu[i]) for i in range(Qt_uu.shape[0])]\n",
        "                        Qt_uu_inv = torch.stack(Qt_uu_inv)\n",
        "                        Kt = -Qt_uu_inv.bmm(Qt_ux)\n",
        "                        kt = bmv(-Qt_uu_inv, qt_u)\n",
        "                        # Qt_uu_LU = Qt_uu.lu()\n",
        "                        # Kt = -Qt_ux.lu_solve(*Qt_uu_LU)\n",
        "                        # kt = -qt_u.lu_solve(*Qt_uu_LU)\n",
        "                    else:\n",
        "                        # Solve with zero constraints on the active controls.\n",
        "                        I = u_zero_I[t].float()\n",
        "                        notI = 1-I\n",
        "                        qt_u_ = qt_u.clone()\n",
        "                        qt_u_[I.bool()] = 0\n",
        "                        Qt_uu_ = Qt_uu.clone()\n",
        "                        if I.is_cuda:\n",
        "                            notI_ = notI.float()\n",
        "                            Qt_uu_I = (1-bger(notI_, notI_)).type_as(I)\n",
        "                        else:\n",
        "                            Qt_uu_I = 1-bger(notI, notI)\n",
        "                        Qt_uu_[Qt_uu_I.bool()] = 0.\n",
        "                        Qt_uu_[bdiag(I).bool()] += 1e-8\n",
        "                        Qt_ux_ = Qt_ux.clone()\n",
        "                        Qt_ux_[I.unsqueeze(2).repeat(1,1,Qt_ux.size(2)).bool()] = 0.\n",
        "                        if n_ctrl == 1:\n",
        "                            Kt = -(1./Qt_uu_)*Qt_ux_\n",
        "                            kt = -(1./Qt_uu.squeeze(2))*qt_u_\n",
        "                        else:\n",
        "                            # Qt_uu_LU_ = Qt_uu_.lu()\n",
        "                            Qt_uu_LU_ = torch.lu(Qt_uu_)\n",
        "                            Kt = -Qt_ux_.lu_solve(*Qt_uu_LU_)\n",
        "                            kt = -qt_u_.unsqueeze(2).lu_solve(*Qt_uu_LU_).squeeze(2)\n",
        "            else:\n",
        "                assert delta_space\n",
        "                lb = get_bound('lower', t) - u[t]\n",
        "                ub = get_bound('upper', t) - u[t]\n",
        "                if delta_u is not None:\n",
        "                    lb[lb < -delta_u] = -delta_u\n",
        "                    ub[ub > delta_u] = delta_u\n",
        "                kt, Qt_uu_free_LU, If, n_qp_iter = pnqp(Qt_uu, qt_u, lb, ub, x_init=prev_kt, n_iter=20)\n",
        "                if verbose > 1: print('  + n_qp_iter: ', n_qp_iter+1)\n",
        "                n_total_qp_iter += 1+n_qp_iter\n",
        "                prev_kt = kt\n",
        "                Qt_ux_ = Qt_ux.clone()\n",
        "                Qt_ux_[(1-If).unsqueeze(2).repeat(1,1,Qt_ux.size(2)).bool()] = 0\n",
        "                if n_ctrl == 1: Kt = -((1./Qt_uu_free_LU)*Qt_ux_) # Bad naming, Qt_uu_free_LU isn't the LU in this case.\n",
        "                else: Kt = -Qt_ux_.lu_solve(*Qt_uu_free_LU)\n",
        "            Kt_T = Kt.transpose(1,2)\n",
        "            Ks.append(Kt)\n",
        "            ks.append(kt)\n",
        "            Vtp1 = Qt_xx + Qt_xu.bmm(Kt) + Kt_T.bmm(Qt_ux) + Kt_T.bmm(Qt_uu).bmm(Kt)\n",
        "            vtp1 = qt_x + Qt_xu.bmm(kt.unsqueeze(2)).squeeze(2) + Kt_T.bmm(qt_u.unsqueeze(2)).squeeze(2) + Kt_T.bmm(Qt_uu).bmm(kt.unsqueeze(2)).squeeze(2)\n",
        "        return Ks, ks, n_total_qp_iter\n",
        "\n",
        "\n",
        "    def lqr_forward(ctx, x_init, C, c, F, f, Ks, ks):\n",
        "        x = ctx.current_x\n",
        "        u = ctx.current_u\n",
        "        n_batch = C.size(1)\n",
        "        old_cost = get_cost(T, u, true_cost, true_dynamics, x=x)\n",
        "        current_cost = None\n",
        "        alphas = torch.ones(n_batch).type_as(C)\n",
        "        full_du_norm = None\n",
        "        i = 0\n",
        "        while (current_cost is None or (old_cost is not None and torch.any((current_cost > old_cost)).cpu().item() == 1)) and i < max_linesearch_iter:\n",
        "            new_u = []\n",
        "            new_x = [x_init]\n",
        "            dx = [torch.zeros_like(x_init)]\n",
        "            objs = []\n",
        "            for t in range(T):\n",
        "                t_rev = T-1-t\n",
        "                Kt = Ks[t_rev]\n",
        "                kt = ks[t_rev]\n",
        "                new_xt = new_x[t]\n",
        "                xt = x[t]\n",
        "                ut = u[t]\n",
        "                dxt = dx[t]\n",
        "                new_ut = bmv(Kt, dxt) + ut + torch.diag(alphas).mm(kt)\n",
        "                # print(\"in lqrfwd bmv new_ut: \", new_ut)\n",
        "                # Currently unimplemented:\n",
        "                assert not ((delta_u is not None) and (u_lower is None))\n",
        "                if u_zero_I is not None:\n",
        "                    new_ut[u_zero_I[t]] = 0.\n",
        "                if u_lower is not None:\n",
        "                    lb = get_bound('lower', t)\n",
        "                    ub = get_bound('upper', t)\n",
        "                    if delta_u is not None:\n",
        "                        lb_limit, ub_limit = lb, ub\n",
        "                        lb = u[t] - delta_u\n",
        "                        ub = u[t] + delta_u\n",
        "                        I = lb < lb_limit\n",
        "                        lb[I] = lb_limit if isinstance(lb_limit, float) else lb_limit[I]\n",
        "                        I = ub > ub_limit\n",
        "                        ub[I] = ub_limit if isinstance(lb_limit, float) else ub_limit[I]\n",
        "                    # TODO(eugenevinitsky) why do we need to do this here?\n",
        "                    new_ut = eclamp(new_ut, lb, ub)\n",
        "                new_u.append(new_ut)\n",
        "                new_xut = torch.cat((new_xt, new_ut), dim=1)\n",
        "                if t < T-1:\n",
        "                    if isinstance(true_dynamics, LinDx):\n",
        "                        F, f = true_dynamics.F, true_dynamics.f\n",
        "                        new_xtp1 = bmv(F[t], new_xut)\n",
        "                        if f is not None and f.nelement() > 0:\n",
        "                            new_xtp1 += f[t]\n",
        "                    else:\n",
        "\n",
        "                        # new_xt.requires_grad=True\n",
        "                        # print(\"###########\")\n",
        "                        # new_xt=torch.rand(1,n_x)\n",
        "                        # new_ut=torch.rand(1,n_u)\n",
        "                        # print(\"in lqrfwd new_xt, ut: \",new_xt, new_ut)\n",
        "                        # new_xtp1 = true_dynamics(Variable(new_xt), Variable(new_ut)).data # og, one line only\n",
        "                        new_xtp1 = true_dynamics(new_xt, new_ut).data\n",
        "                        # new_xtp1 = model(new_xt, new_ut).data\n",
        "                        # print(\"in lqrfwd new_xtp1: \",new_xtp1.requires_grad)\n",
        "\n",
        "                    new_x.append(new_xtp1)\n",
        "                    dx.append(new_xtp1 - x[t+1])\n",
        "                if isinstance(true_cost, QuadCost):\n",
        "                    C, c = true_cost.C, true_cost.c\n",
        "                    obj = 0.5*bquad(new_xut, C[t]) + bdot(new_xut, c[t])\n",
        "                else:\n",
        "                    obj = true_cost(new_xut)\n",
        "                objs.append(obj)\n",
        "            objs = torch.stack(objs)\n",
        "            current_cost = torch.sum(objs, dim=0)\n",
        "            new_u = torch.stack(new_u)\n",
        "            new_x = torch.stack(new_x)\n",
        "            if full_du_norm is None:\n",
        "                full_du_norm = (u-new_u).transpose(1,2).contiguous().view(n_batch, -1).norm(2, 1)\n",
        "            alphas[current_cost > old_cost] *= linesearch_decay\n",
        "            i += 1\n",
        "        # If the iteration limit is hit, some alphas are one step too small.\n",
        "        alphas[current_cost > old_cost] /= linesearch_decay\n",
        "        alpha_du_norm = (u-new_u).transpose(1,2).contiguous().view(\n",
        "            n_batch, -1).norm(2, 1)\n",
        "        return new_x, new_u, LqrForOut(objs, full_du_norm, alpha_du_norm, torch.mean(alphas), current_cost)\n",
        "\n",
        "    def get_bound(side, t):\n",
        "        if side == 'lower': v = u_lower\n",
        "        if side == 'upper': v = u_upper\n",
        "        if isinstance(v, float): return v\n",
        "        else: return v[t]\n",
        "\n",
        "    class LQRStepFn(Function):\n",
        "        #staticmethod\n",
        "        def forward(ctx, x_init, C, c, F, f=None):\n",
        "            if no_op_forward:\n",
        "                ctx.save_for_backward(x_init, C, c, F, f, current_x, current_u)\n",
        "                ctx.current_x, ctx.current_u = current_x, current_u\n",
        "                return current_x, current_u\n",
        "            if delta_space:\n",
        "                # Taylor-expand the objective to do the backward pass in the delta space.\n",
        "                assert current_x is not None\n",
        "                assert current_u is not None\n",
        "                c_back = []\n",
        "                for t in range(T):\n",
        "                    xt = current_x[t]\n",
        "                    ut = current_u[t]\n",
        "                    xut = torch.cat((xt, ut), 1)\n",
        "                    c_back.append(bmv(C[t], xut) + c[t])\n",
        "                c_back = torch.stack(c_back)\n",
        "                f_back = None\n",
        "            else:\n",
        "                assert False\n",
        "            ctx.current_x = current_x\n",
        "            ctx.current_u = current_u\n",
        "            Ks, ks, n_total_qp_iter = lqr_backward(ctx, C, c_back, F, f_back)\n",
        "            new_x, new_u, for_out = lqr_forward(ctx, x_init, C, c, F, f, Ks, ks)\n",
        "            ctx.save_for_backward(x_init, C, c, F, f, new_x, new_u)\n",
        "            return new_x, new_u, torch.Tensor([n_total_qp_iter]), \\\n",
        "              for_out.costs, for_out.full_du_norm, for_out.mean_alphas\n",
        "\n",
        "        #staticmethod\n",
        "        def backward(ctx, dl_dx, dl_du, temp=None, temp2=None):\n",
        "            start = time.time()\n",
        "            x_init, C, c, F, f, new_x, new_u = ctx.saved_tensors\n",
        "            r = []\n",
        "            for t in range(T):\n",
        "                rt = torch.cat((dl_dx[t], dl_du[t]), 1)\n",
        "                r.append(rt)\n",
        "            r = torch.stack(r)\n",
        "            if u_lower is None: I = None\n",
        "            else: I = (torch.abs(new_u - u_lower) <= 1e-8) | (torch.abs(new_u - u_upper) <= 1e-8)\n",
        "            dx_init = Variable(torch.zeros_like(x_init))\n",
        "            _mpc = MPC(n_state, n_ctrl, T,\n",
        "                u_zero_I=I, u_init=None,\n",
        "                lqr_iter=1,\n",
        "                verbose=-1,\n",
        "                n_batch=C.size(1),\n",
        "                delta_u=None,\n",
        "                exit_unconverged=False, # It's really bad if this doesn't converge.\n",
        "                eps=back_eps,\n",
        "            )\n",
        "            dx, du, _ = _mpc(dx_init, QuadCost(C, -r), LinDx(F, None))\n",
        "            dx, du = dx.data, du.data\n",
        "            dxu = torch.cat((dx, du), 2)\n",
        "            xu = torch.cat((new_x, new_u), 2)\n",
        "            dC = torch.zeros_like(C)\n",
        "            for t in range(T):\n",
        "                xut = torch.cat((new_x[t], new_u[t]), 1)\n",
        "                dxut = dxu[t]\n",
        "                dCt = -0.5*(bger(dxut, xut) + bger(xut, dxut))\n",
        "                dC[t] = dCt\n",
        "            dc = -dxu\n",
        "            lams = []\n",
        "            prev_lam = None\n",
        "            for t in range(T-1, -1, -1):\n",
        "                Ct_xx = C[t,:,:n_state,:n_state]\n",
        "                Ct_xu = C[t,:,:n_state,n_state:]\n",
        "                ct_x = c[t,:,:n_state]\n",
        "                xt = new_x[t]\n",
        "                ut = new_u[t]\n",
        "                lamt = bmv(Ct_xx, xt) + bmv(Ct_xu, ut) + ct_x\n",
        "                if prev_lam is not None:\n",
        "                    Fxt = F[t,:,:,:n_state].transpose(1, 2)\n",
        "                    lamt += bmv(Fxt, prev_lam)\n",
        "                lams.append(lamt)\n",
        "                prev_lam = lamt\n",
        "            lams = list(reversed(lams))\n",
        "            dlams = []\n",
        "            prev_dlam = None\n",
        "            for t in range(T-1, -1, -1):\n",
        "                dCt_xx = C[t,:,:n_state,:n_state]\n",
        "                dCt_xu = C[t,:,:n_state,n_state:]\n",
        "                drt_x = -r[t,:,:n_state]\n",
        "                dxt = dx[t]\n",
        "                dut = du[t]\n",
        "                dlamt = bmv(dCt_xx, dxt) + bmv(dCt_xu, dut) + drt_x\n",
        "                if prev_dlam is not None:\n",
        "                    Fxt = F[t,:,:,:n_state].transpose(1, 2)\n",
        "                    dlamt += bmv(Fxt, prev_dlam)\n",
        "                dlams.append(dlamt)\n",
        "                prev_dlam = dlamt\n",
        "            dlams = torch.stack(list(reversed(dlams)))\n",
        "            dF = torch.zeros_like(F)\n",
        "            for t in range(T-1):\n",
        "                xut = xu[t]\n",
        "                lamt = lams[t+1]\n",
        "                dxut = dxu[t]\n",
        "                dlamt = dlams[t+1]\n",
        "                dF[t] = -(bger(dlamt, xut) + bger(lamt, dxut))\n",
        "            if f.nelement() > 0:\n",
        "                _dlams = dlams[1:]\n",
        "                assert _dlams.shape == f.shape\n",
        "                df = -_dlams\n",
        "            else:\n",
        "                df = torch.Tensor()\n",
        "            dx_init = -dlams[0]\n",
        "            backward_time = time.time()-start\n",
        "            return dx_init, dC, dc, dF, df\n",
        "    return LQRStepFn.apply\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjGT2NCFiPqs",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title mpc\n",
        "# https://github.com/locuslab/mpc.pytorch/blob/master/mpc/mpc.py\n",
        "import torch\n",
        "from torch.autograd import Function, Variable\n",
        "import numpy as np\n",
        "import numpy.random as npr\n",
        "from collections import namedtuple\n",
        "import sys\n",
        "\n",
        "QuadCost = namedtuple('QuadCost', 'C c')\n",
        "LinDx = namedtuple('LinDx', 'F f')\n",
        "\n",
        "# https://stackoverflow.com/questions/11351032\n",
        "QuadCost.__new__.__defaults__ = (None,) * len(QuadCost._fields)\n",
        "LinDx.__new__.__defaults__ = (None,) * len(LinDx._fields)\n",
        "\n",
        "from enum import Enum\n",
        "class GradMethods(Enum):\n",
        "    AUTO_DIFF = 1\n",
        "    FINITE_DIFF = 2\n",
        "    ANALYTIC = 3\n",
        "    ANALYTIC_CHECK = 4\n",
        "\n",
        "class SlewRateCost(nn.Module):\n",
        "    \"\"\"Hacky way of adding the slew rate penalty to costs.\"\"\"\n",
        "    # TODO: It would be cleaner to update this to just use the slew rate penalty instead of # slew_C\n",
        "    def __init__(self, cost, slew_C, n_state, n_ctrl):\n",
        "        super().__init__()\n",
        "        self.cost = cost\n",
        "        self.slew_C = slew_C\n",
        "        self.n_state = n_state\n",
        "        self.n_ctrl = n_ctrl\n",
        "\n",
        "    def forward(self, tau):\n",
        "        true_tau = tau[:, self.n_ctrl:]\n",
        "        true_cost = self.cost(true_tau)\n",
        "        # The slew constraints are time-invariant.\n",
        "        slew_cost = 0.5 * bquad(tau, self.slew_C[0])\n",
        "        return true_cost + slew_cost\n",
        "\n",
        "    def grad_input(self, x, u):\n",
        "        raise NotImplementedError(\"Implement grad_input\")\n",
        "\n",
        "\n",
        "class MPC(nn.Module):\n",
        "    \"\"\"A differentiable box-constrained iLQR solver.\n",
        "    This provides a differentiable solver for the following box-constrained control problem\n",
        "        with a quadratic cost (defined by C and c) and non-linear dynamics (defined by f):\n",
        "        min_{tau={x,u}} sum_t 0.5 tau_t^T C_t tau_t + c_t^T tau_t\n",
        "                        s.t. x_{t+1} = f(x_t, u_t)\n",
        "                     x_0 = x_init   ;   u_lower <= u <= u_upper\n",
        "    This implements the Control-Limited Differential Dynamic Programming paper with a first-order approximation to the non-linear dynamics: https://homes.cs.washington.edu/~todorov/papers/TassaICRA14.pdf\n",
        "    Some of the notation here is from Sergey Levine's notes: http://rll.berkeley.edu/deeprlcourse/f17docs/lecture_8_model_based_planning.pdf\n",
        "    Required Args: n_state, n_ctrl, T\n",
        "    Optional Args:\n",
        "        grad_method: The method to compute the Jacobian of the dynamics.\n",
        "            GradMethods.ANALYTIC: Use a manually-defined Jacobian. + Fast and accurate, use this if possible\n",
        "            GradMethods.AUTO_DIFF: Use PyTorch's autograd. + Slow\n",
        "            GradMethods.FINITE_DIFF: Use naive finite differences + Inaccurate\"\"\"\n",
        "    def __init__(self, n_state, n_ctrl, T,\n",
        "            u_lower=None, u_upper=None, # u_lower, u_upper: The lower- and upper-bounds on the controls. These can either be floats or shaped as [T, n_batch, n_ctrl]\n",
        "            u_zero_I=None,\n",
        "            u_init=None, # u_init: The initial control sequence, useful for warm-starting: [T, n_batch, n_ctrl]\n",
        "            lqr_iter=10, # lqr_iter: The number of LQR iterations to perform.\n",
        "            grad_method=GradMethods.AUTO_DIFF, # og: GradMethods.ANALYTIC,\n",
        "            delta_u=None, # delta_u (float): The amount each component of the controls is allowed to change in each LQR iteration.\n",
        "            verbose=0, # verbose (int): -1: No output or warnings ; 0 : Warnings ; 1+: Detailed iteration info\n",
        "            eps=1e-7, # eps: Termination threshold, on the norm of the full control step (without line search)\n",
        "            back_eps=1e-7, # back_eps: `eps` value to use in the backwards pass.\n",
        "            n_batch=None, # n_batch: May be necessary for now if it can't be inferred. TODO: Infer, potentially remove this.\n",
        "            linesearch_decay=0.2, # 0.2 # linesearch_decay (float): Multiplicative decay factor for the line search.\n",
        "            max_linesearch_iter=5, # 10 # max_linesearch_iter (int): Can be used to disable the line search. if 1 is used for some problems the line search can be harmful.\n",
        "            exit_unconverged=False, # True # exit_unconverged: Assert False if a fixed point is not reached.\n",
        "            detach_unconverged=False, # True # detach_unconverged: Detach examples from the graph that do not hit a fixed point so they are not differentiated through.\n",
        "            backprop=True, # backprop: Allow the solver to be differentiated through.\n",
        "            slew_rate_penalty=None, # slew_rate_penalty (float): Penalty term applied to ||u_t - u_{t+1}||_2^2 in the objective.\n",
        "            prev_ctrl=None, # prev_ctrl: The previous nominal control sequence to initialize the solver with.\n",
        "            not_improved_lim=5, # not_improved_lim: The number of iterations to allow that don't improve the objective before returning early.\n",
        "            best_cost_eps=1e-4 # best_cost_eps: Absolute threshold for the best cost to be updated.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert (u_lower is None) == (u_upper is None)\n",
        "        assert max_linesearch_iter > 0\n",
        "        self.n_state = n_state\n",
        "        self.n_ctrl = n_ctrl\n",
        "        self.T = T\n",
        "        self.u_lower = u_lower\n",
        "        self.u_upper = u_upper\n",
        "        if not isinstance(u_lower, float): self.u_lower = detach_maybe(self.u_lower)\n",
        "        if not isinstance(u_upper, float): self.u_upper = detach_maybe(self.u_upper)\n",
        "        self.u_zero_I = detach_maybe(u_zero_I)\n",
        "        self.u_init = detach_maybe(u_init)\n",
        "        self.lqr_iter = lqr_iter\n",
        "        self.grad_method = grad_method\n",
        "        self.delta_u = delta_u\n",
        "        self.verbose = verbose\n",
        "        self.eps = eps\n",
        "        self.back_eps = back_eps\n",
        "        self.n_batch = n_batch\n",
        "        self.linesearch_decay = linesearch_decay\n",
        "        self.max_linesearch_iter = max_linesearch_iter\n",
        "        self.exit_unconverged = exit_unconverged\n",
        "        self.detach_unconverged = detach_unconverged\n",
        "        self.backprop = backprop\n",
        "        self.not_improved_lim = not_improved_lim\n",
        "        self.best_cost_eps = best_cost_eps\n",
        "        self.slew_rate_penalty = slew_rate_penalty\n",
        "        self.prev_ctrl = prev_ctrl\n",
        "\n",
        "    def forward(self, x_init, cost, dx):\n",
        "        # print(\"in mpcfwd x_init: \",x_init.requires_grad,x_init)\n",
        "        # QuadCost.C: [T, n_batch, n_tau, n_tau]\n",
        "        # QuadCost.c: [T, n_batch, n_tau]\n",
        "        assert isinstance(cost, QuadCost) or isinstance(cost, nn.Module) or isinstance(cost, Function)\n",
        "        assert isinstance(dx, LinDx) or isinstance(dx, nn.Module) or isinstance(dx, Function)\n",
        "        # TODO: Clean up inferences, expansions, and assumptions made here.\n",
        "        if self.n_batch is not None: n_batch = self.n_batch\n",
        "        elif isinstance(cost, QuadCost) and cost.C.ndimension() == 4: n_batch = cost.C.size(1)\n",
        "        else:\n",
        "            print('MPC Error: Could not infer batch size, pass in as n_batch')\n",
        "            sys.exit(-1)\n",
        "        # if c.ndimension() == 2:\n",
        "        #     c = c.unsqueeze(1).expand(self.T, n_batch, -1)\n",
        "        if isinstance(cost, QuadCost):\n",
        "            C, c = cost\n",
        "            if C.ndimension() == 2: # Add the time and batch dimensions.\n",
        "                C = C.unsqueeze(0).unsqueeze(0).expand(self.T, n_batch, self.n_state+self.n_ctrl, -1)\n",
        "            elif C.ndimension() == 3: # Add the batch dimension.\n",
        "                C = C.unsqueeze(1).expand(self.T, n_batch, self.n_state+self.n_ctrl, -1)\n",
        "            if c.ndimension() == 1: # Add the time and batch dimensions.\n",
        "                c = c.unsqueeze(0).unsqueeze(0).expand(self.T, n_batch, -1)\n",
        "            elif c.ndimension() == 2: # Add the batch dimension.\n",
        "                c = c.unsqueeze(1).expand(self.T, n_batch, -1)\n",
        "            if C.ndimension() != 4 or c.ndimension() != 3:\n",
        "                print('MPC Error: Unexpected QuadCost shape.')\n",
        "                sys.exit(-1)\n",
        "            cost = QuadCost(C, c)\n",
        "        assert x_init.ndimension() == 2 and x_init.size(0) == n_batch\n",
        "        if self.u_init is None:\n",
        "            u = torch.zeros(self.T, n_batch, self.n_ctrl).type_as(x_init.data)\n",
        "        else:\n",
        "            u = self.u_init\n",
        "            if u.ndimension() == 2: u = u.unsqueeze(1).expand(self.T, n_batch, -1).clone()\n",
        "        u = u.type_as(x_init.data)\n",
        "        if self.verbose > 0:\n",
        "            print('Initial mean(cost): {:.4e}'.format(torch.mean(get_cost(self.T, u, cost, dx, x_init=x_init)).item()))\n",
        "        best = None\n",
        "        n_not_improved = 0\n",
        "        for i in range(self.lqr_iter):\n",
        "            u = Variable(detach_maybe(u), requires_grad=True)\n",
        "            # Linearize the dynamics around the current trajectory.\n",
        "            x = get_traj(self.T, u, x_init=x_init, dynamics=dx)\n",
        "            if isinstance(dx, LinDx): F, f = dx.F, dx.f\n",
        "            else: F, f = self.linearize_dynamics(x, detach_maybe(u), dx, diff=False)\n",
        "            if isinstance(cost, QuadCost): C, c = cost.C, cost.c\n",
        "            else: C, c, _ = self.approximate_cost(x, detach_maybe(u), cost, diff=False)\n",
        "            # print(\"in mpc b4 x_init: \",x_init.requires_grad,x_init)\n",
        "            x, u, n_total_qp_iter, costs, full_du_norm, mean_alphas = self.solve_lqr_subproblem(x_init, C, c, F, f, cost, dx, x, u)\n",
        "            n_not_improved += 1\n",
        "            assert x.ndimension() == 3\n",
        "            assert u.ndimension() == 3\n",
        "            if best is None:\n",
        "                best = {\n",
        "                    'x': list(torch.split(x, split_size_or_sections=1, dim=1)),\n",
        "                    'u': list(torch.split(u, split_size_or_sections=1, dim=1)),\n",
        "                    'costs': costs,\n",
        "                    'full_du_norm': full_du_norm,\n",
        "                }\n",
        "            else:\n",
        "                for j in range(n_batch):\n",
        "                    if costs[j] <= best['costs'][j] + self.best_cost_eps:\n",
        "                        n_not_improved = 0\n",
        "                        best['x'][j] = x[:,j].unsqueeze(1)\n",
        "                        best['u'][j] = u[:,j].unsqueeze(1)\n",
        "                        best['costs'][j] = costs[j]\n",
        "                        best['full_du_norm'][j] = full_du_norm[j]\n",
        "            # if self.verbose > 0:\n",
        "            #     table_log('lqr', (\n",
        "            #         ('iter', i),\n",
        "            #         ('mean(cost)', torch.mean(best['costs']).item(), '{:.4e}'),\n",
        "            #         ('||full_du||_max', max(full_du_norm).item(), '{:.2e}'),\n",
        "            #         # ('||alpha_du||_max', max(alpha_du_norm), '{:.2e}'),\n",
        "            #         # TODO: alphas, total_qp_iters here is for the current iterate, not the best\n",
        "            #         ('mean(alphas)', mean_alphas.item(), '{:.2e}'),\n",
        "            #         ('total_qp_iters', n_total_qp_iter),\n",
        "            #     ))\n",
        "            if max(full_du_norm) < self.eps or n_not_improved > self.not_improved_lim: break\n",
        "        x = torch.cat(best['x'], dim=1)\n",
        "        u = torch.cat(best['u'], dim=1)\n",
        "        full_du_norm = best['full_du_norm']\n",
        "        if isinstance(dx, LinDx): F, f = dx.F, dx.f\n",
        "        else: F, f = self.linearize_dynamics(x, u, dx, diff=True)\n",
        "        if isinstance(cost, QuadCost): C, c = cost.C, cost.c\n",
        "        else: C, c, _ = self.approximate_cost(x, u, cost, diff=True)\n",
        "        x, u = self.solve_lqr_subproblem(x_init, C, c, F, f, cost, dx, x, u, no_op_forward=True)\n",
        "        if self.detach_unconverged:\n",
        "            if max(best['full_du_norm']) > self.eps:\n",
        "                if self.exit_unconverged:\n",
        "                    assert False\n",
        "                if self.verbose >= 0:\n",
        "                    print(\"LQR Warning: All examples did not converge to a fixed point.\")\n",
        "                    print(\"Detaching and *not* backpropping through the bad examples.\")\n",
        "                I = full_du_norm < self.eps\n",
        "                Ix = Variable(I.unsqueeze(0).unsqueeze(2).expand_as(x)).type_as(x.data)\n",
        "                Iu = Variable(I.unsqueeze(0).unsqueeze(2).expand_as(u)).type_as(u.data)\n",
        "                x = x*Ix + x.clone().detach()*(1.-Ix)\n",
        "                u = u*Iu + u.clone().detach()*(1.-Iu)\n",
        "        costs = best['costs']\n",
        "        return (x, u, costs)\n",
        "\n",
        "    def solve_lqr_subproblem(self, x_init, C, c, F, f, cost, dynamics, x, u, no_op_forward=False):\n",
        "        if self.slew_rate_penalty is None or isinstance(cost, nn.Module):\n",
        "            _lqr = LQRStep(n_state=self.n_state, n_ctrl=self.n_ctrl, T=self.T,\n",
        "                u_lower=self.u_lower, u_upper=self.u_upper,\n",
        "                u_zero_I=self.u_zero_I,\n",
        "                true_cost=cost,\n",
        "                true_dynamics=dynamics,\n",
        "                delta_u=self.delta_u,\n",
        "                linesearch_decay=self.linesearch_decay,\n",
        "                max_linesearch_iter=self.max_linesearch_iter,\n",
        "                delta_space=True,\n",
        "                current_x=x, current_u=u,\n",
        "                back_eps=self.back_eps,\n",
        "                no_op_forward=no_op_forward,\n",
        "            )\n",
        "            e = Variable(torch.Tensor())\n",
        "            # print(\"in solve_lqr_sp x_init: \",x_init.requires_grad,x_init)\n",
        "            return _lqr(x_init, C, c, F, f if f is not None else e)\n",
        "        else:\n",
        "            nsc = self.n_state + self.n_ctrl\n",
        "            _n_state = nsc\n",
        "            _nsc = _n_state + self.n_ctrl\n",
        "            n_batch = C.size(1)\n",
        "            _C = torch.zeros(self.T, n_batch, _nsc, _nsc).type_as(C)\n",
        "            half_gamI = self.slew_rate_penalty*torch.eye(\n",
        "                self.n_ctrl).unsqueeze(0).unsqueeze(0).repeat(self.T, n_batch, 1, 1)\n",
        "            _C[:,:,:self.n_ctrl,:self.n_ctrl] = half_gamI\n",
        "            _C[:,:,-self.n_ctrl:,:self.n_ctrl] = -half_gamI\n",
        "            _C[:,:,:self.n_ctrl,-self.n_ctrl:] = -half_gamI\n",
        "            _C[:,:,-self.n_ctrl:,-self.n_ctrl:] = half_gamI\n",
        "            slew_C = _C.clone()\n",
        "            _C = _C + torch.nn.ZeroPad2d((self.n_ctrl, 0, self.n_ctrl, 0))(C)\n",
        "            _c = torch.cat((torch.zeros(self.T, n_batch, self.n_ctrl).type_as(c),c), 2)\n",
        "            _F0 = torch.cat((torch.zeros(self.n_ctrl, self.n_state+self.n_ctrl), torch.eye(self.n_ctrl),), 1).type_as(F).unsqueeze(0).unsqueeze(0).repeat(self.T-1, n_batch, 1, 1)\n",
        "            _F1 = torch.cat((torch.zeros(self.T-1, n_batch, self.n_state, self.n_ctrl).type_as(F),F), 3)\n",
        "            _F = torch.cat((_F0, _F1), 2)\n",
        "            if f is not None: _f = torch.cat((torch.zeros(self.T-1, n_batch, self.n_ctrl).type_as(f),f), 2)\n",
        "            else: _f = Variable(torch.Tensor())\n",
        "            u_data = detach_maybe(u)\n",
        "            if self.prev_ctrl is not None:\n",
        "                prev_u = self.prev_ctrl\n",
        "                if prev_u.ndimension() == 1: prev_u = prev_u.unsqueeze(0)\n",
        "                if prev_u.ndimension() == 2: prev_u = prev_u.unsqueeze(0)\n",
        "                prev_u = prev_u.data\n",
        "            else:\n",
        "                prev_u = torch.zeros(1, n_batch, self.n_ctrl).type_as(u)\n",
        "            utm1s = torch.cat((prev_u, u_data[:-1])).clone()\n",
        "            _x = torch.cat((utm1s, x), 2)\n",
        "            _x_init = torch.cat((Variable(prev_u[0]), x_init), 1)\n",
        "            if not isinstance(dynamics, LinDx): _dynamics = CtrlPassthroughDynamics(dynamics)\n",
        "            else: _dynamics = None\n",
        "            if isinstance(cost, QuadCost): _true_cost = QuadCost(_C, _c)\n",
        "            else: _true_cost = SlewRateCost(cost, slew_C, self.n_state, self.n_ctrl)\n",
        "            _lqr = LQRStep(n_state=_n_state, n_ctrl=self.n_ctrl, T=self.T,\n",
        "                u_lower=self.u_lower, u_upper=self.u_upper,\n",
        "                u_zero_I=self.u_zero_I,\n",
        "                true_cost=_true_cost,\n",
        "                true_dynamics=_dynamics,\n",
        "                delta_u=self.delta_u,\n",
        "                linesearch_decay=self.linesearch_decay,\n",
        "                max_linesearch_iter=self.max_linesearch_iter,\n",
        "                delta_space=True,\n",
        "                current_x=_x, current_u=u,\n",
        "                back_eps=self.back_eps,\n",
        "                no_op_forward=no_op_forward,\n",
        "            )\n",
        "            x, *rest = _lqr(_x_init, _C, _c, _F, _f)\n",
        "            x = x[:,:,self.n_ctrl:]\n",
        "            return [x] + rest\n",
        "\n",
        "    def approximate_cost(self, x, u, Cf, diff=True):\n",
        "        with torch.enable_grad():\n",
        "            tau = torch.cat((x, u), dim=2).data\n",
        "            tau = Variable(tau, requires_grad=True)\n",
        "            if self.slew_rate_penalty is not None:\n",
        "                print(\"\"\"MPC Error: Using a non-convex cost with a slew rate penalty is not yet implemented. The current implementation does not correctly do a line search. More details: https://github.com/locuslab/mpc.pytorch/issues/12\"\"\")\n",
        "                sys.exit(-1)\n",
        "                differences = tau[1:, :, -self.n_ctrl:] - tau[:-1, :, -self.n_ctrl:]\n",
        "                slew_penalty = (self.slew_rate_penalty * differences.pow(2)).sum(-1)\n",
        "            costs = list()\n",
        "            hessians = list()\n",
        "            grads = list()\n",
        "            for t in range(self.T):\n",
        "                tau_t = tau[t]\n",
        "                if self.slew_rate_penalty is not None: cost = Cf(tau_t) + (slew_penalty[t-1] if t > 0 else 0)\n",
        "                else: cost = Cf(tau_t)\n",
        "                grad = torch.autograd.grad(cost.sum(), tau_t, create_graph=True, retain_graph=True)[0]\n",
        "                hessian = list()\n",
        "                for v_i in range(tau.shape[2]):\n",
        "                    hessian.append(torch.autograd.grad(grad[:, v_i].sum(), tau_t, retain_graph=True)[0])\n",
        "                hessian = torch.stack(hessian, dim=-1)\n",
        "                costs.append(cost)\n",
        "                grads.append(grad - bmv(hessian, tau_t))\n",
        "                hessians.append(hessian)\n",
        "            costs = torch.stack(costs, dim=0)\n",
        "            grads = torch.stack(grads, dim=0)\n",
        "            hessians = torch.stack(hessians, dim=0)\n",
        "            if not diff: return hessians.data, grads.data, costs.data\n",
        "            return hessians, grads, costs\n",
        "\n",
        "    def linearize_dynamics(self, x, u, dynamics, diff):\n",
        "        # TODO: Cleanup variable usage.\n",
        "        n_batch = x[0].size(0)\n",
        "        if self.grad_method == GradMethods.ANALYTIC:\n",
        "            _u = Variable(u[:-1].view(-1, self.n_ctrl), requires_grad=True)\n",
        "            _x = Variable(x[:-1].contiguous().view(-1, self.n_state), requires_grad=True)\n",
        "            # This inefficiently calls dynamics again, but is worth it because\n",
        "            # we can efficiently compute grad_input for every time step at once.\n",
        "            # print(\"in lin dy _x: \",_x.requires_grad) # 2 not here\n",
        "            _new_x = dynamics(_x, _u)\n",
        "            # print(\"in lin dy _new_x: \",_new_x.requires_grad)\n",
        "            # This check is a little expensive and should only be done if modifying this code.\n",
        "            # assert torch.abs(_new_x.data - torch.cat(x[1:])).max() <= 1e-6\n",
        "            if not diff:\n",
        "                _new_x = _new_x.data\n",
        "                _x = _x.data\n",
        "                _u = _u.data\n",
        "            R, S = dynamics.grad_input(_x, _u)\n",
        "            f = _new_x - bmv(R, _x) - bmv(S, _u)\n",
        "            f = f.view(self.T-1, n_batch, self.n_state)\n",
        "            R = R.contiguous().view(self.T-1, n_batch, self.n_state, self.n_state)\n",
        "            S = S.contiguous().view(self.T-1, n_batch, self.n_state, self.n_ctrl)\n",
        "            F = torch.cat((R, S), 3)\n",
        "            if not diff: F, f = list(map(Variable, [F, f]))\n",
        "            return F, f\n",
        "        else:\n",
        "            # TODO: This is inefficient and confusing.\n",
        "            x_init = x[0]\n",
        "            x = [x_init]\n",
        "            F, f = [], []\n",
        "            for t in range(self.T):\n",
        "                if t < self.T-1:\n",
        "                    xt = Variable(x[t], requires_grad=True)\n",
        "                    ut = Variable(u[t], requires_grad=True)\n",
        "                    xut = torch.cat((xt, ut), 1)\n",
        "                    # print(\"in lin dy xt: \",xt.requires_grad,xt) # 2 not here\n",
        "                    new_x = dynamics(xt, ut)\n",
        "                    # print(\"in lin dy new_x: \",new_x.requires_grad,new_x) # 2 not here\n",
        "                    # Linear dynamics approximation.\n",
        "                    if self.grad_method in [GradMethods.AUTO_DIFF, GradMethods.ANALYTIC_CHECK]:\n",
        "                        Rt, St = [], []\n",
        "                        for j in range(self.n_state):\n",
        "                            Rj, Sj = torch.autograd.grad(new_x[:,j].sum(), [xt, ut], retain_graph=True)\n",
        "                            if not diff: Rj, Sj = Rj.data, Sj.data\n",
        "                            Rt.append(Rj)\n",
        "                            St.append(Sj)\n",
        "                        Rt = torch.stack(Rt, dim=1)\n",
        "                        St = torch.stack(St, dim=1)\n",
        "                        if self.grad_method == GradMethods.ANALYTIC_CHECK:\n",
        "                            assert False # Not updated\n",
        "                            Rt_autograd, St_autograd = Rt, St\n",
        "                            Rt, St = dynamics.grad_input(xt, ut)\n",
        "                            eps = 1e-8\n",
        "                            if torch.max(torch.abs(Rt-Rt_autograd)).data[0] > eps or \\\n",
        "                            torch.max(torch.abs(St-St_autograd)).data[0] > eps: print('''nmpc.ANALYTIC_CHECK error: The analytic derivative of the dynamics function may be off.''')\n",
        "                            else: print('''nmpc.ANALYTIC_CHECK: The analytic derivative of the dynamics function seems correct. Re-run with GradMethods.ANALYTIC to continue.''')\n",
        "                            sys.exit(0)\n",
        "                    elif self.grad_method == GradMethods.FINITE_DIFF:\n",
        "                        Rt, St = [], []\n",
        "                        for i in range(n_batch):\n",
        "                            Ri = jacobian(lambda s: dynamics(s, ut[i]), xt[i], 1e-4)\n",
        "                            Si = jacobian(lambda a : dynamics(xt[i], a), ut[i], 1e-4)\n",
        "                            if not diff:\n",
        "                                Ri, Si = Ri.data, Si.data\n",
        "                            Rt.append(Ri)\n",
        "                            St.append(Si)\n",
        "                        Rt = torch.stack(Rt)\n",
        "                        St = torch.stack(St)\n",
        "                    else: assert False\n",
        "                    Ft = torch.cat((Rt, St), 2)\n",
        "                    F.append(Ft)\n",
        "                    if not diff: xt, ut, new_x = xt.data, ut.data, new_x.data\n",
        "                    ft = new_x - bmv(Rt, xt) - bmv(St, ut)\n",
        "                    f.append(ft)\n",
        "                if t < self.T-1: x.append(detach_maybe(new_x))\n",
        "            F = torch.stack(F, 0)\n",
        "            f = torch.stack(f, 0)\n",
        "            if not diff: F, f = list(map(Variable, [F, f]))\n",
        "            return F, f\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y67R8bVQkXDW",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title pendulum\n",
        "# # https://github.com/locuslab/mpc.pytorch/blob/master/mpc/env_dx/pendulum.py\n",
        "# import torch\n",
        "# from torch.autograd import Function, Variable\n",
        "# import torch.nn.functional as F\n",
        "# from torch import nn\n",
        "# import numpy as np\n",
        "# import os\n",
        "# import matplotlib\n",
        "# matplotlib.use('Agg')\n",
        "# import matplotlib.pyplot as plt\n",
        "# plt.style.use('bmh')\n",
        "\n",
        "# class PendulumDx(nn.Module):\n",
        "#     def __init__(self, params=None, simple=True):\n",
        "#         super().__init__()\n",
        "#         self.simple = simple\n",
        "#         self.max_torque = 2.0\n",
        "#         self.dt = 0.05\n",
        "#         self.n_state = 3\n",
        "#         self.n_ctrl = 1\n",
        "#         if params is None:\n",
        "#             if simple:\n",
        "#                 # gravity (g), mass (m), length (l)\n",
        "#                 self.params = Variable(torch.Tensor((10., 1., 1.)))\n",
        "#             else:\n",
        "#                 # gravity (g), mass (m), length (l), damping (d), gravity bias (b)\n",
        "#                 self.params = Variable(torch.Tensor((10., 1., 1., 0., 0.)))\n",
        "#         else:\n",
        "#             self.params = params\n",
        "#         assert len(self.params) == 3 if simple else 5\n",
        "#         self.goal_state = torch.Tensor([1., 0., 0.])\n",
        "#         self.goal_weights = torch.Tensor([1., 1., 0.1])\n",
        "#         self.ctrl_penalty = 0.001\n",
        "#         # self.lower, self.upper = -2., 2.\n",
        "#         # self.mpc_eps = 1e-3\n",
        "#         # self.linesearch_decay = 0.2\n",
        "#         # self.max_linesearch_iter = 5\n",
        "\n",
        "#     def forward(self, x, u):\n",
        "#         squeeze = x.ndimension() == 1\n",
        "#         if squeeze:\n",
        "#             x = x.unsqueeze(0)\n",
        "#             u = u.unsqueeze(0)\n",
        "#         assert x.ndimension() == 2\n",
        "#         assert x.shape[0] == u.shape[0]\n",
        "#         assert x.shape[1] == 3\n",
        "#         assert u.shape[1] == 1\n",
        "#         assert u.ndimension() == 2\n",
        "#         if x.is_cuda and not self.params.is_cuda:\n",
        "#             self.params = self.params.cuda()\n",
        "#         if not hasattr(self, 'simple') or self.simple:\n",
        "#             g, m, l = torch.unbind(self.params)\n",
        "#         else:\n",
        "#             g, m, l, d, b = torch.unbind(self.params)\n",
        "#         u = torch.clamp(u, -self.max_torque, self.max_torque)[:,0]\n",
        "#         cos_th, sin_th, dth = torch.unbind(x, dim=1)\n",
        "#         th = torch.atan2(sin_th, cos_th)\n",
        "#         if not hasattr(self, 'simple') or self.simple:\n",
        "#             newdth = dth + self.dt*(-3.*g/(2.*l) * (-sin_th) + 3. * u / (m*l**2))\n",
        "#         else:\n",
        "#             sin_th_bias = torch.sin(th + b)\n",
        "#             newdth = dth + self.dt*(-3.*g/(2.*l)*(-sin_th_bias) + 3.*u/(m*l**2) - d*th)\n",
        "#         newth = th + newdth*self.dt\n",
        "#         state = torch.stack((torch.cos(newth), torch.sin(newth), newdth), dim=1)\n",
        "#         if squeeze:\n",
        "#             state = state.squeeze(0)\n",
        "#         return state\n",
        "\n",
        "#     def get_frame(self, x, ax=None):\n",
        "#         x = get_data_maybe(x.view(-1))\n",
        "#         assert len(x) == 3\n",
        "#         l = self.params[2].item()\n",
        "#         cos_th, sin_th, dth = torch.unbind(x)\n",
        "#         th = np.arctan2(sin_th, cos_th)\n",
        "#         x = sin_th*l\n",
        "#         y = cos_th*l\n",
        "#         if ax is None:\n",
        "#             fig, ax = plt.subplots(figsize=(6,6))\n",
        "#         else:\n",
        "#             fig = ax.get_figure()\n",
        "#         ax.plot((0,x), (0, y), color='k')\n",
        "#         ax.set_xlim((-l*1.2, l*1.2))\n",
        "#         ax.set_ylim((-l*1.2, l*1.2))\n",
        "#         return fig, ax\n",
        "\n",
        "#     # def get_true_obj(self): #cost terms for the swingup\n",
        "#     #     q = torch.cat((self.goal_weights, self.ctrl_penalty*torch.ones(self.n_ctrl)))\n",
        "#     #     assert not hasattr(self, 'mpc_lin')\n",
        "#     #     px = -torch.sqrt(self.goal_weights)*self.goal_state #+ self.mpc_lin\n",
        "#     #     p = torch.cat((px, torch.zeros(self.n_ctrl)))\n",
        "#     #     return Variable(q), Variable(p)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "872dRoSSkGe9"
      },
      "source": [
        "### wwwwwww"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODWgA7GsewY7",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title setup\n",
        "# !pip install mpc\n",
        "# https://locuslab.github.io/mpc.pytorch/\n",
        "# https://github.com/locuslab/mpc.pytorch/tree/master/examples\n",
        "# https://colab.research.google.com/github/locuslab/mpc.pytorch/blob/master/examples/Pendulum%20Control.ipynb\n",
        "import torch\n",
        "import numpy as np\n",
        "import numpy.random as npr\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import io\n",
        "import base64\n",
        "import tempfile\n",
        "from IPython.display import HTML\n",
        "from tqdm import tqdm\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title PendulumDx\n",
        "\n",
        "class PendulumDx(nn.Module):\n",
        "    def __init__(self, params=None, simple=True):\n",
        "        super().__init__()\n",
        "        self.n_state = 3\n",
        "        self.n_ctrl = 1\n",
        "\n",
        "    def forward(self, x, u):\n",
        "        dt = 0.05 # 0.025\n",
        "        m, g, l = 1, 10, 1\n",
        "        sin, cos, omega = x.split(1,dim=1)\n",
        "        theta = torch.arctan2(sin, cos)\n",
        "        alpha = (u[0] - m*g*l*torch.sin(theta + torch.pi))/(m*l**2) # angular velocity\n",
        "        theta_n = theta + omega*dt\n",
        "        out =torch.cat((torch.sin(theta_n), torch.cos(theta_n), omega + alpha*dt), dim=1)\n",
        "        return out\n",
        "\n",
        "# # Pendulum = Dynamics(f)\n",
        "# x_goal = torch.tensor([0., 1., 0.])\n",
        "# Q  = torch.diag(torch.tensor([0., 1., 0.1])) # state Running cost\n",
        "# R  = torch.diag(torch.tensor([0.1])) # control cost\n",
        "# QT = torch.diag(torch.tensor([0., 100., 100.])) # state Terminal cost\n"
      ],
      "metadata": {
        "id": "Ol4hMthPQw2u",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title down down\n",
        "# # simpe copied https://locuslab.github.io/mpc.pytorch/\n",
        "# # https://github.com/locuslab/mpc.pytorch/blob/master/examples/Pendulum%20Control.ipynb\n",
        "# # https://github.com/locuslab/mpc.pytorch/blob/master/examples/Cartpole%20Control.ipynb\n",
        "# # params = torch.tensor((10., 1., 1.)) # Gravity, mass, length.\n",
        "# dx = PendulumDx()#params, simple=True)\n",
        "\n",
        "# # n_batch, T, mpc_T = 1, 125, 20 # 16, 100, 20 ; batch size, epochs, ? larger solves\n",
        "# n_batch, T, mpc_T = 1, 125, 20\n",
        "\n",
        "# torch.manual_seed(0)\n",
        "# # xinit = torch.tensor([0., -1., 0.])\n",
        "# xinit = torch.tensor([0., -1., 0.]).unsqueeze(0).repeat(n_batch, 1)\n",
        "\n",
        "\n",
        "# x = xinit # init state\n",
        "# # u_init = None # initial control?\n",
        "# u_init = torch.rand(1,dx.n_ctrl)\n",
        "# u = u_init\n",
        "# goal_weights = torch.Tensor([1., 1., 0.1])\n",
        "# goal_state = torch.Tensor([0., 1. ,0.])\n",
        "# ctrl_penalty = torch.Tensor([0.001])\n",
        "# q = torch.cat([goal_weights, ctrl_penalty]) # [1.0000, 1.0000, 0.1000, 0.0010]\n",
        "# px = -torch.sqrt(goal_weights)*goal_state # [-0., -1., -0.]\n",
        "# p = torch.cat((px, torch.zeros(dx.n_ctrl)))\n",
        "# Q = torch.diag(q).unsqueeze(0).unsqueeze(0).repeat(mpc_T, n_batch, 1, 1) # [mpc_T, n_batch, n_state+n_ctrl, n_state+n_ctrl]\n",
        "# p = p.unsqueeze(0).repeat(mpc_T, n_batch, 1) # [mpc_T, n_batch, n_state+n_ctrl] : [-0., -1., -0.,  0.]\n",
        "\n",
        "\n",
        "# xs = xinit\n",
        "# us = u_init\n",
        "\n",
        "# for t in tqdm(range(T)):\n",
        "#     # x1, u1, cost = MPC(\n",
        "#     nominal_states, nominal_actions, nominal_objs = MPC(\n",
        "#         dx.n_state, dx.n_ctrl, mpc_T, # state dim, action dim,\n",
        "#         u_init=u,\n",
        "#         u_lower=-2., u_upper=2., # +-0.5\n",
        "#         lqr_iter=5, # 50 num LQR iterations to perform\n",
        "#         # verbose=0,\n",
        "#         exit_unconverged=False,\n",
        "#         detach_unconverged=False,\n",
        "#         linesearch_decay=0.2, #dx.linesearch_decay,\n",
        "#         max_linesearch_iter=5,#dx.max_linesearch_iter,\n",
        "#         # grad_method=GradMethods.AUTO_DIFF,\n",
        "#         eps=1e-2,\n",
        "#     )(x, QuadCost(Q, p), dx)\n",
        "\n",
        "#     next_action = nominal_actions[0]\n",
        "#     u = torch.cat((nominal_actions[1:], torch.zeros(1, n_batch, dx.n_ctrl)), dim=0)\n",
        "#     u[-2] = u[-3]\n",
        "#     # print(nominal_objs)\n",
        "#     x = dx(x, next_action)\n",
        "#     us = torch.cat((us,next_action),0)\n",
        "#     xs = torch.cat((xs,x),0)\n",
        "\n",
        "# #Plot theta and action trajectory\n",
        "# import matplotlib.pyplot as plt\n",
        "# theta = torch.arctan2(xs[:, 0], xs[:, 1]).detach()\n",
        "# theta = torch.where(theta < 0, 2*torch.pi+theta, theta)\n",
        "# plt.plot(theta)\n",
        "# plt.plot(us.detach())\n",
        "# plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xFjk1V67gDVY",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title down func\n",
        "# simple copied https://locuslab.github.io/mpc.pytorch/\n",
        "# https://github.com/locuslab/mpc.pytorch/blob/master/examples/Pendulum%20Control.ipynb\n",
        "# https://github.com/locuslab/mpc.pytorch/blob/master/examples/Cartpole%20Control.ipynb\n",
        "\n",
        "dx = PendulumDx()#params, simple=True)\n",
        "\n",
        "torch.manual_seed(0)\n",
        "x = torch.tensor([0., -1., 0.]) # xinit # init state\n",
        "u = torch.rand(1,dx.n_ctrl) # u_init # initial control\n",
        "# goal_weights = torch.Tensor([1., 1., 0.1])\n",
        "goal_weights = torch.Tensor([1., 1., .1])*1\n",
        "goal_state = torch.Tensor([0., 1. ,0.])\n",
        "ctrl_penalty = torch.Tensor([0.001])\n",
        "\n",
        "\n",
        "def locuslab_mpc(x, goal_state, dx, u=None):\n",
        "    # n_batch, T, mpc_T = 1, 125, 20 # 16, 100, 20 ; batch size, epochs, ? larger solves\n",
        "    n_batch, T, mpc_T = 1, 125, 20\n",
        "    q = torch.cat([goal_weights, ctrl_penalty]) # [1.0000, 1.0000, 0.1000, 0.0010]\n",
        "    px = -torch.sqrt(goal_weights)*goal_state # [-0., -1., -0.]\n",
        "    # p = torch.cat((px, torch.zeros(dx.n_ctrl)))\n",
        "    p = torch.cat((px, torch.ones(dx.n_ctrl)*0.))\n",
        "    Q = torch.diag(q).unsqueeze(0).unsqueeze(0).repeat(mpc_T, n_batch, 1, 1) # [mpc_T, n_batch, n_state+n_ctrl, n_state+n_ctrl]\n",
        "    p = p.unsqueeze(0).repeat(mpc_T, n_batch, 1) # [mpc_T, n_batch, n_state+n_ctrl] : [-0., -1., -0.,  0.]\n",
        "    if x.ndim == 1: x = x.unsqueeze(0).repeat(n_batch, 1)\n",
        "    if u==None: u = torch.rand(1,dx.n_ctrl)\n",
        "    xs = x\n",
        "    us = u\n",
        "    # print('u1',u)\n",
        "\n",
        "    for t in tqdm(range(T)):\n",
        "        x1, u1, cost = MPC(\n",
        "            dx.n_state, dx.n_ctrl, mpc_T, # state dim, action dim,\n",
        "            u_init=u,\n",
        "            u_lower=-2., u_upper=2., # +-0.5\n",
        "            lqr_iter=5, # 50 num LQR iterations to perform\n",
        "            # verbose=0,\n",
        "            # exit_unconverged=False,\n",
        "            # detach_unconverged=False,\n",
        "            linesearch_decay=0.2, #dx.linesearch_decay,\n",
        "            max_linesearch_iter=5,#dx.max_linesearch_iter,\n",
        "            # grad_method=GradMethods.AUTO_DIFF,\n",
        "            eps=1e-2,\n",
        "        )(x, QuadCost(Q, p), dx)\n",
        "\n",
        "        next_action = u1[0]\n",
        "        # u = torch.cat((u1[1:], torch.zeros(1, n_batch, dx.n_ctrl)), dim=0)\n",
        "        u = torch.cat((u1[1:], torch.randn(1, n_batch, dx.n_ctrl)), dim=0) # zeros randn\n",
        "        # print('u2',u)\n",
        "        u[-2] = u[-3]\n",
        "        # print(cost)\n",
        "        x = dx(x, next_action)\n",
        "        us = torch.cat((us,next_action),0)\n",
        "        xs = torch.cat((xs,x),0)\n",
        "        # break\n",
        "    return xs, us\n",
        "\n",
        "xs, us = locuslab_mpc(x, goal_state, dx)\n",
        "#Plot theta and action trajectory\n",
        "import matplotlib.pyplot as plt\n",
        "theta = torch.arctan2(xs[:, 0], xs[:, 1]).detach()\n",
        "theta = torch.where(theta < 0, 2*torch.pi+theta, theta)\n",
        "plt.plot(theta)\n",
        "plt.plot(us.detach())\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "uV60myg44rZX",
        "outputId": "b2193e9a-1b3b-4c90-f36d-f2060a12cdb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 22%|██▏       | 28/125 [00:05<00:28,  3.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WARNING] pnqp warning: Did not converge\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [00:23<00:00,  5.22it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def cost(u, x, x_des):\n",
        "    \"\"\"u[i] <- steering angle for step i\n",
        "    x <- (X, Y, theta, velocity) initial state\n",
        "    x_des[i] <- (X, Y, theta, velocity) desired state at step i\"\"\"\n",
        "    cost_val = 0.0\n",
        "    for i in range(horizon):\n",
        "        x = bicycle_model(x, u[i])\n",
        "        cost_val += ((x[0] - x_des[i, 0]) ** 2 + (x[1] - x_des[i, 1]) ** 2 + K * u[i] ** 2)\n",
        "    return cost_val\n",
        "\n",
        "# scipy.optimize.minimize, method=\"SLSQP\"\n",
        "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html\n",
        "\n",
        "# all differentiable\n",
        "\n",
        "\n",
        "\n",
        "# slew_rate_penalty λ: penalizes the slew rate λ/2 *||ut-ut+1||2 ^2\n",
        "\n",
        "\n",
        "def bquad(x, Q):\n",
        "    return x.unsqueeze(1).bmm(Q).bmm(x.unsqueeze(2)).squeeze(1).squeeze(1)\n",
        "\n",
        "def bdot(x, y):\n",
        "    return torch.bmm(x.unsqueeze(1), y.unsqueeze(2)).squeeze(1).squeeze(1)\n",
        "\n",
        "def get_cost(T, u, cost, dynamics=None, x_init=None, x=None):\n",
        "    assert x_init is not None or x is not None\n",
        "    x = get_traj(T, u, x_init, dynamics)\n",
        "    objs = []\n",
        "    for t in range(T):\n",
        "        xt = x[t]\n",
        "        ut = u[t]\n",
        "        xut = torch.cat((xt, ut), 1)\n",
        "        obj = 0.5*bquad(xut, C[t]) + bdot(xut, c[t])\n",
        "        # obj = cost(xut)\n",
        "        objs.append(obj)\n",
        "    total_obj = torch.sum(torch.stack(objs, dim=0), dim=0)\n",
        "    return total_obj\n",
        "\n",
        "\n",
        "# tao = cat(x,u)\n",
        "# C*tao^2 + c*tao\n",
        "\n",
        "x = torch.tensor([0., -1., 0.]) # xinit # init state\n",
        "u = torch.rand(1,dx.n_ctrl) # u_init # initial control\n",
        "# goal_weights = torch.Tensor([1., 1., 0.1])\n",
        "goal_weights = torch.Tensor([1., 1., .01])*1\n",
        "goal_state = torch.Tensor([0., 1. ,0.])\n",
        "ctrl_penalty = torch.Tensor([0.001])\n",
        "\n",
        "\n",
        "def locuslab_mpc(x, goal_state, dx, u=None):\n",
        "    # n_batch, T, mpc_T = 1, 125, 20 # 16, 100, 20 ; batch size, epochs, ? larger solves\n",
        "    n_batch, T, mpc_T = 1, 125, 20\n",
        "    q = torch.cat([goal_weights, ctrl_penalty]) # [1.0000, 1.0000, 0.1000, 0.0010]\n",
        "    Q = torch.diag(q).unsqueeze(0).unsqueeze(0).repeat(mpc_T, n_batch, 1, 1) # [mpc_T, n_batch, n_state+n_ctrl, n_state+n_ctrl]\n",
        "    px = -torch.sqrt(goal_weights)*goal_state # [-0., -1., -0.]\n",
        "    p = torch.cat((px, torch.zeros(dx.n_ctrl)))\n",
        "    p = p.unsqueeze(0).repeat(mpc_T, n_batch, 1) # [mpc_T, n_batch, n_state+n_ctrl] : [-0., -1., -0.,  0.]\n",
        "\n",
        "# QuadCost(Q, p)\n",
        "\n",
        "# box-constrained control problem with a quadratic cost (defined by C and c) and non-linear dynamics (defined by f):\n",
        "#         min_{tau={x,u}} sum_t 0.5 tau_t^T C_t tau_t + c_t^T tau_t\n",
        "#                     s.t. x_{t+1} = f(x_t, u_t)\n",
        "#             x_0 = x_init   ;   u_lower <= u <= u_upper\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# RL like Q func + terminal state\n",
        "# Sum Q(xt,ut) + cost(xT)\n",
        "\n",
        "# tao = cat((x_t - xtarget_t) , u)\n",
        "# quadratic cost func: taoT *Q *tao + p * tao\n",
        "\n",
        "# terminal cost\n",
        "\n",
        "# dut = ut - ut-1\n",
        "# slew: dutT * S * dut\n",
        "\n",
        "# Obstacle avoidance: l_obs = exp(-||xt - x_obs||^2 /σ)\n",
        "\n",
        "# Energy cost: K *tao^2\n",
        "\n",
        "\n",
        "goal_weights = torch.Tensor([1., 1., .01])*1\n",
        "goal_state = torch.Tensor([0., 1. ,0.])\n",
        "ctrl_penalty = torch.Tensor([0.001])\n",
        "def control_cost(lx, lu, goal_state, goal_weights=None, ctrl_penalty=None):\n",
        "    if goal_weights is None: goal_weights = torch.ones(dx.n_state)\n",
        "    if ctrl_penalty is None: ctrl_penalty = torch.ones(dx.n_ctrl)\n",
        "    T = lx.shape[-2]\n",
        "    Q = torch.diag(torch.cat([goal_weights, ctrl_penalty], dim=-1))\n",
        "    R = torch.cat([torch.ones(dx.n_state), torch.ones(dx.n_ctrl)], dim=-1)\n",
        "    P = 10. * torch.diag(torch.ones(dx.n_state))\n",
        "\n",
        "    x_diff = lx - goal_state.unsqueeze(0)\n",
        "    # print(x_diff.shape, lu.shape)\n",
        "    tao = torch.cat((x_diff, lu), dim=-1)#.unsqueeze(0) # [batch, T, n_state+n_ctrl]\n",
        "    # if lu.ndim == 2: lu = lu.unsqueeze(0)\n",
        "    quad = torch.einsum('btd,dd,btd->b', tao, Q, tao).mean() #+ torch.einsum('btd,d->b', tao, R) # quad = tao @ Q @ tao.T# + R @ tao\n",
        "\n",
        "    # terminal_diff = lx[:,-1] - goal_state  # Terminal state\n",
        "    # terminal_cost = torch.einsum('bd,dd,bd->b', terminal_diff, P, terminal_diff)\n",
        "\n",
        "\n",
        "    du = lu[:,1:]-lu[:,:-1] # [batch, T-1, n_ctrl]\n",
        "    slew = 0.01 * torch.einsum('btu,btu->b', du, du).mean() # slew = 0.01 * du.T @ du\n",
        "    cost = quad + slew# + terminal_cost\n",
        "    return cost\n",
        "b=4\n",
        "T=10\n",
        "# lx = torch.rand(T, dx.n_state)\n",
        "# lu = torch.rand(T, dx.n_ctrl)\n",
        "lx = torch.rand(b,T, dx.n_state)\n",
        "lu = torch.rand(b,T, dx.n_ctrl)\n",
        "\n",
        "\n",
        "cost = control_cost(lx, lu, goal_state, goal_weights, ctrl_penalty)\n",
        "print(cost)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eJjmGLjPh9n",
        "outputId": "cbad739c-d8a3-4201-e461-10974cabbaa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([10.4950, 14.1678, 19.8147, 17.0197])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# torch.backends.cuda.matmul.allow_tf32 = True\n",
        "# torch.backends.cudnn.allow_tf32 = True\n",
        "# scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "\n",
        "dx = PendulumDx()#params, simple=True)\n",
        "\n",
        "torch.manual_seed(0)\n",
        "x = torch.tensor([0., -1., 0.]) # xinit # init state\n",
        "u = torch.rand(1,dx.n_ctrl) # u_init # initial control\n",
        "goal_weights = torch.Tensor([1., 1., 0.1])\n",
        "# goal_weights = torch.Tensor([1., 1., 1.])*1\n",
        "goal_state = torch.Tensor([0., 1. ,0.])\n",
        "ctrl_penalty = torch.Tensor([0.001])\n",
        "\n",
        "\n",
        "def locuslab_mpc(x, goal_state, dx, u=None):\n",
        "    # n_batch, T, mpc_T = 1, 125, 20 # 16, 100, 20 ; batch size, epochs, ? larger solves\n",
        "    n_batch, T, mpc_T = 1, 125, 20\n",
        "    q = torch.cat([goal_weights, ctrl_penalty]) # [1.0000, 1.0000, 0.1000, 0.0010]\n",
        "    px = -torch.sqrt(goal_weights)*goal_state # [-0., -1., -0.]\n",
        "    # p = torch.cat((px, torch.zeros(dx.n_ctrl)))\n",
        "    p = torch.cat((px, torch.ones(dx.n_ctrl)*0.))\n",
        "    Q = torch.diag(q).unsqueeze(0).unsqueeze(0).repeat(mpc_T, n_batch, 1, 1) # [mpc_T, n_batch, n_state+n_ctrl, n_state+n_ctrl]\n",
        "    p = p.unsqueeze(0).repeat(mpc_T, n_batch, 1) # [mpc_T, n_batch, n_state+n_ctrl] : [-0., -1., -0.,  0.]\n",
        "    if x.ndim == 1: x = x.unsqueeze(0).repeat(n_batch, 1)\n",
        "    if u==None: u = torch.rand(1,dx.n_ctrl)\n",
        "    xs = x\n",
        "    us = u\n",
        "    # print('u1',u)\n",
        "\n",
        "    for t in tqdm(range(T)):\n",
        "        # x1, u1, cost = MPC(\n",
        "        #     dx.n_state, dx.n_ctrl, mpc_T, # state dim, action dim,\n",
        "        #     u_init=u,\n",
        "        #     u_lower=-2., u_upper=2., # +-0.5\n",
        "        #     lqr_iter=5, # 50 num LQR iterations to perform\n",
        "        # )(x, QuadCost(Q, p), dx)\n",
        "        next_action = u1[0]\n",
        "        u = torch.cat((u1[1:], torch.randn(1, n_batch, dx.n_ctrl)), dim=0) # zeros randn\n",
        "        x = dx(x, next_action)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "x = torch.tensor([0., -1., 0.])\n",
        "\n",
        "def rnn_it(x,u,dx):\n",
        "    lx = x.unsqueeze(1) # [batch,1,n_state]\n",
        "    batch, n_state = x.shape\n",
        "    lx = torch.empty((batch,0,n_state),device=device)\n",
        "    T = u.shape[1]\n",
        "    for t in range(T):\n",
        "        x = dx(x, u[:,t])\n",
        "        lx = torch.cat([lx, x.unsqueeze(1)], dim=1) # [batch,T,n_state]\n",
        "    return lx\n",
        "\n",
        "\n",
        "batch = 1\n",
        "T=128\n",
        "x = torch.tensor([0., -1., 0.]).unsqueeze(0).repeat(batch, 1)\n",
        "# lu = nn.Parameter(torch.rand((batch, T, dx.n_ctrl),device=device))\n",
        "\n",
        "def MPCoptim(x,lu,dx):\n",
        "    lu = nn.Parameter(torch.rand((batch, T, dx.n_ctrl),device=device))\n",
        "    optim = torch.optim.AdamW([lu], 1e-1, (0.9, 0.999)) # lr = 1e-4 #3e-4\n",
        "    optim = torch.optim.SGD([lu], 1e-1)\n",
        "\n",
        "    for _ in range(5):\n",
        "        lx = rnn_it(x,lu,dx)\n",
        "        # print(lx.shape, lu.shape)\n",
        "        cost = control_cost(lx, lu, goal_state, goal_weights, ctrl_penalty)\n",
        "        print(cost)\n",
        "        # lu = torch.clamp(lu, min=-2, max=2)\n",
        "        with torch.no_grad():\n",
        "            lu.clamp_(min=-2, max=2)\n",
        "        cost.backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "    return lx, lu, cost\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for t in tqdm(range(T)):\n",
        "    # x1, u1, cost = MPCoptim(x,lu,dx\n",
        "    #     dx.n_state, dx.n_ctrl, mpc_T, # state dim, action dim,\n",
        "    #     u_init=u,\n",
        "    #     u_lower=-2., u_upper=2., # +-0.5\n",
        "    #     lqr_iter=5, # 50 num LQR iterations to perform\n",
        "    # )(x, QuadCost(Q, p), dx)\n",
        "    next_action = u1[0]\n",
        "    u = torch.cat((u1[1:], torch.randn(1, n_batch, dx.n_ctrl)), dim=0) # zeros randn\n",
        "    x = dx(x, next_action)\n",
        "\n",
        "\n",
        "xs, us = lx[0], lu[0]\n",
        "# xs, us = locuslab_mpc(x, goal_state, dx)\n",
        "#Plot theta and action trajectory\n",
        "import matplotlib.pyplot as plt\n",
        "theta = torch.arctan2(xs[:, 0], xs[:, 1]).detach()\n",
        "theta = torch.where(theta < 0, 2*torch.pi+theta, theta)\n",
        "plt.plot(theta)\n",
        "plt.plot(us.detach())\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "684CBzdQ1DzA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5c46d10d-dce3-44ed-ebf6-8e416701dae2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([554.7758], grad_fn=<AddBackward0>)\n",
            "tensor([553.8606], grad_fn=<AddBackward0>)\n",
            "tensor([553.7541], grad_fn=<AddBackward0>)\n",
            "tensor([551.2542], grad_fn=<AddBackward0>)\n",
            "tensor([550.8805], grad_fn=<AddBackward0>)\n",
            "tensor([550.4525], grad_fn=<AddBackward0>)\n",
            "tensor([548.2002], grad_fn=<AddBackward0>)\n",
            "tensor([545.4667], grad_fn=<AddBackward0>)\n",
            "tensor([544.0389], grad_fn=<AddBackward0>)\n",
            "tensor([543.4832], grad_fn=<AddBackward0>)\n",
            "tensor([541.1202], grad_fn=<AddBackward0>)\n",
            "tensor([537.3198], grad_fn=<AddBackward0>)\n",
            "tensor([533.8461], grad_fn=<AddBackward0>)\n",
            "tensor([530.7807], grad_fn=<AddBackward0>)\n",
            "tensor([527.4944], grad_fn=<AddBackward0>)\n",
            "tensor([524.7640], grad_fn=<AddBackward0>)\n",
            "tensor([522.7231], grad_fn=<AddBackward0>)\n",
            "tensor([519.7449], grad_fn=<AddBackward0>)\n",
            "tensor([516.3202], grad_fn=<AddBackward0>)\n",
            "tensor([513.3864], grad_fn=<AddBackward0>)\n",
            "tensor([510.6393], grad_fn=<AddBackward0>)\n",
            "tensor([507.9872], grad_fn=<AddBackward0>)\n",
            "tensor([505.8650], grad_fn=<AddBackward0>)\n",
            "tensor([503.5495], grad_fn=<AddBackward0>)\n",
            "tensor([500.7223], grad_fn=<AddBackward0>)\n",
            "tensor([498.1424], grad_fn=<AddBackward0>)\n",
            "tensor([495.7885], grad_fn=<AddBackward0>)\n",
            "tensor([493.5140], grad_fn=<AddBackward0>)\n",
            "tensor([491.6018], grad_fn=<AddBackward0>)\n",
            "tensor([489.2494], grad_fn=<AddBackward0>)\n",
            "tensor([486.9386], grad_fn=<AddBackward0>)\n",
            "tensor([484.8215], grad_fn=<AddBackward0>)\n",
            "tensor([482.9267], grad_fn=<AddBackward0>)\n",
            "tensor([480.8376], grad_fn=<AddBackward0>)\n",
            "tensor([478.8455], grad_fn=<AddBackward0>)\n",
            "tensor([476.9198], grad_fn=<AddBackward0>)\n",
            "tensor([475.0071], grad_fn=<AddBackward0>)\n",
            "tensor([473.1561], grad_fn=<AddBackward0>)\n",
            "tensor([471.8118], grad_fn=<AddBackward0>)\n",
            "tensor([471.0620], grad_fn=<AddBackward0>)\n",
            "tensor([472.9173], grad_fn=<AddBackward0>)\n",
            "tensor([469.8412], grad_fn=<AddBackward0>)\n",
            "tensor([465.6049], grad_fn=<AddBackward0>)\n",
            "tensor([467.9144], grad_fn=<AddBackward0>)\n",
            "tensor([466.0637], grad_fn=<AddBackward0>)\n",
            "tensor([462.5827], grad_fn=<AddBackward0>)\n",
            "tensor([464.7662], grad_fn=<AddBackward0>)\n",
            "tensor([462.9919], grad_fn=<AddBackward0>)\n",
            "tensor([459.9971], grad_fn=<AddBackward0>)\n",
            "tensor([461.8522], grad_fn=<AddBackward0>)\n",
            "tensor([460.8660], grad_fn=<AddBackward0>)\n",
            "tensor([457.7719], grad_fn=<AddBackward0>)\n",
            "tensor([458.7265], grad_fn=<AddBackward0>)\n",
            "tensor([459.3647], grad_fn=<AddBackward0>)\n",
            "tensor([456.5734], grad_fn=<AddBackward0>)\n",
            "tensor([455.3729], grad_fn=<AddBackward0>)\n",
            "tensor([456.8495], grad_fn=<AddBackward0>)\n",
            "tensor([456.6830], grad_fn=<AddBackward0>)\n",
            "tensor([454.0880], grad_fn=<AddBackward0>)\n",
            "tensor([452.8625], grad_fn=<AddBackward0>)\n",
            "tensor([453.8196], grad_fn=<AddBackward0>)\n",
            "tensor([454.7407], grad_fn=<AddBackward0>)\n",
            "tensor([453.8223], grad_fn=<AddBackward0>)\n",
            "tensor([451.5881], grad_fn=<AddBackward0>)\n",
            "tensor([450.3065], grad_fn=<AddBackward0>)\n",
            "tensor([450.5265], grad_fn=<AddBackward0>)\n",
            "tensor([451.5370], grad_fn=<AddBackward0>)\n",
            "tensor([452.7164], grad_fn=<AddBackward0>)\n",
            "tensor([452.9167], grad_fn=<AddBackward0>)\n",
            "tensor([450.7277], grad_fn=<AddBackward0>)\n",
            "tensor([448.1085], grad_fn=<AddBackward0>)\n",
            "tensor([447.6521], grad_fn=<AddBackward0>)\n",
            "tensor([449.0044], grad_fn=<AddBackward0>)\n",
            "tensor([450.6700], grad_fn=<AddBackward0>)\n",
            "tensor([450.5814], grad_fn=<AddBackward0>)\n",
            "tensor([448.2541], grad_fn=<AddBackward0>)\n",
            "tensor([445.9063], grad_fn=<AddBackward0>)\n",
            "tensor([445.6880], grad_fn=<AddBackward0>)\n",
            "tensor([447.1825], grad_fn=<AddBackward0>)\n",
            "tensor([448.7436], grad_fn=<AddBackward0>)\n",
            "tensor([448.9701], grad_fn=<AddBackward0>)\n",
            "tensor([446.7808], grad_fn=<AddBackward0>)\n",
            "tensor([444.2596], grad_fn=<AddBackward0>)\n",
            "tensor([443.7308], grad_fn=<AddBackward0>)\n",
            "tensor([445.0086], grad_fn=<AddBackward0>)\n",
            "tensor([446.9071], grad_fn=<AddBackward0>)\n",
            "tensor([447.5636], grad_fn=<AddBackward0>)\n",
            "tensor([446.0859], grad_fn=<AddBackward0>)\n",
            "tensor([443.3560], grad_fn=<AddBackward0>)\n",
            "tensor([441.9697], grad_fn=<AddBackward0>)\n",
            "tensor([442.7172], grad_fn=<AddBackward0>)\n",
            "tensor([444.4866], grad_fn=<AddBackward0>)\n",
            "tensor([446.1604], grad_fn=<AddBackward0>)\n",
            "tensor([445.8499], grad_fn=<AddBackward0>)\n",
            "tensor([443.5130], grad_fn=<AddBackward0>)\n",
            "tensor([441.0190], grad_fn=<AddBackward0>)\n",
            "tensor([440.4485], grad_fn=<AddBackward0>)\n",
            "tensor([441.6948], grad_fn=<AddBackward0>)\n",
            "tensor([443.5094], grad_fn=<AddBackward0>)\n",
            "tensor([444.8791], grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title RNN\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# self.pred = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=1, batch_first=True, dropout=0.3)\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, num_layers=1, batch_first=True, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.fcin = nn.Linear(in_dim, d_model, bias=False)\n",
        "        self.normin = nn.RMSNorm(d_model)\n",
        "        self.bn = nn.BatchNorm1d(d_model)\n",
        "        # self.normout = nn.RMSNorm(d_model)\n",
        "        self.act = nn.GELU() # GELU SiLU\n",
        "        self.num_layers = num_layers\n",
        "        # self.rnn = nn.GRU(d_model, d_model, num_layers=num_layers, batch_first=True, dropout=0.3)\n",
        "        self.rnn = nn.GRU(in_dim, d_model, num_layers=num_layers, batch_first=True, dropout=0.3)\n",
        "        # self.drop = nn.Dropout(.2)\n",
        "        # self.fc = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.fc = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x, h0=None,c0=None): # [batch_size, seq_len, in_dim]\n",
        "        # x = self.fcin(x)\n",
        "        # # # x = self.drop(x)\n",
        "        # # # x = self.normin(x)\n",
        "        # x = self.act(x)\n",
        "        # h0 = self.normin(h0)\n",
        "        out, h0 = self.rnn(x, h0) # [batch_size, seq_len, d_model], [num_layers, batch_size, d_model]\n",
        "        # out, _ = self.lstm(x, (h0,c0))\n",
        "        # out = out[:, -1, :] # [batch_size, d_model]\n",
        "        # out = self.drop(out)\n",
        "        out = self.fc(out) # [batch_size, seq_len, out_dim]\n",
        "        # out = self.normout(out)\n",
        "        # out = self.act(out)\n",
        "        return out, h0\n",
        "\n",
        "# model = RNN(input_size, hidden_size).to(device)\n",
        "# print(model)\n",
        "# out, h0 = self.gru(sxaz, self.h0.detach()) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "\n"
      ],
      "metadata": {
        "id": "Jlq_AJDDzWdO",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def train_jepa(model, dataloader, optim, bptt=25): #32\n",
        "    model.train()\n",
        "    for batch, (state, action, reward) in enumerate(dataloader): # collate: [seq_len, batch_length], default: [batch_size, seq_len]\n",
        "        h0 = torch.zeros((self.jepa.pred.num_layers, batch_size, self.d_model), device=device) # [num_layers, batch, d_model]\n",
        "        # sy_ = self.jepa.enc(torch.zeros((batch_size, 3,64,64), device=device)).unsqueeze(1) # [batch_size, 1, d_model]\n",
        "        sy_ = self.jepa.enc(torch.zeros((batch_size, *self.in_dim), device=device)).unsqueeze(1) # [batch_size, 1, d_model]\n",
        "        state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "        for st, act, rwd in zip(torch.split(state, bptt, dim=1), torch.split(action, bptt, dim=1), torch.split(reward, bptt, dim=1)):\n",
        "            with torch.cuda.amp.autocast():\n",
        "                # print(st.shape, st.dtype, type(st))\n",
        "                lsy = self.jepa.enc(st.flatten(end_dim=1)).unflatten(0, (batch_size, -1)) # [batch_size, bptt, d_model]\n",
        "                # lsy = self.jepa.enc(st) # [batch_size, bptt, d_model]\n",
        "                # la = self.emb(act) # [batch_size, bptt, dim_a]\n",
        "                la = act # [batch_size, bptt, dim_a]\n",
        "                lz = self.argm(lsy, sy_.squeeze(1), h0, la, rwd) # [batch_size, bptt, d_model],\n",
        "                # with torch.no_grad(): lz.mul_(torch.rand_like(lz).uniform_(0.5)).mul_((torch.rand_like(lz)>0.1).bool()) # dropout without scaling\n",
        "                with torch.no_grad(): lz.mul_(torch.rand_like(lz).uniform_(0)).mul_((torch.rand_like(lz)>0.5).bool()) # dropout without scaling\n",
        "                lsy_, lh0 = self.rnn_it(sy_.squeeze(1), la, lz, h0)\n",
        "                repr_loss = F.mse_loss(lsy, lsy_) # [batch_size, bptt, d_model]\n",
        "\n",
        "                # std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(lsy.flatten(end_dim=1)))\n",
        "                # jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "                syh0 = torch.cat([lsy, lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch_size,bptt,d_model], [bptt,num_layers,batch_size,d_model] -> [batch_size*bptt, (1+num_layers)*d_model] # not lsy_, else unstable\n",
        "                clossl = self.tcost.loss(syh0, rwd.flatten())\n",
        "                closs = self.closs_coeff * clossl\n",
        "                # loss = jloss + closs\n",
        "\n",
        "                st_ = self.jepa.dec(lsy)\n",
        "                ae_loss = F.mse_loss(st, st_)\n",
        "                loss = self.jepa.sim_coeff * repr_loss + ae_loss + closs\n",
        "\n",
        "            # torch.norm(lsy-torch.cat([sy_,lsy[:-1]], dim=1), dim=-1) # -(z*torch.log(z)).sum(-1) # Shannon entropy archive.is/CaYrq\n",
        "            # prob = F.softmax(output, dim=-1)\n",
        "            # entropy = -torch.sum(prob * torch.log(prob + 1e-5), dim=-1)\n",
        "\n",
        "            # print(\"norm\", torch.norm(sy, dim=-1)[0].item())\n",
        "            norm = torch.norm(lsy[0][0], dim=-1).item()\n",
        "            z_norm = torch.norm(lz[0][-1], dim=-1)\n",
        "            # print(\"clossl, wrong\", clossl.item(), mask.sum())\n",
        "            scaler.scale(loss).backward()\n",
        "            # torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "            scaler.step(optim)\n",
        "            scaler.update()\n",
        "            optim.zero_grad()\n",
        "            sy_, h0 = sy_.detach(), h0.detach()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1Ccrpnk6xK9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(nominal_actions)\n"
      ],
      "metadata": {
        "id": "aAdHlG2N7YtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"1\",u_init)\n",
        "u_init = torch.cat((nominal_actions[1:], torch.zeros(1, n_batch, dx.n_ctrl)), dim=0)\n",
        "print(\"2\",u_init)\n",
        "u_init[-2] = u_init[-3]\n",
        "print(\"3\",u_init)\n"
      ],
      "metadata": {
        "id": "GYFgBt_C6_Y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title spiral\n",
        "import torch\n",
        "\n",
        "\n",
        "n_x = 2\n",
        "n_u = 2\n",
        "dt = 0.025\n",
        "# x_goal = torch.tensor([0., 0.1])\n",
        "# # Q  = torch.diag(torch.tensor([1., 0.1],device=device)) # state Running cost\n",
        "# Q  = torch.diag(torch.tensor([1., 1.],device=device)) # state Running cost\n",
        "# R  = torch.diag(torch.tensor([0.1, 0.1],device=device)) # control cost\n",
        "# QT = torch.diag(torch.tensor([100., 100.],device=device)) # state Terminal cost\n",
        "# # cons = Bounded(u, high = [2], low = [-2])\n",
        "# # SwingUpCost = Cost.QR(Q, R, QT, x_goal, cons)\n",
        "# # controller = iLQR(Pendulum, SwingUpCost)\n",
        "\n",
        "# #initial state #theta = pi --> sin(theta) = 0, cos(theta) = -1\n",
        "# # x0 = np.array([0, -1, 0])\n",
        "# x0 = torch.tensor([0, -10])\n",
        "\n",
        "\n",
        "# n_batch, T, mpc_T = 1, 125, 20 # 16, 100, 20 ; batch size, epochs, ? larger solves\n",
        "n_batch, T, mpc_T = 2, 125, 20\n",
        "\n",
        "torch.manual_seed(0)\n",
        "xinit = torch.tensor([0., -10]).unsqueeze(0).repeat(n_batch, 1)\n",
        "\n",
        "x = xinit # init state\n",
        "# u_init = None # initial control?\n",
        "u_init = torch.rand(1,n_u)\n",
        "\n",
        "goal_weights = torch.Tensor([1., 1.])\n",
        "x_goal = torch.Tensor([0., 0.1])\n",
        "ctrl_penalty = 0.001 #0.001\n",
        "q = torch.cat((goal_weights, ctrl_penalty*torch.ones(n_u))) # [1.0000, 1.0000, 0.1000, 0.0010]\n",
        "px = -torch.sqrt(goal_weights)*x_goal # [-0., -1., -0.]\n",
        "p = torch.cat((px, torch.zeros(n_u)))\n",
        "Q = torch.diag(q).unsqueeze(0).unsqueeze(0).repeat(mpc_T, n_batch, 1, 1) # [mpc_T, n_batch, n_x+n_u, n_x+n_u]\n",
        "p = p.unsqueeze(0).repeat(mpc_T, n_batch, 1) # [mpc_T, n_batch, n_x+n_u] : [-0., -1., -0.,  0.]\n",
        "\n",
        "xs = xinit\n",
        "us = u_init\n",
        "\n",
        "# print(xs)\n",
        "\n",
        "\n",
        "\n",
        "class ff(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.n_x = 2\n",
        "        self.n_u = 2\n",
        "\n",
        "    def forward(self, x, u):\n",
        "        # print(x.shape, u.shape)\n",
        "        x1,x2=x.split([1,1],dim=-1)\n",
        "        dx=0.1\n",
        "        xdot = torch.cat((x2*dx,-x1*dx),dim=-1)\n",
        "        uv = x+xdot + u\n",
        "        # print(uv.shape)\n",
        "        return uv\n",
        "        # return xdot + u # for plotting, is just xdot, not x+xdot\n",
        "\n",
        "f=ff()\n",
        "\n",
        "\n",
        "for t in tqdm(range(T)):\n",
        "    nominal_states, nominal_actions, nominal_objs = MPC(\n",
        "        n_x, n_u, mpc_T, # state dim, action dim,\n",
        "        u_init=u_init,\n",
        "        # u_lower=-2., u_upper=2., # +-0.5\n",
        "        # u_lower=-0.5, u_upper=0.5,\n",
        "        u_lower=-0.1, u_upper=0.1,\n",
        "        lqr_iter=50, # 50 num LQR iterations to perform\n",
        "        verbose=0,\n",
        "        exit_unconverged=False,\n",
        "        detach_unconverged=False,\n",
        "        linesearch_decay=0.2, #dx.linesearch_decay,\n",
        "        max_linesearch_iter=5,#dx.max_linesearch_iter,\n",
        "        grad_method=GradMethods.AUTO_DIFF,\n",
        "        eps=1e-2,\n",
        "    )(x, QuadCost(Q, p), f)\n",
        "\n",
        "    next_action = nominal_actions[0]\n",
        "    u_init = torch.cat((nominal_actions[1:], torch.zeros(1, n_batch, n_u)), dim=0)\n",
        "    u_init[-2] = u_init[-3]\n",
        "    x = f(x, next_action)\n",
        "    # print(x.shape,u_init.shape)\n",
        "    # xs.append(x)\n",
        "    us = torch.cat((us,next_action),0)\n",
        "    xs = torch.cat((xs,x),0)\n",
        "# print(torch.tensor(xs).shape)\n",
        "# print(xs.shape)\n",
        "# print(u_init.shape)\n",
        "\n",
        "xs=xs.detach()\n",
        "us=us.detach()\n",
        "X, Y = xs[:-1,0], xs[:-1,1]\n",
        "u, v = xs[1:,0]-X, xs[1:,1]-Y\n",
        "import matplotlib.pyplot as plt\n",
        "plt.quiver(X, Y, u, v, angles='xy', scale_units='xy', scale=1)\n",
        "plt.quiver(X, Y, us[:,0], us[:,1], angles='xy', scale_units='xy', scale=1,color='red')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "n3lEckRNEpT3",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYOJaMvCczrj",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title pytorch dx\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "# again\n",
        "# 24\n",
        "n_state = 3\n",
        "n_ctrl = 2#3\n",
        "torch.manual_seed(1)\n",
        "class model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.param = nn.Parameter(torch.rand(1,2), requires_grad=True)\n",
        "        self.lst=[7,10,7]\n",
        "        self.lin = nn.Sequential(\n",
        "            # nn.Linear(n_ctrl + n_state, self.lst[0]), nn.ReLU(),\n",
        "            nn.Linear(n_state, self.lst[0]), nn.ReLU(), # ReLU Sigmoid Tanh\n",
        "            # nn.Linear(self.lst[0], self.lst[1]), nn.Tanh(),\n",
        "            nn.Linear(self.lst[0], self.lst[-1]), nn.ReLU(), #nn.ReLU(),\n",
        "            nn.Linear(self.lst[-1], n_state)\n",
        "        )\n",
        "        self.n_state = n_state\n",
        "        self.n_ctrl = n_ctrl\n",
        "    def forward(self, x, u=None): # state, control\n",
        "        if u==None: u=self.param\n",
        "        # else:\n",
        "        #     with torch.no_grad():\n",
        "        #         self.param=nn.Parameter(u)\n",
        "        # print(\"xu\",x.shape, u.shape) # [1, 2] [1, n_ctrl]\n",
        "        # sx = torch.cat([u, x], dim=-1)\n",
        "        # x1 = self.lin(sx)\n",
        "        # x1=x+0.9*x1\n",
        "        print(\"pred fwd x.requires_grad: \",x.requires_grad,not x.requires_grad)\n",
        "        x1 = self.lin(x)\n",
        "        u1 = torch.cat([u, torch.zeros(1,self.n_state-self.n_ctrl)], dim=-1)\n",
        "        x1=x+0.1*x1+0.9*u1\n",
        "        return x1\n",
        "\n",
        "dx=model()\n",
        "input_shape=n_state\n",
        "# dx = StableDynamicsModel(input_shape, control_size=n_ctrl, alpha=0.9, layer_sizes=[64, 64],\n",
        "#             lr=3e-4, lyapunov_lr=3e-4, lyapunov_eps=1e-3)\n",
        "\n",
        "n_batch, T, mpc_T = 1, 30, 20 # 16, 100, 20 ; batch size, epochs, ? larger solves\n",
        "xinit = torch.tensor([[-0.20]*n_state]) # assert x_init.ndimension() == 2 and x_init.size(0) == n_batch\n",
        "# xinit = torch.tensor([[-0.20]*n_state],requires_grad=True)\n",
        "x = xinit # init state\n",
        "u_init = None # initial control?\n",
        "print(\"x.requires_grad: \",x.requires_grad)\n",
        "\n",
        "# goal_weights = torch.Tensor((0.9, 0.8, 0.7)) # 1., 1., 0.1\n",
        "goal_weights = torch.Tensor([1.0]*n_state)\n",
        "# goal_weights = torch.linspace(1, 0.1, n_state)\n",
        "\n",
        "# goal_state = torch.Tensor((1., 0. ,0.))\n",
        "# goal_state = torch.Tensor((-0.5, 0.1))\n",
        "goal_state = torch.Tensor([-0.50]*n_state)\n",
        "\n",
        "ctrl_penalty = 0.001\n",
        "q = torch.cat((goal_weights, ctrl_penalty*torch.ones(n_ctrl))) # goal_weights.shape + n_ctrl.shape\n",
        "Q = torch.diag(q).unsqueeze(0).unsqueeze(0).repeat(mpc_T, n_batch, 1, 1) # [50, 1, 5, 5]\n",
        "px = -torch.sqrt(goal_weights)*goal_state\n",
        "# px = -goal_weights*goal_state\n",
        "# px = -goal_state\n",
        "p = torch.cat((px, torch.zeros(n_ctrl)))\n",
        "p = p.unsqueeze(0).repeat(mpc_T, n_batch, 1)\n",
        "\n",
        "\n",
        "for t in range(T):\n",
        "    nominal_states, nominal_actions, nominal_objs = MPC(\n",
        "        n_state, n_ctrl, mpc_T, # state dim, action dim,\n",
        "        u_init=u_init,\n",
        "        u_lower=-1., u_upper=1., # -1,1\n",
        "        # u_lower=-0.8, u_upper=0.8, #\n",
        "        lqr_iter=50, # 50 num LQR iterations to perform\n",
        "        verbose=0, #0\n",
        "        exit_unconverged=False,\n",
        "        detach_unconverged=False,\n",
        "        linesearch_decay=0.2, #0.2 dx.linesearch_decay,\n",
        "        max_linesearch_iter=5,#5 dx.max_linesearch_iter,\n",
        "        grad_method=GradMethods.AUTO_DIFF,\n",
        "        eps=1e-2,\n",
        "    )(x, QuadCost(Q, p), dx)\n",
        "\n",
        "    next_action = nominal_actions[0]\n",
        "    u_init = torch.cat((nominal_actions[1:], torch.zeros(1, n_batch, n_ctrl)), dim=0)\n",
        "    # print('u_init',u_init.shape) #[100, 1, 1]\n",
        "    # u_init[-2] = u_init[-3]\n",
        "    print(t, x.detach().numpy(), next_action.detach().numpy())\n",
        "    print(x.requires_grad)\n",
        "    x = dx(x, next_action)\n",
        "    # x.requires_grad=True\n",
        "    error = nn.MSELoss()(x,goal_state).item()\n",
        "    print(\"error\",error)\n",
        "    if error <1e-10: break\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "H=torch.rand(1,2,2)\n",
        "H_lu = torch.lu(H) # (1,2,2) , (1,2)\n",
        "q=torch.rand(1,2)\n",
        "# print(\"H: \",H)\n",
        "# print(\"H_lu: \",H_lu)\n",
        "# print(\"q: \",q)\n",
        "\n",
        "x_init = -q.unsqueeze(2).lu_solve(*H_lu).squeeze(2) # Clamped in the x assignment.\n",
        "\n",
        "# x_init = -q.unsqueeze(2) # (1,2,1)\n",
        "# # print(\"x_init: \",x_init)\n",
        "# x_init = x_init.lu_solve(*H_lu) # (1,2,1)\n",
        "# # print(\"x_init: \",x_init)\n",
        "# x_init = x_init.squeeze(2)\n",
        "# print(\"x_init: \",x_init) # (1,2)\n",
        "# print(\"x_init s: \",x_init.shape)\n",
        "\n",
        "H_lu = torch.lu(H.squeeze(0))\n",
        "x_init = torch.linalg.lu_solve(*H_lu, -q.unsqueeze(2)).squeeze(2)\n",
        "# print(\"x_init: \",x_init)\n",
        "# print(\"x_init s: \",x_init.shape)\n",
        "\n",
        "# A = torch.randn(3, 3)\n",
        "# LU, pivots = torch.linalg.lu_factor(A) #(3,3) , 3\n",
        "# B = torch.randn(3, 2)\n",
        "# print(\"LU: \",LU)\n",
        "# print(\"pivots: \",pivots)\n",
        "# X = torch.linalg.lu_solve(LU, pivots, B)  # (3,2)\n",
        "# print(\"X: \",X)\n"
      ],
      "metadata": {
        "id": "jJ99M6mirRT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def fd(x, u=None):\n",
        "    xu = x.squeeze()\n",
        "    if not xu.requires_grad:\n",
        "        xu.requires_grad = True\n",
        "    f = nn.Linear(3,3)(xu.squeeze()).relu()\n",
        "    xu = xu.unsqueeze(0)\n",
        "    print(\"xu.requires_grad: \",xu.requires_grad,xu)\n",
        "    return xu\n",
        "\n",
        "# x=torch.tensor(3.,requires_grad=True)\n",
        "x=torch.rand(3,requires_grad=True)\n",
        "xu=fd(x)\n",
        "\n",
        "print(xu)\n",
        "print(xu.requires_grad)\n"
      ],
      "metadata": {
        "id": "3-W9b3GTDIqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title locuslab me3\n",
        "# https://github.com/locuslab/stable_dynamics/blob/master/models/stabledynamics.py\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "# Learning Stable Deep Dynamics Models https://arxiv.org/pdf/2001.06116.pdf\n",
        "# class Dynamics(nn.Module):\n",
        "class StableDynamics(nn.Module):\n",
        "    def __init__(self, fhat, V, alpha=0.01):\n",
        "        super().__init__()\n",
        "        self.fhat = fhat\n",
        "        self.V = V\n",
        "        self.alpha = alpha\n",
        "\n",
        "    # def forward(self, x, u=None):\n",
        "    def forward(self, u):\n",
        "        # print(\"fwd x\",x.shape)\n",
        "        # xu = x#.squeeze()\n",
        "        # if len(x.shape) == 1: x = x.unsqueeze(0)\n",
        "        # if u is not None:\n",
        "        #     # print(\"fwd u\",u.shape)\n",
        "        #     # if len(u.shape) == 1: u = u.unsqueeze(0)\n",
        "        #     xu = torch.cat([x, u], dim=-1)#.squeeze()\n",
        "        # if not xu.requires_grad: xu.requires_grad = True\n",
        "        # if not x.requires_grad: x.requires_grad = True\n",
        "        # if len(xu.shape) == 1: xu = xu.unsqueeze(0)\n",
        "\n",
        "\n",
        "        fx = self.fhat(u)\n",
        "        # fx = fx / fx.norm(p=2, dim=1, keepdim=True).clamp(min=1.0) # if SCALE_FX:\n",
        "        # Vx = self.V(x)\n",
        "        # print(xu.requires_grad)\n",
        "        Vx = self.V(fx)\n",
        "        # print(Vx.requires_grad)\n",
        "\n",
        "        # x.retain_grad()\n",
        "        # xu.retain_grad()\n",
        "        # lyapunov.backward(gradient=torch.ones_like(lyapunov), retain_graph=True)\n",
        "        # Vx.backward()\n",
        "        # Vx.backward(retain_graph=True)\n",
        "        # grad_v = xu.grad.clone()\n",
        "\n",
        "\n",
        "        # print(\"dy fwd Vx\",Vx,[a.requires_grad for a in Vx])\n",
        "        gV = torch.autograd.grad([a for a in Vx], [fx], create_graph=True, only_inputs=True)[0]\n",
        "        # gV = torch.autograd.grad(Vx, xu)[0]\n",
        "        # gV = torch.autograd.grad(Vx, x)[0]\n",
        "        # print(\"dy fwd gV\",fx.shape, gV.shape)\n",
        "        # print(\"dy fwd gV\",fx, gV)\n",
        "        # rv = fx - gV * (F.relu((gV*fx).sum(dim=1) + self.alpha*Vx[:,0])/(gV**2).sum(dim=1))[:,None]\n",
        "        if len(u.shape) == 1: rv = fx - gV * (F.relu((gV*fx).sum() + self.alpha*Vx[0])/(gV**2).sum())[None]\n",
        "        else: rv = fx - gV * (F.relu((gV*fx).sum(dim=1) + self.alpha*Vx[:,0])/(gV**2).sum(dim=1))[:,None]\n",
        "        return rv\n",
        "\n",
        "# Input Convex Neural Networks https://arxiv.org/pdf/1609.07152.pdf\n",
        "class ICNN(nn.Module):\n",
        "    def __init__(self, layer_sizes, activation=F.relu_):\n",
        "        super().__init__()\n",
        "        self.W = nn.ParameterList([nn.Parameter(torch.Tensor(l, layer_sizes[0])) for l in layer_sizes[1:]])\n",
        "        self.U = nn.ParameterList([nn.Parameter(torch.Tensor(layer_sizes[i+1], layer_sizes[i])) for i in range(1,len(layer_sizes)-1)])\n",
        "        self.bias = nn.ParameterList([nn.Parameter(torch.Tensor(l)) for l in layer_sizes[1:]])\n",
        "        self.act = activation\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self): # copying from PyTorch Linear\n",
        "        for W in self.W:\n",
        "            nn.init.kaiming_uniform_(W, a=5**0.5)\n",
        "        for U in self.U:\n",
        "            nn.init.kaiming_uniform_(U, a=5**0.5)\n",
        "        for i,b in enumerate(self.bias):\n",
        "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.W[i])\n",
        "            bound = 1 / (fan_in**0.5)\n",
        "            nn.init.uniform_(b, -bound, bound)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = F.linear(x, self.W[0], self.bias[0])\n",
        "        z = self.act(z)\n",
        "        for W,b,U in zip(self.W[1:-1], self.bias[1:-1], self.U[:-1]):\n",
        "            z = F.linear(x, W, b) + F.linear(z, F.softplus(U)) / U.shape[0]\n",
        "            z = self.act(z)\n",
        "        return F.linear(x, self.W[-1], self.bias[-1]) + F.linear(z, F.softplus(self.U[-1])) / self.U[-1].shape[0]\n",
        "\n",
        "\n",
        "lsd = 2 #2 # 320\n",
        "ph_dim = 64 # 40\n",
        "h_dim = 64 # 100\n",
        "\n",
        "n_x = 3 # lsd # 3\n",
        "n_u = 2 #0#2\n",
        "\n",
        "# fhat = nn.Sequential(nn.Linear(n_x + n_u, h_dim), nn.ReLU(),\n",
        "fhat = nn.Sequential(nn.Linear(n_u, h_dim), nn.ReLU(),\n",
        "                    nn.Linear(h_dim, h_dim), nn.ReLU(),\n",
        "                    nn.Linear(h_dim, n_x))\n",
        "                    # nn.Linear(h_dim, n_u))\n",
        "                    # nn.Linear(h_dim, n_x + n_u))\n",
        "\n",
        "\n",
        "V = ICNN([n_x, ph_dim, ph_dim, 1]) # ICNN\n",
        "\n",
        "# model = Dynamics(fhat, V, alpha=0.01)\n",
        "model = StableDynamics(fhat, V, alpha=0.01)\n",
        "\n",
        "\n",
        "# import torch # torch.__version__\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# model = torch.compile(model.to(device),mode=\"max-autotune\")\n",
        "\n",
        "\n",
        "x=torch.rand(1,n_x, requires_grad=True)\n",
        "u=torch.rand(1,n_u, requires_grad=True)\n",
        "# x=torch.rand(1,n_x, requires_grad=False)\n",
        "# u=torch.rand(1,n_u, requires_grad=False)\n",
        "xu=torch.cat((x,u),-1)\n",
        "# print(xu.shape)\n",
        "# print(V(xu))\n",
        "\n",
        "# xhat=model(xu)\n",
        "# print(V(xu).shape) # [1,1]\n",
        "# xhat=model(x, u)\n",
        "# xhat=model(x)\n",
        "xhat=model(u)\n",
        "print(xhat)\n",
        "\n",
        "\n",
        "from torch.autograd.functional import jacobian, hessian\n",
        "# jac = jacobian(model,(x,u))\n",
        "jac = jacobian(model, u)\n",
        "print(\"jac\",jac)\n"
      ],
      "metadata": {
        "id": "9gyLidHAz5hM",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title locuslab x,u\n",
        "# https://github.com/locuslab/stable_dynamics/blob/master/models/stabledynamics.py\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "# Learning Stable Deep Dynamics Models https://arxiv.org/pdf/2001.06116.pdf\n",
        "class StableDynamics(nn.Module):\n",
        "    def __init__(self, fhat, V, alpha=0.01):\n",
        "        super().__init__()\n",
        "        self.fhat = fhat\n",
        "        self.V = V\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, x, u):\n",
        "        with torch.enable_grad():\n",
        "            # print('x,u',x,u)\n",
        "\n",
        "            if not x.requires_grad: x.requires_grad = True\n",
        "            if not u.requires_grad: u.requires_grad = True\n",
        "            # if len(xu.shape) == 1: xu = xu.unsqueeze(0)\n",
        "            x.retain_grad()\n",
        "            xu = torch.cat([x, u], dim=-1)#.squeeze()\n",
        "            # xu = torch.stack([x, u], dim=-1)\n",
        "            # if not xu.requires_grad: xu.requires_grad = True\n",
        "            # xu.retain_grad()\n",
        "\n",
        "            # print('x,u,xu',x,u,xu)\n",
        "            # print('x,u,xu',x.dtype,u.dtype,xu.dtype)\n",
        "\n",
        "            fx = self.fhat(xu)\n",
        "            # fx = fx / fx.norm(p=2, dim=1, keepdim=True).clamp(min=1.0) # if SCALE_FX:\n",
        "            Vx = self.V(xu)\n",
        "            # if not Vx.requires_grad: Vx.requires_grad = True\n",
        "            # Vx.retain_grad()\n",
        "\n",
        "            # lyapunov.backward(gradient=torch.ones_like(lyapunov), retain_graph=True)\n",
        "            # Vx.backward()\n",
        "            # Vx.backward(retain_graph=True)\n",
        "            # grad_v = xu.grad.clone()\n",
        "\n",
        "            # print(\"dy fwd Vx\",Vx,[a.requires_grad for a in Vx],x)\n",
        "            # print(\"dy fwd Vx\",Vx,Vx[0])\n",
        "            gV = torch.autograd.grad([a for a in Vx], [x], create_graph=True, only_inputs=True)[0]\n",
        "            # gV = torch.autograd.grad([a for a in Vx], x, create_graph=True, only_inputs=True)[0]\n",
        "            # gV = torch.autograd.grad(Vx, xu)[0]\n",
        "            # gV = torch.autograd.grad(Vx, x)[0]\n",
        "            # print(\"dy fwd gV\",fx.shape, gV.shape)\n",
        "            # print(\"dy fwd gV\",fx, gV)\n",
        "            rv = fx - gV * (F.relu((gV*fx).sum(dim=1) + self.alpha*Vx[:,0])/(gV**2).sum(dim=1))[:,None]\n",
        "            # return rv\n",
        "            return rv+x\n",
        "\n",
        "# Input Convex Neural Networks https://arxiv.org/pdf/1609.07152.pdf\n",
        "class ICNN(nn.Module):\n",
        "    def __init__(self, layer_sizes, activation=F.relu_):\n",
        "        super().__init__()\n",
        "        self.W = nn.ParameterList([nn.Parameter(torch.Tensor(l, layer_sizes[0])) for l in layer_sizes[1:]])\n",
        "        self.U = nn.ParameterList([nn.Parameter(torch.Tensor(layer_sizes[i+1], layer_sizes[i])) for i in range(1,len(layer_sizes)-1)])\n",
        "        self.bias = nn.ParameterList([nn.Parameter(torch.Tensor(l)) for l in layer_sizes[1:]])\n",
        "        self.act = activation\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self): # copying from PyTorch Linear\n",
        "        for W in self.W:\n",
        "            nn.init.kaiming_uniform_(W, a=5**0.5)\n",
        "        for U in self.U:\n",
        "            nn.init.kaiming_uniform_(U, a=5**0.5)\n",
        "        for i,b in enumerate(self.bias):\n",
        "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.W[i])\n",
        "            bound = 1 / (fan_in**0.5)\n",
        "            nn.init.uniform_(b, -bound, bound)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = F.linear(x, self.W[0], self.bias[0])\n",
        "        z = self.act(z)\n",
        "        for W,b,U in zip(self.W[1:-1], self.bias[1:-1], self.U[:-1]):\n",
        "            z = F.linear(x, W, b) + F.linear(z, F.softplus(U)) / U.shape[0]\n",
        "            z = self.act(z)\n",
        "        return F.linear(x, self.W[-1], self.bias[-1]) + F.linear(z, F.softplus(self.U[-1])) / self.U[-1].shape[0]\n",
        "\n",
        "\n",
        "lsd = 2 #2 # 320\n",
        "ph_dim = 64 # 40\n",
        "h_dim = 64 # 100\n",
        "\n",
        "n_x = 3 # lsd # 3\n",
        "n_u = 1 #0#2\n",
        "\n",
        "fhat = nn.Sequential(nn.Linear(n_x + n_u, h_dim), nn.ReLU(),\n",
        "                    nn.Linear(h_dim, h_dim), nn.ReLU(),\n",
        "                    nn.Linear(h_dim, n_x))\n",
        "V = ICNN([n_x + n_u, ph_dim, ph_dim, 1]) # ICNN\n",
        "model = StableDynamics(fhat, V, alpha=0.01)\n",
        "\n",
        "\n",
        "# x=torch.rand(1,n_x, requires_grad=True)\n",
        "# u=torch.rand(1,n_u, requires_grad=True)\n",
        "x=torch.rand(1,n_x)\n",
        "u=torch.rand(1,n_u)\n",
        "# print(xu.shape)\n",
        "# print(V(xu))\n",
        "# print(V(xu).shape) # [1,1]\n",
        "xhat=model(x, u)\n",
        "print(xhat)\n",
        "\n",
        "\n",
        "from torch.autograd.functional import jacobian, hessian\n",
        "jac = jacobian(model,(x,u))\n",
        "print(\"jac\",jac)\n"
      ],
      "metadata": {
        "id": "dIkH9To89iKb",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # self.fc1 = nn.Linear(2, 4)\n",
        "    self.fc1 = nn.Linear(4, 2)\n",
        "\n",
        "#   def forward(self, x):\n",
        "#     return self.fc1(x)\n",
        "  def forward(self, x,u):\n",
        "\n",
        "    if not x.requires_grad: x.requires_grad = True\n",
        "    # if len(xu.shape) == 1: xu = xu.unsqueeze(0)\n",
        "    x.retain_grad()\n",
        "    xu = torch.cat([x, u], dim=-1)#.squeeze()\n",
        "    if not xu.requires_grad: xu.requires_grad = True\n",
        "    xu.retain_grad()\n",
        "\n",
        "    xu = torch.cat([x, u], dim=-1)#.squeeze()\n",
        "    out = self.fc1(xu)\n",
        "    # gradient = torch.autograd.grad(out, x)\n",
        "    gradient = torch.autograd.grad([a for a in out], x, create_graph=True)\n",
        "        # gV = torch.autograd.grad([a for a in Vx], x, , only_inputs=True)[0]\n",
        "\n",
        "    print(gradient)\n",
        "    return out\n",
        "\n",
        "net = Model()\n",
        "# input_x = torch.tensor([1.0, 2], requires_grad=True) # But for requires_grad=True, it will produce your error\n",
        "# out_y = net(input_x)\n",
        "# gradient = torch.autograd.grad(out_y[1], input_x)\n",
        "# x = torch.tensor([1.0, 2, 1], requires_grad=True)\n",
        "# u = torch.tensor([1.0], requires_grad=True)\n",
        "x = torch.tensor([1.0, 2, 1])\n",
        "u = torch.tensor([1.0])\n",
        "out = net(x,u)\n",
        "# gradient = torch.autograd.grad(out[1], x)\n",
        "# gradient = torch.autograd.grad(out, x)\n",
        "gradient = torch.autograd.grad([a for a in out], x)\n",
        "print(gradient)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "5TAGKCiBaho3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# new_xt.requires_grad=True\n",
        "new_xt=torch.rand(1,n_x)\n",
        "new_ut=torch.rand(1,n_u)\n",
        "print(\"in lqrfwd new_xt, ut: \",new_xt, new_ut)\n",
        "# new_xtp1 = true_dynamics(Variable(new_xt), Variable(new_ut)).data # og, one line only\n",
        "new_xtp1 = model(new_xt, new_ut).data\n",
        "# print(\"in lqrfwd new_xtp1: \",new_xtp1.requires_grad)\n"
      ],
      "metadata": {
        "id": "mZZKletiB3dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title down func\n",
        "# simpe copied https://locuslab.github.io/mpc.pytorch/\n",
        "# https://github.com/locuslab/mpc.pytorch/blob/master/examples/Pendulum%20Control.ipynb\n",
        "# https://github.com/locuslab/mpc.pytorch/blob/master/examples/Cartpole%20Control.ipynb\n",
        "torch.manual_seed(0)\n",
        "\n",
        "class StableDx(nn.Module):\n",
        "    def __init__(self, params=None, simple=True):\n",
        "        super().__init__()\n",
        "        self.n_x = 3\n",
        "        self.n_u = 1\n",
        "\n",
        "    def forward(self, x, u):\n",
        "        dt = 0.05\n",
        "        xhat=model(x, u)\n",
        "        out = x+ xhat*dt\n",
        "        return out\n",
        "\n",
        "# dx = PendulumDx()#params, simple=True)\n",
        "dx = StableDx()\n",
        "\n",
        "\n",
        "# n_batch, T, mpc_T = 1, 125, 20 # 16, 100, 20 ; batch size, epochs, ? larger solves\n",
        "n_batch, T, mpc_T = 1, 125, 20\n",
        "\n",
        "# xinit = torch.tensor([0., -1., 0.])\n",
        "x = torch.tensor([0., -1., 0.]).unsqueeze(0).repeat(n_batch, 1) # xinit # init state\n",
        "# u_init = None # initial control?\n",
        "u = torch.rand(1,dx.n_u) # u_init # initial control\n",
        "# u=us\n",
        "\n",
        "# x.requires_grad = True\n",
        "# u.requires_grad = True\n",
        "\n",
        "goal_weights = torch.Tensor([1., 1., 0.1])\n",
        "goal_state = torch.Tensor([0., 1. ,0.])\n",
        "ctrl_penalty = torch.Tensor([0.001])\n",
        "\n",
        "\n",
        "q = torch.cat([goal_weights, ctrl_penalty]) # [1.0000, 1.0000, 0.1000, 0.0010]\n",
        "px = -torch.sqrt(goal_weights)*goal_state # [-0., -1., -0.]\n",
        "p = torch.cat((px, torch.zeros(dx.n_u)))\n",
        "Q = torch.diag(q).unsqueeze(0).unsqueeze(0).repeat(mpc_T, n_batch, 1, 1) # [mpc_T, n_batch, n_state+n_ctrl, n_state+n_ctrl]\n",
        "p = p.unsqueeze(0).repeat(mpc_T, n_batch, 1) # [mpc_T, n_batch, n_state+n_ctrl] : [-0., -1., -0.,  0.]\n",
        "\n",
        "xs = x\n",
        "us = u\n",
        "\n",
        "# out=dx(x,u)\n",
        "# print(out)\n",
        "\n",
        "for t in tqdm(range(T)):\n",
        "    nominal_states, nominal_actions, nominal_objs = MPC(\n",
        "        dx.n_x, dx.n_u, mpc_T, # state dim, action dim,\n",
        "        u_init=u,\n",
        "        u_lower=-2., u_upper=2., # +-0.5\n",
        "        lqr_iter=5, # 50 num LQR iterations to perform\n",
        "        linesearch_decay=0.2, #dx.linesearch_decay,\n",
        "        max_linesearch_iter=5,#dx.max_linesearch_iter,\n",
        "        eps=1e-2,\n",
        "    )(x, QuadCost(Q, p), dx)\n",
        "\n",
        "    next_action = nominal_actions[0]\n",
        "    u = torch.cat((nominal_actions[1:], torch.zeros(1, n_batch, dx.n_u)), dim=0)\n",
        "    u[-2] = u[-3]\n",
        "    # print(nominal_objs)\n",
        "    x = dx(x, next_action)\n",
        "    us = torch.cat((us,next_action),0)\n",
        "    xs = torch.cat((xs,x),0)\n",
        "\n",
        "\n",
        "# #Plot theta and action trajectory\n",
        "# import matplotlib.pyplot as plt\n",
        "# theta = torch.arctan2(xs[:, 0], xs[:, 1]).detach()\n",
        "# theta = torch.where(theta < 0, 2*torch.pi+theta, theta)\n",
        "# plt.plot(theta)\n",
        "# plt.plot(us.detach())\n",
        "# plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4Ct1MxSDZVET",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(xs)"
      ],
      "metadata": {
        "id": "l7_LWpfVADk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title streamplot model\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "# torch method\n",
        "x = torch.arange(-2.0, 2.0, 0.4)\n",
        "y = torch.arange(-2.0, 2.0, 0.4)\n",
        "# x = torch.arange(-10.0, 10.0, 2.)\n",
        "# y = torch.arange(-10.0, 10.0, 2.)\n",
        "# y = torch.arange(-9., 9., 2.)\n",
        "X, Y = torch.meshgrid(x, y,indexing='xy')\n",
        "# X, Y = torch.meshgrid(x, y)\n",
        "\n",
        "lx=len(x)\n",
        "ly=len(y)\n",
        "X=X.reshape(lx*ly,1).requires_grad_()\n",
        "Y=Y.reshape(lx*ly,1).requires_grad_()\n",
        "# print(X)\n",
        "# print(Y)\n",
        "XY=torch.cat([X,Y], dim=-1)\n",
        "\n",
        "\n",
        "# ph_dim = 64\n",
        "# h_dim = 64\n",
        "# n_x = 2\n",
        "# n_u = 0\n",
        "# fhat = nn.Sequential(nn.Linear(n_x + n_u, h_dim), nn.ReLU(),\n",
        "#                     nn.Linear(h_dim, h_dim), nn.ReLU(),\n",
        "#                     nn.Linear(h_dim, n_x))\n",
        "# # V = ICNN([n_x + n_u, ph_dim, ph_dim, 1]) # ICNN\n",
        "# # V = PosDefICNN([n_x + n_u, ph_dim, ph_dim, 1], eps=projfn_eps, negative_slope=0.3) # PSICNN\n",
        "# V = MakePSD(ICNN([n_x + n_u, ph_dim, ph_dim, 1]), lsd, eps=projfn_eps, d=1.0) # PSD\n",
        "\n",
        "# model = Dynamics(fhat, V, alpha=0.01)\n",
        "# uv = model(XY)\n",
        "# # w = V(XY)\n",
        "# uv = fhat(XY)\n",
        "# uvw = model(XY)\n",
        "U=torch.rand(1,n_u, requires_grad=True).repeat(XY.shape[0],1)\n",
        "\n",
        "uv=model(XY,U)\n",
        "\n",
        "# u,v,w=uvw.split([1,1,1],dim=-1)\n",
        "u,v=uv.split([1,1],dim=-1)\n",
        "\n",
        "# print(uv.shape)\n",
        "# print(uv)\n",
        "\n",
        "uv=uv.detach()\n",
        "# u,v=uv.split([1,1],dim=-1)\n",
        "\n",
        "# u,v,w=u.detach(),v.detach(),w.detach()\n",
        "u,v=u.detach(),v.detach()\n",
        "X,Y=X.detach(),Y.detach()\n",
        "\n",
        "X, Y = X.numpy(), Y.numpy()\n",
        "u, v = u.numpy(), v.numpy()\n",
        "u=u.reshape((lx,ly))\n",
        "v=v.reshape((lx,ly))\n",
        "# uv=uv.reshape((lx,ly))\n",
        "X=X.reshape((lx,ly))\n",
        "Y=Y.reshape((lx,ly))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "print(X.shape, Y.shape, u.shape, v.shape)\n",
        "# ax.streamplot(X, Y, u, v)\n",
        "ax.quiver(X, Y, u, v)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Uy5ZuV6HsF4b",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(XY.shape)\n",
        "print(U.shape)\n",
        "print(U)\n",
        "print(U.repeat(XY.shape[0],1).shape)\n",
        "print(U.repeat(XY.shape[0],1))\n"
      ],
      "metadata": {
        "id": "7HYN1b2Vvyrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###mario"
      ],
      "metadata": {
        "id": "w7lhMcwLtMb5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qQjuJIepAvA"
      },
      "source": [
        "#### setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1GD7lk8H13h",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title mario setup\n",
        "# # https://github.com/kimhc6028/pytorch-noreward-rl\n",
        "# https://stackoverflow.com/questions/67808779/running-gym-atari-in-google-colab\n",
        "# %pip install -U gym\n",
        "%pip install gym==0.24.1\n",
        "%pip install -U gym[atari,accept-rom-license]\n",
        "# !pip install gym[box2d]\n",
        "# import gym\n",
        "# pip install gym-super-mario-bros==7.4.0\n",
        "\n",
        "!pip install gym-super-mario-bros nes-py\n",
        "# https://github.com/Kautenja/gym-super-mario-bros\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT\n",
        "# env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
        "# env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
        "\n",
        "\n",
        "\n",
        "# Initialize Super Mario environment (in v0.26 change render mode to 'human' to see results on the screen)\n",
        "if gym.__version__ < '0.26':\n",
        "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", new_step_api=True)\n",
        "else:\n",
        "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", render_mode='rgb', apply_api_compatibility=True)\n",
        "\n",
        "# Limit the action-space to\n",
        "\n",
        "\n",
        "!pip install colabgymrender\n",
        "!pip install perceiver-pytorch\n",
        "\n",
        "import gym\n",
        "class SparseEnv(gym.Wrapper): #https://alexandervandekleut.github.io/gym-wrappers/\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.env = env\n",
        "        self.total_rewards = 0\n",
        "    def step(self, action):\n",
        "        observation, reward, done, info = self.env.step(action)\n",
        "        self.total_rewards += reward\n",
        "        if done: return observation, self.total_rewards, done, info\n",
        "        else:\n",
        "            self.total_rewards = 0\n",
        "            return observation, 0, done, info\n",
        "    def reset(self):\n",
        "        self.total_rewards = 0\n",
        "        return self.env.reset()\n",
        "# env = SparseEnv(gym.make(\"LunarLander-v2\"))\n",
        "\n",
        "class MarioSparse(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.env = env\n",
        "        self.total_score = 0\n",
        "    def step(self, action):\n",
        "        observation, reward, done, info = self.env.step(action)\n",
        "        life = info['life']\n",
        "        score = info['score']\n",
        "        self.total_score += score\n",
        "        # print(\"MarioSparse\",life,score)\n",
        "        # if done: return observation, self.total_rewards, done, info\n",
        "        if life<2: return observation, score, True, info # lost one life, end env\n",
        "        else:\n",
        "            # self.total_score = 0\n",
        "            return observation, score, False, info\n",
        "    def reset(self):\n",
        "        # self.total_score = 0\n",
        "        return self.env.reset()\n",
        "# env = MarioSparse(env)\n",
        "\n",
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nes_py.wrappers import JoypadSpace\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
        "# env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
        "# env = gym_super_mario_bros.make('SuperMarioBros-v0', new_step_api=True)\n",
        "# env = gym_super_mario_bros.make('SuperMarioBros-v0', render_mode='rgb', apply_api_compatibility=True)\n",
        "# env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", new_step_api=True)\n",
        "\n",
        "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
        "\n",
        "# https://pypi.org/project/gym-super-mario-bros/\n",
        "# SuperMarioBros-<world>-<stage>-v<version>\n",
        "# <world> is a number in {1, 2, 3, 4, 5, 6, 7, 8} indicating the world\n",
        "# <stage> is a number in {1, 2, 3, 4} indicating the stage within a world\n",
        "\n",
        "\n",
        "done = True\n",
        "for step in range(5000):\n",
        "    if done:\n",
        "        state = env.reset()\n",
        "    state, reward, done, info = env.step(env.action_space.sample())\n",
        "    env.render()\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "Fu8Vhh7NwqPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/master/2_gym_wrappers_saving_loading.ipynb\n",
        "import gym\n",
        "class SparseEnv(gym.Wrapper): #https://alexandervandekleut.github.io/gym-wrappers/\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.env = env\n",
        "        self.total_rewards = 0\n",
        "    def step(self, action):\n",
        "        observation, reward, done, info = self.env.step(action)\n",
        "        self.total_rewards += reward\n",
        "        if done:\n",
        "            reward = self.total_rewards\n",
        "            # return observation, self.total_rewards, done, info\n",
        "        else:\n",
        "            reward = 0\n",
        "            # self.total_rewards = 0\n",
        "            # return observation, 0, done, info\n",
        "        return observation, reward, done, info\n",
        "    def reset(self):\n",
        "        self.total_rewards = 0\n",
        "        return self.env.reset()\n",
        "# env = SparseEnv(gym.make(\"LunarLander-v2\"))\n",
        "\n",
        "class MarioSparse(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.env = env\n",
        "        self.total_score = 0\n",
        "    def step(self, action):\n",
        "        observation, reward, done, info = self.env.step(action)\n",
        "        life = info['life']\n",
        "        score = info['score']\n",
        "        self.total_score += score\n",
        "        if life<2:\n",
        "            print(\"MarioSparse: died\")\n",
        "            # return observation, score, True, info # lost one life, end env\n",
        "            done = True\n",
        "        # else:\n",
        "            # self.total_score = 0\n",
        "        return observation, score, done, info\n",
        "    def reset(self):\n",
        "        self.total_score = 0\n",
        "        return self.env.reset()\n",
        "# env = MarioSparse(env)\n",
        "\n",
        "class MarioEarlyStop(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.env = env\n",
        "        self.max_pos = 0\n",
        "        self.count_step = 0\n",
        "    def step(self, action):\n",
        "        observation, reward, done, info = self.env.step(action)\n",
        "        x_pos = info['x_pos']\n",
        "        if x_pos <= self.max_pos: self.count_step += 1\n",
        "        else:\n",
        "            self.max_pos = x_pos\n",
        "            self.count_step = 0\n",
        "        if self.count_step > 500:\n",
        "            print(\"MarioEarlyStop: early stop \", self.max_pos)\n",
        "            # return observation, reward, True, info # early stop\n",
        "            done = True\n",
        "        # else:\n",
        "        return observation, reward, done, info\n",
        "    def reset(self):\n",
        "        self.max_pos = 0\n",
        "        self.count_step = 0\n",
        "        return self.env.reset()\n",
        "# env = MarioEarlyStop(env)\n"
      ],
      "metadata": {
        "id": "FMmEy1s-0rfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjPLqIBgH9xJ"
      },
      "outputs": [],
      "source": [
        "# main.py\n",
        "# https://github.com/kimhc6028/pytorch-noreward-rl/blob/master/main.py\n",
        "# import os, sys, cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import gym\n",
        "\n",
        "lr=0.001\n",
        "gamma=0.99\n",
        "tau=1.00\n",
        "seed=1\n",
        "num_processes=4\n",
        "num_steps=20\n",
        "max_episode_length=500 # 10000\n",
        "# env_name='PongDeterministic-v4'\n",
        "# env_name='LunarLander-v2'\n",
        "# env_name='MontezumaRevengeDeterministic-v4'\n",
        "# env_name='MontezumaRevengeDeterministic-ram-v4'\n",
        "\n",
        "no_shared=False\n",
        "eta=0.01\n",
        "beta=0.2\n",
        "lmbda=0.1\n",
        "outdir=\"output\"\n",
        "record='store_true'\n",
        "num_episodes=10#100\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "# env = gym.make(env_name)\n",
        "# env = SparseEnv(env)\n",
        "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
        "env = JoypadSpace(env, COMPLEX_MOVEMENT) # SIMPLE_MOVEMENT COMPLEX_MOVEMENT\n",
        "env = MarioSparse(env)\n",
        "# query_environment(\"MountainCar-v0\")\n",
        "\n",
        "# print(env.observation_space.shape, env.action_space) # (210, 160, 3) Discrete(18)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFrRKvOwhYM_"
      },
      "source": [
        "#### wwwwwwwww"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6fkhEcBB3tM"
      },
      "outputs": [],
      "source": [
        "\n",
        "import gym\n",
        "from colabgymrender.recorder import Recorder\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT\n",
        "\n",
        "# # env = gym.make(\"MontezumaRevengeDeterministic-v4\")\n",
        "# env = SparseEnv(env)\n",
        "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
        "env = JoypadSpace(env, COMPLEX_MOVEMENT) # SIMPLE_MOVEMENT COMPLEX_MOVEMENT\n",
        "env = MarioSparse(env)\n",
        "env = Recorder(env, './video')\n",
        "\n",
        "state = env.reset()\n",
        "\n",
        "# model = ActorCritic(env.observation_space.shape, env.action_space)\n",
        "# model.load_state_dict(shared_model.state_dict())\n",
        "# model.eval()\n",
        "# cx = torch.zeros(1, 256)\n",
        "# hx = torch.zeros(1, 256)\n",
        "# torch.manual_seed(6)\n",
        "x=0\n",
        "\n",
        "while True:\n",
        "    # state = torch.from_numpy(state.copy()).type(torch.float)\n",
        "    # value, logit, (hx, cx) = model((state.unsqueeze(0), (hx, cx)), icm = False)\n",
        "    # prob = F.softmax(logit, dim=1) #from train\n",
        "    # action = prob.multinomial(1).data\n",
        "    # state, reward, done, _ = env.step(action.item())\n",
        "\n",
        "    print(env.action_space.sample())\n",
        "    # state, reward, done, info = env.step(env.action_space.sample())\n",
        "    state, reward, done, info = env.step(5)\n",
        "    env.render()\n",
        "    # try:\n",
        "    #     action=int(acts[x])\n",
        "    # except:\n",
        "    #     action = 10\n",
        "    # # print(\"action\",action)\n",
        "    # # action = env.action_space.sample()\n",
        "    # state, reward, done, info = env.step(action)\n",
        "    x+=1\n",
        "    if done: break\n",
        "env.play()\n",
        "print(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://stackoverflow.com/questions/57377185/how-play-mp4-video-in-google-colab\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4_path='/content/video/1656241889.873849.mp4'\n",
        "# mp4 = open('video.mp4','rb').read()\n",
        "mp4 = open(mp4_path,'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ],
      "metadata": {
        "id": "07i9SI6uHZKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyq1p-53dPBg"
      },
      "outputs": [],
      "source": [
        "# @title video base\n",
        "\n",
        "import gym\n",
        "from colabgymrender.recorder import Recorder\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT\n",
        "\n",
        "# env = gym.make(\"MontezumaRevengeDeterministic-v4\")\n",
        "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
        "env = JoypadSpace(env, COMPLEX_MOVEMENT)\n",
        "env = Recorder(env, './video')\n",
        "state = env.reset()\n",
        "\n",
        "while True:\n",
        "    action = env.action_space.sample()\n",
        "    state, reward, done, info = env.step(action)\n",
        "    # state, reward, done, _, info = env.step(action) # observation, reward, terminated, truncated, info\n",
        "    # state, reward, done, truncated, info = env.step(action) # observation, reward, terminated, truncated, info\n",
        "\n",
        "    if done: break #can only break when done, else error\n",
        "env.play()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eiY5i13mrmb"
      },
      "outputs": [],
      "source": [
        "# @title og video\n",
        "!pip install colabgymrender\n",
        "import gym\n",
        "from colabgymrender.recorder import Recorder\n",
        "\n",
        "env = gym.make(\"MontezumaRevengeDeterministic-v4\")\n",
        "env = Recorder(env, './video')\n",
        "observation = env.reset()\n",
        "terminal= False\n",
        "while not done:\n",
        "    action =env.action_space.sample()\n",
        "    observation, reward, done, info = env.step(action)\n",
        "env.play()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "NxrfE91Cy820"
      ],
      "authorship_tag": "ABX9TyNFVov5nUAfBnM2oet0jljh",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}