{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/Seq_JEPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XwpnHW4wn9S1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "964b0af8-90b8-405b-c51e-fa79acccaa54",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:04<00:00, 41.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# @title data\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "transform = transforms.Compose([transforms.ToTensor(),])\n",
        "\n",
        "# train_data = torchvision.datasets.MNIST(root=\"data\", train=True, download=True,transform=transform) # do not normalise! want img in [0,1)\n",
        "# test_data = torchvision.datasets.MNIST(root=\"data\", train=False, download=True, transform=transform) #opt no download\n",
        "# # batch_size = 32 # 64 512\n",
        "# batch_size = 32 if torch.cuda.is_available() else 32\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "train_data = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
        "test_data = torchvision.datasets.CIFAR10(root='data', train=False, download=True, transform=transform)\n",
        "batch_size = 128 # 128 1024\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "\n",
        "# dataiter = iter(train_data)\n",
        "# x,y = next(dataiter)\n",
        "# print(x.shape) # [3, 32, 32]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title (Learned)RotEmb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, seq_len=512, base=10000):\n",
        "        super().__init__()\n",
        "        self.dim, self.base = dim, base\n",
        "        theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "        pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "        angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "        self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.base)\n",
        "        return x * self.rot_emb[:seq_len]\n",
        "\n",
        "# class LearnedRoPE(nn.Module): # learnt RoPE ; each tok is 1 pos\n",
        "#     def __init__(self, dim):\n",
        "#         super().__init__()\n",
        "#         self.weights = nn.Parameter(torch.randn(1, dim//2))\n",
        "\n",
        "#     def forward(self, x): #\n",
        "#         batch, seq_len, dim = x.shape\n",
        "#         # if rot_emb.shape[0] < seq_len: self.__init__(dim, seq_len)\n",
        "#         pos = torch.arange(seq_len).unsqueeze(1)\n",
        "#         angles = (self.weights * pos * 2*torch.pi).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "#         rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "#         return x * rot_emb.flatten(-2).unsqueeze(0)\n",
        "\n",
        "\n",
        "class LearnedRoPE(nn.Module): # learnt RoPE ; each tok is 1 pos\n",
        "    def __init__(self, dim, seq_len=512):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.weights = nn.Parameter(torch.randn(1, dim//2))\n",
        "        pos = torch.arange(seq_len).unsqueeze(1)\n",
        "        angles = (self.weights * pos * 2*torch.pi)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "        self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "    def forward(self, x): # [batch, seq_len, dim]\n",
        "        seq_len = x.size(1)\n",
        "        if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len)\n",
        "        return x * self.rot_emb[:seq_len]\n",
        "\n",
        "\n",
        "class RotEmb(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, top=torch.pi, base=10000):\n",
        "        super().__init__()\n",
        "        self.theta = top / (base ** (torch.arange(0, dim, step=2, device=device) / dim))\n",
        "        # self.theta = top / (base ** torch.linspace(0, 1, dim//2, device=device))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (pos.unsqueeze(-1) * self.theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [seq_len, dim]\n",
        "\n",
        "class LearnedRotEmb(nn.Module): # pos in R\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.weights = nn.Parameter(torch.randn((1, dim//2), device=device))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (self.weights * pos.unsqueeze(-1) * 2*torch.pi).unsqueeze(-1) # [batch, 1] * [1, dim//2] -> [batch, dim//2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [batch, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [batch, dim]\n",
        "\n"
      ],
      "metadata": {
        "id": "npc_xGtOz7DC",
        "cellView": "form"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ViT me simple\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    \"\"\"Zero out the parameters of a module and return it.\"\"\"\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads=None, d_head=8, cond_dim=None, dropout=0.): # .1\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_head = d_head\n",
        "        self.n_heads = d_model // d_head\n",
        "        # self.d_head = d_model // n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or d_model, 2*d_model, bias=False)\n",
        "        # self.k = nn.Sequential(nn.Dropout(dropout), nn.Linear(cond_dim, d_model, bias=False))\n",
        "        # self.lin = nn.Linear(d_model, d_model)\n",
        "        self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        # self.lin = nn.Sequential(nn.Dropout(dropout), zero_module(nn.Linear(d_model, d_model)))\n",
        "        self.drop = nn.Dropout(dropout) # indp before q,k,v; after linout\n",
        "        self.scale = self.d_head ** -.5\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [batch, T, d_model]=[batch, h*w, c], [batch, num_tok, cond_dim], [batch,T]\n",
        "        batch = x.shape[0]\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        Q = self.q(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        # K = self.k(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        K, V = self.kv(cond).view(batch, -1, self.n_heads, 2*self.d_head).transpose(1, 2).chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # linear attention # Softmax(Q) @ (Softmax(K).T @ V)\n",
        "        if mask != None:\n",
        "            mask = mask[:, None, :, None] # [batch,T] -> [batch,1,T,1]\n",
        "            K, V = K.masked_fill(mask, -torch.finfo(x.dtype).max), V.masked_fill(mask, -torch.finfo(x.dtype).max)\n",
        "        Q, K = Q.softmax(dim=-1)*self.scale, K.softmax(dim=-2)\n",
        "        context = K.transpose(-2,-1) @ V # [batch, n_heads, d_head, d_head]\n",
        "        out = Q @ context # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # (quadratic) attention # Softmax(Q @ K.T) @ V\n",
        "        # attn = Q @ K.transpose(-2,-1) * self.scale # [batch, n_heads, T] # [batch, n_heads, T, T/num_tok]\n",
        "        # if mask != None: attn = attn.masked_fill(mask[:, None, :, None], -torch.finfo(attn.dtype).max) # [batch,T]->[batch,1,T,1]\n",
        "        # attention = torch.softmax(attn, dim=-1)\n",
        "        # out = self.drop(attention) @ V # [batch, n_heads, T, d_head]\n",
        "\n",
        "        out = out.transpose(1, 2).flatten(2)\n",
        "        return self.drop(self.lin(out)) # [batch, T, d_model]\n",
        "\n",
        "# if self, dont pass cond_dim in init, dont pass cond in fwd\n",
        "# Softmax(Q @ K.T) @ V ~ Softmax(Q) @ Softmax(K).T @ V\n",
        "\n",
        "# https://github.com/lucidrains/x-transformers/blob/main/x_transformers/x_transformers.py#L1855\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_head=4, cond_dim=None, ff_mult=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm1 = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.norm2 = nn.RMSNorm(d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.cond_dim = cond_dim\n",
        "        self.self = MultiHeadAttention(d_model, d_head=d_head, dropout=0)\n",
        "        if self.cond_dim!=None: self.cross = MultiHeadAttention(d_model, d_head=d_head, cond_dim=cond_dim, dropout=0)\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        ff_dim=d_model*ff_mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), nn.Dropout(dropout), nn.Linear(d_model, ff_dim), act, # ReLU GELU\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "            nn.RMSNorm(d_model), nn.Dropout(dropout), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            # nn.RMSNorm(d_model), act, nn.Dropout(dropout), nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,c,h,w], [batch, num_tok, cond_dim], [batch,T]\n",
        "        # bchw = x.shape\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "\n",
        "        if self.cond_dim==None: cond=None # is self attn\n",
        "        x = x + self.drop(self.self(self.norm1(x)))\n",
        "        if self.cond_dim!=None: x = x + self.cross(self.norm2(x), cond, mask)\n",
        "        x = x + self.ff(x)\n",
        "        # x = x + self.drop(self.ff(x))\n",
        "\n",
        "        # return x.transpose(1,2).reshape(*bchw)\n",
        "        return x\n",
        "\n",
        "\n",
        "# class SimpleViT(nn.Module):\n",
        "#     def __init__(self, in_dim, out_dim, dim, depth, heads, mlp_dim, channels=3, dim_head=8):\n",
        "#         super().__init__()\n",
        "#         self.to_patch_embedding = nn.Sequential( # in, out, kernel, stride, pad\n",
        "#             nn.Conv2d(in_dim, dim, kernel_size=7, stride=2, padding=7//2, bias=False), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "#             # nn.Conv2d(in_dim, dim, kernel_size=7, stride=2, padding=7//2, bias=False), nn.MaxPool2d(2,2)\n",
        "#             )\n",
        "#         # self.pos_embedding = LearnedRoPE2D(dim) # LearnedRoPE2D, RoPE2D\n",
        "#         self.positional_emb = nn.Parameter(torch.zeros(1, 8*8, dim)) # positional_embedding == 'learnable'\n",
        "#         self.transformer = AttentionBlock(d_model=dim, d_head=dim_head)\n",
        "#         self.transformer_blocks = nn.ModuleList([AttentionBlock(d_model=dim, d_head=dim_head) for i in range(depth)])\n",
        "\n",
        "#         self.attention_pool = nn.Linear(dim, 1)\n",
        "#         self.out = nn.Linear(dim, out_dim, bias=False)\n",
        "\n",
        "#     def forward(self, img):\n",
        "#         device = img.device\n",
        "#         x = self.to_patch_embedding(img)\n",
        "#         # x = self.pos_embedding(x)\n",
        "#         x = x.flatten(-2).transpose(-2,-1) # b c h w -> b (h w) c\n",
        "#         x = x + self.positional_emb\n",
        "#         x = self.transformer(x)\n",
        "#         # for blk in self.predictor_blocks: x = blk(x)\n",
        "\n",
        "#         # x = x.flatten(-2).mean(dim=-1) # meal pool\n",
        "#         attn_weights = self.attention_pool(x).squeeze(-1) # [batch, (h,w)] # seq_pool\n",
        "#         x = (attn_weights.softmax(dim = 1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, (h,w)] @ [batch, (h,w), dim]\n",
        "#         # cls_token = repeat(self.class_emb, '1 1 d -> b 1 d', b = b)\n",
        "#         # x = torch.cat((cls_token, x), dim=1)\n",
        "#         # x = x[:, 0] # first token\n",
        "#         return self.out(x)\n",
        "\n",
        "# ijepa: 224*224 -> 14*14 = 196\n",
        "# 32*7\n",
        "# me: 32*32=1024 -> 256\n",
        "\n",
        "import torch\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "def apply_masks(x, masks):\n",
        "    \"\"\":param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
        "    :param masks: list of tensors containing indices of patches in [N] to keep\"\"\"\n",
        "    all_x = []\n",
        "    for m in masks: # [1,T]\n",
        "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [1,T,dim]\n",
        "        all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "    return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "# batch, seq_len, d_model = 2,7,4\n",
        "# x = torch.rand(batch, seq_len, d_model)\n",
        "# print(x)\n",
        "# masks = [torch.randint(0,seq_len,(2,3,))]\n",
        "# print(masks)\n",
        "# out = apply_masks(x, masks)\n",
        "# print(out)\n",
        "# # print(out.shape)\n",
        "\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        # self.positional_emb = nn.Parameter(torch.zeros(1, 32*32//4, d_model)) # positional_embedding == 'learnable'\n",
        "        # encoder_layers = nn.TransformerEncoderLayer(d_model, d_model//d_head, d_hid or d_model, dropout, batch_first=True)\n",
        "        # self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.zeros(1,1,d_model)) # randn zeros\n",
        "        nn.init.trunc_normal_(self.cls, std=.02)\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.lin = nn.Linear(d_model, out_dim)# if out_dim != d_model else None\n",
        "\n",
        "    def forward(self, src, context_indices, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        src = self.embed(src) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = src.shape\n",
        "        src = src * self.pos_encoder(context_indices)\n",
        "        # src = src + self.positional_emb[:,context_indices]\n",
        "        # src = apply_masks(src, [context_indices])\n",
        "\n",
        "        pred_tokens = self.cls * self.pos_encoder(trg_indices) # [M, num_trg_toks, d_model]\n",
        "        # pred_tokens = self.cls + self.positional_emb[0,trg_indices]\n",
        "        pred_tokens = pred_tokens.repeat(batch, 1, 1) # [batch*M, num_trg_toks, d_model]\n",
        "        # print(pred_tokens.requires_grad)\n",
        "        # print(\"pred fwd\", src.shape, pred_tokens.shape)\n",
        "        # src = src.repeat_interleave(trg_indices.shape[0], dim=0) # [batch, seq_len, d_model] -> [batch*M, seq_len, d_model]\n",
        "        src = torch.cat([src, pred_tokens], dim=1) # [batch*M, seq_len+num_trg_toks, d_model]\n",
        "\n",
        "        out = self.transformer_encoder(src) # float [seq_len, batch_size, d_model]\n",
        "        out = self.norm(out)\n",
        "        out = out[:,seq:] # [batch*M, num_trg_toks, d_model]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.MaxPool1d(2,2), #nn.MaxPool1d(3, 2, 3//2),\n",
        "            nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.Conv1d(d_model, d_model,3,2,3//2)\n",
        "            # nn.Conv1d(in_dim, d_model,2,2,0), # like patch\n",
        "            )\n",
        "        self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        # self.positional_emb = nn.Parameter(torch.zeros(1, 32*32//4, d_model)) # positional_embedding == 'learnable'\n",
        "        # encoder_layers = nn.TransformerEncoderLayer(d_model, d_model//d_head, d_hid or d_model, dropout, batch_first=True)\n",
        "        # self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, src, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # src = self.embed(src) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        src = self.embed(src.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "\n",
        "        batch, seq, dim = src.shape\n",
        "\n",
        "        # # # src = self.pos_encoder(src)\n",
        "        # if context_indices != None:\n",
        "        # print(src.shape, self.pos_encoder(context_indices).shape)\n",
        "        #     src = src * self.pos_encoder(context_indices) # context/predictor # src = src + self.positional_emb[:,context_indices]\n",
        "        # else: src = src * self.pos_encoder(torch.arange(seq, device=device).unsqueeze(0)) # target # src = src + self.positional_emb[:,:seq]\n",
        "\n",
        "\n",
        "        # src = self.pos_encoder(src)\n",
        "        src = src * self.pos_encoder(torch.arange(seq, device=device)).unsqueeze(0)\n",
        "        # src = src + self.positional_emb\n",
        "        if context_indices != None:\n",
        "            src = apply_masks(src, [context_indices])\n",
        "\n",
        "\n",
        "        out = self.transformer_encoder(src) # float [seq_len, batch_size, d_model]\n",
        "        out = self.norm(out)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "batch, seq_len, d_model = 4,1024,4\n",
        "in_dim = 3\n",
        "model = TransformerModel(in_dim, d_model, d_head=4, nlayers=2, dropout=0.).to(device)\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "# # print(out)\n",
        "\n"
      ],
      "metadata": {
        "id": "GHzr3NVs2EcG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5394b34-e17a-4ac4-d4ed-4c9d29216db1",
        "cellView": "form"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 256, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title SeqJEPA mae like\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def multiblock(seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "    return target_mask\n",
        "\n",
        "# https://arxiv.org/pdf/2210.07224\n",
        "def randpatch(seq, mask_size=4, gamma=0.75): # num patches of seq, mask patch size, masking ratio\n",
        "    mask = torch.rand(seq//mask_size)<gamma\n",
        "    mask = mask.repeat_interleave(mask_size, dim=-1)\n",
        "    return mask # [seq] , True -> mask\n",
        "\n",
        "class SeqJEPA(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, nlayers=2, d_head=4):\n",
        "        super().__init__()\n",
        "        if out_dim is None: out_dim = d_model\n",
        "        self.patch_size = 4\n",
        "        self.context_encoder = TransformerModel(in_dim, d_model, out_dim=out_dim, d_head=d_head, nlayers=nlayers, dropout=0.)\n",
        "        # self.predicter = TransformerPredictor(out_dim, d_model//2, out_dim, d_head=d_head, nlayers=1, dropout=0.)\n",
        "        self.predicter = TransformerPredictor(out_dim, d_model//2, out_dim, d_head=d_head, nlayers=nlayers//2, dropout=0.)\n",
        "        import copy\n",
        "        self.target_encoder = copy.deepcopy(self.context_encoder)\n",
        "        self.target_encoder.requires_grad_(False)\n",
        "        self.classifier = nn.Linear(out_dim, 10)\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]\n",
        "        batch, seq, dim = x.shape\n",
        "        target_mask = randpatch(seq//self.patch_size, mask_size=16, gamma=.75) # [seq]\n",
        "        context_mask = ~multiblock(seq//self.patch_size, min_s=0.85, max_s=1., M=1)|target_mask # [1, seq], True->Mask\n",
        "        target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "\n",
        "        context_indices = (~context_mask[0]).nonzero().squeeze(-1) # int idx [num_context_toks] , idx of context not masked\n",
        "        sx = self.context_encoder(x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "\n",
        "        trg_indices = target_mask.nonzero().squeeze(-1) # int idx [1, num_trg_toks] , idx of targets that are masked\n",
        "        sy_ = self.predicter(sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "\n",
        "        # with torch.no_grad(): sy = self.target_encoder(x.detach())[torch.arange(batch).unsqueeze(-1), trg_indices.repeat(batch,1)] # [batch, num_trg_toks, out_dim]\n",
        "        with torch.no_grad():\n",
        "            sy = self.target_encoder(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "            # print(sy.shape, trg_indices.shape)\n",
        "            sy = apply_masks(sy, [trg_indices])\n",
        "\n",
        "        # print(x[0][0][:8])\n",
        "        # print(sy_[0][0][:8])\n",
        "        # print(sy.shape, sy_.shape)\n",
        "        # loss = F.smooth_l1_loss(sy, sy_)\n",
        "        loss = F.mse_loss(sy.detach(), sy_)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        sx = self.target_encoder(x)\n",
        "        out = sx.mean(dim=1)\n",
        "        return out\n",
        "\n",
        "\n",
        "    def classify(self, x): # [batch, T, 3]\n",
        "        sx = self.target_encoder(x).mean(dim=1)\n",
        "        out = self.classifier(sx)\n",
        "        return out\n",
        "\n",
        "# min_s=0.15, max_s, M\n",
        "# trg.15.2M4 C.85 1\n",
        "\n",
        "seq_jepa = SeqJEPA(in_dim=3, d_model=32, out_dim=16, nlayers=6, d_head=4).to(device)#.to(torch.float)\n",
        "# optim = torch.optim.AdamW(seq_jepa.parameters(), lr=1e-3) # 1e-3?\n",
        "optim = torch.optim.AdamW(seq_jepa.parameters(), lr=1e-3, weight_decay=1e-3) # lr1e-3? wd0\n",
        "print(sum(p.numel() for p in seq_jepa.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.parameters())) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.predicter.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.context_encoder.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.target_encoder.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# d_model^2 * nlayers\n",
        "\n",
        "x = torch.rand((2,1024,3), device=device)\n",
        "out = seq_jepa.loss(x)\n",
        "print(out.shape)\n",
        "# print(x.is_leaf) # T\n"
      ],
      "metadata": {
        "id": "xy4iC46Vnlog",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "097dd9f6-1dff-4edb-cc93-9a4d1eeafcfe"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47994\n",
            "torch.Size([])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(16).to(device)\n",
        "# coptim = torch.optim.AdamW(classifier.parameters(), lr=1e-3)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-4)\n",
        "# optim = torch.optim.AdamW([seq_jepa.parameters(), classifier.parameters()], lr=1e-3) # 1e-3?\n",
        "# optim = torch.optim.AdamW([{'params': seq_jepa.parameters()}, {'params': classifier.parameters(), 'lr': 1e-3}], lr=1e-3)\n"
      ],
      "metadata": {
        "id": "wjK1TVwBh2u8"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(seq_jepa.classifier.weight[0])\n",
        "# for n, p in seq_jepa.target_encoder.named_parameters():\n",
        "#     print(n,p)\n",
        "optim.param_groups[0]['lr'] = 1e-5\n"
      ],
      "metadata": {
        "id": "7IACKDymhit7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "34de8800-fa4b-4460-803c-3cd8dbfb6c70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91747de6-b6ed-4a46-f533-159cf25a9bed",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "strain 1.9725005626678467\n",
            "strain 1.4923175573349\n",
            "strain 2.4748504161834717\n",
            "strain 2.146953582763672\n",
            "strain 2.130214214324951\n",
            "strain 1.0188136100769043\n",
            "strain 2.0012104511260986\n",
            "strain 2.0318374633789062\n",
            "strain 0.23669365048408508\n",
            "strain 2.0906624794006348\n",
            "strain 0.9756841659545898\n",
            "strain 3.1644692420959473\n",
            "strain 0.5517369508743286\n",
            "strain 0.7892791032791138\n",
            "classify 2.40478515625\n",
            "classify 2.5474853515625\n",
            "classify 2.53765869140625\n",
            "classify 2.4656982421875\n",
            "classify 2.52545166015625\n",
            "classify 2.44598388671875\n",
            "classify 2.42584228515625\n",
            "classify 2.4417724609375\n",
            "classify 2.5081787109375\n",
            "classify 2.42352294921875\n",
            "classify 2.50543212890625\n",
            "0.0625\n",
            "0.109375\n",
            "0.0859375\n",
            "0.1484375\n",
            "0.09375\n",
            "0.125\n",
            "0.1328125\n",
            "0.078125\n",
            "0.0859375\n",
            "0.0625\n",
            "0.1171875\n",
            "strain 2.548215389251709\n",
            "strain 1.1648629903793335\n",
            "strain 1.1255807876586914\n",
            "strain 1.662674069404602\n",
            "strain 0.44187405705451965\n",
            "strain 0.5419080853462219\n",
            "strain 0.4615772068500519\n",
            "strain 1.9454289674758911\n",
            "strain 1.7061108350753784\n",
            "strain 1.0806082487106323\n",
            "strain 0.10757958889007568\n",
            "strain 1.778277039527893\n",
            "strain 2.828643321990967\n",
            "strain 0.6607523560523987\n",
            "strain 1.9429481029510498\n",
            "strain 1.8174989223480225\n",
            "strain 0.5859742760658264\n",
            "strain 1.7791821956634521\n",
            "strain 1.9904515743255615\n",
            "strain 1.5900250673294067\n",
            "strain 2.633511781692505\n",
            "strain 0.39975714683532715\n",
            "strain 1.4172919988632202\n",
            "strain 1.4357746839523315\n",
            "strain 0.13356363773345947\n",
            "strain 1.9633320569992065\n",
            "strain 0.08714045584201813\n",
            "strain 0.08719632029533386\n",
            "strain 0.4684613049030304\n",
            "strain 1.8085685968399048\n",
            "strain 2.2580316066741943\n",
            "strain 2.458712339401245\n",
            "strain 2.5526716709136963\n",
            "strain 1.6337428092956543\n",
            "strain 0.14918969571590424\n",
            "strain 1.2622156143188477\n",
            "strain 0.29594358801841736\n",
            "strain 1.9352396726608276\n",
            "strain 3.064817428588867\n",
            "strain 1.9001915454864502\n",
            "strain 2.363837480545044\n",
            "strain 1.5632129907608032\n",
            "strain 3.764150857925415\n",
            "strain 0.9291238784790039\n",
            "strain 1.1028211116790771\n",
            "strain 1.1486284732818604\n",
            "strain 0.13863907754421234\n",
            "strain 1.2589079141616821\n",
            "strain 1.550412654876709\n",
            "strain 1.226357340812683\n",
            "strain 1.7506697177886963\n",
            "classify 2.52923583984375\n",
            "classify 2.490234375\n",
            "classify 2.4462890625\n",
            "classify 2.59906005859375\n",
            "classify 2.42730712890625\n",
            "classify 2.455810546875\n",
            "classify 2.3497314453125\n",
            "classify 2.51458740234375\n",
            "classify 2.488037109375\n",
            "classify 2.402099609375\n",
            "classify 2.46099853515625\n",
            "0.0703125\n",
            "0.109375\n",
            "0.09375\n",
            "0.1484375\n",
            "0.1015625\n",
            "0.125\n",
            "0.1328125\n",
            "0.078125\n",
            "0.0859375\n",
            "0.0546875\n",
            "0.1171875\n",
            "strain 0.09162824600934982\n",
            "strain 1.6970720291137695\n",
            "strain 1.0469474792480469\n",
            "strain 0.7553510069847107\n",
            "strain 1.2030538320541382\n",
            "strain 0.3266165852546692\n",
            "strain 2.1520349979400635\n",
            "strain 0.23630239069461823\n",
            "strain 1.983022689819336\n",
            "strain 2.402205467224121\n",
            "strain 0.706943690776825\n",
            "strain 3.5036962032318115\n",
            "strain 1.5698480606079102\n",
            "strain 1.356015920639038\n",
            "strain 2.4169938564300537\n",
            "strain 1.4528303146362305\n",
            "strain 0.6116021275520325\n",
            "strain 1.1511882543563843\n",
            "strain 2.5594611167907715\n",
            "strain 4.310072422027588\n",
            "strain 2.0863900184631348\n",
            "strain 3.69158673286438\n",
            "strain 2.5501344203948975\n",
            "strain 2.3176753520965576\n",
            "strain 1.2337936162948608\n",
            "strain 3.5326929092407227\n",
            "strain 1.6256871223449707\n",
            "strain 0.9876920580863953\n",
            "strain 0.39865684509277344\n",
            "strain 2.2115070819854736\n",
            "strain 0.840691328048706\n",
            "strain 1.395323634147644\n",
            "strain 1.5073107481002808\n",
            "strain 1.9076130390167236\n",
            "strain 2.305119037628174\n",
            "strain 0.8115283846855164\n",
            "strain 0.8633392453193665\n",
            "strain 2.088406801223755\n",
            "strain 1.4040056467056274\n",
            "strain 2.158903121948242\n",
            "strain 1.8695199489593506\n",
            "strain 1.9008874893188477\n",
            "strain 1.7086797952651978\n",
            "strain 1.9102586507797241\n",
            "strain 0.466651052236557\n",
            "strain 2.685774564743042\n",
            "strain 1.3777779340744019\n",
            "strain 1.8039066791534424\n",
            "strain 1.4189258813858032\n",
            "strain 0.15112511813640594\n",
            "strain 1.206931471824646\n",
            "classify 2.504150390625\n",
            "classify 2.40301513671875\n",
            "classify 2.5211181640625\n",
            "classify 2.56341552734375\n",
            "classify 2.42596435546875\n",
            "classify 2.52899169921875\n",
            "classify 2.5501708984375\n",
            "classify 2.410888671875\n",
            "classify 2.48455810546875\n",
            "classify 2.4263916015625\n",
            "classify 2.47052001953125\n",
            "0.0625\n",
            "0.1015625\n",
            "0.0859375\n",
            "0.1484375\n",
            "0.09375\n",
            "0.125\n",
            "0.1328125\n",
            "0.078125\n",
            "0.0859375\n",
            "0.0546875\n",
            "0.1171875\n",
            "strain 1.3523820638656616\n",
            "strain 0.11577446013689041\n",
            "strain 1.881638526916504\n",
            "strain 2.5114948749542236\n",
            "strain 0.10872971266508102\n",
            "strain 1.5806307792663574\n",
            "strain 1.368374228477478\n",
            "strain 1.5289759635925293\n",
            "strain 1.2431517839431763\n",
            "strain 2.1364102363586426\n",
            "strain 2.2411677837371826\n",
            "strain 3.008610248565674\n",
            "strain 1.0471117496490479\n",
            "strain 2.7495148181915283\n",
            "strain 1.5229687690734863\n",
            "strain 1.0251131057739258\n",
            "strain 2.0930678844451904\n",
            "strain 2.634117603302002\n",
            "strain 1.0826079845428467\n",
            "strain 1.0254771709442139\n",
            "strain 0.22340689599514008\n",
            "strain 2.5444412231445312\n",
            "strain 0.6482036113739014\n",
            "strain 0.27651917934417725\n",
            "strain 2.558541774749756\n",
            "strain 2.1063530445098877\n",
            "strain 1.4498136043548584\n",
            "strain 0.8879408836364746\n",
            "strain 3.440861940383911\n",
            "strain 1.2258219718933105\n",
            "strain 0.3569832742214203\n",
            "strain 1.892551302909851\n",
            "strain 2.56860089302063\n",
            "strain 2.5968234539031982\n",
            "strain 2.4920523166656494\n",
            "strain 1.177370309829712\n",
            "strain 0.8403339385986328\n",
            "strain 0.3701232075691223\n",
            "strain 2.313394784927368\n",
            "strain 2.0641086101531982\n",
            "strain 0.14539355039596558\n",
            "strain 2.1215970516204834\n",
            "strain 3.5420567989349365\n",
            "strain 1.2613798379898071\n",
            "strain 0.741406261920929\n",
            "strain 1.2181708812713623\n",
            "strain 2.4118263721466064\n",
            "strain 0.7585234642028809\n",
            "strain 0.1257137656211853\n",
            "strain 3.1426119804382324\n",
            "strain 2.077393054962158\n",
            "classify 2.442901611328125\n",
            "classify 2.47283935546875\n",
            "classify 2.52294921875\n",
            "classify 2.5238037109375\n",
            "classify 2.476318359375\n",
            "classify 2.56085205078125\n",
            "classify 2.521087646484375\n",
            "classify 2.47442626953125\n",
            "classify 2.54522705078125\n",
            "classify 2.558013916015625\n",
            "classify 2.41314697265625\n",
            "0.0703125\n",
            "0.1015625\n",
            "0.09375\n",
            "0.140625\n",
            "0.09375\n",
            "0.125\n",
            "0.1328125\n",
            "0.0703125\n",
            "0.078125\n",
            "0.0625\n",
            "0.1171875\n",
            "strain 1.0890759229660034\n",
            "strain 1.449312448501587\n",
            "strain 1.3319591283798218\n",
            "strain 2.230111837387085\n",
            "strain 1.0997370481491089\n",
            "strain 2.6905930042266846\n",
            "strain 2.826030969619751\n",
            "strain 1.7861051559448242\n",
            "strain 0.9724113345146179\n",
            "strain 2.303831100463867\n",
            "strain 0.5099223852157593\n",
            "strain 0.1889163702726364\n",
            "strain 2.5308213233947754\n",
            "strain 2.17227840423584\n",
            "strain 1.4200584888458252\n",
            "strain 1.2781425714492798\n",
            "strain 1.2499816417694092\n",
            "strain 2.7577943801879883\n",
            "strain 1.7935059070587158\n",
            "strain 2.180417060852051\n",
            "strain 2.284707546234131\n",
            "strain 3.3355700969696045\n",
            "strain 1.628208041191101\n",
            "strain 0.3215302526950836\n",
            "strain 0.7316114902496338\n",
            "strain 0.2665841579437256\n",
            "strain 2.0595173835754395\n",
            "strain 1.3650115728378296\n",
            "strain 2.44118595123291\n",
            "strain 1.8839244842529297\n",
            "strain 0.2248823642730713\n",
            "strain 0.2966148555278778\n",
            "strain 1.806533694267273\n",
            "strain 2.535741090774536\n",
            "strain 2.031827211380005\n",
            "strain 1.6367896795272827\n",
            "strain 0.6616566181182861\n",
            "strain 1.7210274934768677\n",
            "strain 0.9394165277481079\n",
            "strain 2.191852331161499\n",
            "strain 2.042036771774292\n",
            "strain 1.7978127002716064\n",
            "strain 3.752803087234497\n",
            "strain 0.5890063047409058\n",
            "strain 2.0926854610443115\n",
            "strain 3.1321768760681152\n",
            "strain 0.14240270853042603\n",
            "strain 2.3239567279815674\n",
            "strain 2.524104595184326\n",
            "strain 2.5194520950317383\n",
            "strain 1.216689109802246\n",
            "classify 2.58489990234375\n",
            "classify 2.59039306640625\n",
            "classify 2.49822998046875\n",
            "classify 2.55499267578125\n",
            "classify 2.4344482421875\n",
            "classify 2.5570068359375\n",
            "classify 2.44207763671875\n",
            "classify 2.43792724609375\n",
            "classify 2.490478515625\n",
            "classify 2.45574951171875\n",
            "classify 2.49169921875\n",
            "0.0703125\n",
            "0.1015625\n",
            "0.0859375\n",
            "0.140625\n",
            "0.09375\n",
            "0.125\n",
            "0.1328125\n",
            "0.0703125\n",
            "0.078125\n",
            "0.0625\n",
            "0.1171875\n",
            "strain 0.40449774265289307\n",
            "strain 1.184626817703247\n",
            "strain 1.1209594011306763\n",
            "strain 2.7004268169403076\n",
            "strain 0.43521955609321594\n",
            "strain 2.7178151607513428\n",
            "strain 1.744577407836914\n",
            "strain 1.6058642864227295\n",
            "strain 2.4084362983703613\n",
            "strain 2.257657051086426\n",
            "strain 2.2995100021362305\n",
            "strain 2.793417453765869\n",
            "strain 0.2851255238056183\n",
            "strain 1.4146910905838013\n",
            "strain 2.621323585510254\n",
            "strain 2.403477191925049\n",
            "strain 1.2774168252944946\n",
            "strain 1.429910659790039\n",
            "strain 1.3612109422683716\n",
            "strain 2.5409908294677734\n",
            "strain 2.6570160388946533\n",
            "strain 0.20196959376335144\n",
            "strain 2.240504503250122\n",
            "strain 2.3296608924865723\n",
            "strain 2.315251111984253\n",
            "strain 1.6742353439331055\n",
            "strain 2.497992753982544\n",
            "strain 1.9955635070800781\n",
            "strain 1.4944934844970703\n",
            "strain 0.28900012373924255\n",
            "strain 3.435617208480835\n",
            "strain 2.0500359535217285\n",
            "strain 1.8070828914642334\n",
            "strain 1.8599534034729004\n",
            "strain 2.0939624309539795\n",
            "strain 0.1562914252281189\n",
            "strain 1.82395339012146\n",
            "strain 1.6805775165557861\n",
            "strain 1.2518013715744019\n",
            "strain 2.313180446624756\n",
            "strain 2.3193466663360596\n",
            "strain 2.6707749366760254\n",
            "strain 1.2840582132339478\n",
            "strain 0.12341323494911194\n",
            "strain 2.044093132019043\n",
            "strain 2.546088457107544\n",
            "strain 2.2529807090759277\n",
            "strain 2.0746960639953613\n",
            "strain 3.9001011848449707\n",
            "strain 3.4883768558502197\n",
            "strain 0.836622953414917\n",
            "classify 2.41046142578125\n",
            "classify 2.46343994140625\n",
            "classify 2.433349609375\n",
            "classify 2.57550048828125\n",
            "classify 2.55718994140625\n",
            "classify 2.54840087890625\n",
            "classify 2.5379638671875\n",
            "classify 2.46148681640625\n",
            "classify 2.45074462890625\n",
            "classify 2.49560546875\n",
            "classify 2.5435791015625\n",
            "0.0703125\n",
            "0.1015625\n",
            "0.0859375\n",
            "0.140625\n",
            "0.09375\n",
            "0.125\n",
            "0.125\n",
            "0.078125\n",
            "0.078125\n",
            "0.0625\n",
            "0.1171875\n",
            "strain 2.1592869758605957\n",
            "strain 2.197072982788086\n",
            "strain 1.7611455917358398\n",
            "strain 2.737276792526245\n",
            "strain 2.68809175491333\n",
            "strain 2.029088258743286\n",
            "strain 1.8393690586090088\n",
            "strain 0.7950069904327393\n",
            "strain 3.1137502193450928\n",
            "strain 2.466947317123413\n",
            "strain 2.3832664489746094\n",
            "strain 1.9469218254089355\n",
            "strain 2.4265224933624268\n",
            "strain 2.3343186378479004\n",
            "strain 1.6588300466537476\n",
            "strain 3.087315320968628\n",
            "strain 1.8165955543518066\n",
            "strain 2.45243501663208\n",
            "strain 4.082670211791992\n",
            "strain 2.1825625896453857\n",
            "strain 1.2332531213760376\n",
            "strain 2.903597354888916\n",
            "strain 2.4189467430114746\n",
            "strain 1.6780061721801758\n",
            "strain 0.5733814239501953\n",
            "strain 1.807420015335083\n",
            "strain 2.1288774013519287\n",
            "strain 2.286052703857422\n",
            "strain 2.340475559234619\n",
            "strain 0.4371301829814911\n",
            "strain 0.198805570602417\n",
            "strain 2.7281980514526367\n",
            "strain 0.16087836027145386\n",
            "strain 2.2358791828155518\n",
            "strain 2.358769178390503\n",
            "strain 2.0804641246795654\n",
            "strain 2.939302444458008\n",
            "strain 2.2097668647766113\n",
            "strain 2.4190664291381836\n",
            "strain 0.7842145562171936\n",
            "strain 0.1666918247938156\n",
            "strain 2.614762306213379\n",
            "strain 2.061286449432373\n",
            "strain 2.8346071243286133\n",
            "strain 3.816584348678589\n",
            "strain 1.5613038539886475\n",
            "strain 1.9304618835449219\n",
            "strain 1.691452980041504\n",
            "strain 2.134545087814331\n",
            "strain 2.23295521736145\n",
            "strain 2.431687593460083\n",
            "classify 2.437164306640625\n",
            "classify 2.46405029296875\n",
            "classify 2.49658203125\n",
            "classify 2.50665283203125\n",
            "classify 2.50201416015625\n",
            "classify 2.53192138671875\n",
            "classify 2.560302734375\n",
            "classify 2.5245361328125\n",
            "classify 2.49420166015625\n",
            "classify 2.48028564453125\n",
            "classify 2.45660400390625\n",
            "0.0703125\n",
            "0.09375\n",
            "0.0859375\n",
            "0.140625\n",
            "0.09375\n",
            "0.125\n",
            "0.125\n",
            "0.078125\n",
            "0.0859375\n",
            "0.0546875\n",
            "0.1171875\n",
            "strain 0.29939043521881104\n",
            "strain 1.734819769859314\n",
            "strain 1.015363097190857\n",
            "strain 2.027987480163574\n",
            "strain 1.073665976524353\n",
            "strain 2.300323247909546\n",
            "strain 3.1369380950927734\n",
            "strain 2.5161266326904297\n",
            "strain 2.084542989730835\n",
            "strain 2.3957624435424805\n",
            "strain 0.9566265344619751\n",
            "strain 1.4688180685043335\n",
            "strain 1.9787251949310303\n",
            "strain 2.1326513290405273\n",
            "strain 2.240044116973877\n",
            "strain 4.073025703430176\n",
            "strain 1.4248125553131104\n",
            "strain 2.7626209259033203\n",
            "strain 2.516608715057373\n",
            "strain 0.4321979582309723\n",
            "strain 0.2990747094154358\n",
            "strain 2.018916130065918\n",
            "strain 2.3141870498657227\n",
            "strain 2.2727930545806885\n",
            "strain 0.8943789601325989\n",
            "strain 3.031550884246826\n",
            "strain 0.898871123790741\n",
            "strain 0.3885323405265808\n",
            "strain 1.0610562562942505\n",
            "strain 1.6976343393325806\n",
            "strain 1.024636149406433\n",
            "strain 2.4634690284729004\n",
            "strain 2.099165201187134\n",
            "strain 2.0893309116363525\n",
            "strain 1.3615624904632568\n",
            "strain 3.390641689300537\n",
            "strain 2.3038766384124756\n",
            "strain 0.10006885230541229\n",
            "strain 1.8047984838485718\n",
            "strain 2.312608480453491\n",
            "strain 2.620683193206787\n",
            "strain 1.14850914478302\n",
            "strain 2.9246580600738525\n",
            "strain 0.6289678812026978\n",
            "strain 0.26669809222221375\n",
            "strain 1.8807123899459839\n",
            "strain 2.313558578491211\n",
            "strain 0.08891292661428452\n",
            "strain 0.490513414144516\n",
            "strain 1.7562575340270996\n",
            "strain 2.9496171474456787\n",
            "classify 2.4620361328125\n",
            "classify 2.384613037109375\n",
            "classify 2.62591552734375\n",
            "classify 2.595947265625\n",
            "classify 2.48907470703125\n",
            "classify 2.563720703125\n",
            "classify 2.48846435546875\n",
            "classify 2.5333251953125\n",
            "classify 2.549713134765625\n",
            "classify 2.50640869140625\n",
            "classify 2.547088623046875\n",
            "0.0703125\n",
            "0.1015625\n",
            "0.0859375\n",
            "0.140625\n",
            "0.09375\n",
            "0.125\n",
            "0.125\n",
            "0.078125\n",
            "0.0859375\n",
            "0.0546875\n",
            "0.1171875\n",
            "strain 0.49496519565582275\n",
            "strain 2.682084083557129\n",
            "strain 3.2368900775909424\n",
            "strain 2.095856189727783\n",
            "strain 0.374236524105072\n",
            "strain 2.981135606765747\n",
            "strain 3.180269479751587\n",
            "strain 2.9010536670684814\n",
            "strain 2.0355823040008545\n",
            "strain 2.303577184677124\n",
            "strain 2.2463722229003906\n",
            "strain 0.754871129989624\n",
            "strain 0.09270234405994415\n",
            "strain 2.3196218013763428\n",
            "strain 1.0927523374557495\n",
            "strain 0.1371549665927887\n",
            "strain 1.7269399166107178\n",
            "strain 2.184304714202881\n",
            "strain 2.5011374950408936\n",
            "strain 3.0247859954833984\n",
            "strain 2.353860855102539\n",
            "strain 0.46064484119415283\n",
            "strain 2.5091793537139893\n",
            "strain 0.09777097404003143\n",
            "strain 2.820326805114746\n",
            "strain 2.9718780517578125\n",
            "strain 3.3330087661743164\n",
            "strain 2.455625295639038\n",
            "strain 2.0836174488067627\n",
            "strain 2.0889437198638916\n",
            "strain 1.008548617362976\n",
            "strain 0.13289965689182281\n",
            "strain 1.734813928604126\n",
            "strain 2.076237440109253\n",
            "strain 0.2624965012073517\n",
            "strain 0.09346050024032593\n",
            "strain 2.1372711658477783\n",
            "strain 2.6847667694091797\n",
            "strain 1.6624293327331543\n",
            "strain 1.6722958087921143\n",
            "strain 0.6948036551475525\n",
            "strain 1.7936477661132812\n",
            "strain 1.6332110166549683\n",
            "strain 3.566037178039551\n",
            "strain 2.727079153060913\n",
            "strain 1.1976088285446167\n",
            "strain 2.0919265747070312\n",
            "strain 2.0898988246917725\n",
            "strain 2.53804087638855\n",
            "strain 0.6845053434371948\n",
            "strain 1.6477012634277344\n",
            "classify 2.4324951171875\n",
            "classify 2.60272216796875\n",
            "classify 2.54888916015625\n",
            "classify 2.5721435546875\n",
            "classify 2.53643798828125\n",
            "classify 2.4923095703125\n",
            "classify 2.51617431640625\n",
            "classify 2.501617431640625\n",
            "classify 2.359222412109375\n",
            "classify 2.52789306640625\n",
            "classify 2.599822998046875\n",
            "0.078125\n",
            "0.09375\n",
            "0.0859375\n",
            "0.140625\n",
            "0.0859375\n",
            "0.125\n",
            "0.1328125\n",
            "0.078125\n",
            "0.0859375\n",
            "0.0546875\n",
            "0.1171875\n",
            "strain 1.1632205247879028\n",
            "strain 6.539644718170166\n",
            "strain 2.4905433654785156\n",
            "strain 2.879270553588867\n",
            "strain 1.5825117826461792\n",
            "strain 2.4359753131866455\n",
            "strain 2.473461151123047\n",
            "strain 0.5400119423866272\n",
            "strain 2.6203815937042236\n",
            "strain 2.840648651123047\n",
            "strain 1.9203977584838867\n",
            "strain 0.8216876983642578\n",
            "strain 1.4348171949386597\n",
            "strain 1.4591583013534546\n",
            "strain 4.831875801086426\n",
            "strain 0.11904847621917725\n",
            "strain 1.4808491468429565\n",
            "strain 0.6888891458511353\n",
            "strain 5.174224376678467\n",
            "strain 1.5270963907241821\n",
            "strain 2.233219623565674\n",
            "strain 0.1572461724281311\n",
            "strain 2.4833738803863525\n",
            "strain 2.796727180480957\n",
            "strain 3.3646960258483887\n",
            "strain 2.8370306491851807\n",
            "strain 1.342955470085144\n",
            "strain 0.1177162155508995\n",
            "strain 0.4813549220561981\n",
            "strain 0.2906281650066376\n",
            "strain 0.9966102838516235\n",
            "strain 1.0285143852233887\n",
            "strain 2.3040356636047363\n",
            "strain 2.49017333984375\n",
            "strain 2.568401336669922\n",
            "strain 2.7483010292053223\n",
            "strain 2.8943891525268555\n",
            "strain 2.410818338394165\n",
            "strain 1.6417081356048584\n",
            "strain 2.86507248878479\n",
            "strain 2.7953948974609375\n",
            "strain 3.8704328536987305\n",
            "strain 1.9523041248321533\n",
            "strain 4.641168594360352\n",
            "strain 4.206528663635254\n",
            "strain 0.09674766659736633\n",
            "strain 2.3120830059051514\n",
            "strain 2.3713319301605225\n",
            "strain 1.1359342336654663\n",
            "strain 3.877225160598755\n",
            "strain 3.0253615379333496\n",
            "classify 2.501129150390625\n",
            "classify 2.49761962890625\n",
            "classify 2.57177734375\n",
            "classify 2.491851806640625\n",
            "classify 2.474761962890625\n",
            "classify 2.432769775390625\n",
            "classify 2.584381103515625\n",
            "classify 2.442718505859375\n",
            "classify 2.577728271484375\n",
            "classify 2.643157958984375\n",
            "classify 2.4473876953125\n",
            "0.078125\n",
            "0.09375\n",
            "0.0859375\n",
            "0.140625\n",
            "0.0859375\n",
            "0.125\n",
            "0.125\n",
            "0.078125\n",
            "0.0859375\n",
            "0.0625\n",
            "0.1171875\n",
            "strain 1.7931115627288818\n",
            "strain 3.157766342163086\n",
            "strain 1.5905152559280396\n",
            "strain 1.342617154121399\n",
            "strain 2.5168137550354004\n",
            "strain 0.92440265417099\n",
            "strain 1.709915041923523\n",
            "strain 1.665974736213684\n",
            "strain 1.303987741470337\n",
            "strain 2.1165857315063477\n",
            "strain 0.44409188628196716\n",
            "strain 2.717780590057373\n",
            "strain 2.429588794708252\n",
            "strain 1.159630537033081\n",
            "strain 3.551079511642456\n",
            "strain 0.31825000047683716\n",
            "strain 0.3240995705127716\n",
            "strain 2.1635990142822266\n",
            "strain 2.8517234325408936\n",
            "strain 2.8945226669311523\n",
            "strain 1.7131799459457397\n",
            "strain 2.23614239692688\n",
            "strain 1.7187445163726807\n",
            "strain 3.2809200286865234\n",
            "strain 0.7115378975868225\n",
            "strain 2.747147798538208\n",
            "strain 1.5519222021102905\n",
            "strain 0.5972791910171509\n",
            "strain 0.0947505384683609\n",
            "strain 0.16072049736976624\n",
            "strain 0.371154248714447\n",
            "strain 3.3127198219299316\n",
            "strain 0.6512922048568726\n",
            "strain 1.9328670501708984\n",
            "strain 0.5800478458404541\n",
            "strain 0.8986380696296692\n",
            "strain 2.356743335723877\n",
            "strain 1.9456361532211304\n",
            "strain 2.237156629562378\n",
            "strain 1.0716146230697632\n",
            "strain 3.7424910068511963\n",
            "strain 1.7605679035186768\n",
            "strain 1.8142341375350952\n",
            "strain 0.3757902681827545\n",
            "strain 0.07182798534631729\n",
            "strain 2.384338140487671\n",
            "strain 0.2893626093864441\n",
            "strain 0.13538287580013275\n",
            "strain 1.0627672672271729\n",
            "strain 0.0615549311041832\n",
            "strain 0.8951815962791443\n",
            "classify 2.47125244140625\n",
            "classify 2.529022216796875\n",
            "classify 2.510955810546875\n",
            "classify 2.502899169921875\n",
            "classify 2.482879638671875\n",
            "classify 2.494171142578125\n",
            "classify 2.532745361328125\n",
            "classify 2.574432373046875\n",
            "classify 2.50244140625\n",
            "classify 2.45526123046875\n",
            "classify 2.519805908203125\n",
            "0.078125\n",
            "0.09375\n",
            "0.0859375\n",
            "0.140625\n",
            "0.0859375\n",
            "0.125\n",
            "0.125\n",
            "0.078125\n",
            "0.0859375\n",
            "0.0625\n",
            "0.1171875\n",
            "strain 0.11669395118951797\n",
            "strain 0.767436683177948\n",
            "strain 2.6173293590545654\n",
            "strain 1.6459007263183594\n",
            "strain 0.04632063955068588\n",
            "strain 1.5372191667556763\n",
            "strain 0.6210561394691467\n",
            "strain 1.5659831762313843\n",
            "strain 2.451925754547119\n",
            "strain 0.0998806357383728\n",
            "strain 0.04879620671272278\n",
            "strain 1.8065427541732788\n",
            "strain 3.6725523471832275\n",
            "strain 0.6937143802642822\n",
            "strain 2.734286069869995\n",
            "strain 4.772304534912109\n",
            "strain 2.0688579082489014\n",
            "strain 0.7937332987785339\n",
            "strain 1.9016976356506348\n",
            "strain 0.9154103398323059\n",
            "strain 2.015920639038086\n",
            "strain 1.6764729022979736\n",
            "strain 0.2991674244403839\n",
            "strain 2.8344104290008545\n",
            "strain 2.5618865489959717\n",
            "strain 3.323335886001587\n",
            "strain 1.9673694372177124\n",
            "strain 1.5847114324569702\n",
            "strain 0.7118940949440002\n",
            "strain 0.7715746760368347\n",
            "strain 2.1940877437591553\n",
            "strain 3.0493004322052\n",
            "strain 1.138104796409607\n",
            "strain 2.329022169113159\n",
            "strain 3.0790810585021973\n",
            "strain 1.8757121562957764\n",
            "strain 3.0086891651153564\n",
            "strain 4.011178016662598\n",
            "strain 1.2034775018692017\n",
            "strain 1.5684928894042969\n",
            "strain 1.2053765058517456\n",
            "strain 3.821218490600586\n",
            "strain 2.4109714031219482\n",
            "strain 2.114224910736084\n",
            "strain 2.991992473602295\n",
            "strain 0.11832789331674576\n",
            "strain 1.4033225774765015\n",
            "strain 3.3412563800811768\n",
            "strain 0.8927541375160217\n",
            "strain 3.5002079010009766\n",
            "strain 4.115106105804443\n",
            "classify 2.496002197265625\n",
            "classify 2.53277587890625\n",
            "classify 2.570526123046875\n",
            "classify 2.642608642578125\n",
            "classify 2.423248291015625\n",
            "classify 2.537872314453125\n",
            "classify 2.513671875\n",
            "classify 2.567352294921875\n",
            "classify 2.5721435546875\n",
            "classify 2.5699462890625\n",
            "classify 2.458892822265625\n",
            "0.078125\n",
            "0.09375\n",
            "0.0859375\n",
            "0.140625\n",
            "0.0859375\n",
            "0.125\n",
            "0.1328125\n",
            "0.078125\n",
            "0.0859375\n",
            "0.0625\n",
            "0.1171875\n",
            "strain 2.0968899726867676\n",
            "strain 2.1844518184661865\n",
            "strain 1.211413025856018\n",
            "strain 1.3001072406768799\n",
            "strain 3.7223150730133057\n",
            "strain 2.5260024070739746\n",
            "strain 1.9957038164138794\n",
            "strain 2.164498805999756\n",
            "strain 1.4980239868164062\n",
            "strain 1.2127776145935059\n",
            "strain 2.5705819129943848\n",
            "strain 3.29884934425354\n",
            "strain 3.607968807220459\n",
            "strain 2.649984121322632\n",
            "strain 0.3569065034389496\n",
            "strain 3.1424217224121094\n",
            "strain 3.1296095848083496\n",
            "strain 1.725554347038269\n",
            "strain 3.1163480281829834\n",
            "strain 2.843060255050659\n",
            "strain 0.3446774184703827\n",
            "strain 0.26605653762817383\n",
            "strain 3.216012954711914\n",
            "strain 2.9827003479003906\n",
            "strain 2.478719711303711\n",
            "strain 1.2377760410308838\n",
            "strain 1.8117332458496094\n",
            "strain 2.2860302925109863\n",
            "strain 2.237502336502075\n",
            "strain 2.4469544887542725\n",
            "strain 1.1475344896316528\n",
            "strain 2.9589381217956543\n",
            "strain 0.47059643268585205\n",
            "strain 2.6745190620422363\n",
            "strain 1.9440207481384277\n",
            "strain 0.36294442415237427\n",
            "strain 0.7002237439155579\n",
            "strain 1.5455567836761475\n",
            "strain 0.47723448276519775\n",
            "strain 3.0633530616760254\n",
            "strain 2.5686089992523193\n",
            "strain 1.583858847618103\n",
            "strain 0.46726346015930176\n",
            "strain 1.4571256637573242\n",
            "strain 1.6602954864501953\n",
            "strain 6.18909215927124\n",
            "strain 2.8743042945861816\n",
            "strain 3.181882381439209\n",
            "strain 0.3370007276535034\n",
            "strain 3.4803125858306885\n",
            "strain 1.4805363416671753\n",
            "classify 2.625274658203125\n",
            "classify 2.435455322265625\n",
            "classify 2.529327392578125\n",
            "classify 2.600860595703125\n",
            "classify 2.577056884765625\n",
            "classify 2.446533203125\n",
            "classify 2.540313720703125\n",
            "classify 2.595947265625\n",
            "classify 2.533599853515625\n",
            "classify 2.594451904296875\n",
            "classify 2.53472900390625\n",
            "0.078125\n",
            "0.09375\n",
            "0.0859375\n",
            "0.140625\n",
            "0.0859375\n",
            "0.125\n",
            "0.1328125\n",
            "0.078125\n",
            "0.0859375\n",
            "0.0546875\n",
            "0.1171875\n",
            "strain 0.6766472458839417\n",
            "strain 3.8116488456726074\n",
            "strain 0.592854917049408\n",
            "strain 2.2270278930664062\n",
            "strain 2.6073110103607178\n",
            "strain 2.2340710163116455\n",
            "strain 2.020819664001465\n",
            "strain 2.0660762786865234\n",
            "strain 1.6503522396087646\n",
            "strain 3.5749146938323975\n",
            "strain 0.42046964168548584\n",
            "strain 1.8024617433547974\n",
            "strain 2.4502763748168945\n",
            "strain 2.9446616172790527\n",
            "strain 1.734399437904358\n",
            "strain 0.4837561547756195\n",
            "strain 1.5690921545028687\n",
            "strain 3.0413806438446045\n",
            "strain 0.34571245312690735\n",
            "strain 1.6896899938583374\n",
            "strain 2.278615713119507\n",
            "strain 0.23551242053508759\n",
            "strain 1.8861918449401855\n",
            "strain 1.3395076990127563\n",
            "strain 2.7353439331054688\n",
            "strain 3.6343865394592285\n",
            "strain 3.0534160137176514\n",
            "strain 1.8783608675003052\n",
            "strain 3.82490873336792\n",
            "strain 3.0856175422668457\n",
            "strain 2.711402177810669\n",
            "strain 0.27768707275390625\n",
            "strain 0.2522968053817749\n",
            "strain 3.1692824363708496\n",
            "strain 2.2351527214050293\n",
            "strain 2.98128080368042\n",
            "strain 2.840087413787842\n",
            "strain 5.281383514404297\n",
            "strain 2.7202115058898926\n",
            "strain 0.8422015905380249\n",
            "strain 2.4945571422576904\n",
            "strain 2.898192882537842\n",
            "strain 2.3561642169952393\n",
            "strain 2.545293092727661\n",
            "strain 3.110220193862915\n",
            "strain 2.8793885707855225\n",
            "strain 3.8372390270233154\n",
            "strain 0.7233231663703918\n",
            "strain 2.4067068099975586\n",
            "strain 1.239883303642273\n",
            "strain 2.795295000076294\n",
            "classify 2.575714111328125\n",
            "classify 2.50848388671875\n",
            "classify 2.430145263671875\n",
            "classify 2.5439453125\n",
            "classify 2.472625732421875\n",
            "classify 2.61517333984375\n",
            "classify 2.643524169921875\n",
            "classify 2.54449462890625\n",
            "classify 2.498748779296875\n",
            "classify 2.5997314453125\n",
            "classify 2.440704345703125\n",
            "0.078125\n",
            "0.09375\n",
            "0.0859375\n",
            "0.140625\n",
            "0.0859375\n",
            "0.125\n",
            "0.125\n",
            "0.078125\n",
            "0.0859375\n",
            "0.0546875\n",
            "0.1171875\n",
            "strain 3.4026379585266113\n",
            "strain 1.8807306289672852\n",
            "strain 1.9338563680648804\n",
            "strain 1.558125376701355\n",
            "strain 2.969606876373291\n",
            "strain 3.8198978900909424\n",
            "strain 1.7837891578674316\n",
            "strain 1.8615573644638062\n",
            "strain 0.4811689853668213\n",
            "strain 3.5043773651123047\n",
            "strain 0.7728701829910278\n",
            "strain 2.903545379638672\n",
            "strain 0.5824567675590515\n",
            "strain 2.3344926834106445\n",
            "strain 3.24078369140625\n",
            "strain 1.7199546098709106\n",
            "strain 3.779346227645874\n",
            "strain 2.6675984859466553\n",
            "strain 3.000542640686035\n",
            "strain 2.5159690380096436\n",
            "strain 1.165234923362732\n",
            "strain 0.6672179102897644\n",
            "strain 1.830566167831421\n",
            "strain 1.8040622472763062\n",
            "strain 2.08183217048645\n",
            "strain 2.947188377380371\n",
            "strain 3.6547048091888428\n",
            "strain 1.9857524633407593\n",
            "strain 2.448777914047241\n",
            "strain 3.9517900943756104\n",
            "strain 1.8746943473815918\n",
            "strain 2.6524741649627686\n",
            "strain 0.5948325395584106\n",
            "strain 0.8856668472290039\n",
            "strain 2.2260589599609375\n",
            "strain 3.671421766281128\n",
            "strain 0.6429627537727356\n",
            "strain 2.8412420749664307\n",
            "strain 1.4814499616622925\n",
            "strain 4.131143569946289\n",
            "strain 5.0724053382873535\n",
            "strain 5.58642578125\n",
            "strain 1.1181931495666504\n",
            "strain 3.8822245597839355\n",
            "strain 1.164502739906311\n",
            "strain 1.9716742038726807\n",
            "strain 2.5303966999053955\n",
            "strain 2.4363889694213867\n",
            "strain 2.1463587284088135\n",
            "strain 3.037871837615967\n",
            "strain 3.389573097229004\n",
            "classify 2.5328369140625\n",
            "classify 2.689788818359375\n",
            "classify 2.503265380859375\n",
            "classify 2.493316650390625\n",
            "classify 2.5999755859375\n",
            "classify 2.50665283203125\n",
            "classify 2.720306396484375\n",
            "classify 2.66143798828125\n",
            "classify 2.525238037109375\n",
            "classify 2.5479736328125\n",
            "classify 2.546966552734375\n",
            "0.078125\n",
            "0.09375\n",
            "0.0859375\n",
            "0.140625\n",
            "0.0859375\n",
            "0.125\n",
            "0.1171875\n",
            "0.078125\n",
            "0.078125\n",
            "0.0546875\n",
            "0.1171875\n",
            "strain 2.621772289276123\n",
            "strain 2.739614486694336\n",
            "strain 4.752679824829102\n",
            "strain 2.7139182090759277\n",
            "strain 3.4063191413879395\n",
            "strain 0.8480076193809509\n",
            "strain 2.8380074501037598\n",
            "strain 2.6733219623565674\n",
            "strain 2.602707624435425\n",
            "strain 2.814596176147461\n",
            "strain 0.3031372129917145\n",
            "strain 1.027296781539917\n",
            "strain 3.213395833969116\n",
            "strain 3.008817195892334\n",
            "strain 4.308716773986816\n",
            "strain 6.958827972412109\n",
            "strain 1.3454068899154663\n",
            "strain 3.936744213104248\n",
            "strain 4.647619724273682\n",
            "strain 1.6399255990982056\n",
            "strain 3.162299156188965\n",
            "strain 2.2162017822265625\n",
            "strain 0.6132187843322754\n",
            "strain 3.6126646995544434\n",
            "strain 2.977257490158081\n",
            "strain 2.9312028884887695\n",
            "strain 1.691927433013916\n",
            "strain 1.569920301437378\n",
            "strain 2.888923406600952\n",
            "strain 2.1752572059631348\n",
            "strain 2.9456851482391357\n",
            "strain 2.679786205291748\n",
            "strain 2.9719080924987793\n",
            "strain 3.301272392272949\n",
            "strain 1.9843313694000244\n",
            "strain 1.1708109378814697\n",
            "strain 1.530959963798523\n",
            "strain 2.0006346702575684\n",
            "strain 0.8630281686782837\n",
            "strain 3.250265598297119\n",
            "strain 1.946152687072754\n",
            "strain 1.510534405708313\n",
            "strain 1.2349889278411865\n",
            "strain 3.4681692123413086\n",
            "strain 2.2045116424560547\n",
            "strain 0.9833188652992249\n",
            "strain 2.652578353881836\n",
            "strain 2.916203737258911\n",
            "strain 2.0272960662841797\n",
            "strain 1.27535080909729\n",
            "strain 0.40916937589645386\n",
            "classify 2.690948486328125\n",
            "classify 2.58740234375\n",
            "classify 2.595428466796875\n",
            "classify 2.6060791015625\n",
            "classify 2.62860107421875\n",
            "classify 2.58636474609375\n",
            "classify 2.7381591796875\n",
            "classify 2.610137939453125\n",
            "classify 2.589111328125\n",
            "classify 2.54815673828125\n",
            "classify 2.52703857421875\n",
            "0.078125\n",
            "0.09375\n",
            "0.0859375\n",
            "0.140625\n",
            "0.0859375\n",
            "0.125\n",
            "0.1171875\n",
            "0.078125\n",
            "0.078125\n",
            "0.0546875\n",
            "0.1171875\n",
            "strain 0.5499336123466492\n",
            "strain 2.5160326957702637\n",
            "strain 3.2029502391815186\n",
            "strain 3.7467706203460693\n",
            "strain 1.547701358795166\n",
            "strain 3.3851311206817627\n",
            "strain 0.22189076244831085\n",
            "strain 2.7421538829803467\n",
            "strain 0.7983615398406982\n",
            "strain 2.7473933696746826\n",
            "strain 3.9871723651885986\n",
            "strain 0.17555439472198486\n",
            "strain 1.7440203428268433\n",
            "strain 2.5163190364837646\n",
            "strain 4.584910869598389\n",
            "strain 3.881441354751587\n",
            "strain 3.529360294342041\n",
            "strain 0.336744099855423\n",
            "strain 2.35422945022583\n",
            "strain 0.722192108631134\n",
            "strain 5.35156774520874\n",
            "strain 3.1418211460113525\n",
            "strain 1.1056103706359863\n",
            "strain 1.2530384063720703\n",
            "strain 0.8018643260002136\n",
            "strain 1.479179859161377\n",
            "strain 3.9024789333343506\n",
            "strain 3.0024542808532715\n",
            "strain 1.7471303939819336\n",
            "strain 3.464258909225464\n",
            "strain 3.130992889404297\n",
            "strain 3.510289192199707\n",
            "strain 3.0793521404266357\n",
            "strain 4.048583984375\n",
            "strain 0.5595836043357849\n",
            "strain 2.9586551189422607\n",
            "strain 0.6283882856369019\n",
            "strain 1.2884193658828735\n",
            "strain 2.2914938926696777\n",
            "strain 3.0100951194763184\n",
            "strain 2.731109619140625\n",
            "strain 2.913245677947998\n",
            "strain 1.2713909149169922\n",
            "strain 2.065248966217041\n",
            "strain 2.9675230979919434\n",
            "strain 1.5763205289840698\n",
            "strain 1.013439416885376\n",
            "strain 0.7572648525238037\n",
            "strain 3.797855854034424\n",
            "strain 3.4021482467651367\n",
            "strain 3.261497974395752\n",
            "classify 2.48345947265625\n",
            "classify 2.614898681640625\n",
            "classify 2.45709228515625\n",
            "classify 2.55914306640625\n",
            "classify 2.5391845703125\n",
            "classify 2.521240234375\n",
            "classify 2.498291015625\n",
            "classify 2.444732666015625\n",
            "classify 2.44091796875\n",
            "classify 2.566558837890625\n",
            "classify 2.583099365234375\n",
            "0.0703125\n",
            "0.09375\n",
            "0.0859375\n",
            "0.140625\n",
            "0.0859375\n",
            "0.125\n",
            "0.1171875\n",
            "0.078125\n",
            "0.078125\n",
            "0.0546875\n",
            "0.1171875\n",
            "strain 2.8272738456726074\n",
            "strain 3.059243679046631\n",
            "strain 2.220500946044922\n",
            "strain 2.748311996459961\n",
            "strain 3.366342306137085\n",
            "strain 4.316388130187988\n",
            "strain 6.112374305725098\n",
            "strain 0.24924993515014648\n",
            "strain 3.188977003097534\n",
            "strain 2.307569742202759\n",
            "strain 2.0375447273254395\n",
            "strain 1.4353079795837402\n",
            "strain 2.4302477836608887\n",
            "strain 0.5965856909751892\n",
            "strain 1.4908199310302734\n",
            "strain 3.1938023567199707\n",
            "strain 0.7569945454597473\n",
            "strain 1.745671272277832\n",
            "strain 0.874818742275238\n",
            "strain 1.959851622581482\n",
            "strain 1.0356957912445068\n",
            "strain 0.9123854637145996\n",
            "strain 2.618091106414795\n",
            "strain 4.149744987487793\n",
            "strain 3.1955089569091797\n",
            "strain 0.2532857656478882\n",
            "strain 3.454331159591675\n",
            "strain 0.6797860264778137\n",
            "strain 2.2123401165008545\n",
            "strain 2.419680595397949\n",
            "strain 3.8197498321533203\n",
            "strain 2.7490720748901367\n",
            "strain 2.1322267055511475\n",
            "strain 4.4540324211120605\n",
            "strain 1.6942180395126343\n",
            "strain 4.677501678466797\n",
            "strain 1.6363238096237183\n",
            "strain 0.36726143956184387\n",
            "strain 1.4972470998764038\n",
            "strain 2.2189507484436035\n",
            "strain 0.12147842347621918\n",
            "strain 1.4654241800308228\n",
            "strain 0.19786256551742554\n",
            "strain 2.1964735984802246\n",
            "strain 2.2699506282806396\n",
            "strain 1.1921342611312866\n",
            "strain 2.6627604961395264\n",
            "strain 5.554558753967285\n",
            "strain 2.0857527256011963\n",
            "strain 1.3993704319000244\n",
            "strain 0.12238339334726334\n",
            "classify 2.562103271484375\n",
            "classify 2.555938720703125\n",
            "classify 2.54718017578125\n",
            "classify 2.392547607421875\n",
            "classify 2.659332275390625\n",
            "classify 2.654754638671875\n",
            "classify 2.541961669921875\n",
            "classify 2.596221923828125\n",
            "classify 2.421173095703125\n",
            "classify 2.575531005859375\n",
            "classify 2.644744873046875\n",
            "0.0703125\n",
            "0.09375\n",
            "0.0859375\n",
            "0.140625\n",
            "0.09375\n",
            "0.125\n",
            "0.125\n",
            "0.078125\n",
            "0.078125\n",
            "0.0546875\n",
            "0.1171875\n",
            "strain 5.872201442718506\n",
            "strain 0.3123592138290405\n",
            "strain 1.752394437789917\n",
            "strain 3.07075834274292\n",
            "strain 2.2288708686828613\n",
            "strain 1.8765068054199219\n",
            "strain 3.418524980545044\n",
            "strain 3.053283929824829\n",
            "strain 0.1052263155579567\n",
            "strain 2.7330880165100098\n",
            "strain 3.190537929534912\n",
            "strain 4.075107574462891\n",
            "strain 0.7728632688522339\n",
            "strain 0.12203754484653473\n",
            "strain 1.636897087097168\n",
            "strain 2.673583507537842\n",
            "strain 2.7829463481903076\n",
            "strain 3.061047077178955\n",
            "strain 0.0964442640542984\n",
            "strain 1.916682243347168\n",
            "strain 0.9651284217834473\n",
            "strain 4.1126909255981445\n",
            "strain 2.584315061569214\n",
            "strain 2.8059463500976562\n",
            "strain 1.2726459503173828\n",
            "strain 2.5282883644104004\n",
            "strain 3.705085277557373\n",
            "strain 3.502011775970459\n",
            "strain 0.6301287412643433\n",
            "strain 3.5516865253448486\n",
            "strain 3.118934392929077\n",
            "strain 1.561586618423462\n",
            "strain 0.5483109951019287\n",
            "strain 0.4100058078765869\n",
            "strain 1.8350640535354614\n",
            "strain 1.1245087385177612\n",
            "strain 2.562746286392212\n",
            "strain 0.6379676461219788\n",
            "strain 2.463470935821533\n",
            "strain 1.1787351369857788\n",
            "strain 0.4483959674835205\n",
            "strain 2.0493314266204834\n",
            "strain 2.447678327560425\n",
            "strain 1.4485172033309937\n",
            "strain 7.161139965057373\n",
            "strain 2.4538676738739014\n",
            "strain 3.0241899490356445\n",
            "strain 2.044670820236206\n",
            "strain 0.08064842224121094\n",
            "strain 2.1412880420684814\n",
            "strain 1.202257513999939\n",
            "classify 2.6251220703125\n",
            "classify 2.579620361328125\n",
            "classify 2.596466064453125\n",
            "classify 2.673309326171875\n",
            "classify 2.606231689453125\n",
            "classify 2.5780029296875\n",
            "classify 2.621917724609375\n",
            "classify 2.57220458984375\n",
            "classify 2.705108642578125\n",
            "classify 2.65863037109375\n",
            "classify 2.6669921875\n",
            "0.0625\n",
            "0.09375\n",
            "0.0859375\n",
            "0.140625\n",
            "0.09375\n",
            "0.125\n",
            "0.125\n",
            "0.078125\n",
            "0.078125\n",
            "0.0546875\n",
            "0.1171875\n",
            "strain 2.8241357803344727\n",
            "strain 0.14658322930335999\n",
            "strain 1.54452383518219\n",
            "strain 5.0530619621276855\n",
            "strain 4.390837669372559\n",
            "strain 3.7486588954925537\n",
            "strain 3.314608573913574\n",
            "strain 1.2063394784927368\n",
            "strain 1.1161202192306519\n",
            "strain 2.5203399658203125\n",
            "strain 2.398015022277832\n",
            "strain 6.3094658851623535\n",
            "strain 5.217082500457764\n",
            "strain 3.6465282440185547\n",
            "strain 2.0626823902130127\n",
            "strain 0.8172351121902466\n",
            "strain 2.1240475177764893\n",
            "strain 1.7289869785308838\n",
            "strain 4.384760856628418\n",
            "strain 2.705561399459839\n",
            "strain 1.8984426259994507\n",
            "strain 0.6878288984298706\n",
            "strain 2.3247883319854736\n",
            "strain 7.070248126983643\n",
            "strain 2.671813488006592\n",
            "strain 3.891808271408081\n",
            "strain 4.18138313293457\n",
            "strain 1.2927751541137695\n",
            "strain 0.8479365706443787\n",
            "strain 3.6111257076263428\n",
            "strain 4.46783447265625\n",
            "strain 3.097524642944336\n",
            "strain 2.967580556869507\n",
            "strain 1.767563819885254\n",
            "strain 4.694244384765625\n",
            "strain 3.1288013458251953\n",
            "strain 1.146675705909729\n",
            "strain 2.2014880180358887\n",
            "strain 3.799374580383301\n",
            "strain 0.9014127850532532\n",
            "strain 2.1893043518066406\n",
            "strain 2.9951212406158447\n",
            "strain 4.933584690093994\n",
            "strain 4.359957695007324\n",
            "strain 2.6175906658172607\n",
            "strain 0.7520384192466736\n",
            "strain 2.359639883041382\n",
            "strain 3.235994338989258\n",
            "strain 4.2394609451293945\n",
            "strain 3.9113261699676514\n",
            "strain 0.5788225531578064\n",
            "classify 2.656768798828125\n",
            "classify 2.558135986328125\n",
            "classify 2.594757080078125\n",
            "classify 2.63092041015625\n",
            "classify 2.6185302734375\n",
            "classify 2.545257568359375\n",
            "classify 2.457672119140625\n",
            "classify 2.473785400390625\n",
            "classify 2.6356201171875\n",
            "classify 2.540191650390625\n",
            "classify 2.534576416015625\n",
            "0.078125\n",
            "0.09375\n",
            "0.0859375\n",
            "0.140625\n",
            "0.0859375\n",
            "0.125\n",
            "0.1171875\n",
            "0.078125\n",
            "0.078125\n",
            "0.0546875\n",
            "0.1171875\n",
            "strain 0.2924501597881317\n",
            "strain 4.528817176818848\n",
            "strain 0.36337828636169434\n",
            "strain 0.9580934047698975\n",
            "strain 2.9843151569366455\n",
            "strain 0.4251419007778168\n",
            "strain 1.7911498546600342\n",
            "strain 3.119750499725342\n",
            "strain 4.323367595672607\n",
            "strain 2.4609882831573486\n",
            "strain 2.865722179412842\n",
            "strain 4.40075159072876\n",
            "strain 0.4363887310028076\n",
            "strain 2.8674168586730957\n",
            "strain 0.5763137936592102\n",
            "strain 2.4142873287200928\n",
            "strain 3.5864553451538086\n",
            "strain 1.8751153945922852\n",
            "strain 0.18537889420986176\n",
            "strain 2.2152533531188965\n",
            "strain 3.0081119537353516\n",
            "strain 3.0168232917785645\n",
            "strain 4.15654182434082\n",
            "strain 3.7044084072113037\n",
            "strain 3.741084098815918\n",
            "strain 2.6897590160369873\n",
            "strain 0.9965704679489136\n",
            "strain 4.221522331237793\n",
            "strain 5.758359432220459\n",
            "strain 3.802513837814331\n",
            "strain 3.7273778915405273\n",
            "strain 4.328519821166992\n",
            "strain 1.2060558795928955\n",
            "strain 2.58148455619812\n",
            "strain 3.823108196258545\n",
            "strain 2.587411642074585\n",
            "strain 2.00302791595459\n",
            "strain 0.8482331037521362\n",
            "strain 3.887666940689087\n",
            "strain 3.687258005142212\n",
            "strain 3.793428897857666\n",
            "strain 4.672858715057373\n",
            "strain 2.9855358600616455\n",
            "strain 3.58693265914917\n",
            "strain 1.4038795232772827\n",
            "strain 0.2867181897163391\n",
            "strain 2.2224690914154053\n",
            "strain 0.8602416515350342\n",
            "strain 2.3101186752319336\n",
            "strain 2.6722097396850586\n",
            "strain 3.7286453247070312\n",
            "classify 2.550628662109375\n",
            "classify 2.673736572265625\n",
            "classify 2.561798095703125\n",
            "classify 2.5054931640625\n",
            "classify 2.559417724609375\n",
            "classify 2.6639404296875\n",
            "classify 2.593048095703125\n",
            "classify 2.62921142578125\n",
            "classify 2.65771484375\n",
            "classify 2.5416259765625\n",
            "classify 2.525543212890625\n",
            "0.078125\n",
            "0.09375\n",
            "0.0859375\n",
            "0.140625\n",
            "0.0859375\n",
            "0.125\n",
            "0.1171875\n",
            "0.078125\n",
            "0.078125\n",
            "0.0546875\n",
            "0.1171875\n",
            "strain 1.2498878240585327\n",
            "strain 3.214609146118164\n",
            "strain 2.8634612560272217\n",
            "strain 3.2663609981536865\n",
            "strain 2.1304574012756348\n",
            "strain 0.25985416769981384\n",
            "strain 3.3370063304901123\n",
            "strain 4.424675941467285\n",
            "strain 3.668386459350586\n",
            "strain 1.3011281490325928\n",
            "strain 4.035405158996582\n",
            "strain 2.9149398803710938\n",
            "strain 2.6856167316436768\n",
            "strain 2.894151210784912\n",
            "strain 0.9609743356704712\n",
            "strain 3.8296446800231934\n",
            "strain 1.6939504146575928\n",
            "strain 3.2349393367767334\n",
            "strain 3.701221466064453\n",
            "strain 3.4396839141845703\n",
            "strain 2.8580219745635986\n",
            "strain 2.901954174041748\n",
            "strain 2.495288133621216\n",
            "strain 0.6630984544754028\n",
            "strain 2.364797353744507\n",
            "strain 0.41394680738449097\n",
            "strain 2.611046075820923\n",
            "strain 4.681769371032715\n",
            "strain 5.240591049194336\n",
            "strain 0.22230692207813263\n",
            "strain 0.25529518723487854\n",
            "strain 0.22212280333042145\n",
            "strain 1.2576887607574463\n",
            "strain 1.5320299863815308\n",
            "strain 0.8942208290100098\n",
            "strain 1.600246548652649\n",
            "strain 4.284099578857422\n",
            "strain 2.567440986633301\n",
            "strain 3.163469076156616\n",
            "strain 3.7435712814331055\n",
            "strain 4.146210670471191\n",
            "strain 2.3711044788360596\n",
            "strain 1.8383716344833374\n",
            "strain 1.7096877098083496\n",
            "strain 2.190042018890381\n",
            "strain 3.417050361633301\n",
            "strain 4.084954738616943\n",
            "strain 2.138791799545288\n",
            "strain 1.917411208152771\n",
            "strain 1.5471069812774658\n",
            "strain 2.209115982055664\n",
            "classify 2.581756591796875\n",
            "classify 2.5797119140625\n",
            "classify 2.70501708984375\n",
            "classify 2.621429443359375\n",
            "classify 2.710601806640625\n",
            "classify 2.68621826171875\n",
            "classify 2.586029052734375\n",
            "classify 2.55450439453125\n",
            "classify 2.72882080078125\n",
            "classify 2.575531005859375\n",
            "classify 2.65386962890625\n",
            "0.078125\n",
            "0.1015625\n",
            "0.0859375\n",
            "0.140625\n",
            "0.09375\n",
            "0.125\n",
            "0.125\n",
            "0.078125\n",
            "0.078125\n",
            "0.0546875\n",
            "0.1171875\n",
            "strain 4.554955959320068\n",
            "strain 4.238173007965088\n",
            "strain 2.903182029724121\n",
            "strain 3.0207157135009766\n",
            "strain 0.1961781233549118\n",
            "strain 3.865569591522217\n",
            "strain 0.10334950685501099\n",
            "strain 3.3571484088897705\n",
            "strain 2.5456714630126953\n",
            "strain 4.021684646606445\n",
            "strain 2.8119373321533203\n",
            "strain 3.9402780532836914\n",
            "strain 4.199770927429199\n",
            "strain 4.299429416656494\n",
            "strain 1.2676023244857788\n",
            "strain 3.726027250289917\n",
            "strain 1.6275697946548462\n",
            "strain 2.958829879760742\n",
            "strain 1.0564430952072144\n",
            "strain 1.305967926979065\n",
            "strain 2.243042469024658\n",
            "strain 2.3938777446746826\n",
            "strain 4.663935661315918\n",
            "strain 2.4324631690979004\n",
            "strain 5.281408786773682\n",
            "strain 0.327457994222641\n",
            "strain 4.425753593444824\n",
            "strain 0.6852036714553833\n",
            "strain 4.038572788238525\n",
            "strain 3.897735118865967\n",
            "strain 0.3336995542049408\n",
            "strain 0.7589520812034607\n",
            "strain 3.284538984298706\n",
            "strain 7.871794700622559\n",
            "strain 3.5226712226867676\n",
            "strain 1.411874532699585\n",
            "strain 2.3370718955993652\n",
            "strain 6.272799491882324\n",
            "strain 2.635728597640991\n",
            "strain 3.55277943611145\n",
            "strain 0.696965754032135\n",
            "strain 1.4617280960083008\n",
            "strain 2.6030521392822266\n",
            "strain 3.9692037105560303\n",
            "strain 3.236585855484009\n",
            "strain 3.701967716217041\n",
            "strain 2.6791961193084717\n",
            "strain 6.678849220275879\n",
            "strain 2.852625608444214\n",
            "strain 0.12799257040023804\n",
            "strain 7.057218551635742\n",
            "classify 2.707611083984375\n",
            "classify 2.734649658203125\n",
            "classify 2.595123291015625\n",
            "classify 2.50177001953125\n",
            "classify 2.532135009765625\n",
            "classify 2.6588134765625\n",
            "classify 2.539306640625\n",
            "classify 2.654693603515625\n",
            "classify 2.6234130859375\n",
            "classify 2.574462890625\n",
            "classify 2.609588623046875\n",
            "0.078125\n",
            "0.1015625\n",
            "0.0859375\n",
            "0.140625\n",
            "0.09375\n",
            "0.125\n",
            "0.1171875\n",
            "0.0859375\n",
            "0.078125\n",
            "0.0546875\n",
            "0.1171875\n",
            "strain 2.0577352046966553\n",
            "strain 1.7862097024917603\n",
            "strain 0.9379183650016785\n",
            "strain 0.1209322065114975\n",
            "strain 0.16293756663799286\n",
            "strain 4.311005592346191\n",
            "strain 2.3991711139678955\n",
            "strain 3.9931540489196777\n",
            "strain 3.51842999458313\n",
            "strain 3.078643560409546\n",
            "strain 5.229872703552246\n",
            "strain 3.171696662902832\n",
            "strain 3.455862045288086\n",
            "strain 6.409704208374023\n",
            "strain 3.363668203353882\n",
            "strain 2.0491769313812256\n",
            "strain 1.16323983669281\n",
            "strain 4.334681510925293\n",
            "strain 0.7641035318374634\n",
            "strain 1.7837226390838623\n",
            "strain 0.16184081137180328\n",
            "strain 0.33773860335350037\n",
            "strain 2.8470492362976074\n",
            "strain 3.574418544769287\n",
            "strain 2.274700403213501\n",
            "strain 2.3130035400390625\n",
            "strain 0.6259255409240723\n",
            "strain 4.204580783843994\n",
            "strain 3.897531509399414\n",
            "strain 3.924140453338623\n",
            "strain 1.6030726432800293\n",
            "strain 0.39696067571640015\n",
            "strain 3.6221911907196045\n",
            "strain 2.860806941986084\n",
            "strain 3.0019478797912598\n",
            "strain 4.2383713722229\n",
            "strain 3.317697286605835\n",
            "strain 2.954998254776001\n",
            "strain 2.410984992980957\n",
            "strain 2.322819709777832\n",
            "strain 2.5754611492156982\n",
            "strain 3.0098304748535156\n",
            "strain 1.669202208518982\n",
            "strain 2.523364543914795\n",
            "strain 2.9907422065734863\n",
            "strain 3.2101001739501953\n",
            "strain 0.9519307613372803\n",
            "strain 2.3248770236968994\n",
            "strain 3.8146185874938965\n",
            "strain 0.516708254814148\n",
            "strain 1.647838830947876\n",
            "classify 2.598724365234375\n",
            "classify 2.500823974609375\n",
            "classify 2.60968017578125\n",
            "classify 2.623138427734375\n",
            "classify 2.557403564453125\n",
            "classify 2.580596923828125\n",
            "classify 2.557525634765625\n",
            "classify 2.514434814453125\n",
            "classify 2.651214599609375\n",
            "classify 2.54571533203125\n",
            "classify 2.5555419921875\n",
            "0.078125\n",
            "0.09375\n",
            "0.0859375\n",
            "0.140625\n",
            "0.0859375\n",
            "0.125\n",
            "0.1328125\n",
            "0.078125\n",
            "0.078125\n",
            "0.0546875\n",
            "0.1171875\n",
            "strain 1.1539543867111206\n",
            "strain 0.09374623000621796\n",
            "strain 0.6095787882804871\n",
            "strain 2.430246353149414\n",
            "strain 4.133143901824951\n",
            "strain 3.1937777996063232\n",
            "strain 2.572783946990967\n",
            "strain 3.311006546020508\n",
            "strain 1.6848773956298828\n",
            "strain 2.2104530334472656\n",
            "strain 1.4302122592926025\n",
            "strain 2.3962459564208984\n",
            "strain 1.0857881307601929\n",
            "strain 0.8056408166885376\n",
            "strain 2.0199596881866455\n",
            "strain 2.498157262802124\n",
            "strain 3.2013533115386963\n",
            "strain 0.7773503661155701\n",
            "strain 3.266340970993042\n",
            "strain 3.4708175659179688\n",
            "strain 3.903700351715088\n",
            "strain 4.610240936279297\n",
            "strain 3.4835410118103027\n",
            "strain 2.1043355464935303\n",
            "strain 1.3286056518554688\n",
            "strain 0.07018758356571198\n",
            "strain 4.137378692626953\n",
            "strain 0.12408319860696793\n",
            "strain 0.11344043165445328\n",
            "strain 1.4045875072479248\n",
            "strain 6.2184858322143555\n",
            "strain 0.3546207845211029\n",
            "strain 2.737276554107666\n",
            "strain 0.5136560201644897\n",
            "strain 3.8865134716033936\n",
            "strain 1.925576090812683\n",
            "strain 3.660505771636963\n",
            "strain 0.6740732192993164\n",
            "strain 3.679922103881836\n",
            "strain 0.06413810700178146\n",
            "strain 3.2474892139434814\n",
            "strain 7.199982643127441\n",
            "strain 2.252316474914551\n",
            "strain 2.2096705436706543\n",
            "strain 6.190423488616943\n",
            "strain 4.905669689178467\n",
            "strain 0.8314859867095947\n",
            "strain 0.48083919286727905\n",
            "strain 3.1061692237854004\n",
            "strain 0.10108271986246109\n",
            "strain 2.2706196308135986\n",
            "classify 2.7183837890625\n",
            "classify 2.54644775390625\n",
            "classify 2.619964599609375\n",
            "classify 2.508544921875\n",
            "classify 2.6129150390625\n",
            "classify 2.539642333984375\n",
            "classify 2.536163330078125\n",
            "classify 2.55450439453125\n",
            "classify 2.6455078125\n",
            "classify 2.588958740234375\n",
            "classify 2.4793701171875\n",
            "0.078125\n",
            "0.1015625\n",
            "0.0859375\n",
            "0.1484375\n",
            "0.0859375\n",
            "0.125\n",
            "0.125\n",
            "0.0859375\n",
            "0.078125\n",
            "0.0546875\n",
            "0.1171875\n",
            "strain 3.009505271911621\n",
            "strain 3.5211143493652344\n",
            "strain 0.5312408208847046\n",
            "strain 1.6774861812591553\n",
            "strain 3.3134069442749023\n",
            "strain 4.693804740905762\n",
            "strain 5.420444965362549\n",
            "strain 1.6911289691925049\n",
            "strain 0.6448789834976196\n",
            "strain 0.19464735686779022\n",
            "strain 4.149472713470459\n",
            "strain 2.400545358657837\n",
            "strain 3.761631727218628\n",
            "strain 4.703209400177002\n",
            "strain 2.191694498062134\n",
            "strain 1.6798450946807861\n",
            "strain 0.4905327260494232\n",
            "strain 2.626805543899536\n",
            "strain 1.8457797765731812\n",
            "strain 2.625572919845581\n",
            "strain 1.1887774467468262\n",
            "strain 2.7962968349456787\n",
            "strain 2.9722490310668945\n",
            "strain 4.059863090515137\n",
            "strain 3.588080883026123\n",
            "strain 2.9340386390686035\n",
            "strain 3.9508326053619385\n",
            "strain 5.148558616638184\n",
            "strain 2.6363885402679443\n",
            "strain 4.065749645233154\n",
            "strain 4.304537296295166\n",
            "strain 2.9231128692626953\n",
            "strain 4.097531795501709\n",
            "strain 1.3825000524520874\n",
            "strain 2.7124552726745605\n",
            "strain 2.057162284851074\n",
            "strain 2.0031259059906006\n",
            "strain 3.746152877807617\n",
            "strain 3.868288516998291\n",
            "strain 2.667607307434082\n",
            "strain 4.795117378234863\n",
            "strain 1.826573133468628\n",
            "strain 0.43962836265563965\n",
            "strain 0.633676290512085\n",
            "strain 3.4455718994140625\n",
            "strain 4.640419960021973\n",
            "strain 1.020998239517212\n",
            "strain 0.6357073187828064\n",
            "strain 2.11375093460083\n",
            "strain 2.946993827819824\n",
            "strain 3.792032480239868\n",
            "classify 2.55279541015625\n",
            "classify 2.650909423828125\n",
            "classify 2.735687255859375\n",
            "classify 2.641937255859375\n",
            "classify 2.68328857421875\n",
            "classify 2.68194580078125\n",
            "classify 2.5997314453125\n",
            "classify 2.496124267578125\n",
            "classify 2.577239990234375\n",
            "classify 2.59344482421875\n",
            "classify 2.566802978515625\n",
            "0.078125\n",
            "0.109375\n",
            "0.0859375\n",
            "0.1484375\n",
            "0.0859375\n",
            "0.125\n",
            "0.125\n",
            "0.0859375\n",
            "0.078125\n",
            "0.0546875\n",
            "0.1171875\n",
            "strain 2.9269440174102783\n",
            "strain 1.5921857357025146\n",
            "strain 2.7828357219696045\n",
            "strain 3.0199127197265625\n",
            "strain 1.4246563911437988\n",
            "strain 0.9386460185050964\n",
            "strain 4.67164945602417\n",
            "strain 1.646148443222046\n",
            "strain 0.2920667231082916\n",
            "strain 4.92378044128418\n",
            "strain 4.199623107910156\n",
            "strain 0.222383052110672\n",
            "strain 2.180082321166992\n",
            "strain 2.827756643295288\n",
            "strain 1.7361254692077637\n",
            "strain 3.1166203022003174\n",
            "strain 6.416159152984619\n",
            "strain 1.0753648281097412\n",
            "strain 2.34610915184021\n",
            "strain 2.272401809692383\n",
            "strain 2.974733829498291\n",
            "strain 4.851367950439453\n",
            "strain 2.6665589809417725\n",
            "strain 2.3731086254119873\n",
            "strain 3.3656928539276123\n",
            "strain 3.3554847240448\n",
            "strain 4.278984069824219\n",
            "strain 3.968407154083252\n",
            "strain 1.8750779628753662\n",
            "strain 1.948185920715332\n",
            "strain 1.8388495445251465\n",
            "strain 3.424487352371216\n",
            "strain 0.12704205513000488\n",
            "strain 4.069530487060547\n",
            "strain 3.830552339553833\n",
            "strain 3.167130947113037\n",
            "strain 0.3930438458919525\n",
            "strain 3.733339309692383\n",
            "strain 3.316438674926758\n",
            "strain 3.556135416030884\n",
            "strain 3.693751335144043\n",
            "strain 0.5980104804039001\n",
            "strain 4.107954502105713\n",
            "strain 4.347933769226074\n",
            "strain 4.493157386779785\n",
            "strain 3.3193976879119873\n",
            "strain 2.1224708557128906\n",
            "strain 3.3020284175872803\n",
            "strain 2.300441265106201\n",
            "strain 1.1959173679351807\n",
            "strain 3.3596441745758057\n",
            "classify 2.568511962890625\n",
            "classify 2.483551025390625\n",
            "classify 2.608551025390625\n",
            "classify 2.56292724609375\n",
            "classify 2.38458251953125\n",
            "classify 2.587310791015625\n",
            "classify 2.6243896484375\n",
            "classify 2.618133544921875\n",
            "classify 2.606475830078125\n",
            "classify 2.599151611328125\n",
            "classify 2.640167236328125\n",
            "0.078125\n",
            "0.109375\n",
            "0.0859375\n",
            "0.1484375\n",
            "0.0859375\n",
            "0.125\n",
            "0.125\n",
            "0.0859375\n",
            "0.0625\n",
            "0.0546875\n",
            "0.1171875\n",
            "strain 1.9736530780792236\n",
            "strain 2.8498008251190186\n",
            "strain 2.5376691818237305\n",
            "strain 4.151795387268066\n",
            "strain 3.4644484519958496\n",
            "strain 0.3224295973777771\n",
            "strain 0.31297239661216736\n",
            "strain 3.2621121406555176\n",
            "strain 0.703514039516449\n",
            "strain 0.1555221825838089\n",
            "strain 3.34480357170105\n",
            "strain 1.615152359008789\n",
            "strain 4.571463584899902\n",
            "strain 3.731405735015869\n",
            "strain 0.9722934365272522\n",
            "strain 4.4693603515625\n",
            "strain 1.035762071609497\n",
            "strain 0.815729558467865\n",
            "strain 2.5863523483276367\n",
            "strain 2.7460668087005615\n",
            "strain 4.02869176864624\n",
            "strain 4.820043563842773\n",
            "strain 0.3288758397102356\n",
            "strain 0.5165623426437378\n",
            "strain 3.446946620941162\n",
            "strain 4.082235336303711\n",
            "strain 4.355401039123535\n",
            "strain 2.7450826168060303\n",
            "strain 1.8850215673446655\n",
            "strain 2.021933078765869\n",
            "strain 2.864656448364258\n",
            "strain 0.18386192619800568\n",
            "strain 5.2483038902282715\n",
            "strain 1.5625299215316772\n",
            "strain 3.264930009841919\n",
            "strain 0.36721938848495483\n",
            "strain 0.2865821123123169\n",
            "strain 3.1570122241973877\n",
            "strain 2.506399154663086\n",
            "strain 0.07220897823572159\n",
            "strain 0.08149615675210953\n",
            "strain 4.6843976974487305\n",
            "strain 3.6444594860076904\n",
            "strain 2.826958417892456\n",
            "strain 1.389533281326294\n",
            "strain 0.7582003474235535\n",
            "strain 4.126625061035156\n",
            "strain 2.9686973094940186\n",
            "strain 1.911665439605713\n",
            "strain 0.24537985026836395\n",
            "strain 0.05259928107261658\n",
            "classify 2.6025390625\n",
            "classify 2.5340576171875\n",
            "classify 2.468658447265625\n",
            "classify 2.644134521484375\n",
            "classify 2.65399169921875\n",
            "classify 2.582489013671875\n",
            "classify 2.741485595703125\n",
            "classify 2.641937255859375\n",
            "classify 2.55767822265625\n",
            "classify 2.599761962890625\n",
            "classify 2.4962158203125\n",
            "0.078125\n",
            "0.1171875\n",
            "0.078125\n",
            "0.140625\n",
            "0.0859375\n",
            "0.125\n",
            "0.125\n",
            "0.0859375\n",
            "0.0703125\n",
            "0.0546875\n",
            "0.1171875\n",
            "strain 4.0481343269348145\n",
            "strain 4.163440704345703\n",
            "strain 1.9216827154159546\n",
            "strain 3.9660513401031494\n",
            "strain 3.675405502319336\n",
            "strain 2.3586878776550293\n",
            "strain 2.4805047512054443\n",
            "strain 4.3786420822143555\n",
            "strain 3.897645950317383\n",
            "strain 1.871170163154602\n",
            "strain 1.5642328262329102\n",
            "strain 0.05027490481734276\n",
            "strain 2.140272855758667\n",
            "strain 1.292283535003662\n",
            "strain 2.220175266265869\n",
            "strain 3.4648284912109375\n",
            "strain 0.21627794206142426\n",
            "strain 4.322432518005371\n",
            "strain 0.23046815395355225\n",
            "strain 2.6555275917053223\n",
            "strain 4.1261305809021\n",
            "strain 3.381812572479248\n",
            "strain 0.6349016427993774\n",
            "strain 1.4755077362060547\n",
            "strain 0.0620327889919281\n",
            "strain 0.05293382331728935\n",
            "strain 0.643408477306366\n",
            "strain 5.606107234954834\n",
            "strain 3.5998828411102295\n",
            "strain 1.7874364852905273\n",
            "strain 4.250479698181152\n",
            "strain 5.482832431793213\n",
            "strain 4.357468605041504\n",
            "strain 0.41582444310188293\n",
            "strain 4.211132526397705\n",
            "strain 4.645100116729736\n",
            "strain 4.243446350097656\n",
            "strain 0.5104432106018066\n",
            "strain 4.218522548675537\n",
            "strain 4.284294128417969\n",
            "strain 1.7001579999923706\n",
            "strain 3.520333766937256\n",
            "strain 1.1394109725952148\n",
            "strain 3.723267078399658\n",
            "strain 0.9763912558555603\n",
            "strain 4.325242519378662\n",
            "strain 0.5112308859825134\n",
            "strain 2.6727843284606934\n",
            "strain 3.392638683319092\n",
            "strain 3.160670757293701\n",
            "strain 0.10477906465530396\n",
            "classify 2.49951171875\n",
            "classify 2.568878173828125\n",
            "classify 2.53192138671875\n",
            "classify 2.53564453125\n",
            "classify 2.55029296875\n",
            "classify 2.55279541015625\n",
            "classify 2.60540771484375\n",
            "classify 2.5186767578125\n",
            "classify 2.626922607421875\n",
            "classify 2.664520263671875\n",
            "classify 2.652801513671875\n",
            "0.078125\n",
            "0.1171875\n",
            "0.078125\n",
            "0.140625\n",
            "0.0859375\n",
            "0.125\n",
            "0.125\n",
            "0.0859375\n",
            "0.0703125\n",
            "0.0546875\n",
            "0.1171875\n",
            "strain 6.900553226470947\n",
            "strain 4.582525730133057\n",
            "strain 1.6292542219161987\n",
            "strain 2.1620049476623535\n",
            "strain 3.8278965950012207\n",
            "strain 4.454314231872559\n",
            "strain 3.8993020057678223\n",
            "strain 1.6208949089050293\n",
            "strain 4.184803485870361\n",
            "strain 2.658219337463379\n",
            "strain 3.6112313270568848\n",
            "strain 2.605733633041382\n",
            "strain 5.275124549865723\n",
            "strain 3.0680439472198486\n",
            "strain 0.4744328260421753\n",
            "strain 3.5487802028656006\n",
            "strain 2.6783928871154785\n",
            "strain 4.154557228088379\n",
            "strain 1.0208308696746826\n",
            "strain 2.050502061843872\n",
            "strain 0.13793858885765076\n",
            "strain 0.29787206649780273\n",
            "strain 5.382925510406494\n",
            "strain 1.2170072793960571\n",
            "strain 2.6019346714019775\n",
            "strain 2.554295301437378\n",
            "strain 3.661066770553589\n",
            "strain 1.4113049507141113\n",
            "strain 3.0080039501190186\n",
            "strain 1.1627613306045532\n",
            "strain 1.6065670251846313\n",
            "strain 3.708958148956299\n",
            "strain 0.14888910949230194\n",
            "strain 4.785505771636963\n",
            "strain 1.8214164972305298\n",
            "strain 0.1627076119184494\n",
            "strain 4.006329536437988\n",
            "strain 2.3941805362701416\n",
            "strain 1.6216156482696533\n",
            "strain 1.7067879438400269\n",
            "strain 9.885899543762207\n",
            "strain 1.8369026184082031\n",
            "strain 4.119261741638184\n",
            "strain 3.3447301387786865\n",
            "strain 4.277249336242676\n",
            "strain 0.712767481803894\n",
            "strain 0.11028041690587997\n",
            "strain 2.598907232284546\n",
            "strain 4.725537300109863\n",
            "strain 1.0404618978500366\n",
            "strain 0.1099616140127182\n",
            "classify 2.66455078125\n",
            "classify 2.63287353515625\n",
            "classify 2.4495849609375\n",
            "classify 2.601593017578125\n",
            "classify 2.3792724609375\n",
            "classify 2.615814208984375\n",
            "classify 2.594390869140625\n",
            "classify 2.569183349609375\n",
            "classify 2.518341064453125\n",
            "classify 2.62835693359375\n",
            "classify 2.57965087890625\n",
            "0.078125\n",
            "0.1171875\n",
            "0.078125\n",
            "0.140625\n",
            "0.0859375\n",
            "0.125\n",
            "0.125\n",
            "0.078125\n",
            "0.078125\n",
            "0.0625\n",
            "0.1171875\n",
            "strain 1.0948134660720825\n",
            "strain 1.594923734664917\n",
            "strain 2.315798759460449\n",
            "strain 3.663301467895508\n",
            "strain 3.4683454036712646\n",
            "strain 1.9039263725280762\n",
            "strain 2.590895414352417\n",
            "strain 4.631524562835693\n",
            "strain 4.491162300109863\n",
            "strain 0.1300979107618332\n",
            "strain 2.738415241241455\n",
            "strain 0.054149579256772995\n",
            "strain 3.4655187129974365\n",
            "strain 1.8514854907989502\n",
            "strain 2.3844258785247803\n",
            "strain 1.5179506540298462\n",
            "strain 2.632507801055908\n",
            "strain 8.58155632019043\n",
            "strain 3.2120375633239746\n",
            "strain 3.4349205493927\n",
            "strain 1.9057564735412598\n",
            "strain 1.8063771724700928\n",
            "strain 0.08779048919677734\n",
            "strain 1.0428179502487183\n",
            "strain 0.09668070822954178\n",
            "strain 3.325962781906128\n",
            "strain 0.21712179481983185\n",
            "strain 2.4292452335357666\n",
            "strain 0.7520512938499451\n",
            "strain 0.9500684142112732\n",
            "strain 4.566452980041504\n",
            "strain 1.9697916507720947\n",
            "strain 5.342088222503662\n",
            "strain 5.029061317443848\n",
            "strain 1.5868269205093384\n",
            "strain 2.7368052005767822\n",
            "strain 3.636406183242798\n",
            "strain 4.076946258544922\n",
            "strain 3.926720380783081\n",
            "strain 4.103028774261475\n",
            "strain 0.9602762460708618\n",
            "strain 4.5456109046936035\n",
            "strain 3.0975637435913086\n",
            "strain 2.2685534954071045\n",
            "strain 2.6734743118286133\n",
            "strain 1.1081087589263916\n",
            "strain 0.3436369299888611\n",
            "strain 6.9592132568359375\n",
            "strain 4.845729351043701\n",
            "strain 3.3213348388671875\n",
            "strain 2.54892897605896\n",
            "classify 2.5369873046875\n",
            "classify 2.65301513671875\n",
            "classify 2.627655029296875\n",
            "classify 2.6646728515625\n",
            "classify 2.620269775390625\n",
            "classify 2.66717529296875\n",
            "classify 2.60125732421875\n",
            "classify 2.57257080078125\n",
            "classify 2.6983642578125\n",
            "classify 2.54052734375\n",
            "classify 2.68353271484375\n",
            "0.078125\n",
            "0.1171875\n",
            "0.078125\n",
            "0.15625\n",
            "0.09375\n",
            "0.125\n",
            "0.125\n",
            "0.0859375\n",
            "0.0703125\n",
            "0.0546875\n",
            "0.109375\n",
            "strain 0.5937955975532532\n",
            "strain 1.5572137832641602\n",
            "strain 2.509761095046997\n",
            "strain 3.4749112129211426\n",
            "strain 5.323986053466797\n",
            "strain 2.193577766418457\n",
            "strain 3.708611488342285\n",
            "strain 5.1911940574646\n",
            "strain 4.128354072570801\n",
            "strain 1.3553720712661743\n",
            "strain 2.97434139251709\n",
            "strain 3.0840659141540527\n",
            "strain 4.508570671081543\n",
            "strain 6.34681510925293\n",
            "strain 4.441387176513672\n",
            "strain 4.181180953979492\n",
            "strain 2.487208843231201\n",
            "strain 3.651674747467041\n",
            "strain 1.3468514680862427\n",
            "strain 0.2058047354221344\n",
            "strain 4.362340450286865\n",
            "strain 0.9247189164161682\n",
            "strain 4.173489093780518\n",
            "strain 4.566106796264648\n",
            "strain 3.7922518253326416\n",
            "strain 8.285510063171387\n",
            "strain 2.380290985107422\n",
            "strain 0.34570276737213135\n",
            "strain 2.3300352096557617\n",
            "strain 7.045881271362305\n",
            "strain 3.4975171089172363\n",
            "strain 0.39553651213645935\n",
            "strain 0.9506264925003052\n",
            "strain 2.4333114624023438\n",
            "strain 4.726068019866943\n",
            "strain 3.779470920562744\n",
            "strain 5.33142614364624\n",
            "strain 3.776054859161377\n",
            "strain 2.3818302154541016\n",
            "strain 3.9678359031677246\n",
            "strain 2.1776976585388184\n",
            "strain 2.131751775741577\n",
            "strain 0.30360665917396545\n",
            "strain 0.2564060389995575\n",
            "strain 0.22749249637126923\n",
            "strain 3.3402554988861084\n",
            "strain 3.121345281600952\n",
            "strain 2.573258876800537\n",
            "strain 0.28386813402175903\n",
            "strain 3.112922191619873\n",
            "strain 4.278188228607178\n",
            "classify 2.60772705078125\n",
            "classify 2.5489501953125\n",
            "classify 2.5712890625\n",
            "classify 2.6036376953125\n",
            "classify 2.630340576171875\n",
            "classify 2.615570068359375\n",
            "classify 2.59100341796875\n",
            "classify 2.6318359375\n",
            "classify 2.691253662109375\n",
            "classify 2.60760498046875\n",
            "classify 2.535125732421875\n",
            "0.078125\n",
            "0.1171875\n",
            "0.0859375\n",
            "0.15625\n",
            "0.1015625\n",
            "0.125\n",
            "0.125\n",
            "0.0859375\n",
            "0.078125\n",
            "0.0546875\n",
            "0.109375\n",
            "strain 3.158010244369507\n",
            "strain 0.7494968175888062\n",
            "strain 2.836913824081421\n",
            "strain 2.3163363933563232\n",
            "strain 3.3351008892059326\n",
            "strain 3.815906524658203\n",
            "strain 4.457440376281738\n",
            "strain 1.1209044456481934\n",
            "strain 4.459046363830566\n",
            "strain 1.6212568283081055\n",
            "strain 3.6825878620147705\n",
            "strain 3.274519920349121\n",
            "strain 0.9556436538696289\n",
            "strain 3.889742136001587\n",
            "strain 0.1333961933851242\n",
            "strain 0.32465505599975586\n",
            "strain 0.17039206624031067\n",
            "strain 1.2853169441223145\n",
            "strain 6.9102020263671875\n",
            "strain 4.9890899658203125\n",
            "strain 3.439182996749878\n",
            "strain 5.890010833740234\n",
            "strain 3.2262840270996094\n",
            "strain 3.6543684005737305\n",
            "strain 3.9893972873687744\n",
            "strain 2.683314800262451\n",
            "strain 0.2080863118171692\n",
            "strain 3.8986856937408447\n",
            "strain 3.502870559692383\n",
            "strain 3.631877899169922\n",
            "strain 0.892596423625946\n",
            "strain 2.4959847927093506\n",
            "strain 4.459996223449707\n",
            "strain 3.656507730484009\n",
            "strain 1.4798532724380493\n",
            "strain 3.3430895805358887\n",
            "strain 2.604109048843384\n",
            "strain 3.603886365890503\n",
            "strain 2.3520565032958984\n",
            "strain 4.260258674621582\n",
            "strain 4.344902038574219\n",
            "strain 4.702093601226807\n",
            "strain 3.221623420715332\n",
            "strain 4.463447093963623\n",
            "strain 9.104490280151367\n",
            "strain 5.636289119720459\n",
            "strain 2.2078983783721924\n",
            "strain 3.8292226791381836\n",
            "strain 3.1167359352111816\n",
            "strain 3.7592086791992188\n",
            "strain 2.6341168880462646\n",
            "classify 2.543670654296875\n",
            "classify 2.58135986328125\n",
            "classify 2.668182373046875\n",
            "classify 2.56488037109375\n",
            "classify 2.62579345703125\n",
            "classify 2.702789306640625\n",
            "classify 2.666015625\n",
            "classify 2.562713623046875\n",
            "classify 2.548919677734375\n",
            "classify 2.581573486328125\n",
            "classify 2.727142333984375\n",
            "0.078125\n",
            "0.1171875\n",
            "0.0859375\n",
            "0.15625\n",
            "0.1015625\n",
            "0.125\n",
            "0.125\n",
            "0.0859375\n",
            "0.078125\n",
            "0.0546875\n",
            "0.109375\n",
            "strain 0.167263001203537\n",
            "strain 3.6382033824920654\n",
            "strain 2.684819221496582\n",
            "strain 3.725447177886963\n",
            "strain 0.671288013458252\n",
            "strain 2.056431770324707\n",
            "strain 2.8777589797973633\n",
            "strain 3.7181472778320312\n",
            "strain 1.6328456401824951\n",
            "strain 3.6833560466766357\n",
            "strain 0.3465428650379181\n",
            "strain 3.89052677154541\n",
            "strain 4.2966718673706055\n",
            "strain 4.067319393157959\n",
            "strain 0.7638475894927979\n",
            "strain 3.2279627323150635\n",
            "strain 4.519505977630615\n",
            "strain 0.7676724791526794\n",
            "strain 0.9428119659423828\n",
            "strain 3.715139389038086\n",
            "strain 5.536456108093262\n",
            "strain 3.4798009395599365\n",
            "strain 2.8387622833251953\n",
            "strain 0.29486343264579773\n",
            "strain 4.229256629943848\n",
            "strain 5.226221084594727\n",
            "strain 1.5280227661132812\n",
            "strain 4.397727966308594\n",
            "strain 0.26828470826148987\n",
            "strain 3.6855244636535645\n",
            "strain 0.3370388150215149\n",
            "strain 3.6048214435577393\n",
            "strain 0.31847473978996277\n",
            "strain 5.4792399406433105\n",
            "strain 0.15482456982135773\n",
            "strain 4.235265254974365\n",
            "strain 3.7194998264312744\n",
            "strain 5.6111159324646\n",
            "strain 1.0139325857162476\n",
            "strain 4.425258636474609\n",
            "strain 0.31848740577697754\n",
            "strain 4.454646110534668\n",
            "strain 1.023996353149414\n",
            "strain 1.6470777988433838\n",
            "strain 4.458427906036377\n",
            "strain 0.24833548069000244\n",
            "strain 3.9152660369873047\n",
            "strain 0.4837208688259125\n",
            "strain 3.247041940689087\n",
            "strain 4.630061149597168\n",
            "strain 3.7334773540496826\n",
            "classify 2.617401123046875\n",
            "classify 2.620452880859375\n",
            "classify 2.578155517578125\n",
            "classify 2.6171875\n",
            "classify 2.534423828125\n",
            "classify 2.508819580078125\n",
            "classify 2.56024169921875\n",
            "classify 2.68585205078125\n",
            "classify 2.607421875\n",
            "classify 2.57220458984375\n",
            "classify 2.636077880859375\n",
            "0.078125\n",
            "0.1171875\n",
            "0.09375\n",
            "0.15625\n",
            "0.1015625\n",
            "0.125\n",
            "0.140625\n",
            "0.0859375\n",
            "0.078125\n",
            "0.0546875\n",
            "0.109375\n",
            "strain 2.088705539703369\n",
            "strain 5.612327575683594\n",
            "strain 4.362916946411133\n",
            "strain 4.173769474029541\n",
            "strain 4.377021789550781\n",
            "strain 1.4887864589691162\n",
            "strain 3.176363706588745\n",
            "strain 4.547558784484863\n",
            "strain 1.7779215574264526\n",
            "strain 1.373563528060913\n",
            "strain 5.288464546203613\n",
            "strain 3.8792693614959717\n",
            "strain 4.395658016204834\n",
            "strain 1.9959346055984497\n",
            "strain 2.33187198638916\n",
            "strain 2.401655912399292\n",
            "strain 3.557432174682617\n",
            "strain 0.09390509128570557\n",
            "strain 1.4134410619735718\n",
            "strain 0.37354961037635803\n",
            "strain 4.153543472290039\n",
            "strain 3.459928512573242\n",
            "strain 1.8349179029464722\n",
            "strain 6.745814800262451\n",
            "strain 5.2476725578308105\n",
            "strain 3.2676517963409424\n",
            "strain 3.654822826385498\n",
            "strain 2.022805690765381\n",
            "strain 3.46490216255188\n",
            "strain 3.6450226306915283\n",
            "strain 3.8408634662628174\n",
            "strain 0.10837092250585556\n",
            "strain 3.8188629150390625\n",
            "strain 0.1388566642999649\n",
            "strain 4.53132438659668\n",
            "strain 4.571567058563232\n",
            "strain 5.29805850982666\n",
            "strain 3.394970417022705\n",
            "strain 1.949288010597229\n",
            "strain 3.911418914794922\n",
            "strain 3.347886562347412\n",
            "strain 1.8079859018325806\n",
            "strain 3.921224594116211\n",
            "strain 4.883078575134277\n",
            "strain 2.267005443572998\n",
            "strain 2.864144802093506\n",
            "strain 3.419532299041748\n",
            "strain 0.8280045986175537\n",
            "strain 4.33064079284668\n",
            "strain 3.7644903659820557\n",
            "strain 3.457509756088257\n",
            "classify 2.586181640625\n",
            "classify 2.47747802734375\n",
            "classify 2.50787353515625\n",
            "classify 2.6226806640625\n",
            "classify 2.62469482421875\n",
            "classify 2.6297607421875\n",
            "classify 2.425079345703125\n",
            "classify 2.620819091796875\n",
            "classify 2.6488037109375\n",
            "classify 2.7117919921875\n",
            "classify 2.637481689453125\n",
            "0.0859375\n",
            "0.1171875\n",
            "0.09375\n",
            "0.1640625\n",
            "0.1015625\n",
            "0.1328125\n",
            "0.140625\n",
            "0.0859375\n",
            "0.078125\n",
            "0.046875\n",
            "0.109375\n",
            "strain 2.511955976486206\n",
            "strain 4.139615535736084\n",
            "strain 2.084282875061035\n",
            "strain 4.168403148651123\n",
            "strain 0.4830692410469055\n",
            "strain 0.19578328728675842\n",
            "strain 0.32885947823524475\n",
            "strain 4.20201301574707\n",
            "strain 0.8853805661201477\n",
            "strain 0.8634423613548279\n",
            "strain 0.46980175375938416\n",
            "strain 3.7091002464294434\n",
            "strain 2.045339345932007\n",
            "strain 3.1931867599487305\n",
            "strain 1.7656817436218262\n",
            "strain 3.7810473442077637\n",
            "strain 1.2830890417099\n",
            "strain 3.937612295150757\n",
            "strain 4.714445114135742\n",
            "strain 4.063233852386475\n",
            "strain 3.5007400512695312\n",
            "strain 3.9151248931884766\n",
            "strain 2.3301925659179688\n",
            "strain 1.7671564817428589\n",
            "strain 2.7810449600219727\n",
            "strain 4.289377689361572\n",
            "strain 2.884089946746826\n",
            "strain 2.298506259918213\n",
            "strain 3.031921863555908\n",
            "strain 4.069072246551514\n",
            "strain 3.6530303955078125\n",
            "strain 3.253512382507324\n",
            "strain 0.8365366458892822\n",
            "strain 0.10381617397069931\n",
            "strain 3.4516990184783936\n",
            "strain 4.819991588592529\n",
            "strain 4.627011299133301\n",
            "strain 3.835106611251831\n",
            "strain 4.033529281616211\n",
            "strain 4.03270149230957\n",
            "strain 3.09367299079895\n",
            "strain 4.4705634117126465\n",
            "strain 3.027794599533081\n",
            "strain 3.035993814468384\n",
            "strain 1.1304779052734375\n",
            "strain 3.8735930919647217\n",
            "strain 2.528963804244995\n",
            "strain 5.312922954559326\n",
            "strain 0.7269452214241028\n",
            "strain 0.2841547131538391\n",
            "strain 4.062655925750732\n",
            "classify 2.60491943359375\n",
            "classify 2.5677490234375\n",
            "classify 2.61309814453125\n",
            "classify 2.531097412109375\n",
            "classify 2.593536376953125\n",
            "classify 2.667205810546875\n",
            "classify 2.578765869140625\n",
            "classify 2.564056396484375\n",
            "classify 2.562164306640625\n",
            "classify 2.57781982421875\n",
            "classify 2.620941162109375\n",
            "0.0859375\n",
            "0.1171875\n",
            "0.09375\n",
            "0.1640625\n",
            "0.1015625\n",
            "0.125\n",
            "0.1328125\n",
            "0.0859375\n",
            "0.078125\n",
            "0.0546875\n",
            "0.109375\n",
            "strain 3.115377902984619\n",
            "strain 3.346107006072998\n",
            "strain 4.267834663391113\n",
            "strain 2.2181200981140137\n",
            "strain 2.157310962677002\n",
            "strain 3.526474714279175\n",
            "strain 3.5623302459716797\n",
            "strain 2.088644027709961\n",
            "strain 4.405904769897461\n",
            "strain 3.7002458572387695\n",
            "strain 4.067122936248779\n",
            "strain 6.521604537963867\n",
            "strain 0.5234810709953308\n",
            "strain 3.4107577800750732\n",
            "strain 1.593851089477539\n",
            "strain 1.6758201122283936\n",
            "strain 2.8287532329559326\n",
            "strain 5.188229560852051\n",
            "strain 3.4785773754119873\n",
            "strain 0.27896061539649963\n",
            "strain 4.455804824829102\n",
            "strain 4.309071063995361\n",
            "strain 1.7069653272628784\n",
            "strain 1.0613577365875244\n",
            "strain 3.832216501235962\n",
            "strain 0.3437158465385437\n",
            "strain 3.771350622177124\n",
            "strain 1.3523505926132202\n",
            "strain 2.451155424118042\n",
            "strain 2.04099702835083\n",
            "strain 4.4418487548828125\n",
            "strain 3.126349449157715\n",
            "strain 0.31556448340415955\n",
            "strain 0.35787248611450195\n",
            "strain 2.6389341354370117\n",
            "strain 4.3354902267456055\n",
            "strain 0.17021271586418152\n",
            "strain 4.168441295623779\n",
            "strain 1.1841274499893188\n",
            "strain 0.10944106429815292\n",
            "strain 0.6126147508621216\n",
            "strain 0.10251443833112717\n",
            "strain 1.4706107378005981\n",
            "strain 2.5067806243896484\n",
            "strain 2.5362162590026855\n",
            "strain 1.173806071281433\n",
            "strain 1.5979822874069214\n",
            "strain 4.160134315490723\n",
            "strain 2.3916878700256348\n",
            "strain 1.953230381011963\n",
            "strain 6.232691287994385\n",
            "classify 2.63214111328125\n",
            "classify 2.58392333984375\n",
            "classify 2.452911376953125\n",
            "classify 2.528289794921875\n",
            "classify 2.642547607421875\n",
            "classify 2.6343994140625\n",
            "classify 2.723175048828125\n",
            "classify 2.662628173828125\n",
            "classify 2.73699951171875\n",
            "classify 2.630615234375\n",
            "classify 2.732879638671875\n",
            "0.09375\n",
            "0.109375\n",
            "0.1015625\n",
            "0.1640625\n",
            "0.1015625\n",
            "0.1328125\n",
            "0.140625\n",
            "0.0859375\n",
            "0.078125\n",
            "0.0546875\n",
            "0.1171875\n",
            "strain 2.1807174682617188\n",
            "strain 4.567992687225342\n",
            "strain 0.08804265409708023\n",
            "strain 2.188159227371216\n",
            "strain 2.1683456897735596\n",
            "strain 2.199174165725708\n",
            "strain 2.6110658645629883\n",
            "strain 0.06652481853961945\n",
            "strain 2.9917163848876953\n",
            "strain 3.734426259994507\n",
            "strain 0.4911661744117737\n",
            "strain 0.5585185885429382\n",
            "strain 3.757655143737793\n",
            "strain 2.866274356842041\n",
            "strain 3.571779489517212\n",
            "strain 0.5314953923225403\n",
            "strain 1.3093020915985107\n",
            "strain 4.690448760986328\n",
            "strain 2.8134896755218506\n",
            "strain 1.355789303779602\n",
            "strain 3.929611921310425\n",
            "strain 1.9517967700958252\n",
            "strain 3.5024702548980713\n",
            "strain 4.719064235687256\n",
            "strain 4.427886486053467\n",
            "strain 3.723266839981079\n",
            "strain 2.594219923019409\n",
            "strain 0.5648927092552185\n",
            "strain 4.341029167175293\n",
            "strain 3.990713596343994\n",
            "strain 2.3737077713012695\n",
            "strain 2.6555943489074707\n",
            "strain 4.707729339599609\n",
            "strain 3.1170406341552734\n",
            "strain 0.8611682057380676\n",
            "strain 1.8019099235534668\n",
            "strain 1.284125804901123\n",
            "strain 3.7873635292053223\n",
            "strain 0.8729819655418396\n",
            "strain 2.173699140548706\n",
            "strain 3.018282890319824\n",
            "strain 1.6188470125198364\n",
            "strain 1.7551263570785522\n",
            "strain 0.8308659791946411\n",
            "strain 0.792114794254303\n",
            "strain 0.29406246542930603\n",
            "strain 3.2562785148620605\n",
            "strain 2.5962653160095215\n",
            "strain 3.2912437915802\n",
            "strain 3.003364086151123\n",
            "strain 2.6976852416992188\n",
            "classify 2.58880615234375\n",
            "classify 2.647613525390625\n",
            "classify 2.5968017578125\n",
            "classify 2.5224609375\n",
            "classify 2.640869140625\n",
            "classify 2.57958984375\n",
            "classify 2.60308837890625\n",
            "classify 2.6446533203125\n",
            "classify 2.5472412109375\n",
            "classify 2.571990966796875\n",
            "classify 2.619049072265625\n",
            "0.09375\n",
            "0.109375\n",
            "0.1015625\n",
            "0.1640625\n",
            "0.1015625\n",
            "0.1328125\n",
            "0.140625\n",
            "0.0859375\n",
            "0.078125\n",
            "0.0546875\n",
            "0.1171875\n",
            "strain 5.443671703338623\n",
            "strain 4.720122814178467\n",
            "strain 2.8920295238494873\n",
            "strain 2.1436355113983154\n",
            "strain 2.5385282039642334\n",
            "strain 1.8542693853378296\n",
            "strain 0.7187673449516296\n",
            "strain 0.7278501987457275\n",
            "strain 2.4992358684539795\n",
            "strain 0.26251503825187683\n",
            "strain 1.281590223312378\n",
            "strain 5.403201580047607\n",
            "strain 4.648380756378174\n",
            "strain 3.1352200508117676\n",
            "strain 1.3899059295654297\n",
            "strain 2.843794822692871\n",
            "strain 4.751562595367432\n",
            "strain 5.191434860229492\n",
            "strain 3.9055545330047607\n",
            "strain 0.8612687587738037\n",
            "strain 1.327390193939209\n",
            "strain 1.7664352655410767\n",
            "strain 5.550368309020996\n",
            "strain 6.205843925476074\n",
            "strain 3.6076786518096924\n",
            "strain 6.493549823760986\n",
            "strain 0.9299632906913757\n",
            "strain 1.1814191341400146\n",
            "strain 3.0276660919189453\n",
            "strain 1.3020886182785034\n",
            "strain 3.0393073558807373\n",
            "strain 1.4025866985321045\n",
            "strain 3.75862455368042\n",
            "strain 4.808388710021973\n",
            "strain 4.702929973602295\n",
            "strain 3.964893341064453\n",
            "strain 2.7433981895446777\n",
            "strain 2.9437620639801025\n",
            "strain 3.324784994125366\n",
            "strain 4.9317169189453125\n",
            "strain 0.3827832043170929\n",
            "strain 7.203216552734375\n",
            "strain 3.7757678031921387\n",
            "strain 5.519099712371826\n",
            "strain 2.6160004138946533\n",
            "strain 4.088047027587891\n",
            "strain 4.293800354003906\n",
            "strain 2.0829715728759766\n",
            "strain 0.49998021125793457\n",
            "strain 6.06227445602417\n",
            "strain 4.940350532531738\n",
            "classify 2.4632568359375\n",
            "classify 2.6103515625\n",
            "classify 2.566680908203125\n",
            "classify 2.660919189453125\n",
            "classify 2.55908203125\n",
            "classify 2.508544921875\n",
            "classify 2.5755615234375\n",
            "classify 2.520355224609375\n",
            "classify 2.593353271484375\n",
            "classify 2.753173828125\n",
            "classify 2.6339111328125\n",
            "0.09375\n",
            "0.1171875\n",
            "0.109375\n",
            "0.1640625\n",
            "0.1015625\n",
            "0.1328125\n",
            "0.1484375\n",
            "0.0859375\n",
            "0.078125\n",
            "0.046875\n",
            "0.1171875\n",
            "strain 0.4079568386077881\n",
            "strain 3.5089211463928223\n",
            "strain 2.8155412673950195\n",
            "strain 1.766029953956604\n",
            "strain 4.03132438659668\n",
            "strain 3.0436434745788574\n",
            "strain 2.457350730895996\n",
            "strain 1.9623761177062988\n",
            "strain 3.014946460723877\n",
            "strain 3.392228364944458\n",
            "strain 4.280457496643066\n",
            "strain 0.4819585680961609\n",
            "strain 1.5981135368347168\n",
            "strain 1.8302236795425415\n",
            "strain 2.4456627368927\n",
            "strain 4.969484806060791\n",
            "strain 5.223206520080566\n",
            "strain 3.187368869781494\n",
            "strain 2.5111007690429688\n",
            "strain 1.2810554504394531\n",
            "strain 2.5267364978790283\n",
            "strain 5.175261497497559\n",
            "strain 4.976008892059326\n",
            "strain 4.217837333679199\n",
            "strain 3.018264055252075\n",
            "strain 4.312979698181152\n",
            "strain 1.8700464963912964\n",
            "strain 7.018289089202881\n",
            "strain 5.279998779296875\n",
            "strain 1.8163444995880127\n",
            "strain 1.9241297245025635\n",
            "strain 0.483277291059494\n",
            "strain 0.7598012685775757\n",
            "strain 3.9741268157958984\n",
            "strain 2.5207395553588867\n",
            "strain 4.118538856506348\n",
            "strain 4.982766151428223\n",
            "strain 0.34524771571159363\n",
            "strain 5.059286117553711\n",
            "strain 6.663006782531738\n",
            "strain 4.043420314788818\n",
            "strain 0.31863895058631897\n",
            "strain 0.6115597486495972\n",
            "strain 1.9479639530181885\n",
            "strain 4.097592830657959\n",
            "strain 3.842569589614868\n",
            "strain 2.9949254989624023\n",
            "strain 3.933391571044922\n",
            "strain 1.5340548753738403\n",
            "strain 2.2449631690979004\n",
            "strain 3.015014171600342\n",
            "classify 2.6474609375\n",
            "classify 2.5966796875\n",
            "classify 2.594970703125\n",
            "classify 2.701416015625\n",
            "classify 2.506683349609375\n",
            "classify 2.608184814453125\n",
            "classify 2.716583251953125\n",
            "classify 2.65325927734375\n",
            "classify 2.656341552734375\n",
            "classify 2.613555908203125\n",
            "classify 2.7374267578125\n",
            "0.09375\n",
            "0.1171875\n",
            "0.109375\n",
            "0.1640625\n",
            "0.1015625\n",
            "0.1328125\n",
            "0.1484375\n",
            "0.0859375\n",
            "0.078125\n",
            "0.0546875\n",
            "0.1171875\n",
            "strain 1.521525502204895\n",
            "strain 5.927346706390381\n",
            "strain 1.4357744455337524\n",
            "strain 3.558255195617676\n",
            "strain 0.45297354459762573\n",
            "strain 3.634538412094116\n",
            "strain 5.449143886566162\n",
            "strain 5.526335716247559\n",
            "strain 0.723505973815918\n",
            "strain 4.150694847106934\n",
            "strain 0.2156583070755005\n",
            "strain 1.2592304944992065\n",
            "strain 4.285132884979248\n",
            "strain 0.7091140747070312\n",
            "strain 4.796572685241699\n",
            "strain 7.5480780601501465\n",
            "strain 10.237848281860352\n",
            "strain 2.083024501800537\n",
            "strain 4.950687408447266\n",
            "strain 4.423797607421875\n",
            "strain 4.571506500244141\n",
            "strain 1.6278616189956665\n",
            "strain 5.3792266845703125\n",
            "strain 1.963784098625183\n",
            "strain 5.39931058883667\n",
            "strain 3.29530668258667\n",
            "strain 2.2540555000305176\n",
            "strain 2.3628220558166504\n",
            "strain 2.0125234127044678\n",
            "strain 1.6001756191253662\n",
            "strain 4.088712215423584\n",
            "strain 6.450076580047607\n",
            "strain 6.039178371429443\n",
            "strain 2.389247179031372\n",
            "strain 3.7351267337799072\n",
            "strain 1.2322003841400146\n",
            "strain 1.0238487720489502\n",
            "strain 7.084840297698975\n",
            "strain 3.2815449237823486\n",
            "strain 1.2424193620681763\n",
            "strain 1.1688064336776733\n",
            "strain 3.1768176555633545\n",
            "strain 5.6041178703308105\n",
            "strain 1.7463140487670898\n",
            "strain 5.019525527954102\n",
            "strain 8.612168312072754\n",
            "strain 5.338893413543701\n",
            "strain 1.5128992795944214\n",
            "strain 5.762864112854004\n",
            "strain 3.5675387382507324\n",
            "strain 1.1882941722869873\n",
            "classify 2.62701416015625\n",
            "classify 2.6943359375\n",
            "classify 2.578216552734375\n",
            "classify 2.58184814453125\n",
            "classify 2.4990234375\n",
            "classify 2.62933349609375\n",
            "classify 2.56201171875\n",
            "classify 2.553955078125\n",
            "classify 2.4620361328125\n",
            "classify 2.688018798828125\n",
            "classify 2.516265869140625\n",
            "0.0859375\n",
            "0.1171875\n",
            "0.1171875\n",
            "0.1640625\n",
            "0.109375\n",
            "0.125\n",
            "0.15625\n",
            "0.0859375\n",
            "0.0859375\n",
            "0.0546875\n",
            "0.1171875\n",
            "strain 3.2973573207855225\n",
            "strain 11.193982124328613\n",
            "strain 3.9044532775878906\n",
            "strain 6.647337436676025\n",
            "strain 7.972164154052734\n",
            "strain 2.083664655685425\n",
            "strain 4.59559965133667\n",
            "strain 1.8635493516921997\n",
            "strain 8.759074211120605\n",
            "strain 3.398515224456787\n",
            "strain 5.435415267944336\n",
            "strain 3.684112071990967\n",
            "strain 2.7189342975616455\n",
            "strain 3.6405887603759766\n",
            "strain 5.547080993652344\n",
            "strain 3.3620529174804688\n",
            "strain 4.5666351318359375\n",
            "strain 4.154561996459961\n",
            "strain 2.9328649044036865\n",
            "strain 3.805922508239746\n",
            "strain 3.0243136882781982\n",
            "strain 3.2436416149139404\n",
            "strain 2.5823628902435303\n",
            "strain 6.0757246017456055\n",
            "strain 3.002375602722168\n",
            "strain 4.51897668838501\n",
            "strain 4.332962989807129\n",
            "strain 3.043130397796631\n",
            "strain 3.5093564987182617\n",
            "strain 5.244935512542725\n",
            "strain 3.2053894996643066\n",
            "strain 5.967624664306641\n",
            "strain 2.941659927368164\n",
            "strain 1.7098450660705566\n",
            "strain 3.568837881088257\n",
            "strain 5.220962047576904\n",
            "strain 2.951810359954834\n",
            "strain 3.407942771911621\n",
            "strain 2.7685940265655518\n",
            "strain 2.7017765045166016\n",
            "strain 4.229941368103027\n",
            "strain 4.920970916748047\n",
            "strain 3.5626792907714844\n",
            "strain 4.469096660614014\n",
            "strain 4.716762065887451\n",
            "strain 1.0222434997558594\n",
            "strain 3.4952428340911865\n",
            "strain 2.8494391441345215\n",
            "strain 5.546629905700684\n",
            "strain 0.7382184863090515\n",
            "strain 4.967715263366699\n",
            "classify 2.56494140625\n",
            "classify 2.67547607421875\n",
            "classify 2.468231201171875\n",
            "classify 2.554290771484375\n",
            "classify 2.434112548828125\n",
            "classify 2.715423583984375\n",
            "classify 2.80029296875\n",
            "classify 2.578582763671875\n",
            "classify 2.536712646484375\n",
            "classify 2.64935302734375\n",
            "classify 2.600341796875\n",
            "0.0859375\n",
            "0.1171875\n",
            "0.1171875\n",
            "0.1640625\n",
            "0.109375\n",
            "0.125\n",
            "0.15625\n",
            "0.0859375\n",
            "0.0859375\n",
            "0.0625\n",
            "0.1171875\n",
            "strain 0.6568562984466553\n",
            "strain 4.316195011138916\n",
            "strain 0.6745752096176147\n",
            "strain 1.1459987163543701\n",
            "strain 3.617342710494995\n",
            "strain 6.121922492980957\n",
            "strain 0.8234080076217651\n",
            "strain 4.447793006896973\n",
            "strain 2.803175687789917\n",
            "strain 3.915628433227539\n",
            "strain 2.9093399047851562\n",
            "strain 2.778595209121704\n",
            "strain 1.4326450824737549\n",
            "strain 5.092374324798584\n",
            "strain 0.7948381900787354\n",
            "strain 1.4330781698226929\n",
            "strain 2.758272647857666\n",
            "strain 4.303605079650879\n",
            "strain 0.639068067073822\n",
            "strain 3.855590343475342\n",
            "strain 4.089984893798828\n",
            "strain 3.4068806171417236\n",
            "strain 4.322731018066406\n",
            "strain 4.65955114364624\n",
            "strain 0.6127867102622986\n",
            "strain 2.193957567214966\n",
            "strain 3.00262713432312\n",
            "strain 2.3137176036834717\n",
            "strain 4.712071418762207\n",
            "strain 1.9025481939315796\n",
            "strain 4.784193515777588\n",
            "strain 0.4217231869697571\n",
            "strain 4.029068946838379\n",
            "strain 0.9366102814674377\n",
            "strain 1.6587181091308594\n",
            "strain 2.2139651775360107\n",
            "strain 4.355111598968506\n",
            "strain 4.268312931060791\n",
            "strain 3.7239723205566406\n",
            "strain 5.516970157623291\n",
            "strain 2.664161205291748\n",
            "strain 6.582679271697998\n",
            "strain 2.8869500160217285\n",
            "strain 6.045796871185303\n",
            "strain 3.5777955055236816\n",
            "strain 4.1575608253479\n",
            "strain 0.4162704348564148\n",
            "strain 4.379557132720947\n",
            "strain 5.471348762512207\n",
            "strain 0.30420324206352234\n",
            "strain 4.776297092437744\n",
            "classify 2.56719970703125\n",
            "classify 2.5927734375\n",
            "classify 2.640716552734375\n",
            "classify 2.591644287109375\n",
            "classify 2.65216064453125\n",
            "classify 2.436065673828125\n",
            "classify 2.618011474609375\n",
            "classify 2.67181396484375\n",
            "classify 2.579986572265625\n",
            "classify 2.504425048828125\n",
            "classify 2.534881591796875\n",
            "0.09375\n",
            "0.125\n",
            "0.1171875\n",
            "0.1640625\n",
            "0.109375\n",
            "0.125\n",
            "0.1484375\n",
            "0.0859375\n",
            "0.078125\n",
            "0.0625\n",
            "0.1171875\n",
            "strain 3.0893406867980957\n",
            "strain 4.597674369812012\n",
            "strain 3.1011343002319336\n",
            "strain 0.4416517913341522\n",
            "strain 0.2779594361782074\n",
            "strain 3.877924919128418\n",
            "strain 1.2727123498916626\n",
            "strain 4.377026557922363\n",
            "strain 3.8061041831970215\n",
            "strain 2.8598551750183105\n",
            "strain 2.72489070892334\n",
            "strain 0.30689534544944763\n",
            "strain 4.842723846435547\n",
            "strain 2.0567429065704346\n",
            "strain 3.066739320755005\n",
            "strain 0.381896048784256\n",
            "strain 4.845541477203369\n",
            "strain 0.35947185754776\n",
            "strain 0.37336599826812744\n",
            "strain 3.885762929916382\n",
            "strain 0.395187109708786\n",
            "strain 4.847395420074463\n",
            "strain 3.812722682952881\n",
            "strain 0.8361881375312805\n",
            "strain 3.4339964389801025\n",
            "strain 0.2997746467590332\n",
            "strain 3.6026105880737305\n",
            "strain 2.94783616065979\n",
            "strain 4.439968585968018\n",
            "strain 4.703197479248047\n",
            "strain 1.8871526718139648\n",
            "strain 3.940967559814453\n",
            "strain 3.7244489192962646\n",
            "strain 2.938289165496826\n",
            "strain 0.8738709688186646\n",
            "strain 2.5314929485321045\n",
            "strain 0.23919302225112915\n",
            "strain 0.19327068328857422\n",
            "strain 2.7852370738983154\n",
            "strain 2.62369966506958\n",
            "strain 2.2286689281463623\n",
            "strain 2.3332340717315674\n",
            "strain 0.11696898937225342\n",
            "strain 5.218282222747803\n",
            "strain 2.7801284790039062\n",
            "strain 3.249981641769409\n",
            "strain 2.2984023094177246\n",
            "strain 5.86958122253418\n",
            "strain 4.608310699462891\n",
            "strain 2.711149215698242\n",
            "strain 0.25014495849609375\n",
            "classify 2.480316162109375\n",
            "classify 2.7213134765625\n",
            "classify 2.58428955078125\n",
            "classify 2.5745849609375\n",
            "classify 2.567108154296875\n",
            "classify 2.5299072265625\n",
            "classify 2.6363525390625\n",
            "classify 2.68310546875\n",
            "classify 2.557220458984375\n",
            "classify 2.623260498046875\n",
            "classify 2.57293701171875\n",
            "0.0859375\n",
            "0.125\n",
            "0.1171875\n",
            "0.1640625\n",
            "0.109375\n",
            "0.125\n",
            "0.15625\n",
            "0.0859375\n",
            "0.0859375\n",
            "0.0625\n",
            "0.1171875\n",
            "strain 1.5874946117401123\n",
            "strain 3.4822497367858887\n",
            "strain 3.656830072402954\n",
            "strain 7.473644733428955\n",
            "strain 0.6672953367233276\n",
            "strain 5.55183219909668\n",
            "strain 0.7646151781082153\n",
            "strain 1.3405072689056396\n",
            "strain 3.158292770385742\n",
            "strain 2.954821825027466\n",
            "strain 2.489737033843994\n",
            "strain 4.54747200012207\n",
            "strain 2.992581605911255\n",
            "strain 4.664442539215088\n",
            "strain 4.098506927490234\n",
            "strain 4.679393768310547\n",
            "strain 5.171584129333496\n",
            "strain 4.603055000305176\n",
            "strain 1.0194357633590698\n",
            "strain 2.722947359085083\n",
            "strain 2.966484546661377\n",
            "strain 3.4960081577301025\n",
            "strain 3.360931873321533\n",
            "strain 3.750593900680542\n",
            "strain 0.9237303733825684\n",
            "strain 0.16664226353168488\n",
            "strain 1.766868233680725\n",
            "strain 2.2393805980682373\n",
            "strain 3.5021917819976807\n",
            "strain 3.8223981857299805\n",
            "strain 3.134326219558716\n",
            "strain 4.128937244415283\n",
            "strain 2.528346300125122\n",
            "strain 2.2750775814056396\n",
            "strain 3.6819701194763184\n",
            "strain 0.14815989136695862\n",
            "strain 0.2389850616455078\n",
            "strain 4.238248825073242\n",
            "strain 0.16242974996566772\n",
            "strain 0.3099881410598755\n",
            "strain 3.051154613494873\n",
            "strain 4.307298183441162\n",
            "strain 1.218961477279663\n",
            "strain 3.3873751163482666\n",
            "strain 3.2822835445404053\n",
            "strain 0.2250993847846985\n",
            "strain 3.264939546585083\n",
            "strain 4.836068630218506\n",
            "strain 4.5654778480529785\n",
            "strain 6.289890766143799\n",
            "strain 3.859640598297119\n",
            "classify 2.61767578125\n",
            "classify 2.532958984375\n",
            "classify 2.576690673828125\n",
            "classify 2.62274169921875\n",
            "classify 2.6090087890625\n",
            "classify 2.666168212890625\n",
            "classify 2.733245849609375\n",
            "classify 2.683319091796875\n",
            "classify 2.60894775390625\n",
            "classify 2.5560302734375\n",
            "classify 2.594970703125\n",
            "0.0859375\n",
            "0.125\n",
            "0.1171875\n",
            "0.1640625\n",
            "0.109375\n",
            "0.125\n",
            "0.15625\n",
            "0.0859375\n",
            "0.0859375\n",
            "0.0625\n",
            "0.1171875\n",
            "strain 0.7750293612480164\n",
            "strain 0.522958517074585\n",
            "strain 0.4558553695678711\n",
            "strain 4.5037126541137695\n",
            "strain 0.22390535473823547\n",
            "strain 6.745798110961914\n",
            "strain 4.358865737915039\n",
            "strain 1.3672292232513428\n",
            "strain 2.524271011352539\n",
            "strain 4.431038856506348\n",
            "strain 1.3007322549819946\n",
            "strain 2.427225351333618\n",
            "strain 4.989218235015869\n",
            "strain 5.372402667999268\n",
            "strain 3.222226619720459\n",
            "strain 0.1859869360923767\n",
            "strain 3.62058424949646\n",
            "strain 2.6341512203216553\n",
            "strain 3.0970396995544434\n",
            "strain 4.340176582336426\n",
            "strain 0.0813431665301323\n",
            "strain 1.953730583190918\n",
            "strain 0.10624945163726807\n",
            "strain 3.074899435043335\n",
            "strain 0.11584466695785522\n",
            "strain 1.5127216577529907\n",
            "strain 2.588563919067383\n",
            "strain 0.09406237304210663\n",
            "strain 2.944962978363037\n",
            "strain 3.7294998168945312\n",
            "strain 4.414614677429199\n",
            "strain 1.6216999292373657\n",
            "strain 4.780797004699707\n",
            "strain 2.7329232692718506\n",
            "strain 2.015171766281128\n",
            "strain 1.8787084817886353\n",
            "strain 5.898479461669922\n",
            "strain 0.11748580634593964\n",
            "strain 3.9181203842163086\n",
            "strain 3.3987748622894287\n",
            "strain 2.3967626094818115\n",
            "strain 0.16647158563137054\n",
            "strain 4.51686429977417\n",
            "strain 1.0721688270568848\n",
            "strain 3.45886492729187\n",
            "strain 4.277176856994629\n",
            "strain 1.0994528532028198\n",
            "strain 3.626845359802246\n",
            "strain 4.5799665451049805\n",
            "strain 3.25333309173584\n",
            "strain 4.860111713409424\n",
            "classify 2.53021240234375\n",
            "classify 2.466400146484375\n",
            "classify 2.580596923828125\n",
            "classify 2.604522705078125\n",
            "classify 2.45880126953125\n",
            "classify 2.4510498046875\n",
            "classify 2.64764404296875\n",
            "classify 2.63568115234375\n",
            "classify 2.50347900390625\n",
            "classify 2.557342529296875\n",
            "classify 2.508514404296875\n",
            "0.0859375\n",
            "0.125\n",
            "0.1171875\n",
            "0.1640625\n",
            "0.109375\n",
            "0.125\n",
            "0.15625\n",
            "0.0859375\n",
            "0.0859375\n",
            "0.0625\n",
            "0.1171875\n",
            "strain 2.5172173976898193\n",
            "strain 0.2857533395290375\n",
            "strain 3.004441499710083\n",
            "strain 5.409858703613281\n",
            "strain 4.169308662414551\n",
            "strain 1.0141419172286987\n",
            "strain 4.007646560668945\n",
            "strain 4.908042907714844\n",
            "strain 3.0634608268737793\n",
            "strain 4.3706817626953125\n",
            "strain 1.1893976926803589\n",
            "strain 0.23481044173240662\n",
            "strain 4.1726908683776855\n",
            "strain 0.11052937805652618\n",
            "strain 3.066241502761841\n",
            "strain 0.7654477953910828\n",
            "strain 3.9306752681732178\n",
            "strain 3.3108932971954346\n",
            "strain 4.536560535430908\n",
            "strain 2.580821990966797\n",
            "strain 0.1961526870727539\n",
            "strain 3.442310333251953\n",
            "strain 4.854739665985107\n",
            "strain 0.09673850983381271\n",
            "strain 0.7433013916015625\n",
            "strain 0.0925525650382042\n",
            "strain 4.682558059692383\n",
            "strain 4.5404815673828125\n",
            "strain 1.4580025672912598\n",
            "strain 3.4193432331085205\n",
            "strain 3.4966955184936523\n",
            "strain 6.354994773864746\n",
            "strain 4.9663872718811035\n",
            "strain 2.363475799560547\n",
            "strain 2.689483642578125\n",
            "strain 2.2813212871551514\n",
            "strain 3.9465694427490234\n",
            "strain 3.20585036277771\n",
            "strain 0.8534020781517029\n",
            "strain 2.45805025100708\n",
            "strain 3.1364405155181885\n",
            "strain 0.6395969390869141\n",
            "strain 2.5545897483825684\n",
            "strain 4.3312668800354\n",
            "strain 5.030600070953369\n",
            "strain 2.089099168777466\n",
            "strain 3.292572021484375\n",
            "strain 0.5532817840576172\n",
            "strain 0.14816096425056458\n",
            "strain 1.7166199684143066\n",
            "strain 1.086779236793518\n",
            "classify 2.610870361328125\n",
            "classify 2.453826904296875\n",
            "classify 2.614288330078125\n",
            "classify 2.718475341796875\n",
            "classify 2.634124755859375\n",
            "classify 2.53564453125\n",
            "classify 2.63592529296875\n",
            "classify 2.519775390625\n",
            "classify 2.590850830078125\n",
            "classify 2.568328857421875\n",
            "classify 2.561065673828125\n",
            "0.0859375\n",
            "0.1171875\n",
            "0.1171875\n",
            "0.1640625\n",
            "0.109375\n",
            "0.125\n",
            "0.15625\n",
            "0.0859375\n",
            "0.0859375\n",
            "0.0625\n",
            "0.1171875\n",
            "strain 1.6148570775985718\n",
            "strain 2.9657092094421387\n",
            "strain 2.6069748401641846\n",
            "strain 1.2187260389328003\n",
            "strain 5.998262405395508\n",
            "strain 0.34159547090530396\n",
            "strain 0.10474643111228943\n",
            "strain 3.218439817428589\n",
            "strain 1.9088289737701416\n",
            "strain 5.478271484375\n",
            "strain 2.4729971885681152\n",
            "strain 2.611873149871826\n",
            "strain 4.512354850769043\n",
            "strain 4.5511345863342285\n",
            "strain 0.042309392243623734\n",
            "strain 2.2898972034454346\n",
            "strain 4.253901958465576\n",
            "strain 4.095403671264648\n",
            "strain 0.054546333849430084\n",
            "strain 4.745367050170898\n",
            "strain 2.0802440643310547\n",
            "strain 0.0353107675909996\n",
            "strain 1.6552634239196777\n",
            "strain 3.5632405281066895\n",
            "strain 2.720640182495117\n",
            "strain 2.2375268936157227\n",
            "strain 2.713613510131836\n",
            "strain 0.5886346101760864\n",
            "strain 1.977278470993042\n",
            "strain 2.773528814315796\n",
            "strain 1.9966882467269897\n",
            "strain 3.3708865642547607\n",
            "strain 4.260159015655518\n",
            "strain 5.7783684730529785\n",
            "strain 0.033473074436187744\n",
            "strain 7.96841287612915\n",
            "strain 2.0664384365081787\n",
            "strain 0.08618602901697159\n",
            "strain 2.8090498447418213\n",
            "strain 4.343155860900879\n",
            "strain 4.9587202072143555\n",
            "strain 2.6790847778320312\n",
            "strain 0.06490836292505264\n",
            "strain 5.36994743347168\n",
            "strain 3.341905355453491\n",
            "strain 4.28731632232666\n",
            "strain 4.84306001663208\n",
            "strain 3.1849513053894043\n",
            "strain 2.3739402294158936\n",
            "strain 5.321082592010498\n",
            "strain 4.378928184509277\n",
            "classify 2.613677978515625\n",
            "classify 2.49725341796875\n",
            "classify 2.652679443359375\n",
            "classify 2.706207275390625\n",
            "classify 2.377685546875\n",
            "classify 2.640228271484375\n",
            "classify 2.446258544921875\n",
            "classify 2.565704345703125\n",
            "classify 2.568756103515625\n",
            "classify 2.465789794921875\n",
            "classify 2.55718994140625\n",
            "0.0859375\n",
            "0.1171875\n",
            "0.1171875\n",
            "0.1640625\n",
            "0.109375\n",
            "0.125\n",
            "0.15625\n",
            "0.0859375\n",
            "0.0859375\n",
            "0.0625\n",
            "0.1171875\n",
            "strain 0.03928736224770546\n",
            "strain 2.5855729579925537\n",
            "strain 1.906980276107788\n",
            "strain 4.15260124206543\n",
            "strain 2.5660130977630615\n",
            "strain 4.611064434051514\n",
            "strain 0.06465834379196167\n",
            "strain 2.6051862239837646\n",
            "strain 1.404530644416809\n",
            "strain 2.993911027908325\n",
            "strain 1.697422742843628\n",
            "strain 2.4698989391326904\n",
            "strain 3.995868444442749\n",
            "strain 6.683237552642822\n",
            "strain 4.0108256340026855\n",
            "strain 4.1496500968933105\n",
            "strain 0.9113329648971558\n",
            "strain 2.016364336013794\n",
            "strain 3.4996700286865234\n",
            "strain 1.54922616481781\n",
            "strain 4.995654582977295\n",
            "strain 1.7072943449020386\n",
            "strain 2.1976184844970703\n",
            "strain 2.103959798812866\n",
            "strain 1.2465170621871948\n",
            "strain 0.16638371348381042\n",
            "strain 4.391241073608398\n",
            "strain 0.14437395334243774\n",
            "strain 3.215263843536377\n",
            "strain 6.328339576721191\n",
            "strain 0.25881433486938477\n",
            "strain 4.590651988983154\n",
            "strain 5.137887477874756\n",
            "strain 3.7490038871765137\n",
            "strain 3.0843589305877686\n",
            "strain 0.6321297883987427\n",
            "strain 5.873046875\n",
            "strain 1.1115092039108276\n",
            "strain 1.7456105947494507\n",
            "strain 2.961282730102539\n",
            "strain 5.017733097076416\n",
            "strain 0.3639451265335083\n",
            "strain 3.1000173091888428\n",
            "strain 4.009895324707031\n",
            "strain 5.258325576782227\n",
            "strain 2.4192676544189453\n",
            "strain 3.2774453163146973\n",
            "strain 2.395036458969116\n",
            "strain 3.870964527130127\n",
            "strain 4.463770866394043\n",
            "strain 4.323098659515381\n",
            "classify 2.537261962890625\n",
            "classify 2.448089599609375\n",
            "classify 2.7642822265625\n",
            "classify 2.6939697265625\n",
            "classify 2.6923828125\n",
            "classify 2.661529541015625\n",
            "classify 2.57000732421875\n",
            "classify 2.58721923828125\n",
            "classify 2.69793701171875\n",
            "classify 2.724761962890625\n",
            "classify 2.603729248046875\n",
            "0.0859375\n",
            "0.1171875\n",
            "0.109375\n",
            "0.1640625\n",
            "0.109375\n",
            "0.1328125\n",
            "0.1484375\n",
            "0.0859375\n",
            "0.078125\n",
            "0.0625\n",
            "0.1171875\n",
            "strain 1.7699331045150757\n",
            "strain 4.35651159286499\n",
            "strain 0.09082924574613571\n",
            "strain 2.7184903621673584\n",
            "strain 1.2267059087753296\n",
            "strain 1.7665785551071167\n",
            "strain 4.8621087074279785\n",
            "strain 3.389220714569092\n",
            "strain 2.760740280151367\n",
            "strain 1.3140983581542969\n",
            "strain 3.7359619140625\n",
            "strain 0.9820918440818787\n",
            "strain 3.9131367206573486\n",
            "strain 1.9663453102111816\n",
            "strain 3.2312090396881104\n",
            "strain 3.7289464473724365\n",
            "strain 3.845048427581787\n",
            "strain 1.7213189601898193\n",
            "strain 4.581158638000488\n",
            "strain 0.14270737767219543\n",
            "strain 0.24372422695159912\n",
            "strain 4.124939918518066\n",
            "strain 0.6645082831382751\n",
            "strain 4.29964542388916\n",
            "strain 2.3598010540008545\n",
            "strain 7.397017955780029\n",
            "strain 2.0765209197998047\n",
            "strain 0.15909627079963684\n",
            "strain 0.7383288145065308\n",
            "strain 0.7406579852104187\n",
            "strain 1.289581060409546\n",
            "strain 4.063424587249756\n",
            "strain 0.6776780486106873\n",
            "strain 4.430285453796387\n",
            "strain 3.9213826656341553\n",
            "strain 4.02492618560791\n",
            "strain 0.0825754925608635\n",
            "strain 1.5544869899749756\n",
            "strain 3.081136465072632\n",
            "strain 1.638903260231018\n",
            "strain 3.7717626094818115\n",
            "strain 0.4964887499809265\n",
            "strain 3.068885326385498\n",
            "strain 3.4452216625213623\n",
            "strain 1.6535090208053589\n",
            "strain 2.1912178993225098\n",
            "strain 4.304394245147705\n",
            "strain 4.609586715698242\n",
            "strain 2.688319206237793\n",
            "strain 1.5060272216796875\n",
            "strain 3.0128235816955566\n",
            "classify 2.503692626953125\n",
            "classify 2.63726806640625\n",
            "classify 2.515411376953125\n",
            "classify 2.604095458984375\n",
            "classify 2.58795166015625\n",
            "classify 2.658966064453125\n",
            "classify 2.625946044921875\n",
            "classify 2.612701416015625\n",
            "classify 2.610015869140625\n",
            "classify 2.58563232421875\n",
            "classify 2.597412109375\n",
            "0.0859375\n",
            "0.1171875\n",
            "0.109375\n",
            "0.1640625\n",
            "0.109375\n",
            "0.1328125\n",
            "0.1484375\n",
            "0.0859375\n",
            "0.078125\n",
            "0.0625\n",
            "0.1171875\n",
            "strain 1.6106120347976685\n",
            "strain 1.0810503959655762\n",
            "strain 0.3054064214229584\n",
            "strain 4.09152889251709\n",
            "strain 0.5359675288200378\n",
            "strain 5.4604034423828125\n",
            "strain 3.119494676589966\n",
            "strain 3.6102826595306396\n",
            "strain 3.714970111846924\n",
            "strain 5.501609802246094\n",
            "strain 4.450972080230713\n",
            "strain 1.9277273416519165\n",
            "strain 3.4546711444854736\n",
            "strain 0.7420926094055176\n",
            "strain 1.5926915407180786\n",
            "strain 2.9544661045074463\n",
            "strain 3.6394965648651123\n",
            "strain 5.602940082550049\n",
            "strain 5.390646934509277\n",
            "strain 1.1166514158248901\n",
            "strain 1.0808311700820923\n",
            "strain 4.513774394989014\n",
            "strain 1.1303205490112305\n",
            "strain 2.9914350509643555\n",
            "strain 2.7638602256774902\n",
            "strain 6.4797821044921875\n",
            "strain 1.8254896402359009\n",
            "strain 4.139232635498047\n",
            "strain 4.352360725402832\n",
            "strain 2.199357748031616\n",
            "strain 3.3386082649230957\n",
            "strain 3.1117591857910156\n",
            "strain 3.9938039779663086\n",
            "strain 2.30295729637146\n",
            "strain 0.5096034407615662\n",
            "strain 4.211777210235596\n",
            "strain 2.2201414108276367\n",
            "strain 5.004518985748291\n",
            "strain 3.783214569091797\n",
            "strain 4.569977760314941\n",
            "strain 2.8512697219848633\n",
            "strain 2.9900248050689697\n",
            "strain 3.3155949115753174\n",
            "strain 2.875229835510254\n",
            "strain 0.9280919432640076\n",
            "strain 3.918484687805176\n",
            "strain 2.759946584701538\n",
            "strain 2.972529172897339\n",
            "strain 4.788249492645264\n",
            "strain 0.06055232509970665\n",
            "strain 5.198711395263672\n",
            "classify 2.8663330078125\n",
            "classify 2.584228515625\n",
            "classify 2.552398681640625\n",
            "classify 2.690185546875\n",
            "classify 2.610626220703125\n",
            "classify 2.63653564453125\n",
            "classify 2.573333740234375\n",
            "classify 2.50537109375\n",
            "classify 2.4208984375\n",
            "classify 2.640289306640625\n",
            "classify 2.609527587890625\n",
            "0.09375\n",
            "0.125\n",
            "0.109375\n",
            "0.1640625\n",
            "0.109375\n",
            "0.125\n",
            "0.1484375\n",
            "0.0859375\n",
            "0.0859375\n",
            "0.0625\n",
            "0.1171875\n",
            "strain 2.386798620223999\n",
            "strain 3.5304651260375977\n",
            "strain 3.387132167816162\n",
            "strain 3.160383701324463\n",
            "strain 2.679135322570801\n",
            "strain 0.12807072699069977\n",
            "strain 0.17003314197063446\n",
            "strain 1.2356826066970825\n",
            "strain 2.4120559692382812\n",
            "strain 6.905755043029785\n",
            "strain 0.09082895517349243\n",
            "strain 4.1325225830078125\n",
            "strain 4.597747802734375\n",
            "strain 1.8361033201217651\n",
            "strain 4.4629435539245605\n",
            "strain 4.197878837585449\n",
            "strain 1.8421475887298584\n",
            "strain 4.409060955047607\n",
            "strain 0.5590788722038269\n",
            "strain 4.673779487609863\n",
            "strain 4.246369361877441\n",
            "strain 1.343681812286377\n",
            "strain 3.9941787719726562\n",
            "strain 5.888795375823975\n",
            "strain 0.24313732981681824\n",
            "strain 1.361701488494873\n",
            "strain 3.8528225421905518\n",
            "strain 5.947824954986572\n",
            "strain 1.8676837682724\n",
            "strain 3.534158706665039\n",
            "strain 3.2730965614318848\n",
            "strain 3.2344894409179688\n",
            "strain 2.293607473373413\n",
            "strain 2.9440526962280273\n",
            "strain 5.657549858093262\n",
            "strain 3.99630069732666\n",
            "strain 0.046410560607910156\n",
            "strain 4.1562981605529785\n",
            "strain 2.4582462310791016\n",
            "strain 4.652153491973877\n",
            "strain 5.63861608505249\n",
            "strain 0.6324132084846497\n",
            "strain 4.0727152824401855\n",
            "strain 3.66288685798645\n",
            "strain 4.454675674438477\n",
            "strain 0.03518827259540558\n",
            "strain 0.49822741746902466\n",
            "strain 0.9443790316581726\n",
            "strain 4.433783531188965\n",
            "strain 2.9094676971435547\n",
            "strain 3.394690990447998\n",
            "classify 2.667327880859375\n",
            "classify 2.57476806640625\n",
            "classify 2.729248046875\n",
            "classify 2.535614013671875\n",
            "classify 2.518768310546875\n",
            "classify 2.683074951171875\n",
            "classify 2.47003173828125\n",
            "classify 2.709869384765625\n",
            "classify 2.55340576171875\n",
            "classify 2.49786376953125\n",
            "classify 2.58514404296875\n",
            "0.09375\n",
            "0.1171875\n",
            "0.109375\n",
            "0.1640625\n",
            "0.109375\n",
            "0.1328125\n",
            "0.140625\n",
            "0.0859375\n",
            "0.078125\n",
            "0.0625\n",
            "0.1171875\n",
            "strain 0.6854873895645142\n",
            "strain 4.524947166442871\n",
            "strain 6.168086528778076\n",
            "strain 2.5752811431884766\n",
            "strain 0.5585417151451111\n",
            "strain 4.292399883270264\n",
            "strain 4.9227614402771\n",
            "strain 5.062304973602295\n",
            "strain 2.979468822479248\n",
            "strain 1.564205527305603\n",
            "strain 2.725907564163208\n",
            "strain 1.371862769126892\n",
            "strain 3.4588115215301514\n",
            "strain 1.2027925252914429\n",
            "strain 2.351189136505127\n",
            "strain 4.539668083190918\n",
            "strain 3.6624820232391357\n",
            "strain 3.6860413551330566\n",
            "strain 2.837035894393921\n",
            "strain 2.047057628631592\n",
            "strain 2.3987722396850586\n",
            "strain 3.7859444618225098\n",
            "strain 4.271088600158691\n",
            "strain 0.06572479754686356\n",
            "strain 1.755378007888794\n",
            "strain 3.145399808883667\n",
            "strain 4.873095989227295\n",
            "strain 0.0531257688999176\n",
            "strain 2.5629382133483887\n",
            "strain 6.2745680809021\n",
            "strain 3.0019142627716064\n",
            "strain 0.9146108031272888\n",
            "strain 3.5471861362457275\n",
            "strain 0.12368913739919662\n",
            "strain 2.8404576778411865\n",
            "strain 3.873014450073242\n",
            "strain 4.4892778396606445\n",
            "strain 2.7882697582244873\n",
            "strain 0.09120168536901474\n",
            "strain 1.1314092874526978\n",
            "strain 1.4091161489486694\n",
            "strain 1.3723740577697754\n",
            "strain 2.4803671836853027\n",
            "strain 4.29600191116333\n",
            "strain 2.371229887008667\n",
            "strain 2.9567081928253174\n",
            "strain 4.325982570648193\n",
            "strain 4.62494421005249\n",
            "strain 2.6084482669830322\n",
            "strain 3.0241775512695312\n",
            "strain 5.439493179321289\n",
            "classify 2.59820556640625\n",
            "classify 2.561920166015625\n",
            "classify 2.487335205078125\n",
            "classify 2.494659423828125\n",
            "classify 2.572113037109375\n",
            "classify 2.7220458984375\n",
            "classify 2.65789794921875\n",
            "classify 2.59002685546875\n",
            "classify 2.54931640625\n",
            "classify 2.5791015625\n",
            "classify 2.5548095703125\n",
            "0.09375\n",
            "0.125\n",
            "0.109375\n",
            "0.1640625\n",
            "0.109375\n",
            "0.1328125\n",
            "0.140625\n",
            "0.0859375\n",
            "0.078125\n",
            "0.0625\n",
            "0.1171875\n",
            "strain 7.565903663635254\n",
            "strain 2.9443888664245605\n",
            "strain 0.5938845872879028\n",
            "strain 3.298003673553467\n",
            "strain 5.584484100341797\n",
            "strain 2.0699234008789062\n",
            "strain 1.5739213228225708\n",
            "strain 1.8896872997283936\n",
            "strain 2.9054155349731445\n",
            "strain 0.2292911410331726\n",
            "strain 1.6781270503997803\n",
            "strain 4.188507080078125\n",
            "strain 1.1573231220245361\n",
            "strain 4.008735656738281\n",
            "strain 3.5496809482574463\n",
            "strain 0.0582316592335701\n",
            "strain 3.803797960281372\n",
            "strain 3.1931846141815186\n",
            "strain 0.06658519804477692\n",
            "strain 4.331519603729248\n",
            "strain 5.347733974456787\n",
            "strain 4.443961143493652\n",
            "strain 4.067407131195068\n",
            "strain 4.4158935546875\n",
            "strain 4.781116962432861\n",
            "strain 1.300676941871643\n",
            "strain 4.053910732269287\n",
            "strain 3.45873761177063\n",
            "strain 4.133350849151611\n",
            "strain 2.486543893814087\n",
            "strain 0.6881746649742126\n",
            "strain 0.4850134551525116\n",
            "strain 0.35415738821029663\n",
            "strain 2.0025787353515625\n",
            "strain 2.586297035217285\n",
            "strain 3.8246729373931885\n",
            "strain 2.9280641078948975\n",
            "strain 0.3420020043849945\n",
            "strain 3.6002604961395264\n",
            "strain 2.2234036922454834\n",
            "strain 3.716797351837158\n",
            "strain 0.37508219480514526\n",
            "strain 2.943405866622925\n",
            "strain 3.2711291313171387\n",
            "strain 4.496852874755859\n",
            "strain 3.862077236175537\n",
            "strain 0.8524593710899353\n",
            "strain 3.3193747997283936\n",
            "strain 5.717268943786621\n",
            "strain 1.8588680028915405\n",
            "strain 2.1697113513946533\n",
            "classify 2.59869384765625\n",
            "classify 2.686279296875\n",
            "classify 2.761810302734375\n",
            "classify 2.507476806640625\n",
            "classify 2.5965576171875\n",
            "classify 2.58843994140625\n",
            "classify 2.547393798828125\n",
            "classify 2.62841796875\n",
            "classify 2.533416748046875\n",
            "classify 2.531494140625\n",
            "classify 2.59503173828125\n",
            "0.09375\n",
            "0.125\n",
            "0.109375\n",
            "0.1640625\n",
            "0.1171875\n",
            "0.1328125\n",
            "0.140625\n",
            "0.0859375\n",
            "0.078125\n",
            "0.0625\n",
            "0.1171875\n",
            "strain 0.07633846253156662\n",
            "strain 3.219921827316284\n",
            "strain 3.7577381134033203\n",
            "strain 3.234057903289795\n",
            "strain 1.407508373260498\n",
            "strain 2.436122417449951\n",
            "strain 3.380770206451416\n",
            "strain 0.05193518474698067\n",
            "strain 4.066515922546387\n",
            "strain 3.8554391860961914\n",
            "strain 2.1717052459716797\n",
            "strain 1.590719223022461\n",
            "strain 2.753396987915039\n",
            "strain 3.413303852081299\n",
            "strain 1.2041752338409424\n",
            "strain 4.121927738189697\n",
            "strain 5.977007865905762\n",
            "strain 2.076662302017212\n",
            "strain 4.466662406921387\n",
            "strain 2.942532777786255\n",
            "strain 4.977289199829102\n",
            "strain 3.8037495613098145\n",
            "strain 0.9333575367927551\n",
            "strain 5.557363033294678\n",
            "strain 2.7118210792541504\n",
            "strain 3.974766731262207\n",
            "strain 0.10448821634054184\n",
            "strain 0.3232036232948303\n",
            "strain 4.198482513427734\n",
            "strain 3.1265714168548584\n",
            "strain 4.137779235839844\n",
            "strain 3.937971353530884\n",
            "strain 4.140625953674316\n",
            "strain 3.059244394302368\n",
            "strain 3.790797233581543\n",
            "strain 0.07624951004981995\n",
            "strain 2.6233434677124023\n",
            "strain 2.2381091117858887\n",
            "strain 0.07981399446725845\n",
            "strain 1.7798047065734863\n",
            "strain 3.5335640907287598\n",
            "strain 5.637656211853027\n",
            "strain 5.389334678649902\n",
            "strain 2.019972801208496\n",
            "strain 3.845696210861206\n",
            "strain 1.748356819152832\n",
            "strain 0.06306338310241699\n",
            "strain 5.739477157592773\n",
            "strain 5.924717903137207\n",
            "strain 0.05483974516391754\n",
            "strain 3.7780368328094482\n",
            "classify 2.704986572265625\n",
            "classify 2.650634765625\n",
            "classify 2.61212158203125\n",
            "classify 2.59857177734375\n",
            "classify 2.5628662109375\n",
            "classify 2.62225341796875\n",
            "classify 2.756134033203125\n",
            "classify 2.53143310546875\n",
            "classify 2.54193115234375\n",
            "classify 2.526641845703125\n",
            "classify 2.5194091796875\n",
            "0.09375\n",
            "0.125\n",
            "0.109375\n",
            "0.1640625\n",
            "0.1171875\n",
            "0.140625\n",
            "0.140625\n",
            "0.0859375\n",
            "0.078125\n",
            "0.0625\n",
            "0.1171875\n",
            "strain 3.7550463676452637\n",
            "strain 2.405609607696533\n",
            "strain 5.285707473754883\n",
            "strain 2.3009262084960938\n",
            "strain 4.526152610778809\n",
            "strain 4.88327693939209\n",
            "strain 2.373802900314331\n",
            "strain 3.0360794067382812\n",
            "strain 4.071235179901123\n",
            "strain 1.2218377590179443\n",
            "strain 0.06321961432695389\n",
            "strain 3.8865926265716553\n",
            "strain 1.0071080923080444\n",
            "strain 0.31492024660110474\n",
            "strain 5.324143886566162\n",
            "strain 2.73439884185791\n",
            "strain 2.119495153427124\n",
            "strain 3.897451400756836\n",
            "strain 3.577566385269165\n",
            "strain 3.34411358833313\n",
            "strain 2.8537793159484863\n",
            "strain 1.555906891822815\n",
            "strain 3.734304428100586\n",
            "strain 1.1028474569320679\n",
            "strain 3.5799460411071777\n",
            "strain 5.221741199493408\n",
            "strain 1.5756441354751587\n",
            "strain 2.39701509475708\n",
            "strain 0.069954052567482\n",
            "strain 4.11530876159668\n",
            "strain 3.072791337966919\n",
            "strain 1.3993842601776123\n",
            "strain 0.945615291595459\n",
            "strain 3.4092791080474854\n",
            "strain 4.409054756164551\n",
            "strain 0.05683133378624916\n",
            "strain 3.2903459072113037\n",
            "strain 3.2546005249023438\n",
            "strain 3.553983211517334\n",
            "strain 2.845510959625244\n",
            "strain 0.9156840443611145\n",
            "strain 5.035083293914795\n",
            "strain 2.9442365169525146\n",
            "strain 5.149655818939209\n",
            "strain 1.9726730585098267\n",
            "strain 2.5158801078796387\n",
            "strain 5.776989936828613\n",
            "strain 5.978514671325684\n",
            "strain 2.28395414352417\n",
            "strain 2.569108247756958\n",
            "strain 0.6086236834526062\n",
            "classify 2.48431396484375\n",
            "classify 2.60882568359375\n",
            "classify 2.543212890625\n",
            "classify 2.683746337890625\n",
            "classify 2.605743408203125\n",
            "classify 2.48162841796875\n",
            "classify 2.478790283203125\n",
            "classify 2.647552490234375\n",
            "classify 2.611083984375\n",
            "classify 2.47906494140625\n",
            "classify 2.56610107421875\n",
            "0.09375\n",
            "0.125\n",
            "0.109375\n",
            "0.1640625\n",
            "0.1171875\n",
            "0.1328125\n",
            "0.140625\n",
            "0.0859375\n",
            "0.078125\n",
            "0.0625\n",
            "0.1171875\n",
            "strain 0.26575449109077454\n",
            "strain 1.7240525484085083\n",
            "strain 3.8912882804870605\n",
            "strain 2.7117509841918945\n",
            "strain 3.3844921588897705\n",
            "strain 0.8946293592453003\n",
            "strain 4.545119762420654\n",
            "strain 3.263369083404541\n",
            "strain 1.0450562238693237\n",
            "strain 2.2911479473114014\n",
            "strain 2.1697258949279785\n",
            "strain 1.2138973474502563\n",
            "strain 2.96511173248291\n",
            "strain 2.9387998580932617\n",
            "strain 0.6118403673171997\n",
            "strain 4.2659592628479\n",
            "strain 3.0693492889404297\n",
            "strain 4.039610862731934\n",
            "strain 0.5169745683670044\n",
            "strain 3.3373806476593018\n",
            "strain 2.4913926124572754\n",
            "strain 3.671722412109375\n",
            "strain 1.1114143133163452\n",
            "strain 2.9157817363739014\n",
            "strain 4.104733943939209\n",
            "strain 0.14074935019016266\n",
            "strain 6.183445930480957\n",
            "strain 0.2762317657470703\n",
            "strain 2.9747848510742188\n",
            "strain 4.725406169891357\n",
            "strain 4.629922866821289\n",
            "strain 4.751889228820801\n",
            "strain 1.163735032081604\n",
            "strain 0.2919189929962158\n",
            "strain 2.5616652965545654\n",
            "strain 0.13629098236560822\n",
            "strain 1.0105043649673462\n",
            "strain 5.912150859832764\n",
            "strain 0.6392679214477539\n",
            "strain 2.5737760066986084\n",
            "strain 2.889864683151245\n",
            "strain 0.9074785113334656\n",
            "strain 3.011930227279663\n",
            "strain 0.8607797026634216\n",
            "strain 4.759709358215332\n",
            "strain 1.5440952777862549\n",
            "strain 2.4652087688446045\n",
            "strain 4.530492305755615\n",
            "strain 4.3060622215271\n",
            "strain 4.403441905975342\n",
            "strain 1.1292682886123657\n",
            "classify 2.431304931640625\n",
            "classify 2.629302978515625\n",
            "classify 2.5665283203125\n",
            "classify 2.592681884765625\n",
            "classify 2.54974365234375\n",
            "classify 2.629638671875\n",
            "classify 2.490325927734375\n",
            "classify 2.657745361328125\n",
            "classify 2.658050537109375\n",
            "classify 2.564056396484375\n",
            "classify 2.650543212890625\n",
            "0.09375\n",
            "0.125\n",
            "0.109375\n",
            "0.1640625\n",
            "0.1171875\n",
            "0.140625\n",
            "0.140625\n",
            "0.0859375\n",
            "0.078125\n",
            "0.0625\n",
            "0.1171875\n",
            "strain 0.03881869465112686\n",
            "strain 0.9905081987380981\n",
            "strain 4.092152118682861\n",
            "strain 3.298069715499878\n",
            "strain 2.8434865474700928\n",
            "strain 3.356597900390625\n",
            "strain 3.658689022064209\n",
            "strain 3.360056161880493\n",
            "strain 0.7950568795204163\n",
            "strain 3.324244737625122\n",
            "strain 0.9347671270370483\n",
            "strain 2.173166275024414\n",
            "strain 0.6045104265213013\n",
            "strain 4.985409736633301\n",
            "strain 1.9466042518615723\n",
            "strain 1.8659512996673584\n",
            "strain 1.8155512809753418\n",
            "strain 2.5330071449279785\n",
            "strain 4.458657264709473\n",
            "strain 1.54571533203125\n",
            "strain 6.270293712615967\n",
            "strain 0.4477553963661194\n",
            "strain 2.048555374145508\n",
            "strain 1.2557473182678223\n",
            "strain 0.34880000352859497\n",
            "strain 3.271724224090576\n",
            "strain 3.3600223064422607\n",
            "strain 0.9031652212142944\n",
            "strain 2.8657870292663574\n",
            "strain 2.8300342559814453\n",
            "strain 6.1096978187561035\n",
            "strain 2.6347415447235107\n",
            "strain 4.241583824157715\n",
            "strain 2.718747615814209\n",
            "strain 4.060957431793213\n",
            "strain 2.7064099311828613\n",
            "strain 0.06709220260381699\n",
            "strain 2.3967843055725098\n",
            "strain 2.3337314128875732\n",
            "strain 0.3794025480747223\n",
            "strain 0.9025823473930359\n",
            "strain 1.3576844930648804\n",
            "strain 0.352920800447464\n",
            "strain 1.7734020948410034\n",
            "strain 5.007200241088867\n",
            "strain 1.6076563596725464\n",
            "strain 4.293035984039307\n",
            "strain 2.5986945629119873\n",
            "strain 0.9334251284599304\n",
            "strain 1.0484806299209595\n",
            "strain 0.13679111003875732\n",
            "classify 2.598052978515625\n",
            "classify 2.712554931640625\n",
            "classify 2.517730712890625\n",
            "classify 2.819549560546875\n",
            "classify 2.600616455078125\n",
            "classify 2.5819091796875\n",
            "classify 2.53680419921875\n",
            "classify 2.548980712890625\n",
            "classify 2.597900390625\n",
            "classify 2.407562255859375\n",
            "classify 2.398468017578125\n",
            "0.09375\n",
            "0.125\n",
            "0.109375\n",
            "0.1640625\n",
            "0.1171875\n",
            "0.140625\n",
            "0.140625\n",
            "0.0859375\n",
            "0.078125\n",
            "0.0625\n",
            "0.1171875\n",
            "strain 4.8416748046875\n",
            "strain 1.7187769412994385\n",
            "strain 1.501346230506897\n",
            "strain 5.30758810043335\n",
            "strain 4.874340057373047\n",
            "strain 2.0398688316345215\n",
            "strain 0.15061946213245392\n",
            "strain 5.00784158706665\n",
            "strain 4.733723163604736\n",
            "strain 4.832378387451172\n",
            "strain 4.764251708984375\n",
            "strain 1.4909236431121826\n",
            "strain 2.7221460342407227\n",
            "strain 0.2742392420768738\n",
            "strain 4.32440185546875\n",
            "strain 0.2597988247871399\n",
            "strain 5.7938947677612305\n",
            "strain 4.211833477020264\n",
            "strain 2.6549105644226074\n",
            "strain 3.848599672317505\n",
            "strain 3.677213430404663\n",
            "strain 1.1345936059951782\n",
            "strain 6.958962440490723\n",
            "strain 0.5264142155647278\n",
            "strain 2.2183520793914795\n",
            "strain 7.0769362449646\n",
            "strain 3.2263710498809814\n",
            "strain 1.3275721073150635\n",
            "strain 4.40923547744751\n",
            "strain 2.6434645652770996\n",
            "strain 2.237664222717285\n",
            "strain 4.622287273406982\n",
            "strain 2.1733880043029785\n",
            "strain 4.135342597961426\n",
            "strain 4.500910758972168\n",
            "strain 0.3344840407371521\n",
            "strain 3.782484292984009\n",
            "strain 2.8704872131347656\n",
            "strain 2.921009063720703\n",
            "strain 3.9569811820983887\n",
            "strain 4.633744239807129\n",
            "strain 0.5277933478355408\n",
            "strain 2.1909661293029785\n",
            "strain 2.34689998626709\n",
            "strain 4.4411211013793945\n",
            "strain 5.26285982131958\n",
            "strain 2.2569165229797363\n",
            "strain 1.8025455474853516\n",
            "strain 4.643272399902344\n",
            "strain 2.4385786056518555\n",
            "strain 3.985715866088867\n",
            "classify 2.63458251953125\n",
            "classify 2.418670654296875\n",
            "classify 2.468048095703125\n",
            "classify 2.60711669921875\n",
            "classify 2.64794921875\n",
            "classify 2.583465576171875\n",
            "classify 2.625640869140625\n",
            "classify 2.469390869140625\n",
            "classify 2.718170166015625\n",
            "classify 2.633758544921875\n",
            "classify 2.628082275390625\n",
            "0.09375\n",
            "0.125\n",
            "0.109375\n",
            "0.1640625\n",
            "0.1171875\n",
            "0.140625\n",
            "0.140625\n",
            "0.0859375\n",
            "0.078125\n",
            "0.0625\n",
            "0.1171875\n",
            "strain 5.8343505859375\n",
            "strain 0.8835778832435608\n",
            "strain 0.906646192073822\n",
            "strain 3.316068172454834\n",
            "strain 6.583098411560059\n",
            "strain 4.531961917877197\n",
            "strain 2.490805149078369\n",
            "strain 0.7985202074050903\n",
            "strain 6.056735515594482\n",
            "strain 5.162555694580078\n",
            "strain 4.397640705108643\n",
            "strain 1.553523302078247\n",
            "strain 2.5338754653930664\n",
            "strain 2.527785062789917\n",
            "strain 5.214936256408691\n",
            "strain 2.3459694385528564\n",
            "strain 4.7097859382629395\n",
            "strain 1.6286896467208862\n",
            "strain 3.1284279823303223\n",
            "strain 0.8556796312332153\n",
            "strain 4.257564544677734\n",
            "strain 3.442952871322632\n",
            "strain 0.57811439037323\n",
            "strain 3.158435583114624\n",
            "strain 5.747391700744629\n",
            "strain 0.9000872373580933\n",
            "strain 3.2545225620269775\n",
            "strain 2.2867822647094727\n",
            "strain 0.620638370513916\n",
            "strain 0.16411316394805908\n",
            "strain 1.6119450330734253\n",
            "strain 0.2521117627620697\n",
            "strain 2.9715611934661865\n",
            "strain 4.1980791091918945\n",
            "strain 5.528220176696777\n",
            "strain 5.200572490692139\n",
            "strain 2.0717010498046875\n",
            "strain 5.376956462860107\n",
            "strain 5.162524223327637\n",
            "strain 3.029686212539673\n",
            "strain 3.9931492805480957\n",
            "strain 2.3386402130126953\n",
            "strain 4.468100070953369\n",
            "strain 6.12127161026001\n",
            "strain 2.2540764808654785\n",
            "strain 3.7589688301086426\n",
            "strain 1.5314013957977295\n",
            "strain 1.4432623386383057\n",
            "strain 6.081640720367432\n",
            "strain 3.9499335289001465\n",
            "strain 0.34702634811401367\n",
            "classify 2.65185546875\n",
            "classify 2.57464599609375\n",
            "classify 2.68975830078125\n",
            "classify 2.587860107421875\n",
            "classify 2.50738525390625\n",
            "classify 2.60711669921875\n",
            "classify 2.649139404296875\n",
            "classify 2.527252197265625\n",
            "classify 2.47650146484375\n",
            "classify 2.595794677734375\n",
            "classify 2.57574462890625\n",
            "0.09375\n",
            "0.125\n",
            "0.109375\n",
            "0.1640625\n",
            "0.1171875\n",
            "0.1328125\n",
            "0.140625\n",
            "0.0859375\n",
            "0.078125\n",
            "0.0625\n",
            "0.1171875\n",
            "strain 1.5355292558670044\n",
            "strain 3.207486867904663\n",
            "strain 2.1967220306396484\n",
            "strain 2.6788790225982666\n",
            "strain 3.180405855178833\n",
            "strain 4.009424686431885\n",
            "strain 1.4259916543960571\n",
            "strain 4.456945896148682\n",
            "strain 6.189150810241699\n",
            "strain 4.350597381591797\n",
            "strain 4.209803104400635\n",
            "strain 4.141328811645508\n",
            "strain 3.524251937866211\n",
            "strain 3.9340696334838867\n",
            "strain 4.459077835083008\n",
            "strain 4.82230281829834\n",
            "strain 2.9228150844573975\n",
            "strain 0.15155141055583954\n",
            "strain 0.21803723275661469\n",
            "strain 2.8287353515625\n",
            "strain 6.502809047698975\n",
            "strain 3.326326847076416\n",
            "strain 3.534512996673584\n",
            "strain 2.238136053085327\n",
            "strain 2.1933767795562744\n",
            "strain 4.746252536773682\n",
            "strain 4.006845474243164\n",
            "strain 0.28258687257766724\n",
            "strain 2.174764394760132\n",
            "strain 1.3793673515319824\n",
            "strain 5.1096696853637695\n",
            "strain 0.2976055145263672\n",
            "strain 3.7221217155456543\n",
            "strain 0.5051811337471008\n",
            "strain 0.13052743673324585\n",
            "strain 2.4609858989715576\n",
            "strain 4.171831130981445\n",
            "strain 4.4615583419799805\n",
            "strain 3.0835390090942383\n",
            "strain 5.002340316772461\n",
            "strain 3.2495276927948\n",
            "strain 5.342344284057617\n",
            "strain 2.7730495929718018\n",
            "strain 0.5597907304763794\n",
            "strain 0.482198029756546\n",
            "strain 0.8029219508171082\n",
            "strain 4.070563793182373\n",
            "strain 3.577554225921631\n",
            "strain 3.617525339126587\n",
            "strain 0.1414814591407776\n",
            "strain 3.4681801795959473\n",
            "classify 2.515411376953125\n",
            "classify 2.588104248046875\n",
            "classify 2.682769775390625\n",
            "classify 2.583221435546875\n",
            "classify 2.479339599609375\n",
            "classify 2.730194091796875\n",
            "classify 2.5382080078125\n",
            "classify 2.463348388671875\n",
            "classify 2.538177490234375\n",
            "classify 2.5247802734375\n",
            "classify 2.576873779296875\n",
            "0.09375\n",
            "0.125\n",
            "0.109375\n",
            "0.1640625\n",
            "0.1171875\n",
            "0.1328125\n",
            "0.140625\n",
            "0.0859375\n",
            "0.078125\n",
            "0.0625\n",
            "0.1171875\n",
            "strain 5.030510902404785\n",
            "strain 1.4929907321929932\n",
            "strain 1.977494478225708\n",
            "strain 2.7375786304473877\n",
            "strain 3.936828374862671\n",
            "strain 4.361472129821777\n",
            "strain 3.641810178756714\n",
            "strain 4.310617446899414\n",
            "strain 2.7940776348114014\n",
            "strain 4.304119110107422\n",
            "strain 4.6774396896362305\n",
            "strain 3.9526562690734863\n",
            "strain 2.858827590942383\n",
            "strain 4.137106418609619\n",
            "strain 1.5447760820388794\n",
            "strain 4.67362117767334\n",
            "strain 0.5173699259757996\n",
            "strain 4.274677753448486\n",
            "strain 1.5899884700775146\n",
            "strain 5.220703125\n",
            "strain 1.3840144872665405\n",
            "strain 0.284819096326828\n",
            "strain 0.6219158172607422\n",
            "strain 0.8794118762016296\n",
            "strain 3.702167272567749\n",
            "strain 2.7922353744506836\n",
            "strain 4.067072868347168\n",
            "strain 4.533870697021484\n",
            "strain 2.075235605239868\n",
            "strain 2.8447203636169434\n",
            "strain 1.620025396347046\n",
            "strain 3.7578370571136475\n",
            "strain 6.188912868499756\n",
            "strain 4.460088729858398\n",
            "strain 1.2556730508804321\n",
            "strain 3.8433797359466553\n",
            "strain 1.5192464590072632\n",
            "strain 4.641355037689209\n",
            "strain 3.614794969558716\n",
            "strain 4.871701240539551\n",
            "strain 2.480591058731079\n",
            "strain 2.2678768634796143\n",
            "strain 2.229518175125122\n",
            "strain 2.6646013259887695\n",
            "strain 0.6257908940315247\n",
            "strain 8.154284477233887\n",
            "strain 0.13547924160957336\n",
            "strain 4.011422634124756\n",
            "strain 3.017514228820801\n",
            "strain 3.0440847873687744\n",
            "strain 2.0317115783691406\n",
            "classify 2.66778564453125\n",
            "classify 2.544952392578125\n",
            "classify 2.5718994140625\n",
            "classify 2.743316650390625\n",
            "classify 2.60186767578125\n",
            "classify 2.50213623046875\n",
            "classify 2.528594970703125\n",
            "classify 2.459686279296875\n",
            "classify 2.5252685546875\n",
            "classify 2.535369873046875\n",
            "classify 2.662841796875\n",
            "0.09375\n",
            "0.125\n",
            "0.109375\n",
            "0.1640625\n",
            "0.1171875\n",
            "0.1328125\n",
            "0.140625\n",
            "0.0859375\n",
            "0.078125\n",
            "0.0703125\n",
            "0.1171875\n",
            "strain 1.2898234128952026\n",
            "strain 4.857219696044922\n",
            "strain 1.2646806240081787\n",
            "strain 4.001945495605469\n",
            "strain 4.947930812835693\n",
            "strain 1.915147304534912\n",
            "strain 5.486722946166992\n",
            "strain 2.8690404891967773\n",
            "strain 0.4218243360519409\n",
            "strain 2.380382537841797\n",
            "strain 2.218411445617676\n",
            "strain 4.3309431076049805\n",
            "strain 6.62941312789917\n",
            "strain 1.085838794708252\n",
            "strain 3.844075918197632\n",
            "strain 0.30298495292663574\n",
            "strain 0.581019937992096\n",
            "strain 0.26069894433021545\n",
            "strain 2.5851993560791016\n",
            "strain 3.3486831188201904\n",
            "strain 6.039791107177734\n",
            "strain 0.3481255769729614\n",
            "strain 4.031667709350586\n",
            "strain 2.416642427444458\n",
            "strain 2.0319132804870605\n",
            "strain 0.2704598307609558\n",
            "strain 2.0812199115753174\n",
            "strain 5.803199291229248\n",
            "strain 4.2256855964660645\n",
            "strain 0.5188994407653809\n",
            "strain 2.7391514778137207\n",
            "strain 3.7769901752471924\n",
            "strain 1.0327670574188232\n",
            "strain 2.738870620727539\n",
            "strain 3.968649387359619\n",
            "strain 3.3980538845062256\n",
            "strain 0.18536226451396942\n",
            "strain 5.392252445220947\n",
            "strain 0.8425315618515015\n",
            "strain 3.21036958694458\n",
            "strain 1.2339186668395996\n",
            "strain 0.6944085955619812\n",
            "strain 3.0889687538146973\n",
            "strain 3.653651714324951\n",
            "strain 1.4251879453659058\n",
            "strain 1.905273199081421\n",
            "strain 0.9867119789123535\n",
            "strain 5.019583225250244\n",
            "strain 4.903860092163086\n",
            "strain 2.4337217807769775\n",
            "strain 0.9061752557754517\n",
            "classify 2.640106201171875\n",
            "classify 2.591583251953125\n",
            "classify 2.50433349609375\n",
            "classify 2.61956787109375\n",
            "classify 2.61553955078125\n",
            "classify 2.5555419921875\n",
            "classify 2.6131591796875\n",
            "classify 2.546417236328125\n",
            "classify 2.614501953125\n",
            "classify 2.762451171875\n",
            "classify 2.591278076171875\n",
            "0.09375\n",
            "0.125\n",
            "0.109375\n",
            "0.1640625\n",
            "0.1171875\n",
            "0.1328125\n",
            "0.140625\n",
            "0.0859375\n",
            "0.078125\n",
            "0.0625\n",
            "0.1171875\n",
            "strain 0.1280541718006134\n",
            "strain 0.4243549108505249\n",
            "strain 3.8186280727386475\n",
            "strain 5.261012554168701\n",
            "strain 3.202298879623413\n",
            "strain 2.423314094543457\n",
            "strain 3.7198948860168457\n",
            "strain 1.374830722808838\n",
            "strain 3.7862091064453125\n",
            "strain 4.206829071044922\n",
            "strain 0.16238892078399658\n",
            "strain 2.4636566638946533\n",
            "strain 3.466613292694092\n",
            "strain 4.154749393463135\n",
            "strain 0.19391043484210968\n",
            "strain 0.17599189281463623\n",
            "strain 2.4485044479370117\n",
            "strain 4.670875072479248\n",
            "strain 1.056128978729248\n",
            "strain 1.228179693222046\n",
            "strain 3.431375741958618\n",
            "strain 2.288632869720459\n",
            "strain 2.921978712081909\n",
            "strain 7.03902006149292\n",
            "strain 2.568598747253418\n",
            "strain 1.6259548664093018\n",
            "strain 3.55795955657959\n",
            "strain 1.9974730014801025\n",
            "strain 5.0251946449279785\n",
            "strain 0.1099318191409111\n",
            "strain 5.368742942810059\n",
            "strain 3.794961929321289\n",
            "strain 2.810683488845825\n",
            "strain 2.108747959136963\n",
            "strain 3.1447651386260986\n",
            "strain 4.433653354644775\n",
            "strain 4.13864278793335\n",
            "strain 1.5322813987731934\n",
            "strain 1.4784172773361206\n",
            "strain 1.9941105842590332\n",
            "strain 3.4254415035247803\n",
            "strain 3.5599262714385986\n",
            "strain 2.1558592319488525\n",
            "strain 2.018665313720703\n",
            "strain 1.7235722541809082\n",
            "strain 0.3148267865180969\n",
            "strain 1.8139991760253906\n",
            "strain 7.932733058929443\n",
            "strain 3.8523662090301514\n",
            "strain 4.2466230392456055\n",
            "strain 0.3296923339366913\n",
            "classify 2.64404296875\n",
            "classify 2.55523681640625\n",
            "classify 2.65643310546875\n",
            "classify 2.520782470703125\n",
            "classify 2.65167236328125\n",
            "classify 2.50909423828125\n",
            "classify 2.515716552734375\n",
            "classify 2.6512451171875\n",
            "classify 2.57977294921875\n",
            "classify 2.605621337890625\n",
            "classify 2.467742919921875\n",
            "0.09375\n",
            "0.125\n",
            "0.109375\n",
            "0.1640625\n",
            "0.1171875\n",
            "0.1328125\n",
            "0.140625\n",
            "0.0859375\n",
            "0.078125\n",
            "0.0703125\n",
            "0.1171875\n",
            "strain 0.08145198225975037\n",
            "strain 4.137274265289307\n",
            "strain 1.7197344303131104\n",
            "strain 3.9553189277648926\n",
            "strain 1.9145100116729736\n",
            "strain 4.110504150390625\n",
            "strain 2.9450759887695312\n",
            "strain 2.617936372756958\n",
            "strain 2.0566132068634033\n",
            "strain 4.217682361602783\n",
            "strain 3.4269018173217773\n",
            "strain 1.1658446788787842\n",
            "strain 0.18103383481502533\n",
            "strain 4.3371148109436035\n",
            "strain 1.570144534111023\n",
            "strain 5.253929138183594\n",
            "strain 5.228646278381348\n",
            "strain 4.3583149909973145\n",
            "strain 0.3314039409160614\n",
            "strain 2.3063483238220215\n",
            "strain 1.963301181793213\n",
            "strain 0.23207266628742218\n",
            "strain 0.8177647590637207\n",
            "strain 5.288629531860352\n",
            "strain 1.7782131433486938\n",
            "strain 0.06886756420135498\n",
            "strain 1.930150032043457\n",
            "strain 2.450072765350342\n",
            "strain 5.374490261077881\n",
            "strain 1.8501414060592651\n",
            "strain 4.314338684082031\n",
            "strain 3.177008628845215\n",
            "strain 1.0370136499404907\n",
            "strain 3.759351968765259\n",
            "strain 1.4453506469726562\n",
            "strain 5.302428245544434\n",
            "strain 2.853611469268799\n",
            "strain 1.8013659715652466\n",
            "strain 0.08715403079986572\n",
            "strain 3.5508832931518555\n",
            "strain 9.493953704833984\n",
            "strain 6.017614841461182\n",
            "strain 4.249813079833984\n",
            "strain 0.9782019853591919\n",
            "strain 2.40775203704834\n",
            "strain 4.448433876037598\n",
            "strain 0.2391739934682846\n",
            "strain 5.3017449378967285\n",
            "strain 3.887946605682373\n",
            "strain 2.661623001098633\n",
            "strain 1.8775914907455444\n",
            "classify 2.51837158203125\n",
            "classify 2.61285400390625\n",
            "classify 2.5478515625\n",
            "classify 2.69769287109375\n",
            "classify 2.506866455078125\n",
            "classify 2.5133056640625\n",
            "classify 2.57598876953125\n",
            "classify 2.68878173828125\n",
            "classify 2.5201416015625\n",
            "classify 2.63201904296875\n",
            "classify 2.61865234375\n",
            "0.09375\n",
            "0.125\n",
            "0.109375\n",
            "0.1640625\n",
            "0.109375\n",
            "0.1328125\n",
            "0.140625\n",
            "0.0859375\n",
            "0.078125\n",
            "0.0625\n",
            "0.1171875\n",
            "strain 5.5958476066589355\n",
            "strain 4.840607643127441\n",
            "strain 4.710258483886719\n",
            "strain 3.7024261951446533\n",
            "strain 1.7995926141738892\n",
            "strain 3.5773766040802\n",
            "strain 5.128940582275391\n",
            "strain 7.38877010345459\n",
            "strain 4.566143035888672\n",
            "strain 2.7532806396484375\n",
            "strain 3.760704278945923\n",
            "strain 1.946744680404663\n",
            "strain 3.628258466720581\n",
            "strain 3.3516054153442383\n",
            "strain 2.6125433444976807\n",
            "strain 2.7927255630493164\n",
            "strain 0.28250089287757874\n",
            "strain 1.8441882133483887\n",
            "strain 6.292006492614746\n",
            "strain 2.3461079597473145\n",
            "strain 2.507338762283325\n",
            "strain 3.926881790161133\n",
            "strain 3.7477455139160156\n",
            "strain 3.3111302852630615\n",
            "strain 2.113816499710083\n",
            "strain 0.5369535684585571\n",
            "strain 0.8285123705863953\n",
            "strain 3.310746669769287\n",
            "strain 4.2008585929870605\n",
            "strain 4.867444038391113\n",
            "strain 3.1077933311462402\n",
            "strain 0.7751044034957886\n",
            "strain 0.33042749762535095\n",
            "strain 0.6488267779350281\n",
            "strain 3.391127824783325\n",
            "strain 4.912861347198486\n",
            "strain 0.3094785213470459\n",
            "strain 4.857824802398682\n",
            "strain 2.2516276836395264\n",
            "strain 0.21731363236904144\n",
            "strain 3.494889497756958\n",
            "strain 2.6597065925598145\n",
            "strain 0.7400944828987122\n",
            "strain 0.15783849358558655\n",
            "strain 6.164378643035889\n",
            "strain 2.671982526779175\n",
            "strain 3.7696828842163086\n",
            "strain 1.6982077360153198\n",
            "strain 1.9874303340911865\n",
            "strain 1.867049217224121\n",
            "strain 1.8356094360351562\n",
            "classify 2.57525634765625\n",
            "classify 2.6805419921875\n",
            "classify 2.63983154296875\n",
            "classify 2.63037109375\n",
            "classify 2.5992431640625\n",
            "classify 2.5596923828125\n",
            "classify 2.5997314453125\n",
            "classify 2.58197021484375\n",
            "classify 2.47271728515625\n",
            "classify 2.72705078125\n",
            "classify 2.6666259765625\n",
            "0.09375\n",
            "0.125\n",
            "0.109375\n",
            "0.1640625\n",
            "0.1171875\n",
            "0.1328125\n",
            "0.140625\n",
            "0.0859375\n",
            "0.078125\n",
            "0.0703125\n",
            "0.1171875\n",
            "strain 4.1993255615234375\n",
            "strain 1.3176374435424805\n",
            "strain 4.890906810760498\n",
            "strain 6.066064357757568\n",
            "strain 3.1985530853271484\n",
            "strain 0.5252313017845154\n",
            "strain 3.8399434089660645\n",
            "strain 0.18676623702049255\n",
            "strain 0.2502072751522064\n",
            "strain 3.02293062210083\n",
            "strain 1.038313865661621\n",
            "strain 2.995591402053833\n",
            "strain 0.7152506113052368\n",
            "strain 0.5078402757644653\n",
            "strain 1.3763163089752197\n",
            "strain 0.3830604553222656\n",
            "strain 5.389781475067139\n",
            "strain 7.605005264282227\n",
            "strain 0.8113852143287659\n",
            "strain 0.27928557991981506\n",
            "strain 3.5762014389038086\n",
            "strain 5.574561595916748\n",
            "strain 0.7994381189346313\n",
            "strain 1.797872543334961\n",
            "strain 1.9732481241226196\n",
            "strain 2.534569263458252\n",
            "strain 4.920243740081787\n",
            "strain 4.092962741851807\n",
            "strain 0.03912091627717018\n",
            "strain 0.05616986006498337\n",
            "strain 6.200043201446533\n",
            "strain 3.6571877002716064\n",
            "strain 0.046946749091148376\n",
            "strain 0.8949070572853088\n",
            "strain 3.748626470565796\n",
            "strain 2.884737730026245\n",
            "strain 4.152544021606445\n",
            "strain 0.2658764123916626\n",
            "strain 4.361280918121338\n",
            "strain 1.0668803453445435\n",
            "strain 0.5671484470367432\n",
            "strain 6.730679035186768\n",
            "strain 3.974092960357666\n",
            "strain 1.6145482063293457\n",
            "strain 1.1978261470794678\n",
            "strain 0.22924068570137024\n",
            "strain 2.595510482788086\n",
            "strain 3.487241506576538\n",
            "strain 3.93345308303833\n",
            "strain 0.5471727848052979\n",
            "strain 0.8282897472381592\n",
            "classify 2.597625732421875\n",
            "classify 2.6287841796875\n",
            "classify 2.47943115234375\n",
            "classify 2.64886474609375\n",
            "classify 2.60919189453125\n",
            "classify 2.50634765625\n",
            "classify 2.6541748046875\n",
            "classify 2.49664306640625\n",
            "classify 2.55029296875\n",
            "classify 2.57891845703125\n",
            "classify 2.60321044921875\n",
            "0.09375\n",
            "0.125\n",
            "0.109375\n",
            "0.1640625\n",
            "0.1171875\n",
            "0.1328125\n",
            "0.140625\n",
            "0.0859375\n",
            "0.078125\n",
            "0.0625\n",
            "0.1171875\n",
            "strain 0.10303449630737305\n",
            "strain 3.3387815952301025\n",
            "strain 0.20277424156665802\n",
            "strain 2.8295860290527344\n",
            "strain 0.4680016338825226\n",
            "strain 2.6325337886810303\n",
            "strain 1.3148329257965088\n",
            "strain 2.872889518737793\n",
            "strain 4.731456756591797\n",
            "strain 0.34431010484695435\n",
            "strain 3.4732837677001953\n",
            "strain 2.7334001064300537\n",
            "strain 3.6014575958251953\n",
            "strain 2.8697011470794678\n",
            "strain 1.8271632194519043\n",
            "strain 5.056738376617432\n",
            "strain 3.9985601902008057\n",
            "strain 2.2806737422943115\n",
            "strain 3.9258179664611816\n",
            "strain 0.6262485384941101\n",
            "strain 0.6083440780639648\n",
            "strain 5.3752665519714355\n",
            "strain 1.9636120796203613\n",
            "strain 3.6986775398254395\n",
            "strain 0.12120237201452255\n",
            "strain 2.685704231262207\n",
            "strain 6.659340858459473\n",
            "strain 4.6562089920043945\n",
            "strain 4.542829990386963\n",
            "strain 0.5271213054656982\n",
            "strain 3.8729982376098633\n",
            "strain 5.49391508102417\n",
            "strain 3.311464548110962\n",
            "strain 3.305553913116455\n",
            "strain 4.662849426269531\n",
            "strain 4.03855562210083\n",
            "strain 3.5103158950805664\n",
            "strain 4.289679050445557\n",
            "strain 4.221750736236572\n",
            "strain 0.3696539103984833\n",
            "strain 3.7146801948547363\n",
            "strain 0.5810078978538513\n",
            "strain 3.30479097366333\n",
            "strain 1.2396116256713867\n",
            "strain 0.09951399266719818\n",
            "strain 4.0409698486328125\n",
            "strain 2.1058058738708496\n",
            "strain 3.7852797508239746\n",
            "strain 6.929330348968506\n",
            "strain 3.0982227325439453\n",
            "strain 7.039419174194336\n",
            "classify 2.46466064453125\n",
            "classify 2.5216064453125\n",
            "classify 2.681732177734375\n",
            "classify 2.69000244140625\n",
            "classify 2.52984619140625\n",
            "classify 2.5799560546875\n",
            "classify 2.51605224609375\n",
            "classify 2.753692626953125\n",
            "classify 2.468597412109375\n",
            "classify 2.555419921875\n",
            "classify 2.587158203125\n",
            "0.09375\n",
            "0.125\n",
            "0.109375\n",
            "0.1640625\n",
            "0.109375\n",
            "0.1328125\n",
            "0.140625\n",
            "0.0859375\n",
            "0.078125\n",
            "0.0703125\n",
            "0.1171875\n",
            "strain 3.188711643218994\n",
            "strain 2.949801445007324\n",
            "strain 3.9923794269561768\n",
            "strain 3.676179885864258\n",
            "strain 2.32186222076416\n",
            "strain 1.9315752983093262\n",
            "strain 3.252002716064453\n",
            "strain 4.42708683013916\n",
            "strain 3.294795513153076\n",
            "strain 1.7800734043121338\n",
            "strain 0.12248437106609344\n",
            "strain 3.8751072883605957\n",
            "strain 2.4786078929901123\n",
            "strain 2.9746994972229004\n",
            "strain 5.835326671600342\n",
            "strain 3.678131103515625\n",
            "strain 1.7193140983581543\n",
            "strain 5.296813011169434\n",
            "strain 0.133276104927063\n",
            "strain 0.2957443594932556\n",
            "strain 5.808022975921631\n",
            "strain 4.5354905128479\n",
            "strain 0.8939127922058105\n",
            "strain 4.05724573135376\n",
            "strain 1.7199558019638062\n",
            "strain 3.4952733516693115\n",
            "strain 0.3156815469264984\n",
            "strain 3.6650798320770264\n",
            "strain 0.9052698016166687\n",
            "strain 0.5022123456001282\n",
            "strain 0.36971619725227356\n",
            "strain 5.5301194190979\n",
            "strain 3.8521809577941895\n",
            "strain 1.6062240600585938\n",
            "strain 3.000072479248047\n",
            "strain 2.149172782897949\n",
            "strain 0.24875527620315552\n",
            "strain 3.379270076751709\n",
            "strain 2.802365779876709\n",
            "strain 0.13974791765213013\n",
            "strain 1.69822096824646\n",
            "strain 3.724565029144287\n",
            "strain 3.192675828933716\n",
            "strain 2.2324655055999756\n",
            "strain 1.5488622188568115\n",
            "strain 0.720309853553772\n",
            "strain 0.21340219676494598\n",
            "strain 0.12028270959854126\n",
            "strain 2.088928461074829\n",
            "strain 1.9684293270111084\n",
            "strain 3.9330434799194336\n",
            "classify 2.61395263671875\n",
            "classify 2.51153564453125\n",
            "classify 2.4825439453125\n",
            "classify 2.581298828125\n",
            "classify 2.5079345703125\n",
            "classify 2.6580810546875\n",
            "classify 2.5826416015625\n",
            "classify 2.461181640625\n",
            "classify 2.60101318359375\n",
            "classify 2.47357177734375\n",
            "classify 2.60186767578125\n",
            "0.09375\n",
            "0.125\n",
            "0.109375\n",
            "0.1640625\n",
            "0.1171875\n",
            "0.1328125\n",
            "0.140625\n",
            "0.0859375\n",
            "0.078125\n",
            "0.0625\n",
            "0.1171875\n"
          ]
        }
      ],
      "source": [
        "# @title strain ctrain test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "def strain(model, dataloader, optim, scheduler=None): # train function with automatic mixed precision\n",
        "# def strain(model, train_iter, optim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.train()\n",
        "    for i, (img, _) in enumerate(dataloader):\n",
        "        img = img.to(device).flatten(2).transpose(-2,-1)#.to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            loss = model.loss(img)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # total_norm = 0\n",
        "        # for p in model.parameters(): total_norm += p.grad.data.norm(2).item() ** 2\n",
        "        # total_norm = total_norm**.5\n",
        "        # print('total_norm', total_norm)\n",
        "        # for p in list(filter(lambda p: p.grad is not None, model.parameters())):\n",
        "        #     print(p.grad.data.norm(2).item())\n",
        "        # print(\"max grad norm\", max([p.grad.data.norm(2).item() for p in list(filter(lambda p: p.grad is not None, model.parameters()))]))\n",
        "        scaler.unscale_(optim)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 10) # 0.5\n",
        "\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            m=0.999 # 0.99 m = next(momentum_scheduler)\n",
        "            norms=[]\n",
        "            for param_q, param_k in zip(model.context_encoder.parameters(), model.target_encoder.parameters()):\n",
        "                # norm = ((param_k.data-param_q.detach().data)**2).sum()**.5\n",
        "                # # # print(param_k.data.shape, norm)\n",
        "                # norms.append(norm.item())\n",
        "                # # if norm>.01:\n",
        "                param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "            # print(norms)\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        print(\"strain\",loss.item())\n",
        "        # for param in seq_jepa.context_encoder.cls: print(param.data)\n",
        "        # for param in seq_jepa.predicter.cls: print(param.data)\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=50: break\n",
        "\n",
        "\n",
        "def ctrain(model, classifier, dataloader, coptim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.eval()\n",
        "    classifier.train()\n",
        "    for i, (img, y) in enumerate(dataloader):\n",
        "        img, y = img.to(device), y.to(device) # [batch, ]\n",
        "        img = img.flatten(2).transpose(-2,-1).to(torch.bfloat16)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            with torch.no_grad():\n",
        "                sx = model(img).detach()\n",
        "            # print(sx[0][0])\n",
        "            y_ = classifier(sx)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "        # print(classifier.classifier.weight[0])\n",
        "        # print(y_.shape, y.shape)\n",
        "        # print(loss)\n",
        "        coptim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 5) # .5\n",
        "        scaler.step(coptim)\n",
        "        scaler.update()\n",
        "        print(\"classify\",loss.item())\n",
        "        try: wandb.log({\"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    for i, (img, y) in enumerate(dataloader):\n",
        "        img, y = img.to(device), y.to(device) # [batch, ]\n",
        "        img = img.flatten(2).transpose(-2,-1)#.to(torch.bfloat16)\n",
        "        with torch.no_grad():\n",
        "            sx = model(img)\n",
        "            y_ = classifier(sx)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        print(correct/len(y))\n",
        "        try: wandb.log({\"correct\": correct/len(y)})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "for i in range(100):\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    strain(seq_jepa, train_loader, optim)\n",
        "    ctrain(seq_jepa, classifier, train_loader, coptim)\n",
        "    test(seq_jepa, classifier, test_loader)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# @title supervised train test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "def strain(model, dataloader, optim, scheduler=None): # train function with automatic mixed precision\n",
        "# def strain(model, train_iter, optim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.train()\n",
        "    for i, (img, y) in enumerate(dataloader):\n",
        "        img = img.to(device).flatten(2).transpose(-2,-1)#.to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "        y = y.to(device)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            y_ = model.classify(img)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        # print(loss)\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # for p in list(filter(lambda p: p.grad is not None, model.parameters())):\n",
        "        #     print(p.grad.data.norm(2).item())\n",
        "        # print(\"max grad norm\", max([p.grad.data.norm(2).item() for p in list(filter(lambda p: p.grad is not None, model.parameters()))]))\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 10) # 0.5\n",
        "\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        print(\"strain\",loss.item())\n",
        "        # for param in seq_jepa.context_encoder.cls: print(param.data)\n",
        "        # for param in seq_jepa.predicter.cls: print(param.data)\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=50: break\n",
        "\n",
        "\n",
        "def test(model, dataloader):\n",
        "    model.eval()\n",
        "    for i, (img, y) in enumerate(dataloader):\n",
        "        img, y = img.to(device), y.to(device) # [batch, ]\n",
        "        img = img.flatten(2).transpose(-2,-1)#.to(torch.bfloat16)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            y_ = model.classify(img)\n",
        "\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        print(correct/len(y))\n",
        "        try: wandb.log({\"correct\": correct/len(y)})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "for i in range(100):\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    strain(seq_jepa, train_loader, optim)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "    test(seq_jepa, test_loader)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4J2ahp3wmqcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for param in seq_jepa.context_encoder.cls: print(param.data)\n",
        "# print(seq_jepa.context_encoder.cls.is_leaf)\n",
        "# seq=40\n",
        "# src = seq_jepa.context_encoder.pos_encoder(torch.arange(seq, device=device))\n",
        "# for x in src:\n",
        "#     print(x)\n",
        "# print(.999**50)"
      ],
      "metadata": {
        "id": "WgN5LlxWsPzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "2Nd-sGe6Ku4S",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "outputId": "a5bbb7a9-068b-4d59-c7e4-f626ad0fa3bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>▄▄▅▁▄▄▄▄▄▄▁█▄▆▅█▇█▆█▆▆▄▆▃▅▄▄▄▂▃▁▃▂▅▄█▃▅▇</td></tr><tr><td>correct</td><td>▃▃▁▂▄▃▅▅▂▇▄▆█▆▂▃▇▆▃▆▅▆▆▆▇▃██▃▆▄▃▃▆▅▃▃▃▅▁</td></tr><tr><td>loss</td><td>▁▁▂▁▁▂▁▃▂▂▂▄▁▁▁▆▁▂▅▁█▁▁▁▁▁▁▃▂▂▁▂▂▂▂▃▄▄▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>2.27008</td></tr><tr><td>correct</td><td>0.14062</td></tr><tr><td>loss</td><td>11.15625</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">dutiful-jazz-66</strong> at: <a href='https://wandb.ai/bobdole/SeqJEPA/runs/bujavymm' target=\"_blank\">https://wandb.ai/bobdole/SeqJEPA/runs/bujavymm</a><br> View project at: <a href='https://wandb.ai/bobdole/SeqJEPA' target=\"_blank\">https://wandb.ai/bobdole/SeqJEPA</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250313_070911-bujavymm/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250313_074905-7k3s0z1i</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/SeqJEPA/runs/7k3s0z1i' target=\"_blank\">rural-tree-67</a></strong> to <a href='https://wandb.ai/bobdole/SeqJEPA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/SeqJEPA' target=\"_blank\">https://wandb.ai/bobdole/SeqJEPA</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/SeqJEPA/runs/7k3s0z1i' target=\"_blank\">https://wandb.ai/bobdole/SeqJEPA/runs/7k3s0z1i</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "!pip install -q wandb\n",
        "import wandb # https://docs.wandb.ai/quickstart\n",
        "wandb.login(key='487a2109e55dce4e13fc70681781de9f50f27be7')\n",
        "try: run.finish()\n",
        "except NameError: pass\n",
        "run = wandb.init(project=\"SeqJEPA\", config={\"model\": \"res18\",})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title save/load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder='/content/drive/MyDrive/jepa/'\n",
        "\n",
        "# # modelsd, optimsd = torch.load(folder+'SeqJEPA.pkl', map_location=device).values()\n",
        "# modelsd, optimsd = torch.load('SeqJEPA.pkl', map_location=device).values()\n",
        "# seq_jepa.load_state_dict(modelsd, strict=False)\n",
        "# optim.load_state_dict(optimsd)"
      ],
      "metadata": {
        "id": "JT8AgOx0E_KM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b3d3969-da28-4cc0-c4de-f0a327ea266f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = {'model': seq_jepa.state_dict(), 'optimizer': optim.state_dict()}\n",
        "torch.save(checkpoint, folder+'SeqJEPA.pkl')\n",
        "# torch.save(checkpoint, 'SeqJEPA.pkl')"
      ],
      "metadata": {
        "id": "CpbLPvjiE-pD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## drawer"
      ],
      "metadata": {
        "id": "0vScvFGGSeN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(4,100,3)\n",
        "\n",
        "x=x.transpose(-2,-1) # [batch, dim, seq]\n",
        " # in, out, kernel, stride, pad\n",
        "# net = nn.Conv1d(3,16,7,2,7//2)\n",
        "net = nn.Sequential(nn.Conv1d(3,16,7,2,7//2), nn.MaxPool1d(3, 2, 3//2))\n",
        "# net = nn.Conv1d(3,16,(7,3),2,3//2)\n",
        "out = net(x)\n",
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_YBe6-eZ2Zq",
        "outputId": "09b27f75-ba04-4aaf-f5e4-1b9c1dc8d112"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 16, 25])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test repeat_interleave\n",
        "# x = torch.rand(3,5)\n",
        "x = torch.rand(3,5,7)\n",
        "print(x)\n",
        "# out = x.repeat(2,1) # [2*3,5]\n",
        "# print(out)\n",
        "# x_ = out.reshape(2,3,5)\n",
        "# print(x_)\n",
        "# out = x.repeat_interleave(2,1,1) # [3*2,5]\n",
        "out = x.repeat_interleave(2,dim=0) # [3*2,5]\n",
        "# print(out)\n",
        "# x_ = out.reshape(2,3,5,7)\n",
        "x_ = out.reshape(3,2,5,7)\n",
        "print(x_)\n",
        "\n",
        "\n",
        "# batch=1\n",
        "# seq_len=5\n",
        "# dim=4\n",
        "# positional_emb = torch.rand(batch, seq_len, dim)\n",
        "# print(positional_emb)\n",
        "# num_tok=2\n",
        "# trg_indices=torch.randint(0,seq_len,(M, num_tok))\n",
        "# print(trg_indices)\n",
        "# # pos_embs = positional_emb.gather(1, trg_indices.unsqueeze(0))\n",
        "# pos_embs = positional_emb[torch.arange(batch)[...,None,None],trg_indices.unsqueeze(0)]\n",
        "# # pos_embs = positional_emb[0,trg_indices]\n",
        "# # [batch, M, num_tok, dim]\n",
        "# print(pos_embs.shape)\n",
        "# print(pos_embs)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "R6kiiqw3uIdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test ropes\n",
        "# for i, (img, _) in enumerate(test_loader):\n",
        "#     break\n",
        "# print(img.shape) # [batch, 1, 28, 28]\n",
        "# print(img[0]) # [0,1)\n",
        "d_model=8\n",
        "# rot_emb = RotEmb(d_model, top=torch.pi, base=10)\n",
        "# x = torch.linspace(0,1,20)\n",
        "# out = rot_emb(x)\n",
        "# print(out.shape)\n",
        "# print(out)\n",
        "\n",
        "# # pos_encoder = RoPE(d_model, seq_len=15, base=100)\n",
        "# pos_encoder = RoPE(d_model, seq_len=15, base=10000)\n",
        "x = torch.ones(1, 10, d_model)\n",
        "# # x = torch.ones(1, 10, d_model)\n",
        "# out = pos_encoder(x)\n",
        "# # print(out.shape)\n",
        "# for x in out[0]:\n",
        "#     print(x)\n",
        "\n",
        "\n",
        "# pos_encoder = LearnedRoPE(d_model, seq_len=512)\n",
        "# out = pos_encoder(x)\n",
        "# for o in out[0]:\n",
        "#     print(o)\n",
        "\n",
        "# pos_encoder.weights = nn.Parameter(torch.randn(1, d_model//2))\n",
        "# out = pos_encoder(x)\n",
        "# for o in out[0]:\n",
        "#     print(o)\n",
        "\n",
        "\n",
        "# tensor([-0.0177, -0.9998,  0.8080, -0.5892, -0.7502, -0.6612,  0.3885,  0.9214])\n"
      ],
      "metadata": {
        "id": "8r_GjI_vCMyo",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Transformer Model\n",
        "import torch\n",
        "from torch import nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        # self.embed = nn.Sequential(\n",
        "        #     nn.Linear(in_dim, d_model), #act,\n",
        "        #     # nn.Linear(d_model, d_model), act,\n",
        "        # )\n",
        "        self.embed = nn.Linear(in_dim, d_model) if in_dim != d_model else None\n",
        "        # self.embed = nn.Sequential(nn.Conv1d(in_dim,d_model,7,2,7//2), nn.MaxPool1d(3, 2, 3//2))\n",
        "        self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, d_hid or d_model, dropout, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.d_model = d_model\n",
        "        self.cls = nn.Parameter(torch.zeros(1,1,d_model)) # randn\n",
        "        out_dim = out_dim or d_model\n",
        "        # self.lin = nn.Linear(d_model, out_dim)\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim != d_model else None\n",
        "        self.norm = nn.LayerNorm(out_dim)\n",
        "\n",
        "\n",
        "    def forward(self, src, src_key_padding_mask=None, cls_mask=None, context_indices=None, trg_indices=None): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # if cls_mask != None: src[cls_mask] = self.cls.to(src.dtype)\n",
        "\n",
        "        if self.embed != None: src = self.embed(src) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = src.shape\n",
        "        # src = self.pos_encoder(src)\n",
        "        if context_indices != None:\n",
        "            # print(src.shape, context_indices.shape, self.pos_encoder(context_indices).shape) # [2, 88, 32]) torch.Size([88]) torch.Size([88, 32]\n",
        "            # print(src[0], self.pos_encoder(context_indices)[0])\n",
        "            src = src * self.pos_encoder(context_indices) # context/predictor # src = src + self.positional_emb[:,context_indices]\n",
        "            # print(src[0])\n",
        "        else: src = src * self.pos_encoder(torch.arange(seq, device=device)) # target # src = src + self.positional_emb[:,:seq]\n",
        "            # print(\"trans fwd\", src.shape, self.pos_encoder(src).shape)\n",
        "\n",
        "        if trg_indices != None: # [M, num_trg_toks]\n",
        "            pred_tokens = self.cls * self.pos_encoder(trg_indices) # [M, num_trg_toks, d_model] # pred_tokens = self.cls + self.positional_emb[0,trg_indices]\n",
        "            pred_tokens = pred_tokens.repeat(batch, 1, 1) # [batch*M, num_trg_toks, d_model]\n",
        "            # print(pred_tokens.requires_grad)\n",
        "            src = src.repeat_interleave(trg_indices.shape[0], dim=0) # [batch, seq_len, d_model] -> [batch*M, seq_len, d_model]\n",
        "            src = torch.cat([src, pred_tokens], dim=1) # [batch*M, seq_len+num_trg_toks, d_model]\n",
        "\n",
        "        out = self.transformer_encoder(src, src_key_padding_mask=src_key_padding_mask) # float [seq_len, batch_size, d_model]\n",
        "        if trg_indices != None:\n",
        "            # print(out.shape)\n",
        "            out = out[:,seq:] # [batch*M, num_trg_toks, d_model]\n",
        "        if self.lin != None: out = self.lin(out)\n",
        "        out = self.norm(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "batch, seq_len, d_model = 4,7,64\n",
        "in_dim = 3\n",
        "model = TransformerModel(in_dim, d_model, nhead=8, nlayers=2, dropout=0.).to(device)\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "# print(out.shape)\n",
        "# # print(out)\n"
      ],
      "metadata": {
        "id": "-8i1WLsvH_vn",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title SeqJEPA old\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def multiblock(batch, seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "        mask_len = torch.rand(batch, M) * (max_s - min_s) + min_s # in (min_s, max_s)\n",
        "        mask_pos = torch.rand(batch, M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "        mask_len, mask_pos = mask_len * seq, mask_pos * seq\n",
        "        indices = torch.arange(seq)[None,None,...]\n",
        "        target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [batch, M, seq]\n",
        "        target_mask = target_mask.any(1) # True -> masked # multiblock masking # [batch, seq]\n",
        "        return target_mask\n",
        "\n",
        "\n",
        "\n",
        "class SeqJEPA(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.in_dim = in_dim\n",
        "        # if out_dim is None: self.out_dim = in_dim\n",
        "        if out_dim is None: out_dim = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.d_model = d_model\n",
        "        act = nn.GELU() # ReLU GELU SiLU\n",
        "\n",
        "        dim=8\n",
        "\n",
        "\n",
        "        self.calorie_emb = RotEmb(dim, top=1, base=10) # 0-4\n",
        "        self.steps_emb = RotEmb(dim, top=1, base=10) # 0-5\n",
        "        self.enc0 = nn.Sequential(\n",
        "            nn.Linear(dim*2, d_model), act,\n",
        "            nn.Linear(d_model, d_model), act,\n",
        "        )\n",
        "        self.hour_emb = CircularEmb(dim, torch.pi/2) # circular (0,1) # 0-24\n",
        "        self.temp_emb = RotEmb(dim, top=1/3, base=10) # 18-35\n",
        "        self.heart_emb = RotEmb(dim, top=1/3, base=100) # 50-200\n",
        "        self.enc1 = nn.Sequential(\n",
        "            nn.Linear(dim*3, d_model), act,\n",
        "        )\n",
        "\n",
        "        # self.transformer = TransformerClassifier(d_model, out_dim=out_dim, nhead=8, nlayers=2)\n",
        "        # # self.fc = nn.Linear(d_model, out_dim)\n",
        "        self.context_encoder = TransformerModel(d_model, out_dim=out_dim, nhead=8, nlayers=2, dropout=0.)\n",
        "        self.predicter = TransformerModel(out_dim, nhead=8, nlayers=1, dropout=0.)\n",
        "        self.target_encoder = AveragedModel(self.context_encoder, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    def loss(self, hr, temp, heart): # [batch, 1+T]\n",
        "        # batch = hr.size(0)\n",
        "        res_id, activeKilocalories, steps = hr[:,0], temp[:,0], heart[:,0]\n",
        "        # enc_summary = self.enc0(torch.cat([self.calorie_emb(activeKilocalories), self.steps_emb(steps)], dim=-1)) # [batch, d_model]\n",
        "        enc_summary = self.enc0(torch.cat([self.calorie_emb(torch.log(activeKilocalories.clamp(min=1))), self.steps_emb(torch.log(steps.clamp(min=1)))], dim=-1)) # [batch, d_model]\n",
        "        x = self.enc1(torch.cat([self.hour_emb(hr[:,1:]/24), self.temp_emb(temp[:,1:]), self.heart_emb(heart[:,1:])], dim=-1)) # [batch, T, d_model]\n",
        "        # print('violet fwd', hr.shape, enc_summary.shape, x.shape)\n",
        "\n",
        "        # # mask=(torch.rand(self.seq_len)<.1) # True -> masked # random masking\n",
        "        # mask=(torch.rand_like(x)<.1) # True -> masked # random masking\n",
        "\n",
        "        # patches pad -enc> patch_enc\n",
        "        # rearrange patch in padded/masked, -pred> pred of masked\n",
        "        # mse tgt_enc(img)\n",
        "\n",
        "        # average L2 distance between the predicted patch-level representations sˆy(i) and the target patch-level representation sy(i);\n",
        "        # F.smooth_l1_loss\n",
        "\n",
        "        batch, seq, dim = x.shape\n",
        "\n",
        "        # mask=(torch.rand(batch, seq)<.75) # True -> masked # random masking\n",
        "        # sorted_mask, ids = mask.sort(dim=1)\n",
        "        # # sorted_x = x[torch.arange(batch)[...,None,None],ids]\n",
        "        # sorted_x = x[torch.arange(batch).unsqueeze(-1),ids]\n",
        "        # sorted_mask = torch.cat([torch.zeros((batch, 1), dtype=torch.bool), sorted_mask], dim=1) # True->mask\n",
        "        # sorted_x = torch.cat([enc_summary.unsqueeze(1), sorted_x], dim=1)\n",
        "        # sorted_sx = self.context_encoder(sorted_x, src_key_padding_mask=sorted_mask)\n",
        "        # # torch.zeros_like(sorted_sx)\n",
        "        # sx = sorted_sx[torch.arange(batch).unsqueeze(-1),ids.argsort(1)]\n",
        "        # sy_ = self.predicter(sx, src_key_padding_mask=mask)\n",
        "        # sy = self.target_encoder(x)*~mask\n",
        "\n",
        "\n",
        "        # target_mask = multiblock(batch, seq, min_s=0.15, max_s=0.2, M=4)\n",
        "        # context_mask = ~multiblock(batch, seq, min_s=0.85, max_s=1., M=1)|target_mask\n",
        "        # sx = self.context_encoder(x, src_key_padding_mask=context_mask)\n",
        "        # sy_ = self.predicter(sx, cls_mask=target_mask)*target_mask.unsqueeze(-1)\n",
        "        # sy_ = self.predicter(sx, cls_mask=target_mask)\n",
        "        # # print(sy_.shape, target_mask.shape)\n",
        "        # sy = self.target_encoder(x)*target_mask.unsqueeze(-1)\n",
        "\n",
        "\n",
        "        M=4\n",
        "        target_mask = multiblock(M*batch, seq, min_s=0.15, max_s=0.2, M=1) # mask out targets to be predicted # [4*batch, seq]\n",
        "        context_mask = ~multiblock(batch, seq, min_s=0.85, max_s=1., M=1)|target_mask.reshape(batch,M,seq).any(1) # [batch, seq]\n",
        "\n",
        "\n",
        "        x = torch.cat([enc_summary.unsqueeze(1), x], dim=1) # [batch, 1+T, d_model]\n",
        "        target_mask = torch.cat([torch.zeros((M*batch, 1), dtype=bool), target_mask],dim=1) # [4*batch, 1+T]\n",
        "        context_mask = torch.cat([torch.zeros((batch, 1), dtype=bool), context_mask],dim=1) # [batch, 1+T]\n",
        "\n",
        "        # x = x.repeat(4,1,1)\n",
        "        # context_mask = context_mask.repeat(4,1)\n",
        "        sx = self.context_encoder(x, src_key_padding_mask=context_mask).repeat(M,1,1)\n",
        "        sy_ = self.predicter(sx, cls_mask=target_mask)*target_mask.unsqueeze(-1)\n",
        "        # sy_ = self.predicter(sx, cls_mask=target_mask)\n",
        "        # print(sy_.shape, target_mask.shape)\n",
        "        sy = self.target_encoder(x).repeat(M,1,1)*target_mask.unsqueeze(-1)\n",
        "\n",
        "        loss = F.smooth_l1_loss(sy, sy_)\n",
        "        return loss\n",
        "\n",
        "    # def forward(self, hr, temp, heart): # [batch, 1+T]\n",
        "    #     # batch = hr.size(0)\n",
        "    #     # batch, seq, dim = x.shape\n",
        "    #     res_id, activeKilocalories, steps = hr[:,0], temp[:,0], heart[:,0]\n",
        "    #     enc_summary = self.enc0(torch.cat([self.calorie_emb(activeKilocalories), self.steps_emb(steps)], dim=-1)) # [batch, d_model]\n",
        "    #     x = self.enc1(torch.cat([self.hour_emb(hr[:,1:]/24), self.temp_emb(temp[:,1:]), self.heart_emb(heart[:,1:])], dim=-1)) # [batch, T, d_model]\n",
        "\n",
        "    #     x = torch.cat([enc_summary.unsqueeze(1), x], dim=1)\n",
        "    #     # sx = self.context_encoder(x)\n",
        "    #     sx = self.target_encoder(x)\n",
        "    #     out = sx.mean(dim=1)\n",
        "    #     return out\n",
        "\n",
        "\n",
        "\n",
        "seq_jepa = SeqJEPA(in_dim=16, d_model=32, out_dim=16, num_layers=1).to(device)#.to(torch.float)\n",
        "soptim = torch.optim.AdamW(seq_jepa.parameters(), lr=1e-3)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "O-OU1MaKF7BZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ba31127-d6e5-438a-aede-5c789557ab39",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title SeqJEPA\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "def multiblock(seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "    return target_mask\n",
        "\n",
        "\n",
        "class SeqJEPA(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, num_layers=1, num_classes=10):\n",
        "        super().__init__()\n",
        "        if out_dim is None: out_dim = d_model\n",
        "        act = nn.GELU() # ReLU GELU SiLU\n",
        "        # self.rot_emb = RotEmb(d_model, top=1, base=10)\n",
        "        # self.rot_emb = RotEmb(d_model, top=torch.pi, base=10)\n",
        "        # self.rot_emb = RoPE(d_model, seq_len=1024, base=10)\n",
        "        # self.rot_emb = LearnedRotEmb(dim)\n",
        "        self.encode = nn.Sequential(\n",
        "            nn.Linear(in_dim, d_model), #act,\n",
        "            # nn.Linear(d_model, d_model), act,\n",
        "        )\n",
        "        # self.encode = nn.Sequential(nn.Conv1d(in_dim, d_model,7,2,7//2), nn.MaxPool1d(3, 2, 3//2))\n",
        "        self.context_encoder = TransformerModel(d_model, d_model, out_dim=out_dim, nhead=4, nlayers=2, dropout=0.)\n",
        "        self.predicter = TransformerModel(out_dim, d_model//2, out_dim, nhead=4, nlayers=1, dropout=0.)\n",
        "        self.target_encoder = AveragedModel(self.context_encoder, multi_avg_fn=get_ema_multi_avg_fn(0.85)) # 0.95 0.999\n",
        "        self.target_encoder.requires_grad_(False)\n",
        "        # self.classifier = nn.Linear(out_dim, num_classes)\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]\n",
        "        x = self.encode(x) # [batch, T, d_model]\n",
        "        # x = self.encode(x.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "\n",
        "        batch, seq, dim = x.shape\n",
        "        M=1 # 4\n",
        "        # target_mask = multiblock(seq, min_s=0.15, max_s=0.2, M=M) # mask out targets to be predicted # [M, seq]\n",
        "        target_mask = multiblock(seq, min_s=0.15, max_s=0.2, M=4)#.any(0) # mask out targets to be predicted # [M, seq]\n",
        "        context_mask = ~multiblock(seq, min_s=0.85, max_s=1., M=1)|target_mask.reshape(M,seq).any(0) # [1, seq], True->Mask\n",
        "        target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "        # print(target_mask, context_mask)\n",
        "        sorted_mask, ids = context_mask.int().sort(dim=1, stable=True)\n",
        "        context_indices = ids[~sorted_mask.bool()] # int idx [num_context_toks] , idx of context not masked\n",
        "        sorted_x = x[torch.arange(batch).unsqueeze(-1), context_indices] # [batch, num_context_toks, d_model]\n",
        "        # print(sorted_x.shape, context_indices.shape)[2, 9, 32]) torch.Size([9])\n",
        "        # print(sorted_x[0])\n",
        "        # print(sorted_x.is_leaf)\n",
        "        sorted_sx = self.context_encoder(sorted_x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "        # del sorted_x\n",
        "        # print(sorted_sx[0])\n",
        "\n",
        "        sorted_mask, ids = target_mask.int().sort(dim=1, stable=True)\n",
        "        # print(sorted_mask.shape, ids.shape, ids[~sorted_mask.bool()].shape, sorted_mask.sum(-1))\n",
        "        trg_indices = ids[sorted_mask.bool()].reshape(M,-1) # int idx [M, num_trg_toks] , idx of targets that are masked\n",
        "        # sorted_x = x[torch.arange(batch)[...,None,None], indices] # [batch, M, seq-num_trg_toks, dim]\n",
        "        # sx = sorted_sx[torch.arange(batch).unsqueeze(-1),ids.argsort(1)]\n",
        "        # print(sorted_sx.shape, trg_indices.shape)\n",
        "        sy_ = self.predicter(sorted_sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "        # sx = self.context_encoder(x, src_key_padding_mask=context_mask).repeat(M,1,1)\n",
        "        # # sy_ = self.predicter(sx, cls_mask=target_mask)*target_mask.unsqueeze(-1)\n",
        "        # sy_ = self.predicter(sx, trg_mask=target_mask)*target_mask.unsqueeze(-1)\n",
        "\n",
        "        # print(sy_.shape, target_mask.shape)\n",
        "        # print(self.target_encoder(x).repeat_interleave(M, dim=0).shape, trg_indices.repeat(batch,1).shape)\n",
        "        with torch.no_grad():\n",
        "            sy = self.target_encoder(x).repeat_interleave(M, dim=0)[torch.arange(batch*M).unsqueeze(-1), trg_indices.repeat(batch,1)] # [batch*M, num_trg_toks, out_dim]\n",
        "\n",
        "        # print(sy_[0])\n",
        "        # print(sy.shape, sy_.shape)\n",
        "        # loss = F.smooth_l1_loss(sy, sy_)\n",
        "        loss = F.mse_loss(sy, sy_)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]\n",
        "        batch, seq, dim = x.shape\n",
        "        target_mask = randpatch(seq//self.patch_size, mask_size=16, gamma=.75) # [seq]\n",
        "        context_mask = ~multiblock(seq//self.patch_size, min_s=0.85, max_s=1., M=1)|target_mask # [1, seq], True->Mask\n",
        "        target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "        # print(target_mask, context_mask)\n",
        "        sorted_mask, ids = context_mask.int().sort(dim=1, stable=True)\n",
        "        context_indices = ids[~sorted_mask.bool()]#.unsqueeze(0) # int idx [num_context_toks] , idx of context not masked\n",
        "        sorted_x = x[torch.arange(batch).unsqueeze(-1), context_indices] # [batch, num_context_toks, 3]\n",
        "        print(sorted_x.shape, context_indices.shape)\n",
        "        sorted_sx = self.context_encoder(sorted_x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "\n",
        "        sorted_mask, ids = target_mask.int().sort(dim=-1, stable=True)\n",
        "        trg_indices = ids[sorted_mask.bool()].unsqueeze(0) # int idx [1, num_trg_toks] , idx of targets that are masked\n",
        "        print(sorted_sx.shape, context_indices.shape)\n",
        "        sy_ = self.predicter(sorted_sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "\n",
        "        with torch.no_grad(): sy = self.target_encoder(x.detach())[torch.arange(batch).unsqueeze(-1), trg_indices.repeat(batch,1)] # [batch, num_trg_toks, out_dim]\n",
        "        # print(x[0][0][:8])\n",
        "        # print(sy_[0][0][:8])\n",
        "        # print(sy.shape, sy_.shape)\n",
        "        # loss = F.smooth_l1_loss(sy, sy_)\n",
        "        loss = F.mse_loss(sy.detach(), sy_)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        x = self.encode(x)\n",
        "        # x = self.encode(x.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "        sx = self.target_encoder(x)\n",
        "        out = sx.mean(dim=1)\n",
        "        return out\n",
        "\n",
        "\n",
        "# min_s=0.15, max_s, M\n",
        "# trg.15.2M4 C.85 1\n",
        "\n",
        "seq_jepa = SeqJEPA(in_dim=3, d_model=32, out_dim=16, num_layers=1).to(device)#.to(torch.float)\n",
        "optim = torch.optim.AdamW(seq_jepa.parameters(), lr=1e-4) # 1e-3?\n",
        "print(sum(p.numel() for p in seq_jepa.parameters() if p.requires_grad)) # 59850\n",
        "\n",
        "\n",
        "# x = torch.rand((2,20,3), device=device)\n",
        "# out = seq_jepa.loss(x)\n",
        "# print(out.shape)\n",
        "# print(x.is_leaf) # T\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ijepa vision_transformer.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/models/vision_transformer.py\n",
        "import math\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from src.utils.tensors import (trunc_normal_, repeat_interleave_batch)\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid height and width\n",
        "    return:\n",
        "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid_h = np.arange(grid_size, dtype=float)\n",
        "    grid_w = np.arange(grid_size, dtype=float)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    assert embed_dim % 2 == 0\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid length\n",
        "    return:\n",
        "    pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid = np.arange(grid_size, dtype=float)\n",
        "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=float)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega   # (D/2,)\n",
        "    pos = pos.reshape(-1)   # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
        "    emb = np.concatenate([np.sin(out), np.cos(out)], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x, attn\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x, return_attention=False):\n",
        "        y, attn = self.attn(self.norm1(x))\n",
        "        if return_attention:\n",
        "            return attn\n",
        "        x = x + self.drop_path(y)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvEmbed(nn.Module):\n",
        "    \"\"\"3x3 Convolution stems for ViT following ViTC models\"\"\"\n",
        "    def __init__(self, channels, strides, img_size=224, in_chans=3, batch_norm=True):\n",
        "        super().__init__()\n",
        "        # Build the stems\n",
        "        stem = []\n",
        "        channels = [in_chans] + channels\n",
        "        for i in range(len(channels) - 2):\n",
        "            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3, stride=strides[i], padding=1, bias=(not batch_norm))]\n",
        "            if batch_norm:\n",
        "                stem += [nn.BatchNorm2d(channels[i+1])]\n",
        "            stem += [nn.ReLU(inplace=True)]\n",
        "        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n",
        "        self.stem = nn.Sequential(*stem)\n",
        "\n",
        "        # Comptute the number of patches\n",
        "        stride_prod = int(np.prod(strides))\n",
        "        self.num_patches = (img_size[0] // stride_prod)**2\n",
        "\n",
        "    def forward(self, x):\n",
        "        p = self.stem(x)\n",
        "        return p.flatten(2).transpose(1, 2)\n",
        "\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "def apply_masks(x, masks):\n",
        "    \"\"\":param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
        "    :param masks: list of tensors containing indices of patches in [N] to keep\"\"\"\n",
        "    all_x = []\n",
        "    for m in masks:\n",
        "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [batch,T,dim]\n",
        "        all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "    return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "\n",
        "class VisionTransformerPredictor(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, num_patches, embed_dim=768, predictor_embed_dim=384,\n",
        "        depth=6, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.init_std = init_std\n",
        "        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        # --\n",
        "        self.predictor_pos_embed = nn.Parameter(torch.zeros(1, num_patches, predictor_embed_dim), requires_grad=False)\n",
        "        predictor_pos_embed = get_2d_sincos_pos_embed(self.predictor_pos_embed.shape[-1], int(num_patches**.5), cls_token=False)\n",
        "        self.predictor_pos_embed.data.copy_(torch.from_numpy(predictor_pos_embed).float().unsqueeze(0)) # [1, num_patches, predictor_embed_dim]\n",
        "        # --\n",
        "        self.predictor_blocks = nn.ModuleList([Block(dim=predictor_embed_dim, num_heads=num_heads, drop=drop_rate, drop_path=dpr[i]) for i in range(depth)])\n",
        "        self.predictor_norm = nn.LayerNorm(predictor_embed_dim)\n",
        "        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n",
        "        # ------\n",
        "        # trunc_normal_(self.mask_token, std=self.init_std)\n",
        "\n",
        "    def forward(self, x, masks_x, masks):\n",
        "        assert (masks is not None) and (masks_x is not None), 'Cannot run predictor without mask indices'\n",
        "        if not isinstance(masks_x, list): masks_x = [masks_x]\n",
        "        if not isinstance(masks, list): masks = [masks]\n",
        "\n",
        "        # -- Batch Size\n",
        "        B = len(x) // len(masks_x)\n",
        "\n",
        "        # -- map from encoder-dim to pedictor-dim\n",
        "        x = self.predictor_embed(x) # [batch, num_patches, predictor_embed_dim]\n",
        "\n",
        "        # -- add positional embedding to x tokens\n",
        "        x_pos_embed = self.predictor_pos_embed.repeat(B, 1, 1) # [batch, num_patches, predictor_embed_dim]\n",
        "        x += apply_masks(x_pos_embed, masks_x)\n",
        "\n",
        "        _, N_ctxt, D = x.shape\n",
        "\n",
        "        # -- concat mask tokens to x\n",
        "        pos_embs = self.predictor_pos_embed.repeat(B, 1, 1)\n",
        "        pos_embs = apply_masks(pos_embs, masks)\n",
        "        pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x))\n",
        "        # --\n",
        "        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1)\n",
        "        # --\n",
        "        pred_tokens += pos_embs\n",
        "        x = x.repeat(len(masks), 1, 1)\n",
        "        x = torch.cat([x, pred_tokens], dim=1)\n",
        "\n",
        "        # -- fwd prop\n",
        "        for blk in self.predictor_blocks:\n",
        "            x = blk(x)\n",
        "        x = self.predictor_norm(x)\n",
        "\n",
        "        # -- return preds for mask tokens\n",
        "        x = x[:, N_ctxt:]\n",
        "        x = self.predictor_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, img_size=[224], patch_size=16, in_chans=3, embed_dim=768, predictor_embed_dim=384, depth=12, predictor_depth=12,\n",
        "        num_heads=12, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0, norm_layer=nn.LayerNorm, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_features = self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.init_std = init_std\n",
        "        # --\n",
        "        self.patch_embed = PatchEmbed(img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        # --\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=False)\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "        # --\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "    def forward(self, x, masks=None):\n",
        "        if masks is not None:\n",
        "            if not isinstance(masks, list): masks = [masks]\n",
        "\n",
        "        x = self.patch_embed(x)\n",
        "        B, N, D = x.shape # [batch, T, d_model]\n",
        "\n",
        "        # -- add positional embedding to x\n",
        "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
        "        x = x + pos_embed\n",
        "\n",
        "        if masks is not None: x = apply_masks(x, masks)\n",
        "\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            x = blk(x)\n",
        "\n",
        "        if self.norm is not None: x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "    def interpolate_pos_encoding(self, x, pos_embed):\n",
        "        npatch = x.shape[1] - 1\n",
        "        N = pos_embed.shape[1] - 1\n",
        "        if npatch == N: return pos_embed\n",
        "        class_emb, pos_embed = pos_embed[:, 0], pos_embed[:, 1:]\n",
        "        dim = x.shape[-1]\n",
        "        pos_embed = nn.functional.interpolate(pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2), scale_factor=math.sqrt(npatch / N), mode='bicubic',)\n",
        "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
        "\n",
        "# from functools import partial\n",
        "# def vit_predictor(**kwargs):\n",
        "#     model = VisionTransformerPredictor(\n",
        "#         mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "#     return model\n",
        "\n",
        "def vit(patch_size=16, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, # tiny\n",
        "        # patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, # small\n",
        "        # patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, # base\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "bNjp4xFsGPoa",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title show collated_masks\n",
        "# print(collated_batch, collated_masks_enc, collated_masks_pred)\n",
        "# print(collated_batch) # tensor([0, 1, 2, 3]) batch\n",
        "# print(collated_masks_enc) # nenc*[M*[pred mask blocks patch ind]]\n",
        "# print(collated_masks_pred) # npred * [M * [context]]\n",
        "\n",
        "\n",
        "# print(len(collated_masks_enc[0]), len(collated_masks_pred[0]))\n",
        "# print(collated_masks_enc[0], collated_masks_pred[0])\n",
        "\n",
        "h,w=14,14\n",
        "img = torch.ones(h*w)\n",
        "# img[collated_masks_enc[0]]=0\n",
        "img[collated_masks_pred[0][2]]=0\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    # plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.imshow(npimg)\n",
        "    plt.show()\n",
        "\n",
        "# imshow(img.reshape(h,w))\n",
        "\n",
        "# for masks in collated_masks_pred:\n",
        "#     for mask in masks:\n",
        "#         img = torch.ones(h*w)\n",
        "#         img[mask]=0\n",
        "#         imshow(img.reshape(h,w))\n",
        "\n",
        "\n",
        "for masks in collated_masks_enc:\n",
        "    for mask in masks:\n",
        "        img = torch.ones(h*w)\n",
        "        img[mask]=0\n",
        "        imshow(img.reshape(h,w))\n"
      ],
      "metadata": {
        "id": "tf4T37woBV2V",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aH1UOD4WW8I2",
        "outputId": "6fafea1b-0af2-4bc7-fa26-46b5e56549b4",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vit fwd torch.Size([4, 3, 224, 224])\n",
            "vit fwd torch.Size([4, 196, 768])\n",
            "vit fwd torch.Size([4, 11, 768])\n",
            "predictor fwd1 torch.Size([4, 11, 384]) torch.Size([4, 196, 384])\n"
          ]
        }
      ],
      "source": [
        "# @title ijepa vision_transformer.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/models/vision_transformer.py\n",
        "import math\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# from src.utils.tensors import (trunc_normal_, repeat_interleave_batch)\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid height and width\n",
        "    return:\n",
        "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid_h = np.arange(grid_size, dtype=float)\n",
        "    grid_w = np.arange(grid_size, dtype=float)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    assert embed_dim % 2 == 0\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid length\n",
        "    return:\n",
        "    pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid = np.arange(grid_size, dtype=float)\n",
        "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=float)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega   # (D/2,)\n",
        "    pos = pos.reshape(-1)   # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
        "    emb = np.concatenate([np.sin(out), np.cos(out)], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "def repeat_interleave_batch(x, B, repeat):\n",
        "    N = len(x) // B\n",
        "    x = torch.cat([\n",
        "        torch.cat([x[i*B:(i+1)*B] for _ in range(repeat)], dim=0)\n",
        "        for i in range(N)\n",
        "    ], dim=0)\n",
        "    return x\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x, attn\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x, return_attention=False):\n",
        "        y, attn = self.attn(self.norm1(x))\n",
        "        if return_attention:\n",
        "            return attn\n",
        "        x = x + self.drop_path(y)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvEmbed(nn.Module):\n",
        "    \"\"\"3x3 Convolution stems for ViT following ViTC models\"\"\"\n",
        "    def __init__(self, channels, strides, img_size=224, in_chans=3, batch_norm=True):\n",
        "        super().__init__()\n",
        "        # Build the stems\n",
        "        stem = []\n",
        "        channels = [in_chans] + channels\n",
        "        for i in range(len(channels) - 2):\n",
        "            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3, stride=strides[i], padding=1, bias=(not batch_norm))]\n",
        "            if batch_norm:\n",
        "                stem += [nn.BatchNorm2d(channels[i+1])]\n",
        "            stem += [nn.ReLU(inplace=True)]\n",
        "        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n",
        "        self.stem = nn.Sequential(*stem)\n",
        "\n",
        "        # Comptute the number of patches\n",
        "        stride_prod = int(np.prod(strides))\n",
        "        self.num_patches = (img_size[0] // stride_prod)**2\n",
        "\n",
        "    def forward(self, x):\n",
        "        p = self.stem(x)\n",
        "        return p.flatten(2).transpose(1, 2)\n",
        "\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "def apply_masks(x, masks):\n",
        "    \"\"\":param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
        "    :param masks: list of tensors containing indices of patches in [N] to keep\"\"\"\n",
        "    all_x = []\n",
        "    for m in masks:\n",
        "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [batch,T,dim]\n",
        "        all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "    return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "\n",
        "class VisionTransformerPredictor(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, num_patches, embed_dim=768, predictor_embed_dim=384,\n",
        "        depth=6, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.init_std = init_std\n",
        "        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        # --\n",
        "        self.predictor_pos_embed = nn.Parameter(torch.zeros(1, num_patches, predictor_embed_dim), requires_grad=False)\n",
        "        predictor_pos_embed = get_2d_sincos_pos_embed(self.predictor_pos_embed.shape[-1], int(num_patches**.5), cls_token=False)\n",
        "        self.predictor_pos_embed.data.copy_(torch.from_numpy(predictor_pos_embed).float().unsqueeze(0)) # [1, num_patches, predictor_embed_dim]\n",
        "        # --\n",
        "        self.predictor_blocks = nn.ModuleList([Block(dim=predictor_embed_dim, num_heads=num_heads, drop=drop_rate, drop_path=dpr[i]) for i in range(depth)])\n",
        "        self.predictor_norm = nn.LayerNorm(predictor_embed_dim)\n",
        "        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n",
        "        # ------\n",
        "        # trunc_normal_(self.mask_token, std=self.init_std)\n",
        "\n",
        "    def forward(self, x, masks_x, masks): # [batch, num_context_patches, embed_dim], nenc*[batch, num_context_patches], npred*[batch, num_target_patches]\n",
        "        assert (masks is not None) and (masks_x is not None), 'Cannot run predictor without mask indices'\n",
        "        if not isinstance(masks_x, list): masks_x = [masks_x] # context mask\n",
        "        if not isinstance(masks, list): masks = [masks] # pred mask\n",
        "\n",
        "        # -- Batch Size\n",
        "        B = len(x) // len(masks_x)\n",
        "        # print(\"predictor fwd0\", x.shape) # [3, 121, 768])\n",
        "        # -- map from encoder-dim to pedictor-dim\n",
        "        x = self.predictor_embed(x) # [batch, num_patches, predictor_embed_dim]\n",
        "\n",
        "        # -- add positional embedding to x tokens\n",
        "        x_pos_embed = self.predictor_pos_embed.repeat(B, 1, 1) # [batch, num_patches, predictor_embed_dim]\n",
        "        print(\"predictor fwd1\", x.shape, x_pos_embed.shape) # [3, 121 or 11, 384], [3, 196, 384]\n",
        "        # print(\"predictor fwd masks_x\", masks_x)\n",
        "        x += apply_masks(x_pos_embed, masks_x) # get pos emb of context patches\n",
        "\n",
        "        # print(\"predictor fwd x\", x.shape) # [4, 11, 384])\n",
        "        _, N_ctxt, D = x.shape\n",
        "\n",
        "        # -- concat mask tokens to x\n",
        "        pos_embs = self.predictor_pos_embed.repeat(B, 1, 1) # [batch, num_patches, predictor_embed_dim]\n",
        "        pos_embs = apply_masks(pos_embs, masks) # [16, 104, predictor_embed_dim]\n",
        "        pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x)) # [16, 104, predictor_embed_dim]\n",
        "        # --\n",
        "        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1) # [16, 104, predictor_embed_dim]\n",
        "        # --\n",
        "        # print(\"predictor fwd pred_tokens\", pred_tokens.shape)\n",
        "\n",
        "        pred_tokens += pos_embs\n",
        "        x = x.repeat(len(masks), 1, 1)\n",
        "        x = torch.cat([x, pred_tokens], dim=1)\n",
        "\n",
        "        # -- fwd prop\n",
        "        for blk in self.predictor_blocks:\n",
        "            x = blk(x)\n",
        "        x = self.predictor_norm(x)\n",
        "\n",
        "        # -- return preds for mask tokens\n",
        "        x = x[:, N_ctxt:]\n",
        "        x = self.predictor_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, img_size=[224], patch_size=16, in_chans=3, embed_dim=768, predictor_embed_dim=384, depth=12, predictor_depth=12,\n",
        "        num_heads=12, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0, norm_layer=nn.LayerNorm, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_features = self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.init_std = init_std\n",
        "        # --\n",
        "        self.patch_embed = PatchEmbed(img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        # --\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=False)\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "        # --\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "    def forward(self, x, masks=None): # [batch,3,224,224]\n",
        "        if masks is not None:\n",
        "            if not isinstance(masks, list): masks = [masks]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        x = self.patch_embed(x)\n",
        "        B, N, D = x.shape # [batch, num_patches, embed_dim]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        # -- add positional embedding to x\n",
        "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
        "        x = x + pos_embed\n",
        "\n",
        "        if masks is not None: x = apply_masks(x, masks) # [batch, num_context_patches, embed_dim]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            x = blk(x)\n",
        "\n",
        "        if self.norm is not None: x = self.norm(x)\n",
        "        return x # [batch, num_context_patches, embed_dim]\n",
        "\n",
        "    def interpolate_pos_encoding(self, x, pos_embed):\n",
        "        npatch = x.shape[1] - 1\n",
        "        N = pos_embed.shape[1] - 1\n",
        "        if npatch == N: return pos_embed\n",
        "        class_emb, pos_embed = pos_embed[:, 0], pos_embed[:, 1:]\n",
        "        dim = x.shape[-1]\n",
        "        pos_embed = nn.functional.interpolate(pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2), scale_factor=math.sqrt(npatch / N), mode='bicubic',)\n",
        "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
        "\n",
        "# from functools import partial\n",
        "# def vit_predictor(**kwargs):\n",
        "#     model = VisionTransformerPredictor(\n",
        "#         mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "#     return model\n",
        "\n",
        "def vit(patch_size=16, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, # tiny\n",
        "        # patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, # small\n",
        "        # patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, # base\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "# encoder = vit()\n",
        "encoder = VisionTransformer(\n",
        "        patch_size=16, embed_dim=768, depth=1, num_heads=3, mlp_ratio=1,\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
        "\n",
        "# num_patches = (224/patch_size)^2\n",
        "# predictor = VisionTransformerPredictor(num_patches, embed_dim=768, predictor_embed_dim=384, depth=6, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02, **kwargs)\n",
        "predictor = VisionTransformerPredictor(num_patches=196, embed_dim=768, predictor_embed_dim=384, depth=1, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02)\n",
        "\n",
        "batch = 4\n",
        "img = torch.rand(batch, 3, 224, 224)\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/train.py\n",
        "mask_collator = MaskCollator(\n",
        "        input_size=(224, 224),\n",
        "        patch_size=16,\n",
        "        enc_mask_scale=(0.2, 0.8),\n",
        "        pred_mask_scale=(0.2, 0.8),\n",
        "        aspect_ratio=(0.3, 3.0),\n",
        "        nenc=1,\n",
        "        npred=4,\n",
        "        min_keep=4,\n",
        "        # allow_overlap=True)\n",
        "        allow_overlap=False)\n",
        "# self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
        "\n",
        "\n",
        "\n",
        "# v = mask_collator.step()\n",
        "# should pass the collater a list batch of idx?\n",
        "# collated_batch, collated_masks_enc, collated_masks_pred = mask_collator([batch,3,8])\n",
        "collated_batch, collated_masks_enc, collated_masks_pred = mask_collator([0,1,2,3])\n",
        "# print(v)\n",
        "\n",
        "\n",
        "import copy\n",
        "target_encoder = copy.deepcopy(encoder)\n",
        "\n",
        "\n",
        "def forward_target():\n",
        "    with torch.no_grad():\n",
        "        h = target_encoder(imgs)\n",
        "        h = F.layer_norm(h, (h.size(-1),))  # normalize over feature-dim\n",
        "        B = len(h)\n",
        "        # -- create targets (masked regions of h)\n",
        "        h = apply_masks(h, masks_pred)\n",
        "        h = repeat_interleave_batch(h, B, repeat=len(masks_enc))\n",
        "        return h\n",
        "\n",
        "def forward_context():\n",
        "    z = encoder(imgs, masks_enc)\n",
        "    z = predictor(z, masks_enc, masks_pred)\n",
        "    return z\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    m = next(momentum_scheduler)\n",
        "                    for param_q, param_k in zip(encoder.parameters(), target_encoder.parameters()):\n",
        "                        param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "\n",
        "\n",
        "# # num_context_patches = 121 if allow_overlap=True else 11\n",
        "z = encoder(img, collated_masks_enc) # [batch, num_context_patches, embed_dim]\n",
        "# print(len(collated_masks_enc), len(collated_masks_pred)) # nenc, npred\n",
        "# print(collated_masks_enc[0].shape, collated_masks_pred[0].shape) # , [batch, num_context_patches = 121 or 11], [batch, num_target_patches]\n",
        "out = predictor(z, collated_masks_enc, collated_masks_pred)\n",
        "# # print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test multiblock params\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# dataiter = iter(train_data)\n",
        "img,y = next(dataiter)\n",
        "img = img.unsqueeze(0)\n",
        "b,c,h,w = img.shape\n",
        "img = img.flatten(2).transpose(-2,-1).to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "print(img.shape)\n",
        "batch, seq,_ = img.shape\n",
        "M=4\n",
        "target_mask = multiblock(seq, min_s=0.15, max_s=0.2, M=M) # mask out targets to be predicted # [M, seq]\n",
        "context_mask = ~multiblock(seq, min_s=0.85, max_s=1., M=1)|target_mask.reshape(M,seq).any(0) # [1, seq], True->Mask\n",
        "target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "# print(target_mask, context_mask)\n",
        "\n",
        "\n",
        "# print(target_mask.shape, context_mask.shape)\n",
        "# target_img, context_img = img*target_mask.unsqueeze(-1), img*context_mask.unsqueeze(-1)\n",
        "target_img, context_img = img*~target_mask.unsqueeze(-1), img*~context_mask.unsqueeze(-1)\n",
        "# print(target_img.shape, context_img.shape)\n",
        "target_img, context_img = target_img.transpose(-2,-1).reshape(M,c,h,w), context_img.transpose(-2,-1).reshape(b,c,h,w)\n",
        "target_img, context_img = target_img.float(), context_img.float()\n",
        "\n",
        "# imshow(out.detach().cpu())\n",
        "imshow(target_img[0])\n",
        "imshow(context_img[0])\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "D6lVtbS5OHIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test multiblock mask\n",
        "def multiblock(seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    # mask_len, mask_pos = mask_len * seq, mask_pos * seq\n",
        "    indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "    return target_mask\n",
        "\n",
        "\n",
        "seq=400\n",
        "M=4\n",
        "\n",
        "# target_mask = multiblock(M, seq, min_s=0.15, max_s=0.2, M=1) # mask out targets to be predicted # [4*batch, seq]\n",
        "# context_mask = ~multiblock(1, seq, min_s=0.85, max_s=1., M=1)|target_mask.reshape(M,seq).any(0) # [batch, seq]\n",
        "\n",
        "\n",
        "target_mask = multiblock(seq, min_s=0.15, max_s=0.2, M=4) # mask out targets to be predicted # [4*batch, seq]\n",
        "context_mask = ~multiblock(seq, min_s=0.85, max_s=1., M=1)|target_mask.reshape(M,seq).any(0) # [batch, seq]\n",
        "\n",
        "# target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "\n",
        "# print(target_mask)\n",
        "# print(context_mask)\n",
        "\n",
        "print(target_mask.sum(-1))\n",
        "print(context_mask.sum(-1))\n",
        "\n",
        "\n",
        "# sorted_mask, ids = context_mask.sort(dim=1, stable=True)\n",
        "# indices = ids[~sorted_mask]#.reshape(M,-1) # int idx [num_context_toks] , idx of context not masked\n",
        "# sorted_x = x[torch.arange(batch).unsqueeze(-1), indices] # [batch, seq-num_trg_toks, dim]\n",
        "# print(indices)\n",
        "# print(x)\n",
        "# print(sorted_x)\n",
        "\n",
        "\n",
        "\n",
        "# sorted_mask, ids = target_mask.sort(dim=1, descending=False, stable=True)\n",
        "# # print(sorted_mask, ids)\n",
        "# indices = ids[~sorted_mask].reshape(M,-1) # int idx [M, seq-num_trg_toks] , idx of target not masked\n",
        "# print(indices)\n",
        "# batch=3\n",
        "# dim=2\n",
        "# # indices = indices.expand(batch,-1,-1)\n",
        "# # x = torch.arange(batch*seq).reshape(batch,seq).to(device)\n",
        "# x = torch.rand(batch, seq, dim)\n",
        "# print(x)\n",
        "# sorted_x = x[torch.arange(batch)[...,None,None], indices]\n",
        "# print(sorted_x.shape) # [batch, M, seq-num_trg_toks, dim]\n",
        "# print(sorted_x)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "p8xv1tpKIhim",
        "outputId": "e6613fee-2d32-4c6a-ef96-dee18da3aa6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([73, 73, 73, 73])\n",
            "tensor([248])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ijepa multiblock down\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "import math\n",
        "from multiprocessing import Value\n",
        "import torch\n",
        "_GLOBAL_SEED = 0\n",
        "\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=(224, 224),\n",
        "        patch_size=16,\n",
        "        enc_mask_scale=(0.2, 0.8),\n",
        "        pred_mask_scale=(0.2, 0.8),\n",
        "        aspect_ratio=(0.3, 3.0),\n",
        "        nenc=1,\n",
        "        npred=2,\n",
        "        min_keep=4,\n",
        "        allow_overlap=False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if not isinstance(input_size, tuple):\n",
        "            input_size = (input_size, ) * 2\n",
        "        self.patch_size = patch_size\n",
        "        self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.aspect_ratio = aspect_ratio\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "        self._itr_counter = Value('i', -1)  # collator is shared across worker processes\n",
        "\n",
        "    def step(self):\n",
        "        i = self._itr_counter\n",
        "        with i.get_lock():\n",
        "            i.value += 1\n",
        "            v = i.value\n",
        "        return v\n",
        "\n",
        "    def _sample_block_size(self, generator, scale, aspect_ratio_scale):\n",
        "        _rand = torch.rand(1, generator=generator).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.height * self.width * mask_scale)\n",
        "        # -- Sample block aspect-ratio\n",
        "        min_ar, max_ar = aspect_ratio_scale\n",
        "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
        "        while h >= self.height:\n",
        "            h -= 1\n",
        "        while w >= self.width:\n",
        "            w -= 1\n",
        "\n",
        "        return (h, w)\n",
        "\n",
        "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
        "        h, w = b_size\n",
        "\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # --\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            top = torch.randint(0, self.height - h, (1,))\n",
        "            left = torch.randint(0, self.width - w, (1,))\n",
        "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
        "            mask[top:top+h, left:left+w] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
        "        mask_complement[top:top+h, left:left+w] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        B = len(batch)\n",
        "\n",
        "        collated_batch = torch.utils.data.default_collate(batch)\n",
        "\n",
        "        seed = self.step()\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(seed)\n",
        "        p_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.pred_mask_scale,\n",
        "            aspect_ratio_scale=self.aspect_ratio)\n",
        "        e_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.enc_mask_scale,\n",
        "            aspect_ratio_scale=(1., 1.))\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.height * self.width\n",
        "        min_keep_enc = self.height * self.width\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                print(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "\n",
        "        return collated_batch, collated_masks_enc, collated_masks_pred\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "COvEnBXPYth3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}