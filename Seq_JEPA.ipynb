{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/Seq_JEPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FA00-tf6MJ-s"
      },
      "outputs": [],
      "source": [
        "# @title rohanpandey WISDM\n",
        "# https://github.com/rohanpandey/Analysis--WISDM-Smartphone-and-Smartwatch-Activity/blob/master/dataset_creation.ipynb\n",
        "! wget https://archive.ics.uci.edu/static/public/507/wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset.zip\n",
        "!unzip wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset.zip\n",
        "!unzip wisdm-dataset.zip\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "folder1=glob.glob(\"wisdm-dataset/raw/*\")\n",
        "# print(folder1)\n",
        "column_names = ['ID', 'activity','timestamp','x','y','z']\n",
        "overall_dataframe=pd.DataFrame(columns = column_names)\n",
        "for subfolder in folder1:\n",
        "    parent_dir = \"./processed/\"\n",
        "    path = os.path.join(parent_dir, subfolder.split('\\\\')[-1])\n",
        "    if not os.path.exists(path): os.makedirs(path)\n",
        "    folder2=glob.glob(subfolder+\"/*\")\n",
        "    for subsubfolder in folder2:\n",
        "        activity_dataframe = pd.DataFrame(columns = column_names)\n",
        "        subfolder_path = os.path.join(path, subsubfolder.split('/')[-1])\n",
        "        if not os.path.exists(subfolder_path): os.makedirs(subfolder_path)\n",
        "        files=glob.glob(subsubfolder+\"/*\")\n",
        "        for file in files:\n",
        "            # print(file)\n",
        "            df = pd.read_csv(file, sep=\",\",header=None)\n",
        "            df.columns = ['ID','activity','timestamp','x','y','z']\n",
        "            # activity_dataframe=activity_dataframe.append(df)\n",
        "            activity_dataframe = pd.concat([activity_dataframe, df], ignore_index=True)\n",
        "\n",
        "        activity_dataframe['z']=activity_dataframe['z'].str[:-1]\n",
        "        # activity_dataframe['meter']=subsubfolder.split('/')[-1]\n",
        "        # activity_dataframe['device']=subfolder.split('/')[-1]\n",
        "        activity_dataframe.to_csv(subfolder_path+'/data.csv',index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "CNVNCV8CGtR_"
      },
      "outputs": [],
      "source": [
        "# @title wisdm dataloader\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "# /content/processed/wisdm-dataset/raw/phone/accel/data.csv\n",
        "# /content/processed/wisdm-dataset/raw/watch/gyro/data.csv\n",
        "\n",
        "class BufferDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        # df_keep = pd.read_csv(\"data.csv\")#[['ID','activity','timestamp','x','y','z']]\n",
        "        df_keep = pd.read_csv(\"/content/processed/wisdm-dataset/raw/watch/accel/data.csv\")\n",
        "\n",
        "        user_acts = dict(tuple(df_keep.groupby(['ID','activity'])[['timestamp','x','y','z']]))\n",
        "        self.data = [[d.to_numpy(), a] for a, d in user_acts.items()]\n",
        "        self.act_dict = {i: act for i, act in enumerate(df_keep['activity'].unique())}\n",
        "        self.act_invdict = {v: k for k, v in self.act_dict.items()} # {'A': 0, 'B': 1, ...\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, id_act = self.data[idx]\n",
        "        id_act = self.process(id_act)\n",
        "        return torch.tensor(x[:3500]), id_act # 3567\n",
        "\n",
        "    def process(self, id_act):\n",
        "        return int(id_act[0]), self.act_invdict[id_act[1]]\n",
        "\n",
        "    def transform(self, act_list): # rand resize crop, rand mask # https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html\n",
        "        act_list = np.array(act_list)\n",
        "        try: hr, temp, heart= zip(*act_list)\n",
        "        except ValueError: print('err:',act_list)\n",
        "        kind = 'nearest' if len(act_list)>self.seq_len/.7 else 'linear'\n",
        "        temp_interpolator = scipy.interpolate.interp1d(hr, temp, kind=kind) # linear nearest quadratic cubic\n",
        "        heart_interpolator = scipy.interpolate.interp1d(hr, heart, kind=kind) # linear nearest quadratic cubic\n",
        "        hr_ = np.sort(np.random.uniform(hr[0], hr[-1], round(self.seq_len*random.uniform(1,1/.7))))\n",
        "        temp_, heart_ = temp_interpolator(hr_), heart_interpolator(hr_)\n",
        "        act_list = list(zip(hr_, temp_, heart_))\n",
        "\n",
        "        idx = torch.randint(len(act_list)-self.seq_len+1, size=(1,))\n",
        "        # return act_list[idx: idx+self.seq_len]\n",
        "        act_list = act_list[idx: idx+self.seq_len]\n",
        "\n",
        "        # mask=(torch.rand(self.seq_len)<.1) # True -> masked # random masking\n",
        "        # # act_list[mask] = self.pad[0]\n",
        "        # act_list = [self.pad[0] if m else a for m,a in zip(mask, act_list)]\n",
        "        return act_list\n",
        "\n",
        "    def add(self, episode):\n",
        "        self.data.append(episode)\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    # print(npimg.shape) # (3, 64, 64)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# df_keep = pd.read_csv(\"data.csv\")#[['ID','activity','timestamp','x','y','z']]\n",
        "# user_acts = dict(tuple(df_keep.groupby(['ID','activity'])[['timestamp','x','y','z']]))\n",
        "# dataset = [[d.to_numpy(), a] for a, d in user_acts.items()]\n",
        "\n",
        "\n",
        "train_data = BufferDataset() # one line of poem is roughly 50 characters\n",
        "\n",
        "dataset_size = len(train_data)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(0.7 * dataset_size))\n",
        "np.random.seed(0)\n",
        "np.random.shuffle(indices)\n",
        "train_indices, val_indices = indices[:split], indices[split:]\n",
        "\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "valid_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 64 #512\n",
        "# train_loader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "# test_loader = DataLoader(train_data, sampler=valid_sampler, batch_size=batch_size, num_workers=2, drop_last=True)\n",
        "train_loader = DataLoader(train_data, sampler=train_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "test_loader = DataLoader(train_data, sampler=valid_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4c4r5Hkry99",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title tsai\n",
        "# https://timeseriesai.github.io/tsai/\n",
        "!pip install -qU tsai # 3mins\n",
        "import tsai\n",
        "from tsai.data.external import get_UCR_data, get_UCR_multivariate_list\n",
        "\n",
        "# l = get_UCR_multivariate_list()\n",
        "# print(len(l), l)\n",
        "# X_train, y_train, X_valid, y_valid = get_UCR_data(dsid) # tsai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRuBXTauj2f4",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title tslearn\n",
        "!pip install -qU tslearn\n",
        "from tslearn.datasets import UCR_UEA_datasets # https://tslearn.readthedocs.io/en/latest/gen_modules/datasets/tslearn.datasets.UCR_UEA_datasets.html\n",
        "\n",
        "# l = UCR_UEA_datasets().list_datasets() # 30 ['ArticularyWordRecognition', 'AtrialFibrillation', 'BasicMotions', 'CharacterTrajectories', 'Cricket', 'DuckDuckGeese', 'EigenWorms', 'Epilepsy', 'EthanolConcentration', 'ERing', 'FaceDetection', 'FingerMovements', 'HandMovementDirection', 'Handwriting', 'Heartbeat', 'InsectWingbeat', 'JapaneseVowels', 'Libras', 'LSST', 'MotorImagery', 'NATOPS', 'PenDigits', 'PEMS-SF', 'Phoneme', 'RacketSports', 'SelfRegulationSCP1', 'SelfRegulationSCP2', 'SpokenArabicDigits', 'StandWalkJump', 'UWaveGestureLibrary']\n",
        "# l = UCR_UEA_datasets().list_multivariate_datasets() # same ^\n",
        "# l = UCR_UEA_datasets().list_univariate_datasets() # 0 []\n",
        "# print(len(l), l)\n",
        "\n",
        "# for dataset_name in data_loader.list_datasets(): # 30 ['ArticularyWordRecognition', 'AtrialFibrillation', 'BasicMotions', 'CharacterTrajectories', 'Cricket', 'DuckDuckGeese', 'EigenWorms', 'Epilepsy', 'EthanolConcentration', 'ERing', 'FaceDetection', 'FingerMovements', 'HandMovementDirection', 'Handwriting', 'Heartbeat', 'InsectWingbeat', 'JapaneseVowels', 'Libras', 'LSST', 'MotorImagery', 'NATOPS', 'PenDigits', 'PEMS-SF', 'Phoneme', 'RacketSports', 'SelfRegulationSCP1', 'SelfRegulationSCP2', 'SpokenArabicDigits', 'StandWalkJump', 'UWaveGestureLibrary']\n",
        "#     X_train, y_train, X_test, y_test = data_loader.load_dataset(dataset_name)\n",
        "\n",
        "# X_train, y_train, X_test, y_test = UCR_UEA_datasets().load_dataset('AtrialFibrillation') # tslearn\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ep7xumXZke5y",
        "outputId": "a2ecdb4b-7a3b-4bce-94e1-a0036a86891d",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.0/37.0 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.7/142.7 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hTraining data shape: (15, 2)\n",
            "Training labels shape: (15,)\n",
            "Testing data shape: (15, 2)\n",
            "Testing labels shape: (15,)\n",
            "dim_0    0     -0.34086\n",
            "1     -0.38038\n",
            "2     -0.34580\n",
            "3...\n",
            "dim_1    0      0.14820\n",
            "1      0.13338\n",
            "2      0.10868\n",
            "3...\n",
            "Name: 0, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# @title sktime\n",
        "!pip install -q sktime\n",
        "from sktime.datasets import load_UCR_UEA_dataset\n",
        "import numpy as np\n",
        "# https://www.sktime.net/en/v0.32.2/examples/AA_datatypes_and_datasets.html#Section-3.2.3:-time-series-classification-data-sets-from-the-UCR/UEA-time-series-classification-repository\n",
        "\n",
        "X_train, y_train = load_UCR_UEA_dataset('AtrialFibrillation')\n",
        "X_train, y_train = load_UCR_UEA_dataset('AtrialFibrillation', split=\"train\")\n",
        "X_test, y_test = load_UCR_UEA_dataset('AtrialFibrillation', split=\"test\")\n",
        "\n",
        "print(f\"Training data shape: {X_train.shape}\")\n",
        "print(f\"Training labels shape: {y_train.shape}\")\n",
        "print(f\"Testing data shape: {X_test.shape}\")\n",
        "print(f\"Testing labels shape: {y_test.shape}\")\n",
        "\n",
        "# X_train and X_test will typically be Pandas DataFrames or NumPy arrays\n",
        "# y_train and y_test will be NumPy arrays or Pandas Series\n",
        "\n",
        "# print(X_train, y_train)\n",
        "# print(X_train[0], y_train[0])\n",
        "# print(X_train)\n",
        "print(X_train.iloc[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Gr_588txsd6",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title test load all datasets\n",
        "l = get_UCR_multivariate_list()\n",
        "print(len(l), l)\n",
        "for i, dataset_name in enumerate(get_UCR_multivariate_list()):\n",
        "# for dataset_name in get_UCR_multivariate_list()[15:20]:\n",
        "    print(dataset_name)\n",
        "    # if dataset_name in ['DuckDuckGeese','FaceDetection','InsectWingbeat','PEMS-SF']:\n",
        "    #     print('skip', dataset_name)\n",
        "    #     continue\n",
        "    # try: X_train, y_train, X_valid, y_valid = get_UCR_data(dataset_name) # tsai\n",
        "    # # InsectWingbeat, PEMS-SF slow\n",
        "\n",
        "    # if dataset_name in ['AtrialFibrillation', 'CharacterTrajectories','DuckDuckGeese','EigenWorms','ERing','InsectWingbeat','JapaneseVowels','SpokenArabicDigits']:\n",
        "    # # EigenWorms slow\n",
        "    #     print('skip', dataset_name)\n",
        "    #     continue\n",
        "    # try: X_train, y_train, X_test, y_test = UCR_UEA_datasets().load_dataset(dataset_name) # tslearn\n",
        "\n",
        "    if dataset_name in ['InsectWingbeat']:\n",
        "        print('skip', dataset_name)\n",
        "        continue\n",
        "    try:\n",
        "        X_train, y_train = load_UCR_UEA_dataset(dataset_name)\n",
        "        X_train, y_train = load_UCR_UEA_dataset(dataset_name, split=\"train\")\n",
        "        X_test, y_test = load_UCR_UEA_dataset(dataset_name, split=\"test\")\n",
        "    # InsectWingbeat slow oom\n",
        "\n",
        "    # who has DuckDuckGeese','FaceDetection','InsectWingbeat\n",
        "\n",
        "\n",
        "    except Exception as e: print(e); continue\n",
        "    print(dataset_name, X_train.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgLjldpPdSsQ",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title time series DataLoader\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "dataset_name = 'EthanolConcentration'\n",
        "# X_train, y_train, X_test, y_test = get_UCR_data(dataset_name) # tsai\n",
        "X_train, y_train, X_test, y_test = UCR_UEA_datasets().load_dataset(dataset_name) # tslearn\n",
        "\n",
        "\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        chars = sorted(list(set(y)))\n",
        "        self.vocab_size = len(chars) #\n",
        "        self.stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "        self.itos = {i:ch for i,ch in enumerate(chars)}\n",
        "        self.X = torch.tensor(X) # (N, 1, T)\n",
        "        self.y = self.data_process(y) #\n",
        "\n",
        "    def data_process(self, data): # str\n",
        "        return torch.tensor([self.stoi.get(c) for c in data]) #\n",
        "    def __len__(self): return len(self.y)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "train_data = TimeSeriesDataset(X_train, y_train)\n",
        "test_data = TimeSeriesDataset(X_test, y_test)\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=2)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=2)\n",
        "\n",
        "# print(X_train, y_train, X_test, y_test)\n",
        "# for x,y in train_loader:\n",
        "# # for x,y in train_data:\n",
        "#     print(x.shape, y.shape) # (261, 1751, 3)\n",
        "#     print(x, y)\n",
        "#     break\n",
        "\n",
        "# print(x.shape[-1])\n",
        "# print(X_train.shape[-1])\n",
        "# print(train_data.vocab_size) # 4\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title plot data\n",
        "# print(X_train, y_train, X_test, y_test)\n",
        "# for x,y in train_loader:\n",
        "# for x,y in train_data:\n",
        "for i, (x,y) in enumerate(train_data):\n",
        "    # print(x.shape, y.shape)\n",
        "    # print(x, y)\n",
        "    # break\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.rcParams[\"figure.figsize\"] = (8,4)\n",
        "    # plt.plot(x[:,0])\n",
        "    for j in range(x.shape[-1]):\n",
        "        plt.plot(x[:,j])\n",
        "    plt.show()\n",
        "    if i>=3: break\n",
        "\n",
        "# print(x.shape[-1])\n",
        "# print(X_train.shape[-1])\n",
        "# print(train_data.vocab_size)\n",
        "\n"
      ],
      "metadata": {
        "id": "3B3t5tSmK0Fv",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6T4F651kmGh"
      },
      "outputs": [],
      "source": [
        "# @title AttentionBlock\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class SelfAttn(nn.Module):\n",
        "    def __init__(self, dim, n_heads):\n",
        "        super().__init__()\n",
        "        self.dim, self.n_heads = dim, n_heads\n",
        "        d_head = dim//n_heads\n",
        "        self.qkv = nn.Linear(dim, dim*3, bias=False)\n",
        "        # self.lin = nn.Linear(dim, dim)\n",
        "        self.lin = zero_module(nn.Linear(dim, dim))\n",
        "        # self.rope = RoPE(d_head, seq_len=512, base=10000)\n",
        "        # self.rope = RoPE2D(d_head, h=64, w=64, base=100)\n",
        "        self.scale = d_head**-.5\n",
        "\n",
        "    def forward(self, x): # [b,t,d]\n",
        "        q,k,v = self.qkv(x).unflatten(-1, (self.n_heads,-1)).transpose(1,2).chunk(3, dim=-1) # [b, t, n_heads, d_head] -> [b, n_heads, t, d_head]\n",
        "        # q, k = self.rope(q), self.rope(k)\n",
        "        q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2)\n",
        "        context = k.transpose(-2,-1) @ v # [batch, n_heads, d_head, d_head]\n",
        "        x = q @ context # [b, n_heads, t, d_head]\n",
        "        # print('SelfAttn', x.shape)\n",
        "        x = x.transpose(1,2).flatten(2)\n",
        "        return self.lin(x)\n",
        "\n",
        "\n",
        "\n",
        "class SwiGLU(nn.Module): # https://arxiv.org/pdf/2002.05202\n",
        "    def __init__(self, d_model, ff_dim): # d_model * 3*ff_dim params\n",
        "        super().__init__()\n",
        "        self.lin0 = nn.Linear(d_model, 2*ff_dim, bias=False)\n",
        "        self.lin1 = zero_module(nn.Linear(ff_dim, d_model, bias=False))\n",
        "\n",
        "    def forward(self, x): # [b,t,d]\n",
        "        x0, x1 = self.lin0(x).chunk(2, dim=-1)\n",
        "        return self.lin1(x0*F.silu(x1))\n",
        "\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, mult=4, drop=0.):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        # self.norm = nn.RMSNorm(d_model, elementwise_affine=False) # LayerNorm RMSNorm\n",
        "        self.norm1, self.norm2 = nn.LayerNorm(d_model), nn.LayerNorm(d_model)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "        self.attn = SelfAttn(d_model, n_heads)\n",
        "        # act = nn.GELU() # ReLU GELU\n",
        "        act = Swwish()\n",
        "        ff_dim=d_model*mult\n",
        "        self.ff = nn.Sequential(\n",
        "            # nn.RMSNorm(d_model), act, nn.Dropout(drop), nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Dropout(drop), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            nn.RMSNorm(d_model), nn.Linear(d_model, ff_dim), act,\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(drop), nn.Linear(d_model, ff_dim), act,\n",
        "            nn.RMSNorm(ff_dim), nn.Dropout(drop), zero_module(nn.Linear(ff_dim, d_model))\n",
        "        )\n",
        "        # self.ff = SwiGLU(d_model, ff_dim)\n",
        "\n",
        "\n",
        "    def forward(self, x): # [batch, seq_len, d_model]\n",
        "        # print('attnblk fwd',x.shape)\n",
        "        x = x + self.drop(self.attn(self.norm1(x)))\n",
        "        x = x + self.ff(x)\n",
        "        # x = x + self.drop(self.ff(self.norm2(x)))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fh9o__m2-j7K",
        "outputId": "f412c2b0-c375-4617-e7b7-002f76904460"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/268.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m266.2/268.0 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.0/268.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25htorch.Size([64, 200])\n"
          ]
        }
      ],
      "source": [
        "# @title simplex\n",
        "!pip install -q opensimplex\n",
        "import opensimplex\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def simplexmask1d(seq=512, ctx_scale=(.85,1), trg_scale=(.6,.8), B=64, chaos=[1,.5]):\n",
        "    i = np.linspace(0, chaos[0], num=seq) # 2-5\n",
        "    noise = opensimplex.noise2array(i, np.random.randint(1e10, size=B)) # [B, seq]\n",
        "    noise = torch.from_numpy(noise)\n",
        "    # trunc_normal = torch.fmod(torch.randn(2)*.3,1)/2 + .5\n",
        "    # print(trunc_normal)\n",
        "    ctx_mask_scale = torch.rand(1) * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    trg_mask_scale = torch.rand(1) * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "    # ctx_mask_scale = trunc_normal[0] * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    # trg_mask_scale = trunc_normal[1] * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "\n",
        "    ctx_len, trg_len = int(seq*ctx_mask_scale), int(seq*trg_mask_scale)\n",
        "    val, trg_index = torch.topk(noise, trg_len, dim=1, sorted=False)\n",
        "    ctx_len = ctx_len - trg_len\n",
        "\n",
        "    remove_mask = torch.ones((B,seq), dtype=bool) # [B, S]\n",
        "    remove_mask.scatter_(1, trg_index, False).flatten()\n",
        "    ind = torch.arange(seq).unsqueeze(0).repeat(B,1)[remove_mask].reshape(B, -1)\n",
        "\n",
        "    i = np.linspace(0, chaos[1], num=seq) # 2-5\n",
        "    noise = opensimplex.noise2array(i, np.random.randint(1e10, size=B)) # [B, seq]\n",
        "    noise = torch.from_numpy(noise)[remove_mask].reshape(B, -1)\n",
        "    val, ctx_ind = torch.topk(noise, ctx_len, dim=1, sorted=False)\n",
        "    ctx_index = ind[torch.arange(B).unsqueeze(-1), ctx_ind]\n",
        "    return ctx_index, trg_index\n",
        "\n",
        "b=64\n",
        "# ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.7,.8), trg_scale=(.4,.6), B=b, chaos=[3,.5])\n",
        "ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.85,1), trg_scale=(.7,.8), B=b, chaos=[3,.5])\n",
        "# ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.8,.9), trg_scale=(.7,.8), B=b, chaos=[1,.5])\n",
        "mask = torch.zeros(b ,200)\n",
        "mask[torch.arange(b).unsqueeze(-1), trg_index] = 1\n",
        "mask[torch.arange(b).unsqueeze(-1), ctx_index] = .5\n",
        "# mask = mask[None,...]\n",
        "# mask = mask[:,None,None,:]#.repeat(1,3,1,1)\n",
        "print(mask.shape)\n",
        "\n",
        "# def imshow(img):\n",
        "#     npimg = img.numpy()\n",
        "#     plt.rcParams[\"figure.figsize\"] = (8,4)\n",
        "#     # plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "#     plt.pcolormesh(np.transpose(npimg, (1, 2, 0)))\n",
        "#     plt.show()\n",
        "# # imshow(mask[0])\n",
        "# import torchvision\n",
        "# imshow(torchvision.utils.make_grid(mask, nrow=1))\n",
        "\n",
        "# print(index)\n",
        "# print(index.shape)\n",
        "# print(mask)\n",
        "# print(mask.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3mc56_TRvSE"
      },
      "outputs": [],
      "source": [
        "trunc_normal = torch.fmod(torch.randn(100),1)/2 + .5\n",
        "print(trunc_normal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Xn7WZShwWxF8"
      },
      "outputs": [],
      "source": [
        "# @title ijepa multiblock 1d\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "import torch\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(self, length=200,\n",
        "        enc_mask_scale=(0.2, 0.8), pred_mask_scale=(0.2, 0.8),\n",
        "        nenc=1, npred=2, min_keep=4, allow_overlap=False):\n",
        "        super().__init__()\n",
        "        self.length = length\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "\n",
        "    def _sample_block_size(self, scale):\n",
        "        _rand = torch.rand(1).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.length * mask_scale) # num patches to keep\n",
        "        # -- Sample block aspect-ratio\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        l = max_keep#int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        while l >= self.length: l -= 1 # crop mask to be smaller than img\n",
        "        return l\n",
        "\n",
        "    def _sample_block_mask(self, length, acceptable_regions=None):\n",
        "        l = length\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # --\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            left = torch.randint(0, self.length - l, (1,))\n",
        "            mask = torch.zeros(self.length, dtype=torch.int32)\n",
        "            mask[left:left+l] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones(self.length, dtype=torch.int32)\n",
        "        mask_complement[left:left+l] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, B):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        p_size = self._sample_block_size(scale=self.pred_mask_scale)\n",
        "        e_size = self._sample_block_size(scale=self.enc_mask_scale)\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.length\n",
        "        min_keep_enc = self.length\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                print(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "        return collated_masks_enc, collated_masks_pred\n",
        "\n",
        "batch=64\n",
        "length=200\n",
        "mask_collator = MaskCollator(length=length, enc_mask_scale=(.85, 1.), pred_mask_scale=(.15, .2),\n",
        "        nenc=1, npred=4, min_keep=4,\n",
        "        # allow_overlap=True)\n",
        "        allow_overlap=False)\n",
        "\n",
        "collated_masks_enc, collated_masks_pred = mask_collator(batch)\n",
        "context_indices, trg_indices = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "# print(context_indices.shape, trg_indices.shape)\n",
        "\n",
        "\n",
        "# plt.pcolormesh(mask)\n",
        "b=64\n",
        "mask = torch.zeros(batch ,length)\n",
        "mask[torch.arange(batch).unsqueeze(-1), trg_indices] = 1\n",
        "mask[torch.arange(batch).unsqueeze(-1), context_indices] = .5\n",
        "# mask = mask[None,...]\n",
        "# print(mask.shape)\n",
        "# mask = mask[:,None,None,:]#.repeat(1,3,1,1)\n",
        "# print(mask.shape)\n",
        "\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# def imshow(img):\n",
        "#     npimg = img.numpy()\n",
        "#     plt.rcParams[\"figure.figsize\"] = (8,4)\n",
        "#     # plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "#     plt.pcolormesh(np.transpose(npimg, (1, 2, 0)))\n",
        "#     # plt.imshow(npimg)\n",
        "#     plt.show()\n",
        "# # imshow(mask)\n",
        "# import torchvision\n",
        "# print(torchvision.utils.make_grid(mask, nrow=1).shape)\n",
        "# # imshow(torchvision.utils.make_grid(mask, nrow=8))\n",
        "# imshow(torchvision.utils.make_grid(mask, nrow=1))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## visualise mask distribution"
      ],
      "metadata": {
        "id": "oeazTakLj_Nn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37BYpacmIcw1"
      },
      "outputs": [],
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (4,4)\n",
        "# plt.plot(ttc,ttt)\n",
        "plt.scatter(ttc,ttt)\n",
        "plt.xlabel('context masks')\n",
        "plt.ylabel('target masks')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58nNtUss4YFX"
      },
      "outputs": [],
      "source": [
        "ttc=[]\n",
        "ttt=[]\n",
        "def mean(x): return sum(x)/len(x)\n",
        "\n",
        "for i in range(1000):\n",
        "    # collated_masks_enc, collated_masks_pred = mask_collator(1)\n",
        "    # context_indices, trg_indices = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "    # context_indices, trg_indices = simplexmask1d(seq=200, ctx_scale=(.8,1), trg_scale=(.2,.8), B=1, chaos=[3,.5])\n",
        "    context_indices, trg_indices = simplexmask1d(seq=200, ctx_scale=(.85,1), trg_scale=(.7,.8), B=1, chaos=[1,.5])\n",
        "    ttc.append(context_indices.shape[-1])\n",
        "    ttt.append(trg_indices.shape[-1])\n",
        "\n",
        "print(mean(ttc), mean(ttt))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"figure.figsize\"] = (4,4)\n",
        "plt.hist(ttc, bins=20, alpha=.5, label='context mask')\n",
        "plt.hist(ttt, bins=20, alpha=.5, label='target mask')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## bdnffdb"
      ],
      "metadata": {
        "id": "FqlsFoUJqeGD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ge36SCxOl2Oq"
      },
      "outputs": [],
      "source": [
        "# @title RoPE\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def RoPE(dim, seq_len=512, base=10000):\n",
        "    theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "    pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "    angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "    rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [1, seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "    return rot_emb\n",
        "\n",
        "# class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "#     def __init__(self, dim, seq_len=512, base=10000):\n",
        "#         super().__init__()\n",
        "#         self.dim, self.base = dim, base\n",
        "#         theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "#         pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "#         angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "#         self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         seq_len = x.size(1)\n",
        "#         if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.base)\n",
        "#         return x * self.rot_emb[:seq_len]\n",
        "\n",
        "class RotEmb(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, top=torch.pi, base=10000):\n",
        "        super().__init__()\n",
        "        self.theta = top / (base ** (torch.arange(0, dim, step=2, device=device) / dim))\n",
        "        # self.theta = top / (base ** torch.linspace(0, 1, dim//2, device=device))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (pos.unsqueeze(-1) * self.theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [seq_len, dim]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "n9a9OwgKjTUP"
      },
      "outputs": [],
      "source": [
        "# @title snake\n",
        "# https://github.com/Aria-K-Alethia/BigCodec/blob/main/vq/activations.py\n",
        "# https://github.com/zhenye234/X-Codec-2.0/blob/main/vq/activations.py#L62\n",
        "# Implementation adapted from https://github.com/EdwardDixon/snake under the MIT license.\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# class Snake(nn.Module):\n",
        "#     def __init__(self, in_features, alpha=1.0, alpha_logscale=False):\n",
        "#         super().__init__()\n",
        "#         # self.in_features = in_features\n",
        "#         self.alpha_logscale = alpha_logscale\n",
        "#         if self.alpha_logscale: # log scale alphas initialized to zeros\n",
        "#             self.alpha = nn.Parameter(torch.zeros(in_features) * alpha)\n",
        "#         else: # linear scale alphas initialized to ones\n",
        "#             self.alpha = nn.Parameter(torch.ones(in_features) * alpha)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         alpha = self.alpha.unsqueeze(0).unsqueeze(-1) # line up with x to [B, C, T]\n",
        "#         if self.alpha_logscale:\n",
        "#             alpha = torch.exp(alpha)\n",
        "#         x = x + (1.0 / (alpha + 1e-9)) * torch.pow(torch.sin(x * alpha), 2) # Snake ∶= x + 1/a * sin^2(ax)\n",
        "#         return x\n",
        "\n",
        "\n",
        "class Snake(nn.Module):\n",
        "    def __init__(self, dim, *args, **kwargs):\n",
        "        super().__init__()\n",
        "        # self.alpha = nn.Parameter(torch.zeros(1,dim,1)).exp() # alpha_logscale=True\n",
        "        self.alpha = nn.Parameter(torch.ones(1,dim,1)*1.) # 1.\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + (1.0 / (self.alpha + 1e-9)) * torch.pow(torch.sin(x * self.alpha), 2) # Snake ∶= x + 1/a * sin^2(ax)\n",
        "\n",
        "\n",
        "@torch.jit.script\n",
        "def snake(x, alpha): # [b,c,t], [1,c,1]\n",
        "    return x + (alpha + 1e-9).reciprocal() * torch.sin(alpha * x).pow(2) # [b,c,t]\n",
        "\n",
        "class Snake1d(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.alpha = nn.Parameter(torch.zeros(1,dim,1)).exp()\n",
        "        self.alpha = nn.Parameter(torch.ones(1,dim,1)*1.) # 1.\n",
        "\n",
        "    def forward(self, x):\n",
        "        return snake(x, self.alpha)\n",
        "\n",
        "\n",
        "class SnakeBeta(nn.Module):\n",
        "    def __init__(self, in_features, alpha=1.0, alpha_logscale=False):\n",
        "        super().__init__()\n",
        "        # self.in_features = in_features\n",
        "        self.alpha_logscale = alpha_logscale\n",
        "        if self.alpha_logscale: # log scale alphas initialized to zeros\n",
        "            self.alpha = nn.Parameter(torch.zeros(in_features) * alpha)\n",
        "            self.beta = nn.Parameter(torch.zeros(in_features) * alpha)\n",
        "        else: # linear scale alphas initialized to ones\n",
        "            self.alpha = nn.Parameter(torch.ones(in_features) * alpha)\n",
        "            self.beta = nn.Parameter(torch.ones(in_features) * alpha)\n",
        "\n",
        "    def forward(self, x): # [b,c,t]\n",
        "        alpha = self.alpha.unsqueeze(0).unsqueeze(-1) # line up with x to [B, C, T]\n",
        "        beta = self.beta.unsqueeze(0).unsqueeze(-1)\n",
        "        if self.alpha_logscale:\n",
        "            alpha = torch.exp(alpha)\n",
        "            beta = torch.exp(beta)\n",
        "        x = x + (1. / (beta + 1e-9)) * pow(torch.sin(x * alpha), 2) # SnakeBeta ∶= x + 1/b *sin^2(ax)\n",
        "        return x # [b,c,t]\n",
        "\n",
        "b,c,t = 5,16,7\n",
        "# a1 = Snake(c)\n",
        "a1 = Snake(c, alpha_logscale=True) # 70.4 µs 69.9\n",
        "# a1 = Snake1d(c) # 47.8 µs 48.3\n",
        "# a1 = SnakeBeta(256)\n",
        "x = torch.randn(b,c,t)\n",
        "# x = a1(x)\n",
        "# print(x.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title swwish\n",
        "@torch.jit.script\n",
        "def learntswwish(x, alpha): # [b,c,t], [1,c,1]\n",
        "    # print('alpha', alpha.shape, x.shape)\n",
        "    # alpha = alpha.exp()\n",
        "    return .5 * (1 + x - torch.cos(alpha * x)) # [b,c,t]\n",
        "    # return .5 * (1/alpha + x - torch.cos(alpha * x)/alpha) # [b,c,t]\n",
        "    # return .5 * (1/alpha + x - torch.cos(1.25*alpha * x)/alpha) # [b,c,t]\n",
        "\n",
        "class LearntSwwish(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.alpha = nn.Parameter(torch.zeros(1,dim,1))#.exp()\n",
        "        self.alpha = nn.Parameter(torch.ones(1,dim,1)*1.) # 1.\n",
        "        self.alpha = nn.Parameter(torch.zeros(dim))#.exp()\n",
        "        # self.alpha = nn.Parameter(torch.randn(dim).abs()*4)\n",
        "        self.alpha = nn.Parameter(torch.randn(dim,1)*30) #4 20\n",
        "        # self.alpha = nn.Parameter(torch.ones(1,dim)*1.) # 1.\n",
        "\n",
        "    def forward(self, x):\n",
        "        return learntswwish(x, self.alpha)\n",
        "\n",
        "class Swwish(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # return .5 * (1 + x - x.cos())\n",
        "        return .5 * (1 + x - 1.25*x.cos())\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "MXZJBLF3D2Cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-zdjdJixtOu",
        "outputId": "8e1b3f66-45c7-49d3-de8e-4571d6bdf73d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34928\n",
            "torch.Size([4, 219, 16])\n"
          ]
        }
      ],
      "source": [
        "# @title TransformerModel/Predictor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, n_heads=4, d_hid=None, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        # self.pos_emb = nn.Parameter(torch.randn(1, 256, d_model)*.02) # 200\n",
        "        self.pos_emb = RoPE(d_model, seq_len=256, base=10000) # 256\n",
        "\n",
        "        # self.transformer = nn.Sequential(*[AttentionBlock(d_model, n_heads=n_heads) for _ in range(nlayers)])\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, n_heads=n_heads, drop=drop) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.randn(1,1,d_model)*0.02) # randn zeros\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim)# if out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = x.shape\n",
        "        # x = x * self.pos_enc(context_indices)\n",
        "        # print(\"Trans pred\",x.shape, self.pos_emb[0,context_indices].shape)\n",
        "        x = x + self.pos_emb[0,context_indices]\n",
        "        # x = x * self.pos_emb[0,context_indices]\n",
        "        # print('pred fwd', self.pos_emb[:,context_indices].shape)\n",
        "\n",
        "        # pred_tokens = self.cls * self.pos_enc(trg_indices) # [M, num_trg_toks, d_model]\n",
        "        pred_tokens = self.cls + self.pos_emb[0,trg_indices]\n",
        "        # pred_tokens = self.cls * self.pos_emb[0,trg_indices]\n",
        "        # print(\"pred fwd\", x.shape, pred_tokens.shape)\n",
        "        x = torch.cat([x, pred_tokens], dim=1) # [batch, seq_len+num_trg_toks, d_model]\n",
        "        out = self.transformer(x)\n",
        "\n",
        "        out = self.norm(out)\n",
        "        out = out[:,seq:] # [batch, num_trg_toks, d_model]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "class SLSTM(nn.Module):\n",
        "    def __init__(self, d_model, num_layers=2, batch_first=True):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(d_model, d_model, num_layers)\n",
        "\n",
        "    def forward(self, x): # [b,c,t]\n",
        "        x = x + self.lstm(x.transpose(-2,-1))[0].transpose(-2,-1) # skip=True\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, drop=0):\n",
        "    def __init__(self, patch_size, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0):\n",
        "        super().__init__()\n",
        "        patch_size=8\n",
        "        act = nn.ReLU() # ReLU SiLU GELU\n",
        "        # act = LearntSwwish(d_model) # SnakeBeta LearntSwwish\n",
        "        # act1 = LearntSwwish(d_model) # SnakeBeta LearntSwwish\n",
        "        # act = Swwish()\n",
        "        self.embed = nn.Sequential(\n",
        "            # # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.MaxPool1d(2,2), #nn.MaxPool1d(3, 2, 3//2),\n",
        "            # # nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.Conv1d(d_model, d_model,3,2,3//2)\n",
        "            nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), act, #nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2), nn.BatchNorm1d(d_model), act, #nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2),\n",
        "            # nn.Conv1d(in_dim, d_model, patch_size, patch_size), # like patch\n",
        "            # nn.Conv1d(in_dim, d_model, 1, 1), # like patch\n",
        "\n",
        "            # nn.Conv2d(d_model, d_model,(in_dim,3),2,3//2),\n",
        "            # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.Dropout(drop), nn.BatchNorm1d(d_model), snake,\n",
        "            # SLSTM(d_model),\n",
        "\n",
        "            )\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        # self.pos_emb = nn.Parameter(torch.randn(1, 256, d_model)*.02) # 200\n",
        "        self.pos_emb = RoPE(d_model, seq_len=256, base=10000) # 256\n",
        "        # self.transformer = nn.Sequential(*[AttentionBlock(d_model, n_heads=n_heads) for _ in range(nlayers)])\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, n_heads=n_heads, drop=drop) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "        # try: print(\"Trans fwd\",x.shape, context_indices.shape)\n",
        "        # except: print(\"Trans fwd noind\",x.shape)\n",
        "        # x = self.pos_enc(x)\n",
        "        x = x + self.pos_emb[:,:x.shape[1]]\n",
        "        # x = x * self.pos_emb[:,:x.shape[1]]\n",
        "        if context_indices != None: x = x[torch.arange(x.shape[0]).unsqueeze(-1), context_indices] # [batch, num_context_toks, d_model]\n",
        "\n",
        "        # print(\"TransformerModel\",x.shape)\n",
        "        x = self.transformer(x)\n",
        "        out = self.norm(x)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out\n",
        "\n",
        "batch, seq_len, d_model = 4,1751,16 # wisdm 3500, ethol conc 1751\n",
        "in_dim = 3\n",
        "patch_size=32\n",
        "model = TransformerModel(patch_size, in_dim, d_model, n_heads=4, nlayers=10, drop=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "# # # print(out)\n",
        "# model = TransformerPredictor(in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1).to(device)\n",
        "# out = model(out)\n",
        "# print(out.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ardu1zJwdHM3",
        "outputId": "ae6334b9-eb0a-44d8-d8ea-61bea80ff06b",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "92800\n",
            "torch.Size([])\n"
          ]
        }
      ],
      "source": [
        "# @title SeqJEPA\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class SeqJEPA(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, nlayers=2, n_heads=4, drop=0):\n",
        "        super().__init__()\n",
        "        if out_dim is None: out_dim = d_model\n",
        "        self.patch_size = 8 # 8 32\n",
        "        self.student = TransformerModel(self.patch_size, in_dim, d_model, out_dim=out_dim, n_heads=n_heads, nlayers=nlayers, drop=drop)\n",
        "        self.predicter = TransformerPredictor(out_dim, d_model//2, out_dim, n_heads=4, nlayers=1, drop=drop)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "        # self.transform = RandomResizedCrop1d(3500, scale=(.8,1.))\n",
        "\n",
        "    #     self.apply(self.init_weights)\n",
        "    # def init_weights(self, m):\n",
        "    #     if isinstance(m, (nn.Linear, nn.Conv1d)):\n",
        "    #         torch.nn.init.normal_(m.weight, std=.02)\n",
        "    #         if m.bias is not None: nn.init.zeros_(m.bias)\n",
        "\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]\n",
        "        batch, seq, dim = x.shape\n",
        "        # print(x.shape)\n",
        "        # target_mask = multiblock(seq//self.patch_size, min_s=.2, max_s=.3, M=4, B=1).any(1).squeeze(1) # best.2.3M4 og.15.2M4# mask out targets to be predicted # [M, seq]\n",
        "        # # target_mask = randpatch(seq//self.patch_size, mask_size=8, gamma=.9).unsqueeze(0) # 8.9 [seq]\n",
        "\n",
        "        # print(target_mask.shape, x.shape)\n",
        "        # context_mask = ~multiblock(seq//self.patch_size, min_s=.85, max_s=1, M=1, B=1).squeeze(1)|target_mask # og .85,1.M1 # [1, seq], True->Mask\n",
        "        # context_mask = torch.zeros((1,seq//self.patch_size), dtype=bool)|target_mask # [1,h,w], True->Mask\n",
        "\n",
        "        # context_indices, trg_indices = simplexmask1d(seq//self.patch_size, ctx_scale=(.85,1), trg_scale=(.7,.8), B=batch, chaos=[3,.5])\n",
        "        # context_indices, trg_indices = simplexmask1d(seq//self.patch_size, ctx_scale=(.8,.9), trg_scale=(.7,.8), B=batch, chaos=[1,.5])\n",
        "        # context_indices, trg_indices = simplexmask1d(seq//self.patch_size, ctx_scale=(.8,1), trg_scale=(.2,.8), B=batch, chaos=[1,.5])\n",
        "        # print(context_indices.shape, trg_indices.shape)\n",
        "\n",
        "        # context_indices = context_indices.repeat(batch,1)\n",
        "        # trg_indices = trg_indices.repeat(batch,1)\n",
        "        # context_mask = ~context_mask|target_mask # [1,]\n",
        "        # context_indices = (~context_mask).nonzero()[:,1].unsqueeze(0).repeat(batch,1)\n",
        "        # # print(trg_indices.shape, context_indices.shape)\n",
        "        # # print(context_mask.shape,target_mask.shape, x.shape)\n",
        "        # target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "        # # # target_mask, context_mask = target_mask.repeat(batch,1), context_mask.repeat(batch,1)\n",
        "        # # x_ = x * F.adaptive_avg_pool1d((~context_mask).float(), x.shape[1]).unsqueeze(-1) # zero masked locations\n",
        "\n",
        "        # context_indices = (~context_mask).nonzero()[:,1].unflatten(0, (batch,-1)) # int idx [num_context_toks] , idx of context not masked\n",
        "        # trg_indices = target_mask.nonzero()[:,1].unflatten(0, (batch,-1)) # int idx [num_trg_toks] , idx of targets that are masked\n",
        "\n",
        "\n",
        "        # mask_collator = MaskCollator(length=seq//self.patch_size, enc_mask_scale=(.85,1), pred_mask_scale=(.15,.2), nenc=1, npred=4, min_keep=4, allow_overlap=False)\n",
        "        mask_collator = MaskCollator(length=seq//self.patch_size, enc_mask_scale=(.85,1), pred_mask_scale=(.2,.25), nenc=1, npred=4, min_keep=4, allow_overlap=False)\n",
        "        collated_masks_enc, collated_masks_pred = mask_collator(batch) # idx of ctx, idx of masked trg\n",
        "        # # collated_masks_enc, collated_masks_pred = mask_collator(1) # idx of ctx, idx of masked trg\n",
        "        context_indices, trg_indices = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "        # # # zero_mask = torch.zeros(batch ,seq//self.patch_size, device=device)\n",
        "        # # # zero_mask[torch.arange(batch).unsqueeze(-1), context_indices] = 1\n",
        "        # # zero_mask = torch.zeros(1 ,seq//self.patch_size, device=device)\n",
        "        # # zero_mask[:, context_indices] = 1\n",
        "        # # x_ = x * F.adaptive_avg_pool1d(zero_mask, x.shape[1]).unsqueeze(-1) # zero masked locations\n",
        "        # # context_indices, trg_indices = context_indices.repeat(batch,1), trg_indices.repeat(batch,1)\n",
        "\n",
        "\n",
        "        # print('x_',x_.shape, context_indices.shape, trg_indices.shape)\n",
        "\n",
        "        sx = self.student(x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "        # print('seq_jepa loss sx',sx.shape)\n",
        "        sy_ = self.predicter(sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "        sy_ = F.layer_norm(sy_, (sy_.size(-1),))\n",
        "        with torch.no_grad():\n",
        "            sy = self.teacher(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "            sy = sy[torch.arange(sy.shape[0]).unsqueeze(-1), trg_indices] # [batch, num_context_toks, d_model] # nan bec len(trg_ind)==0 # print('loss sy',torch.isnan(sy).any())\n",
        "            sy = F.layer_norm(sy, (sy.size(-1),))\n",
        "        loss = F.mse_loss(sy, sy_)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        sx = self.student(x)\n",
        "        out = sx.mean(dim=1)\n",
        "        return out\n",
        "\n",
        "# min_s=0.15, max_s, M\n",
        "# trg.15.2M4 C.85 1\n",
        "\n",
        "# 1e-2,1e-3 < 3e-3,1e-3\n",
        "# patch16 < patch32\n",
        "# NoPE good but sus\n",
        "\n",
        "# ctx/trg sacle min/max, num blk,\n",
        "\n",
        "in_dim = X_train.shape[-1] # 3\n",
        "out_dim = train_data.vocab_size # 16\n",
        "d_model=64\n",
        "# seq_jepa = SeqJEPA(in_dim=in_dim, d_model=d_model, out_dim=None, nlayers=1, n_heads=8).to(device)#.to(torch.float)\n",
        "seq_jepa = SeqJEPA(in_dim=in_dim, d_model=d_model, out_dim=None, nlayers=1, n_heads=8, drop=.1).to(device)#.to(torch.float)\n",
        "optim = torch.optim.AdamW(seq_jepa.parameters(), lr=1e-3) # 1e-3?\n",
        "# optim = torch.optim.AdamW([{'params': seq_jepa.student.parameters()},\n",
        "#     {'params': seq_jepa.predicter.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # default 1e-2, 5e-2\n",
        "    # {'params': seq_jepa.predicter.parameters(), 'lr': 1e-2}], lr=1e-3, weight_decay=1e-2)\n",
        "# import bitsandbytes as bnb\n",
        "# optim = bnb.optim.Lion8bit(seq_jepa.parameters(), lr=1e-3, betas=(0.9, 0.99), weight_decay=1e-2)\n",
        "# optim = bnb.optim.Lion8bit(seq_jepa.parameters(), lr=3e-4, betas=(0.9, 0.99), weight_decay=3e-3)\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/configs/in1k_vith14_ep300.yaml\n",
        "# d_model 1024,384\n",
        "# depth 12,6/12\n",
        "# wd 5e-2 - 4e-1\n",
        "# adamw 1e-4 - 1e-3 - 1e-6\n",
        "# ema 0.996-1\n",
        "\n",
        "print(sum(p.numel() for p in seq_jepa.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.parameters())) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.predicter.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.student.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.teacher.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# d_model^2 * nlayers\n",
        "\n",
        "x = torch.rand((24, 1700, in_dim), device=device)\n",
        "out = seq_jepa.loss(x)\n",
        "print(out.shape)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(d_model, out_dim).to(device)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n",
        "# optim = torch.optim.AdamW([{'params': seq_jepa.parameters()}, {'params': classifier.parameters(), 'lr': 1e-3}], lr=1e-3)\n",
        "# coptim = torch.optim.AdamW(classifier.parameters(), lr=1e-3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## violet vicreg rankme"
      ],
      "metadata": {
        "id": "u99QqyJCqp_P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "m4rj4LfPuN1H"
      },
      "outputs": [],
      "source": [
        "# @title TransformerVICReg\n",
        "import torch\n",
        "from torch import nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class TransformerVICReg(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, n_heads=4, d_hid=None, nlayers=1, drop=0):\n",
        "        super().__init__()\n",
        "        act = nn.GELU()\n",
        "        # patch_size=4\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Linear(in_dim, d_model), act\n",
        "            # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(3,2, 3//2),\n",
        "            nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2),\n",
        "            # nn.Conv1d(in_dim, d_model, patch_size, patch_size), # patch\n",
        "            )\n",
        "        self.pos_enc = RoPE(d_model, seq_len=200, base=10000) # 10000\n",
        "        # self.pos_emb = nn.Parameter(torch.randn(1, 200, d_model))\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=200, base=10000).unsqueeze(0), requires_grad=False)\n",
        "\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, n_heads=n_heads) for _ in range(nlayers)])\n",
        "\n",
        "        # out_dim = out_dim or d_model\n",
        "        self.lin = nn.Linear(d_model, out_dim)\n",
        "        self.attn_pool = nn.Linear(d_model, 1, bias=False)\n",
        "\n",
        "        dim_v = d_model * 4\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(out_dim, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v, bias=False),\n",
        "            )\n",
        "\n",
        "    def forward(self, x): # [b,t,d] / [b,c,h,w]\n",
        "        # x = self.embed(x).flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c]\n",
        "        x = self.embed(x.transpose(-2,-1)).transpose(-2,-1) # [b,t,d]\n",
        "        # x = self.embed(x)\n",
        "        x = self.pos_enc(x)\n",
        "        # x = x + self.pos_emb[:,:x.shape[1]]\n",
        "\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        attn = self.attn_pool(x).squeeze(-1) # [batch, seq] # seq_pool\n",
        "        out = (torch.softmax(attn, dim=-1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, seq] @ [batch, seq, dim] -> [batch, dim]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch, ntoken]\n",
        "\n",
        "    def expand(self, x):\n",
        "        sx = self.forward(x)\n",
        "        vx = self.exp(sx)\n",
        "        return vx\n",
        "\n",
        "batch, seq_len = 4,3500\n",
        "in_dim, d_model, out_dim=16,64,16\n",
        "model = TransformerVICReg(in_dim, d_model, out_dim, n_heads=8, nlayers=1, drop=0.).to(device)\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "# x =  torch.rand((batch, in_dim, 32,32), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObiHp-LSuRBA",
        "outputId": "c6ce8734-e0a0-47b2-b11d-e0f8e53af582"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "in vicreg  1.9939420276162247e-16 24.746832251548767 1.6621681808715039e-09\n",
            "(tensor(7.9758e-18, device='cuda:0', grad_fn=<MseLossBackward0>), tensor(0.9899, device='cuda:0', grad_fn=<AddBackward0>), tensor(1.6622e-09, device='cuda:0', grad_fn=<AddBackward0>))\n"
          ]
        }
      ],
      "source": [
        "# @title Violet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class Violet(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, nlayers=2, n_heads=4):\n",
        "        super().__init__()\n",
        "        out_dim = out_dim or d_model\n",
        "        self.student = TransformerVICReg(in_dim, d_model, out_dim=out_dim, n_heads=n_heads, nlayers=nlayers, drop=0.)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "\n",
        "        # vicreg\n",
        "        self.sim_coeff=25. # 10.0 # 25.0 # λ\n",
        "        self.std_coeff=25. # 1.0 # 25.0 # µ\n",
        "        self.cov_coeff=1. # 25.0 # 1.0 # ν\n",
        "\n",
        "        # self.transform = RandomResizedCrop1d(3500, scale=(.8,1.))\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]c/ [b,c,h,w]\n",
        "        # print(x.shape)\n",
        "        # self.transform(x)\n",
        "        vx = self.student.expand(x) # [batch, num_context_toks, out_dim]\n",
        "        with torch.no_grad(): vy = self.teacher.expand(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "        loss = self.vicreg(vx, vy)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        return self.student(x)\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size, num_features = x.shape\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = self.sim_coeff * repr_loss + self.std_coeff * std_loss + self.cov_coeff * cov_loss\n",
        "        print(\"in vicreg \",self.sim_coeff * repr_loss.item() , self.std_coeff * std_loss.item() , self.cov_coeff * cov_loss.item())\n",
        "        # return loss\n",
        "        return repr_loss, std_loss, cov_loss\n",
        "\n",
        "# lr1e-3\n",
        "# n_heads=8 < 4\n",
        "\n",
        "in_dim=3\n",
        "violet = Violet(in_dim, d_model=64, out_dim=16, nlayers=1, n_heads=8).to(device)\n",
        "voptim = torch.optim.AdamW(violet.parameters(), lr=1e-3) # 1e-3?\n",
        "# voptim = torch.optim.AdamW([{'params': violet.student.transformer.parameters()},\n",
        "#     {'params': violet.student.exp.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # default 1e-2\n",
        "# voptim = torch.optim.AdamW([{'params': violet.student.exp.parameters(), 'lr': 3e-3},\n",
        "#     {'params': [p for n, p in violet.named_parameters() if 'student.exp' not in n]}], lr=1e-3, weight_decay=1e-2) # default 1e-2\n",
        "\n",
        "# print(sum(p.numel() for p in violet.parameters() if p.requires_grad)) # 27584\n",
        "\n",
        "\n",
        "x = torch.rand((2,1000,in_dim), device=device)\n",
        "loss = violet.loss(x)\n",
        "# print(out.shape)\n",
        "print(loss)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(16, 18).to(device) # torch/autograd/graph.py RuntimeError: CUDA error: device-side assert triggered CUDA kernel errors\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IACKDymhit7"
      },
      "outputs": [],
      "source": [
        "# optim.param_groups[0]['lr'] = 1e-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7zghfu_jeOQk"
      },
      "outputs": [],
      "source": [
        "# @title RankMe\n",
        "# RankMe: Assessing the Downstream Performance of Pretrained Self-Supervised Representations by Their Rank jun 2023 https://arxiv.org/pdf/2210.02885\n",
        "import torch\n",
        "\n",
        "# https://github.com/Spidartist/IJEPA_endoscopy/blob/main/src/helper.py#L22\n",
        "def RankMe(Z):\n",
        "    \"\"\"\n",
        "    Calculate the RankMe score (the higher, the better).\n",
        "    RankMe(Z) = exp(-sum_{k=1}^{min(N, K)} p_k * log(p_k)),\n",
        "    where p_k = sigma_k (Z) / ||sigma_k (Z)||_1 + epsilon\n",
        "    where sigma_k is the kth singular value of Z.\n",
        "    where Z is the matrix of embeddings (N × K)\n",
        "    \"\"\"\n",
        "    # compute the singular values of the embeddings\n",
        "    # _u, s, _vh = torch.linalg.svd(Z, full_matrices=False)  # s.shape = (min(N, K),)\n",
        "    # s = torch.linalg.svd(Z, full_matrices=False).S\n",
        "    s = torch.linalg.svdvals(Z)\n",
        "    p = s / torch.sum(s, axis=0) + 1e-7\n",
        "    return torch.exp(-torch.sum(p * torch.log(p)))\n",
        "\n",
        "# Z = torch.randn(5, 3)\n",
        "# o = RankMe(Z)\n",
        "# print(o)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Pj25OrEk14pR"
      },
      "outputs": [],
      "source": [
        "# @title LiDAR stable_ssl next\n",
        "# https://github.com/rbalestr-lab/stable-ssl/blob/main/stable_ssl/monitors.py#L106\n",
        "\n",
        "def LiDAR(embeddings, eps=1e-7, delta=1e-3):\n",
        "    embeddings = embeddings.unflatten(0, (-1,2))\n",
        "    (local_n, q, d), device = embeddings.shape, embeddings.device\n",
        "\n",
        "    class_means = embeddings.mean(dim=1) # mu_x\n",
        "    grand_mean_local = class_means.mean(dim=0) # mu\n",
        "    # print(embeddings.shape, class_means.shape, grand_mean_local.shape) # [50, 2, 64], [50, 64], [64]\n",
        "\n",
        "    # local_Sb = torch.zeros(d, d, device=device)\n",
        "    # local_Sw = torch.zeros(d, d, device=device)\n",
        "\n",
        "\n",
        "    # print(diff_b.shape, diff_w.shape) # [64,1], [64,1]\n",
        "    # print(local_Sb.shape, local_Sw.shape) # [64,64], [64,64]\n",
        "\n",
        "    diff_b = (class_means - grand_mean_local).unsqueeze(-1)\n",
        "    # print(diff_b.shape)\n",
        "    # # local_Sb = (diff_b @ diff_b.T).sum()\n",
        "    local_Sb = (diff_b @ diff_b.transpose(-2,-1)).sum(0)\n",
        "    # # print(embeddings.shape, class_means.shape)\n",
        "    diff_w = (embeddings - class_means.unsqueeze(1)).reshape(-1,d,1)\n",
        "    # # print(diff_w.shape)\n",
        "    # # local_Sw = (diff_w @ diff_w.T).sum()\n",
        "    local_Sw = (diff_w @ diff_w.transpose(-2,-1)).sum(0)\n",
        "\n",
        "    # print(local_Sb.shape, local_Sw.shape)\n",
        "    S_b = local_Sb / (local_n - 1)\n",
        "    S_w = local_Sw / (local_n * (q - 1))\n",
        "    S_w += delta * torch.eye(d, device=device)\n",
        "    # print(S_w.shape, d)\n",
        "\n",
        "    eigvals_w, eigvecs_w = torch.linalg.eigh(S_w)\n",
        "    eigvals_w = torch.clamp(eigvals_w, min=eps)\n",
        "\n",
        "    invsqrt_w = (eigvecs_w * (1.0 / torch.sqrt(eigvals_w))) @ eigvecs_w.transpose(-1, -2)\n",
        "    Sigma_lidar = invsqrt_w @ S_b @ invsqrt_w\n",
        "\n",
        "    # lam, _ = torch.linalg.eigh(Sigma_lidar)\n",
        "    lam = torch.linalg.eigh(Sigma_lidar)[0]\n",
        "    # print(lam)\n",
        "    # lam = torch.clamp(lam, min=0.0)\n",
        "\n",
        "    p = lam / lam.sum() + eps\n",
        "    # print(p)\n",
        "    # p = s / torch.sum(s, axis=0) + 1e-7\n",
        "    return torch.exp(-torch.sum(p * torch.log(p)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## fdb"
      ],
      "metadata": {
        "id": "MN5sSWa-qonm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2Nd-sGe6Ku4S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "outputId": "9994a3e7-2e9e-440e-b412-f4585a8ebef2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>▅▅▅▅▅▅▅▄▄▅▄▄▄▄▄▄▄▄▄▃▄▅▄▄▃▄▄▅▃▄▃▅▁▄▄▄▄▄▄█</td></tr><tr><td>correct</td><td>▁▂▂▂▂▄▂▃▄▃▃█▄▂▂▂▃▄▂▃▃▃▃▂▂▃▃▃▃▃▃▃▂▃▂▁▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>1.22344</td></tr><tr><td>correct</td><td>0.28571</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">jumping-blaze-7</strong> at: <a href='https://wandb.ai/bobdole/ucr/runs/fb1gccci' target=\"_blank\">https://wandb.ai/bobdole/ucr/runs/fb1gccci</a><br> View project at: <a href='https://wandb.ai/bobdole/ucr' target=\"_blank\">https://wandb.ai/bobdole/ucr</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250529_094156-fb1gccci/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250529_100200-3nwmfi97</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/ucr/runs/3nwmfi97' target=\"_blank\">twilight-shadow-8</a></strong> to <a href='https://wandb.ai/bobdole/ucr' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/ucr' target=\"_blank\">https://wandb.ai/bobdole/ucr</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/ucr/runs/3nwmfi97' target=\"_blank\">https://wandb.ai/bobdole/ucr/runs/3nwmfi97</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "!pip install -q wandb\n",
        "import wandb # https://docs.wandb.ai/quickstart\n",
        "wandb.login(key='487a2109e55dce4e13fc70681781de9f50f27be7')\n",
        "try: run.finish()\n",
        "except NameError: pass\n",
        "run = wandb.init(project=\"ucr\", config={\"model\": \"res18\",}) # violet SeqJEPA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-eTjgAhmp_t",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b04ab03-d2db-4c64-942b-737ab5ddad82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "0.21875\n",
            "0.3125\n",
            "0.3125\n",
            "0.375\n",
            "0.2857142857142857\n",
            "strain 0.20989978313446045\n",
            "strain 0.179549902677536\n",
            "strain 0.15811152756214142\n",
            "strain 0.25306791067123413\n",
            "strain 0.1468600630760193\n",
            "classify 1.350830078125\n",
            "classify 1.376953125\n",
            "classify 1.34326171875\n",
            "classify 1.3590087890625\n",
            "classify 1.3562500476837158\n",
            "0.265625\n",
            "0.328125\n",
            "0.359375\n",
            "0.28125\n",
            "0.14285714285714285\n",
            "strain 0.18901881575584412\n",
            "strain 0.22099332511425018\n",
            "strain 0.2300403118133545\n",
            "strain 0.192032128572464\n",
            "strain 0.26415395736694336\n",
            "classify 1.3546142578125\n",
            "classify 1.37060546875\n",
            "classify 1.3438720703125\n",
            "classify 1.36279296875\n",
            "classify 1.3250000476837158\n",
            "0.3125\n",
            "0.34375\n",
            "0.3125\n",
            "0.28125\n",
            "0.42857142857142855\n",
            "strain 0.17512769997119904\n",
            "strain 0.18137767910957336\n",
            "strain 0.12477006018161774\n",
            "strain 0.14279912412166595\n",
            "strain 0.04268673062324524\n",
            "classify 1.376708984375\n",
            "classify 1.3485107421875\n",
            "classify 1.37353515625\n",
            "classify 1.3338623046875\n",
            "classify 1.3406250476837158\n",
            "0.28125\n",
            "0.296875\n",
            "0.359375\n",
            "0.265625\n",
            "0.14285714285714285\n",
            "strain 0.14750537276268005\n",
            "strain 0.25808313488960266\n",
            "strain 0.25257760286331177\n",
            "strain 0.10336139053106308\n",
            "strain 0.056495167315006256\n",
            "classify 1.370849609375\n",
            "classify 1.3555908203125\n",
            "classify 1.3580322265625\n",
            "classify 1.356689453125\n",
            "classify 1.1984374523162842\n",
            "0.3125\n",
            "0.359375\n",
            "0.265625\n",
            "0.34375\n",
            "0.2857142857142857\n",
            "strain 0.1672694981098175\n",
            "strain 0.155173197388649\n",
            "strain 0.19019721448421478\n",
            "strain 0.2796412706375122\n",
            "strain 0.042058657854795456\n",
            "classify 1.3740234375\n",
            "classify 1.3448486328125\n",
            "classify 1.355224609375\n",
            "classify 1.3494873046875\n",
            "classify 1.357812523841858\n",
            "0.296875\n",
            "0.34375\n",
            "0.375\n",
            "0.28125\n",
            "0.2857142857142857\n",
            "strain 0.20211122930049896\n",
            "strain 0.16326174139976501\n",
            "strain 0.11791476607322693\n",
            "strain 0.19049131870269775\n",
            "strain 0.18789632618427277\n",
            "classify 1.323974609375\n",
            "classify 1.37353515625\n",
            "classify 1.3446044921875\n",
            "classify 1.3812255859375\n",
            "classify 1.4921875\n",
            "0.296875\n",
            "0.28125\n",
            "0.28125\n",
            "0.28125\n",
            "0.5714285714285714\n",
            "strain 0.15414616465568542\n",
            "strain 0.1697516292333603\n",
            "strain 0.14382335543632507\n",
            "strain 0.20715895295143127\n",
            "strain 0.17637784779071808\n",
            "classify 1.365234375\n",
            "classify 1.3267822265625\n",
            "classify 1.3524169921875\n",
            "classify 1.3826904296875\n",
            "classify 1.376562476158142\n",
            "0.25\n",
            "0.28125\n",
            "0.359375\n",
            "0.359375\n",
            "0.42857142857142855\n",
            "strain 0.2029394954442978\n",
            "strain 0.20472049713134766\n",
            "strain 0.1857082098722458\n",
            "strain 0.22384369373321533\n",
            "strain 0.043533533811569214\n",
            "classify 1.326416015625\n",
            "classify 1.332763671875\n",
            "classify 1.4034423828125\n",
            "classify 1.3704833984375\n",
            "classify 1.3468749523162842\n",
            "0.28125\n",
            "0.359375\n",
            "0.296875\n",
            "0.3125\n",
            "0.42857142857142855\n",
            "strain 0.1637028306722641\n",
            "strain 0.08668369054794312\n",
            "strain 0.25207728147506714\n",
            "strain 0.15745575726032257\n",
            "strain 0.2718733251094818\n",
            "classify 1.398681640625\n",
            "classify 1.336669921875\n",
            "classify 1.3218994140625\n",
            "classify 1.3946533203125\n",
            "classify 1.3078124523162842\n",
            "0.28125\n",
            "0.296875\n",
            "0.3125\n",
            "0.3125\n",
            "0.14285714285714285\n",
            "strain 0.1812104433774948\n",
            "strain 0.1735752522945404\n",
            "strain 0.2126520723104477\n",
            "strain 0.2211710512638092\n",
            "strain 0.07970685511827469\n",
            "classify 1.3280029296875\n",
            "classify 1.3758544921875\n",
            "classify 1.37060546875\n",
            "classify 1.3626708984375\n",
            "classify 1.4249999523162842\n",
            "0.3125\n",
            "0.390625\n",
            "0.328125\n",
            "0.234375\n",
            "0.14285714285714285\n",
            "strain 0.19040022790431976\n",
            "strain 0.1804654598236084\n",
            "strain 0.22050578892230988\n",
            "strain 0.2465486079454422\n",
            "strain 0.06057049706578255\n",
            "classify 1.3662109375\n",
            "classify 1.343994140625\n",
            "classify 1.3740234375\n",
            "classify 1.3448486328125\n",
            "classify 1.3671875\n",
            "0.234375\n",
            "0.34375\n",
            "0.375\n",
            "0.3125\n",
            "0.5714285714285714\n",
            "strain 0.20314742624759674\n",
            "strain 0.15021854639053345\n",
            "strain 0.13885746896266937\n",
            "strain 0.2125692218542099\n",
            "strain 0.07592357695102692\n",
            "classify 1.3634033203125\n",
            "classify 1.344970703125\n",
            "classify 1.3817138671875\n",
            "classify 1.345703125\n",
            "classify 1.274999976158142\n",
            "0.3125\n",
            "0.28125\n",
            "0.3125\n",
            "0.40625\n",
            "0.14285714285714285\n",
            "strain 0.19320163130760193\n",
            "strain 0.22181397676467896\n",
            "strain 0.23086822032928467\n",
            "strain 0.18114688992500305\n",
            "strain 0.05429873615503311\n",
            "classify 1.3463134765625\n",
            "classify 1.36572265625\n",
            "classify 1.3609619140625\n",
            "classify 1.349853515625\n",
            "classify 1.4109375476837158\n",
            "0.375\n",
            "0.265625\n",
            "0.359375\n",
            "0.3125\n",
            "0.2857142857142857\n",
            "strain 0.15307842195034027\n",
            "strain 0.24458415806293488\n",
            "strain 0.18041862547397614\n",
            "strain 0.11340206861495972\n",
            "strain 0.05273749306797981\n",
            "classify 1.366943359375\n",
            "classify 1.37744140625\n",
            "classify 1.354736328125\n",
            "classify 1.3297119140625\n",
            "classify 1.357812523841858\n",
            "0.34375\n",
            "0.3125\n",
            "0.3125\n",
            "0.3125\n",
            "0.42857142857142855\n",
            "strain 0.17760151624679565\n",
            "strain 0.23814289271831512\n",
            "strain 0.22852517664432526\n",
            "strain 0.15641039609909058\n",
            "strain 0.4189419448375702\n",
            "classify 1.3746337890625\n",
            "classify 1.3785400390625\n",
            "classify 1.3438720703125\n",
            "classify 1.343017578125\n",
            "classify 1.264062523841858\n",
            "0.28125\n",
            "0.40625\n",
            "0.25\n",
            "0.296875\n",
            "0.2857142857142857\n",
            "strain 0.17270122468471527\n",
            "strain 0.15826000273227692\n",
            "strain 0.2229882776737213\n",
            "strain 0.2204452008008957\n",
            "strain 0.12366659194231033\n",
            "classify 1.3555908203125\n",
            "classify 1.37548828125\n",
            "classify 1.3492431640625\n",
            "classify 1.37060546875\n",
            "classify 1.303125023841858\n",
            "0.375\n",
            "0.28125\n",
            "0.265625\n",
            "0.296875\n",
            "0.2857142857142857\n",
            "strain 0.21622778475284576\n",
            "strain 0.2225475013256073\n",
            "strain 0.21207740902900696\n",
            "strain 0.17483767867088318\n",
            "strain 0.06232432276010513\n",
            "classify 1.3511962890625\n",
            "classify 1.3387451171875\n",
            "classify 1.34619140625\n",
            "classify 1.419677734375\n",
            "classify 1.2937500476837158\n",
            "0.28125\n",
            "0.265625\n",
            "0.359375\n",
            "0.328125\n",
            "0.0\n",
            "strain 0.259977787733078\n",
            "strain 0.1579553484916687\n",
            "strain 0.15444868803024292\n",
            "strain 0.18196415901184082\n",
            "strain 0.17776958644390106\n",
            "classify 1.351806640625\n",
            "classify 1.4093017578125\n",
            "classify 1.3447265625\n",
            "classify 1.355224609375\n",
            "classify 1.259374976158142\n",
            "0.375\n",
            "0.265625\n",
            "0.28125\n",
            "0.28125\n",
            "0.14285714285714285\n",
            "strain 0.18426957726478577\n",
            "strain 0.27402693033218384\n",
            "strain 0.27604395151138306\n",
            "strain 0.13546806573867798\n",
            "strain 0.14176109433174133\n",
            "classify 1.349609375\n",
            "classify 1.35693359375\n",
            "classify 1.37841796875\n",
            "classify 1.37548828125\n",
            "classify 1.365625023841858\n",
            "0.265625\n",
            "0.390625\n",
            "0.296875\n",
            "0.28125\n",
            "0.42857142857142855\n",
            "strain 0.20679619908332825\n",
            "strain 0.1805068701505661\n",
            "strain 0.19138674437999725\n",
            "strain 0.17989076673984528\n",
            "strain 0.13429009914398193\n",
            "classify 1.342529296875\n",
            "classify 1.385009765625\n",
            "classify 1.39501953125\n",
            "classify 1.336669921875\n",
            "classify 1.357812523841858\n",
            "0.25\n",
            "0.3125\n",
            "0.359375\n",
            "0.34375\n",
            "0.0\n",
            "strain 0.18603038787841797\n",
            "strain 0.12911635637283325\n",
            "strain 0.19285960495471954\n",
            "strain 0.21341948211193085\n",
            "strain 0.11386822909116745\n",
            "classify 1.3408203125\n",
            "classify 1.3681640625\n",
            "classify 1.36572265625\n",
            "classify 1.368896484375\n",
            "classify 1.46875\n",
            "0.34375\n",
            "0.25\n",
            "0.328125\n",
            "0.328125\n",
            "0.2857142857142857\n",
            "strain 0.18765461444854736\n",
            "strain 0.16611754894256592\n",
            "strain 0.14387163519859314\n",
            "strain 0.2392452359199524\n",
            "strain 0.2268354594707489\n",
            "classify 1.3441162109375\n",
            "classify 1.373046875\n",
            "classify 1.36767578125\n",
            "classify 1.3519287109375\n",
            "classify 1.4718749523162842\n",
            "0.328125\n",
            "0.3125\n",
            "0.25\n",
            "0.296875\n",
            "0.7142857142857143\n",
            "strain 0.2079111933708191\n",
            "strain 0.16296106576919556\n",
            "strain 0.21383482217788696\n",
            "strain 0.2048645317554474\n",
            "strain 0.16237549483776093\n",
            "classify 1.3466796875\n",
            "classify 1.37744140625\n",
            "classify 1.36572265625\n",
            "classify 1.359619140625\n",
            "classify 1.345312476158142\n",
            "0.3125\n",
            "0.3125\n",
            "0.28125\n",
            "0.328125\n",
            "0.2857142857142857\n",
            "strain 0.15303097665309906\n",
            "strain 0.2236470729112625\n",
            "strain 0.20756374299526215\n",
            "strain 0.15717753767967224\n",
            "strain 0.1503409445285797\n",
            "classify 1.3719482421875\n",
            "classify 1.353271484375\n",
            "classify 1.355224609375\n",
            "classify 1.3782958984375\n",
            "classify 1.4015624523162842\n",
            "0.265625\n",
            "0.296875\n",
            "0.421875\n",
            "0.328125\n",
            "0.42857142857142855\n",
            "strain 0.21515142917633057\n",
            "strain 0.19236327707767487\n",
            "strain 0.22528669238090515\n",
            "strain 0.19550155103206635\n",
            "strain 0.08023618906736374\n",
            "classify 1.3687744140625\n",
            "classify 1.363037109375\n",
            "classify 1.383056640625\n",
            "classify 1.3555908203125\n",
            "classify 1.2890625\n",
            "0.34375\n",
            "0.359375\n",
            "0.234375\n",
            "0.359375\n",
            "0.42857142857142855\n",
            "strain 0.10595785826444626\n",
            "strain 0.1988173872232437\n",
            "strain 0.18663419783115387\n",
            "strain 0.1513553410768509\n",
            "strain 0.17755691707134247\n",
            "classify 1.361572265625\n",
            "classify 1.34619140625\n",
            "classify 1.37939453125\n",
            "classify 1.3740234375\n",
            "classify 1.337499976158142\n",
            "0.28125\n",
            "0.28125\n",
            "0.328125\n",
            "0.328125\n",
            "0.5714285714285714\n",
            "strain 0.20348986983299255\n",
            "strain 0.23941470682621002\n",
            "strain 0.2520572543144226\n",
            "strain 0.1427769511938095\n",
            "strain 0.0660601258277893\n",
            "classify 1.387451171875\n",
            "classify 1.369873046875\n",
            "classify 1.3577880859375\n",
            "classify 1.3343505859375\n",
            "classify 1.4968750476837158\n",
            "0.390625\n",
            "0.25\n",
            "0.328125\n",
            "0.234375\n",
            "0.42857142857142855\n",
            "strain 0.20055027306079865\n",
            "strain 0.27954795956611633\n",
            "strain 0.13872915506362915\n",
            "strain 0.14783571660518646\n",
            "strain 0.04392557591199875\n",
            "classify 1.3592529296875\n",
            "classify 1.3787841796875\n",
            "classify 1.3690185546875\n",
            "classify 1.35595703125\n",
            "classify 1.3203125\n",
            "0.296875\n",
            "0.421875\n",
            "0.234375\n",
            "0.296875\n",
            "0.14285714285714285\n",
            "strain 0.20413760840892792\n",
            "strain 0.21412968635559082\n",
            "strain 0.17119474709033966\n",
            "strain 0.168409526348114\n",
            "strain 0.037440139800310135\n",
            "classify 1.3704833984375\n",
            "classify 1.3665771484375\n",
            "classify 1.387451171875\n",
            "classify 1.333984375\n",
            "classify 1.321874976158142\n",
            "0.375\n",
            "0.28125\n",
            "0.28125\n",
            "0.375\n",
            "0.0\n",
            "strain 0.2105577290058136\n",
            "strain 0.18881699442863464\n",
            "strain 0.19201482832431793\n",
            "strain 0.1888418048620224\n",
            "strain 0.06456359475851059\n",
            "classify 1.375244140625\n",
            "classify 1.3570556640625\n",
            "classify 1.36328125\n",
            "classify 1.357666015625\n",
            "classify 1.368749976158142\n",
            "0.390625\n",
            "0.34375\n",
            "0.234375\n",
            "0.3125\n",
            "0.0\n",
            "strain 0.22295500338077545\n",
            "strain 0.13538934290409088\n",
            "strain 0.16740386188030243\n",
            "strain 0.18660534918308258\n",
            "strain 0.04915798828005791\n",
            "classify 1.3829345703125\n",
            "classify 1.358154296875\n",
            "classify 1.366455078125\n",
            "classify 1.3492431640625\n",
            "classify 1.34375\n",
            "0.203125\n",
            "0.328125\n",
            "0.359375\n",
            "0.3125\n",
            "0.14285714285714285\n",
            "strain 0.270389586687088\n",
            "strain 0.23287679255008698\n",
            "strain 0.2714777886867523\n",
            "strain 0.17729389667510986\n",
            "strain 0.05143638700246811\n",
            "classify 1.385498046875\n",
            "classify 1.378662109375\n",
            "classify 1.3389892578125\n",
            "classify 1.3427734375\n",
            "classify 1.375\n",
            "0.296875\n",
            "0.375\n",
            "0.34375\n",
            "0.265625\n",
            "0.42857142857142855\n",
            "strain 0.1701280176639557\n",
            "strain 0.1578705608844757\n",
            "strain 0.1559121459722519\n",
            "strain 0.08199121803045273\n",
            "strain 0.0510353185236454\n",
            "classify 1.3653564453125\n",
            "classify 1.3602294921875\n",
            "classify 1.36328125\n",
            "classify 1.355712890625\n",
            "classify 1.342187523841858\n",
            "0.28125\n",
            "0.328125\n",
            "0.328125\n",
            "0.34375\n",
            "0.14285714285714285\n",
            "strain 0.092779241502285\n",
            "strain 0.22268687188625336\n",
            "strain 0.10627185553312302\n",
            "strain 0.2836746573448181\n",
            "strain 0.0728682354092598\n",
            "classify 1.3154296875\n",
            "classify 1.3905029296875\n",
            "classify 1.354736328125\n",
            "classify 1.38330078125\n",
            "classify 1.337499976158142\n",
            "0.40625\n",
            "0.28125\n",
            "0.296875\n",
            "0.296875\n",
            "0.14285714285714285\n",
            "strain 0.2974948287010193\n",
            "strain 0.15281589329242706\n",
            "strain 0.18993467092514038\n",
            "strain 0.2132961004972458\n",
            "strain 0.0674520954489708\n",
            "classify 1.3580322265625\n",
            "classify 1.3619384765625\n",
            "classify 1.38818359375\n",
            "classify 1.33544921875\n",
            "classify 1.373437523841858\n",
            "0.203125\n",
            "0.34375\n",
            "0.34375\n",
            "0.375\n",
            "0.42857142857142855\n",
            "strain 0.25808149576187134\n",
            "strain 0.21855174005031586\n",
            "strain 0.12205950170755386\n",
            "strain 0.08268245309591293\n",
            "strain 0.03643745929002762\n",
            "classify 1.3466796875\n",
            "classify 1.38232421875\n",
            "classify 1.3433837890625\n",
            "classify 1.35693359375\n",
            "classify 1.5578124523162842\n",
            "0.375\n",
            "0.28125\n",
            "0.296875\n",
            "0.296875\n",
            "0.42857142857142855\n",
            "strain 0.18939542770385742\n",
            "strain 0.10669997334480286\n",
            "strain 0.20611663162708282\n",
            "strain 0.17675632238388062\n",
            "strain 0.04190937802195549\n",
            "classify 1.3759765625\n",
            "classify 1.366943359375\n",
            "classify 1.35791015625\n",
            "classify 1.341064453125\n",
            "classify 1.3718750476837158\n",
            "0.328125\n",
            "0.3125\n",
            "0.265625\n",
            "0.34375\n",
            "0.2857142857142857\n",
            "strain 0.23644717037677765\n",
            "strain 0.13898998498916626\n",
            "strain 0.15795311331748962\n",
            "strain 0.24831606447696686\n",
            "strain 0.04539163038134575\n",
            "classify 1.3475341796875\n",
            "classify 1.3646240234375\n",
            "classify 1.36767578125\n",
            "classify 1.34521484375\n",
            "classify 1.501562476158142\n",
            "0.25\n",
            "0.375\n",
            "0.359375\n",
            "0.28125\n",
            "0.2857142857142857\n",
            "strain 0.24288414418697357\n",
            "strain 0.22238299250602722\n",
            "strain 0.23687487840652466\n",
            "strain 0.16919580101966858\n",
            "strain 0.14898033440113068\n",
            "classify 1.3345947265625\n",
            "classify 1.3541259765625\n",
            "classify 1.3681640625\n",
            "classify 1.381103515625\n",
            "classify 1.329687476158142\n",
            "0.25\n",
            "0.34375\n",
            "0.28125\n",
            "0.34375\n",
            "0.14285714285714285\n",
            "strain 0.15691861510276794\n",
            "strain 0.33185380697250366\n",
            "strain 0.19102269411087036\n",
            "strain 0.14837656915187836\n",
            "strain 0.08049426227807999\n",
            "classify 1.36572265625\n",
            "classify 1.3681640625\n",
            "classify 1.3603515625\n",
            "classify 1.357177734375\n",
            "classify 1.2765624523162842\n",
            "0.359375\n",
            "0.25\n",
            "0.28125\n",
            "0.328125\n",
            "0.2857142857142857\n",
            "strain 0.2525225281715393\n",
            "strain 0.1921614408493042\n",
            "strain 0.18986447155475616\n",
            "strain 0.18403127789497375\n",
            "strain 0.23673301935195923\n",
            "classify 1.38134765625\n",
            "classify 1.345703125\n",
            "classify 1.385498046875\n",
            "classify 1.3267822265625\n",
            "classify 1.439062476158142\n",
            "0.203125\n",
            "0.3125\n",
            "0.328125\n",
            "0.3125\n",
            "0.14285714285714285\n",
            "strain 0.28231388330459595\n",
            "strain 0.14405764639377594\n",
            "strain 0.2174377143383026\n",
            "strain 0.17179855704307556\n",
            "strain 0.08539603650569916\n",
            "classify 1.3349609375\n",
            "classify 1.3743896484375\n",
            "classify 1.354248046875\n",
            "classify 1.365234375\n",
            "classify 1.475000023841858\n",
            "0.265625\n",
            "0.328125\n",
            "0.21875\n",
            "0.296875\n",
            "0.42857142857142855\n",
            "strain 0.22481052577495575\n",
            "strain 0.3582836389541626\n",
            "strain 0.16097858548164368\n",
            "strain 0.14723721146583557\n",
            "strain 0.13677100837230682\n",
            "classify 1.33740234375\n",
            "classify 1.3553466796875\n",
            "classify 1.3638916015625\n",
            "classify 1.3736572265625\n",
            "classify 1.399999976158142\n",
            "0.265625\n",
            "0.28125\n",
            "0.28125\n",
            "0.328125\n",
            "0.2857142857142857\n",
            "strain 0.0938991904258728\n",
            "strain 0.21585778892040253\n",
            "strain 0.2103024274110794\n",
            "strain 0.11859627813100815\n",
            "strain 0.1728203445672989\n",
            "classify 1.367431640625\n",
            "classify 1.34423828125\n",
            "classify 1.3612060546875\n",
            "classify 1.35498046875\n",
            "classify 1.4328124523162842\n",
            "0.28125\n",
            "0.21875\n",
            "0.296875\n",
            "0.328125\n",
            "0.2857142857142857\n",
            "strain 0.13516604900360107\n",
            "strain 0.15852989256381989\n",
            "strain 0.2687077224254608\n",
            "strain 0.11937334388494492\n",
            "strain 0.11589568108320236\n",
            "classify 1.359619140625\n",
            "classify 1.340087890625\n",
            "classify 1.3377685546875\n",
            "classify 1.390625\n",
            "classify 1.4609375\n",
            "0.390625\n",
            "0.265625\n",
            "0.28125\n",
            "0.265625\n",
            "0.2857142857142857\n",
            "strain 0.1951359659433365\n",
            "strain 0.20720519125461578\n",
            "strain 0.23130398988723755\n",
            "strain 0.22527702152729034\n",
            "strain 0.0796927958726883\n",
            "classify 1.378662109375\n",
            "classify 1.362548828125\n",
            "classify 1.3499755859375\n",
            "classify 1.3521728515625\n",
            "classify 1.342187523841858\n",
            "0.359375\n",
            "0.28125\n",
            "0.234375\n",
            "0.296875\n",
            "0.14285714285714285\n",
            "strain 0.1092822402715683\n",
            "strain 0.09385877847671509\n",
            "strain 0.230918750166893\n",
            "strain 0.19010111689567566\n",
            "strain 0.25535309314727783\n",
            "classify 1.378662109375\n",
            "classify 1.3409423828125\n",
            "classify 1.3592529296875\n",
            "classify 1.3433837890625\n",
            "classify 1.5031249523162842\n",
            "0.328125\n",
            "0.34375\n",
            "0.28125\n",
            "0.234375\n",
            "0.2857142857142857\n",
            "strain 0.15626853704452515\n",
            "strain 0.19259826838970184\n",
            "strain 0.2207193523645401\n",
            "strain 0.1304919272661209\n",
            "strain 0.18516336381435394\n",
            "classify 1.343994140625\n",
            "classify 1.364990234375\n",
            "classify 1.32421875\n",
            "classify 1.4027099609375\n",
            "classify 1.3203125\n",
            "0.28125\n",
            "0.296875\n",
            "0.234375\n",
            "0.390625\n",
            "0.5714285714285714\n",
            "strain 0.20179742574691772\n",
            "strain 0.239643856883049\n",
            "strain 0.15172062814235687\n",
            "strain 0.172830268740654\n",
            "strain 0.09029533714056015\n",
            "classify 1.3787841796875\n",
            "classify 1.3369140625\n",
            "classify 1.344970703125\n",
            "classify 1.372314453125\n",
            "classify 1.5390625\n",
            "0.234375\n",
            "0.25\n",
            "0.390625\n",
            "0.328125\n",
            "0.2857142857142857\n",
            "strain 0.2728566825389862\n",
            "strain 0.2860090136528015\n",
            "strain 0.18060076236724854\n",
            "strain 0.25072699785232544\n",
            "strain 0.2099994719028473\n",
            "classify 1.3612060546875\n",
            "classify 1.3671875\n",
            "classify 1.375732421875\n",
            "classify 1.3365478515625\n",
            "classify 1.4640624523162842\n",
            "0.203125\n",
            "0.265625\n",
            "0.25\n",
            "0.390625\n",
            "0.42857142857142855\n",
            "strain 0.2372499704360962\n",
            "strain 0.2209974080324173\n",
            "strain 0.1674787402153015\n",
            "strain 0.15331993997097015\n",
            "strain 0.06590157002210617\n",
            "classify 1.3636474609375\n",
            "classify 1.34765625\n",
            "classify 1.3634033203125\n",
            "classify 1.3682861328125\n",
            "classify 1.420312523841858\n",
            "0.28125\n",
            "0.28125\n",
            "0.234375\n",
            "0.3125\n",
            "0.14285714285714285\n",
            "strain 0.14897601306438446\n",
            "strain 0.21054837107658386\n",
            "strain 0.1795920878648758\n",
            "strain 0.19179153442382812\n",
            "strain 0.10239983350038528\n",
            "classify 1.3388671875\n",
            "classify 1.3619384765625\n",
            "classify 1.3704833984375\n",
            "classify 1.3690185546875\n",
            "classify 1.296875\n",
            "0.296875\n",
            "0.21875\n",
            "0.359375\n",
            "0.234375\n",
            "0.2857142857142857\n",
            "strain 0.20731669664382935\n",
            "strain 0.21317365765571594\n",
            "strain 0.2820955514907837\n",
            "strain 0.13338851928710938\n",
            "strain 0.11979374289512634\n",
            "classify 1.3507080078125\n",
            "classify 1.347412109375\n",
            "classify 1.3583984375\n",
            "classify 1.385498046875\n",
            "classify 1.295312523841858\n",
            "0.390625\n",
            "0.296875\n",
            "0.34375\n",
            "0.28125\n",
            "0.14285714285714285\n",
            "strain 0.15314145386219025\n",
            "strain 0.29729780554771423\n",
            "strain 0.19259460270404816\n",
            "strain 0.16411106288433075\n",
            "strain 0.03751147538423538\n",
            "classify 1.3564453125\n",
            "classify 1.3759765625\n",
            "classify 1.35791015625\n",
            "classify 1.3441162109375\n",
            "classify 1.373437523841858\n",
            "0.28125\n",
            "0.296875\n",
            "0.28125\n",
            "0.3125\n",
            "0.14285714285714285\n",
            "strain 0.13052336871623993\n",
            "strain 0.15157361328601837\n",
            "strain 0.2908693253993988\n",
            "strain 0.16992220282554626\n",
            "strain 0.06383373588323593\n",
            "classify 1.3455810546875\n",
            "classify 1.3746337890625\n",
            "classify 1.34765625\n",
            "classify 1.3619384765625\n",
            "classify 1.5\n",
            "0.3125\n",
            "0.359375\n",
            "0.21875\n",
            "0.25\n",
            "0.14285714285714285\n",
            "strain 0.24876169860363007\n",
            "strain 0.21367251873016357\n",
            "strain 0.13390883803367615\n",
            "strain 0.24738596379756927\n",
            "strain 0.04918676242232323\n",
            "classify 1.3406982421875\n",
            "classify 1.3472900390625\n",
            "classify 1.3709716796875\n",
            "classify 1.3751220703125\n",
            "classify 1.3468749523162842\n",
            "0.328125\n",
            "0.3125\n",
            "0.3125\n",
            "0.203125\n",
            "0.14285714285714285\n",
            "strain 0.12412191927433014\n",
            "strain 0.1790577620267868\n",
            "strain 0.17502859234809875\n",
            "strain 0.1631697416305542\n",
            "strain 0.059971146285533905\n",
            "classify 1.36376953125\n",
            "classify 1.36669921875\n",
            "classify 1.35107421875\n",
            "classify 1.34765625\n",
            "classify 1.3562500476837158\n",
            "0.28125\n",
            "0.359375\n",
            "0.1875\n",
            "0.3125\n",
            "0.42857142857142855\n",
            "strain 0.11540542542934418\n",
            "strain 0.1421969085931778\n",
            "strain 0.14831899106502533\n",
            "strain 0.16720128059387207\n",
            "strain 0.05155434459447861\n",
            "classify 1.3582763671875\n",
            "classify 1.3697509765625\n",
            "classify 1.359619140625\n",
            "classify 1.3408203125\n",
            "classify 1.359375\n",
            "0.265625\n",
            "0.34375\n",
            "0.171875\n",
            "0.359375\n",
            "0.14285714285714285\n",
            "strain 0.1503489762544632\n",
            "strain 0.219350203871727\n",
            "strain 0.15947562456130981\n",
            "strain 0.1770097315311432\n",
            "strain 0.05237794294953346\n",
            "classify 1.3621826171875\n",
            "classify 1.3433837890625\n",
            "classify 1.3870849609375\n",
            "classify 1.3448486328125\n",
            "classify 1.3468749523162842\n",
            "0.359375\n",
            "0.28125\n",
            "0.25\n",
            "0.265625\n",
            "0.14285714285714285\n",
            "strain 0.19274701178073883\n",
            "strain 0.2899513244628906\n",
            "strain 0.17005029320716858\n",
            "strain 0.23888908326625824\n",
            "strain 0.05320097878575325\n",
            "classify 1.3895263671875\n",
            "classify 1.344970703125\n",
            "classify 1.341064453125\n",
            "classify 1.3648681640625\n",
            "classify 1.314062476158142\n",
            "0.3125\n",
            "0.328125\n",
            "0.25\n",
            "0.296875\n",
            "0.42857142857142855\n",
            "strain 0.13971079885959625\n",
            "strain 0.10820061713457108\n",
            "strain 0.15385515987873077\n",
            "strain 0.19814878702163696\n",
            "strain 0.21955257654190063\n",
            "classify 1.3345947265625\n",
            "classify 1.375244140625\n",
            "classify 1.3538818359375\n",
            "classify 1.35400390625\n",
            "classify 1.4562499523162842\n",
            "0.3125\n",
            "0.234375\n",
            "0.28125\n",
            "0.28125\n",
            "0.2857142857142857\n",
            "strain 0.1963118314743042\n",
            "strain 0.17679260671138763\n",
            "strain 0.16766826808452606\n",
            "strain 0.15676622092723846\n",
            "strain 0.06593737751245499\n",
            "classify 1.3489990234375\n",
            "classify 1.33349609375\n",
            "classify 1.332275390625\n",
            "classify 1.403564453125\n",
            "classify 1.3671875\n",
            "0.203125\n",
            "0.34375\n",
            "0.3125\n",
            "0.265625\n",
            "0.14285714285714285\n",
            "strain 0.10466982424259186\n",
            "strain 0.14692051708698273\n",
            "strain 0.23660291731357574\n",
            "strain 0.18222345411777496\n",
            "strain 0.31227827072143555\n",
            "classify 1.353271484375\n",
            "classify 1.343994140625\n",
            "classify 1.3863525390625\n",
            "classify 1.3372802734375\n",
            "classify 1.3250000476837158\n",
            "0.25\n",
            "0.28125\n",
            "0.265625\n",
            "0.328125\n",
            "0.2857142857142857\n",
            "strain 0.1579955816268921\n",
            "strain 0.1165902242064476\n",
            "strain 0.18886905908584595\n",
            "strain 0.18550114333629608\n",
            "strain 0.27802574634552\n",
            "classify 1.34521484375\n",
            "classify 1.372314453125\n",
            "classify 1.377197265625\n",
            "classify 1.3226318359375\n",
            "classify 1.3953125476837158\n",
            "0.28125\n",
            "0.265625\n",
            "0.28125\n",
            "0.296875\n",
            "0.42857142857142855\n",
            "strain 0.1855212152004242\n",
            "strain 0.1552180051803589\n",
            "strain 0.15106895565986633\n",
            "strain 0.20068775117397308\n",
            "strain 0.1528996229171753\n",
            "classify 1.384765625\n",
            "classify 1.358642578125\n",
            "classify 1.3221435546875\n",
            "classify 1.356689453125\n",
            "classify 1.3624999523162842\n",
            "0.296875\n",
            "0.375\n",
            "0.25\n",
            "0.296875\n",
            "0.14285714285714285\n",
            "strain 0.12152595072984695\n",
            "strain 0.23987169563770294\n",
            "strain 0.1759105622768402\n",
            "strain 0.18601201474666595\n",
            "strain 0.11782501637935638\n",
            "classify 1.3553466796875\n",
            "classify 1.3336181640625\n",
            "classify 1.367431640625\n",
            "classify 1.3673095703125\n",
            "classify 1.334375023841858\n",
            "0.1875\n",
            "0.390625\n",
            "0.28125\n",
            "0.28125\n",
            "0.2857142857142857\n",
            "strain 0.25414273142814636\n",
            "strain 0.1752595603466034\n",
            "strain 0.17548060417175293\n",
            "strain 0.13260824978351593\n",
            "strain 0.24503937363624573\n",
            "classify 1.3521728515625\n",
            "classify 1.376708984375\n",
            "classify 1.354248046875\n",
            "classify 1.3631591796875\n",
            "classify 1.248437523841858\n",
            "0.234375\n",
            "0.296875\n",
            "0.34375\n",
            "0.28125\n",
            "0.2857142857142857\n",
            "strain 0.13168032467365265\n",
            "strain 0.17771059274673462\n",
            "strain 0.2043331265449524\n",
            "strain 0.15837515890598297\n",
            "strain 0.30866414308547974\n",
            "classify 1.35595703125\n",
            "classify 1.3482666015625\n",
            "classify 1.35400390625\n",
            "classify 1.3736572265625\n",
            "classify 1.375\n",
            "0.296875\n",
            "0.3125\n",
            "0.234375\n",
            "0.296875\n",
            "0.14285714285714285\n",
            "strain 0.1483033150434494\n",
            "strain 0.2138177454471588\n",
            "strain 0.13565433025360107\n",
            "strain 0.24297982454299927\n",
            "strain 0.04844919592142105\n",
            "classify 1.36572265625\n",
            "classify 1.354248046875\n",
            "classify 1.3504638671875\n",
            "classify 1.35302734375\n",
            "classify 1.407812476158142\n",
            "0.203125\n",
            "0.328125\n",
            "0.40625\n",
            "0.234375\n",
            "0.14285714285714285\n",
            "strain 0.23046685755252838\n",
            "strain 0.2102215588092804\n",
            "strain 0.19377462565898895\n",
            "strain 0.1967020332813263\n",
            "strain 0.04522412270307541\n",
            "classify 1.3575439453125\n",
            "classify 1.363525390625\n",
            "classify 1.330078125\n",
            "classify 1.37255859375\n",
            "classify 1.3250000476837158\n",
            "0.359375\n",
            "0.296875\n",
            "0.265625\n",
            "0.296875\n",
            "0.42857142857142855\n",
            "strain 0.1694444864988327\n",
            "strain 0.23115016520023346\n",
            "strain 0.10880851745605469\n",
            "strain 0.15810418128967285\n",
            "strain 0.07016148418188095\n",
            "classify 1.3486328125\n",
            "classify 1.3526611328125\n",
            "classify 1.33837890625\n",
            "classify 1.3780517578125\n",
            "classify 1.34375\n",
            "0.3125\n",
            "0.296875\n",
            "0.34375\n",
            "0.25\n",
            "0.0\n",
            "strain 0.21224252879619598\n",
            "strain 0.1791190207004547\n",
            "strain 0.160681813955307\n",
            "strain 0.2128211408853531\n",
            "strain 0.04745469614863396\n",
            "classify 1.36181640625\n",
            "classify 1.3394775390625\n",
            "classify 1.3582763671875\n",
            "classify 1.3489990234375\n",
            "classify 1.421875\n",
            "0.328125\n",
            "0.265625\n",
            "0.203125\n",
            "0.328125\n",
            "0.2857142857142857\n",
            "strain 0.18220560252666473\n",
            "strain 0.21889393031597137\n",
            "strain 0.2029174566268921\n",
            "strain 0.1282423436641693\n",
            "strain 0.078217051923275\n",
            "classify 1.35791015625\n",
            "classify 1.3729248046875\n",
            "classify 1.34228515625\n",
            "classify 1.334228515625\n",
            "classify 1.407812476158142\n",
            "0.3125\n",
            "0.375\n",
            "0.234375\n",
            "0.28125\n",
            "0.2857142857142857\n",
            "strain 0.16783292591571808\n",
            "strain 0.13563217222690582\n",
            "strain 0.18490493297576904\n",
            "strain 0.22160813212394714\n",
            "strain 0.12776261568069458\n",
            "classify 1.32421875\n",
            "classify 1.3701171875\n",
            "classify 1.353271484375\n",
            "classify 1.3668212890625\n",
            "classify 1.3171875476837158\n",
            "0.34375\n",
            "0.3125\n",
            "0.296875\n",
            "0.265625\n",
            "0.42857142857142855\n",
            "strain 0.16892164945602417\n",
            "strain 0.23912054300308228\n",
            "strain 0.2412189096212387\n",
            "strain 0.13611085712909698\n",
            "strain 0.07719994336366653\n",
            "classify 1.3267822265625\n",
            "classify 1.347900390625\n",
            "classify 1.3487548828125\n",
            "classify 1.386474609375\n",
            "classify 1.4953124523162842\n",
            "0.25\n",
            "0.296875\n",
            "0.296875\n",
            "0.359375\n",
            "0.0\n",
            "strain 0.24096588790416718\n",
            "strain 0.19878904521465302\n",
            "strain 0.16128776967525482\n",
            "strain 0.22731204330921173\n",
            "strain 0.2580593526363373\n",
            "classify 1.3629150390625\n",
            "classify 1.3643798828125\n",
            "classify 1.3353271484375\n",
            "classify 1.35693359375\n",
            "classify 1.2468750476837158\n",
            "0.359375\n",
            "0.3125\n",
            "0.265625\n",
            "0.328125\n",
            "0.0\n",
            "strain 0.10437195003032684\n",
            "strain 0.262127161026001\n",
            "strain 0.1159040555357933\n",
            "strain 0.1631813794374466\n",
            "strain 0.21355430781841278\n",
            "classify 1.35595703125\n",
            "classify 1.375244140625\n",
            "classify 1.3582763671875\n",
            "classify 1.3184814453125\n",
            "classify 1.3203125\n",
            "0.28125\n",
            "0.28125\n",
            "0.328125\n",
            "0.328125\n",
            "0.2857142857142857\n",
            "strain 0.20442606508731842\n",
            "strain 0.11467266082763672\n",
            "strain 0.16592393815517426\n",
            "strain 0.21167168021202087\n",
            "strain 0.32775959372520447\n",
            "classify 1.3472900390625\n",
            "classify 1.35888671875\n",
            "classify 1.3482666015625\n",
            "classify 1.361328125\n",
            "classify 1.3156249523162842\n",
            "0.265625\n",
            "0.296875\n",
            "0.28125\n",
            "0.40625\n",
            "0.42857142857142855\n",
            "strain 0.19985568523406982\n",
            "strain 0.15707476437091827\n",
            "strain 0.2514069080352783\n",
            "strain 0.16894595324993134\n",
            "strain 0.1735243946313858\n",
            "classify 1.3638916015625\n",
            "classify 1.3331298828125\n",
            "classify 1.3382568359375\n",
            "classify 1.3800048828125\n",
            "classify 1.3125\n",
            "0.3125\n",
            "0.390625\n",
            "0.265625\n",
            "0.28125\n",
            "0.14285714285714285\n",
            "strain 0.23289862275123596\n",
            "strain 0.13248135149478912\n",
            "strain 0.21366022527217865\n",
            "strain 0.16881707310676575\n",
            "strain 0.06263595074415207\n",
            "classify 1.370361328125\n",
            "classify 1.3306884765625\n",
            "classify 1.366943359375\n",
            "classify 1.3468017578125\n",
            "classify 1.3546874523162842\n",
            "0.328125\n",
            "0.28125\n",
            "0.328125\n",
            "0.328125\n",
            "0.2857142857142857\n",
            "strain 0.13987983763217926\n",
            "strain 0.1419050693511963\n",
            "strain 0.23371541500091553\n",
            "strain 0.197527676820755\n",
            "strain 0.08742355555295944\n",
            "classify 1.3505859375\n",
            "classify 1.3603515625\n",
            "classify 1.351318359375\n",
            "classify 1.35546875\n",
            "classify 1.342187523841858\n",
            "0.28125\n",
            "0.1875\n",
            "0.328125\n",
            "0.328125\n",
            "0.5714285714285714\n",
            "strain 0.17507196962833405\n",
            "strain 0.18004851043224335\n",
            "strain 0.22094199061393738\n",
            "strain 0.2487802952528\n",
            "strain 0.13783122599124908\n",
            "classify 1.363525390625\n",
            "classify 1.3658447265625\n",
            "classify 1.348876953125\n",
            "classify 1.34521484375\n",
            "classify 1.2999999523162842\n",
            "0.3125\n",
            "0.328125\n",
            "0.265625\n",
            "0.3125\n",
            "0.42857142857142855\n",
            "strain 0.15124648809432983\n",
            "strain 0.24464942514896393\n",
            "strain 0.2024911493062973\n",
            "strain 0.09013763070106506\n",
            "strain 0.04101869836449623\n",
            "classify 1.3682861328125\n",
            "classify 1.343505859375\n",
            "classify 1.3502197265625\n",
            "classify 1.35986328125\n",
            "classify 1.329687476158142\n",
            "0.3125\n",
            "0.21875\n",
            "0.265625\n",
            "0.3125\n",
            "0.42857142857142855\n",
            "strain 0.2156476080417633\n",
            "strain 0.19088803231716156\n",
            "strain 0.15583381056785583\n",
            "strain 0.13760490715503693\n",
            "strain 0.14796534180641174\n",
            "classify 1.3651123046875\n",
            "classify 1.325439453125\n",
            "classify 1.340087890625\n",
            "classify 1.381103515625\n",
            "classify 1.342187523841858\n",
            "0.296875\n",
            "0.375\n",
            "0.234375\n",
            "0.359375\n",
            "0.2857142857142857\n",
            "strain 0.255326509475708\n",
            "strain 0.18591395020484924\n",
            "strain 0.23657278716564178\n",
            "strain 0.14771920442581177\n",
            "strain 0.22736668586730957\n",
            "classify 1.33447265625\n",
            "classify 1.3797607421875\n",
            "classify 1.3487548828125\n",
            "classify 1.3463134765625\n",
            "classify 1.3312499523162842\n",
            "0.359375\n",
            "0.265625\n",
            "0.265625\n",
            "0.234375\n",
            "0.14285714285714285\n",
            "strain 0.2174094021320343\n",
            "strain 0.2235051393508911\n",
            "strain 0.13790276646614075\n",
            "strain 0.18883229792118073\n",
            "strain 0.0782671645283699\n",
            "classify 1.3453369140625\n",
            "classify 1.34521484375\n",
            "classify 1.35009765625\n",
            "classify 1.37109375\n",
            "classify 1.3546874523162842\n",
            "0.375\n",
            "0.265625\n",
            "0.359375\n",
            "0.28125\n",
            "0.42857142857142855\n",
            "strain 0.12456618249416351\n",
            "strain 0.17575204372406006\n",
            "strain 0.19353708624839783\n",
            "strain 0.1495124250650406\n",
            "strain 0.19994691014289856\n",
            "classify 1.3416748046875\n",
            "classify 1.34130859375\n",
            "classify 1.372314453125\n",
            "classify 1.3487548828125\n",
            "classify 1.375\n",
            "0.359375\n",
            "0.28125\n",
            "0.265625\n",
            "0.265625\n",
            "0.2857142857142857\n",
            "strain 0.13831938803195953\n",
            "strain 0.2256329208612442\n",
            "strain 0.1699703484773636\n",
            "strain 0.11518720537424088\n",
            "strain 0.03771123290061951\n",
            "classify 1.367431640625\n",
            "classify 1.3382568359375\n",
            "classify 1.357177734375\n",
            "classify 1.3465576171875\n",
            "classify 1.350000023841858\n",
            "0.34375\n",
            "0.296875\n",
            "0.25\n",
            "0.265625\n",
            "0.2857142857142857\n",
            "strain 0.20124198496341705\n",
            "strain 0.1619146317243576\n",
            "strain 0.1509450376033783\n",
            "strain 0.18287375569343567\n",
            "strain 0.061957985162734985\n",
            "classify 1.32415771484375\n",
            "classify 1.3544921875\n",
            "classify 1.3701171875\n",
            "classify 1.3580322265625\n",
            "classify 1.3468749523162842\n",
            "0.296875\n",
            "0.21875\n",
            "0.28125\n",
            "0.296875\n",
            "0.2857142857142857\n",
            "strain 0.17600733041763306\n",
            "strain 0.1392994523048401\n",
            "strain 0.2647955119609833\n",
            "strain 0.14677424728870392\n",
            "strain 0.06160082295536995\n",
            "classify 1.356201171875\n",
            "classify 1.3309326171875\n",
            "classify 1.3609619140625\n",
            "classify 1.3634033203125\n",
            "classify 1.2804687023162842\n",
            "0.25\n",
            "0.25\n",
            "0.3125\n",
            "0.296875\n",
            "0.42857142857142855\n",
            "strain 0.08034408092498779\n",
            "strain 0.2778148055076599\n",
            "strain 0.14002124965190887\n",
            "strain 0.15543563663959503\n",
            "strain 0.13406726717948914\n",
            "classify 1.3773193359375\n",
            "classify 1.3363037109375\n",
            "classify 1.3485107421875\n",
            "classify 1.338134765625\n",
            "classify 1.4093749523162842\n",
            "0.359375\n",
            "0.34375\n",
            "0.203125\n",
            "0.265625\n",
            "0.0\n",
            "strain 0.21855930984020233\n",
            "strain 0.15325073897838593\n",
            "strain 0.14197257161140442\n",
            "strain 0.10005407780408859\n",
            "strain 0.05390544608235359\n",
            "classify 1.3658447265625\n",
            "classify 1.352294921875\n",
            "classify 1.35394287109375\n",
            "classify 1.3408203125\n",
            "classify 1.384374976158142\n",
            "0.3125\n",
            "0.28125\n",
            "0.28125\n",
            "0.265625\n",
            "0.2857142857142857\n",
            "strain 0.13288947939872742\n",
            "strain 0.18985949456691742\n",
            "strain 0.12118014693260193\n",
            "strain 0.11235403269529343\n",
            "strain 0.06718898564577103\n",
            "classify 1.331298828125\n",
            "classify 1.357421875\n",
            "classify 1.366943359375\n",
            "classify 1.3602294921875\n",
            "classify 1.326562523841858\n",
            "0.296875\n",
            "0.203125\n",
            "0.265625\n",
            "0.3125\n",
            "0.42857142857142855\n",
            "strain 0.20912952721118927\n",
            "strain 0.19239173829555511\n",
            "strain 0.07866018265485764\n",
            "strain 0.19948816299438477\n",
            "strain 0.11277976632118225\n",
            "classify 1.32861328125\n",
            "classify 1.3463134765625\n",
            "classify 1.3226318359375\n",
            "classify 1.4005126953125\n",
            "classify 1.4265625476837158\n",
            "0.21875\n",
            "0.296875\n",
            "0.28125\n",
            "0.328125\n",
            "0.2857142857142857\n",
            "strain 0.12190967053174973\n",
            "strain 0.28542259335517883\n",
            "strain 0.1929527074098587\n",
            "strain 0.18228773772716522\n",
            "strain 0.06430432200431824\n",
            "classify 1.365966796875\n",
            "classify 1.3499755859375\n",
            "classify 1.3397216796875\n",
            "classify 1.3409423828125\n",
            "classify 1.373437523841858\n",
            "0.328125\n",
            "0.265625\n",
            "0.3125\n",
            "0.203125\n",
            "0.42857142857142855\n",
            "strain 0.16245810687541962\n",
            "strain 0.1597280502319336\n",
            "strain 0.1423313468694687\n",
            "strain 0.18316194415092468\n",
            "strain 0.2181922197341919\n",
            "classify 1.3509521484375\n",
            "classify 1.34490966796875\n",
            "classify 1.3182373046875\n",
            "classify 1.3907470703125\n",
            "classify 1.3156249523162842\n",
            "0.328125\n",
            "0.28125\n",
            "0.3125\n",
            "0.203125\n",
            "0.2857142857142857\n",
            "strain 0.17932656407356262\n",
            "strain 0.24323183298110962\n",
            "strain 0.14228036999702454\n",
            "strain 0.16370390355587006\n",
            "strain 0.28845882415771484\n",
            "classify 1.3099365234375\n",
            "classify 1.3720703125\n",
            "classify 1.369384765625\n",
            "classify 1.3526611328125\n",
            "classify 1.3781249523162842\n",
            "0.296875\n",
            "0.25\n",
            "0.265625\n",
            "0.359375\n",
            "0.42857142857142855\n",
            "strain 0.1521764099597931\n",
            "strain 0.24523670971393585\n",
            "strain 0.16242696344852448\n",
            "strain 0.15678030252456665\n",
            "strain 0.06524938344955444\n",
            "classify 1.34619140625\n",
            "classify 1.4051513671875\n",
            "classify 1.357421875\n",
            "classify 1.29833984375\n",
            "classify 1.3390624523162842\n",
            "0.21875\n",
            "0.25\n",
            "0.28125\n",
            "0.40625\n",
            "0.42857142857142855\n",
            "strain 0.25804784893989563\n",
            "strain 0.1879480630159378\n",
            "strain 0.10429268330335617\n",
            "strain 0.209812194108963\n",
            "strain 0.1320159137248993\n",
            "classify 1.363037109375\n",
            "classify 1.36474609375\n",
            "classify 1.35595703125\n",
            "classify 1.3189697265625\n",
            "classify 1.3359375\n",
            "0.234375\n",
            "0.3125\n",
            "0.3125\n",
            "0.28125\n",
            "0.42857142857142855\n",
            "strain 0.15853290259838104\n",
            "strain 0.22484365105628967\n",
            "strain 0.2674950361251831\n",
            "strain 0.1615912914276123\n",
            "strain 0.03891737014055252\n",
            "classify 1.3541259765625\n",
            "classify 1.35888671875\n",
            "classify 1.3209228515625\n",
            "classify 1.3701171875\n",
            "classify 1.3171875476837158\n",
            "0.34375\n",
            "0.28125\n",
            "0.296875\n",
            "0.234375\n",
            "0.2857142857142857\n",
            "strain 0.12587477266788483\n",
            "strain 0.07038978487253189\n",
            "strain 0.19265933334827423\n",
            "strain 0.2082972526550293\n",
            "strain 0.049131665378808975\n",
            "classify 1.330322265625\n",
            "classify 1.35693359375\n",
            "classify 1.36181640625\n",
            "classify 1.34912109375\n",
            "classify 1.4015624523162842\n",
            "0.21875\n",
            "0.296875\n",
            "0.34375\n",
            "0.375\n",
            "0.5714285714285714\n",
            "strain 0.20887970924377441\n",
            "strain 0.20107753574848175\n",
            "strain 0.2580370306968689\n",
            "strain 0.28228551149368286\n",
            "strain 0.11770934611558914\n",
            "classify 1.35009765625\n",
            "classify 1.368408203125\n",
            "classify 1.3311767578125\n",
            "classify 1.3408203125\n",
            "classify 1.4015624523162842\n",
            "0.28125\n",
            "0.265625\n",
            "0.25\n",
            "0.328125\n",
            "0.42857142857142855\n",
            "strain 0.1371040791273117\n",
            "strain 0.17021261155605316\n",
            "strain 0.12470469623804092\n",
            "strain 0.20832088589668274\n",
            "strain 0.23545019328594208\n",
            "classify 1.3111572265625\n",
            "classify 1.36712646484375\n",
            "classify 1.35302734375\n",
            "classify 1.3580322265625\n",
            "classify 1.350000023841858\n",
            "0.25\n",
            "0.296875\n",
            "0.3125\n",
            "0.359375\n",
            "0.5714285714285714\n",
            "strain 0.15030263364315033\n",
            "strain 0.23322181403636932\n",
            "strain 0.191743403673172\n",
            "strain 0.13658814132213593\n",
            "strain 0.10363045334815979\n",
            "classify 1.361572265625\n",
            "classify 1.34521484375\n",
            "classify 1.3516845703125\n",
            "classify 1.3468017578125\n",
            "classify 1.265625\n",
            "0.28125\n",
            "0.296875\n",
            "0.21875\n",
            "0.34375\n",
            "0.14285714285714285\n",
            "strain 0.20205773413181305\n",
            "strain 0.1478528529405594\n",
            "strain 0.24296541512012482\n",
            "strain 0.1952412873506546\n",
            "strain 0.0867035910487175\n",
            "classify 1.36865234375\n",
            "classify 1.3868408203125\n",
            "classify 1.3260498046875\n",
            "classify 1.3441162109375\n",
            "classify 1.307031273841858\n",
            "0.265625\n",
            "0.359375\n",
            "0.25\n",
            "0.3125\n",
            "0.14285714285714285\n",
            "strain 0.25937241315841675\n",
            "strain 0.21424159407615662\n",
            "strain 0.24982765316963196\n",
            "strain 0.08862029016017914\n",
            "strain 0.09243854135274887\n",
            "classify 1.31964111328125\n",
            "classify 1.3355712890625\n",
            "classify 1.36676025390625\n",
            "classify 1.38262939453125\n",
            "classify 1.446874976158142\n",
            "0.234375\n",
            "0.265625\n",
            "0.25\n",
            "0.359375\n",
            "0.2857142857142857\n",
            "strain 0.1808234453201294\n",
            "strain 0.18332511186599731\n",
            "strain 0.11320482194423676\n",
            "strain 0.17005228996276855\n",
            "strain 0.18880817294120789\n",
            "classify 1.348876953125\n",
            "classify 1.321044921875\n",
            "classify 1.37158203125\n",
            "classify 1.3524169921875\n",
            "classify 1.4890625476837158\n",
            "0.28125\n",
            "0.359375\n",
            "0.28125\n",
            "0.28125\n",
            "0.42857142857142855\n",
            "strain 0.2686038911342621\n",
            "strain 0.25951123237609863\n",
            "strain 0.11185909807682037\n",
            "strain 0.1521998643875122\n",
            "strain 0.05700809881091118\n",
            "classify 1.3541259765625\n",
            "classify 1.34417724609375\n",
            "classify 1.35498046875\n",
            "classify 1.3544921875\n",
            "classify 1.3125\n",
            "0.234375\n",
            "0.296875\n",
            "0.296875\n",
            "0.25\n",
            "0.2857142857142857\n",
            "strain 0.21861664950847626\n",
            "strain 0.13246777653694153\n",
            "strain 0.16692912578582764\n",
            "strain 0.22942876815795898\n",
            "strain 0.04902965575456619\n",
            "classify 1.326904296875\n",
            "classify 1.36376953125\n",
            "classify 1.3544921875\n",
            "classify 1.3592529296875\n",
            "classify 1.3234374523162842\n",
            "0.25\n",
            "0.25\n",
            "0.375\n",
            "0.3125\n",
            "0.42857142857142855\n",
            "strain 0.2403980940580368\n",
            "strain 0.1483430117368698\n",
            "strain 0.1201939582824707\n",
            "strain 0.14551976323127747\n",
            "strain 0.0645006000995636\n",
            "classify 1.36553955078125\n",
            "classify 1.370361328125\n",
            "classify 1.331298828125\n",
            "classify 1.3326416015625\n",
            "classify 1.2468750476837158\n",
            "0.3125\n",
            "0.359375\n",
            "0.1875\n",
            "0.25\n",
            "0.5714285714285714\n",
            "strain 0.09505517035722733\n",
            "strain 0.19985051453113556\n",
            "strain 0.13226377964019775\n",
            "strain 0.1776076704263687\n",
            "strain 0.3419761657714844\n",
            "classify 1.33233642578125\n",
            "classify 1.3685302734375\n",
            "classify 1.378173828125\n",
            "classify 1.3177490234375\n",
            "classify 1.2687499523162842\n",
            "0.21875\n",
            "0.328125\n",
            "0.28125\n",
            "0.328125\n",
            "0.42857142857142855\n",
            "strain 0.2213057577610016\n",
            "strain 0.21188800036907196\n",
            "strain 0.2082444280385971\n",
            "strain 0.17452996969223022\n",
            "strain 0.09879685938358307\n",
            "classify 1.3095703125\n",
            "classify 1.37744140625\n",
            "classify 1.3482666015625\n",
            "classify 1.35693359375\n",
            "classify 1.2390625476837158\n",
            "0.359375\n",
            "0.1875\n",
            "0.203125\n",
            "0.3125\n",
            "0.42857142857142855\n",
            "strain 0.1799534261226654\n",
            "strain 0.19417139887809753\n",
            "strain 0.20988602936267853\n",
            "strain 0.2474987506866455\n",
            "strain 0.09266676008701324\n",
            "classify 1.34014892578125\n",
            "classify 1.34954833984375\n",
            "classify 1.34283447265625\n",
            "classify 1.34423828125\n",
            "classify 1.376562476158142\n",
            "0.234375\n",
            "0.3125\n",
            "0.28125\n",
            "0.21875\n",
            "0.42857142857142855\n",
            "strain 0.18038499355316162\n",
            "strain 0.19581781327724457\n",
            "strain 0.1862279176712036\n",
            "strain 0.18139055371284485\n",
            "strain 0.09899584203958511\n",
            "classify 1.33251953125\n",
            "classify 1.340087890625\n",
            "classify 1.352294921875\n",
            "classify 1.3629150390625\n",
            "classify 1.314062476158142\n",
            "0.4375\n",
            "0.21875\n",
            "0.21875\n",
            "0.25\n",
            "0.14285714285714285\n",
            "strain 0.2446928173303604\n",
            "strain 0.08985888957977295\n",
            "strain 0.1219966858625412\n",
            "strain 0.11398681998252869\n",
            "strain 0.22914373874664307\n",
            "classify 1.3262939453125\n",
            "classify 1.34088134765625\n",
            "classify 1.376220703125\n",
            "classify 1.35443115234375\n",
            "classify 1.271093726158142\n",
            "0.3125\n",
            "0.3125\n",
            "0.296875\n",
            "0.296875\n",
            "0.42857142857142855\n",
            "strain 0.16293539106845856\n",
            "strain 0.21253767609596252\n",
            "strain 0.16637542843818665\n",
            "strain 0.24161849915981293\n",
            "strain 0.05752113461494446\n",
            "classify 1.37744140625\n",
            "classify 1.3206787109375\n",
            "classify 1.34442138671875\n",
            "classify 1.35986328125\n",
            "classify 1.29296875\n",
            "0.375\n",
            "0.328125\n",
            "0.234375\n",
            "0.28125\n",
            "0.42857142857142855\n",
            "strain 0.12754245102405548\n",
            "strain 0.15423466265201569\n",
            "strain 0.12892462313175201\n",
            "strain 0.2724454402923584\n",
            "strain 0.18164730072021484\n",
            "classify 1.34259033203125\n",
            "classify 1.388916015625\n",
            "classify 1.302978515625\n",
            "classify 1.35552978515625\n",
            "classify 1.4015624523162842\n",
            "0.328125\n",
            "0.34375\n",
            "0.25\n",
            "0.3125\n",
            "0.2857142857142857\n",
            "strain 0.19313104450702667\n",
            "strain 0.14492963254451752\n",
            "strain 0.09590525925159454\n",
            "strain 0.06817127764225006\n",
            "strain 0.2945730984210968\n",
            "classify 1.3477783203125\n",
            "classify 1.3585205078125\n",
            "classify 1.32147216796875\n",
            "classify 1.35986328125\n",
            "classify 1.4171874523162842\n",
            "0.28125\n",
            "0.40625\n",
            "0.328125\n",
            "0.203125\n",
            "0.42857142857142855\n",
            "strain 0.2315804362297058\n",
            "strain 0.16680525243282318\n",
            "strain 0.15159466862678528\n",
            "strain 0.27289342880249023\n",
            "strain 0.05283724516630173\n",
            "classify 1.334716796875\n",
            "classify 1.34051513671875\n",
            "classify 1.363037109375\n",
            "classify 1.35302734375\n",
            "classify 1.329687476158142\n",
            "0.375\n",
            "0.296875\n",
            "0.328125\n",
            "0.25\n",
            "0.14285714285714285\n",
            "strain 0.20880508422851562\n",
            "strain 0.2519848644733429\n",
            "strain 0.22188395261764526\n",
            "strain 0.2065228521823883\n",
            "strain 0.17175938189029694\n",
            "classify 1.33355712890625\n",
            "classify 1.343994140625\n",
            "classify 1.3408203125\n",
            "classify 1.3714599609375\n",
            "classify 1.3171875476837158\n",
            "0.34375\n",
            "0.34375\n",
            "0.203125\n",
            "0.328125\n",
            "0.42857142857142855\n",
            "strain 0.2463904321193695\n",
            "strain 0.1640452891588211\n",
            "strain 0.07489638030529022\n",
            "strain 0.24914048612117767\n",
            "strain 0.13276927173137665\n",
            "classify 1.322265625\n",
            "classify 1.344482421875\n",
            "classify 1.3629150390625\n",
            "classify 1.3641357421875\n",
            "classify 1.265625\n",
            "0.296875\n",
            "0.296875\n",
            "0.3125\n",
            "0.34375\n",
            "0.0\n",
            "strain 0.09158126264810562\n",
            "strain 0.13942402601242065\n",
            "strain 0.14563718438148499\n",
            "strain 0.2628605365753174\n",
            "strain 0.03752436116337776\n",
            "classify 1.351806640625\n",
            "classify 1.35595703125\n",
            "classify 1.316650390625\n",
            "classify 1.35906982421875\n",
            "classify 1.446874976158142\n",
            "0.25\n",
            "0.234375\n",
            "0.375\n",
            "0.375\n",
            "0.2857142857142857\n",
            "strain 0.14337950944900513\n",
            "strain 0.1394636034965515\n",
            "strain 0.1781233698129654\n",
            "strain 0.21278080344200134\n",
            "strain 0.07123639434576035\n",
            "classify 1.325927734375\n",
            "classify 1.3643798828125\n",
            "classify 1.350830078125\n",
            "classify 1.3438720703125\n",
            "classify 1.3406250476837158\n",
            "0.34375\n",
            "0.265625\n",
            "0.234375\n",
            "0.25\n",
            "0.14285714285714285\n",
            "strain 0.1406310349702835\n",
            "strain 0.16741497814655304\n",
            "strain 0.13811269402503967\n",
            "strain 0.15876197814941406\n",
            "strain 0.044778335839509964\n",
            "classify 1.3302001953125\n",
            "classify 1.35595703125\n",
            "classify 1.35968017578125\n",
            "classify 1.33306884765625\n",
            "classify 1.420312523841858\n",
            "0.265625\n",
            "0.25\n",
            "0.390625\n",
            "0.3125\n",
            "0.2857142857142857\n",
            "strain 0.18123963475227356\n",
            "strain 0.29614338278770447\n",
            "strain 0.20867057144641876\n",
            "strain 0.13781902194023132\n",
            "strain 0.3124854266643524\n",
            "classify 1.34344482421875\n",
            "classify 1.3353271484375\n",
            "classify 1.346435546875\n",
            "classify 1.364501953125\n",
            "classify 1.298437476158142\n",
            "0.34375\n",
            "0.15625\n",
            "0.3125\n",
            "0.359375\n",
            "0.2857142857142857\n",
            "strain 0.11066531389951706\n",
            "strain 0.1752672642469406\n",
            "strain 0.18336902558803558\n",
            "strain 0.18431879580020905\n",
            "strain 0.15407472848892212\n",
            "classify 1.3436279296875\n",
            "classify 1.372802734375\n",
            "classify 1.32330322265625\n",
            "classify 1.343505859375\n",
            "classify 1.306249976158142\n",
            "0.390625\n",
            "0.265625\n",
            "0.25\n",
            "0.203125\n",
            "0.2857142857142857\n",
            "strain 0.1466199904680252\n",
            "strain 0.1165417954325676\n",
            "strain 0.1830500215291977\n",
            "strain 0.14490334689617157\n",
            "strain 0.0745677724480629\n",
            "classify 1.31292724609375\n",
            "classify 1.32373046875\n",
            "classify 1.36175537109375\n",
            "classify 1.383544921875\n",
            "classify 1.389062523841858\n",
            "0.375\n",
            "0.234375\n",
            "0.171875\n",
            "0.390625\n",
            "0.2857142857142857\n",
            "strain 0.1299837976694107\n",
            "strain 0.19006136059761047\n",
            "strain 0.19221161305904388\n",
            "strain 0.2528553307056427\n",
            "strain 0.06141578406095505\n",
            "classify 1.3511962890625\n",
            "classify 1.375\n",
            "classify 1.3695068359375\n",
            "classify 1.306396484375\n",
            "classify 1.235937476158142\n",
            "0.359375\n",
            "0.21875\n",
            "0.25\n",
            "0.328125\n",
            "0.14285714285714285\n",
            "strain 0.15306945145130157\n",
            "strain 0.15284888446331024\n",
            "strain 0.17180728912353516\n",
            "strain 0.14003923535346985\n",
            "strain 0.06693714112043381\n",
            "classify 1.3431396484375\n",
            "classify 1.3250732421875\n",
            "classify 1.3551025390625\n",
            "classify 1.3653564453125\n",
            "classify 1.396875023841858\n",
            "0.296875\n",
            "0.140625\n",
            "0.296875\n",
            "0.390625\n",
            "0.2857142857142857\n",
            "strain 0.17769105732440948\n",
            "strain 0.08806434273719788\n",
            "strain 0.1539740413427353\n",
            "strain 0.195816770195961\n",
            "strain 0.2632235586643219\n",
            "classify 1.3604736328125\n",
            "classify 1.3348388671875\n",
            "classify 1.34326171875\n",
            "classify 1.35400390625\n",
            "classify 1.360937476158142\n",
            "0.234375\n",
            "0.328125\n",
            "0.21875\n",
            "0.296875\n",
            "0.2857142857142857\n",
            "strain 0.2071179449558258\n",
            "strain 0.15394507348537445\n",
            "strain 0.1597406566143036\n",
            "strain 0.1619836390018463\n",
            "strain 0.06593941897153854\n",
            "classify 1.32794189453125\n",
            "classify 1.332275390625\n",
            "classify 1.4093017578125\n",
            "classify 1.321533203125\n",
            "classify 1.357812523841858\n",
            "0.3125\n",
            "0.265625\n",
            "0.234375\n",
            "0.28125\n",
            "0.42857142857142855\n",
            "strain 0.20442038774490356\n",
            "strain 0.2421250343322754\n",
            "strain 0.15988458693027496\n",
            "strain 0.1600680649280548\n",
            "strain 0.08631755411624908\n",
            "classify 1.36431884765625\n",
            "classify 1.33758544921875\n",
            "classify 1.3543701171875\n",
            "classify 1.3394775390625\n",
            "classify 1.342187523841858\n",
            "0.28125\n",
            "0.21875\n",
            "0.359375\n",
            "0.234375\n",
            "0.42857142857142855\n",
            "strain 0.20639774203300476\n",
            "strain 0.14907045662403107\n",
            "strain 0.15734484791755676\n",
            "strain 0.2784467935562134\n",
            "strain 0.05990312993526459\n",
            "classify 1.31500244140625\n",
            "classify 1.39013671875\n",
            "classify 1.3468017578125\n",
            "classify 1.34307861328125\n",
            "classify 1.350000023841858\n",
            "0.28125\n",
            "0.234375\n",
            "0.328125\n",
            "0.3125\n",
            "0.42857142857142855\n",
            "strain 0.15129907429218292\n",
            "strain 0.22077353298664093\n",
            "strain 0.18565770983695984\n",
            "strain 0.1531437635421753\n",
            "strain 0.19009846448898315\n",
            "classify 1.32293701171875\n",
            "classify 1.35174560546875\n",
            "classify 1.3778076171875\n",
            "classify 1.3360595703125\n",
            "classify 1.3234374523162842\n",
            "0.21875\n",
            "0.25\n",
            "0.3125\n",
            "0.34375\n",
            "0.0\n",
            "strain 0.20767895877361298\n",
            "strain 0.14257089793682098\n",
            "strain 0.17726008594036102\n",
            "strain 0.18198320269584656\n",
            "strain 0.07257302105426788\n",
            "classify 1.355224609375\n",
            "classify 1.309326171875\n",
            "classify 1.35260009765625\n",
            "classify 1.37255859375\n",
            "classify 1.4249999523162842\n",
            "0.34375\n",
            "0.328125\n",
            "0.296875\n",
            "0.28125\n",
            "0.2857142857142857\n",
            "strain 0.11347544938325882\n",
            "strain 0.15599772334098816\n",
            "strain 0.20060057938098907\n",
            "strain 0.18119844794273376\n",
            "strain 0.07034338265657425\n",
            "classify 1.3487548828125\n",
            "classify 1.35589599609375\n",
            "classify 1.3358154296875\n",
            "classify 1.36376953125\n",
            "classify 1.251562476158142\n",
            "0.21875\n",
            "0.3125\n",
            "0.28125\n",
            "0.375\n",
            "0.5714285714285714\n",
            "strain 0.1158047765493393\n",
            "strain 0.24781405925750732\n",
            "strain 0.17220495641231537\n",
            "strain 0.10349935293197632\n",
            "strain 0.22867286205291748\n",
            "classify 1.3680419921875\n",
            "classify 1.3421630859375\n",
            "classify 1.3492431640625\n",
            "classify 1.3331298828125\n",
            "classify 1.2578125\n",
            "0.21875\n",
            "0.296875\n",
            "0.375\n",
            "0.234375\n",
            "0.5714285714285714\n",
            "strain 0.2557452321052551\n",
            "strain 0.24835516512393951\n",
            "strain 0.11325523257255554\n",
            "strain 0.20431725680828094\n",
            "strain 0.38714292645454407\n",
            "classify 1.3336181640625\n",
            "classify 1.39990234375\n",
            "classify 1.2952880859375\n",
            "classify 1.3543701171875\n",
            "classify 1.381250023841858\n",
            "0.3125\n",
            "0.25\n",
            "0.265625\n",
            "0.34375\n",
            "0.14285714285714285\n",
            "strain 0.22810214757919312\n",
            "strain 0.3193686604499817\n",
            "strain 0.09241098165512085\n",
            "strain 0.19185402989387512\n",
            "strain 0.09885580837726593\n",
            "classify 1.3304443359375\n",
            "classify 1.3662109375\n",
            "classify 1.36279296875\n",
            "classify 1.33642578125\n",
            "classify 1.3203125\n",
            "0.25\n",
            "0.234375\n",
            "0.234375\n",
            "0.359375\n",
            "0.7142857142857143\n",
            "strain 0.18873462080955505\n",
            "strain 0.15951289236545563\n",
            "strain 0.17304252088069916\n",
            "strain 0.11880344152450562\n",
            "strain 0.04956570640206337\n",
            "classify 1.36083984375\n",
            "classify 1.355712890625\n",
            "classify 1.3475341796875\n",
            "classify 1.344970703125\n",
            "classify 1.2218749523162842\n",
            "0.3125\n",
            "0.265625\n",
            "0.34375\n",
            "0.34375\n",
            "0.14285714285714285\n",
            "strain 0.1076919212937355\n",
            "strain 0.21029846370220184\n",
            "strain 0.2039034217596054\n",
            "strain 0.19738240540027618\n",
            "strain 0.05400550737977028\n",
            "classify 1.34356689453125\n",
            "classify 1.334228515625\n",
            "classify 1.3480224609375\n",
            "classify 1.373046875\n",
            "classify 1.3640625476837158\n",
            "0.265625\n",
            "0.25\n",
            "0.390625\n",
            "0.234375\n",
            "0.0\n",
            "strain 0.1405480057001114\n",
            "strain 0.12038050591945648\n",
            "strain 0.18562205135822296\n",
            "strain 0.2011449784040451\n",
            "strain 0.04607764631509781\n",
            "classify 1.3309326171875\n",
            "classify 1.3544921875\n",
            "classify 1.352783203125\n",
            "classify 1.353759765625\n",
            "classify 1.5046875476837158\n",
            "0.25\n",
            "0.3125\n",
            "0.34375\n",
            "0.34375\n",
            "0.2857142857142857\n",
            "strain 0.1812622994184494\n",
            "strain 0.18117310106754303\n",
            "strain 0.18600071966648102\n",
            "strain 0.23659004271030426\n",
            "strain 0.04597141966223717\n",
            "classify 1.327392578125\n",
            "classify 1.35791015625\n",
            "classify 1.359619140625\n",
            "classify 1.357421875\n",
            "classify 1.287500023841858\n",
            "0.34375\n",
            "0.28125\n",
            "0.265625\n",
            "0.265625\n",
            "0.42857142857142855\n",
            "strain 0.1383296102285385\n",
            "strain 0.20262378454208374\n",
            "strain 0.17013651132583618\n",
            "strain 0.1809263825416565\n",
            "strain 0.4247758686542511\n",
            "classify 1.3614501953125\n",
            "classify 1.3406982421875\n",
            "classify 1.35858154296875\n",
            "classify 1.3267822265625\n",
            "classify 1.3875000476837158\n",
            "0.1875\n",
            "0.265625\n",
            "0.328125\n",
            "0.359375\n",
            "0.2857142857142857\n",
            "strain 0.1558564156293869\n",
            "strain 0.20406422019004822\n",
            "strain 0.1680307388305664\n",
            "strain 0.1562437117099762\n",
            "strain 0.12480553239583969\n",
            "classify 1.3360595703125\n",
            "classify 1.3497314453125\n",
            "classify 1.34375\n",
            "classify 1.3590087890625\n",
            "classify 1.3515625\n",
            "0.25\n",
            "0.375\n",
            "0.28125\n",
            "0.328125\n",
            "0.0\n",
            "strain 0.14634652435779572\n",
            "strain 0.21221402287483215\n",
            "strain 0.18242214620113373\n",
            "strain 0.17407163977622986\n",
            "strain 0.11876536160707474\n",
            "classify 1.3287353515625\n",
            "classify 1.346435546875\n",
            "classify 1.358642578125\n",
            "classify 1.34521484375\n",
            "classify 1.407812476158142\n",
            "0.359375\n",
            "0.28125\n",
            "0.34375\n",
            "0.28125\n",
            "0.14285714285714285\n",
            "strain 0.18177753686904907\n",
            "strain 0.15551641583442688\n",
            "strain 0.24680553376674652\n",
            "strain 0.18703433871269226\n",
            "strain 0.22224219143390656\n",
            "classify 1.33831787109375\n",
            "classify 1.3592529296875\n",
            "classify 1.360107421875\n",
            "classify 1.32867431640625\n",
            "classify 1.259374976158142\n",
            "0.265625\n",
            "0.390625\n",
            "0.328125\n",
            "0.296875\n",
            "0.2857142857142857\n",
            "strain 0.18194721639156342\n",
            "strain 0.2352176308631897\n",
            "strain 0.18307003378868103\n",
            "strain 0.24694916605949402\n",
            "strain 0.18605177104473114\n",
            "classify 1.3404541015625\n",
            "classify 1.3304443359375\n",
            "classify 1.36444091796875\n",
            "classify 1.3641357421875\n",
            "classify 1.3046875\n",
            "0.40625\n",
            "0.1875\n",
            "0.453125\n",
            "0.234375\n",
            "0.42857142857142855\n",
            "strain 0.21558204293251038\n",
            "strain 0.1686074435710907\n",
            "strain 0.10472738742828369\n",
            "strain 0.19879339635372162\n",
            "strain 0.12149051576852798\n",
            "classify 1.34326171875\n",
            "classify 1.3255615234375\n",
            "classify 1.35028076171875\n",
            "classify 1.34814453125\n",
            "classify 1.478124976158142\n",
            "0.265625\n",
            "0.265625\n",
            "0.390625\n",
            "0.34375\n",
            "0.42857142857142855\n",
            "strain 0.2222781777381897\n",
            "strain 0.1911495476961136\n",
            "strain 0.2428167462348938\n",
            "strain 0.1572970747947693\n",
            "strain 0.08880787342786789\n",
            "classify 1.33270263671875\n",
            "classify 1.33154296875\n",
            "classify 1.331298828125\n",
            "classify 1.36810302734375\n",
            "classify 1.365625023841858\n",
            "0.296875\n",
            "0.3125\n",
            "0.34375\n",
            "0.359375\n",
            "0.42857142857142855\n",
            "strain 0.22088880836963654\n",
            "strain 0.21199683845043182\n",
            "strain 0.2539938986301422\n",
            "strain 0.21380741894245148\n",
            "strain 0.06921041011810303\n",
            "classify 1.3359375\n",
            "classify 1.33056640625\n",
            "classify 1.3310546875\n",
            "classify 1.3580322265625\n",
            "classify 1.359375\n",
            "0.3125\n",
            "0.375\n",
            "0.328125\n",
            "0.328125\n",
            "0.14285714285714285\n",
            "strain 0.2302256077528\n",
            "strain 0.17672434449195862\n",
            "strain 0.1407775729894638\n",
            "strain 0.17612546682357788\n",
            "strain 0.20538784563541412\n",
            "classify 1.3604736328125\n",
            "classify 1.3369140625\n",
            "classify 1.344970703125\n",
            "classify 1.31640625\n",
            "classify 1.3640625476837158\n",
            "0.234375\n",
            "0.328125\n",
            "0.3125\n",
            "0.296875\n",
            "0.42857142857142855\n",
            "strain 0.21976207196712494\n",
            "strain 0.17038825154304504\n",
            "strain 0.17971940338611603\n",
            "strain 0.1671769917011261\n",
            "strain 0.05728389695286751\n",
            "classify 1.345703125\n",
            "classify 1.3533935546875\n",
            "classify 1.32366943359375\n",
            "classify 1.3404541015625\n",
            "classify 1.295312523841858\n",
            "0.34375\n",
            "0.234375\n",
            "0.265625\n",
            "0.265625\n",
            "0.14285714285714285\n",
            "strain 0.13159547746181488\n",
            "strain 0.24049140512943268\n",
            "strain 0.20508520305156708\n",
            "strain 0.15264803171157837\n",
            "strain 0.2404780089855194\n",
            "classify 1.324951171875\n",
            "classify 1.34149169921875\n",
            "classify 1.35870361328125\n",
            "classify 1.3358154296875\n",
            "classify 1.350000023841858\n",
            "0.28125\n",
            "0.359375\n",
            "0.21875\n",
            "0.375\n",
            "0.42857142857142855\n",
            "strain 0.18736381828784943\n",
            "strain 0.17806334793567657\n",
            "strain 0.16013126075267792\n",
            "strain 0.1579568237066269\n",
            "strain 0.13632622361183167\n",
            "classify 1.34173583984375\n",
            "classify 1.3499755859375\n",
            "classify 1.3486328125\n",
            "classify 1.3204345703125\n",
            "classify 1.314062476158142\n",
            "0.328125\n",
            "0.28125\n",
            "0.265625\n",
            "0.203125\n",
            "0.42857142857142855\n",
            "strain 0.1959938108921051\n",
            "strain 0.15170730650424957\n",
            "strain 0.1737499237060547\n",
            "strain 0.19459252059459686\n",
            "strain 0.16318681836128235\n",
            "classify 1.344970703125\n",
            "classify 1.3370361328125\n",
            "classify 1.3492431640625\n",
            "classify 1.3262939453125\n",
            "classify 1.4093749523162842\n",
            "0.28125\n",
            "0.28125\n",
            "0.21875\n",
            "0.3125\n",
            "0.14285714285714285\n",
            "strain 0.1343698352575302\n",
            "strain 0.16189585626125336\n",
            "strain 0.14229780435562134\n",
            "strain 0.1417517513036728\n",
            "strain 0.13954685628414154\n",
            "classify 1.3609619140625\n",
            "classify 1.32177734375\n",
            "classify 1.336669921875\n",
            "classify 1.33880615234375\n",
            "classify 1.453125\n",
            "0.3125\n",
            "0.25\n",
            "0.328125\n",
            "0.34375\n",
            "0.2857142857142857\n",
            "strain 0.24241621792316437\n",
            "strain 0.17351748049259186\n",
            "strain 0.24084468185901642\n",
            "strain 0.15420842170715332\n",
            "strain 0.08742119371891022\n",
            "classify 1.32501220703125\n",
            "classify 1.3487548828125\n",
            "classify 1.3621826171875\n",
            "classify 1.330322265625\n",
            "classify 1.3156249523162842\n",
            "0.3125\n",
            "0.328125\n",
            "0.34375\n",
            "0.1875\n",
            "0.42857142857142855\n",
            "strain 0.2871021330356598\n",
            "strain 0.15550443530082703\n",
            "strain 0.13836733996868134\n",
            "strain 0.25585874915122986\n",
            "strain 0.13660892844200134\n",
            "classify 1.3336181640625\n",
            "classify 1.35711669921875\n",
            "classify 1.348876953125\n",
            "classify 1.31689453125\n",
            "classify 1.34375\n",
            "0.28125\n",
            "0.265625\n",
            "0.265625\n",
            "0.359375\n",
            "0.42857142857142855\n",
            "strain 0.19917722046375275\n",
            "strain 0.13812053203582764\n",
            "strain 0.15757331252098083\n",
            "strain 0.13975082337856293\n",
            "strain 0.06989093869924545\n",
            "classify 1.34228515625\n",
            "classify 1.2960205078125\n",
            "classify 1.3690185546875\n",
            "classify 1.34649658203125\n",
            "classify 1.3406250476837158\n",
            "0.375\n",
            "0.328125\n",
            "0.234375\n",
            "0.203125\n",
            "0.0\n",
            "strain 0.15382325649261475\n",
            "strain 0.25489455461502075\n",
            "strain 0.07086716592311859\n",
            "strain 0.1562327891588211\n",
            "strain 0.09685515612363815\n",
            "classify 1.3509521484375\n",
            "classify 1.3343505859375\n",
            "classify 1.3408203125\n",
            "classify 1.335693359375\n",
            "classify 1.328125\n",
            "0.390625\n",
            "0.25\n",
            "0.234375\n",
            "0.234375\n",
            "0.42857142857142855\n",
            "strain 0.18701834976673126\n",
            "strain 0.20491091907024384\n",
            "strain 0.16833241283893585\n",
            "strain 0.23473598062992096\n",
            "strain 0.26380741596221924\n",
            "classify 1.337646484375\n",
            "classify 1.3502197265625\n",
            "classify 1.32928466796875\n",
            "classify 1.344970703125\n",
            "classify 1.4093749523162842\n",
            "0.25\n",
            "0.359375\n",
            "0.265625\n",
            "0.328125\n",
            "0.42857142857142855\n",
            "strain 0.1901971995830536\n",
            "strain 0.08192433416843414\n",
            "strain 0.11296456307172775\n",
            "strain 0.19282031059265137\n",
            "strain 0.1107027679681778\n",
            "classify 1.32867431640625\n",
            "classify 1.370361328125\n",
            "classify 1.3182373046875\n",
            "classify 1.3524169921875\n",
            "classify 1.28125\n",
            "0.265625\n",
            "0.296875\n",
            "0.375\n",
            "0.25\n",
            "0.0\n",
            "strain 0.15454450249671936\n",
            "strain 0.19170218706130981\n",
            "strain 0.22134074568748474\n",
            "strain 0.14667615294456482\n",
            "strain 0.0644291341304779\n",
            "classify 1.354736328125\n",
            "classify 1.32330322265625\n",
            "classify 1.3370361328125\n",
            "classify 1.3446044921875\n",
            "classify 1.373437523841858\n",
            "0.28125\n",
            "0.21875\n",
            "0.296875\n",
            "0.3125\n",
            "0.14285714285714285\n",
            "strain 0.21953347325325012\n",
            "strain 0.13412122428417206\n",
            "strain 0.14646366238594055\n",
            "strain 0.13284434378147125\n",
            "strain 0.12712769210338593\n",
            "classify 1.312744140625\n",
            "classify 1.358642578125\n",
            "classify 1.3505859375\n",
            "classify 1.3515625\n",
            "classify 1.2859375476837158\n",
            "0.359375\n",
            "0.359375\n",
            "0.234375\n",
            "0.203125\n",
            "0.2857142857142857\n",
            "strain 0.22752416133880615\n",
            "strain 0.269723117351532\n",
            "strain 0.11700800061225891\n",
            "strain 0.21927104890346527\n",
            "strain 0.05591895058751106\n",
            "classify 1.35546875\n",
            "classify 1.32373046875\n",
            "classify 1.33599853515625\n",
            "classify 1.36224365234375\n",
            "classify 1.298437476158142\n",
            "0.28125\n",
            "0.296875\n",
            "0.25\n",
            "0.296875\n",
            "0.14285714285714285\n",
            "strain 0.16723743081092834\n",
            "strain 0.17734481394290924\n",
            "strain 0.19528546929359436\n",
            "strain 0.1786230206489563\n",
            "strain 0.18754476308822632\n",
            "classify 1.31756591796875\n",
            "classify 1.35260009765625\n",
            "classify 1.36767578125\n",
            "classify 1.32763671875\n",
            "classify 1.365625023841858\n",
            "0.375\n",
            "0.375\n",
            "0.25\n",
            "0.21875\n",
            "0.14285714285714285\n",
            "strain 0.17976810038089752\n",
            "strain 0.22936810553073883\n",
            "strain 0.1829395294189453\n",
            "strain 0.25388479232788086\n",
            "strain 0.05926032364368439\n",
            "classify 1.33978271484375\n",
            "classify 1.3441162109375\n",
            "classify 1.3385009765625\n",
            "classify 1.343017578125\n",
            "classify 1.3671875\n",
            "0.25\n",
            "0.21875\n",
            "0.390625\n",
            "0.265625\n",
            "0.2857142857142857\n",
            "strain 0.14644810557365417\n",
            "strain 0.15171733498573303\n",
            "strain 0.2017572671175003\n",
            "strain 0.08705784380435944\n",
            "strain 0.03971495106816292\n",
            "classify 1.34735107421875\n",
            "classify 1.360107421875\n",
            "classify 1.31854248046875\n",
            "classify 1.3538818359375\n",
            "classify 1.1828124523162842\n",
            "0.25\n",
            "0.328125\n",
            "0.296875\n",
            "0.28125\n",
            "0.14285714285714285\n",
            "strain 0.23833239078521729\n",
            "strain 0.19773408770561218\n",
            "strain 0.21113891899585724\n",
            "strain 0.1476089507341385\n",
            "strain 0.116811104118824\n",
            "classify 1.3717041015625\n",
            "classify 1.340576171875\n",
            "classify 1.34130859375\n",
            "classify 1.3177490234375\n",
            "classify 1.2374999523162842\n",
            "0.296875\n",
            "0.28125\n",
            "0.25\n",
            "0.296875\n",
            "0.42857142857142855\n",
            "strain 0.14915822446346283\n",
            "strain 0.21941664814949036\n",
            "strain 0.19521014392375946\n",
            "strain 0.13510486483573914\n",
            "strain 0.09254316985607147\n",
            "classify 1.3638916015625\n",
            "classify 1.305419921875\n",
            "classify 1.3394775390625\n",
            "classify 1.35504150390625\n",
            "classify 1.384374976158142\n",
            "0.3125\n",
            "0.3125\n",
            "0.265625\n",
            "0.203125\n",
            "0.42857142857142855\n",
            "strain 0.1257176697254181\n",
            "strain 0.2103772610425949\n",
            "strain 0.2535047233104706\n",
            "strain 0.2147207111120224\n",
            "strain 0.32293280959129333\n",
            "classify 1.32666015625\n",
            "classify 1.3363037109375\n",
            "classify 1.3446044921875\n",
            "classify 1.35198974609375\n",
            "classify 1.373437523841858\n",
            "0.265625\n",
            "0.25\n",
            "0.234375\n",
            "0.390625\n",
            "0.42857142857142855\n",
            "strain 0.2852286398410797\n",
            "strain 0.2396056056022644\n",
            "strain 0.11851391196250916\n",
            "strain 0.2085803896188736\n",
            "strain 0.17299185693264008\n",
            "classify 1.359375\n",
            "classify 1.33544921875\n",
            "classify 1.3184814453125\n",
            "classify 1.34686279296875\n",
            "classify 1.3468749523162842\n",
            "0.328125\n",
            "0.203125\n",
            "0.359375\n",
            "0.28125\n",
            "0.2857142857142857\n",
            "strain 0.19001884758472443\n",
            "strain 0.18947820365428925\n",
            "strain 0.1993720829486847\n",
            "strain 0.23051020503044128\n",
            "strain 0.058111000806093216\n",
            "classify 1.32952880859375\n",
            "classify 1.355224609375\n",
            "classify 1.34100341796875\n",
            "classify 1.32672119140625\n",
            "classify 1.4109375476837158\n",
            "0.265625\n",
            "0.21875\n",
            "0.296875\n",
            "0.40625\n",
            "0.2857142857142857\n",
            "strain 0.14927884936332703\n",
            "strain 0.15510129928588867\n",
            "strain 0.16497887670993805\n",
            "strain 0.13801053166389465\n",
            "strain 0.22204479575157166\n",
            "classify 1.31512451171875\n",
            "classify 1.34130859375\n",
            "classify 1.3582763671875\n",
            "classify 1.343017578125\n",
            "classify 1.399999976158142\n",
            "0.21875\n",
            "0.234375\n",
            "0.359375\n",
            "0.375\n",
            "0.0\n",
            "strain 0.13693305850028992\n",
            "strain 0.22855335474014282\n",
            "strain 0.11015903949737549\n",
            "strain 0.21152909100055695\n",
            "strain 0.049193862825632095\n",
            "classify 1.335205078125\n",
            "classify 1.327392578125\n",
            "classify 1.34088134765625\n",
            "classify 1.3612060546875\n",
            "classify 1.365625023841858\n",
            "0.1875\n",
            "0.265625\n",
            "0.421875\n",
            "0.34375\n",
            "0.2857142857142857\n",
            "strain 0.0923757404088974\n",
            "strain 0.17287229001522064\n",
            "strain 0.20701295137405396\n",
            "strain 0.1292993277311325\n",
            "strain 0.03975541889667511\n",
            "classify 1.350830078125\n",
            "classify 1.3251953125\n",
            "classify 1.3564453125\n",
            "classify 1.33441162109375\n",
            "classify 1.3015625476837158\n",
            "0.265625\n",
            "0.359375\n",
            "0.265625\n",
            "0.296875\n",
            "0.14285714285714285\n",
            "strain 0.22044074535369873\n",
            "strain 0.18795786798000336\n",
            "strain 0.15767265856266022\n",
            "strain 0.20371569693088531\n",
            "strain 0.04926479980349541\n",
            "classify 1.32373046875\n",
            "classify 1.3443603515625\n",
            "classify 1.3345947265625\n",
            "classify 1.3524169921875\n",
            "classify 1.3640625476837158\n",
            "0.34375\n",
            "0.265625\n",
            "0.328125\n",
            "0.234375\n",
            "0.5714285714285714\n",
            "strain 0.18608444929122925\n",
            "strain 0.18492408096790314\n",
            "strain 0.13392753899097443\n",
            "strain 0.1152656227350235\n",
            "strain 0.09868664294481277\n",
            "classify 1.3831787109375\n",
            "classify 1.3172607421875\n",
            "classify 1.31646728515625\n",
            "classify 1.3360595703125\n",
            "classify 1.399999976158142\n",
            "0.296875\n",
            "0.28125\n",
            "0.296875\n",
            "0.359375\n",
            "0.42857142857142855\n",
            "strain 0.14540612697601318\n",
            "strain 0.22152847051620483\n",
            "strain 0.22129544615745544\n",
            "strain 0.1179201677441597\n",
            "strain 0.23498287796974182\n",
            "classify 1.34613037109375\n",
            "classify 1.304931640625\n",
            "classify 1.354248046875\n",
            "classify 1.3621826171875\n",
            "classify 1.310937523841858\n",
            "0.296875\n",
            "0.34375\n",
            "0.25\n",
            "0.1875\n",
            "0.5714285714285714\n",
            "strain 0.1483851671218872\n",
            "strain 0.268871545791626\n",
            "strain 0.17822924256324768\n",
            "strain 0.23047570884227753\n",
            "strain 0.07285618782043457\n",
            "classify 1.33648681640625\n",
            "classify 1.3419189453125\n",
            "classify 1.3514404296875\n",
            "classify 1.3367919921875\n",
            "classify 1.3562500476837158\n",
            "0.296875\n",
            "0.296875\n",
            "0.296875\n",
            "0.296875\n",
            "0.14285714285714285\n",
            "strain 0.10368761420249939\n",
            "strain 0.21210555732250214\n",
            "strain 0.17731545865535736\n",
            "strain 0.1958700716495514\n",
            "strain 0.12713542580604553\n",
            "classify 1.3526611328125\n",
            "classify 1.342529296875\n",
            "classify 1.3568115234375\n",
            "classify 1.3172607421875\n",
            "classify 1.368749976158142\n",
            "0.234375\n",
            "0.328125\n",
            "0.265625\n",
            "0.328125\n",
            "0.2857142857142857\n",
            "strain 0.1813787817955017\n",
            "strain 0.20813482999801636\n",
            "strain 0.19417539238929749\n",
            "strain 0.1410718411207199\n",
            "strain 0.03833058848977089\n",
            "classify 1.332275390625\n",
            "classify 1.3492431640625\n",
            "classify 1.3226318359375\n",
            "classify 1.3572998046875\n",
            "classify 1.3156249523162842\n",
            "0.265625\n",
            "0.203125\n",
            "0.34375\n",
            "0.25\n",
            "0.14285714285714285\n",
            "strain 0.21161532402038574\n",
            "strain 0.15519563853740692\n",
            "strain 0.21508871018886566\n",
            "strain 0.1549886018037796\n",
            "strain 0.12447509169578552\n",
            "classify 1.33782958984375\n",
            "classify 1.3201904296875\n",
            "classify 1.34796142578125\n",
            "classify 1.3514404296875\n",
            "classify 1.2578125\n",
            "0.171875\n",
            "0.296875\n",
            "0.28125\n",
            "0.3125\n",
            "0.14285714285714285\n",
            "strain 0.15709467232227325\n",
            "strain 0.23487316071987152\n",
            "strain 0.2648083567619324\n",
            "strain 0.16308918595314026\n",
            "strain 0.07119391858577728\n",
            "classify 1.341064453125\n",
            "classify 1.30255126953125\n",
            "classify 1.3448486328125\n",
            "classify 1.36468505859375\n",
            "classify 1.353124976158142\n",
            "0.21875\n",
            "0.28125\n",
            "0.296875\n",
            "0.265625\n",
            "0.14285714285714285\n",
            "strain 0.11626444011926651\n",
            "strain 0.24717730283737183\n",
            "strain 0.15062561631202698\n",
            "strain 0.18465788662433624\n",
            "strain 0.04959006607532501\n",
            "classify 1.33087158203125\n",
            "classify 1.3167724609375\n",
            "classify 1.3541259765625\n",
            "classify 1.3431396484375\n",
            "classify 1.3859374523162842\n",
            "0.15625\n",
            "0.3125\n",
            "0.328125\n",
            "0.359375\n",
            "0.14285714285714285\n",
            "strain 0.17616154253482819\n",
            "strain 0.26763108372688293\n",
            "strain 0.1937035769224167\n",
            "strain 0.21883051097393036\n",
            "strain 0.1127246543765068\n",
            "classify 1.353759765625\n",
            "classify 1.30340576171875\n",
            "classify 1.33148193359375\n",
            "classify 1.35858154296875\n",
            "classify 1.3671875\n",
            "0.234375\n",
            "0.203125\n",
            "0.34375\n",
            "0.296875\n",
            "0.5714285714285714\n",
            "strain 0.14116457104682922\n",
            "strain 0.285584956407547\n",
            "strain 0.16072630882263184\n",
            "strain 0.2726965546607971\n",
            "strain 0.04930909350514412\n",
            "classify 1.299560546875\n",
            "classify 1.32745361328125\n",
            "classify 1.36090087890625\n",
            "classify 1.35595703125\n",
            "classify 1.4031250476837158\n",
            "0.203125\n",
            "0.359375\n",
            "0.265625\n",
            "0.265625\n",
            "0.2857142857142857\n",
            "strain 0.15406164526939392\n",
            "strain 0.1314895749092102\n",
            "strain 0.17544026672840118\n",
            "strain 0.18497388064861298\n",
            "strain 0.05870947241783142\n",
            "classify 1.36981201171875\n",
            "classify 1.30096435546875\n",
            "classify 1.33001708984375\n",
            "classify 1.35040283203125\n",
            "classify 1.2109375\n",
            "0.359375\n",
            "0.328125\n",
            "0.234375\n",
            "0.234375\n",
            "0.0\n",
            "strain 0.12283538281917572\n",
            "strain 0.1957707405090332\n",
            "strain 0.13109250366687775\n",
            "strain 0.18160779774188995\n",
            "strain 0.1549306958913803\n",
            "classify 1.35723876953125\n",
            "classify 1.325439453125\n",
            "classify 1.302978515625\n",
            "classify 1.34649658203125\n",
            "classify 1.5187499523162842\n",
            "0.28125\n",
            "0.34375\n",
            "0.3125\n",
            "0.296875\n",
            "0.14285714285714285\n",
            "strain 0.23023086786270142\n",
            "strain 0.1288185566663742\n",
            "strain 0.13482466340065002\n",
            "strain 0.16779808700084686\n",
            "strain 0.2048727571964264\n",
            "classify 1.34771728515625\n",
            "classify 1.329345703125\n",
            "classify 1.314697265625\n",
            "classify 1.36541748046875\n",
            "classify 1.328125\n",
            "0.328125\n",
            "0.25\n",
            "0.34375\n",
            "0.3125\n",
            "0.2857142857142857\n",
            "strain 0.16621653735637665\n",
            "strain 0.17072060704231262\n",
            "strain 0.14342275261878967\n",
            "strain 0.1565510779619217\n",
            "strain 0.04354703798890114\n",
            "classify 1.33392333984375\n",
            "classify 1.3541259765625\n",
            "classify 1.326171875\n",
            "classify 1.3306884765625\n",
            "classify 1.4031250476837158\n",
            "0.28125\n",
            "0.3125\n",
            "0.3125\n",
            "0.21875\n",
            "0.42857142857142855\n",
            "strain 0.24051302671432495\n",
            "strain 0.14087654650211334\n",
            "strain 0.20956867933273315\n",
            "strain 0.1791778802871704\n",
            "strain 0.1862599104642868\n",
            "classify 1.3370361328125\n",
            "classify 1.3353271484375\n",
            "classify 1.36077880859375\n",
            "classify 1.312744140625\n",
            "classify 1.2882812023162842\n",
            "0.28125\n",
            "0.34375\n",
            "0.265625\n",
            "0.28125\n",
            "0.2857142857142857\n",
            "strain 0.21776455640792847\n",
            "strain 0.13565005362033844\n",
            "strain 0.17801575362682343\n",
            "strain 0.0848749503493309\n",
            "strain 0.04077720642089844\n",
            "classify 1.327880859375\n",
            "classify 1.3369140625\n",
            "classify 1.34375\n",
            "classify 1.32537841796875\n",
            "classify 1.412500023841858\n",
            "0.1875\n",
            "0.234375\n",
            "0.359375\n",
            "0.390625\n",
            "0.2857142857142857\n",
            "strain 0.15713830292224884\n",
            "strain 0.20250652730464935\n",
            "strain 0.21284446120262146\n",
            "strain 0.21489444375038147\n",
            "strain 0.06854244321584702\n",
            "classify 1.363525390625\n",
            "classify 1.34027099609375\n",
            "classify 1.31951904296875\n",
            "classify 1.313720703125\n",
            "classify 1.368749976158142\n",
            "0.328125\n",
            "0.234375\n",
            "0.328125\n",
            "0.28125\n",
            "0.14285714285714285\n",
            "strain 0.2371743768453598\n",
            "strain 0.17645291984081268\n",
            "strain 0.16443435847759247\n",
            "strain 0.20002034306526184\n",
            "strain 0.03775480017066002\n",
            "classify 1.30889892578125\n",
            "classify 1.3355712890625\n",
            "classify 1.31939697265625\n",
            "classify 1.37677001953125\n",
            "classify 1.2578125\n",
            "0.234375\n",
            "0.21875\n",
            "0.3125\n",
            "0.328125\n",
            "0.2857142857142857\n",
            "strain 0.2349395453929901\n",
            "strain 0.13944096863269806\n",
            "strain 0.2261073887348175\n",
            "strain 0.16511350870132446\n",
            "strain 0.3190757930278778\n",
            "classify 1.3428955078125\n",
            "classify 1.32232666015625\n",
            "classify 1.3408203125\n",
            "classify 1.3416748046875\n",
            "classify 1.1492187976837158\n",
            "0.234375\n",
            "0.234375\n",
            "0.296875\n",
            "0.34375\n",
            "0.14285714285714285\n",
            "strain 0.1062866821885109\n",
            "strain 0.20255416631698608\n",
            "strain 0.10410384833812714\n",
            "strain 0.3050207495689392\n",
            "strain 0.05752778798341751\n",
            "classify 1.3477783203125\n",
            "classify 1.3265380859375\n",
            "classify 1.31671142578125\n",
            "classify 1.353271484375\n",
            "classify 1.2414062023162842\n",
            "0.265625\n",
            "0.296875\n",
            "0.234375\n",
            "0.3125\n",
            "0.14285714285714285\n",
            "strain 0.12445378303527832\n",
            "strain 0.1400040090084076\n",
            "strain 0.21534521877765656\n",
            "strain 0.23889894783496857\n",
            "strain 0.07865355163812637\n",
            "classify 1.3218994140625\n",
            "classify 1.33660888671875\n",
            "classify 1.35113525390625\n",
            "classify 1.3428955078125\n",
            "classify 1.2078125476837158\n",
            "0.25\n",
            "0.3125\n",
            "0.296875\n",
            "0.25\n",
            "0.2857142857142857\n",
            "strain 0.22594903409481049\n",
            "strain 0.16274228692054749\n",
            "strain 0.0969974622130394\n",
            "strain 0.25919684767723083\n",
            "strain 0.03646346181631088\n",
            "classify 1.35089111328125\n",
            "classify 1.3037109375\n",
            "classify 1.33453369140625\n",
            "classify 1.3419189453125\n",
            "classify 1.423437476158142\n",
            "0.28125\n",
            "0.328125\n",
            "0.25\n",
            "0.28125\n",
            "0.2857142857142857\n",
            "strain 0.15709368884563446\n",
            "strain 0.21174922585487366\n",
            "strain 0.11310632526874542\n",
            "strain 0.19503577053546906\n",
            "strain 0.03284039720892906\n",
            "classify 1.338134765625\n",
            "classify 1.34027099609375\n",
            "classify 1.3114013671875\n",
            "classify 1.35986328125\n",
            "classify 1.267968773841858\n",
            "0.328125\n",
            "0.28125\n",
            "0.234375\n",
            "0.265625\n",
            "0.2857142857142857\n",
            "strain 0.10439307987689972\n",
            "strain 0.17710062861442566\n",
            "strain 0.1999605894088745\n",
            "strain 0.20028221607208252\n",
            "strain 0.12893103063106537\n",
            "classify 1.30047607421875\n",
            "classify 1.35369873046875\n",
            "classify 1.35052490234375\n",
            "classify 1.3426513671875\n",
            "classify 1.321874976158142\n",
            "0.25\n",
            "0.25\n",
            "0.296875\n",
            "0.390625\n",
            "0.2857142857142857\n",
            "strain 0.12169086933135986\n",
            "strain 0.20538899302482605\n",
            "strain 0.10400255024433136\n",
            "strain 0.14401790499687195\n",
            "strain 0.1497277021408081\n",
            "classify 1.29034423828125\n",
            "classify 1.34527587890625\n",
            "classify 1.35980224609375\n",
            "classify 1.3555908203125\n",
            "classify 1.3171875476837158\n",
            "0.234375\n",
            "0.296875\n",
            "0.34375\n",
            "0.28125\n",
            "0.42857142857142855\n",
            "strain 0.10654699802398682\n",
            "strain 0.1183677539229393\n",
            "strain 0.214455246925354\n",
            "strain 0.13902291655540466\n",
            "strain 0.1035870611667633\n",
            "classify 1.31939697265625\n",
            "classify 1.3472900390625\n",
            "classify 1.36883544921875\n",
            "classify 1.32318115234375\n",
            "classify 1.3468749523162842\n",
            "0.359375\n",
            "0.25\n",
            "0.28125\n",
            "0.34375\n",
            "0.0\n",
            "strain 0.11221686005592346\n",
            "strain 0.1863047331571579\n",
            "strain 0.2310894876718521\n",
            "strain 0.2375955879688263\n",
            "strain 0.12448284775018692\n",
            "classify 1.330810546875\n",
            "classify 1.34991455078125\n",
            "classify 1.311279296875\n",
            "classify 1.349853515625\n",
            "classify 1.3718750476837158\n",
            "0.328125\n",
            "0.21875\n",
            "0.28125\n",
            "0.328125\n",
            "0.2857142857142857\n",
            "strain 0.09779830276966095\n",
            "strain 0.1498822420835495\n",
            "strain 0.25755831599235535\n",
            "strain 0.2282431572675705\n",
            "strain 0.12966786324977875\n",
            "classify 1.34423828125\n",
            "classify 1.35626220703125\n",
            "classify 1.298583984375\n",
            "classify 1.33984375\n",
            "classify 1.303125023841858\n",
            "0.28125\n",
            "0.328125\n",
            "0.234375\n",
            "0.296875\n",
            "0.42857142857142855\n",
            "strain 0.16011548042297363\n",
            "strain 0.20355699956417084\n",
            "strain 0.12550216913223267\n",
            "strain 0.15983539819717407\n",
            "strain 0.04301053658127785\n",
            "classify 1.3509521484375\n",
            "classify 1.2930908203125\n",
            "classify 1.3438720703125\n",
            "classify 1.34515380859375\n",
            "classify 1.4187500476837158\n",
            "0.265625\n",
            "0.28125\n",
            "0.25\n",
            "0.265625\n",
            "0.2857142857142857\n",
            "strain 0.13359037041664124\n",
            "strain 0.1319027990102768\n",
            "strain 0.2384154051542282\n",
            "strain 0.11169323325157166\n",
            "strain 0.03788979351520538\n",
            "classify 1.32366943359375\n",
            "classify 1.363525390625\n",
            "classify 1.31610107421875\n",
            "classify 1.3424072265625\n",
            "classify 1.295312523841858\n",
            "0.203125\n",
            "0.3125\n",
            "0.265625\n",
            "0.328125\n",
            "0.14285714285714285\n",
            "strain 0.17012207210063934\n",
            "strain 0.19238436222076416\n",
            "strain 0.20277062058448792\n",
            "strain 0.2421247810125351\n",
            "strain 0.0468936488032341\n",
            "classify 1.32269287109375\n",
            "classify 1.36285400390625\n",
            "classify 1.304931640625\n",
            "classify 1.3563232421875\n",
            "classify 1.2999999523162842\n",
            "0.40625\n",
            "0.203125\n",
            "0.3125\n",
            "0.21875\n",
            "0.14285714285714285\n",
            "strain 0.21570707857608795\n",
            "strain 0.18232092261314392\n",
            "strain 0.25012195110321045\n",
            "strain 0.17495863139629364\n",
            "strain 0.052411600947380066\n",
            "classify 1.3515625\n",
            "classify 1.36065673828125\n",
            "classify 1.3154296875\n",
            "classify 1.31610107421875\n",
            "classify 1.3250000476837158\n",
            "0.21875\n",
            "0.359375\n",
            "0.25\n",
            "0.28125\n",
            "0.2857142857142857\n",
            "strain 0.17921984195709229\n",
            "strain 0.29378071427345276\n",
            "strain 0.22378867864608765\n",
            "strain 0.19436998665332794\n",
            "strain 0.0604538656771183\n",
            "classify 1.32000732421875\n",
            "classify 1.2991943359375\n",
            "classify 1.37969970703125\n",
            "classify 1.33477783203125\n",
            "classify 1.412500023841858\n",
            "0.359375\n",
            "0.28125\n",
            "0.328125\n",
            "0.203125\n",
            "0.0\n",
            "strain 0.1868065893650055\n",
            "strain 0.15284329652786255\n",
            "strain 0.18283528089523315\n",
            "strain 0.2301676720380783\n",
            "strain 0.06067093834280968\n",
            "classify 1.31451416015625\n",
            "classify 1.3477783203125\n",
            "classify 1.3594970703125\n",
            "classify 1.3265380859375\n",
            "classify 1.21484375\n",
            "0.21875\n",
            "0.25\n",
            "0.296875\n",
            "0.296875\n",
            "0.2857142857142857\n",
            "strain 0.13809475302696228\n",
            "strain 0.11432291567325592\n",
            "strain 0.13145487010478973\n",
            "strain 0.2546185255050659\n",
            "strain 0.054195694625377655\n",
            "classify 1.34088134765625\n",
            "classify 1.34844970703125\n",
            "classify 1.3311767578125\n",
            "classify 1.33270263671875\n",
            "classify 1.243749976158142\n",
            "0.3125\n",
            "0.21875\n",
            "0.359375\n",
            "0.28125\n",
            "0.42857142857142855\n",
            "strain 0.20190519094467163\n",
            "strain 0.15232978761196136\n",
            "strain 0.14202730357646942\n",
            "strain 0.16538266837596893\n",
            "strain 0.23656852543354034\n",
            "classify 1.3243408203125\n",
            "classify 1.32421875\n",
            "classify 1.35821533203125\n",
            "classify 1.331787109375\n",
            "classify 1.3250000476837158\n",
            "0.25\n",
            "0.265625\n",
            "0.296875\n",
            "0.28125\n",
            "0.2857142857142857\n",
            "strain 0.20364440977573395\n",
            "strain 0.13155940175056458\n",
            "strain 0.22112855315208435\n",
            "strain 0.15797093510627747\n",
            "strain 0.17213577032089233\n",
            "classify 1.3504638671875\n",
            "classify 1.35809326171875\n",
            "classify 1.30108642578125\n",
            "classify 1.316162109375\n",
            "classify 1.4812500476837158\n",
            "0.203125\n",
            "0.34375\n",
            "0.203125\n",
            "0.265625\n",
            "0.7142857142857143\n",
            "strain 0.1523105651140213\n",
            "strain 0.1850641518831253\n",
            "strain 0.23449687659740448\n",
            "strain 0.2508457899093628\n",
            "strain 0.06667491793632507\n",
            "classify 1.35235595703125\n",
            "classify 1.3389892578125\n",
            "classify 1.315673828125\n",
            "classify 1.33001708984375\n",
            "classify 1.420312523841858\n",
            "0.234375\n",
            "0.265625\n",
            "0.3125\n",
            "0.3125\n",
            "0.2857142857142857\n",
            "strain 0.21280385553836823\n",
            "strain 0.17243929207324982\n",
            "strain 0.0949561819434166\n",
            "strain 0.2871413230895996\n",
            "strain 0.1745835691690445\n",
            "classify 1.33697509765625\n",
            "classify 1.3251953125\n",
            "classify 1.34375\n",
            "classify 1.32684326171875\n",
            "classify 1.390625\n",
            "0.328125\n",
            "0.203125\n",
            "0.25\n",
            "0.3125\n",
            "0.2857142857142857\n",
            "strain 0.16770027577877045\n",
            "strain 0.21182982623577118\n",
            "strain 0.23046590387821198\n",
            "strain 0.2092665433883667\n",
            "strain 0.040768396109342575\n",
            "classify 1.33331298828125\n",
            "classify 1.354248046875\n",
            "classify 1.32403564453125\n",
            "classify 1.32275390625\n",
            "classify 1.337499976158142\n",
            "0.1875\n",
            "0.359375\n",
            "0.296875\n",
            "0.234375\n",
            "0.2857142857142857\n",
            "strain 0.16600409150123596\n",
            "strain 0.16839313507080078\n",
            "strain 0.2093413919210434\n",
            "strain 0.19852331280708313\n",
            "strain 0.12154506891965866\n",
            "classify 1.30328369140625\n",
            "classify 1.33636474609375\n",
            "classify 1.33685302734375\n",
            "classify 1.3621826171875\n",
            "classify 1.376562476158142\n",
            "0.203125\n",
            "0.25\n",
            "0.296875\n",
            "0.3125\n",
            "0.2857142857142857\n",
            "strain 0.14243203401565552\n",
            "strain 0.1736484169960022\n",
            "strain 0.17859506607055664\n",
            "strain 0.2442014217376709\n",
            "strain 0.08373861014842987\n",
            "classify 1.31671142578125\n",
            "classify 1.3521728515625\n",
            "classify 1.329833984375\n",
            "classify 1.328857421875\n",
            "classify 1.399999976158142\n",
            "0.296875\n",
            "0.328125\n",
            "0.25\n",
            "0.234375\n",
            "0.2857142857142857\n",
            "strain 0.15175491571426392\n",
            "strain 0.18481038510799408\n",
            "strain 0.21528775990009308\n",
            "strain 0.26258596777915955\n",
            "strain 0.28626662492752075\n",
            "classify 1.33746337890625\n",
            "classify 1.3138427734375\n",
            "classify 1.32720947265625\n",
            "classify 1.33941650390625\n",
            "classify 1.3328125476837158\n",
            "0.265625\n",
            "0.203125\n",
            "0.296875\n",
            "0.296875\n",
            "0.42857142857142855\n",
            "strain 0.2897426187992096\n",
            "strain 0.12259262800216675\n",
            "strain 0.21756407618522644\n",
            "strain 0.12811507284641266\n",
            "strain 0.098721444606781\n",
            "classify 1.32330322265625\n",
            "classify 1.35284423828125\n",
            "classify 1.32958984375\n",
            "classify 1.315185546875\n",
            "classify 1.2687499523162842\n",
            "0.328125\n",
            "0.25\n",
            "0.296875\n",
            "0.25\n",
            "0.14285714285714285\n",
            "strain 0.21854834258556366\n",
            "strain 0.13112090528011322\n",
            "strain 0.20335350930690765\n",
            "strain 0.21497130393981934\n",
            "strain 0.06178457289934158\n",
            "classify 1.3377685546875\n",
            "classify 1.3211669921875\n",
            "classify 1.344970703125\n",
            "classify 1.32373046875\n",
            "classify 1.25\n",
            "0.3125\n",
            "0.234375\n",
            "0.3125\n",
            "0.28125\n",
            "0.5714285714285714\n",
            "strain 0.23040108382701874\n",
            "strain 0.13381551206111908\n",
            "strain 0.2131105363368988\n",
            "strain 0.10154709219932556\n",
            "strain 0.1644253432750702\n",
            "classify 1.31964111328125\n",
            "classify 1.32745361328125\n",
            "classify 1.3084716796875\n",
            "classify 1.36090087890625\n",
            "classify 1.392187476158142\n",
            "0.296875\n",
            "0.1875\n",
            "0.359375\n",
            "0.265625\n",
            "0.42857142857142855\n",
            "strain 0.17769254744052887\n",
            "strain 0.1364295929670334\n",
            "strain 0.181305930018425\n",
            "strain 0.19530494511127472\n",
            "strain 0.2812952995300293\n",
            "classify 1.336669921875\n",
            "classify 1.323486328125\n",
            "classify 1.33502197265625\n",
            "classify 1.3140869140625\n",
            "classify 1.3390624523162842\n",
            "0.34375\n",
            "0.25\n",
            "0.265625\n",
            "0.3125\n",
            "0.2857142857142857\n",
            "strain 0.2815049886703491\n",
            "strain 0.095374695956707\n",
            "strain 0.17852428555488586\n",
            "strain 0.1803307682275772\n",
            "strain 0.04415594041347504\n",
            "classify 1.35064697265625\n",
            "classify 1.34637451171875\n",
            "classify 1.29705810546875\n",
            "classify 1.33282470703125\n",
            "classify 1.34375\n",
            "0.34375\n",
            "0.234375\n",
            "0.34375\n",
            "0.203125\n",
            "0.5714285714285714\n",
            "strain 0.20170828700065613\n",
            "strain 0.13902926445007324\n",
            "strain 0.21878105401992798\n",
            "strain 0.2541281282901764\n",
            "strain 0.10490904003381729\n",
            "classify 1.29241943359375\n",
            "classify 1.34075927734375\n",
            "classify 1.3636474609375\n",
            "classify 1.34326171875\n",
            "classify 1.359375\n",
            "0.1875\n",
            "0.359375\n",
            "0.328125\n",
            "0.28125\n",
            "0.2857142857142857\n",
            "strain 0.17372915148735046\n",
            "strain 0.2123749852180481\n",
            "strain 0.22232814133167267\n",
            "strain 0.21177935600280762\n",
            "strain 0.11077085882425308\n",
            "classify 1.335205078125\n",
            "classify 1.3055419921875\n",
            "classify 1.3262939453125\n",
            "classify 1.35986328125\n",
            "classify 1.296875\n",
            "0.390625\n",
            "0.296875\n",
            "0.265625\n",
            "0.265625\n",
            "0.0\n",
            "strain 0.20627863705158234\n",
            "strain 0.15471151471138\n",
            "strain 0.23128433525562286\n",
            "strain 0.13648980855941772\n",
            "strain 0.12746547162532806\n",
            "classify 1.3416748046875\n",
            "classify 1.32745361328125\n",
            "classify 1.315673828125\n",
            "classify 1.3441162109375\n",
            "classify 1.1749999523162842\n",
            "0.25\n",
            "0.296875\n",
            "0.28125\n",
            "0.3125\n",
            "0.7142857142857143\n",
            "strain 0.22155079245567322\n",
            "strain 0.19184798002243042\n",
            "strain 0.2017621099948883\n",
            "strain 0.1167973205447197\n",
            "strain 0.08218371123075485\n",
            "classify 1.3292236328125\n",
            "classify 1.3118896484375\n",
            "classify 1.36456298828125\n",
            "classify 1.3037109375\n",
            "classify 1.459375023841858\n",
            "0.3125\n",
            "0.25\n",
            "0.28125\n",
            "0.28125\n",
            "0.2857142857142857\n",
            "strain 0.20094843208789825\n",
            "strain 0.14199614524841309\n",
            "strain 0.0718483179807663\n",
            "strain 0.16149452328681946\n",
            "strain 0.27215924859046936\n",
            "classify 1.36968994140625\n",
            "classify 1.32366943359375\n",
            "classify 1.309814453125\n",
            "classify 1.3154296875\n",
            "classify 1.278906226158142\n",
            "0.21875\n",
            "0.265625\n",
            "0.375\n",
            "0.3125\n",
            "0.2857142857142857\n",
            "strain 0.15192082524299622\n",
            "strain 0.10948149114847183\n",
            "strain 0.15185672044754028\n",
            "strain 0.18741333484649658\n",
            "strain 0.08137789368629456\n",
            "classify 1.33660888671875\n",
            "classify 1.33349609375\n",
            "classify 1.32891845703125\n",
            "classify 1.31646728515625\n",
            "classify 1.353124976158142\n",
            "0.3125\n",
            "0.296875\n",
            "0.203125\n",
            "0.359375\n",
            "0.7142857142857143\n",
            "strain 0.27472400665283203\n",
            "strain 0.2641903758049011\n",
            "strain 0.14819267392158508\n",
            "strain 0.14908678829669952\n",
            "strain 0.03436046093702316\n",
            "classify 1.37811279296875\n",
            "classify 1.32733154296875\n",
            "classify 1.31866455078125\n",
            "classify 1.30523681640625\n",
            "classify 1.1906249523162842\n",
            "0.3125\n",
            "0.234375\n",
            "0.203125\n",
            "0.375\n",
            "0.5714285714285714\n",
            "strain 0.2549052834510803\n",
            "strain 0.23798243701457977\n",
            "strain 0.19218404591083527\n",
            "strain 0.13021525740623474\n",
            "strain 0.1801094114780426\n",
            "classify 1.28973388671875\n",
            "classify 1.32220458984375\n",
            "classify 1.34967041015625\n",
            "classify 1.3553466796875\n",
            "classify 1.3328125476837158\n",
            "0.25\n",
            "0.296875\n",
            "0.21875\n",
            "0.359375\n",
            "0.42857142857142855\n",
            "strain 0.24251355230808258\n",
            "strain 0.1046084314584732\n",
            "strain 0.1687016487121582\n",
            "strain 0.16720181703567505\n",
            "strain 0.19337759912014008\n",
            "classify 1.2886962890625\n",
            "classify 1.33953857421875\n",
            "classify 1.359619140625\n",
            "classify 1.33575439453125\n",
            "classify 1.282812476158142\n",
            "0.25\n",
            "0.234375\n",
            "0.359375\n",
            "0.328125\n",
            "0.14285714285714285\n",
            "strain 0.14521940052509308\n",
            "strain 0.19179891049861908\n",
            "strain 0.2834121882915497\n",
            "strain 0.23483382165431976\n",
            "strain 0.05792449042201042\n",
            "classify 1.3369140625\n",
            "classify 1.31585693359375\n",
            "classify 1.33050537109375\n",
            "classify 1.32464599609375\n",
            "classify 1.381250023841858\n",
            "0.296875\n",
            "0.1875\n",
            "0.234375\n",
            "0.359375\n",
            "0.14285714285714285\n",
            "strain 0.1385103315114975\n",
            "strain 0.18373654782772064\n",
            "strain 0.10625939816236496\n",
            "strain 0.21572990715503693\n",
            "strain 0.16708636283874512\n",
            "classify 1.35662841796875\n",
            "classify 1.29010009765625\n",
            "classify 1.35601806640625\n",
            "classify 1.32330322265625\n",
            "classify 1.2890625\n",
            "0.3125\n",
            "0.234375\n",
            "0.359375\n",
            "0.28125\n",
            "0.0\n",
            "strain 0.24784743785858154\n",
            "strain 0.08967027813196182\n",
            "strain 0.18088507652282715\n",
            "strain 0.19655796885490417\n",
            "strain 0.1112956628203392\n",
            "classify 1.31878662109375\n",
            "classify 1.3331298828125\n",
            "classify 1.31585693359375\n",
            "classify 1.35565185546875\n",
            "classify 1.256250023841858\n",
            "0.328125\n",
            "0.234375\n",
            "0.328125\n",
            "0.25\n",
            "0.2857142857142857\n",
            "strain 0.1751180738210678\n",
            "strain 0.20697270333766937\n",
            "strain 0.17057740688323975\n",
            "strain 0.17267677187919617\n",
            "strain 0.05602273344993591\n",
            "classify 1.30377197265625\n",
            "classify 1.35528564453125\n",
            "classify 1.30059814453125\n",
            "classify 1.330078125\n",
            "classify 1.493749976158142\n",
            "0.28125\n",
            "0.328125\n",
            "0.296875\n",
            "0.234375\n",
            "0.0\n",
            "strain 0.16575761139392853\n",
            "strain 0.1865820288658142\n",
            "strain 0.24175967276096344\n",
            "strain 0.12664735317230225\n",
            "strain 0.09870236366987228\n",
            "classify 1.33038330078125\n",
            "classify 1.3009033203125\n",
            "classify 1.3248291015625\n",
            "classify 1.34661865234375\n",
            "classify 1.2703125476837158\n",
            "0.25\n",
            "0.296875\n",
            "0.25\n",
            "0.34375\n",
            "0.0\n",
            "strain 0.23638290166854858\n",
            "strain 0.1641356647014618\n",
            "strain 0.176829993724823\n",
            "strain 0.14410202205181122\n",
            "strain 0.03751782700419426\n",
            "classify 1.34393310546875\n",
            "classify 1.35406494140625\n",
            "classify 1.31402587890625\n",
            "classify 1.29339599609375\n",
            "classify 1.23828125\n",
            "0.28125\n",
            "0.328125\n",
            "0.3125\n",
            "0.3125\n",
            "0.2857142857142857\n",
            "strain 0.16421467065811157\n",
            "strain 0.2122873067855835\n",
            "strain 0.24820207059383392\n",
            "strain 0.13155116140842438\n",
            "strain 0.05222274735569954\n",
            "classify 1.347412109375\n",
            "classify 1.28338623046875\n",
            "classify 1.3699951171875\n",
            "classify 1.29180908203125\n",
            "classify 1.407812476158142\n",
            "0.28125\n",
            "0.265625\n",
            "0.203125\n",
            "0.34375\n",
            "0.42857142857142855\n",
            "strain 0.13112382590770721\n",
            "strain 0.1657448709011078\n",
            "strain 0.2298652082681656\n",
            "strain 0.16636361181735992\n",
            "strain 0.04311486706137657\n",
            "classify 1.36492919921875\n",
            "classify 1.324951171875\n",
            "classify 1.27850341796875\n",
            "classify 1.3175048828125\n",
            "classify 1.4874999523162842\n",
            "0.296875\n",
            "0.3125\n",
            "0.28125\n",
            "0.171875\n",
            "0.42857142857142855\n",
            "strain 0.11572011560201645\n",
            "strain 0.15396545827388763\n",
            "strain 0.14516636729240417\n",
            "strain 0.25620076060295105\n",
            "strain 0.045240215957164764\n",
            "classify 1.3070068359375\n",
            "classify 1.350830078125\n",
            "classify 1.3304443359375\n",
            "classify 1.3214111328125\n",
            "classify 1.2734375\n",
            "0.1875\n",
            "0.390625\n",
            "0.359375\n",
            "0.25\n",
            "0.14285714285714285\n",
            "strain 0.2235795557498932\n",
            "strain 0.16850966215133667\n",
            "strain 0.21839989721775055\n",
            "strain 0.11015044152736664\n",
            "strain 0.19383637607097626\n",
            "classify 1.3175048828125\n",
            "classify 1.33746337890625\n",
            "classify 1.3179931640625\n",
            "classify 1.34356689453125\n",
            "classify 1.170312523841858\n",
            "0.25\n",
            "0.265625\n",
            "0.34375\n",
            "0.25\n",
            "0.2857142857142857\n",
            "strain 0.15565378963947296\n",
            "strain 0.09054470807313919\n",
            "strain 0.09125737845897675\n",
            "strain 0.1442691832780838\n",
            "strain 0.04418208822607994\n",
            "classify 1.32684326171875\n",
            "classify 1.3275146484375\n",
            "classify 1.306640625\n",
            "classify 1.34686279296875\n",
            "classify 1.334375023841858\n",
            "0.328125\n",
            "0.296875\n",
            "0.28125\n",
            "0.234375\n",
            "0.0\n",
            "strain 0.21738596260547638\n",
            "strain 0.17949168384075165\n",
            "strain 0.13792642951011658\n",
            "strain 0.08706150949001312\n",
            "strain 0.10392261296510696\n",
            "classify 1.3294677734375\n",
            "classify 1.34307861328125\n",
            "classify 1.366455078125\n",
            "classify 1.2781982421875\n",
            "classify 1.3039062023162842\n",
            "0.234375\n",
            "0.265625\n",
            "0.265625\n",
            "0.390625\n",
            "0.2857142857142857\n",
            "strain 0.1868373155593872\n",
            "strain 0.18448370695114136\n",
            "strain 0.16269353032112122\n",
            "strain 0.1419648677110672\n",
            "strain 0.2645941972732544\n",
            "classify 1.32763671875\n",
            "classify 1.33135986328125\n",
            "classify 1.3277587890625\n",
            "classify 1.33319091796875\n",
            "classify 1.3328125476837158\n",
            "0.28125\n",
            "0.34375\n",
            "0.3125\n",
            "0.21875\n",
            "0.14285714285714285\n",
            "strain 0.14312753081321716\n",
            "strain 0.26930513978004456\n",
            "strain 0.16818803548812866\n",
            "strain 0.14326705038547516\n",
            "strain 0.11534032970666885\n",
            "classify 1.33905029296875\n",
            "classify 1.3560791015625\n",
            "classify 1.322509765625\n",
            "classify 1.30413818359375\n",
            "classify 1.3234374523162842\n",
            "0.4375\n",
            "0.21875\n",
            "0.296875\n",
            "0.328125\n",
            "0.0\n",
            "strain 0.14739328622817993\n",
            "strain 0.10203591734170914\n",
            "strain 0.1510104089975357\n",
            "strain 0.14517010748386383\n",
            "strain 0.07933320850133896\n",
            "classify 1.343017578125\n",
            "classify 1.319580078125\n",
            "classify 1.318603515625\n",
            "classify 1.3310546875\n",
            "classify 1.3859374523162842\n",
            "0.34375\n",
            "0.375\n",
            "0.28125\n",
            "0.203125\n",
            "0.5714285714285714\n",
            "strain 0.2323669046163559\n",
            "strain 0.17186115682125092\n",
            "strain 0.16312232613563538\n",
            "strain 0.17259816825389862\n",
            "strain 0.2066245824098587\n",
            "classify 1.30908203125\n",
            "classify 1.3111572265625\n",
            "classify 1.3240966796875\n",
            "classify 1.37841796875\n",
            "classify 1.274999976158142\n",
            "0.390625\n",
            "0.25\n",
            "0.25\n",
            "0.359375\n",
            "0.2857142857142857\n",
            "strain 0.24155734479427338\n",
            "strain 0.13831299543380737\n",
            "strain 0.21202678978443146\n",
            "strain 0.11881199479103088\n",
            "strain 0.06516872346401215\n",
            "classify 1.36138916015625\n",
            "classify 1.31890869140625\n",
            "classify 1.32562255859375\n",
            "classify 1.2991943359375\n",
            "classify 1.4171874523162842\n",
            "0.28125\n",
            "0.234375\n",
            "0.296875\n",
            "0.296875\n",
            "0.14285714285714285\n",
            "strain 0.2296060025691986\n",
            "strain 0.1822989583015442\n",
            "strain 0.10467374324798584\n",
            "strain 0.14882683753967285\n",
            "strain 0.07108644396066666\n",
            "classify 1.32415771484375\n",
            "classify 1.3609619140625\n",
            "classify 1.34002685546875\n",
            "classify 1.292724609375\n",
            "classify 1.2390625476837158\n",
            "0.203125\n",
            "0.375\n",
            "0.265625\n",
            "0.328125\n",
            "0.2857142857142857\n",
            "strain 0.2572719156742096\n",
            "strain 0.212412029504776\n",
            "strain 0.16869889199733734\n",
            "strain 0.2182796597480774\n",
            "strain 0.10029388219118118\n",
            "classify 1.32080078125\n",
            "classify 1.3302001953125\n",
            "classify 1.34979248046875\n",
            "classify 1.3309326171875\n",
            "classify 1.3875000476837158\n",
            "0.296875\n",
            "0.296875\n",
            "0.28125\n",
            "0.328125\n",
            "0.2857142857142857\n",
            "strain 0.20111863315105438\n",
            "strain 0.16031067073345184\n",
            "strain 0.1726839244365692\n",
            "strain 0.2423079013824463\n",
            "strain 0.1478247344493866\n",
            "classify 1.3994140625\n",
            "classify 1.3177490234375\n",
            "classify 1.279052734375\n",
            "classify 1.3480224609375\n",
            "classify 1.279687523841858\n",
            "0.296875\n",
            "0.234375\n",
            "0.265625\n",
            "0.484375\n",
            "0.14285714285714285\n",
            "strain 0.2396858036518097\n",
            "strain 0.1804109811782837\n",
            "strain 0.09717300534248352\n",
            "strain 0.28119996190071106\n",
            "strain 0.08422303199768066\n",
            "classify 1.27886962890625\n",
            "classify 1.36932373046875\n",
            "classify 1.36016845703125\n",
            "classify 1.3155517578125\n",
            "classify 1.337499976158142\n",
            "0.296875\n",
            "0.234375\n",
            "0.3125\n",
            "0.28125\n",
            "0.2857142857142857\n",
            "strain 0.1989392787218094\n",
            "strain 0.2729116976261139\n",
            "strain 0.08994991332292557\n",
            "strain 0.13494619727134705\n",
            "strain 0.06856068968772888\n",
            "classify 1.333251953125\n",
            "classify 1.32745361328125\n",
            "classify 1.32159423828125\n",
            "classify 1.3228759765625\n",
            "classify 1.423437476158142\n",
            "0.296875\n",
            "0.296875\n",
            "0.34375\n",
            "0.15625\n",
            "0.42857142857142855\n",
            "strain 0.1964934915304184\n",
            "strain 0.166146919131279\n",
            "strain 0.07540824264287949\n",
            "strain 0.15289393067359924\n",
            "strain 0.0833321064710617\n",
            "classify 1.30859375\n",
            "classify 1.36761474609375\n",
            "classify 1.2801513671875\n",
            "classify 1.3553466796875\n",
            "classify 1.251562476158142\n",
            "0.34375\n",
            "0.375\n",
            "0.234375\n",
            "0.28125\n",
            "0.2857142857142857\n",
            "strain 0.21660438179969788\n",
            "strain 0.1511707901954651\n",
            "strain 0.17515401542186737\n",
            "strain 0.2282755821943283\n",
            "strain 0.22234828770160675\n",
            "classify 1.314697265625\n",
            "classify 1.3172607421875\n",
            "classify 1.30584716796875\n",
            "classify 1.3619384765625\n",
            "classify 1.3640625476837158\n",
            "0.203125\n",
            "0.1875\n",
            "0.25\n",
            "0.40625\n",
            "0.2857142857142857\n",
            "strain 0.19112516939640045\n",
            "strain 0.24231557548046112\n",
            "strain 0.15431134402751923\n",
            "strain 0.2198544442653656\n",
            "strain 0.2638609707355499\n",
            "classify 1.3480224609375\n",
            "classify 1.352294921875\n",
            "classify 1.30963134765625\n",
            "classify 1.30010986328125\n",
            "classify 1.3484375476837158\n",
            "0.3125\n",
            "0.1875\n",
            "0.296875\n",
            "0.34375\n",
            "0.2857142857142857\n",
            "strain 0.12292814999818802\n",
            "strain 0.15531907975673676\n",
            "strain 0.28581899404525757\n",
            "strain 0.24636361002922058\n",
            "strain 0.14176045358181\n",
            "classify 1.336181640625\n",
            "classify 1.347412109375\n",
            "classify 1.3123779296875\n",
            "classify 1.33575439453125\n",
            "classify 1.2820312976837158\n",
            "0.359375\n",
            "0.296875\n",
            "0.15625\n",
            "0.265625\n",
            "0.14285714285714285\n",
            "strain 0.25932711362838745\n",
            "strain 0.18774250149726868\n",
            "strain 0.15658767521381378\n",
            "strain 0.187313050031662\n",
            "strain 0.0471646785736084\n",
            "classify 1.30377197265625\n",
            "classify 1.34283447265625\n",
            "classify 1.35321044921875\n",
            "classify 1.319580078125\n",
            "classify 1.329687476158142\n",
            "0.296875\n",
            "0.234375\n",
            "0.234375\n",
            "0.265625\n",
            "0.42857142857142855\n",
            "strain 0.1988103836774826\n",
            "strain 0.22321327030658722\n",
            "strain 0.23059333860874176\n",
            "strain 0.20676389336585999\n",
            "strain 0.1607193797826767\n",
            "classify 1.33642578125\n",
            "classify 1.3333740234375\n",
            "classify 1.33343505859375\n",
            "classify 1.3175048828125\n",
            "classify 1.4093749523162842\n",
            "0.28125\n",
            "0.203125\n",
            "0.265625\n",
            "0.296875\n",
            "0.0\n",
            "strain 0.16950339078903198\n",
            "strain 0.19453832507133484\n",
            "strain 0.24269595742225647\n",
            "strain 0.1179293692111969\n",
            "strain 0.06791675835847855\n",
            "classify 1.3419189453125\n",
            "classify 1.337646484375\n",
            "classify 1.33642578125\n",
            "classify 1.3184814453125\n",
            "classify 1.368749976158142\n",
            "0.40625\n",
            "0.28125\n",
            "0.390625\n",
            "0.234375\n",
            "0.2857142857142857\n",
            "strain 0.12654393911361694\n",
            "strain 0.1887805014848709\n",
            "strain 0.22766390442848206\n",
            "strain 0.10386257618665695\n",
            "strain 0.05587680637836456\n",
            "classify 1.350341796875\n",
            "classify 1.3148193359375\n",
            "classify 1.33001708984375\n",
            "classify 1.3023681640625\n",
            "classify 1.564062476158142\n",
            "0.3125\n",
            "0.328125\n",
            "0.25\n",
            "0.296875\n",
            "0.2857142857142857\n",
            "strain 0.21175454556941986\n",
            "strain 0.1425143927335739\n",
            "strain 0.19918769598007202\n",
            "strain 0.18595314025878906\n",
            "strain 0.11533654481172562\n",
            "classify 1.31494140625\n",
            "classify 1.34161376953125\n",
            "classify 1.3448486328125\n",
            "classify 1.299072265625\n",
            "classify 1.384374976158142\n",
            "0.25\n",
            "0.21875\n",
            "0.3125\n",
            "0.34375\n",
            "0.2857142857142857\n",
            "strain 0.24577267467975616\n",
            "strain 0.18138884007930756\n",
            "strain 0.17451827228069305\n",
            "strain 0.18106916546821594\n",
            "strain 0.04079451411962509\n",
            "classify 1.3507080078125\n",
            "classify 1.31329345703125\n",
            "classify 1.30621337890625\n",
            "classify 1.33673095703125\n",
            "classify 1.2609374523162842\n",
            "0.3125\n",
            "0.296875\n",
            "0.25\n",
            "0.28125\n",
            "0.42857142857142855\n",
            "strain 0.10542583465576172\n",
            "strain 0.15724396705627441\n",
            "strain 0.22916467487812042\n",
            "strain 0.27415886521339417\n",
            "strain 0.17437131702899933\n",
            "classify 1.32366943359375\n",
            "classify 1.31988525390625\n",
            "classify 1.3826904296875\n",
            "classify 1.30706787109375\n",
            "classify 1.3203125\n",
            "0.359375\n",
            "0.3125\n",
            "0.265625\n",
            "0.265625\n",
            "0.14285714285714285\n",
            "strain 0.14450672268867493\n",
            "strain 0.2013234794139862\n",
            "strain 0.1689537912607193\n",
            "strain 0.25387880206108093\n",
            "strain 0.08711408078670502\n",
            "classify 1.32080078125\n",
            "classify 1.32562255859375\n",
            "classify 1.3555908203125\n",
            "classify 1.3375244140625\n",
            "classify 1.310156226158142\n",
            "0.234375\n",
            "0.390625\n",
            "0.28125\n",
            "0.3125\n",
            "0.42857142857142855\n",
            "strain 0.2529143989086151\n",
            "strain 0.11518001556396484\n",
            "strain 0.14485573768615723\n",
            "strain 0.1035878136754036\n",
            "strain 0.05009317025542259\n",
            "classify 1.34521484375\n",
            "classify 1.34063720703125\n",
            "classify 1.29803466796875\n",
            "classify 1.33294677734375\n",
            "classify 1.2625000476837158\n",
            "0.296875\n",
            "0.265625\n",
            "0.28125\n",
            "0.234375\n",
            "0.14285714285714285\n",
            "strain 0.2759649455547333\n",
            "strain 0.18274898827075958\n",
            "strain 0.20911768078804016\n",
            "strain 0.1832437813282013\n",
            "strain 0.22562387585639954\n",
            "classify 1.34234619140625\n",
            "classify 1.3023681640625\n",
            "classify 1.32464599609375\n",
            "classify 1.34893798828125\n",
            "classify 1.4031250476837158\n",
            "0.28125\n",
            "0.25\n",
            "0.375\n",
            "0.234375\n",
            "0.2857142857142857\n",
            "strain 0.24304506182670593\n",
            "strain 0.18637067079544067\n",
            "strain 0.2325630486011505\n",
            "strain 0.17477712035179138\n",
            "strain 0.03450983017683029\n",
            "classify 1.329345703125\n",
            "classify 1.3099365234375\n",
            "classify 1.34979248046875\n",
            "classify 1.33001708984375\n",
            "classify 1.3828125\n",
            "0.1875\n",
            "0.265625\n",
            "0.25\n",
            "0.375\n",
            "0.14285714285714285\n",
            "strain 0.1473444253206253\n",
            "strain 0.253622829914093\n",
            "strain 0.13666287064552307\n",
            "strain 0.17003440856933594\n",
            "strain 0.35392385721206665\n",
            "classify 1.306640625\n",
            "classify 1.35821533203125\n",
            "classify 1.35455322265625\n",
            "classify 1.2900390625\n",
            "classify 1.443750023841858\n",
            "0.234375\n",
            "0.34375\n",
            "0.3125\n",
            "0.234375\n",
            "0.2857142857142857\n",
            "strain 0.2539259195327759\n",
            "strain 0.17985497415065765\n",
            "strain 0.19468003511428833\n",
            "strain 0.1815325915813446\n",
            "strain 0.0881730169057846\n",
            "classify 1.293212890625\n",
            "classify 1.35101318359375\n",
            "classify 1.340087890625\n",
            "classify 1.32769775390625\n",
            "classify 1.3781249523162842\n",
            "0.28125\n",
            "0.21875\n",
            "0.28125\n",
            "0.328125\n",
            "0.42857142857142855\n",
            "strain 0.12865838408470154\n",
            "strain 0.283865362405777\n",
            "strain 0.2571490406990051\n",
            "strain 0.17516158521175385\n",
            "strain 0.2040850818157196\n",
            "classify 1.33160400390625\n",
            "classify 1.343017578125\n",
            "classify 1.29345703125\n",
            "classify 1.33197021484375\n",
            "classify 1.490625023841858\n",
            "0.328125\n",
            "0.25\n",
            "0.3125\n",
            "0.140625\n",
            "0.42857142857142855\n",
            "strain 0.20436139404773712\n",
            "strain 0.19829370081424713\n",
            "strain 0.22267450392246246\n",
            "strain 0.17570987343788147\n",
            "strain 0.2516029477119446\n",
            "classify 1.314208984375\n",
            "classify 1.35943603515625\n",
            "classify 1.34173583984375\n",
            "classify 1.32806396484375\n",
            "classify 1.166406273841858\n",
            "0.28125\n",
            "0.3125\n",
            "0.3125\n",
            "0.265625\n",
            "0.14285714285714285\n",
            "strain 0.18146611750125885\n",
            "strain 0.11191552877426147\n",
            "strain 0.2250937670469284\n",
            "strain 0.18970558047294617\n",
            "strain 0.06831187009811401\n",
            "classify 1.298095703125\n",
            "classify 1.3160400390625\n",
            "classify 1.349853515625\n",
            "classify 1.34466552734375\n",
            "classify 1.381250023841858\n",
            "0.34375\n",
            "0.1875\n",
            "0.234375\n",
            "0.25\n",
            "0.7142857142857143\n",
            "strain 0.15532560646533966\n",
            "strain 0.16334806382656097\n",
            "strain 0.08044605702161789\n",
            "strain 0.18523192405700684\n",
            "strain 0.15266354382038116\n",
            "classify 1.37152099609375\n",
            "classify 1.29217529296875\n",
            "classify 1.31915283203125\n",
            "classify 1.32330322265625\n",
            "classify 1.328125\n",
            "0.296875\n",
            "0.1875\n",
            "0.3125\n",
            "0.296875\n",
            "0.2857142857142857\n",
            "strain 0.24971236288547516\n",
            "strain 0.1920989602804184\n",
            "strain 0.2109813541173935\n",
            "strain 0.23673288524150848\n",
            "strain 0.034964002668857574\n",
            "classify 1.306396484375\n",
            "classify 1.33917236328125\n",
            "classify 1.34405517578125\n",
            "classify 1.333251953125\n",
            "classify 1.189843773841858\n",
            "0.296875\n",
            "0.3125\n",
            "0.234375\n",
            "0.328125\n",
            "0.14285714285714285\n",
            "strain 0.09156835079193115\n",
            "strain 0.22893565893173218\n",
            "strain 0.29354992508888245\n",
            "strain 0.1749441921710968\n",
            "strain 0.33923906087875366\n",
            "classify 1.306396484375\n",
            "classify 1.34564208984375\n",
            "classify 1.34381103515625\n",
            "classify 1.3145751953125\n",
            "classify 1.279687523841858\n",
            "0.296875\n",
            "0.28125\n",
            "0.375\n",
            "0.234375\n",
            "0.2857142857142857\n",
            "strain 0.16548104584217072\n",
            "strain 0.1557648926973343\n",
            "strain 0.14647319912910461\n",
            "strain 0.2746104896068573\n",
            "strain 0.051933009177446365\n",
            "classify 1.30401611328125\n",
            "classify 1.35321044921875\n",
            "classify 1.31317138671875\n",
            "classify 1.34295654296875\n",
            "classify 1.2000000476837158\n",
            "0.265625\n",
            "0.265625\n",
            "0.265625\n",
            "0.265625\n",
            "0.2857142857142857\n",
            "strain 0.24916359782218933\n",
            "strain 0.17213360965251923\n",
            "strain 0.21546180546283722\n",
            "strain 0.15230949223041534\n",
            "strain 0.03729568421840668\n",
            "classify 1.3348388671875\n",
            "classify 1.33795166015625\n",
            "classify 1.3099365234375\n",
            "classify 1.32293701171875\n",
            "classify 1.3828125\n",
            "0.3125\n",
            "0.34375\n",
            "0.21875\n",
            "0.25\n",
            "0.2857142857142857\n",
            "strain 0.1896023452281952\n",
            "strain 0.15368054807186127\n",
            "strain 0.1878545880317688\n",
            "strain 0.26383963227272034\n",
            "strain 0.05768886208534241\n",
            "classify 1.32666015625\n",
            "classify 1.3095703125\n",
            "classify 1.32269287109375\n",
            "classify 1.3482666015625\n",
            "classify 1.26953125\n",
            "0.265625\n",
            "0.328125\n",
            "0.28125\n",
            "0.21875\n",
            "0.14285714285714285\n",
            "strain 0.09971360862255096\n",
            "strain 0.1833471655845642\n",
            "strain 0.16252721846103668\n",
            "strain 0.2499287873506546\n",
            "strain 0.06906461715698242\n",
            "classify 1.3499755859375\n",
            "classify 1.31573486328125\n",
            "classify 1.33203125\n",
            "classify 1.29901123046875\n",
            "classify 1.3828125\n",
            "0.171875\n",
            "0.34375\n",
            "0.359375\n",
            "0.203125\n",
            "0.14285714285714285\n",
            "strain 0.19701112806797028\n",
            "strain 0.23773182928562164\n",
            "strain 0.1136268898844719\n",
            "strain 0.20794519782066345\n",
            "strain 0.10306525975465775\n",
            "classify 1.309326171875\n",
            "classify 1.33367919921875\n",
            "classify 1.3160400390625\n",
            "classify 1.35198974609375\n",
            "classify 1.3937499523162842\n",
            "0.3125\n",
            "0.234375\n",
            "0.328125\n",
            "0.234375\n",
            "0.2857142857142857\n",
            "strain 0.16099189221858978\n",
            "strain 0.14726881682872772\n",
            "strain 0.1424480378627777\n",
            "strain 0.1258379966020584\n",
            "strain 0.10670069605112076\n",
            "classify 1.3433837890625\n",
            "classify 1.33770751953125\n",
            "classify 1.32086181640625\n",
            "classify 1.3057861328125\n",
            "classify 1.263281226158142\n",
            "0.21875\n",
            "0.328125\n",
            "0.234375\n",
            "0.28125\n",
            "0.14285714285714285\n",
            "strain 0.2531777620315552\n",
            "strain 0.10947436094284058\n",
            "strain 0.17459283769130707\n",
            "strain 0.14162708818912506\n",
            "strain 0.08688899874687195\n",
            "classify 1.339111328125\n",
            "classify 1.3487548828125\n",
            "classify 1.279052734375\n",
            "classify 1.329345703125\n",
            "classify 1.3515625\n",
            "0.375\n",
            "0.1875\n",
            "0.234375\n",
            "0.28125\n",
            "0.14285714285714285\n",
            "strain 0.2152586430311203\n",
            "strain 0.16038982570171356\n",
            "strain 0.1600916087627411\n",
            "strain 0.11301512271165848\n",
            "strain 0.08095008134841919\n",
            "classify 1.3175048828125\n",
            "classify 1.2918701171875\n",
            "classify 1.32611083984375\n",
            "classify 1.35260009765625\n",
            "classify 1.421875\n",
            "0.234375\n",
            "0.296875\n",
            "0.328125\n",
            "0.203125\n",
            "0.42857142857142855\n",
            "strain 0.1753993034362793\n",
            "strain 0.23644985258579254\n",
            "strain 0.21532857418060303\n",
            "strain 0.138401061296463\n",
            "strain 0.04127134382724762\n",
            "classify 1.3489990234375\n",
            "classify 1.3009033203125\n",
            "classify 1.34710693359375\n",
            "classify 1.29595947265625\n",
            "classify 1.3078124523162842\n",
            "0.3125\n",
            "0.328125\n",
            "0.21875\n",
            "0.296875\n",
            "0.14285714285714285\n",
            "strain 0.15740740299224854\n",
            "strain 0.2282545268535614\n",
            "strain 0.21558542549610138\n",
            "strain 0.2589283585548401\n",
            "strain 0.04895678162574768\n",
            "classify 1.371826171875\n",
            "classify 1.27569580078125\n",
            "classify 1.3189697265625\n",
            "classify 1.3292236328125\n",
            "classify 1.286718726158142\n",
            "0.296875\n",
            "0.21875\n",
            "0.234375\n",
            "0.375\n",
            "0.42857142857142855\n",
            "strain 0.20117226243019104\n",
            "strain 0.11424790322780609\n",
            "strain 0.19041594862937927\n",
            "strain 0.1980247050523758\n",
            "strain 0.09685232490301132\n",
            "classify 1.30255126953125\n",
            "classify 1.317138671875\n",
            "classify 1.359619140625\n",
            "classify 1.309326171875\n",
            "classify 1.350000023841858\n",
            "0.21875\n",
            "0.265625\n",
            "0.25\n",
            "0.375\n",
            "0.14285714285714285\n",
            "strain 0.19159311056137085\n",
            "strain 0.1538897156715393\n",
            "strain 0.1969035267829895\n",
            "strain 0.15786996483802795\n",
            "strain 0.07787013798952103\n",
            "classify 1.29559326171875\n",
            "classify 1.36334228515625\n",
            "classify 1.3466796875\n",
            "classify 1.29058837890625\n",
            "classify 1.3210937976837158\n",
            "0.25\n",
            "0.390625\n",
            "0.328125\n",
            "0.234375\n",
            "0.0\n",
            "strain 0.18018929660320282\n",
            "strain 0.18707269430160522\n",
            "strain 0.2712937295436859\n",
            "strain 0.14744554460048676\n",
            "strain 0.04771490767598152\n",
            "classify 1.3316650390625\n",
            "classify 1.3497314453125\n",
            "classify 1.3277587890625\n",
            "classify 1.30621337890625\n",
            "classify 1.2312500476837158\n",
            "0.234375\n",
            "0.171875\n",
            "0.328125\n",
            "0.328125\n",
            "0.0\n",
            "strain 0.21259529888629913\n",
            "strain 0.09469344466924667\n",
            "strain 0.12971383333206177\n",
            "strain 0.1613941639661789\n",
            "strain 0.05478851869702339\n",
            "classify 1.32489013671875\n",
            "classify 1.336181640625\n",
            "classify 1.30120849609375\n",
            "classify 1.33721923828125\n",
            "classify 1.3796875476837158\n",
            "0.265625\n",
            "0.3125\n",
            "0.203125\n",
            "0.265625\n",
            "0.14285714285714285\n",
            "strain 0.20594777166843414\n",
            "strain 0.26836755871772766\n",
            "strain 0.16389888525009155\n",
            "strain 0.16153687238693237\n",
            "strain 0.12433435022830963\n",
            "classify 1.34307861328125\n",
            "classify 1.33551025390625\n",
            "classify 1.3084716796875\n",
            "classify 1.32275390625\n",
            "classify 1.3234374523162842\n",
            "0.296875\n",
            "0.375\n",
            "0.34375\n",
            "0.265625\n",
            "0.14285714285714285\n",
            "strain 0.172811359167099\n",
            "strain 0.17131750285625458\n",
            "strain 0.21223683655261993\n",
            "strain 0.15673589706420898\n",
            "strain 0.041596848517656326\n",
            "classify 1.33929443359375\n",
            "classify 1.328369140625\n",
            "classify 1.328125\n",
            "classify 1.32525634765625\n",
            "classify 1.11328125\n",
            "0.28125\n",
            "0.265625\n",
            "0.25\n",
            "0.265625\n",
            "0.0\n",
            "strain 0.14540110528469086\n",
            "strain 0.09595378488302231\n",
            "strain 0.14237172901630402\n",
            "strain 0.12378066033124924\n",
            "strain 0.15137922763824463\n",
            "classify 1.3402099609375\n",
            "classify 1.30047607421875\n",
            "classify 1.33258056640625\n",
            "classify 1.35357666015625\n",
            "classify 1.236718773841858\n",
            "0.28125\n",
            "0.21875\n",
            "0.296875\n",
            "0.328125\n",
            "0.2857142857142857\n",
            "strain 0.23679563403129578\n",
            "strain 0.19674034416675568\n",
            "strain 0.20140738785266876\n",
            "strain 0.09993334859609604\n",
            "strain 0.053584929555654526\n",
            "classify 1.316650390625\n",
            "classify 1.333984375\n",
            "classify 1.31402587890625\n",
            "classify 1.33636474609375\n",
            "classify 1.399999976158142\n",
            "0.21875\n",
            "0.21875\n",
            "0.296875\n",
            "0.328125\n",
            "0.2857142857142857\n",
            "strain 0.19541506469249725\n",
            "strain 0.2598130702972412\n",
            "strain 0.1338929980993271\n",
            "strain 0.22277583181858063\n",
            "strain 0.10987648367881775\n",
            "classify 1.34014892578125\n",
            "classify 1.318115234375\n",
            "classify 1.343017578125\n",
            "classify 1.29925537109375\n",
            "classify 1.3859374523162842\n",
            "0.359375\n",
            "0.265625\n",
            "0.265625\n",
            "0.25\n",
            "0.0\n",
            "strain 0.16680529713630676\n",
            "strain 0.1744501143693924\n",
            "strain 0.0897461548447609\n",
            "strain 0.1746300309896469\n",
            "strain 0.06371766328811646\n",
            "classify 1.337646484375\n",
            "classify 1.29656982421875\n",
            "classify 1.34161376953125\n",
            "classify 1.35321044921875\n",
            "classify 1.341406226158142\n",
            "0.4375\n",
            "0.21875\n",
            "0.265625\n",
            "0.328125\n",
            "0.42857142857142855\n",
            "strain 0.1302197277545929\n",
            "strain 0.17242474853992462\n",
            "strain 0.17969928681850433\n",
            "strain 0.22391101717948914\n",
            "strain 0.06980740278959274\n",
            "classify 1.31573486328125\n",
            "classify 1.319580078125\n",
            "classify 1.33050537109375\n",
            "classify 1.35302734375\n",
            "classify 1.3171875476837158\n",
            "0.265625\n",
            "0.34375\n",
            "0.28125\n",
            "0.234375\n",
            "0.5714285714285714\n",
            "strain 0.09386187791824341\n",
            "strain 0.20063412189483643\n",
            "strain 0.2178635448217392\n",
            "strain 0.1674584597349167\n",
            "strain 0.06547673046588898\n",
            "classify 1.364501953125\n",
            "classify 1.3206787109375\n",
            "classify 1.3291015625\n",
            "classify 1.290771484375\n",
            "classify 1.4093749523162842\n",
            "0.3125\n",
            "0.359375\n",
            "0.25\n",
            "0.296875\n",
            "0.42857142857142855\n",
            "strain 0.1954159289598465\n",
            "strain 0.17115440964698792\n",
            "strain 0.2042490690946579\n",
            "strain 0.18924540281295776\n",
            "strain 0.07788913697004318\n",
            "classify 1.35772705078125\n",
            "classify 1.3167724609375\n",
            "classify 1.31280517578125\n",
            "classify 1.31134033203125\n",
            "classify 1.475000023841858\n",
            "0.28125\n",
            "0.28125\n",
            "0.34375\n",
            "0.34375\n",
            "0.2857142857142857\n",
            "strain 0.11695768684148788\n",
            "strain 0.16132885217666626\n",
            "strain 0.16237427294254303\n",
            "strain 0.1522529572248459\n",
            "strain 0.04039274528622627\n",
            "classify 1.29754638671875\n",
            "classify 1.28619384765625\n",
            "classify 1.388671875\n",
            "classify 1.33221435546875\n",
            "classify 1.4484374523162842\n",
            "0.28125\n",
            "0.328125\n",
            "0.375\n",
            "0.203125\n",
            "0.14285714285714285\n",
            "strain 0.1747574657201767\n",
            "strain 0.14764848351478577\n",
            "strain 0.18424491584300995\n",
            "strain 0.23307083547115326\n",
            "strain 0.2758570909500122\n",
            "classify 1.32305908203125\n",
            "classify 1.336669921875\n",
            "classify 1.32122802734375\n",
            "classify 1.32318115234375\n",
            "classify 1.31640625\n",
            "0.234375\n",
            "0.328125\n",
            "0.265625\n",
            "0.296875\n",
            "0.5714285714285714\n",
            "strain 0.1887062042951584\n",
            "strain 0.17445960640907288\n",
            "strain 0.2635849714279175\n",
            "strain 0.23577725887298584\n",
            "strain 0.04928236082196236\n",
            "classify 1.3323974609375\n",
            "classify 1.35821533203125\n",
            "classify 1.2919921875\n",
            "classify 1.34124755859375\n",
            "classify 1.2687499523162842\n",
            "0.328125\n",
            "0.3125\n",
            "0.296875\n",
            "0.265625\n",
            "0.14285714285714285\n",
            "strain 0.17832064628601074\n",
            "strain 0.18170422315597534\n",
            "strain 0.16376830637454987\n",
            "strain 0.285606324672699\n",
            "strain 0.03359873592853546\n",
            "classify 1.31719970703125\n",
            "classify 1.32061767578125\n",
            "classify 1.37530517578125\n",
            "classify 1.32769775390625\n",
            "classify 1.0187499523162842\n",
            "0.328125\n",
            "0.3125\n",
            "0.28125\n",
            "0.28125\n",
            "0.42857142857142855\n",
            "strain 0.16378842294216156\n",
            "strain 0.17585138976573944\n",
            "strain 0.23524674773216248\n",
            "strain 0.15335026383399963\n",
            "strain 0.1711578071117401\n",
            "classify 1.2882080078125\n",
            "classify 1.30560302734375\n",
            "classify 1.3726806640625\n",
            "classify 1.341552734375\n",
            "classify 1.322656273841858\n",
            "0.328125\n",
            "0.25\n",
            "0.296875\n",
            "0.3125\n",
            "0.14285714285714285\n",
            "strain 0.15437588095664978\n",
            "strain 0.1370038241147995\n",
            "strain 0.2078956812620163\n",
            "strain 0.17071768641471863\n",
            "strain 0.22416117787361145\n",
            "classify 1.3668212890625\n",
            "classify 1.336669921875\n",
            "classify 1.27667236328125\n",
            "classify 1.32379150390625\n",
            "classify 1.290624976158142\n",
            "0.28125\n",
            "0.28125\n",
            "0.25\n",
            "0.359375\n",
            "0.14285714285714285\n",
            "strain 0.08272086083889008\n",
            "strain 0.15205436944961548\n",
            "strain 0.1606818437576294\n",
            "strain 0.18136559426784515\n",
            "strain 0.03638983145356178\n",
            "classify 1.3662109375\n",
            "classify 1.30218505859375\n",
            "classify 1.31390380859375\n",
            "classify 1.311767578125\n",
            "classify 1.40625\n",
            "0.25\n",
            "0.28125\n",
            "0.40625\n",
            "0.234375\n",
            "0.2857142857142857\n",
            "strain 0.13388122618198395\n",
            "strain 0.2329750806093216\n",
            "strain 0.18573519587516785\n",
            "strain 0.23933938145637512\n",
            "strain 0.0654241070151329\n",
            "classify 1.34326171875\n",
            "classify 1.341796875\n",
            "classify 1.26800537109375\n",
            "classify 1.3377685546875\n",
            "classify 1.353124976158142\n",
            "0.234375\n",
            "0.28125\n",
            "0.25\n",
            "0.34375\n",
            "0.2857142857142857\n",
            "strain 0.10843493789434433\n",
            "strain 0.16368365287780762\n",
            "strain 0.17296470701694489\n",
            "strain 0.20681697130203247\n",
            "strain 0.08359429240226746\n",
            "classify 1.3326416015625\n",
            "classify 1.3037109375\n",
            "classify 1.345947265625\n",
            "classify 1.31329345703125\n",
            "classify 1.3671875\n",
            "0.3125\n",
            "0.265625\n",
            "0.390625\n",
            "0.25\n",
            "0.14285714285714285\n",
            "strain 0.10057209432125092\n",
            "strain 0.17263588309288025\n",
            "strain 0.14308685064315796\n",
            "strain 0.1602030098438263\n",
            "strain 0.2896803021430969\n",
            "classify 1.3218994140625\n",
            "classify 1.3515625\n",
            "classify 1.30230712890625\n",
            "classify 1.31597900390625\n",
            "classify 1.396875023841858\n",
            "0.296875\n",
            "0.359375\n",
            "0.21875\n",
            "0.234375\n",
            "0.2857142857142857\n",
            "strain 0.15326359868049622\n",
            "strain 0.19202971458435059\n",
            "strain 0.1404389590024948\n",
            "strain 0.08710649609565735\n",
            "strain 0.28880852460861206\n",
            "classify 1.2845458984375\n",
            "classify 1.3289794921875\n",
            "classify 1.322021484375\n",
            "classify 1.34246826171875\n",
            "classify 1.467187523841858\n",
            "0.296875\n",
            "0.1875\n",
            "0.296875\n",
            "0.28125\n",
            "0.2857142857142857\n",
            "strain 0.18624897301197052\n",
            "strain 0.10552564263343811\n",
            "strain 0.1232164055109024\n",
            "strain 0.2890594005584717\n",
            "strain 0.031007466837763786\n",
            "classify 1.31951904296875\n",
            "classify 1.34930419921875\n",
            "classify 1.318115234375\n",
            "classify 1.2889404296875\n",
            "classify 1.5359375476837158\n",
            "0.265625\n",
            "0.421875\n",
            "0.21875\n",
            "0.25\n",
            "0.14285714285714285\n",
            "strain 0.1655762493610382\n",
            "strain 0.16520655155181885\n",
            "strain 0.26867425441741943\n",
            "strain 0.09968268871307373\n",
            "strain 0.07764323055744171\n",
            "classify 1.323974609375\n",
            "classify 1.326171875\n",
            "classify 1.34063720703125\n",
            "classify 1.3138427734375\n",
            "classify 1.091406226158142\n",
            "0.28125\n",
            "0.265625\n",
            "0.234375\n",
            "0.25\n",
            "0.42857142857142855\n",
            "strain 0.15129613876342773\n",
            "strain 0.20968103408813477\n",
            "strain 0.14338380098342896\n",
            "strain 0.17740412056446075\n",
            "strain 0.05727783218026161\n",
            "classify 1.357177734375\n",
            "classify 1.3148193359375\n",
            "classify 1.30548095703125\n",
            "classify 1.30426025390625\n",
            "classify 1.3828125\n",
            "0.1875\n",
            "0.265625\n",
            "0.3125\n",
            "0.265625\n",
            "0.2857142857142857\n",
            "strain 0.12126270681619644\n",
            "strain 0.19384461641311646\n",
            "strain 0.10152467340230942\n",
            "strain 0.23456254601478577\n",
            "strain 0.3545241355895996\n",
            "classify 1.36083984375\n",
            "classify 1.2987060546875\n",
            "classify 1.3017578125\n",
            "classify 1.3121337890625\n",
            "classify 1.514062523841858\n",
            "0.25\n",
            "0.265625\n",
            "0.265625\n",
            "0.21875\n",
            "0.5714285714285714\n",
            "strain 0.09517168998718262\n",
            "strain 0.15075568854808807\n",
            "strain 0.1424439400434494\n",
            "strain 0.26806291937828064\n",
            "strain 0.04207945987582207\n",
            "classify 1.3470458984375\n",
            "classify 1.29852294921875\n",
            "classify 1.37835693359375\n",
            "classify 1.30255126953125\n",
            "classify 1.247656226158142\n",
            "0.328125\n",
            "0.1875\n",
            "0.234375\n",
            "0.265625\n",
            "0.2857142857142857\n",
            "strain 0.18376226723194122\n",
            "strain 0.24577564001083374\n",
            "strain 0.1743929088115692\n",
            "strain 0.19472196698188782\n",
            "strain 0.04832318052649498\n",
            "classify 1.33984375\n",
            "classify 1.29974365234375\n",
            "classify 1.31640625\n",
            "classify 1.36224365234375\n",
            "classify 1.3781249523162842\n",
            "0.28125\n",
            "0.234375\n",
            "0.34375\n",
            "0.203125\n",
            "0.42857142857142855\n",
            "strain 0.14078092575073242\n",
            "strain 0.19457559287548065\n",
            "strain 0.1848393976688385\n",
            "strain 0.2046946883201599\n",
            "strain 0.08719759434461594\n",
            "classify 1.341552734375\n",
            "classify 1.33551025390625\n",
            "classify 1.3072509765625\n",
            "classify 1.328125\n",
            "classify 1.236718773841858\n",
            "0.3125\n",
            "0.1875\n",
            "0.1875\n",
            "0.359375\n",
            "0.2857142857142857\n",
            "strain 0.20909303426742554\n",
            "strain 0.16567766666412354\n",
            "strain 0.18867413699626923\n",
            "strain 0.14469249546527863\n",
            "strain 0.08293502032756805\n",
            "classify 1.32183837890625\n",
            "classify 1.2999267578125\n",
            "classify 1.3218994140625\n",
            "classify 1.35443115234375\n",
            "classify 1.2296874523162842\n",
            "0.234375\n",
            "0.34375\n",
            "0.28125\n",
            "0.234375\n",
            "0.2857142857142857\n",
            "strain 0.1345992088317871\n",
            "strain 0.1655283272266388\n",
            "strain 0.14638355374336243\n",
            "strain 0.22207000851631165\n",
            "strain 0.20283880829811096\n",
            "classify 1.31591796875\n",
            "classify 1.31195068359375\n",
            "classify 1.33624267578125\n",
            "classify 1.3092041015625\n",
            "classify 1.420312523841858\n",
            "0.234375\n",
            "0.296875\n",
            "0.296875\n",
            "0.203125\n",
            "0.2857142857142857\n",
            "strain 0.13832354545593262\n",
            "strain 0.23978012800216675\n",
            "strain 0.11188934743404388\n",
            "strain 0.22060073912143707\n",
            "strain 0.11181742697954178\n",
            "classify 1.355712890625\n",
            "classify 1.36077880859375\n",
            "classify 1.29815673828125\n",
            "classify 1.2674560546875\n",
            "classify 1.2429687976837158\n",
            "0.234375\n",
            "0.265625\n",
            "0.28125\n",
            "0.25\n",
            "0.2857142857142857\n",
            "strain 0.1846533864736557\n",
            "strain 0.1293898969888687\n",
            "strain 0.21776992082595825\n",
            "strain 0.12502335011959076\n",
            "strain 0.20300254225730896\n",
            "classify 1.303466796875\n",
            "classify 1.333251953125\n",
            "classify 1.34283447265625\n",
            "classify 1.28961181640625\n",
            "classify 1.399999976158142\n",
            "0.265625\n",
            "0.1875\n",
            "0.28125\n",
            "0.3125\n",
            "0.2857142857142857\n",
            "strain 0.11754576861858368\n",
            "strain 0.19137832522392273\n",
            "strain 0.1971985101699829\n",
            "strain 0.18134188652038574\n",
            "strain 0.3073473572731018\n",
            "classify 1.336669921875\n",
            "classify 1.301513671875\n",
            "classify 1.321533203125\n",
            "classify 1.324462890625\n",
            "classify 1.3093750476837158\n",
            "0.296875\n",
            "0.234375\n",
            "0.3125\n",
            "0.25\n",
            "0.7142857142857143\n",
            "strain 0.09713320434093475\n",
            "strain 0.2643480896949768\n",
            "strain 0.21103370189666748\n",
            "strain 0.2133752554655075\n",
            "strain 0.18563345074653625\n",
            "classify 1.35418701171875\n",
            "classify 1.27313232421875\n",
            "classify 1.343994140625\n",
            "classify 1.32794189453125\n",
            "classify 1.2687499523162842\n",
            "0.28125\n",
            "0.25\n",
            "0.28125\n",
            "0.21875\n",
            "0.7142857142857143\n",
            "strain 0.13081665337085724\n",
            "strain 0.26384374499320984\n",
            "strain 0.29697468876838684\n",
            "strain 0.12738311290740967\n",
            "strain 0.04468945413827896\n",
            "classify 1.3150634765625\n",
            "classify 1.34283447265625\n",
            "classify 1.29833984375\n",
            "classify 1.32476806640625\n",
            "classify 1.4265625476837158\n",
            "0.296875\n",
            "0.34375\n",
            "0.3125\n",
            "0.1875\n",
            "0.42857142857142855\n",
            "strain 0.22359444200992584\n",
            "strain 0.20092642307281494\n",
            "strain 0.1314779818058014\n",
            "strain 0.23240496218204498\n",
            "strain 0.41176068782806396\n",
            "classify 1.3095703125\n",
            "classify 1.32464599609375\n",
            "classify 1.3214111328125\n",
            "classify 1.3365478515625\n",
            "classify 1.298437476158142\n",
            "0.34375\n",
            "0.25\n",
            "0.28125\n",
            "0.328125\n",
            "0.2857142857142857\n",
            "strain 0.1571817249059677\n",
            "strain 0.1646428406238556\n",
            "strain 0.1767740547657013\n",
            "strain 0.21376089751720428\n",
            "strain 0.152348130941391\n",
            "classify 1.32208251953125\n",
            "classify 1.34710693359375\n",
            "classify 1.33636474609375\n",
            "classify 1.31964111328125\n",
            "classify 1.1062500476837158\n",
            "0.3125\n",
            "0.28125\n",
            "0.265625\n",
            "0.34375\n",
            "0.5714285714285714\n",
            "strain 0.20192968845367432\n",
            "strain 0.26350903511047363\n",
            "strain 0.15005823969841003\n",
            "strain 0.20958974957466125\n",
            "strain 0.08032173663377762\n",
            "classify 1.36370849609375\n",
            "classify 1.3170166015625\n",
            "classify 1.349609375\n",
            "classify 1.2882080078125\n",
            "classify 1.435937523841858\n",
            "0.375\n",
            "0.25\n",
            "0.21875\n",
            "0.359375\n",
            "0.42857142857142855\n",
            "strain 0.3359142541885376\n",
            "strain 0.2005292922258377\n",
            "strain 0.1875022053718567\n",
            "strain 0.1734819859266281\n",
            "strain 0.14772744476795197\n",
            "classify 1.35638427734375\n",
            "classify 1.35198974609375\n",
            "classify 1.33319091796875\n",
            "classify 1.28131103515625\n",
            "classify 1.389062523841858\n",
            "0.359375\n",
            "0.265625\n",
            "0.296875\n",
            "0.328125\n",
            "0.14285714285714285\n",
            "strain 0.15309244394302368\n",
            "strain 0.13874147832393646\n",
            "strain 0.2842724323272705\n",
            "strain 0.2652941048145294\n",
            "strain 0.11736484616994858\n",
            "classify 1.29461669921875\n",
            "classify 1.3240966796875\n",
            "classify 1.3995361328125\n",
            "classify 1.29986572265625\n",
            "classify 1.4249999523162842\n",
            "0.296875\n",
            "0.375\n",
            "0.28125\n",
            "0.296875\n",
            "0.5714285714285714\n"
          ]
        }
      ],
      "source": [
        "# @title strain ctrain test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "# name = 'violet'\n",
        "# train_summary_writer = tf.summary.create_file_writer('logs/'+name+'/train')\n",
        "# test_summary_writer = tf.summary.create_file_writer('logs/'+name+'/test')\n",
        "\n",
        "def strain(model, dataloader, optim, scheduler=None):\n",
        "    model.train()\n",
        "    for i, (x, _) in enumerate(dataloader):\n",
        "        # x = x[...,1:].to(device).to(torch.bfloat16) # for wisdm?\n",
        "        x = x.to(device).to(torch.bfloat16)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            loss = model.loss(x)\n",
        "\n",
        "            # repr_loss, std_loss, cov_loss = model.loss(x)\n",
        "            # loss = model.sim_coeff * repr_loss + model.std_coeff * std_loss + model.cov_coeff * cov_loss\n",
        "\n",
        "        optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # total_norm = 0\n",
        "        # for p in model.parameters(): total_norm += p.grad.data.norm(2).item() ** 2\n",
        "        # total_norm = total_norm**.5\n",
        "        # print('total_norm', total_norm)\n",
        "        # for p in list(filter(lambda p: p.grad is not None, model.parameters())):\n",
        "        #     print(p.grad.data.norm(2).item())\n",
        "        # print(\"max grad norm\", max([p.grad.data.norm(2).item() for p in list(filter(lambda p: p.grad is not None, model.parameters()))]))\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 5) # 0.5\n",
        "\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            m=0.999 # 0.99 m = next(momentum_scheduler)\n",
        "            for param_q, param_k in zip(model.student.parameters(), model.teacher.parameters()):\n",
        "                param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        print(\"strain\",loss.item())\n",
        "        # for param in seq_jepa.context_encoder.cls: print(param.data)\n",
        "        # for param in seq_jepa.predicter.cls: print(param.data)\n",
        "        try: wandb.log({\"loss\": loss.item(), \"repr/I\": repr_loss.item(), \"std/V\": std_loss.item(), \"cov/C\": cov_loss.item()})\n",
        "        # try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        # with train_summary_writer.as_default(): tf.summary.scalar('strain', loss.item(), step=i)\n",
        "        if i>=500: break\n",
        "\n",
        "\n",
        "def ctrain(model, classifier, dataloader, coptim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.eval()\n",
        "    classifier.train()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        # x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        # x, y = x[...,1:].to(device).to(torch.bfloat16), y[1].to(device) # [batch, ] # (id, activity)\n",
        "        x, y = x.to(device).to(torch.bfloat16), y.to(device) # [batch, ] # (id, activity)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            with torch.no_grad():\n",
        "                sx = model(x).detach()\n",
        "            y_ = classifier(sx)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "        coptim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 5) # .5\n",
        "        scaler.step(coptim)\n",
        "        scaler.update()\n",
        "        print(\"classify\",loss.item())\n",
        "        try: wandb.log({\"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        # with test_summary_writer.as_default(): tf.summary.scalar('closs', loss.item(), step=i)\n",
        "        if i>=100: break\n",
        "\n",
        "\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        # x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        # x, y = x[...,1:].to(device).to(torch.float), y[1].to(device) # [batch, ] # (id, activity)\n",
        "        x, y = x.to(device).to(torch.float), y.to(device) # [batch, ] # (id, activity)\n",
        "        with torch.no_grad():\n",
        "            sx = model(x)\n",
        "            try:\n",
        "                rankme = RankMe(sx).item()\n",
        "                lidar = LiDAR(sx).item()\n",
        "            except NameError: pass\n",
        "            y_ = classifier(sx)\n",
        "        test_loss = F.cross_entropy(y_, y)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        print(correct/len(y))\n",
        "        try: wandb.log({\"correct\": correct/len(y)})\n",
        "        # try: wandb.log({\"correct\": correct/len(y), \"rankme\": rankme, \"lidar\": lidar})\n",
        "        except NameError: pass\n",
        "        if i>=100: break\n",
        "\n",
        "\n",
        "for i in range(5000): #\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # np.random.shuffle(train_indices); np.random.shuffle(val_indices) # for wisdm\n",
        "    # train_sampler, valid_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(val_indices)\n",
        "    # # batch_size = 64 #512\n",
        "    # train_loader = DataLoader(train_data, sampler=train_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "    # test_loader = DataLoader(train_data, sampler=valid_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True)\n",
        "\n",
        "    strain(seq_jepa, train_loader, optim)\n",
        "    ctrain(seq_jepa, classifier, train_loader, coptim)\n",
        "    test(seq_jepa, classifier, test_loader)\n",
        "\n",
        "    # strain(violet, train_loader, voptim)\n",
        "    # ctrain(violet, classifier, train_loader, coptim)\n",
        "    # test(violet, classifier, test_loader)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5p6LJ2qPqom"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir logs/gradient_tape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4J2ahp3wmqcg",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title supervised train test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "def strain(model, classifier, dataloader, optim, coptim, scheduler=None):\n",
        "    model.train()\n",
        "    classifier.train()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        # y = y.to(device)\n",
        "        # x, y = x[...,1:].to(device).to(torch.bfloat16), y[1].to(device) # [batch, ] # (id, activity)\n",
        "        x, y = x.to(device).to(torch.bfloat16), y.to(device) # [batch, ] # (id, activity)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        coptim.zero_grad()\n",
        "        # print(loss)\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # for p in list(filter(lambda p: p.grad is not None, model.parameters())):\n",
        "        #     print(p.grad.data.norm(2).item())\n",
        "        # print(\"max grad norm\", max([p.grad.data.norm(2).item() for p in list(filter(lambda p: p.grad is not None, model.parameters()))]))\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 10) # 0.5\n",
        "\n",
        "        scaler.step(optim)\n",
        "        scaler.step(coptim)\n",
        "        scaler.update()\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        print(\"strain\",loss.item())\n",
        "        # for param in seq_jepa.context_encoder.cls: print(param.data)\n",
        "        # for param in seq_jepa.predicter.cls: print(param.data)\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=50: break\n",
        "\n",
        "\n",
        "# def test(model, dataloader):\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        # x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        # x, y = x[...,1:].to(device).to(torch.float), y[1].to(device) # [batch, ] # (id, activity)\n",
        "        x, y = x.to(device).to(torch.float), y.to(device) # [batch, ] # (id, activity)\n",
        "        with torch.no_grad():\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "        loss = F.cross_entropy(y_, y)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        print(correct/len(y))\n",
        "        try: wandb.log({\"correct\": correct/len(y), \"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "for i in range(1000):\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # np.random.shuffle(train_indices); np.random.shuffle(val_indices)\n",
        "    # train_sampler, valid_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(val_indices)\n",
        "    # # batch_size = 64 #512\n",
        "    # train_loader = DataLoader(train_data, sampler=train_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "    # test_loader = DataLoader(train_data, sampler=valid_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True)\n",
        "\n",
        "    strain(seq_jepa, classifier, train_loader, optim, coptim)\n",
        "    test(seq_jepa, classifier, test_loader)\n",
        "    # strain(violet, train_loader, voptim)\n",
        "    # test(violet, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JT8AgOx0E_KM",
        "outputId": "19308cfe-2f31-472c-bfcc-0066ae14644f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# @title save/load\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# folder='/content/drive/MyDrive/jepa/'\n",
        "\n",
        "# # modelsd, optimsd = torch.load(folder+'SeqJEPA.pkl', map_location=device).values()\n",
        "# modelsd, optimsd = torch.load('SeqJEPA.pkl', map_location=device).values()\n",
        "# seq_jepa.load_state_dict(modelsd, strict=False)\n",
        "# optim.load_state_dict(optimsd)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "modelsd = torch.load(folder+'roberta.pkl', map_location=device)['model']#.values()\n",
        "# print(modelsd)\n",
        "model_mlm.load_state_dict(modelsd, strict=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpbLPvjiE-pD"
      },
      "outputs": [],
      "source": [
        "# checkpoint = {'model': seq_jepa.state_dict(), 'optimizer': optim.state_dict()}\n",
        "# checkpoint = {'model': model_mlm.state_dict()}\n",
        "# torch.save(checkpoint, folder+'roberta.pkl')\n",
        "# torch.save(checkpoint, folder+'SeqJEPA.pkl')\n",
        "# torch.save(checkpoint, 'SeqJEPA.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vScvFGGSeN6"
      },
      "source": [
        "## drawer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pQfM2fYvcTh6"
      },
      "outputs": [],
      "source": [
        "# @title masks\n",
        "import torch\n",
        "\n",
        "def multiblock(seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "    return target_mask\n",
        "\n",
        "def multiblock(seq, min_s, max_s, B=64, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    # mask_len = torch.rand(B, 1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_len = torch.rand(1, M) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(B, M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    # indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    indices = torch.arange(seq)[None,None,...] # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [B, M, seq]\n",
        "    return target_mask\n",
        "\n",
        "def multiblock2d(hw=(8,8), scale=(.15,.2), aspect_ratio=(.75,1.5), M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_aspect = torch.rand(1) * (aspect_ratio[1] - aspect_ratio[0]) + aspect_ratio[0] # in (min_s, max_s) # all blocks same size\n",
        "    mask_scale = torch.rand(1) * (scale[1] - scale[0]) + scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    h = (mask_scale/mask_aspect)**.5# h*(h*aspect) = scale\n",
        "    w = h * mask_aspect\n",
        "    h_pos, w_pos = torch.rand(M)*(1-w), torch.rand(M)*(1-h) # in (0, 1 - mask_len)\n",
        "    h_len, h_pos = (h*hw[0]).int(), h_pos*hw[0]\n",
        "    w_len, w_pos = (w*hw[1]).int(), w_pos*hw[1]\n",
        "    h_ind, w_ind = torch.arange(hw[0]).unsqueeze(0), torch.arange(hw[1]).unsqueeze(0) # [1, seq]\n",
        "    h_mask = (h_ind>=h_pos.unsqueeze(-1)) & (h_ind<(h_pos+h_len).unsqueeze(-1)) # [M, seq]\n",
        "    w_mask = (w_ind>=w_pos.unsqueeze(-1)) & (w_ind<(w_pos+w_len).unsqueeze(-1)) # [M, seq]\n",
        "    target_mask = h_mask.unsqueeze(-1) & w_mask.unsqueeze(-2) # [M, seq, seq]\n",
        "    return target_mask\n",
        "\n",
        "def multiblock2d(hw=(8,8), scale=(.15,.2), aspect_ratio=(.75,1.5), B=64, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_aspect = torch.rand(B, 1) * (aspect_ratio[1] - aspect_ratio[0]) + aspect_ratio[0] # in (min_s, max_s) # all blocks same size\n",
        "    mask_scale = torch.rand(B, 1) * (scale[1] - scale[0]) + scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    h = (mask_scale/mask_aspect)**.5# h*(h*aspect) = scale\n",
        "    w = h * mask_aspect\n",
        "    h_pos, w_pos = torch.rand(M)*(1-w), torch.rand(M)*(1-h) # in (0, 1 - mask_len)\n",
        "    h_len, h_pos = (h*hw[0]).int(), h_pos*hw[0]\n",
        "    w_len, w_pos = (w*hw[1]).int(), w_pos*hw[1]\n",
        "    h_ind, w_ind = torch.arange(hw[0]).unsqueeze(0), torch.arange(hw[1]).unsqueeze(0) # [1, seq]\n",
        "    h_mask = (h_ind>=h_pos.unsqueeze(-1)) & (h_ind<(h_pos+h_len).unsqueeze(-1)) # [M, seq]\n",
        "    w_mask = (w_ind>=w_pos.unsqueeze(-1)) & (w_ind<(w_pos+w_len).unsqueeze(-1)) # [M, seq]\n",
        "    target_mask = h_mask.unsqueeze(-1) & w_mask.unsqueeze(-2) # [M, seq, seq]\n",
        "    return target_mask\n",
        "\n",
        "# https://arxiv.org/pdf/2210.07224\n",
        "def randpatch(seq, mask_size=8, gamma=0.9): # num patches of seq, mask patch size, masking ratio\n",
        "    # mask = torch.rand(seq//mask_size)<gamma\n",
        "    length = seq//mask_size\n",
        "    g = torch.normal(gamma, std=.1, size=(1,)).clamp(.5,.9)\n",
        "    # g = gamma\n",
        "    idx = torch.randperm(length)[:int(length*g)]\n",
        "    mask = torch.zeros(length, dtype=bool)\n",
        "    mask[idx] = True\n",
        "    mask = mask.repeat_interleave(mask_size, dim=-1)\n",
        "    return mask # [seq] , True -> mask\n",
        "\n",
        "\n",
        "import torch\n",
        "def apply_masks(x, mask): # [b,t,d], [mask_size] # https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "    mask_keep = mask.unsqueeze(-1).repeat(x.size(0), 1, x.size(-1)) # [batch,T,dim]\n",
        "    return torch.gather(x, dim=1, index=mask_keep) # [batch,mask_size,dim]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9rPxvrrsYI_W"
      },
      "outputs": [],
      "source": [
        "# @title simplex\n",
        "# !pip install -q opensimplex\n",
        "import opensimplex\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def simplexmask(hw=(8,8), scale=(.15,.2)):\n",
        "    ix = iy = np.linspace(0, 1, num=8)\n",
        "    ix, iy = ix+np.random.randint(1e10), iy+np.random.randint(1e10)\n",
        "    y=opensimplex.noise2array(ix, iy)\n",
        "    y = torch.from_numpy(y)\n",
        "    mask_scale = torch.rand(1) * (scale[1] - scale[0]) + scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    yy = y.flatten().sort()[0][int(hw[0]*hw[1]*mask_scale)]\n",
        "    mask = (y<=yy.item())\n",
        "    return mask # T/F [h,w]\n",
        "\n",
        "def simplexmask1d(seq=512, scale=(.15,.2), chaos=2):\n",
        "    i = np.linspace(0, chaos, num=seq) # 2\n",
        "    j = np.random.randint(1e10, size=1)\n",
        "    y=opensimplex.noise2array(i, j) # [1, seq]\n",
        "    # plt.pcolormesh(y)\n",
        "    # plt.show()\n",
        "    y = torch.from_numpy(y)\n",
        "    # print(y.shape)\n",
        "    mask_scale = torch.rand(1) * (scale[1] - scale[0]) + scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    # print(a.shape, int(seq*mask_scale))\n",
        "    val, ind = y.sort()\n",
        "    yy = val[:,int(seq*mask_scale)]\n",
        "    mask = (y<=yy.item())\n",
        "    index = ind[:,:int(seq*mask_scale)]\n",
        "    return index, mask # T/F [h,w]\n",
        "\n",
        "def simplexmask1d(seq=512, ctx_scale=(.85,1), trg_scale=(.6,.8), B=64, chaos=2):\n",
        "    i = np.linspace(0, chaos, num=seq) # 2-5\n",
        "    j = np.random.randint(1e10, size=B)\n",
        "    noise = opensimplex.noise2array(i, j) # [B, seq]\n",
        "    # plt.pcolormesh(noise[:1])\n",
        "    # plt.rcParams[\"figure.figsize\"] = (20,3)\n",
        "    # plt.show()\n",
        "    noise = torch.from_numpy(noise)\n",
        "    ctx_mask_scale = torch.rand(1) * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    trg_mask_scale = torch.rand(1) * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "    val, ind = noise.sort()\n",
        "    trg_index = ind[:,-int(seq*trg_mask_scale):]\n",
        "    ctx_index = ind[:,-int(seq*ctx_mask_scale):-int(seq*trg_mask_scale)] # ctx wraps trg ; most similar to multiblock\n",
        "    # ctx_index = ind[:,:int(seq*ctx_mask_scale)-int(seq*trg_mask_scale)] # ctx hug bottom\n",
        "    # ctx_index = ind[:,-int(seq*ctx_mask_scale)-int(seq*trg_mask_scale):-int(seq*trg_mask_scale)] # ctx wraps trg ; most similar to multiblock\n",
        "    # ctx_index = ind[:,:int(seq*ctx_mask_scale)] # ctx hug bottom\n",
        "    return ctx_index, trg_index\n",
        "\n",
        "\n",
        "# mask = torch.zeros(1 ,200)\n",
        "# mask[:, trg_index[:1]] = 1\n",
        "# mask[:, ctx_index[:1]] = .5\n",
        "# plt.rcParams[\"figure.figsize\"] = (20,1)\n",
        "# plt.pcolormesh(mask)\n",
        "# plt.show()\n",
        "\n",
        "def simplexmask1d(seq=512, ctx_scale=(.85,1), trg_scale=(.6,.8), B=64, chaos=[1,.5]):\n",
        "    i = np.linspace(0, chaos[0], num=seq) # 2-5\n",
        "    noise = opensimplex.noise2array(i, np.random.randint(1e10, size=B)) # [B, seq]\n",
        "    noise = torch.from_numpy(noise)\n",
        "    ctx_mask_scale = torch.rand(1) * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    trg_mask_scale = torch.rand(1) * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "\n",
        "    ctx_len, trg_len = int(seq*ctx_mask_scale), int(seq*trg_mask_scale)\n",
        "    val, trg_index = torch.topk(noise, trg_len, dim=1, sorted=False)\n",
        "    ctx_len = ctx_len - trg_len\n",
        "\n",
        "    remove_mask = torch.ones((B,seq), dtype=bool) # [B, S]\n",
        "    remove_mask.scatter_(1, trg_index, False).flatten()\n",
        "    ind = torch.arange(seq).unsqueeze(0).repeat(B,1)[remove_mask].reshape(B, -1)\n",
        "    print(ind.shape, ind)\n",
        "\n",
        "    i = np.linspace(0, chaos[1], num=seq) # 2-5\n",
        "    noise = opensimplex.noise2array(i, np.random.randint(1e10, size=B)) # [B, seq]\n",
        "    noise = torch.from_numpy(noise)[remove_mask].reshape(B, -1)\n",
        "    val, ctx_ind = torch.topk(noise, ctx_len, dim=1, sorted=False)\n",
        "    print(ctx_ind.shape, ctx_ind)\n",
        "\n",
        "\n",
        "    ctx_index = ind[torch.arange(B).unsqueeze(-1), ctx_ind]\n",
        "    return ctx_index, trg_index\n",
        "\n",
        "\n",
        "# mask = simplexmask(hw=(8,8), scale=(.6,.8))\n",
        "# index, mask = simplexmask1d(seq=100, scale=(.7,.8))\n",
        "# ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.85,1), trg_scale=(.6,.8), B=64, chaos=5)\n",
        "# ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.85,1), trg_scale=(.7,.8), B=64, chaos=2)\n",
        "# ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.7,.9), trg_scale=(.6,.7), B=64, chaos=5)\n",
        "# ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.2,.3), trg_scale=(.4,.5), B=64, chaos=3)\n",
        "# ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.1,.3), trg_scale=(.4,.6), B=64, chaos=3)\n",
        "# # print(trg_index[0], ctx_index[0])\n",
        "\n",
        "ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.85,1), trg_scale=(.7,.8), B=64, chaos=[3,.5])\n",
        "mask = torch.zeros(b ,200)\n",
        "mask[torch.arange(b).unsqueeze(-1), trg_index] = 1\n",
        "mask[torch.arange(b).unsqueeze(-1), ctx_index] = .5\n",
        "mask = mask[None,...]\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "imshow(mask)\n",
        "import torchvision\n",
        "imshow(torchvision.utils.make_grid(mask, nrow=8))\n",
        "\n",
        "# print(index)\n",
        "# print(index.shape)\n",
        "# print(mask)\n",
        "# print(mask.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwpnHW4wn9S1",
        "outputId": "40413ecd-7f4b-49a2-984b-fe812ec57e26"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:08<00:00, 19.5MB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title data\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "transform = transforms.Compose([transforms.ToTensor(),])\n",
        "\n",
        "# MNIST CIFAR10\n",
        "train_data = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
        "test_data = torchvision.datasets.CIFAR10(root='data', train=False, download=True, transform=transform)\n",
        "batch_size = 128 # 128 1024\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    # print(npimg.shape) # (3, 64, 64)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# dataiter = iter(train_data)\n",
        "# x,y = next(dataiter)\n",
        "# print(x.shape) # [3, 32, 32]\n",
        "# imshow(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AtPUcVu2kSLI"
      },
      "outputs": [],
      "source": [
        "# @title TransformerClassifier\n",
        "import math\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class TransformerClassifier(nn.Module):\n",
        "    # def __init__(self, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, drop = 0.):\n",
        "    def __init__(self, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, drop=0):\n",
        "        super().__init__()\n",
        "        # self.embedding = nn.Embedding(ntoken, d_model)\n",
        "        self.pos_encoder = RoPE(d_model, seq_len=200, base=10000)\n",
        "        # d_hid = d_hid or d_model#*2\n",
        "        # encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, d_hid, drop, batch_first=True)\n",
        "        # self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "\n",
        "        out_dim = out_dim or d_model\n",
        "        self.lin = nn.Linear(d_model, out_dim)\n",
        "        # self.lin = nn.Linear(d_model*2, out_dim)\n",
        "        # self.cls = nn.Parameter(torch.randn(1,1,d_model))\n",
        "        self.attention_pool = nn.Linear(d_model, 1, bias=False)\n",
        "        # self.out = nn.Sequential(\n",
        "        #     nn.Dropout(drop), nn.Linear(d_model, 3), nn.SiLU(),\n",
        "        #     nn.Dropout(drop), nn.Linear(3, out_dim), nn.Sigmoid()\n",
        "        # )\n",
        "\n",
        "    def forward(self, x, src_key_padding_mask = None): # [batch, seq, d_model], [batch, seq] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # batch, seq_len, d_model = x.shape\n",
        "        # x = torch.cat([self.cls.repeat(batch,1,1), x], dim=1)\n",
        "        # src_key_padding_mask = torch.cat([torch.zeros((batch, 1), dtype=torch.bool), src_key_padding_mask], dim=1)\n",
        "        x = self.pos_encoder(x)\n",
        "\n",
        "        x = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
        "        # print(\"fwd\",out.shape) # float [batch, seq_len, d_model]\n",
        "        # out = x.mean(dim=1) # average pool\n",
        "        # out = x.max(dim=1)#[0]\n",
        "\n",
        "        attn = self.attention_pool(x).squeeze(-1) # [batch, seq] # seq_pool\n",
        "        out = (torch.softmax(attn, dim=-1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, seq] @ [batch, seq, dim] -> [batch, dim]\n",
        "        # out = self.out(out) # optional (from GlobalContext) like squeeze excitation\n",
        "\n",
        "        # out = out[:, 0] # first token\n",
        "        # out = torch.cat([out, mean_pool], dim=-1)\n",
        "\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "batch, seq_len, d_model = 4,7,512\n",
        "model = TransformerModel(in_dim, d_model, d_head=4, nlayers=2, dropout=0.).to(device)\n",
        "# model = TransformerClassifier(d_model, nhead=8, nlayers=2, dropout=0.).to(device)\n",
        "x =  torch.rand(batch, seq_len, d_model)\n",
        "src_key_padding_mask = torch.stack([(torch.arange(seq_len) < seq_len - v) for v in torch.randint(seq_len, (batch,))]) # True will be ignored # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "print(src_key_padding_mask)\n",
        "out = model(x, src_key_padding_mask)\n",
        "print(out.shape)\n",
        "\n",
        "# GlobalAveragePooling1D layer, followed by a Dense layer. The final output of the transformer is produced by a softmax layer,\n",
        "# x = GlobalAveragePooling1D()(x)\n",
        "# x = Dropout(0.1)(x)\n",
        "# x = Dense(20, activation=\"relu\")(x)\n",
        "# x = Dropout(0.1)(x)\n",
        "# outputs = Dense(2, activation=\"softmax\")(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-8i1WLsvH_vn"
      },
      "outputs": [],
      "source": [
        "# @title Transformer Model\n",
        "import torch\n",
        "from torch import nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        # self.embed = nn.Sequential(\n",
        "        #     nn.Linear(in_dim, d_model), #act,\n",
        "        #     # nn.Linear(d_model, d_model), act,\n",
        "        # )\n",
        "        self.embed = nn.Linear(in_dim, d_model) if in_dim != d_model else None\n",
        "        # self.embed = nn.Sequential(nn.Conv1d(in_dim,d_model,7,2,7//2), nn.MaxPool1d(3, 2, 3//2))\n",
        "        self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, d_hid or d_model, dropout, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.d_model = d_model\n",
        "        self.cls = nn.Parameter(torch.zeros(1,1,d_model)) # randn\n",
        "        out_dim = out_dim or d_model\n",
        "        # self.lin = nn.Linear(d_model, out_dim)\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim != d_model else None\n",
        "        self.norm = nn.LayerNorm(out_dim)\n",
        "\n",
        "\n",
        "    def forward(self, src, src_key_padding_mask=None, cls_mask=None, context_indices=None, trg_indices=None): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # if cls_mask != None: src[cls_mask] = self.cls.to(src.dtype)\n",
        "\n",
        "        if self.embed != None: src = self.embed(src) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = src.shape\n",
        "        # src = self.pos_encoder(src)\n",
        "        if context_indices != None:\n",
        "            # print(src.shape, context_indices.shape, self.pos_encoder(context_indices).shape) # [2, 88, 32]) torch.Size([88]) torch.Size([88, 32]\n",
        "            # print(src[0], self.pos_encoder(context_indices)[0])\n",
        "            src = src * self.pos_encoder(context_indices) # context/predictor # src = src + self.positional_emb[:,context_indices]\n",
        "            # print(src[0])\n",
        "        else: src = src * self.pos_encoder(torch.arange(seq, device=device)) # target # src = src + self.positional_emb[:,:seq]\n",
        "            # print(\"trans fwd\", src.shape, self.pos_encoder(src).shape)\n",
        "\n",
        "        if trg_indices != None: # [M, num_trg_toks]\n",
        "            pred_tokens = self.cls * self.pos_encoder(trg_indices) # [M, num_trg_toks, d_model] # pred_tokens = self.cls + self.positional_emb[0,trg_indices]\n",
        "            pred_tokens = pred_tokens.repeat(batch, 1, 1) # [batch*M, num_trg_toks, d_model]\n",
        "            # print(pred_tokens.requires_grad)\n",
        "            src = src.repeat_interleave(trg_indices.shape[0], dim=0) # [batch, seq_len, d_model] -> [batch*M, seq_len, d_model]\n",
        "            src = torch.cat([src, pred_tokens], dim=1) # [batch*M, seq_len+num_trg_toks, d_model]\n",
        "\n",
        "        out = self.transformer_encoder(src, src_key_padding_mask=src_key_padding_mask) # float [seq_len, batch_size, d_model]\n",
        "        if trg_indices != None:\n",
        "            # print(out.shape)\n",
        "            out = out[:,seq:] # [batch*M, num_trg_toks, d_model]\n",
        "        if self.lin != None: out = self.lin(out)\n",
        "        out = self.norm(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "batch, seq_len, d_model = 4,7,64\n",
        "in_dim = 3\n",
        "model = TransformerModel(in_dim, d_model, nhead=8, nlayers=2, dropout=0.).to(device)\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "# print(out.shape)\n",
        "# # print(out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## mae"
      ],
      "metadata": {
        "id": "5tqJqL5Vj2vL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDJvKFcMGIa7",
        "outputId": "df899e65-2888-4fa3-b772-11c7af625a0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "38128\n",
            "torch.Size([4, 110, 16])\n"
          ]
        }
      ],
      "source": [
        "# @title mae me enc,dec\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def random_masking(length, mask_ratio, b=64):\n",
        "    noise = torch.rand(b, length)\n",
        "    len_mask = int(length * mask_ratio)\n",
        "    _, msk_ind = torch.topk(selected_probs, k=len_mask, dim=-1, sorted=False) # val, ind -> [b,len_mask]\n",
        "    _, keep_ind = torch.topk(selected_probs, k=length-len_mask, largest=False, dim=-1, sorted=False) # val, ind -> [b,len_keep]\n",
        "    return msk_ind, keep_ind\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, n_heads=4, d_hid=None, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, 200, d_model)*.02)\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=200, base=10000), requires_grad=False)\n",
        "\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, n_heads=n_heads) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.randn(1,1,d_model)*0.02) # randn zeros\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim)# if out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = x.shape\n",
        "        # x = x * self.pos_enc(context_indices)\n",
        "        # print(\"Trans pred\",x.shape, self.pos_emb[0,context_indices].shape)\n",
        "        x = x + self.pos_emb[0,context_indices]\n",
        "        # print('pred fwd', self.pos_emb[:,context_indices].shape)\n",
        "\n",
        "        # pred_tokens = self.cls * self.pos_enc(trg_indices) # [M, num_trg_toks, d_model]\n",
        "        pred_tokens = self.cls + self.pos_emb[0,trg_indices]\n",
        "        # print(\"pred fwd\", x.shape, pred_tokens.shape)\n",
        "        x = torch.cat([x, pred_tokens], dim=1) # [batch, seq_len+num_trg_toks, d_model]\n",
        "        out = self.transformer(x)\n",
        "\n",
        "        out = self.norm(out)\n",
        "        out = out[:,seq:] # [batch, num_trg_toks, d_model]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, drop=0):\n",
        "    def __init__(self, patch_size, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0):\n",
        "        super().__init__()\n",
        "        # patch_size=32\n",
        "        self.embed = nn.Sequential(\n",
        "            # # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.MaxPool1d(2,2), #nn.MaxPool1d(3, 2, 3//2),\n",
        "            # # nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.Conv1d(d_model, d_model,3,2,3//2)\n",
        "            nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2),\n",
        "            # nn.Conv1d(in_dim, d_model, patch_size, patch_size), # like patch\n",
        "\n",
        "            # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.Dropout(drop), nn.BatchNorm1d(d_model), snake,\n",
        "            # lstm\n",
        "\n",
        "            )\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, 200, d_model)*.02)\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=200, base=10000), requires_grad=False)#.unsqueeze(0)\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, n_heads=n_heads) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "        # try: print(\"Trans fwd\",x.shape, context_indices.shape)\n",
        "        # except: print(\"Trans fwd noind\",x.shape)\n",
        "        # x = self.pos_enc(x)\n",
        "        x = x + self.pos_emb[:,:x.shape[1]]\n",
        "        if context_indices != None: x = x[torch.arange(x.shape[0]).unsqueeze(-1), context_indices] # [batch, num_context_toks, d_model]\n",
        "\n",
        "        # print(\"TransformerModel\",x.shape)\n",
        "        x = self.transformer(x)\n",
        "        out = self.norm(x)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "batch, seq_len, d_model = 4,3500,16\n",
        "in_dim = 3\n",
        "patch_size=32\n",
        "model = TransformerModel(patch_size, in_dim, d_model, n_heads=4, nlayers=10, drop=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "# # # print(out)\n",
        "# model = TransformerPredictor(in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1).to(device)\n",
        "# out = model(out)\n",
        "# print(out.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PE3XAPkCZ2oM",
        "outputId": "d9395c50-9307-46bb-c035-e5bc63f2fa23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "109920\n",
            "torch.Size([])\n"
          ]
        }
      ],
      "source": [
        "# @title MAE me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class SeqJEPA(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, nlayers=2, n_heads=4):\n",
        "        super().__init__()\n",
        "        if out_dim is None: out_dim = d_model\n",
        "        self.patch_size = 32\n",
        "        self.student = TransformerModel(self.patch_size, in_dim, d_model, out_dim=out_dim, n_heads=n_heads, nlayers=nlayers, drop=0.)\n",
        "        self.predicter = TransformerPredictor(out_dim, d_model//2, out_dim, n_heads=4, nlayers=1, drop=0.)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "        # self.transform = RandomResizedCrop1d(3500, scale=(.8,1.))\n",
        "\n",
        "    def loss(self, x): # [b,t,d]\n",
        "        b,t,d = x.shape\n",
        "        msk_ind, keep_ind = random_masking(length, mask_ratio, b=b)\n",
        "\n",
        "        sx = self.student(x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "        # print('seq_jepa loss sx',sx.shape)\n",
        "        sy_ = self.predicter(sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "        sy_ = F.layer_norm(sy_, (sy_.size(-1),))\n",
        "        with torch.no_grad():\n",
        "            sy = self.teacher(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "            sy = sy[torch.arange(sy.shape[0]).unsqueeze(-1), trg_indices] # [batch, num_context_toks, d_model] # nan bec len(trg_ind)==0 # print('loss sy',torch.isnan(sy).any())\n",
        "            sy = F.layer_norm(sy, (sy.size(-1),))\n",
        "        loss = F.mse_loss(sy, sy_)\n",
        "        return loss\n",
        "\n",
        "\n",
        "msk_ind, keep_ind\n",
        "\n",
        "        sx = self.encoder(x, context_indices=keep_ind) # [batch, num_context_toks, out_dim]\n",
        "        x_ = self.decoder(sx, context_indices=keep_ind, msk_ind) # [batch, num_context_toks, out_dim]\n",
        "\n",
        "        loss = (pred - target) ** 2\n",
        "        loss = loss.mean(dim=-1)  # [N, L], mean loss per patch\n",
        "        loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches\n",
        "\n",
        "pred, target =\n",
        "        # loss = F.mse_loss(pred, target)\n",
        "\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        sx = self.student(x)\n",
        "        out = sx.mean(dim=1)\n",
        "        return out\n",
        "\n",
        "# min_s=0.15, max_s, M\n",
        "# trg.15.2M4 C.85 1\n",
        "\n",
        "# 1e-2,1e-3 < 3e-3,1e-3\n",
        "# patch16 < patch32\n",
        "# NoPE good but sus\n",
        "\n",
        "# ctx/trg sacle min/max, num blk,\n",
        "\n",
        "\n",
        "# seq_jepa = SeqJEPA(in_dim=3, d_model=32, out_dim=16, nlayers=4, n_heads=4).to(device)#.to(torch.float)\n",
        "seq_jepa = SeqJEPA(in_dim=3, d_model=64, out_dim=16, nlayers=1, n_heads=8).to(device)#.to(torch.float)\n",
        "optim = torch.optim.AdamW(seq_jepa.parameters(), lr=1e-3) # 1e-3?\n",
        "# optim = torch.optim.AdamW([{'params': seq_jepa.student.parameters()},\n",
        "#     {'params': seq_jepa.predicter.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # default 1e-2, 5e-2\n",
        "    # {'params': seq_jepa.predicter.parameters(), 'lr': 1e-2}], lr=1e-3, weight_decay=1e-2)\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/configs/in1k_vith14_ep300.yaml\n",
        "# d_model 1024,384\n",
        "# depth 12,6/12\n",
        "# wd 5e-2 - 4e-1\n",
        "# adamw 1e-4 - 1e-3 - 1e-6\n",
        "# ema 0.996-1\n",
        "\n",
        "print(sum(p.numel() for p in seq_jepa.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.parameters())) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.predicter.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.student.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.teacher.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# d_model^2 * nlayers\n",
        "\n",
        "x = torch.rand((24, 3500, in_dim), device=device)\n",
        "out = seq_jepa.loss(x)\n",
        "print(out.shape)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(16, 18).to(device)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n",
        "# optim = torch.optim.AdamW([{'params': seq_jepa.parameters()}, {'params': classifier.parameters(), 'lr': 1e-3}], lr=1e-3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ABygeAwL5N-6"
      },
      "outputs": [],
      "source": [
        "# @title facebookresearch/mae models_mae.py\n",
        "# https://github.com/facebookresearch/mae/blob/main/models_mae.py\n",
        "from functools import partial\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from timm.models.vision_transformer import PatchEmbed, Block\n",
        "\n",
        "\n",
        "class MaskedAutoencoderViT(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3,\n",
        "                 embed_dim=1024, depth=24, num_heads=16,\n",
        "                 decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
        "                 mlp_ratio=4., norm_layer=nn.LayerNorm, norm_pix_loss=False):\n",
        "        super().__init__()\n",
        "\n",
        "        # --------------------------------------------------------------------------\n",
        "        # MAE encoder specifics\n",
        "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        # self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True, qk_scale=None, norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "        # --------------------------------------------------------------------------\n",
        "\n",
        "        # --------------------------------------------------------------------------\n",
        "        # MAE decoder specifics\n",
        "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
        "        # self.decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, decoder_embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
        "\n",
        "        self.decoder_blocks = nn.ModuleList([\n",
        "            Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=True, qk_scale=None, norm_layer=norm_layer)\n",
        "            for i in range(decoder_depth)])\n",
        "\n",
        "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
        "        self.decoder_pred = nn.Linear(decoder_embed_dim, patch_size**2 * in_chans, bias=True) # decoder to patch\n",
        "        # --------------------------------------------------------------------------\n",
        "\n",
        "    def random_masking(self, x, mask_ratio):\n",
        "        \"\"\"\n",
        "        Perform per-sample random masking by per-sample shuffling.\n",
        "        Per-sample shuffling is done by argsort random noise.\n",
        "        x: [N, L, D], sequence\n",
        "        \"\"\"\n",
        "        N, L, D = x.shape  # batch, length, dim\n",
        "        len_keep = int(L * (1 - mask_ratio))\n",
        "\n",
        "        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n",
        "        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
        "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
        "\n",
        "        # keep the first subset\n",
        "        ids_keep = ids_shuffle[:, :len_keep]\n",
        "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
        "\n",
        "        # generate the binary mask: 0 is keep, 1 is remove\n",
        "        mask = torch.ones([N, L], device=x.device)\n",
        "        mask[:, :len_keep] = 0\n",
        "        # unshuffle to get the binary mask\n",
        "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
        "        return x_masked, mask, ids_restore\n",
        "\n",
        "    def forward_encoder(self, x, mask_ratio):\n",
        "        x = self.patch_embed(x)\n",
        "        x = x + self.pos_embed[:, 1:, :]\n",
        "\n",
        "        # masking: length -> length * mask_ratio\n",
        "        x, mask, ids_restore = self.random_masking(x, mask_ratio)\n",
        "\n",
        "        # append cls token\n",
        "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
        "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        for blk in self.blocks: x = blk(x)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        return x, mask, ids_restore\n",
        "\n",
        "    def forward_decoder(self, x, ids_restore):\n",
        "        x = self.decoder_embed(x)\n",
        "\n",
        "        # append mask tokens to sequence\n",
        "        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
        "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
        "        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
        "        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
        "\n",
        "        x = x + self.decoder_pos_embed\n",
        "\n",
        "        for blk in self.decoder_blocks: x = blk(x)\n",
        "        x = self.decoder_norm(x)\n",
        "        x = self.decoder_pred(x)\n",
        "        x = x[:, 1:, :]\n",
        "        return x\n",
        "\n",
        "    def forward_loss(self, imgs, pred, mask):\n",
        "        \"\"\"\n",
        "        imgs: [N, 3, H, W]\n",
        "        pred: [N, L, p*p*3]\n",
        "        mask: [N, L], 0 is keep, 1 is remove,\n",
        "        \"\"\"\n",
        "        target = self.patchify(imgs)\n",
        "        # if self.norm_pix_loss:\n",
        "        #     mean = target.mean(dim=-1, keepdim=True)\n",
        "        #     var = target.var(dim=-1, keepdim=True)\n",
        "        #     target = (target - mean) / (var + 1.e-6)**.5\n",
        "\n",
        "        loss = (pred - target) ** 2\n",
        "        loss = loss.mean(dim=-1)  # [N, L], mean loss per patch\n",
        "        # loss = F.mse_loss(pred, target)\n",
        "\n",
        "        loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches\n",
        "        return loss\n",
        "\n",
        "    def forward(self, imgs, mask_ratio=0.75):\n",
        "        latent, mask, ids_restore = self.forward_encoder(imgs, mask_ratio)\n",
        "        pred = self.forward_decoder(latent, ids_restore)  # [N, L, p*p*3]\n",
        "        loss = self.forward_loss(imgs, pred, mask)\n",
        "        return loss, pred, mask\n",
        "\n",
        "\n",
        "model = MaskedAutoencoderViT(\n",
        "    patch_size=16, embed_dim=768, depth=12, num_heads=12, # B16\n",
        "    # patch_size=16, embed_dim=1024, depth=24, num_heads=16, # L16\n",
        "    # patch_size=14, embed_dim=1280, depth=32, num_heads=16, # H14\n",
        "    decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
        "    mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## text classification, roberta"
      ],
      "metadata": {
        "id": "rpBQCgArjj7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU datasets"
      ],
      "metadata": {
        "id": "DkiHS6yPkaVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "o4xSm_eyhSK0",
        "outputId": "23c3cd4c-7330-4292-fbee-0a1d1cef4d38"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'train_tok' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-8fcae1a8af8a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_side\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_tok' is not defined"
          ]
        }
      ],
      "source": [
        "# @title yelp data\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataset = load_dataset(\"yelp_polarity\") # yelp_polarity yelp_review_full\n",
        "# # dataset = load_dataset(\"yelp_review_full\") # yelp_polarity yelp_review_full\n",
        "# print(dataset[\"train\"][0]) # {'text': \"Unfortunately, the ... to give Dr. Goldberg 2 stars.\", 'label': 0}\n",
        "# print(len(dataset))\n",
        "# print(len(dataset[\"train\"]))\n",
        "# # train_text = dataset[\"train\"][:10]['text']\n",
        "# train_text = dataset[\"train\"]['text']\n",
        "# # train_tok = [enc.encode(text) for text in train_text]\n",
        "# train_tok = [torch.tensor(enc.encode(text)) for text in train_text]\n",
        "# # print(train_tok)\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    train_text = dataset[\"train\"]['text']\n",
        "    train_tok = [torch.tensor(enc.encode(text)) for text in train_text]\n",
        "\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "# def tokenize(example): return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\")\n",
        "# tokenized = dataset.map(tokenize, batched=True)\n",
        "# tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "\n",
        "# train_loader = DataLoader(tokenized[\"train\"], batch_size=32, shuffle=True)\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "x = pad_sequence(train_tok, batch_first=True, padding_value=0, padding_side='left')\n",
        "print(x)\n",
        "\n",
        "def left_pad(batch, pad_value=0):\n",
        "    # batch: list of 1D tensors\n",
        "    lengths = torch.tensor([len(x) for x in batch])\n",
        "    max_len = lengths.max()\n",
        "\n",
        "    # Preallocate padded tensor\n",
        "    # padded = torch.full((len(batch), max_len), pad_value, dtype=batch[0].dtype)\n",
        "    padded = torch.full((len(batch), max_len), pad_value)\n",
        "\n",
        "    # for i, x in enumerate(batch):\n",
        "    #     padded[i, -x.size(0):] = x  # align to right, pad left\n",
        "    padded[torch.arange(len(batch)).unsqueeze(-1), -lengths:] = batch\n",
        "    return padded, lengths\n",
        "\n",
        "# left_pad(train_tok)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HZejCTckoQG",
        "outputId": "8a7253cf-65bc-471e-8b90-3c1590594fc3",
        "cellView": "form"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[31373, 995] hello world\n"
          ]
        }
      ],
      "source": [
        "# @title tiktoken\n",
        "# https://github.com/openai/tiktoken/tree/main\n",
        "import tiktoken\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\") # gpt2 r50k_base p50k_base p50k_edit cl100k_base o200k_base # https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py\n",
        "# enc = tiktoken.encoding_for_model(\"gpt-4o\") # https://github.com/openai/tiktoken/blob/main/tiktoken/model.py#L24\n",
        "tok = enc.encode(\"hello world\")\n",
        "out = enc.decode(tok)\n",
        "print(tok, out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "i1iatz1SSK3s"
      },
      "outputs": [],
      "source": [
        "# @title tiktoken dataloader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import tiktoken # https://github.com/openai/tiktoken/tree/main\n",
        "\n",
        "\n",
        "# train_text = dataset[\"train\"]['text']\n",
        "# # train_tok = [enc.encode(text) for text in train_text]\n",
        "# train_tok = [torch.tensor(enc.encode(text)) for text in train_text]\n",
        "\n",
        "\n",
        "class CharDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, raw_data, seq_len):\n",
        "        # data = ''.join(raw_data)\n",
        "        # data = raw_data['text']\n",
        "        self.enc = tiktoken.get_encoding(\"gpt2\") # https://github.com/openai/tiktoken/blob/main/tiktoken/core.py\n",
        "        self.vocab_size = self.enc.n_vocab # gpt2:50257\n",
        "        self.data = self.data_process(data) # list of int\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def data_process(self, data): # str 10780437\n",
        "        return torch.tensor(self.enc.encode(data))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)//(self.seq_len+1)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        dix = self.data[idx*(self.seq_len+1) : (idx+1)*(self.seq_len+1)]\n",
        "        x, y = dix[:-1], dix[1:]\n",
        "        return x, y\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
        "\n",
        "train_data = CharDataset(dataset[\"train\"], seq_len) # one line of poem is roughly 50 characters\n",
        "\n",
        "\n",
        "seq_len = 100 # 128\n",
        "train_data = CharDataset(text, seq_len) # one line of poem is roughly 50 characters\n",
        "test_data = CharDataset(test_text, seq_len) # one line of poem is roughly 50 characters\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 64 #512\n",
        "train_loader = DataLoader(train_data, shuffle=True, pin_memory=True, batch_size=batch_size, num_workers=2) # num_workers = 4\n",
        "test_loader = DataLoader(test_data, shuffle=True, pin_memory=True, batch_size=batch_size, num_workers=0)\n",
        "\n",
        "# https://github.com/openai/tiktoken/blob/main/tiktoken/core.py\n",
        "def encode(context):\n",
        "    if type(context) == str: return torch.tensor([train_loader.dataset.enc.encode(context)], device=device)\n",
        "    elif type(context) == list: return train_loader.dataset.enc.encode_batch(context)\n",
        "    else: raise Exception\n",
        "def decode(x): return train_loader.dataset.enc.decode(list(x))\n",
        "# for x,y in train_loader:\n",
        "#     break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVHe2tXTmacN",
        "outputId": "63efc423-8a46-490e-885b-4cdb9751cc8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 31414, 232, 2]\n",
            "[0, 20920, 232, 2]\n",
            "{'input_ids': tensor([[    0, 31414,     6,   127,  2335,    16, 11962,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.94"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# @title hf roberta\n",
        "# https://huggingface.co/docs/transformers/en/model_doc/roberta\n",
        "import torch\n",
        "from transformers import AutoTokenizer, RobertaConfig, RobertaModel\n",
        "from transformers import RobertaForMaskedLM, RobertaForSequenceClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
        "print(tokenizer(\"Hello world\")[\"input_ids\"])\n",
        "print(tokenizer(\" Hello world\")[\"input_ids\"])\n",
        "# {'input_ids': tensor([[    0,   133,   812,     9,  1470,    16, 50264,     4,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
        "\n",
        "config = RobertaConfig()\n",
        "# model = RobertaModel(config)\n",
        "model = RobertaForMaskedLM(config)\n",
        "# model = RobertaForSequenceClassification(config)\n",
        "\n",
        "# inputs = tokenizer(\"The capital of France is <mask>.\", return_tensors=\"pt\")\n",
        "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
        "print(inputs)\n",
        "with torch.no_grad():\n",
        "    # logits = model(**inputs).logits\n",
        "    logits = model(**inputs)\n",
        "\n",
        "# LM: last_hidden_state, pooler_output, hidden_states=None, past_key_values=None, attentions=None, cross_attentions\n",
        "\n",
        "\n",
        "# predicted_class_id = logits.argmax().item()\n",
        "# model.config.id2label[predicted_class_id]\n",
        "\n",
        "# # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n",
        "# num_labels = len(model.config.id2label)\n",
        "# # model = RobertaForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-emotion\", num_labels=num_labels)\n",
        "# model = RobertaForSequenceClassification(config)\n",
        "# labels = torch.tensor([1])\n",
        "# loss = model(**inputs, labels=labels).loss\n",
        "# round(loss.item(), 2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # print(logits.keys())\n",
        "# # retrieve index of <mask>\n",
        "# mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
        "\n",
        "# predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
        "# tokenizer.decode(predicted_token_id)\n",
        "\n",
        "# labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\")[\"input_ids\"]\n",
        "# # mask labels of non-<mask> tokens\n",
        "# labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)\n",
        "\n",
        "# outputs = model(**inputs, labels=labels)\n",
        "# round(outputs.loss.item(), 2)\n",
        "\n",
        "# last_hidden_states = outputs.last_hidden_state\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLiCsK97Tz3H",
        "outputId": "d839e6f6-f7bf-4d24-fdc8-4a6fe602a2ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': [0, 31414, 232, 2], 'attention_mask': [1, 1, 1, 1]}\n",
            "{'input_ids': [[0, 31414, 232, 2], [0, 36807, 571, 306, 2]], 'attention_mask': [[1, 1, 1, 1], [1, 1, 1, 1, 1]]}\n"
          ]
        }
      ],
      "source": [
        "# print(logits)\n",
        "# # loss = model(**inputs, labels=labels).loss\n",
        "# loss = model(**inputs)\n",
        "\n",
        "# MaskedLMOutput = loss, logits, hidden_states=None, attentions\n",
        "# print(tokenizer(\"Hello world\")[\"input_ids\"])\n",
        "print(tokenizer(\"Hello world\")) # input_ids attention_mask\n",
        "print(tokenizer([\"Hello world\",\"dfg4\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtKHgsmyvj5G",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title hf data\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "dataset = load_dataset(\"yelp_polarity\") # yelp_polarity yelp_review_full\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
        "\n",
        "import os\n",
        "def tokenize(examples): return tokenizer(examples[\"text\"], truncation=True, max_length=256)\n",
        "tok_dataset = dataset.map(tokenize, batched=True, num_proc=os.cpu_count(), # Use multiple processes for faster tokenization\n",
        "    remove_columns=[\"text\"] # Remove the original text column\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4yn7hlrBREhk"
      },
      "outputs": [],
      "source": [
        "# @title gemini roberta\n",
        "from transformers import AutoTokenizer, RobertaConfig, RobertaForMaskedLM\n",
        "\n",
        "# config = RobertaConfig() # vocab_size = 50265, hidden_size = 768, num_hidden_layers = 12, num_attention_heads = 12, intermediate_size = 3072, hidden_act = 'gelu', hidden_dropout_prob = 0.1, attention_probs_dropout_prob = 0.1, max_position_embeddings = 512, type_vocab_size = 2, initializer_range = 0.02, layer_norm_eps = 1e-12, pad_token_id = 1, bos_token_id = 0eos_token_id = 2, position_embedding_type = 'absolute'\n",
        "config = RobertaConfig(vocab_size = 50265, hidden_size = 64, num_hidden_layers = 1, num_attention_heads = 8, intermediate_size = 256, hidden_act = 'gelu', hidden_dropout_prob = 0., attention_probs_dropout_prob = 0.)\n",
        "model_mlm = RobertaForMaskedLM(config)\n",
        "\n",
        "\n",
        "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "collator = DataCollatorForLanguageModeling(tokenizer) # Masked Language Model (MLM); .15,.8,.1 # https://huggingface.co/docs/transformers/en/main_classes/data_collator#transformers.DataCollatorForLanguageModeling\n",
        "import torch\n",
        "\n",
        "train_args = TrainingArguments(\n",
        "    # output_dir=MODEL_OUTPUT_DIR_STAGE1, overwrite_output_dir=True,\n",
        "    num_train_epochs=1,\n",
        "    prediction_loss_only=True, # Only compute loss, no predictions during eval\n",
        "    # optim='adamw_torch',\n",
        "    optim='adamw_torch_fused',\n",
        "    learning_rate=3e-4,\n",
        "#     lr_scheduler_type (str or SchedulerType, optional, defaults to \"linear\") — The scheduler type to use. See the documentation of SchedulerType for all possible values.\n",
        "# lr_scheduler_kwargs (‘dict’, optional, defaults to {}) —\n",
        "    # warmup_steps=0.1 * NUM_TRAIN_EPOCHS_STAGE1 * (len(tokenized_dataset_mlm) // PER_DEVICE_BATCH_SIZE), # 10% warmup\n",
        "    warmup_ratio=0.1,\n",
        "    # weight_decay=0.01,\n",
        "    # report_to=\"tensorboard\",\n",
        "    report_to=\"wandb\", # 487a2109e55dce4e13fc70681781de9f50f27be7\n",
        "    fp16=torch.cuda.is_available(), # Enable mixed precision if GPU is available\n",
        ") # https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments\n",
        "\n",
        "# half_precision_backend=\"auto\"\n",
        "# bf16=True\n",
        "\n",
        "# trainer_mlm = Trainer(model=model_mlm, args=train_args, train_dataset=tok_dataset['train'].remove_columns(\"label\"), data_collator=collator)\n",
        "# trainer_mlm.train()\n",
        "\n",
        "# # trainer_mlm.save_model(MODEL_OUTPUT_DIR_STAGE1)\n",
        "\n",
        "# eval_results = trainer_mlm.evaluate()\n",
        "# perplexity = math.exp(eval_results[\"eval_loss\"])\n",
        "# print('perplexity', perplexity)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iISjeDVDugDQ",
        "outputId": "72490fdd-f071-4424-d149-d70e4a77dd4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 1, 0, 0]\n",
            "[[0, 16861, 6, 5, 8413, 9, 145, 925, 4, 18835, 18, 3186, 16, 10, 7230, 9, 5, 676, 38, 348, 56, 19, 98, 171, 97, 3333, 11, 14415, 480, 205, 3299, 6, 6587, 813, 4, 1437, 85, 1302, 14, 39, 813, 1622, 393, 5274, 5, 1028, 4, 1437, 85, 2333, 1239, 132, 722, 9, 6636, 1765, 7, 120, 41, 1948, 4, 1437, 3394, 34, 86, 13, 14, 50, 1072, 7, 432, 19, 24, 116, 1437, 38, 33, 422, 88, 42, 936, 19, 171, 97, 3333, 8, 38, 95, 218, 75, 120, 24, 4, 1437, 370, 33, 558, 1138, 6, 47, 33, 1484, 19, 1131, 782, 6, 596, 965, 75, 1268, 15635, 5, 1028, 116, 1437, 85, 18, 42494, 8, 45, 173, 5, 29223, 1258, 4, 1437, 85, 18, 19, 9917, 14, 38, 619, 14, 38, 33, 7, 492, 925, 4, 18835, 132, 2690, 4, 2], [0, 9325, 225, 164, 7, 925, 4, 18835, 13, 81, 158, 107, 4, 38, 206, 38, 21, 65, 9, 39, 112, 620, 1484, 77, 37, 554, 23, 24294, 21963, 4, 91, 18, 57, 372, 81, 5, 107, 8, 16, 269, 70, 59, 5, 380, 2170, 4, 85, 16, 142, 9, 123, 6, 45, 127, 122, 320, 821, 3892, 925, 4, 1190, 1529, 6, 14, 38, 303, 66, 38, 33, 19961, 1001, 7823, 4, 91, 17384, 70, 1735, 19, 47, 8, 16, 182, 3186, 8, 2969, 4, 91, 630, 75, 1679, 8, 6990, 70, 5, 235, 1142, 4, 12178, 10675, 8, 1072, 7, 28, 1682, 11, 5, 14018, 15, 358, 6659, 9, 110, 1131, 474, 8, 110, 301, 4, 2], [0, 100, 218, 75, 216, 99, 925, 4, 18835, 21, 101, 137, 1437, 1375, 7, 2605, 6, 53, 905, 162, 1137, 47, 6, 4062, 2547, 18463, 2547, 31, 42, 3299, 8, 42, 558, 4, 38, 21, 164, 7, 925, 4, 1436, 137, 37, 314, 8, 18835, 362, 81, 77, 1436, 314, 4, 91, 16, 45, 10, 10837, 3299, 4, 91, 16, 129, 2509, 11, 5, 1029, 12, 14066, 8, 519, 47, 283, 11, 13, 8456, 4885, 5622, 358, 353, 4, 91, 40, 45, 492, 4885, 5622, 8, 115, 540, 59, 1484, 18, 613, 5458, 4, 28386, 7, 120, 110, 1814, 360, 7107, 409, 15288, 20400, 149, 42, 2173, 16, 10, 8018, 4, 178, 7, 146, 3510, 190, 3007, 6, 39, 558, 813, 16, 30740, 4, 1814, 207, 9, 5, 86, 77, 47, 486, 5, 558, 6, 51, 581, 342, 47, 149, 7, 10, 2236, 7107, 6, 14, 8228, 19551, 655, 5274, 50, 2886, 110, 486, 4, 1868, 127, 4194, 408, 8, 1623, 33, 1276, 7, 989, 42, 1524, 71, 7242, 215, 8413, 4, 20, 1445, 558, 34, 41, 6784, 101, 51, 32, 608, 47, 10, 4402, 4, 12192, 162, 10, 1108, 328, 9631, 409, 31, 42, 22053, 8, 5, 1524, 4, 370, 6565, 357, 8, 51, 40, 45, 28, 89, 77, 47, 269, 240, 106, 4, 38, 33, 393, 1299, 19411, 7, 3116, 10, 1099, 1551, 59, 1268, 454, 38, 1145, 42, 31790, 10525, 13, 10, 3299, 54, 16, 70, 59, 5, 418, 4, 2], [0, 100, 437, 2410, 42, 1551, 7, 492, 47, 10, 3885, 62, 137, 47, 192, 42, 12521, 4, 20, 558, 813, 8, 942, 32, 182, 542, 23878, 4, 38, 314, 10, 1579, 19, 1533, 82, 2624, 127, 1087, 6, 8, 117, 65, 655, 373, 162, 124, 4, 38, 56, 7, 1368, 9834, 106, 7, 120, 41, 1948, 59, 127, 1087, 4, 44128, 282, 37457, 282, 32703, 6, 8, 144, 505, 6, 146, 686, 110, 1911, 16, 164, 7, 1719, 925, 4, 18835, 18, 5695, 8, 1925, 173, 4, 91, 5131, 7, 162, 14, 38, 120, 10, 2166, 6, 8, 37, 1467, 38, 21, 10, 1294, 142, 38, 174, 123, 4, 38, 300, 5, 2166, 626, 4, 6811, 6, 38, 303, 66, 127, 474, 1911, 630, 75, 582, 13, 2097, 3693, 5695, 4, 38, 829, 41, 68, 3913, 4, 612, 1087, 13, 5, 1925, 173, 4, 38, 64, 75, 582, 13, 127, 1087, 142, 38, 437, 10, 1294, 8, 218, 75, 33, 143, 1055, 3041, 23, 42, 595, 86, 4, 38, 64, 75, 679, 5, 12521, 1979, 75, 492, 162, 10, 3885, 62, 7, 146, 686, 127, 1911, 74, 1719, 173, 14, 938, 75, 2139, 8, 21, 14657, 2097, 3693, 4, 20, 558, 64, 75, 109, 932, 7, 244, 162, 1719, 5, 1087, 4, 96, 1285, 6, 5, 558, 813, 26, 5, 15, 687, 16, 15, 162, 7, 146, 686, 127, 1911, 4865, 5695, 4, 4967, 4193, 21172, 1068, 328, 2]]\n",
            "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
          ]
        }
      ],
      "source": [
        "print(tok_dataset['train']['label'][:4])\n",
        "print(tok_dataset['train']['input_ids'][:4])\n",
        "print(tok_dataset['train']['attention_mask'][:4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "gy1anCqbHN59"
      },
      "outputs": [],
      "source": [
        "# @title chatgpt roberta\n",
        "from transformers import RobertaConfig, RobertaForMaskedLM, RobertaTokenizerFast\n",
        "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")  # small for testing\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
        "def tokenize_function(example): return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
        "\n",
        "config = RobertaConfig(vocab_size=tokenizer.vocab_size, max_position_embeddings=128, num_attention_heads=8, num_hidden_layers=6, hidden_size=512, intermediate_size=2048)\n",
        "model = RobertaForMaskedLM(config)\n",
        "\n",
        "training_args = TrainingArguments(output_dir=\"./roberta-small\", overwrite_output_dir=True, num_train_epochs=5, per_device_train_batch_size=16, evaluation_strategy=\"no\", save_steps=10_000, save_total_limit=2, logging_steps=500)\n",
        "trainer = Trainer(model=model, args=training_args, train_dataset=tokenized_datasets[\"train\"], tokenizer=tokenizer, data_collator=data_collator)\n",
        "\n",
        "trainer.train()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pt98LQu2EC4K"
      },
      "source": [
        "## trash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WGmRZ_ojEFGc"
      },
      "outputs": [],
      "source": [
        "# @title facebookresearch/mae models_mae.py\n",
        "# https://github.com/facebookresearch/mae/blob/main/models_mae.py\n",
        "from functools import partial\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from timm.models.vision_transformer import PatchEmbed, Block\n",
        "from util.pos_embed import get_2d_sincos_pos_embed\n",
        "\n",
        "\n",
        "class MaskedAutoencoderViT(nn.Module):\n",
        "    \"\"\" Masked Autoencoder with VisionTransformer backbone\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3,\n",
        "                 embed_dim=1024, depth=24, num_heads=16,\n",
        "                 decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
        "                 mlp_ratio=4., norm_layer=nn.LayerNorm, norm_pix_loss=False):\n",
        "        super().__init__()\n",
        "\n",
        "        # --------------------------------------------------------------------------\n",
        "        # MAE encoder specifics\n",
        "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True, qk_scale=None, norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "        # --------------------------------------------------------------------------\n",
        "\n",
        "        # --------------------------------------------------------------------------\n",
        "        # MAE decoder specifics\n",
        "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
        "        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, decoder_embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
        "        self.decoder_blocks = nn.ModuleList([\n",
        "            Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=True, qk_scale=None, norm_layer=norm_layer)\n",
        "            for i in range(decoder_depth)])\n",
        "\n",
        "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
        "        self.decoder_pred = nn.Linear(decoder_embed_dim, patch_size**2 * in_chans, bias=True) # decoder to patch\n",
        "        # --------------------------------------------------------------------------\n",
        "        self.norm_pix_loss = norm_pix_loss\n",
        "        self.initialize_weights()\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        # initialization\n",
        "        # initialize (and freeze) pos_embed by sin-cos embedding\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=True)\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "\n",
        "        decoder_pos_embed = get_2d_sincos_pos_embed(self.decoder_pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=True)\n",
        "        self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).float().unsqueeze(0))\n",
        "\n",
        "        # initialize patch_embed like nn.Linear (instead of nn.Conv2d)\n",
        "        w = self.patch_embed.proj.weight.data\n",
        "        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
        "\n",
        "        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n",
        "        torch.nn.init.normal_(self.cls_token, std=.02)\n",
        "        torch.nn.init.normal_(self.mask_token, std=.02)\n",
        "\n",
        "        # initialize nn.Linear and nn.LayerNorm\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            # we use xavier_uniform following official JAX ViT:\n",
        "            torch.nn.init.xavier_uniform_(m.weight)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    def patchify(self, imgs): # [b,3,h,w]\n",
        "        p = self.patch_embed.patch_size[0]\n",
        "        assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0\n",
        "        h = w = imgs.shape[2] // p\n",
        "        x = imgs.reshape(imgs.shape[0], 3, h, p, w, p) # [b,3,h/p,p,w/p,p]\n",
        "        x = torch.einsum('nchpwq->nhwpqc', x) # [b,h/p,w/p,p,p,3]\n",
        "        x = x.reshape(imgs.shape[0], h * w, p**2 * 3) # [b, h/p *w/p, p*p*3]\n",
        "        return x # [b, h/p *w/p, p*p*3] ~ [b,t,d]\n",
        "\n",
        "    def unpatchify(self, x):\n",
        "        \"\"\"\n",
        "        x: (N, L, patch_size**2 *3)\n",
        "        imgs: (N, 3, H, W)\n",
        "        \"\"\"\n",
        "        p = self.patch_embed.patch_size[0]\n",
        "        h = w = int(x.shape[1]**.5)\n",
        "        assert h * w == x.shape[1]\n",
        "\n",
        "        x = x.reshape(shape=(x.shape[0], h, w, p, p, 3))\n",
        "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
        "        imgs = x.reshape(shape=(x.shape[0], 3, h * p, h * p))\n",
        "        return imgs\n",
        "\n",
        "    def random_masking(self, x, mask_ratio):\n",
        "        \"\"\"\n",
        "        Perform per-sample random masking by per-sample shuffling.\n",
        "        Per-sample shuffling is done by argsort random noise.\n",
        "        x: [N, L, D], sequence\n",
        "        \"\"\"\n",
        "        N, L, D = x.shape  # batch, length, dim\n",
        "        len_keep = int(L * (1 - mask_ratio))\n",
        "\n",
        "        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n",
        "\n",
        "        # sort noise for each sample\n",
        "        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
        "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
        "\n",
        "        # keep the first subset\n",
        "        ids_keep = ids_shuffle[:, :len_keep]\n",
        "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
        "\n",
        "        # generate the binary mask: 0 is keep, 1 is remove\n",
        "        mask = torch.ones([N, L], device=x.device)\n",
        "        mask[:, :len_keep] = 0\n",
        "        # unshuffle to get the binary mask\n",
        "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
        "\n",
        "        return x_masked, mask, ids_restore\n",
        "\n",
        "    def forward_encoder(self, x, mask_ratio):\n",
        "        # embed patches\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        # add pos embed w/o cls token\n",
        "        x = x + self.pos_embed[:, 1:, :]\n",
        "\n",
        "        # masking: length -> length * mask_ratio\n",
        "        x, mask, ids_restore = self.random_masking(x, mask_ratio)\n",
        "\n",
        "        # append cls token\n",
        "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
        "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        # apply Transformer blocks\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        return x, mask, ids_restore\n",
        "\n",
        "    def forward_decoder(self, x, ids_restore):\n",
        "        # embed tokens\n",
        "        x = self.decoder_embed(x)\n",
        "\n",
        "        # append mask tokens to sequence\n",
        "        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
        "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
        "        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
        "        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
        "\n",
        "        # add pos embed\n",
        "        x = x + self.decoder_pos_embed\n",
        "\n",
        "        # apply Transformer blocks\n",
        "        for blk in self.decoder_blocks:\n",
        "            x = blk(x)\n",
        "        x = self.decoder_norm(x)\n",
        "\n",
        "        # predictor projection\n",
        "        x = self.decoder_pred(x)\n",
        "\n",
        "        # remove cls token\n",
        "        x = x[:, 1:, :]\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward_loss(self, imgs, pred, mask):\n",
        "        \"\"\"\n",
        "        imgs: [N, 3, H, W]\n",
        "        pred: [N, L, p*p*3]\n",
        "        mask: [N, L], 0 is keep, 1 is remove,\n",
        "        \"\"\"\n",
        "        target = self.patchify(imgs)\n",
        "        if self.norm_pix_loss:\n",
        "            mean = target.mean(dim=-1, keepdim=True)\n",
        "            var = target.var(dim=-1, keepdim=True)\n",
        "            target = (target - mean) / (var + 1.e-6)**.5\n",
        "\n",
        "        loss = (pred - target) ** 2\n",
        "        loss = loss.mean(dim=-1)  # [N, L], mean loss per patch\n",
        "\n",
        "        loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches\n",
        "        return loss\n",
        "\n",
        "    def forward(self, imgs, mask_ratio=0.75):\n",
        "        latent, mask, ids_restore = self.forward_encoder(imgs, mask_ratio)\n",
        "        pred = self.forward_decoder(latent, ids_restore)  # [N, L, p*p*3]\n",
        "        loss = self.forward_loss(imgs, pred, mask)\n",
        "        return loss, pred, mask\n",
        "\n",
        "\n",
        "def mae_vit_base_patch16_dec512d8b(**kwargs):\n",
        "    model = MaskedAutoencoderViT(\n",
        "        patch_size=16, embed_dim=768, depth=12, num_heads=12,\n",
        "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
        "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def mae_vit_large_patch16_dec512d8b(**kwargs):\n",
        "    model = MaskedAutoencoderViT(\n",
        "        patch_size=16, embed_dim=1024, depth=24, num_heads=16,\n",
        "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
        "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def mae_vit_huge_patch14_dec512d8b(**kwargs):\n",
        "    model = MaskedAutoencoderViT(\n",
        "        patch_size=14, embed_dim=1280, depth=32, num_heads=16,\n",
        "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
        "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "# set recommended archs\n",
        "mae_vit_base_patch16 = mae_vit_base_patch16_dec512d8b  # decoder: 512 dim, 8 blocks\n",
        "mae_vit_large_patch16 = mae_vit_large_patch16_dec512d8b  # decoder: 512 dim, 8 blocks\n",
        "mae_vit_huge_patch14 = mae_vit_huge_patch14_dec512d8b  # decoder: 512 dim, 8 blocks\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "0vScvFGGSeN6"
      ],
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}