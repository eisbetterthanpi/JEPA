{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/Seq_JEPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XwpnHW4wn9S1",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b3dee1a-ad58-4766-db2a-561569251649"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:05<00:00, 28.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# @title data\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "transform = transforms.Compose([transforms.ToTensor(),])\n",
        "\n",
        "# train_data = torchvision.datasets.MNIST(root=\"data\", train=True, download=True,transform=transform) # do not normalise! want img in [0,1)\n",
        "# test_data = torchvision.datasets.MNIST(root=\"data\", train=False, download=True, transform=transform) #opt no download\n",
        "# # batch_size = 32 # 64 512\n",
        "# batch_size = 32 if torch.cuda.is_available() else 32\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "train_data = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
        "test_data = torchvision.datasets.CIFAR10(root='data', train=False, download=True, transform=transform)\n",
        "batch_size = 128 # 4\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "\n",
        "# dataiter = iter(train_data)\n",
        "# x,y = next(dataiter)\n",
        "# print(x.shape) # [3, 32, 32]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title (Learned)RotEmb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, seq_len=512, base=10000):\n",
        "        super().__init__()\n",
        "        self.dim, self.base = dim, base\n",
        "        theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "        pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "        angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "        self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.base)\n",
        "        return x * self.rot_emb[:seq_len]\n",
        "\n",
        "# class LearnedRoPE(nn.Module): # learnt RoPE ; each tok is 1 pos\n",
        "#     def __init__(self, dim):\n",
        "#         super().__init__()\n",
        "#         self.weights = nn.Parameter(torch.randn(1, dim//2))\n",
        "\n",
        "#     def forward(self, x): #\n",
        "#         batch, seq_len, dim = x.shape\n",
        "#         # if rot_emb.shape[0] < seq_len: self.__init__(dim, seq_len)\n",
        "#         pos = torch.arange(seq_len).unsqueeze(1)\n",
        "#         angles = (self.weights * pos * 2*torch.pi).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "#         rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "#         return x * rot_emb.flatten(-2).unsqueeze(0)\n",
        "\n",
        "\n",
        "class LearnedRoPE(nn.Module): # learnt RoPE ; each tok is 1 pos\n",
        "    def __init__(self, dim, seq_len=512):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.weights = nn.Parameter(torch.randn(1, dim//2))\n",
        "        pos = torch.arange(seq_len).unsqueeze(1)\n",
        "        angles = (self.weights * pos * 2*torch.pi)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "        self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "    def forward(self, x): # [batch, seq_len, dim]\n",
        "        seq_len = x.size(1)\n",
        "        if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len)\n",
        "        return x * self.rot_emb[:seq_len]\n",
        "\n",
        "\n",
        "class RotEmb(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, top=torch.pi, base=10000):\n",
        "        super().__init__()\n",
        "        self.theta = top / (base ** (torch.arange(0, dim, step=2, device=device) / dim))\n",
        "        # self.theta = top / (base ** torch.linspace(0, 1, dim//2, device=device))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (pos.unsqueeze(-1) * self.theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [seq_len, dim]\n",
        "\n",
        "class LearnedRotEmb(nn.Module): # pos in R\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.weights = nn.Parameter(torch.randn((1, dim//2), device=device))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (self.weights * pos.unsqueeze(-1) * 2*torch.pi).unsqueeze(-1) # [batch, 1] * [1, dim//2] -> [batch, dim//2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [batch, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [batch, dim]\n",
        "\n"
      ],
      "metadata": {
        "id": "npc_xGtOz7DC",
        "cellView": "form"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Transformer Model\n",
        "import torch\n",
        "from torch import nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Sequential(\n",
        "            nn.Linear(in_dim, d_model), #act,\n",
        "            # nn.Linear(d_model, d_model), act,\n",
        "        )\n",
        "        # self.embed = nn.Sequential(nn.Conv1d(in_dim,d_model,7,2,7//2), nn.MaxPool1d(3, 2, 3//2))\n",
        "        self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, d_hid or d_model, dropout, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.d_model = d_model\n",
        "        self.cls = nn.Parameter(torch.zeros(1,1,d_model)) # randn\n",
        "        self.lin = nn.Linear(d_model, out_dim or d_model)\n",
        "        self.norm = nn.LayerNorm(out_dim or d_model)\n",
        "\n",
        "\n",
        "    def forward(self, src, src_key_padding_mask=None, cls_mask=None, context_indices=None, trg_indices=None): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # if cls_mask != None: src[cls_mask] = self.cls.to(src.dtype)\n",
        "\n",
        "        src = self.embed(src) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = src.shape\n",
        "        # src = self.pos_encoder(src)\n",
        "        if context_indices != None:\n",
        "            # print(src.shape, context_indices.shape, self.pos_encoder(context_indices).shape) # [2, 88, 32]) torch.Size([88]) torch.Size([88, 32]\n",
        "            # print(src[0], self.pos_encoder(context_indices)[0])\n",
        "            src = src * self.pos_encoder(context_indices) # context/predictor # src = src + self.positional_emb[:,context_indices]\n",
        "            # print(src[0])\n",
        "        else: src = src * self.pos_encoder(torch.arange(seq, device=device)) # target # src = src + self.positional_emb[:,:seq]\n",
        "            # print(\"trans fwd\", src.shape, self.pos_encoder(src).shape)\n",
        "\n",
        "        if trg_indices != None: # [M, num_trg_toks]\n",
        "            pred_tokens = self.cls * self.pos_encoder(trg_indices) # [M, num_trg_toks, d_model] # pred_tokens = self.cls + self.positional_emb[0,trg_indices]\n",
        "            pred_tokens = pred_tokens.repeat(batch, 1, 1) # [batch*M, num_trg_toks, d_model]\n",
        "            # print(pred_tokens.requires_grad)\n",
        "            src = src.repeat_interleave(trg_indices.shape[0], dim=0) # [batch, seq_len, d_model] -> [batch*M, seq_len, d_model]\n",
        "            src = torch.cat([src, pred_tokens], dim=1) # [batch*M, seq_len+num_trg_toks, d_model]\n",
        "\n",
        "        out = self.transformer_encoder(src, src_key_padding_mask=src_key_padding_mask) # float [seq_len, batch_size, d_model]\n",
        "        if trg_indices != None:\n",
        "            # print(out.shape)\n",
        "            out = out[:,seq:] # [batch*M, num_trg_toks, d_model]\n",
        "            out = self.lin(out)\n",
        "        out = self.norm(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "batch, seq_len, d_model = 4,7,64\n",
        "in_dim = 3\n",
        "model = TransformerModel(in_dim, d_model, nhead=8, nlayers=2, dropout=0.).to(device)\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "# print(out.shape)\n",
        "# # print(out)\n"
      ],
      "metadata": {
        "id": "-8i1WLsvH_vn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ba31127-d6e5-438a-aede-5c789557ab39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a1848e7-f336-4e3a-86d4-b9589ca56e63",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16928\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "# @title SeqJEPA\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "def multiblock(seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "    return target_mask\n",
        "\n",
        "\n",
        "class SeqJEPA(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, num_layers=1, num_classes=10):\n",
        "        super().__init__()\n",
        "        if out_dim is None: out_dim = d_model\n",
        "        act = nn.GELU() # ReLU GELU SiLU\n",
        "        # self.rot_emb = RotEmb(d_model, top=1, base=10)\n",
        "        # self.rot_emb = RotEmb(d_model, top=torch.pi, base=10)\n",
        "        # self.rot_emb = RoPE(d_model, seq_len=1024, base=10)\n",
        "        # self.rot_emb = LearnedRotEmb(dim)\n",
        "        self.encode = nn.Sequential(\n",
        "            nn.Linear(in_dim, d_model), #act,\n",
        "            # nn.Linear(d_model, d_model), act,\n",
        "        )\n",
        "        # self.encode = nn.Sequential(nn.Conv1d(in_dim, d_model,7,2,7//2), nn.MaxPool1d(3, 2, 3//2))\n",
        "        self.context_encoder = TransformerModel(d_model, d_model, out_dim=out_dim, nhead=4, nlayers=2, dropout=0.)\n",
        "        self.predicter = TransformerModel(out_dim, d_model//2, out_dim, nhead=4, nlayers=1, dropout=0.)\n",
        "        self.target_encoder = AveragedModel(self.context_encoder, multi_avg_fn=get_ema_multi_avg_fn(0.85)) # 0.95 0.999\n",
        "        self.target_encoder.requires_grad_(False)\n",
        "        # self.classifier = nn.Linear(out_dim, num_classes)\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]\n",
        "        x = self.encode(x) # [batch, T, d_model]\n",
        "        # x = self.encode(x.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "\n",
        "        batch, seq, dim = x.shape\n",
        "        M=1 # 4\n",
        "        # target_mask = multiblock(seq, min_s=0.15, max_s=0.2, M=M) # mask out targets to be predicted # [M, seq]\n",
        "        target_mask = multiblock(seq, min_s=0.15, max_s=0.2, M=4)#.any(0) # mask out targets to be predicted # [M, seq]\n",
        "        context_mask = ~multiblock(seq, min_s=0.85, max_s=1., M=1)|target_mask.reshape(M,seq).any(0) # [1, seq], True->Mask\n",
        "        target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "        # print(target_mask, context_mask)\n",
        "        sorted_mask, ids = context_mask.int().sort(dim=1, stable=True)\n",
        "        context_indices = ids[~sorted_mask.bool()] # int idx [num_context_toks] , idx of context not masked\n",
        "        sorted_x = x[torch.arange(batch).unsqueeze(-1), context_indices] # [batch, num_context_toks, d_model]\n",
        "        # print(sorted_x.shape, context_indices.shape)[2, 9, 32]) torch.Size([9])\n",
        "        # print(sorted_x[0])\n",
        "        # print(sorted_x.is_leaf)\n",
        "        sorted_sx = self.context_encoder(sorted_x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "        # del sorted_x\n",
        "        # print(sorted_sx[0])\n",
        "\n",
        "        sorted_mask, ids = target_mask.int().sort(dim=1, stable=True)\n",
        "        # print(sorted_mask.shape, ids.shape, ids[~sorted_mask.bool()].shape, sorted_mask.sum(-1))\n",
        "        trg_indices = ids[sorted_mask.bool()].reshape(M,-1) # int idx [M, num_trg_toks] , idx of targets that are masked\n",
        "        # sorted_x = x[torch.arange(batch)[...,None,None], indices] # [batch, M, seq-num_trg_toks, dim]\n",
        "        # sx = sorted_sx[torch.arange(batch).unsqueeze(-1),ids.argsort(1)]\n",
        "        # print(sorted_sx.shape, trg_indices.shape)\n",
        "        sy_ = self.predicter(sorted_sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "        # sx = self.context_encoder(x, src_key_padding_mask=context_mask).repeat(M,1,1)\n",
        "        # # sy_ = self.predicter(sx, cls_mask=target_mask)*target_mask.unsqueeze(-1)\n",
        "        # sy_ = self.predicter(sx, trg_mask=target_mask)*target_mask.unsqueeze(-1)\n",
        "\n",
        "        # print(sy_.shape, target_mask.shape)\n",
        "        # print(self.target_encoder(x).repeat_interleave(M, dim=0).shape, trg_indices.repeat(batch,1).shape)\n",
        "        with torch.no_grad():\n",
        "            sy = self.target_encoder(x).repeat_interleave(M, dim=0)[torch.arange(batch*M).unsqueeze(-1), trg_indices.repeat(batch,1)] # [batch*M, num_trg_toks, out_dim]\n",
        "\n",
        "        # print(sy_[0])\n",
        "        # print(sy.shape, sy_.shape)\n",
        "        # loss = F.smooth_l1_loss(sy, sy_)\n",
        "        loss = F.mse_loss(sy, sy_)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        x = self.encode(x)\n",
        "        # x = self.encode(x.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "        sx = self.target_encoder(x)\n",
        "        out = sx.mean(dim=1)\n",
        "        return out\n",
        "\n",
        "\n",
        "# min_s=0.15, max_s, M\n",
        "# trg.15.2M4 C.85 1\n",
        "\n",
        "seq_jepa = SeqJEPA(in_dim=3, d_model=32, out_dim=16, num_layers=1).to(device)#.to(torch.float)\n",
        "optim = torch.optim.AdamW(seq_jepa.parameters(), lr=1e-4) # 1e-3?\n",
        "print(sum(p.numel() for p in seq_jepa.parameters() if p.requires_grad)) # 59850\n",
        "\n",
        "\n",
        "# x = torch.rand((2,20,3), device=device)\n",
        "# out = seq_jepa.loss(x)\n",
        "# print(out.shape)\n",
        "# print(x.is_leaf) # T\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title SeqJEPA mae like\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def multiblock(seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "    return target_mask\n",
        "\n",
        "# https://arxiv.org/pdf/2210.07224\n",
        "def randpatch(seq, mask_size=4, gamma=0.75): # num patches of seq, mask patch size, masking ratio\n",
        "    mask = torch.rand(seq//mask_size)<gamma\n",
        "    mask = mask.repeat_interleave(mask_size, dim=-1)\n",
        "    return mask # [seq] , True -> mask\n",
        "\n",
        "class SeqJEPA(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, num_layers=1, num_classes=10):\n",
        "        super().__init__()\n",
        "        if out_dim is None: out_dim = d_model\n",
        "        act = nn.GELU() # ReLU GELU SiLU\n",
        "        # self.encode = nn.Sequential(\n",
        "        #     nn.Linear(in_dim, d_model), #act,\n",
        "        #     # nn.Linear(d_model, d_model), act,\n",
        "        # )\n",
        "        self.encode = nn.Sequential(\n",
        "            # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.MaxPool1d(3, 2, 3//2),\n",
        "            # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(in_dim, d_model,7,2,7//2),\n",
        "            )\n",
        "        self.context_encoder = TransformerModel(d_model, d_model, out_dim=out_dim, nhead=4, nlayers=2, dropout=0.)\n",
        "        self.predicter = TransformerModel(out_dim, d_model//2, out_dim, nhead=4, nlayers=1, dropout=0.)\n",
        "        # self.target_encoder = AveragedModel(self.context_encoder, multi_avg_fn=get_ema_multi_avg_fn(0.99)) # 0.95 0.999\n",
        "        # self.target_encoder.requires_grad_(False)\n",
        "        import copy\n",
        "        self.target_encoder = copy.deepcopy(encoder)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def loss(self, sequence): # [batch, T, 3]\n",
        "        # x = self.encode(x) # [batch, T, d_model]\n",
        "        x = self.encode(sequence.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "\n",
        "        batch, seq, dim = x.shape\n",
        "        M=1 # 4\n",
        "        # target_mask = multiblock(seq, min_s=0.15, max_s=0.2, M=M) # mask out targets to be predicted # [M, seq]\n",
        "        # target_mask = multiblock(seq, min_s=0.15, max_s=0.2, M=4)#.any(0) # mask out targets to be predicted # [M, seq]\n",
        "        # context_mask = ~multiblock(seq, min_s=0.85, max_s=1., M=1)|target_mask.reshape(M,seq).any(0) # [1, seq], True->Mask\n",
        "        # print(seq)\n",
        "        target_mask = randpatch(seq, mask_size=16, gamma=.75) # [seq]\n",
        "        context_mask = ~multiblock(seq, min_s=0.85, max_s=1., M=1)|target_mask # [1, seq], True->Mask\n",
        "        target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "        # print(target_mask, context_mask)\n",
        "        sorted_mask, ids = context_mask.int().sort(dim=1, stable=True)\n",
        "        context_indices = ids[~sorted_mask.bool()] # int idx [num_context_toks] , idx of context not masked\n",
        "        # print(x.shape, context_indices.shape)\n",
        "        sorted_x = x[torch.arange(batch).unsqueeze(-1), context_indices] # [batch, num_context_toks, d_model]\n",
        "        # print(sorted_x.shape, context_indices.shape)[2, 9, 32]) torch.Size([9])\n",
        "        # print(sorted_x[0])\n",
        "        # print(len(context_indices))\n",
        "        sorted_sx = self.context_encoder(sorted_x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "        # print(sorted_sx[0])\n",
        "\n",
        "        sorted_mask, ids = target_mask.int().sort(dim=-1, stable=True)\n",
        "        # print(sorted_mask.shape, ids.shape, ids[~sorted_mask.bool()].shape, sorted_mask.sum(-1))\n",
        "        trg_indices = ids[sorted_mask.bool()].unsqueeze(0) # int idx [1, num_trg_toks] , idx of targets that are masked\n",
        "        # print(sorted_sx.shape, trg_indices.shape)\n",
        "        sy_ = self.predicter(sorted_sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "\n",
        "        # mae not\n",
        "        # sy_ = self.context_encoder(x, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "\n",
        "\n",
        "        # print(self.target_encoder(x).shape, trg_indices.shape)\n",
        "        # print(self.target_encoder(x).repeat_interleave(M, dim=0).shape, trg_indices.repeat(batch,1).shape)\n",
        "        with torch.no_grad():\n",
        "            # sy = self.target_encoder(x)[torch.arange(batch).unsqueeze(-1), trg_indices.repeat(batch,1)] # [batch, num_trg_toks, out_dim]\n",
        "            sy = self.target_encoder(x.detach())\n",
        "            sy = F.layer_norm(sy, (sy.size(-1),))\n",
        "            sy = sy[torch.arange(batch).unsqueeze(-1), trg_indices.squeeze(0)] # [batch, num_trg_toks, out_dim]\n",
        "        # print(x[0][0][:8])\n",
        "        # print(sy_[0][0][:8])\n",
        "        # print(sy.shape, sy_.shape)\n",
        "        # loss = F.smooth_l1_loss(sy, sy_)\n",
        "        loss = F.mse_loss(sy.detach(), sy_)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        # x = self.encode(x)\n",
        "        x = self.encode(x.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "        sx = self.target_encoder(x)\n",
        "        out = sx.mean(dim=1)\n",
        "        return out\n",
        "\n",
        "\n",
        "# min_s=0.15, max_s, M\n",
        "# trg.15.2M4 C.85 1\n",
        "\n",
        "seq_jepa = SeqJEPA(in_dim=3, d_model=32, out_dim=16, num_layers=1).to(device)#.to(torch.float)\n",
        "optim = torch.optim.AdamW(seq_jepa.parameters(), lr=1e-3) # 1e-3?\n",
        "print(sum(p.numel() for p in seq_jepa.parameters() if p.requires_grad)) # 59850\n",
        "\n",
        "\n",
        "x = torch.rand((2,400,3), device=device)\n",
        "out = seq_jepa.loss(x)\n",
        "print(out.shape)\n",
        "# print(x.is_leaf) # T\n"
      ],
      "metadata": {
        "id": "xy4iC46Vnlog",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "7b68ef23-cc7b-4990-81de-febd477520fe"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32048\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (200) must match the size of tensor b (192) at non-singleton dimension 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-0691c168a81a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq_jepa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;31m# print(x.is_leaf) # T\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-0691c168a81a>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# print(seq)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mtarget_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandpatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.75\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [seq]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mcontext_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mmultiblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_s\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.85\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_s\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m|\u001b[0m\u001b[0mtarget_mask\u001b[0m \u001b[0;31m# [1, seq], True->Mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mtarget_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# print(target_mask, context_mask)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (200) must match the size of tensor b (192) at non-singleton dimension 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ijepa multiblock down\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "import math\n",
        "from multiprocessing import Value\n",
        "import torch\n",
        "_GLOBAL_SEED = 0\n",
        "\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=(224, 224),\n",
        "        patch_size=16,\n",
        "        enc_mask_scale=(0.2, 0.8),\n",
        "        pred_mask_scale=(0.2, 0.8),\n",
        "        aspect_ratio=(0.3, 3.0),\n",
        "        nenc=1,\n",
        "        npred=2,\n",
        "        min_keep=4,\n",
        "        allow_overlap=False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if not isinstance(input_size, tuple):\n",
        "            input_size = (input_size, ) * 2\n",
        "        self.patch_size = patch_size\n",
        "        self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.aspect_ratio = aspect_ratio\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "        self._itr_counter = Value('i', -1)  # collator is shared across worker processes\n",
        "\n",
        "    def step(self):\n",
        "        i = self._itr_counter\n",
        "        with i.get_lock():\n",
        "            i.value += 1\n",
        "            v = i.value\n",
        "        return v\n",
        "\n",
        "    def _sample_block_size(self, generator, scale, aspect_ratio_scale):\n",
        "        _rand = torch.rand(1, generator=generator).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.height * self.width * mask_scale)\n",
        "        # -- Sample block aspect-ratio\n",
        "        min_ar, max_ar = aspect_ratio_scale\n",
        "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
        "        while h >= self.height:\n",
        "            h -= 1\n",
        "        while w >= self.width:\n",
        "            w -= 1\n",
        "\n",
        "        return (h, w)\n",
        "\n",
        "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
        "        h, w = b_size\n",
        "\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # --\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            top = torch.randint(0, self.height - h, (1,))\n",
        "            left = torch.randint(0, self.width - w, (1,))\n",
        "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
        "            mask[top:top+h, left:left+w] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
        "        mask_complement[top:top+h, left:left+w] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        B = len(batch)\n",
        "\n",
        "        collated_batch = torch.utils.data.default_collate(batch)\n",
        "\n",
        "        seed = self.step()\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(seed)\n",
        "        p_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.pred_mask_scale,\n",
        "            aspect_ratio_scale=self.aspect_ratio)\n",
        "        e_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.enc_mask_scale,\n",
        "            aspect_ratio_scale=(1., 1.))\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.height * self.width\n",
        "        min_keep_enc = self.height * self.width\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                print(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "\n",
        "        return collated_batch, collated_masks_enc, collated_masks_pred\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "COvEnBXPYth3"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aH1UOD4WW8I2",
        "outputId": "6fafea1b-0af2-4bc7-fa26-46b5e56549b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vit fwd torch.Size([4, 3, 224, 224])\n",
            "vit fwd torch.Size([4, 196, 768])\n",
            "vit fwd torch.Size([4, 11, 768])\n",
            "predictor fwd1 torch.Size([4, 11, 384]) torch.Size([4, 196, 384])\n"
          ]
        }
      ],
      "source": [
        "# @title ijepa vision_transformer.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/models/vision_transformer.py\n",
        "import math\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# from src.utils.tensors import (trunc_normal_, repeat_interleave_batch)\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid height and width\n",
        "    return:\n",
        "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid_h = np.arange(grid_size, dtype=float)\n",
        "    grid_w = np.arange(grid_size, dtype=float)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    assert embed_dim % 2 == 0\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid length\n",
        "    return:\n",
        "    pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid = np.arange(grid_size, dtype=float)\n",
        "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=float)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega   # (D/2,)\n",
        "    pos = pos.reshape(-1)   # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
        "    emb = np.concatenate([np.sin(out), np.cos(out)], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "def repeat_interleave_batch(x, B, repeat):\n",
        "    N = len(x) // B\n",
        "    x = torch.cat([\n",
        "        torch.cat([x[i*B:(i+1)*B] for _ in range(repeat)], dim=0)\n",
        "        for i in range(N)\n",
        "    ], dim=0)\n",
        "    return x\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x, attn\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x, return_attention=False):\n",
        "        y, attn = self.attn(self.norm1(x))\n",
        "        if return_attention:\n",
        "            return attn\n",
        "        x = x + self.drop_path(y)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvEmbed(nn.Module):\n",
        "    \"\"\"3x3 Convolution stems for ViT following ViTC models\"\"\"\n",
        "    def __init__(self, channels, strides, img_size=224, in_chans=3, batch_norm=True):\n",
        "        super().__init__()\n",
        "        # Build the stems\n",
        "        stem = []\n",
        "        channels = [in_chans] + channels\n",
        "        for i in range(len(channels) - 2):\n",
        "            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3, stride=strides[i], padding=1, bias=(not batch_norm))]\n",
        "            if batch_norm:\n",
        "                stem += [nn.BatchNorm2d(channels[i+1])]\n",
        "            stem += [nn.ReLU(inplace=True)]\n",
        "        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n",
        "        self.stem = nn.Sequential(*stem)\n",
        "\n",
        "        # Comptute the number of patches\n",
        "        stride_prod = int(np.prod(strides))\n",
        "        self.num_patches = (img_size[0] // stride_prod)**2\n",
        "\n",
        "    def forward(self, x):\n",
        "        p = self.stem(x)\n",
        "        return p.flatten(2).transpose(1, 2)\n",
        "\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "def apply_masks(x, masks):\n",
        "    \"\"\":param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
        "    :param masks: list of tensors containing indices of patches in [N] to keep\"\"\"\n",
        "    all_x = []\n",
        "    for m in masks:\n",
        "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [batch,T,dim]\n",
        "        all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "    return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "\n",
        "class VisionTransformerPredictor(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, num_patches, embed_dim=768, predictor_embed_dim=384,\n",
        "        depth=6, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.init_std = init_std\n",
        "        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        # --\n",
        "        self.predictor_pos_embed = nn.Parameter(torch.zeros(1, num_patches, predictor_embed_dim), requires_grad=False)\n",
        "        predictor_pos_embed = get_2d_sincos_pos_embed(self.predictor_pos_embed.shape[-1], int(num_patches**.5), cls_token=False)\n",
        "        self.predictor_pos_embed.data.copy_(torch.from_numpy(predictor_pos_embed).float().unsqueeze(0)) # [1, num_patches, predictor_embed_dim]\n",
        "        # --\n",
        "        self.predictor_blocks = nn.ModuleList([Block(dim=predictor_embed_dim, num_heads=num_heads, drop=drop_rate, drop_path=dpr[i]) for i in range(depth)])\n",
        "        self.predictor_norm = nn.LayerNorm(predictor_embed_dim)\n",
        "        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n",
        "        # ------\n",
        "        # trunc_normal_(self.mask_token, std=self.init_std)\n",
        "\n",
        "    def forward(self, x, masks_x, masks): # [batch, num_context_patches, embed_dim], nenc*[batch, num_context_patches], npred*[batch, num_target_patches]\n",
        "        assert (masks is not None) and (masks_x is not None), 'Cannot run predictor without mask indices'\n",
        "        if not isinstance(masks_x, list): masks_x = [masks_x] # context mask\n",
        "        if not isinstance(masks, list): masks = [masks] # pred mask\n",
        "\n",
        "        # -- Batch Size\n",
        "        B = len(x) // len(masks_x)\n",
        "        # print(\"predictor fwd0\", x.shape) # [3, 121, 768])\n",
        "        # -- map from encoder-dim to pedictor-dim\n",
        "        x = self.predictor_embed(x) # [batch, num_patches, predictor_embed_dim]\n",
        "\n",
        "        # -- add positional embedding to x tokens\n",
        "        x_pos_embed = self.predictor_pos_embed.repeat(B, 1, 1) # [batch, num_patches, predictor_embed_dim]\n",
        "        print(\"predictor fwd1\", x.shape, x_pos_embed.shape) # [3, 121 or 11, 384], [3, 196, 384]\n",
        "        # print(\"predictor fwd masks_x\", masks_x)\n",
        "        x += apply_masks(x_pos_embed, masks_x) # get pos emb of context patches\n",
        "\n",
        "        # print(\"predictor fwd x\", x.shape) # [4, 11, 384])\n",
        "        _, N_ctxt, D = x.shape\n",
        "\n",
        "        # -- concat mask tokens to x\n",
        "        pos_embs = self.predictor_pos_embed.repeat(B, 1, 1) # [batch, num_patches, predictor_embed_dim]\n",
        "        pos_embs = apply_masks(pos_embs, masks) # [16, 104, predictor_embed_dim]\n",
        "        pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x)) # [16, 104, predictor_embed_dim]\n",
        "        # --\n",
        "        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1) # [16, 104, predictor_embed_dim]\n",
        "        # --\n",
        "        # print(\"predictor fwd pred_tokens\", pred_tokens.shape)\n",
        "\n",
        "        pred_tokens += pos_embs\n",
        "        x = x.repeat(len(masks), 1, 1)\n",
        "        x = torch.cat([x, pred_tokens], dim=1)\n",
        "\n",
        "        # -- fwd prop\n",
        "        for blk in self.predictor_blocks:\n",
        "            x = blk(x)\n",
        "        x = self.predictor_norm(x)\n",
        "\n",
        "        # -- return preds for mask tokens\n",
        "        x = x[:, N_ctxt:]\n",
        "        x = self.predictor_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, img_size=[224], patch_size=16, in_chans=3, embed_dim=768, predictor_embed_dim=384, depth=12, predictor_depth=12,\n",
        "        num_heads=12, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0, norm_layer=nn.LayerNorm, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_features = self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.init_std = init_std\n",
        "        # --\n",
        "        self.patch_embed = PatchEmbed(img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        # --\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=False)\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "        # --\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "    def forward(self, x, masks=None): # [batch,3,224,224]\n",
        "        if masks is not None:\n",
        "            if not isinstance(masks, list): masks = [masks]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        x = self.patch_embed(x)\n",
        "        B, N, D = x.shape # [batch, num_patches, embed_dim]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        # -- add positional embedding to x\n",
        "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
        "        x = x + pos_embed\n",
        "\n",
        "        if masks is not None: x = apply_masks(x, masks) # [batch, num_context_patches, embed_dim]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            x = blk(x)\n",
        "\n",
        "        if self.norm is not None: x = self.norm(x)\n",
        "        return x # [batch, num_context_patches, embed_dim]\n",
        "\n",
        "    def interpolate_pos_encoding(self, x, pos_embed):\n",
        "        npatch = x.shape[1] - 1\n",
        "        N = pos_embed.shape[1] - 1\n",
        "        if npatch == N: return pos_embed\n",
        "        class_emb, pos_embed = pos_embed[:, 0], pos_embed[:, 1:]\n",
        "        dim = x.shape[-1]\n",
        "        pos_embed = nn.functional.interpolate(pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2), scale_factor=math.sqrt(npatch / N), mode='bicubic',)\n",
        "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
        "\n",
        "# from functools import partial\n",
        "# def vit_predictor(**kwargs):\n",
        "#     model = VisionTransformerPredictor(\n",
        "#         mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "#     return model\n",
        "\n",
        "def vit(patch_size=16, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, # tiny\n",
        "        # patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, # small\n",
        "        # patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, # base\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "# encoder = vit()\n",
        "encoder = VisionTransformer(\n",
        "        patch_size=16, embed_dim=768, depth=1, num_heads=3, mlp_ratio=1,\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
        "\n",
        "# num_patches = (224/patch_size)^2\n",
        "# predictor = VisionTransformerPredictor(num_patches, embed_dim=768, predictor_embed_dim=384, depth=6, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02, **kwargs)\n",
        "predictor = VisionTransformerPredictor(num_patches=196, embed_dim=768, predictor_embed_dim=384, depth=1, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02)\n",
        "\n",
        "batch = 4\n",
        "img = torch.rand(batch, 3, 224, 224)\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/train.py\n",
        "mask_collator = MaskCollator(\n",
        "        input_size=(224, 224),\n",
        "        patch_size=16,\n",
        "        enc_mask_scale=(0.2, 0.8),\n",
        "        pred_mask_scale=(0.2, 0.8),\n",
        "        aspect_ratio=(0.3, 3.0),\n",
        "        nenc=1,\n",
        "        npred=4,\n",
        "        min_keep=4,\n",
        "        # allow_overlap=True)\n",
        "        allow_overlap=False)\n",
        "# self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
        "\n",
        "\n",
        "\n",
        "# v = mask_collator.step()\n",
        "# should pass the collater a list batch of idx?\n",
        "# collated_batch, collated_masks_enc, collated_masks_pred = mask_collator([batch,3,8])\n",
        "collated_batch, collated_masks_enc, collated_masks_pred = mask_collator([0,1,2,3])\n",
        "# print(v)\n",
        "\n",
        "\n",
        "import copy\n",
        "target_encoder = copy.deepcopy(encoder)\n",
        "\n",
        "\n",
        "def forward_target():\n",
        "    with torch.no_grad():\n",
        "        h = target_encoder(imgs)\n",
        "        h = F.layer_norm(h, (h.size(-1),))  # normalize over feature-dim\n",
        "        B = len(h)\n",
        "        # -- create targets (masked regions of h)\n",
        "        h = apply_masks(h, masks_pred)\n",
        "        h = repeat_interleave_batch(h, B, repeat=len(masks_enc))\n",
        "        return h\n",
        "\n",
        "def forward_context():\n",
        "    z = encoder(imgs, masks_enc)\n",
        "    z = predictor(z, masks_enc, masks_pred)\n",
        "    return z\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    m = next(momentum_scheduler)\n",
        "                    for param_q, param_k in zip(encoder.parameters(), target_encoder.parameters()):\n",
        "                        param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "\n",
        "\n",
        "# # num_context_patches = 121 if allow_overlap=True else 11\n",
        "z = encoder(img, collated_masks_enc) # [batch, num_context_patches, embed_dim]\n",
        "# print(len(collated_masks_enc), len(collated_masks_pred)) # nenc, npred\n",
        "# print(collated_masks_enc[0].shape, collated_masks_pred[0].shape) # , [batch, num_context_patches = 121 or 11], [batch, num_target_patches]\n",
        "out = predictor(z, collated_masks_enc, collated_masks_pred)\n",
        "# # print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(collated_batch, collated_masks_enc, collated_masks_pred)\n",
        "# print(collated_batch) # tensor([0, 1, 2, 3]) batch\n",
        "# print(collated_masks_enc) # nenc*[M*[pred mask blocks patch ind]]\n",
        "# print(collated_masks_pred) # npred * [M * [context]]\n",
        "\n",
        "\n",
        "# print(len(collated_masks_enc[0]), len(collated_masks_pred[0]))\n",
        "# print(collated_masks_enc[0], collated_masks_pred[0])\n",
        "\n",
        "h,w=14,14\n",
        "img = torch.ones(h*w)\n",
        "# img[collated_masks_enc[0]]=0\n",
        "img[collated_masks_pred[0][2]]=0\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    # plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.imshow(npimg)\n",
        "    plt.show()\n",
        "\n",
        "imshow(img.reshape(h,w))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "tf4T37woBV2V",
        "outputId": "f91bdd79-79fe-4f09-8469-5605f3b351b7"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAGHNJREFUeJzt3X9MVYfdx/EPQrkwHrgVOoEbodLGxqpUbVGjNJtGUkOsrc/SOhu6Ek22ZcMpknTINnSN1VvdZow/gtVkrUv81T+qdSa1YdRqTP2BUpqabaiRKSkB1sTeixhvCfc8fyzePVSsguf2y719v5Lzxz3ncM/3xO28cy6nlwTHcRwBAPAtG2E9AADgu4kAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAE0nWA3xdOBxWe3u70tPTlZCQYD0OAGCQHMdRd3e3fD6fRoy4833OsAtQe3u78vLyrMcAANyntrY2jR49+o7bh12A0tPTJUlXmsYo43/i5xPC/32s0HoEIC4cuPCZ9Qi4i+D1sB5+8l+R6/mdDLsA3frYLeN/RigjPX4ClJTwgPUIQFyIp+tCvLvbr1H4lwQAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwEbUAbdu2TWPGjFFKSoqmT5+uM2fOROtQAIAYFJUA7d+/X1VVVVq9erWampo0adIkzZ07V11dXdE4HAAgBkUlQBs3btRPf/pTLV68WOPHj9f27dv1ve99T3/+85+jcTgAQAxyPUBfffWVzp07p5KSkv8eZMQIlZSU6OTJk7ftHwqFFAwG+y0AgPjneoC++OIL9fX1KTs7u9/67OxsdXR03La/3++X1+uNLHwRKQB8N5g/BVdTU6NAIBBZ2trarEcCAHwLXP8y0oceekiJiYnq7Ozst76zs1M5OTm37e/xeOTxeNweAwAwzLl+B5ScnKynnnpKDQ0NkXXhcFgNDQ2aMWOG24cDAMSoqPw5hqqqKpWXl6uoqEjTpk3Tpk2b1NPTo8WLF0fjcACAGBSVAP34xz/Wv//9b61atUodHR2aPHmyjhw5ctuDCQCA766o/UG6pUuXaunSpdF6ewBAjDN/Cg4A8N1EgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmXA+Q3+/X1KlTlZ6erlGjRmnBggVqaWlx+zAAgBjneoCOHTumiooKnTp1SvX19ert7dUzzzyjnp4etw8FAIhhSW6/4ZEjR/q9fvvttzVq1CidO3dOP/jBD9w+HAAgRrkeoK8LBAKSpMzMzAG3h0IhhUKhyOtgMBjtkQAAw0BUH0IIh8OqrKxUcXGxJk6cOOA+fr9fXq83suTl5UVzJADAMBHVAFVUVOj8+fPat2/fHfepqalRIBCILG1tbdEcCQAwTETtI7ilS5fq8OHDOn78uEaPHn3H/TwejzweT7TGAAAMU64HyHEc/epXv9KBAwf00UcfqaCgwO1DAADigOsBqqio0J49e/Tee+8pPT1dHR0dkiSv16vU1FS3DwcAiFGu/w6orq5OgUBAs2bNUm5ubmTZv3+/24cCAMSwqHwEBwDA3fBdcAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATUQ/QG2+8oYSEBFVWVkb7UACAGBLVADU2NurNN9/UE088Ec3DAABiUNQCdP36dZWVlWnnzp0aOXJktA4DAIhRUQtQRUWF5s2bp5KSkm/cLxQKKRgM9lsAAPEvKRpvum/fPjU1NamxsfGu+/r9fr322mvRGAMAMIy5fgfU1tam5cuXa/fu3UpJSbnr/jU1NQoEApGlra3N7ZEAAMOQ63dA586dU1dXl5588snIur6+Ph0/flxbt25VKBRSYmJiZJvH45HH43F7DADAMOd6gObMmaPPPvus37rFixdr3Lhxqq6u7hcfAMB3l+sBSk9P18SJE/utS0tLU1ZW1m3rAQDfXXwTAgDARFSegvu6jz766Ns4DAAghnAHBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJqISoM8//1wvv/yysrKylJqaqsLCQp09ezYahwIAxKgkt9/w2rVrKi4u1uzZs/X+++/r+9//vi5evKiRI0e6fSgAQAxzPUDr169XXl6e3nrrrci6goICtw8DAIhxrn8Ed+jQIRUVFenFF1/UqFGjNGXKFO3cufOO+4dCIQWDwX4LACD+uR6gy5cvq66uTmPHjtUHH3ygX/ziF1q2bJl27do14P5+v19erzey5OXluT0SAGAYSnAcx3HzDZOTk1VUVKSPP/44sm7ZsmVqbGzUyZMnb9s/FAopFApFXgeDQeXl5enahUeUkR4/D+nN9U22HgGICx+0N1uPgLsIdoc18rHLCgQCysjIuON+rl/hc3NzNX78+H7rHn/8cV29enXA/T0ejzIyMvotAID453qAiouL1dLS0m/dhQsX9PDDD7t9KABADHM9QCtWrNCpU6e0bt06Xbp0SXv27NGOHTtUUVHh9qEAADHM9QBNnTpVBw4c0N69ezVx4kStWbNGmzZtUllZmduHAgDEMNf/OyBJevbZZ/Xss89G460BAHEifh4zAwDEFAIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMOF6gPr6+lRbW6uCggKlpqbq0Ucf1Zo1a+Q4jtuHAgDEsCS333D9+vWqq6vTrl27NGHCBJ09e1aLFy+W1+vVsmXL3D4cACBGuR6gjz/+WM8//7zmzZsnSRozZoz27t2rM2fOuH0oAEAMc/0juJkzZ6qhoUEXLlyQJH366ac6ceKESktLB9w/FAopGAz2WwAA8c/1O6CVK1cqGAxq3LhxSkxMVF9fn9auXauysrIB9/f7/XrttdfcHgMAMMy5fgf0zjvvaPfu3dqzZ4+ampq0a9cu/fGPf9SuXbsG3L+mpkaBQCCytLW1uT0SAGAYcv0O6NVXX9XKlSu1aNEiSVJhYaGuXLkiv9+v8vLy2/b3eDzyeDxujwEAGOZcvwO6ceOGRozo/7aJiYkKh8NuHwoAEMNcvwOaP3++1q5dq/z8fE2YMEGffPKJNm7cqCVLlrh9KABADHM9QFu2bFFtba1++ctfqqurSz6fTz//+c+1atUqtw8FAIhhCc4w+4qCYDAor9eraxceUUZ6/HxT0FzfZOsRgLjwQXuz9Qi4i2B3WCMfu6xAIKCMjIw77hc/V3gAQEwhQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATgw7Q8ePHNX/+fPl8PiUkJOjgwYP9tjuOo1WrVik3N1epqakqKSnRxYsX3ZoXABAnBh2gnp4eTZo0Sdu2bRtw+4YNG7R582Zt375dp0+fVlpamubOnaubN2/e97AAgPiRNNgfKC0tVWlp6YDbHMfRpk2b9Lvf/U7PP/+8JOkvf/mLsrOzdfDgQS1atOj+pgUAxA1XfwfU2tqqjo4OlZSURNZ5vV5Nnz5dJ0+eHPBnQqGQgsFgvwUAEP9cDVBHR4ckKTs7u9/67OzsyLav8/v98nq9kSUvL8/NkQAAw5T5U3A1NTUKBAKRpa2tzXokAMC3wNUA5eTkSJI6Ozv7re/s7Ixs+zqPx6OMjIx+CwAg/rkaoIKCAuXk5KihoSGyLhgM6vTp05oxY4abhwIAxLhBPwV3/fp1Xbp0KfK6tbVVzc3NyszMVH5+viorK/X6669r7NixKigoUG1trXw+nxYsWODm3ACAGDfoAJ09e1azZ8+OvK6qqpIklZeX6+2339avf/1r9fT06Gc/+5m+/PJLPf300zpy5IhSUlLcmxoAEPMSHMdxrIf4/4LBoLxer65deEQZ6ebPSLhmrm+y9QhAXPigvdl6BNxFsDuskY9dViAQ+Mbf68fPFR4AEFMIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMDEoAN0/PhxzZ8/Xz6fTwkJCTp48GBkW29vr6qrq1VYWKi0tDT5fD698soram9vd3NmAEAcGHSAenp6NGnSJG3btu22bTdu3FBTU5Nqa2vV1NSkd999Vy0tLXruuedcGRYAED+SBvsDpaWlKi0tHXCb1+tVfX19v3Vbt27VtGnTdPXqVeXn5w9tSgBA3Bl0gAYrEAgoISFBDz744IDbQ6GQQqFQ5HUwGIz2SACAYSCqDyHcvHlT1dXVeumll5SRkTHgPn6/X16vN7Lk5eVFcyQAwDARtQD19vZq4cKFchxHdXV1d9yvpqZGgUAgsrS1tUVrJADAMBKVj+BuxefKlSv68MMP73j3I0kej0cejycaYwAAhjHXA3QrPhcvXtTRo0eVlZXl9iEAAHFg0AG6fv26Ll26FHnd2tqq5uZmZWZmKjc3Vy+88IKampp0+PBh9fX1qaOjQ5KUmZmp5ORk9yYHAMS0QQfo7Nmzmj17duR1VVWVJKm8vFy///3vdejQIUnS5MmT+/3c0aNHNWvWrKFPCgCIK4MO0KxZs+Q4zh23f9M2AABu4bvgAAAmCBAAwAQBAgCYIEAAABMECABgggABAExE/duw8R8ftDdbjwAAwwp3QAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJhIsh7g6xzHkSQFr4eNJwEADMWt6/et6/mdDLsAdXd3S5IefvJftoMAAO5Ld3e3vF7vHbcnOHdL1LcsHA6rvb1d6enpSkhIuOefCwaDysvLU1tbmzIyMqI44bcj3s5H4pxiBec0/A3383EcR93d3fL5fBox4s6/6Rl2d0AjRozQ6NGjh/zzGRkZw/IfZKji7XwkzilWcE7D33A+n2+687mFhxAAACYIEADARNwEyOPxaPXq1fJ4PNajuCLezkfinGIF5zT8xcv5DLuHEAAA3w1xcwcEAIgtBAgAYIIAAQBMECAAgIm4CNC2bds0ZswYpaSkaPr06Tpz5oz1SEPm9/s1depUpaena9SoUVqwYIFaWlqsx3LNG2+8oYSEBFVWVlqPct8+//xzvfzyy8rKylJqaqoKCwt19uxZ67GGpK+vT7W1tSooKFBqaqoeffRRrVmz5q7f5TWcHD9+XPPnz5fP51NCQoIOHjzYb7vjOFq1apVyc3OVmpqqkpISXbx40WbYe/RN59Tb26vq6moVFhYqLS1NPp9Pr7zyitrb2+0GHqSYD9D+/ftVVVWl1atXq6mpSZMmTdLcuXPV1dVlPdqQHDt2TBUVFTp16pTq6+vV29urZ555Rj09Pdaj3bfGxka9+eabeuKJJ6xHuW/Xrl1TcXGxHnjgAb3//vv6+9//rj/96U8aOXKk9WhDsn79etXV1Wnr1q36xz/+ofXr12vDhg3asmWL9Wj3rKenR5MmTdK2bdsG3L5hwwZt3rxZ27dv1+nTp5WWlqa5c+fq5s2b3/Kk9+6bzunGjRtqampSbW2tmpqa9O6776qlpUXPPfecwaRD5MS4adOmORUVFZHXfX19js/nc/x+v+FU7unq6nIkOceOHbMe5b50d3c7Y8eOderr650f/vCHzvLly61Hui/V1dXO008/bT2Ga+bNm+csWbKk37of/ehHTllZmdFE90eSc+DAgcjrcDjs5OTkOH/4wx8i67788kvH4/E4e/fuNZhw8L5+TgM5c+aMI8m5cuXKtzPUfYrpO6CvvvpK586dU0lJSWTdiBEjVFJSopMnTxpO5p5AICBJyszMNJ7k/lRUVGjevHn9/q1i2aFDh1RUVKQXX3xRo0aN0pQpU7Rz507rsYZs5syZamho0IULFyRJn376qU6cOKHS0lLjydzR2tqqjo6Ofv/783q9mj59etxcK6T/XC8SEhL04IMPWo9yT4bdl5EOxhdffKG+vj5lZ2f3W5+dna1//vOfRlO5JxwOq7KyUsXFxZo4caL1OEO2b98+NTU1qbGx0XoU11y+fFl1dXWqqqrSb37zGzU2NmrZsmVKTk5WeXm59XiDtnLlSgWDQY0bN06JiYnq6+vT2rVrVVZWZj2aKzo6OiRpwGvFrW2x7ubNm6qurtZLL700bL+g9OtiOkDxrqKiQufPn9eJEyesRxmytrY2LV++XPX19UpJSbEexzXhcFhFRUVat26dJGnKlCk6f/68tm/fHpMBeuedd7R7927t2bNHEyZMUHNzsyorK+Xz+WLyfL5rent7tXDhQjmOo7q6Outx7llMfwT30EMPKTExUZ2dnf3Wd3Z2Kicnx2gqdyxdulSHDx/W0aNH7+vPU1g7d+6curq69OSTTyopKUlJSUk6duyYNm/erKSkJPX19VmPOCS5ubkaP358v3WPP/64rl69ajTR/Xn11Ve1cuVKLVq0SIWFhfrJT36iFStWyO/3W4/milvXg3i8VtyKz5UrV1RfXx8zdz9SjAcoOTlZTz31lBoaGiLrwuGwGhoaNGPGDMPJhs5xHC1dulQHDhzQhx9+qIKCAuuR7sucOXP02Wefqbm5ObIUFRWprKxMzc3NSkxMtB5xSIqLi297PP7ChQt6+OGHjSa6Pzdu3LjtD4clJiYqHA4bTeSugoIC5eTk9LtWBINBnT59OmavFdJ/43Px4kX97W9/U1ZWlvVIgxLzH8FVVVWpvLxcRUVFmjZtmjZt2qSenh4tXrzYerQhqaio0J49e/Tee+8pPT098vm01+tVamqq8XSDl56eftvvr9LS0pSVlRXTv9dasWKFZs6cqXXr1mnhwoU6c+aMduzYoR07dliPNiTz58/X2rVrlZ+frwkTJuiTTz7Rxo0btWTJEuvR7tn169d16dKlyOvW1lY1NzcrMzNT+fn5qqys1Ouvv66xY8eqoKBAtbW18vl8WrBggd3Qd/FN55Sbm6sXXnhBTU1NOnz4sPr6+iLXi8zMTCUnJ1uNfe+sH8Nzw5YtW5z8/HwnOTnZmTZtmnPq1CnrkYZM0oDLW2+9ZT2aa+LhMWzHcZy//vWvzsSJEx2Px+OMGzfO2bFjh/VIQxYMBp3ly5c7+fn5TkpKivPII484v/3tb51QKGQ92j07evTogP/fKS8vdxznP49i19bWOtnZ2Y7H43HmzJnjtLS02A59F990Tq2trXe8Xhw9etR69HvCn2MAAJiI6d8BAQBiFwECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABg4v8AXOIgZimPv6sAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# for masks in collated_masks_pred:\n",
        "#     for mask in masks:\n",
        "#         img = torch.ones(h*w)\n",
        "#         img[mask]=0\n",
        "#         imshow(img.reshape(h,w))\n",
        "\n",
        "\n",
        "for masks in collated_masks_enc:\n",
        "    for mask in masks:\n",
        "        img = torch.ones(h*w)\n",
        "        img[mask]=0\n",
        "        imshow(img.reshape(h,w))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "03qzLihCI64O",
        "outputId": "5bc74090-ad4b-42a6-a0f2-b6861bcf1e7b"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for param in seq_jepa.context_encoder.parameters():\n",
        "#     print(param.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ua3GIpEmniwM",
        "outputId": "49ffb855-deb9-413e-abf9-82ebb3db3c61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# context_encoder = TransformerModel(d_model, d_model, out_dim=out_dim, nhead=4, nlayers=2, dropout=0.)\n",
        "# self.predicter = TransformerModel(out_dim, d_model//2, out_dim, nhead=4, nlayers=1, dropout=0.)\n",
        "# self.target_encoder\n",
        "print(sum(p.numel() for p in seq_jepa.target_encoder.parameters() if p.requires_grad)) # 59850\n",
        "# https://openreview.net/pdf?id=MO1OLAKcsJ\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8F8cuvFwMaYk",
        "outputId": "3d85990c-4b33-40e6-c63b-2d99c4411b78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test multiblock mask\n",
        "def multiblock(seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    # mask_len, mask_pos = mask_len * seq, mask_pos * seq\n",
        "    indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "    return target_mask\n",
        "\n",
        "\n",
        "seq=400\n",
        "M=4\n",
        "\n",
        "# target_mask = multiblock(M, seq, min_s=0.15, max_s=0.2, M=1) # mask out targets to be predicted # [4*batch, seq]\n",
        "# context_mask = ~multiblock(1, seq, min_s=0.85, max_s=1., M=1)|target_mask.reshape(M,seq).any(0) # [batch, seq]\n",
        "\n",
        "\n",
        "target_mask = multiblock(seq, min_s=0.15, max_s=0.2, M=4) # mask out targets to be predicted # [4*batch, seq]\n",
        "context_mask = ~multiblock(seq, min_s=0.85, max_s=1., M=1)|target_mask.reshape(M,seq).any(0) # [batch, seq]\n",
        "\n",
        "# target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "\n",
        "# print(target_mask)\n",
        "# print(context_mask)\n",
        "\n",
        "print(target_mask.sum(-1))\n",
        "print(context_mask.sum(-1))\n",
        "\n",
        "\n",
        "# sorted_mask, ids = context_mask.sort(dim=1, stable=True)\n",
        "# indices = ids[~sorted_mask]#.reshape(M,-1) # int idx [num_context_toks] , idx of context not masked\n",
        "# sorted_x = x[torch.arange(batch).unsqueeze(-1), indices] # [batch, seq-num_trg_toks, dim]\n",
        "# print(indices)\n",
        "# print(x)\n",
        "# print(sorted_x)\n",
        "\n",
        "\n",
        "\n",
        "# sorted_mask, ids = target_mask.sort(dim=1, descending=False, stable=True)\n",
        "# # print(sorted_mask, ids)\n",
        "# indices = ids[~sorted_mask].reshape(M,-1) # int idx [M, seq-num_trg_toks] , idx of target not masked\n",
        "# print(indices)\n",
        "# batch=3\n",
        "# dim=2\n",
        "# # indices = indices.expand(batch,-1,-1)\n",
        "# # x = torch.arange(batch*seq).reshape(batch,seq).to(device)\n",
        "# x = torch.rand(batch, seq, dim)\n",
        "# print(x)\n",
        "# sorted_x = x[torch.arange(batch)[...,None,None], indices]\n",
        "# print(sorted_x.shape) # [batch, M, seq-num_trg_toks, dim]\n",
        "# print(sorted_x)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "p8xv1tpKIhim",
        "outputId": "e6613fee-2d32-4c6a-ef96-dee18da3aa6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([73, 73, 73, 73])\n",
            "tensor([248])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(16).to(device)\n",
        "# coptim = torch.optim.AdamW(classifier.parameters(), lr=1e-3)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "id": "wjK1TVwBh2u8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(seq_jepa.classifier.weight[0])\n",
        "# for n, p in seq_jepa.target_encoder.named_parameters():\n",
        "#     print(n,p)\n",
        "optim.param_groups[0]['lr'] = 1e-5\n"
      ],
      "metadata": {
        "id": "7IACKDymhit7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test multiblock params\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# dataiter = iter(train_data)\n",
        "img,y = next(dataiter)\n",
        "img = img.unsqueeze(0)\n",
        "b,c,h,w = img.shape\n",
        "img = img.flatten(2).transpose(-2,-1).to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "print(img.shape)\n",
        "batch, seq,_ = img.shape\n",
        "M=4\n",
        "target_mask = multiblock(seq, min_s=0.15, max_s=0.2, M=M) # mask out targets to be predicted # [M, seq]\n",
        "context_mask = ~multiblock(seq, min_s=0.85, max_s=1., M=1)|target_mask.reshape(M,seq).any(0) # [1, seq], True->Mask\n",
        "target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "# print(target_mask, context_mask)\n",
        "\n",
        "\n",
        "# print(target_mask.shape, context_mask.shape)\n",
        "# target_img, context_img = img*target_mask.unsqueeze(-1), img*context_mask.unsqueeze(-1)\n",
        "target_img, context_img = img*~target_mask.unsqueeze(-1), img*~context_mask.unsqueeze(-1)\n",
        "# print(target_img.shape, context_img.shape)\n",
        "target_img, context_img = target_img.transpose(-2,-1).reshape(M,c,h,w), context_img.transpose(-2,-1).reshape(b,c,h,w)\n",
        "target_img, context_img = target_img.float(), context_img.float()\n",
        "\n",
        "# imshow(out.detach().cpu())\n",
        "imshow(target_img[0])\n",
        "imshow(context_img[0])\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "D6lVtbS5OHIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "34de8800-fa4b-4460-803c-3cd8dbfb6c70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0280f2b9-6f24-4ac7-b581-5fbb9ac12726"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "strain 1.1552958488464355\n",
            "strain 1.0704407691955566\n",
            "strain 0.9711266160011292\n",
            "strain 0.9005165100097656\n",
            "strain 0.8340376615524292\n",
            "strain 0.7421780824661255\n",
            "strain 0.6472571492195129\n",
            "strain 0.6140769124031067\n",
            "strain 0.5460553169250488\n",
            "strain 0.505041778087616\n",
            "strain 0.4577639400959015\n",
            "strain 0.42256808280944824\n",
            "strain 0.39006826281547546\n",
            "strain 0.35492464900016785\n",
            "strain 0.3293170630931854\n",
            "strain 0.29724881052970886\n",
            "strain 0.29417821764945984\n",
            "strain 0.25827911496162415\n",
            "strain 0.23480743169784546\n",
            "strain 0.23319143056869507\n",
            "strain 0.2108106166124344\n",
            "strain 0.18544282019138336\n",
            "strain 0.1860089898109436\n",
            "strain 0.1739669144153595\n",
            "strain 0.153715118765831\n",
            "strain 0.15700477361679077\n",
            "strain 0.13227126002311707\n",
            "strain 0.13519373536109924\n",
            "strain 0.12162256240844727\n",
            "strain 0.11306873708963394\n",
            "strain 0.11244712769985199\n",
            "strain 0.10203242301940918\n",
            "strain 0.09124936163425446\n",
            "strain 0.09037154912948608\n",
            "strain 0.08320996910333633\n",
            "strain 0.0772230476140976\n",
            "strain 0.07183892279863358\n",
            "strain 0.06892077624797821\n",
            "strain 0.06854923069477081\n",
            "strain 0.062312714755535126\n",
            "strain 0.05993054434657097\n",
            "strain 0.061357077211141586\n",
            "strain 0.057531364262104034\n",
            "strain 0.05564126372337341\n",
            "strain 0.04894221946597099\n",
            "strain 0.04388995096087456\n",
            "strain 0.04628865793347359\n",
            "strain 0.04458217695355415\n",
            "strain 0.04027014598250389\n",
            "strain 0.039192166179418564\n",
            "strain 0.038194917142391205\n",
            "strain 0.038498491048812866\n",
            "strain 0.03478943556547165\n",
            "strain 0.03390251472592354\n",
            "strain 0.03349953889846802\n",
            "strain 0.030792957171797752\n",
            "strain 0.03246428817510605\n",
            "strain 0.031920310109853745\n",
            "strain 0.029280006885528564\n",
            "strain 0.02684575691819191\n",
            "strain 0.029655899852514267\n",
            "strain 0.029564669355750084\n",
            "strain 0.025873271748423576\n",
            "strain 0.024095945060253143\n",
            "strain 0.024665554985404015\n",
            "strain 0.024230029433965683\n",
            "strain 0.02121173031628132\n",
            "strain 0.020793458446860313\n",
            "strain 0.020498160272836685\n",
            "strain 0.02249491587281227\n",
            "strain 0.022096656262874603\n",
            "strain 0.01946396566927433\n",
            "strain 0.0201867688447237\n",
            "strain 0.01874823309481144\n",
            "strain 0.018550919368863106\n",
            "strain 0.018430665135383606\n",
            "strain 0.01572244241833687\n",
            "strain 0.016926852986216545\n",
            "strain 0.014599382877349854\n",
            "strain 0.016092421486973763\n",
            "strain 0.014706030488014221\n",
            "strain 0.01574438251554966\n",
            "strain 0.014726105146110058\n",
            "strain 0.013280271552503109\n",
            "strain 0.014228301122784615\n",
            "strain 0.013067211024463177\n",
            "strain 0.01445735152810812\n",
            "strain 0.013121088035404682\n",
            "strain 0.012825529091060162\n",
            "strain 0.011948492377996445\n",
            "strain 0.012264891527593136\n",
            "strain 0.011186182498931885\n",
            "strain 0.012414729222655296\n",
            "strain 0.011302845552563667\n",
            "strain 0.010908124037086964\n",
            "strain 0.010509492829442024\n",
            "strain 0.010712415911257267\n",
            "strain 0.010429790243506432\n",
            "strain 0.010579529218375683\n",
            "strain 0.010254022665321827\n",
            "strain 0.009964284487068653\n",
            "strain 0.009447448886930943\n",
            "strain 0.010602571070194244\n",
            "strain 0.009524340741336346\n",
            "strain 0.00859504658728838\n",
            "strain 0.008981555700302124\n",
            "strain 0.008881055749952793\n",
            "strain 0.007980020716786385\n",
            "strain 0.008539807982742786\n",
            "strain 0.007424124050885439\n",
            "strain 0.008510425686836243\n",
            "strain 0.009263998828828335\n",
            "strain 0.0070006875321269035\n",
            "strain 0.007208559196442366\n",
            "strain 0.007313998881727457\n",
            "strain 0.007378030568361282\n",
            "strain 0.007293639238923788\n",
            "strain 0.007066840305924416\n",
            "strain 0.007616862654685974\n",
            "strain 0.006457763258367777\n",
            "strain 0.006960611324757338\n",
            "strain 0.006605572532862425\n",
            "strain 0.006134033203125\n",
            "strain 0.008093641139566898\n",
            "strain 0.006544800475239754\n",
            "strain 0.005979385692626238\n",
            "strain 0.006223389413207769\n",
            "strain 0.006684292107820511\n",
            "strain 0.006161769852042198\n",
            "strain 0.00552735012024641\n",
            "strain 0.00574916647747159\n",
            "strain 0.005715875420719385\n",
            "strain 0.005590375978499651\n",
            "strain 0.005556389689445496\n",
            "strain 0.005745932459831238\n",
            "strain 0.005447981413453817\n",
            "strain 0.005393325816839933\n",
            "strain 0.005200666841119528\n",
            "strain 0.004596225451678038\n",
            "strain 0.00487957987934351\n",
            "strain 0.0049401563592255116\n",
            "strain 0.004358706995844841\n",
            "strain 0.0045340717770159245\n",
            "strain 0.004875799175351858\n",
            "strain 0.00467523792758584\n",
            "strain 0.004495340399444103\n",
            "strain 0.0043167429976165295\n",
            "strain 0.004461549688130617\n",
            "strain 0.004362260922789574\n",
            "strain 0.0045483410358428955\n",
            "strain 0.004154468886554241\n",
            "strain 0.003966260701417923\n",
            "strain 0.00367836095392704\n",
            "strain 0.004095320124179125\n",
            "strain 0.00412365049123764\n",
            "strain 0.0035720719024538994\n",
            "strain 0.0037898486480116844\n",
            "strain 0.0037250686436891556\n",
            "strain 0.0038496986962854862\n",
            "strain 0.0034811615478247404\n",
            "strain 0.003500637598335743\n",
            "strain 0.0034729638136923313\n",
            "strain 0.0036170610692352057\n",
            "strain 0.0037654966581612825\n",
            "strain 0.0038944880943745375\n",
            "strain 0.0033657762687653303\n",
            "strain 0.0035384688526391983\n",
            "strain 0.0035638678818941116\n",
            "strain 0.0035368602257221937\n",
            "strain 0.003280187724158168\n",
            "strain 0.0036710419226437807\n",
            "strain 0.0031798891723155975\n",
            "strain 0.0029044323600828648\n",
            "strain 0.0030199119355529547\n",
            "strain 0.003429725067690015\n",
            "strain 0.0031277642119675875\n",
            "strain 0.002876516431570053\n",
            "strain 0.0030592861585319042\n",
            "strain 0.0028965480159968138\n",
            "strain 0.003004196099936962\n",
            "strain 0.0027912738732993603\n",
            "strain 0.0029603573493659496\n",
            "strain 0.002958259778097272\n",
            "strain 0.002693675458431244\n",
            "strain 0.0027678378392010927\n",
            "strain 0.0026196062099188566\n",
            "strain 0.002698301337659359\n",
            "strain 0.0025240795221179724\n",
            "strain 0.0026401004288345575\n",
            "strain 0.0024694008752703667\n",
            "strain 0.0023409887216985226\n",
            "strain 0.0029165279120206833\n",
            "strain 0.0028197362553328276\n",
            "strain 0.002435136353597045\n",
            "strain 0.0023018610663712025\n",
            "strain 0.0024638280738145113\n",
            "strain 0.0025938856415450573\n",
            "strain 0.0025457378942519426\n",
            "strain 0.0023117808159440756\n",
            "strain 0.0020931584294885397\n",
            "strain 0.002298865234479308\n",
            "strain 0.00226531527005136\n",
            "strain 0.0023799759801477194\n",
            "strain 0.0023204595781862736\n",
            "strain 0.0022677432280033827\n",
            "strain 0.0024655333254486322\n",
            "strain 0.0023631250951439142\n",
            "strain 0.002355307573452592\n",
            "strain 0.002288487972691655\n",
            "strain 0.002074933610856533\n",
            "strain 0.0017077468801289797\n",
            "strain 0.0019578661303967237\n",
            "strain 0.002168983919546008\n",
            "strain 0.002019446110352874\n",
            "strain 0.0018777484074234962\n",
            "strain 0.0021918541751801968\n",
            "strain 0.001988939940929413\n",
            "strain 0.001921414164826274\n",
            "strain 0.0019391922978684306\n",
            "strain 0.0019537596963346004\n",
            "strain 0.0020576566457748413\n",
            "strain 0.0017933107446879148\n",
            "strain 0.0019293082877993584\n",
            "strain 0.0019037399906665087\n",
            "strain 0.0017867365386337042\n",
            "strain 0.0017106900922954082\n",
            "strain 0.0016943695954978466\n",
            "strain 0.0018292563036084175\n",
            "strain 0.0016274572117254138\n",
            "strain 0.0016878970200195909\n",
            "strain 0.0015847578179091215\n",
            "strain 0.0015370137989521027\n",
            "strain 0.0014685973292216659\n",
            "strain 0.0016770710935816169\n",
            "strain 0.0015087354695424438\n",
            "strain 0.0015659754863008857\n",
            "strain 0.0015775517094880342\n",
            "strain 0.0016488715773448348\n",
            "strain 0.0016359997680410743\n",
            "strain 0.0014347727410495281\n",
            "strain 0.0014800216304138303\n",
            "strain 0.0014010806335136294\n",
            "strain 0.0014083492569625378\n",
            "strain 0.0015845721354708076\n",
            "strain 0.0014637650456279516\n",
            "strain 0.001324658514931798\n",
            "strain 0.0014418887440115213\n",
            "strain 0.0013320369180291891\n",
            "strain 0.0013538678176701069\n",
            "strain 0.0013967889826744795\n",
            "strain 0.0013867774978280067\n",
            "strain 0.0013502184301614761\n",
            "strain 0.0014439933001995087\n",
            "strain 0.0013921541394665837\n",
            "strain 0.0013912171125411987\n",
            "strain 0.001279774704016745\n",
            "strain 0.0014420337975025177\n",
            "strain 0.0013124027755111456\n",
            "strain 0.0013993436004966497\n",
            "strain 0.0013290437636896968\n",
            "strain 0.0012875112006440759\n",
            "strain 0.0012994545977562666\n",
            "strain 0.0012477547861635685\n",
            "strain 0.0012376008089631796\n",
            "strain 0.0012695236364379525\n",
            "strain 0.001225248328410089\n",
            "strain 0.001138373394496739\n",
            "strain 0.001210963586345315\n",
            "strain 0.0012310231104493141\n",
            "strain 0.0010957373306155205\n",
            "strain 0.001249641994945705\n",
            "strain 0.001191025017760694\n",
            "strain 0.001186066074296832\n",
            "strain 0.0011767297983169556\n",
            "strain 0.0012796670198440552\n",
            "strain 0.0011406854027882218\n",
            "strain 0.001222667982801795\n",
            "strain 0.001103400718420744\n",
            "strain 0.0012384452857077122\n",
            "strain 0.0011167328339070082\n",
            "strain 0.001050827675499022\n",
            "strain 0.001088742632418871\n",
            "strain 0.001128755509853363\n",
            "strain 0.0011261975159868598\n",
            "strain 0.0010561870876699686\n",
            "strain 0.000996538088656962\n",
            "strain 0.000997219467535615\n",
            "strain 0.0009941428434103727\n",
            "strain 0.0010090284049510956\n",
            "strain 0.0009922662284225225\n",
            "strain 0.001005551079288125\n",
            "strain 0.0009800094412639737\n",
            "strain 0.0010868672979995608\n",
            "strain 0.0010611049365252256\n",
            "strain 0.0009737313375808299\n",
            "strain 0.0009097399888560176\n",
            "strain 0.0009325718856416643\n",
            "strain 0.001062059891410172\n",
            "strain 0.0008838095236569643\n",
            "strain 0.0009443695889785886\n",
            "strain 0.0009646678226999938\n",
            "strain 0.0008693699492141604\n",
            "strain 0.0008959110709838569\n",
            "strain 0.001004109508357942\n",
            "strain 0.0008746854728087783\n",
            "strain 0.0009381066774949431\n",
            "strain 0.0009211486321873963\n",
            "strain 0.0008880264358595014\n",
            "strain 0.0008777628536336124\n",
            "strain 0.0009127114317379892\n",
            "strain 0.0008137642871588469\n",
            "strain 0.0009069739608094096\n",
            "strain 0.0008204181212931871\n",
            "strain 0.00084850745042786\n",
            "strain 0.0009152613929472864\n",
            "strain 0.0009758732048794627\n",
            "strain 0.0008572512888349593\n",
            "strain 0.0007517208578065038\n",
            "strain 0.000853287463542074\n",
            "strain 0.0008171302615664899\n",
            "strain 0.0008444480481557548\n",
            "strain 0.0007680989801883698\n",
            "strain 0.0008314992301166058\n",
            "strain 0.0008061761036515236\n",
            "strain 0.0008322872454300523\n",
            "strain 0.0007866756059229374\n",
            "strain 0.0008009315934032202\n",
            "strain 0.0008006698917597532\n",
            "strain 0.0006737209623679519\n",
            "strain 0.0007203415152616799\n",
            "strain 0.000836644961964339\n",
            "strain 0.0007396072032861412\n",
            "strain 0.0006450827931985259\n",
            "strain 0.0007667045574635267\n",
            "strain 0.0007816581055521965\n",
            "strain 0.0006899773143231869\n",
            "strain 0.0007246176246553659\n",
            "strain 0.0008037359220907092\n",
            "strain 0.0007609208696521819\n",
            "strain 0.0007205200381577015\n",
            "strain 0.0007263563456945121\n",
            "strain 0.0007139595109038055\n",
            "strain 0.0007439746404998004\n",
            "strain 0.0006697321077808738\n",
            "strain 0.0006471651722677052\n",
            "strain 0.0006817820249125361\n",
            "strain 0.0006912592798471451\n",
            "strain 0.0006509532104246318\n",
            "strain 0.0006708874716423452\n",
            "strain 0.0006286195712164044\n",
            "strain 0.0006964699132367969\n",
            "strain 0.0006867232732474804\n",
            "strain 0.000678516342304647\n",
            "strain 0.0007302460144273937\n",
            "strain 0.000631919305305928\n",
            "strain 0.0006941670435480773\n",
            "strain 0.0006204629899002612\n",
            "strain 0.0006904394249431789\n",
            "strain 0.000674689479637891\n",
            "strain 0.0006361766718327999\n",
            "strain 0.0006688270368613303\n",
            "strain 0.000683510908856988\n",
            "strain 0.0006551627884618938\n",
            "strain 0.0006071941461414099\n",
            "strain 0.0005894963978789747\n",
            "strain 0.000620147620793432\n",
            "strain 0.000639355625025928\n",
            "strain 0.0006290902383625507\n",
            "strain 0.0006076565477997065\n",
            "strain 0.0005782974185422063\n",
            "strain 0.0005448737647384405\n",
            "strain 0.0006205540848895907\n",
            "strain 0.0006402021390385926\n",
            "strain 0.0005748198600485921\n",
            "strain 0.0006213834858499467\n",
            "strain 0.0006131528061814606\n",
            "strain 0.0005978615372441709\n",
            "strain 0.000583479821216315\n",
            "strain 0.0005320859490893781\n",
            "strain 0.0005441566463559866\n",
            "strain 0.0006103349733166397\n",
            "strain 0.0005794939934276044\n",
            "strain 0.0005297152674756944\n",
            "strain 0.00054570147767663\n",
            "strain 0.0005138939013704658\n",
            "strain 0.0006186561076901853\n",
            "strain 0.0005968321929685771\n",
            "strain 0.000561571097932756\n",
            "strain 0.000542272871825844\n",
            "strain 0.0005367823177948594\n",
            "strain 0.0005327421240508556\n",
            "classify 2.41937255859375\n",
            "classify 2.37689208984375\n",
            "classify 2.3187255859375\n",
            "classify 2.36407470703125\n",
            "classify 2.3695068359375\n",
            "classify 2.37982177734375\n",
            "classify 2.30908203125\n",
            "classify 2.4105224609375\n",
            "classify 2.4256591796875\n",
            "classify 2.393310546875\n",
            "0.1171875\n",
            "0.0625\n",
            "0.1015625\n",
            "0.1015625\n",
            "0.078125\n",
            "0.125\n",
            "0.125\n",
            "0.1015625\n",
            "0.125\n",
            "0.1171875\n",
            "strain 0.0005476611549966037\n",
            "strain 0.0005496531957760453\n",
            "strain 0.0005053931963630021\n",
            "strain 0.0005145301693119109\n",
            "strain 0.0004905083333142102\n",
            "strain 0.000468552258098498\n",
            "strain 0.0004976391792297363\n",
            "strain 0.00048723354120738804\n",
            "strain 0.0005165807087905705\n",
            "strain 0.0004894555313512683\n",
            "strain 0.0004792613326571882\n",
            "strain 0.0004894296871498227\n",
            "strain 0.0005403644172474742\n",
            "strain 0.00047912116860970855\n",
            "strain 0.0004986769636161625\n",
            "strain 0.0005498703103512526\n",
            "strain 0.0004951056907884777\n",
            "strain 0.00047781108878552914\n",
            "strain 0.000444254168542102\n",
            "strain 0.00048231417895294726\n",
            "strain 0.0004810647515114397\n",
            "strain 0.0004796907596755773\n",
            "strain 0.0005127075710333884\n",
            "strain 0.00043294180068187416\n",
            "strain 0.0005072779022157192\n",
            "strain 0.0004466191167011857\n",
            "strain 0.0005244279163889587\n",
            "strain 0.0004840806359425187\n",
            "strain 0.0004519086505752057\n",
            "strain 0.0004957314813509583\n",
            "strain 0.0004797881410922855\n",
            "strain 0.0004853664431720972\n",
            "strain 0.00046717276563867927\n",
            "strain 0.00046822638250887394\n",
            "strain 0.000480713410070166\n",
            "strain 0.0004669364425353706\n",
            "strain 0.0005084576550871134\n",
            "strain 0.0004572783363983035\n",
            "strain 0.0004865740775130689\n",
            "strain 0.0004451396525837481\n",
            "strain 0.00044843435171060264\n",
            "strain 0.0004242957220412791\n",
            "strain 0.0004359438898973167\n",
            "strain 0.0004529317084234208\n",
            "strain 0.0004985045525245368\n",
            "strain 0.00042051132186315954\n",
            "strain 0.0004395439464133233\n",
            "strain 0.0004397295415401459\n",
            "strain 0.00044721754966303706\n",
            "strain 0.0004281318688299507\n",
            "strain 0.000395629700506106\n",
            "strain 0.00043188061681576073\n",
            "strain 0.00041947246063500643\n",
            "strain 0.0003854134993162006\n",
            "strain 0.0004124757833778858\n",
            "strain 0.0003856993280351162\n",
            "strain 0.00043012824608013034\n",
            "strain 0.0003922321193385869\n",
            "strain 0.00042308526462875307\n",
            "strain 0.00039615240530110896\n",
            "strain 0.0004062896769028157\n",
            "strain 0.00037375741521827877\n",
            "strain 0.00038024800596758723\n",
            "strain 0.0003538875898811966\n",
            "strain 0.00038636341923847795\n",
            "strain 0.0003933033440262079\n",
            "strain 0.00040218402864411473\n",
            "strain 0.0003727117436937988\n",
            "strain 0.0003346736484672874\n",
            "strain 0.00039419386303052306\n",
            "strain 0.0004211963387206197\n",
            "strain 0.000360120931873098\n",
            "strain 0.00036538412678055465\n",
            "strain 0.00037226121639832854\n",
            "strain 0.0003670692094601691\n",
            "strain 0.0003967310767620802\n",
            "strain 0.0003227034176234156\n",
            "strain 0.0003750403120648116\n",
            "strain 0.0004199761897325516\n",
            "strain 0.0003704715345520526\n",
            "strain 0.0003615497553255409\n",
            "strain 0.000376556912669912\n",
            "strain 0.0003572602290660143\n",
            "strain 0.00036148980143480003\n",
            "strain 0.0003613267617765814\n",
            "strain 0.0004135736671742052\n",
            "strain 0.0003485523338895291\n",
            "strain 0.00037957096355967224\n",
            "strain 0.0003688578144647181\n",
            "strain 0.0003214931348338723\n",
            "strain 0.00032715193810872734\n",
            "strain 0.0003435287799220532\n",
            "strain 0.00039803734398446977\n",
            "strain 0.00038197447429411113\n",
            "strain 0.00035758240846917033\n",
            "strain 0.0003858843119814992\n",
            "strain 0.00032762871705926955\n",
            "strain 0.00032335109426639974\n",
            "strain 0.00035518044023774564\n",
            "strain 0.00033446180168539286\n",
            "strain 0.00032510055461898446\n",
            "strain 0.0003291343746241182\n",
            "strain 0.0003323010168969631\n",
            "strain 0.0003327975864522159\n",
            "strain 0.00033419381361454725\n",
            "strain 0.00035256074625067413\n",
            "strain 0.00035457214107736945\n",
            "strain 0.0003234757168684155\n",
            "strain 0.0003061486058868468\n",
            "strain 0.00034068673267029226\n",
            "strain 0.00034704795689322054\n",
            "strain 0.00036422579432837665\n",
            "strain 0.0003479501756373793\n",
            "strain 0.0003472482494544238\n",
            "strain 0.0003144664515275508\n",
            "strain 0.00034325203159824014\n",
            "strain 0.00035610003396868706\n",
            "strain 0.0003393747319933027\n",
            "strain 0.000304522953229025\n",
            "strain 0.0003306410799268633\n",
            "strain 0.00036197787267155945\n",
            "strain 0.00032991045736707747\n",
            "strain 0.0003181346401106566\n",
            "strain 0.00030050880741328\n",
            "strain 0.00030829443130642176\n",
            "strain 0.0002989272470586002\n",
            "strain 0.00031585327815264463\n",
            "strain 0.00029256311245262623\n",
            "strain 0.0003274069749750197\n",
            "strain 0.00029788052779622376\n",
            "strain 0.00030355813214555383\n",
            "strain 0.00030878017423674464\n",
            "strain 0.0003051607636734843\n",
            "strain 0.00029624372837133706\n",
            "strain 0.0002851482422556728\n",
            "strain 0.0003150974807795137\n",
            "strain 0.00028210971504449844\n",
            "strain 0.0003086082870140672\n",
            "strain 0.0003054825065191835\n",
            "strain 0.0003064046613872051\n",
            "strain 0.00033486747997812927\n",
            "strain 0.00028028638917021453\n",
            "strain 0.00030158893787302077\n",
            "strain 0.00030129836522974074\n",
            "strain 0.00027438002871349454\n",
            "strain 0.00030541818705387414\n",
            "strain 0.00027097045676782727\n",
            "strain 0.0002768136910162866\n",
            "strain 0.0002620201848912984\n",
            "strain 0.0002522601280361414\n",
            "strain 0.0002937346580438316\n",
            "strain 0.0002776431501843035\n",
            "strain 0.0003238215867895633\n",
            "strain 0.00026859264471568167\n",
            "strain 0.00030942607554607093\n",
            "strain 0.00025713536888360977\n",
            "strain 0.0002915925288107246\n",
            "strain 0.0002703815698623657\n",
            "strain 0.0002976959804072976\n",
            "strain 0.000283700181171298\n",
            "strain 0.00029329131939448416\n",
            "strain 0.0002643385960254818\n",
            "strain 0.0002570576616562903\n",
            "strain 0.00026536561199463904\n",
            "strain 0.0002757337933871895\n",
            "strain 0.00029514136258512735\n",
            "strain 0.00027677210164256394\n",
            "strain 0.00028500379994511604\n",
            "strain 0.0002676850708667189\n",
            "strain 0.0002853548794519156\n",
            "strain 0.00028324476443231106\n",
            "strain 0.00026082180556841195\n",
            "strain 0.00026414584135636687\n",
            "strain 0.0003094107669312507\n",
            "strain 0.00026256212731823325\n",
            "strain 0.00026192617951892316\n",
            "strain 0.00024281980586238205\n",
            "strain 0.00028096800087951124\n",
            "strain 0.00024791399482637644\n",
            "strain 0.0002623293548822403\n",
            "strain 0.0002742816577665508\n",
            "strain 0.00024792065960355103\n",
            "strain 0.0002577141858637333\n",
            "strain 0.0002870418247766793\n",
            "strain 0.00026591483037918806\n",
            "strain 0.00027086088084615767\n",
            "strain 0.00023192711523734033\n",
            "strain 0.00023602521105203778\n",
            "strain 0.0002564366441220045\n",
            "strain 0.00027153806877322495\n",
            "strain 0.00023900814994703978\n",
            "strain 0.0002590077347122133\n",
            "strain 0.0002453065535519272\n",
            "strain 0.00023879394575487822\n",
            "strain 0.00025392475072294474\n",
            "strain 0.00026725989300757647\n",
            "strain 0.0002438230876578018\n",
            "strain 0.00025800481671467423\n",
            "strain 0.00025719619588926435\n",
            "strain 0.0002479091635905206\n",
            "strain 0.0002527642354834825\n",
            "strain 0.0002753533481154591\n",
            "strain 0.0002280230837641284\n",
            "strain 0.0002548153861425817\n",
            "strain 0.0002352661977056414\n",
            "strain 0.0002472889609634876\n",
            "strain 0.0002523162111174315\n",
            "strain 0.00024267390836030245\n",
            "strain 0.0002517853572499007\n",
            "strain 0.0002471116604283452\n",
            "strain 0.00025147819542326033\n",
            "strain 0.00024018458498176187\n",
            "strain 0.0002741332573350519\n",
            "strain 0.00024517718702554703\n",
            "strain 0.00022101028298493475\n",
            "strain 0.0002475850924383849\n",
            "strain 0.0002698589814826846\n",
            "strain 0.0002410540182609111\n",
            "strain 0.0002472391934134066\n",
            "strain 0.0002489884209353477\n",
            "strain 0.00025417684810236096\n",
            "strain 0.00023706733190920204\n",
            "strain 0.00024141724861692637\n",
            "strain 0.00024703730014152825\n",
            "strain 0.00025190593441948295\n",
            "strain 0.00024369503080379218\n",
            "strain 0.00021448300685733557\n",
            "strain 0.00022927179816178977\n",
            "strain 0.00024770398158580065\n",
            "strain 0.00023307600349653512\n",
            "strain 0.00021868890326004475\n",
            "strain 0.00023364441585727036\n",
            "strain 0.00022556481417268515\n",
            "strain 0.0002643394691403955\n",
            "strain 0.00023730911198072135\n",
            "strain 0.00023267650976777077\n",
            "strain 0.00021553560509346426\n",
            "strain 0.0002336167817702517\n",
            "strain 0.00020419876091182232\n",
            "strain 0.0002389295696048066\n",
            "strain 0.0002238214947283268\n",
            "strain 0.00025124428793787956\n",
            "strain 0.0002022814442170784\n",
            "strain 0.00022472566342912614\n",
            "strain 0.0002013020566664636\n",
            "strain 0.00020941885304637253\n",
            "strain 0.00024025581660680473\n",
            "strain 0.00024521458544768393\n",
            "strain 0.0002563120797276497\n",
            "strain 0.00020234857220202684\n",
            "strain 0.00022172703756950796\n",
            "strain 0.00021829112665727735\n",
            "strain 0.0002252755220979452\n",
            "strain 0.00024066139303613454\n",
            "strain 0.00022992564481683075\n",
            "strain 0.0002398708020336926\n",
            "strain 0.00021946465130895376\n",
            "strain 0.0002287572860950604\n",
            "strain 0.00023479090305045247\n",
            "strain 0.00021045075845904648\n",
            "strain 0.00022045699006412178\n",
            "strain 0.0002047916641458869\n",
            "strain 0.00020422946545295417\n",
            "strain 0.00022052002896089107\n",
            "strain 0.0002100794663419947\n",
            "strain 0.0002344137610634789\n",
            "strain 0.00020907916768919677\n",
            "strain 0.0002287651877850294\n",
            "strain 0.00022574490867555141\n",
            "strain 0.00022311677457764745\n",
            "strain 0.0002438883821014315\n",
            "strain 0.00022245071886572987\n",
            "strain 0.0001940504153026268\n",
            "strain 0.00021818811364937574\n",
            "strain 0.0001945725380210206\n",
            "strain 0.00021559039305429906\n",
            "strain 0.00021228961122687906\n",
            "strain 0.00020774448057636619\n",
            "strain 0.0001843045320129022\n",
            "strain 0.00019811908714473248\n",
            "strain 0.00021578608721029013\n",
            "strain 0.00022957443434279412\n",
            "strain 0.00019025879737455398\n",
            "strain 0.00020922694238834083\n",
            "strain 0.0001800558384275064\n",
            "strain 0.00022018449089955539\n",
            "strain 0.00023013970348984003\n",
            "strain 0.0002019769453909248\n",
            "strain 0.00021382563863880932\n",
            "strain 0.00020735680300276726\n",
            "strain 0.00022141917725093663\n",
            "strain 0.00019516792963258922\n",
            "strain 0.00022279101540334523\n",
            "strain 0.00020704817143268883\n",
            "strain 0.0002106249739881605\n",
            "strain 0.00020836733165197074\n",
            "strain 0.0001977756473934278\n",
            "strain 0.00019565637921914458\n",
            "strain 0.00020578077237587422\n",
            "strain 0.00020514809875749052\n",
            "strain 0.0002116430550813675\n",
            "strain 0.00019422928744461387\n",
            "strain 0.00020866036356892437\n",
            "strain 0.00021860486594960093\n",
            "strain 0.00020671874517574906\n",
            "strain 0.00019600446103140712\n",
            "strain 0.00019998107745777816\n",
            "strain 0.00020416860934346914\n",
            "strain 0.00020943429262842983\n",
            "strain 0.00021801982074975967\n",
            "strain 0.00020083508570678532\n",
            "strain 0.0002014463534578681\n",
            "strain 0.00020247178326826543\n",
            "strain 0.00019522791262716055\n",
            "strain 0.00019676006922964007\n",
            "strain 0.00020064925774931908\n",
            "strain 0.00022466990048997104\n",
            "strain 0.00020473093900363892\n",
            "strain 0.0002077084209304303\n",
            "strain 0.00017978425603359938\n",
            "strain 0.00019143050303682685\n",
            "strain 0.0002190594532294199\n",
            "strain 0.0002021936816163361\n",
            "strain 0.00020486398716457188\n",
            "strain 0.00020539412798825651\n",
            "strain 0.00019898357277270406\n",
            "strain 0.00020030030282214284\n",
            "strain 0.00020145955204498023\n",
            "strain 0.0002058903337456286\n",
            "strain 0.00020170827338006347\n",
            "strain 0.0002031277836067602\n",
            "strain 0.00021067715715616941\n",
            "strain 0.00019181928655598313\n",
            "strain 0.00016050180420279503\n",
            "strain 0.0001827184169087559\n",
            "strain 0.0002125448372680694\n",
            "strain 0.00022199121303856373\n",
            "strain 0.00018400528642814606\n",
            "strain 0.0001767306966939941\n",
            "strain 0.00021010746422689408\n",
            "strain 0.00021198169270064682\n",
            "strain 0.00020445430709514767\n",
            "strain 0.00020347752433735877\n",
            "strain 0.00021325288980733603\n",
            "strain 0.0001951212325366214\n",
            "strain 0.00022045362857170403\n",
            "strain 0.00019272266945336014\n",
            "strain 0.00020235171541571617\n",
            "strain 0.00018569480744190514\n",
            "strain 0.00017710971587803215\n",
            "strain 0.00018141523469239473\n",
            "strain 0.00020611674699466676\n",
            "strain 0.0001822466147132218\n",
            "strain 0.00019297853577882051\n",
            "strain 0.00019191333558410406\n",
            "strain 0.00020176236284896731\n",
            "strain 0.00015856126265134662\n",
            "strain 0.0002091399219352752\n",
            "strain 0.00019281347340438515\n",
            "strain 0.00020313174172770232\n",
            "strain 0.00018499622819945216\n",
            "strain 0.00018639295012690127\n",
            "strain 0.00022800177976023406\n",
            "strain 0.00017056078650057316\n",
            "strain 0.00019202350813429803\n",
            "strain 0.00021454106899909675\n",
            "strain 0.00017493903578724712\n",
            "strain 0.00017929535533767194\n",
            "strain 0.0002158501447411254\n",
            "strain 0.00018790348258335143\n",
            "strain 0.00018291175365447998\n",
            "strain 0.00021118766744621098\n",
            "strain 0.00020905239216517657\n",
            "strain 0.00020139814296271652\n",
            "strain 0.00018384691793471575\n",
            "strain 0.00017111515626311302\n",
            "strain 0.00018602184718474746\n",
            "strain 0.00019503678777255118\n",
            "strain 0.00017962377751246095\n",
            "strain 0.00019895497825928032\n",
            "strain 0.00019513737061060965\n",
            "classify 2.37445068359375\n",
            "classify 2.396728515625\n",
            "classify 2.42401123046875\n",
            "classify 2.38104248046875\n",
            "classify 2.45001220703125\n",
            "classify 2.367431640625\n",
            "classify 2.404541015625\n",
            "classify 2.38494873046875\n",
            "classify 2.398193359375\n",
            "classify 2.36627197265625\n",
            "0.0703125\n",
            "0.0859375\n",
            "0.1171875\n",
            "0.0625\n",
            "0.09375\n",
            "0.109375\n",
            "0.0859375\n",
            "0.1171875\n",
            "0.0625\n",
            "0.0703125\n",
            "strain 0.00019957195036113262\n",
            "strain 0.00018671416910365224\n",
            "strain 0.00020517673692665994\n",
            "strain 0.00021404695871751755\n",
            "strain 0.00020070263417437673\n",
            "strain 0.00017366760584991425\n",
            "strain 0.00020238581055309623\n",
            "strain 0.00018468611233402044\n",
            "strain 0.00019759382121264935\n",
            "strain 0.00019848420924972743\n",
            "strain 0.00020937115186825395\n",
            "strain 0.00019130190776195377\n",
            "strain 0.00017221347661688924\n",
            "strain 0.0001936866610776633\n",
            "strain 0.00016614612832199782\n",
            "strain 0.0001669720804784447\n",
            "strain 0.00018270357395522296\n",
            "strain 0.00018073395767714828\n",
            "strain 0.0001654375810176134\n",
            "strain 0.00018225856183562428\n",
            "strain 0.00017171671788673848\n",
            "strain 0.00019142388191539794\n",
            "strain 0.00017900274542625993\n",
            "strain 0.0001834547583712265\n",
            "strain 0.00017304462380707264\n",
            "strain 0.0002069554611807689\n",
            "strain 0.00018642919894773513\n",
            "strain 0.000188345045899041\n",
            "strain 0.00019128785061184317\n",
            "strain 0.000172876549186185\n",
            "strain 0.0001879602059489116\n",
            "strain 0.00018411013297736645\n",
            "strain 0.00020354011212475598\n",
            "strain 0.00019588803115766495\n",
            "strain 0.00018667847325559705\n",
            "strain 0.00018313851614948362\n",
            "strain 0.0001866872189566493\n",
            "strain 0.00017998651310335845\n",
            "strain 0.00020681148453149945\n",
            "strain 0.00015554581477772444\n",
            "strain 0.00019024594803340733\n",
            "strain 0.00019766790501307696\n",
            "strain 0.00019925607193727046\n",
            "strain 0.00021397195814643055\n",
            "strain 0.00020419486099854112\n",
            "strain 0.00018376557272858918\n",
            "strain 0.0001672065700404346\n",
            "strain 0.00018848282343242317\n",
            "strain 0.0001832576235756278\n",
            "strain 0.00018177206220570952\n",
            "strain 0.00019568174320738763\n",
            "strain 0.00022348079073708504\n",
            "strain 0.00018314775661565363\n",
            "strain 0.0001803188060875982\n",
            "strain 0.00017355229647364467\n",
            "strain 0.00021430625929497182\n",
            "strain 0.0001878882758319378\n",
            "strain 0.00019649053865578026\n",
            "strain 0.00018240824283566326\n",
            "strain 0.00017557796672917902\n",
            "strain 0.00018087492207996547\n",
            "strain 0.00018638964684214443\n",
            "strain 0.0001810318644857034\n",
            "strain 0.00018561458273325115\n",
            "strain 0.00019571400480344892\n",
            "strain 0.00017517687228973955\n",
            "strain 0.00017980769916903228\n",
            "strain 0.0001883713121060282\n",
            "strain 0.00017518042295705527\n",
            "strain 0.00018581617041490972\n",
            "strain 0.00018720448133535683\n",
            "strain 0.000177001507836394\n",
            "strain 0.00017147471953649074\n",
            "strain 0.00017367879627272487\n",
            "strain 0.00019261203124187887\n",
            "strain 0.0001984618866117671\n",
            "strain 0.00017104280414059758\n",
            "strain 0.0001908730046125129\n",
            "strain 0.00016430353571195155\n",
            "strain 0.00017202952585648745\n",
            "strain 0.00018781283870339394\n",
            "strain 0.00018585672660265118\n",
            "strain 0.00017709791427478194\n",
            "strain 0.00019770770450122654\n",
            "strain 0.00019711330241989344\n",
            "strain 0.0002023068955168128\n",
            "strain 0.00018015129899140447\n",
            "strain 0.0001937565830303356\n",
            "strain 0.0001918135822052136\n",
            "strain 0.0001741132145980373\n",
            "strain 0.0001870656560640782\n",
            "strain 0.00017084322462324053\n",
            "strain 0.0001979343214770779\n",
            "strain 0.0001934379688464105\n",
            "strain 0.00018979725427925587\n",
            "strain 0.00018171078409068286\n",
            "strain 0.00016796983254607767\n",
            "strain 0.00018320661911275238\n",
            "strain 0.00017356540774926543\n",
            "strain 0.00018250622088089585\n",
            "strain 0.0001615631626918912\n",
            "strain 0.00017192622181028128\n",
            "strain 0.00019967011758126318\n",
            "strain 0.0001931820879690349\n",
            "strain 0.00018671674479264766\n",
            "strain 0.00019293070363346487\n",
            "strain 0.0001699924177955836\n",
            "strain 0.00017575790116097778\n",
            "strain 0.00017157722322735935\n",
            "strain 0.00020451027376111597\n",
            "strain 0.00019555518520064652\n",
            "strain 0.00018217643082607538\n",
            "strain 0.00015404804435092956\n",
            "strain 0.00018560407625045627\n",
            "strain 0.00018014665693044662\n",
            "strain 0.00016551197040826082\n",
            "strain 0.00017025385750457644\n",
            "strain 0.00017883408872876316\n",
            "strain 0.00015838758554309607\n",
            "strain 0.0001885709643829614\n",
            "strain 0.0001941198279382661\n",
            "strain 0.00017907566507346928\n",
            "strain 0.00018158002058044076\n",
            "strain 0.00018691725563257933\n",
            "strain 0.00017366923566441983\n",
            "strain 0.00020630743529181927\n",
            "strain 0.00020971389312762767\n",
            "strain 0.00017498516535852104\n",
            "strain 0.00020434010366443545\n",
            "strain 0.0002068453759420663\n",
            "strain 0.00017753278370946646\n",
            "strain 0.0001725388428894803\n",
            "strain 0.00018746564455796033\n",
            "strain 0.0001778018631739542\n",
            "strain 0.0001795811258489266\n",
            "strain 0.00018719623039942235\n",
            "strain 0.00017624374595470726\n",
            "strain 0.00018235124298371375\n",
            "strain 0.00018103347974829376\n",
            "strain 0.00016889294784050435\n",
            "strain 0.00016998950741253793\n",
            "strain 0.00017313148418907076\n",
            "strain 0.00018933136016130447\n",
            "strain 0.00019585980044212192\n",
            "strain 0.00019938600598834455\n",
            "strain 0.000161023810505867\n",
            "strain 0.00015201859059743583\n",
            "strain 0.0001718903222354129\n",
            "strain 0.00018002797150984406\n",
            "strain 0.000179600203409791\n",
            "strain 0.0001725245383568108\n",
            "strain 0.000192964929738082\n",
            "strain 0.00018474967509973794\n",
            "strain 0.00016918237088248134\n",
            "strain 0.00018343792180530727\n",
            "strain 0.00020677941211033612\n",
            "strain 0.0001805272331694141\n",
            "strain 0.00019858244922943413\n",
            "strain 0.00018091815582010895\n",
            "strain 0.0001606190053280443\n",
            "strain 0.00017496028158348054\n",
            "strain 0.00015389533655252308\n",
            "strain 0.0001811541587812826\n",
            "strain 0.00018685123359318823\n",
            "strain 0.0001888096594484523\n",
            "strain 0.00018445264140609652\n",
            "strain 0.00020916065841447562\n",
            "strain 0.0001729458017507568\n",
            "strain 0.0001694396632956341\n",
            "strain 0.000191695595276542\n",
            "strain 0.0001822403137339279\n",
            "strain 0.00018989475211128592\n",
            "strain 0.00015873955271672457\n",
            "strain 0.0001699790736893192\n",
            "strain 0.00017782297800295055\n",
            "strain 0.00018129982345271856\n",
            "strain 0.00018143911438528448\n",
            "strain 0.00016469844558741897\n",
            "strain 0.00018439549603499472\n",
            "strain 0.00018412833742331713\n",
            "strain 0.00016749883070588112\n",
            "strain 0.000190680890227668\n",
            "strain 0.00018090188677888364\n",
            "strain 0.0001821815239964053\n",
            "strain 0.00017268225201405585\n",
            "strain 0.0001817426091292873\n",
            "strain 0.00017283267516177148\n",
            "strain 0.0001866329403128475\n",
            "strain 0.00018054654356092215\n",
            "strain 0.0001891507999971509\n",
            "strain 0.00018189111142419279\n",
            "strain 0.00019687021267600358\n",
            "strain 0.00021822593407705426\n",
            "strain 0.0001835454604588449\n",
            "strain 0.00018926536722574383\n",
            "strain 0.0002233810373581946\n",
            "strain 0.00018009444465860724\n",
            "strain 0.00018645336967892945\n",
            "strain 0.00019541289657354355\n",
            "strain 0.00017188939091283828\n",
            "strain 0.00018589920364320278\n",
            "strain 0.0001777687284629792\n",
            "strain 0.0001934027241077274\n",
            "strain 0.00020304870849940926\n",
            "strain 0.00020268246589694172\n",
            "strain 0.00017492641927674413\n",
            "strain 0.0001875378511613235\n",
            "strain 0.0002090255729854107\n",
            "strain 0.00018195238953921944\n",
            "strain 0.00018615977023728192\n",
            "strain 0.00021592115808743984\n",
            "strain 0.00019490101840347052\n",
            "strain 0.00018359498062636703\n",
            "strain 0.00020545959705486894\n",
            "strain 0.00019132066518068314\n",
            "strain 0.00020015356130898\n",
            "strain 0.0001794662675820291\n",
            "strain 0.00019136675109621137\n",
            "strain 0.0001983233232749626\n",
            "strain 0.0001883348450064659\n",
            "strain 0.00019773181702475995\n",
            "strain 0.00017804742674343288\n",
            "strain 0.00021827344608027488\n",
            "strain 0.00016613371553830802\n",
            "strain 0.00021189884864725173\n",
            "strain 0.00018627347890287638\n",
            "strain 0.00019164333934895694\n",
            "strain 0.00018650473793968558\n",
            "strain 0.0002015649079112336\n",
            "strain 0.00020657482673414052\n",
            "strain 0.0001750737865222618\n",
            "strain 0.00019323383457958698\n",
            "strain 0.00021173976710997522\n",
            "strain 0.00020421597582753748\n",
            "strain 0.00016937943291850388\n",
            "strain 0.00019015224825125188\n",
            "strain 0.00019210037135053426\n",
            "strain 0.0002046560257440433\n",
            "strain 0.00018482735322322696\n",
            "strain 0.00016797221906017512\n",
            "strain 0.00018519209697842598\n",
            "strain 0.00017546174058225006\n",
            "strain 0.00018907393678091466\n",
            "strain 0.00017460387607570738\n",
            "strain 0.00018336798530071974\n",
            "strain 0.00019659822282847017\n",
            "strain 0.00020986743038520217\n",
            "strain 0.00017850489530246705\n",
            "strain 0.00018990473472513258\n",
            "strain 0.00022445169452112168\n",
            "strain 0.00019725036690942943\n",
            "strain 0.0002047046582447365\n",
            "strain 0.0002033630444202572\n",
            "strain 0.0001983690744964406\n",
            "strain 0.00020782585488632321\n",
            "strain 0.00022546712716575712\n",
            "strain 0.00019600924861151725\n",
            "strain 0.00020483083790168166\n",
            "strain 0.0001891397259896621\n",
            "strain 0.00019916676683351398\n",
            "strain 0.00020099188259337097\n",
            "strain 0.00022286621970124543\n",
            "strain 0.00018406282470095903\n",
            "strain 0.00020089656754862517\n",
            "strain 0.00018850788183044642\n",
            "strain 0.00021815898071508855\n",
            "strain 0.00023084036365617067\n",
            "strain 0.00022625386191066355\n",
            "strain 0.00019218451052438468\n",
            "strain 0.0002043693675659597\n",
            "strain 0.00019665306899696589\n",
            "strain 0.00017816918261814862\n",
            "strain 0.00022790646471548826\n",
            "strain 0.00022189109586179256\n",
            "strain 0.00021705093968193978\n",
            "strain 0.00023649644572287798\n",
            "strain 0.0002043236600002274\n",
            "strain 0.00020546124142128974\n",
            "strain 0.00019944320956710726\n",
            "strain 0.00024604963255114853\n",
            "strain 0.00020196393597871065\n",
            "strain 0.00020174156816210598\n",
            "strain 0.00021542550530284643\n",
            "strain 0.00021837391250301152\n",
            "strain 0.00020851123554166406\n",
            "strain 0.00020261343161109835\n",
            "strain 0.00020944909192621708\n",
            "strain 0.00021290361473802477\n",
            "strain 0.00018998043378815055\n",
            "strain 0.00020099924586247653\n",
            "strain 0.00022246789012569934\n",
            "strain 0.00020304039935581386\n",
            "strain 0.00022490778064820915\n",
            "strain 0.00022111754515208304\n",
            "strain 0.00019540786161087453\n",
            "strain 0.00019886215159203857\n",
            "strain 0.00017982342978939414\n",
            "strain 0.00021201762137934566\n",
            "strain 0.00021453280351124704\n",
            "strain 0.00022315279056783766\n",
            "strain 0.0002155109978048131\n",
            "strain 0.00022367248311638832\n",
            "strain 0.00019686517771333456\n",
            "strain 0.0002444216806907207\n",
            "strain 0.0002165683836210519\n",
            "strain 0.000223213093704544\n",
            "strain 0.00021359260426834226\n",
            "strain 0.0001993330952245742\n",
            "strain 0.00019512030121404678\n",
            "strain 0.0002171677042497322\n",
            "strain 0.00021798773377668113\n",
            "strain 0.00022018358868081123\n",
            "strain 0.0001832322741393\n",
            "strain 0.00021335840574465692\n",
            "strain 0.00021487624326255172\n",
            "strain 0.00022613901819568127\n",
            "strain 0.00021792270126752555\n",
            "strain 0.00022027902014087886\n",
            "strain 0.00020867763669230044\n",
            "strain 0.00021731166634708643\n",
            "strain 0.00023226061603054404\n",
            "strain 0.000233371727517806\n",
            "strain 0.00021753330656792969\n",
            "strain 0.0001994976046262309\n",
            "strain 0.00023747910745441914\n",
            "strain 0.0001998383813770488\n",
            "strain 0.00022686267038807273\n",
            "strain 0.00023676238197367638\n",
            "strain 0.0002053378993878141\n",
            "strain 0.0002499597903806716\n",
            "strain 0.0002421807002974674\n",
            "strain 0.00022160506341606379\n",
            "strain 0.00021817305241711438\n",
            "strain 0.0002652557741384953\n",
            "strain 0.00019999842334073037\n",
            "strain 0.00023742279154248536\n",
            "strain 0.00023096178483683616\n",
            "strain 0.00024948251666501164\n",
            "strain 0.0002296047896379605\n",
            "strain 0.00021613016724586487\n",
            "strain 0.0002636656572576612\n",
            "strain 0.00021487317280843854\n",
            "strain 0.00023002902162261307\n",
            "strain 0.0002334934688406065\n",
            "strain 0.0002267698582727462\n",
            "strain 0.00022537301992997527\n",
            "strain 0.00021951821690890938\n",
            "strain 0.0002213863772340119\n",
            "strain 0.0002601620217319578\n",
            "strain 0.00020878424402326345\n",
            "strain 0.00021105444466229528\n",
            "strain 0.00021237133478280157\n",
            "strain 0.00021386047592386603\n",
            "strain 0.00031942545319907367\n",
            "strain 0.00024261260114144534\n",
            "strain 0.0002459985844325274\n",
            "strain 0.00023996531672310084\n",
            "strain 0.00025688030291348696\n",
            "strain 0.0002528750919736922\n",
            "strain 0.0002451154578011483\n",
            "strain 0.00025132973678410053\n",
            "strain 0.00023324618814513087\n",
            "strain 0.00023373040312435478\n",
            "strain 0.0002340987412026152\n",
            "strain 0.0002446372527629137\n",
            "strain 0.00022308554616756737\n",
            "strain 0.00022318316041491926\n",
            "strain 0.00027735426556319\n",
            "strain 0.0002448770101182163\n",
            "strain 0.0002490839397069067\n",
            "strain 0.00021757694776169956\n",
            "strain 0.00023693099501542747\n",
            "strain 0.00024193289573304355\n",
            "strain 0.00026418696506880224\n",
            "strain 0.0002498102548997849\n",
            "strain 0.00022514753800351173\n",
            "strain 0.0002286881790496409\n",
            "strain 0.0002709434775169939\n",
            "strain 0.0002623701002448797\n",
            "strain 0.0003002463490702212\n",
            "strain 0.00026127268210984766\n",
            "classify 2.37200927734375\n",
            "classify 2.45440673828125\n",
            "classify 2.3837890625\n",
            "classify 2.4241943359375\n",
            "classify 2.44183349609375\n",
            "classify 2.45733642578125\n",
            "classify 2.362060546875\n",
            "classify 2.4495849609375\n",
            "classify 2.410888671875\n",
            "classify 2.39239501953125\n",
            "0.1015625\n",
            "0.109375\n",
            "0.125\n",
            "0.109375\n",
            "0.0859375\n",
            "0.109375\n",
            "0.0703125\n",
            "0.09375\n",
            "0.125\n",
            "0.0859375\n",
            "strain 0.0002573110396042466\n",
            "strain 0.00026038108626380563\n",
            "strain 0.00025128666311502457\n",
            "strain 0.00030087443883530796\n",
            "strain 0.0002660699828993529\n",
            "strain 0.00027249270351603627\n",
            "strain 0.00026109127793461084\n",
            "strain 0.00026874535251408815\n",
            "strain 0.0002799610956571996\n",
            "strain 0.00031364994356408715\n",
            "strain 0.00027476230752654374\n",
            "strain 0.000249330245424062\n",
            "strain 0.00022673924104310572\n",
            "strain 0.00026769645046442747\n",
            "strain 0.0002793140592984855\n",
            "strain 0.0002853460900951177\n",
            "strain 0.00025442198966629803\n",
            "strain 0.0002461668918840587\n",
            "strain 0.00026941229589283466\n",
            "strain 0.0002478012756910175\n",
            "strain 0.00026676803827285767\n",
            "strain 0.00031888263765722513\n",
            "strain 0.0002959232369903475\n",
            "strain 0.0002873452613130212\n",
            "strain 0.0002667564549483359\n",
            "strain 0.00028636917704716325\n",
            "strain 0.00027757775387726724\n",
            "strain 0.00026970342150889337\n",
            "strain 0.0002933334617409855\n",
            "strain 0.0002828319265972823\n",
            "strain 0.0002937261597253382\n",
            "strain 0.00026246620109304786\n",
            "strain 0.0003174207522533834\n",
            "strain 0.0002893411729019135\n",
            "strain 0.0002684192731976509\n",
            "strain 0.0003063456970266998\n",
            "strain 0.00030428721220232546\n",
            "strain 0.00029131569317542017\n",
            "strain 0.0002708153915591538\n",
            "strain 0.0002855507773347199\n",
            "strain 0.0002986009931191802\n",
            "strain 0.0002923108986578882\n",
            "strain 0.00032537832157686353\n",
            "strain 0.00030292553128674626\n",
            "strain 0.00035330449463799596\n",
            "strain 0.00029809196712449193\n",
            "strain 0.00028847428620792925\n",
            "strain 0.0002962324651889503\n",
            "strain 0.0002865661808755249\n",
            "strain 0.0002705370425246656\n",
            "strain 0.00032189174089580774\n",
            "strain 0.0003063950571231544\n",
            "strain 0.00029371617711149156\n",
            "strain 0.00031115126330405474\n",
            "strain 0.00031170868896879256\n",
            "strain 0.00030171932303346694\n",
            "strain 0.00031890804530121386\n",
            "strain 0.0002849049342330545\n",
            "strain 0.0002935252559836954\n",
            "strain 0.0003033389803022146\n",
            "strain 0.0002829086733981967\n",
            "strain 0.0003320476971566677\n",
            "strain 0.000309904309688136\n",
            "strain 0.0003109186072833836\n",
            "strain 0.00034324629814364016\n",
            "strain 0.00031935161678120494\n",
            "strain 0.0003673777391668409\n",
            "strain 0.0003362287243362516\n",
            "strain 0.00029420136706903577\n",
            "strain 0.00031938907341100276\n",
            "strain 0.0003529754467308521\n",
            "strain 0.0003184294328093529\n",
            "strain 0.00033915392123162746\n",
            "strain 0.0003432488883845508\n",
            "strain 0.0003249134460929781\n",
            "strain 0.0002814933832269162\n",
            "strain 0.00030258652986958623\n",
            "strain 0.0003008067433256656\n",
            "strain 0.00030210986733436584\n",
            "strain 0.00036558392457664013\n",
            "strain 0.0003566748055163771\n",
            "strain 0.0003828522458206862\n",
            "strain 0.0003308502782601863\n",
            "strain 0.0003491809475235641\n",
            "strain 0.00032418331829831004\n",
            "strain 0.000292306300252676\n",
            "strain 0.00030312867602333426\n",
            "strain 0.0003322238626424223\n",
            "strain 0.00032876781187951565\n",
            "strain 0.00033353071194142103\n",
            "strain 0.00032379303593188524\n",
            "strain 0.00037594279274344444\n",
            "strain 0.0003152858407702297\n",
            "strain 0.0003455811820458621\n",
            "strain 0.0003545909421518445\n",
            "strain 0.0003679778310470283\n",
            "strain 0.000361364014679566\n",
            "strain 0.0003210709837730974\n",
            "strain 0.00037250760942697525\n",
            "strain 0.00034721201518550515\n",
            "strain 0.00046906634815968573\n",
            "strain 0.0003734686761163175\n",
            "strain 0.0003263368853367865\n",
            "strain 0.0003304936981294304\n",
            "strain 0.0003698713262565434\n",
            "strain 0.0003650675935205072\n",
            "strain 0.0003249188303016126\n",
            "strain 0.00035499592195264995\n",
            "strain 0.00033551646629348397\n",
            "strain 0.00041781264008022845\n",
            "strain 0.0004543586110230535\n",
            "strain 0.00038547872100025415\n",
            "strain 0.0003708401054609567\n",
            "strain 0.0003266983258072287\n",
            "strain 0.0003547614032868296\n",
            "strain 0.0003984798677265644\n",
            "strain 0.00041686295298859477\n",
            "strain 0.0003436827100813389\n",
            "strain 0.00036985561018809676\n",
            "strain 0.00039286096580326557\n",
            "strain 0.0003986024239566177\n",
            "strain 0.00038517569191753864\n",
            "strain 0.00044407890527509153\n",
            "strain 0.0004146384308114648\n",
            "strain 0.0004562650865409523\n",
            "strain 0.0004496446345001459\n",
            "strain 0.0004599416861310601\n",
            "strain 0.0004401796031743288\n",
            "strain 0.0003835553943645209\n",
            "strain 0.000394861534005031\n",
            "strain 0.0004976685740984976\n",
            "strain 0.0004284379247110337\n",
            "strain 0.00041355149005539715\n",
            "strain 0.0004841828776989132\n",
            "strain 0.00046849410864524543\n",
            "strain 0.00045167937059886754\n",
            "strain 0.0003859486896544695\n",
            "strain 0.00042259329347871244\n",
            "strain 0.00036451523192226887\n",
            "strain 0.00048753415467217565\n",
            "strain 0.00045179316657595336\n",
            "strain 0.00042443181155249476\n",
            "strain 0.0005035197827965021\n",
            "strain 0.0004718316486105323\n",
            "strain 0.00042584689799696207\n",
            "strain 0.0004537390486802906\n",
            "strain 0.0004965657135471702\n",
            "strain 0.0004362646723166108\n",
            "strain 0.0004575692000798881\n",
            "strain 0.000477275054436177\n",
            "strain 0.0004300479486119002\n",
            "strain 0.0004743914178106934\n",
            "strain 0.0004666712193284184\n",
            "strain 0.00044089125003665686\n",
            "strain 0.00046766517334617674\n",
            "strain 0.00046638684580102563\n",
            "strain 0.0004473322769626975\n",
            "strain 0.00045528000919148326\n",
            "strain 0.00043141786591149867\n",
            "strain 0.0004585283750202507\n",
            "strain 0.00039595802081748843\n",
            "strain 0.0004677751276176423\n",
            "strain 0.0004597296938300133\n",
            "strain 0.00040184546378441155\n",
            "strain 0.0005773069569841027\n",
            "strain 0.00047602015547454357\n",
            "strain 0.00046262601972557604\n",
            "strain 0.00046787451719865203\n",
            "strain 0.0005003323312848806\n",
            "strain 0.0004930043942295015\n",
            "strain 0.0004242363211233169\n",
            "strain 0.00044978869846090674\n",
            "strain 0.00048591155791655183\n",
            "strain 0.0005716420710086823\n",
            "strain 0.0005372209125198424\n",
            "strain 0.00048613184480927885\n",
            "strain 0.000460001960163936\n",
            "strain 0.00043575355084612966\n",
            "strain 0.0005393438041210175\n",
            "strain 0.00047433082363568246\n",
            "strain 0.000451741274446249\n",
            "strain 0.0005943977157585323\n",
            "strain 0.000510189332999289\n",
            "strain 0.0005725930677726865\n",
            "strain 0.0004921905929222703\n",
            "strain 0.0005221659084782004\n",
            "strain 0.0004925487446598709\n",
            "strain 0.000532854232005775\n",
            "strain 0.0005597772542387247\n",
            "strain 0.0005815763724967837\n",
            "strain 0.0004911033902317286\n",
            "strain 0.0004714174719993025\n",
            "strain 0.0005830974550917745\n",
            "strain 0.0005941827548667789\n",
            "strain 0.0005400103982537985\n",
            "strain 0.0005076713860034943\n",
            "strain 0.0005611528758890927\n",
            "strain 0.0005332341534085572\n",
            "strain 0.0005526804598048329\n",
            "strain 0.0005393424071371555\n",
            "strain 0.0005348570412024856\n",
            "strain 0.0005486965528689325\n",
            "strain 0.0006107059889473021\n",
            "strain 0.0006204490782693028\n",
            "strain 0.0005190969677641988\n",
            "strain 0.000547201547306031\n",
            "strain 0.0005707170348614454\n",
            "strain 0.0005451839533634484\n",
            "strain 0.0006492237444035709\n",
            "strain 0.0005128983175382018\n",
            "strain 0.0005460001993924379\n",
            "strain 0.0006363461725413799\n",
            "strain 0.0005476696533150971\n",
            "strain 0.0006193584995344281\n",
            "strain 0.0005855186027474701\n",
            "strain 0.0006030925433151424\n",
            "strain 0.0005620602751150727\n",
            "strain 0.000636321899946779\n",
            "strain 0.0005544135347008705\n",
            "strain 0.0005803476669825613\n",
            "strain 0.0006192249129526317\n",
            "strain 0.0006642509833909571\n",
            "strain 0.0006121300393715501\n",
            "strain 0.00068556924816221\n",
            "strain 0.0005621037562377751\n",
            "strain 0.0006983753992244601\n",
            "strain 0.0005134378443472087\n",
            "strain 0.0007096515037119389\n",
            "strain 0.0006118420860730112\n",
            "strain 0.0007486928370781243\n",
            "strain 0.0006028260104358196\n",
            "strain 0.0006308317533694208\n",
            "strain 0.0005395562620833516\n",
            "strain 0.0006656332989223301\n",
            "strain 0.0005344843957573175\n",
            "strain 0.0006940463208593428\n",
            "strain 0.0006306409486569464\n",
            "strain 0.00060564826708287\n",
            "strain 0.0006283017573878169\n",
            "strain 0.0006879682769067585\n",
            "strain 0.0006573402788490057\n",
            "strain 0.0006488118087872863\n",
            "strain 0.0007439093315042555\n",
            "strain 0.0006290546152740717\n",
            "strain 0.0007099407375790179\n",
            "strain 0.0006453006062656641\n",
            "strain 0.0006083708722144365\n",
            "strain 0.0006935920682735741\n",
            "strain 0.0006118942983448505\n",
            "strain 0.0006665343535132706\n",
            "strain 0.0006573593127541244\n",
            "strain 0.0008550042985007167\n",
            "strain 0.0008110876078717411\n",
            "strain 0.0007188291638158262\n",
            "strain 0.0007484809029847383\n",
            "strain 0.0007291787769645452\n",
            "strain 0.0007182115223258734\n",
            "strain 0.0007341416785493493\n",
            "strain 0.0006941888714209199\n",
            "strain 0.0008416515775024891\n",
            "strain 0.0008682857151143253\n",
            "strain 0.0007779742009006441\n",
            "strain 0.0008993374067358673\n",
            "strain 0.0007268404588103294\n",
            "strain 0.0008338906918652356\n",
            "strain 0.0006863270536996424\n",
            "strain 0.0007339401636272669\n",
            "strain 0.0007982135866768658\n",
            "strain 0.0006819225382059813\n",
            "strain 0.0008612234960310161\n",
            "strain 0.0008308604592457414\n",
            "strain 0.0007175539503805339\n",
            "strain 0.0007920206990092993\n",
            "strain 0.0008271929691545665\n",
            "strain 0.0007502820226363838\n",
            "strain 0.0010806729551404715\n",
            "strain 0.0007713957456871867\n",
            "strain 0.0009154602885246277\n",
            "strain 0.0007917768671177328\n",
            "strain 0.0008870337042026222\n",
            "strain 0.0008825485128909349\n",
            "strain 0.0007575589115731418\n",
            "strain 0.0008554545347578824\n",
            "strain 0.0007602825644426048\n",
            "strain 0.0010133249452337623\n",
            "strain 0.0007621360127814114\n",
            "strain 0.0008451026515103877\n",
            "strain 0.0008057389059104025\n",
            "strain 0.0008978211553767323\n",
            "strain 0.000809038057923317\n",
            "strain 0.0009495047270320356\n",
            "strain 0.0009442980517633259\n",
            "strain 0.0008730366826057434\n",
            "strain 0.0009109660750254989\n",
            "strain 0.0008074582437984645\n",
            "strain 0.0008618001593276858\n",
            "strain 0.0007633219938725233\n",
            "strain 0.0008205109625123441\n",
            "strain 0.0007886025123298168\n",
            "strain 0.000756947323679924\n",
            "strain 0.0008513604407198727\n",
            "strain 0.0010312885278835893\n",
            "strain 0.0009719926747493446\n",
            "strain 0.000976563198491931\n",
            "strain 0.0010406037326902151\n",
            "strain 0.0009052337263710797\n",
            "strain 0.001088260323740542\n",
            "strain 0.0009488850482739508\n",
            "strain 0.0008624077308923006\n",
            "strain 0.0011228048242628574\n",
            "strain 0.0008448236039839685\n",
            "strain 0.0010854635620489717\n",
            "strain 0.0008221007883548737\n",
            "strain 0.0009491403470747173\n",
            "strain 0.0008406908018514514\n",
            "strain 0.001096366555429995\n",
            "strain 0.000979426666162908\n",
            "strain 0.0009576723095960915\n",
            "strain 0.0007188498275354505\n",
            "strain 0.0009326597210019827\n",
            "strain 0.001106776180677116\n",
            "strain 0.0010041923960670829\n",
            "strain 0.0008939711260609329\n",
            "strain 0.0010166667634621263\n",
            "strain 0.00105130800511688\n",
            "strain 0.0009881837759166956\n",
            "strain 0.0011830502189695835\n",
            "strain 0.0009192873258143663\n",
            "strain 0.0009607115061953664\n",
            "strain 0.0011149081401526928\n",
            "strain 0.0010489261476323009\n",
            "strain 0.0010500240605324507\n",
            "strain 0.0008554260130040348\n",
            "strain 0.0009717812645249069\n",
            "strain 0.000998831121250987\n",
            "strain 0.0012368662282824516\n",
            "strain 0.000922694627661258\n",
            "strain 0.0011021856917068362\n",
            "strain 0.0011539289262145758\n",
            "strain 0.0009413574589416385\n",
            "strain 0.0015064930776134133\n",
            "strain 0.0011764317750930786\n",
            "strain 0.0012943242909386754\n",
            "strain 0.0010176023934036493\n",
            "strain 0.0013966286787763238\n",
            "strain 0.0012514623813331127\n",
            "strain 0.0011711654951795936\n",
            "strain 0.0010213075438514352\n",
            "strain 0.0011660881573334336\n",
            "strain 0.0013893453869968653\n",
            "strain 0.0009681396186351776\n",
            "strain 0.0018792729824781418\n",
            "strain 0.0011198943248018622\n",
            "strain 0.0017859674990177155\n",
            "strain 0.0010782851604744792\n",
            "strain 0.0015663016820326447\n",
            "strain 0.0011086473241448402\n",
            "strain 0.0010696913814172149\n",
            "strain 0.0011849366128444672\n",
            "strain 0.0011558864498510957\n",
            "strain 0.001351701095700264\n",
            "strain 0.0010535966139286757\n",
            "strain 0.0012365906732156873\n",
            "strain 0.0012105355272069573\n",
            "strain 0.001164296525530517\n",
            "strain 0.0009922252502292395\n",
            "strain 0.001098626060411334\n",
            "strain 0.0012675280449911952\n",
            "strain 0.0013582597021013498\n",
            "strain 0.0012378034880384803\n",
            "strain 0.001120440079830587\n",
            "strain 0.0012045917101204395\n",
            "strain 0.0011119339615106583\n",
            "strain 0.001270262524485588\n",
            "strain 0.001028447295539081\n",
            "strain 0.0012749810703098774\n",
            "strain 0.001425627851858735\n",
            "strain 0.0013788372743874788\n",
            "strain 0.0009969641687348485\n",
            "strain 0.001373790088109672\n",
            "strain 0.0014398416969925165\n",
            "classify 2.38702392578125\n",
            "classify 2.440185546875\n",
            "classify 2.40185546875\n",
            "classify 2.3896484375\n",
            "classify 2.38397216796875\n",
            "classify 2.39422607421875\n",
            "classify 2.3594970703125\n",
            "classify 2.4644775390625\n",
            "classify 2.40301513671875\n",
            "classify 2.4366455078125\n",
            "0.0859375\n",
            "0.0703125\n",
            "0.1015625\n",
            "0.0703125\n",
            "0.109375\n",
            "0.1484375\n",
            "0.109375\n",
            "0.109375\n",
            "0.1171875\n",
            "0.0859375\n",
            "strain 0.0011826858390122652\n",
            "strain 0.0010868827812373638\n",
            "strain 0.001261096796952188\n",
            "strain 0.0013721485156565905\n",
            "strain 0.001134511549025774\n",
            "strain 0.001172768883407116\n",
            "strain 0.001616085646674037\n",
            "strain 0.0013145434204488993\n",
            "strain 0.0014791414141654968\n",
            "strain 0.0015606088563799858\n",
            "strain 0.0012736546341329813\n",
            "strain 0.0013082044897601008\n",
            "strain 0.0012343146372586489\n",
            "strain 0.00115520681720227\n",
            "strain 0.0013791410019621253\n",
            "strain 0.0015445549506694078\n",
            "strain 0.001264167600311339\n",
            "strain 0.0015757622895762324\n",
            "strain 0.0011712976265698671\n",
            "strain 0.0013875566655769944\n",
            "strain 0.001194117241539061\n",
            "strain 0.0013933532172814012\n",
            "strain 0.001416597398929298\n",
            "strain 0.0013429627288132906\n",
            "strain 0.001267888117581606\n",
            "strain 0.0011651738313958049\n",
            "strain 0.0017474479973316193\n",
            "strain 0.001397261512465775\n",
            "strain 0.001649928162805736\n",
            "strain 0.0011935296934098005\n",
            "strain 0.0017573590157553554\n",
            "strain 0.0014095938531681895\n",
            "strain 0.0014713216805830598\n",
            "strain 0.0012626658426597714\n",
            "strain 0.001389021286740899\n",
            "strain 0.0017248698277398944\n",
            "strain 0.0016546370461583138\n",
            "strain 0.0015612001297995448\n",
            "strain 0.0014764288207516074\n",
            "strain 0.0017029340378940105\n",
            "strain 0.0017201260197907686\n",
            "strain 0.0013742940500378609\n",
            "strain 0.0017873042961582541\n",
            "strain 0.00137985625769943\n",
            "strain 0.0017226464115083218\n",
            "strain 0.0016544946702197194\n",
            "strain 0.0016812735702842474\n",
            "strain 0.0016065578674897552\n",
            "strain 0.0017210389487445354\n",
            "strain 0.001506048603914678\n",
            "strain 0.001578384661115706\n",
            "strain 0.0013322274899110198\n",
            "strain 0.0014068951131775975\n",
            "strain 0.001351332408376038\n",
            "strain 0.0014447951689362526\n",
            "strain 0.0012335991486907005\n",
            "strain 0.0017467753496021032\n",
            "strain 0.0014933990314602852\n",
            "strain 0.001332803163677454\n",
            "strain 0.0013071438297629356\n",
            "strain 0.001271083950996399\n",
            "strain 0.001470242626965046\n",
            "strain 0.0012826565653085709\n",
            "strain 0.0013401572359725833\n",
            "strain 0.001304120640270412\n",
            "strain 0.0015894313110038638\n",
            "strain 0.0012874711537733674\n",
            "strain 0.0013307431945577264\n",
            "strain 0.0013150495942682028\n",
            "strain 0.0014438214711844921\n",
            "strain 0.00171533296816051\n",
            "strain 0.0015035455580800772\n",
            "strain 0.0018093830440193415\n",
            "strain 0.0012956652790307999\n",
            "strain 0.001474187825806439\n",
            "strain 0.0016094375168904662\n",
            "strain 0.0018135950667783618\n",
            "strain 0.001517943455837667\n",
            "strain 0.0015145213110372424\n",
            "strain 0.0017641359008848667\n",
            "strain 0.001693423488177359\n",
            "strain 0.0018232876900583506\n",
            "strain 0.0015045043546706438\n",
            "strain 0.0015009334310889244\n",
            "strain 0.0013594981282949448\n",
            "strain 0.00150684907566756\n",
            "strain 0.001417092396877706\n",
            "strain 0.0016755112446844578\n",
            "strain 0.0014635866973549128\n",
            "strain 0.0013358587166294456\n",
            "strain 0.0015667318366467953\n",
            "strain 0.0015392581699416041\n",
            "strain 0.0018489460926502943\n",
            "strain 0.0017766227247193456\n",
            "strain 0.001611331943422556\n",
            "strain 0.0014258058508858085\n",
            "strain 0.001169360475614667\n",
            "strain 0.0013389895902946591\n",
            "strain 0.0015945722116157413\n",
            "strain 0.0015265935799106956\n",
            "strain 0.0016216050134971738\n",
            "strain 0.0015682074008509517\n",
            "strain 0.001540370867587626\n",
            "strain 0.0018958611181005836\n",
            "strain 0.0016472863499075174\n",
            "strain 0.001869370462372899\n",
            "strain 0.001755598234012723\n",
            "strain 0.0017969748005270958\n",
            "strain 0.002053755335509777\n",
            "strain 0.0013094603782519698\n",
            "strain 0.00256781536154449\n",
            "strain 0.001569736166857183\n",
            "strain 0.001978783868253231\n",
            "strain 0.0016210729954764247\n",
            "strain 0.0017923542764037848\n",
            "strain 0.001673209248110652\n",
            "strain 0.0015015029348433018\n",
            "strain 0.0020203448366373777\n",
            "strain 0.0015166481025516987\n",
            "strain 0.0015101477038115263\n",
            "strain 0.002122329082340002\n",
            "strain 0.001559693831950426\n",
            "strain 0.0014197009149938822\n",
            "strain 0.001438323874026537\n",
            "strain 0.0017089156899601221\n",
            "strain 0.0018035703105852008\n",
            "strain 0.0020461087115108967\n",
            "strain 0.0014212122187018394\n",
            "strain 0.0013777994317933917\n",
            "strain 0.002080450300127268\n",
            "strain 0.0012376437662169337\n",
            "strain 0.001970545621588826\n",
            "strain 0.0016289468621835113\n",
            "strain 0.0019274847581982613\n",
            "strain 0.0018319800728932023\n",
            "strain 0.0017395460745319724\n",
            "strain 0.0014721517218276858\n",
            "strain 0.0014008539728820324\n",
            "strain 0.0013835944700986147\n",
            "strain 0.001674720086157322\n",
            "strain 0.001355151180177927\n",
            "strain 0.001838770927861333\n",
            "strain 0.0015373268397524953\n",
            "strain 0.0017618011916056275\n",
            "strain 0.0017424755496904254\n",
            "strain 0.0022344253957271576\n",
            "strain 0.0017968113534152508\n",
            "strain 0.0018620789051055908\n",
            "strain 0.0019522991497069597\n",
            "strain 0.0016159481601789594\n",
            "strain 0.0020951328333467245\n",
            "strain 0.0013612014008685946\n",
            "strain 0.0017417713534086943\n",
            "strain 0.00167517748195678\n",
            "strain 0.0012697263155132532\n",
            "strain 0.0012899644207209349\n",
            "strain 0.0019465431105345488\n",
            "strain 0.0012659706408157945\n",
            "strain 0.0013254249934107065\n",
            "strain 0.0019051760900765657\n",
            "strain 0.0015034906100481749\n",
            "strain 0.0013721069553866982\n",
            "strain 0.0012786074075847864\n",
            "strain 0.00212394492700696\n",
            "strain 0.0017465264536440372\n",
            "strain 0.0013329184148460627\n",
            "strain 0.0016319305868819356\n",
            "strain 0.0016053806757554412\n",
            "strain 0.001284779398702085\n",
            "strain 0.00149655737914145\n",
            "strain 0.001404004404321313\n",
            "strain 0.0018410461489111185\n",
            "strain 0.0016707443865016103\n",
            "strain 0.0021819728426635265\n",
            "strain 0.00172812445089221\n",
            "strain 0.002232522005215287\n",
            "strain 0.0022057448513805866\n",
            "strain 0.00217439909465611\n",
            "strain 0.00225639040581882\n",
            "strain 0.0016683214344084263\n",
            "strain 0.0023326619993895292\n",
            "strain 0.001877731760032475\n",
            "strain 0.0022793880198150873\n",
            "strain 0.0019931334536522627\n",
            "strain 0.001827172702178359\n",
            "strain 0.0018255121540278196\n",
            "strain 0.0016085132956504822\n",
            "strain 0.002302797045558691\n",
            "strain 0.0026210930664092302\n",
            "strain 0.001447970513254404\n",
            "strain 0.0022162378299981356\n",
            "strain 0.00176223274320364\n",
            "strain 0.002138324547559023\n",
            "strain 0.0016942723887041211\n",
            "strain 0.0016502673970535398\n",
            "strain 0.0018053922103717923\n",
            "strain 0.0019436735892668366\n",
            "strain 0.002031616633757949\n",
            "strain 0.0014650830999016762\n",
            "strain 0.0016036214074119925\n",
            "strain 0.002164664911106229\n",
            "strain 0.001788766821846366\n",
            "strain 0.0018386573065072298\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-3f12361a6634>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;31m# test(seq_jepa, test_loader)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0mtrain_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_jepa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0mtrain_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_jepa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0mtest_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_jepa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-3f12361a6634>\u001b[0m in \u001b[0;36mstrain\u001b[0;34m(model, dataloader, optim, scheduler)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# loss.backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m         ), \"No inf checks were recorded for this optimizer.\"\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    350\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    349\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    350\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# @title strain ctrain test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "def strain(model, dataloader, optim, scheduler=None): # train function with automatic mixed precision\n",
        "# def strain(model, train_iter, optim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.train()\n",
        "    for i, (img, _) in enumerate(dataloader):\n",
        "    # for i in range(50):\n",
        "    #     try: img, _ = next(train_iter)\n",
        "    #     except StopIteration:\n",
        "    #         train_iter = iter(train_loader)\n",
        "    #         img, _ = next(train_iter)\n",
        "\n",
        "        img = img.to(device) # [batch, ]\n",
        "        img = img.flatten(2).transpose(-2,-1)#.to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            loss = model.loss(img)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        # print(loss)\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # total_norm = 0\n",
        "        # for p in model.parameters(): total_norm += p.grad.data.norm(2).item() ** 2\n",
        "        # total_norm = total_norm**.5\n",
        "        # print('total_norm', total_norm)\n",
        "        # for p in list(filter(lambda p: p.grad is not None, model.parameters())):\n",
        "        #     print(p.grad.data.norm(2).item())\n",
        "        scaler.unscale_(optim)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5) # 0.5\n",
        "\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "        # loss.backward()\n",
        "        # optim.step()\n",
        "        # model.target_encoder.update_parameters(model.context_encoder)\n",
        "        # # update_bn(dataloader, model.target_encoder)\n",
        "        # update_bn(train_iter, model.target_encoder)\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            m=0.99 # m = next(momentum_scheduler)\n",
        "            for param_q, param_k in zip(model.context_encoder.parameters(), model.target_encoder.parameters()):\n",
        "                param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        print(\"strain\",loss.item())\n",
        "        # for param in seq_jepa.context_encoder.cls: print(param.data)\n",
        "        # for param in seq_jepa.predicter.cls: print(param.data)\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        # if i >= 50: break\n",
        "    return train_iter\n",
        "\n",
        "\n",
        "# def ctrain(model, classifier, dataloader, coptim, scheduler=None): # train function with automatic mixed precision\n",
        "def ctrain(model, classifier, train_iter, coptim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.eval()\n",
        "    classifier.train()\n",
        "    # for i, (img, y) in enumerate(dataloader):\n",
        "    for i in range(10):\n",
        "        try: img, y = next(train_iter)\n",
        "        except StopIteration:\n",
        "            train_iter = iter(train_loader)\n",
        "            img, y = next(train_iter)\n",
        "        # print(\"ctrain\",y)\n",
        "        img, y = img.to(device), y.to(device) # [batch, ]\n",
        "        # img = img.flatten(start_dim=1)\n",
        "        img = img.flatten(2).transpose(-2,-1).to(torch.bfloat16)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            with torch.no_grad():\n",
        "                sx = model(img).detach()\n",
        "            # print(sx[0][0])\n",
        "            y_ = classifier(sx)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "        # print(classifier.classifier.weight[0])\n",
        "        # print(y_.shape, y.shape)\n",
        "        # print(loss)\n",
        "        coptim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 5) # .5\n",
        "        scaler.step(coptim)\n",
        "        scaler.update()\n",
        "        print(\"classify\",loss.item())\n",
        "        try: wandb.log({\"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        # if i >= 10: break\n",
        "    return train_iter\n",
        "\n",
        "\n",
        "# def test(model, dataloader):\n",
        "def test(model, classifier, test_iter):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    # for i, (img, y) in enumerate(dataloader):\n",
        "    for i in range(10):\n",
        "        try: img, y = next(test_iter)\n",
        "        except StopIteration:\n",
        "            test_iter = iter(test_loader)\n",
        "            img, y = next(test_iter)\n",
        "        # print(\"test\",y)\n",
        "\n",
        "        img, y = img.to(device), y.to(device) # [batch, ]\n",
        "        # img = img.flatten(start_dim=1).to(torch.bfloat16)\n",
        "        img = img.flatten(2).transpose(-2,-1)#.to(torch.bfloat16)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            sx = model(img)\n",
        "            y_ = classifier(sx)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        print(correct/len(y))\n",
        "        try: wandb.log({\"correct\": correct/len(y)})\n",
        "        except NameError: pass\n",
        "        # if i >= 10: break\n",
        "    return test_iter\n",
        "\n",
        "train_iter = iter(train_loader)\n",
        "test_iter = iter(test_loader)\n",
        "for i in range(100):\n",
        "    # strain(seq_jepa, train_loader, optim)\n",
        "    # ctrain(seq_jepa, classifier, train_loader, coptim)\n",
        "    # test(seq_jepa, test_loader)\n",
        "\n",
        "    train_iter = strain(seq_jepa, train_iter, optim)\n",
        "    train_iter = ctrain(seq_jepa, classifier, train_iter, coptim)\n",
        "    test_iter = test(seq_jepa, classifier, test_iter)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for param in seq_jepa.context_encoder.cls: print(param.data)\n",
        "print(seq_jepa.context_encoder.cls.is_leaf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YfG77_KYLAk",
        "outputId": "7de8faeb-5a28-4759-b16b-d0dd84a254f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# seq=40\n",
        "# src = seq_jepa.context_encoder.pos_encoder(torch.arange(seq, device=device))\n",
        "# for x in src:\n",
        "#     print(x)\n",
        "print(.999**50)\n"
      ],
      "metadata": {
        "id": "WgN5LlxWsPzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title save/load\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# folder='/content/drive/MyDrive/jepa/'\n",
        "\n",
        "# modelsd, optimsd = torch.load(folder+'SeqJEPA.pkl', map_location=device).values()\n",
        "modelsd, optimsd = torch.load('SeqJEPA.pkl', map_location=device).values()\n",
        "seq_jepa.load_state_dict(modelsd, strict=False)\n",
        "optim.load_state_dict(optimsd)"
      ],
      "metadata": {
        "id": "JT8AgOx0E_KM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccf3d860-88ab-49de-8fd6-b05cd9032475"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-45af7927d988>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  modelsd, optimsd = torch.load('SeqJEPA.pkl', map_location=device).values()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = {'model': seq_jepa.state_dict(), 'optimizer': optim.state_dict()}\n",
        "# # torch.save(checkpoint, folder+'SeqJEPA.pkl')\n",
        "torch.save(checkpoint, 'SeqJEPA.pkl')"
      ],
      "metadata": {
        "id": "CpbLPvjiE-pD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2Nd-sGe6Ku4S",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "outputId": "ee959618-a34e-46a6-e488-d0376c5ea8b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>▄▃▅▃▄▆▅▂▅██▂▅▂▄▅▇▅▃▆▆▄▆▅▅▅▆▅▁▃▆▅▄▅▅▃▆▆▄▃</td></tr><tr><td>correct</td><td>▅▁▄▄▂▆▄▆▅▁▄▂▆▆▄▅▂▃▅▁▅▃▁▂▄▆▅▃▅▂▆▃▃▂▄▅█▅▅▃</td></tr><tr><td>loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>2.81348</td></tr><tr><td>correct</td><td>0.08594</td></tr><tr><td>loss</td><td>0.0001</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">snowy-elevator-24</strong> at: <a href='https://wandb.ai/bobdole/SeqJEPA/runs/9b13s2wo' target=\"_blank\">https://wandb.ai/bobdole/SeqJEPA/runs/9b13s2wo</a><br> View project at: <a href='https://wandb.ai/bobdole/SeqJEPA' target=\"_blank\">https://wandb.ai/bobdole/SeqJEPA</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250311_013343-9b13s2wo/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250311_014429-inu45szp</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/SeqJEPA/runs/inu45szp' target=\"_blank\">dandy-sun-25</a></strong> to <a href='https://wandb.ai/bobdole/SeqJEPA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/SeqJEPA' target=\"_blank\">https://wandb.ai/bobdole/SeqJEPA</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/SeqJEPA/runs/inu45szp' target=\"_blank\">https://wandb.ai/bobdole/SeqJEPA/runs/inu45szp</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "!pip install -q wandb\n",
        "import wandb # https://docs.wandb.ai/quickstart\n",
        "wandb.login(key='487a2109e55dce4e13fc70681781de9f50f27be7')\n",
        "try: run.finish()\n",
        "except NameError: pass\n",
        "run = wandb.init(project=\"SeqJEPA\", config={\"model\": \"res18\",})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(4,100,3)\n",
        "\n",
        "x=x.transpose(-2,-1) # [batch, dim, seq]\n",
        " # in, out, kernel, stride, pad\n",
        "# net = nn.Conv1d(3,16,7,2,7//2)\n",
        "net = nn.Sequential(nn.Conv1d(3,16,7,2,7//2), nn.MaxPool1d(3, 2, 3//2))\n",
        "# net = nn.Conv1d(3,16,(7,3),2,3//2)\n",
        "out = net(x)\n",
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_YBe6-eZ2Zq",
        "outputId": "09b27f75-ba04-4aaf-f5e4-1b9c1dc8d112"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 16, 25])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test repeat_interleave\n",
        "# x = torch.rand(3,5)\n",
        "x = torch.rand(3,5,7)\n",
        "print(x)\n",
        "# out = x.repeat(2,1) # [2*3,5]\n",
        "# print(out)\n",
        "# x_ = out.reshape(2,3,5)\n",
        "# print(x_)\n",
        "# out = x.repeat_interleave(2,1,1) # [3*2,5]\n",
        "out = x.repeat_interleave(2,dim=0) # [3*2,5]\n",
        "# print(out)\n",
        "# x_ = out.reshape(2,3,5,7)\n",
        "x_ = out.reshape(3,2,5,7)\n",
        "print(x_)\n",
        "\n",
        "\n",
        "# batch=1\n",
        "# seq_len=5\n",
        "# dim=4\n",
        "# positional_emb = torch.rand(batch, seq_len, dim)\n",
        "# print(positional_emb)\n",
        "# num_tok=2\n",
        "# trg_indices=torch.randint(0,seq_len,(M, num_tok))\n",
        "# print(trg_indices)\n",
        "# # pos_embs = positional_emb.gather(1, trg_indices.unsqueeze(0))\n",
        "# pos_embs = positional_emb[torch.arange(batch)[...,None,None],trg_indices.unsqueeze(0)]\n",
        "# # pos_embs = positional_emb[0,trg_indices]\n",
        "# # [batch, M, num_tok, dim]\n",
        "# print(pos_embs.shape)\n",
        "# print(pos_embs)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "R6kiiqw3uIdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test ropes\n",
        "# for i, (img, _) in enumerate(test_loader):\n",
        "#     break\n",
        "# print(img.shape) # [batch, 1, 28, 28]\n",
        "# print(img[0]) # [0,1)\n",
        "d_model=8\n",
        "# rot_emb = RotEmb(d_model, top=torch.pi, base=10)\n",
        "# x = torch.linspace(0,1,20)\n",
        "# out = rot_emb(x)\n",
        "# print(out.shape)\n",
        "# print(out)\n",
        "\n",
        "# # pos_encoder = RoPE(d_model, seq_len=15, base=100)\n",
        "# pos_encoder = RoPE(d_model, seq_len=15, base=10000)\n",
        "x = torch.ones(1, 10, d_model)\n",
        "# # x = torch.ones(1, 10, d_model)\n",
        "# out = pos_encoder(x)\n",
        "# # print(out.shape)\n",
        "# for x in out[0]:\n",
        "#     print(x)\n",
        "\n",
        "\n",
        "# pos_encoder = LearnedRoPE(d_model, seq_len=512)\n",
        "# out = pos_encoder(x)\n",
        "# for o in out[0]:\n",
        "#     print(o)\n",
        "\n",
        "# pos_encoder.weights = nn.Parameter(torch.randn(1, d_model//2))\n",
        "# out = pos_encoder(x)\n",
        "# for o in out[0]:\n",
        "#     print(o)\n",
        "\n",
        "\n",
        "# tensor([-0.0177, -0.9998,  0.8080, -0.5892, -0.7502, -0.6612,  0.3885,  0.9214])\n"
      ],
      "metadata": {
        "id": "8r_GjI_vCMyo",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title SeqJEPA\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def multiblock(batch, seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "        mask_len = torch.rand(batch, M) * (max_s - min_s) + min_s # in (min_s, max_s)\n",
        "        mask_pos = torch.rand(batch, M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "        mask_len, mask_pos = mask_len * seq, mask_pos * seq\n",
        "        indices = torch.arange(seq)[None,None,...]\n",
        "        target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [batch, M, seq]\n",
        "        target_mask = target_mask.any(1) # True -> masked # multiblock masking # [batch, seq]\n",
        "        return target_mask\n",
        "\n",
        "\n",
        "\n",
        "class SeqJEPA(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.in_dim = in_dim\n",
        "        # if out_dim is None: self.out_dim = in_dim\n",
        "        if out_dim is None: out_dim = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.d_model = d_model\n",
        "        act = nn.GELU() # ReLU GELU SiLU\n",
        "\n",
        "        dim=8\n",
        "\n",
        "\n",
        "        self.calorie_emb = RotEmb(dim, top=1, base=10) # 0-4\n",
        "        self.steps_emb = RotEmb(dim, top=1, base=10) # 0-5\n",
        "        self.enc0 = nn.Sequential(\n",
        "            nn.Linear(dim*2, d_model), act,\n",
        "            nn.Linear(d_model, d_model), act,\n",
        "        )\n",
        "        self.hour_emb = CircularEmb(dim, torch.pi/2) # circular (0,1) # 0-24\n",
        "        self.temp_emb = RotEmb(dim, top=1/3, base=10) # 18-35\n",
        "        self.heart_emb = RotEmb(dim, top=1/3, base=100) # 50-200\n",
        "        self.enc1 = nn.Sequential(\n",
        "            nn.Linear(dim*3, d_model), act,\n",
        "        )\n",
        "\n",
        "        # self.transformer = TransformerClassifier(d_model, out_dim=out_dim, nhead=8, nlayers=2)\n",
        "        # # self.fc = nn.Linear(d_model, out_dim)\n",
        "        self.context_encoder = TransformerModel(d_model, out_dim=out_dim, nhead=8, nlayers=2, dropout=0.)\n",
        "        self.predicter = TransformerModel(out_dim, nhead=8, nlayers=1, dropout=0.)\n",
        "        self.target_encoder = AveragedModel(self.context_encoder, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    def loss(self, hr, temp, heart): # [batch, 1+T]\n",
        "        # batch = hr.size(0)\n",
        "        res_id, activeKilocalories, steps = hr[:,0], temp[:,0], heart[:,0]\n",
        "        # enc_summary = self.enc0(torch.cat([self.calorie_emb(activeKilocalories), self.steps_emb(steps)], dim=-1)) # [batch, d_model]\n",
        "        enc_summary = self.enc0(torch.cat([self.calorie_emb(torch.log(activeKilocalories.clamp(min=1))), self.steps_emb(torch.log(steps.clamp(min=1)))], dim=-1)) # [batch, d_model]\n",
        "        x = self.enc1(torch.cat([self.hour_emb(hr[:,1:]/24), self.temp_emb(temp[:,1:]), self.heart_emb(heart[:,1:])], dim=-1)) # [batch, T, d_model]\n",
        "        # print('violet fwd', hr.shape, enc_summary.shape, x.shape)\n",
        "\n",
        "        # # mask=(torch.rand(self.seq_len)<.1) # True -> masked # random masking\n",
        "        # mask=(torch.rand_like(x)<.1) # True -> masked # random masking\n",
        "\n",
        "        # patches pad -enc> patch_enc\n",
        "        # rearrange patch in padded/masked, -pred> pred of masked\n",
        "        # mse tgt_enc(img)\n",
        "\n",
        "        # average L2 distance between the predicted patch-level representations sˆy(i) and the target patch-level representation sy(i);\n",
        "        # F.smooth_l1_loss\n",
        "\n",
        "        batch, seq, dim = x.shape\n",
        "\n",
        "        # mask=(torch.rand(batch, seq)<.75) # True -> masked # random masking\n",
        "        # sorted_mask, ids = mask.sort(dim=1)\n",
        "        # # sorted_x = x[torch.arange(batch)[...,None,None],ids]\n",
        "        # sorted_x = x[torch.arange(batch).unsqueeze(-1),ids]\n",
        "        # sorted_mask = torch.cat([torch.zeros((batch, 1), dtype=torch.bool), sorted_mask], dim=1) # True->mask\n",
        "        # sorted_x = torch.cat([enc_summary.unsqueeze(1), sorted_x], dim=1)\n",
        "        # sorted_sx = self.context_encoder(sorted_x, src_key_padding_mask=sorted_mask)\n",
        "        # # torch.zeros_like(sorted_sx)\n",
        "        # sx = sorted_sx[torch.arange(batch).unsqueeze(-1),ids.argsort(1)]\n",
        "        # sy_ = self.predicter(sx, src_key_padding_mask=mask)\n",
        "        # sy = self.target_encoder(x)*~mask\n",
        "\n",
        "\n",
        "        # target_mask = multiblock(batch, seq, min_s=0.15, max_s=0.2, M=4)\n",
        "        # context_mask = ~multiblock(batch, seq, min_s=0.85, max_s=1., M=1)|target_mask\n",
        "        # sx = self.context_encoder(x, src_key_padding_mask=context_mask)\n",
        "        # sy_ = self.predicter(sx, cls_mask=target_mask)*target_mask.unsqueeze(-1)\n",
        "        # sy_ = self.predicter(sx, cls_mask=target_mask)\n",
        "        # # print(sy_.shape, target_mask.shape)\n",
        "        # sy = self.target_encoder(x)*target_mask.unsqueeze(-1)\n",
        "\n",
        "\n",
        "        M=4\n",
        "        target_mask = multiblock(M*batch, seq, min_s=0.15, max_s=0.2, M=1) # mask out targets to be predicted # [4*batch, seq]\n",
        "        context_mask = ~multiblock(batch, seq, min_s=0.85, max_s=1., M=1)|target_mask.reshape(batch,M,seq).any(1) # [batch, seq]\n",
        "\n",
        "\n",
        "        x = torch.cat([enc_summary.unsqueeze(1), x], dim=1) # [batch, 1+T, d_model]\n",
        "        target_mask = torch.cat([torch.zeros((M*batch, 1), dtype=bool), target_mask],dim=1) # [4*batch, 1+T]\n",
        "        context_mask = torch.cat([torch.zeros((batch, 1), dtype=bool), context_mask],dim=1) # [batch, 1+T]\n",
        "\n",
        "        # x = x.repeat(4,1,1)\n",
        "        # context_mask = context_mask.repeat(4,1)\n",
        "        sx = self.context_encoder(x, src_key_padding_mask=context_mask).repeat(M,1,1)\n",
        "        sy_ = self.predicter(sx, cls_mask=target_mask)*target_mask.unsqueeze(-1)\n",
        "        # sy_ = self.predicter(sx, cls_mask=target_mask)\n",
        "        # print(sy_.shape, target_mask.shape)\n",
        "        sy = self.target_encoder(x).repeat(M,1,1)*target_mask.unsqueeze(-1)\n",
        "\n",
        "        loss = F.smooth_l1_loss(sy, sy_)\n",
        "        return loss\n",
        "\n",
        "    # def forward(self, hr, temp, heart): # [batch, 1+T]\n",
        "    #     # batch = hr.size(0)\n",
        "    #     # batch, seq, dim = x.shape\n",
        "    #     res_id, activeKilocalories, steps = hr[:,0], temp[:,0], heart[:,0]\n",
        "    #     enc_summary = self.enc0(torch.cat([self.calorie_emb(activeKilocalories), self.steps_emb(steps)], dim=-1)) # [batch, d_model]\n",
        "    #     x = self.enc1(torch.cat([self.hour_emb(hr[:,1:]/24), self.temp_emb(temp[:,1:]), self.heart_emb(heart[:,1:])], dim=-1)) # [batch, T, d_model]\n",
        "\n",
        "    #     x = torch.cat([enc_summary.unsqueeze(1), x], dim=1)\n",
        "    #     # sx = self.context_encoder(x)\n",
        "    #     sx = self.target_encoder(x)\n",
        "    #     out = sx.mean(dim=1)\n",
        "    #     return out\n",
        "\n",
        "\n",
        "\n",
        "seq_jepa = SeqJEPA(in_dim=16, d_model=32, out_dim=16, num_layers=1).to(device)#.to(torch.float)\n",
        "soptim = torch.optim.AdamW(seq_jepa.parameters(), lr=1e-3)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "O-OU1MaKF7BZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ijepa vision_transformer.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/models/vision_transformer.py\n",
        "import math\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from src.utils.tensors import (trunc_normal_, repeat_interleave_batch)\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid height and width\n",
        "    return:\n",
        "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid_h = np.arange(grid_size, dtype=float)\n",
        "    grid_w = np.arange(grid_size, dtype=float)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    assert embed_dim % 2 == 0\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid length\n",
        "    return:\n",
        "    pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid = np.arange(grid_size, dtype=float)\n",
        "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=float)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega   # (D/2,)\n",
        "    pos = pos.reshape(-1)   # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
        "    emb = np.concatenate([np.sin(out), np.cos(out)], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x, attn\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x, return_attention=False):\n",
        "        y, attn = self.attn(self.norm1(x))\n",
        "        if return_attention:\n",
        "            return attn\n",
        "        x = x + self.drop_path(y)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvEmbed(nn.Module):\n",
        "    \"\"\"3x3 Convolution stems for ViT following ViTC models\"\"\"\n",
        "    def __init__(self, channels, strides, img_size=224, in_chans=3, batch_norm=True):\n",
        "        super().__init__()\n",
        "        # Build the stems\n",
        "        stem = []\n",
        "        channels = [in_chans] + channels\n",
        "        for i in range(len(channels) - 2):\n",
        "            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3, stride=strides[i], padding=1, bias=(not batch_norm))]\n",
        "            if batch_norm:\n",
        "                stem += [nn.BatchNorm2d(channels[i+1])]\n",
        "            stem += [nn.ReLU(inplace=True)]\n",
        "        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n",
        "        self.stem = nn.Sequential(*stem)\n",
        "\n",
        "        # Comptute the number of patches\n",
        "        stride_prod = int(np.prod(strides))\n",
        "        self.num_patches = (img_size[0] // stride_prod)**2\n",
        "\n",
        "    def forward(self, x):\n",
        "        p = self.stem(x)\n",
        "        return p.flatten(2).transpose(1, 2)\n",
        "\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "def apply_masks(x, masks):\n",
        "    \"\"\":param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
        "    :param masks: list of tensors containing indices of patches in [N] to keep\"\"\"\n",
        "    all_x = []\n",
        "    for m in masks:\n",
        "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [batch,T,dim]\n",
        "        all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "    return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "\n",
        "class VisionTransformerPredictor(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, num_patches, embed_dim=768, predictor_embed_dim=384,\n",
        "        depth=6, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.init_std = init_std\n",
        "        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        # --\n",
        "        self.predictor_pos_embed = nn.Parameter(torch.zeros(1, num_patches, predictor_embed_dim), requires_grad=False)\n",
        "        predictor_pos_embed = get_2d_sincos_pos_embed(self.predictor_pos_embed.shape[-1], int(num_patches**.5), cls_token=False)\n",
        "        self.predictor_pos_embed.data.copy_(torch.from_numpy(predictor_pos_embed).float().unsqueeze(0)) # [1, num_patches, predictor_embed_dim]\n",
        "        # --\n",
        "        self.predictor_blocks = nn.ModuleList([Block(dim=predictor_embed_dim, num_heads=num_heads, drop=drop_rate, drop_path=dpr[i]) for i in range(depth)])\n",
        "        self.predictor_norm = nn.LayerNorm(predictor_embed_dim)\n",
        "        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n",
        "        # ------\n",
        "        # trunc_normal_(self.mask_token, std=self.init_std)\n",
        "\n",
        "    def forward(self, x, masks_x, masks):\n",
        "        assert (masks is not None) and (masks_x is not None), 'Cannot run predictor without mask indices'\n",
        "        if not isinstance(masks_x, list): masks_x = [masks_x]\n",
        "        if not isinstance(masks, list): masks = [masks]\n",
        "\n",
        "        # -- Batch Size\n",
        "        B = len(x) // len(masks_x)\n",
        "\n",
        "        # -- map from encoder-dim to pedictor-dim\n",
        "        x = self.predictor_embed(x) # [batch, num_patches, predictor_embed_dim]\n",
        "\n",
        "        # -- add positional embedding to x tokens\n",
        "        x_pos_embed = self.predictor_pos_embed.repeat(B, 1, 1) # [batch, num_patches, predictor_embed_dim]\n",
        "        x += apply_masks(x_pos_embed, masks_x)\n",
        "\n",
        "        _, N_ctxt, D = x.shape\n",
        "\n",
        "        # -- concat mask tokens to x\n",
        "        pos_embs = self.predictor_pos_embed.repeat(B, 1, 1)\n",
        "        pos_embs = apply_masks(pos_embs, masks)\n",
        "        pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x))\n",
        "        # --\n",
        "        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1)\n",
        "        # --\n",
        "        pred_tokens += pos_embs\n",
        "        x = x.repeat(len(masks), 1, 1)\n",
        "        x = torch.cat([x, pred_tokens], dim=1)\n",
        "\n",
        "        # -- fwd prop\n",
        "        for blk in self.predictor_blocks:\n",
        "            x = blk(x)\n",
        "        x = self.predictor_norm(x)\n",
        "\n",
        "        # -- return preds for mask tokens\n",
        "        x = x[:, N_ctxt:]\n",
        "        x = self.predictor_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, img_size=[224], patch_size=16, in_chans=3, embed_dim=768, predictor_embed_dim=384, depth=12, predictor_depth=12,\n",
        "        num_heads=12, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0, norm_layer=nn.LayerNorm, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_features = self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.init_std = init_std\n",
        "        # --\n",
        "        self.patch_embed = PatchEmbed(img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        # --\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=False)\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "        # --\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "    def forward(self, x, masks=None):\n",
        "        if masks is not None:\n",
        "            if not isinstance(masks, list): masks = [masks]\n",
        "\n",
        "        x = self.patch_embed(x)\n",
        "        B, N, D = x.shape # [batch, T, d_model]\n",
        "\n",
        "        # -- add positional embedding to x\n",
        "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
        "        x = x + pos_embed\n",
        "\n",
        "        if masks is not None: x = apply_masks(x, masks)\n",
        "\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            x = blk(x)\n",
        "\n",
        "        if self.norm is not None: x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "    def interpolate_pos_encoding(self, x, pos_embed):\n",
        "        npatch = x.shape[1] - 1\n",
        "        N = pos_embed.shape[1] - 1\n",
        "        if npatch == N: return pos_embed\n",
        "        class_emb, pos_embed = pos_embed[:, 0], pos_embed[:, 1:]\n",
        "        dim = x.shape[-1]\n",
        "        pos_embed = nn.functional.interpolate(pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2), scale_factor=math.sqrt(npatch / N), mode='bicubic',)\n",
        "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
        "\n",
        "# from functools import partial\n",
        "# def vit_predictor(**kwargs):\n",
        "#     model = VisionTransformerPredictor(\n",
        "#         mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "#     return model\n",
        "\n",
        "def vit(patch_size=16, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, # tiny\n",
        "        # patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, # small\n",
        "        # patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, # base\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "bNjp4xFsGPoa",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}