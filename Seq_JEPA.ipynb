{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0vScvFGGSeN6",
        "8DxjYI9RMQoP"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/Seq_JEPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwpnHW4wn9S1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77ff714b-55bd-4179-b3e4-c38b818c4aab",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 62.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title data\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "transform = transforms.Compose([transforms.ToTensor(),])\n",
        "\n",
        "# train_data = torchvision.datasets.MNIST(root=\"data\", train=True, download=True,transform=transform) # do not normalise! want img in [0,1)\n",
        "# test_data = torchvision.datasets.MNIST(root=\"data\", train=False, download=True, transform=transform) #opt no download\n",
        "# # batch_size = 32 # 64 512\n",
        "# batch_size = 32 if torch.cuda.is_available() else 32\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "train_data = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
        "test_data = torchvision.datasets.CIFAR10(root='data', train=False, download=True, transform=transform)\n",
        "batch_size = 128 # 128 1024\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "\n",
        "# dataiter = iter(train_data)\n",
        "# x,y = next(dataiter)\n",
        "# print(x.shape) # [3, 32, 32]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title rohanpandey WISDM\n",
        "# https://github.com/rohanpandey/Analysis--WISDM-Smartphone-and-Smartwatch-Activity/blob/master/dataset_creation.ipynb\n",
        "! wget https://archive.ics.uci.edu/static/public/507/wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset.zip\n",
        "!unzip wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset.zip\n",
        "!unzip wisdm-dataset.zip\n",
        "\n",
        "import glob\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "folder1=glob.glob(\"wisdm-dataset/raw/*\")\n",
        "# print(folder1)\n",
        "column_names = ['ID', 'activity','timestamp','x','y','z']\n",
        "overall_dataframe=pd.DataFrame(columns = column_names)\n",
        "for subfolder in folder1:\n",
        "    parent_dir = \"./processed/\"\n",
        "    path = os.path.join(parent_dir, subfolder.split('\\\\')[-1])\n",
        "    if not os.path.exists(path): os.makedirs(path)\n",
        "    folder2=glob.glob(subfolder+\"/*\")\n",
        "    for subsubfolder in folder2:\n",
        "        activity_dataframe = pd.DataFrame(columns = column_names)\n",
        "        subfolder_path = os.path.join(path, subsubfolder.split('/')[-1])\n",
        "        if not os.path.exists(subfolder_path): os.makedirs(subfolder_path)\n",
        "        files=glob.glob(subsubfolder+\"/*\")\n",
        "        for file in files:\n",
        "            # print(file)\n",
        "            df = pd.read_csv(file, sep=\",\",header=None)\n",
        "            df.columns = ['ID','activity','timestamp','x','y','z']\n",
        "            # activity_dataframe=activity_dataframe.append(df)\n",
        "            activity_dataframe = pd.concat([activity_dataframe, df], ignore_index=True)\n",
        "\n",
        "        activity_dataframe['z']=activity_dataframe['z'].str[:-1]\n",
        "        # activity_dataframe['meter']=subsubfolder.split('/')[-1]\n",
        "        # activity_dataframe['device']=subfolder.split('/')[-1]\n",
        "        activity_dataframe.to_csv(subfolder_path+'/data.csv',index=False)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "FA00-tf6MJ-s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86444b75-71f1-4e72-b43a-0bdaed7ff77f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-20 01:14:53--  https://archive.ics.uci.edu/static/public/507/wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset.zip’\n",
            "\n",
            "wisdm+smartphone+an     [                <=> ] 295.92M  18.4MB/s    in 9.9s    \n",
            "\n",
            "2025-03-20 01:15:03 (29.8 MB/s) - ‘wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset.zip’ saved [310292805]\n",
            "\n",
            "Archive:  wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset.zip\n",
            " extracting: WISDM-dataset-description.pdf  \n",
            " extracting: wisdm-dataset.zip       \n",
            "Archive:  wisdm-dataset.zip\n",
            "   creating: wisdm-dataset/\n",
            "  inflating: wisdm-dataset/WISDM-dataset-description.pdf  \n",
            "   creating: wisdm-dataset/arffmagic-master/\n",
            "  inflating: wisdm-dataset/arffmagic-master/Makefile  \n",
            "  inflating: wisdm-dataset/arffmagic-master/.DS_Store  \n",
            " extracting: wisdm-dataset/arffmagic-master/README.md  \n",
            "   creating: wisdm-dataset/arffmagic-master/src/\n",
            "  inflating: wisdm-dataset/arffmagic-master/src/arff.cpp  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/comparator.h  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/chunk.h  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/main.cpp  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/attribute.h  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/libmfcc.c  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/raw.h  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/try.h  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/write.h  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/chunk.cpp  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/raw.cpp  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/globals.h  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/write.cpp  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/arff.h  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/except.h  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/read.cpp  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/attribute.cpp  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/libmfcc.h  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/globals.cpp  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/funcmap.cpp  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/read.h  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/funcmap.h  \n",
            "   creating: wisdm-dataset/arffmagic-master/bin/\n",
            "  inflating: wisdm-dataset/arffmagic-master/bin/magic  \n",
            "   creating: wisdm-dataset/arffmagic-master/bin/arffmagic.dSYM/\n",
            "   creating: wisdm-dataset/arffmagic-master/bin/arffmagic.dSYM/Contents/\n",
            "  inflating: wisdm-dataset/arffmagic-master/bin/arffmagic.dSYM/Contents/Info.plist  \n",
            "   creating: wisdm-dataset/arffmagic-master/bin/arffmagic.dSYM/Contents/Resources/\n",
            "   creating: wisdm-dataset/arffmagic-master/bin/arffmagic.dSYM/Contents/Resources/DWARF/\n",
            "  inflating: wisdm-dataset/arffmagic-master/bin/arffmagic.dSYM/Contents/Resources/DWARF/arffmagic  \n",
            "  inflating: wisdm-dataset/arffmagic-master/bin/.DS_Store  \n",
            "  inflating: wisdm-dataset/arffmagic-master/bin/arffmagic  \n",
            "   creating: wisdm-dataset/arffmagic-master/build/\n",
            "  inflating: wisdm-dataset/arffmagic-master/build/libmfcc.o  \n",
            "  inflating: wisdm-dataset/arffmagic-master/build/write.o  \n",
            "  inflating: wisdm-dataset/arffmagic-master/build/attribute.o  \n",
            "  inflating: wisdm-dataset/arffmagic-master/build/arff.o  \n",
            "  inflating: wisdm-dataset/arffmagic-master/build/read.o  \n",
            "  inflating: wisdm-dataset/arffmagic-master/build/raw.o  \n",
            "  inflating: wisdm-dataset/arffmagic-master/build/funcmap.o  \n",
            "  inflating: wisdm-dataset/arffmagic-master/build/main.o  \n",
            "  inflating: wisdm-dataset/arffmagic-master/build/globals.o  \n",
            "  inflating: wisdm-dataset/arffmagic-master/build/chunk.o  \n",
            "  inflating: wisdm-dataset/README.txt  \n",
            "   creating: wisdm-dataset/raw/\n",
            "   creating: wisdm-dataset/raw/phone/\n",
            "   creating: wisdm-dataset/raw/phone/gyro/\n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1631_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1644_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1639_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1621_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1609_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1627_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1628_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1620_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1633_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1638_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1614_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1625_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1612_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1640_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1616_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1647_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1646_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1637_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1608_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1642_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1624_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/.DS_Store  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1643_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1645_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1641_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1626_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1623_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1607_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1629_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1610_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1649_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1648_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1636_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1630_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1600_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1603_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1606_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1604_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1611_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1632_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1618_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1635_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1605_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1602_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1613_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1617_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1619_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1615_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1650_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1634_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1622_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1601_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/.DS_Store  \n",
            "   creating: wisdm-dataset/raw/phone/accel/\n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1635_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1608_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1649_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1647_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1619_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1621_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1630_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1639_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1615_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1611_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1616_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1644_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1648_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1645_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1641_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1603_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1628_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1610_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1634_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1600_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/.DS_Store  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1613_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1624_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1620_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1606_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1633_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1632_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1626_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1643_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1637_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1640_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1627_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1650_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1614_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1617_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1618_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1642_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1612_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1646_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1601_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1605_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1623_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1607_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1638_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1636_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1629_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1604_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1622_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1625_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1602_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1609_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1631_accel_phone.txt  \n",
            "   creating: wisdm-dataset/raw/watch/\n",
            "   creating: wisdm-dataset/raw/watch/gyro/\n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1629_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1633_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1626_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1644_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1625_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1649_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1631_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1632_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1622_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1628_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1602_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1603_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1610_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1609_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1647_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1638_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1642_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1627_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1601_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1646_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1643_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1636_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1635_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1617_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/.DS_Store  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1612_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1620_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1623_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1615_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1621_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1645_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1641_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1616_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1608_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1605_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1607_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1648_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1600_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1637_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1604_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1639_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1611_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1613_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1614_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1640_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1606_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1630_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1618_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1634_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1650_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1619_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1624_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/.DS_Store  \n",
            "   creating: wisdm-dataset/raw/watch/accel/\n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1626_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1631_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1605_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1608_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1600_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1650_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1623_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1612_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1637_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1620_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1633_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1618_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1635_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1619_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1611_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1614_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1601_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1616_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/.DS_Store  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1645_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1606_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1615_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1638_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1636_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1607_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1627_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1625_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1640_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1643_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1602_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1644_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1629_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1642_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1610_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1624_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1603_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1621_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1646_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1628_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1604_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1634_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1613_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1647_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1609_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1630_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1639_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1617_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1648_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1632_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1649_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1641_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1622_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/.DS_Store  \n",
            "   creating: wisdm-dataset/arff_files/\n",
            "   creating: wisdm-dataset/arff_files/phone/\n",
            "   creating: wisdm-dataset/arff_files/phone/gyro/\n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1610_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1612_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1637_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1622_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1604_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1639_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1621_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1623_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1640_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1645_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1602_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1649_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1600_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1644_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1646_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1632_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1608_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1635_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1606_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1629_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1617_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1628_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1636_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1601_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/.DS_Store  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1615_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1611_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1620_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1647_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1631_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1648_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1603_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1627_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1619_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1607_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1643_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1624_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1638_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1618_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1605_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1625_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1626_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1650_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1616_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1609_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1613_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1630_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1634_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1641_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1633_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1642_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/.DS_Store  \n",
            "   creating: wisdm-dataset/arff_files/phone/accel/\n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1633_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1612_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1620_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1628_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1602_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1636_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1604_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1638_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1647_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1607_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1648_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1643_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1627_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1626_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1611_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1600_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1635_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1613_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1617_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1641_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1642_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1637_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1621_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/.DS_Store  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1632_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1629_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1630_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1603_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1631_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1615_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1609_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1644_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1616_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1634_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1622_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1649_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1645_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1606_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1601_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1639_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1610_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1608_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1624_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1618_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1650_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1646_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1625_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1640_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1619_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1605_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1623_accel_phone.arff  \n",
            "   creating: wisdm-dataset/arff_files/watch/\n",
            "   creating: wisdm-dataset/arff_files/watch/gyro/\n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1620_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1604_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1613_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1631_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1635_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1639_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1628_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1603_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1632_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1626_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1618_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1627_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1609_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1646_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1650_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1629_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1621_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1607_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1611_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1637_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1602_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1617_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1619_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/.DS_Store  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1643_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1630_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1641_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1647_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1616_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1648_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1623_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1606_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1634_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1622_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1645_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1642_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1633_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1615_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1640_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1605_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1610_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1624_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1638_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1612_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1600_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1636_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1601_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1644_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1608_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1625_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1649_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/.DS_Store  \n",
            "   creating: wisdm-dataset/arff_files/watch/accel/\n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1623_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1608_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1641_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1635_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1643_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1604_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1633_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1630_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1609_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1649_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1622_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1607_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1634_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1618_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1610_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1650_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1647_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1642_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1646_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1617_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1637_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1626_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1645_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1612_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1615_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/.DS_Store  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1620_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1632_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1613_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1616_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1644_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1628_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1603_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1606_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1625_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1648_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1639_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1602_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1619_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1636_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1638_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1640_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1624_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1600_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1629_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1631_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1611_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1601_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1621_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1627_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1605_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/.DS_Store  \n",
            "  inflating: wisdm-dataset/change_raw_act.pl  \n",
            "  inflating: wisdm-dataset/.activity_key.txt.swp  \n",
            "  inflating: wisdm-dataset/activity_key.txt  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-3fbefc6072f4>:30: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  activity_dataframe = pd.concat([activity_dataframe, df], ignore_index=True)\n",
            "<ipython-input-1-3fbefc6072f4>:30: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  activity_dataframe = pd.concat([activity_dataframe, df], ignore_index=True)\n",
            "<ipython-input-1-3fbefc6072f4>:30: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  activity_dataframe = pd.concat([activity_dataframe, df], ignore_index=True)\n",
            "<ipython-input-1-3fbefc6072f4>:30: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  activity_dataframe = pd.concat([activity_dataframe, df], ignore_index=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test WISDM\n",
        "\n",
        "# print(activity_dataframe)\n",
        "# print(activity_dataframe['activity'])\n",
        "# activity_dataframe.to_csv('data.csv',index=False)\n",
        "\n",
        "# data = pd.read_csv(\"/data.csv\", index_col =\"Name\")\n",
        "# data = pd.read_csv(\"data.csv\")\n",
        "\n",
        "# a = list(activity_dataframe['timestamp'])\n",
        "# print(a)\n",
        "# print([x-y for x,y in zip(a[1:],a[:-1])])\n",
        "\n",
        "# print(data.loc[0])\n",
        "# p = data.loc[0]\n",
        "# print(len(p))\n",
        "# ID, activity, timestamp, x, y, z, meter, device = data.loc[0]\n",
        "# print(activity)\n",
        "\n",
        "# print(data.loc[1639])\n",
        "\n",
        "# userids = data['ID'].unique()#for id in userids\n",
        "# print(data['activity'].unique())\n",
        "# df_keep = data[['ID','activity','timestamp','x','y','z']]\n",
        "\n",
        "\n",
        "# grouped = df_keep.groupby(['ID'])\n",
        "# grouped = df_keep.groupby(['ID','activity'])\n",
        "# print(len(grouped))\n",
        "# user_acts = dict(tuple(df_keep.groupby(['ID','activity'])))\n",
        "# print(user_acts)\n",
        "# print(len(user_acts))\n",
        "# temp_df = df[df['ID'] == id]\n",
        "\n",
        "# print(len(df_keep))\n",
        "# print(len(data))\n",
        "\n",
        "\n",
        "\n",
        "# act = [[a, d[['timestamp','x','y','z']].to_numpy()] for a, d in user_acts.items()]\n",
        "# act = [[(int(a[0]), ), d[['timestamp','x','y','z']].to_numpy()] for a, d in user_acts.items()]\n",
        "# print(act)\n",
        "# print(act[0])\n",
        "# # print(act[0][1])\n",
        "# print(len(act[1]))\n",
        "# print([len(a[1]) for a in act])\n",
        "# print(min([len(a[1]) for a in act])) # 3567\n",
        "\n",
        "\n",
        "# act_dict = {i: act for i, act in enumerate(data['activity'].unique())}\n",
        "# act_invdict = {v: k for k, v in act_dict.items()}\n",
        "# print(act_invdict)\n",
        "\n",
        "# torch.tensor(act[0][1][:3500])\n",
        "\n",
        "# id_act, x = act[0]\n",
        "# id_act = self.process(id_act)\n",
        "# return id_act,\n",
        "# torch.tensor(x[:3500]) # 3567\n",
        "\n",
        "\n",
        "# /content/processed/wisdm-dataset/raw/phone/accel/data.csv\n",
        "# /content/processed/wisdm-dataset/raw/watch/gyro/data.csv\n",
        "\n",
        "# data0 = pd.read_csv(\"/content/processed/wisdm-dataset/raw/watch/accel/data.csv\")\n",
        "# data1 = pd.read_csv(\"/content/processed/wisdm-dataset/raw/watch/gyro/data.csv\")\n",
        "\n",
        "# user_acts0 = dict(tuple(data0.groupby(['ID','activity'])))\n",
        "# user_acts1 = dict(tuple(data1.groupby(['ID','activity'])))\n",
        "\n",
        "for (a0,d0), (a1,d1) in zip(user_acts0.items(), user_acts1.items()):\n",
        "    print(a0,a1)\n",
        "    print(len(d0),len(d1))\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rV7WLiNT4EAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title buffer dataloader\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# /content/processed/wisdm-dataset/raw/phone/accel/data.csv\n",
        "# /content/processed/wisdm-dataset/raw/watch/gyro/data.csv\n",
        "\n",
        "class BufferDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        # df_keep = pd.read_csv(\"data.csv\")#[['ID','activity','timestamp','x','y','z']]\n",
        "        df_keep = pd.read_csv(\"/content/processed/wisdm-dataset/raw/watch/accel/data.csv\")\n",
        "\n",
        "        user_acts = dict(tuple(df_keep.groupby(['ID','activity'])[['timestamp','x','y','z']]))\n",
        "        self.data = [[d.to_numpy(), a] for a, d in user_acts.items()]\n",
        "        self.act_dict = {i: act for i, act in enumerate(df_keep['activity'].unique())}\n",
        "        self.act_invdict = {v: k for k, v in self.act_dict.items()} # {'A': 0, 'B': 1, ...\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, id_act = self.data[idx]\n",
        "        id_act = self.process(id_act)\n",
        "        return torch.tensor(x[:3500]), id_act # 3567\n",
        "\n",
        "    def process(self, id_act):\n",
        "        return int(id_act[0]), self.act_invdict[id_act[1]]\n",
        "\n",
        "    def transform(self, act_list): # rand resize crop, rand mask # https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html\n",
        "        act_list = np.array(act_list)\n",
        "        try: hr, temp, heart= zip(*act_list)\n",
        "        except ValueError: print('err:',act_list)\n",
        "        kind = 'nearest' if len(act_list)>self.seq_len/.7 else 'linear'\n",
        "        temp_interpolator = scipy.interpolate.interp1d(hr, temp, kind=kind) # linear nearest quadratic cubic\n",
        "        heart_interpolator = scipy.interpolate.interp1d(hr, heart, kind=kind) # linear nearest quadratic cubic\n",
        "        hr_ = np.sort(np.random.uniform(hr[0], hr[-1], round(self.seq_len*random.uniform(1,1/.7))))\n",
        "        temp_, heart_ = temp_interpolator(hr_), heart_interpolator(hr_)\n",
        "        act_list = list(zip(hr_, temp_, heart_))\n",
        "\n",
        "        idx = torch.randint(len(act_list)-self.seq_len+1, size=(1,))\n",
        "        # return act_list[idx: idx+self.seq_len]\n",
        "        act_list = act_list[idx: idx+self.seq_len]\n",
        "\n",
        "        # mask=(torch.rand(self.seq_len)<.1) # True -> masked # random masking\n",
        "        # # act_list[mask] = self.pad[0]\n",
        "        # act_list = [self.pad[0] if m else a for m,a in zip(mask, act_list)]\n",
        "        return act_list\n",
        "\n",
        "    def add(self, episode):\n",
        "        self.data.append(episode)\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    # print(npimg.shape) # (3, 64, 64)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# df_keep = pd.read_csv(\"data.csv\")#[['ID','activity','timestamp','x','y','z']]\n",
        "# user_acts = dict(tuple(df_keep.groupby(['ID','activity'])[['timestamp','x','y','z']]))\n",
        "# dataset = [[d.to_numpy(), a] for a, d in user_acts.items()]\n",
        "\n",
        "\n",
        "train_data = BufferDataset() # one line of poem is roughly 50 characters\n",
        "\n",
        "dataset_size = len(train_data)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(0.7 * dataset_size))\n",
        "np.random.seed(0)\n",
        "np.random.shuffle(indices)\n",
        "train_indices, val_indices = indices[:split], indices[split:]\n",
        "\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "valid_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 64 #512\n",
        "train_loader = DataLoader(train_data, sampler=train_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "test_loader = DataLoader(train_data, sampler=valid_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True)\n"
      ],
      "metadata": {
        "id": "CNVNCV8CGtR_",
        "cellView": "form"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title har_cnn\n",
        "# https://arxiv.org/pdf/2209.08335v1\n",
        "# https://github.com/Lou1sM/HAR/blob/master/har_cnn.py\n",
        "import sys\n",
        "import json\n",
        "from hmmlearn import hmm\n",
        "from copy import deepcopy\n",
        "import os\n",
        "import math\n",
        "from pdb import set_trace\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import time\n",
        "from torch.utils import data\n",
        "import cl_args\n",
        "from dl_utils.misc import asMinutes,check_dir\n",
        "#from dl_utils.label_funcs import accuracy, mean_f1, debable, translate_labellings, get_num_labels, label_counts, dummy_labels, avoid_minus_ones_lf_wrapper,masked_mode,acc_by_label, get_trans_dict\n",
        "from label_funcs_tmp import accuracy, mean_f1, translate_labellings, get_num_labels, label_counts, dummy_labels, avoid_minus_ones_lf_wrapper,masked_mode,acc_by_label, get_trans_dict\n",
        "from dl_utils.tensor_funcs import noiseify, numpyify, cudify\n",
        "from make_dsets import make_single_dset, make_dsets_by_user\n",
        "from sklearn.metrics import normalized_mutual_info_score,adjusted_rand_score\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from project_config import get_dataset_info_object\n",
        "\n",
        "rari = lambda x,y: round(adjusted_rand_score(x,y),4)\n",
        "rnmi = lambda x,y: round(normalized_mutual_info_score(x,y),4)\n",
        "\n",
        "class EncByLayer(nn.Module):\n",
        "    def __init__(self,x_filters,y_filters,x_strides,y_strides,max_pools,show_shapes):\n",
        "        super().__init__()\n",
        "        self.show_shapes = show_shapes\n",
        "        num_layers = len(x_filters)\n",
        "        assert all(len(x)==num_layers for x in (y_filters,x_strides,y_strides,max_pools))\n",
        "        ncvs = [1]+[4*2**i for i in range(num_layers)]\n",
        "        # conv_layers = []\n",
        "        # for i in range(num_layers):\n",
        "        #     conv_layer = nn.Sequential(\n",
        "        #         nn.Conv2d(ncvs[i],ncvs[i+1],(x_filters[i],y_filters[i]),(x_strides[i],y_strides[i])), nn.BatchNorm2d(ncvs[i+1]) if i<num_layers-1 else nn.Identity(), nn.LeakyReLU(0.3), nn.MaxPool2d(max_pools[i])\n",
        "        #     )\n",
        "        #     conv_layers.append(conv_layer)\n",
        "        # self.conv_layers = nn.ModuleList(conv_layers)\n",
        "\n",
        "        self.conv_layers = nn.Sequential(*[\n",
        "                nn.Conv2d(ncvs[i],ncvs[i+1],(x_filters[i],y_filters[i]),(x_strides[i],y_strides[i])), nn.BatchNorm2d(ncvs[i+1]) if i<num_layers-1 else nn.Identity(), nn.LeakyReLU(0.3), nn.MaxPool2d(max_pools[i])\n",
        "             for i in range(nlayers)])\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        # if self.show_shapes: print(x.shape)\n",
        "        # for conv_layer in self.conv_layers:\n",
        "        x = conv_layer(x)\n",
        "            # if self.show_shapes: print(x.shape)\n",
        "        return x\n",
        "\n",
        "class DecByLayer(nn.Module):\n",
        "    def __init__(self,x_filters,y_filters,x_strides,y_strides,show_shapes):\n",
        "        super().__init__()\n",
        "        self.show_shapes = show_shapes\n",
        "        num_layers = len(x_filters)\n",
        "        assert all(len(x)==num_layers for x in (y_filters,x_strides,y_strides))\n",
        "        ncvs = [4*2**i for i in reversed(range(num_layers))]+[1]\n",
        "        conv_trans_layers = [nn.Sequential(\n",
        "                nn.ConvTranspose2d(ncvs[i],ncvs[i+1],(x_filters[i],y_filters[i]),(x_strides[i],y_strides[i])), nn.BatchNorm2d(ncvs[i+1]), nn.LeakyReLU(0.3),\n",
        "                )\n",
        "            for i in range(num_layers)]\n",
        "        self.conv_trans_layers = nn.ModuleList(conv_trans_layers)\n",
        "\n",
        "    def forward(self,x):\n",
        "        if self.show_shapes: print(x.shape)\n",
        "        for conv_trans_layer in self.conv_trans_layers:\n",
        "            x = conv_trans_layer(x)\n",
        "            if self.show_shapes: print(x.shape)\n",
        "        return x\n",
        "\n",
        "class Var_BS_MLP(nn.Module):\n",
        "    def __init__(self,input_size,hidden_size,output_size):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(nn.Linear(input_size,hidden_size), nn.BatchNorm1d(hidden_size), nn.LeakyReLU(0.3), nn.Linear(hidden_size,output_size))\n",
        "\n",
        "    def forward(self,x):\n",
        "        return self.mlp(x)\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n",
        "    dset_info_object = get_dataset_info_object(args.dset)\n",
        "    num_classes = args.num_classes if args.num_classes != -1 else dset_info_object.num_classes\n",
        "    if args.dset == 'UCI_feat':\n",
        "        enc = nn.Sequential(nn.Linear(561,500),nn.ReLU(),\n",
        "                            nn.Linear(500,500),nn.ReLU(),\n",
        "                            nn.Linear(500,2000),nn.ReLU(),\n",
        "                            nn.Linear(2000,6),nn.ReLU()).cuda()\n",
        "        dec = nn.Sequential(nn.Linear(6,2000),nn.ReLU(),\n",
        "                            nn.Linear(2000,500),nn.ReLU(),\n",
        "                            nn.Linear(500,500),nn.ReLU(),\n",
        "                            nn.Linear(500,561),nn.ReLU()).cuda()\n",
        "        mlp = Var_BS_MLP(6,256,num_classes).cuda()\n",
        "    else:\n",
        "        if args.window_size == 512:\n",
        "            x_filters = (50,40,7,4)\n",
        "            x_strides = (2,2,1,1)\n",
        "            max_pools = ((2,1),(2,1),(2,1),(2,1))\n",
        "        elif args.window_size == 100:\n",
        "            x_filters = (20,20,5,3)\n",
        "            x_strides = (1,1,1,1)\n",
        "            max_pools = ((2,1),(2,1),(2,1),1)\n",
        "        y_filters = (1,1,1,dset_info_object.num_channels)\n",
        "        y_strides = (1,1,1,1)\n",
        "        enc = EncByLayer(x_filters,y_filters,x_strides,y_strides,max_pools,show_shapes=args.show_shapes).cuda()\n",
        "        #if args.is_n2d:\n",
        "        x_filters_trans = (15,10,15,11)\n",
        "        x_strides_trans = (2,3,3,3)\n",
        "        y_filters_trans = (dset_info_object.num_channels,1,1,1)\n",
        "        dec = DecByLayer(x_filters_trans,y_filters_trans,x_strides_trans,y_strides,show_shapes=args.show_shapes).cuda()\n",
        "\n",
        "        optional_umap_like_net_in = Var_BS_MLP(32,256,2).cuda()\n",
        "        optional_umap_like_net_out = Var_BS_MLP(2,256,2).cuda()\n",
        "        if ARGS.is_uln:\n",
        "            enc = nn.Sequential(enc,nn.Flatten(1),Var_BS_MLP(32,256,2).cuda())\n",
        "            dec = nn.Sequential(Var_BS_MLP(32,256,2).cuda(),nn.Unflatten(2,(32,1,1)),dec)\n",
        "        mlp = Var_BS_MLP(2 if ARGS.is_uln else 32,256,num_classes).cuda()\n",
        "    if args.load_pretrained:\n",
        "        enc.load_state_dict(torch.load('enc_pretrained.pt'))\n",
        "    subj_ids = args.subj_ids\n",
        "\n",
        "    metric_dict = {'acc':accuracy,'nmi':rnmi,'ari':rari,'f1':mean_f1}\n",
        "    har = HARLearner(enc=enc,mlp=mlp,dec=dec,num_classes=num_classes,args=args,metric_dict=metric_dict)\n",
        "\n",
        "    start_time = time.time()\n",
        "    already_exists = check_dir(f\"experiments/{args.exp_name}/preds\")\n",
        "    check_dir(f\"experiments/{args.exp_name}/best_preds\")\n",
        "    if args.show_shapes:\n",
        "        dset_train, selected_acts = make_single_dset(args,subj_ids)\n",
        "        num_ftrs = dset_train.x.shape[-1]\n",
        "        print(num_ftrs)\n",
        "        lat = enc(torch.ones((2,1,args.window_size,num_ftrs),device='cuda'))\n",
        "        dec(lat)\n",
        "        sys.exit()\n",
        "    dsets_by_id = make_dsets_by_user(args,subj_ids)\n",
        "    if args.is_n2d:\n",
        "        for subj_id, (dset,sa) in dsets_by_id.items():\n",
        "            print(\"n2ding\", subj_id)\n",
        "            har.n2d_abl(subj_id,dset)\n",
        "    elif not args.subject_independent:\n",
        "        bad_ids = []\n",
        "        for user_id, (dset,sa) in dsets_by_id.items():\n",
        "            n = get_num_labels(dset.y)\n",
        "            if n < dset_info_object.num_classes/2:\n",
        "                print(f\"Excluding user {user_id}, only has {n} different labels, out of {num_classes}\")\n",
        "                bad_ids.append(user_id)\n",
        "        if not args.bad_ids: dsets_by_id = {k:v for k,v in dsets_by_id.items() if k not in bad_ids}\n",
        "        print('reloading clusterings for', [x for x in subj_ids[:args.reload_ids] if x not in bad_ids])\n",
        "        for rid in subj_ids[:args.reload_ids]:\n",
        "            if rid in bad_ids: continue\n",
        "            print('reloading clusterings for', rid)\n",
        "            rdset,sa = dsets_by_id.pop(rid)\n",
        "            best_preds = np.load(f'experiments/{args.exp_name}/best_preds/{rid}.npy')\n",
        "            preds = np.load(f'experiments/{args.exp_name}/preds/{rid}.npy')\n",
        "            har.log_preds_and_scores(rid,preds,best_preds,numpyify(rdset.y))\n",
        "        print('clustering remaining ids', [x for x in subj_ids[args.reload_ids:]], 'from scratch\\n')\n",
        "\n",
        "        print(\"CLUSTERING EACH DSET SEPARATELY\")\n",
        "        for subj_id, (dset,sa) in dsets_by_id.items():\n",
        "            print(\"clustering\", subj_id)\n",
        "            har.pseudo_label_cluster_meta_meta_loop(subj_id,dset)\n",
        "    elif args.subject_independent:\n",
        "        print(\"CLUSTERING AS SINGLE DSET\")\n",
        "        one_big_dset, selected_acts = make_single_dset(args,subj_ids)\n",
        "        har.pseudo_label_cluster_meta_meta_loop('all',one_big_dset)\n",
        "\n",
        "    results_file_path = f'experiments/{args.exp_name}/results.txt'\n",
        "    har.total_time = time.time() - start_time\n",
        "    har.log_final_scores(results_file_path)\n",
        "    har.express_times(results_file_path)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    ARGS, need_umap = cl_args.get_cl_args()\n",
        "    if need_umap: import umap\n",
        "    main(ARGS)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "N-4I7A5hJU7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title (Learned)RotEmb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, seq_len=512, base=10000):\n",
        "        super().__init__()\n",
        "        self.dim, self.base = dim, base\n",
        "        theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "        pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "        angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "        self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.base)\n",
        "        return x * self.rot_emb[:seq_len]\n",
        "\n",
        "# class LearnedRoPE(nn.Module): # learnt RoPE ; each tok is 1 pos\n",
        "#     def __init__(self, dim):\n",
        "#         super().__init__()\n",
        "#         self.weights = nn.Parameter(torch.randn(1, dim//2))\n",
        "\n",
        "#     def forward(self, x): #\n",
        "#         batch, seq_len, dim = x.shape\n",
        "#         # if rot_emb.shape[0] < seq_len: self.__init__(dim, seq_len)\n",
        "#         pos = torch.arange(seq_len).unsqueeze(1)\n",
        "#         angles = (self.weights * pos * 2*torch.pi).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "#         rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "#         return x * rot_emb.flatten(-2).unsqueeze(0)\n",
        "\n",
        "\n",
        "class LearnedRoPE(nn.Module): # learnt RoPE ; each tok is 1 pos\n",
        "    def __init__(self, dim, seq_len=512):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.weights = nn.Parameter(torch.randn(1, dim//2))\n",
        "        pos = torch.arange(seq_len).unsqueeze(1)\n",
        "        angles = (self.weights * pos * 2*torch.pi)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "        self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "    def forward(self, x): # [batch, seq_len, dim]\n",
        "        seq_len = x.size(1)\n",
        "        if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len)\n",
        "        return x * self.rot_emb[:seq_len]\n",
        "\n",
        "\n",
        "class RotEmb(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, top=torch.pi, base=10000):\n",
        "        super().__init__()\n",
        "        self.theta = top / (base ** (torch.arange(0, dim, step=2, device=device) / dim))\n",
        "        # self.theta = top / (base ** torch.linspace(0, 1, dim//2, device=device))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (pos.unsqueeze(-1) * self.theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [seq_len, dim]\n",
        "\n",
        "class LearnedRotEmb(nn.Module): # pos in R\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.weights = nn.Parameter(torch.randn((1, dim//2), device=device))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (self.weights * pos.unsqueeze(-1) * 2*torch.pi).unsqueeze(-1) # [batch, 1] * [1, dim//2] -> [batch, dim//2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [batch, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [batch, dim]\n",
        "\n"
      ],
      "metadata": {
        "id": "npc_xGtOz7DC",
        "cellView": "form"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ViT me simple wisdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    \"\"\"Zero out the parameters of a module and return it.\"\"\"\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads=None, d_head=8, cond_dim=None, dropout=0.): # .1\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_head = d_head\n",
        "        self.n_heads = d_model // d_head\n",
        "        # self.d_head = d_model // n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or d_model, 2*d_model, bias=False)\n",
        "        # self.k = nn.Sequential(nn.Dropout(dropout), nn.Linear(cond_dim, d_model, bias=False))\n",
        "        # self.lin = nn.Linear(d_model, d_model)\n",
        "        self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        # self.lin = nn.Sequential(nn.Dropout(dropout), zero_module(nn.Linear(d_model, d_model)))\n",
        "        self.drop = nn.Dropout(dropout) # indp before q,k,v; after linout\n",
        "        self.scale = self.d_head ** -.5\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [batch, T, d_model]=[batch, h*w, c], [batch, num_tok, cond_dim], [batch,T]\n",
        "        batch = x.shape[0]\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        Q = self.q(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        # K = self.k(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        K, V = self.kv(cond).view(batch, -1, self.n_heads, 2*self.d_head).transpose(1, 2).chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # linear attention # Softmax(Q) @ (Softmax(K).T @ V)\n",
        "        if mask != None:\n",
        "            mask = mask[:, None, :, None] # [batch,T] -> [batch,1,T,1]\n",
        "            K, V = K.masked_fill(mask, -torch.finfo(x.dtype).max), V.masked_fill(mask, -torch.finfo(x.dtype).max)\n",
        "        Q, K = Q.softmax(dim=-1)*self.scale, K.softmax(dim=-2)\n",
        "        context = K.transpose(-2,-1) @ V # [batch, n_heads, d_head, d_head]\n",
        "        out = Q @ context # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # (quadratic) attention # Softmax(Q @ K.T) @ V\n",
        "        # attn = Q @ K.transpose(-2,-1) * self.scale # [batch, n_heads, T] # [batch, n_heads, T, T/num_tok]\n",
        "        # if mask != None: attn = attn.masked_fill(mask[:, None, :, None], -torch.finfo(attn.dtype).max) # [batch,T]->[batch,1,T,1]\n",
        "        # attention = torch.softmax(attn, dim=-1)\n",
        "        # out = self.drop(attention) @ V # [batch, n_heads, T, d_head]\n",
        "\n",
        "        out = out.transpose(1, 2).flatten(2)\n",
        "        return self.drop(self.lin(out)) # [batch, T, d_model]\n",
        "\n",
        "# if self, dont pass cond_dim in init, dont pass cond in fwd\n",
        "# Softmax(Q @ K.T) @ V ~ Softmax(Q) @ Softmax(K).T @ V\n",
        "\n",
        "# https://github.com/lucidrains/x-transformers/blob/main/x_transformers/x_transformers.py#L1855\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_head=4, cond_dim=None, ff_mult=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm1 = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.norm2 = nn.RMSNorm(d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.cond_dim = cond_dim\n",
        "        self.self = MultiHeadAttention(d_model, d_head=d_head, dropout=0)\n",
        "        if self.cond_dim!=None: self.cross = MultiHeadAttention(d_model, d_head=d_head, cond_dim=cond_dim, dropout=0)\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        ff_dim=d_model*ff_mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), nn.Dropout(dropout), nn.Linear(d_model, ff_dim), act, # ReLU GELU\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "            nn.RMSNorm(d_model), nn.Dropout(dropout), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            # nn.RMSNorm(d_model), act, nn.Dropout(dropout), nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,c,h,w], [batch, num_tok, cond_dim], [batch,T]\n",
        "        # bchw = x.shape\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "\n",
        "        if self.cond_dim==None: cond=None # is self attn\n",
        "        x = x + self.drop(self.self(self.norm1(x)))\n",
        "        if self.cond_dim!=None: x = x + self.cross(self.norm2(x), cond, mask)\n",
        "        x = x + self.ff(x)\n",
        "        # x = x + self.drop(self.ff(x))\n",
        "\n",
        "        # return x.transpose(1,2).reshape(*bchw)\n",
        "        return x\n",
        "\n",
        "import torch\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "def apply_masks(x, masks):\n",
        "    \"\"\":param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
        "    :param masks: list of tensors containing indices of patches in [N] to keep\"\"\"\n",
        "    all_x = []\n",
        "    for m in masks: # [1,T]\n",
        "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [1,T,dim]\n",
        "        all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "    return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        # self.positional_emb = nn.Parameter(torch.zeros(1, 32*32//4, d_model)) # positional_embedding == 'learnable'\n",
        "        # encoder_layers = nn.TransformerEncoderLayer(d_model, d_model//d_head, d_hid or d_model, dropout, batch_first=True)\n",
        "        # self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.zeros(1,1,d_model)) # randn zeros\n",
        "        nn.init.trunc_normal_(self.cls, std=.02)\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model)\n",
        "        self.lin = nn.Linear(d_model, out_dim)# if out_dim != d_model else None\n",
        "\n",
        "    def forward(self, src, context_indices, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        src = self.embed(src) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = src.shape\n",
        "        src = src * self.pos_encoder(context_indices)\n",
        "        # src = src + self.positional_emb[:,context_indices]\n",
        "        # src = apply_masks(src, [context_indices])\n",
        "\n",
        "        pred_tokens = self.cls * self.pos_encoder(trg_indices) # [M, num_trg_toks, d_model]\n",
        "        # pred_tokens = self.cls + self.positional_emb[0,trg_indices]\n",
        "        pred_tokens = pred_tokens.repeat(batch, 1, 1) # [batch*M, num_trg_toks, d_model]\n",
        "        # print(pred_tokens.requires_grad)\n",
        "        # print(\"pred fwd\", src.shape, pred_tokens.shape)\n",
        "        # src = src.repeat_interleave(trg_indices.shape[0], dim=0) # [batch, seq_len, d_model] -> [batch*M, seq_len, d_model]\n",
        "        src = torch.cat([src, pred_tokens], dim=1) # [batch*M, seq_len+num_trg_toks, d_model]\n",
        "\n",
        "        out = self.transformer_encoder(src) # float [seq_len, batch_size, d_model]\n",
        "        out = self.norm(out)\n",
        "        out = out[:,seq:] # [batch*M, num_trg_toks, d_model]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "\n",
        "        # self.calorie_emb = RotEmb(dim, top=1, base=10) # 0-4\n",
        "        # self.steps_emb = RotEmb(dim, top=1, base=10) # 0-5\n",
        "        # self.enc0 = nn.Sequential(\n",
        "        #     nn.Linear(dim*2, d_model), act,\n",
        "        #     nn.Linear(d_model, d_model), act,\n",
        "        # )\n",
        "        # self.hour_emb = CircularEmb(dim, torch.pi/2) # circular (0,1) # 0-24\n",
        "        # self.temp_emb = RotEmb(dim, top=1/3, base=10) # 18-35\n",
        "        # self.heart_emb = RotEmb(dim, top=1/3, base=100) # 50-200\n",
        "        # self.enc1 = nn.Sequential(\n",
        "        #     nn.Linear(dim*3, d_model), act,\n",
        "        # )\n",
        "\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.MaxPool1d(2,2), #nn.MaxPool1d(3, 2, 3//2),\n",
        "            # nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.Conv1d(d_model, d_model,3,2,3//2)\n",
        "            nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2),\n",
        "            # nn.Conv1d(in_dim, d_model,2,2,0), # like patch\n",
        "            )\n",
        "        self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        # self.positional_emb = nn.Parameter(torch.zeros(1, 32*32//4, d_model)) # positional_embedding == 'learnable'\n",
        "        # encoder_layers = nn.TransformerEncoderLayer(d_model, d_model//d_head, d_hid or d_model, dropout, batch_first=True)\n",
        "        # self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model)\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, src, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "\n",
        "        # src = self.embed(src) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        src = self.embed(src.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "\n",
        "        batch, seq, dim = src.shape\n",
        "\n",
        "        # # # src = self.pos_encoder(src)\n",
        "        # if context_indices != None:\n",
        "        # print(src.shape, self.pos_encoder(context_indices).shape)\n",
        "        #     src = src * self.pos_encoder(context_indices) # context/predictor # src = src + self.positional_emb[:,context_indices]\n",
        "        # else: src = src * self.pos_encoder(torch.arange(seq, device=device).unsqueeze(0)) # target # src = src + self.positional_emb[:,:seq]\n",
        "\n",
        "\n",
        "        # src = self.pos_encoder(src)\n",
        "        src = src * self.pos_encoder(torch.arange(seq, device=device)).unsqueeze(0)\n",
        "        # src = src + self.positional_emb\n",
        "        if context_indices != None:\n",
        "            src = apply_masks(src, [context_indices])\n",
        "\n",
        "\n",
        "        out = self.transformer_encoder(src) # float [seq_len, batch_size, d_model]\n",
        "        out = self.norm(out)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "batch, seq_len, d_model = 4,1024,16\n",
        "in_dim = 3\n",
        "model = TransformerModel(in_dim, d_model, d_head=4, nlayers=2, dropout=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "# # print(out)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bnjJHfKj1_g",
        "outputId": "220f093b-eeb1-457f-ffe6-b087afed86d9",
        "cellView": "form"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5104\n",
            "torch.Size([4, 32, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "d929355f-be2b-47a1-82ff-a7630671f47f",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title RNN pytorch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, num_layers=1):\n",
        "        super().__init__()\n",
        "        if out_dim is None: out_dim = in_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.d_model = d_model\n",
        "        # self.rnn = nn.RNN(d_model, d_model, num_layers, batch_first=True)\n",
        "        self.rnn = nn.GRU(in_dim, d_model, num_layers, batch_first=True)\n",
        "        # self.lstm = nn.LSTM(d_model, d_model, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(d_model, out_dim)\n",
        "\n",
        "        # self.emb = nn.Embedding(in_dim, d_model)\n",
        "        self.mask_vec = nn.Parameter(torch.randn(in_dim))\n",
        "\n",
        "\n",
        "    # def forward(self, x, hc=None): # lstm [batch_size, seq, in_dim]\n",
        "    #     x = self.emb(x)\n",
        "    #     if hc is None:\n",
        "    #         h0 = torch.zeros((self.num_layers, x.size(0), self.d_model), device=device)\n",
        "    #         c0 = torch.zeros((self.num_layers, x.size(0), self.d_model), device=device)\n",
        "    #     else: h0,c0 = hc\n",
        "    #     # print(x.shape, h0.shape,c0.shape)\n",
        "    #     out, (h,c) = self.lstm(x, (h0,c0)) # [batch, seq_len, d_model], ([num_layers, batch, d_model] )\n",
        "    #     # out = out[:, -1, :] # out: (n, 128)\n",
        "    #     out = self.fc(out) # out: (n, 10)\n",
        "    #     return out, (h, c)\n",
        "\n",
        "    def reset(self, batch):\n",
        "        h0 = torch.zeros((self.num_layers, batch, self.d_model), device=device)\n",
        "        return h0\n",
        "\n",
        "    def forward(self, x, h0=None, mask=None): # rnn/gru # [batch, T, in_dim], [num_layers, batch, d_model], [batch, T] True->masked\n",
        "        # x = self.emb(x)\n",
        "        if h0==None: h0 = self.reset(x.shape[0])\n",
        "        # print(x.shape, h0.shape)\n",
        "        if mask!=None: x[mask] = self.mask_vec.to(x.dtype)\n",
        "        out, h = self.rnn(x, h0) #\n",
        "        out = out[:, -1, :] # out: (n, 128)\n",
        "        out = self.fc(out) # out: (n, 10)\n",
        "        return out, h # [batch, out_dim], [num_layers, batch, d_model]\n",
        "\n",
        "\n",
        "# hidden_size = 128\n",
        "# num_layers = 2\n",
        "# input_size = num_classes = 4\n",
        "\n",
        "# model = RNN(input_size, hidden_size, num_classes, num_layers).to(device)\n",
        "# # # print(model)\n",
        "# # print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "# batch=2\n",
        "# seq_len=3\n",
        "# x=torch.rand(batch,seq_len,input_size).to(device)\n",
        "# h=torch.rand(num_layers,batch,hidden_size).to(device)\n",
        "# mask=(torch.rand(batch,seq_len)<.5)#.expand(-1,-1,x.size(-1))\n",
        "# print(mask)\n",
        "# print(x)\n",
        "# out,h = model(x, h, mask)\n",
        "# print(out.shape)\n",
        "# print(h.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title TransformerClassifier\n",
        "import math\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class TransformerClassifier(nn.Module):\n",
        "    # def __init__(self, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, drop = 0.):\n",
        "    def __init__(self, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, drop=0):\n",
        "        super().__init__()\n",
        "        # self.embedding = nn.Embedding(ntoken, d_model)\n",
        "        self.pos_encoder = RoPE(d_model, seq_len=15, base=10000)\n",
        "        # d_hid = d_hid or d_model#*2\n",
        "        # encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, d_hid, drop, batch_first=True)\n",
        "        # self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "\n",
        "        out_dim = out_dim or d_model\n",
        "        self.lin = nn.Linear(d_model, out_dim)\n",
        "        # self.lin = nn.Linear(d_model*2, out_dim)\n",
        "        self.cls = nn.Parameter(torch.randn(1,1,d_model))\n",
        "        self.attention_pool = nn.Linear(d_model, 1, bias=False)\n",
        "        # self.out = nn.Sequential(\n",
        "        #     nn.Dropout(drop), nn.Linear(d_model, 3), nn.SiLU(),\n",
        "        #     nn.Dropout(drop), nn.Linear(3, out_dim), nn.Sigmoid()\n",
        "        # )\n",
        "\n",
        "    def forward(self, x, src_key_padding_mask = None): # [batch, seq, d_model], [batch, seq] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # batch, seq_len, d_model = x.shape\n",
        "        # x = torch.cat([self.cls.repeat(batch,1,1), x], dim=1)\n",
        "        # src_key_padding_mask = torch.cat([torch.zeros((batch, 1), dtype=torch.bool), src_key_padding_mask], dim=1)\n",
        "        x = self.pos_encoder(x)\n",
        "\n",
        "        x = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
        "        # print(\"fwd\",out.shape) # float [batch, seq_len, d_model]\n",
        "        # out = x.mean(dim=1) # average pool\n",
        "        # out = x.max(dim=1)#[0]\n",
        "\n",
        "        attn = self.attention_pool(x).squeeze(-1) # [batch, seq] # seq_pool\n",
        "        out = (torch.softmax(attn, dim=-1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, seq] @ [batch, seq, dim] -> [batch, dim]\n",
        "        # out = self.out(out) # optional (from GlobalContext) like squeeze excitation\n",
        "\n",
        "        # out = out[:, 0] # first token\n",
        "        # out = torch.cat([out, mean_pool], dim=-1)\n",
        "\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "batch, seq_len, d_model = 4,7,512\n",
        "model = TransformerModel(in_dim, d_model, d_head=4, nlayers=2, dropout=0.).to(device)\n",
        "# model = TransformerClassifier(d_model, nhead=8, nlayers=2, dropout=0.).to(device)\n",
        "x =  torch.rand(batch, seq_len, d_model)\n",
        "src_key_padding_mask = torch.stack([(torch.arange(seq_len) < seq_len - v) for v in torch.randint(seq_len, (batch,))]) # True will be ignored # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "print(src_key_padding_mask)\n",
        "out = model(x, src_key_padding_mask)\n",
        "print(out.shape)\n",
        "\n",
        "# GlobalAveragePooling1D layer, followed by a Dense layer. The final output of the transformer is produced by a softmax layer,\n",
        "# x = GlobalAveragePooling1D()(x)\n",
        "# x = Dropout(0.1)(x)\n",
        "# x = Dense(20, activation=\"relu\")(x)\n",
        "# x = Dropout(0.1)(x)\n",
        "# outputs = Dense(2, activation=\"softmax\")(x)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ODpKypTCsfIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Violet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class Violet(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.in_dim = in_dim\n",
        "        # if out_dim is None: self.out_dim = in_dim\n",
        "        if out_dim is None: out_dim = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.d_model = d_model\n",
        "        act = nn.GELU() # ReLU GELU SiLU\n",
        "\n",
        "        dim=8\n",
        "\n",
        "        # self.emb = nn.Embedding(in_dim, d_model)\n",
        "\n",
        "        self.rnn = RNN(d_model, d_model, d_model, num_layers)\n",
        "        self.rnn_out = nn.Sequential(\n",
        "            nn.Linear(d_model*2, out_dim), act,\n",
        "        )\n",
        "\n",
        "        self.transformer = TransformerClassifier(d_model, out_dim=out_dim, nhead=8, nlayers=num_layers)\n",
        "\n",
        "        # vicreg\n",
        "        dim_v = d_model * 4\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(out_dim, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v, bias=False),\n",
        "            )\n",
        "        self.d_model = d_model\n",
        "        self.sim_coeff=25. # 10.0 # 25.0 # λ\n",
        "        self.std_coeff=25. # 1.0 # 25.0 # µ\n",
        "        self.cov_coeff=1. # 25.0 # 1.0 # ν\n",
        "\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size, num_features = x.shape\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = self.sim_coeff * repr_loss + self.std_coeff * std_loss + self.cov_coeff * cov_loss\n",
        "        print(\"in vicreg \",self.sim_coeff * repr_loss.item() , self.std_coeff * std_loss.item() , self.cov_coeff * cov_loss.item())\n",
        "        # return self.std_coeff * std_loss, self.cov_coeff * cov_loss\n",
        "        # return loss\n",
        "        return repr_loss, std_loss, cov_loss\n",
        "\n",
        "    def loss(self, sx, sy):\n",
        "        vx = self.exp(sx)\n",
        "        vy = self.exp(sy)\n",
        "        loss = self.vicreg(vx,vy)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    # def forward(self, act_list):\n",
        "    def forward(self, hr, temp, heart): # [batch, 1+T]\n",
        "        batch = hr.size(0)\n",
        "        res_id, activeKilocalories, steps = hr[:,0], temp[:,0], heart[:,0]\n",
        "        enc_summary = self.enc0(torch.cat([self.calorie_emb(activeKilocalories), self.steps_emb(steps)], dim=-1)) # [batch, d_model]\n",
        "        x = self.enc1(torch.cat([self.hour_emb(hr[:,1:]/24), self.temp_emb(temp[:,1:]), self.heart_emb(heart[:,1:])], dim=-1)) # [batch, T, d_model]\n",
        "        # print('violet fwd', x.shape)\n",
        "\n",
        "        mask=(hr[:,1:]==-1)#.reshape(batch, 2, -1).flatten(end_dim=1)#.int().unsqueeze(-1) # x.masked_fill_(mask,torch.zeros(1)) ;  # [batch, T], True->mask\n",
        "        x_, _ = self.rnn(x, mask=mask) # [batch*2, d_model*3//4]\n",
        "        # x_, _ = self.rnn(x) # [batch, 1, d_model]\n",
        "        x_ = self.rnn_out(torch.cat([x_.squeeze(1), enc_summary], dim=-1))\n",
        "\n",
        "        # src_mask = (src != PAD_IDX).unsqueeze(1).unsqueeze(2).to(device) # [batch_size, 1, src_len]?\n",
        "        y_ = torch.cat([enc_summary.unsqueeze(1), x], dim=1)\n",
        "        # src_key_padding_mask=(hr[:,1:]!=-1) # True if not pad\n",
        "        src_key_padding_mask = torch.cat([torch.zeros((batch, 1), dtype=torch.bool), (hr[:,1:]==-1)], dim=1) # True->mask\n",
        "        y_ = self.transformer(y_, src_key_padding_mask=src_key_padding_mask) # [batch, d_model]\n",
        "\n",
        "        return x_, y_\n",
        "\n",
        "# violet = Violet(in_dim=16, d_model=32, num_layers=1).to(device)\n",
        "# voptim = torch.optim.AdamW(violet.parameters(), lr=1e-3)\n"
      ],
      "metadata": {
        "id": "5qwg9dG4sQ3h",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title SeqJEPA mae like\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def symlog(x): return torch.sign(x) * torch.log(torch.abs(x) + 1.0)\n",
        "def symexp(x): return torch.sign(x) * (torch.exp(torch.abs(x)) - 1.0)\n",
        "\n",
        "\n",
        "def multiblock(seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "    return target_mask\n",
        "\n",
        "# https://arxiv.org/pdf/2210.07224\n",
        "def randpatch(seq, mask_size=8, gamma=0.9): # num patches of seq, mask patch size, masking ratio\n",
        "    # mask = torch.rand(seq//mask_size)<gamma\n",
        "    length = seq//mask_size\n",
        "    g = torch.normal(gamma, std=.1, size=(1,)).clamp(.5,.9)\n",
        "    # g = gamma\n",
        "    idx = torch.randperm(length)[:int(length*g)]\n",
        "    mask = torch.zeros(length, dtype=bool)\n",
        "    mask[idx] = True\n",
        "    mask = mask.repeat_interleave(mask_size, dim=-1)\n",
        "    return mask # [seq] , True -> mask\n",
        "\n",
        "class SeqJEPA(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, nlayers=2, d_head=4):\n",
        "        super().__init__()\n",
        "        if out_dim is None: out_dim = d_model\n",
        "        self.patch_size = 32 # 4\n",
        "        self.context_encoder = TransformerModel(in_dim, d_model, out_dim=out_dim, d_head=d_head, nlayers=nlayers, dropout=0.)\n",
        "        # self.predicter = TransformerPredictor(out_dim, d_model//2, out_dim, d_head=d_head, nlayers=1, dropout=0.)\n",
        "        self.predicter = TransformerPredictor(out_dim, d_model//2, out_dim, d_head=d_head, nlayers=nlayers//2, dropout=0.)\n",
        "        import copy\n",
        "        self.target_encoder = copy.deepcopy(self.context_encoder)\n",
        "        self.target_encoder.requires_grad_(False)\n",
        "        self.classifier = nn.Linear(out_dim, 18) # 10 18\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]\n",
        "        batch, seq, dim = x.shape\n",
        "        # print(x.shape)\n",
        "        target_mask = multiblock(seq//self.patch_size, min_s=0.15, max_s=0.2, M=4).any(0) # best.2.3M4 og.15.2M4# mask out targets to be predicted # [M, seq]\n",
        "        # target_mask = randpatch(seq//self.patch_size, mask_size=8, gamma=.9) # 8.9 [seq]\n",
        "        context_mask = ~multiblock(seq//self.patch_size, min_s=0.85, max_s=1., M=1)|target_mask # og .85,1.M1 # [1, seq], True->Mask\n",
        "        target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "\n",
        "        context_indices = (~context_mask[0]).nonzero().squeeze(-1) # int idx [num_context_toks] , idx of context not masked\n",
        "        sx = self.context_encoder(x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "\n",
        "        trg_indices = target_mask.nonzero().squeeze(-1) # int idx [1, num_trg_toks] , idx of targets that are masked\n",
        "        sy_ = self.predicter(sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "        with torch.no_grad():\n",
        "            sy = self.target_encoder(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "            sy = apply_masks(sy, [trg_indices])\n",
        "\n",
        "        loss = F.mse_loss(sy.detach(), sy_)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        # sx = self.target_encoder(x)\n",
        "        sx = self.context_encoder(x)\n",
        "        out = sx.mean(dim=1)\n",
        "        return out\n",
        "\n",
        "    def classify(self, x): # [batch, T, 3]\n",
        "        sx = self.forward(x)\n",
        "        out = self.classifier(sx)\n",
        "        return out\n",
        "\n",
        "# min_s=0.15, max_s, M\n",
        "# trg.15.2M4 C.85 1\n",
        "\n",
        "seq_jepa = SeqJEPA(in_dim=3, d_model=32, out_dim=16, nlayers=6, d_head=4).to(device)#.to(torch.float)\n",
        "# optim = torch.optim.AdamW(seq_jepa.parameters(), lr=1e-3) # 1e-3?\n",
        "# optim = torch.optim.AdamW(seq_jepa.parameters(), lr=1e-3, weight_decay=0) # lr1e-3? wd0\n",
        "optim = torch.optim.AdamW([{'params': seq_jepa.context_encoder.parameters()},\n",
        "    # {'params': seq_jepa.predicter.parameters(), 'lr': 1e-2}], lr=1e-3)#, weight_decay=0) default 1e-2\n",
        "    {'params': seq_jepa.predicter.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=0) # default 1e-2\n",
        "\n",
        "print(sum(p.numel() for p in seq_jepa.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.parameters())) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.predicter.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.context_encoder.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.target_encoder.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# d_model^2 * nlayers\n",
        "\n",
        "x = torch.rand((2,1024,3), device=device)\n",
        "out = seq_jepa.loss(x)\n",
        "print(out.shape)\n",
        "# print(x.is_leaf) # T\n"
      ],
      "metadata": {
        "id": "xy4iC46Vnlog",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b8c507b-613d-42e6-c5c2-9ab6a583e7cf"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "51250\n",
            "torch.Size([])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "# classifier = Classifier(16).to(device)\n",
        "classifier = Classifier(16, 18).to(device)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n",
        "# optim = torch.optim.AdamW([{'params': seq_jepa.parameters()}, {'params': classifier.parameters(), 'lr': 1e-3}], lr=1e-3)\n"
      ],
      "metadata": {
        "id": "wjK1TVwBh2u8"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(seq_jepa.classifier.weight[0])\n",
        "# for n, p in seq_jepa.target_encoder.named_parameters():\n",
        "#     print(n,p)\n",
        "# optim.param_groups[0]['lr'] = 1e-3"
      ],
      "metadata": {
        "id": "7IACKDymhit7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34de8800-fa4b-4460-803c-3cd8dbfb6c70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7259af7-c598-45d9-d7e5-a028c50b9ab5",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "strain 0.7285206317901611\n",
            "strain 1.1406519412994385\n",
            "strain 0.5530383586883545\n",
            "strain 0.4450576603412628\n",
            "strain 0.4074890911579132\n",
            "strain 0.4067075848579407\n",
            "strain 0.33448904752731323\n",
            "strain 0.3466510474681854\n",
            "strain 0.3153342604637146\n",
            "classify 2.962646484375\n",
            "classify 2.89697265625\n",
            "classify 2.931396484375\n",
            "classify 2.92138671875\n",
            "classify 2.9921875\n",
            "classify 2.955810546875\n",
            "classify 2.913330078125\n",
            "classify 2.961181640625\n",
            "classify 2.903076171875\n",
            "0.109375\n",
            "0.046875\n",
            "0.0625\n",
            "0.015625\n",
            "strain 0.34224775433540344\n",
            "strain 0.3509915769100189\n",
            "strain 0.3694908022880554\n",
            "strain 0.35405975580215454\n",
            "strain 0.3133199214935303\n",
            "strain 0.3603319525718689\n",
            "strain 0.3668791949748993\n",
            "strain 0.27403005957603455\n",
            "strain 0.3204294741153717\n",
            "classify 3.028076171875\n",
            "classify 2.924072265625\n",
            "classify 2.984130859375\n",
            "classify 2.939697265625\n",
            "classify 3.00537109375\n",
            "classify 2.95068359375\n",
            "classify 2.963134765625\n",
            "classify 2.990234375\n",
            "classify 2.947998046875\n",
            "0.015625\n",
            "0.0625\n",
            "0.015625\n",
            "0.046875\n",
            "strain 0.2609326243400574\n",
            "strain 0.2995047867298126\n",
            "strain 0.31155744194984436\n",
            "strain 0.28372257947921753\n",
            "strain 0.2864666283130646\n",
            "strain 0.32087644934654236\n",
            "strain 0.26814237236976624\n",
            "strain 0.3013210892677307\n",
            "strain 0.2838725447654724\n",
            "classify 2.96435546875\n",
            "classify 2.9755859375\n",
            "classify 2.963623046875\n",
            "classify 2.881591796875\n",
            "classify 3.077392578125\n",
            "classify 2.990234375\n",
            "classify 3.02490234375\n",
            "classify 2.93115234375\n",
            "classify 2.977294921875\n",
            "0.0625\n",
            "0.03125\n",
            "0.03125\n",
            "0.046875\n",
            "strain 0.2853407561779022\n",
            "strain 0.24622337520122528\n",
            "strain 0.2889861762523651\n",
            "strain 0.3573779761791229\n",
            "strain 0.2906782925128937\n",
            "strain 0.24967359006404877\n",
            "strain 0.2741585969924927\n",
            "strain 0.24606196582317352\n",
            "strain 0.29038384556770325\n",
            "classify 2.9873046875\n",
            "classify 2.982177734375\n",
            "classify 2.975830078125\n",
            "classify 3.02734375\n",
            "classify 2.969970703125\n",
            "classify 2.96875\n",
            "classify 2.92626953125\n",
            "classify 2.93017578125\n",
            "classify 3.00830078125\n",
            "0.0625\n",
            "0.0625\n",
            "0.03125\n",
            "0.03125\n",
            "strain 0.2774607837200165\n",
            "strain 0.2609573006629944\n",
            "strain 0.2922440767288208\n",
            "strain 0.26112842559814453\n",
            "strain 0.24494367837905884\n",
            "strain 0.26570194959640503\n",
            "strain 0.19330887496471405\n",
            "strain 0.2623307704925537\n",
            "strain 0.2870192527770996\n",
            "classify 2.9755859375\n",
            "classify 2.885986328125\n",
            "classify 3.02880859375\n",
            "classify 3.03955078125\n",
            "classify 3.01953125\n",
            "classify 2.973388671875\n",
            "classify 2.91162109375\n",
            "classify 2.94384765625\n",
            "classify 2.926513671875\n",
            "0.046875\n",
            "0.046875\n",
            "0.0625\n",
            "0.09375\n",
            "strain 0.33792510628700256\n",
            "strain 0.3200584650039673\n",
            "strain 0.2933732569217682\n",
            "strain 0.3310893177986145\n",
            "strain 0.2020520269870758\n",
            "strain 0.352526992559433\n",
            "strain 0.2600333094596863\n",
            "strain 0.3231629729270935\n",
            "strain 0.3028540015220642\n",
            "classify 2.98974609375\n",
            "classify 2.991455078125\n",
            "classify 3.0009765625\n",
            "classify 2.991455078125\n",
            "classify 2.885009765625\n",
            "classify 3.048828125\n",
            "classify 2.888916015625\n",
            "classify 2.9892578125\n",
            "classify 3.01708984375\n",
            "0.03125\n",
            "0.046875\n",
            "0.046875\n",
            "0.09375\n",
            "strain 0.24623234570026398\n",
            "strain 0.2756774127483368\n",
            "strain 0.3821062445640564\n",
            "strain 0.3021966516971588\n",
            "strain 0.2255687713623047\n",
            "strain 0.3145839273929596\n",
            "strain 0.2665881812572479\n",
            "strain 0.29351168870925903\n",
            "strain 0.298500657081604\n",
            "classify 2.996337890625\n",
            "classify 2.952392578125\n",
            "classify 3.00634765625\n",
            "classify 2.9251708984375\n",
            "classify 2.9541015625\n",
            "classify 3.04345703125\n",
            "classify 3.0594482421875\n",
            "classify 2.98681640625\n",
            "classify 3.04736328125\n",
            "0.078125\n",
            "0.078125\n",
            "0.078125\n",
            "0.078125\n",
            "strain 0.2210227996110916\n",
            "strain 0.2532693147659302\n",
            "strain 0.26946407556533813\n",
            "strain 0.2959176003932953\n",
            "strain 0.2878901958465576\n",
            "strain 0.27683836221694946\n",
            "strain 0.27720147371292114\n",
            "strain 0.2837485373020172\n",
            "strain 0.2562776207923889\n",
            "classify 3.021484375\n",
            "classify 3.0672607421875\n",
            "classify 3.014892578125\n",
            "classify 3.0430908203125\n",
            "classify 3.0130615234375\n",
            "classify 2.875732421875\n",
            "classify 2.9835205078125\n",
            "classify 2.9541015625\n",
            "classify 3.06787109375\n",
            "0.078125\n",
            "0.078125\n",
            "0.078125\n",
            "0.171875\n",
            "strain 0.2068488597869873\n",
            "strain 0.2699626684188843\n",
            "strain 0.29236122965812683\n",
            "strain 0.24109378457069397\n",
            "strain 0.3000081181526184\n",
            "strain 0.2673676013946533\n",
            "strain 0.29562440514564514\n",
            "strain 0.26240217685699463\n",
            "strain 0.20827238261699677\n",
            "classify 2.9527587890625\n",
            "classify 3.0050048828125\n",
            "classify 2.897216796875\n",
            "classify 3.08154296875\n",
            "classify 2.92919921875\n",
            "classify 2.9649658203125\n",
            "classify 2.951171875\n",
            "classify 2.9423828125\n",
            "classify 2.9212646484375\n",
            "0.078125\n",
            "0.109375\n",
            "0.046875\n",
            "0.078125\n",
            "strain 0.2547648549079895\n",
            "strain 0.3112925589084625\n",
            "strain 0.2778766453266144\n",
            "strain 0.2745804488658905\n",
            "strain 0.2831968665122986\n",
            "strain 0.25429704785346985\n",
            "strain 0.19862811267375946\n",
            "strain 0.28056949377059937\n",
            "strain 0.30359214544296265\n",
            "classify 3.0067138671875\n",
            "classify 2.9642333984375\n",
            "classify 3.02978515625\n",
            "classify 2.9725341796875\n",
            "classify 2.9915771484375\n",
            "classify 2.9525146484375\n",
            "classify 2.972412109375\n",
            "classify 2.913818359375\n",
            "classify 3.097412109375\n",
            "0.0625\n",
            "0.09375\n",
            "0.15625\n",
            "0.0625\n",
            "strain 0.29677319526672363\n",
            "strain 0.3007972240447998\n",
            "strain 0.1911008358001709\n",
            "strain 0.25080227851867676\n",
            "strain 0.31125912070274353\n",
            "strain 0.2674936056137085\n",
            "strain 0.3171505928039551\n",
            "strain 0.23018504679203033\n",
            "strain 0.2824433743953705\n",
            "classify 2.90478515625\n",
            "classify 2.9561767578125\n",
            "classify 2.9619140625\n",
            "classify 3.044189453125\n",
            "classify 2.97314453125\n",
            "classify 2.99658203125\n",
            "classify 2.8731689453125\n",
            "classify 3.10400390625\n",
            "classify 2.942138671875\n",
            "0.125\n",
            "0.078125\n",
            "0.0625\n",
            "0.0625\n",
            "strain 0.15764154493808746\n",
            "strain 0.29126936197280884\n",
            "strain 0.30215269327163696\n",
            "strain 0.29707425832748413\n",
            "strain 0.2505001723766327\n",
            "strain 0.2627376317977905\n",
            "strain 0.17391614615917206\n",
            "strain 0.26098397374153137\n",
            "strain 0.23539257049560547\n",
            "classify 2.9349365234375\n",
            "classify 3.00244140625\n",
            "classify 2.9036865234375\n",
            "classify 3.01025390625\n",
            "classify 2.9324951171875\n",
            "classify 2.96630859375\n",
            "classify 2.966796875\n",
            "classify 2.984375\n",
            "classify 3.00341796875\n",
            "0.09375\n",
            "0.03125\n",
            "0.09375\n",
            "0.078125\n",
            "strain 0.3435058295726776\n",
            "strain 0.3231871724128723\n",
            "strain 0.3367132842540741\n",
            "strain 0.29328039288520813\n",
            "strain 0.2890998125076294\n",
            "strain 0.1503804326057434\n",
            "strain 0.189248725771904\n",
            "strain 0.1572151482105255\n",
            "strain 0.2705496847629547\n",
            "classify 3.044677734375\n",
            "classify 2.9307861328125\n",
            "classify 2.9844970703125\n",
            "classify 3.0458984375\n",
            "classify 3.079833984375\n",
            "classify 2.878662109375\n",
            "classify 3.058349609375\n",
            "classify 3.056884765625\n",
            "classify 2.9775390625\n",
            "0.09375\n",
            "0.0625\n",
            "0.0625\n",
            "0.078125\n",
            "strain 0.28846868872642517\n",
            "strain 0.29785090684890747\n",
            "strain 0.23133593797683716\n",
            "strain 0.29312026500701904\n",
            "strain 0.23214593529701233\n",
            "strain 0.34478697180747986\n",
            "strain 0.2633311450481415\n",
            "strain 0.2857368290424347\n",
            "strain 0.26207271218299866\n",
            "classify 2.91259765625\n",
            "classify 2.877685546875\n",
            "classify 2.9793701171875\n",
            "classify 3.0360107421875\n",
            "classify 2.996826171875\n",
            "classify 2.9158935546875\n",
            "classify 3.04638671875\n",
            "classify 2.9512939453125\n",
            "classify 3.10693359375\n",
            "0.03125\n",
            "0.09375\n",
            "0.078125\n",
            "0.0625\n",
            "strain 0.3059341013431549\n",
            "strain 0.2284218668937683\n",
            "strain 0.24139165878295898\n",
            "strain 0.23190496861934662\n",
            "strain 0.29455748200416565\n",
            "strain 0.2915779650211334\n",
            "strain 0.27202701568603516\n",
            "strain 0.34964242577552795\n",
            "strain 0.2555175721645355\n",
            "classify 3.017578125\n",
            "classify 2.9725341796875\n",
            "classify 3.0360107421875\n",
            "classify 3.017333984375\n",
            "classify 3.021484375\n",
            "classify 2.8868408203125\n",
            "classify 2.99951171875\n",
            "classify 2.85302734375\n",
            "classify 2.98095703125\n",
            "0.09375\n",
            "0.125\n",
            "0.046875\n",
            "0.109375\n",
            "strain 0.3409101366996765\n",
            "strain 0.325833797454834\n",
            "strain 0.2655141055583954\n",
            "strain 0.27183443307876587\n",
            "strain 0.2598864436149597\n",
            "strain 0.3070334792137146\n",
            "strain 0.34217819571495056\n",
            "strain 0.2816035747528076\n",
            "strain 0.2914752662181854\n",
            "classify 3.10791015625\n",
            "classify 2.9251708984375\n",
            "classify 2.9061279296875\n",
            "classify 2.8599853515625\n",
            "classify 3.003662109375\n",
            "classify 2.92626953125\n",
            "classify 3.035888671875\n",
            "classify 2.974609375\n",
            "classify 3.0067138671875\n",
            "0.046875\n",
            "0.046875\n",
            "0.140625\n",
            "0.140625\n",
            "strain 0.23823364078998566\n",
            "strain 0.308315634727478\n",
            "strain 0.30297359824180603\n",
            "strain 0.24344861507415771\n",
            "strain 0.3064596652984619\n",
            "strain 0.29234930872917175\n",
            "strain 0.20343457162380219\n",
            "strain 0.22022397816181183\n",
            "strain 0.3008377254009247\n",
            "classify 3.0230712890625\n",
            "classify 2.966796875\n",
            "classify 2.9288330078125\n",
            "classify 2.9908447265625\n",
            "classify 3.069580078125\n",
            "classify 2.909912109375\n",
            "classify 2.9744873046875\n",
            "classify 2.9971923828125\n",
            "classify 2.9130859375\n",
            "0.046875\n",
            "0.09375\n",
            "0.125\n",
            "0.15625\n",
            "strain 0.3100590407848358\n",
            "strain 0.3283795714378357\n",
            "strain 0.28777649998664856\n",
            "strain 0.17276695370674133\n",
            "strain 0.30197134613990784\n",
            "strain 0.3398241102695465\n",
            "strain 0.287884384393692\n",
            "strain 0.3157544732093811\n",
            "strain 0.2269904762506485\n",
            "classify 2.9456787109375\n",
            "classify 2.9925537109375\n",
            "classify 2.9532470703125\n",
            "classify 2.978271484375\n",
            "classify 2.958740234375\n",
            "classify 3.063720703125\n",
            "classify 3.048583984375\n",
            "classify 2.960205078125\n",
            "classify 2.914306640625\n",
            "0.0625\n",
            "0.09375\n",
            "0.125\n",
            "0.140625\n",
            "strain 0.2545439600944519\n",
            "strain 0.25185534358024597\n",
            "strain 0.2090509980916977\n",
            "strain 0.269866406917572\n",
            "strain 0.2600499093532562\n",
            "strain 0.1950225830078125\n",
            "strain 0.23705288767814636\n",
            "strain 0.24236029386520386\n",
            "strain 0.2644694447517395\n",
            "classify 2.820068359375\n",
            "classify 3.1131591796875\n",
            "classify 2.9677734375\n",
            "classify 2.98046875\n",
            "classify 2.953125\n",
            "classify 2.9437255859375\n",
            "classify 2.9591064453125\n",
            "classify 3.048828125\n",
            "classify 2.876953125\n",
            "0.078125\n",
            "0.078125\n",
            "0.078125\n",
            "0.09375\n",
            "strain 0.2911198139190674\n",
            "strain 0.18110111355781555\n",
            "strain 0.28397393226623535\n",
            "strain 0.13203147053718567\n",
            "strain 0.27078160643577576\n",
            "strain 0.21429243683815002\n",
            "strain 0.3108785152435303\n",
            "strain 0.2943115532398224\n",
            "strain 0.29890838265419006\n",
            "classify 3.0113525390625\n",
            "classify 3.015869140625\n",
            "classify 2.886962890625\n",
            "classify 2.9649658203125\n",
            "classify 3.010986328125\n",
            "classify 3.0047607421875\n",
            "classify 2.9775390625\n",
            "classify 2.999755859375\n",
            "classify 3.0006103515625\n",
            "0.078125\n",
            "0.09375\n",
            "0.09375\n",
            "0.125\n",
            "strain 0.2747098207473755\n",
            "strain 0.17306941747665405\n",
            "strain 0.30113452672958374\n",
            "strain 0.32522958517074585\n",
            "strain 0.16260623931884766\n",
            "strain 0.2735380232334137\n",
            "strain 0.3031417429447174\n",
            "strain 0.28131580352783203\n",
            "strain 0.22866159677505493\n",
            "classify 2.998291015625\n",
            "classify 3.0478515625\n",
            "classify 3.1055908203125\n",
            "classify 3.01171875\n",
            "classify 2.897705078125\n",
            "classify 3.016845703125\n",
            "classify 2.9993896484375\n",
            "classify 2.95458984375\n",
            "classify 2.9608154296875\n",
            "0.203125\n",
            "0.015625\n",
            "0.078125\n",
            "0.109375\n",
            "strain 0.26090848445892334\n",
            "strain 0.2893994152545929\n",
            "strain 0.20938114821910858\n",
            "strain 0.2271794229745865\n",
            "strain 0.23643822968006134\n",
            "strain 0.3022075891494751\n",
            "strain 0.3212123513221741\n",
            "strain 0.2904517650604248\n",
            "strain 0.13238170742988586\n",
            "classify 3.0068359375\n",
            "classify 2.9427490234375\n",
            "classify 3.0400390625\n",
            "classify 3.03955078125\n",
            "classify 3.0025634765625\n",
            "classify 2.953369140625\n",
            "classify 2.976318359375\n",
            "classify 3.077880859375\n",
            "classify 2.9913330078125\n",
            "0.203125\n",
            "0.046875\n",
            "0.109375\n",
            "0.09375\n",
            "strain 0.14297135174274445\n",
            "strain 0.26903825998306274\n",
            "strain 0.3399457335472107\n",
            "strain 0.3015328645706177\n",
            "strain 0.2943691611289978\n",
            "strain 0.31391990184783936\n",
            "strain 0.27576565742492676\n",
            "strain 0.2711279094219208\n",
            "strain 0.1909579634666443\n",
            "classify 2.90625\n",
            "classify 3.0899658203125\n",
            "classify 2.9935302734375\n",
            "classify 3.0579833984375\n",
            "classify 3.0677490234375\n",
            "classify 3.044189453125\n",
            "classify 2.928466796875\n",
            "classify 3.0169677734375\n",
            "classify 3.0316162109375\n",
            "0.09375\n",
            "0.125\n",
            "0.0625\n",
            "0.09375\n",
            "strain 0.3058216869831085\n",
            "strain 0.23313893377780914\n",
            "strain 0.2734489440917969\n",
            "strain 0.23641055822372437\n",
            "strain 0.2030808925628662\n",
            "strain 0.2729989290237427\n",
            "strain 0.19050031900405884\n",
            "strain 0.3270408809185028\n",
            "strain 0.2095535397529602\n",
            "classify 2.9071044921875\n",
            "classify 2.984375\n",
            "classify 2.9522705078125\n",
            "classify 3.036865234375\n",
            "classify 2.9844970703125\n",
            "classify 3.0299072265625\n",
            "classify 2.9326171875\n",
            "classify 3.0\n",
            "classify 3.0460205078125\n",
            "0.109375\n",
            "0.09375\n",
            "0.078125\n",
            "0.09375\n",
            "strain 0.19444017112255096\n",
            "strain 0.2690337002277374\n",
            "strain 0.36367127299308777\n",
            "strain 0.31613320112228394\n",
            "strain 0.3190000057220459\n",
            "strain 0.3201046884059906\n",
            "strain 0.17024482786655426\n",
            "strain 0.12226267904043198\n",
            "strain 0.32918304204940796\n",
            "classify 2.93896484375\n",
            "classify 2.900146484375\n",
            "classify 2.9951171875\n",
            "classify 2.916748046875\n",
            "classify 2.9080810546875\n",
            "classify 2.96484375\n",
            "classify 2.8951416015625\n",
            "classify 2.97802734375\n",
            "classify 3.0018310546875\n",
            "0.046875\n",
            "0.078125\n",
            "0.15625\n",
            "0.109375\n",
            "strain 0.2812619209289551\n",
            "strain 0.31361252069473267\n",
            "strain 0.2566477954387665\n",
            "strain 0.2155640721321106\n",
            "strain 0.27872195839881897\n",
            "strain 0.3306032419204712\n",
            "strain 0.2767278850078583\n",
            "strain 0.26268041133880615\n",
            "strain 0.2224261313676834\n",
            "classify 2.861083984375\n",
            "classify 3.0272216796875\n",
            "classify 3.0595703125\n",
            "classify 2.9232177734375\n",
            "classify 3.01513671875\n",
            "classify 2.8974609375\n",
            "classify 2.9095458984375\n",
            "classify 2.9730224609375\n",
            "classify 2.8572998046875\n",
            "0.09375\n",
            "0.078125\n",
            "0.171875\n",
            "0.0625\n",
            "strain 0.24521012604236603\n",
            "strain 0.3429611623287201\n",
            "strain 0.2629062533378601\n",
            "strain 0.1883867233991623\n",
            "strain 0.2815486490726471\n",
            "strain 0.17665040493011475\n",
            "strain 0.2208838164806366\n",
            "strain 0.30917948484420776\n",
            "strain 0.2894367575645447\n",
            "classify 3.0958251953125\n",
            "classify 2.904296875\n",
            "classify 3.0552978515625\n",
            "classify 2.8970947265625\n",
            "classify 2.9068603515625\n",
            "classify 2.8248291015625\n",
            "classify 2.982177734375\n",
            "classify 2.945556640625\n",
            "classify 2.908935546875\n",
            "0.078125\n",
            "0.046875\n",
            "0.0625\n",
            "0.078125\n",
            "strain 0.3130294978618622\n",
            "strain 0.23015598952770233\n",
            "strain 0.28899744153022766\n",
            "strain 0.19588258862495422\n",
            "strain 0.293428897857666\n",
            "strain 0.23929405212402344\n",
            "strain 0.2343691736459732\n",
            "strain 0.2706870436668396\n",
            "strain 0.2624700367450714\n",
            "classify 2.949462890625\n",
            "classify 3.0489501953125\n",
            "classify 3.0712890625\n",
            "classify 3.0279541015625\n",
            "classify 2.893310546875\n",
            "classify 2.982666015625\n",
            "classify 3.041015625\n",
            "classify 2.8673095703125\n",
            "classify 2.848876953125\n",
            "0.078125\n",
            "0.09375\n",
            "0.078125\n",
            "0.078125\n",
            "strain 0.3016861379146576\n",
            "strain 0.19450950622558594\n",
            "strain 0.2582389712333679\n",
            "strain 0.29678434133529663\n",
            "strain 0.23121535778045654\n",
            "strain 0.16404573619365692\n",
            "strain 0.1750796139240265\n",
            "strain 0.2211836576461792\n",
            "strain 0.2822013795375824\n",
            "classify 2.9100341796875\n",
            "classify 2.8653564453125\n",
            "classify 2.9827880859375\n",
            "classify 2.9996337890625\n",
            "classify 3.0474853515625\n",
            "classify 2.9573974609375\n",
            "classify 2.998291015625\n",
            "classify 3.0067138671875\n",
            "classify 2.88330078125\n",
            "0.046875\n",
            "0.09375\n",
            "0.03125\n",
            "0.109375\n",
            "strain 0.2425631284713745\n",
            "strain 0.20550690591335297\n",
            "strain 0.17935985326766968\n",
            "strain 0.18310979008674622\n",
            "strain 0.1086813434958458\n",
            "strain 0.3404114246368408\n",
            "strain 0.28732794523239136\n",
            "strain 0.34643152356147766\n",
            "strain 0.13054771721363068\n",
            "classify 2.9197998046875\n",
            "classify 2.9912109375\n",
            "classify 3.0482177734375\n",
            "classify 2.9306640625\n",
            "classify 3.0379638671875\n",
            "classify 2.93505859375\n",
            "classify 2.92431640625\n",
            "classify 2.9771728515625\n",
            "classify 2.93212890625\n",
            "0.078125\n",
            "0.078125\n",
            "0.078125\n",
            "0.09375\n"
          ]
        }
      ],
      "source": [
        "# @title strain ctrain test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "def strain(model, dataloader, optim, scheduler=None): # train function with automatic mixed precision\n",
        "# def strain(model, train_iter, optim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.train()\n",
        "    for i, (x, _) in enumerate(dataloader):\n",
        "        # x = x.to(device).flatten(2).transpose(-2,-1)#.to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "        x = x[...,1:].to(device).to(torch.bfloat16)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            loss = model.loss(x)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # total_norm = 0\n",
        "        # for p in model.parameters(): total_norm += p.grad.data.norm(2).item() ** 2\n",
        "        # total_norm = total_norm**.5\n",
        "        # print('total_norm', total_norm)\n",
        "        # for p in list(filter(lambda p: p.grad is not None, model.parameters())):\n",
        "        #     print(p.grad.data.norm(2).item())\n",
        "        # print(\"max grad norm\", max([p.grad.data.norm(2).item() for p in list(filter(lambda p: p.grad is not None, model.parameters()))]))\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 5) # 0.5\n",
        "\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            m=0.999 # 0.99 m = next(momentum_scheduler)\n",
        "            norms=[]\n",
        "            for param_q, param_k in zip(model.context_encoder.parameters(), model.target_encoder.parameters()):\n",
        "                # norm = ((param_k.data-param_q.detach().data)**2).sum()**.5\n",
        "                # # # print(param_k.data.shape, norm)\n",
        "                # norms.append(norm.item())\n",
        "                # # if norm>.01:\n",
        "                param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "            # print(norms)\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        print(\"strain\",loss.item())\n",
        "        # for param in seq_jepa.context_encoder.cls: print(param.data)\n",
        "        # for param in seq_jepa.predicter.cls: print(param.data)\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=50: break\n",
        "\n",
        "\n",
        "def ctrain(model, classifier, dataloader, coptim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.eval()\n",
        "    classifier.train()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        # x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        # x = x.flatten(2).transpose(-2,-1).to(torch.bfloat16)\n",
        "        x, y = x[...,1:].to(device).to(torch.bfloat16), y[1].to(device) # [batch, ] # (id, activity)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            with torch.no_grad():\n",
        "                sx = model(x).detach()\n",
        "            # print(sx[0][0])\n",
        "            y_ = classifier(sx)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "        # print(classifier.classifier.weight[0])\n",
        "        # print(y_.shape, y.shape)\n",
        "        # print(loss)\n",
        "        coptim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 5) # .5\n",
        "        scaler.step(coptim)\n",
        "        scaler.update()\n",
        "        print(\"classify\",loss.item())\n",
        "        try: wandb.log({\"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        # x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        # x = x.flatten(2).transpose(-2,-1)#.to(torch.bfloat16)\n",
        "        x, y = x[...,1:].to(device).to(torch.float), y[1].to(device) # [batch, ] # (id, activity)\n",
        "        with torch.no_grad():\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        print(correct/len(y))\n",
        "        try: wandb.log({\"correct\": correct/len(y)})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "# for i in range(1):\n",
        "for i in range(1000):\n",
        "    # train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    # test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    np.random.shuffle(train_indices); np.random.shuffle(val_indices)\n",
        "    train_sampler, valid_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(val_indices)\n",
        "    # batch_size = 64 #512\n",
        "    train_loader = DataLoader(train_data, sampler=train_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "    test_loader = DataLoader(train_data, sampler=valid_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True)\n",
        "\n",
        "    strain(seq_jepa, train_loader, optim)\n",
        "    ctrain(seq_jepa, classifier, train_loader, coptim)\n",
        "    test(seq_jepa, classifier, test_loader)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "2Nd-sGe6Ku4S",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "outputId": "801fbed0-7f41-4200-8477-f0d086269d45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>▆▇█▄▇▅▆▄▅▅▅▄▆▃▂▅▄▃▄▃▃▄▂▂▂▃▃▂▁▂▂▃▃▄▂▄▁▂▃▃</td></tr><tr><td>correct</td><td>▂▁▂▃▂▃▁▁▅▅▄▄▂▅▇▂▅▅▃▄█▄▇▇▇▄▄█▄▇▅▅▇▄▅▇▂▄▅▇</td></tr><tr><td>loss</td><td>▁▁▁▁▁▁▁▂▁▂▁▁▁▁▁▃▁▃▄▂▃▁▃▁▂▁▂▁█▂▃▁▅█▄▃▂▁▁▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>2.73706</td></tr><tr><td>correct</td><td>0.0625</td></tr><tr><td>loss</td><td>1.24412</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">proud-donkey-112</strong> at: <a href='https://wandb.ai/bobdole/SeqJEPA/runs/cvd28s2z' target=\"_blank\">https://wandb.ai/bobdole/SeqJEPA/runs/cvd28s2z</a><br> View project at: <a href='https://wandb.ai/bobdole/SeqJEPA' target=\"_blank\">https://wandb.ai/bobdole/SeqJEPA</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250320_023858-cvd28s2z/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250320_025408-78hiznp6</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/SeqJEPA/runs/78hiznp6' target=\"_blank\">stilted-oath-113</a></strong> to <a href='https://wandb.ai/bobdole/SeqJEPA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/SeqJEPA' target=\"_blank\">https://wandb.ai/bobdole/SeqJEPA</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/SeqJEPA/runs/78hiznp6' target=\"_blank\">https://wandb.ai/bobdole/SeqJEPA/runs/78hiznp6</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "!pip install -q wandb\n",
        "import wandb # https://docs.wandb.ai/quickstart\n",
        "wandb.login(key='487a2109e55dce4e13fc70681781de9f50f27be7')\n",
        "try: run.finish()\n",
        "except NameError: pass\n",
        "run = wandb.init(project=\"SeqJEPA\", config={\"model\": \"res18\",})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# @title supervised train test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "def strain(model, dataloader, optim, scheduler=None): # train function with automatic mixed precision\n",
        "# def strain(model, train_iter, optim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.train()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        # x = x.to(device).flatten(2).transpose(-2,-1)#.to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "        # y = y.to(device)\n",
        "        x, y = x[...,1:].to(device).to(torch.bfloat16), y[1].to(device) # [batch, ] # (id, activity)\n",
        "\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            y_ = model.classify(x)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        # print(loss)\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # for p in list(filter(lambda p: p.grad is not None, model.parameters())):\n",
        "        #     print(p.grad.data.norm(2).item())\n",
        "        # print(\"max grad norm\", max([p.grad.data.norm(2).item() for p in list(filter(lambda p: p.grad is not None, model.parameters()))]))\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 10) # 0.5\n",
        "\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        print(\"strain\",loss.item())\n",
        "        # for param in seq_jepa.context_encoder.cls: print(param.data)\n",
        "        # for param in seq_jepa.predicter.cls: print(param.data)\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=50: break\n",
        "\n",
        "\n",
        "def test(model, dataloader):\n",
        "    model.eval()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        # x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        # img = img.flatten(2).transpose(-2,-1)#.to(torch.bfloat16)\n",
        "        x, y = x[...,1:].to(device).to(torch.float), y[1].to(device) # [batch, ] # (id, activity)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            y_ = model.classify(x)\n",
        "\n",
        "        loss = F.cross_entropy(y_, y)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        print(correct/len(y))\n",
        "        try: wandb.log({\"correct\": correct/len(y), \"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "for i in range(1000):\n",
        "    # train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    # test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    np.random.shuffle(train_indices); np.random.shuffle(val_indices)\n",
        "    train_sampler, valid_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(val_indices)\n",
        "    # batch_size = 64 #512\n",
        "    train_loader = DataLoader(train_data, sampler=train_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "    test_loader = DataLoader(train_data, sampler=valid_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True)\n",
        "\n",
        "    strain(seq_jepa, train_loader, optim)\n",
        "    test(seq_jepa, test_loader)\n",
        "\n"
      ],
      "metadata": {
        "id": "4J2ahp3wmqcg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c60ad9fe-0323-4109-ea1b-cb5251b74f62",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "strain 2.90185546875\n",
            "strain 2.883544921875\n",
            "strain 2.81884765625\n",
            "strain 2.8778076171875\n",
            "strain 2.8040771484375\n",
            "strain 2.7733154296875\n",
            "strain 2.826171875\n",
            "strain 2.68115234375\n",
            "strain 2.6766357421875\n",
            "0.171875\n",
            "0.25\n",
            "0.328125\n",
            "0.21875\n",
            "strain 2.75244140625\n",
            "strain 2.66162109375\n",
            "strain 2.621826171875\n",
            "strain 2.5975341796875\n",
            "strain 2.5076904296875\n",
            "strain 2.47119140625\n",
            "strain 2.539306640625\n",
            "strain 2.5870361328125\n",
            "strain 2.4456787109375\n",
            "0.109375\n",
            "0.15625\n",
            "0.265625\n",
            "0.15625\n",
            "strain 2.4534912109375\n",
            "strain 2.4541015625\n",
            "strain 2.4285888671875\n",
            "strain 2.6151123046875\n",
            "strain 2.3382568359375\n",
            "strain 2.339599609375\n",
            "strain 2.4173583984375\n",
            "strain 2.37158203125\n",
            "strain 2.36669921875\n",
            "0.234375\n",
            "0.265625\n",
            "0.328125\n",
            "0.34375\n",
            "strain 2.35546875\n",
            "strain 2.1541748046875\n",
            "strain 2.2535400390625\n",
            "strain 2.36083984375\n",
            "strain 2.36083984375\n",
            "strain 2.169921875\n",
            "strain 2.3350830078125\n",
            "strain 2.233154296875\n",
            "strain 2.16357421875\n",
            "0.328125\n",
            "0.34375\n",
            "0.3125\n",
            "0.21875\n",
            "strain 2.1015625\n",
            "strain 2.0985107421875\n",
            "strain 2.0369873046875\n",
            "strain 2.0947265625\n",
            "strain 2.1025390625\n",
            "strain 2.110595703125\n",
            "strain 2.21575927734375\n",
            "strain 2.21783447265625\n",
            "strain 2.15069580078125\n",
            "0.296875\n",
            "0.34375\n",
            "0.296875\n",
            "0.375\n",
            "strain 1.9632568359375\n",
            "strain 2.10943603515625\n",
            "strain 2.16656494140625\n",
            "strain 1.98907470703125\n",
            "strain 1.950927734375\n",
            "strain 2.07073974609375\n",
            "strain 2.02618408203125\n",
            "strain 2.06048583984375\n",
            "strain 2.0902099609375\n",
            "0.328125\n",
            "0.359375\n",
            "0.359375\n",
            "0.390625\n",
            "strain 1.9324951171875\n",
            "strain 2.08404541015625\n",
            "strain 1.880859375\n",
            "strain 1.84869384765625\n",
            "strain 1.9844970703125\n",
            "strain 2.03173828125\n",
            "strain 1.9237060546875\n",
            "strain 1.84161376953125\n",
            "strain 1.96368408203125\n",
            "0.3125\n",
            "0.296875\n",
            "0.359375\n",
            "0.390625\n",
            "strain 2.07452392578125\n",
            "strain 1.93731689453125\n",
            "strain 1.9327392578125\n",
            "strain 1.837158203125\n",
            "strain 1.86578369140625\n",
            "strain 1.8359375\n",
            "strain 1.72039794921875\n",
            "strain 1.95550537109375\n",
            "strain 1.8167724609375\n",
            "0.453125\n",
            "0.390625\n",
            "0.40625\n",
            "0.421875\n",
            "strain 1.86712646484375\n",
            "strain 1.82708740234375\n",
            "strain 1.7467041015625\n",
            "strain 1.71551513671875\n",
            "strain 1.71826171875\n",
            "strain 1.77642822265625\n",
            "strain 1.760101318359375\n",
            "strain 1.5123291015625\n",
            "strain 1.91802978515625\n",
            "0.375\n",
            "0.4375\n",
            "0.390625\n",
            "0.390625\n",
            "strain 1.63751220703125\n",
            "strain 1.64141845703125\n",
            "strain 1.535888671875\n",
            "strain 1.745208740234375\n",
            "strain 1.6146240234375\n",
            "strain 1.8463134765625\n",
            "strain 1.481781005859375\n",
            "strain 1.598785400390625\n",
            "strain 1.81884765625\n",
            "0.515625\n",
            "0.453125\n",
            "0.4375\n",
            "0.328125\n",
            "strain 1.73870849609375\n",
            "strain 1.69049072265625\n",
            "strain 1.479949951171875\n",
            "strain 1.574737548828125\n",
            "strain 1.696075439453125\n",
            "strain 1.6357421875\n",
            "strain 1.52105712890625\n",
            "strain 1.46392822265625\n",
            "strain 1.573333740234375\n",
            "0.421875\n",
            "0.578125\n",
            "0.578125\n",
            "0.578125\n",
            "strain 1.70465087890625\n",
            "strain 1.520050048828125\n",
            "strain 1.5335693359375\n",
            "strain 1.58331298828125\n",
            "strain 1.63018798828125\n",
            "strain 1.604400634765625\n",
            "strain 1.564300537109375\n",
            "strain 1.4930419921875\n",
            "strain 1.484100341796875\n",
            "0.515625\n",
            "0.4375\n",
            "0.546875\n",
            "0.40625\n",
            "strain 1.579254150390625\n",
            "strain 1.613311767578125\n",
            "strain 1.71881103515625\n",
            "strain 1.3773193359375\n",
            "strain 1.54364013671875\n",
            "strain 1.8370513916015625\n",
            "strain 1.59002685546875\n",
            "strain 1.391387939453125\n",
            "strain 1.37506103515625\n",
            "0.546875\n",
            "0.546875\n",
            "0.453125\n",
            "0.453125\n",
            "strain 1.470916748046875\n",
            "strain 1.553009033203125\n",
            "strain 1.42242431640625\n",
            "strain 1.53173828125\n",
            "strain 1.574859619140625\n",
            "strain 1.510223388671875\n",
            "strain 1.561065673828125\n",
            "strain 1.48974609375\n",
            "strain 1.436370849609375\n",
            "0.53125\n",
            "0.53125\n",
            "0.5\n",
            "0.5\n",
            "strain 1.493865966796875\n",
            "strain 1.5069580078125\n",
            "strain 1.235992431640625\n",
            "strain 1.2886962890625\n",
            "strain 1.389892578125\n",
            "strain 1.33154296875\n",
            "strain 1.62139892578125\n",
            "strain 1.360443115234375\n",
            "strain 1.3087615966796875\n",
            "0.625\n",
            "0.484375\n",
            "0.5\n",
            "0.46875\n",
            "strain 1.25067138671875\n",
            "strain 1.355316162109375\n",
            "strain 1.5737762451171875\n",
            "strain 1.209320068359375\n",
            "strain 1.604248046875\n",
            "strain 1.208709716796875\n",
            "strain 1.20904541015625\n",
            "strain 1.2989349365234375\n",
            "strain 1.3617706298828125\n",
            "0.53125\n",
            "0.59375\n",
            "0.609375\n",
            "0.515625\n",
            "strain 1.2132568359375\n",
            "strain 1.1857452392578125\n",
            "strain 1.3445587158203125\n",
            "strain 1.3484039306640625\n",
            "strain 1.4256744384765625\n",
            "strain 1.42620849609375\n",
            "strain 1.2280731201171875\n",
            "strain 1.48388671875\n",
            "strain 1.062774658203125\n",
            "0.5625\n",
            "0.5\n",
            "0.515625\n",
            "0.53125\n",
            "strain 1.3317108154296875\n",
            "strain 1.532073974609375\n",
            "strain 1.2469482421875\n",
            "strain 1.28338623046875\n",
            "strain 1.07763671875\n",
            "strain 1.275146484375\n",
            "strain 1.3449859619140625\n",
            "strain 1.319915771484375\n",
            "strain 1.3854827880859375\n",
            "0.53125\n",
            "0.59375\n",
            "0.578125\n",
            "0.546875\n",
            "strain 1.0335845947265625\n",
            "strain 1.1907196044921875\n",
            "strain 1.3814773559570312\n",
            "strain 1.2965927124023438\n",
            "strain 1.2859268188476562\n",
            "strain 1.3097686767578125\n",
            "strain 1.4043807983398438\n",
            "strain 1.1981582641601562\n",
            "strain 1.662506103515625\n",
            "0.515625\n",
            "0.515625\n",
            "0.5\n",
            "0.484375\n",
            "strain 1.307861328125\n",
            "strain 1.1993560791015625\n",
            "strain 1.653594970703125\n",
            "strain 1.2784271240234375\n",
            "strain 1.45574951171875\n",
            "strain 1.185211181640625\n",
            "strain 1.4702529907226562\n",
            "strain 1.246429443359375\n",
            "strain 1.2350845336914062\n",
            "0.609375\n",
            "0.65625\n",
            "0.625\n",
            "0.515625\n",
            "strain 1.091796875\n",
            "strain 1.1798095703125\n",
            "strain 1.4531784057617188\n",
            "strain 1.159637451171875\n",
            "strain 1.2174224853515625\n",
            "strain 1.1980514526367188\n",
            "strain 1.0938949584960938\n",
            "strain 1.1147994995117188\n",
            "strain 1.1536483764648438\n",
            "0.609375\n",
            "0.5625\n",
            "0.5625\n",
            "0.5625\n",
            "strain 0.9462203979492188\n",
            "strain 1.31671142578125\n",
            "strain 1.2225418090820312\n",
            "strain 1.246429443359375\n",
            "strain 1.06585693359375\n",
            "strain 1.2159194946289062\n",
            "strain 1.1869354248046875\n",
            "strain 1.1645431518554688\n",
            "strain 1.52386474609375\n",
            "0.59375\n",
            "0.65625\n",
            "0.640625\n",
            "0.484375\n",
            "strain 1.007293701171875\n",
            "strain 1.090606689453125\n",
            "strain 1.0631637573242188\n",
            "strain 1.2292709350585938\n",
            "strain 1.0807876586914062\n",
            "strain 0.9835739135742188\n",
            "strain 1.2775192260742188\n",
            "strain 1.2072830200195312\n",
            "strain 1.2879180908203125\n",
            "0.5\n",
            "0.578125\n",
            "0.59375\n",
            "0.625\n",
            "strain 1.0377960205078125\n",
            "strain 1.0224990844726562\n",
            "strain 0.9455337524414062\n",
            "strain 1.0740737915039062\n",
            "strain 1.2490005493164062\n",
            "strain 1.305877685546875\n",
            "strain 1.1239776611328125\n",
            "strain 0.9745559692382812\n",
            "strain 1.3012847900390625\n",
            "0.53125\n",
            "0.5\n",
            "0.5625\n",
            "0.578125\n",
            "strain 1.1015243530273438\n",
            "strain 0.991180419921875\n",
            "strain 1.0654525756835938\n",
            "strain 1.07220458984375\n",
            "strain 1.2099990844726562\n",
            "strain 1.3434677124023438\n",
            "strain 1.1082992553710938\n",
            "strain 1.1638412475585938\n",
            "strain 1.1944580078125\n",
            "0.546875\n",
            "0.625\n",
            "0.53125\n",
            "0.578125\n",
            "strain 0.9312973022460938\n",
            "strain 0.9442787170410156\n",
            "strain 0.9093360900878906\n",
            "strain 0.9199638366699219\n",
            "strain 1.7937088012695312\n",
            "strain 1.4186019897460938\n",
            "strain 0.978668212890625\n",
            "strain 1.2080307006835938\n",
            "strain 1.3937530517578125\n",
            "0.640625\n",
            "0.578125\n",
            "0.5\n",
            "0.546875\n",
            "strain 1.1130523681640625\n",
            "strain 1.2340087890625\n",
            "strain 0.9416656494140625\n",
            "strain 1.1885108947753906\n",
            "strain 1.013671875\n",
            "strain 1.3641777038574219\n",
            "strain 1.1477127075195312\n",
            "strain 1.0407142639160156\n",
            "strain 0.9820671081542969\n",
            "0.515625\n",
            "0.484375\n",
            "0.609375\n",
            "0.65625\n",
            "strain 1.0527725219726562\n",
            "strain 1.0173683166503906\n",
            "strain 1.0901336669921875\n",
            "strain 0.9404449462890625\n",
            "strain 1.1410236358642578\n",
            "strain 1.0121726989746094\n",
            "strain 0.9428253173828125\n",
            "strain 0.8428840637207031\n",
            "strain 1.0430793762207031\n",
            "0.625\n",
            "0.578125\n",
            "0.640625\n",
            "0.625\n",
            "strain 0.9264144897460938\n",
            "strain 1.0098686218261719\n",
            "strain 1.0167007446289062\n",
            "strain 1.0789604187011719\n",
            "strain 0.9694919586181641\n",
            "strain 0.9247779846191406\n",
            "strain 0.9545974731445312\n",
            "strain 1.1007614135742188\n",
            "strain 0.9645614624023438\n",
            "0.5625\n",
            "0.625\n",
            "0.703125\n",
            "0.59375\n",
            "strain 0.7627296447753906\n",
            "strain 1.1102485656738281\n",
            "strain 0.8657493591308594\n",
            "strain 0.7366695404052734\n",
            "strain 1.2023468017578125\n",
            "strain 1.0929031372070312\n",
            "strain 0.9039802551269531\n",
            "strain 0.7936153411865234\n",
            "strain 1.2713851928710938\n",
            "0.578125\n",
            "0.5625\n",
            "0.578125\n",
            "0.609375\n",
            "strain 1.021505355834961\n",
            "strain 1.0896892547607422\n",
            "strain 0.9869136810302734\n",
            "strain 0.9163093566894531\n",
            "strain 1.0476646423339844\n",
            "strain 1.223684310913086\n",
            "strain 1.1938552856445312\n",
            "strain 0.9691925048828125\n",
            "strain 0.9141578674316406\n",
            "0.5625\n",
            "0.59375\n",
            "0.640625\n",
            "0.65625\n",
            "strain 1.1790008544921875\n",
            "strain 1.0109405517578125\n",
            "strain 0.9578857421875\n",
            "strain 0.904449462890625\n",
            "strain 0.9031906127929688\n",
            "strain 1.10870361328125\n",
            "strain 0.9009113311767578\n",
            "strain 0.8834609985351562\n",
            "strain 0.9494991302490234\n",
            "0.546875\n",
            "0.609375\n",
            "0.640625\n",
            "0.71875\n",
            "strain 0.9021720886230469\n",
            "strain 0.9886665344238281\n",
            "strain 1.0157508850097656\n",
            "strain 1.3161773681640625\n",
            "strain 0.9530487060546875\n",
            "strain 1.015045166015625\n",
            "strain 1.2442207336425781\n",
            "strain 0.7163009643554688\n",
            "strain 0.7691459655761719\n",
            "0.578125\n",
            "0.671875\n",
            "0.578125\n",
            "0.671875\n",
            "strain 1.0203590393066406\n",
            "strain 0.7879180908203125\n",
            "strain 0.753021240234375\n",
            "strain 1.0266494750976562\n",
            "strain 1.0566673278808594\n",
            "strain 0.9079971313476562\n",
            "strain 0.8311367034912109\n",
            "strain 0.8900680541992188\n",
            "strain 1.1651496887207031\n",
            "0.640625\n",
            "0.625\n",
            "0.734375\n",
            "0.625\n",
            "strain 0.9566898345947266\n",
            "strain 1.0684776306152344\n",
            "strain 0.6663150787353516\n",
            "strain 1.035736083984375\n",
            "strain 0.6485462188720703\n",
            "strain 0.938385009765625\n",
            "strain 0.9668769836425781\n",
            "strain 0.730133056640625\n",
            "strain 1.0997200012207031\n",
            "0.625\n",
            "0.671875\n",
            "0.59375\n",
            "0.609375\n",
            "strain 0.8044567108154297\n",
            "strain 0.9651575088500977\n",
            "strain 0.8661880493164062\n",
            "strain 1.0122337341308594\n",
            "strain 1.4542388916015625\n",
            "strain 0.9358406066894531\n",
            "strain 0.9461154937744141\n",
            "strain 1.0737419128417969\n",
            "strain 0.8477058410644531\n",
            "0.546875\n",
            "0.578125\n",
            "0.5\n",
            "0.53125\n",
            "strain 0.7635440826416016\n",
            "strain 0.8841552734375\n",
            "strain 0.9113082885742188\n",
            "strain 0.6972160339355469\n",
            "strain 0.9768581390380859\n",
            "strain 1.037008285522461\n",
            "strain 1.1628799438476562\n",
            "strain 0.8239850997924805\n",
            "strain 0.7693767547607422\n",
            "0.59375\n",
            "0.53125\n",
            "0.625\n",
            "0.640625\n",
            "strain 0.9016799926757812\n",
            "strain 0.9438199996948242\n",
            "strain 0.8033275604248047\n",
            "strain 0.9654407501220703\n",
            "strain 1.0230178833007812\n",
            "strain 1.0859527587890625\n",
            "strain 0.6840591430664062\n",
            "strain 0.7202587127685547\n",
            "strain 0.7373714447021484\n",
            "0.640625\n",
            "0.59375\n",
            "0.65625\n",
            "0.671875\n",
            "strain 0.7881622314453125\n",
            "strain 0.7898273468017578\n",
            "strain 0.8504352569580078\n",
            "strain 1.0373649597167969\n",
            "strain 0.8947563171386719\n",
            "strain 1.0399341583251953\n",
            "strain 0.6996040344238281\n",
            "strain 0.5789127349853516\n",
            "strain 1.045419692993164\n",
            "0.71875\n",
            "0.609375\n",
            "0.640625\n",
            "0.625\n",
            "strain 0.9080181121826172\n",
            "strain 0.6246376037597656\n",
            "strain 0.9045619964599609\n",
            "strain 0.6114559173583984\n",
            "strain 0.8912868499755859\n",
            "strain 0.83544921875\n",
            "strain 0.7915916442871094\n",
            "strain 0.7729873657226562\n",
            "strain 0.782318115234375\n",
            "0.609375\n",
            "0.609375\n",
            "0.625\n",
            "0.734375\n",
            "strain 0.5849628448486328\n",
            "strain 0.6433773040771484\n",
            "strain 0.7130012512207031\n",
            "strain 0.6910076141357422\n",
            "strain 0.8473033905029297\n",
            "strain 0.6789493560791016\n",
            "strain 0.8369083404541016\n",
            "strain 0.8584175109863281\n",
            "strain 0.8761749267578125\n",
            "0.625\n",
            "0.671875\n",
            "0.609375\n",
            "0.71875\n",
            "strain 0.7816371917724609\n",
            "strain 0.8220252990722656\n",
            "strain 0.6355037689208984\n",
            "strain 0.6361494064331055\n",
            "strain 0.6420097351074219\n",
            "strain 0.8182029724121094\n",
            "strain 0.8781547546386719\n",
            "strain 0.8578052520751953\n",
            "strain 0.7834129333496094\n",
            "0.625\n",
            "0.6875\n",
            "0.65625\n",
            "0.59375\n",
            "strain 0.6224708557128906\n",
            "strain 0.7189483642578125\n",
            "strain 0.6769084930419922\n",
            "strain 0.7780847549438477\n",
            "strain 1.0314397811889648\n",
            "strain 0.981231689453125\n",
            "strain 0.6359777450561523\n",
            "strain 0.7586078643798828\n",
            "strain 0.5004005432128906\n",
            "0.65625\n",
            "0.65625\n",
            "0.703125\n",
            "0.6875\n",
            "strain 0.8614177703857422\n",
            "strain 0.7743663787841797\n",
            "strain 0.7952194213867188\n",
            "strain 0.9381046295166016\n",
            "strain 0.6380605697631836\n",
            "strain 0.7831897735595703\n",
            "strain 0.6475982666015625\n",
            "strain 0.9132728576660156\n",
            "strain 0.8260517120361328\n",
            "0.703125\n",
            "0.640625\n",
            "0.625\n",
            "0.703125\n",
            "strain 0.6118907928466797\n",
            "strain 0.5708580017089844\n",
            "strain 0.5819110870361328\n",
            "strain 0.7412662506103516\n",
            "strain 0.6472568511962891\n",
            "strain 0.7690601348876953\n",
            "strain 0.8891563415527344\n",
            "strain 0.6299524307250977\n",
            "strain 0.6305274963378906\n",
            "0.65625\n",
            "0.671875\n",
            "0.65625\n",
            "0.640625\n",
            "strain 0.7800025939941406\n",
            "strain 0.6490521430969238\n",
            "strain 0.6598348617553711\n",
            "strain 0.6541662216186523\n",
            "strain 0.5258073806762695\n",
            "strain 0.7396430969238281\n",
            "strain 0.6089630126953125\n",
            "strain 0.6633768081665039\n",
            "strain 0.6067829132080078\n",
            "0.640625\n",
            "0.671875\n",
            "0.703125\n",
            "0.640625\n",
            "strain 0.5051441192626953\n",
            "strain 0.5331387519836426\n",
            "strain 0.6585464477539062\n",
            "strain 0.9540185928344727\n",
            "strain 0.619877815246582\n",
            "strain 0.5287189483642578\n",
            "strain 0.6252784729003906\n",
            "strain 0.6025362014770508\n",
            "strain 0.6572265625\n",
            "0.625\n",
            "0.625\n",
            "0.671875\n",
            "0.578125\n",
            "strain 0.8185195922851562\n",
            "strain 0.6403837203979492\n",
            "strain 0.5286483764648438\n",
            "strain 0.5490856170654297\n",
            "strain 0.5712738037109375\n",
            "strain 0.5087404251098633\n",
            "strain 0.768031120300293\n",
            "strain 0.7087435722351074\n",
            "strain 0.6122570037841797\n",
            "0.671875\n",
            "0.671875\n",
            "0.6875\n",
            "0.671875\n",
            "strain 0.5927715301513672\n",
            "strain 0.6105289459228516\n",
            "strain 0.3784060478210449\n",
            "strain 0.5792760848999023\n",
            "strain 0.5784006118774414\n",
            "strain 0.6204524040222168\n",
            "strain 0.7496814727783203\n",
            "strain 0.6345596313476562\n",
            "strain 0.6403398513793945\n",
            "0.625\n",
            "0.796875\n",
            "0.609375\n",
            "0.609375\n",
            "strain 0.5988082885742188\n",
            "strain 0.5950775146484375\n",
            "strain 0.49097442626953125\n",
            "strain 0.7932796478271484\n",
            "strain 0.6566057205200195\n",
            "strain 0.6149349212646484\n",
            "strain 0.6631479263305664\n",
            "strain 0.499849796295166\n",
            "strain 0.5373630523681641\n",
            "0.71875\n",
            "0.546875\n",
            "0.71875\n",
            "0.71875\n",
            "strain 0.4785928726196289\n",
            "strain 0.5557918548583984\n",
            "strain 0.5037922859191895\n",
            "strain 0.5537471771240234\n",
            "strain 0.7609462738037109\n",
            "strain 0.5257625579833984\n",
            "strain 0.4905261993408203\n",
            "strain 0.8213958740234375\n",
            "strain 0.8255033493041992\n",
            "0.5625\n",
            "0.625\n",
            "0.5625\n",
            "0.71875\n",
            "strain 0.48998451232910156\n",
            "strain 0.9273958206176758\n",
            "strain 0.6629476547241211\n",
            "strain 0.6296768188476562\n",
            "strain 0.6032090187072754\n",
            "strain 0.4931154251098633\n",
            "strain 0.49866485595703125\n",
            "strain 0.7596044540405273\n",
            "strain 0.4935312271118164\n",
            "0.703125\n",
            "0.703125\n",
            "0.640625\n",
            "0.734375\n",
            "strain 0.5518178939819336\n",
            "strain 0.46777963638305664\n",
            "strain 0.5777363777160645\n",
            "strain 0.7431516647338867\n",
            "strain 0.6878290176391602\n",
            "strain 0.5651898384094238\n",
            "strain 0.6966524124145508\n",
            "strain 0.531285285949707\n",
            "strain 0.6520929336547852\n",
            "0.671875\n",
            "0.640625\n",
            "0.640625\n",
            "0.6875\n",
            "strain 0.6204454898834229\n",
            "strain 0.8392462730407715\n",
            "strain 0.5680074691772461\n",
            "strain 0.6173949241638184\n",
            "strain 0.5725879669189453\n",
            "strain 0.45412635803222656\n",
            "strain 0.5101308822631836\n",
            "strain 0.5925178527832031\n",
            "strain 0.4375114440917969\n",
            "0.625\n",
            "0.640625\n",
            "0.671875\n",
            "0.625\n",
            "strain 0.38392162322998047\n",
            "strain 0.4726905822753906\n",
            "strain 0.6612296104431152\n",
            "strain 0.5729122161865234\n",
            "strain 0.4678926467895508\n",
            "strain 0.6547684669494629\n",
            "strain 0.48032665252685547\n",
            "strain 0.8285284042358398\n",
            "strain 0.6316909790039062\n",
            "0.625\n",
            "0.765625\n",
            "0.609375\n",
            "0.640625\n",
            "strain 0.5757026672363281\n",
            "strain 0.5231256484985352\n",
            "strain 0.5653657913208008\n",
            "strain 0.6023683547973633\n",
            "strain 0.6113882064819336\n",
            "strain 0.8804159164428711\n",
            "strain 0.614537239074707\n",
            "strain 0.6709384918212891\n",
            "strain 0.5653505325317383\n",
            "0.703125\n",
            "0.671875\n",
            "0.671875\n",
            "0.625\n",
            "strain 0.8322763442993164\n",
            "strain 0.5061154365539551\n",
            "strain 0.45740795135498047\n",
            "strain 0.6962528228759766\n",
            "strain 0.4489097595214844\n",
            "strain 0.3397235870361328\n",
            "strain 0.552708625793457\n",
            "strain 0.7914342880249023\n",
            "strain 0.5607404708862305\n",
            "0.640625\n",
            "0.6875\n",
            "0.71875\n",
            "0.703125\n",
            "strain 0.48325538635253906\n",
            "strain 0.46050214767456055\n",
            "strain 0.40065908432006836\n",
            "strain 0.4018683433532715\n",
            "strain 0.5028400421142578\n",
            "strain 0.5710930824279785\n",
            "strain 0.628054141998291\n",
            "strain 0.4941682815551758\n",
            "strain 0.7748928070068359\n",
            "0.609375\n",
            "0.703125\n",
            "0.703125\n",
            "0.71875\n",
            "strain 0.6306838989257812\n",
            "strain 0.45679569244384766\n",
            "strain 0.4404892921447754\n",
            "strain 0.507805347442627\n",
            "strain 0.4711723327636719\n",
            "strain 0.4406609535217285\n",
            "strain 0.6536722183227539\n",
            "strain 0.4292769432067871\n",
            "strain 0.4210062026977539\n",
            "0.625\n",
            "0.640625\n",
            "0.71875\n",
            "0.546875\n",
            "strain 0.45279932022094727\n",
            "strain 0.400083065032959\n",
            "strain 0.416348934173584\n",
            "strain 0.5928792953491211\n",
            "strain 0.4553546905517578\n",
            "strain 0.554471492767334\n",
            "strain 0.6400060653686523\n",
            "strain 0.4527168273925781\n",
            "strain 0.4040555953979492\n",
            "0.703125\n",
            "0.59375\n",
            "0.6875\n",
            "0.703125\n",
            "strain 0.7283172607421875\n",
            "strain 0.5443897247314453\n",
            "strain 0.40776681900024414\n",
            "strain 0.40188169479370117\n",
            "strain 0.5220241546630859\n",
            "strain 0.5103225708007812\n",
            "strain 0.3389410972595215\n",
            "strain 0.48836326599121094\n",
            "strain 0.5060949325561523\n",
            "0.625\n",
            "0.6875\n",
            "0.6875\n",
            "0.640625\n",
            "strain 0.37828683853149414\n",
            "strain 0.4312715530395508\n",
            "strain 0.4470996856689453\n",
            "strain 0.5493674278259277\n",
            "strain 0.35599613189697266\n",
            "strain 0.35947275161743164\n",
            "strain 0.4136648178100586\n",
            "strain 0.44184303283691406\n",
            "strain 0.5594639778137207\n",
            "0.703125\n",
            "0.6875\n",
            "0.703125\n",
            "0.6875\n",
            "strain 0.4170088768005371\n",
            "strain 0.6345396041870117\n",
            "strain 0.5157690048217773\n",
            "strain 0.4598960876464844\n",
            "strain 0.5874595642089844\n",
            "strain 0.7105188369750977\n",
            "strain 0.5235433578491211\n",
            "strain 0.48369455337524414\n",
            "strain 0.43607139587402344\n",
            "0.71875\n",
            "0.640625\n",
            "0.578125\n",
            "0.640625\n",
            "strain 0.7047700881958008\n",
            "strain 0.4153556823730469\n",
            "strain 0.4820289611816406\n",
            "strain 0.6210904121398926\n",
            "strain 0.5350584983825684\n",
            "strain 0.42069149017333984\n",
            "strain 0.4754786491394043\n",
            "strain 0.3897857666015625\n",
            "strain 0.5914816856384277\n",
            "0.59375\n",
            "0.640625\n",
            "0.71875\n",
            "0.65625\n",
            "strain 0.473355770111084\n",
            "strain 0.6197915077209473\n",
            "strain 0.39573097229003906\n",
            "strain 0.4953145980834961\n",
            "strain 0.4136772155761719\n",
            "strain 0.6539192199707031\n",
            "strain 0.5855207443237305\n",
            "strain 0.3398594856262207\n",
            "strain 0.2991361618041992\n",
            "0.625\n",
            "0.625\n",
            "0.625\n",
            "0.609375\n",
            "strain 0.5472450256347656\n",
            "strain 0.4482579231262207\n",
            "strain 0.32807278633117676\n",
            "strain 0.5003786087036133\n",
            "strain 0.4597766399383545\n",
            "strain 0.29804515838623047\n",
            "strain 0.46914148330688477\n",
            "strain 0.6016976833343506\n",
            "strain 0.3594388961791992\n",
            "0.640625\n",
            "0.609375\n",
            "0.75\n",
            "0.765625\n",
            "strain 0.3434438705444336\n",
            "strain 0.3473696708679199\n",
            "strain 0.4589381217956543\n",
            "strain 0.2437911033630371\n",
            "strain 0.5784730911254883\n",
            "strain 0.373354434967041\n",
            "strain 0.46721935272216797\n",
            "strain 0.45540809631347656\n",
            "strain 0.4603290557861328\n",
            "0.609375\n",
            "0.765625\n",
            "0.640625\n",
            "0.640625\n",
            "strain 0.4155430793762207\n",
            "strain 0.38016700744628906\n",
            "strain 0.42735815048217773\n",
            "strain 0.369229793548584\n",
            "strain 0.4140768051147461\n",
            "strain 0.24538516998291016\n",
            "strain 0.4769105911254883\n",
            "strain 0.38674068450927734\n",
            "strain 0.4158816337585449\n",
            "0.671875\n",
            "0.609375\n",
            "0.671875\n",
            "0.578125\n",
            "strain 0.34583425521850586\n",
            "strain 0.3450808525085449\n",
            "strain 0.48876142501831055\n",
            "strain 0.31990814208984375\n",
            "strain 0.3116631507873535\n",
            "strain 0.36977481842041016\n",
            "strain 0.4347953796386719\n",
            "strain 0.3777937889099121\n",
            "strain 0.3326144218444824\n",
            "0.734375\n",
            "0.640625\n",
            "0.609375\n",
            "0.65625\n",
            "strain 0.26913022994995117\n",
            "strain 0.31244659423828125\n",
            "strain 0.29517626762390137\n",
            "strain 0.3988828659057617\n",
            "strain 0.7387411594390869\n",
            "strain 0.3691086769104004\n",
            "strain 0.3669266700744629\n",
            "strain 0.6015515327453613\n",
            "strain 0.31679439544677734\n",
            "0.65625\n",
            "0.671875\n",
            "0.59375\n",
            "0.578125\n",
            "strain 0.43904685974121094\n",
            "strain 0.29935646057128906\n",
            "strain 0.5987710952758789\n",
            "strain 0.3862128257751465\n",
            "strain 0.2634446620941162\n",
            "strain 0.38416385650634766\n",
            "strain 0.27094030380249023\n",
            "strain 0.4447956085205078\n",
            "strain 0.38683319091796875\n",
            "0.609375\n",
            "0.609375\n",
            "0.6875\n",
            "0.671875\n",
            "strain 0.42486071586608887\n",
            "strain 0.35460662841796875\n",
            "strain 0.1814568042755127\n",
            "strain 0.34023237228393555\n",
            "strain 0.43947744369506836\n",
            "strain 0.5239748954772949\n",
            "strain 0.4354846477508545\n",
            "strain 0.4356412887573242\n",
            "strain 0.3454313278198242\n",
            "0.671875\n",
            "0.734375\n",
            "0.546875\n",
            "0.59375\n",
            "strain 0.3715238571166992\n",
            "strain 0.27223730087280273\n",
            "strain 0.2586174011230469\n",
            "strain 0.29198646545410156\n",
            "strain 0.36956024169921875\n",
            "strain 0.38259387016296387\n",
            "strain 0.3589754104614258\n",
            "strain 0.5480175018310547\n",
            "strain 0.4039039611816406\n",
            "0.609375\n",
            "0.625\n",
            "0.65625\n",
            "0.640625\n",
            "strain 0.22974634170532227\n",
            "strain 0.3730354309082031\n",
            "strain 0.44168806076049805\n",
            "strain 0.44606447219848633\n",
            "strain 0.42552733421325684\n",
            "strain 0.44260239601135254\n",
            "strain 0.3337540626525879\n",
            "strain 0.3215031623840332\n",
            "strain 0.41410255432128906\n",
            "0.703125\n",
            "0.640625\n",
            "0.5625\n",
            "0.671875\n",
            "strain 0.5664207935333252\n",
            "strain 0.25336575508117676\n",
            "strain 0.5894255638122559\n",
            "strain 0.43475341796875\n",
            "strain 0.370389461517334\n",
            "strain 0.5594630241394043\n",
            "strain 0.6736373901367188\n",
            "strain 0.32820868492126465\n",
            "strain 0.28365230560302734\n",
            "0.625\n",
            "0.65625\n",
            "0.734375\n",
            "0.5625\n",
            "strain 0.25804758071899414\n",
            "strain 0.4406900405883789\n",
            "strain 0.24103736877441406\n",
            "strain 0.43634796142578125\n",
            "strain 0.3933701515197754\n",
            "strain 0.27126455307006836\n",
            "strain 0.2593574523925781\n",
            "strain 0.35117483139038086\n",
            "strain 0.23958730697631836\n",
            "0.546875\n",
            "0.703125\n",
            "0.75\n",
            "0.625\n",
            "strain 0.47499656677246094\n",
            "strain 0.6083879470825195\n",
            "strain 0.5195212364196777\n",
            "strain 0.30863332748413086\n",
            "strain 0.3783426284790039\n",
            "strain 0.3440821170806885\n",
            "strain 0.28406381607055664\n",
            "strain 0.2398548126220703\n",
            "strain 0.5264642238616943\n",
            "0.75\n",
            "0.671875\n",
            "0.578125\n",
            "0.65625\n",
            "strain 0.3317394256591797\n",
            "strain 0.34158849716186523\n",
            "strain 0.21948623657226562\n",
            "strain 0.4291083812713623\n",
            "strain 0.3397390842437744\n",
            "strain 0.43997955322265625\n",
            "strain 0.41998839378356934\n",
            "strain 0.2885127067565918\n",
            "strain 0.5325424671173096\n",
            "0.59375\n",
            "0.59375\n",
            "0.5625\n",
            "0.71875\n",
            "strain 0.3417246341705322\n",
            "strain 0.33083653450012207\n",
            "strain 0.6627912521362305\n",
            "strain 0.23540258407592773\n",
            "strain 0.42678213119506836\n",
            "strain 0.24304282665252686\n",
            "strain 0.33187270164489746\n",
            "strain 0.22165393829345703\n",
            "strain 0.509148120880127\n",
            "0.765625\n",
            "0.625\n",
            "0.703125\n",
            "0.515625\n",
            "strain 0.22848987579345703\n",
            "strain 0.28620076179504395\n",
            "strain 0.37313318252563477\n",
            "strain 0.3770890235900879\n",
            "strain 0.3239731788635254\n",
            "strain 0.22405433654785156\n",
            "strain 0.2990093231201172\n",
            "strain 0.36632823944091797\n",
            "strain 0.38862037658691406\n",
            "0.671875\n",
            "0.703125\n",
            "0.640625\n",
            "0.671875\n",
            "strain 0.26633787155151367\n",
            "strain 0.15619897842407227\n",
            "strain 0.2844247817993164\n",
            "strain 0.3826267719268799\n",
            "strain 0.2773141860961914\n",
            "strain 0.2860379219055176\n",
            "strain 0.2036147117614746\n",
            "strain 0.23074865341186523\n",
            "strain 0.5082988739013672\n",
            "0.609375\n",
            "0.625\n",
            "0.640625\n",
            "0.671875\n",
            "strain 0.19657325744628906\n",
            "strain 0.30974578857421875\n",
            "strain 0.3811168670654297\n",
            "strain 0.36946165561676025\n",
            "strain 0.34050703048706055\n",
            "strain 0.26003170013427734\n",
            "strain 0.22080636024475098\n",
            "strain 0.24967336654663086\n",
            "strain 0.367856502532959\n",
            "0.5625\n",
            "0.65625\n",
            "0.75\n",
            "0.609375\n",
            "strain 0.252244234085083\n",
            "strain 0.32183384895324707\n",
            "strain 0.20718836784362793\n",
            "strain 0.2581777572631836\n",
            "strain 0.24888384342193604\n",
            "strain 0.25998830795288086\n",
            "strain 0.24798941612243652\n",
            "strain 0.335821270942688\n",
            "strain 0.37739133834838867\n",
            "0.671875\n",
            "0.65625\n",
            "0.5625\n",
            "0.8125\n",
            "strain 0.23294639587402344\n",
            "strain 0.22268462181091309\n",
            "strain 0.2420787811279297\n",
            "strain 0.2800791263580322\n",
            "strain 0.2634572982788086\n",
            "strain 0.279677152633667\n",
            "strain 0.20418095588684082\n",
            "strain 0.3186330795288086\n",
            "strain 0.43832993507385254\n",
            "0.65625\n",
            "0.609375\n",
            "0.65625\n",
            "0.703125\n",
            "strain 0.17526984214782715\n",
            "strain 0.27208447456359863\n",
            "strain 0.15346121788024902\n",
            "strain 0.378558874130249\n",
            "strain 0.3086998462677002\n",
            "strain 0.19401240348815918\n",
            "strain 0.15669560432434082\n",
            "strain 0.3107588291168213\n",
            "strain 0.26917457580566406\n",
            "0.609375\n",
            "0.5625\n",
            "0.640625\n",
            "0.671875\n",
            "strain 0.20917773246765137\n",
            "strain 0.31800031661987305\n",
            "strain 0.1990189552307129\n",
            "strain 0.3055086135864258\n",
            "strain 0.2569465637207031\n",
            "strain 0.18368768692016602\n",
            "strain 0.3267946243286133\n",
            "strain 0.28101181983947754\n",
            "strain 0.24863195419311523\n",
            "0.671875\n",
            "0.578125\n",
            "0.65625\n",
            "0.640625\n",
            "strain 0.13278532028198242\n",
            "strain 0.19633769989013672\n",
            "strain 0.16362690925598145\n",
            "strain 0.205824613571167\n",
            "strain 0.1918320655822754\n",
            "strain 0.16298460960388184\n",
            "strain 0.19953632354736328\n",
            "strain 0.22573304176330566\n",
            "strain 0.19718241691589355\n",
            "0.6875\n",
            "0.75\n",
            "0.640625\n",
            "0.59375\n",
            "strain 0.19653773307800293\n",
            "strain 0.14484691619873047\n",
            "strain 0.35269880294799805\n",
            "strain 0.26293253898620605\n",
            "strain 0.23550724983215332\n",
            "strain 0.1521596908569336\n",
            "strain 0.2260289192199707\n",
            "strain 0.25197601318359375\n",
            "strain 0.23984718322753906\n",
            "0.625\n",
            "0.640625\n",
            "0.703125\n",
            "0.578125\n",
            "strain 0.15376496315002441\n",
            "strain 0.37549495697021484\n",
            "strain 0.168512225151062\n",
            "strain 0.2736396789550781\n",
            "strain 0.19051408767700195\n",
            "strain 0.19298315048217773\n",
            "strain 0.2494065761566162\n",
            "strain 0.3128941059112549\n",
            "strain 0.2814791202545166\n",
            "0.59375\n",
            "0.75\n",
            "0.609375\n",
            "0.78125\n",
            "strain 0.2732522487640381\n",
            "strain 0.23502862453460693\n",
            "strain 0.17202472686767578\n",
            "strain 0.2852663993835449\n",
            "strain 0.2157745361328125\n",
            "strain 0.1732172966003418\n",
            "strain 0.24049901962280273\n",
            "strain 0.22552251815795898\n",
            "strain 0.3113369941711426\n",
            "0.5625\n",
            "0.65625\n",
            "0.734375\n",
            "0.625\n",
            "strain 0.26487016677856445\n",
            "strain 0.14664268493652344\n",
            "strain 0.13445639610290527\n",
            "strain 0.24822068214416504\n",
            "strain 0.09799766540527344\n",
            "strain 0.1519458293914795\n",
            "strain 0.21732783317565918\n",
            "strain 0.24915146827697754\n",
            "strain 0.2031935453414917\n",
            "0.6875\n",
            "0.6875\n",
            "0.59375\n",
            "0.734375\n",
            "strain 0.20027315616607666\n",
            "strain 0.19986820220947266\n",
            "strain 0.2595663070678711\n",
            "strain 0.16968047618865967\n",
            "strain 0.10547041893005371\n",
            "strain 0.32067251205444336\n",
            "strain 0.29798245429992676\n",
            "strain 0.13578534126281738\n",
            "strain 0.2323920726776123\n",
            "0.59375\n",
            "0.6875\n",
            "0.59375\n",
            "0.71875\n",
            "strain 0.1839679479598999\n",
            "strain 0.23097562789916992\n",
            "strain 0.11186385154724121\n",
            "strain 0.24856090545654297\n",
            "strain 0.3248145580291748\n",
            "strain 0.4630436897277832\n",
            "strain 0.2776665687561035\n",
            "strain 0.13072049617767334\n",
            "strain 0.13686776161193848\n",
            "0.671875\n",
            "0.59375\n",
            "0.578125\n",
            "0.546875\n",
            "strain 0.12750029563903809\n",
            "strain 0.2665414810180664\n",
            "strain 0.21218490600585938\n",
            "strain 0.19390761852264404\n",
            "strain 0.22413849830627441\n",
            "strain 0.20149946212768555\n",
            "strain 0.16807150840759277\n",
            "strain 0.31514501571655273\n",
            "strain 0.17318511009216309\n",
            "0.671875\n",
            "0.609375\n",
            "0.671875\n",
            "0.6875\n",
            "strain 0.1283947229385376\n",
            "strain 0.110565185546875\n",
            "strain 0.1700882911682129\n",
            "strain 0.16270828247070312\n",
            "strain 0.17842745780944824\n",
            "strain 0.244232177734375\n",
            "strain 0.3708322048187256\n",
            "strain 0.11026883125305176\n",
            "strain 0.34947800636291504\n",
            "0.65625\n",
            "0.59375\n",
            "0.734375\n",
            "0.671875\n",
            "strain 0.16235780715942383\n",
            "strain 0.21344232559204102\n",
            "strain 0.2235853672027588\n",
            "strain 0.23451638221740723\n",
            "strain 0.27854466438293457\n",
            "strain 0.6908373832702637\n",
            "strain 0.15927839279174805\n",
            "strain 0.5291836261749268\n",
            "strain 0.28277015686035156\n",
            "0.578125\n",
            "0.609375\n",
            "0.734375\n",
            "0.6875\n",
            "strain 0.1725304126739502\n",
            "strain 0.36457228660583496\n",
            "strain 0.3327651023864746\n",
            "strain 0.3699147701263428\n",
            "strain 0.1334214210510254\n",
            "strain 0.21345651149749756\n",
            "strain 0.1543576717376709\n",
            "strain 0.2641026973724365\n",
            "strain 0.3423008918762207\n",
            "0.546875\n",
            "0.609375\n",
            "0.765625\n",
            "0.65625\n",
            "strain 0.37786269187927246\n",
            "strain 0.22297453880310059\n",
            "strain 0.12670373916625977\n",
            "strain 0.08343327045440674\n",
            "strain 0.2895381450653076\n",
            "strain 0.2296733856201172\n",
            "strain 0.5293488502502441\n",
            "strain 0.2902970314025879\n",
            "strain 0.2981821298599243\n",
            "0.6875\n",
            "0.59375\n",
            "0.71875\n",
            "0.5625\n",
            "strain 0.22637593746185303\n",
            "strain 0.25332319736480713\n",
            "strain 0.20287489891052246\n",
            "strain 0.3063395023345947\n",
            "strain 0.21511459350585938\n",
            "strain 0.18415462970733643\n",
            "strain 0.18065929412841797\n",
            "strain 0.2647128105163574\n",
            "strain 0.22193217277526855\n",
            "0.59375\n",
            "0.609375\n",
            "0.65625\n",
            "0.625\n",
            "strain 0.13949036598205566\n",
            "strain 0.3197094202041626\n",
            "strain 0.14598190784454346\n",
            "strain 0.11108660697937012\n",
            "strain 0.15680885314941406\n",
            "strain 0.35163116455078125\n",
            "strain 0.1754591464996338\n",
            "strain 0.2984222173690796\n",
            "strain 0.10142207145690918\n",
            "0.703125\n",
            "0.6875\n",
            "0.578125\n",
            "0.703125\n",
            "strain 0.26695096492767334\n",
            "strain 0.25412821769714355\n",
            "strain 0.17415952682495117\n",
            "strain 0.10807013511657715\n",
            "strain 0.14190101623535156\n",
            "strain 0.10989296436309814\n",
            "strain 0.3044649362564087\n",
            "strain 0.07036483287811279\n",
            "strain 0.15159916877746582\n",
            "0.609375\n",
            "0.671875\n",
            "0.6875\n",
            "0.671875\n",
            "strain 0.15274453163146973\n",
            "strain 0.14262676239013672\n",
            "strain 0.10560250282287598\n",
            "strain 0.21042120456695557\n",
            "strain 0.33165013790130615\n",
            "strain 0.13664305210113525\n",
            "strain 0.1418318748474121\n",
            "strain 0.12792611122131348\n",
            "strain 0.16121995449066162\n",
            "0.6875\n",
            "0.625\n",
            "0.5625\n",
            "0.578125\n",
            "strain 0.10950374603271484\n",
            "strain 0.32203471660614014\n",
            "strain 0.1170661449432373\n",
            "strain 0.2336256504058838\n",
            "strain 0.12535154819488525\n",
            "strain 0.10658097267150879\n",
            "strain 0.21785640716552734\n",
            "strain 0.19652605056762695\n",
            "strain 0.15644288063049316\n",
            "0.71875\n",
            "0.640625\n",
            "0.65625\n",
            "0.625\n",
            "strain 0.10820186138153076\n",
            "strain 0.13057255744934082\n",
            "strain 0.09165692329406738\n",
            "strain 0.1658686399459839\n",
            "strain 0.30049705505371094\n",
            "strain 0.12290799617767334\n",
            "strain 0.0816044807434082\n",
            "strain 0.18237066268920898\n",
            "strain 0.2270364761352539\n",
            "0.640625\n",
            "0.671875\n",
            "0.609375\n",
            "0.59375\n",
            "strain 0.32471275329589844\n",
            "strain 0.16771602630615234\n",
            "strain 0.11342549324035645\n",
            "strain 0.21423208713531494\n",
            "strain 0.37032032012939453\n",
            "strain 0.1870129108428955\n",
            "strain 0.13411128520965576\n",
            "strain 0.18893176317214966\n",
            "strain 0.18413794040679932\n",
            "0.578125\n",
            "0.625\n",
            "0.71875\n",
            "0.59375\n",
            "strain 0.19168663024902344\n",
            "strain 0.11575055122375488\n",
            "strain 0.20702409744262695\n",
            "strain 0.24547898769378662\n",
            "strain 0.24249756336212158\n",
            "strain 0.11163461208343506\n",
            "strain 0.07318854331970215\n",
            "strain 0.21980106830596924\n",
            "strain 0.13438260555267334\n",
            "0.671875\n",
            "0.625\n",
            "0.640625\n",
            "0.6875\n",
            "strain 0.1714775562286377\n",
            "strain 0.1618032455444336\n",
            "strain 0.13371610641479492\n",
            "strain 0.1094897985458374\n",
            "strain 0.0942087173461914\n",
            "strain 0.19481348991394043\n",
            "strain 0.09629297256469727\n",
            "strain 0.22879672050476074\n",
            "strain 0.21944499015808105\n",
            "0.65625\n",
            "0.671875\n",
            "0.59375\n",
            "0.65625\n",
            "strain 0.16493093967437744\n",
            "strain 0.17771291732788086\n",
            "strain 0.1526193618774414\n",
            "strain 0.1030353307723999\n",
            "strain 0.35838842391967773\n",
            "strain 0.23555123805999756\n",
            "strain 0.14581608772277832\n",
            "strain 0.1455782651901245\n",
            "strain 0.16350042819976807\n",
            "0.640625\n",
            "0.609375\n",
            "0.671875\n",
            "0.59375\n",
            "strain 0.13883495330810547\n",
            "strain 0.09131264686584473\n",
            "strain 0.2410029172897339\n",
            "strain 0.09071683883666992\n",
            "strain 0.34454572200775146\n",
            "strain 0.16749954223632812\n",
            "strain 0.10821700096130371\n",
            "strain 0.20553743839263916\n",
            "strain 0.10522747039794922\n",
            "0.671875\n",
            "0.640625\n",
            "0.640625\n",
            "0.59375\n",
            "strain 0.09888577461242676\n",
            "strain 0.10387659072875977\n",
            "strain 0.1653512716293335\n",
            "strain 0.08137691020965576\n",
            "strain 0.18115711212158203\n",
            "strain 0.10518467426300049\n",
            "strain 0.1592409610748291\n",
            "strain 0.15209829807281494\n",
            "strain 0.13296949863433838\n",
            "0.703125\n",
            "0.59375\n",
            "0.546875\n",
            "0.609375\n",
            "strain 0.3928319215774536\n",
            "strain 0.1572263240814209\n",
            "strain 0.1552276611328125\n",
            "strain 0.15172827243804932\n",
            "strain 0.1294546127319336\n",
            "strain 0.14146554470062256\n",
            "strain 0.19537734985351562\n",
            "strain 0.27918386459350586\n",
            "strain 0.32493090629577637\n",
            "0.5625\n",
            "0.78125\n",
            "0.609375\n",
            "0.625\n",
            "strain 0.3155031204223633\n",
            "strain 0.14172661304473877\n",
            "strain 0.09016263484954834\n",
            "strain 0.18542611598968506\n",
            "strain 0.29987895488739014\n",
            "strain 0.17095935344696045\n",
            "strain 0.22250843048095703\n",
            "strain 0.2123885154724121\n",
            "strain 0.17212796211242676\n",
            "0.65625\n",
            "0.6875\n",
            "0.625\n",
            "0.625\n",
            "strain 0.09577882289886475\n",
            "strain 0.2489517331123352\n",
            "strain 0.36053264141082764\n",
            "strain 0.31246471405029297\n",
            "strain 0.07762026786804199\n",
            "strain 0.2365560531616211\n",
            "strain 0.18239569664001465\n",
            "strain 0.1326202154159546\n",
            "strain 0.25159168243408203\n",
            "0.5625\n",
            "0.671875\n",
            "0.609375\n",
            "0.640625\n",
            "strain 0.16341817378997803\n",
            "strain 0.12073087692260742\n",
            "strain 0.0779792070388794\n",
            "strain 0.4314289093017578\n",
            "strain 0.15754294395446777\n",
            "strain 0.13785076141357422\n",
            "strain 0.38921070098876953\n",
            "strain 0.09683859348297119\n",
            "strain 0.15256226062774658\n",
            "0.546875\n",
            "0.609375\n",
            "0.5625\n",
            "0.65625\n",
            "strain 0.10759234428405762\n",
            "strain 0.2478022575378418\n",
            "strain 0.24726557731628418\n",
            "strain 0.16813790798187256\n",
            "strain 0.2771472930908203\n",
            "strain 0.08785593509674072\n",
            "strain 0.12144708633422852\n",
            "strain 0.31525081396102905\n",
            "strain 0.11958712339401245\n",
            "0.703125\n",
            "0.5625\n",
            "0.703125\n",
            "0.703125\n",
            "strain 0.1852428913116455\n",
            "strain 0.07313013076782227\n",
            "strain 0.1730022430419922\n",
            "strain 0.10869979858398438\n",
            "strain 0.18059134483337402\n",
            "strain 0.15928256511688232\n",
            "strain 0.16606861352920532\n",
            "strain 0.2812556028366089\n",
            "strain 0.17265203595161438\n",
            "0.625\n",
            "0.640625\n",
            "0.625\n",
            "0.5\n",
            "strain 0.14823049306869507\n",
            "strain 0.11119318008422852\n",
            "strain 0.10375827550888062\n",
            "strain 0.10452014207839966\n",
            "strain 0.18037939071655273\n",
            "strain 0.3171467185020447\n",
            "strain 0.1112520694732666\n",
            "strain 0.13918590545654297\n",
            "strain 0.1339629888534546\n",
            "0.6875\n",
            "0.65625\n",
            "0.640625\n",
            "0.640625\n",
            "strain 0.1122732162475586\n",
            "strain 0.12131518125534058\n",
            "strain 0.1259145736694336\n",
            "strain 0.15537869930267334\n",
            "strain 0.12653511762619019\n",
            "strain 0.058905601501464844\n",
            "strain 0.18868887424468994\n",
            "strain 0.13963735103607178\n",
            "strain 0.22828960418701172\n",
            "0.640625\n",
            "0.609375\n",
            "0.671875\n",
            "0.59375\n",
            "strain 0.12674474716186523\n",
            "strain 0.06638753414154053\n",
            "strain 0.08619832992553711\n",
            "strain 0.05817532539367676\n",
            "strain 0.1081247329711914\n",
            "strain 0.09857207536697388\n",
            "strain 0.15720665454864502\n",
            "strain 0.059014320373535156\n",
            "strain 0.16641902923583984\n",
            "0.734375\n",
            "0.671875\n",
            "0.6875\n",
            "0.640625\n",
            "strain 0.20949316024780273\n",
            "strain 0.0901607871055603\n",
            "strain 0.05188062787055969\n",
            "strain 0.053795039653778076\n",
            "strain 0.12909209728240967\n",
            "strain 0.13571858406066895\n",
            "strain 0.0878148078918457\n",
            "strain 0.09242135286331177\n",
            "strain 0.09409135580062866\n",
            "0.765625\n",
            "0.625\n",
            "0.75\n",
            "0.625\n",
            "strain 0.0930907130241394\n",
            "strain 0.04304862022399902\n",
            "strain 0.08225321769714355\n",
            "strain 0.041761159896850586\n",
            "strain 0.06213700771331787\n",
            "strain 0.08101743459701538\n",
            "strain 0.04488229751586914\n",
            "strain 0.06968826055526733\n",
            "strain 0.034084439277648926\n",
            "0.6875\n",
            "0.71875\n",
            "0.671875\n",
            "0.515625\n",
            "strain 0.046364665031433105\n",
            "strain 0.08165347576141357\n",
            "strain 0.03346329927444458\n",
            "strain 0.09282708168029785\n",
            "strain 0.052979469299316406\n",
            "strain 0.15749216079711914\n",
            "strain 0.1350751519203186\n",
            "strain 0.044501662254333496\n",
            "strain 0.07382655143737793\n",
            "0.703125\n",
            "0.703125\n",
            "0.625\n",
            "0.625\n",
            "strain 0.06961476802825928\n",
            "strain 0.058283090591430664\n",
            "strain 0.043419837951660156\n",
            "strain 0.06515073776245117\n",
            "strain 0.04644995927810669\n",
            "strain 0.06612831354141235\n",
            "strain 0.07217419147491455\n",
            "strain 0.08495908975601196\n",
            "strain 0.10366392135620117\n",
            "0.640625\n",
            "0.671875\n",
            "0.640625\n",
            "0.515625\n",
            "strain 0.07022124528884888\n",
            "strain 0.08965641260147095\n",
            "strain 0.06701052188873291\n",
            "strain 0.12302756309509277\n",
            "strain 0.1458531618118286\n",
            "strain 0.08988785743713379\n",
            "strain 0.21548503637313843\n",
            "strain 0.05222058296203613\n",
            "strain 0.027615070343017578\n",
            "0.703125\n",
            "0.625\n",
            "0.75\n",
            "0.59375\n",
            "strain 0.05588656663894653\n",
            "strain 0.11363810300827026\n",
            "strain 0.05559486150741577\n",
            "strain 0.050245702266693115\n",
            "strain 0.09934252500534058\n",
            "strain 0.09408634901046753\n",
            "strain 0.04054868221282959\n",
            "strain 0.050327420234680176\n",
            "strain 0.1683971881866455\n",
            "0.578125\n",
            "0.703125\n",
            "0.5625\n",
            "0.671875\n",
            "strain 0.054688453674316406\n",
            "strain 0.03970527648925781\n",
            "strain 0.029099106788635254\n",
            "strain 0.06729501485824585\n",
            "strain 0.050698935985565186\n",
            "strain 0.039856672286987305\n",
            "strain 0.13034337759017944\n",
            "strain 0.11081677675247192\n",
            "strain 0.02527230978012085\n",
            "0.703125\n",
            "0.671875\n",
            "0.5625\n",
            "0.671875\n",
            "strain 0.050421178340911865\n",
            "strain 0.029066264629364014\n",
            "strain 0.04545116424560547\n",
            "strain 0.16640573740005493\n",
            "strain 0.10922539234161377\n",
            "strain 0.051262736320495605\n",
            "strain 0.06254500150680542\n",
            "strain 0.08324289321899414\n",
            "strain 0.1514902114868164\n",
            "0.671875\n",
            "0.640625\n",
            "0.6875\n",
            "0.6875\n",
            "strain 0.09704488515853882\n",
            "strain 0.03557479381561279\n",
            "strain 0.07495766878128052\n",
            "strain 0.2295805811882019\n",
            "strain 0.06441843509674072\n",
            "strain 0.10293984413146973\n",
            "strain 0.06895303726196289\n",
            "strain 0.037078261375427246\n",
            "strain 0.08253991603851318\n",
            "0.640625\n",
            "0.71875\n",
            "0.59375\n",
            "0.703125\n",
            "strain 0.18600589036941528\n",
            "strain 0.08168673515319824\n",
            "strain 0.2618192434310913\n",
            "strain 0.08862477540969849\n",
            "strain 0.09136521816253662\n",
            "strain 0.27476584911346436\n",
            "strain 0.374948650598526\n",
            "strain 0.06980198621749878\n",
            "strain 0.09102421998977661\n",
            "0.578125\n",
            "0.671875\n",
            "0.625\n",
            "0.5625\n",
            "strain 0.27163660526275635\n",
            "strain 0.17886406183242798\n",
            "strain 0.3478647470474243\n",
            "strain 0.18722239136695862\n",
            "strain 0.31497305631637573\n",
            "strain 0.21428310871124268\n",
            "strain 0.24127507209777832\n",
            "strain 0.1669824719429016\n",
            "strain 0.1802806258201599\n",
            "0.640625\n",
            "0.6875\n",
            "0.59375\n",
            "0.546875\n",
            "strain 0.2285507321357727\n",
            "strain 0.20073169469833374\n",
            "strain 0.40531349182128906\n",
            "strain 0.08752113580703735\n",
            "strain 0.18145954608917236\n",
            "strain 0.3177017271518707\n",
            "strain 0.2153896689414978\n",
            "strain 0.23530584573745728\n",
            "strain 0.2368825078010559\n",
            "0.546875\n",
            "0.671875\n",
            "0.578125\n",
            "0.671875\n",
            "strain 0.16791421175003052\n",
            "strain 0.21559292078018188\n",
            "strain 0.22300982475280762\n",
            "strain 0.12908929586410522\n",
            "strain 0.17945647239685059\n",
            "strain 0.12278831005096436\n",
            "strain 0.23332589864730835\n",
            "strain 0.17677545547485352\n",
            "strain 0.052645087242126465\n",
            "0.71875\n",
            "0.59375\n",
            "0.65625\n",
            "0.65625\n",
            "strain 0.179265558719635\n",
            "strain 0.08828604221343994\n",
            "strain 0.3198930025100708\n",
            "strain 0.09479331970214844\n",
            "strain 0.17409050464630127\n",
            "strain 0.08330261707305908\n",
            "strain 0.11032170057296753\n",
            "strain 0.09293729066848755\n",
            "strain 0.16611427068710327\n",
            "0.625\n",
            "0.71875\n",
            "0.59375\n",
            "0.5625\n",
            "strain 0.1052861213684082\n",
            "strain 0.0948142409324646\n",
            "strain 0.11243021488189697\n",
            "strain 0.08441179990768433\n",
            "strain 0.08207887411117554\n",
            "strain 0.24963819980621338\n",
            "strain 0.09354186058044434\n",
            "strain 0.05583786964416504\n",
            "strain 0.10180044174194336\n",
            "0.609375\n",
            "0.640625\n",
            "0.640625\n",
            "0.625\n",
            "strain 0.1085924506187439\n",
            "strain 0.1580493450164795\n",
            "strain 0.10441160202026367\n",
            "strain 0.05735057592391968\n",
            "strain 0.08284348249435425\n",
            "strain 0.08971863985061646\n",
            "strain 0.06310665607452393\n",
            "strain 0.29859107732772827\n",
            "strain 0.12665462493896484\n",
            "0.65625\n",
            "0.65625\n",
            "0.671875\n",
            "0.65625\n",
            "strain 0.12128707766532898\n",
            "strain 0.05738174915313721\n",
            "strain 0.034570157527923584\n",
            "strain 0.077059805393219\n",
            "strain 0.21794819831848145\n",
            "strain 0.2524335980415344\n",
            "strain 0.07490351796150208\n",
            "strain 0.21010321378707886\n",
            "strain 0.1506933867931366\n",
            "0.640625\n",
            "0.65625\n",
            "0.53125\n",
            "0.625\n",
            "strain 0.0840005874633789\n",
            "strain 0.1544986367225647\n",
            "strain 0.10000407695770264\n",
            "strain 0.04648420214653015\n",
            "strain 0.1831558346748352\n",
            "strain 0.16740310192108154\n",
            "strain 0.1697753667831421\n",
            "strain 0.20535606145858765\n",
            "strain 0.18122488260269165\n",
            "0.546875\n",
            "0.640625\n",
            "0.609375\n",
            "0.671875\n",
            "strain 0.14998817443847656\n",
            "strain 0.0673527717590332\n",
            "strain 0.06438285112380981\n",
            "strain 0.07567745447158813\n",
            "strain 0.12254670262336731\n",
            "strain 0.21374154090881348\n",
            "strain 0.08590042591094971\n",
            "strain 0.03224092721939087\n",
            "strain 0.135229229927063\n",
            "0.640625\n",
            "0.71875\n",
            "0.640625\n",
            "0.671875\n",
            "strain 0.055475443601608276\n",
            "strain 0.0674411952495575\n",
            "strain 0.15432879328727722\n",
            "strain 0.045025259256362915\n",
            "strain 0.26091575622558594\n",
            "strain 0.2730928659439087\n",
            "strain 0.07353994250297546\n",
            "strain 0.076547771692276\n",
            "strain 0.09852457046508789\n",
            "0.609375\n",
            "0.75\n",
            "0.59375\n",
            "0.640625\n",
            "strain 0.04981139302253723\n",
            "strain 0.11762022972106934\n",
            "strain 0.0625084638595581\n",
            "strain 0.10788309574127197\n",
            "strain 0.20142126083374023\n",
            "strain 0.11780929565429688\n",
            "strain 0.20424699783325195\n",
            "strain 0.04765421152114868\n",
            "strain 0.12059229612350464\n",
            "0.671875\n",
            "0.578125\n",
            "0.6875\n",
            "0.671875\n",
            "strain 0.08036428689956665\n",
            "strain 0.06621116399765015\n",
            "strain 0.10935640335083008\n",
            "strain 0.14293479919433594\n",
            "strain 0.18224525451660156\n",
            "strain 0.08509781956672668\n",
            "strain 0.1068418025970459\n",
            "strain 0.08907681703567505\n",
            "strain 0.08897143602371216\n",
            "0.53125\n",
            "0.59375\n",
            "0.765625\n",
            "0.59375\n",
            "strain 0.05934259295463562\n",
            "strain 0.04526019096374512\n",
            "strain 0.026835381984710693\n",
            "strain 0.18388020992279053\n",
            "strain 0.18657606840133667\n",
            "strain 0.15135833621025085\n",
            "strain 0.08030784130096436\n",
            "strain 0.04314619302749634\n",
            "strain 0.023768484592437744\n",
            "0.59375\n",
            "0.59375\n",
            "0.609375\n",
            "0.671875\n",
            "strain 0.040963321924209595\n",
            "strain 0.17223915457725525\n",
            "strain 0.17368045449256897\n",
            "strain 0.07055222988128662\n",
            "strain 0.1641446053981781\n",
            "strain 0.1607380509376526\n",
            "strain 0.07100248336791992\n",
            "strain 0.20892542600631714\n",
            "strain 0.1310783326625824\n",
            "0.59375\n",
            "0.59375\n",
            "0.578125\n",
            "0.65625\n",
            "strain 0.05771136283874512\n",
            "strain 0.06626230478286743\n",
            "strain 0.083060622215271\n",
            "strain 0.07199227809906006\n",
            "strain 0.07566076517105103\n",
            "strain 0.037447452545166016\n",
            "strain 0.03464710712432861\n",
            "strain 0.09035754203796387\n",
            "strain 0.04167240858078003\n",
            "0.703125\n",
            "0.625\n",
            "0.625\n",
            "0.5\n",
            "strain 0.02419102191925049\n",
            "strain 0.02116525173187256\n",
            "strain 0.02960306406021118\n",
            "strain 0.11829626560211182\n",
            "strain 0.03570759296417236\n",
            "strain 0.07793986797332764\n",
            "strain 0.05702650547027588\n",
            "strain 0.059280216693878174\n",
            "strain 0.06699952483177185\n",
            "0.625\n",
            "0.625\n",
            "0.65625\n",
            "0.640625\n",
            "strain 0.08329421281814575\n",
            "strain 0.01631590723991394\n",
            "strain 0.05289018154144287\n",
            "strain 0.023480474948883057\n",
            "strain 0.03856751322746277\n",
            "strain 0.08225405216217041\n",
            "strain 0.04432690143585205\n",
            "strain 0.043959081172943115\n",
            "strain 0.025044262409210205\n",
            "0.609375\n",
            "0.640625\n",
            "0.65625\n",
            "0.609375\n",
            "strain 0.03273892402648926\n",
            "strain 0.024753659963607788\n",
            "strain 0.03459087014198303\n",
            "strain 0.02736160159111023\n",
            "strain 0.20915329456329346\n",
            "strain 0.025317668914794922\n",
            "strain 0.09697535634040833\n",
            "strain 0.021138817071914673\n",
            "strain 0.09200766682624817\n",
            "0.6875\n",
            "0.609375\n",
            "0.625\n",
            "0.703125\n",
            "strain 0.07834184169769287\n",
            "strain 0.02267146110534668\n",
            "strain 0.04938730597496033\n",
            "strain 0.062094300985336304\n",
            "strain 0.054416656494140625\n",
            "strain 0.15539634227752686\n",
            "strain 0.049042969942092896\n",
            "strain 0.06178992986679077\n",
            "strain 0.03565347194671631\n",
            "0.59375\n",
            "0.578125\n",
            "0.609375\n",
            "0.671875\n",
            "strain 0.04002940654754639\n",
            "strain 0.0420798659324646\n",
            "strain 0.02810591459274292\n",
            "strain 0.06200671195983887\n",
            "strain 0.0829397439956665\n",
            "strain 0.2185143530368805\n",
            "strain 0.10323426127433777\n",
            "strain 0.1848442554473877\n",
            "strain 0.05623805522918701\n",
            "0.578125\n",
            "0.625\n",
            "0.578125\n",
            "0.71875\n",
            "strain 0.016557157039642334\n",
            "strain 0.04818320274353027\n",
            "strain 0.03515315055847168\n",
            "strain 0.12134820222854614\n",
            "strain 0.07108494639396667\n",
            "strain 0.05038362741470337\n",
            "strain 0.04160100221633911\n",
            "strain 0.02876114845275879\n",
            "strain 0.04773664474487305\n",
            "0.625\n",
            "0.6875\n",
            "0.546875\n",
            "0.53125\n",
            "strain 0.10516351461410522\n",
            "strain 0.07004174590110779\n",
            "strain 0.06271255016326904\n",
            "strain 0.018516629934310913\n",
            "strain 0.07887345552444458\n",
            "strain 0.051913291215896606\n",
            "strain 0.15299487113952637\n",
            "strain 0.05827200412750244\n",
            "strain 0.071920245885849\n",
            "0.59375\n",
            "0.671875\n",
            "0.578125\n",
            "0.703125\n",
            "strain 0.039425671100616455\n",
            "strain 0.03833645582199097\n",
            "strain 0.06542819738388062\n",
            "strain 0.12220507860183716\n",
            "strain 0.04226681590080261\n",
            "strain 0.10916081070899963\n",
            "strain 0.04330793023109436\n",
            "strain 0.047999799251556396\n",
            "strain 0.08257097005844116\n",
            "0.609375\n",
            "0.671875\n",
            "0.75\n",
            "0.640625\n",
            "strain 0.13474060595035553\n",
            "strain 0.02360987663269043\n",
            "strain 0.05322052538394928\n",
            "strain 0.0323832631111145\n",
            "strain 0.0728769302368164\n",
            "strain 0.09993499517440796\n",
            "strain 0.05881690979003906\n",
            "strain 0.035605013370513916\n",
            "strain 0.04406476020812988\n",
            "0.5\n",
            "0.625\n",
            "0.671875\n",
            "0.796875\n",
            "strain 0.050118088722229004\n",
            "strain 0.021225392818450928\n",
            "strain 0.012995302677154541\n",
            "strain 0.06621599197387695\n",
            "strain 0.05894571542739868\n",
            "strain 0.2392100691795349\n",
            "strain 0.03601127862930298\n",
            "strain 0.025003641843795776\n",
            "strain 0.030630022287368774\n",
            "0.625\n",
            "0.640625\n",
            "0.671875\n",
            "0.609375\n",
            "strain 0.05439591407775879\n",
            "strain 0.03666424751281738\n",
            "strain 0.07186183333396912\n",
            "strain 0.08295372128486633\n",
            "strain 0.06872382760047913\n",
            "strain 0.062014758586883545\n",
            "strain 0.06068161129951477\n",
            "strain 0.0697748064994812\n",
            "strain 0.2050667703151703\n",
            "0.625\n",
            "0.765625\n",
            "0.609375\n",
            "0.71875\n",
            "strain 0.0947190523147583\n",
            "strain 0.04726892709732056\n",
            "strain 0.06192395091056824\n",
            "strain 0.050260186195373535\n",
            "strain 0.054984286427497864\n",
            "strain 0.02260667085647583\n",
            "strain 0.05174744129180908\n",
            "strain 0.11163768172264099\n",
            "strain 0.08392661809921265\n",
            "0.609375\n",
            "0.59375\n",
            "0.609375\n",
            "0.578125\n",
            "strain 0.08739456534385681\n",
            "strain 0.038866251707077026\n",
            "strain 0.05456387996673584\n",
            "strain 0.09327930212020874\n",
            "strain 0.03688172996044159\n",
            "strain 0.09650483727455139\n",
            "strain 0.023261308670043945\n",
            "strain 0.011676520109176636\n",
            "strain 0.07563403248786926\n",
            "0.734375\n",
            "0.671875\n",
            "0.578125\n",
            "0.609375\n",
            "strain 0.01781439781188965\n",
            "strain 0.12475240230560303\n",
            "strain 0.026468664407730103\n",
            "strain 0.07911980152130127\n",
            "strain 0.04571545124053955\n",
            "strain 0.05306708812713623\n",
            "strain 0.18062469363212585\n",
            "strain 0.014209508895874023\n",
            "strain 0.0706070065498352\n",
            "0.6875\n",
            "0.6875\n",
            "0.609375\n",
            "0.5\n",
            "strain 0.018601536750793457\n",
            "strain 0.03233852982521057\n",
            "strain 0.09185662865638733\n",
            "strain 0.0103834867477417\n",
            "strain 0.015292108058929443\n",
            "strain 0.03705519437789917\n",
            "strain 0.07958963513374329\n",
            "strain 0.09094039350748062\n",
            "strain 0.019095510244369507\n",
            "0.640625\n",
            "0.625\n",
            "0.671875\n",
            "0.609375\n",
            "strain 0.009673088788986206\n",
            "strain 0.025933057069778442\n",
            "strain 0.09424576163291931\n",
            "strain 0.055732667446136475\n",
            "strain 0.13145437836647034\n",
            "strain 0.05671054124832153\n",
            "strain 0.24681958556175232\n",
            "strain 0.03935509920120239\n",
            "strain 0.039668142795562744\n",
            "0.625\n",
            "0.625\n",
            "0.625\n",
            "0.6875\n",
            "strain 0.039364784955978394\n",
            "strain 0.07269945740699768\n",
            "strain 0.040995121002197266\n",
            "strain 0.10363727807998657\n",
            "strain 0.13954904675483704\n",
            "strain 0.07177338004112244\n",
            "strain 0.0348583459854126\n",
            "strain 0.09283894300460815\n",
            "strain 0.02805149555206299\n",
            "0.578125\n",
            "0.625\n",
            "0.671875\n",
            "0.734375\n",
            "strain 0.06919330358505249\n",
            "strain 0.07316035032272339\n",
            "strain 0.011812388896942139\n",
            "strain 0.04107685387134552\n",
            "strain 0.0733538269996643\n",
            "strain 0.017741531133651733\n",
            "strain 0.06854304671287537\n",
            "strain 0.0892375111579895\n",
            "strain 0.03522178530693054\n",
            "0.6875\n",
            "0.65625\n",
            "0.609375\n",
            "0.609375\n",
            "strain 0.15608331561088562\n",
            "strain 0.0338040292263031\n",
            "strain 0.11758530139923096\n",
            "strain 0.05026817321777344\n",
            "strain 0.047744154930114746\n",
            "strain 0.07280966639518738\n",
            "strain 0.1639898717403412\n",
            "strain 0.10048502683639526\n",
            "strain 0.07488173246383667\n",
            "0.640625\n",
            "0.625\n",
            "0.578125\n",
            "0.625\n",
            "strain 0.08247143030166626\n",
            "strain 0.2797967791557312\n",
            "strain 0.12866517901420593\n",
            "strain 0.12037399411201477\n",
            "strain 0.05038076639175415\n",
            "strain 0.07384064793586731\n",
            "strain 0.06667453050613403\n",
            "strain 0.1989058405160904\n",
            "strain 0.1553255021572113\n",
            "0.671875\n",
            "0.734375\n",
            "0.640625\n",
            "0.546875\n",
            "strain 0.16646990180015564\n",
            "strain 0.043855905532836914\n",
            "strain 0.03669458627700806\n",
            "strain 0.029446348547935486\n",
            "strain 0.16093820333480835\n",
            "strain 0.02723643183708191\n",
            "strain 0.1083517074584961\n",
            "strain 0.07668992877006531\n",
            "strain 0.08545947074890137\n",
            "0.625\n",
            "0.640625\n",
            "0.625\n",
            "0.640625\n",
            "strain 0.07799836993217468\n",
            "strain 0.014842808246612549\n",
            "strain 0.02608601748943329\n",
            "strain 0.0188748836517334\n",
            "strain 0.05726921558380127\n",
            "strain 0.12094983458518982\n",
            "strain 0.10208828747272491\n",
            "strain 0.07402116060256958\n",
            "strain 0.04665267467498779\n",
            "0.578125\n",
            "0.671875\n",
            "0.546875\n",
            "0.71875\n",
            "strain 0.04138818383216858\n",
            "strain 0.0582256019115448\n",
            "strain 0.07931926846504211\n",
            "strain 0.06662917137145996\n",
            "strain 0.026755020022392273\n",
            "strain 0.08444958925247192\n",
            "strain 0.036626219749450684\n",
            "strain 0.12855681777000427\n",
            "strain 0.027433335781097412\n",
            "0.65625\n",
            "0.625\n",
            "0.78125\n",
            "0.625\n",
            "strain 0.0715569257736206\n",
            "strain 0.03921133279800415\n",
            "strain 0.033394306898117065\n",
            "strain 0.045023828744888306\n",
            "strain 0.040794432163238525\n",
            "strain 0.08831915259361267\n",
            "strain 0.06727278232574463\n",
            "strain 0.021754369139671326\n",
            "strain 0.1652568280696869\n",
            "0.578125\n",
            "0.65625\n",
            "0.59375\n",
            "0.59375\n",
            "strain 0.1211182177066803\n",
            "strain 0.11282452940940857\n",
            "strain 0.05576682090759277\n",
            "strain 0.010757297277450562\n",
            "strain 0.05052027106285095\n",
            "strain 0.03993779420852661\n",
            "strain 0.03110925853252411\n",
            "strain 0.30414777994155884\n",
            "strain 0.16855770349502563\n",
            "0.578125\n",
            "0.625\n",
            "0.609375\n",
            "0.75\n",
            "strain 0.139943927526474\n",
            "strain 0.02992457151412964\n",
            "strain 0.015059366822242737\n",
            "strain 0.09211507439613342\n",
            "strain 0.12873196601867676\n",
            "strain 0.06611940264701843\n",
            "strain 0.02231687307357788\n",
            "strain 0.0793602466583252\n",
            "strain 0.04634203016757965\n",
            "0.59375\n",
            "0.625\n",
            "0.75\n",
            "0.65625\n",
            "strain 0.053087860345840454\n",
            "strain 0.01598295569419861\n",
            "strain 0.021466881036758423\n",
            "strain 0.06461426615715027\n",
            "strain 0.04908710718154907\n",
            "strain 0.10878247022628784\n",
            "strain 0.0360390841960907\n",
            "strain 0.1412704885005951\n",
            "strain 0.04098963737487793\n",
            "0.75\n",
            "0.625\n",
            "0.671875\n",
            "0.65625\n",
            "strain 0.014920562505722046\n",
            "strain 0.03912118077278137\n",
            "strain 0.0298432856798172\n",
            "strain 0.07703667879104614\n",
            "strain 0.05672471225261688\n",
            "strain 0.04798540472984314\n",
            "strain 0.02528741955757141\n",
            "strain 0.15618571639060974\n",
            "strain 0.03127184510231018\n",
            "0.625\n",
            "0.671875\n",
            "0.625\n",
            "0.59375\n",
            "strain 0.08010441064834595\n",
            "strain 0.014091581106185913\n",
            "strain 0.019377902150154114\n",
            "strain 0.03197607398033142\n",
            "strain 0.046109795570373535\n",
            "strain 0.016339778900146484\n",
            "strain 0.11807230114936829\n",
            "strain 0.0480707585811615\n",
            "strain 0.20035521686077118\n",
            "0.640625\n",
            "0.625\n",
            "0.6875\n",
            "0.640625\n",
            "strain 0.01690700650215149\n",
            "strain 0.04380132257938385\n",
            "strain 0.03940016031265259\n",
            "strain 0.0639561265707016\n",
            "strain 0.11541615426540375\n",
            "strain 0.054549217224121094\n",
            "strain 0.11572454869747162\n",
            "strain 0.025043010711669922\n",
            "strain 0.011808305978775024\n",
            "0.640625\n",
            "0.578125\n",
            "0.609375\n",
            "0.734375\n",
            "strain 0.09439274668693542\n",
            "strain 0.04701673984527588\n",
            "strain 0.16526660323143005\n",
            "strain 0.021362021565437317\n",
            "strain 0.015545159578323364\n",
            "strain 0.19538383185863495\n",
            "strain 0.05481727421283722\n",
            "strain 0.09294690191745758\n",
            "strain 0.015782415866851807\n",
            "0.59375\n",
            "0.453125\n",
            "0.578125\n",
            "0.796875\n",
            "strain 0.038464292883872986\n",
            "strain 0.0723518431186676\n",
            "strain 0.07656817138195038\n",
            "strain 0.19242903590202332\n",
            "strain 0.15110018849372864\n",
            "strain 0.021504834294319153\n",
            "strain 0.15196585655212402\n",
            "strain 0.04144677519798279\n",
            "strain 0.2021133005619049\n",
            "0.640625\n",
            "0.6875\n",
            "0.71875\n",
            "0.625\n",
            "strain 0.035774603486061096\n",
            "strain 0.10043534636497498\n",
            "strain 0.28476276993751526\n",
            "strain 0.09314721822738647\n",
            "strain 0.042634740471839905\n",
            "strain 0.03661513328552246\n",
            "strain 0.07649576663970947\n",
            "strain 0.031068086624145508\n",
            "strain 0.10366347432136536\n",
            "0.78125\n",
            "0.640625\n",
            "0.546875\n",
            "0.5625\n",
            "strain 0.06613892316818237\n",
            "strain 0.04045313596725464\n",
            "strain 0.09342995285987854\n",
            "strain 0.0354183167219162\n",
            "strain 0.12994574010372162\n",
            "strain 0.0171147882938385\n",
            "strain 0.05189262330532074\n",
            "strain 0.09093688428401947\n",
            "strain 0.16005325317382812\n",
            "0.5625\n",
            "0.6875\n",
            "0.609375\n",
            "0.640625\n",
            "strain 0.0497632771730423\n",
            "strain 0.08190083503723145\n",
            "strain 0.020648717880249023\n",
            "strain 0.017585664987564087\n",
            "strain 0.06260204315185547\n",
            "strain 0.02324122190475464\n",
            "strain 0.02738456428050995\n",
            "strain 0.029801294207572937\n",
            "strain 0.12371020019054413\n",
            "0.703125\n",
            "0.671875\n",
            "0.578125\n",
            "0.5625\n",
            "strain 0.06056229770183563\n",
            "strain 0.13137918710708618\n",
            "strain 0.03204847872257233\n",
            "strain 0.021233990788459778\n",
            "strain 0.04977095127105713\n",
            "strain 0.06643450260162354\n",
            "strain 0.08560201525688171\n",
            "strain 0.042513757944107056\n",
            "strain 0.06902606785297394\n",
            "0.65625\n",
            "0.75\n",
            "0.609375\n",
            "0.59375\n",
            "strain 0.012882143259048462\n",
            "strain 0.05544942617416382\n",
            "strain 0.03150755167007446\n",
            "strain 0.021306633949279785\n",
            "strain 0.05366441607475281\n",
            "strain 0.09688378870487213\n",
            "strain 0.08138205111026764\n",
            "strain 0.0375552773475647\n",
            "strain 0.13410042226314545\n",
            "0.703125\n",
            "0.609375\n",
            "0.65625\n",
            "0.703125\n",
            "strain 0.03753599524497986\n",
            "strain 0.031866565346717834\n",
            "strain 0.014203257858753204\n",
            "strain 0.02046319842338562\n",
            "strain 0.022266969084739685\n",
            "strain 0.15689320862293243\n",
            "strain 0.048609763383865356\n",
            "strain 0.07599148154258728\n",
            "strain 0.14559200406074524\n",
            "0.609375\n",
            "0.703125\n",
            "0.640625\n",
            "0.625\n",
            "strain 0.041041865944862366\n",
            "strain 0.034291595220565796\n",
            "strain 0.058086931705474854\n",
            "strain 0.10141649842262268\n",
            "strain 0.2992096543312073\n",
            "strain 0.2152932733297348\n",
            "strain 0.10793536901473999\n",
            "strain 0.030028313398361206\n",
            "strain 0.036554932594299316\n",
            "0.59375\n",
            "0.5625\n",
            "0.625\n",
            "0.734375\n",
            "strain 0.11185908317565918\n",
            "strain 0.08742935955524445\n",
            "strain 0.019034676253795624\n",
            "strain 0.04014917463064194\n",
            "strain 0.19082185626029968\n",
            "strain 0.06274168193340302\n",
            "strain 0.19581395387649536\n",
            "strain 0.35937023162841797\n",
            "strain 0.1628662645816803\n",
            "0.71875\n",
            "0.640625\n",
            "0.53125\n",
            "0.578125\n",
            "strain 0.013243503868579865\n",
            "strain 0.069220632314682\n",
            "strain 0.017782136797904968\n",
            "strain 0.03239437937736511\n",
            "strain 0.13733617961406708\n",
            "strain 0.03060510754585266\n",
            "strain 0.14449699223041534\n",
            "strain 0.0204048752784729\n",
            "strain 0.05717964470386505\n",
            "0.59375\n",
            "0.578125\n",
            "0.65625\n",
            "0.6875\n",
            "strain 0.04994186758995056\n",
            "strain 0.06637601554393768\n",
            "strain 0.05019278824329376\n",
            "strain 0.02064451575279236\n",
            "strain 0.022416770458221436\n",
            "strain 0.038018882274627686\n",
            "strain 0.1613471955060959\n",
            "strain 0.022264748811721802\n",
            "strain 0.18936830759048462\n",
            "0.609375\n",
            "0.640625\n",
            "0.59375\n",
            "0.671875\n",
            "strain 0.049679309129714966\n",
            "strain 0.060633584856987\n",
            "strain 0.036039263010025024\n",
            "strain 0.07815086841583252\n",
            "strain 0.04033268988132477\n",
            "strain 0.025991998612880707\n",
            "strain 0.03682662546634674\n",
            "strain 0.027791589498519897\n",
            "strain 0.033183276653289795\n",
            "0.6875\n",
            "0.609375\n",
            "0.546875\n",
            "0.6875\n",
            "strain 0.039104312658309937\n",
            "strain 0.14483359456062317\n",
            "strain 0.03823985159397125\n",
            "strain 0.03862661123275757\n",
            "strain 0.11883461475372314\n",
            "strain 0.02963392436504364\n",
            "strain 0.10182349383831024\n",
            "strain 0.05288730561733246\n",
            "strain 0.03657469153404236\n",
            "0.71875\n",
            "0.546875\n",
            "0.671875\n",
            "0.765625\n",
            "strain 0.036626264452934265\n",
            "strain 0.08420391380786896\n",
            "strain 0.23488068580627441\n",
            "strain 0.0627962052822113\n",
            "strain 0.04895973205566406\n",
            "strain 0.12641194462776184\n",
            "strain 0.07072333991527557\n",
            "strain 0.04255346208810806\n",
            "strain 0.07931740581989288\n",
            "0.59375\n",
            "0.609375\n",
            "0.6875\n",
            "0.609375\n",
            "strain 0.1385761797428131\n",
            "strain 0.14033135771751404\n",
            "strain 0.009373679757118225\n",
            "strain 0.06475433707237244\n",
            "strain 0.03702419996261597\n",
            "strain 0.06774246692657471\n",
            "strain 0.06850898265838623\n",
            "strain 0.08579933643341064\n",
            "strain 0.10957443714141846\n",
            "0.515625\n",
            "0.671875\n",
            "0.65625\n",
            "0.546875\n",
            "strain 0.13213902711868286\n",
            "strain 0.022899091243743896\n",
            "strain 0.05223575234413147\n",
            "strain 0.027881205081939697\n",
            "strain 0.08058139681816101\n",
            "strain 0.12204590439796448\n",
            "strain 0.11153741180896759\n",
            "strain 0.10778555274009705\n",
            "strain 0.11519035696983337\n",
            "0.640625\n",
            "0.6875\n",
            "0.6875\n",
            "0.625\n",
            "strain 0.01634475588798523\n",
            "strain 0.03429992496967316\n",
            "strain 0.023713380098342896\n",
            "strain 0.023348703980445862\n",
            "strain 0.030680105090141296\n",
            "strain 0.0289345383644104\n",
            "strain 0.031238466501235962\n",
            "strain 0.03421953320503235\n",
            "strain 0.1590721309185028\n",
            "0.71875\n",
            "0.625\n",
            "0.703125\n",
            "0.5625\n",
            "strain 0.029224641621112823\n",
            "strain 0.08197756111621857\n",
            "strain 0.008528418838977814\n",
            "strain 0.1880333423614502\n",
            "strain 0.01823057234287262\n",
            "strain 0.03330966830253601\n",
            "strain 0.03532567620277405\n",
            "strain 0.04599282145500183\n",
            "strain 0.11127236485481262\n",
            "0.640625\n",
            "0.65625\n",
            "0.640625\n",
            "0.59375\n",
            "strain 0.040603458881378174\n",
            "strain 0.024335741996765137\n",
            "strain 0.045481592416763306\n",
            "strain 0.035750359296798706\n",
            "strain 0.018269971013069153\n",
            "strain 0.05026564002037048\n",
            "strain 0.030349597334861755\n",
            "strain 0.08381851017475128\n",
            "strain 0.019055992364883423\n",
            "0.671875\n",
            "0.671875\n",
            "0.5625\n",
            "0.75\n",
            "strain 0.020050838589668274\n",
            "strain 0.018189698457717896\n",
            "strain 0.00795513391494751\n",
            "strain 0.025159038603305817\n",
            "strain 0.05214610695838928\n",
            "strain 0.13202786445617676\n",
            "strain 0.04567374289035797\n",
            "strain 0.050454869866371155\n",
            "strain 0.015666604042053223\n",
            "0.625\n",
            "0.65625\n",
            "0.6875\n",
            "0.671875\n",
            "strain 0.03298698365688324\n",
            "strain 0.080013208091259\n",
            "strain 0.09253227710723877\n",
            "strain 0.062049657106399536\n",
            "strain 0.06155262887477875\n",
            "strain 0.019566982984542847\n",
            "strain 0.01665462553501129\n",
            "strain 0.018874049186706543\n",
            "strain 0.009190812706947327\n",
            "0.65625\n",
            "0.484375\n",
            "0.671875\n",
            "0.6875\n",
            "strain 0.02778017520904541\n",
            "strain 0.021204419434070587\n",
            "strain 0.026104748249053955\n",
            "strain 0.012563154101371765\n",
            "strain 0.017655372619628906\n",
            "strain 0.028793901205062866\n",
            "strain 0.014051258563995361\n",
            "strain 0.02482418715953827\n",
            "strain 0.007129237055778503\n",
            "0.578125\n",
            "0.59375\n",
            "0.65625\n",
            "0.6875\n",
            "strain 0.016109347343444824\n",
            "strain 0.019064292311668396\n",
            "strain 0.004229247570037842\n",
            "strain 0.007958292961120605\n",
            "strain 0.00886756181716919\n",
            "strain 0.007354632019996643\n",
            "strain 0.0052518099546432495\n",
            "strain 0.0098237544298172\n",
            "strain 0.015885695815086365\n",
            "0.6875\n",
            "0.671875\n",
            "0.65625\n",
            "0.65625\n",
            "strain 0.015071868896484375\n",
            "strain 0.005595743656158447\n",
            "strain 0.018267929553985596\n",
            "strain 0.005645006895065308\n",
            "strain 0.0191337913274765\n",
            "strain 0.03282329440116882\n",
            "strain 0.014632701873779297\n",
            "strain 0.021851208060979843\n",
            "strain 0.010688647627830505\n",
            "0.671875\n",
            "0.640625\n",
            "0.625\n",
            "0.640625\n",
            "strain 0.032716408371925354\n",
            "strain 0.004314780235290527\n",
            "strain 0.004091635346412659\n",
            "strain 0.004393860697746277\n",
            "strain 0.005920909345149994\n",
            "strain 0.005248665809631348\n",
            "strain 0.017513498663902283\n",
            "strain 0.016603998839855194\n",
            "strain 0.021900266408920288\n",
            "0.53125\n",
            "0.625\n",
            "0.71875\n",
            "0.71875\n",
            "strain 0.006583109498023987\n",
            "strain 0.050788819789886475\n",
            "strain 0.02131025493144989\n",
            "strain 0.008138567209243774\n",
            "strain 0.01804036647081375\n",
            "strain 0.009350195527076721\n",
            "strain 0.0064094215631484985\n",
            "strain 0.010587483644485474\n",
            "strain 0.02586422860622406\n",
            "0.625\n",
            "0.71875\n",
            "0.640625\n",
            "0.578125\n",
            "strain 0.02737470716238022\n",
            "strain 0.01179562509059906\n",
            "strain 0.02337665855884552\n",
            "strain 0.021790742874145508\n",
            "strain 0.012629032135009766\n",
            "strain 0.0236709862947464\n",
            "strain 0.004864349961280823\n",
            "strain 0.01374642550945282\n",
            "strain 0.010931029915809631\n",
            "0.59375\n",
            "0.71875\n",
            "0.65625\n",
            "0.609375\n",
            "strain 0.005623906850814819\n",
            "strain 0.020901456475257874\n",
            "strain 0.007321640849113464\n",
            "strain 0.0036167800426483154\n",
            "strain 0.04637607932090759\n",
            "strain 0.008504033088684082\n",
            "strain 0.020330388098955154\n",
            "strain 0.015510380268096924\n",
            "strain 0.004895538091659546\n",
            "0.6875\n",
            "0.640625\n",
            "0.609375\n",
            "0.6875\n",
            "strain 0.08093427121639252\n",
            "strain 0.006876230239868164\n",
            "strain 0.007206663489341736\n",
            "strain 0.004659876227378845\n",
            "strain 0.016143381595611572\n",
            "strain 0.02556288242340088\n",
            "strain 0.0063636526465415955\n",
            "strain 0.0049171894788742065\n",
            "strain 0.0072630345821380615\n",
            "0.609375\n",
            "0.625\n",
            "0.703125\n",
            "0.71875\n",
            "strain 0.004155948758125305\n",
            "strain 0.03333692252635956\n",
            "strain 0.03409425914287567\n",
            "strain 0.008278191089630127\n",
            "strain 0.006611675024032593\n",
            "strain 0.023233748972415924\n",
            "strain 0.007146105170249939\n",
            "strain 0.0023861080408096313\n",
            "strain 0.051213935017585754\n",
            "0.640625\n",
            "0.578125\n",
            "0.71875\n",
            "0.703125\n",
            "strain 0.021903008222579956\n",
            "strain 0.01597529649734497\n",
            "strain 0.007710278034210205\n",
            "strain 0.027979932725429535\n",
            "strain 0.02647113800048828\n",
            "strain 0.053608238697052\n",
            "strain 0.01718425750732422\n",
            "strain 0.006140321493148804\n",
            "strain 0.07858974486589432\n",
            "0.609375\n",
            "0.640625\n",
            "0.6875\n",
            "0.65625\n",
            "strain 0.024502620100975037\n",
            "strain 0.030445478856563568\n",
            "strain 0.02853819727897644\n",
            "strain 0.027431264519691467\n",
            "strain 0.019328013062477112\n",
            "strain 0.07448053359985352\n",
            "strain 0.037388548254966736\n",
            "strain 0.04368706792593002\n",
            "strain 0.010994523763656616\n",
            "0.703125\n",
            "0.6875\n",
            "0.640625\n",
            "0.609375\n",
            "strain 0.011996537446975708\n",
            "strain 0.03159631788730621\n",
            "strain 0.048873960971832275\n",
            "strain 0.03483155369758606\n",
            "strain 0.04934243857860565\n",
            "strain 0.015659794211387634\n",
            "strain 0.01664060354232788\n",
            "strain 0.0424041748046875\n",
            "strain 0.03463081270456314\n",
            "0.734375\n",
            "0.671875\n",
            "0.734375\n",
            "0.578125\n",
            "strain 0.04912559315562248\n",
            "strain 0.016751445829868317\n",
            "strain 0.028356537222862244\n",
            "strain 0.058493971824645996\n",
            "strain 0.028048649430274963\n",
            "strain 0.08637979626655579\n",
            "strain 0.02644382417201996\n",
            "strain 0.021554134786128998\n",
            "strain 0.11086277663707733\n",
            "0.6875\n",
            "0.6875\n",
            "0.609375\n",
            "0.671875\n",
            "strain 0.14531302452087402\n",
            "strain 0.012589849531650543\n",
            "strain 0.018208906054496765\n",
            "strain 0.006758466362953186\n",
            "strain 0.04985378682613373\n",
            "strain 0.11359044909477234\n",
            "strain 0.06271632015705109\n",
            "strain 0.006598442792892456\n",
            "strain 0.02578861266374588\n",
            "0.703125\n",
            "0.53125\n",
            "0.65625\n",
            "0.734375\n",
            "strain 0.07008301466703415\n",
            "strain 0.03496788442134857\n",
            "strain 0.005390644073486328\n",
            "strain 0.03568248450756073\n",
            "strain 0.10915200412273407\n",
            "strain 0.030442461371421814\n",
            "strain 0.013410434126853943\n",
            "strain 0.006629355251789093\n",
            "strain 0.011549487709999084\n",
            "0.75\n",
            "0.671875\n",
            "0.5625\n",
            "0.640625\n",
            "strain 0.022919312119483948\n",
            "strain 0.0034314244985580444\n",
            "strain 0.016378387808799744\n",
            "strain 0.012196958065032959\n",
            "strain 0.036327064037323\n",
            "strain 0.06122171878814697\n",
            "strain 0.007878914475440979\n",
            "strain 0.01551806926727295\n",
            "strain 0.016185685992240906\n",
            "0.578125\n",
            "0.625\n",
            "0.71875\n",
            "0.5625\n",
            "strain 0.021767839789390564\n",
            "strain 0.00936417281627655\n",
            "strain 0.008175425231456757\n",
            "strain 0.02315947413444519\n",
            "strain 0.048817455768585205\n",
            "strain 0.1090363934636116\n",
            "strain 0.06068494915962219\n",
            "strain 0.021534815430641174\n",
            "strain 0.004923045635223389\n",
            "0.671875\n",
            "0.625\n",
            "0.625\n",
            "0.703125\n",
            "strain 0.011581048369407654\n",
            "strain 0.026567205786705017\n",
            "strain 0.13347069919109344\n",
            "strain 0.013960763812065125\n",
            "strain 0.04071436822414398\n",
            "strain 0.029257863759994507\n",
            "strain 0.00542856752872467\n",
            "strain 0.011993519961833954\n",
            "strain 0.018661722540855408\n",
            "0.71875\n",
            "0.734375\n",
            "0.609375\n",
            "0.640625\n",
            "strain 0.10964898020029068\n",
            "strain 0.04177837073802948\n",
            "strain 0.0053808242082595825\n",
            "strain 0.013654112815856934\n",
            "strain 0.039271093904972076\n",
            "strain 0.2120397686958313\n",
            "strain 0.009773418307304382\n",
            "strain 0.03466765582561493\n",
            "strain 0.11962088942527771\n",
            "0.578125\n",
            "0.59375\n",
            "0.671875\n",
            "0.75\n",
            "strain 0.020874835550785065\n",
            "strain 0.0744396299123764\n",
            "strain 0.17438586056232452\n",
            "strain 0.02535153180360794\n",
            "strain 0.06653790920972824\n",
            "strain 0.01973237842321396\n",
            "strain 0.08648968487977982\n",
            "strain 0.03584358096122742\n",
            "strain 0.08077605068683624\n",
            "0.65625\n",
            "0.59375\n",
            "0.703125\n",
            "0.578125\n",
            "strain 0.07116244733333588\n",
            "strain 0.033453285694122314\n",
            "strain 0.02157743275165558\n",
            "strain 0.044167742133140564\n",
            "strain 0.016499407589435577\n",
            "strain 0.09385907649993896\n",
            "strain 0.028217464685440063\n",
            "strain 0.1016150489449501\n",
            "strain 0.01372411847114563\n",
            "0.65625\n",
            "0.71875\n",
            "0.640625\n",
            "0.640625\n",
            "strain 0.022779889404773712\n",
            "strain 0.020054660737514496\n",
            "strain 0.03641733527183533\n",
            "strain 0.027727782726287842\n",
            "strain 0.12104703485965729\n",
            "strain 0.018291130661964417\n",
            "strain 0.012988992035388947\n",
            "strain 0.023294486105442047\n",
            "strain 0.021917469799518585\n",
            "0.625\n",
            "0.65625\n",
            "0.640625\n",
            "0.59375\n",
            "strain 0.02445538341999054\n",
            "strain 0.04108322784304619\n",
            "strain 0.09761762619018555\n",
            "strain 0.12096136063337326\n",
            "strain 0.006132803857326508\n",
            "strain 0.0191289484500885\n",
            "strain 0.007199309766292572\n",
            "strain 0.04892344772815704\n",
            "strain 0.059772148728370667\n",
            "0.6875\n",
            "0.609375\n",
            "0.546875\n",
            "0.59375\n",
            "strain 0.09143050760030746\n",
            "strain 0.005955882370471954\n",
            "strain 0.053910523653030396\n",
            "strain 0.00663343071937561\n",
            "strain 0.06457400321960449\n",
            "strain 0.033929064869880676\n",
            "strain 0.01918419450521469\n",
            "strain 0.01405586302280426\n",
            "strain 0.07106012105941772\n",
            "0.65625\n",
            "0.71875\n",
            "0.625\n",
            "0.671875\n",
            "strain 0.05357176810503006\n",
            "strain 0.06900891661643982\n",
            "strain 0.02269510179758072\n",
            "strain 0.013648658990859985\n",
            "strain 0.13758961856365204\n",
            "strain 0.04920326918363571\n",
            "strain 0.030071094632148743\n",
            "strain 0.0183994323015213\n",
            "strain 0.02671869844198227\n",
            "0.625\n",
            "0.625\n",
            "0.625\n",
            "0.6875\n",
            "strain 0.015135683119297028\n",
            "strain 0.12846331298351288\n",
            "strain 0.04635255038738251\n",
            "strain 0.13745729625225067\n",
            "strain 0.017311394214630127\n",
            "strain 0.12267154455184937\n",
            "strain 0.037304386496543884\n",
            "strain 0.012269958853721619\n",
            "strain 0.023602694272994995\n",
            "0.59375\n",
            "0.5\n",
            "0.609375\n",
            "0.5625\n",
            "strain 0.031117305159568787\n",
            "strain 0.015436559915542603\n",
            "strain 0.036895155906677246\n",
            "strain 0.035168662667274475\n",
            "strain 0.07561902701854706\n",
            "strain 0.01224491000175476\n",
            "strain 0.019965000450611115\n",
            "strain 0.049546726047992706\n",
            "strain 0.0603959858417511\n",
            "0.625\n",
            "0.703125\n",
            "0.546875\n",
            "0.734375\n",
            "strain 0.08321227133274078\n",
            "strain 0.012997861951589584\n",
            "strain 0.037170059978961945\n",
            "strain 0.12120193243026733\n",
            "strain 0.006705678999423981\n",
            "strain 0.06410608440637589\n",
            "strain 0.04095838963985443\n",
            "strain 0.059319108724594116\n",
            "strain 0.03996264934539795\n",
            "0.5625\n",
            "0.671875\n",
            "0.5625\n",
            "0.5625\n",
            "strain 0.03764022886753082\n",
            "strain 0.027927979826927185\n",
            "strain 0.15473967790603638\n",
            "strain 0.005904927849769592\n",
            "strain 0.01917298138141632\n",
            "strain 0.035405516624450684\n",
            "strain 0.03248681128025055\n",
            "strain 0.052948638796806335\n",
            "strain 0.10434089601039886\n",
            "0.578125\n",
            "0.5625\n",
            "0.625\n",
            "0.734375\n",
            "strain 0.17919810116291046\n",
            "strain 0.005068957805633545\n",
            "strain 0.01918172836303711\n",
            "strain 0.008388914167881012\n",
            "strain 0.2181074172258377\n",
            "strain 0.075661301612854\n",
            "strain 0.040749356150627136\n",
            "strain 0.01677577942609787\n",
            "strain 0.11884309351444244\n",
            "0.65625\n",
            "0.65625\n",
            "0.625\n",
            "0.59375\n",
            "strain 0.07808119058609009\n",
            "strain 0.054559044539928436\n",
            "strain 0.059578150510787964\n",
            "strain 0.013224400579929352\n",
            "strain 0.028473615646362305\n",
            "strain 0.03844780474901199\n",
            "strain 0.04071953147649765\n",
            "strain 0.02551986277103424\n",
            "strain 0.05867050588130951\n",
            "0.53125\n",
            "0.6875\n",
            "0.703125\n",
            "0.578125\n",
            "strain 0.017871111631393433\n",
            "strain 0.10288037359714508\n",
            "strain 0.006124041974544525\n",
            "strain 0.01997225731611252\n",
            "strain 0.02171420305967331\n",
            "strain 0.13015374541282654\n",
            "strain 0.06038060784339905\n",
            "strain 0.03217576444149017\n",
            "strain 0.6812992691993713\n",
            "0.65625\n",
            "0.640625\n",
            "0.59375\n",
            "0.5625\n",
            "strain 0.07620756328105927\n",
            "strain 0.00978054478764534\n",
            "strain 0.009500782936811447\n",
            "strain 0.07155852019786835\n",
            "strain 0.025712084025144577\n",
            "strain 0.08476859331130981\n",
            "strain 0.03465660661458969\n",
            "strain 0.09138889610767365\n",
            "strain 0.14249131083488464\n",
            "0.671875\n",
            "0.671875\n",
            "0.5625\n",
            "0.625\n",
            "strain 0.08092478662729263\n",
            "strain 0.05606190115213394\n",
            "strain 0.02378404140472412\n",
            "strain 0.08868075907230377\n",
            "strain 0.0473036915063858\n",
            "strain 0.058831483125686646\n",
            "strain 0.11930359899997711\n",
            "strain 0.06335499882698059\n",
            "strain 0.13529831171035767\n",
            "0.515625\n",
            "0.609375\n",
            "0.625\n",
            "0.59375\n",
            "strain 0.04638686776161194\n",
            "strain 0.018245533108711243\n",
            "strain 0.07418081909418106\n",
            "strain 0.16178052127361298\n",
            "strain 0.06069377437233925\n",
            "strain 0.044934824109077454\n",
            "strain 0.02306106686592102\n",
            "strain 0.023540154099464417\n",
            "strain 0.014643505215644836\n",
            "0.640625\n",
            "0.625\n",
            "0.625\n",
            "0.625\n",
            "strain 0.029578428715467453\n",
            "strain 0.03657171130180359\n",
            "strain 0.056770339608192444\n",
            "strain 0.009363196790218353\n",
            "strain 0.02430465817451477\n",
            "strain 0.06635808944702148\n",
            "strain 0.022416986525058746\n",
            "strain 0.03318687155842781\n",
            "strain 0.08109661936759949\n",
            "0.53125\n",
            "0.65625\n",
            "0.671875\n",
            "0.625\n",
            "strain 0.027602404356002808\n",
            "strain 0.04101315140724182\n",
            "strain 0.051794007420539856\n",
            "strain 0.04528752714395523\n",
            "strain 0.05047449842095375\n",
            "strain 0.03984828293323517\n",
            "strain 0.01791796088218689\n",
            "strain 0.07664689421653748\n",
            "strain 0.01549820601940155\n",
            "0.625\n",
            "0.734375\n",
            "0.671875\n",
            "0.578125\n",
            "strain 0.039737552404403687\n",
            "strain 0.1598890721797943\n",
            "strain 0.008764833211898804\n",
            "strain 0.1109851747751236\n",
            "strain 0.029160700738430023\n",
            "strain 0.15193267166614532\n",
            "strain 0.039153143763542175\n",
            "strain 0.013960577547550201\n",
            "strain 0.013218149542808533\n",
            "0.578125\n",
            "0.671875\n",
            "0.625\n",
            "0.703125\n",
            "strain 0.003674447536468506\n",
            "strain 0.021053828299045563\n",
            "strain 0.02412322163581848\n",
            "strain 0.03796248883008957\n",
            "strain 0.04043242335319519\n",
            "strain 0.021626930683851242\n",
            "strain 0.04448084905743599\n",
            "strain 0.1261802613735199\n",
            "strain 0.01613987237215042\n",
            "0.578125\n",
            "0.578125\n",
            "0.65625\n",
            "0.625\n",
            "strain 0.0075246356427669525\n",
            "strain 0.05811884254217148\n",
            "strain 0.004001624882221222\n",
            "strain 0.01104407012462616\n",
            "strain 0.012208856642246246\n",
            "strain 0.018869958817958832\n",
            "strain 0.10438218712806702\n",
            "strain 0.00864170491695404\n",
            "strain 0.04976814240217209\n",
            "0.65625\n",
            "0.640625\n",
            "0.625\n",
            "0.5625\n",
            "strain 0.00849883258342743\n",
            "strain 0.027992695569992065\n",
            "strain 0.04120250791311264\n",
            "strain 0.013654518872499466\n",
            "strain 0.01236627995967865\n",
            "strain 0.05483512580394745\n",
            "strain 0.006401820108294487\n",
            "strain 0.005542203783988953\n",
            "strain 0.09008542448282242\n",
            "0.671875\n",
            "0.625\n",
            "0.671875\n",
            "0.625\n",
            "strain 0.003619857132434845\n",
            "strain 0.012060075998306274\n",
            "strain 0.004585046321153641\n",
            "strain 0.007000461220741272\n",
            "strain 0.006241820752620697\n",
            "strain 0.00883849710226059\n",
            "strain 0.024946436285972595\n",
            "strain 0.06017870083451271\n",
            "strain 0.014812953770160675\n",
            "0.671875\n",
            "0.640625\n",
            "0.625\n",
            "0.6875\n",
            "strain 0.007454138249158859\n",
            "strain 0.04649338871240616\n",
            "strain 0.013287581503391266\n",
            "strain 0.09768205881118774\n",
            "strain 0.019001953303813934\n",
            "strain 0.02802230417728424\n",
            "strain 0.08169233798980713\n",
            "strain 0.007881686091423035\n",
            "strain 0.03415083885192871\n",
            "0.65625\n",
            "0.609375\n",
            "0.671875\n",
            "0.65625\n",
            "strain 0.008729703724384308\n",
            "strain 0.02915292978286743\n",
            "strain 0.008970364928245544\n",
            "strain 0.10402403771877289\n",
            "strain 0.01798618584871292\n",
            "strain 0.15215390920639038\n",
            "strain 0.014408417046070099\n",
            "strain 0.09382174909114838\n",
            "strain 0.058949172496795654\n",
            "0.609375\n",
            "0.609375\n",
            "0.703125\n",
            "0.6875\n",
            "strain 0.017920203506946564\n",
            "strain 0.013910666108131409\n",
            "strain 0.02428331971168518\n",
            "strain 0.027745738625526428\n",
            "strain 0.012634068727493286\n",
            "strain 0.055679380893707275\n",
            "strain 0.09603650122880936\n",
            "strain 0.021482747048139572\n",
            "strain 0.0343499556183815\n",
            "0.640625\n",
            "0.671875\n",
            "0.625\n",
            "0.671875\n",
            "strain 0.03270891308784485\n",
            "strain 0.027218051254749298\n",
            "strain 0.015710964798927307\n",
            "strain 0.042274706065654755\n",
            "strain 0.017770826816558838\n",
            "strain 0.07553951442241669\n",
            "strain 0.34767946600914\n",
            "strain 0.03789307922124863\n",
            "strain 0.133065328001976\n",
            "0.578125\n",
            "0.625\n",
            "0.71875\n",
            "0.625\n",
            "strain 0.005331438034772873\n",
            "strain 0.027720453217625618\n",
            "strain 0.04453497380018234\n",
            "strain 0.06028738245368004\n",
            "strain 0.06218257546424866\n",
            "strain 0.1612955629825592\n",
            "strain 0.07609346508979797\n",
            "strain 0.02023741975426674\n",
            "strain 0.008634954690933228\n",
            "0.640625\n",
            "0.59375\n",
            "0.671875\n",
            "0.71875\n",
            "strain 0.06259672343730927\n",
            "strain 0.09040798991918564\n",
            "strain 0.06681135296821594\n",
            "strain 0.01183868944644928\n",
            "strain 0.021365582942962646\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-52e2c1e78027>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_sampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mstrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_jepa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_jepa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-52e2c1e78027>\u001b[0m in \u001b[0;36mstrain\u001b[0;34m(model, dataloader, optim, scheduler)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# print(loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# for p in list(filter(lambda p: p.grad is not None, model.parameters())):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for param in seq_jepa.context_encoder.cls: print(param.data)\n",
        "# print(seq_jepa.context_encoder.cls.is_leaf)\n",
        "# seq=40\n",
        "# src = seq_jepa.context_encoder.pos_encoder(torch.arange(seq, device=device))\n",
        "# for x in src:\n",
        "#     print(x)\n",
        "# print(.999**50)"
      ],
      "metadata": {
        "id": "WgN5LlxWsPzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title save/load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder='/content/drive/MyDrive/jepa/'\n",
        "\n",
        "# # modelsd, optimsd = torch.load(folder+'SeqJEPA.pkl', map_location=device).values()\n",
        "# modelsd, optimsd = torch.load('SeqJEPA.pkl', map_location=device).values()\n",
        "# seq_jepa.load_state_dict(modelsd, strict=False)\n",
        "# optim.load_state_dict(optimsd)"
      ],
      "metadata": {
        "id": "JT8AgOx0E_KM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b3d3969-da28-4cc0-c4de-f0a327ea266f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = {'model': seq_jepa.state_dict(), 'optimizer': optim.state_dict()}\n",
        "torch.save(checkpoint, folder+'SeqJEPA.pkl')\n",
        "# torch.save(checkpoint, 'SeqJEPA.pkl')"
      ],
      "metadata": {
        "id": "CpbLPvjiE-pD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## drawer"
      ],
      "metadata": {
        "id": "0vScvFGGSeN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title evergarmin buffer dataloader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "import numpy as np\n",
        "import scipy.interpolate\n",
        "\n",
        "class BufferDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, activities, seq_len):\n",
        "        # self.data = [step for episode in self.process(buffer) for step in episode] # 0.00053\n",
        "        self.data = [self.process(activity) for activity in activities] # 0.00053\n",
        "        self.data = [x for x in self.data if x!=None and x!=[]]\n",
        "        self.seq_len = seq_len\n",
        "        self.pad = [(-1,-1,-1)]*(self.seq_len) # pad. need to mask later # torch.full((self.seq_len,), -1)\n",
        "        # normalise\n",
        "\n",
        "    def __len__(self):\n",
        "        # return len(self.data)//self.seq_len\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        act_list = self.data[idx].copy()\n",
        "        summary = list(act_list.pop(0))\n",
        "        # act_list = self.pad[len(act_list):] + act_list # forward padding\n",
        "        act_list = [summary] + self.transform(act_list) # summary, aug(x)\n",
        "        hr, temp, heart= zip(*act_list)\n",
        "        # print(hr, temp, heart)\n",
        "        # return hr, temp, heart\n",
        "        return torch.tensor(hr), torch.tensor(temp), torch.tensor(heart, dtype=torch.float)\n",
        "\n",
        "\n",
        "\n",
        "    def process(self, activity):\n",
        "        res_id = activity[\"res_id\"]\n",
        "        try: activeKilocalories = activity[\"summary\"][\"activeKilocalories\"]\n",
        "        except KeyError: activeKilocalories = 0.\n",
        "        try: steps = activity[\"summary\"][\"steps\"]\n",
        "        except KeyError: steps = 0.\n",
        "        # (res_id, activeKilocalories, steps)\n",
        "        res_id = float(res_id[3:]) # remove 'RES' and turn id into float, in line with other data\n",
        "        act_list = [(res_id, activeKilocalories, steps)]\n",
        "        samples = activity[\"samples\"]\n",
        "\n",
        "        if len(samples)==0: return\n",
        "\n",
        "        for sample in samples:\n",
        "            try:\n",
        "                startTimeInSeconds = sample[\"startTimeInSeconds\"]\n",
        "                hour_decimal = (startTimeInSeconds / 3600) % 24\n",
        "            except KeyError: hour_decimal = 12.\n",
        "            try: airTemperatureCelcius = sample[\"airTemperatureCelcius\"]\n",
        "            except KeyError: airTemperatureCelcius = 28. # singapore average temperature?\n",
        "            try: heartRate = sample[\"heartRate\"]\n",
        "            except KeyError: heartRate = 80.\n",
        "            # (hour_decimal, airTemperatureCelcius, heartRate)\n",
        "            act_list.append((hour_decimal, airTemperatureCelcius, heartRate))\n",
        "        return act_list\n",
        "\n",
        "    def transform(self, act_list): # rand resize crop, rand mask\n",
        "\n",
        "        # https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html\n",
        "        act_list = np.array(act_list)\n",
        "        try: hr, temp, heart= zip(*act_list)\n",
        "        except ValueError: print('err:',act_list)\n",
        "        kind = 'nearest' if len(act_list)>self.seq_len/.7 else 'linear'\n",
        "        temp_interpolator = scipy.interpolate.interp1d(hr, temp, kind=kind) # linear nearest quadratic cubic\n",
        "        heart_interpolator = scipy.interpolate.interp1d(hr, heart, kind=kind) # linear nearest quadratic cubic\n",
        "        hr_ = np.sort(np.random.uniform(hr[0], hr[-1], round(self.seq_len*random.uniform(1,1/.7))))\n",
        "        temp_, heart_ = temp_interpolator(hr_), heart_interpolator(hr_)\n",
        "        act_list = list(zip(hr_, temp_, heart_))\n",
        "\n",
        "        idx = torch.randint(len(act_list)-self.seq_len+1, size=(1,))\n",
        "        # return act_list[idx: idx+self.seq_len]\n",
        "        act_list = act_list[idx: idx+self.seq_len]\n",
        "\n",
        "        # mask=(torch.rand(self.seq_len)<.1) # True -> masked # random masking\n",
        "        # # act_list[mask] = self.pad[0]\n",
        "        # act_list = [self.pad[0] if m else a for m,a in zip(mask, act_list)]\n",
        "        return act_list\n",
        "\n",
        "    def push(self, activities):\n",
        "        # self.data.append(self.process(activity))\n",
        "        self.data.extend([self.process(activity) for activity in activities])\n",
        "\n",
        "\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(30, 14))\n",
        "    print(npimg.shape)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "seq_len = 200 #26/51 # 50\n",
        "train_data = BufferDataset(activities, seq_len)\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 128 #128\n",
        "# train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # [3,batch, T]\n",
        "train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, drop_last=True) # [3,batch, T]\n",
        "\n",
        "\n",
        "for batch, (hr, temp, heart) in enumerate(train_loader):\n",
        "    pass\n",
        "    # print(hr[6])\n",
        "    # for h in hr:\n",
        "    #     print(h)\n",
        "    # break\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "DNCJFn5kNuua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ViT me simple save\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    \"\"\"Zero out the parameters of a module and return it.\"\"\"\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads=None, d_head=8, cond_dim=None, dropout=0.): # .1\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_head = d_head\n",
        "        self.n_heads = d_model // d_head\n",
        "        # self.d_head = d_model // n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or d_model, 2*d_model, bias=False)\n",
        "        # self.k = nn.Sequential(nn.Dropout(dropout), nn.Linear(cond_dim, d_model, bias=False))\n",
        "        # self.lin = nn.Linear(d_model, d_model)\n",
        "        self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        # self.lin = nn.Sequential(nn.Dropout(dropout), zero_module(nn.Linear(d_model, d_model)))\n",
        "        self.drop = nn.Dropout(dropout) # indp before q,k,v; after linout\n",
        "        self.scale = self.d_head ** -.5\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [batch, T, d_model]=[batch, h*w, c], [batch, num_tok, cond_dim], [batch,T]\n",
        "        batch = x.shape[0]\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        Q = self.q(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        # K = self.k(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        K, V = self.kv(cond).view(batch, -1, self.n_heads, 2*self.d_head).transpose(1, 2).chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # linear attention # Softmax(Q) @ (Softmax(K).T @ V)\n",
        "        if mask != None:\n",
        "            mask = mask[:, None, :, None] # [batch,T] -> [batch,1,T,1]\n",
        "            K, V = K.masked_fill(mask, -torch.finfo(x.dtype).max), V.masked_fill(mask, -torch.finfo(x.dtype).max)\n",
        "        Q, K = Q.softmax(dim=-1)*self.scale, K.softmax(dim=-2)\n",
        "        context = K.transpose(-2,-1) @ V # [batch, n_heads, d_head, d_head]\n",
        "        out = Q @ context # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # (quadratic) attention # Softmax(Q @ K.T) @ V\n",
        "        # attn = Q @ K.transpose(-2,-1) * self.scale # [batch, n_heads, T] # [batch, n_heads, T, T/num_tok]\n",
        "        # if mask != None: attn = attn.masked_fill(mask[:, None, :, None], -torch.finfo(attn.dtype).max) # [batch,T]->[batch,1,T,1]\n",
        "        # attention = torch.softmax(attn, dim=-1)\n",
        "        # out = self.drop(attention) @ V # [batch, n_heads, T, d_head]\n",
        "\n",
        "        out = out.transpose(1, 2).flatten(2)\n",
        "        return self.drop(self.lin(out)) # [batch, T, d_model]\n",
        "\n",
        "# if self, dont pass cond_dim in init, dont pass cond in fwd\n",
        "# Softmax(Q @ K.T) @ V ~ Softmax(Q) @ Softmax(K).T @ V\n",
        "\n",
        "# https://github.com/lucidrains/x-transformers/blob/main/x_transformers/x_transformers.py#L1855\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_head=4, cond_dim=None, ff_mult=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm1 = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.norm2 = nn.RMSNorm(d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.cond_dim = cond_dim\n",
        "        self.self = MultiHeadAttention(d_model, d_head=d_head, dropout=0)\n",
        "        if self.cond_dim!=None: self.cross = MultiHeadAttention(d_model, d_head=d_head, cond_dim=cond_dim, dropout=0)\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        ff_dim=d_model*ff_mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), nn.Dropout(dropout), nn.Linear(d_model, ff_dim), act, # ReLU GELU\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "            nn.RMSNorm(d_model), nn.Dropout(dropout), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            # nn.RMSNorm(d_model), act, nn.Dropout(dropout), nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,c,h,w], [batch, num_tok, cond_dim], [batch,T]\n",
        "        # bchw = x.shape\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "\n",
        "        if self.cond_dim==None: cond=None # is self attn\n",
        "        x = x + self.drop(self.self(self.norm1(x)))\n",
        "        if self.cond_dim!=None: x = x + self.cross(self.norm2(x), cond, mask)\n",
        "        x = x + self.ff(x)\n",
        "        # x = x + self.drop(self.ff(x))\n",
        "\n",
        "        # return x.transpose(1,2).reshape(*bchw)\n",
        "        return x\n",
        "\n",
        "\n",
        "# class SimpleViT(nn.Module):\n",
        "#     def __init__(self, in_dim, out_dim, dim, depth, heads, mlp_dim, channels=3, dim_head=8):\n",
        "#         super().__init__()\n",
        "#         self.to_patch_embedding = nn.Sequential( # in, out, kernel, stride, pad\n",
        "#             nn.Conv2d(in_dim, dim, kernel_size=7, stride=2, padding=7//2, bias=False), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "#             # nn.Conv2d(in_dim, dim, kernel_size=7, stride=2, padding=7//2, bias=False), nn.MaxPool2d(2,2)\n",
        "#             )\n",
        "#         # self.pos_embedding = LearnedRoPE2D(dim) # LearnedRoPE2D, RoPE2D\n",
        "#         self.positional_emb = nn.Parameter(torch.zeros(1, 8*8, dim)) # positional_embedding == 'learnable'\n",
        "#         self.transformer = AttentionBlock(d_model=dim, d_head=dim_head)\n",
        "#         self.transformer_blocks = nn.ModuleList([AttentionBlock(d_model=dim, d_head=dim_head) for i in range(depth)])\n",
        "\n",
        "#         self.attention_pool = nn.Linear(dim, 1)\n",
        "#         self.out = nn.Linear(dim, out_dim, bias=False)\n",
        "\n",
        "#     def forward(self, img):\n",
        "#         device = img.device\n",
        "#         x = self.to_patch_embedding(img)\n",
        "#         # x = self.pos_embedding(x)\n",
        "#         x = x.flatten(-2).transpose(-2,-1) # b c h w -> b (h w) c\n",
        "#         x = x + self.positional_emb\n",
        "#         x = self.transformer(x)\n",
        "#         # for blk in self.predictor_blocks: x = blk(x)\n",
        "\n",
        "#         # x = x.flatten(-2).mean(dim=-1) # meal pool\n",
        "#         attn_weights = self.attention_pool(x).squeeze(-1) # [batch, (h,w)] # seq_pool\n",
        "#         x = (attn_weights.softmax(dim = 1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, (h,w)] @ [batch, (h,w), dim]\n",
        "#         # cls_token = repeat(self.class_emb, '1 1 d -> b 1 d', b = b)\n",
        "#         # x = torch.cat((cls_token, x), dim=1)\n",
        "#         # x = x[:, 0] # first token\n",
        "#         return self.out(x)\n",
        "\n",
        "# ijepa: 224*224 -> 14*14 = 196\n",
        "# 32*7\n",
        "# me: 32*32=1024 -> 256\n",
        "\n",
        "import torch\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "def apply_masks(x, masks):\n",
        "    \"\"\":param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
        "    :param masks: list of tensors containing indices of patches in [N] to keep\"\"\"\n",
        "    all_x = []\n",
        "    for m in masks: # [1,T]\n",
        "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [1,T,dim]\n",
        "        all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "    return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "# batch, seq_len, d_model = 2,7,4\n",
        "# x = torch.rand(batch, seq_len, d_model)\n",
        "# print(x)\n",
        "# masks = [torch.randint(0,seq_len,(2,3,))]\n",
        "# print(masks)\n",
        "# out = apply_masks(x, masks)\n",
        "# print(out)\n",
        "# # print(out.shape)\n",
        "\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        # self.positional_emb = nn.Parameter(torch.zeros(1, 32*32//4, d_model)) # positional_embedding == 'learnable'\n",
        "        # encoder_layers = nn.TransformerEncoderLayer(d_model, d_model//d_head, d_hid or d_model, dropout, batch_first=True)\n",
        "        # self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.zeros(1,1,d_model)) # randn zeros\n",
        "        nn.init.trunc_normal_(self.cls, std=.02)\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model)\n",
        "        self.lin = nn.Linear(d_model, out_dim)# if out_dim != d_model else None\n",
        "\n",
        "    def forward(self, src, context_indices, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        src = self.embed(src) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = src.shape\n",
        "        src = src * self.pos_encoder(context_indices)\n",
        "        # src = src + self.positional_emb[:,context_indices]\n",
        "        # src = apply_masks(src, [context_indices])\n",
        "\n",
        "        pred_tokens = self.cls * self.pos_encoder(trg_indices) # [M, num_trg_toks, d_model]\n",
        "        # pred_tokens = self.cls + self.positional_emb[0,trg_indices]\n",
        "        pred_tokens = pred_tokens.repeat(batch, 1, 1) # [batch*M, num_trg_toks, d_model]\n",
        "        # print(pred_tokens.requires_grad)\n",
        "        # print(\"pred fwd\", src.shape, pred_tokens.shape)\n",
        "        # src = src.repeat_interleave(trg_indices.shape[0], dim=0) # [batch, seq_len, d_model] -> [batch*M, seq_len, d_model]\n",
        "        src = torch.cat([src, pred_tokens], dim=1) # [batch*M, seq_len+num_trg_toks, d_model]\n",
        "\n",
        "        out = self.transformer_encoder(src) # float [seq_len, batch_size, d_model]\n",
        "        out = self.norm(out)\n",
        "        out = out[:,seq:] # [batch*M, num_trg_toks, d_model]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.MaxPool1d(2,2), #nn.MaxPool1d(3, 2, 3//2),\n",
        "            nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.Conv1d(d_model, d_model,3,2,3//2)\n",
        "            # nn.Conv1d(in_dim, d_model,2,2,0), # like patch\n",
        "            )\n",
        "        self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        # self.positional_emb = nn.Parameter(torch.zeros(1, 32*32//4, d_model)) # positional_embedding == 'learnable'\n",
        "        # encoder_layers = nn.TransformerEncoderLayer(d_model, d_model//d_head, d_hid or d_model, dropout, batch_first=True)\n",
        "        # self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model)\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, src, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # src = self.embed(src) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        src = self.embed(src.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "\n",
        "        batch, seq, dim = src.shape\n",
        "\n",
        "        # # # src = self.pos_encoder(src)\n",
        "        # if context_indices != None:\n",
        "        # print(src.shape, self.pos_encoder(context_indices).shape)\n",
        "        #     src = src * self.pos_encoder(context_indices) # context/predictor # src = src + self.positional_emb[:,context_indices]\n",
        "        # else: src = src * self.pos_encoder(torch.arange(seq, device=device).unsqueeze(0)) # target # src = src + self.positional_emb[:,:seq]\n",
        "\n",
        "\n",
        "        # src = self.pos_encoder(src)\n",
        "        src = src * self.pos_encoder(torch.arange(seq, device=device)).unsqueeze(0)\n",
        "        # src = src + self.positional_emb\n",
        "        if context_indices != None:\n",
        "            src = apply_masks(src, [context_indices])\n",
        "\n",
        "\n",
        "        out = self.transformer_encoder(src) # float [seq_len, batch_size, d_model]\n",
        "        out = self.norm(out)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "batch, seq_len, d_model = 4,1024,4\n",
        "in_dim = 3\n",
        "model = TransformerModel(in_dim, d_model, d_head=4, nlayers=2, dropout=0.).to(device)\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "# # print(out)\n",
        "\n"
      ],
      "metadata": {
        "id": "GHzr3NVs2EcG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cecf37be-0d0f-4890-83f3-305b0b01c3ea",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 256, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(4,100,3)\n",
        "\n",
        "x=x.transpose(-2,-1) # [batch, dim, seq]\n",
        " # in, out, kernel, stride, pad\n",
        "# net = nn.Conv1d(3,16,7,2,7//2)\n",
        "net = nn.Sequential(nn.Conv1d(3,16,7,2,7//2), nn.MaxPool1d(3, 2, 3//2))\n",
        "# net = nn.Conv1d(3,16,(7,3),2,3//2)\n",
        "out = net(x)\n",
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_YBe6-eZ2Zq",
        "outputId": "09b27f75-ba04-4aaf-f5e4-1b9c1dc8d112"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 16, 25])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test repeat_interleave\n",
        "# x = torch.rand(3,5)\n",
        "x = torch.rand(3,5,7)\n",
        "print(x)\n",
        "# out = x.repeat(2,1) # [2*3,5]\n",
        "# print(out)\n",
        "# x_ = out.reshape(2,3,5)\n",
        "# print(x_)\n",
        "# out = x.repeat_interleave(2,1,1) # [3*2,5]\n",
        "out = x.repeat_interleave(2,dim=0) # [3*2,5]\n",
        "# print(out)\n",
        "# x_ = out.reshape(2,3,5,7)\n",
        "x_ = out.reshape(3,2,5,7)\n",
        "print(x_)\n",
        "\n",
        "\n",
        "# batch=1\n",
        "# seq_len=5\n",
        "# dim=4\n",
        "# positional_emb = torch.rand(batch, seq_len, dim)\n",
        "# print(positional_emb)\n",
        "# num_tok=2\n",
        "# trg_indices=torch.randint(0,seq_len,(M, num_tok))\n",
        "# print(trg_indices)\n",
        "# # pos_embs = positional_emb.gather(1, trg_indices.unsqueeze(0))\n",
        "# pos_embs = positional_emb[torch.arange(batch)[...,None,None],trg_indices.unsqueeze(0)]\n",
        "# # pos_embs = positional_emb[0,trg_indices]\n",
        "# # [batch, M, num_tok, dim]\n",
        "# print(pos_embs.shape)\n",
        "# print(pos_embs)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "R6kiiqw3uIdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test ropes\n",
        "# for i, (img, _) in enumerate(test_loader):\n",
        "#     break\n",
        "# print(img.shape) # [batch, 1, 28, 28]\n",
        "# print(img[0]) # [0,1)\n",
        "d_model=8\n",
        "# rot_emb = RotEmb(d_model, top=torch.pi, base=10)\n",
        "# x = torch.linspace(0,1,20)\n",
        "# out = rot_emb(x)\n",
        "# print(out.shape)\n",
        "# print(out)\n",
        "\n",
        "# # pos_encoder = RoPE(d_model, seq_len=15, base=100)\n",
        "# pos_encoder = RoPE(d_model, seq_len=15, base=10000)\n",
        "x = torch.ones(1, 10, d_model)\n",
        "# # x = torch.ones(1, 10, d_model)\n",
        "# out = pos_encoder(x)\n",
        "# # print(out.shape)\n",
        "# for x in out[0]:\n",
        "#     print(x)\n",
        "\n",
        "\n",
        "# pos_encoder = LearnedRoPE(d_model, seq_len=512)\n",
        "# out = pos_encoder(x)\n",
        "# for o in out[0]:\n",
        "#     print(o)\n",
        "\n",
        "# pos_encoder.weights = nn.Parameter(torch.randn(1, d_model//2))\n",
        "# out = pos_encoder(x)\n",
        "# for o in out[0]:\n",
        "#     print(o)\n",
        "\n",
        "\n",
        "# tensor([-0.0177, -0.9998,  0.8080, -0.5892, -0.7502, -0.6612,  0.3885,  0.9214])\n"
      ],
      "metadata": {
        "id": "8r_GjI_vCMyo",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Transformer Model\n",
        "import torch\n",
        "from torch import nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        # self.embed = nn.Sequential(\n",
        "        #     nn.Linear(in_dim, d_model), #act,\n",
        "        #     # nn.Linear(d_model, d_model), act,\n",
        "        # )\n",
        "        self.embed = nn.Linear(in_dim, d_model) if in_dim != d_model else None\n",
        "        # self.embed = nn.Sequential(nn.Conv1d(in_dim,d_model,7,2,7//2), nn.MaxPool1d(3, 2, 3//2))\n",
        "        self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, d_hid or d_model, dropout, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.d_model = d_model\n",
        "        self.cls = nn.Parameter(torch.zeros(1,1,d_model)) # randn\n",
        "        out_dim = out_dim or d_model\n",
        "        # self.lin = nn.Linear(d_model, out_dim)\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim != d_model else None\n",
        "        self.norm = nn.LayerNorm(out_dim)\n",
        "\n",
        "\n",
        "    def forward(self, src, src_key_padding_mask=None, cls_mask=None, context_indices=None, trg_indices=None): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # if cls_mask != None: src[cls_mask] = self.cls.to(src.dtype)\n",
        "\n",
        "        if self.embed != None: src = self.embed(src) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = src.shape\n",
        "        # src = self.pos_encoder(src)\n",
        "        if context_indices != None:\n",
        "            # print(src.shape, context_indices.shape, self.pos_encoder(context_indices).shape) # [2, 88, 32]) torch.Size([88]) torch.Size([88, 32]\n",
        "            # print(src[0], self.pos_encoder(context_indices)[0])\n",
        "            src = src * self.pos_encoder(context_indices) # context/predictor # src = src + self.positional_emb[:,context_indices]\n",
        "            # print(src[0])\n",
        "        else: src = src * self.pos_encoder(torch.arange(seq, device=device)) # target # src = src + self.positional_emb[:,:seq]\n",
        "            # print(\"trans fwd\", src.shape, self.pos_encoder(src).shape)\n",
        "\n",
        "        if trg_indices != None: # [M, num_trg_toks]\n",
        "            pred_tokens = self.cls * self.pos_encoder(trg_indices) # [M, num_trg_toks, d_model] # pred_tokens = self.cls + self.positional_emb[0,trg_indices]\n",
        "            pred_tokens = pred_tokens.repeat(batch, 1, 1) # [batch*M, num_trg_toks, d_model]\n",
        "            # print(pred_tokens.requires_grad)\n",
        "            src = src.repeat_interleave(trg_indices.shape[0], dim=0) # [batch, seq_len, d_model] -> [batch*M, seq_len, d_model]\n",
        "            src = torch.cat([src, pred_tokens], dim=1) # [batch*M, seq_len+num_trg_toks, d_model]\n",
        "\n",
        "        out = self.transformer_encoder(src, src_key_padding_mask=src_key_padding_mask) # float [seq_len, batch_size, d_model]\n",
        "        if trg_indices != None:\n",
        "            # print(out.shape)\n",
        "            out = out[:,seq:] # [batch*M, num_trg_toks, d_model]\n",
        "        if self.lin != None: out = self.lin(out)\n",
        "        out = self.norm(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "batch, seq_len, d_model = 4,7,64\n",
        "in_dim = 3\n",
        "model = TransformerModel(in_dim, d_model, nhead=8, nlayers=2, dropout=0.).to(device)\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "# print(out.shape)\n",
        "# # print(out)\n"
      ],
      "metadata": {
        "id": "-8i1WLsvH_vn",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ba31127-d6e5-438a-aede-5c789557ab39",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title SeqJEPA\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "def multiblock(seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "    return target_mask\n",
        "\n",
        "\n",
        "class SeqJEPA(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, num_layers=1, num_classes=10):\n",
        "        super().__init__()\n",
        "        if out_dim is None: out_dim = d_model\n",
        "        act = nn.GELU() # ReLU GELU SiLU\n",
        "        # self.rot_emb = RotEmb(d_model, top=1, base=10)\n",
        "        # self.rot_emb = RotEmb(d_model, top=torch.pi, base=10)\n",
        "        # self.rot_emb = RoPE(d_model, seq_len=1024, base=10)\n",
        "        # self.rot_emb = LearnedRotEmb(dim)\n",
        "        self.encode = nn.Sequential(\n",
        "            nn.Linear(in_dim, d_model), #act,\n",
        "            # nn.Linear(d_model, d_model), act,\n",
        "        )\n",
        "        # self.encode = nn.Sequential(nn.Conv1d(in_dim, d_model,7,2,7//2), nn.MaxPool1d(3, 2, 3//2))\n",
        "        self.context_encoder = TransformerModel(d_model, d_model, out_dim=out_dim, nhead=4, nlayers=2, dropout=0.)\n",
        "        self.predicter = TransformerModel(out_dim, d_model//2, out_dim, nhead=4, nlayers=1, dropout=0.)\n",
        "        self.target_encoder = AveragedModel(self.context_encoder, multi_avg_fn=get_ema_multi_avg_fn(0.85)) # 0.95 0.999\n",
        "        self.target_encoder.requires_grad_(False)\n",
        "        # self.classifier = nn.Linear(out_dim, num_classes)\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]\n",
        "        x = self.encode(x) # [batch, T, d_model]\n",
        "        # x = self.encode(x.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "\n",
        "        batch, seq, dim = x.shape\n",
        "        M=1 # 4\n",
        "        # target_mask = multiblock(seq, min_s=0.15, max_s=0.2, M=M) # mask out targets to be predicted # [M, seq]\n",
        "        target_mask = multiblock(seq, min_s=0.15, max_s=0.2, M=4)#.any(0) # mask out targets to be predicted # [M, seq]\n",
        "        context_mask = ~multiblock(seq, min_s=0.85, max_s=1., M=1)|target_mask.reshape(M,seq).any(0) # [1, seq], True->Mask\n",
        "        target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "        # print(target_mask, context_mask)\n",
        "        sorted_mask, ids = context_mask.int().sort(dim=1, stable=True)\n",
        "        context_indices = ids[~sorted_mask.bool()] # int idx [num_context_toks] , idx of context not masked\n",
        "        sorted_x = x[torch.arange(batch).unsqueeze(-1), context_indices] # [batch, num_context_toks, d_model]\n",
        "        # print(sorted_x.shape, context_indices.shape)[2, 9, 32]) torch.Size([9])\n",
        "        # print(sorted_x[0])\n",
        "        # print(sorted_x.is_leaf)\n",
        "        sorted_sx = self.context_encoder(sorted_x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "        # del sorted_x\n",
        "        # print(sorted_sx[0])\n",
        "\n",
        "        sorted_mask, ids = target_mask.int().sort(dim=1, stable=True)\n",
        "        # print(sorted_mask.shape, ids.shape, ids[~sorted_mask.bool()].shape, sorted_mask.sum(-1))\n",
        "        trg_indices = ids[sorted_mask.bool()].reshape(M,-1) # int idx [M, num_trg_toks] , idx of targets that are masked\n",
        "        # sorted_x = x[torch.arange(batch)[...,None,None], indices] # [batch, M, seq-num_trg_toks, dim]\n",
        "        # sx = sorted_sx[torch.arange(batch).unsqueeze(-1),ids.argsort(1)]\n",
        "        # print(sorted_sx.shape, trg_indices.shape)\n",
        "        sy_ = self.predicter(sorted_sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "        # sx = self.context_encoder(x, src_key_padding_mask=context_mask).repeat(M,1,1)\n",
        "        # # sy_ = self.predicter(sx, cls_mask=target_mask)*target_mask.unsqueeze(-1)\n",
        "        # sy_ = self.predicter(sx, trg_mask=target_mask)*target_mask.unsqueeze(-1)\n",
        "\n",
        "        # print(sy_.shape, target_mask.shape)\n",
        "        # print(self.target_encoder(x).repeat_interleave(M, dim=0).shape, trg_indices.repeat(batch,1).shape)\n",
        "        with torch.no_grad():\n",
        "            sy = self.target_encoder(x).repeat_interleave(M, dim=0)[torch.arange(batch*M).unsqueeze(-1), trg_indices.repeat(batch,1)] # [batch*M, num_trg_toks, out_dim]\n",
        "\n",
        "        # print(sy_[0])\n",
        "        # print(sy.shape, sy_.shape)\n",
        "        # loss = F.smooth_l1_loss(sy, sy_)\n",
        "        loss = F.mse_loss(sy, sy_)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]\n",
        "        batch, seq, dim = x.shape\n",
        "        target_mask = randpatch(seq//self.patch_size, mask_size=16, gamma=.75) # [seq]\n",
        "        context_mask = ~multiblock(seq//self.patch_size, min_s=0.85, max_s=1., M=1)|target_mask # [1, seq], True->Mask\n",
        "        target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "        # print(target_mask, context_mask)\n",
        "        sorted_mask, ids = context_mask.int().sort(dim=1, stable=True)\n",
        "        context_indices = ids[~sorted_mask.bool()]#.unsqueeze(0) # int idx [num_context_toks] , idx of context not masked\n",
        "        sorted_x = x[torch.arange(batch).unsqueeze(-1), context_indices] # [batch, num_context_toks, 3]\n",
        "        print(sorted_x.shape, context_indices.shape)\n",
        "        sorted_sx = self.context_encoder(sorted_x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "\n",
        "        sorted_mask, ids = target_mask.int().sort(dim=-1, stable=True)\n",
        "        trg_indices = ids[sorted_mask.bool()].unsqueeze(0) # int idx [1, num_trg_toks] , idx of targets that are masked\n",
        "        print(sorted_sx.shape, context_indices.shape)\n",
        "        sy_ = self.predicter(sorted_sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "\n",
        "        with torch.no_grad(): sy = self.target_encoder(x.detach())[torch.arange(batch).unsqueeze(-1), trg_indices.repeat(batch,1)] # [batch, num_trg_toks, out_dim]\n",
        "        # print(x[0][0][:8])\n",
        "        # print(sy_[0][0][:8])\n",
        "        # print(sy.shape, sy_.shape)\n",
        "        # loss = F.smooth_l1_loss(sy, sy_)\n",
        "        loss = F.mse_loss(sy.detach(), sy_)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        x = self.encode(x)\n",
        "        # x = self.encode(x.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "        sx = self.target_encoder(x)\n",
        "        out = sx.mean(dim=1)\n",
        "        return out\n",
        "\n",
        "\n",
        "# min_s=0.15, max_s, M\n",
        "# trg.15.2M4 C.85 1\n",
        "\n",
        "seq_jepa = SeqJEPA(in_dim=3, d_model=32, out_dim=16, num_layers=1).to(device)#.to(torch.float)\n",
        "optim = torch.optim.AdamW(seq_jepa.parameters(), lr=1e-4) # 1e-3?\n",
        "print(sum(p.numel() for p in seq_jepa.parameters() if p.requires_grad)) # 59850\n",
        "\n",
        "\n",
        "# x = torch.rand((2,20,3), device=device)\n",
        "# out = seq_jepa.loss(x)\n",
        "# print(out.shape)\n",
        "# print(x.is_leaf) # T\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title SeqJEPA old\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def multiblock(batch, seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "        mask_len = torch.rand(batch, M) * (max_s - min_s) + min_s # in (min_s, max_s)\n",
        "        mask_pos = torch.rand(batch, M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "        mask_len, mask_pos = mask_len * seq, mask_pos * seq\n",
        "        indices = torch.arange(seq)[None,None,...]\n",
        "        target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [batch, M, seq]\n",
        "        target_mask = target_mask.any(1) # True -> masked # multiblock masking # [batch, seq]\n",
        "        return target_mask\n",
        "\n",
        "\n",
        "\n",
        "class SeqJEPA(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.in_dim = in_dim\n",
        "        # if out_dim is None: self.out_dim = in_dim\n",
        "        if out_dim is None: out_dim = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.d_model = d_model\n",
        "        act = nn.GELU() # ReLU GELU SiLU\n",
        "\n",
        "        dim=8\n",
        "\n",
        "\n",
        "        self.calorie_emb = RotEmb(dim, top=1, base=10) # 0-4\n",
        "        self.steps_emb = RotEmb(dim, top=1, base=10) # 0-5\n",
        "        self.enc0 = nn.Sequential(\n",
        "            nn.Linear(dim*2, d_model), act,\n",
        "            nn.Linear(d_model, d_model), act,\n",
        "        )\n",
        "        self.hour_emb = CircularEmb(dim, torch.pi/2) # circular (0,1) # 0-24\n",
        "        self.temp_emb = RotEmb(dim, top=1/3, base=10) # 18-35\n",
        "        self.heart_emb = RotEmb(dim, top=1/3, base=100) # 50-200\n",
        "        self.enc1 = nn.Sequential(\n",
        "            nn.Linear(dim*3, d_model), act,\n",
        "        )\n",
        "\n",
        "        # self.transformer = TransformerClassifier(d_model, out_dim=out_dim, nhead=8, nlayers=2)\n",
        "        # # self.fc = nn.Linear(d_model, out_dim)\n",
        "        self.context_encoder = TransformerModel(d_model, out_dim=out_dim, nhead=8, nlayers=2, dropout=0.)\n",
        "        self.predicter = TransformerModel(out_dim, nhead=8, nlayers=1, dropout=0.)\n",
        "        self.target_encoder = AveragedModel(self.context_encoder, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    def loss(self, hr, temp, heart): # [batch, 1+T]\n",
        "        # batch = hr.size(0)\n",
        "        res_id, activeKilocalories, steps = hr[:,0], temp[:,0], heart[:,0]\n",
        "        # enc_summary = self.enc0(torch.cat([self.calorie_emb(activeKilocalories), self.steps_emb(steps)], dim=-1)) # [batch, d_model]\n",
        "        enc_summary = self.enc0(torch.cat([self.calorie_emb(torch.log(activeKilocalories.clamp(min=1))), self.steps_emb(torch.log(steps.clamp(min=1)))], dim=-1)) # [batch, d_model]\n",
        "        x = self.enc1(torch.cat([self.hour_emb(hr[:,1:]/24), self.temp_emb(temp[:,1:]), self.heart_emb(heart[:,1:])], dim=-1)) # [batch, T, d_model]\n",
        "        # print('violet fwd', hr.shape, enc_summary.shape, x.shape)\n",
        "\n",
        "        # # mask=(torch.rand(self.seq_len)<.1) # True -> masked # random masking\n",
        "        # mask=(torch.rand_like(x)<.1) # True -> masked # random masking\n",
        "\n",
        "        # patches pad -enc> patch_enc\n",
        "        # rearrange patch in padded/masked, -pred> pred of masked\n",
        "        # mse tgt_enc(img)\n",
        "\n",
        "        # average L2 distance between the predicted patch-level representations sˆy(i) and the target patch-level representation sy(i);\n",
        "        # F.smooth_l1_loss\n",
        "\n",
        "        batch, seq, dim = x.shape\n",
        "\n",
        "        # mask=(torch.rand(batch, seq)<.75) # True -> masked # random masking\n",
        "        # sorted_mask, ids = mask.sort(dim=1)\n",
        "        # # sorted_x = x[torch.arange(batch)[...,None,None],ids]\n",
        "        # sorted_x = x[torch.arange(batch).unsqueeze(-1),ids]\n",
        "        # sorted_mask = torch.cat([torch.zeros((batch, 1), dtype=torch.bool), sorted_mask], dim=1) # True->mask\n",
        "        # sorted_x = torch.cat([enc_summary.unsqueeze(1), sorted_x], dim=1)\n",
        "        # sorted_sx = self.context_encoder(sorted_x, src_key_padding_mask=sorted_mask)\n",
        "        # # torch.zeros_like(sorted_sx)\n",
        "        # sx = sorted_sx[torch.arange(batch).unsqueeze(-1),ids.argsort(1)]\n",
        "        # sy_ = self.predicter(sx, src_key_padding_mask=mask)\n",
        "        # sy = self.target_encoder(x)*~mask\n",
        "\n",
        "\n",
        "        # target_mask = multiblock(batch, seq, min_s=0.15, max_s=0.2, M=4)\n",
        "        # context_mask = ~multiblock(batch, seq, min_s=0.85, max_s=1., M=1)|target_mask\n",
        "        # sx = self.context_encoder(x, src_key_padding_mask=context_mask)\n",
        "        # sy_ = self.predicter(sx, cls_mask=target_mask)*target_mask.unsqueeze(-1)\n",
        "        # sy_ = self.predicter(sx, cls_mask=target_mask)\n",
        "        # # print(sy_.shape, target_mask.shape)\n",
        "        # sy = self.target_encoder(x)*target_mask.unsqueeze(-1)\n",
        "\n",
        "\n",
        "        M=4\n",
        "        target_mask = multiblock(M*batch, seq, min_s=0.15, max_s=0.2, M=1) # mask out targets to be predicted # [4*batch, seq]\n",
        "        context_mask = ~multiblock(batch, seq, min_s=0.85, max_s=1., M=1)|target_mask.reshape(batch,M,seq).any(1) # [batch, seq]\n",
        "\n",
        "\n",
        "        x = torch.cat([enc_summary.unsqueeze(1), x], dim=1) # [batch, 1+T, d_model]\n",
        "        target_mask = torch.cat([torch.zeros((M*batch, 1), dtype=bool), target_mask],dim=1) # [4*batch, 1+T]\n",
        "        context_mask = torch.cat([torch.zeros((batch, 1), dtype=bool), context_mask],dim=1) # [batch, 1+T]\n",
        "\n",
        "        # x = x.repeat(4,1,1)\n",
        "        # context_mask = context_mask.repeat(4,1)\n",
        "        sx = self.context_encoder(x, src_key_padding_mask=context_mask).repeat(M,1,1)\n",
        "        sy_ = self.predicter(sx, cls_mask=target_mask)*target_mask.unsqueeze(-1)\n",
        "        # sy_ = self.predicter(sx, cls_mask=target_mask)\n",
        "        # print(sy_.shape, target_mask.shape)\n",
        "        sy = self.target_encoder(x).repeat(M,1,1)*target_mask.unsqueeze(-1)\n",
        "\n",
        "        loss = F.smooth_l1_loss(sy, sy_)\n",
        "        return loss\n",
        "\n",
        "    # def forward(self, hr, temp, heart): # [batch, 1+T]\n",
        "    #     # batch = hr.size(0)\n",
        "    #     # batch, seq, dim = x.shape\n",
        "    #     res_id, activeKilocalories, steps = hr[:,0], temp[:,0], heart[:,0]\n",
        "    #     enc_summary = self.enc0(torch.cat([self.calorie_emb(activeKilocalories), self.steps_emb(steps)], dim=-1)) # [batch, d_model]\n",
        "    #     x = self.enc1(torch.cat([self.hour_emb(hr[:,1:]/24), self.temp_emb(temp[:,1:]), self.heart_emb(heart[:,1:])], dim=-1)) # [batch, T, d_model]\n",
        "\n",
        "    #     x = torch.cat([enc_summary.unsqueeze(1), x], dim=1)\n",
        "    #     # sx = self.context_encoder(x)\n",
        "    #     sx = self.target_encoder(x)\n",
        "    #     out = sx.mean(dim=1)\n",
        "    #     return out\n",
        "\n",
        "\n",
        "\n",
        "seq_jepa = SeqJEPA(in_dim=16, d_model=32, out_dim=16, num_layers=1).to(device)#.to(torch.float)\n",
        "soptim = torch.optim.AdamW(seq_jepa.parameters(), lr=1e-3)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "O-OU1MaKF7BZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ijepa vision_transformer.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/models/vision_transformer.py\n",
        "import math\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from src.utils.tensors import (trunc_normal_, repeat_interleave_batch)\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid height and width\n",
        "    return:\n",
        "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid_h = np.arange(grid_size, dtype=float)\n",
        "    grid_w = np.arange(grid_size, dtype=float)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    assert embed_dim % 2 == 0\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid length\n",
        "    return:\n",
        "    pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid = np.arange(grid_size, dtype=float)\n",
        "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=float)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega   # (D/2,)\n",
        "    pos = pos.reshape(-1)   # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
        "    emb = np.concatenate([np.sin(out), np.cos(out)], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x, attn\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x, return_attention=False):\n",
        "        y, attn = self.attn(self.norm1(x))\n",
        "        if return_attention:\n",
        "            return attn\n",
        "        x = x + self.drop_path(y)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvEmbed(nn.Module):\n",
        "    \"\"\"3x3 Convolution stems for ViT following ViTC models\"\"\"\n",
        "    def __init__(self, channels, strides, img_size=224, in_chans=3, batch_norm=True):\n",
        "        super().__init__()\n",
        "        # Build the stems\n",
        "        stem = []\n",
        "        channels = [in_chans] + channels\n",
        "        for i in range(len(channels) - 2):\n",
        "            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3, stride=strides[i], padding=1, bias=(not batch_norm))]\n",
        "            if batch_norm:\n",
        "                stem += [nn.BatchNorm2d(channels[i+1])]\n",
        "            stem += [nn.ReLU(inplace=True)]\n",
        "        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n",
        "        self.stem = nn.Sequential(*stem)\n",
        "\n",
        "        # Comptute the number of patches\n",
        "        stride_prod = int(np.prod(strides))\n",
        "        self.num_patches = (img_size[0] // stride_prod)**2\n",
        "\n",
        "    def forward(self, x):\n",
        "        p = self.stem(x)\n",
        "        return p.flatten(2).transpose(1, 2)\n",
        "\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "def apply_masks(x, masks):\n",
        "    \"\"\":param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
        "    :param masks: list of tensors containing indices of patches in [N] to keep\"\"\"\n",
        "    all_x = []\n",
        "    for m in masks:\n",
        "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [batch,T,dim]\n",
        "        all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "    return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "\n",
        "class VisionTransformerPredictor(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, num_patches, embed_dim=768, predictor_embed_dim=384,\n",
        "        depth=6, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.init_std = init_std\n",
        "        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        # --\n",
        "        self.predictor_pos_embed = nn.Parameter(torch.zeros(1, num_patches, predictor_embed_dim), requires_grad=False)\n",
        "        predictor_pos_embed = get_2d_sincos_pos_embed(self.predictor_pos_embed.shape[-1], int(num_patches**.5), cls_token=False)\n",
        "        self.predictor_pos_embed.data.copy_(torch.from_numpy(predictor_pos_embed).float().unsqueeze(0)) # [1, num_patches, predictor_embed_dim]\n",
        "        # --\n",
        "        self.predictor_blocks = nn.ModuleList([Block(dim=predictor_embed_dim, num_heads=num_heads, drop=drop_rate, drop_path=dpr[i]) for i in range(depth)])\n",
        "        self.predictor_norm = nn.LayerNorm(predictor_embed_dim)\n",
        "        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n",
        "        # ------\n",
        "        # trunc_normal_(self.mask_token, std=self.init_std)\n",
        "\n",
        "    def forward(self, x, masks_x, masks):\n",
        "        assert (masks is not None) and (masks_x is not None), 'Cannot run predictor without mask indices'\n",
        "        if not isinstance(masks_x, list): masks_x = [masks_x]\n",
        "        if not isinstance(masks, list): masks = [masks]\n",
        "\n",
        "        # -- Batch Size\n",
        "        B = len(x) // len(masks_x)\n",
        "\n",
        "        # -- map from encoder-dim to pedictor-dim\n",
        "        x = self.predictor_embed(x) # [batch, num_patches, predictor_embed_dim]\n",
        "\n",
        "        # -- add positional embedding to x tokens\n",
        "        x_pos_embed = self.predictor_pos_embed.repeat(B, 1, 1) # [batch, num_patches, predictor_embed_dim]\n",
        "        x += apply_masks(x_pos_embed, masks_x)\n",
        "\n",
        "        _, N_ctxt, D = x.shape\n",
        "\n",
        "        # -- concat mask tokens to x\n",
        "        pos_embs = self.predictor_pos_embed.repeat(B, 1, 1)\n",
        "        pos_embs = apply_masks(pos_embs, masks)\n",
        "        pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x))\n",
        "        # --\n",
        "        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1)\n",
        "        # --\n",
        "        pred_tokens += pos_embs\n",
        "        x = x.repeat(len(masks), 1, 1)\n",
        "        x = torch.cat([x, pred_tokens], dim=1)\n",
        "\n",
        "        # -- fwd prop\n",
        "        for blk in self.predictor_blocks:\n",
        "            x = blk(x)\n",
        "        x = self.predictor_norm(x)\n",
        "\n",
        "        # -- return preds for mask tokens\n",
        "        x = x[:, N_ctxt:]\n",
        "        x = self.predictor_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, img_size=[224], patch_size=16, in_chans=3, embed_dim=768, predictor_embed_dim=384, depth=12, predictor_depth=12,\n",
        "        num_heads=12, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0, norm_layer=nn.LayerNorm, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_features = self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.init_std = init_std\n",
        "        # --\n",
        "        self.patch_embed = PatchEmbed(img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        # --\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=False)\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "        # --\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "    def forward(self, x, masks=None):\n",
        "        if masks is not None:\n",
        "            if not isinstance(masks, list): masks = [masks]\n",
        "\n",
        "        x = self.patch_embed(x)\n",
        "        B, N, D = x.shape # [batch, T, d_model]\n",
        "\n",
        "        # -- add positional embedding to x\n",
        "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
        "        x = x + pos_embed\n",
        "\n",
        "        if masks is not None: x = apply_masks(x, masks)\n",
        "\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            x = blk(x)\n",
        "\n",
        "        if self.norm is not None: x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "    def interpolate_pos_encoding(self, x, pos_embed):\n",
        "        npatch = x.shape[1] - 1\n",
        "        N = pos_embed.shape[1] - 1\n",
        "        if npatch == N: return pos_embed\n",
        "        class_emb, pos_embed = pos_embed[:, 0], pos_embed[:, 1:]\n",
        "        dim = x.shape[-1]\n",
        "        pos_embed = nn.functional.interpolate(pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2), scale_factor=math.sqrt(npatch / N), mode='bicubic',)\n",
        "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
        "\n",
        "# from functools import partial\n",
        "# def vit_predictor(**kwargs):\n",
        "#     model = VisionTransformerPredictor(\n",
        "#         mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "#     return model\n",
        "\n",
        "def vit(patch_size=16, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, # tiny\n",
        "        # patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, # small\n",
        "        # patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, # base\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "bNjp4xFsGPoa",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title show collated_masks\n",
        "# print(collated_batch, collated_masks_enc, collated_masks_pred)\n",
        "# print(collated_batch) # tensor([0, 1, 2, 3]) batch\n",
        "# print(collated_masks_enc) # nenc*[M*[pred mask blocks patch ind]]\n",
        "# print(collated_masks_pred) # npred * [M * [context]]\n",
        "\n",
        "\n",
        "# print(len(collated_masks_enc[0]), len(collated_masks_pred[0]))\n",
        "# print(collated_masks_enc[0], collated_masks_pred[0])\n",
        "\n",
        "h,w=14,14\n",
        "img = torch.ones(h*w)\n",
        "# img[collated_masks_enc[0]]=0\n",
        "img[collated_masks_pred[0][2]]=0\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    # plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.imshow(npimg)\n",
        "    plt.show()\n",
        "\n",
        "# imshow(img.reshape(h,w))\n",
        "\n",
        "# for masks in collated_masks_pred:\n",
        "#     for mask in masks:\n",
        "#         img = torch.ones(h*w)\n",
        "#         img[mask]=0\n",
        "#         imshow(img.reshape(h,w))\n",
        "\n",
        "\n",
        "for masks in collated_masks_enc:\n",
        "    for mask in masks:\n",
        "        img = torch.ones(h*w)\n",
        "        img[mask]=0\n",
        "        imshow(img.reshape(h,w))\n"
      ],
      "metadata": {
        "id": "tf4T37woBV2V",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aH1UOD4WW8I2",
        "outputId": "6fafea1b-0af2-4bc7-fa26-46b5e56549b4",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vit fwd torch.Size([4, 3, 224, 224])\n",
            "vit fwd torch.Size([4, 196, 768])\n",
            "vit fwd torch.Size([4, 11, 768])\n",
            "predictor fwd1 torch.Size([4, 11, 384]) torch.Size([4, 196, 384])\n"
          ]
        }
      ],
      "source": [
        "# @title ijepa vision_transformer.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/models/vision_transformer.py\n",
        "import math\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# from src.utils.tensors import (trunc_normal_, repeat_interleave_batch)\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid height and width\n",
        "    return:\n",
        "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid_h = np.arange(grid_size, dtype=float)\n",
        "    grid_w = np.arange(grid_size, dtype=float)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    assert embed_dim % 2 == 0\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid length\n",
        "    return:\n",
        "    pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid = np.arange(grid_size, dtype=float)\n",
        "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=float)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega   # (D/2,)\n",
        "    pos = pos.reshape(-1)   # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
        "    emb = np.concatenate([np.sin(out), np.cos(out)], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "def repeat_interleave_batch(x, B, repeat):\n",
        "    N = len(x) // B\n",
        "    x = torch.cat([\n",
        "        torch.cat([x[i*B:(i+1)*B] for _ in range(repeat)], dim=0)\n",
        "        for i in range(N)\n",
        "    ], dim=0)\n",
        "    return x\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x, attn\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x, return_attention=False):\n",
        "        y, attn = self.attn(self.norm1(x))\n",
        "        if return_attention:\n",
        "            return attn\n",
        "        x = x + self.drop_path(y)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvEmbed(nn.Module):\n",
        "    \"\"\"3x3 Convolution stems for ViT following ViTC models\"\"\"\n",
        "    def __init__(self, channels, strides, img_size=224, in_chans=3, batch_norm=True):\n",
        "        super().__init__()\n",
        "        # Build the stems\n",
        "        stem = []\n",
        "        channels = [in_chans] + channels\n",
        "        for i in range(len(channels) - 2):\n",
        "            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3, stride=strides[i], padding=1, bias=(not batch_norm))]\n",
        "            if batch_norm:\n",
        "                stem += [nn.BatchNorm2d(channels[i+1])]\n",
        "            stem += [nn.ReLU(inplace=True)]\n",
        "        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n",
        "        self.stem = nn.Sequential(*stem)\n",
        "\n",
        "        # Comptute the number of patches\n",
        "        stride_prod = int(np.prod(strides))\n",
        "        self.num_patches = (img_size[0] // stride_prod)**2\n",
        "\n",
        "    def forward(self, x):\n",
        "        p = self.stem(x)\n",
        "        return p.flatten(2).transpose(1, 2)\n",
        "\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "def apply_masks(x, masks):\n",
        "    \"\"\":param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
        "    :param masks: list of tensors containing indices of patches in [N] to keep\"\"\"\n",
        "    all_x = []\n",
        "    for m in masks:\n",
        "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [batch,T,dim]\n",
        "        all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "    return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "\n",
        "class VisionTransformerPredictor(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, num_patches, embed_dim=768, predictor_embed_dim=384,\n",
        "        depth=6, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.init_std = init_std\n",
        "        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        # --\n",
        "        self.predictor_pos_embed = nn.Parameter(torch.zeros(1, num_patches, predictor_embed_dim), requires_grad=False)\n",
        "        predictor_pos_embed = get_2d_sincos_pos_embed(self.predictor_pos_embed.shape[-1], int(num_patches**.5), cls_token=False)\n",
        "        self.predictor_pos_embed.data.copy_(torch.from_numpy(predictor_pos_embed).float().unsqueeze(0)) # [1, num_patches, predictor_embed_dim]\n",
        "        # --\n",
        "        self.predictor_blocks = nn.ModuleList([Block(dim=predictor_embed_dim, num_heads=num_heads, drop=drop_rate, drop_path=dpr[i]) for i in range(depth)])\n",
        "        self.predictor_norm = nn.LayerNorm(predictor_embed_dim)\n",
        "        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n",
        "        # ------\n",
        "        # trunc_normal_(self.mask_token, std=self.init_std)\n",
        "\n",
        "    def forward(self, x, masks_x, masks): # [batch, num_context_patches, embed_dim], nenc*[batch, num_context_patches], npred*[batch, num_target_patches]\n",
        "        assert (masks is not None) and (masks_x is not None), 'Cannot run predictor without mask indices'\n",
        "        if not isinstance(masks_x, list): masks_x = [masks_x] # context mask\n",
        "        if not isinstance(masks, list): masks = [masks] # pred mask\n",
        "\n",
        "        # -- Batch Size\n",
        "        B = len(x) // len(masks_x)\n",
        "        # print(\"predictor fwd0\", x.shape) # [3, 121, 768])\n",
        "        # -- map from encoder-dim to pedictor-dim\n",
        "        x = self.predictor_embed(x) # [batch, num_patches, predictor_embed_dim]\n",
        "\n",
        "        # -- add positional embedding to x tokens\n",
        "        x_pos_embed = self.predictor_pos_embed.repeat(B, 1, 1) # [batch, num_patches, predictor_embed_dim]\n",
        "        print(\"predictor fwd1\", x.shape, x_pos_embed.shape) # [3, 121 or 11, 384], [3, 196, 384]\n",
        "        # print(\"predictor fwd masks_x\", masks_x)\n",
        "        x += apply_masks(x_pos_embed, masks_x) # get pos emb of context patches\n",
        "\n",
        "        # print(\"predictor fwd x\", x.shape) # [4, 11, 384])\n",
        "        _, N_ctxt, D = x.shape\n",
        "\n",
        "        # -- concat mask tokens to x\n",
        "        pos_embs = self.predictor_pos_embed.repeat(B, 1, 1) # [batch, num_patches, predictor_embed_dim]\n",
        "        pos_embs = apply_masks(pos_embs, masks) # [16, 104, predictor_embed_dim]\n",
        "        pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x)) # [16, 104, predictor_embed_dim]\n",
        "        # --\n",
        "        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1) # [16, 104, predictor_embed_dim]\n",
        "        # --\n",
        "        # print(\"predictor fwd pred_tokens\", pred_tokens.shape)\n",
        "\n",
        "        pred_tokens += pos_embs\n",
        "        x = x.repeat(len(masks), 1, 1)\n",
        "        x = torch.cat([x, pred_tokens], dim=1)\n",
        "\n",
        "        # -- fwd prop\n",
        "        for blk in self.predictor_blocks:\n",
        "            x = blk(x)\n",
        "        x = self.predictor_norm(x)\n",
        "\n",
        "        # -- return preds for mask tokens\n",
        "        x = x[:, N_ctxt:]\n",
        "        x = self.predictor_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, img_size=[224], patch_size=16, in_chans=3, embed_dim=768, predictor_embed_dim=384, depth=12, predictor_depth=12,\n",
        "        num_heads=12, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0, norm_layer=nn.LayerNorm, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_features = self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.init_std = init_std\n",
        "        # --\n",
        "        self.patch_embed = PatchEmbed(img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        # --\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=False)\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "        # --\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "    def forward(self, x, masks=None): # [batch,3,224,224]\n",
        "        if masks is not None:\n",
        "            if not isinstance(masks, list): masks = [masks]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        x = self.patch_embed(x)\n",
        "        B, N, D = x.shape # [batch, num_patches, embed_dim]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        # -- add positional embedding to x\n",
        "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
        "        x = x + pos_embed\n",
        "\n",
        "        if masks is not None: x = apply_masks(x, masks) # [batch, num_context_patches, embed_dim]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            x = blk(x)\n",
        "\n",
        "        if self.norm is not None: x = self.norm(x)\n",
        "        return x # [batch, num_context_patches, embed_dim]\n",
        "\n",
        "    def interpolate_pos_encoding(self, x, pos_embed):\n",
        "        npatch = x.shape[1] - 1\n",
        "        N = pos_embed.shape[1] - 1\n",
        "        if npatch == N: return pos_embed\n",
        "        class_emb, pos_embed = pos_embed[:, 0], pos_embed[:, 1:]\n",
        "        dim = x.shape[-1]\n",
        "        pos_embed = nn.functional.interpolate(pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2), scale_factor=math.sqrt(npatch / N), mode='bicubic',)\n",
        "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
        "\n",
        "# from functools import partial\n",
        "# def vit_predictor(**kwargs):\n",
        "#     model = VisionTransformerPredictor(\n",
        "#         mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "#     return model\n",
        "\n",
        "def vit(patch_size=16, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, # tiny\n",
        "        # patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, # small\n",
        "        # patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, # base\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "# encoder = vit()\n",
        "encoder = VisionTransformer(\n",
        "        patch_size=16, embed_dim=768, depth=1, num_heads=3, mlp_ratio=1,\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
        "\n",
        "# num_patches = (224/patch_size)^2\n",
        "# predictor = VisionTransformerPredictor(num_patches, embed_dim=768, predictor_embed_dim=384, depth=6, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02, **kwargs)\n",
        "predictor = VisionTransformerPredictor(num_patches=196, embed_dim=768, predictor_embed_dim=384, depth=1, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02)\n",
        "\n",
        "batch = 4\n",
        "img = torch.rand(batch, 3, 224, 224)\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/train.py\n",
        "mask_collator = MaskCollator(\n",
        "        input_size=(224, 224),\n",
        "        patch_size=16,\n",
        "        enc_mask_scale=(0.2, 0.8),\n",
        "        pred_mask_scale=(0.2, 0.8),\n",
        "        aspect_ratio=(0.3, 3.0),\n",
        "        nenc=1,\n",
        "        npred=4,\n",
        "        min_keep=4,\n",
        "        # allow_overlap=True)\n",
        "        allow_overlap=False)\n",
        "# self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
        "\n",
        "\n",
        "\n",
        "# v = mask_collator.step()\n",
        "# should pass the collater a list batch of idx?\n",
        "# collated_batch, collated_masks_enc, collated_masks_pred = mask_collator([batch,3,8])\n",
        "collated_batch, collated_masks_enc, collated_masks_pred = mask_collator([0,1,2,3])\n",
        "# print(v)\n",
        "\n",
        "\n",
        "import copy\n",
        "target_encoder = copy.deepcopy(encoder)\n",
        "\n",
        "\n",
        "def forward_target():\n",
        "    with torch.no_grad():\n",
        "        h = target_encoder(imgs)\n",
        "        h = F.layer_norm(h, (h.size(-1),))  # normalize over feature-dim\n",
        "        B = len(h)\n",
        "        # -- create targets (masked regions of h)\n",
        "        h = apply_masks(h, masks_pred)\n",
        "        h = repeat_interleave_batch(h, B, repeat=len(masks_enc))\n",
        "        return h\n",
        "\n",
        "def forward_context():\n",
        "    z = encoder(imgs, masks_enc)\n",
        "    z = predictor(z, masks_enc, masks_pred)\n",
        "    return z\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    m = next(momentum_scheduler)\n",
        "                    for param_q, param_k in zip(encoder.parameters(), target_encoder.parameters()):\n",
        "                        param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "\n",
        "\n",
        "# # num_context_patches = 121 if allow_overlap=True else 11\n",
        "z = encoder(img, collated_masks_enc) # [batch, num_context_patches, embed_dim]\n",
        "# print(len(collated_masks_enc), len(collated_masks_pred)) # nenc, npred\n",
        "# print(collated_masks_enc[0].shape, collated_masks_pred[0].shape) # , [batch, num_context_patches = 121 or 11], [batch, num_target_patches]\n",
        "out = predictor(z, collated_masks_enc, collated_masks_pred)\n",
        "# # print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test multiblock params\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "dataiter = iter(train_data)\n",
        "img,y = next(dataiter)\n",
        "img = img.unsqueeze(0)\n",
        "b,c,h,w = img.shape\n",
        "img = img.flatten(2).transpose(-2,-1).to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "print(img.shape)\n",
        "batch, seq,_ = img.shape\n",
        "\n",
        "\n",
        "# target_mask = randpatch(seq, mask_size=8, gamma=.9) # 8.9 [seq]\n",
        "target_mask = multiblock(seq, min_s=0.2, max_s=0.3, M=4).any(0) # best.2.3M4 og.15.2M4# mask out targets to be predicted # [M, seq]\n",
        "target_mask = multiblock(seq, min_s=0.2, max_s=0.3, M=4).any(0)\n",
        "\n",
        "context_mask = ~multiblock(seq, min_s=0.85, max_s=1., M=1)|target_mask # [1, seq], True->Mask\n",
        "\n",
        "# print(target_mask, context_mask)\n",
        "\n",
        "print(target_mask.shape, context_mask.shape)\n",
        "# target_img, context_img = img*target_mask.unsqueeze(-1), img*context_mask.unsqueeze(-1)\n",
        "target_img, context_img = img*~target_mask.unsqueeze(-1), img*~context_mask.unsqueeze(-1)\n",
        "# print(target_img.shape, context_img.shape)\n",
        "target_img, context_img = target_img.transpose(-2,-1).reshape(b,c,h,w), context_img.transpose(-2,-1).reshape(b,c,h,w)\n",
        "target_img, context_img = target_img.float(), context_img.float()\n",
        "\n",
        "# imshow(out.detach().cpu())\n",
        "imshow(target_img[0])\n",
        "imshow(context_img[0])\n"
      ],
      "metadata": {
        "id": "D6lVtbS5OHIv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 879
        },
        "outputId": "13b6ad36-0c36-4d1e-daf0-70b098b51a2e",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1024, 3])\n",
            "torch.Size([1024]) torch.Size([1, 1024])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "trg_msks = []\n",
        "ctx_msks = []\n",
        "for i in range(16):\n",
        "    # target_mask = randpatch(seq, mask_size=8, gamma=.9) # 8.9 [seq]\n",
        "    target_mask = multiblock(seq, min_s=0.2, max_s=0.3, M=4).any(0) # best.2.3M4 og.15.2M4# mask out targets to be predicted # [M, seq]\n",
        "    target_mask = multiblock(seq, min_s=0.2, max_s=0.3, M=4).any(0)\n",
        "\n",
        "    context_mask = ~multiblock(seq, min_s=0.85, max_s=1., M=1)|target_mask # [1, seq], True->Mask\n",
        "\n",
        "    # print(target_mask.shape, context_mask.shape)\n",
        "    target_img, context_img = img*~target_mask.unsqueeze(-1), img*~context_mask.unsqueeze(-1)\n",
        "    target_img, context_img = target_img.transpose(-2,-1).reshape(b,c,h,w).float(), context_img.transpose(-2,-1).reshape(b,c,h,w).float()\n",
        "    trg_msks.append(target_img)\n",
        "    ctx_msks.append(context_img)\n",
        "\n",
        "trg_msks = torch.cat(trg_msks, dim=0)\n",
        "ctx_msks = torch.cat(ctx_msks, dim=0)\n",
        "imshow(torchvision.utils.make_grid(trg_msks.cpu(), nrow=4))\n",
        "imshow(torchvision.utils.make_grid(ctx_msks.cpu(), nrow=4))\n"
      ],
      "metadata": {
        "id": "kiHDuPjB0SBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test multiblock mask\n",
        "def multiblock(seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    # mask_len, mask_pos = mask_len * seq, mask_pos * seq\n",
        "    indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "    return target_mask\n",
        "\n",
        "\n",
        "seq=400\n",
        "M=4\n",
        "\n",
        "# target_mask = multiblock(M, seq, min_s=0.15, max_s=0.2, M=1) # mask out targets to be predicted # [4*batch, seq]\n",
        "# context_mask = ~multiblock(1, seq, min_s=0.85, max_s=1., M=1)|target_mask.reshape(M,seq).any(0) # [batch, seq]\n",
        "\n",
        "\n",
        "target_mask = multiblock(seq, min_s=0.15, max_s=0.2, M=4) # mask out targets to be predicted # [4*batch, seq]\n",
        "context_mask = ~multiblock(seq, min_s=0.85, max_s=1., M=1)|target_mask.reshape(M,seq).any(0) # [batch, seq]\n",
        "\n",
        "# target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "\n",
        "# print(target_mask)\n",
        "# print(context_mask)\n",
        "\n",
        "print(target_mask.sum(-1))\n",
        "print(context_mask.sum(-1))\n",
        "\n",
        "\n",
        "# sorted_mask, ids = context_mask.sort(dim=1, stable=True)\n",
        "# indices = ids[~sorted_mask]#.reshape(M,-1) # int idx [num_context_toks] , idx of context not masked\n",
        "# sorted_x = x[torch.arange(batch).unsqueeze(-1), indices] # [batch, seq-num_trg_toks, dim]\n",
        "# print(indices)\n",
        "# print(x)\n",
        "# print(sorted_x)\n",
        "\n",
        "\n",
        "\n",
        "# sorted_mask, ids = target_mask.sort(dim=1, descending=False, stable=True)\n",
        "# # print(sorted_mask, ids)\n",
        "# indices = ids[~sorted_mask].reshape(M,-1) # int idx [M, seq-num_trg_toks] , idx of target not masked\n",
        "# print(indices)\n",
        "# batch=3\n",
        "# dim=2\n",
        "# # indices = indices.expand(batch,-1,-1)\n",
        "# # x = torch.arange(batch*seq).reshape(batch,seq).to(device)\n",
        "# x = torch.rand(batch, seq, dim)\n",
        "# print(x)\n",
        "# sorted_x = x[torch.arange(batch)[...,None,None], indices]\n",
        "# print(sorted_x.shape) # [batch, M, seq-num_trg_toks, dim]\n",
        "# print(sorted_x)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "p8xv1tpKIhim",
        "outputId": "e6613fee-2d32-4c6a-ef96-dee18da3aa6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([73, 73, 73, 73])\n",
            "tensor([248])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ijepa multiblock down\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "import math\n",
        "from multiprocessing import Value\n",
        "import torch\n",
        "_GLOBAL_SEED = 0\n",
        "\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=(224, 224),\n",
        "        patch_size=16,\n",
        "        enc_mask_scale=(0.2, 0.8),\n",
        "        pred_mask_scale=(0.2, 0.8),\n",
        "        aspect_ratio=(0.3, 3.0),\n",
        "        nenc=1,\n",
        "        npred=2,\n",
        "        min_keep=4,\n",
        "        allow_overlap=False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if not isinstance(input_size, tuple):\n",
        "            input_size = (input_size, ) * 2\n",
        "        self.patch_size = patch_size\n",
        "        self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.aspect_ratio = aspect_ratio\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "        self._itr_counter = Value('i', -1)  # collator is shared across worker processes\n",
        "\n",
        "    def step(self):\n",
        "        i = self._itr_counter\n",
        "        with i.get_lock():\n",
        "            i.value += 1\n",
        "            v = i.value\n",
        "        return v\n",
        "\n",
        "    def _sample_block_size(self, generator, scale, aspect_ratio_scale):\n",
        "        _rand = torch.rand(1, generator=generator).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.height * self.width * mask_scale)\n",
        "        # -- Sample block aspect-ratio\n",
        "        min_ar, max_ar = aspect_ratio_scale\n",
        "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
        "        while h >= self.height:\n",
        "            h -= 1\n",
        "        while w >= self.width:\n",
        "            w -= 1\n",
        "\n",
        "        return (h, w)\n",
        "\n",
        "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
        "        h, w = b_size\n",
        "\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # --\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            top = torch.randint(0, self.height - h, (1,))\n",
        "            left = torch.randint(0, self.width - w, (1,))\n",
        "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
        "            mask[top:top+h, left:left+w] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
        "        mask_complement[top:top+h, left:left+w] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        B = len(batch)\n",
        "\n",
        "        collated_batch = torch.utils.data.default_collate(batch)\n",
        "\n",
        "        seed = self.step()\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(seed)\n",
        "        p_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.pred_mask_scale,\n",
        "            aspect_ratio_scale=self.aspect_ratio)\n",
        "        e_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.enc_mask_scale,\n",
        "            aspect_ratio_scale=(1., 1.))\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.height * self.width\n",
        "        min_keep_enc = self.height * self.width\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                print(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "\n",
        "        return collated_batch, collated_masks_enc, collated_masks_pred\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "COvEnBXPYth3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pamap trash"
      ],
      "metadata": {
        "id": "8DxjYI9RMQoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Lou1sM download_datasets.sh\n",
        "# https://github.com/Lou1sM/HAR/blob/master/download_datasets.sh\n",
        "\n",
        "##!/bin/sh\n",
        "\n",
        "mkdir -p datasets\n",
        "cd datasets\n",
        "\n",
        "#PAMAP\n",
        "# echo -e \"\\n###############\"\n",
        "# echo GETTING AND PREPARING PAMAP DATA\n",
        "# echo -e \"###############\\n\"\n",
        "wget http://archive.ics.uci.edu/ml/machine-learning-databases/00231/PAMAP2_Dataset.zip\n",
        "unzip PAMAP2_Dataset.zip\n",
        "# python ../convert_data_to_np.py PAMAP\n",
        "\n",
        "#UCI\n",
        "# echo -e \"\\n###############\"\n",
        "# echo GETTING AND PREPARING UCI DATA\n",
        "# echo -e \"###############\\n\"\n",
        "mkdir -p UCI2\n",
        "cd UCI2\n",
        "wget http://archive.ics.uci.edu/ml/machine-learning-databases/00341/HAPT%20Data%20Set.zip\n",
        "unzip HAPT\\ Data\\ Set.zip\n",
        "# cd ..\n",
        "# python ../convert_data_to_np.py UCI-raw\n",
        "\n",
        "#mkdir -p capture24\n",
        "#cd capture24/\n",
        "\n",
        "#for i in $(seq -w 151)\n",
        "#do\n",
        "#    curl -JLO \"https://ora.ox.ac.uk/objects/uuid:92650814-a209-4607-9fb5-921eab761c11/download_file?safe_filename=P${i}.csv.gz&type_of_work=Dataset\"\n",
        "#done\n",
        "#\n",
        "#curl -JLO \"https://ora.ox.ac.uk/objects/uuid:92650814-a209-4607-9fb5-921eab761c11/download_file?safe_filename=metadata.csv&type_of_work=Dataset\"\n",
        "#curl -JLO \"https://ora.ox.ac.uk/objects/uuid:92650814-a209-4607-9fb5-921eab761c11/download_file?safe_filename=annotation-label-dictionary.csv&type_of_work=Dataset\"\n",
        "#\n",
        "#\n",
        "#for f in $(ls); do\n",
        "#    if [ ${f: -2} == \"gz\" ]; then\n",
        "#        gunzip $f;\n",
        "#    fi;\n",
        "#done\n",
        "\n",
        "#WISDM-v1\n",
        "# echo -e \"\\n###############\"\n",
        "# echo GETTING AND PREPARING WISDM-v1 DATA\n",
        "# echo -e \"###############\\n\"\n",
        "wget https://www.cis.fordham.edu/wisdm/includes/datasets/latest/WISDM_ar_latest.tar.gz\n",
        "gunzip WISDM_ar_latest.tar.gz\n",
        "tar -xf WISDM_ar_latest.tar\n",
        "\n",
        "# python ../convert_data_to_np.py WISDM-v1\n",
        "\n",
        "#WISDM-watch\n",
        "# echo -e \"\\n###############\"\n",
        "# echo GETTING AND PREPARING WISDM-watch DATA\n",
        "# echo -e \"###############\\n\"\n",
        "wget https://archive.ics.uci.edu/ml/machine-learning-databases/00507/wisdm-dataset.zip\n",
        "unzip wisdm-dataset.zip\n",
        "python ../convert_data_to_np.py WISDM-watch\n",
        "\n",
        "#REALDISP\n",
        "# echo -e \"\\n###############\"\n",
        "# echo GETTING REALDISP DATA\n",
        "# echo -e \"###############\\n\"\n",
        "mkdir -p realdisp\n",
        "cd realdisp\n",
        "mkdir -p RawData\n",
        "wget https://archive.ics.uci.edu/ml/machine-learning-databases/00305/realistic_sensor_displacement.zip\n",
        "unzip realistic_sensor_displacement.zip\n",
        "# cd ..\n",
        "# python ../convert_data_to_np.py REALDISP\n",
        "# cd ..\n",
        "# pwd\n",
        "#HHAR\n",
        "mkdir -p hhar\n",
        "wget http://archive.ics.uci.edu/ml/machine-learning-databases/00344/Activity%20recognition%20exp.zip\n",
        "unzip Activity\\ recognition\\ exp.zip\n",
        "# python ../convert_data_to_np.py HHAR\n",
        "# cd ..\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "cellView": "form",
        "id": "38tWCkK-0Zlu",
        "outputId": "7abe4629-3dcf-4981-810e-9e888be03cb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "leading zeros in decimal integer literals are not permitted; use an 0o prefix for octal integers (<ipython-input-1-94e7ebe34632>, line 10)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-94e7ebe34632>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    wget http://archive.ics.uci.edu/ml/machine-learning-databases/00231/PAMAP2_Dataset.zip\u001b[0m\n\u001b[0m                                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m leading zeros in decimal integer literals are not permitted; use an 0o prefix for octal integers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title PAMAP2\n",
        "# https://arxiv.org/pdf/2209.08335v1\n",
        "# https://archive.ics.uci.edu/dataset/231/pamap2+physical+activity+monitoring\n",
        "# https://archive.ics.uci.edu/static/public/231/pamap2+physical+activity+monitoring.zip\n",
        "# !wget http://archive.ics.uci.edu/ml/machine-learning-databases/00231/PAMAP2_Dataset.zip\n",
        "# !unzip PAMAP2_Dataset.zip\n",
        "\n",
        "import os\n",
        "data_dir = 'PAMAP2_Dataset/Protocol'\n",
        "np_dir = '/content/PAMAP2'\n",
        "if not os.path.isdir(np_dir):\n",
        "    os.makedirs(np_dir)\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def array_from_txt(inpath):\n",
        "    with open(inpath) as f:\n",
        "        d = f.readlines()\n",
        "        array = np.array([[float(x) for x in line.split()] for line in d])\n",
        "    return array\n",
        "\n",
        "def array_expanded(a,expanded_length):\n",
        "    if a.shape[0] >= expanded_length: return a\n",
        "    insert_every = mpf(a.shape[0]/(expanded_length-a.shape[0]))\n",
        "    additional_idxs = (np.arange(expanded_length-a.shape[0])*insert_every).astype(np.int)\n",
        "    values = a[additional_idxs]\n",
        "    expanded = np.insert(a,additional_idxs,values,axis=0)\n",
        "    assert expanded.shape[0] == expanded_length\n",
        "    return expanded\n",
        "\n",
        "def convert(inpath,outpath):\n",
        "    array = array_from_txt(inpath)\n",
        "    timestamps = array[:,0]\n",
        "    labels = array[:,1].astype(int)\n",
        "    # Delete orientation data, which webpage says is 'invalid in this data, and timestamp and label\n",
        "    array = np.delete(array,[0,1,2,13,14,15,16,30,31,32,33,47,48,49,50],1)\n",
        "    np.save(outpath+'_timestamps',timestamps)\n",
        "    np.save(outpath+'_labels',labels)\n",
        "    np.save(outpath,array)\n",
        "\n",
        "for filename in os.listdir(data_dir):\n",
        "    print(data_dir,filename)\n",
        "    # inpath = os.path.join(data_dir,filename)a\n",
        "    inpath = data_dir+'/'+filename\n",
        "    outpath = np_dir+'/'+filename.split('.')[0]\n",
        "    print(outpath)\n",
        "    # outpath = os.path.join(np_dir,filename.split('.')[0])\n",
        "    convert(inpath,outpath)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMvb1-PBQ9u-",
        "outputId": "764e8452-935e-4bcc-aa38-82faddee7da3",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PAMAP2_Dataset/Protocol subject106.dat\n",
            "/content/PAMAP2/subject106\n",
            "PAMAP2_Dataset/Protocol subject101.dat\n",
            "/content/PAMAP2/subject101\n",
            "PAMAP2_Dataset/Protocol subject107.dat\n",
            "/content/PAMAP2/subject107\n",
            "PAMAP2_Dataset/Protocol subject109.dat\n",
            "/content/PAMAP2/subject109\n",
            "PAMAP2_Dataset/Protocol subject103.dat\n",
            "/content/PAMAP2/subject103\n",
            "PAMAP2_Dataset/Protocol subject104.dat\n",
            "/content/PAMAP2/subject104\n",
            "PAMAP2_Dataset/Protocol subject108.dat\n",
            "/content/PAMAP2/subject108\n",
            "PAMAP2_Dataset/Protocol subject105.dat\n",
            "/content/PAMAP2/subject105\n",
            "PAMAP2_Dataset/Protocol subject102.dat\n",
            "/content/PAMAP2/subject102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# def make_pamap_dset_train_val(args,subj_ids):\n",
        "def make_pamap_dset_train_val(subj_ids):\n",
        "    # dset_info_object = PAMAP_INFO.PAMAP_INFO\n",
        "    dset_info_object = PAMAP_INFO\n",
        "    # x_train = np.concatenate([np.load(f'datasets/PAMAP2_Dataset/np_data/subject{s}.npy') for s in subj_ids])\n",
        "    # y_train = np.concatenate([np.load(f'datasets/PAMAP2_Dataset/np_data/subject{s}_labels.npy') for s in subj_ids])\n",
        "    x_train = np.concatenate([np.load(f'PAMAP2/subject{s}.npy') for s in subj_ids])\n",
        "    y_train = np.concatenate([np.load(f'PAMAP2/subject{s}_labels.npy') for s in subj_ids])\n",
        "    x_train = x_train[y_train!=0] # 0 is a transient activity\n",
        "    y_train = y_train[y_train!=0] # 0 is a transient activity\n",
        "    # x_train,y_train,selected_acts = preproc_xys(x_train,y_train,args.step_size,args.window_size,dset_info_object,subj_ids)\n",
        "    x_train,y_train,selected_acts = preproc_xys(x_train,y_train,1,1,dset_info_object,subj_ids)\n",
        "    # dset_train = StepDataset(x_train,y_train,window_size=args.window_size,step_size=args.step_size)\n",
        "    # dset_train = StepDataset(x_train,y_train,window_size=512,step_size=5)\n",
        "    dset_train = StepDataset(x_train,y_train,window_size=1,step_size=1)\n",
        "    return dset_train, selected_acts\n",
        "\n",
        "class HAR_Dataset_Container():\n",
        "    def __init__(self,code_name,dataset_dir_name,possible_subj_ids,num_channels,num_classes,action_name_dict):\n",
        "        self.code_name = code_name\n",
        "        self.dataset_dir_name = dataset_dir_name\n",
        "        self.possible_subj_ids = possible_subj_ids\n",
        "        self.num_channels = num_channels\n",
        "        self.num_classes = num_classes\n",
        "        self.action_name_dict = action_name_dict\n",
        "\n",
        "\n",
        "# PAMAP\n",
        "pamap_ids = [str(x) for x in range(101,110)]\n",
        "pamap_action_name_dict = {1:'lying',2:'sitting',3:'standing',4:'walking',5:'running',6:'cycling',7:'Nordic walking',9:'watching TV',10:'computer work',11:'car driving',12:'ascending stairs',13:'descending stairs',16:'vacuum cleaning',17:'ironing',18:'folding laundry',19:'house cleaning',20:'playing soccer',24:'rope jumping'}\n",
        "PAMAP_INFO = HAR_Dataset_Container(\n",
        "            code_name = 'PAMAP',\n",
        "            dataset_dir_name = 'PAMAP2_Dataset',\n",
        "            possible_subj_ids = pamap_ids,\n",
        "            num_channels = 39,\n",
        "            num_classes = 12,\n",
        "            action_name_dict = pamap_action_name_dict)\n",
        "\n",
        "dset_train, selected_acts = make_pamap_dset_train_val(pamap_ids)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "5TnsfKrMB0yF",
        "outputId": "07859402-b33c-4ede-c389-ef7a59c2f1ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no precomputed datasets, computing from scratch\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "invalid index to scalar variable.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-ddcbc2f0291f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m             action_name_dict = pamap_action_name_dict)\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mdset_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_acts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_pamap_dset_train_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpamap_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-ddcbc2f0291f>\u001b[0m in \u001b[0;36mmake_pamap_dset_train_val\u001b[0;34m(subj_ids)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# 0 is a transient activity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# x_train,y_train,selected_acts = preproc_xys(x_train,y_train,args.step_size,args.window_size,dset_info_object,subj_ids)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mselected_acts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreproc_xys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdset_info_object\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msubj_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;31m# dset_train = StepDataset(x_train,y_train,window_size=args.window_size,step_size=args.step_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# dset_train = StepDataset(x_train,y_train,window_size=512,step_size=5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-a048671b2310>\u001b[0m in \u001b[0;36mpreproc_xys\u001b[0;34m(x, y, step_size, window_size, dset_info_object, subj_ids)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mnum_windows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mstep_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m#mode_labels = np.array([stats.mode(y[w*step_size:w*step_size + window_size]).mode[0] if (y[w*step_size:w*step_size + window_size]==y[w*step_size]).all() else -1 for w in range(num_windows)])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mmode_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstep_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_windows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mselected_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mselected_acts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdset_info_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_name_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mact_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mact_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselected_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-a048671b2310>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mnum_windows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mstep_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m#mode_labels = np.array([stats.mode(y[w*step_size:w*step_size + window_size]).mode[0] if (y[w*step_size:w*step_size + window_size]==y[w*step_size]).all() else -1 for w in range(num_windows)])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mmode_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstep_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_windows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mselected_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mselected_acts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdset_info_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_name_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mact_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mact_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselected_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Lou1sM convert_data_to_np.py\n",
        "# https://github.com/Lou1sM/HAR/blob/master/convert_data_to_np.py\n",
        "from pdb import set_trace\n",
        "from collections import Counter\n",
        "from scipy.fft import fft\n",
        "from scipy import stats\n",
        "from mpmath import mp, mpf\n",
        "#from dl_utils import misc, label_funcs\n",
        "# from dl_utils import misc\n",
        "# import label_funcs_tmp\n",
        "import os\n",
        "from os.path import join\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "def array_from_txt(inpath):\n",
        "    with open(inpath) as f:\n",
        "        d = f.readlines()\n",
        "        array = np.array([[float(x) for x in line.split()] for line in d])\n",
        "    return array\n",
        "\n",
        "def array_expanded(a,expanded_length):\n",
        "    if a.shape[0] >= expanded_length: return a\n",
        "    insert_every = mpf(a.shape[0]/(expanded_length-a.shape[0]))\n",
        "    additional_idxs = (np.arange(expanded_length-a.shape[0])*insert_every).astype(np.int)\n",
        "    values = a[additional_idxs]\n",
        "    expanded = np.insert(a,additional_idxs,values,axis=0)\n",
        "    assert expanded.shape[0] == expanded_length\n",
        "    return expanded\n",
        "\n",
        "def convert(inpath,outpath):\n",
        "    array = array_from_txt(inpath)\n",
        "    timestamps = array[:,0]\n",
        "    labels = array[:,1].astype(np.int)\n",
        "    # Delete orientation data, which webpage says is 'invalid in this data, and timestamp and label\n",
        "    array = np.delete(array,[0,1,2,13,14,15,16,30,31,32,33,47,48,49,50],1)\n",
        "    np.save(outpath+'_timestamps',timestamps)\n",
        "    np.save(outpath+'_labels',labels)\n",
        "    np.save(outpath,array)\n",
        "\n",
        "def expand_and_fill_labels(a,propoer_length):\n",
        "    start_filler = -np.ones(a[0,3])\n",
        "    end_filler = -np.ones(propoer_length-a[-1,4])\n",
        "    nested_lists = [[a[i,2] for _ in range(a[i,4]-a[i,3])] + [-1]*(a[i+1,3]-a[i,4]) for i in range(len(a)-1)] + [[a[-1,2] for _ in range(a[-1,4]-a[-1,3])]]\n",
        "    middle = np.array([item for sublist in nested_lists for item in sublist])\n",
        "    total_label_array = np.concatenate((start_filler,middle,end_filler)).astype(np.int)\n",
        "    return total_label_array\n",
        "\n",
        "def add_dtft(signal):\n",
        "    fft_signal_complex = fft(signal,axis=-1)\n",
        "    fft_signal_modulusses = np.abs(fft_signal_complex)\n",
        "    return np.concatenate((signal,fft_signal_modulusses),axis=-1)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "# if sys.argv[1] == 'PAMAP':\n",
        "data_dir = 'PAMAP2_Dataset/Protocol'\n",
        "np_dir = 'PAMAP2_Dataset/np_data'\n",
        "print(\"\\n#####Preprocessing PAMAP2#####\\n\")\n",
        "if not os.path.isdir(np_dir):\n",
        "    os.makedirs(np_dir)\n",
        "\n",
        "for filename in os.listdir(data_dir):\n",
        "    print(filename)\n",
        "    inpath = join(data_dir,filename)\n",
        "    outpath = join(np_dir,filename.split('.')[0])\n",
        "    convert(inpath,outpath)\n",
        "\n",
        "# elif sys.argv[1] == 'UCI-raw':\n",
        "data_dir = 'UCI2/RawData'\n",
        "np_dir = 'UCI2/np_data'\n",
        "print(\"\\n#####Preprocessing UCI#####\\n\")\n",
        "if not os.path.isdir(np_dir):\n",
        "    os.makedirs(np_dir)\n",
        "\n",
        "raw_label_array = array_from_txt(join(data_dir,'labels.txt')).astype(int)\n",
        "def two_digitify(x): return '0'+str(x) if len(str(x))==1 else str(x)\n",
        "fnames = os.listdir(data_dir)\n",
        "for idx in range(1,31):\n",
        "    print(\"processing user\",idx)\n",
        "    acc_array_list = []\n",
        "    gyro_array_list = []\n",
        "    label_array_list = []\n",
        "    user_idx = two_digitify(idx)\n",
        "    acc_fpaths = sorted([fn for fn in fnames if f'user{user_idx}' in fn and 'acc' in fn])\n",
        "    gyro_fpaths = sorted([fn for fn in fnames if f'user{user_idx}' in fn and 'gyro' in fn])\n",
        "    assert len(acc_fpaths) == len(gyro_fpaths)\n",
        "    for fna,fng in zip(acc_fpaths,gyro_fpaths):\n",
        "        acc_exp_id = int(fna.split('exp')[1][:2])\n",
        "        gyro_exp_id = int(fng.split('exp')[1][:2])\n",
        "        assert acc_exp_id==gyro_exp_id\n",
        "        new_acc_array = array_from_txt(join(data_dir,fna))\n",
        "        new_gyro_array = array_from_txt(join(data_dir,fna))\n",
        "        label_array_block = raw_label_array[raw_label_array[:,0]==acc_exp_id]\n",
        "        filled_label_array_block = expand_and_fill_labels(label_array_block,new_acc_array.shape[0])\n",
        "        assert filled_label_array_block.shape[0] == new_acc_array.shape[0]\n",
        "        assert filled_label_array_block.shape[0] == new_gyro_array.shape[0]\n",
        "        label_array_list.append(filled_label_array_block)\n",
        "        acc_array_list.append(new_acc_array)\n",
        "        gyro_array_list.append(new_gyro_array)\n",
        "    label_array = np.concatenate(label_array_list)\n",
        "    acc_array = np.concatenate(acc_array_list)\n",
        "    gyro_array = np.concatenate(gyro_array_list)\n",
        "    total_array = np.concatenate((acc_array,gyro_array),axis=1)\n",
        "    outpath = join(np_dir,f'user{user_idx}.npy')\n",
        "    np.save(outpath,total_array)\n",
        "    label_outpath = join(np_dir,f'user{user_idx}_labels.npy')\n",
        "    np.save(label_outpath,label_array)\n",
        "\n",
        "# elif sys.argv[1] == 'WISDM-v1':\n",
        "with open('WISDM_ar_v1.1/WISDM_ar_v1.1_raw.txt') as f: text = f.readlines()\n",
        "print(\"\\n#####Preprocessing WISDM-v1#####\\n\")\n",
        "activities_list = ['Jogging','Walking','Upstairs','Downstairs','Standing','Sitting']\n",
        "X_list = []\n",
        "y_list = []\n",
        "users_list = []\n",
        "num_zeros = 0\n",
        "def process_line(line_to_process):\n",
        "    global num_zeros\n",
        "    if float(line_to_process.split(',')[2]) == 0: num_zeros += 1#print(\"Timestamp zero, discarding\")\n",
        "    else:\n",
        "        X_list.append([float(x) for x in line_to_process.split(',')[3:]])\n",
        "        y_list.append(activities_list.index(line_to_process.split(',')[1]))\n",
        "        users_list.append(line_to_process.split(',')[0])\n",
        "for i,raw_line in enumerate(text):\n",
        "    #line = line.replace(';','').replace('\\n','')\n",
        "    if raw_line == '\\n': continue\n",
        "    elif raw_line.endswith(',;\\n'): line = raw_line[:-3]\n",
        "    elif raw_line.endswith(';\\n'): line = raw_line[:-2]\n",
        "    elif raw_line.endswith(',\\n'): line = raw_line[:-2]\n",
        "    else: set_trace()\n",
        "    if len(line.split(',')) == 6:\n",
        "        try: process_line(line)\n",
        "        except: print(f\"Can't process line {i}, even though length 6: {raw_line}\\n\")\n",
        "    else:\n",
        "        print(f\"Bad format at line {i}:\\n{raw_line}\")\n",
        "        try:\n",
        "            line1, line2 = line.split(';')\n",
        "            process_line(line1); process_line(line2)\n",
        "            print(f\"I think this was two lines erroneously put on one line. Processing separately as\\n{line1}\\nand\\n{line2}\")\n",
        "        except: print(\"Can't process this line at all, omitting\")\n",
        "one_big_X_array = np.array(X_list)\n",
        "one_big_y_array = np.array(y_list)\n",
        "one_big_users_array = np.array(users_list)\n",
        "print(one_big_X_array.shape)\n",
        "print(one_big_y_array.shape)\n",
        "print(one_big_users_array.shape)\n",
        "print(f\"Number of zero lines: {num_zeros}\")\n",
        "misc.np_save(one_big_X_array,'wisdm_v1','X.npy')\n",
        "misc.np_save(one_big_y_array,'wisdm_v1','y.npy')\n",
        "misc.np_save(one_big_users_array,'wisdm_v1','users.npy')\n",
        "\n",
        "# elif sys.argv[1] == 'WISDM-watch':\n",
        "p_dir = 'wisdm-dataset/raw/phone'\n",
        "w_dir = 'wisdm-dataset/raw/watch'\n",
        "np_dir = 'wisdm-dataset/np_data'\n",
        "print(\"\\n#####Preprocessing WISDM-watch#####\\n\")\n",
        "if not os.path.isdir(np_dir):\n",
        "    os.makedirs(np_dir)\n",
        "\n",
        "mp.dps = 100 # Avoid floating point errors in label insertion function\n",
        "for user_idx in range(1600,1651):\n",
        "    print('user', user_idx)\n",
        "    phone_acc_path = join(p_dir,'accel',f'data_{user_idx}_accel_phone.txt')\n",
        "    watch_acc_path = join(w_dir,'accel',f'data_{user_idx}_accel_watch.txt')\n",
        "    phone_gyro_path = join(p_dir,'gyro',f'data_{user_idx}_gyro_phone.txt')\n",
        "    watch_gyro_path = join(w_dir,'gyro',f'data_{user_idx}_gyro_watch.txt')\n",
        "\n",
        "    label_codes_list = list('ABCDEFGHIJKLMOPQRS') # Missin 'N' is deliberate\n",
        "    def two_arrays_from_txt(inpath):\n",
        "        with open(inpath) as f:\n",
        "            d = f.readlines()\n",
        "            arr = np.array([[float(x) for x in line.strip(';\\n').split(',')[3:]] for line in d])\n",
        "            label_array = np.array([label_codes_list.index(line.split(',')[1]) for line in d])\n",
        "        return arr, label_array\n",
        "\n",
        "    phone_acc, label_array1 = two_arrays_from_txt(phone_acc_path)\n",
        "    watch_acc, label_array2 = two_arrays_from_txt(watch_acc_path)\n",
        "    phone_gyro, label_array3 = two_arrays_from_txt(phone_gyro_path)\n",
        "    watch_gyro, label_array4 = two_arrays_from_txt(watch_gyro_path)\n",
        "    user_arrays = [phone_acc,watch_acc,phone_gyro,watch_gyro]\n",
        "    label_arrays = [label_array1,label_array2,label_array3,label_array4]\n",
        "    max_len = max([a.shape[0] for a in user_arrays])\n",
        "    equalized_user_arrays = [array_expanded(a,max_len) for a in user_arrays]\n",
        "    equalized_label_arrays = [array_expanded(lab_a,max_len) for lab_a in label_arrays]\n",
        "    total_user_array = np.concatenate(equalized_user_arrays,axis=1)\n",
        "    mode_object = stats.mode(np.stack(equalized_label_arrays,axis=1),axis=1)\n",
        "    mode_labels = mode_object.mode[:,0]\n",
        "    # Print how many windows contained just 1 label, how many 2 etc.\n",
        "    #print('Agreement in labels:',label_funcs_tmp.label_counts(mode_object.count[:,0]))\n",
        "    certains = (mode_object.count == 4)[:,0]\n",
        "    user_fn = f'{user_idx}.npy'\n",
        "    misc.np_save(total_user_array,np_dir,user_fn)\n",
        "    user_labels_fn = f'{user_idx}_labels.npy'\n",
        "    misc.np_save(mode_labels,np_dir,user_labels_fn)\n",
        "    user_certains_fn = f'{user_idx}_certains.npy'\n",
        "    misc.np_save(certains,np_dir,user_certains_fn)\n",
        "\n",
        "# elif sys.argv[1] == 'REALDISP':\n",
        "data_dir = 'realdisp/RawData'\n",
        "np_dir = 'realdisp/np_data'\n",
        "print(\"\\n#####Preprocessing REALDISP#####\\n\")\n",
        "if not os.path.isdir(np_dir):\n",
        "    os.makedirs(np_dir)\n",
        "\n",
        "for filename in os.listdir(data_dir):\n",
        "    print(filename)\n",
        "    if filename == 'dataset manual.pdf': continue\n",
        "    if not filename.split('_')[1].startswith('ideal'):\n",
        "        continue\n",
        "    with open(join(data_dir,filename)) as f: xy = f.readlines()\n",
        "    ar = np.array([[float(item) for item in line.split('\\t')] for line in xy])\n",
        "    x = ar[:,:-1]\n",
        "    y = ar[:,-1].astype(int)\n",
        "\n",
        "    np.save(join(np_dir,filename.split('_')[0]), x)\n",
        "    np.save(join(np_dir,filename.split('_')[0])+'_labels', y)\n",
        "\n",
        "# elif sys.argv[1] == 'Capture24':\n",
        "np_dir = 'capture24/np_data'\n",
        "if not os.path.isdir(np_dir):\n",
        "    os.makedirs(np_dir)\n",
        "name_df = pd.read_csv('capture24/annotation-label-dictionary.csv')\n",
        "#name_conversion_dict = dict(zip(name_df['annotation'],name_df['label:DohertySpecific2018']))\n",
        "name_df = name_df[['annotation','label:DohertySpecific2018']]\n",
        "int_label_converter_df = pd.DataFrame(enumerate(name_df['label:DohertySpecific2018'].unique()),columns=['int_label','label:DohertySpecific2018'])\n",
        "int_label_converter_dict = dict(enumerate(name_df['label:DohertySpecific2018'].unique()))\n",
        "with open('capture24/int_label_converter_df.json','w') as f:\n",
        "    json.dump(int_label_converter_dict,f)\n",
        "name_df = name_df.merge(int_label_converter_df)\n",
        "for fname in os.listdir('capture24'):\n",
        "    if fname.endswith('.gz'): continue\n",
        "    subj_id = fname.split('.')[0]\n",
        "    if not subj_id.startswith('P') and not len(subj_id) == 4: continue # Skip metadata files\n",
        "    print(f\"converting {fname} to np\")\n",
        "    try: df = pd.read_csv(join('capture24',fname))\n",
        "    except: set_trace()\n",
        "    translated_df = df.merge(name_df)\n",
        "    x = translated_df[['x','y','z']].to_numpy()\n",
        "    y = translated_df['int_label'].to_numpy()\n",
        "    np.save(join(np_dir,f'{subj_id}.npy'),x)\n",
        "    np.save(join(np_dir,f'{subj_id}_labels.npy'),y)\n",
        "\n",
        "# elif sys.argv[1] == 'HHAR':\n",
        "data_dir = 'Activity recognition exp'\n",
        "np_dir = 'hhar/np_data'\n",
        "print(\"\\n#####Preprocessing HHAR#####\\n\")\n",
        "if not os.path.isdir(np_dir):\n",
        "    os.makedirs(np_dir)\n",
        "\n",
        "pandaload = lambda path: pd.read_csv(join(data_dir,'Phones_accelerometer.csv')).set_index('Creation_Time').drop(['Index','Arrival_Time','Model','Device'],axis=1).dropna()\n",
        "print('loading dataframes\\n')\n",
        "phone_acc_df = pandaload('Phones_accelerometer.csv')\n",
        "phone_gyro_df = pandaload('Phones_gyroscope.csv')\n",
        "watch_acc_df = pandaload('Watch_accelerometer.csv')\n",
        "watch_gyro_df = pandaload('Watch_gyroscope.csv')\n",
        "activities_list = ['bike', 'sit', 'stand', 'walk', 'stairsup', 'stairsdown']\n",
        "user_list = list('abcdefghi')\n",
        "\n",
        "for user_letter_name in user_list:\n",
        "    print('processing user', user_letter_name)\n",
        "    user_phone_acc = phone_acc_df.loc[phone_acc_df.User==user_letter_name]\n",
        "    user_phone_gyro = phone_gyro_df.loc[phone_gyro_df.User==user_letter_name]\n",
        "    user_watch_acc = watch_acc_df.loc[watch_acc_df.User==user_letter_name]\n",
        "    user_watch_gyro = watch_acc_df.loc[watch_gyro_df.User==user_letter_name]\n",
        "    assert all([user_watch_gyro.shape==d.shape for d in (user_phone_acc,user_phone_gyro,user_watch_acc)])\n",
        "    comb_phone = user_phone_acc.join(user_phone_gyro,how='outer',lsuffix='_acc',rsuffix='_gyro')\n",
        "    comb_watch = user_watch_acc.join(user_watch_gyro,how='outer',lsuffix='_acc',rsuffix='_gyro')\n",
        "    #if not (comb_watch.gt_acc == comb_watch.gt_gyro).all(): set_trace()\n",
        "    #if not (comb_phone.gt_acc == comb_phone.gt_gyro).all(): set_trace()\n",
        "    comb = comb_phone.join(comb_watch,how='outer',lsuffix='_phone',rsuffix='_watch')\n",
        "    duplicate_rows = [x for x,count in Counter(comb.index).items() if count > 1]\n",
        "    if len(duplicate_rows) > 10: set_trace()\n",
        "    elif len(duplicate_rows) > 0:\n",
        "        print( f\"removing {len(duplicate_rows)} duplicate rows\")\n",
        "        comb = comb.drop(duplicate_rows)\n",
        "    if not (comb.gt_acc_phone == comb.gt_acc_watch).all(): set_trace()\n",
        "    user_X_array = comb.drop([c for c in comb.columns if 'User' in c or 'gt' in c],axis=1).to_numpy()\n",
        "    user_y_array = np.array([activities_list.index(a) for a in comb['gt_acc_phone']])\n",
        "    save_path = join(np_dir,f\"{user_list.index(user_letter_name)+1}.npy\")\n",
        "    label_save_path = join(np_dir,f\"{user_list.index(user_letter_name)+1}_labels.npy\")\n",
        "    np.save(save_path,user_X_array,allow_pickle=False)\n",
        "    np.save(label_save_path,user_y_array,allow_pickle=False)\n",
        "    # Make smaller option for testing\n",
        "    np.save(join(np_dir,f\"0.npy\"),user_X_array[::1000],allow_pickle=False)\n",
        "    np.save(join(np_dir,f\"0_labels.npy\"),user_y_array[::1000],allow_pickle=False)\n",
        "\n",
        "else: print('\\nIncorrect or no dataset specified\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGU_rUapvjcE",
        "outputId": "b150f60b-4d01-47b5-b1f4-ea6b20a17027",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Incorrect or no dataset specified\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Lou1sM make_dsets.py\n",
        "# https://github.com/Lou1sM/HAR/blob/master/make_dsets.py\n",
        "import numpy as np\n",
        "# from dl_utils.misc import check_dir, CifarLikeDataset\n",
        "import os\n",
        "import torch\n",
        "# import project_config\n",
        "from scipy import stats\n",
        "from torch.utils import data\n",
        "#from dl_utils import label_funcs\n",
        "# from dl_utils.tensor_funcs import cudify\n",
        "# import label_funcs_tmp\n",
        "from pdb import set_trace\n",
        "from os.path import join\n",
        "\n",
        "\n",
        "class ChunkDataset(data.Dataset):\n",
        "    def __init__(self,x,y):\n",
        "        self.x, self.y = x,y\n",
        "    def __len__(self): return len(self.x)\n",
        "    def __getitem__(self,idx):\n",
        "        batch_x = self.x[idx].unsqueeze(0)\n",
        "        batch_y = self.y[idx]\n",
        "        return batch_x, batch_y, idx\n",
        "\n",
        "class ConcattedDataset(data.Dataset):\n",
        "    \"\"\"Needs datasets to be StepDatasets in order to Concat them.\"\"\"\n",
        "    def __init__(self,xs,ys,window_size,step_size):\n",
        "        self.x, self.y = torch.cat(xs),torch.cat(ys)\n",
        "        self.window_size = window_size\n",
        "        self.step_size = step_size\n",
        "        component_dset_lengths = [((len(x)-self.window_size)//self.step_size + 1) for x in xs]\n",
        "        x_idx_locs = []\n",
        "        block_start_idx = 0\n",
        "        for x in xs:\n",
        "            x_idx_locs += list(range(block_start_idx,block_start_idx+len(x)-window_size+1,step_size))\n",
        "            block_start_idx += len(x)\n",
        "        self.x_idx_locs = np.array(x_idx_locs)\n",
        "        if not len(self.x_idx_locs) == len(self.y): set_trace()\n",
        "\n",
        "    def __len__(self): return len(self.y)\n",
        "    def __getitem__(self,idx):\n",
        "        x_idx = self.x_idx_locs[idx]\n",
        "        batch_x = self.x[x_idx:x_idx + self.window_size].unsqueeze(0)\n",
        "        batch_y = self.y[idx]\n",
        "        return batch_x, batch_y, idx\n",
        "\n",
        "class UCIFeatDataset(data.Dataset):\n",
        "    def __init__(self,x,y,transforms=[]):\n",
        "        self.x, self.y = cudify(x).float(),cudify(y).float()\n",
        "        assert len(self.x) == len(self.y)\n",
        "        for transform in transforms:\n",
        "            self.x = transform(self.x)\n",
        "    def __len__(self): return len(self.x)\n",
        "    def __getitem__(self,idx):\n",
        "        batch_x = self.x[idx]\n",
        "        batch_y = self.y[idx]\n",
        "        return batch_x, batch_y, idx\n",
        "\n",
        "class StepDataset(data.Dataset):\n",
        "    def __init__(self,x,y,window_size,step_size,transforms=[]):\n",
        "        self.x, self.y = cudify(x).float(),cudify(y).float()\n",
        "        self.window_size = window_size\n",
        "        self.step_size = step_size\n",
        "        self.transforms = transforms\n",
        "        self.position = None\n",
        "        self.ensemble_size = None\n",
        "        for transform in transforms:\n",
        "            self.x = transform(self.x)\n",
        "    def __len__(self): return (len(self.x)-self.window_size)//self.step_size + 1\n",
        "    def __getitem__(self,idx):\n",
        "        batch_x = self.x[idx*self.step_size:(idx*self.step_size) + self.window_size].unsqueeze(0)\n",
        "        batch_y = self.y[idx]\n",
        "        return batch_x, batch_y, idx\n",
        "\n",
        "    def put_in_ensemble(self,position,ensemble_size):\n",
        "        self.y += ensemble_size*position\n",
        "        self.position = position\n",
        "        self.ensemble_size = ensemble_size\n",
        "\n",
        "def preproc_xys(x,y,step_size,window_size,dset_info_object,subj_ids):\n",
        "    ids_string = 'all' if set(subj_ids) == set(dset_info_object.possible_subj_ids) else \"-\".join(subj_ids)\n",
        "    precomp_dir = f'datasets/{dset_info_object.dataset_dir_name}/precomputed/{ids_string}step{step_size}_window{window_size}/'\n",
        "    if os.path.isfile(join(precomp_dir,'x.pt')) and os.path.isfile(join(precomp_dir,'y.pt')):\n",
        "        print(\"loading precomputed datasets\")\n",
        "        x = torch.load(join(precomp_dir,'x.pt'))\n",
        "        y = torch.load(join(precomp_dir,'y.pt'))\n",
        "        with open(join(precomp_dir,'selected_acts.txt')) as f: selected_acts = f.readlines()\n",
        "    else:\n",
        "        print(\"no precomputed datasets, computing from scratch\")\n",
        "        xnans = np.isnan(x).any(axis=1)\n",
        "        x = x[~xnans]\n",
        "        y = y[~xnans]\n",
        "        x = x[y!=-1]\n",
        "        y = y[y!=-1]\n",
        "        num_windows = (len(x) - window_size)//step_size + 1\n",
        "        #mode_labels = np.array([stats.mode(y[w*step_size:w*step_size + window_size]).mode[0] if (y[w*step_size:w*step_size + window_size]==y[w*step_size]).all() else -1 for w in range(num_windows)])\n",
        "        mode_labels = np.array([stats.mode(y[w*step_size:w*step_size + window_size]).mode[0] for w in range(num_windows)])\n",
        "        selected_ids = set(mode_labels)\n",
        "        selected_acts = [dset_info_object.action_name_dict[act_id] for act_id in selected_ids]\n",
        "        mode_labels, trans_dict, changed = label_funcs_tmp.compress_labels(mode_labels)\n",
        "        assert len(selected_acts) == len(set(mode_labels))\n",
        "        x = torch.tensor(x).float()\n",
        "        y = torch.tensor(mode_labels).float()\n",
        "        check_dir(precomp_dir)\n",
        "        torch.save(x,join(precomp_dir,'x.pt'))\n",
        "        torch.save(y,join(precomp_dir,'y.pt'))\n",
        "        with open(join(precomp_dir,'selected_acts.txt'),'w') as f:\n",
        "            for a in selected_acts: f.write(a+'\\n')\n",
        "    return x, y, selected_acts\n",
        "\n",
        "def make_pamap_dset_train_val(args,subj_ids):\n",
        "    # dset_info_object = project_config.PAMAP_INFO\n",
        "    dset_info_object = PAMAP_INFO\n",
        "    x_train = np.concatenate([np.load(f'datasets/PAMAP2_Dataset/np_data/subject{s}.npy') for s in subj_ids])\n",
        "    y_train = np.concatenate([np.load(f'datasets/PAMAP2_Dataset/np_data/subject{s}_labels.npy') for s in subj_ids])\n",
        "    x_train = x_train[y_train!=0] # 0 is a transient activity\n",
        "    y_train = y_train[y_train!=0] # 0 is a transient activity\n",
        "    x_train,y_train,selected_acts = preproc_xys(x_train,y_train,args.step_size,args.window_size,dset_info_object,subj_ids)\n",
        "    dset_train = StepDataset(x_train,y_train,window_size=args.window_size,step_size=args.step_size)\n",
        "    return dset_train, selected_acts\n",
        "\n",
        "# def make_uci_dset_train_val(args,subj_ids):\n",
        "#     dset_info_object = project_config.UCI_INFO\n",
        "#     x_train = np.concatenate([np.load(f'datasets/UCI2/np_data/user{s}.npy') for s in subj_ids])\n",
        "#     y_train = np.concatenate([np.load(f'datasets/UCI2/np_data/user{s}_labels.npy') for s in subj_ids])\n",
        "#     x_train = x_train[y_train<7] # Labels still begin at 1 at this point as\n",
        "#     y_train = y_train[y_train<7] # haven't been compressed, so select 1,..,6\n",
        "#     #x_train = x_train[y_train!=-1]\n",
        "#     #y_train = y_train[y_train!=-1]\n",
        "#     #y_val = y_val[y_val!=-1]\n",
        "#     x_train,y_train,selected_acts = preproc_xys(x_train,y_train,args.step_size,args.window_size,dset_info_object,subj_ids)\n",
        "#     dset_train = StepDataset(x_train,y_train,window_size=args.window_size,step_size=args.step_size)\n",
        "#     return dset_train, selected_acts\n",
        "\n",
        "# def make_uci_feat_dset_train_val():\n",
        "#     dset_info_object = project_config.UCI_INFO\n",
        "#     x = np.load(f'datasets/UCI_feat/uci_feat_data.npy')\n",
        "#     y = np.load(f'datasets/UCI_feat/uci_feat_targets.npy')\n",
        "#     selected_acts = dict(enumerate(['walking','upstairs','downstairs','sitting','standing','lying']))\n",
        "#     dset = UCIFeatDataset(x,y)\n",
        "#     #dset.x = dset.data\n",
        "#     #dset.y = dset.targets\n",
        "#     return dset, selected_acts\n",
        "\n",
        "# def make_wisdm_v1_dset_train_val(args,subj_ids):\n",
        "#     dset_info_object = project_config.WISDMv1_INFO\n",
        "#     x = np.load('datasets/wisdm_v1/X.npy')\n",
        "#     y = np.load('datasets/wisdm_v1/y.npy')\n",
        "#     users = np.load('datasets/wisdm_v1/users.npy')\n",
        "#     train_idxs_to_user = np.zeros(users.shape[0]).astype(np.bool)\n",
        "#     for subj_id in subj_ids:\n",
        "#         new_users = users==subj_id\n",
        "#         train_idxs_to_user = np.logical_or(train_idxs_to_user,new_users)\n",
        "#     x_train = x[train_idxs_to_user]\n",
        "#     y_train = y[train_idxs_to_user]\n",
        "#     x_train,y_train,selected_acts = preproc_xys(x_train,y_train,args.step_size,args.window_size,dset_info_object,subj_ids)\n",
        "#     dset_train = StepDataset(x_train,y_train,window_size=args.window_size,step_size=args.step_size)\n",
        "#     return dset_train, selected_acts\n",
        "\n",
        "# def make_wisdm_watch_dset_train_val(args,subj_ids):\n",
        "#     dset_info_object = project_config.WISDMwatch_INFO\n",
        "#     x_train = np.concatenate([np.load(f'datasets/wisdm-dataset/np_data/{s}.npy') for s in subj_ids])\n",
        "#     y_train = np.concatenate([np.load(f'datasets/wisdm-dataset/np_data/{s}_labels.npy') for s in subj_ids])\n",
        "#     certains_train = np.concatenate([np.load(f'datasets/wisdm-dataset/np_data/{s}_certains.npy') for s in subj_ids])\n",
        "#     x_train = x_train[certains_train]\n",
        "#     y_train = y_train[certains_train]\n",
        "#     x_train,y_train,selected_acts = preproc_xys(x_train,y_train,args.step_size,args.window_size,dset_info_object,subj_ids)\n",
        "#     dset_train = StepDataset(x_train,y_train,window_size=args.window_size,step_size=args.step_size)\n",
        "#     return dset_train, selected_acts\n",
        "\n",
        "# def make_realdisp_dset_train_val(args,subj_ids):\n",
        "#     dset_info_object = project_config.REALDISP_INFO\n",
        "#     x_train = np.concatenate([np.load(f'datasets/realdisp/np_data/subject{s}.npy') for s in subj_ids])\n",
        "#     y_train = np.concatenate([np.load(f'datasets/realdisp/np_data/subject{s}_labels.npy') for s in subj_ids])\n",
        "#     x_train = x_train[:,2:] #First two columns are timestamp\n",
        "#     x_train = x_train[y_train!=0] # 0 seems to be a transient activity\n",
        "#     y_train = y_train[y_train!=0] # 0 seems to be a transient activity\n",
        "#     x_train,y_train,selected_acts = preproc_xys(x_train,y_train,args.step_size,args.window_size,dset_info_object,subj_ids)\n",
        "#     dset_train = StepDataset(x_train,y_train,window_size=args.window_size,step_size=args.step_size)\n",
        "#     return dset_train, selected_acts\n",
        "\n",
        "# def make_hhar_dset_train_val(args,subj_ids):\n",
        "#     dset_info_object = project_config.HHAR_INFO\n",
        "#     x_train = np.concatenate([np.load(f'datasets/hhar/np_data/{s}.npy') for s in subj_ids])\n",
        "#     y_train = np.concatenate([np.load(f'datasets/hhar/np_data/{s}_labels.npy') for s in subj_ids])\n",
        "#     x_train,y_train,selected_acts = preproc_xys(x_train,y_train,args.step_size,args.window_size,dset_info_object,subj_ids)\n",
        "#     dset_train = StepDataset(x_train,y_train,window_size=args.window_size,step_size=args.step_size)\n",
        "#     return dset_train, selected_acts\n",
        "\n",
        "def make_capture_dset_train_val(args,subj_ids):\n",
        "    action_name_dict = {0: 'sleep', 1: 'sedentary-screen', 2: 'tasks-moderate', 3: 'sedentary-non-screen', 4: 'walking', 5: 'vehicle', 6: 'bicycling', 7: 'tasks-light', 8: 'sports-continuous', 9: 'sport-interrupted'} # Should also be saved in json file in datasets/capture24\n",
        "    subj_ids = len(subj_ids) - min(2,len(subj_ids)//2)\n",
        "    subj_ids = subj_ids[:subj_ids]\n",
        "    def three_digitify(x): return '00' + str(x) if len(str(x))==1 else '0' + str(x)\n",
        "    x_train = np.concatenate([np.load(f'datasets/capture24/np_data/P{three_digitify(s)}.npy') for s in subj_ids])\n",
        "    y_train = np.concatenate([np.load(f'datasets/capture24/np_data/P{three_digitify(s)}_labels.npy') for s in subj_ids])\n",
        "    x_train,y_train,selected_acts = preproc_xys(x_train,y_train,args.step_size,args.window_size,action_name_dict)\n",
        "    dset_train = StepDataset(x_train,y_train,window_size=args.window_size,step_size=args.step_size)\n",
        "    if len(subj_ids) <= 2: return dset_train, dset_train, selected_acts\n",
        "\n",
        "    # else make val dset\n",
        "    val_ids = subj_ids[subj_ids:]\n",
        "    x_val = np.concatenate([np.load(f'datasets/capture24/np_data/P{three_digitify(s)}.npy') for s in val_ids])\n",
        "    y_val = np.concatenate([np.load(f'datasets/capture24/np_data/P{three_digitify(s)}_labels.npy') for s in val_ids])\n",
        "    x_val,y_val,selected_acts = preproc_xys(x_val,y_val,args.step_size,args.window_size,action_name_dict)\n",
        "    dset_val = StepDataset(x_val,y_val,window_size=args.window_size,step_size=args.step_size)\n",
        "    return dset_train, dset_val, selected_acts\n",
        "\n",
        "def make_single_dset(args,subj_ids):\n",
        "    if args.dset == 'PAMAP':\n",
        "        return make_pamap_dset_train_val(args,subj_ids)\n",
        "    # if args.dset == 'UCI':\n",
        "    #     return make_uci_dset_train_val(args,subj_ids)\n",
        "    # if args.dset == 'UCI_feat':\n",
        "    #     return make_uci_feat_dset_train_val()\n",
        "    # if args.dset == 'WISDM-v1':\n",
        "    #     return make_wisdm_v1_dset_train_val(args,subj_ids)\n",
        "    # if args.dset == 'WISDM-watch':\n",
        "    #     return make_wisdm_watch_dset_train_val(args,subj_ids)\n",
        "    # if args.dset == 'REALDISP':\n",
        "    #     return make_realdisp_dset_train_val(args,subj_ids)\n",
        "    # if args.dset == 'HHAR':\n",
        "    #     return make_hhar_dset_train_val(args,subj_ids)\n",
        "    # if args.dset == 'Capture24':\n",
        "    #     return make_capture_dset_train_val(args,subj_ids)\n",
        "\n",
        "def make_dsets_by_user(args,subj_ids):\n",
        "    dsets_by_id = {}\n",
        "    for subj_id in subj_ids:\n",
        "        dset_subj, selected_acts_subj = make_single_dset(args,[subj_id])\n",
        "        dsets_by_id[subj_id] = dset_subj,selected_acts_subj\n",
        "    return dsets_by_id\n",
        "\n",
        "def chunked_up(x,step_size,window_size):\n",
        "    num_windows = (len(x) - window_size)//step_size + 1\n",
        "    return torch.stack([x[i*step_size:i*step_size+window_size] for i in range(num_windows)])\n",
        "\n",
        "def combine_dsets(dsets):\n",
        "    xs = [d.x for d in dsets]\n",
        "    ys = [d.y for d in dsets]\n",
        "    return ConcattedDataset(xs,ys,dsets[0].window_size,dsets[0].step_size)\n",
        "\n",
        "def combine_dsets_old(dsets):\n",
        "    processed_dset_xs = []\n",
        "    for dset in dsets:\n",
        "        if isinstance(dset,StepDataset):\n",
        "            processed_dset_x = chunked_up(dset.x,dset.step_size,dset.window_size)\n",
        "        elif isinstance(dset,ChunkDataset):\n",
        "            processed_dset_x = dset.x\n",
        "        else:\n",
        "            print(f\"you're trying to combine dsets on a {type(dset)}, but it has to be a dataset\")\n",
        "        processed_dset_xs.append(processed_dset_x)\n",
        "    x = torch.cat(processed_dset_xs)\n",
        "    y = torch.cat([dset.y for dset in dsets])\n",
        "    assert len(x) == len(y)\n",
        "    combined = ChunkDataset(x,y)\n",
        "    return combined\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "nQERarq97cIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Lou1sM project_config.py\n",
        "# https://github.com/Lou1sM/HAR/blob/master/project_config.py\n",
        "import sys\n",
        "import torch\n",
        "\n",
        "class HAR_Dataset_Container():\n",
        "    def __init__(self,code_name,dataset_dir_name,possible_subj_ids,num_channels,num_classes,action_name_dict):\n",
        "        self.code_name = code_name\n",
        "        self.dataset_dir_name = dataset_dir_name\n",
        "        self.possible_subj_ids = possible_subj_ids\n",
        "        self.num_channels = num_channels\n",
        "        self.num_classes = num_classes\n",
        "        self.action_name_dict = action_name_dict\n",
        "\n",
        "\n",
        "# PAMAP\n",
        "pamap_ids = [str(x) for x in range(101,110)]\n",
        "pamap_action_name_dict = {1:'lying',2:'sitting',3:'standing',4:'walking',5:'running',6:'cycling',7:'Nordic walking',9:'watching TV',10:'computer work',11:'car driving',12:'ascending stairs',13:'descending stairs',16:'vacuum cleaning',17:'ironing',18:'folding laundry',19:'house cleaning',20:'playing soccer',24:'rope jumping'}\n",
        "PAMAP_INFO = HAR_Dataset_Container(\n",
        "            code_name = 'PAMAP',\n",
        "            dataset_dir_name = 'PAMAP2_Dataset',\n",
        "            possible_subj_ids = pamap_ids,\n",
        "            num_channels = 39,\n",
        "            num_classes = 12,\n",
        "            action_name_dict = pamap_action_name_dict)\n",
        "\n",
        "# # UCI\n",
        "# def two_digitify(x): return '0' + str(x) if len(str(x))==1 else str(x)\n",
        "# uci_ids = [two_digitify(x) for x in range(1,30)]\n",
        "# uci_action_name_dict = {1:'walking',2:'walking upstairs',3:'walking downstairs',4:'sitting',5:'standing',6:'lying',7:'stand_to_sit',9:'sit_to_stand',10:'sit_to_lit',11:'lie_to_sit',12:'stand_to_lie',13:'lie_to_stand'}\n",
        "# UCI_INFO = HAR_Dataset_Container(\n",
        "#             code_name = 'UCI',\n",
        "#             dataset_dir_name = 'UCI2',\n",
        "#             possible_subj_ids = uci_ids,\n",
        "#             num_channels = 6,\n",
        "#             num_classes = 6,\n",
        "#             action_name_dict = uci_action_name_dict)\n",
        "\n",
        "# # UCI_feat\n",
        "# def two_digitify(x): return '0' + str(x) if len(str(x))==1 else str(x)\n",
        "# uci_feat_action_name_dict = {1:'walking',2:'walking upstairs',3:'walking downstairs',4:'sitting',5:'standing',6:'lying',7:'stand_to_sit',9:'sit_to_stand',10:'sit_to_lit',11:'lie_to_sit',12:'stand_to_lie',13:'lie_to_stand'}\n",
        "# UCI_FEAT_INFO = HAR_Dataset_Container(\n",
        "#             code_name = 'UCI_feat',\n",
        "#             dataset_dir_name = 'UCI2_feat',\n",
        "#             possible_subj_ids = ['0'],\n",
        "#             num_channels = 561,\n",
        "#             num_classes = 6,\n",
        "#             action_name_dict = uci_action_name_dict)\n",
        "\n",
        "# # WISDM-v1\n",
        "# wisdmv1_ids = [str(x) for x in range(1,37)] #Paper says 29 users but ids go up to 36\n",
        "# activities_list = ['Jogging','Walking','Upstairs','Downstairs','Standing','Sitting']\n",
        "# wisdmv1_action_name_dict = dict(zip(range(len(activities_list)),activities_list))\n",
        "# WISDMv1_INFO = HAR_Dataset_Container(\n",
        "#             code_name = 'WISDM-v1',\n",
        "#             dataset_dir_name = 'wisdm_v1',\n",
        "#             possible_subj_ids = wisdmv1_ids,\n",
        "#             num_channels = 3,\n",
        "#             num_classes = 6,\n",
        "#             action_name_dict = wisdmv1_action_name_dict)\n",
        "\n",
        "# # WISDM-watch\n",
        "# wisdmwatch_ids = [str(x) for x in range(1600,1651)]\n",
        "# with open('datasets/wisdm-dataset/activity_key.txt') as f: r=f.readlines()\n",
        "# activities_list = [x.split(' = ')[0] for x in r if ' = ' in x]\n",
        "# wisdmwatch_action_name_dict = dict(zip(range(len(activities_list)),activities_list))\n",
        "# WISDMwatch_INFO = HAR_Dataset_Container(\n",
        "#             code_name = 'WISDM-watch',\n",
        "#             dataset_dir_name = 'wisdm-dataset',\n",
        "#             possible_subj_ids = wisdmwatch_ids,\n",
        "#             num_channels = 12,\n",
        "#             num_classes = 17,\n",
        "#             action_name_dict = wisdmwatch_action_name_dict)\n",
        "\n",
        "# # REALDISP\n",
        "# realdisp_ids = [str(x) for x in range(1,18)]\n",
        "# activities_list = ['Walking','Jogging','Running','Jump up','Jump front & back','Jump sideways','Jump leg/arms open/closed','Jump rope','Trunk twist','Trunk twist','Waist bends forward','Waist rotation','Waist bends','Reach heels backwards','Lateral bend','Lateral bend with arm up','Repetitive forward stretching','Upper trunk and lower body opposite twist','Lateral elevation of arms','Frontal elevation of arms','Frontal hand claps','Frontal crossing of arms','Shoulders high-amplitude rotation','Shoulders low-amplitude rotation','Arms inner rotation','Knees','Heels','Knees bending','Knees','Rotation on the knees','Rowing','Elliptical bike','Cycling']\n",
        "# realdisp_action_name_dict = {i+1:act for i,act in enumerate(activities_list)}\n",
        "# REALDISP_INFO = HAR_Dataset_Container(\n",
        "#             code_name = 'REALDISP',\n",
        "#             dataset_dir_name = 'realdisp',\n",
        "#             possible_subj_ids = realdisp_ids,\n",
        "#             num_channels = 117,\n",
        "#             num_classes = 33,\n",
        "#             action_name_dict = realdisp_action_name_dict)\n",
        "\n",
        "# # HHAR\n",
        "# hhar_ids = [str(x) for x in range(0,10)]\n",
        "# activities_list = ['bike', 'sit', 'stand', 'walk', 'stairsup', 'stairsdown']\n",
        "# hhar_action_name_dict = {i:act for i,act in enumerate(activities_list)}\n",
        "# HHAR_INFO = HAR_Dataset_Container(\n",
        "#             code_name = 'HHAR',\n",
        "#             dataset_dir_name = 'hhar',\n",
        "#             possible_subj_ids = hhar_ids,\n",
        "#             num_channels = 12,\n",
        "#             num_classes = 6,\n",
        "#             action_name_dict = hhar_action_name_dict)\n",
        "\n",
        "# DSET_OBJECTS = [PAMAP_INFO, UCI_INFO, UCI_FEAT_INFO, WISDMv1_INFO, WISDMwatch_INFO,REALDISP_INFO,HHAR_INFO]\n",
        "DSET_OBJECTS = [PAMAP_INFO]\n",
        "\n",
        "\n",
        "def get_dataset_info_object(dset_name):\n",
        "    dsets_by_that_name = [d for d in DSET_OBJECTS if d.code_name == dset_name]\n",
        "    if len(dsets_by_that_name)==0: print(f\"{dset_name} is not a recognized dataset\"); sys.exit()\n",
        "    assert len(dsets_by_that_name)==1\n",
        "    return dsets_by_that_name[0]\n",
        "\n",
        "def get_num_time_points():\n",
        "    for dset_info_object in DSET_OBJECTS:\n",
        "        print(dset_info_object.code_name, sum([torch.load(f'datasets/{dset_info_object.dataset_dir_name}/precomputed/{s}step100_window512/x.pt').shape[0] for s in dset_info_object.possible_subj_ids]))\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "_PZ6amU5BhIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Lou1sM har_cnn.py\n",
        "# https://github.com/Lou1sM/HAR/blob/master/har_cnn.py\n",
        "import argparse\n",
        "import sys\n",
        "# import project_config\n",
        "\n",
        "\n",
        "def get_cl_args():\n",
        "    dset_options = ['PAMAP','UCI','WISDM-v1','WISDM-watch','REALDISP','Capture24']\n",
        "    # dset_options = [di.code_name for di in project_config.DSET_OBJECTS]\n",
        "    training_type_options = ['full','cluster_as_single','cluster_individually','train_frac_gts_as_single','find_similar_users']\n",
        "    parser = argparse.ArgumentParser()\n",
        "    subjs_group = parser.add_mutually_exclusive_group(required=False)\n",
        "    subjs_group.add_argument('--num_subjs',type=int)\n",
        "    subjs_group.add_argument('--subj_ids',type=str,nargs='+',default=['first'])\n",
        "    epochs_group = parser.add_mutually_exclusive_group(required=False)\n",
        "    epochs_group.add_argument('--full_epochs',action='store_true')\n",
        "    epochs_group.add_argument('--short_epochs',action='store_true')\n",
        "    parser.add_argument('--ablate_label_filter',action='store_true')\n",
        "    parser.add_argument('--all_subjs',action='store_true')\n",
        "    parser.add_argument('--bad_ids',action='store_true')\n",
        "    parser.add_argument('--batch_size_train',type=int,default=256)\n",
        "    parser.add_argument('--batch_size_val',type=int,default=1024)\n",
        "    parser.add_argument('--clusterer',type=str,choices=['HMM','GMM'],default='HMM')\n",
        "    parser.add_argument('--compute_cross_metrics',action='store_true')\n",
        "    parser.add_argument('--dec_lr',type=float,default=1e-3)\n",
        "    parser.add_argument('-d','--dset',type=str,default='UCI',choices=dset_options)\n",
        "    parser.add_argument('--enc_lr',type=float,default=1e-3)\n",
        "    parser.add_argument('--exp_name',type=str,default=\"try\")\n",
        "    parser.add_argument('--frac_gt_labels',type=float,default=0.1)\n",
        "    parser.add_argument('--gpu',type=str,default='0')\n",
        "    parser.add_argument('--is_n2d',action='store_true')\n",
        "    parser.add_argument('--is_uln',action='store_true',help='net for dim red. instead of umap')\n",
        "    parser.add_argument('--just_align_time',action='store_true')\n",
        "    parser.add_argument('--load_pretrained',action='store_true')\n",
        "    parser.add_argument('--mlp_lr',type=float,default=1e-3)\n",
        "    parser.add_argument('--no_umap',action='store_true')\n",
        "    parser.add_argument('--noise',type=float,default=1.)\n",
        "    parser.add_argument('--num_classes',type=int,default=-1)\n",
        "    parser.add_argument('--num_meta_epochs',type=int,default=1)\n",
        "    parser.add_argument('--num_meta_meta_epochs',type=int,default=1)\n",
        "    parser.add_argument('--num_pseudo_label_epochs',type=int,default=5)\n",
        "    parser.add_argument('--umap_dim',type=int,default=2)\n",
        "    parser.add_argument('--umap_neighbours',type=int,default=60)\n",
        "    parser.add_argument('--prob_thresh',type=float,default=.95)\n",
        "    parser.add_argument('--reinit',action='store_true')\n",
        "    parser.add_argument('--reload_ids',type=int,default=0)\n",
        "    parser.add_argument('--rlmbda',type=float,default=.1)\n",
        "    parser.add_argument('--show_transitions',action='store_true')\n",
        "    parser.add_argument('--step_size',type=int,default=5)\n",
        "    parser.add_argument('--subject_independent',action='store_true')\n",
        "    parser.add_argument('--test','-t',action='store_true')\n",
        "    parser.add_argument('--train_type',type=str,choices=training_type_options,default='full')\n",
        "    parser.add_argument('--show_shapes',action='store_true',help='print the shapes of hidden layers in enc and dec')\n",
        "    parser.add_argument('--verbose',action='store_true')\n",
        "    parser.add_argument('--window_size',type=int,default=512)\n",
        "    ARGS = parser.parse_args()\n",
        "\n",
        "    need_umap = False\n",
        "    if ARGS.is_uln:\n",
        "        ARGS.no_umap = True\n",
        "    if ARGS.short_epochs:\n",
        "        ARGS.num_meta_meta_epochs = 1\n",
        "        ARGS.num_meta_epochs = 1\n",
        "        ARGS.num_pseudo_label_epochs = 1\n",
        "    elif ARGS.full_epochs:\n",
        "        ARGS.num_meta_meta_epochs = 10\n",
        "        ARGS.num_meta_epochs = 10\n",
        "        ARGS.num_pseudo_label_epochs = 5\n",
        "    if ARGS.test:\n",
        "        ARGS.num_meta_epochs = 1\n",
        "        ARGS.num_meta_meta_epochs = 1\n",
        "        ARGS.num_pseudo_label_epochs = 1\n",
        "    elif not ARGS.no_umap and not ARGS.show_shapes: need_umap = True\n",
        "    print(ARGS)\n",
        "    dset_info_object = project_config.get_dataset_info_object(ARGS.dset)\n",
        "    all_possible_ids = dset_info_object.possible_subj_ids\n",
        "    if ARGS.all_subjs: ARGS.subj_ids=all_possible_ids\n",
        "    elif ARGS.num_subjs is not None: ARGS.subj_ids = all_possible_ids[:ARGS.num_subjs]\n",
        "    elif ARGS.subj_ids == ['first']: ARGS.subj_ids = all_possible_ids[:1]\n",
        "    bad_ids = [x for x in ARGS.subj_ids if x not in all_possible_ids]\n",
        "    if len(bad_ids) > 0 and not (ARGS.test and ARGS.dset=='HHAR'):\n",
        "        print(f\"You have specified non-existent ids: {bad_ids}\\nExistent ids are {all_possible_ids}\"); sys.exit()\n",
        "    return ARGS, need_umap\n",
        "\n",
        "RELEVANT_ARGS = ['ablate_label_filter','clusterer','dset','no_umap','num_meta_epochs','num_meta_meta_epochs','num_pseudo_label_epochs','reinit','step_size','subject_independent']\n",
        "\n",
        "\n",
        "# ARGS, need_umap = cl_args.get_cl_args()\n",
        "ARGS, need_umap = get_cl_args()\n",
        "# if need_umap: import umap\n",
        "# main(ARGS)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "cellView": "form",
        "id": "3UvqfQXfFSu1",
        "outputId": "84e2ee66-1f8e-47bf-88a6-ff9e8a6b9a8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] [--num_subjs NUM_SUBJS | --subj_ids SUBJ_IDS [SUBJ_IDS ...]]\n",
            "                                [--full_epochs | --short_epochs] [--ablate_label_filter]\n",
            "                                [--all_subjs] [--bad_ids] [--batch_size_train BATCH_SIZE_TRAIN]\n",
            "                                [--batch_size_val BATCH_SIZE_VAL] [--clusterer {HMM,GMM}]\n",
            "                                [--compute_cross_metrics] [--dec_lr DEC_LR]\n",
            "                                [-d {PAMAP,UCI,WISDM-v1,WISDM-watch,REALDISP,Capture24}]\n",
            "                                [--enc_lr ENC_LR] [--exp_name EXP_NAME]\n",
            "                                [--frac_gt_labels FRAC_GT_LABELS] [--gpu GPU] [--is_n2d]\n",
            "                                [--is_uln] [--just_align_time] [--load_pretrained]\n",
            "                                [--mlp_lr MLP_LR] [--no_umap] [--noise NOISE]\n",
            "                                [--num_classes NUM_CLASSES] [--num_meta_epochs NUM_META_EPOCHS]\n",
            "                                [--num_meta_meta_epochs NUM_META_META_EPOCHS]\n",
            "                                [--num_pseudo_label_epochs NUM_PSEUDO_LABEL_EPOCHS]\n",
            "                                [--umap_dim UMAP_DIM] [--umap_neighbours UMAP_NEIGHBOURS]\n",
            "                                [--prob_thresh PROB_THRESH] [--reinit] [--reload_ids RELOAD_IDS]\n",
            "                                [--rlmbda RLMBDA] [--show_transitions] [--step_size STEP_SIZE]\n",
            "                                [--subject_independent] [--test]\n",
            "                                [--train_type {full,cluster_as_single,cluster_individually,train_frac_gts_as_single,find_similar_users}]\n",
            "                                [--show_shapes] [--verbose] [--window_size WINDOW_SIZE]\n",
            "colab_kernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-50ea2cb4-a0b3-4dd4-be5b-f719dc2cbc66.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "2",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        }
      ]
    }
  ]
}