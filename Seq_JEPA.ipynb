{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/Seq_JEPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FA00-tf6MJ-s"
      },
      "outputs": [],
      "source": [
        "# @title rohanpandey WISDM\n",
        "# https://github.com/rohanpandey/Analysis--WISDM-Smartphone-and-Smartwatch-Activity/blob/master/dataset_creation.ipynb\n",
        "! wget https://archive.ics.uci.edu/static/public/507/wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset.zip\n",
        "!unzip wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset.zip\n",
        "!unzip wisdm-dataset.zip\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "folder1=glob.glob(\"wisdm-dataset/raw/*\")\n",
        "# print(folder1)\n",
        "column_names = ['ID', 'activity','timestamp','x','y','z']\n",
        "overall_dataframe=pd.DataFrame(columns = column_names)\n",
        "for subfolder in folder1:\n",
        "    parent_dir = \"./processed/\"\n",
        "    path = os.path.join(parent_dir, subfolder.split('\\\\')[-1])\n",
        "    if not os.path.exists(path): os.makedirs(path)\n",
        "    folder2=glob.glob(subfolder+\"/*\")\n",
        "    for subsubfolder in folder2:\n",
        "        activity_dataframe = pd.DataFrame(columns = column_names)\n",
        "        subfolder_path = os.path.join(path, subsubfolder.split('/')[-1])\n",
        "        if not os.path.exists(subfolder_path): os.makedirs(subfolder_path)\n",
        "        files=glob.glob(subsubfolder+\"/*\")\n",
        "        for file in files:\n",
        "            # print(file)\n",
        "            df = pd.read_csv(file, sep=\",\",header=None)\n",
        "            df.columns = ['ID','activity','timestamp','x','y','z']\n",
        "            # activity_dataframe=activity_dataframe.append(df)\n",
        "            activity_dataframe = pd.concat([activity_dataframe, df], ignore_index=True)\n",
        "\n",
        "        activity_dataframe['z']=activity_dataframe['z'].str[:-1]\n",
        "        # activity_dataframe['meter']=subsubfolder.split('/')[-1]\n",
        "        # activity_dataframe['device']=subfolder.split('/')[-1]\n",
        "        activity_dataframe.to_csv(subfolder_path+'/data.csv',index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "CNVNCV8CGtR_"
      },
      "outputs": [],
      "source": [
        "# @title wisdm dataloader\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "# /content/processed/wisdm-dataset/raw/phone/accel/data.csv\n",
        "# /content/processed/wisdm-dataset/raw/watch/gyro/data.csv\n",
        "\n",
        "class BufferDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        # df_keep = pd.read_csv(\"data.csv\")#[['ID','activity','timestamp','x','y','z']]\n",
        "        df_keep = pd.read_csv(\"/content/processed/wisdm-dataset/raw/watch/accel/data.csv\")\n",
        "\n",
        "        user_acts = dict(tuple(df_keep.groupby(['ID','activity'])[['timestamp','x','y','z']]))\n",
        "        self.data = [[d.to_numpy(), a] for a, d in user_acts.items()]\n",
        "        self.act_dict = {i: act for i, act in enumerate(df_keep['activity'].unique())}\n",
        "        self.act_invdict = {v: k for k, v in self.act_dict.items()} # {'A': 0, 'B': 1, ...\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, id_act = self.data[idx]\n",
        "        id_act = self.process(id_act)\n",
        "        return torch.tensor(x[:3500]), id_act # 3567\n",
        "\n",
        "    def process(self, id_act):\n",
        "        return int(id_act[0]), self.act_invdict[id_act[1]]\n",
        "\n",
        "    def transform(self, act_list): # rand resize crop, rand mask # https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html\n",
        "        act_list = np.array(act_list)\n",
        "        try: hr, temp, heart= zip(*act_list)\n",
        "        except ValueError: print('err:',act_list)\n",
        "        kind = 'nearest' if len(act_list)>self.seq_len/.7 else 'linear'\n",
        "        temp_interpolator = scipy.interpolate.interp1d(hr, temp, kind=kind) # linear nearest quadratic cubic\n",
        "        heart_interpolator = scipy.interpolate.interp1d(hr, heart, kind=kind) # linear nearest quadratic cubic\n",
        "        hr_ = np.sort(np.random.uniform(hr[0], hr[-1], round(self.seq_len*random.uniform(1,1/.7))))\n",
        "        temp_, heart_ = temp_interpolator(hr_), heart_interpolator(hr_)\n",
        "        act_list = list(zip(hr_, temp_, heart_))\n",
        "\n",
        "        idx = torch.randint(len(act_list)-self.seq_len+1, size=(1,))\n",
        "        # return act_list[idx: idx+self.seq_len]\n",
        "        act_list = act_list[idx: idx+self.seq_len]\n",
        "\n",
        "        # mask=(torch.rand(self.seq_len)<.1) # True -> masked # random masking\n",
        "        # # act_list[mask] = self.pad[0]\n",
        "        # act_list = [self.pad[0] if m else a for m,a in zip(mask, act_list)]\n",
        "        return act_list\n",
        "\n",
        "    def add(self, episode):\n",
        "        self.data.append(episode)\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    # print(npimg.shape) # (3, 64, 64)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# df_keep = pd.read_csv(\"data.csv\")#[['ID','activity','timestamp','x','y','z']]\n",
        "# user_acts = dict(tuple(df_keep.groupby(['ID','activity'])[['timestamp','x','y','z']]))\n",
        "# dataset = [[d.to_numpy(), a] for a, d in user_acts.items()]\n",
        "\n",
        "\n",
        "train_data = BufferDataset() # one line of poem is roughly 50 characters\n",
        "\n",
        "dataset_size = len(train_data)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(0.7 * dataset_size))\n",
        "np.random.seed(0)\n",
        "np.random.shuffle(indices)\n",
        "train_indices, val_indices = indices[:split], indices[split:]\n",
        "\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "valid_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 64 #512\n",
        "# train_loader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "# test_loader = DataLoader(train_data, sampler=valid_sampler, batch_size=batch_size, num_workers=2, drop_last=True)\n",
        "train_loader = DataLoader(train_data, sampler=train_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "test_loader = DataLoader(train_data, sampler=valid_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4c4r5Hkry99",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title tsai\n",
        "# https://timeseriesai.github.io/tsai/\n",
        "!pip install -qU tsai # 3mins\n",
        "import tsai\n",
        "from tsai.data.external import get_UCR_data, get_UCR_multivariate_list\n",
        "\n",
        "# l = get_UCR_multivariate_list()\n",
        "# print(len(l), l)\n",
        "# X_train, y_train, X_valid, y_valid = get_UCR_data(dsid) # tsai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yRuBXTauj2f4",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d8ce738-fbd8-4389-b032-579a399604a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/374.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# @title tslearn\n",
        "!pip install -qU tslearn\n",
        "from tslearn.datasets import UCR_UEA_datasets # https://tslearn.readthedocs.io/en/latest/gen_modules/datasets/tslearn.datasets.UCR_UEA_datasets.html\n",
        "\n",
        "# l = UCR_UEA_datasets().list_datasets() # 30 ['ArticularyWordRecognition', 'AtrialFibrillation', 'BasicMotions', 'CharacterTrajectories', 'Cricket', 'DuckDuckGeese', 'EigenWorms', 'Epilepsy', 'EthanolConcentration', 'ERing', 'FaceDetection', 'FingerMovements', 'HandMovementDirection', 'Handwriting', 'Heartbeat', 'InsectWingbeat', 'JapaneseVowels', 'Libras', 'LSST', 'MotorImagery', 'NATOPS', 'PenDigits', 'PEMS-SF', 'Phoneme', 'RacketSports', 'SelfRegulationSCP1', 'SelfRegulationSCP2', 'SpokenArabicDigits', 'StandWalkJump', 'UWaveGestureLibrary']\n",
        "# l = UCR_UEA_datasets().list_multivariate_datasets() # same ^\n",
        "# l = UCR_UEA_datasets().list_univariate_datasets() # 0 []\n",
        "# print(len(l), l)\n",
        "\n",
        "# for dataset_name in data_loader.list_datasets(): # 30 ['ArticularyWordRecognition', 'AtrialFibrillation', 'BasicMotions', 'CharacterTrajectories', 'Cricket', 'DuckDuckGeese', 'EigenWorms', 'Epilepsy', 'EthanolConcentration', 'ERing', 'FaceDetection', 'FingerMovements', 'HandMovementDirection', 'Handwriting', 'Heartbeat', 'InsectWingbeat', 'JapaneseVowels', 'Libras', 'LSST', 'MotorImagery', 'NATOPS', 'PenDigits', 'PEMS-SF', 'Phoneme', 'RacketSports', 'SelfRegulationSCP1', 'SelfRegulationSCP2', 'SpokenArabicDigits', 'StandWalkJump', 'UWaveGestureLibrary']\n",
        "#     X_train, y_train, X_test, y_test = data_loader.load_dataset(dataset_name)\n",
        "\n",
        "# X_train, y_train, X_test, y_test = UCR_UEA_datasets().load_dataset('AtrialFibrillation') # tslearn\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ep7xumXZke5y",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title sktime\n",
        "!pip install -q sktime\n",
        "from sktime.datasets import load_UCR_UEA_dataset\n",
        "import numpy as np\n",
        "# https://www.sktime.net/en/v0.32.2/examples/AA_datatypes_and_datasets.html#Section-3.2.3:-time-series-classification-data-sets-from-the-UCR/UEA-time-series-classification-repository\n",
        "\n",
        "X_train, y_train = load_UCR_UEA_dataset('AtrialFibrillation')\n",
        "X_train, y_train = load_UCR_UEA_dataset('AtrialFibrillation', split=\"train\")\n",
        "X_test, y_test = load_UCR_UEA_dataset('AtrialFibrillation', split=\"test\")\n",
        "\n",
        "print(f\"Training data shape: {X_train.shape}\")\n",
        "print(f\"Training labels shape: {y_train.shape}\")\n",
        "print(f\"Testing data shape: {X_test.shape}\")\n",
        "print(f\"Testing labels shape: {y_test.shape}\")\n",
        "\n",
        "# X_train and X_test will typically be Pandas DataFrames or NumPy arrays\n",
        "# y_train and y_test will be NumPy arrays or Pandas Series\n",
        "\n",
        "# print(X_train, y_train)\n",
        "# print(X_train[0], y_train[0])\n",
        "# print(X_train)\n",
        "print(X_train.iloc[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Gr_588txsd6",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title test load all datasets\n",
        "l = get_UCR_multivariate_list()\n",
        "print(len(l), l)\n",
        "for i, dataset_name in enumerate(get_UCR_multivariate_list()):\n",
        "# for dataset_name in get_UCR_multivariate_list()[15:20]:\n",
        "    print(dataset_name)\n",
        "    # if dataset_name in ['DuckDuckGeese','FaceDetection','InsectWingbeat','PEMS-SF']:\n",
        "    #     print('skip', dataset_name)\n",
        "    #     continue\n",
        "    # try: X_train, y_train, X_valid, y_valid = get_UCR_data(dataset_name) # tsai\n",
        "    # # InsectWingbeat, PEMS-SF slow\n",
        "\n",
        "    # if dataset_name in ['AtrialFibrillation', 'CharacterTrajectories','DuckDuckGeese','EigenWorms','ERing','InsectWingbeat','JapaneseVowels','SpokenArabicDigits']:\n",
        "    # # EigenWorms slow\n",
        "    #     print('skip', dataset_name)\n",
        "    #     continue\n",
        "    # try: X_train, y_train, X_test, y_test = UCR_UEA_datasets().load_dataset(dataset_name) # tslearn\n",
        "\n",
        "    if dataset_name in ['InsectWingbeat']:\n",
        "        print('skip', dataset_name)\n",
        "        continue\n",
        "    try:\n",
        "        X_train, y_train = load_UCR_UEA_dataset(dataset_name)\n",
        "        X_train, y_train = load_UCR_UEA_dataset(dataset_name, split=\"train\")\n",
        "        X_test, y_test = load_UCR_UEA_dataset(dataset_name, split=\"test\")\n",
        "    # InsectWingbeat slow oom\n",
        "\n",
        "    # who has DuckDuckGeese','FaceDetection','InsectWingbeat\n",
        "\n",
        "\n",
        "    except Exception as e: print(e); continue\n",
        "    print(dataset_name, X_train.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EgLjldpPdSsQ",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title time series DataLoader\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "dataset_name = 'EthanolConcentration'\n",
        "# X_train, y_train, X_test, y_test = get_UCR_data(dataset_name) # tsai\n",
        "X_train, y_train, X_test, y_test = UCR_UEA_datasets().load_dataset(dataset_name) # tslearn\n",
        "\n",
        "\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        chars = sorted(list(set(y)))\n",
        "        self.vocab_size = len(chars) #\n",
        "        self.stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "        self.itos = {i:ch for i,ch in enumerate(chars)}\n",
        "        self.X = torch.tensor(X) # (N, 1, T)\n",
        "        self.y = self.data_process(y) #\n",
        "\n",
        "    def data_process(self, data): # str\n",
        "        return torch.tensor([self.stoi.get(c) for c in data]) #\n",
        "    def __len__(self): return len(self.y)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "train_data = TimeSeriesDataset(X_train, y_train)\n",
        "test_data = TimeSeriesDataset(X_test, y_test)\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=2)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=2)\n",
        "\n",
        "# print(X_train, y_train, X_test, y_test)\n",
        "# for x,y in train_loader:\n",
        "# # for x,y in train_data:\n",
        "#     print(x.shape, y.shape) # (261, 1751, 3)\n",
        "#     print(x, y)\n",
        "#     break\n",
        "\n",
        "# print(x.shape[-1])\n",
        "# print(X_train.shape[-1])\n",
        "# print(train_data.vocab_size) # 4\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title plot data\n",
        "# print(X_train, y_train, X_test, y_test)\n",
        "# for x,y in train_loader:\n",
        "# for x,y in train_data:\n",
        "for i, (x,y) in enumerate(train_data):\n",
        "    # print(x.shape, y.shape)\n",
        "    # print(x, y)\n",
        "    # break\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.rcParams[\"figure.figsize\"] = (8,4)\n",
        "    # plt.plot(x[:,0])\n",
        "    for j in range(x.shape[-1]):\n",
        "        plt.plot(x[:,j])\n",
        "    plt.show()\n",
        "    if i>=3: break\n",
        "\n",
        "# print(x.shape[-1])\n",
        "# print(X_train.shape[-1])\n",
        "# print(train_data.vocab_size)\n",
        "\n"
      ],
      "metadata": {
        "id": "3B3t5tSmK0Fv",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "f6T4F651kmGh"
      },
      "outputs": [],
      "source": [
        "# @title AttentionBlock\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "import math\n",
        "def StableInit(m): # https://openreview.net/pdf?id=lkRjnNW0gb\n",
        "    if isinstance(m, nn.Linear):\n",
        "        # W ~ N(0, ( 1/ (sqrt(n_in) + sqrt(n_out)) )^2 )\n",
        "        # want std = 1/ (sqrt(n_in) + sqrt(n_out))\n",
        "        # n_in, n_out = module.weight.shape[0], module.weight.shape[1]\n",
        "        n_in, n_out = m.weight.shape\n",
        "        torch.nn.init.normal_(m.weight, std=1/(math.sqrt(n_in)+math.sqrt(n_out)))\n",
        "        if m.bias is not None:\n",
        "            nn.init.zeros_(m.bias)\n",
        "\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class SelfAttn(nn.Module):\n",
        "    def __init__(self, dim, n_heads):\n",
        "        super().__init__()\n",
        "        self.dim, self.n_heads = dim, n_heads\n",
        "        d_head = dim//n_heads\n",
        "        self.qkv = nn.Linear(dim, dim*3, bias=False)\n",
        "        # self.lin = nn.Linear(dim, dim)\n",
        "        self.lin = zero_module(nn.Linear(dim, dim))\n",
        "        # self.rope = RoPE(d_head, seq_len=512, base=10000)\n",
        "        # self.rope = RoPE2D(d_head, h=64, w=64, base=100)\n",
        "        self.scale = d_head**-.5\n",
        "        # torch.nn.init.normal_(self.qkv.weight, std=.02)\n",
        "        self.qkv.apply(StableInit)\n",
        "\n",
        "    def forward(self, x): # [b,t,d]\n",
        "        q,k,v = self.qkv(x).unflatten(-1, (self.n_heads,-1)).transpose(1,2).chunk(3, dim=-1) # [b, t, n_heads, d_head] -> [b, n_heads, t, d_head]\n",
        "        # q, k = self.rope(q), self.rope(k)\n",
        "        q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2)\n",
        "        context = k.transpose(-2,-1) @ v # [batch, n_heads, d_head, d_head]\n",
        "        x = q @ context # [b, n_heads, t, d_head]\n",
        "        # print('SelfAttn', x.shape)\n",
        "        x = x.transpose(1,2).flatten(2)\n",
        "        return self.lin(x)\n",
        "\n",
        "\n",
        "\n",
        "class SwiGLU(nn.Module): # https://arxiv.org/pdf/2002.05202\n",
        "    def __init__(self, d_model, ff_dim): # d_model * 3*ff_dim params\n",
        "        super().__init__()\n",
        "        self.lin0 = nn.Linear(d_model, 2*ff_dim, bias=False)\n",
        "        self.lin1 = zero_module(nn.Linear(ff_dim, d_model, bias=False))\n",
        "\n",
        "    def forward(self, x): # [b,t,d]\n",
        "        x0, x1 = self.lin0(x).chunk(2, dim=-1)\n",
        "        return self.lin1(x0*F.silu(x1))\n",
        "\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, mult=4, drop=0.):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        # self.norm = nn.RMSNorm(d_model, elementwise_affine=False) # LayerNorm RMSNorm\n",
        "        self.norm1, self.norm2 = nn.LayerNorm(d_model), nn.LayerNorm(d_model)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "        self.attn = SelfAttn(d_model, n_heads)\n",
        "        # act = nn.GELU() # ReLU GELU\n",
        "        act = Swwish()\n",
        "        ff_dim=d_model*mult\n",
        "        self.ff = nn.Sequential(\n",
        "            # nn.RMSNorm(d_model), act, nn.Dropout(drop), nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Dropout(drop), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            nn.RMSNorm(d_model), nn.Linear(d_model, ff_dim), act,\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(drop), nn.Linear(d_model, ff_dim), act,\n",
        "            nn.RMSNorm(ff_dim), nn.Dropout(drop), zero_module(nn.Linear(ff_dim, d_model))\n",
        "        )\n",
        "        # self.ff = SwiGLU(d_model, ff_dim)\n",
        "\n",
        "\n",
        "    def forward(self, x): # [batch, seq_len, d_model]\n",
        "        # print('attnblk fwd',x.shape)\n",
        "        x = x + self.drop(self.attn(self.norm1(x)))\n",
        "        x = x + self.ff(x)\n",
        "        # x = x + self.drop(self.ff(self.norm2(x)))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fh9o__m2-j7K",
        "outputId": "f412c2b0-c375-4617-e7b7-002f76904460"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/268.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m266.2/268.0 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.0/268.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25htorch.Size([64, 200])\n"
          ]
        }
      ],
      "source": [
        "# @title simplex\n",
        "!pip install -q opensimplex\n",
        "import opensimplex\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def simplexmask1d(seq=512, ctx_scale=(.85,1), trg_scale=(.6,.8), B=64, chaos=[1,.5]):\n",
        "    i = np.linspace(0, chaos[0], num=seq) # 2-5\n",
        "    noise = opensimplex.noise2array(i, np.random.randint(1e10, size=B)) # [B, seq]\n",
        "    noise = torch.from_numpy(noise)\n",
        "    # trunc_normal = torch.fmod(torch.randn(2)*.3,1)/2 + .5\n",
        "    # print(trunc_normal)\n",
        "    ctx_mask_scale = torch.rand(1) * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    trg_mask_scale = torch.rand(1) * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "    # ctx_mask_scale = trunc_normal[0] * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    # trg_mask_scale = trunc_normal[1] * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "\n",
        "    ctx_len, trg_len = int(seq*ctx_mask_scale), int(seq*trg_mask_scale)\n",
        "    val, trg_index = torch.topk(noise, trg_len, dim=1, sorted=False)\n",
        "    ctx_len = ctx_len - trg_len\n",
        "\n",
        "    remove_mask = torch.ones((B,seq), dtype=bool) # [B, S]\n",
        "    remove_mask.scatter_(1, trg_index, False).flatten()\n",
        "    ind = torch.arange(seq).unsqueeze(0).repeat(B,1)[remove_mask].reshape(B, -1)\n",
        "\n",
        "    i = np.linspace(0, chaos[1], num=seq) # 2-5\n",
        "    noise = opensimplex.noise2array(i, np.random.randint(1e10, size=B)) # [B, seq]\n",
        "    noise = torch.from_numpy(noise)[remove_mask].reshape(B, -1)\n",
        "    val, ctx_ind = torch.topk(noise, ctx_len, dim=1, sorted=False)\n",
        "    ctx_index = ind[torch.arange(B).unsqueeze(-1), ctx_ind]\n",
        "    return ctx_index, trg_index\n",
        "\n",
        "b=64\n",
        "# ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.7,.8), trg_scale=(.4,.6), B=b, chaos=[3,.5])\n",
        "ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.85,1), trg_scale=(.7,.8), B=b, chaos=[3,.5])\n",
        "# ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.8,.9), trg_scale=(.7,.8), B=b, chaos=[1,.5])\n",
        "mask = torch.zeros(b ,200)\n",
        "mask[torch.arange(b).unsqueeze(-1), trg_index] = 1\n",
        "mask[torch.arange(b).unsqueeze(-1), ctx_index] = .5\n",
        "# mask = mask[None,...]\n",
        "# mask = mask[:,None,None,:]#.repeat(1,3,1,1)\n",
        "print(mask.shape)\n",
        "\n",
        "# def imshow(img):\n",
        "#     npimg = img.numpy()\n",
        "#     plt.rcParams[\"figure.figsize\"] = (8,4)\n",
        "#     # plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "#     plt.pcolormesh(np.transpose(npimg, (1, 2, 0)))\n",
        "#     plt.show()\n",
        "# # imshow(mask[0])\n",
        "# import torchvision\n",
        "# imshow(torchvision.utils.make_grid(mask, nrow=1))\n",
        "\n",
        "# print(index)\n",
        "# print(index.shape)\n",
        "# print(mask)\n",
        "# print(mask.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "Xn7WZShwWxF8"
      },
      "outputs": [],
      "source": [
        "# @title ijepa multiblock 1d\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "import torch\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(self, length=200,\n",
        "        enc_mask_scale=(0.2, 0.8), pred_mask_scale=(0.2, 0.8),\n",
        "        nenc=1, npred=2, min_keep=4, allow_overlap=False):\n",
        "        super().__init__()\n",
        "        self.length = length\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "\n",
        "    def _sample_block_size(self, scale):\n",
        "        _rand = torch.rand(1).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.length * mask_scale) # num patches to keep\n",
        "        # -- Sample block aspect-ratio\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        l = max_keep#int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        while l >= self.length: l -= 1 # crop mask to be smaller than img\n",
        "        return l\n",
        "\n",
        "    def _sample_block_mask(self, length, acceptable_regions=None):\n",
        "        l = length\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # --\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            left = torch.randint(0, self.length - l, (1,))\n",
        "            mask = torch.zeros(self.length, dtype=torch.int32)\n",
        "            mask[left:left+l] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones(self.length, dtype=torch.int32)\n",
        "        mask_complement[left:left+l] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, B):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        p_size = self._sample_block_size(scale=self.pred_mask_scale)\n",
        "        e_size = self._sample_block_size(scale=self.enc_mask_scale)\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.length\n",
        "        min_keep_enc = self.length\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                print(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "        return collated_masks_enc, collated_masks_pred\n",
        "\n",
        "batch=64\n",
        "length=200\n",
        "mask_collator = MaskCollator(length=length, enc_mask_scale=(.85, 1.), pred_mask_scale=(.15, .2),\n",
        "        nenc=1, npred=4, min_keep=4,\n",
        "        # allow_overlap=True)\n",
        "        allow_overlap=False)\n",
        "\n",
        "collated_masks_enc, collated_masks_pred = mask_collator(batch)\n",
        "context_indices, trg_indices = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "# print(context_indices.shape, trg_indices.shape)\n",
        "\n",
        "\n",
        "# plt.pcolormesh(mask)\n",
        "b=64\n",
        "mask = torch.zeros(batch ,length)\n",
        "mask[torch.arange(batch).unsqueeze(-1), trg_indices] = 1\n",
        "mask[torch.arange(batch).unsqueeze(-1), context_indices] = .5\n",
        "# mask = mask[None,...]\n",
        "# print(mask.shape)\n",
        "# mask = mask[:,None,None,:]#.repeat(1,3,1,1)\n",
        "# print(mask.shape)\n",
        "\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# def imshow(img):\n",
        "#     npimg = img.numpy()\n",
        "#     plt.rcParams[\"figure.figsize\"] = (8,4)\n",
        "#     # plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "#     plt.pcolormesh(np.transpose(npimg, (1, 2, 0)))\n",
        "#     # plt.imshow(npimg)\n",
        "#     plt.show()\n",
        "# # imshow(mask)\n",
        "# import torchvision\n",
        "# print(torchvision.utils.make_grid(mask, nrow=1).shape)\n",
        "# # imshow(torchvision.utils.make_grid(mask, nrow=8))\n",
        "# imshow(torchvision.utils.make_grid(mask, nrow=1))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## visualise mask distribution"
      ],
      "metadata": {
        "id": "oeazTakLj_Nn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37BYpacmIcw1"
      },
      "outputs": [],
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (4,4)\n",
        "# plt.plot(ttc,ttt)\n",
        "plt.scatter(ttc,ttt)\n",
        "plt.xlabel('context masks')\n",
        "plt.ylabel('target masks')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58nNtUss4YFX"
      },
      "outputs": [],
      "source": [
        "ttc=[]\n",
        "ttt=[]\n",
        "def mean(x): return sum(x)/len(x)\n",
        "\n",
        "for i in range(1000):\n",
        "    # collated_masks_enc, collated_masks_pred = mask_collator(1)\n",
        "    # context_indices, trg_indices = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "    # context_indices, trg_indices = simplexmask1d(seq=200, ctx_scale=(.8,1), trg_scale=(.2,.8), B=1, chaos=[3,.5])\n",
        "    context_indices, trg_indices = simplexmask1d(seq=200, ctx_scale=(.85,1), trg_scale=(.7,.8), B=1, chaos=[1,.5])\n",
        "    ttc.append(context_indices.shape[-1])\n",
        "    ttt.append(trg_indices.shape[-1])\n",
        "\n",
        "print(mean(ttc), mean(ttt))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"figure.figsize\"] = (4,4)\n",
        "plt.hist(ttc, bins=20, alpha=.5, label='context mask')\n",
        "plt.hist(ttt, bins=20, alpha=.5, label='target mask')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## bdnffdb"
      ],
      "metadata": {
        "id": "FqlsFoUJqeGD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "ge36SCxOl2Oq"
      },
      "outputs": [],
      "source": [
        "# @title RoPE\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def RoPE(dim, seq_len=512, base=10000):\n",
        "    theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "    pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "    angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "    rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [1, seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "    return rot_emb\n",
        "\n",
        "# class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "#     def __init__(self, dim, seq_len=512, base=10000):\n",
        "#         super().__init__()\n",
        "#         self.dim, self.base = dim, base\n",
        "#         theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "#         pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "#         angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "#         self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         seq_len = x.size(1)\n",
        "#         if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.base)\n",
        "#         return x * self.rot_emb[:seq_len]\n",
        "\n",
        "class RotEmb(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, top=torch.pi, base=10000):\n",
        "        super().__init__()\n",
        "        self.theta = top / (base ** (torch.arange(0, dim, step=2, device=device) / dim))\n",
        "        # self.theta = top / (base ** torch.linspace(0, 1, dim//2, device=device))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (pos.unsqueeze(-1) * self.theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [seq_len, dim]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "id": "n9a9OwgKjTUP"
      },
      "outputs": [],
      "source": [
        "# @title snake\n",
        "# https://github.com/Aria-K-Alethia/BigCodec/blob/main/vq/activations.py\n",
        "# https://github.com/zhenye234/X-Codec-2.0/blob/main/vq/activations.py#L62\n",
        "# Implementation adapted from https://github.com/EdwardDixon/snake under the MIT license.\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# class Snake(nn.Module):\n",
        "#     def __init__(self, in_features, alpha=1.0, alpha_logscale=False):\n",
        "#         super().__init__()\n",
        "#         # self.in_features = in_features\n",
        "#         self.alpha_logscale = alpha_logscale\n",
        "#         if self.alpha_logscale: # log scale alphas initialized to zeros\n",
        "#             self.alpha = nn.Parameter(torch.zeros(in_features) * alpha)\n",
        "#         else: # linear scale alphas initialized to ones\n",
        "#             self.alpha = nn.Parameter(torch.ones(in_features) * alpha)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         alpha = self.alpha.unsqueeze(0).unsqueeze(-1) # line up with x to [B, C, T]\n",
        "#         if self.alpha_logscale:\n",
        "#             alpha = torch.exp(alpha)\n",
        "#         x = x + (1.0 / (alpha + 1e-9)) * torch.pow(torch.sin(x * alpha), 2) # Snake ∶= x + 1/a * sin^2(ax)\n",
        "#         return x\n",
        "\n",
        "\n",
        "class Snake(nn.Module):\n",
        "    def __init__(self, dim, *args, **kwargs):\n",
        "        super().__init__()\n",
        "        # self.alpha = nn.Parameter(torch.zeros(1,dim,1)).exp() # alpha_logscale=True\n",
        "        self.alpha = nn.Parameter(torch.ones(1,dim,1)*1.) # 1.\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + (1.0 / (self.alpha + 1e-9)) * torch.pow(torch.sin(x * self.alpha), 2) # Snake ∶= x + 1/a * sin^2(ax)\n",
        "\n",
        "\n",
        "@torch.jit.script\n",
        "def snake(x, alpha): # [b,c,t], [1,c,1]\n",
        "    return x + (alpha + 1e-9).reciprocal() * torch.sin(alpha * x).pow(2) # [b,c,t]\n",
        "\n",
        "class Snake1d(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.alpha = nn.Parameter(torch.zeros(1,dim,1)).exp()\n",
        "        self.alpha = nn.Parameter(torch.ones(1,dim,1)*1.) # 1.\n",
        "\n",
        "    def forward(self, x):\n",
        "        return snake(x, self.alpha)\n",
        "\n",
        "\n",
        "class SnakeBeta(nn.Module):\n",
        "    def __init__(self, in_features, alpha=1.0, alpha_logscale=False):\n",
        "        super().__init__()\n",
        "        # self.in_features = in_features\n",
        "        self.alpha_logscale = alpha_logscale\n",
        "        if self.alpha_logscale: # log scale alphas initialized to zeros\n",
        "            self.alpha = nn.Parameter(torch.zeros(in_features) * alpha)\n",
        "            self.beta = nn.Parameter(torch.zeros(in_features) * alpha)\n",
        "        else: # linear scale alphas initialized to ones\n",
        "            self.alpha = nn.Parameter(torch.ones(in_features) * alpha)\n",
        "            self.beta = nn.Parameter(torch.ones(in_features) * alpha)\n",
        "\n",
        "    def forward(self, x): # [b,c,t]\n",
        "        alpha = self.alpha.unsqueeze(0).unsqueeze(-1) # line up with x to [B, C, T]\n",
        "        beta = self.beta.unsqueeze(0).unsqueeze(-1)\n",
        "        if self.alpha_logscale:\n",
        "            alpha = torch.exp(alpha)\n",
        "            beta = torch.exp(beta)\n",
        "        x = x + (1. / (beta + 1e-9)) * pow(torch.sin(x * alpha), 2) # SnakeBeta ∶= x + 1/b *sin^2(ax)\n",
        "        return x # [b,c,t]\n",
        "\n",
        "b,c,t = 5,16,7\n",
        "# a1 = Snake(c)\n",
        "a1 = Snake(c, alpha_logscale=True) # 70.4 µs 69.9\n",
        "# a1 = Snake1d(c) # 47.8 µs 48.3\n",
        "# a1 = SnakeBeta(256)\n",
        "x = torch.randn(b,c,t)\n",
        "# x = a1(x)\n",
        "# print(x.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title swwish\n",
        "@torch.jit.script\n",
        "def learntswwish(x, alpha): # [b,c,t], [1,c,1]\n",
        "    # print('alpha', alpha.shape, x.shape)\n",
        "    # alpha = alpha.exp()\n",
        "    return .5 * (1 + x - torch.cos(alpha * x)) # [b,c,t]\n",
        "    # return .5 * (1/alpha + x - torch.cos(alpha * x)/alpha) # [b,c,t]\n",
        "    # return .5 * (1/alpha + x - torch.cos(1.25*alpha * x)/alpha) # [b,c,t]\n",
        "\n",
        "class LearntSwwish(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.alpha = nn.Parameter(torch.zeros(1,dim,1))#.exp()\n",
        "        self.alpha = nn.Parameter(torch.ones(1,dim,1)*1.) # 1.\n",
        "        self.alpha = nn.Parameter(torch.zeros(dim))#.exp()\n",
        "        # self.alpha = nn.Parameter(torch.randn(dim).abs()*4)\n",
        "        self.alpha = nn.Parameter(torch.randn(dim,1)*30) #4 20\n",
        "        # self.alpha = nn.Parameter(torch.ones(1,dim)*1.) # 1.\n",
        "\n",
        "    def forward(self, x):\n",
        "        return learntswwish(x, self.alpha)\n",
        "\n",
        "class Swwish(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # return .5 * (1 + x - x.cos())\n",
        "        return .5 * (1 + x - 1.25*x.cos())\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "MXZJBLF3D2Cx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title scheduler\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "import math\n",
        "def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, num_cycles=0.5, last_epoch=-1):\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < num_warmup_steps:\n",
        "            return float(current_step) / float(max(1, num_warmup_steps))\n",
        "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
        "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))\n",
        "    return LambdaLR(optimizer, lr_lambda, last_epoch)\n",
        "\n",
        "# total_steps=100\n",
        "# base_lr, max_lr = 3e-5, 3e-4\n",
        "\n",
        "# import torch\n",
        "# model=torch.nn.Linear(2,3)\n",
        "# optim = torch.optim.AdamW(model.parameters(), lr=base_lr, betas=(0.9, 0.999))\n",
        "\n",
        "# scheduler = get_cosine_schedule_with_warmup(optim, num_warmup_steps=20 , num_training_steps=total_steps) # https://docs.pytorch.org/torchtune/0.2/generated/torchtune.modules.get_cosine_schedule_with_warmup.html\n",
        "# # scheduler = torch.optim.lr_scheduler.OneCycleLR(optim, max_lr=max_lr, total_steps=total_steps, pct_start=0.45, anneal_strategy='cos', cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=div_factor, final_div_factor=100.0, three_phase=True,)\n",
        "\n",
        "# lr_lst=[]\n",
        "# import matplotlib.pyplot as plt\n",
        "# for t in range(total_steps):\n",
        "#     lr=optim.param_groups[0][\"lr\"]\n",
        "#     lr_lst.append(lr)\n",
        "#     scheduler.step()\n",
        "# plt.plot(lr_lst)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "3vUVNJc_sy1a"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-zdjdJixtOu",
        "outputId": "5edc4ec8-d03a-49f4-dc45-500711d8738f",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12448\n",
            "torch.Size([4, 219, 16])\n"
          ]
        }
      ],
      "source": [
        "# @title TransformerModel/Predictor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, n_heads=4, d_hid=None, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        # self.pos_emb = nn.Parameter(torch.randn(1, 256, d_model)*.02) # 200\n",
        "        self.pos_emb = RoPE(d_model, seq_len=256, base=10000) # 256\n",
        "\n",
        "        # self.transformer = nn.Sequential(*[AttentionBlock(d_model, n_heads=n_heads) for _ in range(nlayers)])\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, n_heads=n_heads, drop=drop) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.randn(1,1,d_model)*0.02) # randn zeros\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim)# if out_dim != d_model else None\n",
        "\n",
        "        torch.nn.init.normal_(self.embed.weight, std=.02)\n",
        "        if self.lin: torch.nn.init.normal_(self.lin.weight, std=.02)\n",
        "\n",
        "\n",
        "    def forward(self, x, context_indices, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = x.shape\n",
        "        # x = x * self.pos_enc(context_indices)\n",
        "        # print(\"Trans pred\",x.shape, self.pos_emb[0,context_indices].shape)\n",
        "        x = x + self.pos_emb[0,context_indices]\n",
        "        # x = x * self.pos_emb[0,context_indices]\n",
        "        # print('pred fwd', self.pos_emb[:,context_indices].shape)\n",
        "\n",
        "        # pred_tokens = self.cls * self.pos_enc(trg_indices) # [M, num_trg_toks, d_model]\n",
        "        pred_tokens = self.cls + self.pos_emb[0,trg_indices]\n",
        "        # pred_tokens = self.cls * self.pos_emb[0,trg_indices]\n",
        "        # print(\"pred fwd\", x.shape, pred_tokens.shape)\n",
        "        x = torch.cat([x, pred_tokens], dim=1) # [batch, seq_len+num_trg_toks, d_model]\n",
        "        out = self.transformer(x)\n",
        "\n",
        "        out = self.norm(out)\n",
        "        out = out[:,seq:] # [batch, num_trg_toks, d_model]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "# class SLSTM(nn.Module):\n",
        "#     def __init__(self, d_model, num_layers=2, batch_first=True):\n",
        "#         super().__init__()\n",
        "#         self.lstm = nn.LSTM(d_model, d_model, num_layers)\n",
        "\n",
        "#     def forward(self, x): # [b,c,t]\n",
        "#         x = x + self.lstm(x.transpose(-2,-1))[0].transpose(-2,-1) # skip=True\n",
        "#         return x\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, drop=0):\n",
        "    def __init__(self, patch_size, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0):\n",
        "        super().__init__()\n",
        "        patch_size=8\n",
        "        act = nn.ReLU() # ReLU SiLU GELU\n",
        "        # act = LearntSwwish(d_model) # SnakeBeta LearntSwwish\n",
        "        # act1 = LearntSwwish(d_model) # SnakeBeta LearntSwwish\n",
        "        act = Swwish()\n",
        "        self.embed = nn.Sequential(\n",
        "            # # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.MaxPool1d(2,2), #nn.MaxPool1d(3, 2, 3//2),\n",
        "            # # nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.Conv1d(d_model, d_model,3,2,3//2)\n",
        "            nn.Conv1d(in_dim, d_model,7,2,7//2), nn.BatchNorm1d(d_model), act,\n",
        "            nn.Conv1d(d_model, d_model,5,2,5//2), nn.BatchNorm1d(d_model), act,\n",
        "            # nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), act, #nn.MaxPool1d(2,2),\n",
        "            # nn.Conv1d(d_model, d_model,3,2,3//2), nn.BatchNorm1d(d_model), act, #nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2),\n",
        "            # nn.Conv1d(in_dim, d_model, patch_size, patch_size), # like patch\n",
        "            # nn.Conv1d(in_dim, d_model, 1, 1), # like patch\n",
        "\n",
        "            # nn.Conv2d(d_model, d_model,(in_dim,3),2,3//2),\n",
        "            # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.Dropout(drop), nn.BatchNorm1d(d_model), snake,\n",
        "            # SLSTM(d_model),\n",
        "\n",
        "            )\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        # self.pos_emb = nn.Parameter(torch.randn(1, 256, d_model)*.02) # 200\n",
        "        self.pos_emb = RoPE(d_model, seq_len=256, base=10000) # 256\n",
        "        # self.transformer = nn.Sequential(*[AttentionBlock(d_model, n_heads=n_heads) for _ in range(nlayers)])\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, n_heads=n_heads, drop=drop) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "        if self.lin: torch.nn.init.normal_(self.lin.weight, std=.02)\n",
        "\n",
        "        # self.embed.apply(self.init_conv)\n",
        "        self.embed.apply(self.init_weights)\n",
        "\n",
        "    # def init_conv(self, m):\n",
        "    #     if isinstance(m, nn.Conv1d):\n",
        "    #         # nn.init.kaiming_uniform_(m.weight, a=math.sqrt(5))\n",
        "    #         nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "    #         if m.bias is not None:\n",
        "    #             # bound = 1 / math.sqrt(m.in_channels * m.kernel_size * m.kernel_size)\n",
        "    #             # nn.init.uniform_(m.bias, -bound, bound)\n",
        "    #             nn.init.zeros_(m.bias)\n",
        "\n",
        "    def init_weights(self, m):\n",
        "        if isinstance(m, (nn.Linear, nn.Conv1d)):\n",
        "            torch.nn.init.normal_(m.weight, std=.02)\n",
        "            if m.bias is not None: nn.init.zeros_(m.bias)\n",
        "\n",
        "\n",
        "    def forward(self, x, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "        # try: print(\"Trans fwd\",x.shape, context_indices.shape)\n",
        "        # except: print(\"Trans fwd noind\",x.shape)\n",
        "        # x = self.pos_enc(x)\n",
        "        x = x + self.pos_emb[:,:x.shape[1]]\n",
        "        # x = x * self.pos_emb[:,:x.shape[1]]\n",
        "        if context_indices != None: x = x[torch.arange(x.shape[0]).unsqueeze(-1), context_indices] # [batch, num_context_toks, d_model]\n",
        "\n",
        "        # print(\"TransformerModel\",x.shape)\n",
        "        x = self.transformer(x)\n",
        "        out = self.norm(x)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out\n",
        "\n",
        "batch, seq_len, d_model = 4,1751,16 # wisdm 3500, ethol conc 1751\n",
        "in_dim = 3\n",
        "patch_size=32\n",
        "model = TransformerModel(patch_size, in_dim, d_model, n_heads=4, nlayers=3, drop=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "# # # print(out)\n",
        "# model = TransformerPredictor(in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1).to(device)\n",
        "# out = model(out)\n",
        "# print(out.shape)\n",
        "# for name, param in model.named_parameters():\n",
        "#     print(name, param.shape, param[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ardu1zJwdHM3",
        "outputId": "35a3008d-fa4c-4e84-892d-0d32b70d3165"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "101760\n",
            "torch.Size([])\n"
          ]
        }
      ],
      "source": [
        "# @title SeqJEPA\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class SeqJEPA(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, nlayers=2, n_heads=4, drop=0):\n",
        "        super().__init__()\n",
        "        if out_dim is None: out_dim = d_model\n",
        "        self.patch_size = 8 # 8 32\n",
        "        self.student = TransformerModel(self.patch_size, in_dim, d_model, out_dim=out_dim, n_heads=n_heads, nlayers=nlayers, drop=drop)\n",
        "        self.predicter = TransformerPredictor(out_dim, d_model//2, out_dim, n_heads=4, nlayers=1, drop=drop)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "        # self.transform = RandomResizedCrop1d(3500, scale=(.8,1.))\n",
        "\n",
        "    #     self.apply(self.init_weights)\n",
        "    #     self.apply(self.zero_last_layers)\n",
        "    # def init_weights(self, m):\n",
        "    #     if isinstance(m, (nn.Linear, nn.Conv1d)):\n",
        "    #         torch.nn.init.normal_(m.weight, std=.02)\n",
        "    #         if m.bias is not None: nn.init.zeros_(m.bias)\n",
        "\n",
        "    # def zero_last_layers(self, m):\n",
        "    #     children = list(m.children())\n",
        "    #     if not children: return\n",
        "    #     last = children[-1]\n",
        "    #     if isinstance(last, (nn.Linear, nn.Conv2d)):\n",
        "    #         nn.init.zeros_(last.weight)\n",
        "    #         if last.bias is not None: nn.init.zeros_(last.bias)\n",
        "\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]\n",
        "        batch, seq, dim = x.shape\n",
        "        # print(x.shape)\n",
        "        # target_mask = multiblock(seq//self.patch_size, min_s=.2, max_s=.3, M=4, B=1).any(1).squeeze(1) # best.2.3M4 og.15.2M4# mask out targets to be predicted # [M, seq]\n",
        "        # # target_mask = randpatch(seq//self.patch_size, mask_size=8, gamma=.9).unsqueeze(0) # 8.9 [seq]\n",
        "\n",
        "        # print(target_mask.shape, x.shape)\n",
        "        # context_mask = ~multiblock(seq//self.patch_size, min_s=.85, max_s=1, M=1, B=1).squeeze(1)|target_mask # og .85,1.M1 # [1, seq], True->Mask\n",
        "        # context_mask = torch.zeros((1,seq//self.patch_size), dtype=bool)|target_mask # [1,h,w], True->Mask\n",
        "\n",
        "        # context_indices, trg_indices = simplexmask1d(seq//self.patch_size, ctx_scale=(.85,1), trg_scale=(.7,.8), B=batch, chaos=[3,.5])\n",
        "        # context_indices, trg_indices = simplexmask1d(seq//self.patch_size, ctx_scale=(.8,.9), trg_scale=(.7,.8), B=batch, chaos=[1,.5])\n",
        "        # context_indices, trg_indices = simplexmask1d(seq//self.patch_size, ctx_scale=(.8,1), trg_scale=(.2,.8), B=batch, chaos=[1,.5])\n",
        "        # print(context_indices.shape, trg_indices.shape)\n",
        "\n",
        "        # context_indices = context_indices.repeat(batch,1)\n",
        "        # trg_indices = trg_indices.repeat(batch,1)\n",
        "        # context_mask = ~context_mask|target_mask # [1,]\n",
        "        # context_indices = (~context_mask).nonzero()[:,1].unsqueeze(0).repeat(batch,1)\n",
        "        # # print(trg_indices.shape, context_indices.shape)\n",
        "        # # print(context_mask.shape,target_mask.shape, x.shape)\n",
        "        # target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "        # # # target_mask, context_mask = target_mask.repeat(batch,1), context_mask.repeat(batch,1)\n",
        "        # # x_ = x * F.adaptive_avg_pool1d((~context_mask).float(), x.shape[1]).unsqueeze(-1) # zero masked locations\n",
        "\n",
        "        # context_indices = (~context_mask).nonzero()[:,1].unflatten(0, (batch,-1)) # int idx [num_context_toks] , idx of context not masked\n",
        "        # trg_indices = target_mask.nonzero()[:,1].unflatten(0, (batch,-1)) # int idx [num_trg_toks] , idx of targets that are masked\n",
        "\n",
        "\n",
        "        # mask_collator = MaskCollator(length=seq//self.patch_size, enc_mask_scale=(.85,1), pred_mask_scale=(.15,.2), nenc=1, npred=4, min_keep=4, allow_overlap=False)\n",
        "        mask_collator = MaskCollator(length=seq//self.patch_size, enc_mask_scale=(.85,1), pred_mask_scale=(.2,.25), nenc=1, npred=4, min_keep=4, allow_overlap=False)\n",
        "        collated_masks_enc, collated_masks_pred = mask_collator(batch) # idx of ctx, idx of masked trg\n",
        "        # # collated_masks_enc, collated_masks_pred = mask_collator(1) # idx of ctx, idx of masked trg\n",
        "        context_indices, trg_indices = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "        # # # zero_mask = torch.zeros(batch ,seq//self.patch_size, device=device)\n",
        "        # # # zero_mask[torch.arange(batch).unsqueeze(-1), context_indices] = 1\n",
        "        # # zero_mask = torch.zeros(1 ,seq//self.patch_size, device=device)\n",
        "        # # zero_mask[:, context_indices] = 1\n",
        "        # # x_ = x * F.adaptive_avg_pool1d(zero_mask, x.shape[1]).unsqueeze(-1) # zero masked locations\n",
        "        # # context_indices, trg_indices = context_indices.repeat(batch,1), trg_indices.repeat(batch,1)\n",
        "\n",
        "\n",
        "        # print('x_',x_.shape, context_indices.shape, trg_indices.shape)\n",
        "\n",
        "        sx = self.student(x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "        # print('seq_jepa loss sx',sx.shape)\n",
        "        sy_ = self.predicter(sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "        sy_ = F.layer_norm(sy_, (sy_.size(-1),))\n",
        "        with torch.no_grad():\n",
        "            sy = self.teacher(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "            sy = sy[torch.arange(sy.shape[0]).unsqueeze(-1), trg_indices] # [batch, num_context_toks, d_model] # nan bec len(trg_ind)==0 # print('loss sy',torch.isnan(sy).any())\n",
        "            sy = F.layer_norm(sy, (sy.size(-1),))\n",
        "        loss = F.mse_loss(sy, sy_)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        sx = self.student(x)\n",
        "        out = sx.mean(dim=1)\n",
        "        return out\n",
        "\n",
        "# min_s=0.15, max_s, M\n",
        "# trg.15.2M4 C.85 1\n",
        "\n",
        "# 1e-2,1e-3 < 3e-3,1e-3\n",
        "# patch16 < patch32\n",
        "# NoPE good but sus\n",
        "\n",
        "# ctx/trg sacle min/max, num blk,\n",
        "\n",
        "in_dim = X_train.shape[-1] # 3\n",
        "out_dim = train_data.vocab_size # 16\n",
        "d_model=64\n",
        "# seq_jepa = SeqJEPA(in_dim=in_dim, d_model=d_model, out_dim=None, nlayers=1, n_heads=8).to(device)#.to(torch.float)\n",
        "seq_jepa = SeqJEPA(in_dim=in_dim, d_model=d_model, out_dim=None, nlayers=1, n_heads=8, drop=.1).to(device)#.to(torch.float)\n",
        "optim = torch.optim.AdamW(seq_jepa.parameters(), lr=1e-3) # 1e-3?\n",
        "# optim = torch.optim.AdamW([{'params': seq_jepa.student.parameters()},\n",
        "#     {'params': seq_jepa.predicter.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # default 1e-2, 5e-2\n",
        "    # {'params': seq_jepa.predicter.parameters(), 'lr': 1e-2}], lr=1e-3, weight_decay=1e-2)\n",
        "# scheduler = get_cosine_schedule_with_warmup(optim, num_warmup_steps=20 , num_training_steps=total_steps) # https://docs.pytorch.org/torchtune/0.2/generated/torchtune.modules.get_cosine_schedule_with_warmup.html\n",
        "\n",
        "# !pip install -q bitsandbytes\n",
        "# import bitsandbytes as bnb\n",
        "# # optim = bnb.optim.Lion8bit(seq_jepa.parameters(), lr=1e-3, betas=(0.9, 0.99), weight_decay=1e-2)\n",
        "# optim = bnb.optim.Lion(seq_jepa.parameters(), lr=3e-4, betas=(0.9, 0.99), weight_decay=3e-2)\n",
        "# optim = bnb.optim.Lion(seq_jepa.parameters(), lr=1e-6, betas=(0.9, 0.99), weight_decay=1e-5)\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/configs/in1k_vith14_ep300.yaml\n",
        "# d_model 1024,384\n",
        "# depth 12,6/12\n",
        "# wd 5e-2 - 4e-1\n",
        "# adamw 1e-4 - 1e-3 - 1e-6\n",
        "# ema 0.996-1\n",
        "\n",
        "print(sum(p.numel() for p in seq_jepa.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.parameters())) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.predicter.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.student.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.teacher.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# d_model^2 * nlayers\n",
        "\n",
        "x = torch.rand((24, 1700, in_dim), device=device)\n",
        "out = seq_jepa.loss(x)\n",
        "print(out.shape)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(d_model, out_dim).to(device)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n",
        "# optim = torch.optim.AdamW([{'params': seq_jepa.parameters()}, {'params': classifier.parameters(), 'lr': 1e-3}], lr=1e-3)\n",
        "# coptim = torch.optim.AdamW(classifier.parameters(), lr=1e-3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in seq_jepa.named_parameters():\n",
        "    print(name, param.shape, param[0])\n",
        "\n"
      ],
      "metadata": {
        "id": "WgVbsYsu85eT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## violet vicreg rankme"
      ],
      "metadata": {
        "id": "u99QqyJCqp_P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "m4rj4LfPuN1H"
      },
      "outputs": [],
      "source": [
        "# @title TransformerVICReg\n",
        "import torch\n",
        "from torch import nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class TransformerVICReg(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, n_heads=4, d_hid=None, nlayers=1, drop=0):\n",
        "        super().__init__()\n",
        "        act = nn.GELU()\n",
        "        # patch_size=4\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Linear(in_dim, d_model), act\n",
        "            # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(3,2, 3//2),\n",
        "            nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2),\n",
        "            # nn.Conv1d(in_dim, d_model, patch_size, patch_size), # patch\n",
        "            )\n",
        "        self.pos_enc = RoPE(d_model, seq_len=200, base=10000) # 10000\n",
        "        # self.pos_emb = nn.Parameter(torch.randn(1, 200, d_model))\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=200, base=10000).unsqueeze(0), requires_grad=False)\n",
        "\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, n_heads=n_heads) for _ in range(nlayers)])\n",
        "\n",
        "        # out_dim = out_dim or d_model\n",
        "        self.lin = nn.Linear(d_model, out_dim)\n",
        "        self.attn_pool = nn.Linear(d_model, 1, bias=False)\n",
        "\n",
        "        dim_v = d_model * 4\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(out_dim, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v, bias=False),\n",
        "            )\n",
        "\n",
        "    def forward(self, x): # [b,t,d] / [b,c,h,w]\n",
        "        # x = self.embed(x).flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c]\n",
        "        x = self.embed(x.transpose(-2,-1)).transpose(-2,-1) # [b,t,d]\n",
        "        # x = self.embed(x)\n",
        "        x = self.pos_enc(x)\n",
        "        # x = x + self.pos_emb[:,:x.shape[1]]\n",
        "\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        attn = self.attn_pool(x).squeeze(-1) # [batch, seq] # seq_pool\n",
        "        out = (torch.softmax(attn, dim=-1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, seq] @ [batch, seq, dim] -> [batch, dim]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch, ntoken]\n",
        "\n",
        "    def expand(self, x):\n",
        "        sx = self.forward(x)\n",
        "        vx = self.exp(sx)\n",
        "        return vx\n",
        "\n",
        "batch, seq_len = 4,3500\n",
        "in_dim, d_model, out_dim=16,64,16\n",
        "model = TransformerVICReg(in_dim, d_model, out_dim, n_heads=8, nlayers=1, drop=0.).to(device)\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "# x =  torch.rand((batch, in_dim, 32,32), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObiHp-LSuRBA",
        "outputId": "c6ce8734-e0a0-47b2-b11d-e0f8e53af582"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "in vicreg  1.9939420276162247e-16 24.746832251548767 1.6621681808715039e-09\n",
            "(tensor(7.9758e-18, device='cuda:0', grad_fn=<MseLossBackward0>), tensor(0.9899, device='cuda:0', grad_fn=<AddBackward0>), tensor(1.6622e-09, device='cuda:0', grad_fn=<AddBackward0>))\n"
          ]
        }
      ],
      "source": [
        "# @title Violet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class Violet(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, nlayers=2, n_heads=4):\n",
        "        super().__init__()\n",
        "        out_dim = out_dim or d_model\n",
        "        self.student = TransformerVICReg(in_dim, d_model, out_dim=out_dim, n_heads=n_heads, nlayers=nlayers, drop=0.)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "\n",
        "        # vicreg\n",
        "        self.sim_coeff=25. # 10.0 # 25.0 # λ\n",
        "        self.std_coeff=25. # 1.0 # 25.0 # µ\n",
        "        self.cov_coeff=1. # 25.0 # 1.0 # ν\n",
        "\n",
        "        # self.transform = RandomResizedCrop1d(3500, scale=(.8,1.))\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]c/ [b,c,h,w]\n",
        "        # print(x.shape)\n",
        "        # self.transform(x)\n",
        "        vx = self.student.expand(x) # [batch, num_context_toks, out_dim]\n",
        "        with torch.no_grad(): vy = self.teacher.expand(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "        loss = self.vicreg(vx, vy)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        return self.student(x)\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size, num_features = x.shape\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = self.sim_coeff * repr_loss + self.std_coeff * std_loss + self.cov_coeff * cov_loss\n",
        "        print(\"in vicreg \",self.sim_coeff * repr_loss.item() , self.std_coeff * std_loss.item() , self.cov_coeff * cov_loss.item())\n",
        "        # return loss\n",
        "        return repr_loss, std_loss, cov_loss\n",
        "\n",
        "# lr1e-3\n",
        "# n_heads=8 < 4\n",
        "\n",
        "in_dim=3\n",
        "violet = Violet(in_dim, d_model=64, out_dim=16, nlayers=1, n_heads=8).to(device)\n",
        "voptim = torch.optim.AdamW(violet.parameters(), lr=1e-3) # 1e-3?\n",
        "# voptim = torch.optim.AdamW([{'params': violet.student.transformer.parameters()},\n",
        "#     {'params': violet.student.exp.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # default 1e-2\n",
        "# voptim = torch.optim.AdamW([{'params': violet.student.exp.parameters(), 'lr': 3e-3},\n",
        "#     {'params': [p for n, p in violet.named_parameters() if 'student.exp' not in n]}], lr=1e-3, weight_decay=1e-2) # default 1e-2\n",
        "\n",
        "# print(sum(p.numel() for p in violet.parameters() if p.requires_grad)) # 27584\n",
        "\n",
        "\n",
        "x = torch.rand((2,1000,in_dim), device=device)\n",
        "loss = violet.loss(x)\n",
        "# print(out.shape)\n",
        "print(loss)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(16, 18).to(device) # torch/autograd/graph.py RuntimeError: CUDA error: device-side assert triggered CUDA kernel errors\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IACKDymhit7"
      },
      "outputs": [],
      "source": [
        "# optim.param_groups[0]['lr'] = 1e-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7zghfu_jeOQk"
      },
      "outputs": [],
      "source": [
        "# @title RankMe\n",
        "# RankMe: Assessing the Downstream Performance of Pretrained Self-Supervised Representations by Their Rank jun 2023 https://arxiv.org/pdf/2210.02885\n",
        "import torch\n",
        "\n",
        "# https://github.com/Spidartist/IJEPA_endoscopy/blob/main/src/helper.py#L22\n",
        "def RankMe(Z):\n",
        "    \"\"\"\n",
        "    Calculate the RankMe score (the higher, the better).\n",
        "    RankMe(Z) = exp(-sum_{k=1}^{min(N, K)} p_k * log(p_k)),\n",
        "    where p_k = sigma_k (Z) / ||sigma_k (Z)||_1 + epsilon\n",
        "    where sigma_k is the kth singular value of Z.\n",
        "    where Z is the matrix of embeddings (N × K)\n",
        "    \"\"\"\n",
        "    # compute the singular values of the embeddings\n",
        "    # _u, s, _vh = torch.linalg.svd(Z, full_matrices=False)  # s.shape = (min(N, K),)\n",
        "    # s = torch.linalg.svd(Z, full_matrices=False).S\n",
        "    s = torch.linalg.svdvals(Z)\n",
        "    p = s / torch.sum(s, axis=0) + 1e-7\n",
        "    return torch.exp(-torch.sum(p * torch.log(p)))\n",
        "\n",
        "# Z = torch.randn(5, 3)\n",
        "# o = RankMe(Z)\n",
        "# print(o)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Pj25OrEk14pR"
      },
      "outputs": [],
      "source": [
        "# @title LiDAR stable_ssl next\n",
        "# https://github.com/rbalestr-lab/stable-ssl/blob/main/stable_ssl/monitors.py#L106\n",
        "\n",
        "def LiDAR(embeddings, eps=1e-7, delta=1e-3):\n",
        "    embeddings = embeddings.unflatten(0, (-1,2))\n",
        "    (local_n, q, d), device = embeddings.shape, embeddings.device\n",
        "\n",
        "    class_means = embeddings.mean(dim=1) # mu_x\n",
        "    grand_mean_local = class_means.mean(dim=0) # mu\n",
        "    # print(embeddings.shape, class_means.shape, grand_mean_local.shape) # [50, 2, 64], [50, 64], [64]\n",
        "\n",
        "    # local_Sb = torch.zeros(d, d, device=device)\n",
        "    # local_Sw = torch.zeros(d, d, device=device)\n",
        "\n",
        "\n",
        "    # print(diff_b.shape, diff_w.shape) # [64,1], [64,1]\n",
        "    # print(local_Sb.shape, local_Sw.shape) # [64,64], [64,64]\n",
        "\n",
        "    diff_b = (class_means - grand_mean_local).unsqueeze(-1)\n",
        "    # print(diff_b.shape)\n",
        "    # # local_Sb = (diff_b @ diff_b.T).sum()\n",
        "    local_Sb = (diff_b @ diff_b.transpose(-2,-1)).sum(0)\n",
        "    # # print(embeddings.shape, class_means.shape)\n",
        "    diff_w = (embeddings - class_means.unsqueeze(1)).reshape(-1,d,1)\n",
        "    # # print(diff_w.shape)\n",
        "    # # local_Sw = (diff_w @ diff_w.T).sum()\n",
        "    local_Sw = (diff_w @ diff_w.transpose(-2,-1)).sum(0)\n",
        "\n",
        "    # print(local_Sb.shape, local_Sw.shape)\n",
        "    S_b = local_Sb / (local_n - 1)\n",
        "    S_w = local_Sw / (local_n * (q - 1))\n",
        "    S_w += delta * torch.eye(d, device=device)\n",
        "    # print(S_w.shape, d)\n",
        "\n",
        "    eigvals_w, eigvecs_w = torch.linalg.eigh(S_w)\n",
        "    eigvals_w = torch.clamp(eigvals_w, min=eps)\n",
        "\n",
        "    invsqrt_w = (eigvecs_w * (1.0 / torch.sqrt(eigvals_w))) @ eigvecs_w.transpose(-1, -2)\n",
        "    Sigma_lidar = invsqrt_w @ S_b @ invsqrt_w\n",
        "\n",
        "    # lam, _ = torch.linalg.eigh(Sigma_lidar)\n",
        "    lam = torch.linalg.eigh(Sigma_lidar)[0]\n",
        "    # print(lam)\n",
        "    # lam = torch.clamp(lam, min=0.0)\n",
        "\n",
        "    p = lam / lam.sum() + eps\n",
        "    # print(p)\n",
        "    # p = s / torch.sum(s, axis=0) + 1e-7\n",
        "    return torch.exp(-torch.sum(p * torch.log(p)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## fdb"
      ],
      "metadata": {
        "id": "MN5sSWa-qonm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "cellView": "form",
        "id": "2Nd-sGe6Ku4S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "outputId": "62c96df6-26fb-4ca5-a01a-9887c31f5f2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>▆▇▆▆▆▆▆█▅█▅▆▆▇▆▅▅▅▁▆▆▇▅▆▆▄▄▆▅▆▅▅▃▅▆▅▄▃▆▅</td></tr><tr><td>correct</td><td>▇▇█▇▆█▇▇▇▇▇▇▇█▇▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>1.57031</td></tr><tr><td>correct</td><td>0.23194</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">rich-meadow-20</strong> at: <a href='https://wandb.ai/bobdole/ucr/runs/d7ic6ak9' target=\"_blank\">https://wandb.ai/bobdole/ucr/runs/d7ic6ak9</a><br> View project at: <a href='https://wandb.ai/bobdole/ucr' target=\"_blank\">https://wandb.ai/bobdole/ucr</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250530_050954-d7ic6ak9/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250530_051732-kc9oclrp</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/ucr/runs/kc9oclrp' target=\"_blank\">morning-sponge-21</a></strong> to <a href='https://wandb.ai/bobdole/ucr' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/ucr' target=\"_blank\">https://wandb.ai/bobdole/ucr</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/ucr/runs/kc9oclrp' target=\"_blank\">https://wandb.ai/bobdole/ucr/runs/kc9oclrp</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "!pip install -q wandb\n",
        "import wandb # https://docs.wandb.ai/quickstart\n",
        "wandb.login(key='487a2109e55dce4e13fc70681781de9f50f27be7')\n",
        "try: run.finish()\n",
        "except NameError: pass\n",
        "run = wandb.init(project=\"ucr\", config={\"model\": \"res18\",}) # violet SeqJEPA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-eTjgAhmp_t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73d2155f-8c98-4552-b1e3-652298863bfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "classify 1.28033447265625\n",
            "classify 1.29986572265625\n",
            "classify 1.30059814453125\n",
            "classify 1.2742187976837158\n",
            "0.33840304182509506\n",
            "strain 0.11410397291183472\n",
            "strain 0.10772701352834702\n",
            "strain 0.06666778773069382\n",
            "strain 0.21075136959552765\n",
            "strain 0.05656296759843826\n",
            "classify 1.31646728515625\n",
            "classify 1.30548095703125\n",
            "classify 1.302490234375\n",
            "classify 1.2850341796875\n",
            "classify 1.125\n",
            "0.34980988593155893\n",
            "strain 0.07461908459663391\n",
            "strain 0.09637364745140076\n",
            "strain 0.22226205468177795\n",
            "strain 0.05816204100847244\n",
            "strain 0.1188746988773346\n",
            "classify 1.322265625\n",
            "classify 1.29095458984375\n",
            "classify 1.28240966796875\n",
            "classify 1.33306884765625\n",
            "classify 1.029687523841858\n",
            "0.33460076045627374\n",
            "strain 0.20772425830364227\n",
            "strain 0.05987268686294556\n",
            "strain 0.10174576193094254\n",
            "strain 0.20068980753421783\n",
            "strain 0.09089233726263046\n",
            "classify 1.3106689453125\n",
            "classify 1.2913818359375\n",
            "classify 1.28094482421875\n",
            "classify 1.32379150390625\n",
            "classify 1.296875\n",
            "0.3193916349809886\n",
            "strain 0.16272090375423431\n",
            "strain 0.11083138734102249\n",
            "strain 0.058435797691345215\n",
            "strain 0.09354744106531143\n",
            "strain 0.19428245723247528\n",
            "classify 1.2945556640625\n",
            "classify 1.27752685546875\n",
            "classify 1.29730224609375\n",
            "classify 1.352783203125\n",
            "classify 1.278906226158142\n",
            "0.33840304182509506\n",
            "strain 0.07497846335172653\n",
            "strain 0.15417347848415375\n",
            "strain 0.05061781033873558\n",
            "strain 0.18040965497493744\n",
            "strain 0.043462615460157394\n",
            "classify 1.36346435546875\n",
            "classify 1.29620361328125\n",
            "classify 1.26531982421875\n",
            "classify 1.29022216796875\n",
            "classify 1.1843750476837158\n",
            "0.33460076045627374\n",
            "strain 0.24915719032287598\n",
            "strain 0.11615565419197083\n",
            "strain 0.07040981203317642\n",
            "strain 0.08106587827205658\n",
            "strain 0.13237488269805908\n",
            "classify 1.25079345703125\n",
            "classify 1.3228759765625\n",
            "classify 1.30462646484375\n",
            "classify 1.33868408203125\n",
            "classify 1.3328125476837158\n",
            "0.3193916349809886\n",
            "strain 0.21608716249465942\n",
            "strain 0.09208939224481583\n",
            "strain 0.17227087914943695\n",
            "strain 0.07220195233821869\n",
            "strain 0.03938528895378113\n",
            "classify 1.28741455078125\n",
            "classify 1.2933349609375\n",
            "classify 1.36175537109375\n",
            "classify 1.27081298828125\n",
            "classify 1.407812476158142\n",
            "0.3231939163498099\n",
            "strain 0.2195567488670349\n",
            "strain 0.1751716136932373\n",
            "strain 0.07668141275644302\n",
            "strain 0.1114446297287941\n",
            "strain 0.025596151128411293\n",
            "classify 1.302490234375\n",
            "classify 1.28924560546875\n",
            "classify 1.29510498046875\n",
            "classify 1.32855224609375\n",
            "classify 1.3953125476837158\n",
            "0.3041825095057034\n",
            "strain 0.04991487041115761\n",
            "strain 0.16407541930675507\n",
            "strain 0.27441662549972534\n",
            "strain 0.13546474277973175\n",
            "strain 0.0755218118429184\n",
            "classify 1.28839111328125\n",
            "classify 1.302734375\n",
            "classify 1.336669921875\n",
            "classify 1.30224609375\n",
            "classify 1.181249976158142\n",
            "0.311787072243346\n",
            "strain 0.1521354615688324\n",
            "strain 0.0650603175163269\n",
            "strain 0.17625240981578827\n",
            "strain 0.21327713131904602\n",
            "strain 0.038002513349056244\n",
            "classify 1.30316162109375\n",
            "classify 1.31646728515625\n",
            "classify 1.3050537109375\n",
            "classify 1.28277587890625\n",
            "classify 1.303125023841858\n",
            "0.30038022813688214\n",
            "strain 0.1429324448108673\n",
            "strain 0.0898008644580841\n",
            "strain 0.1787128746509552\n",
            "strain 0.20625123381614685\n",
            "strain 0.05825144425034523\n",
            "classify 1.234375\n",
            "classify 1.311767578125\n",
            "classify 1.30609130859375\n",
            "classify 1.36029052734375\n",
            "classify 1.2890625\n",
            "0.2965779467680608\n",
            "strain 0.09194295853376389\n",
            "strain 0.1620277762413025\n",
            "strain 0.14767715334892273\n",
            "strain 0.07780444622039795\n",
            "strain 0.05195748060941696\n",
            "classify 1.32257080078125\n",
            "classify 1.3760986328125\n",
            "classify 1.26422119140625\n",
            "classify 1.25653076171875\n",
            "classify 1.2859375476837158\n",
            "0.30038022813688214\n",
            "strain 0.06145439296960831\n",
            "strain 0.1196645051240921\n",
            "strain 0.19136486947536469\n",
            "strain 0.10313493013381958\n",
            "strain 0.0611884631216526\n",
            "classify 1.3187255859375\n",
            "classify 1.2513427734375\n",
            "classify 1.31494140625\n",
            "classify 1.328369140625\n",
            "classify 1.3953125476837158\n",
            "0.3041825095057034\n",
            "strain 0.22686201333999634\n",
            "strain 0.0905500054359436\n",
            "strain 0.19752278923988342\n",
            "strain 0.06383061408996582\n",
            "strain 0.04955480992794037\n",
            "classify 1.33660888671875\n",
            "classify 1.383544921875\n",
            "classify 1.29638671875\n",
            "classify 1.219482421875\n",
            "classify 1.1687500476837158\n",
            "0.3155893536121673\n",
            "strain 0.07588621228933334\n",
            "strain 0.07508490234613419\n",
            "strain 0.2272973358631134\n",
            "strain 0.07697660475969315\n",
            "strain 0.09450265765190125\n",
            "classify 1.28985595703125\n",
            "classify 1.323974609375\n",
            "classify 1.3370361328125\n",
            "classify 1.26043701171875\n",
            "classify 1.3250000476837158\n",
            "0.3193916349809886\n",
            "strain 0.08517695218324661\n",
            "strain 0.09730423986911774\n",
            "strain 0.08981792628765106\n",
            "strain 0.09286408126354218\n",
            "strain 0.08542566001415253\n",
            "classify 1.32720947265625\n",
            "classify 1.30499267578125\n",
            "classify 1.2806396484375\n",
            "classify 1.33856201171875\n",
            "classify 1.0125000476837158\n",
            "0.3041825095057034\n",
            "strain 0.1283436417579651\n",
            "strain 0.06197771802544594\n",
            "strain 0.1090138629078865\n",
            "strain 0.07914981991052628\n",
            "strain 0.13921192288398743\n",
            "classify 1.30029296875\n",
            "classify 1.3499755859375\n",
            "classify 1.296142578125\n",
            "classify 1.28106689453125\n",
            "classify 1.2843749523162842\n",
            "0.33460076045627374\n",
            "strain 0.12056142091751099\n",
            "strain 0.16208021342754364\n",
            "strain 0.1424165964126587\n",
            "strain 0.11553717404603958\n",
            "strain 0.03296809270977974\n",
            "classify 1.29034423828125\n",
            "classify 1.290283203125\n",
            "classify 1.32183837890625\n",
            "classify 1.3193359375\n",
            "classify 1.3312499523162842\n",
            "0.33460076045627374\n",
            "strain 0.07697351276874542\n",
            "strain 0.09271734207868576\n",
            "strain 0.12309901416301727\n",
            "strain 0.07718182355165482\n",
            "strain 0.05939440429210663\n",
            "classify 1.40106201171875\n",
            "classify 1.2669677734375\n",
            "classify 1.296630859375\n",
            "classify 1.27191162109375\n",
            "classify 1.142968773841858\n",
            "0.33460076045627374\n",
            "strain 0.09734948724508286\n",
            "strain 0.1067308709025383\n",
            "strain 0.15349042415618896\n",
            "strain 0.23881302773952484\n",
            "strain 0.06739751994609833\n",
            "classify 1.296875\n",
            "classify 1.32958984375\n",
            "classify 1.34442138671875\n",
            "classify 1.27056884765625\n",
            "classify 1.1101562976837158\n",
            "0.33840304182509506\n",
            "strain 0.14656059443950653\n",
            "strain 0.16738483309745789\n",
            "strain 0.06987107545137405\n",
            "strain 0.08253839612007141\n",
            "strain 0.07106288522481918\n",
            "classify 1.34197998046875\n",
            "classify 1.31756591796875\n",
            "classify 1.29693603515625\n",
            "classify 1.23907470703125\n",
            "classify 1.5148437023162842\n",
            "0.33460076045627374\n",
            "strain 0.05763635039329529\n",
            "strain 0.058669254183769226\n",
            "strain 0.05958453193306923\n",
            "strain 0.12984243035316467\n",
            "strain 0.08033650368452072\n",
            "classify 1.29327392578125\n",
            "classify 1.33538818359375\n",
            "classify 1.2918701171875\n",
            "classify 1.30767822265625\n",
            "classify 1.228124976158142\n",
            "0.3155893536121673\n",
            "strain 0.11223619431257248\n",
            "strain 0.15407125651836395\n",
            "strain 0.1428496390581131\n",
            "strain 0.09998119622468948\n",
            "strain 0.2936934232711792\n",
            "classify 1.29632568359375\n",
            "classify 1.290771484375\n",
            "classify 1.3076171875\n",
            "classify 1.28656005859375\n",
            "classify 1.709375023841858\n",
            "0.3231939163498099\n",
            "strain 0.09664280712604523\n",
            "strain 0.08992314338684082\n",
            "strain 0.1457243263721466\n",
            "strain 0.1564660668373108\n",
            "strain 0.10503726452589035\n",
            "classify 1.27630615234375\n",
            "classify 1.308349609375\n",
            "classify 1.31158447265625\n",
            "classify 1.31939697265625\n",
            "classify 1.1921875476837158\n",
            "0.3155893536121673\n",
            "strain 0.15733037889003754\n",
            "strain 0.09260108321905136\n",
            "strain 0.27493250370025635\n",
            "strain 0.07071904093027115\n",
            "strain 0.06707537174224854\n",
            "classify 1.33740234375\n",
            "classify 1.30328369140625\n",
            "classify 1.2657470703125\n",
            "classify 1.3123779296875\n",
            "classify 1.1015625\n",
            "0.2889733840304182\n",
            "strain 0.08017721027135849\n",
            "strain 0.22277504205703735\n",
            "strain 0.09074582904577255\n",
            "strain 0.06478717178106308\n",
            "strain 0.12840218842029572\n",
            "classify 1.3109130859375\n",
            "classify 1.28472900390625\n",
            "classify 1.28704833984375\n",
            "classify 1.30206298828125\n",
            "classify 1.5671875476837158\n",
            "0.30798479087452474\n",
            "strain 0.17280223965644836\n",
            "strain 0.09674704074859619\n",
            "strain 0.09591599553823471\n",
            "strain 0.10244833678007126\n",
            "strain 0.041993383318185806\n",
            "classify 1.3333740234375\n",
            "classify 1.3070068359375\n",
            "classify 1.2918701171875\n",
            "classify 1.28326416015625\n",
            "classify 1.2429687976837158\n",
            "0.3041825095057034\n",
            "strain 0.14270254969596863\n",
            "strain 0.10287085920572281\n",
            "strain 0.11923129111528397\n",
            "strain 0.12216593325138092\n",
            "strain 0.31202369928359985\n",
            "classify 1.28399658203125\n",
            "classify 1.24908447265625\n",
            "classify 1.35504150390625\n",
            "classify 1.3316650390625\n",
            "classify 1.099218726158142\n",
            "0.3041825095057034\n",
            "strain 0.08732977509498596\n",
            "strain 0.08224710822105408\n",
            "strain 0.08777384459972382\n",
            "strain 0.2268754243850708\n",
            "strain 0.03207845613360405\n",
            "classify 1.282470703125\n",
            "classify 1.34173583984375\n",
            "classify 1.27508544921875\n",
            "classify 1.30047607421875\n",
            "classify 1.3507812023162842\n",
            "0.30038022813688214\n",
            "strain 0.06136268004775047\n",
            "strain 0.11297554522752762\n",
            "strain 0.19581232964992523\n",
            "strain 0.12129619717597961\n",
            "strain 0.05255140736699104\n",
            "classify 1.30584716796875\n",
            "classify 1.40380859375\n",
            "classify 1.20684814453125\n",
            "classify 1.2706298828125\n",
            "classify 1.51171875\n",
            "0.3155893536121673\n",
            "strain 0.05823354423046112\n",
            "strain 0.11407503485679626\n",
            "strain 0.13298338651657104\n",
            "strain 0.0816371813416481\n",
            "strain 0.16865675151348114\n",
            "classify 1.31378173828125\n",
            "classify 1.3380126953125\n",
            "classify 1.27337646484375\n",
            "classify 1.28741455078125\n",
            "classify 1.2101562023162842\n",
            "0.33079847908745247\n",
            "strain 0.12988220155239105\n",
            "strain 0.10178878903388977\n",
            "strain 0.06596768647432327\n",
            "strain 0.09405067563056946\n",
            "strain 0.18150921165943146\n",
            "classify 1.34844970703125\n",
            "classify 1.32867431640625\n",
            "classify 1.29803466796875\n",
            "classify 1.2388916015625\n",
            "classify 1.196874976158142\n",
            "0.3269961977186312\n",
            "strain 0.14438623189926147\n",
            "strain 0.09406236559152603\n",
            "strain 0.25413140654563904\n",
            "strain 0.08655080199241638\n",
            "strain 0.0518469475209713\n",
            "classify 1.2945556640625\n",
            "classify 1.3179931640625\n",
            "classify 1.308837890625\n",
            "classify 1.3076171875\n",
            "classify 1.0945312976837158\n",
            "0.33079847908745247\n",
            "strain 0.11117330193519592\n",
            "strain 0.13188974559307098\n",
            "strain 0.08458351343870163\n",
            "strain 0.10666920989751816\n",
            "strain 0.03373342007398605\n",
            "classify 1.32025146484375\n",
            "classify 1.296142578125\n",
            "classify 1.33526611328125\n",
            "classify 1.26806640625\n",
            "classify 1.2218749523162842\n",
            "0.3155893536121673\n",
            "strain 0.27192938327789307\n",
            "strain 0.16965241730213165\n",
            "strain 0.08299067616462708\n",
            "strain 0.20848442614078522\n",
            "strain 0.04271519556641579\n",
            "classify 1.3057861328125\n",
            "classify 1.3096923828125\n",
            "classify 1.31658935546875\n",
            "classify 1.29571533203125\n",
            "classify 1.282812476158142\n",
            "0.30038022813688214\n",
            "strain 0.09993001073598862\n",
            "strain 0.1554621160030365\n",
            "strain 0.21297584474086761\n",
            "strain 0.18541106581687927\n",
            "strain 0.034355200827121735\n",
            "classify 1.31719970703125\n",
            "classify 1.29327392578125\n",
            "classify 1.278564453125\n",
            "classify 1.33135986328125\n",
            "classify 1.28125\n",
            "0.28517110266159695\n",
            "strain 0.08843813091516495\n",
            "strain 0.07628251612186432\n",
            "strain 0.11678791046142578\n",
            "strain 0.20453815162181854\n",
            "strain 0.13737984001636505\n",
            "classify 1.2847900390625\n",
            "classify 1.32269287109375\n",
            "classify 1.3021240234375\n",
            "classify 1.29266357421875\n",
            "classify 1.384374976158142\n",
            "0.2965779467680608\n",
            "strain 0.14950309693813324\n",
            "strain 0.08956106007099152\n",
            "strain 0.0748099684715271\n",
            "strain 0.12357229739427567\n",
            "strain 0.06367798894643784\n",
            "classify 1.32586669921875\n",
            "classify 1.25030517578125\n",
            "classify 1.3529052734375\n",
            "classify 1.28179931640625\n",
            "classify 1.3703124523162842\n",
            "0.29277566539923955\n",
            "strain 0.08652859926223755\n",
            "strain 0.07696738839149475\n",
            "strain 0.18816031515598297\n",
            "strain 0.09537380933761597\n",
            "strain 0.03517516329884529\n",
            "classify 1.3094482421875\n",
            "classify 1.23150634765625\n",
            "classify 1.30810546875\n",
            "classify 1.3701171875\n",
            "classify 1.373437523841858\n",
            "0.27756653992395436\n",
            "strain 0.07868833094835281\n",
            "strain 0.20670004189014435\n",
            "strain 0.08397583663463593\n",
            "strain 0.09402354806661606\n",
            "strain 0.11568374186754227\n",
            "classify 1.317626953125\n",
            "classify 1.29327392578125\n",
            "classify 1.28302001953125\n",
            "classify 1.3253173828125\n",
            "classify 1.412500023841858\n",
            "0.30038022813688214\n",
            "strain 0.06196906790137291\n",
            "strain 0.10309430211782455\n",
            "strain 0.11246399581432343\n",
            "strain 0.05782890319824219\n",
            "strain 0.03482630103826523\n",
            "classify 1.2703857421875\n",
            "classify 1.33331298828125\n",
            "classify 1.3492431640625\n",
            "classify 1.2611083984375\n",
            "classify 1.4562499523162842\n",
            "0.30038022813688214\n",
            "strain 0.12780460715293884\n",
            "strain 0.22371625900268555\n",
            "strain 0.12790070474147797\n",
            "strain 0.11916495859622955\n",
            "strain 0.06176183000206947\n",
            "classify 1.2919921875\n",
            "classify 1.32244873046875\n",
            "classify 1.30413818359375\n",
            "classify 1.32257080078125\n",
            "classify 1.326562523841858\n",
            "0.30038022813688214\n",
            "strain 0.09923723340034485\n",
            "strain 0.14971226453781128\n",
            "strain 0.19553574919700623\n",
            "strain 0.12862837314605713\n",
            "strain 0.054672352969646454\n",
            "classify 1.31219482421875\n",
            "classify 1.329345703125\n",
            "classify 1.31768798828125\n",
            "classify 1.25689697265625\n",
            "classify 1.342187523841858\n",
            "0.2965779467680608\n",
            "strain 0.07289715111255646\n",
            "strain 0.0803903117775917\n",
            "strain 0.19313843548297882\n",
            "strain 0.0864245742559433\n",
            "strain 0.03282306715846062\n",
            "classify 1.30291748046875\n",
            "classify 1.28863525390625\n",
            "classify 1.2515869140625\n",
            "classify 1.3671875\n",
            "classify 1.2921874523162842\n",
            "0.29277566539923955\n",
            "strain 0.16901223361492157\n",
            "strain 0.19926044344902039\n",
            "strain 0.09217814356088638\n",
            "strain 0.08291489630937576\n",
            "strain 0.12553638219833374\n",
            "classify 1.3509521484375\n",
            "classify 1.3033447265625\n",
            "classify 1.29583740234375\n",
            "classify 1.2794189453125\n",
            "classify 1.310156226158142\n",
            "0.30038022813688214\n",
            "strain 0.07286514341831207\n",
            "strain 0.228977769613266\n",
            "strain 0.09360425919294357\n",
            "strain 0.09326861053705215\n",
            "strain 0.08100777864456177\n",
            "classify 1.28533935546875\n",
            "classify 1.2984619140625\n",
            "classify 1.31060791015625\n",
            "classify 1.3363037109375\n",
            "classify 1.265625\n",
            "0.29277566539923955\n",
            "strain 0.09341099858283997\n",
            "strain 0.11285753548145294\n",
            "strain 0.0850687325000763\n",
            "strain 0.07874242961406708\n",
            "strain 0.03526759892702103\n",
            "classify 1.33026123046875\n",
            "classify 1.26800537109375\n",
            "classify 1.29547119140625\n",
            "classify 1.32330322265625\n",
            "classify 1.3171875476837158\n",
            "0.2965779467680608\n",
            "strain 0.09216319024562836\n",
            "strain 0.0696268156170845\n",
            "strain 0.05228021368384361\n",
            "strain 0.11907651275396347\n",
            "strain 0.17007139325141907\n",
            "classify 1.22412109375\n",
            "classify 1.35064697265625\n",
            "classify 1.341064453125\n",
            "classify 1.3023681640625\n",
            "classify 1.3054687976837158\n",
            "0.2889733840304182\n",
            "strain 0.10770248621702194\n",
            "strain 0.11304810643196106\n",
            "strain 0.20864617824554443\n",
            "strain 0.07417131215333939\n",
            "strain 0.14718669652938843\n",
            "classify 1.2908935546875\n",
            "classify 1.295166015625\n",
            "classify 1.3046875\n",
            "classify 1.32574462890625\n",
            "classify 1.1328125\n",
            "0.33460076045627374\n",
            "strain 0.13077455759048462\n",
            "strain 0.1914873570203781\n",
            "strain 0.2133866846561432\n",
            "strain 0.08079086244106293\n",
            "strain 0.05094523727893829\n",
            "classify 1.30279541015625\n",
            "classify 1.2823486328125\n",
            "classify 1.3388671875\n",
            "classify 1.2918701171875\n",
            "classify 1.3078124523162842\n",
            "0.34600760456273766\n",
            "strain 0.08375030010938644\n",
            "strain 0.12098123878240585\n",
            "strain 0.2293339967727661\n",
            "strain 0.13922196626663208\n",
            "strain 0.11872407048940659\n",
            "classify 1.32427978515625\n",
            "classify 1.314208984375\n",
            "classify 1.24444580078125\n",
            "classify 1.3668212890625\n",
            "classify 1.193750023841858\n",
            "0.34600760456273766\n",
            "strain 0.061240870505571365\n",
            "strain 0.0727047249674797\n",
            "strain 0.18848878145217896\n",
            "strain 0.14025923609733582\n",
            "strain 0.1290796399116516\n",
            "classify 1.32373046875\n",
            "classify 1.27606201171875\n",
            "classify 1.30169677734375\n",
            "classify 1.3099365234375\n",
            "classify 1.46875\n",
            "0.3574144486692015\n",
            "strain 0.14069172739982605\n",
            "strain 0.07908879220485687\n",
            "strain 0.10742218047380447\n",
            "strain 0.11729922890663147\n",
            "strain 0.03395313769578934\n",
            "classify 1.3028564453125\n",
            "classify 1.30682373046875\n",
            "classify 1.34454345703125\n",
            "classify 1.28704833984375\n",
            "classify 1.173437476158142\n",
            "0.3269961977186312\n",
            "strain 0.15808166563510895\n",
            "strain 0.18159545958042145\n",
            "strain 0.11386698484420776\n",
            "strain 0.09915848076343536\n",
            "strain 0.06337684392929077\n",
            "classify 1.34320068359375\n",
            "classify 1.26605224609375\n",
            "classify 1.30035400390625\n",
            "classify 1.32171630859375\n",
            "classify 1.1398437023162842\n",
            "0.34980988593155893\n",
            "strain 0.08225435018539429\n",
            "strain 0.11554206162691116\n",
            "strain 0.10838170349597931\n",
            "strain 0.05923934653401375\n",
            "strain 0.06370140612125397\n",
            "classify 1.33746337890625\n",
            "classify 1.2908935546875\n",
            "classify 1.2911376953125\n",
            "classify 1.30218505859375\n",
            "classify 1.149999976158142\n",
            "0.3193916349809886\n",
            "strain 0.07855533808469772\n",
            "strain 0.2708060145378113\n",
            "strain 0.15814140439033508\n",
            "strain 0.10836660861968994\n",
            "strain 0.043126944452524185\n",
            "classify 1.34375\n",
            "classify 1.29168701171875\n",
            "classify 1.30767822265625\n",
            "classify 1.293701171875\n",
            "classify 1.174218773841858\n",
            "0.2889733840304182\n",
            "strain 0.20893840491771698\n",
            "strain 0.11158500611782074\n",
            "strain 0.07540037482976913\n",
            "strain 0.23836468160152435\n",
            "strain 0.046767011284828186\n",
            "classify 1.313720703125\n",
            "classify 1.326416015625\n",
            "classify 1.3111572265625\n",
            "classify 1.2720947265625\n",
            "classify 1.20703125\n",
            "0.2889733840304182\n",
            "strain 0.17953336238861084\n",
            "strain 0.1572236567735672\n",
            "strain 0.12223125249147415\n",
            "strain 0.09387709200382233\n",
            "strain 0.20046621561050415\n",
            "classify 1.29571533203125\n",
            "classify 1.285888671875\n",
            "classify 1.3096923828125\n",
            "classify 1.317626953125\n",
            "classify 1.275781273841858\n",
            "0.2889733840304182\n",
            "strain 0.07627208530902863\n",
            "strain 0.0967566967010498\n",
            "strain 0.09889806807041168\n",
            "strain 0.0981326475739479\n",
            "strain 0.12054755538702011\n",
            "classify 1.30780029296875\n",
            "classify 1.3153076171875\n",
            "classify 1.292724609375\n",
            "classify 1.30633544921875\n",
            "classify 1.0968749523162842\n",
            "0.311787072243346\n",
            "strain 0.08467001467943192\n",
            "strain 0.07488884776830673\n",
            "strain 0.11156902462244034\n",
            "strain 0.08609648048877716\n",
            "strain 0.049672931432724\n",
            "classify 1.35723876953125\n",
            "classify 1.28082275390625\n",
            "classify 1.22186279296875\n",
            "classify 1.33074951171875\n",
            "classify 1.4609375\n",
            "0.2965779467680608\n",
            "strain 0.09819499403238297\n",
            "strain 0.22524897754192352\n",
            "strain 0.18700388073921204\n",
            "strain 0.19596436619758606\n",
            "strain 0.053708478808403015\n",
            "classify 1.32049560546875\n",
            "classify 1.2801513671875\n",
            "classify 1.2711181640625\n",
            "classify 1.32574462890625\n",
            "classify 1.3156249523162842\n",
            "0.30038022813688214\n",
            "strain 0.12646667659282684\n",
            "strain 0.08761957287788391\n",
            "strain 0.08449340611696243\n",
            "strain 0.08902668207883835\n",
            "strain 0.05197542533278465\n",
            "classify 1.29693603515625\n",
            "classify 1.276611328125\n",
            "classify 1.283935546875\n",
            "classify 1.33917236328125\n",
            "classify 1.279687523841858\n",
            "0.30798479087452474\n",
            "strain 0.22489769756793976\n",
            "strain 0.184147909283638\n",
            "strain 0.1706826388835907\n",
            "strain 0.11247562617063522\n",
            "strain 0.044206470251083374\n",
            "classify 1.31622314453125\n",
            "classify 1.19891357421875\n",
            "classify 1.36578369140625\n",
            "classify 1.30792236328125\n",
            "classify 1.295312523841858\n",
            "0.2889733840304182\n",
            "strain 0.11667325347661972\n",
            "strain 0.14188523590564728\n",
            "strain 0.1476878672838211\n",
            "strain 0.20015831291675568\n",
            "strain 0.04820679873228073\n",
            "classify 1.37884521484375\n",
            "classify 1.26513671875\n",
            "classify 1.30010986328125\n",
            "classify 1.25494384765625\n",
            "classify 1.21875\n",
            "0.2965779467680608\n",
            "strain 0.09511866420507431\n",
            "strain 0.20726916193962097\n",
            "strain 0.10929056257009506\n",
            "strain 0.09578681737184525\n",
            "strain 0.03965814784169197\n",
            "classify 1.2882080078125\n",
            "classify 1.2608642578125\n",
            "classify 1.32855224609375\n",
            "classify 1.296875\n",
            "classify 1.478124976158142\n",
            "0.3041825095057034\n",
            "strain 0.11360164731740952\n",
            "strain 0.15663503110408783\n",
            "strain 0.23853985965251923\n",
            "strain 0.24880822002887726\n",
            "strain 0.16191375255584717\n",
            "classify 1.33544921875\n",
            "classify 1.27752685546875\n",
            "classify 1.28094482421875\n",
            "classify 1.29656982421875\n",
            "classify 1.3984375\n",
            "0.2813688212927757\n",
            "strain 0.10236328840255737\n",
            "strain 0.09427657723426819\n",
            "strain 0.2279038280248642\n",
            "strain 0.15603812038898468\n",
            "strain 0.07104447484016418\n",
            "classify 1.3070068359375\n",
            "classify 1.29412841796875\n",
            "classify 1.28314208984375\n",
            "classify 1.3310546875\n",
            "classify 1.421875\n",
            "0.3041825095057034\n",
            "strain 0.14794112741947174\n",
            "strain 0.09851198643445969\n",
            "strain 0.15863117575645447\n",
            "strain 0.07364732027053833\n",
            "strain 0.08636283874511719\n",
            "classify 1.34442138671875\n",
            "classify 1.32684326171875\n",
            "classify 1.277099609375\n",
            "classify 1.28167724609375\n",
            "classify 1.345312476158142\n",
            "0.311787072243346\n",
            "strain 0.09639354050159454\n",
            "strain 0.08315815776586533\n",
            "strain 0.10071203112602234\n",
            "strain 0.1114182099699974\n",
            "strain 0.036058709025382996\n",
            "classify 1.2958984375\n",
            "classify 1.3114013671875\n",
            "classify 1.26324462890625\n",
            "classify 1.3480224609375\n",
            "classify 1.279687523841858\n",
            "0.2813688212927757\n",
            "strain 0.11607956886291504\n",
            "strain 0.07215294241905212\n",
            "strain 0.10109864175319672\n",
            "strain 0.08794301748275757\n",
            "strain 0.045805420726537704\n",
            "classify 1.25738525390625\n",
            "classify 1.3013916015625\n",
            "classify 1.33526611328125\n",
            "classify 1.29669189453125\n",
            "classify 1.631250023841858\n",
            "0.2889733840304182\n",
            "strain 0.09664570540189743\n",
            "strain 0.07958487421274185\n",
            "strain 0.13328604400157928\n",
            "strain 0.16391055285930634\n",
            "strain 0.10246758162975311\n",
            "classify 1.2716064453125\n",
            "classify 1.30535888671875\n",
            "classify 1.331787109375\n",
            "classify 1.29510498046875\n",
            "classify 1.173437476158142\n",
            "0.2965779467680608\n",
            "strain 0.08551207184791565\n",
            "strain 0.09662165492773056\n",
            "strain 0.07151234149932861\n",
            "strain 0.07448934018611908\n",
            "strain 0.06687535345554352\n",
            "classify 1.2666015625\n",
            "classify 1.27423095703125\n",
            "classify 1.2711181640625\n",
            "classify 1.3919677734375\n",
            "classify 1.225000023841858\n",
            "0.2813688212927757\n",
            "strain 0.11876112222671509\n",
            "strain 0.10195247828960419\n",
            "strain 0.15318326652050018\n",
            "strain 0.1910676509141922\n",
            "strain 0.07750757038593292\n",
            "classify 1.3192138671875\n",
            "classify 1.327880859375\n",
            "classify 1.23651123046875\n",
            "classify 1.2996826171875\n",
            "classify 1.4500000476837158\n",
            "0.2737642585551331\n",
            "strain 0.26455429196357727\n",
            "strain 0.05273562669754028\n",
            "strain 0.05207847058773041\n",
            "strain 0.10904152691364288\n",
            "strain 0.03746133670210838\n",
            "classify 1.27850341796875\n",
            "classify 1.2578125\n",
            "classify 1.31964111328125\n",
            "classify 1.3203125\n",
            "classify 1.453125\n",
            "0.29277566539923955\n",
            "strain 0.1273561269044876\n",
            "strain 0.07110775262117386\n",
            "strain 0.2061229944229126\n",
            "strain 0.07059930264949799\n",
            "strain 0.046912241727113724\n",
            "classify 1.32940673828125\n",
            "classify 1.318603515625\n",
            "classify 1.23345947265625\n",
            "classify 1.30523681640625\n",
            "classify 1.3875000476837158\n",
            "0.2813688212927757\n",
            "strain 0.07091134041547775\n",
            "strain 0.1718897819519043\n",
            "strain 0.16340503096580505\n",
            "strain 0.10460128635168076\n",
            "strain 0.0274328775703907\n",
            "classify 1.2801513671875\n",
            "classify 1.31854248046875\n",
            "classify 1.30572509765625\n",
            "classify 1.2998046875\n",
            "classify 1.310937523841858\n",
            "0.2813688212927757\n",
            "strain 0.18967682123184204\n",
            "strain 0.12079194188117981\n",
            "strain 0.08434927463531494\n",
            "strain 0.0990104079246521\n",
            "strain 0.0657808855175972\n",
            "classify 1.3421630859375\n",
            "classify 1.30010986328125\n",
            "classify 1.2802734375\n",
            "classify 1.27984619140625\n",
            "classify 1.5710937976837158\n",
            "0.2889733840304182\n",
            "strain 0.189130961894989\n",
            "strain 0.12353802472352982\n",
            "strain 0.07766321301460266\n",
            "strain 0.10500694066286087\n",
            "strain 0.10152933746576309\n",
            "classify 1.31103515625\n",
            "classify 1.286376953125\n",
            "classify 1.3294677734375\n",
            "classify 1.27911376953125\n",
            "classify 1.4500000476837158\n",
            "0.29277566539923955\n",
            "strain 0.06276392936706543\n",
            "strain 0.1804942786693573\n",
            "strain 0.06293433904647827\n",
            "strain 0.06371961534023285\n",
            "strain 0.051268402487039566\n",
            "classify 1.3026123046875\n",
            "classify 1.35467529296875\n",
            "classify 1.25567626953125\n",
            "classify 1.32525634765625\n",
            "classify 1.1359374523162842\n",
            "0.28517110266159695\n",
            "strain 0.1525217592716217\n",
            "strain 0.10057523846626282\n",
            "strain 0.17179974913597107\n",
            "strain 0.053065136075019836\n",
            "strain 0.05118503421545029\n",
            "classify 1.32965087890625\n",
            "classify 1.2635498046875\n",
            "classify 1.28277587890625\n",
            "classify 1.29931640625\n",
            "classify 1.6609375476837158\n",
            "0.30798479087452474\n",
            "strain 0.08292960375547409\n",
            "strain 0.19029657542705536\n",
            "strain 0.08325596898794174\n",
            "strain 0.13351987302303314\n",
            "strain 0.04004548862576485\n",
            "classify 1.29052734375\n",
            "classify 1.2662353515625\n",
            "classify 1.3489990234375\n",
            "classify 1.30572509765625\n",
            "classify 1.384374976158142\n",
            "0.2965779467680608\n",
            "strain 0.1590157002210617\n",
            "strain 0.1265983134508133\n",
            "strain 0.12157316505908966\n",
            "strain 0.12640367448329926\n",
            "strain 0.04942391812801361\n",
            "classify 1.2823486328125\n",
            "classify 1.296142578125\n",
            "classify 1.275390625\n",
            "classify 1.33367919921875\n",
            "classify 1.3718750476837158\n",
            "0.28517110266159695\n",
            "strain 0.11074116080999374\n",
            "strain 0.0987374559044838\n",
            "strain 0.11299514770507812\n",
            "strain 0.07620915025472641\n",
            "strain 0.08306436240673065\n",
            "classify 1.33428955078125\n",
            "classify 1.3072509765625\n",
            "classify 1.24456787109375\n",
            "classify 1.30755615234375\n",
            "classify 1.298437476158142\n",
            "0.3041825095057034\n",
            "strain 0.1343250572681427\n",
            "strain 0.09033548831939697\n",
            "strain 0.15112553536891937\n",
            "strain 0.15383628010749817\n",
            "strain 0.0872902199625969\n",
            "classify 1.28460693359375\n",
            "classify 1.26263427734375\n",
            "classify 1.3509521484375\n",
            "classify 1.28887939453125\n",
            "classify 1.3390624523162842\n",
            "0.311787072243346\n",
            "strain 0.08008549362421036\n",
            "strain 0.13731253147125244\n",
            "strain 0.20300868153572083\n",
            "strain 0.13011367619037628\n",
            "strain 0.1471329778432846\n",
            "classify 1.2886962890625\n",
            "classify 1.33538818359375\n",
            "classify 1.300537109375\n",
            "classify 1.2734375\n",
            "classify 1.3796875476837158\n",
            "0.26996197718631176\n",
            "strain 0.07794377952814102\n",
            "strain 0.08882010728120804\n",
            "strain 0.1239856407046318\n",
            "strain 0.14865213632583618\n",
            "strain 0.05643383041024208\n",
            "classify 1.3218994140625\n",
            "classify 1.2845458984375\n",
            "classify 1.309814453125\n",
            "classify 1.28460693359375\n",
            "classify 1.221093773841858\n",
            "0.29277566539923955\n",
            "strain 0.12644316256046295\n",
            "strain 0.09954172372817993\n",
            "strain 0.11610614508390427\n",
            "strain 0.1665802001953125\n",
            "strain 0.06815417855978012\n",
            "classify 1.29400634765625\n",
            "classify 1.338623046875\n",
            "classify 1.34442138671875\n",
            "classify 1.2459716796875\n",
            "classify 1.1007812023162842\n",
            "0.311787072243346\n",
            "strain 0.06904572248458862\n",
            "strain 0.26247650384902954\n",
            "strain 0.09469617158174515\n",
            "strain 0.08428914099931717\n",
            "strain 0.031638357788324356\n",
            "classify 1.32769775390625\n",
            "classify 1.28857421875\n",
            "classify 1.28729248046875\n",
            "classify 1.3155517578125\n",
            "classify 1.310937523841858\n",
            "0.2965779467680608\n",
            "strain 0.09348630905151367\n",
            "strain 0.14899098873138428\n",
            "strain 0.0788477435708046\n",
            "strain 0.08503001928329468\n",
            "strain 0.07348936051130295\n",
            "classify 1.327392578125\n",
            "classify 1.2789306640625\n",
            "classify 1.321044921875\n",
            "classify 1.31201171875\n",
            "classify 1.1375000476837158\n",
            "0.29277566539923955\n",
            "strain 0.08543641865253448\n",
            "strain 0.18549934029579163\n",
            "strain 0.13830886781215668\n",
            "strain 0.07123734802007675\n",
            "strain 0.0973958969116211\n",
            "classify 1.393798828125\n",
            "classify 1.239501953125\n",
            "classify 1.27801513671875\n",
            "classify 1.3143310546875\n",
            "classify 1.174218773841858\n",
            "0.2813688212927757\n",
            "strain 0.25662344694137573\n",
            "strain 0.10972043126821518\n",
            "strain 0.08889030665159225\n",
            "strain 0.047797899693250656\n",
            "strain 0.03595919534564018\n",
            "classify 1.30230712890625\n",
            "classify 1.31781005859375\n",
            "classify 1.2735595703125\n",
            "classify 1.3140869140625\n",
            "classify 1.291406273841858\n",
            "0.30798479087452474\n",
            "strain 0.11617007851600647\n",
            "strain 0.15462768077850342\n",
            "strain 0.06546508520841599\n",
            "strain 0.20784993469715118\n",
            "strain 0.0666273757815361\n",
            "classify 1.3521728515625\n",
            "classify 1.273193359375\n",
            "classify 1.29144287109375\n",
            "classify 1.3072509765625\n",
            "classify 1.1796875\n",
            "0.3155893536121673\n",
            "strain 0.10991024971008301\n",
            "strain 0.14297893643379211\n",
            "strain 0.1042531430721283\n",
            "strain 0.06406816840171814\n",
            "strain 0.05087752640247345\n",
            "classify 1.28668212890625\n",
            "classify 1.282958984375\n",
            "classify 1.27825927734375\n",
            "classify 1.3199462890625\n",
            "classify 1.7937500476837158\n",
            "0.30038022813688214\n",
            "strain 0.16684603691101074\n",
            "strain 0.16180497407913208\n",
            "strain 0.12601876258850098\n",
            "strain 0.07981845736503601\n",
            "strain 0.09519597142934799\n",
            "classify 1.32464599609375\n",
            "classify 1.26788330078125\n",
            "classify 1.29541015625\n",
            "classify 1.30743408203125\n",
            "classify 1.4296875\n",
            "0.2661596958174905\n",
            "strain 0.07665050774812698\n",
            "strain 0.09362642467021942\n",
            "strain 0.13470999896526337\n",
            "strain 0.19474352896213531\n",
            "strain 0.0688154473900795\n",
            "classify 1.29998779296875\n",
            "classify 1.3092041015625\n",
            "classify 1.2940673828125\n",
            "classify 1.30816650390625\n",
            "classify 1.1789062023162842\n",
            "0.2965779467680608\n",
            "strain 0.210023432970047\n",
            "strain 0.07253866642713547\n",
            "strain 0.21991226077079773\n",
            "strain 0.10681252181529999\n",
            "strain 0.1643192172050476\n",
            "classify 1.28179931640625\n",
            "classify 1.28741455078125\n",
            "classify 1.31341552734375\n",
            "classify 1.31658935546875\n",
            "classify 1.510156273841858\n",
            "0.3155893536121673\n",
            "strain 0.22573137283325195\n",
            "strain 0.19893613457679749\n",
            "strain 0.07980718463659286\n",
            "strain 0.11531466245651245\n",
            "strain 0.036132071167230606\n",
            "classify 1.26422119140625\n",
            "classify 1.32989501953125\n",
            "classify 1.29266357421875\n",
            "classify 1.3331298828125\n",
            "classify 1.197656273841858\n",
            "0.3155893536121673\n",
            "strain 0.06384071707725525\n",
            "strain 0.1151047796010971\n",
            "strain 0.05791422352194786\n",
            "strain 0.13837340474128723\n",
            "strain 0.03517810255289078\n",
            "classify 1.37701416015625\n",
            "classify 1.233642578125\n",
            "classify 1.2928466796875\n",
            "classify 1.290771484375\n",
            "classify 1.389062523841858\n",
            "0.3155893536121673\n",
            "strain 0.07822759449481964\n",
            "strain 0.1144145056605339\n",
            "strain 0.18065699934959412\n",
            "strain 0.14406035840511322\n",
            "strain 0.08590736240148544\n",
            "classify 1.2840576171875\n",
            "classify 1.26190185546875\n",
            "classify 1.3529052734375\n",
            "classify 1.29620361328125\n",
            "classify 1.295312523841858\n",
            "0.30038022813688214\n",
            "strain 0.0999397337436676\n",
            "strain 0.07564085721969604\n",
            "strain 0.07442038506269455\n",
            "strain 0.1505589783191681\n",
            "strain 0.11550939083099365\n",
            "classify 1.32196044921875\n",
            "classify 1.364013671875\n",
            "classify 1.24365234375\n",
            "classify 1.28948974609375\n",
            "classify 1.2273437976837158\n",
            "0.311787072243346\n",
            "strain 0.15255731344223022\n",
            "strain 0.10089831799268723\n",
            "strain 0.0824967473745346\n",
            "strain 0.09550855308771133\n",
            "strain 0.06151467189192772\n",
            "classify 1.27899169921875\n",
            "classify 1.27703857421875\n",
            "classify 1.31024169921875\n",
            "classify 1.369384765625\n",
            "classify 1.2921874523162842\n",
            "0.311787072243346\n",
            "strain 0.1674959361553192\n",
            "strain 0.07481783628463745\n",
            "strain 0.08987519145011902\n",
            "strain 0.1799912452697754\n",
            "strain 0.1933506429195404\n",
            "classify 1.34576416015625\n",
            "classify 1.28826904296875\n",
            "classify 1.28704833984375\n",
            "classify 1.2862548828125\n",
            "classify 1.3203125\n",
            "0.3231939163498099\n",
            "strain 0.06575258076190948\n",
            "strain 0.10081759840250015\n",
            "strain 0.21407483518123627\n",
            "strain 0.15902478992938995\n",
            "strain 0.07078339159488678\n",
            "classify 1.30462646484375\n",
            "classify 1.28668212890625\n",
            "classify 1.33721923828125\n",
            "classify 1.2684326171875\n",
            "classify 1.5906250476837158\n",
            "0.29277566539923955\n",
            "strain 0.18347008526325226\n",
            "strain 0.23273728787899017\n",
            "strain 0.1993517130613327\n",
            "strain 0.08906611800193787\n",
            "strain 0.1543767750263214\n",
            "classify 1.28436279296875\n",
            "classify 1.3040771484375\n",
            "classify 1.31585693359375\n",
            "classify 1.304931640625\n",
            "classify 1.376562476158142\n",
            "0.3231939163498099\n",
            "strain 0.2182411253452301\n",
            "strain 0.11576347798109055\n",
            "strain 0.10555172711610794\n",
            "strain 0.11814115941524506\n",
            "strain 0.045992106199264526\n",
            "classify 1.3350830078125\n",
            "classify 1.29156494140625\n",
            "classify 1.22747802734375\n",
            "classify 1.3580322265625\n",
            "classify 1.451562523841858\n",
            "0.33840304182509506\n",
            "strain 0.13029924035072327\n",
            "strain 0.07089530676603317\n",
            "strain 0.0940667986869812\n",
            "strain 0.16522257030010223\n",
            "strain 0.061137739568948746\n",
            "classify 1.3209228515625\n",
            "classify 1.33148193359375\n",
            "classify 1.26324462890625\n",
            "classify 1.32659912109375\n",
            "classify 1.1015625\n",
            "0.34980988593155893\n",
            "strain 0.1358833909034729\n",
            "strain 0.18045802414417267\n",
            "strain 0.1873929500579834\n",
            "strain 0.10962317883968353\n",
            "strain 0.11850009858608246\n",
            "classify 1.31768798828125\n",
            "classify 1.24456787109375\n",
            "classify 1.32110595703125\n",
            "classify 1.3463134765625\n",
            "classify 1.3078124523162842\n",
            "0.3269961977186312\n",
            "strain 0.07223182171583176\n",
            "strain 0.08236841857433319\n",
            "strain 0.06805524230003357\n",
            "strain 0.14917179942131042\n",
            "strain 0.10889102518558502\n",
            "classify 1.287109375\n",
            "classify 1.32391357421875\n",
            "classify 1.2923583984375\n",
            "classify 1.29937744140625\n",
            "classify 1.3781249523162842\n",
            "0.30798479087452474\n",
            "strain 0.09287011623382568\n",
            "strain 0.11596352607011795\n",
            "strain 0.1278277188539505\n",
            "strain 0.05572377145290375\n",
            "strain 0.06479107588529587\n",
            "classify 1.36181640625\n",
            "classify 1.3023681640625\n",
            "classify 1.32147216796875\n",
            "classify 1.2315673828125\n",
            "classify 1.2218749523162842\n",
            "0.3193916349809886\n",
            "strain 0.18371433019638062\n",
            "strain 0.17261064052581787\n",
            "strain 0.09057129174470901\n",
            "strain 0.12273312360048294\n",
            "strain 0.04195893555879593\n",
            "classify 1.2825927734375\n",
            "classify 1.2891845703125\n",
            "classify 1.34014892578125\n",
            "classify 1.3184814453125\n",
            "classify 1.16015625\n",
            "0.34600760456273766\n",
            "strain 0.13659994304180145\n",
            "strain 0.155496746301651\n",
            "strain 0.1035919189453125\n",
            "strain 0.08159618824720383\n",
            "strain 0.07449360191822052\n",
            "classify 1.36846923828125\n",
            "classify 1.2589111328125\n",
            "classify 1.315185546875\n",
            "classify 1.28057861328125\n",
            "classify 1.2453124523162842\n",
            "0.33079847908745247\n",
            "strain 0.12599439918994904\n",
            "strain 0.10784266144037247\n",
            "strain 0.10158193111419678\n",
            "strain 0.07864225655794144\n",
            "strain 0.05695358291268349\n",
            "classify 1.34796142578125\n",
            "classify 1.29669189453125\n",
            "classify 1.3233642578125\n",
            "classify 1.25567626953125\n",
            "classify 1.3468749523162842\n",
            "0.33079847908745247\n",
            "strain 0.18816563487052917\n",
            "strain 0.1877233237028122\n",
            "strain 0.08876187354326248\n",
            "strain 0.11512669175863266\n",
            "strain 0.05021850764751434\n",
            "classify 1.3115234375\n",
            "classify 1.35711669921875\n",
            "classify 1.26666259765625\n",
            "classify 1.2740478515625\n",
            "classify 1.3820312023162842\n",
            "0.3231939163498099\n",
            "strain 0.10466522723436356\n",
            "strain 0.1020725890994072\n",
            "strain 0.1555936634540558\n",
            "strain 0.07038337737321854\n",
            "strain 0.050004906952381134\n",
            "classify 1.3065185546875\n",
            "classify 1.3157958984375\n",
            "classify 1.28143310546875\n",
            "classify 1.321533203125\n",
            "classify 1.1359374523162842\n",
            "0.3155893536121673\n",
            "strain 0.1721818894147873\n",
            "strain 0.0887480229139328\n",
            "strain 0.09936980158090591\n",
            "strain 0.08818689733743668\n",
            "strain 0.04032270610332489\n",
            "classify 1.32745361328125\n",
            "classify 1.2799072265625\n",
            "classify 1.30621337890625\n",
            "classify 1.308837890625\n",
            "classify 1.170312523841858\n",
            "0.3155893536121673\n",
            "strain 0.10464189201593399\n",
            "strain 0.113898865878582\n",
            "strain 0.08876701444387436\n",
            "strain 0.08377689123153687\n",
            "strain 0.06988871097564697\n",
            "classify 1.30535888671875\n",
            "classify 1.27191162109375\n",
            "classify 1.3236083984375\n",
            "classify 1.31878662109375\n",
            "classify 1.235937476158142\n",
            "0.3041825095057034\n",
            "strain 0.0916995257139206\n",
            "strain 0.16851367056369781\n",
            "strain 0.0940614715218544\n",
            "strain 0.1402299851179123\n",
            "strain 0.05360710993409157\n",
            "classify 1.337158203125\n",
            "classify 1.31866455078125\n",
            "classify 1.22027587890625\n",
            "classify 1.313232421875\n",
            "classify 1.482812523841858\n",
            "0.29277566539923955\n",
            "strain 0.1457335501909256\n",
            "strain 0.09038958698511124\n",
            "strain 0.06282064318656921\n",
            "strain 0.0862341895699501\n",
            "strain 0.05129843205213547\n",
            "classify 1.293212890625\n",
            "classify 1.27178955078125\n",
            "classify 1.29656982421875\n",
            "classify 1.29962158203125\n",
            "classify 1.881250023841858\n",
            "0.3041825095057034\n",
            "strain 0.10231535881757736\n",
            "strain 0.15937624871730804\n",
            "strain 0.06245691329240799\n",
            "strain 0.14194005727767944\n",
            "strain 0.0775880515575409\n",
            "classify 1.31005859375\n",
            "classify 1.3377685546875\n",
            "classify 1.24420166015625\n",
            "classify 1.3184814453125\n",
            "classify 1.322656273841858\n",
            "0.34600760456273766\n",
            "strain 0.10832188278436661\n",
            "strain 0.09731588512659073\n",
            "strain 0.140111044049263\n",
            "strain 0.12078571319580078\n",
            "strain 0.05881849676370621\n",
            "classify 1.33697509765625\n",
            "classify 1.3211669921875\n",
            "classify 1.2735595703125\n",
            "classify 1.282470703125\n",
            "classify 1.1593749523162842\n",
            "0.3269961977186312\n",
            "strain 0.09460975229740143\n",
            "strain 0.10702331364154816\n",
            "strain 0.07363109290599823\n",
            "strain 0.08120544999837875\n",
            "strain 0.03144782409071922\n",
            "classify 1.26312255859375\n",
            "classify 1.3585205078125\n",
            "classify 1.28173828125\n",
            "classify 1.31195068359375\n",
            "classify 1.248437523841858\n",
            "0.3155893536121673\n",
            "strain 0.10547176748514175\n",
            "strain 0.11412473022937775\n",
            "strain 0.09502558410167694\n",
            "strain 0.1375521570444107\n",
            "strain 0.05539197847247124\n",
            "classify 1.2958984375\n",
            "classify 1.32427978515625\n",
            "classify 1.32159423828125\n",
            "classify 1.28369140625\n",
            "classify 1.14453125\n",
            "0.2965779467680608\n",
            "strain 0.07298976182937622\n",
            "strain 0.10676120966672897\n",
            "strain 0.15636380016803741\n",
            "strain 0.06409207731485367\n",
            "strain 0.12136434763669968\n",
            "classify 1.25634765625\n",
            "classify 1.326171875\n",
            "classify 1.31671142578125\n",
            "classify 1.31195068359375\n",
            "classify 1.192968726158142\n",
            "0.311787072243346\n",
            "strain 0.17787620425224304\n",
            "strain 0.13060986995697021\n",
            "strain 0.10851749032735825\n",
            "strain 0.2138923853635788\n",
            "strain 0.36711767315864563\n",
            "classify 1.296142578125\n",
            "classify 1.3253173828125\n",
            "classify 1.29315185546875\n",
            "classify 1.27532958984375\n",
            "classify 1.342187523841858\n",
            "0.3269961977186312\n",
            "strain 0.14663103222846985\n",
            "strain 0.08397841453552246\n",
            "strain 0.09823251515626907\n",
            "strain 0.11991477757692337\n",
            "strain 0.03509993478655815\n",
            "classify 1.32623291015625\n",
            "classify 1.295654296875\n",
            "classify 1.28070068359375\n",
            "classify 1.289306640625\n",
            "classify 1.3984375\n",
            "0.33840304182509506\n",
            "strain 0.09879714250564575\n",
            "strain 0.12011189758777618\n",
            "strain 0.05798383802175522\n",
            "strain 0.09454623609781265\n",
            "strain 0.09142439812421799\n",
            "classify 1.33245849609375\n",
            "classify 1.358642578125\n",
            "classify 1.23333740234375\n",
            "classify 1.28558349609375\n",
            "classify 1.225000023841858\n",
            "0.34220532319391633\n",
            "strain 0.11453180015087128\n",
            "strain 0.12253932654857635\n",
            "strain 0.13370664417743683\n",
            "strain 0.05021129548549652\n",
            "strain 0.03260210528969765\n",
            "classify 1.2867431640625\n",
            "classify 1.2890625\n",
            "classify 1.3360595703125\n",
            "classify 1.28546142578125\n",
            "classify 1.4148437976837158\n",
            "0.35361216730038025\n",
            "strain 0.09676134586334229\n",
            "strain 0.05749104544520378\n",
            "strain 0.10306005924940109\n",
            "strain 0.09713151305913925\n",
            "strain 0.03483843058347702\n",
            "classify 1.30126953125\n",
            "classify 1.3389892578125\n",
            "classify 1.2667236328125\n",
            "classify 1.29766845703125\n",
            "classify 1.318750023841858\n",
            "0.3650190114068441\n",
            "strain 0.1784813553094864\n",
            "strain 0.21527937054634094\n",
            "strain 0.09798350930213928\n",
            "strain 0.13834832608699799\n",
            "strain 0.03778102248907089\n",
            "classify 1.30255126953125\n",
            "classify 1.305908203125\n",
            "classify 1.31890869140625\n",
            "classify 1.275146484375\n",
            "classify 1.3484375476837158\n",
            "0.34220532319391633\n",
            "strain 0.07157257944345474\n",
            "strain 0.07462058216333389\n",
            "strain 0.07543157786130905\n",
            "strain 0.08893810212612152\n",
            "strain 0.1845155954360962\n",
            "classify 1.31988525390625\n",
            "classify 1.2689208984375\n",
            "classify 1.34478759765625\n",
            "classify 1.27935791015625\n",
            "classify 1.306249976158142\n",
            "0.30798479087452474\n",
            "strain 0.07019929587841034\n",
            "strain 0.17195449769496918\n",
            "strain 0.16054405272006989\n",
            "strain 0.19493521749973297\n",
            "strain 0.07914441078901291\n",
            "classify 1.2628173828125\n",
            "classify 1.2855224609375\n",
            "classify 1.28070068359375\n",
            "classify 1.36810302734375\n",
            "classify 1.3859374523162842\n",
            "0.2965779467680608\n",
            "strain 0.08779085427522659\n",
            "strain 0.10302411764860153\n",
            "strain 0.19235169887542725\n",
            "strain 0.07917966693639755\n",
            "strain 0.039996299892663956\n",
            "classify 1.34271240234375\n",
            "classify 1.3333740234375\n",
            "classify 1.27520751953125\n",
            "classify 1.25921630859375\n",
            "classify 1.25\n",
            "0.311787072243346\n",
            "strain 0.21830151975154877\n",
            "strain 0.13069027662277222\n",
            "strain 0.14448034763336182\n",
            "strain 0.19061052799224854\n",
            "strain 0.07242249697446823\n",
            "classify 1.32879638671875\n",
            "classify 1.28436279296875\n",
            "classify 1.30291748046875\n",
            "classify 1.29449462890625\n",
            "classify 1.235937476158142\n",
            "0.30038022813688214\n",
            "strain 0.05707233399152756\n",
            "strain 0.08458887040615082\n",
            "strain 0.12723606824874878\n",
            "strain 0.10813357681035995\n",
            "strain 0.07982894778251648\n",
            "classify 1.28460693359375\n",
            "classify 1.3216552734375\n",
            "classify 1.30633544921875\n",
            "classify 1.282470703125\n",
            "classify 1.360937476158142\n",
            "0.3193916349809886\n",
            "strain 0.18901851773262024\n",
            "strain 0.07727430760860443\n",
            "strain 0.1489492803812027\n",
            "strain 0.15500791370868683\n",
            "strain 0.04748052731156349\n",
            "classify 1.26507568359375\n",
            "classify 1.31671142578125\n",
            "classify 1.30975341796875\n",
            "classify 1.32879638671875\n",
            "classify 1.248437523841858\n",
            "0.3231939163498099\n",
            "strain 0.09214320778846741\n",
            "strain 0.2438034564256668\n",
            "strain 0.15309113264083862\n",
            "strain 0.1350487917661667\n",
            "strain 0.10520077496767044\n",
            "classify 1.30364990234375\n",
            "classify 1.30145263671875\n",
            "classify 1.310791015625\n",
            "classify 1.2945556640625\n",
            "classify 1.3250000476837158\n",
            "0.3269961977186312\n",
            "strain 0.10910414159297943\n",
            "strain 0.14176999032497406\n",
            "strain 0.12735925614833832\n",
            "strain 0.14079879224300385\n",
            "strain 0.22969771921634674\n",
            "classify 1.2532958984375\n",
            "classify 1.355224609375\n",
            "classify 1.2869873046875\n",
            "classify 1.337158203125\n",
            "classify 1.125\n",
            "0.30038022813688214\n",
            "strain 0.1400439292192459\n",
            "strain 0.11198293417692184\n",
            "strain 0.08564934879541397\n",
            "strain 0.0763157457113266\n",
            "strain 0.1441238671541214\n",
            "classify 1.351318359375\n",
            "classify 1.28131103515625\n",
            "classify 1.3372802734375\n",
            "classify 1.257568359375\n",
            "classify 1.3406250476837158\n",
            "0.3193916349809886\n",
            "strain 0.11519437283277512\n",
            "strain 0.09325315058231354\n",
            "strain 0.14563755691051483\n",
            "strain 0.12614822387695312\n",
            "strain 0.03736576437950134\n",
            "classify 1.25970458984375\n",
            "classify 1.27569580078125\n",
            "classify 1.3487548828125\n",
            "classify 1.3497314453125\n",
            "classify 1.208593726158142\n",
            "0.311787072243346\n",
            "strain 0.10727209597826004\n",
            "strain 0.18577641248703003\n",
            "strain 0.284302681684494\n",
            "strain 0.11570116132497787\n",
            "strain 0.030844982713460922\n",
            "classify 1.2598876953125\n",
            "classify 1.27008056640625\n",
            "classify 1.34637451171875\n",
            "classify 1.34344482421875\n",
            "classify 1.376562476158142\n",
            "0.2965779467680608\n",
            "strain 0.0950474813580513\n",
            "strain 0.13095436990261078\n",
            "strain 0.07488340139389038\n",
            "strain 0.23426616191864014\n",
            "strain 0.043266281485557556\n",
            "classify 1.25335693359375\n",
            "classify 1.33154296875\n",
            "classify 1.3612060546875\n",
            "classify 1.2784423828125\n",
            "classify 1.3515625\n",
            "0.28517110266159695\n",
            "strain 0.12361367791891098\n",
            "strain 0.09610523283481598\n",
            "strain 0.13213911652565002\n",
            "strain 0.15339477360248566\n",
            "strain 0.12264884263277054\n",
            "classify 1.281982421875\n",
            "classify 1.37481689453125\n",
            "classify 1.28851318359375\n",
            "classify 1.270263671875\n",
            "classify 1.21875\n",
            "0.311787072243346\n",
            "strain 0.2339218109846115\n",
            "strain 0.21454337239265442\n",
            "strain 0.09377516061067581\n",
            "strain 0.12937483191490173\n",
            "strain 0.05401169881224632\n",
            "classify 1.32696533203125\n",
            "classify 1.27874755859375\n",
            "classify 1.3006591796875\n",
            "classify 1.2822265625\n",
            "classify 1.4656250476837158\n",
            "0.30798479087452474\n",
            "strain 0.09086444228887558\n",
            "strain 0.06201467290520668\n",
            "strain 0.11273501068353653\n",
            "strain 0.16539427638053894\n",
            "strain 0.09903629869222641\n",
            "classify 1.2977294921875\n",
            "classify 1.3026123046875\n",
            "classify 1.3359375\n",
            "classify 1.28955078125\n",
            "classify 1.014062523841858\n",
            "0.311787072243346\n",
            "strain 0.10067254304885864\n",
            "strain 0.1744614541530609\n",
            "strain 0.11065714061260223\n",
            "strain 0.1285155862569809\n",
            "strain 0.07009053975343704\n",
            "classify 1.2965087890625\n",
            "classify 1.29132080078125\n",
            "classify 1.2843017578125\n",
            "classify 1.31842041015625\n",
            "classify 1.376562476158142\n",
            "0.30798479087452474\n",
            "strain 0.09945002943277359\n",
            "strain 0.11689534038305283\n",
            "strain 0.08232661336660385\n",
            "strain 0.14967527985572815\n",
            "strain 0.06647171080112457\n",
            "classify 1.24481201171875\n",
            "classify 1.29827880859375\n",
            "classify 1.3153076171875\n",
            "classify 1.32965087890625\n",
            "classify 1.579687476158142\n",
            "0.33079847908745247\n",
            "strain 0.10564225167036057\n",
            "strain 0.1258690059185028\n",
            "strain 0.1762574166059494\n",
            "strain 0.1485205590724945\n",
            "strain 0.05048235133290291\n",
            "classify 1.36297607421875\n",
            "classify 1.2506103515625\n",
            "classify 1.2847900390625\n",
            "classify 1.323486328125\n",
            "classify 1.131250023841858\n",
            "0.33460076045627374\n",
            "strain 0.0896417498588562\n",
            "strain 0.06063868850469589\n",
            "strain 0.12546274065971375\n",
            "strain 0.07336721569299698\n",
            "strain 0.09463241696357727\n",
            "classify 1.28155517578125\n",
            "classify 1.282958984375\n",
            "classify 1.2637939453125\n",
            "classify 1.37847900390625\n",
            "classify 1.3156249523162842\n",
            "0.30038022813688214\n",
            "strain 0.09429623186588287\n",
            "strain 0.17429709434509277\n",
            "strain 0.15289178490638733\n",
            "strain 0.1334570348262787\n",
            "strain 0.06985949724912643\n",
            "classify 1.3231201171875\n",
            "classify 1.33563232421875\n",
            "classify 1.241455078125\n",
            "classify 1.3040771484375\n",
            "classify 1.240625023841858\n",
            "0.3041825095057034\n",
            "strain 0.08575039356946945\n",
            "strain 0.09887050092220306\n",
            "strain 0.1318618357181549\n",
            "strain 0.08157692849636078\n",
            "strain 0.16028469800949097\n",
            "classify 1.2757568359375\n",
            "classify 1.36737060546875\n",
            "classify 1.31427001953125\n",
            "classify 1.24560546875\n",
            "classify 1.373437523841858\n",
            "0.3231939163498099\n",
            "strain 0.07630406320095062\n",
            "strain 0.12357926368713379\n",
            "strain 0.25008636713027954\n",
            "strain 0.1791507750749588\n",
            "strain 0.10681372135877609\n",
            "classify 1.2742919921875\n",
            "classify 1.30316162109375\n",
            "classify 1.2906494140625\n",
            "classify 1.3670654296875\n",
            "classify 1.149999976158142\n",
            "0.3155893536121673\n",
            "strain 0.13822339475154877\n",
            "strain 0.10973817855119705\n",
            "strain 0.1337243765592575\n",
            "strain 0.09331291913986206\n",
            "strain 0.1967390477657318\n",
            "classify 1.30328369140625\n",
            "classify 1.365478515625\n",
            "classify 1.30926513671875\n",
            "classify 1.24920654296875\n",
            "classify 1.083593726158142\n",
            "0.311787072243346\n",
            "strain 0.09519335627555847\n",
            "strain 0.14847956597805023\n",
            "strain 0.1858208030462265\n",
            "strain 0.22057464718818665\n",
            "strain 0.23584456741809845\n",
            "classify 1.3499755859375\n",
            "classify 1.26141357421875\n",
            "classify 1.29248046875\n",
            "classify 1.273681640625\n",
            "classify 1.3312499523162842\n",
            "0.3155893536121673\n",
            "strain 0.1079106330871582\n",
            "strain 0.1734340637922287\n",
            "strain 0.0969899594783783\n",
            "strain 0.2256087213754654\n",
            "strain 0.1329309344291687\n",
            "classify 1.32745361328125\n",
            "classify 1.2811279296875\n",
            "classify 1.26483154296875\n",
            "classify 1.32305908203125\n",
            "classify 1.171875\n",
            "0.29277566539923955\n",
            "strain 0.10208206623792648\n",
            "strain 0.08631076663732529\n",
            "strain 0.09854734688997269\n",
            "strain 0.19321630895137787\n",
            "strain 0.06025012210011482\n",
            "classify 1.2666015625\n",
            "classify 1.24468994140625\n",
            "classify 1.308837890625\n",
            "classify 1.36083984375\n",
            "classify 1.21484375\n",
            "0.28517110266159695\n",
            "strain 0.2132359743118286\n",
            "strain 0.14411351084709167\n",
            "strain 0.16090136766433716\n",
            "strain 0.12680870294570923\n",
            "strain 0.052429672330617905\n",
            "classify 1.28070068359375\n",
            "classify 1.3255615234375\n",
            "classify 1.30877685546875\n",
            "classify 1.26324462890625\n",
            "classify 1.5281250476837158\n",
            "0.30798479087452474\n",
            "strain 0.09297487884759903\n",
            "strain 0.08086680620908737\n",
            "strain 0.14128540456295013\n",
            "strain 0.1949848234653473\n",
            "strain 0.1669607162475586\n",
            "classify 1.27734375\n",
            "classify 1.34222412109375\n",
            "classify 1.28387451171875\n",
            "classify 1.29034423828125\n",
            "classify 1.329687476158142\n",
            "0.30798479087452474\n",
            "strain 0.09377352893352509\n",
            "strain 0.15639866888523102\n",
            "strain 0.060115568339824677\n",
            "strain 0.2108153998851776\n",
            "strain 0.043438080698251724\n",
            "classify 1.28680419921875\n",
            "classify 1.29010009765625\n",
            "classify 1.32281494140625\n",
            "classify 1.2939453125\n",
            "classify 1.3359375\n",
            "0.29277566539923955\n",
            "strain 0.08139297366142273\n",
            "strain 0.08358323574066162\n",
            "strain 0.11210059374570847\n",
            "strain 0.1649312674999237\n",
            "strain 0.08831919729709625\n",
            "classify 1.25665283203125\n",
            "classify 1.3115234375\n",
            "classify 1.3302001953125\n",
            "classify 1.298095703125\n",
            "classify 1.3875000476837158\n",
            "0.2737642585551331\n",
            "strain 0.12365134805440903\n",
            "strain 0.23940055072307587\n",
            "strain 0.10526089370250702\n",
            "strain 0.12731733918190002\n",
            "strain 0.03972678631544113\n",
            "classify 1.30902099609375\n",
            "classify 1.2869873046875\n",
            "classify 1.30914306640625\n",
            "classify 1.2891845703125\n",
            "classify 1.33203125\n",
            "0.27756653992395436\n",
            "strain 0.16413280367851257\n",
            "strain 0.1624431163072586\n",
            "strain 0.12125454843044281\n",
            "strain 0.09627876430749893\n",
            "strain 0.035983260720968246\n",
            "classify 1.30389404296875\n",
            "classify 1.33319091796875\n",
            "classify 1.29193115234375\n",
            "classify 1.27764892578125\n",
            "classify 1.2195312976837158\n",
            "0.2889733840304182\n",
            "strain 0.09412664920091629\n",
            "strain 0.10886793583631516\n",
            "strain 0.15159687399864197\n",
            "strain 0.0971146747469902\n",
            "strain 0.08728397637605667\n",
            "classify 1.3380126953125\n",
            "classify 1.282470703125\n",
            "classify 1.31793212890625\n",
            "classify 1.2498779296875\n",
            "classify 1.33984375\n",
            "0.30798479087452474\n",
            "strain 0.21807271242141724\n",
            "strain 0.05841199308633804\n",
            "strain 0.13397078216075897\n",
            "strain 0.15313951671123505\n",
            "strain 0.05267210677266121\n",
            "classify 1.2755126953125\n",
            "classify 1.38153076171875\n",
            "classify 1.24005126953125\n",
            "classify 1.306640625\n",
            "classify 1.1984374523162842\n",
            "0.3155893536121673\n",
            "strain 0.17345185577869415\n",
            "strain 0.12480946630239487\n",
            "strain 0.16306978464126587\n",
            "strain 0.17186495661735535\n",
            "strain 0.05173364281654358\n",
            "classify 1.3172607421875\n",
            "classify 1.30279541015625\n",
            "classify 1.27813720703125\n",
            "classify 1.32366943359375\n",
            "classify 1.225000023841858\n",
            "0.30798479087452474\n",
            "strain 0.17785555124282837\n",
            "strain 0.09921601414680481\n",
            "strain 0.13707083463668823\n",
            "strain 0.12422296404838562\n",
            "strain 0.05011989176273346\n",
            "classify 1.3409423828125\n",
            "classify 1.2843017578125\n",
            "classify 1.29693603515625\n",
            "classify 1.2813720703125\n",
            "classify 1.282812476158142\n",
            "0.30798479087452474\n",
            "strain 0.09911246597766876\n",
            "strain 0.09079302847385406\n",
            "strain 0.13748806715011597\n",
            "strain 0.08887454122304916\n",
            "strain 0.05857240781188011\n",
            "classify 1.32373046875\n",
            "classify 1.25592041015625\n",
            "classify 1.316162109375\n",
            "classify 1.32183837890625\n",
            "classify 1.1984374523162842\n",
            "0.3155893536121673\n",
            "strain 0.09469691663980484\n",
            "strain 0.1004902794957161\n",
            "strain 0.20210988819599152\n",
            "strain 0.0846569836139679\n",
            "strain 0.05848054215312004\n",
            "classify 1.27960205078125\n",
            "classify 1.3438720703125\n",
            "classify 1.3016357421875\n",
            "classify 1.30499267578125\n",
            "classify 1.049218773841858\n",
            "0.3155893536121673\n",
            "strain 0.2565521001815796\n",
            "strain 0.08299149572849274\n",
            "strain 0.17053045332431793\n",
            "strain 0.24031047523021698\n",
            "strain 0.05455414205789566\n",
            "classify 1.34197998046875\n",
            "classify 1.2659912109375\n",
            "classify 1.266845703125\n",
            "classify 1.324951171875\n",
            "classify 1.365625023841858\n",
            "0.311787072243346\n",
            "strain 0.08670579642057419\n",
            "strain 0.13593821227550507\n",
            "strain 0.1848413348197937\n",
            "strain 0.06818938255310059\n",
            "strain 0.06298954784870148\n",
            "classify 1.26226806640625\n",
            "classify 1.31317138671875\n",
            "classify 1.30322265625\n",
            "classify 1.31011962890625\n",
            "classify 1.5031249523162842\n",
            "0.30798479087452474\n",
            "strain 0.08631327748298645\n",
            "strain 0.15879473090171814\n",
            "strain 0.11603299528360367\n",
            "strain 0.07092491537332535\n",
            "strain 0.2652367353439331\n",
            "classify 1.28778076171875\n",
            "classify 1.2943115234375\n",
            "classify 1.2725830078125\n",
            "classify 1.326904296875\n",
            "classify 1.46875\n",
            "0.311787072243346\n",
            "strain 0.11928536742925644\n",
            "strain 0.17313025891780853\n",
            "strain 0.1313858926296234\n",
            "strain 0.13362783193588257\n",
            "strain 0.04353366419672966\n",
            "classify 1.311279296875\n",
            "classify 1.3101806640625\n",
            "classify 1.25360107421875\n",
            "classify 1.3197021484375\n",
            "classify 1.302343726158142\n",
            "0.33460076045627374\n",
            "strain 0.18317446112632751\n",
            "strain 0.08547922223806381\n",
            "strain 0.15349633991718292\n",
            "strain 0.06430836021900177\n",
            "strain 0.05564368888735771\n",
            "classify 1.31915283203125\n",
            "classify 1.33935546875\n",
            "classify 1.22369384765625\n",
            "classify 1.3221435546875\n",
            "classify 1.2414062023162842\n",
            "0.311787072243346\n",
            "strain 0.10879968851804733\n",
            "strain 0.05457243695855141\n",
            "strain 0.09012240916490555\n",
            "strain 0.07111959904432297\n",
            "strain 0.13757534325122833\n",
            "classify 1.298583984375\n",
            "classify 1.33343505859375\n",
            "classify 1.22406005859375\n",
            "classify 1.30682373046875\n",
            "classify 1.5515625476837158\n",
            "0.30038022813688214\n",
            "strain 0.14939402043819427\n",
            "strain 0.19654607772827148\n",
            "strain 0.12184286117553711\n",
            "strain 0.1246834248304367\n",
            "strain 0.040525373071432114\n",
            "classify 1.24078369140625\n",
            "classify 1.302490234375\n",
            "classify 1.3138427734375\n",
            "classify 1.3160400390625\n",
            "classify 1.428125023841858\n",
            "0.30798479087452474\n",
            "strain 0.10446369647979736\n",
            "strain 0.21672910451889038\n",
            "strain 0.1278812140226364\n",
            "strain 0.10034079104661942\n",
            "strain 0.09456459432840347\n",
            "classify 1.275634765625\n",
            "classify 1.35687255859375\n",
            "classify 1.2642822265625\n",
            "classify 1.3126220703125\n",
            "classify 1.014062523841858\n",
            "0.28517110266159695\n",
            "strain 0.2361910343170166\n",
            "strain 0.07328920811414719\n",
            "strain 0.11153094470500946\n",
            "strain 0.09787797927856445\n",
            "strain 0.14947310090065002\n",
            "classify 1.30474853515625\n",
            "classify 1.32586669921875\n",
            "classify 1.26043701171875\n",
            "classify 1.2991943359375\n",
            "classify 1.6749999523162842\n",
            "0.2965779467680608\n",
            "strain 0.15107671916484833\n",
            "strain 0.10852024704217911\n",
            "strain 0.11108560115098953\n",
            "strain 0.14746806025505066\n",
            "strain 0.11760630458593369\n",
            "classify 1.2548828125\n",
            "classify 1.33868408203125\n",
            "classify 1.30426025390625\n",
            "classify 1.3087158203125\n",
            "classify 1.271875023841858\n",
            "0.2889733840304182\n",
            "strain 0.0966496616601944\n",
            "strain 0.08667685091495514\n",
            "strain 0.11980772763490677\n",
            "strain 0.20416851341724396\n",
            "strain 0.2040247768163681\n",
            "classify 1.30950927734375\n",
            "classify 1.28729248046875\n",
            "classify 1.32501220703125\n",
            "classify 1.2958984375\n",
            "classify 1.170312523841858\n",
            "0.3155893536121673\n",
            "strain 0.09235494583845139\n",
            "strain 0.1130995973944664\n",
            "strain 0.0959286093711853\n",
            "strain 0.11786697804927826\n",
            "strain 0.10750149190425873\n",
            "classify 1.2711181640625\n",
            "classify 1.28564453125\n",
            "classify 1.37457275390625\n",
            "classify 1.27374267578125\n",
            "classify 1.353124976158142\n",
            "0.3041825095057034\n",
            "strain 0.07259606570005417\n",
            "strain 0.10519677400588989\n",
            "strain 0.10144596546888351\n",
            "strain 0.23252084851264954\n",
            "strain 0.20596036314964294\n",
            "classify 1.30926513671875\n",
            "classify 1.33282470703125\n",
            "classify 1.2059326171875\n",
            "classify 1.36297607421875\n",
            "classify 1.346093773841858\n",
            "0.30798479087452474\n",
            "strain 0.08055426925420761\n",
            "strain 0.057070016860961914\n",
            "strain 0.09868691116571426\n",
            "strain 0.12304763495922089\n",
            "strain 0.06737244129180908\n",
            "classify 1.29827880859375\n",
            "classify 1.33148193359375\n",
            "classify 1.3226318359375\n",
            "classify 1.24554443359375\n",
            "classify 1.3937499523162842\n",
            "0.3269961977186312\n",
            "strain 0.06331024318933487\n",
            "strain 0.12313158810138702\n",
            "strain 0.11101812869310379\n",
            "strain 0.08403825759887695\n",
            "strain 0.09726528078317642\n",
            "classify 1.2559814453125\n",
            "classify 1.312744140625\n",
            "classify 1.3438720703125\n",
            "classify 1.30926513671875\n",
            "classify 1.381250023841858\n",
            "0.3193916349809886\n",
            "strain 0.09222933650016785\n",
            "strain 0.11308582872152328\n",
            "strain 0.14187921583652496\n",
            "strain 0.0739394947886467\n",
            "strain 0.24842850863933563\n",
            "classify 1.35333251953125\n",
            "classify 1.2735595703125\n",
            "classify 1.2462158203125\n",
            "classify 1.354248046875\n",
            "classify 1.3156249523162842\n",
            "0.33460076045627374\n",
            "strain 0.08791093528270721\n",
            "strain 0.1508272886276245\n",
            "strain 0.09942170977592468\n",
            "strain 0.100978322327137\n",
            "strain 0.10697995871305466\n",
            "classify 1.2532958984375\n",
            "classify 1.31097412109375\n",
            "classify 1.2747802734375\n",
            "classify 1.3623046875\n",
            "classify 1.146093726158142\n",
            "0.33079847908745247\n",
            "strain 0.07623455673456192\n",
            "strain 0.19661740958690643\n",
            "strain 0.27906444668769836\n",
            "strain 0.09880813956260681\n",
            "strain 0.06181628629565239\n",
            "classify 1.31451416015625\n",
            "classify 1.3157958984375\n",
            "classify 1.28851318359375\n",
            "classify 1.28033447265625\n",
            "classify 1.373437523841858\n",
            "0.3041825095057034\n",
            "strain 0.21084605157375336\n",
            "strain 0.06561487168073654\n",
            "strain 0.07192064821720123\n",
            "strain 0.09577451646327972\n",
            "strain 0.05167758837342262\n",
            "classify 1.3349609375\n",
            "classify 1.32781982421875\n",
            "classify 1.2841796875\n",
            "classify 1.25994873046875\n",
            "classify 1.232812523841858\n",
            "0.30038022813688214\n",
            "strain 0.09728360921144485\n",
            "strain 0.08241352438926697\n",
            "strain 0.07704953849315643\n",
            "strain 0.1347072571516037\n",
            "strain 0.05432215332984924\n",
            "classify 1.2916259765625\n",
            "classify 1.28240966796875\n",
            "classify 1.292724609375\n",
            "classify 1.32415771484375\n",
            "classify 1.3781249523162842\n",
            "0.2737642585551331\n",
            "strain 0.0986543670296669\n",
            "strain 0.12218267470598221\n",
            "strain 0.11264409869909286\n",
            "strain 0.17020349204540253\n",
            "strain 0.0797024592757225\n",
            "classify 1.29180908203125\n",
            "classify 1.2884521484375\n",
            "classify 1.26141357421875\n",
            "classify 1.3753662109375\n",
            "classify 1.119531273841858\n",
            "0.2737642585551331\n",
            "strain 0.1379614621400833\n",
            "strain 0.17620229721069336\n",
            "strain 0.11117763072252274\n",
            "strain 0.11660729348659515\n",
            "strain 0.07240922749042511\n",
            "classify 1.26177978515625\n",
            "classify 1.29119873046875\n",
            "classify 1.32135009765625\n",
            "classify 1.31396484375\n",
            "classify 1.390625\n",
            "0.3041825095057034\n",
            "strain 0.17984405159950256\n",
            "strain 0.18850675225257874\n",
            "strain 0.19745522737503052\n",
            "strain 0.14100900292396545\n",
            "strain 0.03777850419282913\n",
            "classify 1.267578125\n",
            "classify 1.3140869140625\n",
            "classify 1.3170166015625\n",
            "classify 1.2943115234375\n",
            "classify 1.421875\n",
            "0.30798479087452474\n",
            "strain 0.1351880580186844\n",
            "strain 0.09895076602697372\n",
            "strain 0.13132092356681824\n",
            "strain 0.14333820343017578\n",
            "strain 0.08384378254413605\n",
            "classify 1.288818359375\n",
            "classify 1.28497314453125\n",
            "classify 1.32452392578125\n",
            "classify 1.2939453125\n",
            "classify 1.321874976158142\n",
            "0.30038022813688214\n",
            "strain 0.15275686979293823\n",
            "strain 0.0707366093993187\n",
            "strain 0.13805262744426727\n",
            "strain 0.072538360953331\n",
            "strain 0.24493421614170074\n",
            "classify 1.25762939453125\n",
            "classify 1.35113525390625\n",
            "classify 1.2955322265625\n",
            "classify 1.2869873046875\n",
            "classify 1.30078125\n",
            "0.29277566539923955\n",
            "strain 0.08719092607498169\n",
            "strain 0.06985776126384735\n",
            "strain 0.11486753076314926\n",
            "strain 0.1520794779062271\n",
            "strain 0.0726388543844223\n",
            "classify 1.27020263671875\n",
            "classify 1.3095703125\n",
            "classify 1.28887939453125\n",
            "classify 1.30523681640625\n",
            "classify 1.342187523841858\n",
            "0.3041825095057034\n",
            "strain 0.11930965632200241\n",
            "strain 0.10522729158401489\n",
            "strain 0.19176620244979858\n",
            "strain 0.10617151111364365\n",
            "strain 0.4464899003505707\n",
            "classify 1.35150146484375\n",
            "classify 1.28985595703125\n",
            "classify 1.2874755859375\n",
            "classify 1.28240966796875\n",
            "classify 1.294531226158142\n",
            "0.311787072243346\n",
            "strain 0.2290431708097458\n",
            "strain 0.13216839730739594\n",
            "strain 0.08864033967256546\n",
            "strain 0.09061082452535629\n",
            "strain 0.29515257477760315\n",
            "classify 1.360595703125\n",
            "classify 1.29107666015625\n",
            "classify 1.278076171875\n",
            "classify 1.29150390625\n",
            "classify 1.140625\n",
            "0.3231939163498099\n",
            "strain 0.17259274423122406\n",
            "strain 0.17967960238456726\n",
            "strain 0.07724951207637787\n",
            "strain 0.19722524285316467\n",
            "strain 0.05427948758006096\n",
            "classify 1.3131103515625\n",
            "classify 1.2998046875\n",
            "classify 1.323974609375\n",
            "classify 1.261474609375\n",
            "classify 1.3671875\n",
            "0.30798479087452474\n",
            "strain 0.0881064310669899\n",
            "strain 0.11082014441490173\n",
            "strain 0.1365358680486679\n",
            "strain 0.08163970708847046\n",
            "strain 0.06380434334278107\n",
            "classify 1.341552734375\n",
            "classify 1.23468017578125\n",
            "classify 1.326171875\n",
            "classify 1.30657958984375\n",
            "classify 1.1921875476837158\n",
            "0.3155893536121673\n",
            "strain 0.0744333565235138\n",
            "strain 0.0525006428360939\n",
            "strain 0.09421886503696442\n",
            "strain 0.05897004157304764\n",
            "strain 0.09472586214542389\n",
            "classify 1.287841796875\n",
            "classify 1.249267578125\n",
            "classify 1.34564208984375\n",
            "classify 1.3204345703125\n",
            "classify 1.3117187023162842\n",
            "0.3231939163498099\n",
            "strain 0.13644002377986908\n",
            "strain 0.2017611712217331\n",
            "strain 0.14018592238426208\n",
            "strain 0.10547933727502823\n",
            "strain 0.13537541031837463\n",
            "classify 1.3289794921875\n",
            "classify 1.35748291015625\n",
            "classify 1.251953125\n",
            "classify 1.27581787109375\n",
            "classify 1.5203125476837158\n",
            "0.3269961977186312\n",
            "strain 0.08350301533937454\n",
            "strain 0.10421761870384216\n",
            "strain 0.09072108566761017\n",
            "strain 0.12915757298469543\n",
            "strain 0.051389917731285095\n",
            "classify 1.35638427734375\n",
            "classify 1.32342529296875\n",
            "classify 1.3187255859375\n",
            "classify 1.21051025390625\n",
            "classify 1.359375\n",
            "0.2965779467680608\n",
            "strain 0.09000737220048904\n",
            "strain 0.10096623003482819\n",
            "strain 0.15864427387714386\n",
            "strain 0.09485837817192078\n",
            "strain 0.07162012904882431\n",
            "classify 1.2569580078125\n",
            "classify 1.2879638671875\n",
            "classify 1.33172607421875\n",
            "classify 1.33624267578125\n",
            "classify 1.3328125476837158\n",
            "0.28517110266159695\n",
            "strain 0.09928242117166519\n",
            "strain 0.09679628908634186\n",
            "strain 0.09632725268602371\n",
            "strain 0.20404835045337677\n",
            "strain 0.052874691784381866\n",
            "classify 1.33502197265625\n",
            "classify 1.29547119140625\n",
            "classify 1.238037109375\n",
            "classify 1.35546875\n",
            "classify 1.240625023841858\n",
            "0.3041825095057034\n",
            "strain 0.1472751647233963\n",
            "strain 0.1724245399236679\n",
            "strain 0.15912912786006927\n",
            "strain 0.1966954469680786\n",
            "strain 0.04006713256239891\n",
            "classify 1.26434326171875\n",
            "classify 1.38787841796875\n",
            "classify 1.29150390625\n",
            "classify 1.25787353515625\n",
            "classify 1.4093749523162842\n",
            "0.2889733840304182\n",
            "strain 0.26307353377342224\n",
            "strain 0.12225158512592316\n",
            "strain 0.08354515582323074\n",
            "strain 0.1306139975786209\n",
            "strain 0.13050231337547302\n",
            "classify 1.3533935546875\n",
            "classify 1.25628662109375\n",
            "classify 1.32080078125\n",
            "classify 1.27886962890625\n",
            "classify 1.240625023841858\n",
            "0.28517110266159695\n",
            "strain 0.1740090698003769\n",
            "strain 0.1048765480518341\n",
            "strain 0.09422433376312256\n",
            "strain 0.09715913236141205\n",
            "strain 0.08386882394552231\n",
            "classify 1.29022216796875\n",
            "classify 1.37725830078125\n",
            "classify 1.3111572265625\n",
            "classify 1.21588134765625\n",
            "classify 1.390625\n",
            "0.2889733840304182\n",
            "strain 0.10831605643033981\n",
            "strain 0.20657989382743835\n",
            "strain 0.12143854796886444\n",
            "strain 0.16110676527023315\n",
            "strain 0.149124413728714\n",
            "classify 1.30694580078125\n",
            "classify 1.3267822265625\n",
            "classify 1.2645263671875\n",
            "classify 1.2974853515625\n",
            "classify 1.287500023841858\n",
            "0.2813688212927757\n",
            "strain 0.09122369438409805\n",
            "strain 0.13442188501358032\n",
            "strain 0.09814495593309402\n",
            "strain 0.21291735768318176\n",
            "strain 0.06118256598711014\n",
            "classify 1.308349609375\n",
            "classify 1.309814453125\n",
            "classify 1.27142333984375\n",
            "classify 1.30364990234375\n",
            "classify 1.3210937976837158\n",
            "0.28517110266159695\n",
            "strain 0.15382984280586243\n",
            "strain 0.11393176764249802\n",
            "strain 0.14763838052749634\n",
            "strain 0.17490456998348236\n",
            "strain 0.03857629746198654\n",
            "classify 1.27142333984375\n",
            "classify 1.27813720703125\n",
            "classify 1.3526611328125\n",
            "classify 1.31317138671875\n",
            "classify 1.2312500476837158\n",
            "0.29277566539923955\n",
            "strain 0.288724422454834\n",
            "strain 0.14502328634262085\n",
            "strain 0.06893030554056168\n",
            "strain 0.14806385338306427\n",
            "strain 0.07119681686162949\n",
            "classify 1.29638671875\n",
            "classify 1.335205078125\n",
            "classify 1.2587890625\n",
            "classify 1.318359375\n",
            "classify 1.359375\n",
            "0.2889733840304182\n",
            "strain 0.08675631135702133\n",
            "strain 0.16028301417827606\n",
            "strain 0.1266658753156662\n",
            "strain 0.25237926840782166\n",
            "strain 0.050958361476659775\n",
            "classify 1.31658935546875\n",
            "classify 1.2738037109375\n",
            "classify 1.28546142578125\n",
            "classify 1.30517578125\n",
            "classify 1.4375\n",
            "0.2813688212927757\n",
            "strain 0.09335002303123474\n",
            "strain 0.1530357301235199\n",
            "strain 0.08806972950696945\n",
            "strain 0.09266874939203262\n",
            "strain 0.06444558501243591\n",
            "classify 1.27581787109375\n",
            "classify 1.2628173828125\n",
            "classify 1.30877685546875\n",
            "classify 1.34112548828125\n",
            "classify 1.2531249523162842\n",
            "0.3193916349809886\n",
            "strain 0.05979788303375244\n",
            "strain 0.07809789478778839\n",
            "strain 0.12010402232408524\n",
            "strain 0.1752295196056366\n",
            "strain 0.13752016425132751\n",
            "classify 1.261962890625\n",
            "classify 1.3037109375\n",
            "classify 1.36572265625\n",
            "classify 1.259521484375\n",
            "classify 1.4140625\n",
            "0.3193916349809886\n",
            "strain 0.09495346248149872\n",
            "strain 0.11514411121606827\n",
            "strain 0.0925004780292511\n",
            "strain 0.22654469311237335\n",
            "strain 0.04322434961795807\n",
            "classify 1.3045654296875\n",
            "classify 1.2967529296875\n",
            "classify 1.33282470703125\n",
            "classify 1.26300048828125\n",
            "classify 1.221093773841858\n",
            "0.29277566539923955\n",
            "strain 0.18912020325660706\n",
            "strain 0.06270179152488708\n",
            "strain 0.057594623416662216\n",
            "strain 0.12396932393312454\n",
            "strain 0.06115259975194931\n",
            "classify 1.29931640625\n",
            "classify 1.31207275390625\n",
            "classify 1.32635498046875\n",
            "classify 1.24749755859375\n",
            "classify 1.4015624523162842\n",
            "0.29277566539923955\n",
            "strain 0.1503286361694336\n",
            "strain 0.2142995446920395\n",
            "strain 0.09862139075994492\n",
            "strain 0.21398873627185822\n",
            "strain 0.13558609783649445\n",
            "classify 1.34600830078125\n",
            "classify 1.28448486328125\n",
            "classify 1.23760986328125\n",
            "classify 1.31158447265625\n",
            "classify 1.5390625\n",
            "0.30038022813688214\n",
            "strain 0.07758212834596634\n",
            "strain 0.15091456472873688\n",
            "strain 0.11600437760353088\n",
            "strain 0.16232341527938843\n",
            "strain 0.3238842487335205\n",
            "classify 1.32110595703125\n",
            "classify 1.2691650390625\n",
            "classify 1.25457763671875\n",
            "classify 1.3701171875\n",
            "classify 1.26953125\n",
            "0.3193916349809886\n",
            "strain 0.09968606382608414\n",
            "strain 0.16640277206897736\n",
            "strain 0.10239595174789429\n",
            "strain 0.14945343136787415\n",
            "strain 0.07090659439563751\n",
            "classify 1.2991943359375\n",
            "classify 1.31781005859375\n",
            "classify 1.2698974609375\n",
            "classify 1.34259033203125\n",
            "classify 1.25390625\n",
            "0.2965779467680608\n",
            "strain 0.06129094958305359\n",
            "strain 0.1349801868200302\n",
            "strain 0.217638298869133\n",
            "strain 0.07093622535467148\n",
            "strain 0.04142584651708603\n",
            "classify 1.298095703125\n",
            "classify 1.316650390625\n",
            "classify 1.2847900390625\n",
            "classify 1.31866455078125\n",
            "classify 1.3054687976837158\n",
            "0.2889733840304182\n",
            "strain 0.22741658985614777\n",
            "strain 0.1326388120651245\n",
            "strain 0.07654916495084763\n",
            "strain 0.11683538556098938\n",
            "strain 0.045319974422454834\n",
            "classify 1.35089111328125\n",
            "classify 1.31109619140625\n",
            "classify 1.292236328125\n",
            "classify 1.27313232421875\n",
            "classify 1.251562476158142\n",
            "0.2889733840304182\n",
            "strain 0.2082957923412323\n",
            "strain 0.1325996220111847\n",
            "strain 0.13371682167053223\n",
            "strain 0.0673157274723053\n",
            "strain 0.043244291096925735\n",
            "classify 1.2646484375\n",
            "classify 1.30157470703125\n",
            "classify 1.35931396484375\n",
            "classify 1.28472900390625\n",
            "classify 1.545312523841858\n",
            "0.2813688212927757\n",
            "strain 0.1361396759748459\n",
            "strain 0.15855981409549713\n",
            "strain 0.213908389210701\n",
            "strain 0.22139301896095276\n",
            "strain 0.13052034378051758\n",
            "classify 1.302978515625\n",
            "classify 1.27935791015625\n",
            "classify 1.3232421875\n",
            "classify 1.3046875\n",
            "classify 1.3156249523162842\n",
            "0.26996197718631176\n",
            "strain 0.08826944977045059\n",
            "strain 0.1734066754579544\n",
            "strain 0.08758664131164551\n",
            "strain 0.13612154126167297\n",
            "strain 0.20093488693237305\n",
            "classify 1.2802734375\n",
            "classify 1.2733154296875\n",
            "classify 1.36529541015625\n",
            "classify 1.2894287109375\n",
            "classify 1.328125\n",
            "0.27756653992395436\n",
            "strain 0.16032539308071136\n",
            "strain 0.18388204276561737\n",
            "strain 0.16741503775119781\n",
            "strain 0.09304731339216232\n",
            "strain 0.03912745788693428\n",
            "classify 1.263427734375\n",
            "classify 1.2899169921875\n",
            "classify 1.31085205078125\n",
            "classify 1.3084716796875\n",
            "classify 1.5\n",
            "0.2813688212927757\n",
            "strain 0.09681277722120285\n",
            "strain 0.14025507867336273\n",
            "strain 0.08631440252065659\n",
            "strain 0.06816686689853668\n",
            "strain 0.06270468980073929\n",
            "classify 1.33514404296875\n",
            "classify 1.33795166015625\n",
            "classify 1.28924560546875\n",
            "classify 1.22802734375\n",
            "classify 1.13671875\n",
            "0.28517110266159695\n",
            "strain 0.1498553454875946\n",
            "strain 0.06924018263816833\n",
            "strain 0.1301446259021759\n",
            "strain 0.16534067690372467\n",
            "strain 0.16855676472187042\n",
            "classify 1.2791748046875\n",
            "classify 1.29901123046875\n",
            "classify 1.26971435546875\n",
            "classify 1.3221435546875\n",
            "classify 1.4015624523162842\n",
            "0.28517110266159695\n",
            "strain 0.08723819255828857\n",
            "strain 0.18924836814403534\n",
            "strain 0.0802886113524437\n",
            "strain 0.21360498666763306\n",
            "strain 0.056286051869392395\n",
            "classify 1.2794189453125\n",
            "classify 1.3406982421875\n",
            "classify 1.28363037109375\n",
            "classify 1.3023681640625\n",
            "classify 1.2374999523162842\n",
            "0.27756653992395436\n",
            "strain 0.08618403971195221\n",
            "strain 0.18717406690120697\n",
            "strain 0.10573875159025192\n",
            "strain 0.13748614490032196\n",
            "strain 0.3435266315937042\n",
            "classify 1.2911376953125\n",
            "classify 1.2734375\n",
            "classify 1.31982421875\n",
            "classify 1.3125\n",
            "classify 1.28125\n",
            "0.2661596958174905\n",
            "strain 0.13758769631385803\n",
            "strain 0.06633543968200684\n",
            "strain 0.08320382237434387\n",
            "strain 0.13724510371685028\n",
            "strain 0.05215251073241234\n",
            "classify 1.2783203125\n",
            "classify 1.31396484375\n",
            "classify 1.27545166015625\n",
            "classify 1.3114013671875\n",
            "classify 1.5359375476837158\n",
            "0.27756653992395436\n",
            "strain 0.24889130890369415\n",
            "strain 0.126495361328125\n",
            "strain 0.09262126684188843\n",
            "strain 0.11847834289073944\n",
            "strain 0.11017566174268723\n",
            "classify 1.34490966796875\n",
            "classify 1.3011474609375\n",
            "classify 1.2801513671875\n",
            "classify 1.25494384765625\n",
            "classify 1.2765624523162842\n",
            "0.2737642585551331\n",
            "strain 0.22003987431526184\n",
            "strain 0.11167237907648087\n",
            "strain 0.06776868551969528\n",
            "strain 0.26135629415512085\n",
            "strain 0.5439249873161316\n",
            "classify 1.29022216796875\n",
            "classify 1.24884033203125\n",
            "classify 1.36834716796875\n",
            "classify 1.27239990234375\n",
            "classify 1.25\n",
            "0.2661596958174905\n",
            "strain 0.12573254108428955\n",
            "strain 0.10483578592538834\n",
            "strain 0.19061827659606934\n",
            "strain 0.16826771199703217\n",
            "strain 0.08850622922182083\n",
            "classify 1.30914306640625\n",
            "classify 1.348388671875\n",
            "classify 1.29052734375\n",
            "classify 1.2847900390625\n",
            "classify 1.235937476158142\n",
            "0.3193916349809886\n",
            "strain 0.06899315118789673\n",
            "strain 0.12065370380878448\n",
            "strain 0.20546507835388184\n",
            "strain 0.21477386355400085\n",
            "strain 0.34699928760528564\n",
            "classify 1.2535400390625\n",
            "classify 1.35076904296875\n",
            "classify 1.2833251953125\n",
            "classify 1.35009765625\n",
            "classify 1.2039062976837158\n",
            "0.311787072243346\n",
            "strain 0.15683424472808838\n",
            "strain 0.10299639403820038\n",
            "strain 0.06261537969112396\n",
            "strain 0.16088435053825378\n",
            "strain 0.04047171398997307\n",
            "classify 1.2784423828125\n",
            "classify 1.30914306640625\n",
            "classify 1.28790283203125\n",
            "classify 1.3558349609375\n",
            "classify 1.1375000476837158\n",
            "0.3193916349809886\n",
            "strain 0.15079434216022491\n",
            "strain 0.14508503675460815\n",
            "strain 0.16756245493888855\n",
            "strain 0.1298035979270935\n",
            "strain 0.08062796294689178\n",
            "classify 1.2855224609375\n",
            "classify 1.2879638671875\n",
            "classify 1.2763671875\n",
            "classify 1.3663330078125\n",
            "classify 1.3562500476837158\n",
            "0.2585551330798479\n",
            "strain 0.09709525853395462\n",
            "strain 0.1954621970653534\n",
            "strain 0.14923448860645294\n",
            "strain 0.13595274090766907\n",
            "strain 0.1119905486702919\n",
            "classify 1.26739501953125\n",
            "classify 1.2928466796875\n",
            "classify 1.30712890625\n",
            "classify 1.3387451171875\n",
            "classify 1.4375\n",
            "0.27756653992395436\n",
            "strain 0.0836024284362793\n",
            "strain 0.10061871260404587\n",
            "strain 0.13940371572971344\n",
            "strain 0.14029039442539215\n",
            "strain 0.04696347936987877\n",
            "classify 1.2357177734375\n",
            "classify 1.30670166015625\n",
            "classify 1.29974365234375\n",
            "classify 1.35205078125\n",
            "classify 1.4249999523162842\n",
            "0.2965779467680608\n",
            "strain 0.07901277393102646\n",
            "strain 0.13819700479507446\n",
            "strain 0.22243072092533112\n",
            "strain 0.14369304478168488\n",
            "strain 0.037607211619615555\n",
            "classify 1.30828857421875\n",
            "classify 1.32037353515625\n",
            "classify 1.30804443359375\n",
            "classify 1.286865234375\n",
            "classify 1.2117187976837158\n",
            "0.2813688212927757\n",
            "strain 0.0830824077129364\n",
            "strain 0.14838175475597382\n",
            "strain 0.07305882126092911\n",
            "strain 0.1498328298330307\n",
            "strain 0.09219712018966675\n",
            "classify 1.32318115234375\n",
            "classify 1.27923583984375\n",
            "classify 1.35748291015625\n",
            "classify 1.25653076171875\n",
            "classify 1.359375\n",
            "0.28517110266159695\n",
            "strain 0.0870344340801239\n",
            "strain 0.10981298983097076\n",
            "strain 0.11139002442359924\n",
            "strain 0.06373248994350433\n",
            "strain 0.05698408558964729\n",
            "classify 1.3037109375\n",
            "classify 1.29144287109375\n",
            "classify 1.3167724609375\n",
            "classify 1.3160400390625\n",
            "classify 1.236718773841858\n",
            "0.2965779467680608\n",
            "strain 0.13573439419269562\n",
            "strain 0.09327778965234756\n",
            "strain 0.17335356771945953\n",
            "strain 0.1821032613515854\n",
            "strain 0.11460020393133163\n",
            "classify 1.2489013671875\n",
            "classify 1.3328857421875\n",
            "classify 1.2830810546875\n",
            "classify 1.365966796875\n",
            "classify 1.295312523841858\n",
            "0.28517110266159695\n",
            "strain 0.06789885461330414\n",
            "strain 0.1277100294828415\n",
            "strain 0.23112842440605164\n",
            "strain 0.08782845735549927\n",
            "strain 0.11872980743646622\n",
            "classify 1.32513427734375\n",
            "classify 1.277587890625\n",
            "classify 1.28375244140625\n",
            "classify 1.3494873046875\n",
            "classify 1.1726562976837158\n",
            "0.30038022813688214\n",
            "strain 0.21908685564994812\n",
            "strain 0.14486166834831238\n",
            "strain 0.16846875846385956\n",
            "strain 0.07609863579273224\n",
            "strain 0.22995688021183014\n",
            "classify 1.274658203125\n",
            "classify 1.30987548828125\n",
            "classify 1.31280517578125\n",
            "classify 1.32757568359375\n",
            "classify 1.265625\n",
            "0.2737642585551331\n",
            "strain 0.10194796323776245\n",
            "strain 0.06622536480426788\n",
            "strain 0.08057008683681488\n",
            "strain 0.08696497976779938\n",
            "strain 0.04731857404112816\n",
            "classify 1.296142578125\n",
            "classify 1.3411865234375\n",
            "classify 1.33935546875\n",
            "classify 1.2401123046875\n",
            "classify 1.28125\n",
            "0.28517110266159695\n",
            "strain 0.10582564771175385\n",
            "strain 0.15844719111919403\n",
            "strain 0.10134245455265045\n",
            "strain 0.12512503564357758\n",
            "strain 0.13618281483650208\n",
            "classify 1.315185546875\n",
            "classify 1.33941650390625\n",
            "classify 1.2645263671875\n",
            "classify 1.27960205078125\n",
            "classify 1.4484374523162842\n",
            "0.2813688212927757\n",
            "strain 0.07392674684524536\n",
            "strain 0.21912236511707306\n",
            "strain 0.11491970717906952\n",
            "strain 0.21373781561851501\n",
            "strain 0.14539112150669098\n",
            "classify 1.26519775390625\n",
            "classify 1.36962890625\n",
            "classify 1.2935791015625\n",
            "classify 1.3089599609375\n",
            "classify 1.16796875\n",
            "0.2737642585551331\n",
            "strain 0.13442277908325195\n",
            "strain 0.11532547324895859\n",
            "strain 0.12260577082633972\n",
            "strain 0.14310622215270996\n",
            "strain 0.13510026037693024\n",
            "classify 1.30633544921875\n",
            "classify 1.2840576171875\n",
            "classify 1.322509765625\n",
            "classify 1.30181884765625\n",
            "classify 1.3781249523162842\n",
            "0.29277566539923955\n",
            "strain 0.1297825276851654\n",
            "strain 0.08580752462148666\n",
            "strain 0.1458396166563034\n",
            "strain 0.1081758663058281\n",
            "strain 0.06423777341842651\n",
            "classify 1.346435546875\n",
            "classify 1.29193115234375\n",
            "classify 1.28375244140625\n",
            "classify 1.29437255859375\n",
            "classify 1.3125\n",
            "0.3193916349809886\n",
            "strain 0.1547427624464035\n",
            "strain 0.2699660360813141\n",
            "strain 0.08469218015670776\n",
            "strain 0.23142841458320618\n",
            "strain 0.03545384109020233\n",
            "classify 1.32550048828125\n",
            "classify 1.34808349609375\n",
            "classify 1.25897216796875\n",
            "classify 1.2861328125\n",
            "classify 1.170312523841858\n",
            "0.3041825095057034\n",
            "strain 0.17205320298671722\n",
            "strain 0.0992010086774826\n",
            "strain 0.14560933411121368\n",
            "strain 0.27326512336730957\n",
            "strain 0.08520769327878952\n",
            "classify 1.2943115234375\n",
            "classify 1.33526611328125\n",
            "classify 1.235107421875\n",
            "classify 1.3570556640625\n",
            "classify 1.2765624523162842\n",
            "0.2813688212927757\n",
            "strain 0.09918210655450821\n",
            "strain 0.10873764008283615\n",
            "strain 0.17149853706359863\n",
            "strain 0.06422760337591171\n",
            "strain 0.06418345868587494\n",
            "classify 1.351806640625\n",
            "classify 1.2850341796875\n",
            "classify 1.3121337890625\n",
            "classify 1.284423828125\n",
            "classify 1.2921874523162842\n",
            "0.2813688212927757\n",
            "strain 0.0791926383972168\n",
            "strain 0.17596901953220367\n",
            "strain 0.06598362326622009\n",
            "strain 0.1624244600534439\n",
            "strain 0.14671634137630463\n",
            "classify 1.314208984375\n",
            "classify 1.306396484375\n",
            "classify 1.27386474609375\n",
            "classify 1.3358154296875\n",
            "classify 1.4265625476837158\n",
            "0.27756653992395436\n",
            "strain 0.09738484025001526\n",
            "strain 0.13434778153896332\n",
            "strain 0.08640118688344955\n",
            "strain 0.08478786051273346\n",
            "strain 0.1831338107585907\n",
            "classify 1.32440185546875\n",
            "classify 1.3248291015625\n",
            "classify 1.27911376953125\n",
            "classify 1.30657958984375\n",
            "classify 1.4171874523162842\n",
            "0.2889733840304182\n",
            "strain 0.15586785972118378\n",
            "strain 0.10815561562776566\n",
            "strain 0.11632607877254486\n",
            "strain 0.13398507237434387\n",
            "strain 0.13050541281700134\n",
            "classify 1.32244873046875\n",
            "classify 1.253662109375\n",
            "classify 1.36083984375\n",
            "classify 1.30810546875\n",
            "classify 1.142968773841858\n",
            "0.3041825095057034\n",
            "strain 0.10640658438205719\n",
            "strain 0.20549948513507843\n",
            "strain 0.06986687332391739\n",
            "strain 0.16332004964351654\n",
            "strain 0.3012533187866211\n",
            "classify 1.31011962890625\n",
            "classify 1.3065185546875\n",
            "classify 1.302978515625\n",
            "classify 1.28302001953125\n",
            "classify 1.267187476158142\n",
            "0.3155893536121673\n",
            "strain 0.16839860379695892\n",
            "strain 0.09709060192108154\n",
            "strain 0.1307666152715683\n",
            "strain 0.1600595861673355\n",
            "strain 0.0815119743347168\n",
            "classify 1.281494140625\n",
            "classify 1.30145263671875\n",
            "classify 1.297119140625\n",
            "classify 1.32110595703125\n",
            "classify 1.265625\n",
            "0.311787072243346\n",
            "strain 0.14753614366054535\n",
            "strain 0.15670059621334076\n",
            "strain 0.09169665724039078\n",
            "strain 0.12999191880226135\n",
            "strain 0.12332559376955032\n",
            "classify 1.32952880859375\n",
            "classify 1.28729248046875\n",
            "classify 1.29962158203125\n",
            "classify 1.283935546875\n",
            "classify 1.267968773841858\n",
            "0.30038022813688214\n",
            "strain 0.07436858862638474\n",
            "strain 0.08976326137781143\n",
            "strain 0.20183692872524261\n",
            "strain 0.06504899263381958\n",
            "strain 0.051562823355197906\n",
            "classify 1.26177978515625\n",
            "classify 1.26318359375\n",
            "classify 1.361572265625\n",
            "classify 1.32525634765625\n",
            "classify 1.267968773841858\n",
            "0.2737642585551331\n",
            "strain 0.08243958652019501\n",
            "strain 0.10298699140548706\n",
            "strain 0.0965932160615921\n",
            "strain 0.12822042405605316\n",
            "strain 0.03778832033276558\n",
            "classify 1.345947265625\n",
            "classify 1.3406982421875\n",
            "classify 1.27642822265625\n",
            "classify 1.25848388671875\n",
            "classify 1.28515625\n",
            "0.28517110266159695\n",
            "strain 0.21722698211669922\n",
            "strain 0.15031446516513824\n",
            "strain 0.18301144242286682\n",
            "strain 0.06961864233016968\n",
            "strain 0.08693183958530426\n",
            "classify 1.30535888671875\n",
            "classify 1.32012939453125\n",
            "classify 1.2713623046875\n",
            "classify 1.30181884765625\n",
            "classify 1.359375\n",
            "0.33460076045627374\n",
            "strain 0.22305555641651154\n",
            "strain 0.14873503148555756\n",
            "strain 0.10045313835144043\n",
            "strain 0.19778965413570404\n",
            "strain 0.13394679129123688\n",
            "classify 1.2880859375\n",
            "classify 1.2919921875\n",
            "classify 1.33245849609375\n",
            "classify 1.3038330078125\n",
            "classify 1.2687499523162842\n",
            "0.311787072243346\n",
            "strain 0.10766661167144775\n",
            "strain 0.10926587134599686\n",
            "strain 0.08425240963697433\n",
            "strain 0.12281052768230438\n",
            "strain 0.2824593484401703\n",
            "classify 1.2921142578125\n",
            "classify 1.29217529296875\n",
            "classify 1.32122802734375\n",
            "classify 1.30499267578125\n",
            "classify 1.203125\n",
            "0.2737642585551331\n",
            "strain 0.08810827136039734\n",
            "strain 0.059741273522377014\n",
            "strain 0.17368082702159882\n",
            "strain 0.16229033470153809\n",
            "strain 0.04489682987332344\n",
            "classify 1.272705078125\n",
            "classify 1.25921630859375\n",
            "classify 1.32952880859375\n",
            "classify 1.34765625\n",
            "classify 1.204687476158142\n",
            "0.2737642585551331\n",
            "strain 0.15304839611053467\n",
            "strain 0.0843525379896164\n",
            "strain 0.20111247897148132\n",
            "strain 0.13981497287750244\n",
            "strain 0.08938324451446533\n",
            "classify 1.30718994140625\n",
            "classify 1.31549072265625\n",
            "classify 1.29998779296875\n",
            "classify 1.31292724609375\n",
            "classify 1.234375\n",
            "0.2813688212927757\n",
            "strain 0.06204306706786156\n",
            "strain 0.09289871901273727\n",
            "strain 0.07864508777856827\n",
            "strain 0.2571793794631958\n",
            "strain 0.15312398970127106\n",
            "classify 1.31060791015625\n",
            "classify 1.29901123046875\n",
            "classify 1.2470703125\n",
            "classify 1.32464599609375\n",
            "classify 1.544531226158142\n",
            "0.27756653992395436\n",
            "strain 0.09274473041296005\n",
            "strain 0.061955347657203674\n",
            "strain 0.06068262830376625\n",
            "strain 0.12200386077165604\n",
            "strain 0.09109187126159668\n",
            "classify 1.30145263671875\n",
            "classify 1.30731201171875\n",
            "classify 1.263916015625\n",
            "classify 1.34259033203125\n",
            "classify 1.3312499523162842\n",
            "0.3269961977186312\n",
            "strain 0.12865528464317322\n",
            "strain 0.05455148220062256\n",
            "strain 0.10186045616865158\n",
            "strain 0.21461133658885956\n",
            "strain 0.06656559556722641\n",
            "classify 1.30230712890625\n",
            "classify 1.31768798828125\n",
            "classify 1.2850341796875\n",
            "classify 1.32977294921875\n",
            "classify 1.217187523841858\n",
            "0.3269961977186312\n",
            "strain 0.12159225344657898\n",
            "strain 0.08423972874879837\n",
            "strain 0.1082577109336853\n",
            "strain 0.11737623065710068\n",
            "strain 0.20264804363250732\n",
            "classify 1.315673828125\n",
            "classify 1.2806396484375\n",
            "classify 1.33489990234375\n",
            "classify 1.3040771484375\n",
            "classify 1.252343773841858\n",
            "0.3231939163498099\n",
            "strain 0.08611690253019333\n",
            "strain 0.2037534862756729\n",
            "strain 0.09805162996053696\n",
            "strain 0.11377459764480591\n",
            "strain 0.02988542430102825\n",
            "classify 1.35113525390625\n",
            "classify 1.34674072265625\n",
            "classify 1.27734375\n",
            "classify 1.264892578125\n",
            "classify 1.162500023841858\n",
            "0.3269961977186312\n",
            "strain 0.10439104586839676\n",
            "strain 0.10621720552444458\n",
            "strain 0.2224583625793457\n",
            "strain 0.1150316521525383\n",
            "strain 0.07236772775650024\n",
            "classify 1.2855224609375\n",
            "classify 1.235595703125\n",
            "classify 1.3499755859375\n",
            "classify 1.35845947265625\n",
            "classify 1.428125023841858\n",
            "0.33460076045627374\n",
            "strain 0.12257033586502075\n",
            "strain 0.2683386504650116\n",
            "strain 0.11558054387569427\n",
            "strain 0.13635095953941345\n",
            "strain 0.02262493222951889\n",
            "classify 1.31060791015625\n",
            "classify 1.3238525390625\n",
            "classify 1.30743408203125\n",
            "classify 1.27374267578125\n",
            "classify 1.322656273841858\n",
            "0.33079847908745247\n",
            "strain 0.13791465759277344\n",
            "strain 0.19980080425739288\n",
            "strain 0.1818932741880417\n",
            "strain 0.0668439269065857\n",
            "strain 0.05880384147167206\n",
            "classify 1.332763671875\n",
            "classify 1.2532958984375\n",
            "classify 1.31683349609375\n",
            "classify 1.2901611328125\n",
            "classify 1.6375000476837158\n",
            "0.3193916349809886\n",
            "strain 0.1611274629831314\n",
            "strain 0.2601521611213684\n",
            "strain 0.1315048336982727\n",
            "strain 0.0936167761683464\n",
            "strain 0.051062993705272675\n",
            "classify 1.2955322265625\n",
            "classify 1.30352783203125\n",
            "classify 1.3436279296875\n",
            "classify 1.28125\n",
            "classify 1.3468749523162842\n",
            "0.3269961977186312\n",
            "strain 0.1337355077266693\n",
            "strain 0.07413158565759659\n",
            "strain 0.07795144617557526\n",
            "strain 0.06433718651533127\n",
            "strain 0.09283480793237686\n",
            "classify 1.361083984375\n",
            "classify 1.28558349609375\n",
            "classify 1.289794921875\n",
            "classify 1.290283203125\n",
            "classify 1.3039062023162842\n",
            "0.3193916349809886\n",
            "strain 0.14865265786647797\n",
            "strain 0.1453874558210373\n",
            "strain 0.11084677278995514\n",
            "strain 0.23784078657627106\n",
            "strain 0.07533067464828491\n",
            "classify 1.2938232421875\n",
            "classify 1.28253173828125\n",
            "classify 1.32159423828125\n",
            "classify 1.31353759765625\n",
            "classify 1.3054687976837158\n",
            "0.3193916349809886\n",
            "strain 0.11988211423158646\n",
            "strain 0.1277291476726532\n",
            "strain 0.1156303733587265\n",
            "strain 0.09310220927000046\n",
            "strain 0.08465123176574707\n",
            "classify 1.248291015625\n",
            "classify 1.299072265625\n",
            "classify 1.3350830078125\n",
            "classify 1.3028564453125\n",
            "classify 1.28125\n",
            "0.3193916349809886\n",
            "strain 0.08708598464727402\n",
            "strain 0.10910560190677643\n",
            "strain 0.09654127061367035\n",
            "strain 0.1278102993965149\n",
            "strain 0.09117626398801804\n",
            "classify 1.31011962890625\n",
            "classify 1.30267333984375\n",
            "classify 1.2760009765625\n",
            "classify 1.3236083984375\n",
            "classify 1.2843749523162842\n",
            "0.3041825095057034\n",
            "strain 0.1516420841217041\n",
            "strain 0.06662312895059586\n",
            "strain 0.06760749220848083\n",
            "strain 0.19321191310882568\n",
            "strain 0.061625052243471146\n",
            "classify 1.29742431640625\n",
            "classify 1.276611328125\n",
            "classify 1.30462646484375\n",
            "classify 1.34283447265625\n",
            "classify 1.1453125476837158\n",
            "0.311787072243346\n",
            "strain 0.06464064121246338\n",
            "strain 0.10309986770153046\n",
            "strain 0.08722235262393951\n",
            "strain 0.10926273465156555\n",
            "strain 0.04923900216817856\n",
            "classify 1.30291748046875\n",
            "classify 1.32318115234375\n",
            "classify 1.2486572265625\n",
            "classify 1.33026123046875\n",
            "classify 1.389062523841858\n",
            "0.30798479087452474\n",
            "strain 0.10328907519578934\n",
            "strain 0.0973275750875473\n",
            "strain 0.08477091789245605\n",
            "strain 0.07770829647779465\n",
            "strain 0.08889547735452652\n",
            "classify 1.3480224609375\n",
            "classify 1.31005859375\n",
            "classify 1.28997802734375\n",
            "classify 1.24713134765625\n",
            "classify 1.3781249523162842\n",
            "0.3041825095057034\n",
            "strain 0.11715443432331085\n",
            "strain 0.07065223902463913\n",
            "strain 0.07212870568037033\n",
            "strain 0.0872499942779541\n",
            "strain 0.05262085795402527\n",
            "classify 1.28955078125\n",
            "classify 1.26104736328125\n",
            "classify 1.32000732421875\n",
            "classify 1.31524658203125\n",
            "classify 1.662500023841858\n",
            "0.3155893536121673\n",
            "strain 0.18353663384914398\n",
            "strain 0.04880619794130325\n",
            "strain 0.14948055148124695\n",
            "strain 0.21033190190792084\n",
            "strain 0.14363254606723785\n",
            "classify 1.3160400390625\n",
            "classify 1.24755859375\n",
            "classify 1.29119873046875\n",
            "classify 1.34149169921875\n",
            "classify 1.2890625\n",
            "0.3193916349809886\n",
            "strain 0.188873291015625\n",
            "strain 0.18220284581184387\n",
            "strain 0.12167811393737793\n",
            "strain 0.11756464093923569\n",
            "strain 0.10437338799238205\n",
            "classify 1.2904052734375\n",
            "classify 1.356201171875\n",
            "classify 1.2591552734375\n",
            "classify 1.3228759765625\n",
            "classify 1.279687523841858\n",
            "0.3155893536121673\n",
            "strain 0.12494463473558426\n",
            "strain 0.10147542506456375\n",
            "strain 0.10558373481035233\n",
            "strain 0.07291154563426971\n",
            "strain 0.13752812147140503\n",
            "classify 1.30914306640625\n",
            "classify 1.24676513671875\n",
            "classify 1.33984375\n",
            "classify 1.34417724609375\n",
            "classify 1.373437523841858\n",
            "0.3269961977186312\n",
            "strain 0.10200139880180359\n",
            "strain 0.08560510724782944\n",
            "strain 0.14941652119159698\n",
            "strain 0.14390498399734497\n",
            "strain 0.05713317543268204\n",
            "classify 1.37762451171875\n",
            "classify 1.2974853515625\n",
            "classify 1.2940673828125\n",
            "classify 1.24871826171875\n",
            "classify 1.32421875\n",
            "0.3269961977186312\n",
            "strain 0.1195785328745842\n",
            "strain 0.13833056390285492\n",
            "strain 0.05774061381816864\n",
            "strain 0.07082108408212662\n",
            "strain 0.036253754049539566\n",
            "classify 1.3314208984375\n",
            "classify 1.25946044921875\n",
            "classify 1.3406982421875\n",
            "classify 1.28131103515625\n",
            "classify 1.247656226158142\n",
            "0.3269961977186312\n",
            "strain 0.10101386159658432\n",
            "strain 0.10282861441373825\n",
            "strain 0.06804157793521881\n",
            "strain 0.11301953345537186\n",
            "strain 0.05371293053030968\n",
            "classify 1.317626953125\n",
            "classify 1.30963134765625\n",
            "classify 1.2774658203125\n",
            "classify 1.298095703125\n",
            "classify 1.2531249523162842\n",
            "0.3041825095057034\n",
            "strain 0.13102877140045166\n",
            "strain 0.13021281361579895\n",
            "strain 0.2634250521659851\n",
            "strain 0.2088393121957779\n",
            "strain 0.06252354383468628\n",
            "classify 1.283935546875\n",
            "classify 1.2696533203125\n",
            "classify 1.33740234375\n",
            "classify 1.3087158203125\n",
            "classify 1.30859375\n",
            "0.30038022813688214\n",
            "strain 0.11227364838123322\n",
            "strain 0.0909964069724083\n",
            "strain 0.17242026329040527\n",
            "strain 0.0983634814620018\n",
            "strain 0.041106581687927246\n",
            "classify 1.29681396484375\n",
            "classify 1.3284912109375\n",
            "classify 1.2752685546875\n",
            "classify 1.3123779296875\n",
            "classify 1.435937523841858\n",
            "0.3041825095057034\n",
            "strain 0.11691392213106155\n",
            "strain 0.1483561247587204\n",
            "strain 0.0649423599243164\n",
            "strain 0.09018293023109436\n",
            "strain 0.0736648291349411\n",
            "classify 1.2884521484375\n",
            "classify 1.236083984375\n",
            "classify 1.3153076171875\n",
            "classify 1.333740234375\n",
            "classify 1.6484375\n",
            "0.2813688212927757\n",
            "strain 0.09399119764566422\n",
            "strain 0.0644153580069542\n",
            "strain 0.1923740953207016\n",
            "strain 0.05945887416601181\n",
            "strain 0.05176524445414543\n",
            "classify 1.29937744140625\n",
            "classify 1.32086181640625\n",
            "classify 1.27386474609375\n",
            "classify 1.31390380859375\n",
            "classify 1.334375023841858\n",
            "0.29277566539923955\n",
            "strain 0.06862597167491913\n",
            "strain 0.18605126440525055\n",
            "strain 0.15662816166877747\n",
            "strain 0.0844445675611496\n",
            "strain 0.047181110829114914\n",
            "classify 1.2996826171875\n",
            "classify 1.3330078125\n",
            "classify 1.28826904296875\n",
            "classify 1.29974365234375\n",
            "classify 1.267187476158142\n",
            "0.2889733840304182\n",
            "strain 0.22358368337154388\n",
            "strain 0.12049099057912827\n",
            "strain 0.10816097259521484\n",
            "strain 0.21579578518867493\n",
            "strain 0.10425860434770584\n",
            "classify 1.2852783203125\n",
            "classify 1.2904052734375\n",
            "classify 1.3289794921875\n",
            "classify 1.30059814453125\n",
            "classify 1.33984375\n",
            "0.27756653992395436\n",
            "strain 0.06355028599500656\n",
            "strain 0.1228332370519638\n",
            "strain 0.11059936136007309\n",
            "strain 0.07855351269245148\n",
            "strain 0.034377165138721466\n",
            "classify 1.34368896484375\n",
            "classify 1.28271484375\n",
            "classify 1.32464599609375\n",
            "classify 1.26776123046875\n",
            "classify 1.334375023841858\n",
            "0.30798479087452474\n",
            "strain 0.19187238812446594\n",
            "strain 0.22879143059253693\n",
            "strain 0.14587166905403137\n",
            "strain 0.0723508894443512\n",
            "strain 0.22642874717712402\n",
            "classify 1.33258056640625\n",
            "classify 1.33807373046875\n",
            "classify 1.30877685546875\n",
            "classify 1.2340087890625\n",
            "classify 1.4609375\n",
            "0.3041825095057034\n",
            "strain 0.08359979838132858\n",
            "strain 0.10926196724176407\n",
            "strain 0.09120593219995499\n",
            "strain 0.13050265610218048\n",
            "strain 0.03729882091283798\n",
            "classify 1.35821533203125\n",
            "classify 1.28314208984375\n",
            "classify 1.30712890625\n",
            "classify 1.26702880859375\n",
            "classify 1.439062476158142\n",
            "0.3193916349809886\n",
            "strain 0.09027458727359772\n",
            "strain 0.09638353437185287\n",
            "strain 0.08757628500461578\n",
            "strain 0.1774260550737381\n",
            "strain 0.08501749485731125\n",
            "classify 1.25933837890625\n",
            "classify 1.34600830078125\n",
            "classify 1.31756591796875\n",
            "classify 1.295654296875\n",
            "classify 1.31640625\n",
            "0.3269961977186312\n",
            "strain 0.2107308804988861\n",
            "strain 0.08675619959831238\n",
            "strain 0.07552953064441681\n",
            "strain 0.09713893383741379\n",
            "strain 0.10223089903593063\n",
            "classify 1.2978515625\n",
            "classify 1.2994384765625\n",
            "classify 1.3170166015625\n",
            "classify 1.2974853515625\n",
            "classify 1.360937476158142\n",
            "0.34220532319391633\n",
            "strain 0.09176815301179886\n",
            "strain 0.1236567497253418\n",
            "strain 0.16123062372207642\n",
            "strain 0.17683614790439606\n",
            "strain 0.07346794009208679\n",
            "classify 1.28277587890625\n",
            "classify 1.27239990234375\n",
            "classify 1.36627197265625\n",
            "classify 1.31976318359375\n",
            "classify 1.0789062976837158\n",
            "0.3155893536121673\n",
            "strain 0.18961785733699799\n",
            "strain 0.26459792256355286\n",
            "strain 0.14321845769882202\n",
            "strain 0.07131753861904144\n",
            "strain 0.05159403756260872\n",
            "classify 1.32781982421875\n",
            "classify 1.3299560546875\n",
            "classify 1.28192138671875\n",
            "classify 1.280517578125\n",
            "classify 1.38671875\n",
            "0.3231939163498099\n",
            "strain 0.14423388242721558\n",
            "strain 0.13357077538967133\n",
            "strain 0.1427660882472992\n",
            "strain 0.24130140244960785\n",
            "strain 0.03196226805448532\n",
            "classify 1.3544921875\n",
            "classify 1.301513671875\n",
            "classify 1.28887939453125\n",
            "classify 1.26177978515625\n",
            "classify 1.3351562023162842\n",
            "0.311787072243346\n",
            "strain 0.06825792789459229\n",
            "strain 0.11947129666805267\n",
            "strain 0.06368718296289444\n",
            "strain 0.18382729589939117\n",
            "strain 0.23361337184906006\n",
            "classify 1.30999755859375\n",
            "classify 1.3212890625\n",
            "classify 1.28594970703125\n",
            "classify 1.28472900390625\n",
            "classify 1.3125\n",
            "0.3155893536121673\n",
            "strain 0.22327132523059845\n",
            "strain 0.1528148204088211\n",
            "strain 0.09237026423215866\n",
            "strain 0.07430783659219742\n",
            "strain 0.2076711803674698\n",
            "classify 1.28515625\n",
            "classify 1.288818359375\n",
            "classify 1.34832763671875\n",
            "classify 1.2913818359375\n",
            "classify 1.264062523841858\n",
            "0.30798479087452474\n",
            "strain 0.058646310120821\n",
            "strain 0.1052967980504036\n",
            "strain 0.26450785994529724\n",
            "strain 0.2343599647283554\n",
            "strain 0.07700975239276886\n",
            "classify 1.35699462890625\n",
            "classify 1.27490234375\n",
            "classify 1.36627197265625\n",
            "classify 1.22119140625\n",
            "classify 1.4187500476837158\n",
            "0.2965779467680608\n",
            "strain 0.0688173696398735\n",
            "strain 0.17522595822811127\n",
            "strain 0.16177794337272644\n",
            "strain 0.10777419805526733\n",
            "strain 0.03356728330254555\n",
            "classify 1.30230712890625\n",
            "classify 1.34210205078125\n",
            "classify 1.25982666015625\n",
            "classify 1.3389892578125\n",
            "classify 1.2765624523162842\n",
            "0.3155893536121673\n",
            "strain 0.08524967730045319\n",
            "strain 0.2195328027009964\n",
            "strain 0.06575335562229156\n",
            "strain 0.10856117308139801\n",
            "strain 0.03566731885075569\n",
            "classify 1.33050537109375\n",
            "classify 1.2943115234375\n",
            "classify 1.32135009765625\n",
            "classify 1.2965087890625\n",
            "classify 0.9867187738418579\n",
            "0.3155893536121673\n",
            "strain 0.07692793756723404\n",
            "strain 0.16868135333061218\n",
            "strain 0.1838119477033615\n",
            "strain 0.13812872767448425\n",
            "strain 0.05484839901328087\n",
            "classify 1.26727294921875\n",
            "classify 1.35186767578125\n",
            "classify 1.264404296875\n",
            "classify 1.300537109375\n",
            "classify 1.704687476158142\n",
            "0.3269961977186312\n",
            "strain 0.11608336120843887\n",
            "strain 0.11688985675573349\n",
            "strain 0.09598537534475327\n",
            "strain 0.1705719530582428\n",
            "strain 0.2346421182155609\n",
            "classify 1.27239990234375\n",
            "classify 1.32666015625\n",
            "classify 1.29962158203125\n",
            "classify 1.29632568359375\n",
            "classify 1.490625023841858\n",
            "0.3193916349809886\n",
            "strain 0.0866156741976738\n",
            "strain 0.18137168884277344\n",
            "strain 0.06813676655292511\n",
            "strain 0.09749114513397217\n",
            "strain 0.15711398422718048\n",
            "classify 1.34283447265625\n",
            "classify 1.27197265625\n",
            "classify 1.31439208984375\n",
            "classify 1.297607421875\n",
            "classify 1.372656226158142\n",
            "0.2965779467680608\n",
            "strain 0.1471063792705536\n",
            "strain 0.10156538337469101\n",
            "strain 0.10932208597660065\n",
            "strain 0.07811354845762253\n",
            "strain 0.05269452556967735\n",
            "classify 1.25897216796875\n",
            "classify 1.3594970703125\n",
            "classify 1.3441162109375\n",
            "classify 1.2958984375\n",
            "classify 1.149999976158142\n",
            "0.2889733840304182\n",
            "strain 0.09221718460321426\n",
            "strain 0.20556975901126862\n",
            "strain 0.05963239446282387\n",
            "strain 0.1776304394006729\n",
            "strain 0.06837282329797745\n",
            "classify 1.33502197265625\n",
            "classify 1.297119140625\n",
            "classify 1.281982421875\n",
            "classify 1.32635498046875\n",
            "classify 1.239843726158142\n",
            "0.2889733840304182\n",
            "strain 0.17513403296470642\n",
            "strain 0.08701782673597336\n",
            "strain 0.1655878722667694\n",
            "strain 0.13260795176029205\n",
            "strain 0.04780028015375137\n",
            "classify 1.27020263671875\n",
            "classify 1.27374267578125\n",
            "classify 1.3812255859375\n",
            "classify 1.2955322265625\n",
            "classify 1.6515624523162842\n",
            "0.311787072243346\n",
            "strain 0.2672728896141052\n",
            "strain 0.11051706969738007\n",
            "strain 0.10054302960634232\n",
            "strain 0.0826018899679184\n",
            "strain 0.042275890707969666\n",
            "classify 1.33489990234375\n",
            "classify 1.272705078125\n",
            "classify 1.30780029296875\n",
            "classify 1.325927734375\n",
            "classify 1.2351562976837158\n",
            "0.30038022813688214\n",
            "strain 0.1141752377152443\n",
            "strain 0.08869005739688873\n",
            "strain 0.2345937341451645\n",
            "strain 0.10057855397462845\n",
            "strain 0.0552278570830822\n",
            "classify 1.254150390625\n",
            "classify 1.3192138671875\n",
            "classify 1.34454345703125\n",
            "classify 1.30072021484375\n",
            "classify 1.372656226158142\n",
            "0.2889733840304182\n",
            "strain 0.16699989140033722\n",
            "strain 0.0669255405664444\n",
            "strain 0.09116319566965103\n",
            "strain 0.12313022464513779\n",
            "strain 0.14143243432044983\n",
            "classify 1.33868408203125\n",
            "classify 1.2940673828125\n",
            "classify 1.237548828125\n",
            "classify 1.32989501953125\n",
            "classify 1.482031226158142\n",
            "0.3193916349809886\n",
            "strain 0.0963459312915802\n",
            "strain 0.10312642902135849\n",
            "strain 0.2361568957567215\n",
            "strain 0.12063577026128769\n",
            "strain 0.05096224695444107\n",
            "classify 1.30804443359375\n",
            "classify 1.32177734375\n",
            "classify 1.302001953125\n",
            "classify 1.28009033203125\n",
            "classify 1.609375\n",
            "0.33460076045627374\n",
            "strain 0.13225027918815613\n",
            "strain 0.07050556689500809\n",
            "strain 0.19171854853630066\n",
            "strain 0.15292617678642273\n",
            "strain 0.05376618728041649\n",
            "classify 1.33001708984375\n",
            "classify 1.3450927734375\n",
            "classify 1.2640380859375\n",
            "classify 1.2801513671875\n",
            "classify 1.337499976158142\n",
            "0.3269961977186312\n",
            "strain 0.13834090530872345\n",
            "strain 0.19565749168395996\n",
            "strain 0.08705552667379379\n",
            "strain 0.1262217015028\n",
            "strain 0.05650997534394264\n",
            "classify 1.27276611328125\n",
            "classify 1.3406982421875\n",
            "classify 1.33209228515625\n",
            "classify 1.258056640625\n",
            "classify 1.3406250476837158\n",
            "0.3231939163498099\n",
            "strain 0.17769354581832886\n",
            "strain 0.13402710855007172\n",
            "strain 0.07513914257287979\n",
            "strain 0.1281074434518814\n",
            "strain 0.12030352652072906\n",
            "classify 1.3046875\n",
            "classify 1.32476806640625\n",
            "classify 1.26568603515625\n",
            "classify 1.3048095703125\n",
            "classify 1.29296875\n",
            "0.30798479087452474\n",
            "strain 0.26654088497161865\n",
            "strain 0.06927494704723358\n",
            "strain 0.10410404205322266\n",
            "strain 0.16941623389720917\n",
            "strain 0.22260534763336182\n",
            "classify 1.28369140625\n",
            "classify 1.303955078125\n",
            "classify 1.31048583984375\n",
            "classify 1.3243408203125\n",
            "classify 1.220312476158142\n",
            "0.311787072243346\n",
            "strain 0.28982535004615784\n",
            "strain 0.07564476877450943\n",
            "strain 0.15509860217571259\n",
            "strain 0.1870710253715515\n",
            "strain 0.15215972065925598\n",
            "classify 1.33197021484375\n",
            "classify 1.2718505859375\n",
            "classify 1.3231201171875\n",
            "classify 1.2919921875\n",
            "classify 1.40625\n",
            "0.3041825095057034\n",
            "strain 0.06297918409109116\n",
            "strain 0.06691870093345642\n",
            "strain 0.1991700679063797\n",
            "strain 0.18829770386219025\n",
            "strain 0.10234985500574112\n",
            "classify 1.35870361328125\n",
            "classify 1.29376220703125\n",
            "classify 1.326171875\n",
            "classify 1.24658203125\n",
            "classify 1.279687523841858\n",
            "0.3155893536121673\n",
            "strain 0.11161796003580093\n",
            "strain 0.10335245728492737\n",
            "strain 0.11254557222127914\n",
            "strain 0.11390666663646698\n",
            "strain 0.08421345800161362\n",
            "classify 1.28509521484375\n",
            "classify 1.2576904296875\n",
            "classify 1.3240966796875\n",
            "classify 1.34393310546875\n",
            "classify 1.346093773841858\n",
            "0.33840304182509506\n",
            "strain 0.1475740522146225\n",
            "strain 0.0945407822728157\n",
            "strain 0.06530600786209106\n",
            "strain 0.0865183025598526\n",
            "strain 0.1656206250190735\n",
            "classify 1.28076171875\n",
            "classify 1.27362060546875\n",
            "classify 1.3514404296875\n",
            "classify 1.30029296875\n",
            "classify 1.357812523841858\n",
            "0.33840304182509506\n",
            "strain 0.1663912981748581\n",
            "strain 0.10551081597805023\n",
            "strain 0.1016412153840065\n",
            "strain 0.08693921566009521\n",
            "strain 0.05214044451713562\n",
            "classify 1.25762939453125\n",
            "classify 1.32965087890625\n",
            "classify 1.2928466796875\n",
            "classify 1.33807373046875\n",
            "classify 1.234375\n",
            "0.3193916349809886\n",
            "strain 0.10020288079977036\n",
            "strain 0.19353720545768738\n",
            "strain 0.07441134750843048\n",
            "strain 0.1644638031721115\n",
            "strain 0.08356907218694687\n",
            "classify 1.333251953125\n",
            "classify 1.2933349609375\n",
            "classify 1.29962158203125\n",
            "classify 1.3021240234375\n",
            "classify 1.138281226158142\n",
            "0.33460076045627374\n",
            "strain 0.1713712364435196\n",
            "strain 0.07946783304214478\n",
            "strain 0.1258409172296524\n",
            "strain 0.2612249553203583\n",
            "strain 0.06191006302833557\n",
            "classify 1.2955322265625\n",
            "classify 1.30340576171875\n",
            "classify 1.269775390625\n",
            "classify 1.345458984375\n",
            "classify 1.2687499523162842\n",
            "0.34220532319391633\n",
            "strain 0.10753922909498215\n",
            "strain 0.09046602249145508\n",
            "strain 0.05479889363050461\n",
            "strain 0.06665889918804169\n",
            "strain 0.06174866482615471\n",
            "classify 1.27972412109375\n",
            "classify 1.33795166015625\n",
            "classify 1.3096923828125\n",
            "classify 1.317138671875\n",
            "classify 1.0890624523162842\n",
            "0.33840304182509506\n",
            "strain 0.2561954855918884\n",
            "strain 0.13642992079257965\n",
            "strain 0.11586105078458786\n",
            "strain 0.20312747359275818\n",
            "strain 0.047537900507450104\n",
            "classify 1.30859375\n",
            "classify 1.31646728515625\n",
            "classify 1.31109619140625\n",
            "classify 1.2916259765625\n",
            "classify 1.3289062976837158\n",
            "0.3269961977186312\n",
            "strain 0.05413222312927246\n",
            "strain 0.21465760469436646\n",
            "strain 0.08829602599143982\n",
            "strain 0.14838907122612\n",
            "strain 0.07365977764129639\n",
            "classify 1.25408935546875\n",
            "classify 1.34173583984375\n",
            "classify 1.3424072265625\n",
            "classify 1.2965087890625\n",
            "classify 1.30078125\n",
            "0.33079847908745247\n",
            "strain 0.1017564907670021\n",
            "strain 0.06714460998773575\n",
            "strain 0.1518671065568924\n",
            "strain 0.24178077280521393\n",
            "strain 0.053667400032281876\n",
            "classify 1.31768798828125\n",
            "classify 1.3369140625\n",
            "classify 1.3406982421875\n",
            "classify 1.2288818359375\n",
            "classify 1.3046875\n",
            "0.3155893536121673\n",
            "strain 0.09215300530195236\n",
            "strain 0.17706851661205292\n",
            "strain 0.1243986114859581\n",
            "strain 0.24058128893375397\n",
            "strain 0.0939030572772026\n",
            "classify 1.30914306640625\n",
            "classify 1.30682373046875\n",
            "classify 1.2877197265625\n",
            "classify 1.31402587890625\n",
            "classify 1.21484375\n",
            "0.30038022813688214\n",
            "strain 0.07144469022750854\n",
            "strain 0.11512214690446854\n",
            "strain 0.17793527245521545\n",
            "strain 0.09857863187789917\n",
            "strain 0.09696424752473831\n",
            "classify 1.28179931640625\n",
            "classify 1.3182373046875\n",
            "classify 1.32672119140625\n",
            "classify 1.29962158203125\n",
            "classify 1.3953125476837158\n",
            "0.27756653992395436\n",
            "strain 0.08079978078603745\n",
            "strain 0.08352788537740707\n",
            "strain 0.19316142797470093\n",
            "strain 0.08282031118869781\n",
            "strain 0.0962681993842125\n",
            "classify 1.29766845703125\n",
            "classify 1.2803955078125\n",
            "classify 1.2840576171875\n",
            "classify 1.333984375\n",
            "classify 1.337499976158142\n",
            "0.2813688212927757\n",
            "strain 0.10428264737129211\n",
            "strain 0.08084450662136078\n",
            "strain 0.13208304345607758\n",
            "strain 0.0811731144785881\n",
            "strain 0.08012057095766068\n",
            "classify 1.3243408203125\n",
            "classify 1.2786865234375\n",
            "classify 1.3048095703125\n",
            "classify 1.29290771484375\n",
            "classify 1.3640625476837158\n",
            "0.2889733840304182\n",
            "strain 0.09116748720407486\n",
            "strain 0.09654771536588669\n",
            "strain 0.15622717142105103\n",
            "strain 0.0802818313241005\n",
            "strain 0.06602117419242859\n",
            "classify 1.37615966796875\n",
            "classify 1.27752685546875\n",
            "classify 1.28021240234375\n",
            "classify 1.2969970703125\n",
            "classify 1.396875023841858\n",
            "0.311787072243346\n",
            "strain 0.10673558712005615\n",
            "strain 0.10156986862421036\n",
            "strain 0.19677475094795227\n",
            "strain 0.09284386783838272\n",
            "strain 0.17388887703418732\n",
            "classify 1.2889404296875\n",
            "classify 1.35089111328125\n",
            "classify 1.318603515625\n",
            "classify 1.2816162109375\n",
            "classify 1.235937476158142\n",
            "0.3269961977186312\n",
            "strain 0.08978591859340668\n",
            "strain 0.10809440165758133\n",
            "strain 0.15186242759227753\n",
            "strain 0.1553765833377838\n",
            "strain 0.09037800133228302\n",
            "classify 1.391845703125\n",
            "classify 1.263916015625\n",
            "classify 1.2855224609375\n",
            "classify 1.30438232421875\n",
            "classify 1.119531273841858\n",
            "0.3269961977186312\n",
            "strain 0.08199027925729752\n",
            "strain 0.09958669543266296\n",
            "strain 0.06697236746549606\n",
            "strain 0.1043933629989624\n",
            "strain 0.03179134428501129\n",
            "classify 1.2509765625\n",
            "classify 1.32684326171875\n",
            "classify 1.35595703125\n",
            "classify 1.28692626953125\n",
            "classify 1.2859375476837158\n",
            "0.3193916349809886\n",
            "strain 0.08892317861318588\n",
            "strain 0.08499406278133392\n",
            "strain 0.15733098983764648\n",
            "strain 0.12101519107818604\n",
            "strain 0.05333999544382095\n",
            "classify 1.29058837890625\n",
            "classify 1.26849365234375\n",
            "classify 1.28790283203125\n",
            "classify 1.34393310546875\n",
            "classify 1.482812523841858\n",
            "0.2965779467680608\n",
            "strain 0.11329001933336258\n",
            "strain 0.08125722408294678\n",
            "strain 0.13007453083992004\n",
            "strain 0.09265848994255066\n",
            "strain 0.1426602005958557\n",
            "classify 1.30517578125\n",
            "classify 1.30792236328125\n",
            "classify 1.27099609375\n",
            "classify 1.33404541015625\n",
            "classify 1.2351562976837158\n",
            "0.30038022813688214\n",
            "strain 0.11942565441131592\n",
            "strain 0.2177174687385559\n",
            "strain 0.17745441198349\n",
            "strain 0.17913857102394104\n",
            "strain 0.07504396140575409\n",
            "classify 1.31939697265625\n",
            "classify 1.30755615234375\n",
            "classify 1.30816650390625\n",
            "classify 1.25823974609375\n",
            "classify 1.3312499523162842\n",
            "0.2889733840304182\n",
            "strain 0.2128896415233612\n",
            "strain 0.16546905040740967\n",
            "strain 0.13597486913204193\n",
            "strain 0.19517679512500763\n",
            "strain 0.12723204493522644\n",
            "classify 1.29412841796875\n",
            "classify 1.2947998046875\n",
            "classify 1.29608154296875\n",
            "classify 1.30474853515625\n",
            "classify 1.204687476158142\n",
            "0.29277566539923955\n",
            "strain 0.20604781806468964\n",
            "strain 0.2119305580854416\n",
            "strain 0.12051068991422653\n",
            "strain 0.09356928616762161\n",
            "strain 0.13821516931056976\n",
            "classify 1.344970703125\n",
            "classify 1.268310546875\n",
            "classify 1.33843994140625\n",
            "classify 1.26385498046875\n",
            "classify 1.201562523841858\n",
            "0.2889733840304182\n",
            "strain 0.0836935043334961\n",
            "strain 0.16458146274089813\n",
            "strain 0.2166241854429245\n",
            "strain 0.10798961669206619\n",
            "strain 0.06329672783613205\n",
            "classify 1.29315185546875\n",
            "classify 1.3111572265625\n",
            "classify 1.33465576171875\n",
            "classify 1.29449462890625\n",
            "classify 1.1828124523162842\n",
            "0.2889733840304182\n",
            "strain 0.0976148173213005\n",
            "strain 0.13829302787780762\n",
            "strain 0.15289977192878723\n",
            "strain 0.1164296567440033\n",
            "strain 0.04101276397705078\n",
            "classify 1.31768798828125\n",
            "classify 1.2886962890625\n",
            "classify 1.3389892578125\n",
            "classify 1.25732421875\n",
            "classify 1.2937500476837158\n",
            "0.3193916349809886\n",
            "strain 0.10937272757291794\n",
            "strain 0.12048343569040298\n",
            "strain 0.11239205300807953\n",
            "strain 0.20090103149414062\n",
            "strain 0.14243990182876587\n",
            "classify 1.3043212890625\n",
            "classify 1.3221435546875\n",
            "classify 1.298828125\n",
            "classify 1.292236328125\n",
            "classify 1.2546875476837158\n",
            "0.30038022813688214\n",
            "strain 0.15182998776435852\n",
            "strain 0.10810625553131104\n",
            "strain 0.10214900970458984\n",
            "strain 0.12207883596420288\n",
            "strain 0.04581185430288315\n",
            "classify 1.3489990234375\n",
            "classify 1.27069091796875\n",
            "classify 1.282470703125\n",
            "classify 1.29571533203125\n",
            "classify 1.5109374523162842\n",
            "0.33079847908745247\n",
            "strain 0.09259235113859177\n",
            "strain 0.11016242206096649\n",
            "strain 0.1382276862859726\n",
            "strain 0.2548062205314636\n",
            "strain 0.05336254835128784\n",
            "classify 1.3731689453125\n",
            "classify 1.26220703125\n",
            "classify 1.33074951171875\n",
            "classify 1.26483154296875\n",
            "classify 1.1749999523162842\n",
            "0.33460076045627374\n",
            "strain 0.1049073338508606\n",
            "strain 0.13148993253707886\n",
            "strain 0.13524718582630157\n",
            "strain 0.19405969977378845\n",
            "strain 0.047576215118169785\n",
            "classify 1.30682373046875\n",
            "classify 1.2965087890625\n",
            "classify 1.27984619140625\n",
            "classify 1.34222412109375\n",
            "classify 1.185156226158142\n",
            "0.311787072243346\n",
            "strain 0.16018716990947723\n",
            "strain 0.10737968981266022\n",
            "strain 0.13603639602661133\n",
            "strain 0.12028326839208603\n",
            "strain 0.053610559552907944\n",
            "classify 1.29632568359375\n",
            "classify 1.29180908203125\n",
            "classify 1.33685302734375\n",
            "classify 1.28204345703125\n",
            "classify 1.342187523841858\n",
            "0.3231939163498099\n",
            "strain 0.09813419729471207\n",
            "strain 0.14548254013061523\n",
            "strain 0.29584041237831116\n",
            "strain 0.07321972399950027\n",
            "strain 0.05671247839927673\n",
            "classify 1.291748046875\n",
            "classify 1.28338623046875\n",
            "classify 1.38067626953125\n",
            "classify 1.27264404296875\n",
            "classify 1.1320312023162842\n",
            "0.2965779467680608\n",
            "strain 0.24299417436122894\n",
            "strain 0.0605335459113121\n",
            "strain 0.1403653621673584\n",
            "strain 0.14354835450649261\n",
            "strain 0.06062578409910202\n",
            "classify 1.285888671875\n",
            "classify 1.31689453125\n",
            "classify 1.30792236328125\n",
            "classify 1.2845458984375\n",
            "classify 1.3359375\n",
            "0.2965779467680608\n",
            "strain 0.09413020312786102\n",
            "strain 0.14765816926956177\n",
            "strain 0.18175050616264343\n",
            "strain 0.18070419132709503\n",
            "strain 0.07068336755037308\n",
            "classify 1.32305908203125\n",
            "classify 1.2320556640625\n",
            "classify 1.32415771484375\n",
            "classify 1.3173828125\n",
            "classify 1.389062523841858\n",
            "0.2965779467680608\n",
            "strain 0.14216217398643494\n",
            "strain 0.07469235360622406\n",
            "strain 0.10540473461151123\n",
            "strain 0.11926332861185074\n",
            "strain 0.08860262483358383\n",
            "classify 1.25958251953125\n",
            "classify 1.3177490234375\n",
            "classify 1.2869873046875\n",
            "classify 1.33990478515625\n",
            "classify 1.171875\n",
            "0.2661596958174905\n",
            "strain 0.13736677169799805\n",
            "strain 0.09071659296751022\n",
            "strain 0.1545042246580124\n",
            "strain 0.07926829159259796\n",
            "strain 0.052993547171354294\n",
            "classify 1.31610107421875\n",
            "classify 1.30908203125\n",
            "classify 1.26025390625\n",
            "classify 1.31414794921875\n",
            "classify 1.2804687023162842\n",
            "0.27756653992395436\n",
            "strain 0.20674563944339752\n",
            "strain 0.1521451622247696\n",
            "strain 0.09877663105726242\n",
            "strain 0.2289218157529831\n",
            "strain 0.0721435397863388\n",
            "classify 1.3150634765625\n",
            "classify 1.285888671875\n",
            "classify 1.2811279296875\n",
            "classify 1.30181884765625\n",
            "classify 1.3203125\n",
            "0.29277566539923955\n",
            "strain 0.1415707767009735\n",
            "strain 0.20897924900054932\n",
            "strain 0.07119439542293549\n",
            "strain 0.0682554841041565\n",
            "strain 0.04862760379910469\n",
            "classify 1.3004150390625\n",
            "classify 1.27252197265625\n",
            "classify 1.34832763671875\n",
            "classify 1.27044677734375\n",
            "classify 1.265625\n",
            "0.2889733840304182\n",
            "strain 0.13574443757534027\n",
            "strain 0.11222179234027863\n",
            "strain 0.1321965903043747\n",
            "strain 0.06363081187009811\n",
            "strain 0.06922278553247452\n",
            "classify 1.28411865234375\n",
            "classify 1.2637939453125\n",
            "classify 1.3106689453125\n",
            "classify 1.3173828125\n",
            "classify 1.350000023841858\n",
            "0.2889733840304182\n",
            "strain 0.10739827156066895\n",
            "strain 0.06925342231988907\n",
            "strain 0.1569116711616516\n",
            "strain 0.06943114101886749\n",
            "strain 0.1285964399576187\n",
            "classify 1.33770751953125\n",
            "classify 1.34912109375\n",
            "classify 1.2843017578125\n",
            "classify 1.2276611328125\n",
            "classify 1.1906249523162842\n",
            "0.2965779467680608\n",
            "strain 0.1813460737466812\n",
            "strain 0.18303120136260986\n",
            "strain 0.06868264824151993\n",
            "strain 0.1707572489976883\n",
            "strain 0.040885742753744125\n",
            "classify 1.27734375\n",
            "classify 1.31689453125\n",
            "classify 1.34124755859375\n",
            "classify 1.25347900390625\n",
            "classify 1.3742187023162842\n",
            "0.2965779467680608\n",
            "strain 0.17240682244300842\n",
            "strain 0.26230934262275696\n",
            "strain 0.06795823574066162\n",
            "strain 0.055379126220941544\n",
            "strain 0.050760988146066666\n",
            "classify 1.32403564453125\n",
            "classify 1.30340576171875\n",
            "classify 1.32635498046875\n",
            "classify 1.26263427734375\n",
            "classify 1.0617187023162842\n",
            "0.30798479087452474\n",
            "strain 0.1502688080072403\n",
            "strain 0.08420564234256744\n",
            "strain 0.22357529401779175\n",
            "strain 0.08901993930339813\n",
            "strain 0.07247976213693619\n",
            "classify 1.3087158203125\n",
            "classify 1.315185546875\n",
            "classify 1.287841796875\n",
            "classify 1.29669189453125\n",
            "classify 1.298437476158142\n",
            "0.29277566539923955\n",
            "strain 0.15381097793579102\n",
            "strain 0.24403709173202515\n",
            "strain 0.11832013726234436\n",
            "strain 0.08119384199380875\n",
            "strain 0.10383475571870804\n",
            "classify 1.27276611328125\n",
            "classify 1.262939453125\n",
            "classify 1.36761474609375\n",
            "classify 1.3109130859375\n",
            "classify 1.2234375476837158\n",
            "0.29277566539923955\n",
            "strain 0.15009328722953796\n",
            "strain 0.06638172268867493\n",
            "strain 0.18894073367118835\n",
            "strain 0.16105636954307556\n",
            "strain 0.08027065545320511\n",
            "classify 1.2943115234375\n",
            "classify 1.32977294921875\n",
            "classify 1.269287109375\n",
            "classify 1.31756591796875\n",
            "classify 1.1867187023162842\n",
            "0.2889733840304182\n",
            "strain 0.12652844190597534\n",
            "strain 0.07499302923679352\n",
            "strain 0.11443784087896347\n",
            "strain 0.09064874798059464\n",
            "strain 0.08864341676235199\n",
            "classify 1.3251953125\n",
            "classify 1.2740478515625\n",
            "classify 1.2667236328125\n",
            "classify 1.3192138671875\n",
            "classify 1.600000023841858\n",
            "0.2813688212927757\n",
            "strain 0.17874374985694885\n",
            "strain 0.26467031240463257\n",
            "strain 0.06666777282953262\n",
            "strain 0.09826993942260742\n",
            "strain 0.1838786005973816\n",
            "classify 1.271484375\n",
            "classify 1.28497314453125\n",
            "classify 1.27490234375\n",
            "classify 1.37579345703125\n",
            "classify 1.1640625\n",
            "0.3231939163498099\n",
            "strain 0.17270931601524353\n",
            "strain 0.07888755947351456\n",
            "strain 0.1865854263305664\n",
            "strain 0.13615399599075317\n",
            "strain 0.05839924141764641\n",
            "classify 1.3443603515625\n",
            "classify 1.2843017578125\n",
            "classify 1.32147216796875\n",
            "classify 1.267822265625\n",
            "classify 1.34375\n",
            "0.25475285171102663\n",
            "strain 0.1375858634710312\n",
            "strain 0.09174735844135284\n",
            "strain 0.06566818058490753\n",
            "strain 0.07965297251939774\n",
            "strain 0.0375078059732914\n",
            "classify 1.30743408203125\n",
            "classify 1.30816650390625\n",
            "classify 1.31103515625\n",
            "classify 1.2938232421875\n",
            "classify 1.173437476158142\n",
            "0.26996197718631176\n",
            "strain 0.10656207799911499\n",
            "strain 0.21282687783241272\n",
            "strain 0.0997573733329773\n",
            "strain 0.09775317460298538\n",
            "strain 0.0758422240614891\n",
            "classify 1.26824951171875\n",
            "classify 1.3341064453125\n",
            "classify 1.31658935546875\n",
            "classify 1.28668212890625\n",
            "classify 1.182031273841858\n",
            "0.311787072243346\n",
            "strain 0.14853115379810333\n",
            "strain 0.16558454930782318\n",
            "strain 0.08784578740596771\n",
            "strain 0.09685035049915314\n",
            "strain 0.07198385894298553\n",
            "classify 1.27484130859375\n",
            "classify 1.27020263671875\n",
            "classify 1.3704833984375\n",
            "classify 1.2767333984375\n",
            "classify 1.4148437976837158\n",
            "0.30038022813688214\n",
            "strain 0.06898988783359528\n",
            "strain 0.07031740248203278\n",
            "strain 0.22414225339889526\n",
            "strain 0.16574068367481232\n",
            "strain 0.04844794422388077\n",
            "classify 1.309814453125\n",
            "classify 1.24658203125\n",
            "classify 1.31573486328125\n",
            "classify 1.31219482421875\n",
            "classify 1.2273437976837158\n",
            "0.28517110266159695\n",
            "strain 0.15330961346626282\n",
            "strain 0.08631887286901474\n",
            "strain 0.10744137316942215\n",
            "strain 0.19050335884094238\n",
            "strain 0.07030456513166428\n",
            "classify 1.2864990234375\n",
            "classify 1.27557373046875\n",
            "classify 1.33056640625\n",
            "classify 1.30474853515625\n",
            "classify 1.240625023841858\n",
            "0.27756653992395436\n",
            "strain 0.10080461949110031\n",
            "strain 0.10668867081403732\n",
            "strain 0.09261572360992432\n",
            "strain 0.09859546273946762\n",
            "strain 0.038860321044921875\n",
            "classify 1.292236328125\n",
            "classify 1.2813720703125\n",
            "classify 1.26629638671875\n",
            "classify 1.3514404296875\n",
            "classify 1.2742187976837158\n",
            "0.28517110266159695\n",
            "strain 0.11640041321516037\n",
            "strain 0.06919219344854355\n",
            "strain 0.13299264013767242\n",
            "strain 0.13926799595355988\n",
            "strain 0.07941190898418427\n",
            "classify 1.29248046875\n",
            "classify 1.31805419921875\n",
            "classify 1.30645751953125\n",
            "classify 1.251708984375\n",
            "classify 1.381250023841858\n",
            "0.2889733840304182\n",
            "strain 0.07671888172626495\n",
            "strain 0.18673133850097656\n",
            "strain 0.1264510154724121\n",
            "strain 0.1826447695493698\n",
            "strain 0.03188871592283249\n",
            "classify 1.29644775390625\n",
            "classify 1.33123779296875\n",
            "classify 1.2203369140625\n",
            "classify 1.338623046875\n",
            "classify 1.4453125\n",
            "0.2813688212927757\n",
            "strain 0.1826777160167694\n",
            "strain 0.11251617968082428\n",
            "strain 0.1472102254629135\n",
            "strain 0.10442882031202316\n",
            "strain 0.11853671818971634\n",
            "classify 1.3150634765625\n",
            "classify 1.25177001953125\n",
            "classify 1.29541015625\n",
            "classify 1.3428955078125\n",
            "classify 1.2335937023162842\n",
            "0.2737642585551331\n",
            "strain 0.06241723895072937\n",
            "strain 0.09451323002576828\n",
            "strain 0.159755676984787\n",
            "strain 0.11537902802228928\n",
            "strain 0.054566673934459686\n",
            "classify 1.3140869140625\n",
            "classify 1.35888671875\n",
            "classify 1.2674560546875\n",
            "classify 1.26214599609375\n",
            "classify 1.265625\n",
            "0.2889733840304182\n",
            "strain 0.09419619292020798\n",
            "strain 0.10566318780183792\n",
            "strain 0.07628471404314041\n",
            "strain 0.22186097502708435\n",
            "strain 0.04382633417844772\n",
            "classify 1.29541015625\n",
            "classify 1.28057861328125\n",
            "classify 1.31884765625\n",
            "classify 1.308349609375\n",
            "classify 1.2468750476837158\n",
            "0.2813688212927757\n",
            "strain 0.07841067016124725\n",
            "strain 0.058549582958221436\n",
            "strain 0.14300447702407837\n",
            "strain 0.09858646988868713\n",
            "strain 0.14075808227062225\n",
            "classify 1.2755126953125\n",
            "classify 1.32354736328125\n",
            "classify 1.31549072265625\n",
            "classify 1.276611328125\n",
            "classify 1.244531273841858\n",
            "0.27756653992395436\n",
            "strain 0.0920277088880539\n",
            "strain 0.18146079778671265\n",
            "strain 0.07775422185659409\n",
            "strain 0.09684766829013824\n",
            "strain 0.10911470651626587\n",
            "classify 1.34228515625\n",
            "classify 1.25714111328125\n",
            "classify 1.29339599609375\n",
            "classify 1.31005859375\n",
            "classify 1.295312523841858\n",
            "0.27756653992395436\n",
            "strain 0.17373858392238617\n",
            "strain 0.06462718546390533\n",
            "strain 0.114274762570858\n",
            "strain 0.1279589682817459\n",
            "strain 0.03764508292078972\n",
            "classify 1.25885009765625\n",
            "classify 1.31341552734375\n",
            "classify 1.34368896484375\n",
            "classify 1.2781982421875\n",
            "classify 1.4328124523162842\n",
            "0.28517110266159695\n",
            "strain 0.060501180589199066\n",
            "strain 0.07910829037427902\n",
            "strain 0.05755650997161865\n",
            "strain 0.11355552822351456\n",
            "strain 0.083816759288311\n",
            "classify 1.3193359375\n",
            "classify 1.33984375\n",
            "classify 1.2210693359375\n",
            "classify 1.31146240234375\n",
            "classify 1.1671874523162842\n",
            "0.2889733840304182\n",
            "strain 0.09900449216365814\n",
            "strain 0.0653083324432373\n",
            "strain 0.14164157211780548\n",
            "strain 0.05413629859685898\n",
            "strain 0.05881018191576004\n",
            "classify 1.2720947265625\n",
            "classify 1.30419921875\n",
            "classify 1.3126220703125\n",
            "classify 1.318359375\n",
            "classify 1.1515624523162842\n",
            "0.27756653992395436\n",
            "strain 0.1685502529144287\n",
            "strain 0.08990650624036789\n",
            "strain 0.19946852326393127\n",
            "strain 0.07752551138401031\n",
            "strain 0.06579338014125824\n",
            "classify 1.3240966796875\n",
            "classify 1.2442626953125\n",
            "classify 1.28546142578125\n",
            "classify 1.32244873046875\n",
            "classify 1.3640625476837158\n",
            "0.28517110266159695\n",
            "strain 0.11174729466438293\n",
            "strain 0.08645634353160858\n",
            "strain 0.14739243686199188\n",
            "strain 0.3229139745235443\n",
            "strain 0.06936700642108917\n",
            "classify 1.323974609375\n",
            "classify 1.284423828125\n",
            "classify 1.32672119140625\n",
            "classify 1.26214599609375\n",
            "classify 1.208593726158142\n",
            "0.29277566539923955\n",
            "strain 0.1115746796131134\n",
            "strain 0.1035434678196907\n",
            "strain 0.14515894651412964\n",
            "strain 0.10930385440587997\n",
            "strain 0.1668015718460083\n",
            "classify 1.27313232421875\n",
            "classify 1.2823486328125\n",
            "classify 1.334228515625\n",
            "classify 1.3258056640625\n",
            "classify 1.2335937023162842\n",
            "0.3155893536121673\n",
            "strain 0.18148912489414215\n",
            "strain 0.099944569170475\n",
            "strain 0.07864038646221161\n",
            "strain 0.11745334416627884\n",
            "strain 0.08471719920635223\n",
            "classify 1.30462646484375\n",
            "classify 1.3035888671875\n",
            "classify 1.3038330078125\n",
            "classify 1.285888671875\n",
            "classify 1.3234374523162842\n",
            "0.3231939163498099\n",
            "strain 0.08470525592565536\n",
            "strain 0.18721836805343628\n",
            "strain 0.09050053358078003\n",
            "strain 0.06238149106502533\n",
            "strain 0.09303765743970871\n",
            "classify 1.289306640625\n",
            "classify 1.259033203125\n",
            "classify 1.30120849609375\n",
            "classify 1.35650634765625\n",
            "classify 1.3484375476837158\n",
            "0.311787072243346\n",
            "strain 0.06896481662988663\n",
            "strain 0.18790103495121002\n",
            "strain 0.2580941617488861\n",
            "strain 0.19498175382614136\n",
            "strain 0.057289425283670425\n",
            "classify 1.26904296875\n",
            "classify 1.30657958984375\n",
            "classify 1.34442138671875\n",
            "classify 1.3096923828125\n",
            "classify 1.2335937023162842\n",
            "0.3231939163498099\n",
            "strain 0.08126871287822723\n",
            "strain 0.19614864885807037\n",
            "strain 0.12286506593227386\n",
            "strain 0.16840621829032898\n",
            "strain 0.16141174733638763\n",
            "classify 1.33148193359375\n",
            "classify 1.268798828125\n",
            "classify 1.312255859375\n",
            "classify 1.325439453125\n",
            "classify 1.1984374523162842\n",
            "0.3193916349809886\n",
            "strain 0.0714191198348999\n",
            "strain 0.18151816725730896\n",
            "strain 0.09829068928956985\n",
            "strain 0.11064252257347107\n",
            "strain 0.0494358129799366\n",
            "classify 1.30438232421875\n",
            "classify 1.2667236328125\n",
            "classify 1.3316650390625\n",
            "classify 1.36578369140625\n",
            "classify 1.3328125476837158\n",
            "0.3155893536121673\n",
            "strain 0.05735361576080322\n",
            "strain 0.16770164668560028\n",
            "strain 0.07555638998746872\n",
            "strain 0.16728505492210388\n",
            "strain 0.08593152463436127\n",
            "classify 1.36419677734375\n",
            "classify 1.33441162109375\n",
            "classify 1.2564697265625\n",
            "classify 1.31378173828125\n",
            "classify 1.146875023841858\n",
            "0.3193916349809886\n",
            "strain 0.18750092387199402\n",
            "strain 0.10291467607021332\n",
            "strain 0.12733107805252075\n",
            "strain 0.13657453656196594\n",
            "strain 0.14300405979156494\n",
            "classify 1.4100341796875\n",
            "classify 1.2958984375\n",
            "classify 1.27886962890625\n",
            "classify 1.2808837890625\n",
            "classify 1.295312523841858\n",
            "0.3155893536121673\n",
            "strain 0.21936964988708496\n",
            "strain 0.11402392387390137\n",
            "strain 0.09470781683921814\n",
            "strain 0.13615721464157104\n",
            "strain 0.263520747423172\n",
            "classify 1.25299072265625\n",
            "classify 1.31622314453125\n",
            "classify 1.404052734375\n",
            "classify 1.283203125\n",
            "classify 1.310937523841858\n",
            "0.3193916349809886\n",
            "strain 0.19302651286125183\n",
            "strain 0.16973501443862915\n",
            "strain 0.16742964088916779\n",
            "strain 0.18546569347381592\n",
            "strain 0.054226912558078766\n",
            "classify 1.30267333984375\n",
            "classify 1.30426025390625\n",
            "classify 1.34429931640625\n",
            "classify 1.28607177734375\n",
            "classify 1.2507812976837158\n",
            "0.3269961977186312\n",
            "strain 0.107259601354599\n",
            "strain 0.05841989070177078\n",
            "strain 0.18382038176059723\n",
            "strain 0.14504851400852203\n",
            "strain 0.1672120839357376\n",
            "classify 1.341796875\n",
            "classify 1.3363037109375\n",
            "classify 1.29339599609375\n",
            "classify 1.2529296875\n",
            "classify 1.025781273841858\n",
            "0.3269961977186312\n",
            "strain 0.11220292747020721\n",
            "strain 0.20336425304412842\n",
            "strain 0.10183355957269669\n",
            "strain 0.06946583837270737\n",
            "strain 0.08132714778184891\n",
            "classify 1.26202392578125\n",
            "classify 1.305908203125\n",
            "classify 1.3294677734375\n",
            "classify 1.34771728515625\n",
            "classify 1.150781273841858\n",
            "0.33079847908745247\n",
            "strain 0.07869980484247208\n",
            "strain 0.1132185086607933\n",
            "strain 0.09874335676431656\n",
            "strain 0.16305916011333466\n",
            "strain 0.1197209283709526\n",
            "classify 1.27435302734375\n",
            "classify 1.30865478515625\n",
            "classify 1.32440185546875\n",
            "classify 1.31353759765625\n",
            "classify 1.317968726158142\n",
            "0.33079847908745247\n",
            "strain 0.13170690834522247\n",
            "strain 0.10465864837169647\n",
            "strain 0.07092178612947464\n",
            "strain 0.17208170890808105\n",
            "strain 0.08706416189670563\n",
            "classify 1.29638671875\n",
            "classify 1.23382568359375\n",
            "classify 1.29132080078125\n",
            "classify 1.3896484375\n",
            "classify 1.135156273841858\n",
            "0.3193916349809886\n",
            "strain 0.11080483347177505\n",
            "strain 0.18104740977287292\n",
            "strain 0.13482728600502014\n",
            "strain 0.20590120553970337\n",
            "strain 0.0314033068716526\n",
            "classify 1.316162109375\n",
            "classify 1.34576416015625\n",
            "classify 1.27545166015625\n",
            "classify 1.303466796875\n",
            "classify 1.046875\n",
            "0.3231939163498099\n",
            "strain 0.1399059295654297\n",
            "strain 0.2798558473587036\n",
            "strain 0.11703011393547058\n",
            "strain 0.08930491656064987\n",
            "strain 0.11134927719831467\n",
            "classify 1.2763671875\n",
            "classify 1.3511962890625\n",
            "classify 1.30792236328125\n",
            "classify 1.297607421875\n",
            "classify 1.154687523841858\n",
            "0.30798479087452474\n",
            "strain 0.1166425347328186\n",
            "strain 0.14662273228168488\n",
            "strain 0.09253218024969101\n",
            "strain 0.1307079941034317\n",
            "strain 0.07467760145664215\n",
            "classify 1.364013671875\n",
            "classify 1.2982177734375\n",
            "classify 1.29461669921875\n",
            "classify 1.237060546875\n",
            "classify 1.7109375\n",
            "0.28517110266159695\n",
            "strain 0.15383391082286835\n",
            "strain 0.18818822503089905\n",
            "strain 0.13381239771842957\n",
            "strain 0.12258363515138626\n",
            "strain 0.12375640869140625\n",
            "classify 1.28070068359375\n",
            "classify 1.27337646484375\n",
            "classify 1.3729248046875\n",
            "classify 1.27752685546875\n",
            "classify 1.2390625476837158\n",
            "0.29277566539923955\n",
            "strain 0.15644295513629913\n",
            "strain 0.12533649802207947\n",
            "strain 0.08283790946006775\n",
            "strain 0.12963232398033142\n",
            "strain 0.07174182683229446\n",
            "classify 1.264892578125\n",
            "classify 1.31048583984375\n",
            "classify 1.3062744140625\n",
            "classify 1.3328857421875\n",
            "classify 1.3234374523162842\n",
            "0.27756653992395436\n",
            "strain 0.12705354392528534\n",
            "strain 0.08812879770994186\n",
            "strain 0.1940811425447464\n",
            "strain 0.1357039362192154\n",
            "strain 0.07737140357494354\n",
            "classify 1.2584228515625\n",
            "classify 1.29107666015625\n",
            "classify 1.31787109375\n",
            "classify 1.31976318359375\n",
            "classify 1.282812476158142\n",
            "0.29277566539923955\n",
            "strain 0.08657217025756836\n",
            "strain 0.16398562490940094\n",
            "strain 0.11957557499408722\n",
            "strain 0.236897274851799\n",
            "strain 0.04344449192285538\n",
            "classify 1.234619140625\n",
            "classify 1.2447509765625\n",
            "classify 1.3360595703125\n",
            "classify 1.3731689453125\n",
            "classify 1.2296874523162842\n",
            "0.30038022813688214\n",
            "strain 0.12746165692806244\n",
            "strain 0.19774524867534637\n",
            "strain 0.09829974174499512\n",
            "strain 0.10890045017004013\n",
            "strain 0.04791240021586418\n",
            "classify 1.27252197265625\n",
            "classify 1.29345703125\n",
            "classify 1.3436279296875\n",
            "classify 1.29437255859375\n",
            "classify 1.0710937976837158\n",
            "0.3155893536121673\n",
            "strain 0.0664345845580101\n",
            "strain 0.08100366592407227\n",
            "strain 0.08108113706111908\n",
            "strain 0.14379777014255524\n",
            "strain 0.08062390238046646\n",
            "classify 1.340087890625\n",
            "classify 1.225341796875\n",
            "classify 1.27996826171875\n",
            "classify 1.32769775390625\n",
            "classify 1.431249976158142\n",
            "0.30798479087452474\n",
            "strain 0.06177446246147156\n",
            "strain 0.11661367118358612\n",
            "strain 0.1562403291463852\n",
            "strain 0.0688018947839737\n",
            "strain 0.17054636776447296\n",
            "classify 1.301513671875\n",
            "classify 1.33380126953125\n",
            "classify 1.3016357421875\n",
            "classify 1.25518798828125\n",
            "classify 1.6085937023162842\n",
            "0.2813688212927757\n",
            "strain 0.20435500144958496\n",
            "strain 0.14572913944721222\n",
            "strain 0.20984433591365814\n",
            "strain 0.15547342598438263\n",
            "strain 0.14563249051570892\n",
            "classify 1.32318115234375\n",
            "classify 1.27252197265625\n",
            "classify 1.3236083984375\n",
            "classify 1.296630859375\n",
            "classify 1.2453124523162842\n",
            "0.30038022813688214\n",
            "strain 0.15546806156635284\n",
            "strain 0.05822330340743065\n",
            "strain 0.08038648217916489\n",
            "strain 0.1440064162015915\n",
            "strain 0.13641902804374695\n",
            "classify 1.29150390625\n",
            "classify 1.35821533203125\n",
            "classify 1.33184814453125\n",
            "classify 1.24072265625\n",
            "classify 1.217187523841858\n",
            "0.30798479087452474\n",
            "strain 0.13748562335968018\n",
            "strain 0.17352810502052307\n",
            "strain 0.18019744753837585\n",
            "strain 0.20024558901786804\n",
            "strain 0.1448134183883667\n",
            "classify 1.30804443359375\n",
            "classify 1.3382568359375\n",
            "classify 1.2774658203125\n",
            "classify 1.2694091796875\n",
            "classify 1.3468749523162842\n",
            "0.26996197718631176\n",
            "strain 0.0854971632361412\n",
            "strain 0.08938541263341904\n",
            "strain 0.19743794202804565\n",
            "strain 0.11679486930370331\n",
            "strain 0.06691914051771164\n",
            "classify 1.284423828125\n",
            "classify 1.2181396484375\n",
            "classify 1.36419677734375\n",
            "classify 1.34149169921875\n",
            "classify 1.4484374523162842\n",
            "0.2889733840304182\n",
            "strain 0.11436593532562256\n",
            "strain 0.08385075628757477\n",
            "strain 0.14652609825134277\n",
            "strain 0.07794661819934845\n",
            "strain 0.10342952609062195\n",
            "classify 1.306396484375\n",
            "classify 1.299072265625\n",
            "classify 1.33160400390625\n",
            "classify 1.25201416015625\n",
            "classify 1.639062523841858\n",
            "0.28517110266159695\n",
            "strain 0.08681628853082657\n",
            "strain 0.07861651480197906\n",
            "strain 0.09513365477323532\n",
            "strain 0.10871722549200058\n",
            "strain 0.09401121735572815\n",
            "classify 1.276611328125\n",
            "classify 1.2960205078125\n",
            "classify 1.33160400390625\n",
            "classify 1.321044921875\n",
            "classify 1.107031226158142\n",
            "0.28517110266159695\n",
            "strain 0.0994851291179657\n",
            "strain 0.09064497798681259\n",
            "strain 0.1876496523618698\n",
            "strain 0.13945086300373077\n",
            "strain 0.10643071681261063\n",
            "classify 1.3253173828125\n",
            "classify 1.248779296875\n",
            "classify 1.32342529296875\n",
            "classify 1.3209228515625\n",
            "classify 1.2625000476837158\n",
            "0.2737642585551331\n",
            "strain 0.13158102333545685\n",
            "strain 0.11366477608680725\n",
            "strain 0.14192044734954834\n",
            "strain 0.06293978542089462\n",
            "strain 0.14480870962142944\n",
            "classify 1.32501220703125\n",
            "classify 1.317626953125\n",
            "classify 1.26531982421875\n",
            "classify 1.30474853515625\n",
            "classify 1.4375\n",
            "0.2737642585551331\n",
            "strain 0.076148122549057\n",
            "strain 0.15862220525741577\n",
            "strain 0.08561692386865616\n",
            "strain 0.08660858869552612\n",
            "strain 0.04999187961220741\n",
            "classify 1.25628662109375\n",
            "classify 1.3109130859375\n",
            "classify 1.3388671875\n",
            "classify 1.27923583984375\n",
            "classify 1.389062523841858\n",
            "0.2737642585551331\n",
            "strain 0.2856452763080597\n",
            "strain 0.1676592230796814\n",
            "strain 0.18023452162742615\n",
            "strain 0.09173575043678284\n",
            "strain 0.1156943067908287\n",
            "classify 1.3408203125\n",
            "classify 1.3433837890625\n",
            "classify 1.29827880859375\n",
            "classify 1.24212646484375\n",
            "classify 1.3171875476837158\n",
            "0.2813688212927757\n",
            "strain 0.07514338195323944\n",
            "strain 0.1363726705312729\n",
            "strain 0.16473740339279175\n",
            "strain 0.0981021374464035\n",
            "strain 0.045505158603191376\n",
            "classify 1.28485107421875\n",
            "classify 1.313720703125\n",
            "classify 1.35150146484375\n",
            "classify 1.292236328125\n",
            "classify 1.296875\n",
            "0.27756653992395436\n",
            "strain 0.15518757700920105\n",
            "strain 0.16299092769622803\n",
            "strain 0.11412564665079117\n",
            "strain 0.05173451825976372\n",
            "strain 0.06138001009821892\n",
            "classify 1.327880859375\n",
            "classify 1.31292724609375\n",
            "classify 1.31048583984375\n",
            "classify 1.2845458984375\n",
            "classify 1.2109375\n",
            "0.27756653992395436\n",
            "strain 0.09013576060533524\n",
            "strain 0.15001580119132996\n",
            "strain 0.13948652148246765\n",
            "strain 0.09827721118927002\n",
            "strain 0.17450478672981262\n",
            "classify 1.2491455078125\n",
            "classify 1.304931640625\n",
            "classify 1.3330078125\n",
            "classify 1.32965087890625\n",
            "classify 1.263281226158142\n",
            "0.27756653992395436\n",
            "strain 0.2762441635131836\n",
            "strain 0.13746556639671326\n",
            "strain 0.160292387008667\n",
            "strain 0.16646884381771088\n",
            "strain 0.0810273066163063\n",
            "classify 1.30780029296875\n",
            "classify 1.3570556640625\n",
            "classify 1.2742919921875\n",
            "classify 1.29058837890625\n",
            "classify 1.19921875\n",
            "0.28517110266159695\n",
            "strain 0.13370519876480103\n",
            "strain 0.08462536334991455\n",
            "strain 0.223680779337883\n",
            "strain 0.16319218277931213\n",
            "strain 0.048766352236270905\n",
            "classify 1.2830810546875\n",
            "classify 1.30572509765625\n",
            "classify 1.3470458984375\n",
            "classify 1.30859375\n",
            "classify 1.197656273841858\n",
            "0.27756653992395436\n",
            "strain 0.10378637909889221\n",
            "strain 0.18948225677013397\n",
            "strain 0.20762309432029724\n",
            "strain 0.09693789482116699\n",
            "strain 0.0946177989244461\n",
            "classify 1.260986328125\n",
            "classify 1.30743408203125\n",
            "classify 1.31365966796875\n",
            "classify 1.31243896484375\n",
            "classify 1.4796874523162842\n",
            "0.2813688212927757\n",
            "strain 0.09088554978370667\n",
            "strain 0.06781464070081711\n",
            "strain 0.10398312658071518\n",
            "strain 0.2743707299232483\n",
            "strain 0.10239598900079727\n",
            "classify 1.28448486328125\n",
            "classify 1.32073974609375\n",
            "classify 1.32574462890625\n",
            "classify 1.25396728515625\n",
            "classify 1.546875\n",
            "0.33079847908745247\n",
            "strain 0.12318143993616104\n",
            "strain 0.1218632161617279\n",
            "strain 0.2043253481388092\n",
            "strain 0.11790251731872559\n",
            "strain 0.06756572425365448\n",
            "classify 1.3106689453125\n",
            "classify 1.3104248046875\n",
            "classify 1.29144287109375\n",
            "classify 1.3101806640625\n",
            "classify 1.282812476158142\n",
            "0.29277566539923955\n",
            "strain 0.10491947084665298\n",
            "strain 0.09209999442100525\n",
            "strain 0.16879239678382874\n",
            "strain 0.21685947477817535\n",
            "strain 0.06658656895160675\n",
            "classify 1.3258056640625\n",
            "classify 1.2933349609375\n",
            "classify 1.28326416015625\n",
            "classify 1.28656005859375\n",
            "classify 1.532812476158142\n",
            "0.28517110266159695\n",
            "strain 0.09817039966583252\n",
            "strain 0.15202131867408752\n",
            "strain 0.10166284441947937\n",
            "strain 0.1911919265985489\n",
            "strain 0.07297258079051971\n",
            "classify 1.31195068359375\n",
            "classify 1.25567626953125\n",
            "classify 1.34716796875\n",
            "classify 1.2999267578125\n",
            "classify 1.150781273841858\n",
            "0.2965779467680608\n",
            "strain 0.07146963477134705\n",
            "strain 0.19849978387355804\n",
            "strain 0.22256141901016235\n",
            "strain 0.13420481979846954\n",
            "strain 0.07037293165922165\n",
            "classify 1.28369140625\n",
            "classify 1.34881591796875\n",
            "classify 1.2745361328125\n",
            "classify 1.31768798828125\n",
            "classify 1.365625023841858\n",
            "0.3269961977186312\n",
            "strain 0.11302988976240158\n",
            "strain 0.16440916061401367\n",
            "strain 0.18400239944458008\n",
            "strain 0.1649230420589447\n",
            "strain 0.04382885619997978\n",
            "classify 1.27581787109375\n",
            "classify 1.350341796875\n",
            "classify 1.31591796875\n",
            "classify 1.2901611328125\n",
            "classify 1.2999999523162842\n",
            "0.33460076045627374\n",
            "strain 0.07967481017112732\n",
            "strain 0.11418846249580383\n",
            "strain 0.06222395598888397\n",
            "strain 0.12818500399589539\n",
            "strain 0.09745626151561737\n",
            "classify 1.325927734375\n",
            "classify 1.30072021484375\n",
            "classify 1.294677734375\n",
            "classify 1.28143310546875\n",
            "classify 1.2531249523162842\n",
            "0.311787072243346\n",
            "strain 0.093923419713974\n",
            "strain 0.14977914094924927\n",
            "strain 0.14650800824165344\n",
            "strain 0.14171864092350006\n",
            "strain 0.3578629493713379\n",
            "classify 1.29937744140625\n",
            "classify 1.27728271484375\n",
            "classify 1.33685302734375\n",
            "classify 1.32769775390625\n",
            "classify 1.200781226158142\n",
            "0.35361216730038025\n",
            "strain 0.1354016810655594\n",
            "strain 0.14294563233852386\n",
            "strain 0.07702280580997467\n",
            "strain 0.1784709095954895\n",
            "strain 0.08158058673143387\n",
            "classify 1.28173828125\n",
            "classify 1.32733154296875\n",
            "classify 1.301025390625\n",
            "classify 1.32354736328125\n",
            "classify 1.2687499523162842\n",
            "0.34600760456273766\n",
            "strain 0.1494675576686859\n",
            "strain 0.08135474473237991\n",
            "strain 0.13705064356327057\n",
            "strain 0.07890727370977402\n",
            "strain 0.07431761920452118\n",
            "classify 1.30987548828125\n",
            "classify 1.27777099609375\n",
            "classify 1.30908203125\n",
            "classify 1.30950927734375\n",
            "classify 1.1203124523162842\n",
            "0.28517110266159695\n",
            "strain 0.12405025959014893\n",
            "strain 0.08684640377759933\n",
            "strain 0.1673799306154251\n",
            "strain 0.12293470650911331\n",
            "strain 0.07136347889900208\n",
            "classify 1.36468505859375\n",
            "classify 1.30413818359375\n",
            "classify 1.27874755859375\n",
            "classify 1.2596435546875\n",
            "classify 1.271093726158142\n",
            "0.30798479087452474\n",
            "strain 0.090477854013443\n",
            "strain 0.11381060630083084\n",
            "strain 0.11108970642089844\n",
            "strain 0.19264192879199982\n",
            "strain 0.1920713186264038\n",
            "classify 1.32470703125\n",
            "classify 1.26318359375\n",
            "classify 1.359375\n",
            "classify 1.28399658203125\n",
            "classify 1.17578125\n",
            "0.3269961977186312\n",
            "strain 0.05899129435420036\n",
            "strain 0.10536116361618042\n",
            "strain 0.19598276913166046\n",
            "strain 0.0990709513425827\n",
            "strain 0.11777227371931076\n",
            "classify 1.3338623046875\n",
            "classify 1.34600830078125\n",
            "classify 1.245849609375\n",
            "classify 1.2685546875\n",
            "classify 1.404687523841858\n",
            "0.3231939163498099\n",
            "strain 0.20262570679187775\n",
            "strain 0.10093114525079727\n",
            "strain 0.06972014158964157\n",
            "strain 0.1279100924730301\n",
            "strain 0.16780675947666168\n",
            "classify 1.3328857421875\n",
            "classify 1.2984619140625\n",
            "classify 1.27716064453125\n",
            "classify 1.33245849609375\n",
            "classify 1.036718726158142\n",
            "0.3155893536121673\n",
            "strain 0.16003330051898956\n",
            "strain 0.10663706809282303\n",
            "strain 0.09338542073965073\n",
            "strain 0.09864462912082672\n",
            "strain 0.0350780189037323\n",
            "classify 1.280029296875\n",
            "classify 1.31561279296875\n",
            "classify 1.328125\n",
            "classify 1.25970458984375\n",
            "classify 1.779687523841858\n",
            "0.3041825095057034\n",
            "strain 0.17733298242092133\n",
            "strain 0.18977795541286469\n",
            "strain 0.10044852644205093\n",
            "strain 0.08300678431987762\n",
            "strain 0.04481453821063042\n",
            "classify 1.34307861328125\n",
            "classify 1.34063720703125\n",
            "classify 1.274169921875\n",
            "classify 1.267822265625\n",
            "classify 1.189062476158142\n",
            "0.311787072243346\n",
            "strain 0.110837422311306\n",
            "strain 0.13200218975543976\n",
            "strain 0.177186980843544\n",
            "strain 0.15117450058460236\n",
            "strain 0.04550100862979889\n",
            "classify 1.319091796875\n",
            "classify 1.32574462890625\n",
            "classify 1.26446533203125\n",
            "classify 1.29931640625\n",
            "classify 1.342187523841858\n",
            "0.3193916349809886\n",
            "strain 0.15472450852394104\n",
            "strain 0.06850540637969971\n",
            "strain 0.17220696806907654\n",
            "strain 0.09212004393339157\n",
            "strain 0.09181630611419678\n",
            "classify 1.3255615234375\n",
            "classify 1.31494140625\n",
            "classify 1.27288818359375\n",
            "classify 1.2890625\n",
            "classify 1.3515625\n",
            "0.3155893536121673\n",
            "strain 0.12405791133642197\n",
            "strain 0.10825430601835251\n",
            "strain 0.13474974036216736\n",
            "strain 0.11891813576221466\n",
            "strain 0.07375147193670273\n",
            "classify 1.32745361328125\n",
            "classify 1.28350830078125\n",
            "classify 1.32403564453125\n",
            "classify 1.2706298828125\n",
            "classify 1.318750023841858\n",
            "0.311787072243346\n",
            "strain 0.07881727814674377\n",
            "strain 0.11403508484363556\n",
            "strain 0.09494393318891525\n",
            "strain 0.12643462419509888\n",
            "strain 0.04393817484378815\n",
            "classify 1.312255859375\n",
            "classify 1.363525390625\n",
            "classify 1.27484130859375\n",
            "classify 1.27069091796875\n",
            "classify 1.225000023841858\n",
            "0.3193916349809886\n",
            "strain 0.19125594198703766\n",
            "strain 0.1727287471294403\n",
            "strain 0.06562932580709457\n",
            "strain 0.10893026739358902\n",
            "strain 0.08324351906776428\n",
            "classify 1.3369140625\n",
            "classify 1.311767578125\n",
            "classify 1.2896728515625\n",
            "classify 1.28857421875\n",
            "classify 1.4226562976837158\n",
            "0.311787072243346\n",
            "strain 0.16531388461589813\n",
            "strain 0.13079683482646942\n",
            "strain 0.17007920145988464\n",
            "strain 0.09461653232574463\n",
            "strain 0.05459035933017731\n",
            "classify 1.30560302734375\n",
            "classify 1.27117919921875\n",
            "classify 1.3055419921875\n",
            "classify 1.3282470703125\n",
            "classify 1.5031249523162842\n",
            "0.311787072243346\n",
            "strain 0.21896126866340637\n",
            "strain 0.07374559342861176\n",
            "strain 0.09100200235843658\n",
            "strain 0.19144916534423828\n",
            "strain 0.24874985218048096\n",
            "classify 1.30242919921875\n",
            "classify 1.2928466796875\n",
            "classify 1.3197021484375\n",
            "classify 1.30242919921875\n",
            "classify 1.2101562023162842\n",
            "0.28517110266159695\n",
            "strain 0.2550983726978302\n",
            "strain 0.11303772777318954\n",
            "strain 0.10699006170034409\n",
            "strain 0.2074543982744217\n",
            "strain 0.03463597595691681\n",
            "classify 1.313720703125\n",
            "classify 1.317626953125\n",
            "classify 1.34710693359375\n",
            "classify 1.29571533203125\n",
            "classify 1.1437499523162842\n",
            "0.30038022813688214\n",
            "strain 0.2930501103401184\n",
            "strain 0.07948943227529526\n",
            "strain 0.21911340951919556\n",
            "strain 0.21504348516464233\n",
            "strain 0.35262325406074524\n",
            "classify 1.30975341796875\n",
            "classify 1.27667236328125\n",
            "classify 1.337646484375\n",
            "classify 1.29705810546875\n",
            "classify 1.6171875\n",
            "0.30798479087452474\n",
            "strain 0.16781732439994812\n",
            "strain 0.20527677237987518\n",
            "strain 0.09485708922147751\n",
            "strain 0.11717614531517029\n",
            "strain 0.2301466315984726\n",
            "classify 1.31915283203125\n",
            "classify 1.32373046875\n",
            "classify 1.277099609375\n",
            "classify 1.34234619140625\n",
            "classify 1.244531273841858\n",
            "0.311787072243346\n",
            "strain 0.20792405307292938\n",
            "strain 0.19807904958724976\n",
            "strain 0.2089943289756775\n",
            "strain 0.0807943195104599\n",
            "strain 0.04510127753019333\n",
            "classify 1.29791259765625\n",
            "classify 1.36474609375\n",
            "classify 1.30352783203125\n",
            "classify 1.27728271484375\n",
            "classify 1.1007812023162842\n",
            "0.30038022813688214\n",
            "strain 0.12252621352672577\n",
            "strain 0.0642538070678711\n",
            "strain 0.11431456357240677\n",
            "strain 0.12916482985019684\n",
            "strain 0.049573495984077454\n",
            "classify 1.3013916015625\n",
            "classify 1.27099609375\n",
            "classify 1.32794189453125\n",
            "classify 1.32684326171875\n",
            "classify 1.321874976158142\n",
            "0.2965779467680608\n",
            "strain 0.19284102320671082\n",
            "strain 0.11645533889532089\n",
            "strain 0.24247144162654877\n",
            "strain 0.0867520123720169\n",
            "strain 0.10564737766981125\n",
            "classify 1.27801513671875\n",
            "classify 1.25592041015625\n",
            "classify 1.3419189453125\n",
            "classify 1.360595703125\n",
            "classify 1.2960937023162842\n",
            "0.30798479087452474\n",
            "strain 0.17427028715610504\n"
          ]
        }
      ],
      "source": [
        "# @title strain ctrain test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "# name = 'violet'\n",
        "# train_summary_writer = tf.summary.create_file_writer('logs/'+name+'/train')\n",
        "# test_summary_writer = tf.summary.create_file_writer('logs/'+name+'/test')\n",
        "\n",
        "def strain(model, dataloader, optim, scheduler=None):\n",
        "    model.train()\n",
        "    for i, (x, _) in enumerate(dataloader):\n",
        "        # x = x[...,1:].to(device).to(torch.bfloat16) # for wisdm?\n",
        "        x = x.to(device).to(torch.bfloat16)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            loss = model.loss(x)\n",
        "\n",
        "            # repr_loss, std_loss, cov_loss = model.loss(x)\n",
        "            # loss = model.sim_coeff * repr_loss + model.std_coeff * std_loss + model.cov_coeff * cov_loss\n",
        "\n",
        "        optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # total_norm = 0\n",
        "        # for p in model.parameters(): total_norm += p.grad.data.norm(2).item() ** 2\n",
        "        # total_norm = total_norm**.5\n",
        "        # print('total_norm', total_norm)\n",
        "        # for p in list(filter(lambda p: p.grad is not None, model.parameters())):\n",
        "        #     print(p.grad.data.norm(2).item())\n",
        "        # print(\"max grad norm\", max([p.grad.data.norm(2).item() for p in list(filter(lambda p: p.grad is not None, model.parameters()))]))\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 5) # 0.5\n",
        "\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            m=0.999 # 0.99 m = next(momentum_scheduler)\n",
        "            for param_q, param_k in zip(model.student.parameters(), model.teacher.parameters()):\n",
        "                param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "\n",
        "        if scheduler is not None: scheduler.step()\n",
        "        print(\"strain\",loss.item())\n",
        "        # for param in seq_jepa.context_encoder.cls: print(param.data)\n",
        "        # for param in seq_jepa.predicter.cls: print(param.data)\n",
        "        try: wandb.log({\"loss\": loss.item(), \"repr/I\": repr_loss.item(), \"std/V\": std_loss.item(), \"cov/C\": cov_loss.item()})\n",
        "        # try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        # with train_summary_writer.as_default(): tf.summary.scalar('strain', loss.item(), step=i)\n",
        "        if i>=500: break\n",
        "\n",
        "\n",
        "def ctrain(model, classifier, dataloader, coptim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.eval()\n",
        "    classifier.train()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        # x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        # x, y = x[...,1:].to(device).to(torch.bfloat16), y[1].to(device) # [batch, ] # (id, activity)\n",
        "        x, y = x.to(device).to(torch.bfloat16), y.to(device) # [batch, ] # (id, activity)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            with torch.no_grad():\n",
        "                sx = model(x).detach()\n",
        "            y_ = classifier(sx)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "        coptim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 5) # .5\n",
        "        scaler.step(coptim)\n",
        "        scaler.update()\n",
        "        print(\"classify\",loss.item())\n",
        "        try: wandb.log({\"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        # with test_summary_writer.as_default(): tf.summary.scalar('closs', loss.item(), step=i)\n",
        "        if i>=100: break\n",
        "\n",
        "\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    correct = 0\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        # x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        # x, y = x[...,1:].to(device).to(torch.float), y[1].to(device) # [batch, ] # (id, activity)\n",
        "        x, y = x.to(device).to(torch.float), y.to(device) # [batch, ] # (id, activity)\n",
        "        with torch.no_grad():\n",
        "            sx = model(x)\n",
        "            try:\n",
        "                rankme = RankMe(sx).item()\n",
        "                lidar = LiDAR(sx).item()\n",
        "            except NameError: pass\n",
        "            y_ = classifier(sx)\n",
        "        test_loss = F.cross_entropy(y_, y)\n",
        "        correct += (y==y_.argmax(dim=1)).sum().item()\n",
        "        if i>=100: break\n",
        "    # print(correct/len(y))\n",
        "    print(correct/len(dataloader.dataset))\n",
        "    try: wandb.log({\"correct\": correct/len(dataloader.dataset)})\n",
        "    # try: wandb.log({\"correct\": correct/len(y), \"rankme\": rankme, \"lidar\": lidar})\n",
        "    except NameError: pass\n",
        "\n",
        "\n",
        "# for i in range(50): #\n",
        "for i in range(5000): # 5000\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # np.random.shuffle(train_indices); np.random.shuffle(val_indices) # for wisdm\n",
        "    # train_sampler, valid_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(val_indices)\n",
        "    # # batch_size = 64 #512\n",
        "    # train_loader = DataLoader(train_data, sampler=train_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "    # test_loader = DataLoader(train_data, sampler=valid_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True)\n",
        "\n",
        "    # strain(seq_jepa, train_loader, optim)\n",
        "    strain(seq_jepa, train_loader, optim, scheduler)\n",
        "    ctrain(seq_jepa, classifier, train_loader, coptim)\n",
        "    test(seq_jepa, classifier, test_loader)\n",
        "\n",
        "    # strain(violet, train_loader, voptim)\n",
        "    # ctrain(violet, classifier, train_loader, coptim)\n",
        "    # test(violet, classifier, test_loader)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5p6LJ2qPqom"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir logs/gradient_tape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4J2ahp3wmqcg",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title supervised train test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "def strain(model, classifier, dataloader, optim, coptim, scheduler=None):\n",
        "    model.train()\n",
        "    classifier.train()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        # y = y.to(device)\n",
        "        # x, y = x[...,1:].to(device).to(torch.bfloat16), y[1].to(device) # [batch, ] # (id, activity)\n",
        "        x, y = x.to(device).to(torch.bfloat16), y.to(device) # [batch, ] # (id, activity)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        coptim.zero_grad()\n",
        "        # print(loss)\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # for p in list(filter(lambda p: p.grad is not None, model.parameters())):\n",
        "        #     print(p.grad.data.norm(2).item())\n",
        "        # print(\"max grad norm\", max([p.grad.data.norm(2).item() for p in list(filter(lambda p: p.grad is not None, model.parameters()))]))\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 10) # 0.5\n",
        "\n",
        "        scaler.step(optim)\n",
        "        scaler.step(coptim)\n",
        "        scaler.update()\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        print(\"strain\",loss.item())\n",
        "        # for param in seq_jepa.context_encoder.cls: print(param.data)\n",
        "        # for param in seq_jepa.predicter.cls: print(param.data)\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=50: break\n",
        "\n",
        "\n",
        "# def test(model, dataloader):\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        # x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        # x, y = x[...,1:].to(device).to(torch.float), y[1].to(device) # [batch, ] # (id, activity)\n",
        "        x, y = x.to(device).to(torch.float), y.to(device) # [batch, ] # (id, activity)\n",
        "        with torch.no_grad():\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "        loss = F.cross_entropy(y_, y)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        print(correct/len(y))\n",
        "        try: wandb.log({\"correct\": correct/len(y), \"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "for i in range(1000):\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # np.random.shuffle(train_indices); np.random.shuffle(val_indices)\n",
        "    # train_sampler, valid_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(val_indices)\n",
        "    # # batch_size = 64 #512\n",
        "    # train_loader = DataLoader(train_data, sampler=train_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "    # test_loader = DataLoader(train_data, sampler=valid_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True)\n",
        "\n",
        "    strain(seq_jepa, classifier, train_loader, optim, coptim)\n",
        "    test(seq_jepa, classifier, test_loader)\n",
        "    # strain(violet, train_loader, voptim)\n",
        "    # test(violet, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JT8AgOx0E_KM",
        "outputId": "19308cfe-2f31-472c-bfcc-0066ae14644f",
        "cellView": "form"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# @title save/load\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# folder='/content/drive/MyDrive/jepa/'\n",
        "\n",
        "# # modelsd, optimsd = torch.load(folder+'SeqJEPA.pkl', map_location=device).values()\n",
        "# modelsd, optimsd = torch.load('SeqJEPA.pkl', map_location=device).values()\n",
        "# seq_jepa.load_state_dict(modelsd, strict=False)\n",
        "# optim.load_state_dict(optimsd)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "modelsd = torch.load(folder+'roberta.pkl', map_location=device)['model']#.values()\n",
        "# print(modelsd)\n",
        "model_mlm.load_state_dict(modelsd, strict=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpbLPvjiE-pD"
      },
      "outputs": [],
      "source": [
        "# checkpoint = {'model': seq_jepa.state_dict(), 'optimizer': optim.state_dict()}\n",
        "# checkpoint = {'model': model_mlm.state_dict()}\n",
        "# torch.save(checkpoint, folder+'roberta.pkl')\n",
        "# torch.save(checkpoint, folder+'SeqJEPA.pkl')\n",
        "# torch.save(checkpoint, 'SeqJEPA.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vScvFGGSeN6"
      },
      "source": [
        "## drawer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pQfM2fYvcTh6"
      },
      "outputs": [],
      "source": [
        "# @title masks\n",
        "import torch\n",
        "\n",
        "def multiblock(seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "    return target_mask\n",
        "\n",
        "def multiblock(seq, min_s, max_s, B=64, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    # mask_len = torch.rand(B, 1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_len = torch.rand(1, M) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(B, M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    # indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    indices = torch.arange(seq)[None,None,...] # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [B, M, seq]\n",
        "    return target_mask\n",
        "\n",
        "def multiblock2d(hw=(8,8), scale=(.15,.2), aspect_ratio=(.75,1.5), M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_aspect = torch.rand(1) * (aspect_ratio[1] - aspect_ratio[0]) + aspect_ratio[0] # in (min_s, max_s) # all blocks same size\n",
        "    mask_scale = torch.rand(1) * (scale[1] - scale[0]) + scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    h = (mask_scale/mask_aspect)**.5# h*(h*aspect) = scale\n",
        "    w = h * mask_aspect\n",
        "    h_pos, w_pos = torch.rand(M)*(1-w), torch.rand(M)*(1-h) # in (0, 1 - mask_len)\n",
        "    h_len, h_pos = (h*hw[0]).int(), h_pos*hw[0]\n",
        "    w_len, w_pos = (w*hw[1]).int(), w_pos*hw[1]\n",
        "    h_ind, w_ind = torch.arange(hw[0]).unsqueeze(0), torch.arange(hw[1]).unsqueeze(0) # [1, seq]\n",
        "    h_mask = (h_ind>=h_pos.unsqueeze(-1)) & (h_ind<(h_pos+h_len).unsqueeze(-1)) # [M, seq]\n",
        "    w_mask = (w_ind>=w_pos.unsqueeze(-1)) & (w_ind<(w_pos+w_len).unsqueeze(-1)) # [M, seq]\n",
        "    target_mask = h_mask.unsqueeze(-1) & w_mask.unsqueeze(-2) # [M, seq, seq]\n",
        "    return target_mask\n",
        "\n",
        "def multiblock2d(hw=(8,8), scale=(.15,.2), aspect_ratio=(.75,1.5), B=64, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_aspect = torch.rand(B, 1) * (aspect_ratio[1] - aspect_ratio[0]) + aspect_ratio[0] # in (min_s, max_s) # all blocks same size\n",
        "    mask_scale = torch.rand(B, 1) * (scale[1] - scale[0]) + scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    h = (mask_scale/mask_aspect)**.5# h*(h*aspect) = scale\n",
        "    w = h * mask_aspect\n",
        "    h_pos, w_pos = torch.rand(M)*(1-w), torch.rand(M)*(1-h) # in (0, 1 - mask_len)\n",
        "    h_len, h_pos = (h*hw[0]).int(), h_pos*hw[0]\n",
        "    w_len, w_pos = (w*hw[1]).int(), w_pos*hw[1]\n",
        "    h_ind, w_ind = torch.arange(hw[0]).unsqueeze(0), torch.arange(hw[1]).unsqueeze(0) # [1, seq]\n",
        "    h_mask = (h_ind>=h_pos.unsqueeze(-1)) & (h_ind<(h_pos+h_len).unsqueeze(-1)) # [M, seq]\n",
        "    w_mask = (w_ind>=w_pos.unsqueeze(-1)) & (w_ind<(w_pos+w_len).unsqueeze(-1)) # [M, seq]\n",
        "    target_mask = h_mask.unsqueeze(-1) & w_mask.unsqueeze(-2) # [M, seq, seq]\n",
        "    return target_mask\n",
        "\n",
        "# https://arxiv.org/pdf/2210.07224\n",
        "def randpatch(seq, mask_size=8, gamma=0.9): # num patches of seq, mask patch size, masking ratio\n",
        "    # mask = torch.rand(seq//mask_size)<gamma\n",
        "    length = seq//mask_size\n",
        "    g = torch.normal(gamma, std=.1, size=(1,)).clamp(.5,.9)\n",
        "    # g = gamma\n",
        "    idx = torch.randperm(length)[:int(length*g)]\n",
        "    mask = torch.zeros(length, dtype=bool)\n",
        "    mask[idx] = True\n",
        "    mask = mask.repeat_interleave(mask_size, dim=-1)\n",
        "    return mask # [seq] , True -> mask\n",
        "\n",
        "\n",
        "import torch\n",
        "def apply_masks(x, mask): # [b,t,d], [mask_size] # https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "    mask_keep = mask.unsqueeze(-1).repeat(x.size(0), 1, x.size(-1)) # [batch,T,dim]\n",
        "    return torch.gather(x, dim=1, index=mask_keep) # [batch,mask_size,dim]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9rPxvrrsYI_W"
      },
      "outputs": [],
      "source": [
        "# @title simplex\n",
        "# !pip install -q opensimplex\n",
        "import opensimplex\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def simplexmask(hw=(8,8), scale=(.15,.2)):\n",
        "    ix = iy = np.linspace(0, 1, num=8)\n",
        "    ix, iy = ix+np.random.randint(1e10), iy+np.random.randint(1e10)\n",
        "    y=opensimplex.noise2array(ix, iy)\n",
        "    y = torch.from_numpy(y)\n",
        "    mask_scale = torch.rand(1) * (scale[1] - scale[0]) + scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    yy = y.flatten().sort()[0][int(hw[0]*hw[1]*mask_scale)]\n",
        "    mask = (y<=yy.item())\n",
        "    return mask # T/F [h,w]\n",
        "\n",
        "def simplexmask1d(seq=512, scale=(.15,.2), chaos=2):\n",
        "    i = np.linspace(0, chaos, num=seq) # 2\n",
        "    j = np.random.randint(1e10, size=1)\n",
        "    y=opensimplex.noise2array(i, j) # [1, seq]\n",
        "    # plt.pcolormesh(y)\n",
        "    # plt.show()\n",
        "    y = torch.from_numpy(y)\n",
        "    # print(y.shape)\n",
        "    mask_scale = torch.rand(1) * (scale[1] - scale[0]) + scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    # print(a.shape, int(seq*mask_scale))\n",
        "    val, ind = y.sort()\n",
        "    yy = val[:,int(seq*mask_scale)]\n",
        "    mask = (y<=yy.item())\n",
        "    index = ind[:,:int(seq*mask_scale)]\n",
        "    return index, mask # T/F [h,w]\n",
        "\n",
        "def simplexmask1d(seq=512, ctx_scale=(.85,1), trg_scale=(.6,.8), B=64, chaos=2):\n",
        "    i = np.linspace(0, chaos, num=seq) # 2-5\n",
        "    j = np.random.randint(1e10, size=B)\n",
        "    noise = opensimplex.noise2array(i, j) # [B, seq]\n",
        "    # plt.pcolormesh(noise[:1])\n",
        "    # plt.rcParams[\"figure.figsize\"] = (20,3)\n",
        "    # plt.show()\n",
        "    noise = torch.from_numpy(noise)\n",
        "    ctx_mask_scale = torch.rand(1) * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    trg_mask_scale = torch.rand(1) * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "    val, ind = noise.sort()\n",
        "    trg_index = ind[:,-int(seq*trg_mask_scale):]\n",
        "    ctx_index = ind[:,-int(seq*ctx_mask_scale):-int(seq*trg_mask_scale)] # ctx wraps trg ; most similar to multiblock\n",
        "    # ctx_index = ind[:,:int(seq*ctx_mask_scale)-int(seq*trg_mask_scale)] # ctx hug bottom\n",
        "    # ctx_index = ind[:,-int(seq*ctx_mask_scale)-int(seq*trg_mask_scale):-int(seq*trg_mask_scale)] # ctx wraps trg ; most similar to multiblock\n",
        "    # ctx_index = ind[:,:int(seq*ctx_mask_scale)] # ctx hug bottom\n",
        "    return ctx_index, trg_index\n",
        "\n",
        "\n",
        "# mask = torch.zeros(1 ,200)\n",
        "# mask[:, trg_index[:1]] = 1\n",
        "# mask[:, ctx_index[:1]] = .5\n",
        "# plt.rcParams[\"figure.figsize\"] = (20,1)\n",
        "# plt.pcolormesh(mask)\n",
        "# plt.show()\n",
        "\n",
        "def simplexmask1d(seq=512, ctx_scale=(.85,1), trg_scale=(.6,.8), B=64, chaos=[1,.5]):\n",
        "    i = np.linspace(0, chaos[0], num=seq) # 2-5\n",
        "    noise = opensimplex.noise2array(i, np.random.randint(1e10, size=B)) # [B, seq]\n",
        "    noise = torch.from_numpy(noise)\n",
        "    ctx_mask_scale = torch.rand(1) * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    trg_mask_scale = torch.rand(1) * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "\n",
        "    ctx_len, trg_len = int(seq*ctx_mask_scale), int(seq*trg_mask_scale)\n",
        "    val, trg_index = torch.topk(noise, trg_len, dim=1, sorted=False)\n",
        "    ctx_len = ctx_len - trg_len\n",
        "\n",
        "    remove_mask = torch.ones((B,seq), dtype=bool) # [B, S]\n",
        "    remove_mask.scatter_(1, trg_index, False).flatten()\n",
        "    ind = torch.arange(seq).unsqueeze(0).repeat(B,1)[remove_mask].reshape(B, -1)\n",
        "    print(ind.shape, ind)\n",
        "\n",
        "    i = np.linspace(0, chaos[1], num=seq) # 2-5\n",
        "    noise = opensimplex.noise2array(i, np.random.randint(1e10, size=B)) # [B, seq]\n",
        "    noise = torch.from_numpy(noise)[remove_mask].reshape(B, -1)\n",
        "    val, ctx_ind = torch.topk(noise, ctx_len, dim=1, sorted=False)\n",
        "    print(ctx_ind.shape, ctx_ind)\n",
        "\n",
        "\n",
        "    ctx_index = ind[torch.arange(B).unsqueeze(-1), ctx_ind]\n",
        "    return ctx_index, trg_index\n",
        "\n",
        "\n",
        "# mask = simplexmask(hw=(8,8), scale=(.6,.8))\n",
        "# index, mask = simplexmask1d(seq=100, scale=(.7,.8))\n",
        "# ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.85,1), trg_scale=(.6,.8), B=64, chaos=5)\n",
        "# ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.85,1), trg_scale=(.7,.8), B=64, chaos=2)\n",
        "# ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.7,.9), trg_scale=(.6,.7), B=64, chaos=5)\n",
        "# ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.2,.3), trg_scale=(.4,.5), B=64, chaos=3)\n",
        "# ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.1,.3), trg_scale=(.4,.6), B=64, chaos=3)\n",
        "# # print(trg_index[0], ctx_index[0])\n",
        "\n",
        "ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.85,1), trg_scale=(.7,.8), B=64, chaos=[3,.5])\n",
        "mask = torch.zeros(b ,200)\n",
        "mask[torch.arange(b).unsqueeze(-1), trg_index] = 1\n",
        "mask[torch.arange(b).unsqueeze(-1), ctx_index] = .5\n",
        "mask = mask[None,...]\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "imshow(mask)\n",
        "import torchvision\n",
        "imshow(torchvision.utils.make_grid(mask, nrow=8))\n",
        "\n",
        "# print(index)\n",
        "# print(index.shape)\n",
        "# print(mask)\n",
        "# print(mask.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwpnHW4wn9S1",
        "outputId": "40413ecd-7f4b-49a2-984b-fe812ec57e26"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:08<00:00, 19.5MB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title data\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "transform = transforms.Compose([transforms.ToTensor(),])\n",
        "\n",
        "# MNIST CIFAR10\n",
        "train_data = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
        "test_data = torchvision.datasets.CIFAR10(root='data', train=False, download=True, transform=transform)\n",
        "batch_size = 128 # 128 1024\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    # print(npimg.shape) # (3, 64, 64)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# dataiter = iter(train_data)\n",
        "# x,y = next(dataiter)\n",
        "# print(x.shape) # [3, 32, 32]\n",
        "# imshow(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AtPUcVu2kSLI"
      },
      "outputs": [],
      "source": [
        "# @title TransformerClassifier\n",
        "import math\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class TransformerClassifier(nn.Module):\n",
        "    # def __init__(self, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, drop = 0.):\n",
        "    def __init__(self, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, drop=0):\n",
        "        super().__init__()\n",
        "        # self.embedding = nn.Embedding(ntoken, d_model)\n",
        "        self.pos_encoder = RoPE(d_model, seq_len=200, base=10000)\n",
        "        # d_hid = d_hid or d_model#*2\n",
        "        # encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, d_hid, drop, batch_first=True)\n",
        "        # self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "\n",
        "        out_dim = out_dim or d_model\n",
        "        self.lin = nn.Linear(d_model, out_dim)\n",
        "        # self.lin = nn.Linear(d_model*2, out_dim)\n",
        "        # self.cls = nn.Parameter(torch.randn(1,1,d_model))\n",
        "        self.attention_pool = nn.Linear(d_model, 1, bias=False)\n",
        "        # self.out = nn.Sequential(\n",
        "        #     nn.Dropout(drop), nn.Linear(d_model, 3), nn.SiLU(),\n",
        "        #     nn.Dropout(drop), nn.Linear(3, out_dim), nn.Sigmoid()\n",
        "        # )\n",
        "\n",
        "    def forward(self, x, src_key_padding_mask = None): # [batch, seq, d_model], [batch, seq] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # batch, seq_len, d_model = x.shape\n",
        "        # x = torch.cat([self.cls.repeat(batch,1,1), x], dim=1)\n",
        "        # src_key_padding_mask = torch.cat([torch.zeros((batch, 1), dtype=torch.bool), src_key_padding_mask], dim=1)\n",
        "        x = self.pos_encoder(x)\n",
        "\n",
        "        x = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
        "        # print(\"fwd\",out.shape) # float [batch, seq_len, d_model]\n",
        "        # out = x.mean(dim=1) # average pool\n",
        "        # out = x.max(dim=1)#[0]\n",
        "\n",
        "        attn = self.attention_pool(x).squeeze(-1) # [batch, seq] # seq_pool\n",
        "        out = (torch.softmax(attn, dim=-1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, seq] @ [batch, seq, dim] -> [batch, dim]\n",
        "        # out = self.out(out) # optional (from GlobalContext) like squeeze excitation\n",
        "\n",
        "        # out = out[:, 0] # first token\n",
        "        # out = torch.cat([out, mean_pool], dim=-1)\n",
        "\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "batch, seq_len, d_model = 4,7,512\n",
        "model = TransformerModel(in_dim, d_model, d_head=4, nlayers=2, dropout=0.).to(device)\n",
        "# model = TransformerClassifier(d_model, nhead=8, nlayers=2, dropout=0.).to(device)\n",
        "x =  torch.rand(batch, seq_len, d_model)\n",
        "src_key_padding_mask = torch.stack([(torch.arange(seq_len) < seq_len - v) for v in torch.randint(seq_len, (batch,))]) # True will be ignored # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "print(src_key_padding_mask)\n",
        "out = model(x, src_key_padding_mask)\n",
        "print(out.shape)\n",
        "\n",
        "# GlobalAveragePooling1D layer, followed by a Dense layer. The final output of the transformer is produced by a softmax layer,\n",
        "# x = GlobalAveragePooling1D()(x)\n",
        "# x = Dropout(0.1)(x)\n",
        "# x = Dense(20, activation=\"relu\")(x)\n",
        "# x = Dropout(0.1)(x)\n",
        "# outputs = Dense(2, activation=\"softmax\")(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-8i1WLsvH_vn"
      },
      "outputs": [],
      "source": [
        "# @title Transformer Model\n",
        "import torch\n",
        "from torch import nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        # self.embed = nn.Sequential(\n",
        "        #     nn.Linear(in_dim, d_model), #act,\n",
        "        #     # nn.Linear(d_model, d_model), act,\n",
        "        # )\n",
        "        self.embed = nn.Linear(in_dim, d_model) if in_dim != d_model else None\n",
        "        # self.embed = nn.Sequential(nn.Conv1d(in_dim,d_model,7,2,7//2), nn.MaxPool1d(3, 2, 3//2))\n",
        "        self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, d_hid or d_model, dropout, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.d_model = d_model\n",
        "        self.cls = nn.Parameter(torch.zeros(1,1,d_model)) # randn\n",
        "        out_dim = out_dim or d_model\n",
        "        # self.lin = nn.Linear(d_model, out_dim)\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim != d_model else None\n",
        "        self.norm = nn.LayerNorm(out_dim)\n",
        "\n",
        "\n",
        "    def forward(self, src, src_key_padding_mask=None, cls_mask=None, context_indices=None, trg_indices=None): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # if cls_mask != None: src[cls_mask] = self.cls.to(src.dtype)\n",
        "\n",
        "        if self.embed != None: src = self.embed(src) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = src.shape\n",
        "        # src = self.pos_encoder(src)\n",
        "        if context_indices != None:\n",
        "            # print(src.shape, context_indices.shape, self.pos_encoder(context_indices).shape) # [2, 88, 32]) torch.Size([88]) torch.Size([88, 32]\n",
        "            # print(src[0], self.pos_encoder(context_indices)[0])\n",
        "            src = src * self.pos_encoder(context_indices) # context/predictor # src = src + self.positional_emb[:,context_indices]\n",
        "            # print(src[0])\n",
        "        else: src = src * self.pos_encoder(torch.arange(seq, device=device)) # target # src = src + self.positional_emb[:,:seq]\n",
        "            # print(\"trans fwd\", src.shape, self.pos_encoder(src).shape)\n",
        "\n",
        "        if trg_indices != None: # [M, num_trg_toks]\n",
        "            pred_tokens = self.cls * self.pos_encoder(trg_indices) # [M, num_trg_toks, d_model] # pred_tokens = self.cls + self.positional_emb[0,trg_indices]\n",
        "            pred_tokens = pred_tokens.repeat(batch, 1, 1) # [batch*M, num_trg_toks, d_model]\n",
        "            # print(pred_tokens.requires_grad)\n",
        "            src = src.repeat_interleave(trg_indices.shape[0], dim=0) # [batch, seq_len, d_model] -> [batch*M, seq_len, d_model]\n",
        "            src = torch.cat([src, pred_tokens], dim=1) # [batch*M, seq_len+num_trg_toks, d_model]\n",
        "\n",
        "        out = self.transformer_encoder(src, src_key_padding_mask=src_key_padding_mask) # float [seq_len, batch_size, d_model]\n",
        "        if trg_indices != None:\n",
        "            # print(out.shape)\n",
        "            out = out[:,seq:] # [batch*M, num_trg_toks, d_model]\n",
        "        if self.lin != None: out = self.lin(out)\n",
        "        out = self.norm(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "batch, seq_len, d_model = 4,7,64\n",
        "in_dim = 3\n",
        "model = TransformerModel(in_dim, d_model, nhead=8, nlayers=2, dropout=0.).to(device)\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "# print(out.shape)\n",
        "# # print(out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## mae"
      ],
      "metadata": {
        "id": "5tqJqL5Vj2vL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDJvKFcMGIa7",
        "outputId": "df899e65-2888-4fa3-b772-11c7af625a0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "38128\n",
            "torch.Size([4, 110, 16])\n"
          ]
        }
      ],
      "source": [
        "# @title mae me enc,dec\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def random_masking(length, mask_ratio, b=64):\n",
        "    noise = torch.rand(b, length)\n",
        "    len_mask = int(length * mask_ratio)\n",
        "    _, msk_ind = torch.topk(selected_probs, k=len_mask, dim=-1, sorted=False) # val, ind -> [b,len_mask]\n",
        "    _, keep_ind = torch.topk(selected_probs, k=length-len_mask, largest=False, dim=-1, sorted=False) # val, ind -> [b,len_keep]\n",
        "    return msk_ind, keep_ind\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, n_heads=4, d_hid=None, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, 200, d_model)*.02)\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=200, base=10000), requires_grad=False)\n",
        "\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, n_heads=n_heads) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.randn(1,1,d_model)*0.02) # randn zeros\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim)# if out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = x.shape\n",
        "        # x = x * self.pos_enc(context_indices)\n",
        "        # print(\"Trans pred\",x.shape, self.pos_emb[0,context_indices].shape)\n",
        "        x = x + self.pos_emb[0,context_indices]\n",
        "        # print('pred fwd', self.pos_emb[:,context_indices].shape)\n",
        "\n",
        "        # pred_tokens = self.cls * self.pos_enc(trg_indices) # [M, num_trg_toks, d_model]\n",
        "        pred_tokens = self.cls + self.pos_emb[0,trg_indices]\n",
        "        # print(\"pred fwd\", x.shape, pred_tokens.shape)\n",
        "        x = torch.cat([x, pred_tokens], dim=1) # [batch, seq_len+num_trg_toks, d_model]\n",
        "        out = self.transformer(x)\n",
        "\n",
        "        out = self.norm(out)\n",
        "        out = out[:,seq:] # [batch, num_trg_toks, d_model]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, drop=0):\n",
        "    def __init__(self, patch_size, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0):\n",
        "        super().__init__()\n",
        "        # patch_size=32\n",
        "        self.embed = nn.Sequential(\n",
        "            # # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.MaxPool1d(2,2), #nn.MaxPool1d(3, 2, 3//2),\n",
        "            # # nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.Conv1d(d_model, d_model,3,2,3//2)\n",
        "            nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2),\n",
        "            # nn.Conv1d(in_dim, d_model, patch_size, patch_size), # like patch\n",
        "\n",
        "            # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.Dropout(drop), nn.BatchNorm1d(d_model), snake,\n",
        "            # lstm\n",
        "\n",
        "            )\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, 200, d_model)*.02)\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=200, base=10000), requires_grad=False)#.unsqueeze(0)\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, n_heads=n_heads) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "        # try: print(\"Trans fwd\",x.shape, context_indices.shape)\n",
        "        # except: print(\"Trans fwd noind\",x.shape)\n",
        "        # x = self.pos_enc(x)\n",
        "        x = x + self.pos_emb[:,:x.shape[1]]\n",
        "        if context_indices != None: x = x[torch.arange(x.shape[0]).unsqueeze(-1), context_indices] # [batch, num_context_toks, d_model]\n",
        "\n",
        "        # print(\"TransformerModel\",x.shape)\n",
        "        x = self.transformer(x)\n",
        "        out = self.norm(x)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "batch, seq_len, d_model = 4,3500,16\n",
        "in_dim = 3\n",
        "patch_size=32\n",
        "model = TransformerModel(patch_size, in_dim, d_model, n_heads=4, nlayers=10, drop=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "# # # print(out)\n",
        "# model = TransformerPredictor(in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1).to(device)\n",
        "# out = model(out)\n",
        "# print(out.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PE3XAPkCZ2oM",
        "outputId": "d9395c50-9307-46bb-c035-e5bc63f2fa23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "109920\n",
            "torch.Size([])\n"
          ]
        }
      ],
      "source": [
        "# @title MAE me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class SeqJEPA(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, nlayers=2, n_heads=4):\n",
        "        super().__init__()\n",
        "        if out_dim is None: out_dim = d_model\n",
        "        self.patch_size = 32\n",
        "        self.student = TransformerModel(self.patch_size, in_dim, d_model, out_dim=out_dim, n_heads=n_heads, nlayers=nlayers, drop=0.)\n",
        "        self.predicter = TransformerPredictor(out_dim, d_model//2, out_dim, n_heads=4, nlayers=1, drop=0.)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "        # self.transform = RandomResizedCrop1d(3500, scale=(.8,1.))\n",
        "\n",
        "    def loss(self, x): # [b,t,d]\n",
        "        b,t,d = x.shape\n",
        "        msk_ind, keep_ind = random_masking(length, mask_ratio, b=b)\n",
        "\n",
        "        sx = self.student(x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "        # print('seq_jepa loss sx',sx.shape)\n",
        "        sy_ = self.predicter(sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "        sy_ = F.layer_norm(sy_, (sy_.size(-1),))\n",
        "        with torch.no_grad():\n",
        "            sy = self.teacher(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "            sy = sy[torch.arange(sy.shape[0]).unsqueeze(-1), trg_indices] # [batch, num_context_toks, d_model] # nan bec len(trg_ind)==0 # print('loss sy',torch.isnan(sy).any())\n",
        "            sy = F.layer_norm(sy, (sy.size(-1),))\n",
        "        loss = F.mse_loss(sy, sy_)\n",
        "        return loss\n",
        "\n",
        "\n",
        "msk_ind, keep_ind\n",
        "\n",
        "        sx = self.encoder(x, context_indices=keep_ind) # [batch, num_context_toks, out_dim]\n",
        "        x_ = self.decoder(sx, context_indices=keep_ind, msk_ind) # [batch, num_context_toks, out_dim]\n",
        "\n",
        "        loss = (pred - target) ** 2\n",
        "        loss = loss.mean(dim=-1)  # [N, L], mean loss per patch\n",
        "        loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches\n",
        "\n",
        "pred, target =\n",
        "        # loss = F.mse_loss(pred, target)\n",
        "\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        sx = self.student(x)\n",
        "        out = sx.mean(dim=1)\n",
        "        return out\n",
        "\n",
        "# min_s=0.15, max_s, M\n",
        "# trg.15.2M4 C.85 1\n",
        "\n",
        "# 1e-2,1e-3 < 3e-3,1e-3\n",
        "# patch16 < patch32\n",
        "# NoPE good but sus\n",
        "\n",
        "# ctx/trg sacle min/max, num blk,\n",
        "\n",
        "\n",
        "# seq_jepa = SeqJEPA(in_dim=3, d_model=32, out_dim=16, nlayers=4, n_heads=4).to(device)#.to(torch.float)\n",
        "seq_jepa = SeqJEPA(in_dim=3, d_model=64, out_dim=16, nlayers=1, n_heads=8).to(device)#.to(torch.float)\n",
        "optim = torch.optim.AdamW(seq_jepa.parameters(), lr=1e-3) # 1e-3?\n",
        "# optim = torch.optim.AdamW([{'params': seq_jepa.student.parameters()},\n",
        "#     {'params': seq_jepa.predicter.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # default 1e-2, 5e-2\n",
        "    # {'params': seq_jepa.predicter.parameters(), 'lr': 1e-2}], lr=1e-3, weight_decay=1e-2)\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/configs/in1k_vith14_ep300.yaml\n",
        "# d_model 1024,384\n",
        "# depth 12,6/12\n",
        "# wd 5e-2 - 4e-1\n",
        "# adamw 1e-4 - 1e-3 - 1e-6\n",
        "# ema 0.996-1\n",
        "\n",
        "print(sum(p.numel() for p in seq_jepa.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.parameters())) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.predicter.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.student.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.teacher.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# d_model^2 * nlayers\n",
        "\n",
        "x = torch.rand((24, 3500, in_dim), device=device)\n",
        "out = seq_jepa.loss(x)\n",
        "print(out.shape)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(16, 18).to(device)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n",
        "# optim = torch.optim.AdamW([{'params': seq_jepa.parameters()}, {'params': classifier.parameters(), 'lr': 1e-3}], lr=1e-3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ABygeAwL5N-6"
      },
      "outputs": [],
      "source": [
        "# @title facebookresearch/mae models_mae.py\n",
        "# https://github.com/facebookresearch/mae/blob/main/models_mae.py\n",
        "from functools import partial\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from timm.models.vision_transformer import PatchEmbed, Block\n",
        "\n",
        "\n",
        "class MaskedAutoencoderViT(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3,\n",
        "                 embed_dim=1024, depth=24, num_heads=16,\n",
        "                 decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
        "                 mlp_ratio=4., norm_layer=nn.LayerNorm, norm_pix_loss=False):\n",
        "        super().__init__()\n",
        "\n",
        "        # --------------------------------------------------------------------------\n",
        "        # MAE encoder specifics\n",
        "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        # self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True, qk_scale=None, norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "        # --------------------------------------------------------------------------\n",
        "\n",
        "        # --------------------------------------------------------------------------\n",
        "        # MAE decoder specifics\n",
        "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
        "        # self.decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, decoder_embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
        "\n",
        "        self.decoder_blocks = nn.ModuleList([\n",
        "            Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=True, qk_scale=None, norm_layer=norm_layer)\n",
        "            for i in range(decoder_depth)])\n",
        "\n",
        "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
        "        self.decoder_pred = nn.Linear(decoder_embed_dim, patch_size**2 * in_chans, bias=True) # decoder to patch\n",
        "        # --------------------------------------------------------------------------\n",
        "\n",
        "    def random_masking(self, x, mask_ratio):\n",
        "        \"\"\"\n",
        "        Perform per-sample random masking by per-sample shuffling.\n",
        "        Per-sample shuffling is done by argsort random noise.\n",
        "        x: [N, L, D], sequence\n",
        "        \"\"\"\n",
        "        N, L, D = x.shape  # batch, length, dim\n",
        "        len_keep = int(L * (1 - mask_ratio))\n",
        "\n",
        "        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n",
        "        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
        "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
        "\n",
        "        # keep the first subset\n",
        "        ids_keep = ids_shuffle[:, :len_keep]\n",
        "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
        "\n",
        "        # generate the binary mask: 0 is keep, 1 is remove\n",
        "        mask = torch.ones([N, L], device=x.device)\n",
        "        mask[:, :len_keep] = 0\n",
        "        # unshuffle to get the binary mask\n",
        "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
        "        return x_masked, mask, ids_restore\n",
        "\n",
        "    def forward_encoder(self, x, mask_ratio):\n",
        "        x = self.patch_embed(x)\n",
        "        x = x + self.pos_embed[:, 1:, :]\n",
        "\n",
        "        # masking: length -> length * mask_ratio\n",
        "        x, mask, ids_restore = self.random_masking(x, mask_ratio)\n",
        "\n",
        "        # append cls token\n",
        "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
        "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        for blk in self.blocks: x = blk(x)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        return x, mask, ids_restore\n",
        "\n",
        "    def forward_decoder(self, x, ids_restore):\n",
        "        x = self.decoder_embed(x)\n",
        "\n",
        "        # append mask tokens to sequence\n",
        "        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
        "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
        "        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
        "        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
        "\n",
        "        x = x + self.decoder_pos_embed\n",
        "\n",
        "        for blk in self.decoder_blocks: x = blk(x)\n",
        "        x = self.decoder_norm(x)\n",
        "        x = self.decoder_pred(x)\n",
        "        x = x[:, 1:, :]\n",
        "        return x\n",
        "\n",
        "    def forward_loss(self, imgs, pred, mask):\n",
        "        \"\"\"\n",
        "        imgs: [N, 3, H, W]\n",
        "        pred: [N, L, p*p*3]\n",
        "        mask: [N, L], 0 is keep, 1 is remove,\n",
        "        \"\"\"\n",
        "        target = self.patchify(imgs)\n",
        "        # if self.norm_pix_loss:\n",
        "        #     mean = target.mean(dim=-1, keepdim=True)\n",
        "        #     var = target.var(dim=-1, keepdim=True)\n",
        "        #     target = (target - mean) / (var + 1.e-6)**.5\n",
        "\n",
        "        loss = (pred - target) ** 2\n",
        "        loss = loss.mean(dim=-1)  # [N, L], mean loss per patch\n",
        "        # loss = F.mse_loss(pred, target)\n",
        "\n",
        "        loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches\n",
        "        return loss\n",
        "\n",
        "    def forward(self, imgs, mask_ratio=0.75):\n",
        "        latent, mask, ids_restore = self.forward_encoder(imgs, mask_ratio)\n",
        "        pred = self.forward_decoder(latent, ids_restore)  # [N, L, p*p*3]\n",
        "        loss = self.forward_loss(imgs, pred, mask)\n",
        "        return loss, pred, mask\n",
        "\n",
        "\n",
        "model = MaskedAutoencoderViT(\n",
        "    patch_size=16, embed_dim=768, depth=12, num_heads=12, # B16\n",
        "    # patch_size=16, embed_dim=1024, depth=24, num_heads=16, # L16\n",
        "    # patch_size=14, embed_dim=1280, depth=32, num_heads=16, # H14\n",
        "    decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
        "    mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## text classification, roberta"
      ],
      "metadata": {
        "id": "rpBQCgArjj7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU datasets"
      ],
      "metadata": {
        "id": "DkiHS6yPkaVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "o4xSm_eyhSK0",
        "outputId": "23c3cd4c-7330-4292-fbee-0a1d1cef4d38"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'train_tok' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-8fcae1a8af8a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_side\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_tok' is not defined"
          ]
        }
      ],
      "source": [
        "# @title yelp data\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataset = load_dataset(\"yelp_polarity\") # yelp_polarity yelp_review_full\n",
        "# # dataset = load_dataset(\"yelp_review_full\") # yelp_polarity yelp_review_full\n",
        "# print(dataset[\"train\"][0]) # {'text': \"Unfortunately, the ... to give Dr. Goldberg 2 stars.\", 'label': 0}\n",
        "# print(len(dataset))\n",
        "# print(len(dataset[\"train\"]))\n",
        "# # train_text = dataset[\"train\"][:10]['text']\n",
        "# train_text = dataset[\"train\"]['text']\n",
        "# # train_tok = [enc.encode(text) for text in train_text]\n",
        "# train_tok = [torch.tensor(enc.encode(text)) for text in train_text]\n",
        "# # print(train_tok)\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    train_text = dataset[\"train\"]['text']\n",
        "    train_tok = [torch.tensor(enc.encode(text)) for text in train_text]\n",
        "\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "# def tokenize(example): return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\")\n",
        "# tokenized = dataset.map(tokenize, batched=True)\n",
        "# tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "\n",
        "# train_loader = DataLoader(tokenized[\"train\"], batch_size=32, shuffle=True)\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "x = pad_sequence(train_tok, batch_first=True, padding_value=0, padding_side='left')\n",
        "print(x)\n",
        "\n",
        "def left_pad(batch, pad_value=0):\n",
        "    # batch: list of 1D tensors\n",
        "    lengths = torch.tensor([len(x) for x in batch])\n",
        "    max_len = lengths.max()\n",
        "\n",
        "    # Preallocate padded tensor\n",
        "    # padded = torch.full((len(batch), max_len), pad_value, dtype=batch[0].dtype)\n",
        "    padded = torch.full((len(batch), max_len), pad_value)\n",
        "\n",
        "    # for i, x in enumerate(batch):\n",
        "    #     padded[i, -x.size(0):] = x  # align to right, pad left\n",
        "    padded[torch.arange(len(batch)).unsqueeze(-1), -lengths:] = batch\n",
        "    return padded, lengths\n",
        "\n",
        "# left_pad(train_tok)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HZejCTckoQG",
        "outputId": "8a7253cf-65bc-471e-8b90-3c1590594fc3",
        "cellView": "form"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[31373, 995] hello world\n"
          ]
        }
      ],
      "source": [
        "# @title tiktoken\n",
        "# https://github.com/openai/tiktoken/tree/main\n",
        "import tiktoken\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\") # gpt2 r50k_base p50k_base p50k_edit cl100k_base o200k_base # https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py\n",
        "# enc = tiktoken.encoding_for_model(\"gpt-4o\") # https://github.com/openai/tiktoken/blob/main/tiktoken/model.py#L24\n",
        "tok = enc.encode(\"hello world\")\n",
        "out = enc.decode(tok)\n",
        "print(tok, out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "i1iatz1SSK3s"
      },
      "outputs": [],
      "source": [
        "# @title tiktoken dataloader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import tiktoken # https://github.com/openai/tiktoken/tree/main\n",
        "\n",
        "\n",
        "# train_text = dataset[\"train\"]['text']\n",
        "# # train_tok = [enc.encode(text) for text in train_text]\n",
        "# train_tok = [torch.tensor(enc.encode(text)) for text in train_text]\n",
        "\n",
        "\n",
        "class CharDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, raw_data, seq_len):\n",
        "        # data = ''.join(raw_data)\n",
        "        # data = raw_data['text']\n",
        "        self.enc = tiktoken.get_encoding(\"gpt2\") # https://github.com/openai/tiktoken/blob/main/tiktoken/core.py\n",
        "        self.vocab_size = self.enc.n_vocab # gpt2:50257\n",
        "        self.data = self.data_process(data) # list of int\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def data_process(self, data): # str 10780437\n",
        "        return torch.tensor(self.enc.encode(data))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)//(self.seq_len+1)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        dix = self.data[idx*(self.seq_len+1) : (idx+1)*(self.seq_len+1)]\n",
        "        x, y = dix[:-1], dix[1:]\n",
        "        return x, y\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
        "\n",
        "train_data = CharDataset(dataset[\"train\"], seq_len) # one line of poem is roughly 50 characters\n",
        "\n",
        "\n",
        "seq_len = 100 # 128\n",
        "train_data = CharDataset(text, seq_len) # one line of poem is roughly 50 characters\n",
        "test_data = CharDataset(test_text, seq_len) # one line of poem is roughly 50 characters\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 64 #512\n",
        "train_loader = DataLoader(train_data, shuffle=True, pin_memory=True, batch_size=batch_size, num_workers=2) # num_workers = 4\n",
        "test_loader = DataLoader(test_data, shuffle=True, pin_memory=True, batch_size=batch_size, num_workers=0)\n",
        "\n",
        "# https://github.com/openai/tiktoken/blob/main/tiktoken/core.py\n",
        "def encode(context):\n",
        "    if type(context) == str: return torch.tensor([train_loader.dataset.enc.encode(context)], device=device)\n",
        "    elif type(context) == list: return train_loader.dataset.enc.encode_batch(context)\n",
        "    else: raise Exception\n",
        "def decode(x): return train_loader.dataset.enc.decode(list(x))\n",
        "# for x,y in train_loader:\n",
        "#     break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVHe2tXTmacN",
        "outputId": "63efc423-8a46-490e-885b-4cdb9751cc8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 31414, 232, 2]\n",
            "[0, 20920, 232, 2]\n",
            "{'input_ids': tensor([[    0, 31414,     6,   127,  2335,    16, 11962,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.94"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# @title hf roberta\n",
        "# https://huggingface.co/docs/transformers/en/model_doc/roberta\n",
        "import torch\n",
        "from transformers import AutoTokenizer, RobertaConfig, RobertaModel\n",
        "from transformers import RobertaForMaskedLM, RobertaForSequenceClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
        "print(tokenizer(\"Hello world\")[\"input_ids\"])\n",
        "print(tokenizer(\" Hello world\")[\"input_ids\"])\n",
        "# {'input_ids': tensor([[    0,   133,   812,     9,  1470,    16, 50264,     4,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
        "\n",
        "config = RobertaConfig()\n",
        "# model = RobertaModel(config)\n",
        "model = RobertaForMaskedLM(config)\n",
        "# model = RobertaForSequenceClassification(config)\n",
        "\n",
        "# inputs = tokenizer(\"The capital of France is <mask>.\", return_tensors=\"pt\")\n",
        "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
        "print(inputs)\n",
        "with torch.no_grad():\n",
        "    # logits = model(**inputs).logits\n",
        "    logits = model(**inputs)\n",
        "\n",
        "# LM: last_hidden_state, pooler_output, hidden_states=None, past_key_values=None, attentions=None, cross_attentions\n",
        "\n",
        "\n",
        "# predicted_class_id = logits.argmax().item()\n",
        "# model.config.id2label[predicted_class_id]\n",
        "\n",
        "# # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n",
        "# num_labels = len(model.config.id2label)\n",
        "# # model = RobertaForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-emotion\", num_labels=num_labels)\n",
        "# model = RobertaForSequenceClassification(config)\n",
        "# labels = torch.tensor([1])\n",
        "# loss = model(**inputs, labels=labels).loss\n",
        "# round(loss.item(), 2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # print(logits.keys())\n",
        "# # retrieve index of <mask>\n",
        "# mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
        "\n",
        "# predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
        "# tokenizer.decode(predicted_token_id)\n",
        "\n",
        "# labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\")[\"input_ids\"]\n",
        "# # mask labels of non-<mask> tokens\n",
        "# labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)\n",
        "\n",
        "# outputs = model(**inputs, labels=labels)\n",
        "# round(outputs.loss.item(), 2)\n",
        "\n",
        "# last_hidden_states = outputs.last_hidden_state\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLiCsK97Tz3H",
        "outputId": "d839e6f6-f7bf-4d24-fdc8-4a6fe602a2ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': [0, 31414, 232, 2], 'attention_mask': [1, 1, 1, 1]}\n",
            "{'input_ids': [[0, 31414, 232, 2], [0, 36807, 571, 306, 2]], 'attention_mask': [[1, 1, 1, 1], [1, 1, 1, 1, 1]]}\n"
          ]
        }
      ],
      "source": [
        "# print(logits)\n",
        "# # loss = model(**inputs, labels=labels).loss\n",
        "# loss = model(**inputs)\n",
        "\n",
        "# MaskedLMOutput = loss, logits, hidden_states=None, attentions\n",
        "# print(tokenizer(\"Hello world\")[\"input_ids\"])\n",
        "print(tokenizer(\"Hello world\")) # input_ids attention_mask\n",
        "print(tokenizer([\"Hello world\",\"dfg4\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtKHgsmyvj5G",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title hf data\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "dataset = load_dataset(\"yelp_polarity\") # yelp_polarity yelp_review_full\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
        "\n",
        "import os\n",
        "def tokenize(examples): return tokenizer(examples[\"text\"], truncation=True, max_length=256)\n",
        "tok_dataset = dataset.map(tokenize, batched=True, num_proc=os.cpu_count(), # Use multiple processes for faster tokenization\n",
        "    remove_columns=[\"text\"] # Remove the original text column\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4yn7hlrBREhk"
      },
      "outputs": [],
      "source": [
        "# @title gemini roberta\n",
        "from transformers import AutoTokenizer, RobertaConfig, RobertaForMaskedLM\n",
        "\n",
        "# config = RobertaConfig() # vocab_size = 50265, hidden_size = 768, num_hidden_layers = 12, num_attention_heads = 12, intermediate_size = 3072, hidden_act = 'gelu', hidden_dropout_prob = 0.1, attention_probs_dropout_prob = 0.1, max_position_embeddings = 512, type_vocab_size = 2, initializer_range = 0.02, layer_norm_eps = 1e-12, pad_token_id = 1, bos_token_id = 0eos_token_id = 2, position_embedding_type = 'absolute'\n",
        "config = RobertaConfig(vocab_size = 50265, hidden_size = 64, num_hidden_layers = 1, num_attention_heads = 8, intermediate_size = 256, hidden_act = 'gelu', hidden_dropout_prob = 0., attention_probs_dropout_prob = 0.)\n",
        "model_mlm = RobertaForMaskedLM(config)\n",
        "\n",
        "\n",
        "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "collator = DataCollatorForLanguageModeling(tokenizer) # Masked Language Model (MLM); .15,.8,.1 # https://huggingface.co/docs/transformers/en/main_classes/data_collator#transformers.DataCollatorForLanguageModeling\n",
        "import torch\n",
        "\n",
        "train_args = TrainingArguments(\n",
        "    # output_dir=MODEL_OUTPUT_DIR_STAGE1, overwrite_output_dir=True,\n",
        "    num_train_epochs=1,\n",
        "    prediction_loss_only=True, # Only compute loss, no predictions during eval\n",
        "    # optim='adamw_torch',\n",
        "    optim='adamw_torch_fused',\n",
        "    learning_rate=3e-4,\n",
        "#     lr_scheduler_type (str or SchedulerType, optional, defaults to \"linear\") — The scheduler type to use. See the documentation of SchedulerType for all possible values.\n",
        "# lr_scheduler_kwargs (‘dict’, optional, defaults to {}) —\n",
        "    # warmup_steps=0.1 * NUM_TRAIN_EPOCHS_STAGE1 * (len(tokenized_dataset_mlm) // PER_DEVICE_BATCH_SIZE), # 10% warmup\n",
        "    warmup_ratio=0.1,\n",
        "    # weight_decay=0.01,\n",
        "    # report_to=\"tensorboard\",\n",
        "    report_to=\"wandb\", # 487a2109e55dce4e13fc70681781de9f50f27be7\n",
        "    fp16=torch.cuda.is_available(), # Enable mixed precision if GPU is available\n",
        ") # https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments\n",
        "\n",
        "# half_precision_backend=\"auto\"\n",
        "# bf16=True\n",
        "\n",
        "# trainer_mlm = Trainer(model=model_mlm, args=train_args, train_dataset=tok_dataset['train'].remove_columns(\"label\"), data_collator=collator)\n",
        "# trainer_mlm.train()\n",
        "\n",
        "# # trainer_mlm.save_model(MODEL_OUTPUT_DIR_STAGE1)\n",
        "\n",
        "# eval_results = trainer_mlm.evaluate()\n",
        "# perplexity = math.exp(eval_results[\"eval_loss\"])\n",
        "# print('perplexity', perplexity)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iISjeDVDugDQ",
        "outputId": "72490fdd-f071-4424-d149-d70e4a77dd4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 1, 0, 0]\n",
            "[[0, 16861, 6, 5, 8413, 9, 145, 925, 4, 18835, 18, 3186, 16, 10, 7230, 9, 5, 676, 38, 348, 56, 19, 98, 171, 97, 3333, 11, 14415, 480, 205, 3299, 6, 6587, 813, 4, 1437, 85, 1302, 14, 39, 813, 1622, 393, 5274, 5, 1028, 4, 1437, 85, 2333, 1239, 132, 722, 9, 6636, 1765, 7, 120, 41, 1948, 4, 1437, 3394, 34, 86, 13, 14, 50, 1072, 7, 432, 19, 24, 116, 1437, 38, 33, 422, 88, 42, 936, 19, 171, 97, 3333, 8, 38, 95, 218, 75, 120, 24, 4, 1437, 370, 33, 558, 1138, 6, 47, 33, 1484, 19, 1131, 782, 6, 596, 965, 75, 1268, 15635, 5, 1028, 116, 1437, 85, 18, 42494, 8, 45, 173, 5, 29223, 1258, 4, 1437, 85, 18, 19, 9917, 14, 38, 619, 14, 38, 33, 7, 492, 925, 4, 18835, 132, 2690, 4, 2], [0, 9325, 225, 164, 7, 925, 4, 18835, 13, 81, 158, 107, 4, 38, 206, 38, 21, 65, 9, 39, 112, 620, 1484, 77, 37, 554, 23, 24294, 21963, 4, 91, 18, 57, 372, 81, 5, 107, 8, 16, 269, 70, 59, 5, 380, 2170, 4, 85, 16, 142, 9, 123, 6, 45, 127, 122, 320, 821, 3892, 925, 4, 1190, 1529, 6, 14, 38, 303, 66, 38, 33, 19961, 1001, 7823, 4, 91, 17384, 70, 1735, 19, 47, 8, 16, 182, 3186, 8, 2969, 4, 91, 630, 75, 1679, 8, 6990, 70, 5, 235, 1142, 4, 12178, 10675, 8, 1072, 7, 28, 1682, 11, 5, 14018, 15, 358, 6659, 9, 110, 1131, 474, 8, 110, 301, 4, 2], [0, 100, 218, 75, 216, 99, 925, 4, 18835, 21, 101, 137, 1437, 1375, 7, 2605, 6, 53, 905, 162, 1137, 47, 6, 4062, 2547, 18463, 2547, 31, 42, 3299, 8, 42, 558, 4, 38, 21, 164, 7, 925, 4, 1436, 137, 37, 314, 8, 18835, 362, 81, 77, 1436, 314, 4, 91, 16, 45, 10, 10837, 3299, 4, 91, 16, 129, 2509, 11, 5, 1029, 12, 14066, 8, 519, 47, 283, 11, 13, 8456, 4885, 5622, 358, 353, 4, 91, 40, 45, 492, 4885, 5622, 8, 115, 540, 59, 1484, 18, 613, 5458, 4, 28386, 7, 120, 110, 1814, 360, 7107, 409, 15288, 20400, 149, 42, 2173, 16, 10, 8018, 4, 178, 7, 146, 3510, 190, 3007, 6, 39, 558, 813, 16, 30740, 4, 1814, 207, 9, 5, 86, 77, 47, 486, 5, 558, 6, 51, 581, 342, 47, 149, 7, 10, 2236, 7107, 6, 14, 8228, 19551, 655, 5274, 50, 2886, 110, 486, 4, 1868, 127, 4194, 408, 8, 1623, 33, 1276, 7, 989, 42, 1524, 71, 7242, 215, 8413, 4, 20, 1445, 558, 34, 41, 6784, 101, 51, 32, 608, 47, 10, 4402, 4, 12192, 162, 10, 1108, 328, 9631, 409, 31, 42, 22053, 8, 5, 1524, 4, 370, 6565, 357, 8, 51, 40, 45, 28, 89, 77, 47, 269, 240, 106, 4, 38, 33, 393, 1299, 19411, 7, 3116, 10, 1099, 1551, 59, 1268, 454, 38, 1145, 42, 31790, 10525, 13, 10, 3299, 54, 16, 70, 59, 5, 418, 4, 2], [0, 100, 437, 2410, 42, 1551, 7, 492, 47, 10, 3885, 62, 137, 47, 192, 42, 12521, 4, 20, 558, 813, 8, 942, 32, 182, 542, 23878, 4, 38, 314, 10, 1579, 19, 1533, 82, 2624, 127, 1087, 6, 8, 117, 65, 655, 373, 162, 124, 4, 38, 56, 7, 1368, 9834, 106, 7, 120, 41, 1948, 59, 127, 1087, 4, 44128, 282, 37457, 282, 32703, 6, 8, 144, 505, 6, 146, 686, 110, 1911, 16, 164, 7, 1719, 925, 4, 18835, 18, 5695, 8, 1925, 173, 4, 91, 5131, 7, 162, 14, 38, 120, 10, 2166, 6, 8, 37, 1467, 38, 21, 10, 1294, 142, 38, 174, 123, 4, 38, 300, 5, 2166, 626, 4, 6811, 6, 38, 303, 66, 127, 474, 1911, 630, 75, 582, 13, 2097, 3693, 5695, 4, 38, 829, 41, 68, 3913, 4, 612, 1087, 13, 5, 1925, 173, 4, 38, 64, 75, 582, 13, 127, 1087, 142, 38, 437, 10, 1294, 8, 218, 75, 33, 143, 1055, 3041, 23, 42, 595, 86, 4, 38, 64, 75, 679, 5, 12521, 1979, 75, 492, 162, 10, 3885, 62, 7, 146, 686, 127, 1911, 74, 1719, 173, 14, 938, 75, 2139, 8, 21, 14657, 2097, 3693, 4, 20, 558, 64, 75, 109, 932, 7, 244, 162, 1719, 5, 1087, 4, 96, 1285, 6, 5, 558, 813, 26, 5, 15, 687, 16, 15, 162, 7, 146, 686, 127, 1911, 4865, 5695, 4, 4967, 4193, 21172, 1068, 328, 2]]\n",
            "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
          ]
        }
      ],
      "source": [
        "print(tok_dataset['train']['label'][:4])\n",
        "print(tok_dataset['train']['input_ids'][:4])\n",
        "print(tok_dataset['train']['attention_mask'][:4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "gy1anCqbHN59"
      },
      "outputs": [],
      "source": [
        "# @title chatgpt roberta\n",
        "from transformers import RobertaConfig, RobertaForMaskedLM, RobertaTokenizerFast\n",
        "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")  # small for testing\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
        "def tokenize_function(example): return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
        "\n",
        "config = RobertaConfig(vocab_size=tokenizer.vocab_size, max_position_embeddings=128, num_attention_heads=8, num_hidden_layers=6, hidden_size=512, intermediate_size=2048)\n",
        "model = RobertaForMaskedLM(config)\n",
        "\n",
        "training_args = TrainingArguments(output_dir=\"./roberta-small\", overwrite_output_dir=True, num_train_epochs=5, per_device_train_batch_size=16, evaluation_strategy=\"no\", save_steps=10_000, save_total_limit=2, logging_steps=500)\n",
        "trainer = Trainer(model=model, args=training_args, train_dataset=tokenized_datasets[\"train\"], tokenizer=tokenizer, data_collator=data_collator)\n",
        "\n",
        "trainer.train()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pt98LQu2EC4K"
      },
      "source": [
        "## trash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WGmRZ_ojEFGc"
      },
      "outputs": [],
      "source": [
        "# @title facebookresearch/mae models_mae.py\n",
        "# https://github.com/facebookresearch/mae/blob/main/models_mae.py\n",
        "from functools import partial\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from timm.models.vision_transformer import PatchEmbed, Block\n",
        "from util.pos_embed import get_2d_sincos_pos_embed\n",
        "\n",
        "\n",
        "class MaskedAutoencoderViT(nn.Module):\n",
        "    \"\"\" Masked Autoencoder with VisionTransformer backbone\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3,\n",
        "                 embed_dim=1024, depth=24, num_heads=16,\n",
        "                 decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
        "                 mlp_ratio=4., norm_layer=nn.LayerNorm, norm_pix_loss=False):\n",
        "        super().__init__()\n",
        "\n",
        "        # --------------------------------------------------------------------------\n",
        "        # MAE encoder specifics\n",
        "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True, qk_scale=None, norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "        # --------------------------------------------------------------------------\n",
        "\n",
        "        # --------------------------------------------------------------------------\n",
        "        # MAE decoder specifics\n",
        "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
        "        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, decoder_embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
        "        self.decoder_blocks = nn.ModuleList([\n",
        "            Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=True, qk_scale=None, norm_layer=norm_layer)\n",
        "            for i in range(decoder_depth)])\n",
        "\n",
        "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
        "        self.decoder_pred = nn.Linear(decoder_embed_dim, patch_size**2 * in_chans, bias=True) # decoder to patch\n",
        "        # --------------------------------------------------------------------------\n",
        "        self.norm_pix_loss = norm_pix_loss\n",
        "        self.initialize_weights()\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        # initialization\n",
        "        # initialize (and freeze) pos_embed by sin-cos embedding\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=True)\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "\n",
        "        decoder_pos_embed = get_2d_sincos_pos_embed(self.decoder_pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=True)\n",
        "        self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).float().unsqueeze(0))\n",
        "\n",
        "        # initialize patch_embed like nn.Linear (instead of nn.Conv2d)\n",
        "        w = self.patch_embed.proj.weight.data\n",
        "        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
        "\n",
        "        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n",
        "        torch.nn.init.normal_(self.cls_token, std=.02)\n",
        "        torch.nn.init.normal_(self.mask_token, std=.02)\n",
        "\n",
        "        # initialize nn.Linear and nn.LayerNorm\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            # we use xavier_uniform following official JAX ViT:\n",
        "            torch.nn.init.xavier_uniform_(m.weight)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    def patchify(self, imgs): # [b,3,h,w]\n",
        "        p = self.patch_embed.patch_size[0]\n",
        "        assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0\n",
        "        h = w = imgs.shape[2] // p\n",
        "        x = imgs.reshape(imgs.shape[0], 3, h, p, w, p) # [b,3,h/p,p,w/p,p]\n",
        "        x = torch.einsum('nchpwq->nhwpqc', x) # [b,h/p,w/p,p,p,3]\n",
        "        x = x.reshape(imgs.shape[0], h * w, p**2 * 3) # [b, h/p *w/p, p*p*3]\n",
        "        return x # [b, h/p *w/p, p*p*3] ~ [b,t,d]\n",
        "\n",
        "    def unpatchify(self, x):\n",
        "        \"\"\"\n",
        "        x: (N, L, patch_size**2 *3)\n",
        "        imgs: (N, 3, H, W)\n",
        "        \"\"\"\n",
        "        p = self.patch_embed.patch_size[0]\n",
        "        h = w = int(x.shape[1]**.5)\n",
        "        assert h * w == x.shape[1]\n",
        "\n",
        "        x = x.reshape(shape=(x.shape[0], h, w, p, p, 3))\n",
        "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
        "        imgs = x.reshape(shape=(x.shape[0], 3, h * p, h * p))\n",
        "        return imgs\n",
        "\n",
        "    def random_masking(self, x, mask_ratio):\n",
        "        \"\"\"\n",
        "        Perform per-sample random masking by per-sample shuffling.\n",
        "        Per-sample shuffling is done by argsort random noise.\n",
        "        x: [N, L, D], sequence\n",
        "        \"\"\"\n",
        "        N, L, D = x.shape  # batch, length, dim\n",
        "        len_keep = int(L * (1 - mask_ratio))\n",
        "\n",
        "        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n",
        "\n",
        "        # sort noise for each sample\n",
        "        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
        "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
        "\n",
        "        # keep the first subset\n",
        "        ids_keep = ids_shuffle[:, :len_keep]\n",
        "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
        "\n",
        "        # generate the binary mask: 0 is keep, 1 is remove\n",
        "        mask = torch.ones([N, L], device=x.device)\n",
        "        mask[:, :len_keep] = 0\n",
        "        # unshuffle to get the binary mask\n",
        "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
        "\n",
        "        return x_masked, mask, ids_restore\n",
        "\n",
        "    def forward_encoder(self, x, mask_ratio):\n",
        "        # embed patches\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        # add pos embed w/o cls token\n",
        "        x = x + self.pos_embed[:, 1:, :]\n",
        "\n",
        "        # masking: length -> length * mask_ratio\n",
        "        x, mask, ids_restore = self.random_masking(x, mask_ratio)\n",
        "\n",
        "        # append cls token\n",
        "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
        "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        # apply Transformer blocks\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        return x, mask, ids_restore\n",
        "\n",
        "    def forward_decoder(self, x, ids_restore):\n",
        "        # embed tokens\n",
        "        x = self.decoder_embed(x)\n",
        "\n",
        "        # append mask tokens to sequence\n",
        "        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
        "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
        "        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
        "        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
        "\n",
        "        # add pos embed\n",
        "        x = x + self.decoder_pos_embed\n",
        "\n",
        "        # apply Transformer blocks\n",
        "        for blk in self.decoder_blocks:\n",
        "            x = blk(x)\n",
        "        x = self.decoder_norm(x)\n",
        "\n",
        "        # predictor projection\n",
        "        x = self.decoder_pred(x)\n",
        "\n",
        "        # remove cls token\n",
        "        x = x[:, 1:, :]\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward_loss(self, imgs, pred, mask):\n",
        "        \"\"\"\n",
        "        imgs: [N, 3, H, W]\n",
        "        pred: [N, L, p*p*3]\n",
        "        mask: [N, L], 0 is keep, 1 is remove,\n",
        "        \"\"\"\n",
        "        target = self.patchify(imgs)\n",
        "        if self.norm_pix_loss:\n",
        "            mean = target.mean(dim=-1, keepdim=True)\n",
        "            var = target.var(dim=-1, keepdim=True)\n",
        "            target = (target - mean) / (var + 1.e-6)**.5\n",
        "\n",
        "        loss = (pred - target) ** 2\n",
        "        loss = loss.mean(dim=-1)  # [N, L], mean loss per patch\n",
        "\n",
        "        loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches\n",
        "        return loss\n",
        "\n",
        "    def forward(self, imgs, mask_ratio=0.75):\n",
        "        latent, mask, ids_restore = self.forward_encoder(imgs, mask_ratio)\n",
        "        pred = self.forward_decoder(latent, ids_restore)  # [N, L, p*p*3]\n",
        "        loss = self.forward_loss(imgs, pred, mask)\n",
        "        return loss, pred, mask\n",
        "\n",
        "\n",
        "def mae_vit_base_patch16_dec512d8b(**kwargs):\n",
        "    model = MaskedAutoencoderViT(\n",
        "        patch_size=16, embed_dim=768, depth=12, num_heads=12,\n",
        "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
        "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def mae_vit_large_patch16_dec512d8b(**kwargs):\n",
        "    model = MaskedAutoencoderViT(\n",
        "        patch_size=16, embed_dim=1024, depth=24, num_heads=16,\n",
        "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
        "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def mae_vit_huge_patch14_dec512d8b(**kwargs):\n",
        "    model = MaskedAutoencoderViT(\n",
        "        patch_size=14, embed_dim=1280, depth=32, num_heads=16,\n",
        "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
        "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "# set recommended archs\n",
        "mae_vit_base_patch16 = mae_vit_base_patch16_dec512d8b  # decoder: 512 dim, 8 blocks\n",
        "mae_vit_large_patch16 = mae_vit_large_patch16_dec512d8b  # decoder: 512 dim, 8 blocks\n",
        "mae_vit_huge_patch14 = mae_vit_huge_patch14_dec512d8b  # decoder: 512 dim, 8 blocks\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "0vScvFGGSeN6"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}