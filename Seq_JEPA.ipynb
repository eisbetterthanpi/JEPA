{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/Seq_JEPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XwpnHW4wn9S1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8624f63-c17b-410c-ff6f-061119147138"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# @title data\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "transform = transforms.Compose([transforms.ToTensor(),])\n",
        "\n",
        "# train_data = torchvision.datasets.MNIST(root=\"data\", train=True, download=True,transform=transform) # do not normalise! want img in [0,1)\n",
        "# test_data = torchvision.datasets.MNIST(root=\"data\", train=False, download=True, transform=transform) #opt no download\n",
        "# # batch_size = 32 # 64 512\n",
        "# batch_size = 32 if torch.cuda.is_available() else 32\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "train_data = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
        "test_data = torchvision.datasets.CIFAR10(root='data', train=False, download=True, transform=transform)\n",
        "batch_size = 128 # 128 1024\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "\n",
        "# dataiter = iter(train_data)\n",
        "# x,y = next(dataiter)\n",
        "# print(x.shape) # [3, 32, 32]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title (Learned)RotEmb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, seq_len=512, base=10000):\n",
        "        super().__init__()\n",
        "        self.dim, self.base = dim, base\n",
        "        theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "        pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "        angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "        self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.base)\n",
        "        return x * self.rot_emb[:seq_len]\n",
        "\n",
        "# class LearnedRoPE(nn.Module): # learnt RoPE ; each tok is 1 pos\n",
        "#     def __init__(self, dim):\n",
        "#         super().__init__()\n",
        "#         self.weights = nn.Parameter(torch.randn(1, dim//2))\n",
        "\n",
        "#     def forward(self, x): #\n",
        "#         batch, seq_len, dim = x.shape\n",
        "#         # if rot_emb.shape[0] < seq_len: self.__init__(dim, seq_len)\n",
        "#         pos = torch.arange(seq_len).unsqueeze(1)\n",
        "#         angles = (self.weights * pos * 2*torch.pi).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "#         rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "#         return x * rot_emb.flatten(-2).unsqueeze(0)\n",
        "\n",
        "\n",
        "class LearnedRoPE(nn.Module): # learnt RoPE ; each tok is 1 pos\n",
        "    def __init__(self, dim, seq_len=512):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.weights = nn.Parameter(torch.randn(1, dim//2))\n",
        "        pos = torch.arange(seq_len).unsqueeze(1)\n",
        "        angles = (self.weights * pos * 2*torch.pi)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "        self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "    def forward(self, x): # [batch, seq_len, dim]\n",
        "        seq_len = x.size(1)\n",
        "        if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len)\n",
        "        return x * self.rot_emb[:seq_len]\n",
        "\n",
        "\n",
        "class RotEmb(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, top=torch.pi, base=10000):\n",
        "        super().__init__()\n",
        "        self.theta = top / (base ** (torch.arange(0, dim, step=2, device=device) / dim))\n",
        "        # self.theta = top / (base ** torch.linspace(0, 1, dim//2, device=device))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (pos.unsqueeze(-1) * self.theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [seq_len, dim]\n",
        "\n",
        "class LearnedRotEmb(nn.Module): # pos in R\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.weights = nn.Parameter(torch.randn((1, dim//2), device=device))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (self.weights * pos.unsqueeze(-1) * 2*torch.pi).unsqueeze(-1) # [batch, 1] * [1, dim//2] -> [batch, dim//2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [batch, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [batch, dim]\n",
        "\n"
      ],
      "metadata": {
        "id": "npc_xGtOz7DC",
        "cellView": "form"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ViT me simple\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    \"\"\"Zero out the parameters of a module and return it.\"\"\"\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads=None, d_head=8, cond_dim=None, dropout=0.): # .1\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_head = d_head\n",
        "        self.n_heads = d_model // d_head\n",
        "        # self.d_head = d_model // n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or d_model, 2*d_model, bias=False)\n",
        "        # self.k = nn.Sequential(nn.Dropout(dropout), nn.Linear(cond_dim, d_model, bias=False))\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        # self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        # self.lin = nn.Sequential(nn.Dropout(dropout), zero_module(nn.Linear(d_model, d_model)))\n",
        "        self.drop = nn.Dropout(dropout) # indp before q,k,v; after linout\n",
        "        self.scale = self.d_head ** -.5\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [batch, T, d_model]=[batch, h*w, c], [batch, num_tok, cond_dim], [batch,T]\n",
        "        batch = x.shape[0]\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        Q = self.q(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        # K = self.k(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        K, V = self.kv(cond).view(batch, -1, self.n_heads, 2*self.d_head).transpose(1, 2).chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # linear attention # Softmax(Q) @ (Softmax(K).T @ V)\n",
        "        if mask != None:\n",
        "            mask = mask[:, None, :, None] # [batch,T] -> [batch,1,T,1]\n",
        "            K, V = K.masked_fill(mask, -torch.finfo(x.dtype).max), V.masked_fill(mask, -torch.finfo(x.dtype).max)\n",
        "        Q, K = Q.softmax(dim=-1)*self.scale, K.softmax(dim=-2)\n",
        "        context = K.transpose(-2,-1) @ V # [batch, n_heads, d_head, d_head]\n",
        "        out = Q @ context # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # (quadratic) attention # Softmax(Q @ K.T) @ V\n",
        "        # attn = Q @ K.transpose(-2,-1) * self.scale # [batch, n_heads, T] # [batch, n_heads, T, T/num_tok]\n",
        "        # if mask != None: attn = attn.masked_fill(mask[:, None, :, None], -torch.finfo(attn.dtype).max) # [batch,T]->[batch,1,T,1]\n",
        "        # attention = torch.softmax(attn, dim=-1)\n",
        "        # out = self.drop(attention) @ V # [batch, n_heads, T, d_head]\n",
        "\n",
        "        out = out.transpose(1, 2).flatten(2)\n",
        "        return self.drop(self.lin(out)) # [batch, T, d_model]\n",
        "\n",
        "# if self, dont pass cond_dim in init, dont pass cond in fwd\n",
        "# Softmax(Q @ K.T) @ V ~ Softmax(Q) @ Softmax(K).T @ V\n",
        "\n",
        "# https://github.com/lucidrains/x-transformers/blob/main/x_transformers/x_transformers.py#L1855\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_head=4, cond_dim=None, ff_mult=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm1 = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.norm2 = nn.RMSNorm(d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.cond_dim = cond_dim\n",
        "        self.self = MultiHeadAttention(d_model, d_head=d_head, dropout=0)\n",
        "        if self.cond_dim!=None: self.cross = MultiHeadAttention(d_model, d_head=d_head, cond_dim=cond_dim, dropout=0)\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        ff_dim=d_model*ff_mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), nn.Dropout(dropout), nn.Linear(d_model, ff_dim), act, # ReLU GELU\n",
        "            nn.RMSNorm(d_model), nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(dropout), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            # nn.RMSNorm(d_model), act, nn.Dropout(dropout), nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,c,h,w], [batch, num_tok, cond_dim], [batch,T]\n",
        "        # bchw = x.shape\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "\n",
        "        if self.cond_dim==None: cond=None # is self attn\n",
        "        x = x + self.drop(self.self(self.norm1(x)))\n",
        "        if self.cond_dim!=None: x = x + self.cross(self.norm2(x), cond, mask)\n",
        "        x = x + self.ff(x)\n",
        "        # x = x + self.drop(self.ff(x))\n",
        "\n",
        "        # return x.transpose(1,2).reshape(*bchw)\n",
        "        return x\n",
        "\n",
        "\n",
        "# class SimpleViT(nn.Module):\n",
        "#     def __init__(self, in_dim, out_dim, dim, depth, heads, mlp_dim, channels=3, dim_head=8):\n",
        "#         super().__init__()\n",
        "#         self.to_patch_embedding = nn.Sequential( # in, out, kernel, stride, pad\n",
        "#             nn.Conv2d(in_dim, dim, kernel_size=7, stride=2, padding=7//2, bias=False), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "#             # nn.Conv2d(in_dim, dim, kernel_size=7, stride=2, padding=7//2, bias=False), nn.MaxPool2d(2,2)\n",
        "#             )\n",
        "#         # self.pos_embedding = LearnedRoPE2D(dim) # LearnedRoPE2D, RoPE2D\n",
        "#         self.positional_emb = nn.Parameter(torch.zeros(1, 8*8, dim)) # positional_embedding == 'learnable'\n",
        "#         self.transformer = AttentionBlock(d_model=dim, d_head=dim_head)\n",
        "#         self.transformer_blocks = nn.ModuleList([AttentionBlock(d_model=dim, d_head=dim_head) for i in range(depth)])\n",
        "\n",
        "#         self.attention_pool = nn.Linear(dim, 1)\n",
        "#         self.out = nn.Linear(dim, out_dim, bias=False)\n",
        "\n",
        "#     def forward(self, img):\n",
        "#         device = img.device\n",
        "#         x = self.to_patch_embedding(img)\n",
        "#         # x = self.pos_embedding(x)\n",
        "#         x = x.flatten(-2).transpose(-2,-1) # b c h w -> b (h w) c\n",
        "#         x = x + self.positional_emb\n",
        "#         x = self.transformer(x)\n",
        "#         # for blk in self.predictor_blocks: x = blk(x)\n",
        "\n",
        "#         # x = x.flatten(-2).mean(dim=-1) # meal pool\n",
        "#         attn_weights = self.attention_pool(x).squeeze(-1) # [batch, (h,w)] # seq_pool\n",
        "#         x = (attn_weights.softmax(dim = 1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, (h,w)] @ [batch, (h,w), dim]\n",
        "#         # cls_token = repeat(self.class_emb, '1 1 d -> b 1 d', b = b)\n",
        "#         # x = torch.cat((cls_token, x), dim=1)\n",
        "#         # x = x[:, 0] # first token\n",
        "#         return self.out(x)\n",
        "\n",
        "# ijepa: 224*224 -> 14*14 = 196\n",
        "# 32*7\n",
        "# me: 32*32=1024 -> 256\n",
        "\n",
        "import torch\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "def apply_masks(x, masks):\n",
        "    \"\"\":param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
        "    :param masks: list of tensors containing indices of patches in [N] to keep\"\"\"\n",
        "    all_x = []\n",
        "    for m in masks: # [1,T]\n",
        "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [1,T,dim]\n",
        "        all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "    return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "# batch, seq_len, d_model = 2,7,4\n",
        "# x = torch.rand(batch, seq_len, d_model)\n",
        "# print(x)\n",
        "# masks = [torch.randint(0,seq_len,(2,3,))]\n",
        "# print(masks)\n",
        "# out = apply_masks(x, masks)\n",
        "# print(out)\n",
        "# # print(out.shape)\n",
        "\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model, d_model//d_head, d_hid or d_model, dropout, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        # self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.zeros(1,1,d_model)) # randn zeros\n",
        "        nn.init.trunc_normal_(self.cls, std=.02)\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.lin = nn.Linear(d_model, out_dim)# if out_dim != d_model else None\n",
        "\n",
        "    def forward(self, src, context_indices, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        src = self.embed(src) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = src.shape\n",
        "        src = src * self.pos_encoder(context_indices) # context/predictor # src = src + self.positional_emb[:,context_indices]\n",
        "\n",
        "        # src = apply_masks(src, [context_indices])\n",
        "\n",
        "        pred_tokens = self.cls * self.pos_encoder(trg_indices) # [M, num_trg_toks, d_model] # pred_tokens = self.cls + self.positional_emb[0,trg_indices]\n",
        "        pred_tokens = pred_tokens.repeat(batch, 1, 1) # [batch*M, num_trg_toks, d_model]\n",
        "        # print(pred_tokens.requires_grad)\n",
        "        # print(\"pred fwd\", src.shape, pred_tokens.shape)\n",
        "        # src = src.repeat_interleave(trg_indices.shape[0], dim=0) # [batch, seq_len, d_model] -> [batch*M, seq_len, d_model]\n",
        "        src = torch.cat([src, pred_tokens], dim=1) # [batch*M, seq_len+num_trg_toks, d_model]\n",
        "\n",
        "        out = self.transformer_encoder(src) # float [seq_len, batch_size, d_model]\n",
        "        out = self.norm(out)\n",
        "        out = out[:,seq:] # [batch*M, num_trg_toks, d_model]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.MaxPool1d(2,2), #nn.MaxPool1d(3, 2, 3//2),\n",
        "            nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.Conv1d(d_model, d_model,3,2,3//2)\n",
        "            # nn.Conv1d(in_dim, d_model,2,2,0), # like patch\n",
        "            )\n",
        "        self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model, d_model//d_head, d_hid or d_model, dropout, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        # self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, src, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # src = self.embed(src) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        src = self.embed(src.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "\n",
        "        batch, seq, dim = src.shape\n",
        "\n",
        "        # # # src = self.pos_encoder(src)\n",
        "        # if context_indices != None:\n",
        "        # print(src.shape, self.pos_encoder(context_indices).shape)\n",
        "        #     src = src * self.pos_encoder(context_indices) # context/predictor # src = src + self.positional_emb[:,context_indices]\n",
        "        # else: src = src * self.pos_encoder(torch.arange(seq, device=device).unsqueeze(0)) # target # src = src + self.positional_emb[:,:seq]\n",
        "\n",
        "\n",
        "        # print(\"transmodl fwd\",src.shape, self.pos_encoder(torch.arange(seq, device=device)).shape)\n",
        "        # src = self.pos_encoder(src)\n",
        "        src = src * self.pos_encoder(torch.arange(seq, device=device)).unsqueeze(0) # target # src = src + self.positional_emb[:,:seq]\n",
        "        if context_indices != None:\n",
        "            src = apply_masks(src, [context_indices])\n",
        "\n",
        "\n",
        "        out = self.transformer_encoder(src) # float [seq_len, batch_size, d_model]\n",
        "        out = self.norm(out)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "batch, seq_len, d_model = 4,128,4\n",
        "in_dim = 3\n",
        "model = TransformerModel(in_dim, d_model, d_head=4, nlayers=2, dropout=0.).to(device)\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "# # print(out)\n",
        "\n"
      ],
      "metadata": {
        "id": "GHzr3NVs2EcG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af939a90-a6a2-44e5-f0fe-a65e56b61cb2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 32, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title SeqJEPA mae like\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def multiblock(seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "    return target_mask\n",
        "\n",
        "# https://arxiv.org/pdf/2210.07224\n",
        "def randpatch(seq, mask_size=4, gamma=0.75): # num patches of seq, mask patch size, masking ratio\n",
        "    mask = torch.rand(seq//mask_size)<gamma\n",
        "    mask = mask.repeat_interleave(mask_size, dim=-1)\n",
        "    return mask # [seq] , True -> mask\n",
        "\n",
        "class SeqJEPA(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, nlayers=2, d_head=4):\n",
        "        super().__init__()\n",
        "        if out_dim is None: out_dim = d_model\n",
        "        self.patch_size = 4\n",
        "        self.context_encoder = TransformerModel(in_dim, d_model, out_dim=out_dim, d_head=d_head, nlayers=nlayers, dropout=0.)\n",
        "        # self.predicter = TransformerPredictor(out_dim, d_model//2, out_dim, d_head=d_head, nlayers=1, dropout=0.)\n",
        "        self.predicter = TransformerPredictor(out_dim, d_model//2, out_dim, d_head=d_head, nlayers=nlayers//2, dropout=0.)\n",
        "        import copy\n",
        "        self.target_encoder = copy.deepcopy(self.context_encoder)\n",
        "        self.target_encoder.requires_grad_(False)\n",
        "        self.classifier = nn.Linear(out_dim, 10)\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]\n",
        "        batch, seq, dim = x.shape\n",
        "        target_mask = randpatch(seq//self.patch_size, mask_size=16, gamma=.75) # [seq]\n",
        "        context_mask = ~multiblock(seq//self.patch_size, min_s=0.85, max_s=1., M=1)|target_mask # [1, seq], True->Mask\n",
        "        target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "\n",
        "        context_indices = (~context_mask[0]).nonzero().squeeze(-1) # int idx [num_context_toks] , idx of context not masked\n",
        "        sx = self.context_encoder(x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "\n",
        "        trg_indices = target_mask.nonzero().squeeze(-1) # int idx [1, num_trg_toks] , idx of targets that are masked\n",
        "        sy_ = self.predicter(sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "\n",
        "        # with torch.no_grad(): sy = self.target_encoder(x.detach())[torch.arange(batch).unsqueeze(-1), trg_indices.repeat(batch,1)] # [batch, num_trg_toks, out_dim]\n",
        "        with torch.no_grad():\n",
        "            sy = self.target_encoder(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "            # print(sy.shape, trg_indices.shape)\n",
        "            sy = apply_masks(sy, [trg_indices])\n",
        "\n",
        "        # print(x[0][0][:8])\n",
        "        # print(sy_[0][0][:8])\n",
        "        # print(sy.shape, sy_.shape)\n",
        "        # loss = F.smooth_l1_loss(sy, sy_)\n",
        "        loss = F.mse_loss(sy.detach(), sy_)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        sx = self.target_encoder(x)\n",
        "        out = sx.mean(dim=1)\n",
        "        return out\n",
        "\n",
        "\n",
        "    def classify(self, x): # [batch, T, 3]\n",
        "        sx = self.target_encoder(x).mean(dim=1)\n",
        "        out = self.classifier(sx)\n",
        "        return out\n",
        "\n",
        "# min_s=0.15, max_s, M\n",
        "# trg.15.2M4 C.85 1\n",
        "\n",
        "seq_jepa = SeqJEPA(in_dim=3, d_model=32, out_dim=16, nlayers=6, d_head=4).to(device)#.to(torch.float)\n",
        "optim = torch.optim.AdamW(seq_jepa.parameters(), lr=1e-3) # 1e-3?\n",
        "print(sum(p.numel() for p in seq_jepa.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.parameters())) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.predicter.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.context_encoder.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.target_encoder.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# d_model^2 * nlayers\n",
        "\n",
        "x = torch.rand((2,1024,3), device=device)\n",
        "out = seq_jepa.loss(x)\n",
        "print(out.shape)\n",
        "# print(x.is_leaf) # T\n"
      ],
      "metadata": {
        "id": "xy4iC46Vnlog",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7af8e3d-0480-48c6-eb2c-1dd683d56f61"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48714\n",
            "torch.Size([])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(16).to(device)\n",
        "# coptim = torch.optim.AdamW(classifier.parameters(), lr=1e-3)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-4)\n",
        "# optim = torch.optim.AdamW([seq_jepa.parameters(), classifier.parameters()], lr=1e-3) # 1e-3?\n",
        "optim = torch.optim.AdamW([{'params': seq_jepa.parameters()},\n",
        "                {'params': classifier.parameters(), 'lr': 1e-3}], lr=1e-3)"
      ],
      "metadata": {
        "id": "wjK1TVwBh2u8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(seq_jepa.classifier.weight[0])\n",
        "# for n, p in seq_jepa.target_encoder.named_parameters():\n",
        "#     print(n,p)\n",
        "optim.param_groups[0]['lr'] = 1e-5\n"
      ],
      "metadata": {
        "id": "7IACKDymhit7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34de8800-fa4b-4460-803c-3cd8dbfb6c70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7ff0f09-14d6-431a-d044-6e9fbf6eb047"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "strain 2.1048583984375\n",
            "strain 2.056640625\n",
            "strain 2.0264434814453125\n",
            "strain 2.104400634765625\n",
            "strain 1.96527099609375\n",
            "strain 2.08599853515625\n",
            "strain 2.023590087890625\n",
            "strain 2.1497802734375\n",
            "strain 2.021484375\n",
            "strain 1.943756103515625\n",
            "strain 2.003875732421875\n",
            "strain 1.962066650390625\n",
            "strain 2.016357421875\n",
            "strain 2.0841064453125\n",
            "strain 2.10552978515625\n",
            "strain 1.98309326171875\n",
            "strain 2.048370361328125\n",
            "strain 2.0107421875\n",
            "strain 1.997772216796875\n",
            "strain 1.949371337890625\n",
            "strain 2.100006103515625\n",
            "strain 1.9638519287109375\n",
            "strain 2.038238525390625\n",
            "strain 1.951812744140625\n",
            "strain 1.97174072265625\n",
            "strain 2.045562744140625\n",
            "strain 2.011199951171875\n",
            "strain 1.96417236328125\n",
            "strain 2.105072021484375\n",
            "strain 1.93988037109375\n",
            "strain 2.05950927734375\n",
            "strain 2.049072265625\n",
            "strain 2.116424560546875\n",
            "strain 1.950653076171875\n",
            "strain 2.0191650390625\n",
            "strain 2.045074462890625\n",
            "strain 1.98046875\n",
            "strain 2.150360107421875\n",
            "strain 2.055908203125\n",
            "strain 2.108489990234375\n",
            "strain 2.006072998046875\n",
            "strain 2.04010009765625\n",
            "strain 2.070892333984375\n",
            "strain 1.921600341796875\n",
            "strain 1.977935791015625\n",
            "strain 1.98834228515625\n",
            "strain 2.059906005859375\n",
            "strain 2.03936767578125\n",
            "strain 1.940582275390625\n",
            "strain 2.06683349609375\n",
            "strain 2.063873291015625\n",
            "strain 2.0376434326171875\n",
            "strain 2.0675048828125\n",
            "strain 2.039764404296875\n",
            "strain 1.9977874755859375\n",
            "strain 2.063568115234375\n",
            "strain 2.1142578125\n",
            "strain 2.101470947265625\n",
            "strain 2.037353515625\n",
            "strain 2.054473876953125\n",
            "strain 2.099517822265625\n",
            "strain 1.991546630859375\n",
            "strain 1.9833984375\n",
            "strain 2.098419189453125\n",
            "strain 2.01690673828125\n",
            "strain 1.999298095703125\n",
            "strain 2.07958984375\n",
            "strain 2.00958251953125\n",
            "strain 1.997406005859375\n",
            "strain 2.03173828125\n",
            "strain 2.0626220703125\n",
            "strain 2.16094970703125\n",
            "strain 2.054595947265625\n",
            "strain 2.100494384765625\n",
            "strain 2.03314208984375\n",
            "strain 2.046417236328125\n",
            "strain 2.09405517578125\n",
            "strain 1.9528350830078125\n",
            "strain 2.123199462890625\n",
            "strain 2.0224609375\n",
            "strain 2.064239501953125\n",
            "strain 2.00897216796875\n",
            "strain 2.10357666015625\n",
            "strain 2.0723876953125\n",
            "strain 1.984771728515625\n",
            "strain 2.042877197265625\n",
            "strain 1.988616943359375\n",
            "strain 2.011260986328125\n",
            "strain 1.997100830078125\n",
            "strain 2.094024658203125\n",
            "strain 2.00372314453125\n",
            "strain 2.0479736328125\n",
            "strain 2.009490966796875\n",
            "strain 2.03466796875\n",
            "strain 2.057830810546875\n",
            "strain 2.04437255859375\n",
            "strain 2.088653564453125\n",
            "strain 2.058441162109375\n",
            "strain 2.025634765625\n",
            "strain 2.0811767578125\n",
            "strain 2.00897216796875\n",
            "strain 1.9532470703125\n",
            "strain 2.026458740234375\n",
            "strain 2.057281494140625\n",
            "strain 2.040863037109375\n",
            "strain 2.106414794921875\n",
            "strain 1.976531982421875\n",
            "strain 2.1319580078125\n",
            "strain 2.194854736328125\n",
            "strain 2.0865478515625\n",
            "strain 2.127044677734375\n",
            "strain 2.0103759765625\n",
            "strain 2.04400634765625\n",
            "strain 2.12188720703125\n",
            "strain 1.964874267578125\n",
            "strain 1.988372802734375\n",
            "strain 1.989166259765625\n",
            "strain 2.06549072265625\n",
            "strain 2.014801025390625\n",
            "strain 2.026153564453125\n",
            "strain 2.0594482421875\n",
            "strain 2.06976318359375\n",
            "strain 2.075439453125\n",
            "strain 2.069000244140625\n",
            "strain 2.013458251953125\n",
            "strain 1.95672607421875\n",
            "strain 2.024383544921875\n",
            "strain 1.964996337890625\n",
            "strain 1.983245849609375\n",
            "strain 1.920379638671875\n",
            "strain 2.09466552734375\n",
            "strain 2.061004638671875\n",
            "strain 2.02569580078125\n",
            "strain 2.0503387451171875\n",
            "strain 2.043182373046875\n",
            "strain 2.0507049560546875\n",
            "strain 1.9600830078125\n",
            "strain 2.0167236328125\n",
            "strain 2.006591796875\n",
            "strain 2.03436279296875\n",
            "strain 2.01470947265625\n",
            "strain 2.0906982421875\n",
            "strain 1.9145660400390625\n",
            "strain 2.009033203125\n",
            "strain 1.9443359375\n",
            "strain 2.05975341796875\n",
            "strain 2.152374267578125\n",
            "strain 2.06103515625\n",
            "strain 2.1722412109375\n",
            "strain 1.95233154296875\n",
            "strain 2.001861572265625\n",
            "strain 1.850830078125\n",
            "strain 2.089263916015625\n",
            "strain 2.069976806640625\n",
            "strain 2.06640625\n",
            "strain 1.988861083984375\n",
            "strain 1.9473876953125\n",
            "strain 2.121002197265625\n",
            "strain 1.9500732421875\n",
            "strain 1.99407958984375\n",
            "strain 2.05084228515625\n",
            "strain 1.98736572265625\n",
            "strain 2.048309326171875\n",
            "strain 1.98175048828125\n",
            "strain 2.111785888671875\n",
            "strain 2.03466796875\n",
            "strain 2.0274658203125\n",
            "strain 2.104156494140625\n",
            "strain 1.997528076171875\n",
            "strain 2.048675537109375\n",
            "strain 2.10650634765625\n",
            "strain 2.090118408203125\n",
            "strain 1.960662841796875\n",
            "strain 2.0439453125\n",
            "strain 2.14288330078125\n",
            "strain 1.964202880859375\n",
            "strain 2.01593017578125\n",
            "strain 2.048370361328125\n",
            "strain 1.968109130859375\n",
            "strain 1.97247314453125\n",
            "strain 2.007354736328125\n",
            "strain 2.06744384765625\n",
            "strain 2.171142578125\n",
            "strain 2.02178955078125\n",
            "strain 2.019073486328125\n",
            "strain 1.91424560546875\n",
            "strain 2.1317138671875\n",
            "strain 1.984710693359375\n",
            "strain 2.037200927734375\n",
            "strain 2.029754638671875\n",
            "strain 1.944366455078125\n",
            "strain 2.0098876953125\n",
            "strain 1.945343017578125\n",
            "strain 2.014923095703125\n",
            "strain 1.98504638671875\n",
            "strain 2.0177764892578125\n",
            "strain 1.9947509765625\n",
            "strain 2.089935302734375\n",
            "strain 2.086090087890625\n",
            "strain 2.07177734375\n",
            "strain 2.038177490234375\n",
            "strain 1.998992919921875\n",
            "strain 2.067596435546875\n",
            "strain 1.98431396484375\n",
            "strain 2.1072998046875\n",
            "strain 2.0989990234375\n",
            "strain 2.046112060546875\n",
            "strain 2.0948333740234375\n",
            "strain 1.996612548828125\n",
            "strain 1.921966552734375\n",
            "strain 2.0386962890625\n",
            "strain 2.057098388671875\n",
            "strain 2.0802001953125\n",
            "strain 2.0577392578125\n",
            "strain 2.079315185546875\n",
            "strain 2.16900634765625\n",
            "strain 2.095947265625\n",
            "strain 2.112701416015625\n",
            "strain 2.12274169921875\n",
            "strain 2.072296142578125\n",
            "strain 2.01812744140625\n",
            "strain 1.994049072265625\n",
            "strain 2.036834716796875\n",
            "strain 1.969818115234375\n",
            "strain 1.975494384765625\n",
            "strain 2.030853271484375\n",
            "strain 2.134326219558716\n",
            "0.234375\n",
            "0.25\n",
            "0.234375\n",
            "0.3203125\n",
            "0.2734375\n",
            "0.2734375\n",
            "0.2578125\n",
            "0.28125\n",
            "0.2421875\n",
            "0.203125\n",
            "strain 2.049163818359375\n",
            "strain 2.061614990234375\n",
            "strain 2.0552978515625\n",
            "strain 2.030242919921875\n",
            "strain 2.046112060546875\n",
            "strain 2.14129638671875\n",
            "strain 2.09600830078125\n",
            "strain 2.024017333984375\n",
            "strain 2.10003662109375\n",
            "strain 2.009124755859375\n",
            "strain 2.04925537109375\n",
            "strain 2.04351806640625\n",
            "strain 2.03717041015625\n",
            "strain 2.083160400390625\n",
            "strain 1.9840087890625\n",
            "strain 2.075469970703125\n",
            "strain 2.0732574462890625\n",
            "strain 1.998565673828125\n",
            "strain 2.0067138671875\n",
            "strain 2.043304443359375\n",
            "strain 2.07586669921875\n",
            "strain 2.10150146484375\n",
            "strain 1.9929962158203125\n",
            "strain 2.007781982421875\n",
            "strain 1.975189208984375\n",
            "strain 2.0135498046875\n",
            "strain 2.0108642578125\n",
            "strain 2.0260009765625\n",
            "strain 2.074951171875\n",
            "strain 2.105621337890625\n",
            "strain 1.919921875\n",
            "strain 2.088531494140625\n",
            "strain 2.15655517578125\n",
            "strain 2.07281494140625\n",
            "strain 2.09423828125\n",
            "strain 1.9349365234375\n",
            "strain 1.891693115234375\n",
            "strain 2.048736572265625\n",
            "strain 1.96551513671875\n",
            "strain 1.963714599609375\n",
            "strain 2.0252685546875\n",
            "strain 2.01806640625\n",
            "strain 2.027679443359375\n",
            "strain 2.032470703125\n",
            "strain 2.043609619140625\n",
            "strain 2.04266357421875\n",
            "strain 1.98382568359375\n",
            "strain 2.020477294921875\n",
            "strain 2.02294921875\n",
            "strain 2.09600830078125\n",
            "strain 2.04119873046875\n",
            "strain 2.05218505859375\n",
            "strain 2.00372314453125\n",
            "strain 2.0570068359375\n",
            "strain 1.936676025390625\n",
            "strain 2.08013916015625\n",
            "strain 2.07476806640625\n",
            "strain 1.95819091796875\n",
            "strain 2.074310302734375\n",
            "strain 2.026031494140625\n",
            "strain 1.980438232421875\n",
            "strain 2.086212158203125\n",
            "strain 2.036590576171875\n",
            "strain 1.975341796875\n",
            "strain 1.961700439453125\n",
            "strain 1.95111083984375\n",
            "strain 1.99237060546875\n",
            "strain 1.985137939453125\n",
            "strain 2.080169677734375\n",
            "strain 2.052337646484375\n",
            "strain 2.140228271484375\n",
            "strain 1.908111572265625\n",
            "strain 2.1201171875\n",
            "strain 1.923858642578125\n",
            "strain 2.094970703125\n",
            "strain 2.0569610595703125\n",
            "strain 2.09918212890625\n",
            "strain 2.197906494140625\n",
            "strain 2.09271240234375\n",
            "strain 2.025360107421875\n",
            "strain 2.017730712890625\n",
            "strain 2.01605224609375\n",
            "strain 2.027496337890625\n",
            "strain 2.04571533203125\n",
            "strain 2.1361083984375\n",
            "strain 2.057891845703125\n",
            "strain 2.0458984375\n",
            "strain 2.002166748046875\n",
            "strain 2.01922607421875\n",
            "strain 2.142791748046875\n",
            "strain 2.123443603515625\n",
            "strain 2.0001220703125\n",
            "strain 2.0535888671875\n",
            "strain 2.088897705078125\n",
            "strain 1.9940032958984375\n",
            "strain 2.0009765625\n",
            "strain 2.05712890625\n",
            "strain 2.113739013671875\n",
            "strain 1.941558837890625\n",
            "strain 2.0216064453125\n",
            "strain 2.0142822265625\n",
            "strain 2.044097900390625\n",
            "strain 2.0032958984375\n",
            "strain 2.136077880859375\n",
            "strain 2.057891845703125\n",
            "strain 2.0369873046875\n",
            "strain 2.002593994140625\n",
            "strain 1.990478515625\n",
            "strain 1.995513916015625\n",
            "strain 1.9691619873046875\n",
            "strain 2.08477783203125\n",
            "strain 2.03558349609375\n",
            "strain 2.12945556640625\n",
            "strain 2.1383056640625\n",
            "strain 2.091064453125\n",
            "strain 2.0634765625\n",
            "strain 2.05242919921875\n",
            "strain 2.03497314453125\n",
            "strain 2.1674957275390625\n",
            "strain 1.902252197265625\n",
            "strain 2.018524169921875\n",
            "strain 2.088714599609375\n",
            "strain 1.98590087890625\n",
            "strain 2.163421630859375\n",
            "strain 2.0594329833984375\n",
            "strain 2.002349853515625\n",
            "strain 2.0616455078125\n",
            "strain 2.0367431640625\n",
            "strain 2.06280517578125\n",
            "strain 2.074951171875\n",
            "strain 1.92694091796875\n",
            "strain 2.116485595703125\n",
            "strain 1.96630859375\n",
            "strain 1.981109619140625\n",
            "strain 1.968597412109375\n",
            "strain 2.0034027099609375\n",
            "strain 2.127197265625\n",
            "strain 2.014556884765625\n",
            "strain 2.06451416015625\n",
            "strain 1.971771240234375\n",
            "strain 1.933502197265625\n",
            "strain 2.050018310546875\n",
            "strain 2.09228515625\n",
            "strain 2.0274658203125\n",
            "strain 2.12298583984375\n",
            "strain 2.111785888671875\n",
            "strain 2.015380859375\n",
            "strain 1.872650146484375\n",
            "strain 1.995391845703125\n",
            "strain 2.06268310546875\n",
            "strain 1.996978759765625\n",
            "strain 2.11676025390625\n",
            "strain 1.96295166015625\n",
            "strain 2.11614990234375\n",
            "strain 2.036834716796875\n",
            "strain 2.04608154296875\n",
            "strain 1.877716064453125\n",
            "strain 2.057373046875\n",
            "strain 2.051116943359375\n",
            "strain 2.10955810546875\n",
            "strain 2.10443115234375\n",
            "strain 2.0391845703125\n",
            "strain 2.04058837890625\n",
            "strain 1.979766845703125\n",
            "strain 2.03118896484375\n",
            "strain 2.056549072265625\n",
            "strain 2.10693359375\n",
            "strain 1.944549560546875\n",
            "strain 2.013946533203125\n",
            "strain 2.07745361328125\n",
            "strain 2.037628173828125\n",
            "strain 2.060699462890625\n",
            "strain 1.90484619140625\n",
            "strain 2.149200439453125\n",
            "strain 2.0294189453125\n",
            "strain 2.149322509765625\n",
            "strain 1.9866943359375\n",
            "strain 2.061676025390625\n",
            "strain 2.094635009765625\n",
            "strain 2.108673095703125\n",
            "strain 2.0440216064453125\n",
            "strain 1.99176025390625\n",
            "strain 2.064849853515625\n",
            "strain 2.0484619140625\n",
            "strain 2.026214599609375\n",
            "strain 2.021026611328125\n",
            "strain 2.0465087890625\n",
            "strain 2.05352783203125\n",
            "strain 1.9857177734375\n",
            "strain 2.108642578125\n",
            "strain 1.970245361328125\n",
            "strain 2.124176025390625\n",
            "strain 1.950286865234375\n",
            "strain 2.0399169921875\n",
            "strain 2.158660888671875\n",
            "strain 2.051727294921875\n",
            "strain 1.97601318359375\n",
            "strain 1.98577880859375\n",
            "strain 2.099761962890625\n",
            "strain 1.967132568359375\n",
            "strain 1.99462890625\n",
            "strain 2.02032470703125\n",
            "strain 2.021881103515625\n",
            "strain 1.995819091796875\n",
            "strain 2.026947021484375\n",
            "strain 2.017486572265625\n",
            "strain 2.00872802734375\n",
            "strain 2.061614990234375\n",
            "strain 2.061187744140625\n",
            "strain 2.052276611328125\n",
            "strain 2.001922607421875\n",
            "strain 2.01458740234375\n",
            "strain 1.990936279296875\n",
            "strain 2.113189697265625\n",
            "strain 2.14697265625\n",
            "strain 2.0303802490234375\n",
            "strain 2.047760009765625\n",
            "strain 2.051666259765625\n",
            "strain 2.008148193359375\n",
            "strain 2.000579833984375\n",
            "strain 2.044189453125\n",
            "strain 2.009063720703125\n",
            "strain 2.024810791015625\n",
            "strain 1.9649658203125\n",
            "strain 2.04913330078125\n",
            "strain 2.11077880859375\n",
            "strain 2.0020751953125\n",
            "strain 2.08319091796875\n",
            "strain 1.966644287109375\n",
            "strain 2.019622802734375\n",
            "strain 2.0535888671875\n",
            "strain 2.046142578125\n",
            "strain 1.946441650390625\n",
            "strain 2.016937255859375\n",
            "strain 2.0296630859375\n",
            "strain 2.078125\n",
            "strain 2.030303955078125\n",
            "strain 2.13604736328125\n",
            "strain 2.0633544921875\n",
            "strain 1.9765625\n",
            "strain 2.009521484375\n",
            "strain 1.97064208984375\n",
            "strain 2.127349853515625\n",
            "strain 1.9888916015625\n",
            "strain 2.046539306640625\n",
            "strain 1.9639892578125\n",
            "strain 2.0186767578125\n",
            "strain 1.959716796875\n",
            "strain 2.040924072265625\n",
            "strain 2.023681640625\n",
            "strain 2.0987548828125\n",
            "strain 2.057830810546875\n",
            "strain 2.0687255859375\n",
            "strain 2.01055908203125\n",
            "strain 2.063507080078125\n",
            "strain 1.98785400390625\n",
            "strain 2.04486083984375\n",
            "strain 2.03143310546875\n",
            "strain 1.99041748046875\n",
            "strain 2.014495849609375\n",
            "strain 2.050445556640625\n",
            "strain 2.03997802734375\n",
            "strain 1.99908447265625\n",
            "strain 2.13262939453125\n",
            "strain 2.024749755859375\n",
            "strain 2.029937744140625\n",
            "strain 1.953216552734375\n",
            "strain 2.0992431640625\n",
            "strain 2.078582763671875\n",
            "strain 2.078369140625\n",
            "strain 2.086029052734375\n",
            "strain 2.01275634765625\n",
            "strain 1.996063232421875\n",
            "strain 2.004180908203125\n",
            "strain 1.99334716796875\n",
            "strain 2.011322021484375\n",
            "strain 2.088897705078125\n",
            "strain 2.01055908203125\n",
            "strain 1.987640380859375\n",
            "strain 1.966644287109375\n",
            "strain 2.024627685546875\n",
            "strain 2.0260009765625\n",
            "strain 1.9792633056640625\n",
            "strain 1.984588623046875\n",
            "strain 2.04119873046875\n",
            "strain 2.011688232421875\n",
            "strain 2.0503082275390625\n",
            "strain 2.1207275390625\n",
            "strain 1.979522705078125\n",
            "strain 1.94775390625\n",
            "strain 2.051513671875\n",
            "strain 2.013397216796875\n",
            "strain 2.02825927734375\n",
            "strain 1.97308349609375\n",
            "strain 2.000579833984375\n",
            "strain 1.9901123046875\n",
            "strain 2.04083251953125\n",
            "strain 2.031829833984375\n",
            "strain 2.0422210693359375\n",
            "strain 2.039215087890625\n",
            "strain 1.921783447265625\n",
            "strain 1.99066162109375\n",
            "strain 2.063629150390625\n",
            "strain 1.991973876953125\n",
            "strain 2.0601806640625\n",
            "strain 2.0469970703125\n",
            "strain 2.096099853515625\n",
            "strain 2.106292724609375\n",
            "strain 2.08740234375\n",
            "strain 2.055511474609375\n",
            "strain 1.963775634765625\n",
            "strain 2.079376220703125\n",
            "strain 1.92938232421875\n",
            "strain 1.998443603515625\n",
            "strain 2.0233917236328125\n",
            "strain 2.02276611328125\n",
            "strain 2.02105712890625\n",
            "strain 2.09051513671875\n",
            "strain 2.092193603515625\n",
            "strain 2.09075927734375\n",
            "strain 2.035430908203125\n",
            "strain 1.981414794921875\n",
            "strain 2.026611328125\n",
            "strain 2.144195556640625\n",
            "strain 2.080963134765625\n",
            "strain 2.0714111328125\n",
            "strain 2.023590087890625\n",
            "strain 2.05999755859375\n",
            "strain 1.930419921875\n",
            "strain 2.04278564453125\n",
            "strain 1.994781494140625\n",
            "strain 2.04864501953125\n",
            "strain 2.092926025390625\n",
            "strain 2.108978271484375\n",
            "strain 2.01568603515625\n",
            "strain 2.010589599609375\n",
            "strain 2.146453857421875\n",
            "strain 2.017486572265625\n",
            "strain 2.088653564453125\n",
            "strain 2.15679931640625\n",
            "strain 1.99346923828125\n",
            "strain 2.10357666015625\n",
            "strain 2.074005126953125\n",
            "strain 2.00250244140625\n",
            "strain 2.02679443359375\n",
            "strain 2.03448486328125\n",
            "strain 1.95440673828125\n",
            "strain 1.9762420654296875\n",
            "strain 2.0647125244140625\n",
            "strain 1.99078369140625\n",
            "strain 1.974212646484375\n",
            "strain 1.99713134765625\n",
            "strain 2.05340576171875\n",
            "strain 2.033538818359375\n",
            "strain 2.1375732421875\n",
            "strain 1.976959228515625\n",
            "strain 2.0400390625\n",
            "strain 1.9761962890625\n",
            "strain 2.096710205078125\n",
            "strain 2.04913330078125\n",
            "strain 2.022613525390625\n",
            "strain 2.09503173828125\n",
            "strain 2.029388427734375\n",
            "strain 1.972259521484375\n",
            "strain 2.08367919921875\n",
            "strain 1.997314453125\n",
            "strain 1.9903564453125\n",
            "strain 2.0845947265625\n",
            "strain 2.01483154296875\n",
            "strain 2.06597900390625\n",
            "strain 2.05999755859375\n",
            "strain 1.94537353515625\n",
            "strain 1.974609375\n",
            "strain 1.934600830078125\n",
            "strain 2.0269775390625\n",
            "strain 1.984222412109375\n",
            "strain 2.040435791015625\n",
            "strain 1.987213134765625\n",
            "strain 1.95855712890625\n",
            "strain 2.083526611328125\n",
            "strain 2.0064697265625\n",
            "strain 2.044708251953125\n",
            "strain 2.07525634765625\n",
            "strain 1.974334716796875\n",
            "strain 2.0476531982421875\n",
            "strain 2.009765625\n",
            "strain 2.076812744140625\n",
            "strain 2.0758056640625\n",
            "strain 2.02947998046875\n",
            "strain 2.06378173828125\n",
            "strain 2.198535203933716\n",
            "0.2890625\n",
            "0.3125\n",
            "0.2890625\n",
            "0.25\n",
            "0.2578125\n",
            "0.3046875\n",
            "0.2578125\n",
            "0.2421875\n",
            "0.265625\n",
            "0.265625\n",
            "strain 1.99066162109375\n",
            "strain 1.973724365234375\n",
            "strain 2.08673095703125\n",
            "strain 2.0914306640625\n",
            "strain 2.04388427734375\n",
            "strain 2.0029296875\n",
            "strain 2.004058837890625\n",
            "strain 2.13519287109375\n",
            "strain 2.04791259765625\n",
            "strain 2.087615966796875\n",
            "strain 2.078277587890625\n",
            "strain 1.996063232421875\n",
            "strain 2.0782470703125\n",
            "strain 2.00927734375\n",
            "strain 2.061065673828125\n",
            "strain 1.917388916015625\n",
            "strain 2.02899169921875\n",
            "strain 1.9979248046875\n",
            "strain 2.093475341796875\n",
            "strain 2.0863037109375\n",
            "strain 2.158111572265625\n",
            "strain 2.0718994140625\n",
            "strain 2.12255859375\n",
            "strain 2.034423828125\n",
            "strain 2.06512451171875\n",
            "strain 1.9953155517578125\n",
            "strain 2.011871337890625\n",
            "strain 2.09259033203125\n",
            "strain 2.08441162109375\n",
            "strain 2.044342041015625\n",
            "strain 2.08258056640625\n",
            "strain 2.058837890625\n",
            "strain 2.035736083984375\n",
            "strain 2.04583740234375\n",
            "strain 2.03460693359375\n",
            "strain 2.127655029296875\n",
            "strain 2.047882080078125\n",
            "strain 2.091888427734375\n",
            "strain 2.0748291015625\n",
            "strain 2.042449951171875\n",
            "strain 2.162353515625\n",
            "strain 1.976593017578125\n",
            "strain 2.1502685546875\n",
            "strain 2.028411865234375\n",
            "strain 2.10601806640625\n",
            "strain 2.107574462890625\n",
            "strain 2.024078369140625\n",
            "strain 2.044158935546875\n",
            "strain 2.0352783203125\n",
            "strain 1.978118896484375\n",
            "strain 2.022430419921875\n",
            "strain 1.97991943359375\n",
            "strain 2.068511962890625\n",
            "strain 2.07489013671875\n",
            "strain 2.043487548828125\n",
            "strain 1.9839935302734375\n",
            "strain 2.02325439453125\n",
            "strain 1.98614501953125\n",
            "strain 1.992919921875\n",
            "strain 1.97723388671875\n",
            "strain 2.05657958984375\n",
            "strain 2.004638671875\n",
            "strain 1.95452880859375\n",
            "strain 2.057647705078125\n",
            "strain 1.982330322265625\n",
            "strain 2.0628662109375\n",
            "strain 2.044952392578125\n",
            "strain 2.0751953125\n",
            "strain 2.070037841796875\n",
            "strain 2.107025146484375\n",
            "strain 1.989166259765625\n",
            "strain 2.0243988037109375\n",
            "strain 2.0869140625\n",
            "strain 2.217742919921875\n",
            "strain 2.062164306640625\n",
            "strain 2.11273193359375\n",
            "strain 1.992034912109375\n",
            "strain 1.996978759765625\n",
            "strain 1.95947265625\n",
            "strain 2.030303955078125\n",
            "strain 1.949249267578125\n",
            "strain 2.09039306640625\n",
            "strain 1.972991943359375\n",
            "strain 1.9683837890625\n",
            "strain 2.039520263671875\n",
            "strain 1.9761962890625\n",
            "strain 2.062744140625\n",
            "strain 2.051177978515625\n",
            "strain 2.0317230224609375\n",
            "strain 2.088897705078125\n",
            "strain 1.972869873046875\n",
            "strain 2.026947021484375\n",
            "strain 1.892333984375\n",
            "strain 2.0118408203125\n",
            "strain 2.131500244140625\n",
            "strain 2.100067138671875\n",
            "strain 2.068206787109375\n",
            "strain 2.083099365234375\n",
            "strain 1.9847412109375\n",
            "strain 2.106597900390625\n",
            "strain 2.1048583984375\n",
            "strain 1.94659423828125\n",
            "strain 2.06591796875\n",
            "strain 1.96270751953125\n",
            "strain 1.994049072265625\n",
            "strain 2.042022705078125\n",
            "strain 1.96636962890625\n",
            "strain 1.952789306640625\n",
            "strain 2.062591552734375\n",
            "strain 2.041961669921875\n",
            "strain 2.1009521484375\n",
            "strain 2.0443115234375\n",
            "strain 2.021148681640625\n",
            "strain 1.9859771728515625\n",
            "strain 1.98541259765625\n",
            "strain 2.042724609375\n",
            "strain 1.970306396484375\n",
            "strain 2.041473388671875\n",
            "strain 2.06378173828125\n",
            "strain 1.934051513671875\n",
            "strain 2.0146636962890625\n",
            "strain 2.026336669921875\n",
            "strain 2.050201416015625\n",
            "strain 1.980224609375\n",
            "strain 2.02825927734375\n",
            "strain 1.983123779296875\n",
            "strain 1.986053466796875\n",
            "strain 2.100006103515625\n",
            "strain 1.932647705078125\n",
            "strain 1.994873046875\n",
            "strain 2.058349609375\n",
            "strain 2.03900146484375\n",
            "strain 2.06976318359375\n",
            "strain 1.995819091796875\n",
            "strain 2.101898193359375\n",
            "strain 2.00537109375\n",
            "strain 2.0335693359375\n",
            "strain 2.014984130859375\n",
            "strain 2.046844482421875\n",
            "strain 1.935882568359375\n",
            "strain 2.050811767578125\n",
            "strain 2.03265380859375\n",
            "strain 1.9878387451171875\n",
            "strain 1.98236083984375\n",
            "strain 2.0054931640625\n",
            "strain 2.10601806640625\n",
            "strain 2.015350341796875\n",
            "strain 1.97467041015625\n",
            "strain 2.078765869140625\n",
            "strain 1.9538726806640625\n",
            "strain 2.03570556640625\n",
            "strain 2.018798828125\n",
            "strain 2.029510498046875\n",
            "strain 2.080474853515625\n",
            "strain 2.043304443359375\n",
            "strain 2.079833984375\n",
            "strain 2.048065185546875\n",
            "strain 2.091888427734375\n",
            "strain 2.05474853515625\n",
            "strain 2.118743896484375\n",
            "strain 2.050537109375\n",
            "strain 2.012298583984375\n",
            "strain 2.106719970703125\n",
            "strain 2.081817626953125\n",
            "strain 1.9852294921875\n",
            "strain 2.04840087890625\n",
            "strain 2.041961669921875\n",
            "strain 2.00579833984375\n",
            "strain 1.923095703125\n",
            "strain 1.9537353515625\n",
            "strain 2.04962158203125\n",
            "strain 2.01385498046875\n",
            "strain 2.105194091796875\n",
            "strain 1.94085693359375\n",
            "strain 1.9925537109375\n",
            "strain 2.065277099609375\n",
            "strain 2.096099853515625\n",
            "strain 2.05914306640625\n",
            "strain 2.14013671875\n",
            "strain 1.903167724609375\n",
            "strain 1.964569091796875\n",
            "strain 2.05670166015625\n",
            "strain 2.137664794921875\n",
            "strain 2.0775146484375\n",
            "strain 1.92919921875\n",
            "strain 2.14215087890625\n",
            "strain 1.9822998046875\n",
            "strain 1.94036865234375\n",
            "strain 2.0703125\n",
            "strain 2.101348876953125\n",
            "strain 1.94891357421875\n",
            "strain 1.97833251953125\n",
            "strain 2.06317138671875\n",
            "strain 2.03973388671875\n",
            "strain 2.069091796875\n",
            "strain 2.01300048828125\n",
            "strain 2.0333251953125\n",
            "strain 2.088348388671875\n",
            "strain 2.074554443359375\n",
            "strain 1.95947265625\n",
            "strain 1.9794921875\n",
            "strain 1.99676513671875\n",
            "strain 2.06353759765625\n",
            "strain 2.04010009765625\n",
            "strain 2.153564453125\n",
            "strain 2.074798583984375\n",
            "strain 1.976806640625\n",
            "strain 2.034027099609375\n",
            "strain 1.995086669921875\n",
            "strain 2.00946044921875\n",
            "strain 2.0496826171875\n",
            "strain 1.947357177734375\n",
            "strain 2.0716552734375\n",
            "strain 2.0184326171875\n",
            "strain 2.0304412841796875\n",
            "strain 2.00335693359375\n",
            "strain 1.994049072265625\n",
            "strain 1.928314208984375\n",
            "strain 2.0618896484375\n",
            "strain 1.977630615234375\n",
            "strain 2.024139404296875\n",
            "strain 2.07635498046875\n",
            "strain 1.965728759765625\n",
            "strain 1.910308837890625\n",
            "strain 2.04736328125\n",
            "strain 1.968505859375\n",
            "strain 1.90576171875\n",
            "strain 2.038818359375\n",
            "strain 1.99542236328125\n",
            "strain 2.105499267578125\n",
            "strain 2.0606689453125\n",
            "strain 1.9951171875\n",
            "strain 2.093780517578125\n",
            "strain 2.04583740234375\n",
            "strain 2.04730224609375\n",
            "strain 1.9376220703125\n",
            "strain 2.098388671875\n",
            "strain 2.04443359375\n",
            "strain 2.09722900390625\n",
            "strain 1.97625732421875\n",
            "strain 2.04364013671875\n",
            "strain 2.004730224609375\n",
            "strain 2.01513671875\n",
            "strain 1.9569091796875\n",
            "strain 2.004486083984375\n",
            "strain 2.090301513671875\n",
            "strain 2.02923583984375\n",
            "strain 2.026458740234375\n",
            "strain 1.969879150390625\n",
            "strain 1.984039306640625\n",
            "strain 1.9627685546875\n",
            "strain 2.0091552734375\n",
            "strain 2.111236572265625\n",
            "strain 1.96551513671875\n",
            "strain 2.05621337890625\n",
            "strain 2.157257080078125\n",
            "strain 1.998199462890625\n",
            "strain 2.123077392578125\n",
            "strain 2.076080322265625\n",
            "strain 1.955596923828125\n",
            "strain 2.059844970703125\n",
            "strain 2.0086669921875\n",
            "strain 2.075592041015625\n",
            "strain 1.994781494140625\n",
            "strain 2.05145263671875\n",
            "strain 2.000762939453125\n",
            "strain 2.137603759765625\n",
            "strain 1.96380615234375\n",
            "strain 2.0113372802734375\n",
            "strain 2.095458984375\n",
            "strain 2.04449462890625\n",
            "strain 2.011383056640625\n",
            "strain 2.02703857421875\n",
            "strain 2.03790283203125\n",
            "strain 2.13690185546875\n",
            "strain 2.02972412109375\n",
            "strain 2.0748291015625\n",
            "strain 2.017547607421875\n",
            "strain 2.086181640625\n",
            "strain 2.08514404296875\n",
            "strain 2.130706787109375\n",
            "strain 2.010650634765625\n",
            "strain 2.10003662109375\n",
            "strain 2.04913330078125\n",
            "strain 2.005035400390625\n",
            "strain 1.9579010009765625\n",
            "strain 2.0804443359375\n",
            "strain 2.000762939453125\n",
            "strain 2.03564453125\n",
            "strain 2.07330322265625\n",
            "strain 2.1380615234375\n",
            "strain 2.06854248046875\n",
            "strain 2.03179931640625\n",
            "strain 1.980621337890625\n",
            "strain 2.09515380859375\n",
            "strain 2.0823974609375\n",
            "strain 1.995880126953125\n",
            "strain 2.0019073486328125\n",
            "strain 1.9552001953125\n",
            "strain 1.91748046875\n",
            "strain 2.068115234375\n",
            "strain 1.997344970703125\n",
            "strain 2.02752685546875\n",
            "strain 2.0849609375\n",
            "strain 2.082916259765625\n",
            "strain 2.021209716796875\n",
            "strain 2.082275390625\n",
            "strain 2.075958251953125\n",
            "strain 2.122467041015625\n",
            "strain 1.95697021484375\n",
            "strain 1.912811279296875\n",
            "strain 2.017669677734375\n",
            "strain 1.98760986328125\n",
            "strain 1.990570068359375\n",
            "strain 1.95794677734375\n",
            "strain 2.035736083984375\n",
            "strain 2.001983642578125\n",
            "strain 2.01409912109375\n",
            "strain 1.9368896484375\n",
            "strain 2.03314208984375\n",
            "strain 2.0347442626953125\n",
            "strain 1.99493408203125\n",
            "strain 2.15228271484375\n",
            "strain 1.992156982421875\n",
            "strain 2.012725830078125\n",
            "strain 2.14300537109375\n",
            "strain 2.1102294921875\n",
            "strain 1.9827880859375\n",
            "strain 2.083038330078125\n",
            "strain 2.01318359375\n",
            "strain 2.081939697265625\n",
            "strain 2.02825927734375\n",
            "strain 1.9578857421875\n",
            "strain 2.08941650390625\n",
            "strain 2.036041259765625\n",
            "strain 2.093353271484375\n",
            "strain 2.090911865234375\n",
            "strain 2.028839111328125\n",
            "strain 2.057281494140625\n",
            "strain 1.9922027587890625\n",
            "strain 2.02337646484375\n",
            "strain 2.049560546875\n",
            "strain 2.004730224609375\n",
            "strain 2.101226806640625\n",
            "strain 2.045318603515625\n",
            "strain 1.961181640625\n",
            "strain 1.9745635986328125\n",
            "strain 2.10321044921875\n",
            "strain 2.0809783935546875\n",
            "strain 1.975494384765625\n",
            "strain 1.98773193359375\n",
            "strain 1.966583251953125\n",
            "strain 1.986083984375\n",
            "strain 1.963623046875\n",
            "strain 2.0623779296875\n",
            "strain 1.88873291015625\n",
            "strain 2.0231170654296875\n",
            "strain 2.057769775390625\n",
            "strain 2.053466796875\n",
            "strain 2.09600830078125\n",
            "strain 2.023193359375\n",
            "strain 2.0174560546875\n",
            "strain 2.049560546875\n",
            "strain 2.0692138671875\n",
            "strain 2.028076171875\n",
            "strain 2.142669677734375\n",
            "strain 1.94622802734375\n",
            "strain 2.0791015625\n",
            "strain 2.099456787109375\n",
            "strain 2.050994873046875\n",
            "strain 2.00152587890625\n",
            "strain 2.1366424560546875\n",
            "strain 2.016937255859375\n",
            "strain 2.03985595703125\n",
            "strain 2.118438720703125\n",
            "strain 2.084381103515625\n",
            "strain 2.091033935546875\n",
            "strain 1.98858642578125\n",
            "strain 2.14788818359375\n",
            "strain 2.0323486328125\n",
            "strain 2.115753173828125\n",
            "strain 1.983001708984375\n",
            "strain 2.0416259765625\n",
            "strain 2.09954833984375\n",
            "strain 1.97747802734375\n",
            "strain 2.050445556640625\n",
            "strain 2.056488037109375\n",
            "strain 2.020172119140625\n",
            "strain 2.06964111328125\n",
            "strain 1.9383544921875\n",
            "strain 1.9296386241912842\n",
            "0.2421875\n",
            "0.1953125\n",
            "0.296875\n",
            "0.25\n",
            "0.3203125\n",
            "0.3203125\n",
            "0.296875\n",
            "0.3125\n",
            "0.2265625\n",
            "0.203125\n",
            "strain 2.103424072265625\n",
            "strain 2.06500244140625\n",
            "strain 1.954925537109375\n",
            "strain 2.13482666015625\n",
            "strain 1.9455108642578125\n",
            "strain 2.06597900390625\n",
            "strain 2.0396728515625\n",
            "strain 1.977630615234375\n",
            "strain 2.088043212890625\n",
            "strain 2.000640869140625\n",
            "strain 2.026031494140625\n",
            "strain 2.017669677734375\n",
            "strain 2.06768798828125\n",
            "strain 1.9700927734375\n",
            "strain 2.04278564453125\n",
            "strain 2.0506591796875\n",
            "strain 2.104827880859375\n",
            "strain 2.06103515625\n",
            "strain 1.99468994140625\n",
            "strain 1.9898681640625\n",
            "strain 1.93060302734375\n",
            "strain 1.972198486328125\n",
            "strain 2.0333251953125\n",
            "strain 2.0994873046875\n",
            "strain 2.027130126953125\n",
            "strain 2.06640625\n",
            "strain 2.055389404296875\n",
            "strain 2.085601806640625\n",
            "strain 2.07806396484375\n",
            "strain 2.080535888671875\n",
            "strain 2.071502685546875\n",
            "strain 1.98724365234375\n",
            "strain 2.05364990234375\n",
            "strain 1.977447509765625\n",
            "strain 2.060455322265625\n",
            "strain 1.999664306640625\n",
            "strain 2.0844268798828125\n",
            "strain 2.028900146484375\n",
            "strain 2.0302734375\n",
            "strain 2.142578125\n",
            "strain 1.999420166015625\n",
            "strain 1.92486572265625\n",
            "strain 2.080322265625\n",
            "strain 2.137237548828125\n",
            "strain 1.987548828125\n",
            "strain 2.052947998046875\n",
            "strain 2.0076904296875\n",
            "strain 2.00738525390625\n",
            "strain 2.067901611328125\n",
            "strain 2.101165771484375\n",
            "strain 2.053314208984375\n",
            "strain 2.127166748046875\n",
            "strain 2.0545654296875\n",
            "strain 2.064422607421875\n",
            "strain 1.965911865234375\n",
            "strain 1.997039794921875\n",
            "strain 1.95501708984375\n",
            "strain 1.9898681640625\n",
            "strain 2.037628173828125\n",
            "strain 2.0691680908203125\n",
            "strain 1.992584228515625\n",
            "strain 2.029296875\n",
            "strain 2.102325439453125\n",
            "strain 2.0360107421875\n",
            "strain 2.018951416015625\n",
            "strain 2.058563232421875\n",
            "strain 2.03314208984375\n",
            "strain 2.058319091796875\n",
            "strain 1.997528076171875\n",
            "strain 2.105377197265625\n",
            "strain 2.089508056640625\n",
            "strain 2.0198974609375\n",
            "strain 1.926666259765625\n",
            "strain 1.984710693359375\n",
            "strain 2.055908203125\n",
            "strain 1.9896240234375\n",
            "strain 2.069305419921875\n",
            "strain 2.061065673828125\n",
            "strain 1.90716552734375\n",
            "strain 2.03680419921875\n",
            "strain 2.041534423828125\n",
            "strain 2.034698486328125\n",
            "strain 2.064666748046875\n",
            "strain 2.03521728515625\n",
            "strain 2.075531005859375\n",
            "strain 2.099884033203125\n",
            "strain 1.99908447265625\n",
            "strain 2.037750244140625\n",
            "strain 1.948455810546875\n",
            "strain 1.9657440185546875\n",
            "strain 2.049835205078125\n",
            "strain 1.9669189453125\n",
            "strain 2.032318115234375\n",
            "strain 1.9698638916015625\n",
            "strain 2.090087890625\n",
            "strain 2.090789794921875\n",
            "strain 2.0234222412109375\n",
            "strain 1.923004150390625\n",
            "strain 2.0906982421875\n",
            "strain 1.984405517578125\n",
            "strain 2.085174560546875\n",
            "strain 2.049713134765625\n",
            "strain 2.052886962890625\n",
            "strain 2.11358642578125\n",
            "strain 2.0506591796875\n",
            "strain 1.96343994140625\n",
            "strain 2.065887451171875\n",
            "strain 1.99298095703125\n",
            "strain 2.000335693359375\n",
            "strain 2.023406982421875\n",
            "strain 2.081268310546875\n",
            "strain 2.0533447265625\n",
            "strain 2.06378173828125\n",
            "strain 2.0394287109375\n",
            "strain 2.0616455078125\n",
            "strain 1.985015869140625\n",
            "strain 1.966033935546875\n",
            "strain 2.0892333984375\n",
            "strain 2.0157623291015625\n",
            "strain 2.05572509765625\n",
            "strain 1.979095458984375\n",
            "strain 2.0953369140625\n",
            "strain 2.056182861328125\n",
            "strain 2.092926025390625\n",
            "strain 1.993682861328125\n",
            "strain 2.04815673828125\n",
            "strain 2.111328125\n",
            "strain 1.9925537109375\n",
            "strain 2.0269775390625\n",
            "strain 1.987335205078125\n",
            "strain 2.1044921875\n",
            "strain 1.896942138671875\n",
            "strain 2.059326171875\n",
            "strain 2.118804931640625\n",
            "strain 2.033538818359375\n",
            "strain 1.989410400390625\n",
            "strain 2.01544189453125\n",
            "strain 2.002532958984375\n",
            "strain 2.0499267578125\n",
            "strain 2.002105712890625\n",
            "strain 2.072357177734375\n",
            "strain 2.08441162109375\n",
            "strain 2.015869140625\n",
            "strain 2.015045166015625\n",
            "strain 2.134033203125\n",
            "strain 2.03387451171875\n",
            "strain 1.96868896484375\n",
            "strain 2.07537841796875\n",
            "strain 1.93048095703125\n",
            "strain 2.06439208984375\n",
            "strain 2.12445068359375\n",
            "strain 1.964080810546875\n",
            "strain 2.003265380859375\n",
            "strain 2.0046844482421875\n",
            "strain 2.086334228515625\n",
            "strain 2.080902099609375\n",
            "strain 2.01123046875\n",
            "strain 2.004486083984375\n",
            "strain 2.04833984375\n",
            "strain 2.02630615234375\n",
            "strain 2.040435791015625\n",
            "strain 2.095916748046875\n",
            "strain 2.062652587890625\n",
            "strain 2.113861083984375\n",
            "strain 1.9691314697265625\n",
            "strain 2.02020263671875\n",
            "strain 1.9339599609375\n",
            "strain 1.99444580078125\n",
            "strain 2.10455322265625\n",
            "strain 2.023468017578125\n",
            "strain 2.020751953125\n",
            "strain 1.971466064453125\n",
            "strain 1.975494384765625\n",
            "strain 2.0672607421875\n",
            "strain 2.044158935546875\n",
            "strain 2.182952880859375\n",
            "strain 2.102447509765625\n",
            "strain 2.008209228515625\n",
            "strain 2.003204345703125\n",
            "strain 1.983551025390625\n",
            "strain 1.978057861328125\n",
            "strain 2.102874755859375\n",
            "strain 2.104522705078125\n",
            "strain 2.069671630859375\n",
            "strain 2.05401611328125\n",
            "strain 1.981475830078125\n",
            "strain 2.181182861328125\n",
            "strain 2.06640625\n",
            "strain 1.909759521484375\n",
            "strain 2.045501708984375\n",
            "strain 2.116241455078125\n",
            "strain 2.088653564453125\n",
            "strain 2.04437255859375\n",
            "strain 2.024627685546875\n",
            "strain 2.114593505859375\n",
            "strain 2.15716552734375\n",
            "strain 2.02685546875\n",
            "strain 2.0859375\n",
            "strain 1.9439697265625\n",
            "strain 2.04864501953125\n",
            "strain 2.082305908203125\n",
            "strain 2.080657958984375\n",
            "strain 2.018798828125\n",
            "strain 1.89556884765625\n",
            "strain 2.032562255859375\n",
            "strain 2.0677490234375\n",
            "strain 2.060516357421875\n",
            "strain 2.009765625\n",
            "strain 1.95361328125\n",
            "strain 1.9761810302734375\n",
            "strain 2.0437774658203125\n",
            "strain 2.006103515625\n",
            "strain 2.155059814453125\n",
            "strain 2.076171875\n",
            "strain 1.986724853515625\n",
            "strain 2.02301025390625\n",
            "strain 2.058441162109375\n",
            "strain 1.987060546875\n",
            "strain 2.053009033203125\n",
            "strain 2.039398193359375\n",
            "strain 2.04180908203125\n",
            "strain 2.051422119140625\n",
            "strain 1.9832763671875\n",
            "strain 2.085174560546875\n",
            "strain 2.104400634765625\n",
            "strain 2.0013427734375\n",
            "strain 1.9879150390625\n",
            "strain 2.078369140625\n",
            "strain 2.017669677734375\n",
            "strain 2.015106201171875\n",
            "strain 1.974212646484375\n",
            "strain 1.993072509765625\n",
            "strain 2.00628662109375\n",
            "strain 2.101806640625\n",
            "strain 1.971343994140625\n",
            "strain 2.001007080078125\n",
            "strain 2.03094482421875\n",
            "strain 1.965087890625\n",
            "strain 1.970458984375\n",
            "strain 2.04052734375\n",
            "strain 1.97760009765625\n",
            "strain 2.008331298828125\n",
            "strain 2.08721923828125\n",
            "strain 2.078948974609375\n",
            "strain 2.02508544921875\n",
            "strain 1.929595947265625\n",
            "strain 2.014190673828125\n",
            "strain 1.97723388671875\n",
            "strain 2.076904296875\n",
            "strain 2.082763671875\n",
            "strain 2.0372314453125\n",
            "strain 2.11846923828125\n",
            "strain 2.042083740234375\n",
            "strain 2.02093505859375\n",
            "strain 1.96588134765625\n",
            "strain 1.997467041015625\n",
            "strain 2.01959228515625\n",
            "strain 2.040802001953125\n",
            "strain 1.9984130859375\n",
            "strain 1.95697021484375\n",
            "strain 1.974273681640625\n",
            "strain 2.0172119140625\n",
            "strain 2.020782470703125\n",
            "strain 2.058990478515625\n",
            "strain 2.032073974609375\n",
            "strain 2.12640380859375\n",
            "strain 2.09088134765625\n",
            "strain 1.9386444091796875\n",
            "strain 2.057586669921875\n",
            "strain 2.01593017578125\n",
            "strain 2.0552978515625\n",
            "strain 1.966278076171875\n",
            "strain 2.012115478515625\n",
            "strain 2.072052001953125\n",
            "strain 2.051544189453125\n",
            "strain 2.030426025390625\n",
            "strain 2.073883056640625\n",
            "strain 1.953033447265625\n",
            "strain 2.033447265625\n",
            "strain 1.959625244140625\n",
            "strain 2.087860107421875\n",
            "strain 2.0404052734375\n",
            "strain 1.996002197265625\n",
            "strain 1.92138671875\n",
            "strain 2.12359619140625\n",
            "strain 1.996490478515625\n",
            "strain 2.0987548828125\n",
            "strain 2.04620361328125\n",
            "strain 2.0804443359375\n",
            "strain 1.96270751953125\n",
            "strain 2.1055908203125\n",
            "strain 2.022186279296875\n",
            "strain 2.0302886962890625\n",
            "strain 1.965301513671875\n",
            "strain 2.0064544677734375\n",
            "strain 2.0936279296875\n",
            "strain 2.076324462890625\n",
            "strain 2.008209228515625\n",
            "strain 2.002288818359375\n",
            "strain 1.966033935546875\n",
            "strain 2.1434326171875\n",
            "strain 1.972137451171875\n",
            "strain 1.995849609375\n",
            "strain 2.025787353515625\n",
            "strain 2.002410888671875\n",
            "strain 1.970733642578125\n",
            "strain 2.040924072265625\n",
            "strain 2.001007080078125\n",
            "strain 1.976409912109375\n",
            "strain 2.025177001953125\n",
            "strain 2.064056396484375\n",
            "strain 1.994293212890625\n",
            "strain 1.9826202392578125\n",
            "strain 2.108154296875\n",
            "strain 2.067596435546875\n",
            "strain 2.023284912109375\n",
            "strain 2.182769775390625\n",
            "strain 1.96636962890625\n",
            "strain 1.96917724609375\n",
            "strain 2.015838623046875\n",
            "strain 2.066619873046875\n",
            "strain 2.026580810546875\n",
            "strain 2.1163330078125\n",
            "strain 2.108673095703125\n",
            "strain 2.066162109375\n",
            "strain 2.050689697265625\n",
            "strain 1.96795654296875\n",
            "strain 2.066375732421875\n",
            "strain 2.061248779296875\n",
            "strain 1.91827392578125\n",
            "strain 1.924041748046875\n",
            "strain 1.9805908203125\n",
            "strain 2.033721923828125\n",
            "strain 1.951202392578125\n",
            "strain 2.050567626953125\n",
            "strain 2.089263916015625\n",
            "strain 2.03875732421875\n",
            "strain 2.12353515625\n",
            "strain 1.9493865966796875\n",
            "strain 2.173095703125\n",
            "strain 1.902496337890625\n",
            "strain 2.022735595703125\n",
            "strain 2.046173095703125\n",
            "strain 1.943267822265625\n",
            "strain 2.031158447265625\n",
            "strain 2.149993896484375\n",
            "strain 2.0653839111328125\n",
            "strain 1.98193359375\n",
            "strain 1.98052978515625\n",
            "strain 2.05712890625\n",
            "strain 2.046295166015625\n",
            "strain 2.159637451171875\n",
            "strain 2.0150146484375\n",
            "strain 2.084930419921875\n",
            "strain 2.0633544921875\n",
            "strain 2.0645751953125\n",
            "strain 1.99798583984375\n",
            "strain 1.9660186767578125\n",
            "strain 1.987640380859375\n",
            "strain 2.181976318359375\n",
            "strain 1.896514892578125\n",
            "strain 2.054443359375\n",
            "strain 2.047943115234375\n",
            "strain 2.00946044921875\n",
            "strain 1.995513916015625\n",
            "strain 1.91741943359375\n",
            "strain 2.123565673828125\n",
            "strain 2.13348388671875\n",
            "strain 2.016632080078125\n",
            "strain 1.98736572265625\n",
            "strain 1.9948577880859375\n",
            "strain 2.0543060302734375\n",
            "strain 2.048583984375\n",
            "strain 2.112548828125\n",
            "strain 2.031005859375\n",
            "strain 1.95831298828125\n",
            "strain 2.10400390625\n",
            "strain 1.989532470703125\n",
            "strain 2.09259033203125\n",
            "strain 2.02545166015625\n",
            "strain 2.025543212890625\n",
            "strain 2.070709228515625\n",
            "strain 2.082794189453125\n",
            "strain 2.008514404296875\n",
            "strain 2.04754638671875\n",
            "strain 2.069244384765625\n",
            "strain 2.07720947265625\n",
            "strain 1.975433349609375\n",
            "strain 2.01495361328125\n",
            "strain 1.977142333984375\n",
            "strain 2.0115723609924316\n",
            "0.28125\n",
            "0.328125\n",
            "0.2265625\n",
            "0.2265625\n",
            "0.2109375\n",
            "0.2421875\n",
            "0.2890625\n",
            "0.2890625\n",
            "0.34375\n",
            "0.2109375\n",
            "strain 2.010986328125\n",
            "strain 2.046112060546875\n",
            "strain 1.948272705078125\n",
            "strain 2.08892822265625\n",
            "strain 1.9471435546875\n",
            "strain 2.048828125\n",
            "strain 2.079925537109375\n",
            "strain 2.042388916015625\n",
            "strain 1.998291015625\n",
            "strain 2.04443359375\n",
            "strain 2.043701171875\n",
            "strain 2.03228759765625\n",
            "strain 2.0398406982421875\n",
            "strain 2.001495361328125\n",
            "strain 1.95166015625\n",
            "strain 2.043731689453125\n",
            "strain 2.010955810546875\n",
            "strain 2.0018310546875\n",
            "strain 2.03558349609375\n",
            "strain 1.995635986328125\n",
            "strain 2.0848388671875\n",
            "strain 2.04351806640625\n",
            "strain 1.948974609375\n",
            "strain 1.988128662109375\n",
            "strain 2.045989990234375\n",
            "strain 2.078033447265625\n",
            "strain 2.0113525390625\n",
            "strain 1.96337890625\n",
            "strain 2.080780029296875\n",
            "strain 2.0626220703125\n",
            "strain 2.0477752685546875\n",
            "strain 2.073486328125\n",
            "strain 2.066375732421875\n",
            "strain 1.991058349609375\n",
            "strain 1.9951171875\n",
            "strain 2.06341552734375\n",
            "strain 1.999755859375\n",
            "strain 2.0350341796875\n",
            "strain 2.08135986328125\n",
            "strain 2.04571533203125\n",
            "strain 2.097442626953125\n",
            "strain 2.051361083984375\n",
            "strain 2.063995361328125\n",
            "strain 2.0740966796875\n",
            "strain 2.05645751953125\n",
            "strain 1.961944580078125\n",
            "strain 2.099853515625\n",
            "strain 2.0354766845703125\n",
            "strain 2.005279541015625\n",
            "strain 2.007843017578125\n",
            "strain 2.102264404296875\n",
            "strain 1.964111328125\n",
            "strain 2.087615966796875\n",
            "strain 2.0413360595703125\n",
            "strain 2.029632568359375\n",
            "strain 1.891845703125\n",
            "strain 1.98211669921875\n",
            "strain 2.082489013671875\n",
            "strain 2.066162109375\n",
            "strain 1.98345947265625\n",
            "strain 2.142822265625\n",
            "strain 2.0726318359375\n",
            "strain 2.01434326171875\n",
            "strain 2.061981201171875\n",
            "strain 2.00494384765625\n",
            "strain 1.964324951171875\n",
            "strain 2.0501708984375\n",
            "strain 2.0982666015625\n",
            "strain 2.074981689453125\n",
            "strain 1.975677490234375\n",
            "strain 2.025604248046875\n",
            "strain 1.980499267578125\n",
            "strain 1.9791259765625\n",
            "strain 2.054107666015625\n",
            "strain 2.098297119140625\n",
            "strain 2.0634765625\n",
            "strain 2.031280517578125\n",
            "strain 2.09490966796875\n",
            "strain 2.021087646484375\n",
            "strain 2.06475830078125\n",
            "strain 2.001556396484375\n",
            "strain 2.03448486328125\n",
            "strain 2.018585205078125\n",
            "strain 2.024322509765625\n",
            "strain 2.02044677734375\n",
            "strain 2.15869140625\n",
            "strain 2.0463714599609375\n",
            "strain 1.993011474609375\n",
            "strain 1.964447021484375\n",
            "strain 2.00469970703125\n",
            "strain 2.021087646484375\n",
            "strain 2.08050537109375\n",
            "strain 2.031585693359375\n",
            "strain 2.023651123046875\n",
            "strain 1.8994140625\n",
            "strain 1.954559326171875\n",
            "strain 1.992095947265625\n",
            "strain 2.055999755859375\n",
            "strain 2.06591796875\n",
            "strain 2.00164794921875\n",
            "strain 1.94000244140625\n",
            "strain 2.03955078125\n",
            "strain 2.10174560546875\n",
            "strain 1.97845458984375\n",
            "strain 2.03955078125\n",
            "strain 1.97711181640625\n",
            "strain 2.127044677734375\n",
            "strain 1.929290771484375\n",
            "strain 1.9353790283203125\n",
            "strain 2.04180908203125\n",
            "strain 2.105224609375\n",
            "strain 2.10150146484375\n",
            "strain 2.0599517822265625\n",
            "strain 2.02099609375\n",
            "strain 1.99951171875\n",
            "strain 2.071746826171875\n",
            "strain 2.02301025390625\n",
            "strain 2.010467529296875\n",
            "strain 2.00384521484375\n",
            "strain 1.94317626953125\n",
            "strain 2.019134521484375\n",
            "strain 2.039520263671875\n",
            "strain 2.134033203125\n",
            "strain 2.0823974609375\n",
            "strain 2.028228759765625\n",
            "strain 2.113861083984375\n",
            "strain 1.983306884765625\n",
            "strain 2.0818023681640625\n",
            "strain 2.0156097412109375\n",
            "strain 1.943389892578125\n",
            "strain 2.081939697265625\n",
            "strain 2.022186279296875\n",
            "strain 2.00885009765625\n",
            "strain 1.94036865234375\n",
            "strain 2.082550048828125\n",
            "strain 2.098114013671875\n",
            "strain 2.083404541015625\n",
            "strain 2.121917724609375\n",
            "strain 2.012054443359375\n",
            "strain 2.10357666015625\n",
            "strain 2.135528564453125\n",
            "strain 2.0081787109375\n",
            "strain 2.065673828125\n",
            "strain 2.070648193359375\n",
            "strain 2.04998779296875\n",
            "strain 2.070465087890625\n",
            "strain 2.15728759765625\n",
            "strain 2.14666748046875\n",
            "strain 1.997772216796875\n",
            "strain 1.923828125\n",
            "strain 2.0634765625\n",
            "strain 2.087554931640625\n",
            "strain 2.02197265625\n",
            "strain 2.136566162109375\n",
            "strain 2.101165771484375\n",
            "strain 1.973876953125\n",
            "strain 2.04010009765625\n",
            "strain 2.09466552734375\n",
            "strain 1.93896484375\n",
            "strain 1.96148681640625\n",
            "strain 2.01141357421875\n",
            "strain 2.06231689453125\n",
            "strain 2.06719970703125\n",
            "strain 2.07379150390625\n",
            "strain 1.9554901123046875\n",
            "strain 2.062469482421875\n",
            "strain 1.99188232421875\n",
            "strain 1.970458984375\n",
            "strain 2.00103759765625\n",
            "strain 2.06317138671875\n",
            "strain 2.05291748046875\n",
            "strain 1.9444580078125\n",
            "strain 2.030975341796875\n",
            "strain 2.022186279296875\n",
            "strain 2.044097900390625\n",
            "strain 2.0162353515625\n",
            "strain 1.9901123046875\n",
            "strain 2.156890869140625\n",
            "strain 1.99072265625\n",
            "strain 2.031951904296875\n",
            "strain 2.0606689453125\n",
            "strain 2.06298828125\n",
            "strain 2.03497314453125\n",
            "strain 2.023101806640625\n",
            "strain 2.020263671875\n",
            "strain 2.052398681640625\n",
            "strain 2.020050048828125\n",
            "strain 1.957611083984375\n",
            "strain 2.05767822265625\n",
            "strain 2.09014892578125\n",
            "strain 2.0478515625\n",
            "strain 1.93402099609375\n",
            "strain 2.12054443359375\n",
            "strain 1.98272705078125\n",
            "strain 2.032470703125\n",
            "strain 2.040283203125\n",
            "strain 2.023101806640625\n",
            "strain 2.018341064453125\n",
            "strain 2.011474609375\n",
            "strain 2.0886993408203125\n",
            "strain 2.032806396484375\n",
            "strain 1.99371337890625\n",
            "strain 2.082672119140625\n",
            "strain 2.117401123046875\n",
            "strain 1.9323883056640625\n",
            "strain 1.986328125\n",
            "strain 2.0155029296875\n",
            "strain 2.07373046875\n",
            "strain 2.11590576171875\n",
            "strain 2.0695037841796875\n",
            "strain 2.045867919921875\n",
            "strain 1.989166259765625\n",
            "strain 2.03887939453125\n",
            "strain 2.098114013671875\n",
            "strain 2.110626220703125\n",
            "strain 2.015869140625\n",
            "strain 2.0101318359375\n",
            "strain 1.9590911865234375\n",
            "strain 2.061126708984375\n",
            "strain 2.0196533203125\n",
            "strain 1.986328125\n",
            "strain 2.030059814453125\n",
            "strain 2.04119873046875\n",
            "strain 2.00787353515625\n",
            "strain 2.04443359375\n",
            "strain 1.99273681640625\n",
            "strain 2.2376708984375\n",
            "strain 1.977996826171875\n",
            "strain 2.032257080078125\n",
            "strain 1.976837158203125\n",
            "strain 1.996002197265625\n",
            "strain 2.111541748046875\n",
            "strain 2.0380859375\n",
            "strain 2.016937255859375\n",
            "strain 2.016876220703125\n",
            "strain 1.953155517578125\n",
            "strain 2.0029296875\n",
            "strain 2.123138427734375\n",
            "strain 2.046722412109375\n",
            "strain 1.91552734375\n",
            "strain 2.03692626953125\n",
            "strain 2.12933349609375\n",
            "strain 1.972625732421875\n",
            "strain 1.992340087890625\n",
            "strain 2.0306396484375\n",
            "strain 2.03741455078125\n",
            "strain 1.945648193359375\n",
            "strain 2.058990478515625\n",
            "strain 1.9591064453125\n",
            "strain 2.09820556640625\n",
            "strain 1.996551513671875\n",
            "strain 1.9535980224609375\n",
            "strain 2.121490478515625\n",
            "strain 2.100250244140625\n",
            "strain 1.97320556640625\n",
            "strain 2.06390380859375\n",
            "strain 2.090789794921875\n",
            "strain 2.0176849365234375\n",
            "strain 1.9945068359375\n",
            "strain 2.003997802734375\n",
            "strain 2.040985107421875\n",
            "strain 2.018768310546875\n",
            "strain 1.9954833984375\n",
            "strain 2.038482666015625\n",
            "strain 2.0892333984375\n",
            "strain 2.0400390625\n",
            "strain 2.00146484375\n",
            "strain 1.95867919921875\n",
            "strain 1.974090576171875\n",
            "strain 1.973663330078125\n",
            "strain 2.059906005859375\n",
            "strain 2.007720947265625\n",
            "strain 2.115142822265625\n",
            "strain 2.212493896484375\n",
            "strain 2.0367431640625\n",
            "strain 1.999725341796875\n",
            "strain 2.056365966796875\n",
            "strain 2.1214599609375\n",
            "strain 1.94244384765625\n",
            "strain 2.002777099609375\n",
            "strain 2.05670166015625\n",
            "strain 1.9620361328125\n",
            "strain 2.05133056640625\n",
            "strain 2.016357421875\n",
            "strain 2.084625244140625\n",
            "strain 2.133392333984375\n",
            "strain 2.02545166015625\n",
            "strain 1.9164581298828125\n",
            "strain 2.031463623046875\n",
            "strain 2.075347900390625\n",
            "strain 2.002960205078125\n",
            "strain 2.06884765625\n",
            "strain 2.06622314453125\n",
            "strain 2.053466796875\n",
            "strain 1.971038818359375\n",
            "strain 2.110992431640625\n",
            "strain 2.064178466796875\n",
            "strain 1.98382568359375\n",
            "strain 2.08892822265625\n",
            "strain 1.95147705078125\n",
            "strain 2.01324462890625\n",
            "strain 1.940887451171875\n",
            "strain 2.0706787109375\n",
            "strain 2.021026611328125\n",
            "strain 2.013031005859375\n",
            "strain 2.02655029296875\n",
            "strain 1.991607666015625\n",
            "strain 2.010955810546875\n",
            "strain 2.0316162109375\n",
            "strain 2.072967529296875\n",
            "strain 1.976654052734375\n",
            "strain 2.115692138671875\n",
            "strain 1.940460205078125\n",
            "strain 2.09051513671875\n",
            "strain 2.03802490234375\n",
            "strain 2.065765380859375\n",
            "strain 2.1151123046875\n",
            "strain 2.0543212890625\n",
            "strain 2.015899658203125\n",
            "strain 2.016204833984375\n",
            "strain 1.930206298828125\n",
            "strain 2.06512451171875\n",
            "strain 2.072235107421875\n",
            "strain 2.0984039306640625\n",
            "strain 2.0394287109375\n",
            "strain 2.0045166015625\n",
            "strain 2.12945556640625\n",
            "strain 1.998931884765625\n",
            "strain 2.04766845703125\n",
            "strain 2.01776123046875\n",
            "strain 2.0471954345703125\n",
            "strain 2.003814697265625\n",
            "strain 2.060028076171875\n",
            "strain 2.0651702880859375\n",
            "strain 2.0039520263671875\n",
            "strain 2.025787353515625\n",
            "strain 2.037628173828125\n",
            "strain 2.010986328125\n",
            "strain 2.076263427734375\n",
            "strain 1.979461669921875\n",
            "strain 1.9806365966796875\n",
            "strain 2.14739990234375\n",
            "strain 1.99609375\n",
            "strain 2.056854248046875\n",
            "strain 1.9603271484375\n",
            "strain 2.064849853515625\n",
            "strain 2.023895263671875\n",
            "strain 2.13677978515625\n",
            "strain 2.019775390625\n",
            "strain 1.95013427734375\n",
            "strain 2.0399169921875\n",
            "strain 1.956268310546875\n",
            "strain 1.99267578125\n",
            "strain 2.06072998046875\n",
            "strain 2.009857177734375\n",
            "strain 2.048095703125\n",
            "strain 2.009124755859375\n",
            "strain 1.93017578125\n",
            "strain 2.05999755859375\n",
            "strain 2.029510498046875\n",
            "strain 2.02117919921875\n",
            "strain 2.026214599609375\n",
            "strain 1.96905517578125\n",
            "strain 2.080413818359375\n",
            "strain 2.067291259765625\n",
            "strain 2.097015380859375\n",
            "strain 2.0308837890625\n",
            "strain 2.046966552734375\n",
            "strain 1.98663330078125\n",
            "strain 1.964813232421875\n",
            "strain 2.0601806640625\n",
            "strain 2.012847900390625\n",
            "strain 2.008544921875\n",
            "strain 2.089691162109375\n",
            "strain 2.095977783203125\n",
            "strain 2.05352783203125\n",
            "strain 2.0503997802734375\n",
            "strain 1.9989013671875\n",
            "strain 2.010955810546875\n",
            "strain 1.9305419921875\n",
            "strain 1.99554443359375\n",
            "strain 2.010101318359375\n",
            "strain 1.970550537109375\n",
            "strain 2.060333251953125\n",
            "strain 1.9492034912109375\n",
            "strain 2.12860107421875\n",
            "strain 2.09039306640625\n",
            "strain 1.963836669921875\n",
            "strain 2.06854248046875\n",
            "strain 1.966796875\n",
            "strain 1.998876929283142\n",
            "0.25\n",
            "0.234375\n",
            "0.2265625\n",
            "0.296875\n",
            "0.3125\n",
            "0.28125\n",
            "0.2578125\n",
            "0.3671875\n",
            "0.234375\n",
            "0.265625\n",
            "strain 1.939056396484375\n",
            "strain 2.07159423828125\n",
            "strain 2.107177734375\n",
            "strain 1.9625244140625\n",
            "strain 2.046630859375\n",
            "strain 2.100555419921875\n",
            "strain 2.12115478515625\n",
            "strain 2.04217529296875\n",
            "strain 2.077728271484375\n",
            "strain 2.0625\n",
            "strain 2.07275390625\n",
            "strain 2.04766845703125\n",
            "strain 2.076019287109375\n",
            "strain 2.091522216796875\n",
            "strain 2.04315185546875\n",
            "strain 2.071319580078125\n",
            "strain 2.066253662109375\n",
            "strain 2.0152587890625\n",
            "strain 1.990264892578125\n",
            "strain 2.021881103515625\n",
            "strain 2.07000732421875\n",
            "strain 2.02691650390625\n",
            "strain 2.0415496826171875\n",
            "strain 2.00592041015625\n",
            "strain 1.998779296875\n",
            "strain 2.040191650390625\n",
            "strain 2.07440185546875\n",
            "strain 2.017578125\n",
            "strain 1.9798583984375\n",
            "strain 2.022552490234375\n",
            "strain 2.000579833984375\n",
            "strain 1.970794677734375\n",
            "strain 1.986114501953125\n",
            "strain 2.0106201171875\n",
            "strain 1.986114501953125\n",
            "strain 1.982666015625\n",
            "strain 2.056854248046875\n",
            "strain 1.98944091796875\n",
            "strain 2.0432891845703125\n",
            "strain 2.01458740234375\n",
            "strain 2.003875732421875\n",
            "strain 1.97308349609375\n",
            "strain 2.002471923828125\n",
            "strain 1.994659423828125\n",
            "strain 2.002166748046875\n",
            "strain 2.03387451171875\n",
            "strain 1.987823486328125\n",
            "strain 2.120330810546875\n",
            "strain 2.0672607421875\n",
            "strain 2.0504150390625\n",
            "strain 2.002227783203125\n",
            "strain 2.096282958984375\n",
            "strain 1.9320068359375\n",
            "strain 2.018280029296875\n",
            "strain 2.043975830078125\n",
            "strain 2.023040771484375\n",
            "strain 1.949676513671875\n",
            "strain 2.149139404296875\n",
            "strain 1.952880859375\n",
            "strain 2.081207275390625\n",
            "strain 1.9959716796875\n",
            "strain 2.120941162109375\n",
            "strain 2.079254150390625\n",
            "strain 2.00927734375\n",
            "strain 2.060699462890625\n",
            "strain 2.031402587890625\n",
            "strain 2.000244140625\n",
            "strain 2.09246826171875\n",
            "strain 2.032440185546875\n",
            "strain 1.97027587890625\n",
            "strain 2.07958984375\n",
            "strain 1.975250244140625\n",
            "strain 2.0084228515625\n",
            "strain 2.0595703125\n",
            "strain 2.1376953125\n",
            "strain 1.979400634765625\n",
            "strain 1.987396240234375\n",
            "strain 1.983734130859375\n",
            "strain 2.006744384765625\n",
            "strain 2.125274658203125\n",
            "strain 1.938140869140625\n",
            "strain 2.0120697021484375\n",
            "strain 1.9937744140625\n",
            "strain 2.008514404296875\n",
            "strain 2.00286865234375\n",
            "strain 2.071868896484375\n",
            "strain 2.005218505859375\n",
            "strain 2.011474609375\n",
            "strain 2.087158203125\n",
            "strain 2.015167236328125\n",
            "strain 2.077545166015625\n",
            "strain 2.064910888671875\n",
            "strain 2.012237548828125\n",
            "strain 1.9765167236328125\n",
            "strain 2.10113525390625\n",
            "strain 2.05853271484375\n",
            "strain 2.001434326171875\n",
            "strain 2.020843505859375\n",
            "strain 2.0430908203125\n",
            "strain 2.047821044921875\n",
            "strain 1.941558837890625\n",
            "strain 2.059783935546875\n",
            "strain 1.918060302734375\n",
            "strain 2.046905517578125\n",
            "strain 2.027374267578125\n",
            "strain 1.9923095703125\n",
            "strain 2.0057373046875\n",
            "strain 2.118560791015625\n",
            "strain 2.0230712890625\n",
            "strain 2.123779296875\n",
            "strain 2.010101318359375\n",
            "strain 1.983428955078125\n",
            "strain 1.97161865234375\n",
            "strain 2.06195068359375\n",
            "strain 2.01788330078125\n",
            "strain 2.14251708984375\n",
            "strain 1.998809814453125\n",
            "strain 2.0521240234375\n",
            "strain 2.1224365234375\n",
            "strain 1.9865875244140625\n",
            "strain 1.9594879150390625\n",
            "strain 1.953582763671875\n",
            "strain 2.023712158203125\n",
            "strain 2.094329833984375\n",
            "strain 2.0740966796875\n",
            "strain 1.972747802734375\n",
            "strain 2.0738525390625\n",
            "strain 2.054473876953125\n",
            "strain 2.023651123046875\n",
            "strain 2.134918212890625\n",
            "strain 1.994964599609375\n",
            "strain 1.92669677734375\n",
            "strain 2.055328369140625\n",
            "strain 1.994720458984375\n",
            "strain 2.005218505859375\n",
            "strain 2.0369720458984375\n",
            "strain 2.10076904296875\n",
            "strain 2.0423583984375\n",
            "strain 2.022430419921875\n",
            "strain 2.0040283203125\n",
            "strain 2.14459228515625\n",
            "strain 2.026123046875\n",
            "strain 2.012298583984375\n",
            "strain 1.964630126953125\n",
            "strain 2.0985107421875\n",
            "strain 2.014190673828125\n",
            "strain 2.207916259765625\n",
            "strain 2.015167236328125\n",
            "strain 1.92462158203125\n",
            "strain 2.016937255859375\n",
            "strain 1.98065185546875\n",
            "strain 2.130126953125\n",
            "strain 2.089691162109375\n",
            "strain 2.073272705078125\n",
            "strain 2.02374267578125\n",
            "strain 2.00653076171875\n",
            "strain 1.99993896484375\n",
            "strain 2.0415496826171875\n",
            "strain 1.995880126953125\n",
            "strain 2.0322113037109375\n",
            "strain 1.99151611328125\n",
            "strain 1.9224090576171875\n",
            "strain 2.0175323486328125\n",
            "strain 2.07110595703125\n",
            "strain 2.074615478515625\n",
            "strain 2.007415771484375\n",
            "strain 1.949920654296875\n",
            "strain 1.948577880859375\n",
            "strain 1.949188232421875\n",
            "strain 2.026519775390625\n",
            "strain 1.941741943359375\n",
            "strain 2.010650634765625\n",
            "strain 2.0155029296875\n",
            "strain 2.01397705078125\n",
            "strain 2.04583740234375\n",
            "strain 2.01824951171875\n",
            "strain 2.030517578125\n",
            "strain 2.00262451171875\n",
            "strain 1.996368408203125\n",
            "strain 2.1019287109375\n",
            "strain 2.0006103515625\n",
            "strain 2.097320556640625\n",
            "strain 2.110931396484375\n",
            "strain 2.0413818359375\n",
            "strain 2.104644775390625\n",
            "strain 1.996337890625\n",
            "strain 2.02593994140625\n",
            "strain 2.0499267578125\n",
            "strain 2.018585205078125\n",
            "strain 2.04638671875\n",
            "strain 2.028411865234375\n",
            "strain 2.073028564453125\n",
            "strain 2.050537109375\n",
            "strain 2.013916015625\n",
            "strain 2.023284912109375\n",
            "strain 1.97540283203125\n",
            "strain 2.0179443359375\n",
            "strain 2.043365478515625\n",
            "strain 2.10015869140625\n",
            "strain 1.9972381591796875\n",
            "strain 2.057373046875\n",
            "strain 1.987701416015625\n",
            "strain 1.922760009765625\n",
            "strain 2.171417236328125\n",
            "strain 1.993194580078125\n",
            "strain 2.047332763671875\n",
            "strain 2.05224609375\n",
            "strain 2.05426025390625\n",
            "strain 1.9981689453125\n",
            "strain 2.0147705078125\n",
            "strain 2.077911376953125\n",
            "strain 2.08233642578125\n",
            "strain 1.96661376953125\n",
            "strain 2.019439697265625\n",
            "strain 2.061370849609375\n",
            "strain 2.027069091796875\n",
            "strain 1.926483154296875\n",
            "strain 2.0904541015625\n",
            "strain 1.973724365234375\n",
            "strain 1.95465087890625\n",
            "strain 2.059722900390625\n",
            "strain 2.1131591796875\n",
            "strain 2.058990478515625\n",
            "strain 2.170562744140625\n",
            "strain 2.01202392578125\n",
            "strain 2.044647216796875\n",
            "strain 2.090850830078125\n",
            "strain 2.023345947265625\n",
            "strain 1.966766357421875\n",
            "strain 2.087310791015625\n",
            "strain 2.073028564453125\n",
            "strain 2.0194091796875\n",
            "strain 1.970458984375\n",
            "strain 2.055023193359375\n",
            "strain 2.01934814453125\n",
            "strain 2.061614990234375\n",
            "strain 2.0384521484375\n",
            "strain 2.112579345703125\n",
            "strain 2.007843017578125\n",
            "strain 2.007171630859375\n",
            "strain 1.95184326171875\n",
            "strain 2.129730224609375\n",
            "strain 2.0386962890625\n",
            "strain 1.9775848388671875\n",
            "strain 2.013519287109375\n",
            "strain 2.09246826171875\n",
            "strain 1.95721435546875\n",
            "strain 2.133453369140625\n",
            "strain 2.056732177734375\n",
            "strain 1.997894287109375\n",
            "strain 1.995330810546875\n",
            "strain 2.0164337158203125\n",
            "strain 1.951141357421875\n",
            "strain 1.9254913330078125\n",
            "strain 1.952392578125\n",
            "strain 1.9735107421875\n",
            "strain 2.08428955078125\n",
            "strain 2.055572509765625\n",
            "strain 1.9942626953125\n",
            "strain 2.143646240234375\n",
            "strain 1.99542236328125\n",
            "strain 2.1146240234375\n",
            "strain 2.087799072265625\n",
            "strain 2.00543212890625\n",
            "strain 1.99609375\n",
            "strain 2.006439208984375\n",
            "strain 2.0986328125\n",
            "strain 2.055908203125\n",
            "strain 2.13885498046875\n",
            "strain 2.0333251953125\n",
            "strain 2.03167724609375\n",
            "strain 2.022491455078125\n",
            "strain 2.10284423828125\n",
            "strain 1.985443115234375\n",
            "strain 2.0811004638671875\n",
            "strain 2.026824951171875\n",
            "strain 1.976165771484375\n",
            "strain 2.06353759765625\n",
            "strain 1.968414306640625\n",
            "strain 2.06829833984375\n",
            "strain 2.074737548828125\n",
            "strain 1.9383544921875\n",
            "strain 2.016326904296875\n",
            "strain 2.018035888671875\n",
            "strain 2.0125732421875\n",
            "strain 1.980010986328125\n",
            "strain 1.958282470703125\n",
            "strain 2.013916015625\n",
            "strain 2.09649658203125\n",
            "strain 2.022552490234375\n",
            "strain 1.992889404296875\n",
            "strain 2.1309814453125\n",
            "strain 2.107666015625\n",
            "strain 2.052581787109375\n",
            "strain 2.002044677734375\n",
            "strain 2.074066162109375\n",
            "strain 2.08111572265625\n",
            "strain 2.00494384765625\n",
            "strain 1.984832763671875\n",
            "strain 2.0196533203125\n",
            "strain 2.104644775390625\n",
            "strain 2.10125732421875\n",
            "strain 2.029693603515625\n",
            "strain 1.949615478515625\n",
            "strain 1.99676513671875\n",
            "strain 2.15728759765625\n",
            "strain 1.97625732421875\n",
            "strain 2.076202392578125\n",
            "strain 2.0386962890625\n",
            "strain 1.9815673828125\n",
            "strain 2.13970947265625\n",
            "strain 1.936431884765625\n",
            "strain 2.0367431640625\n",
            "strain 2.0484619140625\n",
            "strain 2.0386962890625\n",
            "strain 1.9915313720703125\n",
            "strain 2.068145751953125\n",
            "strain 2.021148681640625\n",
            "strain 2.04156494140625\n",
            "strain 2.03411865234375\n",
            "strain 2.0025634765625\n",
            "strain 2.0674896240234375\n",
            "strain 2.12457275390625\n",
            "strain 2.0712890625\n",
            "strain 2.04534912109375\n",
            "strain 2.00299072265625\n",
            "strain 2.05560302734375\n",
            "strain 1.963836669921875\n",
            "strain 2.0733642578125\n",
            "strain 2.10321044921875\n",
            "strain 2.011810302734375\n",
            "strain 2.06768798828125\n",
            "strain 2.027618408203125\n",
            "strain 2.01678466796875\n",
            "strain 1.91815185546875\n",
            "strain 2.07659912109375\n",
            "strain 2.003662109375\n",
            "strain 2.0570526123046875\n",
            "strain 2.0487060546875\n",
            "strain 2.023834228515625\n",
            "strain 2.13037109375\n",
            "strain 2.103363037109375\n",
            "strain 2.0772705078125\n",
            "strain 2.127899169921875\n",
            "strain 2.068817138671875\n",
            "strain 1.96240234375\n",
            "strain 2.049591064453125\n",
            "strain 2.042694091796875\n",
            "strain 2.0986328125\n",
            "strain 2.030517578125\n",
            "strain 2.054290771484375\n",
            "strain 1.950653076171875\n",
            "strain 2.079864501953125\n",
            "strain 2.037628173828125\n",
            "strain 1.9267730712890625\n",
            "strain 2.0042266845703125\n",
            "strain 2.04754638671875\n",
            "strain 2.013092041015625\n",
            "strain 2.0113525390625\n",
            "strain 2.02752685546875\n",
            "strain 2.032501220703125\n",
            "strain 1.919891357421875\n",
            "strain 2.04052734375\n",
            "strain 1.987884521484375\n",
            "strain 2.01361083984375\n",
            "strain 1.9573974609375\n",
            "strain 2.065826416015625\n",
            "strain 2.067230224609375\n",
            "strain 2.0787353515625\n",
            "strain 2.0368804931640625\n",
            "strain 2.029449462890625\n",
            "strain 2.091552734375\n",
            "strain 2.03741455078125\n",
            "strain 1.92645263671875\n",
            "strain 2.0350341796875\n",
            "strain 2.064422607421875\n",
            "strain 1.9879302978515625\n",
            "strain 2.0267333984375\n",
            "strain 1.941986083984375\n",
            "strain 2.0025787353515625\n",
            "strain 2.002227783203125\n",
            "strain 2.044097900390625\n",
            "strain 2.053192138671875\n",
            "strain 2.071014404296875\n",
            "strain 1.9579315185546875\n",
            "strain 2.023345947265625\n",
            "strain 2.038848876953125\n",
            "strain 1.987030029296875\n",
            "strain 2.040985107421875\n",
            "strain 2.04058837890625\n",
            "strain 1.940527319908142\n",
            "0.265625\n",
            "0.265625\n",
            "0.2578125\n",
            "0.328125\n",
            "0.2109375\n",
            "0.25\n",
            "0.0625\n",
            "0.3203125\n",
            "0.3203125\n",
            "0.296875\n",
            "strain 2.053253173828125\n",
            "strain 2.040740966796875\n",
            "strain 2.0531005859375\n",
            "strain 1.928375244140625\n",
            "strain 2.072052001953125\n",
            "strain 1.98016357421875\n",
            "strain 2.045989990234375\n",
            "strain 2.003936767578125\n",
            "strain 2.067962646484375\n",
            "strain 2.00927734375\n",
            "strain 2.080169677734375\n",
            "strain 2.0830078125\n",
            "strain 2.161468505859375\n",
            "strain 2.06658935546875\n",
            "strain 1.972564697265625\n",
            "strain 2.12628173828125\n",
            "strain 2.018646240234375\n",
            "strain 2.005096435546875\n",
            "strain 2.03192138671875\n",
            "strain 2.080108642578125\n",
            "strain 2.036407470703125\n",
            "strain 2.0028076171875\n",
            "strain 2.02044677734375\n",
            "strain 2.069061279296875\n",
            "strain 2.090789794921875\n",
            "strain 2.01495361328125\n",
            "strain 1.958160400390625\n",
            "strain 2.078033447265625\n",
            "strain 2.01336669921875\n",
            "strain 2.016754150390625\n",
            "strain 2.100067138671875\n",
            "strain 2.082122802734375\n",
            "strain 2.0025634765625\n",
            "strain 1.956573486328125\n",
            "strain 2.1144561767578125\n",
            "strain 2.009063720703125\n",
            "strain 2.02825927734375\n",
            "strain 2.13177490234375\n",
            "strain 2.108154296875\n",
            "strain 2.039154052734375\n",
            "strain 2.08465576171875\n",
            "strain 2.0714111328125\n",
            "strain 1.9642791748046875\n",
            "strain 2.00439453125\n",
            "strain 2.013092041015625\n",
            "strain 2.071624755859375\n",
            "strain 1.988922119140625\n",
            "strain 2.00762939453125\n",
            "strain 2.0533447265625\n",
            "strain 2.036712646484375\n",
            "strain 1.998321533203125\n",
            "strain 1.98052978515625\n",
            "strain 1.98516845703125\n",
            "strain 1.984100341796875\n",
            "strain 2.01898193359375\n",
            "strain 2.0958099365234375\n",
            "strain 1.985015869140625\n",
            "strain 2.05535888671875\n",
            "strain 2.009613037109375\n",
            "strain 2.139190673828125\n",
            "strain 1.990570068359375\n",
            "strain 1.948974609375\n",
            "strain 1.9537353515625\n",
            "strain 1.9996337890625\n",
            "strain 2.111602783203125\n",
            "strain 2.01513671875\n",
            "strain 2.015045166015625\n",
            "strain 1.979339599609375\n",
            "strain 2.037750244140625\n",
            "strain 2.04547119140625\n",
            "strain 1.979156494140625\n",
            "strain 2.042083740234375\n",
            "strain 2.087554931640625\n",
            "strain 2.174102783203125\n",
            "strain 2.0775146484375\n",
            "strain 1.9686279296875\n",
            "strain 1.9849853515625\n",
            "strain 2.052734375\n",
            "strain 1.963409423828125\n",
            "strain 2.032196044921875\n",
            "strain 2.079742431640625\n",
            "strain 2.122161865234375\n",
            "strain 1.950469970703125\n",
            "strain 1.960296630859375\n",
            "strain 2.015838623046875\n",
            "strain 1.974090576171875\n",
            "strain 1.97711181640625\n",
            "strain 1.948333740234375\n",
            "strain 2.0351715087890625\n",
            "strain 2.042572021484375\n",
            "strain 2.0152587890625\n",
            "strain 2.04644775390625\n",
            "strain 2.011566162109375\n",
            "strain 2.024627685546875\n",
            "strain 2.12109375\n",
            "strain 2.04913330078125\n",
            "strain 2.013763427734375\n",
            "strain 2.0167236328125\n",
            "strain 2.09393310546875\n",
            "strain 2.076385498046875\n",
            "strain 1.9228515625\n",
            "strain 1.98626708984375\n",
            "strain 2.0468597412109375\n",
            "strain 2.16790771484375\n",
            "strain 2.088165283203125\n",
            "strain 1.9400787353515625\n",
            "strain 2.0240478515625\n",
            "strain 1.960693359375\n",
            "strain 1.9795074462890625\n",
            "strain 2.119720458984375\n",
            "strain 2.055511474609375\n",
            "strain 2.045196533203125\n",
            "strain 2.0620574951171875\n",
            "strain 2.031341552734375\n",
            "strain 2.078369140625\n",
            "strain 2.034210205078125\n",
            "strain 2.123260498046875\n",
            "strain 2.045074462890625\n",
            "strain 1.991241455078125\n",
            "strain 2.000640869140625\n",
            "strain 1.997314453125\n",
            "strain 2.109344482421875\n",
            "strain 2.020599365234375\n",
            "strain 1.994384765625\n",
            "strain 2.136138916015625\n",
            "strain 1.994293212890625\n",
            "strain 1.957611083984375\n",
            "strain 2.100372314453125\n",
            "strain 2.061126708984375\n",
            "strain 2.102294921875\n",
            "strain 2.019989013671875\n",
            "strain 1.9251861572265625\n",
            "strain 2.0009765625\n",
            "strain 1.978668212890625\n",
            "strain 2.1083984375\n",
            "strain 2.03326416015625\n",
            "strain 1.990478515625\n",
            "strain 2.057647705078125\n",
            "strain 2.0306396484375\n",
            "strain 2.00970458984375\n",
            "strain 2.00787353515625\n",
            "strain 1.998809814453125\n",
            "strain 2.0512542724609375\n",
            "strain 2.067626953125\n",
            "strain 1.99688720703125\n",
            "strain 1.90106201171875\n",
            "strain 2.0586090087890625\n",
            "strain 1.931793212890625\n",
            "strain 2.156585693359375\n",
            "strain 1.98797607421875\n",
            "strain 2.101531982421875\n",
            "strain 2.04339599609375\n",
            "strain 1.9921875\n",
            "strain 2.1370086669921875\n",
            "strain 2.0477294921875\n",
            "strain 2.015533447265625\n",
            "strain 2.0255126953125\n",
            "strain 2.02545166015625\n",
            "strain 1.95989990234375\n",
            "strain 2.006011962890625\n",
            "strain 2.05938720703125\n",
            "strain 2.070159912109375\n",
            "strain 2.01287841796875\n",
            "strain 1.927215576171875\n",
            "strain 1.9967041015625\n",
            "strain 2.022064208984375\n",
            "strain 2.000091552734375\n",
            "strain 1.99920654296875\n",
            "strain 2.03399658203125\n",
            "strain 2.077972412109375\n",
            "strain 2.111907958984375\n",
            "strain 2.03228759765625\n",
            "strain 2.1246337890625\n",
            "strain 2.01263427734375\n",
            "strain 2.02996826171875\n",
            "strain 2.0001220703125\n",
            "strain 2.040679931640625\n",
            "strain 1.967742919921875\n",
            "strain 2.030029296875\n",
            "strain 1.968170166015625\n",
            "strain 2.0440673828125\n",
            "strain 1.970733642578125\n",
            "strain 2.121185302734375\n",
            "strain 1.99713134765625\n",
            "strain 2.02093505859375\n",
            "strain 1.9642333984375\n",
            "strain 2.03656005859375\n",
            "strain 2.063873291015625\n",
            "strain 1.975616455078125\n",
            "strain 2.048980712890625\n",
            "strain 1.87493896484375\n",
            "strain 1.9837188720703125\n",
            "strain 2.014556884765625\n",
            "strain 2.04205322265625\n",
            "strain 2.07049560546875\n",
            "strain 2.061065673828125\n",
            "strain 1.956390380859375\n",
            "strain 1.96258544921875\n",
            "strain 1.9094696044921875\n",
            "strain 1.98974609375\n",
            "strain 2.084320068359375\n",
            "strain 2.0006103515625\n",
            "strain 2.102386474609375\n",
            "strain 1.99969482421875\n",
            "strain 1.988311767578125\n",
            "strain 2.0186767578125\n",
            "strain 2.104888916015625\n",
            "strain 1.982940673828125\n",
            "strain 2.009735107421875\n",
            "strain 2.07220458984375\n",
            "strain 1.977935791015625\n",
            "strain 2.047454833984375\n",
            "strain 2.01947021484375\n",
            "strain 2.010833740234375\n",
            "strain 2.028228759765625\n",
            "strain 2.038360595703125\n",
            "strain 2.096588134765625\n",
            "strain 1.965972900390625\n",
            "strain 2.027984619140625\n",
            "strain 1.9954833984375\n",
            "strain 2.0648193359375\n",
            "strain 2.03564453125\n",
            "strain 1.9627685546875\n",
            "strain 1.974700927734375\n",
            "strain 2.060028076171875\n",
            "strain 2.0176239013671875\n",
            "strain 2.09210205078125\n",
            "strain 2.003326416015625\n",
            "strain 2.105987548828125\n",
            "strain 2.0237884521484375\n",
            "strain 2.08892822265625\n",
            "strain 1.98193359375\n",
            "strain 2.037017822265625\n",
            "strain 2.08367919921875\n",
            "strain 1.977783203125\n",
            "strain 2.019622802734375\n",
            "strain 2.14532470703125\n",
            "strain 2.048919677734375\n",
            "strain 2.013427734375\n",
            "strain 2.076141357421875\n",
            "strain 2.063995361328125\n",
            "strain 2.0538330078125\n",
            "strain 2.034271240234375\n",
            "strain 2.07025146484375\n",
            "strain 2.079803466796875\n",
            "strain 1.98681640625\n",
            "strain 1.9447021484375\n",
            "strain 2.0384521484375\n",
            "strain 2.103729248046875\n",
            "strain 2.005706787109375\n",
            "strain 1.89373779296875\n",
            "strain 2.119842529296875\n",
            "strain 1.95025634765625\n",
            "strain 1.994049072265625\n",
            "strain 1.9981689453125\n",
            "strain 2.07958984375\n",
            "strain 2.0372314453125\n",
            "strain 2.003814697265625\n",
            "strain 2.05328369140625\n",
            "strain 2.070037841796875\n",
            "strain 1.979644775390625\n",
            "strain 2.06622314453125\n",
            "strain 2.024627685546875\n",
            "strain 2.09478759765625\n",
            "strain 1.9351806640625\n",
            "strain 2.032257080078125\n",
            "strain 2.0496063232421875\n",
            "strain 2.024566650390625\n",
            "strain 2.054046630859375\n",
            "strain 2.0270843505859375\n",
            "strain 2.047393798828125\n",
            "strain 2.116241455078125\n",
            "strain 1.993499755859375\n",
            "strain 2.0601806640625\n",
            "strain 1.96954345703125\n",
            "strain 2.066131591796875\n",
            "strain 1.967681884765625\n",
            "strain 2.00677490234375\n",
            "strain 1.98651123046875\n",
            "strain 2.084381103515625\n",
            "strain 2.084808349609375\n",
            "strain 2.14202880859375\n",
            "strain 1.996551513671875\n",
            "strain 2.12353515625\n",
            "strain 2.064971923828125\n",
            "strain 1.952484130859375\n",
            "strain 1.992919921875\n",
            "strain 2.029632568359375\n",
            "strain 2.00775146484375\n",
            "strain 2.1340484619140625\n",
            "strain 1.98687744140625\n",
            "strain 1.99273681640625\n",
            "strain 2.034454345703125\n",
            "strain 2.029693603515625\n",
            "strain 2.057373046875\n",
            "strain 1.9865875244140625\n",
            "strain 2.05841064453125\n",
            "strain 1.994598388671875\n",
            "strain 2.059051513671875\n",
            "strain 1.96954345703125\n",
            "strain 2.028594970703125\n",
            "strain 2.01678466796875\n",
            "strain 1.980865478515625\n",
            "strain 2.087554931640625\n",
            "strain 2.0550537109375\n",
            "strain 2.048248291015625\n",
            "strain 2.1195068359375\n",
            "strain 2.008636474609375\n",
            "strain 2.094085693359375\n",
            "strain 1.977630615234375\n",
            "strain 1.9807281494140625\n",
            "strain 2.00103759765625\n",
            "strain 1.961822509765625\n",
            "strain 2.011199951171875\n",
            "strain 2.067169189453125\n",
            "strain 2.031280517578125\n",
            "strain 2.03997802734375\n",
            "strain 1.996185302734375\n",
            "strain 2.0693206787109375\n",
            "strain 1.958251953125\n",
            "strain 2.06195068359375\n",
            "strain 2.023223876953125\n",
            "strain 2.056671142578125\n",
            "strain 2.04656982421875\n",
            "strain 2.143157958984375\n",
            "strain 2.0277099609375\n",
            "strain 2.1038818359375\n",
            "strain 1.928314208984375\n",
            "strain 2.04815673828125\n",
            "strain 2.003143310546875\n",
            "strain 2.093414306640625\n",
            "strain 2.074127197265625\n",
            "strain 2.0179290771484375\n",
            "strain 1.980804443359375\n",
            "strain 2.02984619140625\n",
            "strain 1.955322265625\n",
            "strain 2.06524658203125\n",
            "strain 1.93408203125\n",
            "strain 2.0330810546875\n",
            "strain 2.01568603515625\n",
            "strain 2.005462646484375\n",
            "strain 1.98345947265625\n",
            "strain 2.128753662109375\n",
            "strain 2.072265625\n",
            "strain 2.013580322265625\n",
            "strain 2.029205322265625\n",
            "strain 2.01800537109375\n",
            "strain 2.02655029296875\n",
            "strain 2.02490234375\n",
            "strain 1.9652099609375\n",
            "strain 2.05755615234375\n",
            "strain 2.029998779296875\n",
            "strain 2.0114593505859375\n",
            "strain 1.983306884765625\n",
            "strain 2.1260986328125\n",
            "strain 2.026458740234375\n",
            "strain 2.20684814453125\n",
            "strain 1.933135986328125\n",
            "strain 1.9008941650390625\n",
            "strain 2.01824951171875\n",
            "strain 2.02716064453125\n",
            "strain 2.059326171875\n",
            "strain 2.01953125\n",
            "strain 1.99212646484375\n",
            "strain 1.9952545166015625\n",
            "strain 2.100341796875\n",
            "strain 2.079437255859375\n",
            "strain 2.0098114013671875\n",
            "strain 1.9783935546875\n",
            "strain 1.94305419921875\n",
            "strain 2.05255126953125\n",
            "strain 2.044281005859375\n",
            "strain 2.08489990234375\n",
            "strain 1.916534423828125\n",
            "strain 2.09942626953125\n",
            "strain 2.0516357421875\n",
            "strain 1.987884521484375\n",
            "strain 1.90533447265625\n",
            "strain 2.0631103515625\n",
            "strain 1.939788818359375\n",
            "strain 2.108306884765625\n",
            "strain 1.965240478515625\n",
            "strain 2.076324462890625\n",
            "strain 2.0535888671875\n",
            "strain 2.04541015625\n",
            "strain 2.121856689453125\n",
            "strain 2.099151611328125\n",
            "strain 2.1214599609375\n",
            "strain 2.012969970703125\n",
            "strain 2.017181396484375\n",
            "strain 1.9790771007537842\n",
            "0.2734375\n",
            "0.3203125\n",
            "0.265625\n",
            "0.2421875\n",
            "0.3125\n",
            "0.2890625\n",
            "0.203125\n",
            "0.234375\n",
            "0.265625\n",
            "0.2421875\n",
            "strain 2.060211181640625\n",
            "strain 1.993865966796875\n",
            "strain 2.04412841796875\n",
            "strain 2.0532073974609375\n",
            "strain 2.08544921875\n",
            "strain 1.981231689453125\n",
            "strain 2.097686767578125\n",
            "strain 2.020904541015625\n",
            "strain 2.01580810546875\n",
            "strain 1.9779052734375\n",
            "strain 1.995941162109375\n",
            "strain 2.109649658203125\n",
            "strain 2.097259521484375\n",
            "strain 1.99786376953125\n",
            "strain 2.0955810546875\n",
            "strain 2.0845489501953125\n",
            "strain 1.99560546875\n",
            "strain 2.1298828125\n",
            "strain 2.0780029296875\n",
            "strain 2.00982666015625\n",
            "strain 2.00201416015625\n",
            "strain 1.98480224609375\n",
            "strain 1.98541259765625\n",
            "strain 2.060516357421875\n",
            "strain 2.13970947265625\n",
            "strain 2.030181884765625\n",
            "strain 2.0689697265625\n",
            "strain 2.028167724609375\n",
            "strain 2.078460693359375\n",
            "strain 2.088348388671875\n",
            "strain 1.9361572265625\n",
            "strain 1.990142822265625\n",
            "strain 2.0499267578125\n",
            "strain 1.991729736328125\n",
            "strain 2.0749664306640625\n",
            "strain 2.0115966796875\n",
            "strain 2.079498291015625\n",
            "strain 2.1266021728515625\n",
            "strain 2.013275146484375\n",
            "strain 2.050537109375\n",
            "strain 2.094940185546875\n",
            "strain 1.9023284912109375\n",
            "strain 2.068206787109375\n",
            "strain 2.075408935546875\n",
            "strain 2.130523681640625\n",
            "strain 2.038970947265625\n",
            "strain 1.931671142578125\n",
            "strain 1.99505615234375\n",
            "strain 1.98114013671875\n",
            "strain 1.96307373046875\n",
            "strain 2.053680419921875\n",
            "strain 2.042449951171875\n",
            "strain 2.0634613037109375\n",
            "strain 2.066864013671875\n",
            "strain 1.968475341796875\n",
            "strain 1.974151611328125\n",
            "strain 2.041046142578125\n",
            "strain 1.911865234375\n",
            "strain 2.0625\n",
            "strain 1.9461822509765625\n",
            "strain 2.01708984375\n",
            "strain 1.9462890625\n",
            "strain 2.061431884765625\n",
            "strain 1.9376983642578125\n",
            "strain 2.078125\n",
            "strain 1.966400146484375\n",
            "strain 2.07855224609375\n",
            "strain 2.0072021484375\n",
            "strain 2.061553955078125\n",
            "strain 2.1147918701171875\n",
            "strain 2.076416015625\n",
            "strain 1.98046875\n",
            "strain 1.96343994140625\n",
            "strain 2.022857666015625\n",
            "strain 2.0623779296875\n",
            "strain 2.010894775390625\n",
            "strain 2.013885498046875\n",
            "strain 1.97174072265625\n",
            "strain 1.9619140625\n",
            "strain 1.96405029296875\n",
            "strain 2.04681396484375\n",
            "strain 2.1175994873046875\n",
            "strain 2.0426177978515625\n",
            "strain 2.04559326171875\n",
            "strain 1.971282958984375\n",
            "strain 2.022613525390625\n",
            "strain 2.064971923828125\n",
            "strain 2.150421142578125\n",
            "strain 2.06219482421875\n",
            "strain 2.003997802734375\n",
            "strain 1.9741363525390625\n",
            "strain 2.036895751953125\n",
            "strain 2.0821533203125\n",
            "strain 2.1322021484375\n",
            "strain 1.95855712890625\n",
            "strain 2.0764312744140625\n",
            "strain 2.0162353515625\n",
            "strain 1.997894287109375\n",
            "strain 1.98638916015625\n",
            "strain 2.055084228515625\n",
            "strain 1.999664306640625\n",
            "strain 2.020355224609375\n",
            "strain 2.10107421875\n",
            "strain 2.00982666015625\n",
            "strain 2.133087158203125\n",
            "strain 2.05572509765625\n",
            "strain 2.033050537109375\n",
            "strain 1.945098876953125\n",
            "strain 2.113616943359375\n",
            "strain 1.950897216796875\n",
            "strain 2.033294677734375\n",
            "strain 1.933441162109375\n",
            "strain 2.04827880859375\n",
            "strain 2.10540771484375\n",
            "strain 1.9285430908203125\n",
            "strain 2.0191192626953125\n",
            "strain 2.03277587890625\n",
            "strain 2.005645751953125\n",
            "strain 2.122833251953125\n",
            "strain 2.067657470703125\n",
            "strain 2.031707763671875\n",
            "strain 1.92657470703125\n",
            "strain 2.085235595703125\n",
            "strain 2.049835205078125\n",
            "strain 2.154022216796875\n",
            "strain 2.0516357421875\n",
            "strain 1.9693603515625\n",
            "strain 2.0079345703125\n",
            "strain 1.97955322265625\n",
            "strain 2.0618896484375\n",
            "strain 2.0867919921875\n",
            "strain 2.010345458984375\n",
            "strain 1.986358642578125\n",
            "strain 2.0386199951171875\n",
            "strain 2.022308349609375\n",
            "strain 1.95062255859375\n",
            "strain 2.08489990234375\n",
            "strain 2.053497314453125\n",
            "strain 1.9829254150390625\n",
            "strain 2.011993408203125\n",
            "strain 2.07452392578125\n",
            "strain 1.91033935546875\n",
            "strain 2.050262451171875\n",
            "strain 2.030914306640625\n",
            "strain 2.082000732421875\n",
            "strain 2.00970458984375\n",
            "strain 2.076019287109375\n",
            "strain 1.988311767578125\n",
            "strain 2.026824951171875\n",
            "strain 1.9862060546875\n",
            "strain 2.024444580078125\n",
            "strain 1.972442626953125\n",
            "strain 2.06695556640625\n",
            "strain 2.06298828125\n",
            "strain 2.061737060546875\n",
            "strain 2.030181884765625\n",
            "strain 2.051239013671875\n",
            "strain 2.050994873046875\n",
            "strain 1.965362548828125\n",
            "strain 2.029296875\n",
            "strain 1.9850921630859375\n",
            "strain 2.0189971923828125\n",
            "strain 2.063934326171875\n",
            "strain 1.982391357421875\n",
            "strain 2.052398681640625\n",
            "strain 2.032989501953125\n",
            "strain 2.050445556640625\n",
            "strain 2.068572998046875\n",
            "strain 1.8948974609375\n",
            "strain 2.001220703125\n",
            "strain 2.010589599609375\n",
            "strain 2.022552490234375\n",
            "strain 2.0075836181640625\n",
            "strain 2.002838134765625\n",
            "strain 2.09197998046875\n",
            "strain 2.05828857421875\n",
            "strain 2.0323486328125\n",
            "strain 2.029449462890625\n",
            "strain 2.087158203125\n",
            "strain 2.029693603515625\n",
            "strain 1.95184326171875\n",
            "strain 1.964324951171875\n",
            "strain 1.9759521484375\n",
            "strain 2.18011474609375\n",
            "strain 2.0009765625\n",
            "strain 2.031829833984375\n",
            "strain 2.03009033203125\n",
            "strain 2.08209228515625\n",
            "strain 1.9662017822265625\n",
            "strain 2.04754638671875\n",
            "strain 2.14141845703125\n",
            "strain 2.026123046875\n",
            "strain 2.057861328125\n",
            "strain 1.95037841796875\n",
            "strain 2.0745849609375\n",
            "strain 2.044952392578125\n",
            "strain 1.97003173828125\n",
            "strain 2.078582763671875\n",
            "strain 2.0994873046875\n",
            "strain 1.97869873046875\n",
            "strain 2.0811767578125\n",
            "strain 2.00250244140625\n",
            "strain 1.97412109375\n",
            "strain 2.004852294921875\n",
            "strain 2.007568359375\n",
            "strain 1.9056396484375\n",
            "strain 2.116546630859375\n",
            "strain 2.03240966796875\n",
            "strain 1.948638916015625\n",
            "strain 2.123138427734375\n",
            "strain 1.9365234375\n",
            "strain 2.07952880859375\n",
            "strain 1.9358062744140625\n",
            "strain 2.059326171875\n",
            "strain 1.9720458984375\n",
            "strain 2.020599365234375\n",
            "strain 2.106536865234375\n",
            "strain 2.045654296875\n",
            "strain 2.005615234375\n",
            "strain 2.12042236328125\n",
            "strain 2.086181640625\n",
            "strain 1.994964599609375\n",
            "strain 2.113067626953125\n",
            "strain 2.019561767578125\n",
            "strain 2.018096923828125\n",
            "strain 2.068115234375\n",
            "strain 2.094940185546875\n",
            "strain 2.018890380859375\n",
            "strain 1.968658447265625\n",
            "strain 2.101806640625\n",
            "strain 2.012786865234375\n",
            "strain 2.087066650390625\n",
            "strain 2.211212158203125\n",
            "strain 1.98187255859375\n",
            "strain 1.99835205078125\n",
            "strain 1.950836181640625\n",
            "strain 2.050384521484375\n",
            "strain 2.0875244140625\n",
            "strain 2.08135986328125\n",
            "strain 2.04376220703125\n",
            "strain 1.950592041015625\n",
            "strain 1.94232177734375\n",
            "strain 2.05621337890625\n",
            "strain 1.9383544921875\n",
            "strain 2.04327392578125\n",
            "strain 1.96551513671875\n",
            "strain 2.022491455078125\n",
            "strain 1.9268798828125\n",
            "strain 2.224700927734375\n",
            "strain 2.11505126953125\n",
            "strain 2.0214080810546875\n",
            "strain 1.9786376953125\n",
            "strain 2.02294921875\n",
            "strain 1.942840576171875\n",
            "strain 2.0006103515625\n",
            "strain 2.098297119140625\n",
            "strain 2.039520263671875\n",
            "strain 2.081390380859375\n",
            "strain 1.907867431640625\n",
            "strain 2.13116455078125\n",
            "strain 2.0606689453125\n",
            "strain 2.0640869140625\n",
            "strain 2.024688720703125\n",
            "strain 2.049591064453125\n",
            "strain 1.9844970703125\n",
            "strain 2.042510986328125\n",
            "strain 2.212890625\n",
            "strain 2.035186767578125\n",
            "strain 2.107940673828125\n",
            "strain 2.009185791015625\n",
            "strain 2.2607421875\n",
            "strain 2.067169189453125\n",
            "strain 2.158050537109375\n",
            "strain 1.941802978515625\n",
            "strain 1.9879150390625\n",
            "strain 1.99700927734375\n",
            "strain 2.09661865234375\n",
            "strain 2.031646728515625\n",
            "strain 1.98193359375\n",
            "strain 2.067138671875\n",
            "strain 1.96441650390625\n",
            "strain 1.928619384765625\n",
            "strain 2.05615234375\n",
            "strain 2.109161376953125\n",
            "strain 1.966552734375\n",
            "strain 2.047119140625\n",
            "strain 2.02276611328125\n",
            "strain 1.907684326171875\n",
            "strain 2.05474853515625\n",
            "strain 2.02532958984375\n",
            "strain 2.0323486328125\n",
            "strain 2.06109619140625\n",
            "strain 2.087921142578125\n",
            "strain 2.036590576171875\n",
            "strain 2.0294189453125\n",
            "strain 2.038604736328125\n",
            "strain 2.148040771484375\n",
            "strain 2.161102294921875\n",
            "strain 2.047210693359375\n",
            "strain 2.062469482421875\n",
            "strain 2.016357421875\n",
            "strain 2.107879638671875\n",
            "strain 1.99871826171875\n",
            "strain 2.0123748779296875\n",
            "strain 2.033477783203125\n",
            "strain 2.047119140625\n",
            "strain 1.9744873046875\n",
            "strain 2.058349609375\n",
            "strain 2.0342254638671875\n",
            "strain 1.9893798828125\n",
            "strain 1.910064697265625\n",
            "strain 2.109344482421875\n",
            "strain 2.067291259765625\n",
            "strain 2.064208984375\n",
            "strain 2.05401611328125\n",
            "strain 2.015625\n",
            "strain 2.022735595703125\n",
            "strain 2.022216796875\n",
            "strain 1.969635009765625\n",
            "strain 1.989654541015625\n",
            "strain 2.03509521484375\n",
            "strain 2.103485107421875\n",
            "strain 2.019500732421875\n",
            "strain 2.041412353515625\n",
            "strain 1.929351806640625\n",
            "strain 1.931915283203125\n",
            "strain 2.009124755859375\n",
            "strain 2.0682373046875\n",
            "strain 2.086822509765625\n",
            "strain 1.971405029296875\n",
            "strain 1.88385009765625\n",
            "strain 1.93408203125\n",
            "strain 1.905609130859375\n",
            "strain 2.0412139892578125\n",
            "strain 1.93011474609375\n",
            "strain 2.058624267578125\n",
            "strain 2.09259033203125\n",
            "strain 2.0379638671875\n",
            "strain 1.959747314453125\n",
            "strain 1.990447998046875\n",
            "strain 2.106842041015625\n",
            "strain 2.0205078125\n",
            "strain 1.9864959716796875\n",
            "strain 1.961517333984375\n",
            "strain 1.966583251953125\n",
            "strain 1.997955322265625\n",
            "strain 1.9718017578125\n",
            "strain 2.008575439453125\n",
            "strain 2.113922119140625\n",
            "strain 2.0031585693359375\n",
            "strain 2.018157958984375\n",
            "strain 1.9599151611328125\n",
            "strain 2.04388427734375\n",
            "strain 2.06549072265625\n",
            "strain 2.023895263671875\n",
            "strain 2.070068359375\n",
            "strain 1.919158935546875\n",
            "strain 2.011383056640625\n",
            "strain 1.9808197021484375\n",
            "strain 2.038909912109375\n",
            "strain 2.045440673828125\n",
            "strain 1.9539794921875\n",
            "strain 2.012908935546875\n",
            "strain 2.0462646484375\n",
            "strain 1.9443359375\n",
            "strain 1.94342041015625\n",
            "strain 1.929168701171875\n",
            "strain 1.98333740234375\n",
            "strain 2.03955078125\n",
            "strain 1.9623260498046875\n",
            "strain 2.076416015625\n",
            "strain 2.02618408203125\n",
            "strain 1.992919921875\n",
            "strain 2.031768798828125\n",
            "strain 2.029937744140625\n",
            "strain 2.11810302734375\n",
            "strain 2.024688720703125\n",
            "strain 1.955078125\n",
            "strain 2.0706787109375\n",
            "strain 1.986419677734375\n",
            "strain 2.03057861328125\n",
            "strain 2.013397216796875\n",
            "strain 2.07440185546875\n",
            "strain 2.0802001953125\n",
            "strain 2.120849609375\n",
            "strain 2.1195068359375\n",
            "strain 2.02825927734375\n",
            "strain 1.92083740234375\n",
            "strain 2.048675537109375\n",
            "strain 2.08233642578125\n",
            "strain 1.9916503429412842\n",
            "0.2734375\n",
            "0.328125\n",
            "0.265625\n",
            "0.296875\n",
            "0.328125\n",
            "0.28125\n",
            "0.328125\n",
            "0.2734375\n",
            "0.265625\n",
            "0.265625\n",
            "strain 2.044189453125\n",
            "strain 2.0712890625\n",
            "strain 1.997283935546875\n",
            "strain 1.9331207275390625\n",
            "strain 2.1256103515625\n",
            "strain 2.051910400390625\n",
            "strain 2.0013885498046875\n",
            "strain 1.978912353515625\n",
            "strain 2.0372314453125\n",
            "strain 1.991973876953125\n",
            "strain 1.990447998046875\n",
            "strain 2.06817626953125\n",
            "strain 1.973358154296875\n",
            "strain 2.10223388671875\n",
            "strain 1.918060302734375\n",
            "strain 1.982421875\n",
            "strain 2.041107177734375\n",
            "strain 2.011199951171875\n",
            "strain 2.0552978515625\n",
            "strain 2.0543212890625\n",
            "strain 2.06414794921875\n",
            "strain 1.96875\n",
            "strain 2.08465576171875\n",
            "strain 2.0704345703125\n",
            "strain 2.0770263671875\n",
            "strain 1.950958251953125\n",
            "strain 2.04461669921875\n",
            "strain 1.981292724609375\n",
            "strain 1.984344482421875\n",
            "strain 2.060211181640625\n",
            "strain 2.109649658203125\n",
            "strain 2.046356201171875\n",
            "strain 2.005828857421875\n",
            "strain 2.01739501953125\n",
            "strain 2.08538818359375\n",
            "strain 2.068450927734375\n",
            "strain 2.1514892578125\n",
            "strain 1.94036865234375\n",
            "strain 2.011505126953125\n",
            "strain 1.98651123046875\n",
            "strain 1.982330322265625\n",
            "strain 2.09710693359375\n",
            "strain 2.00396728515625\n",
            "strain 2.0663909912109375\n",
            "strain 1.97711181640625\n",
            "strain 2.018463134765625\n",
            "strain 1.9566650390625\n",
            "strain 2.1683349609375\n",
            "strain 2.05096435546875\n",
            "strain 2.001251220703125\n",
            "strain 2.01251220703125\n",
            "strain 2.017974853515625\n",
            "strain 2.13140869140625\n",
            "strain 2.13818359375\n",
            "strain 2.0859375\n",
            "strain 2.0330810546875\n",
            "strain 1.96185302734375\n",
            "strain 2.01995849609375\n",
            "strain 1.922637939453125\n",
            "strain 2.0406494140625\n",
            "strain 2.100311279296875\n",
            "strain 1.966705322265625\n",
            "strain 2.04144287109375\n",
            "strain 1.95977783203125\n",
            "strain 1.98956298828125\n",
            "strain 2.01446533203125\n",
            "strain 1.949310302734375\n",
            "strain 1.9490966796875\n",
            "strain 2.058929443359375\n",
            "strain 2.044281005859375\n",
            "strain 2.041046142578125\n",
            "strain 2.00006103515625\n",
            "strain 2.06146240234375\n",
            "strain 2.074859619140625\n",
            "strain 1.980377197265625\n",
            "strain 2.005401611328125\n",
            "strain 2.081573486328125\n",
            "strain 2.1053466796875\n",
            "strain 2.019622802734375\n",
            "strain 1.988128662109375\n",
            "strain 2.05804443359375\n",
            "strain 2.035614013671875\n",
            "strain 1.99609375\n",
            "strain 1.99749755859375\n",
            "strain 1.946258544921875\n",
            "strain 2.071197509765625\n",
            "strain 2.16033935546875\n",
            "strain 2.0543365478515625\n",
            "strain 2.03631591796875\n",
            "strain 1.98052978515625\n",
            "strain 2.01031494140625\n",
            "strain 2.050506591796875\n",
            "strain 2.03704833984375\n",
            "strain 1.9658966064453125\n",
            "strain 1.967620849609375\n",
            "strain 2.020660400390625\n",
            "strain 2.037841796875\n",
            "strain 1.96807861328125\n",
            "strain 2.135833740234375\n",
            "strain 2.044647216796875\n",
            "strain 2.02227783203125\n",
            "strain 2.06500244140625\n",
            "strain 2.0650634765625\n",
            "strain 2.020416259765625\n",
            "strain 2.0699920654296875\n",
            "strain 2.0982818603515625\n",
            "strain 2.064300537109375\n",
            "strain 1.996612548828125\n",
            "strain 2.0084228515625\n",
            "strain 1.995025634765625\n",
            "strain 2.093841552734375\n",
            "strain 2.010772705078125\n",
            "strain 2.0468597412109375\n",
            "strain 2.020263671875\n",
            "strain 2.097503662109375\n",
            "strain 2.11187744140625\n",
            "strain 2.0353546142578125\n",
            "strain 1.917724609375\n",
            "strain 2.00006103515625\n",
            "strain 2.033721923828125\n",
            "strain 2.143646240234375\n",
            "strain 2.059844970703125\n",
            "strain 1.987396240234375\n",
            "strain 2.059844970703125\n",
            "strain 1.894744873046875\n",
            "strain 2.005950927734375\n",
            "strain 2.07080078125\n",
            "strain 2.059844970703125\n",
            "strain 2.05584716796875\n",
            "strain 2.030059814453125\n",
            "strain 2.0474853515625\n",
            "strain 2.001983642578125\n",
            "strain 2.093780517578125\n",
            "strain 1.976165771484375\n",
            "strain 1.974456787109375\n",
            "strain 2.054779052734375\n",
            "strain 2.03802490234375\n",
            "strain 1.948699951171875\n",
            "strain 2.077606201171875\n",
            "strain 2.053680419921875\n",
            "strain 2.05731201171875\n",
            "strain 2.083465576171875\n",
            "strain 2.033447265625\n",
            "strain 1.934234619140625\n",
            "strain 2.070831298828125\n",
            "strain 2.024169921875\n",
            "strain 1.922088623046875\n",
            "strain 2.03741455078125\n",
            "strain 2.01715087890625\n",
            "strain 2.053741455078125\n",
            "strain 2.02117919921875\n",
            "strain 2.010284423828125\n",
            "strain 1.962799072265625\n",
            "strain 2.00146484375\n",
            "strain 2.08099365234375\n",
            "strain 2.008209228515625\n",
            "strain 2.052490234375\n",
            "strain 2.127899169921875\n",
            "strain 1.98651123046875\n",
            "strain 2.0360107421875\n",
            "strain 2.022796630859375\n",
            "strain 2.057525634765625\n",
            "strain 2.05609130859375\n",
            "strain 1.950775146484375\n",
            "strain 1.984039306640625\n",
            "strain 1.93084716796875\n",
            "strain 2.0989990234375\n",
            "strain 1.945709228515625\n",
            "strain 2.0809326171875\n",
            "strain 2.0082855224609375\n",
            "strain 2.005462646484375\n",
            "strain 2.01171875\n",
            "strain 1.969512939453125\n",
            "strain 1.96923828125\n",
            "strain 2.02178955078125\n",
            "strain 1.994293212890625\n",
            "strain 1.97613525390625\n",
            "strain 1.98577880859375\n",
            "strain 2.007110595703125\n",
            "strain 1.98248291015625\n",
            "strain 2.035858154296875\n",
            "strain 2.1363525390625\n",
            "strain 2.00714111328125\n",
            "strain 1.976715087890625\n",
            "strain 2.070281982421875\n",
            "strain 2.0899200439453125\n",
            "strain 2.002197265625\n",
            "strain 2.03106689453125\n",
            "strain 1.99761962890625\n",
            "strain 2.03662109375\n",
            "strain 2.011871337890625\n",
            "strain 2.010162353515625\n",
            "strain 1.993194580078125\n",
            "strain 2.02801513671875\n",
            "strain 2.13763427734375\n",
            "strain 1.96722412109375\n",
            "strain 2.078765869140625\n",
            "strain 2.0819091796875\n",
            "strain 1.9355926513671875\n",
            "strain 2.020721435546875\n",
            "strain 2.020263671875\n",
            "strain 2.036865234375\n",
            "strain 2.080108642578125\n",
            "strain 2.074066162109375\n",
            "strain 2.046600341796875\n",
            "strain 2.073577880859375\n",
            "strain 2.00048828125\n",
            "strain 2.095703125\n",
            "strain 2.010101318359375\n",
            "strain 2.007171630859375\n",
            "strain 2.01690673828125\n",
            "strain 2.087158203125\n",
            "strain 2.015777587890625\n",
            "strain 1.959716796875\n",
            "strain 2.0491943359375\n",
            "strain 1.9701690673828125\n",
            "strain 2.043304443359375\n",
            "strain 2.0337371826171875\n",
            "strain 2.083587646484375\n",
            "strain 2.031890869140625\n",
            "strain 2.04022216796875\n",
            "strain 1.898773193359375\n",
            "strain 1.9404296875\n",
            "strain 1.9744873046875\n",
            "strain 1.9429779052734375\n",
            "strain 2.0238037109375\n",
            "strain 1.962921142578125\n",
            "strain 1.954345703125\n",
            "strain 1.952301025390625\n",
            "strain 1.9786376953125\n",
            "strain 2.0126953125\n",
            "strain 2.002197265625\n",
            "strain 2.04998779296875\n",
            "strain 2.0810546875\n",
            "strain 2.1116943359375\n",
            "strain 2.115753173828125\n",
            "strain 2.082275390625\n",
            "strain 2.090484619140625\n",
            "strain 2.016693115234375\n",
            "strain 2.08843994140625\n",
            "strain 2.094573974609375\n",
            "strain 1.9974365234375\n",
            "strain 1.9639739990234375\n",
            "strain 2.0699462890625\n",
            "strain 2.03582763671875\n",
            "strain 1.960662841796875\n",
            "strain 2.04339599609375\n",
            "strain 2.07415771484375\n",
            "strain 1.993804931640625\n",
            "strain 2.07977294921875\n",
            "strain 2.150604248046875\n",
            "strain 1.9424591064453125\n",
            "strain 1.953033447265625\n",
            "strain 1.959686279296875\n",
            "strain 1.9661865234375\n",
            "strain 1.952178955078125\n",
            "strain 2.014129638671875\n",
            "strain 2.033538818359375\n",
            "strain 1.998443603515625\n",
            "strain 2.06365966796875\n",
            "strain 2.055755615234375\n",
            "strain 2.1331787109375\n",
            "strain 2.1123046875\n",
            "strain 2.087554931640625\n",
            "strain 2.04241943359375\n",
            "strain 2.019287109375\n",
            "strain 2.030029296875\n",
            "strain 2.04302978515625\n",
            "strain 2.031341552734375\n",
            "strain 2.068878173828125\n",
            "strain 1.993255615234375\n",
            "strain 1.993896484375\n",
            "strain 1.912017822265625\n",
            "strain 2.0894775390625\n",
            "strain 2.06964111328125\n",
            "strain 1.964599609375\n",
            "strain 1.991180419921875\n",
            "strain 2.063018798828125\n",
            "strain 2.0640869140625\n",
            "strain 1.9814453125\n",
            "strain 1.9444580078125\n",
            "strain 2.021026611328125\n",
            "strain 2.07177734375\n",
            "strain 2.044891357421875\n",
            "strain 2.086822509765625\n",
            "strain 2.00390625\n",
            "strain 2.09490966796875\n",
            "strain 2.099578857421875\n",
            "strain 2.023651123046875\n",
            "strain 1.986419677734375\n",
            "strain 2.087066650390625\n",
            "strain 1.9617919921875\n",
            "strain 2.012054443359375\n",
            "strain 2.06280517578125\n",
            "strain 1.97344970703125\n",
            "strain 2.05755615234375\n",
            "strain 2.07928466796875\n",
            "strain 2.089324951171875\n",
            "strain 2.136199951171875\n",
            "strain 1.98724365234375\n",
            "strain 2.068511962890625\n",
            "strain 2.04486083984375\n",
            "strain 2.024932861328125\n",
            "strain 1.963043212890625\n",
            "strain 2.0313720703125\n",
            "strain 2.006439208984375\n",
            "strain 1.952117919921875\n",
            "strain 1.972412109375\n",
            "strain 2.02398681640625\n",
            "strain 2.012786865234375\n",
            "strain 2.10247802734375\n",
            "strain 2.081939697265625\n",
            "strain 2.0286865234375\n",
            "strain 2.043731689453125\n",
            "strain 2.042938232421875\n",
            "strain 1.96197509765625\n",
            "strain 2.169189453125\n",
            "strain 2.078155517578125\n",
            "strain 1.96258544921875\n",
            "strain 1.99468994140625\n",
            "strain 2.04168701171875\n",
            "strain 1.9879150390625\n",
            "strain 1.96832275390625\n",
            "strain 2.0879364013671875\n",
            "strain 1.9945068359375\n",
            "strain 2.03558349609375\n",
            "strain 2.045684814453125\n",
            "strain 1.9905853271484375\n",
            "strain 2.066131591796875\n",
            "strain 2.045989990234375\n",
            "strain 2.019073486328125\n",
            "strain 2.1612548828125\n",
            "strain 2.01458740234375\n",
            "strain 2.102264404296875\n",
            "strain 2.0084228515625\n",
            "strain 2.018585205078125\n",
            "strain 2.135955810546875\n",
            "strain 2.0687713623046875\n",
            "strain 2.098785400390625\n",
            "strain 2.0169525146484375\n",
            "strain 1.9550628662109375\n",
            "strain 1.950286865234375\n",
            "strain 2.08544921875\n",
            "strain 2.018402099609375\n",
            "strain 2.0206298828125\n",
            "strain 2.065399169921875\n",
            "strain 1.8863677978515625\n",
            "strain 2.0715484619140625\n",
            "strain 2.013031005859375\n",
            "strain 1.995452880859375\n",
            "strain 1.994110107421875\n",
            "strain 2.060638427734375\n",
            "strain 2.06396484375\n",
            "strain 2.01348876953125\n",
            "strain 2.094757080078125\n",
            "strain 1.93634033203125\n",
            "strain 1.9137420654296875\n",
            "strain 1.9901123046875\n",
            "strain 2.0960693359375\n",
            "strain 1.952911376953125\n",
            "strain 2.010345458984375\n",
            "strain 1.991668701171875\n",
            "strain 1.980255126953125\n",
            "strain 1.9774169921875\n",
            "strain 2.1123046875\n",
            "strain 2.01171875\n",
            "strain 1.964691162109375\n",
            "strain 1.9689178466796875\n",
            "strain 2.0562744140625\n",
            "strain 1.995880126953125\n",
            "strain 2.062255859375\n",
            "strain 2.0048828125\n",
            "strain 1.991943359375\n",
            "strain 2.108673095703125\n",
            "strain 2.138214111328125\n",
            "strain 2.060546875\n",
            "strain 2.039794921875\n",
            "strain 2.03729248046875\n",
            "strain 2.109771728515625\n",
            "strain 1.9864501953125\n",
            "strain 1.99957275390625\n",
            "strain 2.093475341796875\n",
            "strain 2.16259765625\n",
            "strain 1.95782470703125\n",
            "strain 2.01531982421875\n",
            "strain 2.07635498046875\n",
            "strain 1.986419677734375\n",
            "strain 1.972198486328125\n",
            "strain 2.03997802734375\n",
            "strain 2.084716796875\n",
            "strain 2.1331543922424316\n",
            "0.2734375\n",
            "0.25\n",
            "0.3046875\n",
            "0.2734375\n",
            "0.296875\n",
            "0.234375\n",
            "0.28125\n",
            "0.25\n",
            "0.21875\n",
            "0.3046875\n",
            "strain 2.08837890625\n",
            "strain 2.037322998046875\n",
            "strain 2.1534271240234375\n",
            "strain 2.05059814453125\n",
            "strain 2.0599365234375\n",
            "strain 2.07354736328125\n",
            "strain 1.98431396484375\n",
            "strain 2.080963134765625\n",
            "strain 2.0475006103515625\n",
            "strain 1.991790771484375\n",
            "strain 2.110992431640625\n",
            "strain 2.073455810546875\n",
            "strain 2.0067138671875\n",
            "strain 2.1639862060546875\n",
            "strain 2.01934814453125\n",
            "strain 2.0748291015625\n",
            "strain 1.98602294921875\n",
            "strain 2.040740966796875\n",
            "strain 2.101898193359375\n",
            "strain 2.053375244140625\n",
            "strain 2.122100830078125\n",
            "strain 2.057159423828125\n",
            "strain 2.0240325927734375\n",
            "strain 1.971893310546875\n",
            "strain 2.138458251953125\n",
            "strain 2.009918212890625\n",
            "strain 2.035308837890625\n",
            "strain 2.065460205078125\n",
            "strain 2.1114501953125\n",
            "strain 2.04571533203125\n",
            "strain 2.107421875\n",
            "strain 2.018798828125\n",
            "strain 1.88836669921875\n",
            "strain 2.0377197265625\n",
            "strain 2.062713623046875\n",
            "strain 2.071746826171875\n",
            "strain 2.000640869140625\n",
            "strain 2.07867431640625\n",
            "strain 2.02685546875\n",
            "strain 2.01995849609375\n",
            "strain 2.05889892578125\n",
            "strain 1.98199462890625\n",
            "strain 2.012847900390625\n",
            "strain 2.004486083984375\n",
            "strain 1.904327392578125\n",
            "strain 2.1083984375\n",
            "strain 1.98370361328125\n",
            "strain 2.023773193359375\n",
            "strain 2.050933837890625\n",
            "strain 2.013336181640625\n",
            "strain 1.988372802734375\n",
            "strain 1.957000732421875\n",
            "strain 2.038665771484375\n",
            "strain 2.06683349609375\n",
            "strain 2.038055419921875\n",
            "strain 1.951416015625\n",
            "strain 2.061492919921875\n",
            "strain 2.086578369140625\n",
            "strain 2.117340087890625\n",
            "strain 2.040985107421875\n",
            "strain 2.075286865234375\n",
            "strain 2.08209228515625\n",
            "strain 1.989471435546875\n",
            "strain 2.0416259765625\n",
            "strain 2.121368408203125\n",
            "strain 2.084930419921875\n",
            "strain 2.00823974609375\n",
            "strain 2.105499267578125\n",
            "strain 2.02471923828125\n",
            "strain 1.99444580078125\n",
            "strain 1.975128173828125\n",
            "strain 2.090057373046875\n",
            "strain 2.0391082763671875\n",
            "strain 2.157989501953125\n",
            "strain 1.9417724609375\n",
            "strain 2.13360595703125\n",
            "strain 1.969451904296875\n",
            "strain 2.05230712890625\n",
            "strain 2.0093994140625\n",
            "strain 2.137725830078125\n",
            "strain 1.99737548828125\n",
            "strain 1.998779296875\n",
            "strain 1.9935302734375\n",
            "strain 2.002838134765625\n",
            "strain 2.022735595703125\n",
            "strain 2.060516357421875\n",
            "strain 2.058441162109375\n",
            "strain 1.9696044921875\n",
            "strain 2.04254150390625\n",
            "strain 2.069427490234375\n",
            "strain 2.03424072265625\n",
            "strain 1.96417236328125\n",
            "strain 2.0802001953125\n",
            "strain 2.117431640625\n",
            "strain 2.0762939453125\n",
            "strain 2.114410400390625\n",
            "strain 1.977203369140625\n",
            "strain 2.041748046875\n",
            "strain 1.999847412109375\n",
            "strain 2.0088348388671875\n",
            "strain 2.056610107421875\n",
            "strain 1.94488525390625\n",
            "strain 2.1383056640625\n",
            "strain 2.1185302734375\n",
            "strain 2.02301025390625\n",
            "strain 1.98736572265625\n",
            "strain 1.9765472412109375\n",
            "strain 1.954833984375\n",
            "strain 1.95489501953125\n",
            "strain 1.90875244140625\n",
            "strain 1.9658660888671875\n",
            "strain 2.0683135986328125\n",
            "strain 1.98370361328125\n",
            "strain 2.033203125\n",
            "strain 2.0410614013671875\n",
            "strain 2.02252197265625\n",
            "strain 1.940460205078125\n",
            "strain 2.098907470703125\n",
            "strain 1.991851806640625\n",
            "strain 2.01837158203125\n",
            "strain 2.011322021484375\n",
            "strain 2.022552490234375\n",
            "strain 1.965118408203125\n",
            "strain 1.97491455078125\n",
            "strain 2.027435302734375\n",
            "strain 2.03167724609375\n",
            "strain 1.952362060546875\n",
            "strain 1.982635498046875\n",
            "strain 1.972747802734375\n",
            "strain 2.01312255859375\n",
            "strain 2.037384033203125\n",
            "strain 1.900177001953125\n",
            "strain 2.023101806640625\n",
            "strain 2.03271484375\n",
            "strain 2.001861572265625\n",
            "strain 2.0135498046875\n",
            "strain 1.998077392578125\n",
            "strain 2.02935791015625\n",
            "strain 2.032928466796875\n",
            "strain 2.0322265625\n",
            "strain 1.921356201171875\n",
            "strain 1.912353515625\n",
            "strain 2.0142059326171875\n",
            "strain 2.0521240234375\n",
            "strain 2.071075439453125\n",
            "strain 2.0684814453125\n",
            "strain 2.0543212890625\n",
            "strain 1.99383544921875\n",
            "strain 2.028961181640625\n",
            "strain 2.0384521484375\n",
            "strain 2.03973388671875\n",
            "strain 2.07806396484375\n",
            "strain 2.010040283203125\n",
            "strain 2.026092529296875\n",
            "strain 2.041046142578125\n",
            "strain 2.119354248046875\n",
            "strain 2.0029296875\n",
            "strain 2.018402099609375\n",
            "strain 2.090179443359375\n",
            "strain 1.95947265625\n",
            "strain 1.997894287109375\n",
            "strain 2.09619140625\n",
            "strain 1.95416259765625\n",
            "strain 2.0877685546875\n",
            "strain 2.0540771484375\n",
            "strain 2.027984619140625\n",
            "strain 1.9743499755859375\n",
            "strain 2.112396240234375\n",
            "strain 1.994781494140625\n",
            "strain 1.93060302734375\n",
            "strain 1.9300384521484375\n",
            "strain 1.933990478515625\n",
            "strain 1.997344970703125\n",
            "strain 1.996063232421875\n",
            "strain 1.9639892578125\n",
            "strain 2.12127685546875\n",
            "strain 2.00732421875\n",
            "strain 2.12774658203125\n",
            "strain 2.03802490234375\n",
            "strain 2.057830810546875\n",
            "strain 2.072967529296875\n",
            "strain 2.0877685546875\n",
            "strain 2.0260009765625\n",
            "strain 2.03485107421875\n",
            "strain 1.9639892578125\n",
            "strain 2.147369384765625\n",
            "strain 2.0447235107421875\n",
            "strain 2.105438232421875\n",
            "strain 2.131103515625\n",
            "strain 2.004730224609375\n",
            "strain 2.040924072265625\n",
            "strain 2.06219482421875\n",
            "strain 2.01507568359375\n",
            "strain 2.0802001953125\n",
            "strain 2.032501220703125\n",
            "strain 2.0877685546875\n",
            "strain 2.042266845703125\n",
            "strain 2.084625244140625\n",
            "strain 1.9705810546875\n",
            "strain 2.117706298828125\n",
            "strain 2.01446533203125\n",
            "strain 1.990234375\n",
            "strain 1.9544677734375\n",
            "strain 1.98358154296875\n",
            "strain 1.95074462890625\n",
            "strain 2.0357666015625\n",
            "strain 2.0592041015625\n",
            "strain 2.0843505859375\n",
            "strain 1.986053466796875\n",
            "strain 2.005462646484375\n",
            "strain 1.971832275390625\n",
            "strain 2.170166015625\n",
            "strain 2.109710693359375\n",
            "strain 2.0251617431640625\n",
            "strain 2.0374755859375\n",
            "strain 1.978302001953125\n",
            "strain 1.960906982421875\n",
            "strain 2.10137939453125\n",
            "strain 2.020294189453125\n",
            "strain 2.076568603515625\n",
            "strain 2.142669677734375\n",
            "strain 2.046112060546875\n",
            "strain 1.99432373046875\n",
            "strain 2.02850341796875\n",
            "strain 1.992218017578125\n",
            "strain 1.9671478271484375\n",
            "strain 1.92657470703125\n",
            "strain 1.970855712890625\n",
            "strain 2.05133056640625\n",
            "strain 2.023773193359375\n",
            "strain 1.9122161865234375\n",
            "strain 1.990203857421875\n",
            "strain 2.05157470703125\n",
            "strain 2.07342529296875\n",
            "strain 2.11517333984375\n",
            "strain 1.935546875\n",
            "strain 2.024200439453125\n",
            "strain 1.9954833984375\n",
            "strain 1.9819793701171875\n",
            "strain 2.003265380859375\n",
            "strain 2.050384521484375\n",
            "strain 2.13397216796875\n",
            "strain 2.051971435546875\n",
            "strain 2.053680419921875\n",
            "strain 2.07080078125\n",
            "strain 2.076690673828125\n",
            "strain 1.960662841796875\n",
            "strain 2.030487060546875\n",
            "strain 2.04266357421875\n",
            "strain 2.03460693359375\n",
            "strain 2.027740478515625\n",
            "strain 2.010650634765625\n",
            "strain 2.064544677734375\n",
            "strain 2.056243896484375\n",
            "strain 1.96533203125\n",
            "strain 1.980926513671875\n",
            "strain 1.970947265625\n",
            "strain 2.016998291015625\n",
            "strain 2.074432373046875\n",
            "strain 1.941253662109375\n",
            "strain 2.105682373046875\n",
            "strain 1.970123291015625\n",
            "strain 2.08203125\n",
            "strain 1.998260498046875\n",
            "strain 2.075408935546875\n",
            "strain 1.90863037109375\n",
            "strain 2.078216552734375\n",
            "strain 2.006988525390625\n",
            "strain 2.003631591796875\n",
            "strain 2.00274658203125\n",
            "strain 1.97015380859375\n",
            "strain 2.047821044921875\n",
            "strain 2.141876220703125\n",
            "strain 2.0487060546875\n",
            "strain 2.0335540771484375\n",
            "strain 1.98468017578125\n",
            "strain 2.084136962890625\n",
            "strain 2.1339111328125\n",
            "strain 2.045196533203125\n",
            "strain 1.999786376953125\n",
            "strain 2.000152587890625\n",
            "strain 2.1302490234375\n",
            "strain 2.020538330078125\n",
            "strain 2.002197265625\n",
            "strain 2.09564208984375\n",
            "strain 2.06671142578125\n",
            "strain 2.0181884765625\n",
            "strain 2.067169189453125\n",
            "strain 2.069000244140625\n",
            "strain 2.030792236328125\n",
            "strain 2.044342041015625\n",
            "strain 2.128387451171875\n",
            "strain 1.9994659423828125\n",
            "strain 2.03448486328125\n",
            "strain 2.0548095703125\n",
            "strain 2.125274658203125\n",
            "strain 2.0904541015625\n",
            "strain 1.926300048828125\n",
            "strain 1.992889404296875\n",
            "strain 2.102386474609375\n",
            "strain 2.01556396484375\n",
            "strain 2.002777099609375\n",
            "strain 2.02691650390625\n",
            "strain 1.95672607421875\n",
            "strain 2.12615966796875\n",
            "strain 1.876708984375\n",
            "strain 2.038421630859375\n",
            "strain 2.009918212890625\n",
            "strain 1.984527587890625\n",
            "strain 2.05084228515625\n",
            "strain 2.030792236328125\n",
            "strain 2.005767822265625\n",
            "strain 2.0880126953125\n",
            "strain 1.956207275390625\n",
            "strain 1.976837158203125\n",
            "strain 1.9455718994140625\n",
            "strain 2.016754150390625\n",
            "strain 2.043548583984375\n",
            "strain 2.007293701171875\n",
            "strain 1.97198486328125\n",
            "strain 1.9615478515625\n",
            "strain 1.9550628662109375\n",
            "strain 2.098175048828125\n",
            "strain 2.0130615234375\n",
            "strain 1.9296875\n",
            "strain 2.03289794921875\n",
            "strain 2.0791015625\n",
            "strain 1.95458984375\n",
            "strain 2.029449462890625\n",
            "strain 2.029327392578125\n",
            "strain 1.957611083984375\n",
            "strain 2.02935791015625\n",
            "strain 2.0887908935546875\n",
            "strain 1.986175537109375\n",
            "strain 1.980316162109375\n",
            "strain 1.9151611328125\n",
            "strain 1.993011474609375\n",
            "strain 1.9893798828125\n",
            "strain 2.0230865478515625\n",
            "strain 1.982452392578125\n",
            "strain 2.067169189453125\n",
            "strain 1.972930908203125\n",
            "strain 2.0467071533203125\n",
            "strain 2.0467681884765625\n",
            "strain 2.001953125\n",
            "strain 1.900634765625\n",
            "strain 1.923095703125\n",
            "strain 2.036376953125\n",
            "strain 2.018951416015625\n",
            "strain 2.02374267578125\n",
            "strain 2.03485107421875\n",
            "strain 2.046478271484375\n",
            "strain 1.970611572265625\n",
            "strain 2.069610595703125\n",
            "strain 2.0167999267578125\n",
            "strain 2.110107421875\n",
            "strain 2.038421630859375\n",
            "strain 1.993988037109375\n",
            "strain 1.9200286865234375\n",
            "strain 2.1116790771484375\n",
            "strain 2.079345703125\n",
            "strain 1.999725341796875\n",
            "strain 2.08258056640625\n",
            "strain 2.081878662109375\n",
            "strain 1.959625244140625\n",
            "strain 2.078887939453125\n",
            "strain 1.967010498046875\n",
            "strain 1.928955078125\n",
            "strain 2.066131591796875\n",
            "strain 2.01708984375\n",
            "strain 2.044647216796875\n",
            "strain 2.0191650390625\n",
            "strain 2.007568359375\n",
            "strain 2.03448486328125\n",
            "strain 2.02362060546875\n",
            "strain 2.044281005859375\n",
            "strain 2.0382080078125\n",
            "strain 1.9176788330078125\n",
            "strain 1.93853759765625\n",
            "strain 2.032196044921875\n",
            "strain 2.023895263671875\n",
            "strain 2.04022216796875\n",
            "strain 2.0447845458984375\n",
            "strain 2.0517578125\n",
            "strain 1.96478271484375\n",
            "strain 2.11602783203125\n",
            "strain 1.96923828125\n",
            "strain 2.004364013671875\n",
            "strain 2.021240234375\n",
            "strain 1.9842529296875\n",
            "strain 2.0726561546325684\n",
            "0.3125\n",
            "0.3046875\n",
            "0.265625\n",
            "0.2578125\n",
            "0.3046875\n",
            "0.2734375\n",
            "0.25\n",
            "0.265625\n",
            "0.265625\n",
            "0.2421875\n",
            "strain 2.121368408203125\n",
            "strain 2.053497314453125\n",
            "strain 2.0882568359375\n",
            "strain 2.09228515625\n",
            "strain 2.024749755859375\n",
            "strain 1.9925079345703125\n",
            "strain 2.112060546875\n",
            "strain 1.9945068359375\n",
            "strain 1.949005126953125\n",
            "strain 2.02752685546875\n",
            "strain 2.0465087890625\n",
            "strain 2.031402587890625\n",
            "strain 2.129730224609375\n",
            "strain 1.908477783203125\n",
            "strain 1.929351806640625\n",
            "strain 1.948150634765625\n",
            "strain 2.08380126953125\n",
            "strain 2.0106048583984375\n",
            "strain 2.084503173828125\n",
            "strain 2.03375244140625\n",
            "strain 2.0921630859375\n",
            "strain 2.009979248046875\n",
            "strain 2.020477294921875\n",
            "strain 2.066650390625\n",
            "strain 1.94793701171875\n",
            "strain 2.093353271484375\n",
            "strain 1.9808349609375\n",
            "strain 2.06591796875\n",
            "strain 1.9765625\n",
            "strain 2.040679931640625\n",
            "strain 2.023834228515625\n",
            "strain 2.026275634765625\n",
            "strain 1.88604736328125\n",
            "strain 2.153564453125\n",
            "strain 2.08233642578125\n",
            "strain 2.020050048828125\n",
            "strain 2.034332275390625\n",
            "strain 2.075927734375\n",
            "strain 2.09429931640625\n",
            "strain 2.04791259765625\n",
            "strain 2.0467529296875\n",
            "strain 2.032135009765625\n",
            "strain 2.021331787109375\n",
            "strain 1.9708251953125\n",
            "strain 2.04144287109375\n",
            "strain 2.07562255859375\n",
            "strain 1.986541748046875\n",
            "strain 2.065338134765625\n",
            "strain 2.0870361328125\n",
            "strain 2.04962158203125\n",
            "strain 2.102020263671875\n",
            "strain 2.081573486328125\n",
            "strain 1.99951171875\n",
            "strain 2.018341064453125\n",
            "strain 2.070098876953125\n",
            "strain 2.0156402587890625\n",
            "strain 2.026153564453125\n",
            "strain 2.029144287109375\n",
            "strain 2.014373779296875\n",
            "strain 2.071136474609375\n",
            "strain 2.051483154296875\n",
            "strain 1.9901123046875\n",
            "strain 2.072540283203125\n",
            "strain 1.893707275390625\n",
            "strain 2.03778076171875\n",
            "strain 2.112091064453125\n",
            "strain 1.973907470703125\n",
            "strain 2.042266845703125\n",
            "strain 1.98602294921875\n",
            "strain 1.933074951171875\n",
            "strain 2.05621337890625\n",
            "strain 2.059814453125\n",
            "strain 1.95037841796875\n",
            "strain 2.0201873779296875\n",
            "strain 2.032562255859375\n",
            "strain 2.07269287109375\n",
            "strain 1.994171142578125\n",
            "strain 1.9661865234375\n",
            "strain 1.889373779296875\n",
            "strain 2.12432861328125\n",
            "strain 2.04913330078125\n",
            "strain 2.01324462890625\n",
            "strain 2.04522705078125\n",
            "strain 2.0672607421875\n",
            "strain 2.09423828125\n",
            "strain 2.18548583984375\n",
            "strain 1.8973388671875\n",
            "strain 2.010589599609375\n",
            "strain 2.00421142578125\n",
            "strain 1.933929443359375\n",
            "strain 2.03961181640625\n",
            "strain 2.045562744140625\n",
            "strain 2.011474609375\n",
            "strain 2.03314208984375\n",
            "strain 1.91046142578125\n",
            "strain 2.0643310546875\n",
            "strain 1.988433837890625\n",
            "strain 1.942962646484375\n",
            "strain 2.0008087158203125\n",
            "strain 2.027801513671875\n",
            "strain 2.034454345703125\n",
            "strain 1.981842041015625\n",
            "strain 1.96124267578125\n",
            "strain 2.0096282958984375\n",
            "strain 1.9467926025390625\n",
            "strain 2.0364990234375\n",
            "strain 2.03314208984375\n",
            "strain 2.0025634765625\n",
            "strain 2.0160980224609375\n",
            "strain 2.07855224609375\n",
            "strain 2.0633544921875\n",
            "strain 2.004974365234375\n",
            "strain 2.0949859619140625\n",
            "strain 2.108734130859375\n",
            "strain 2.0372314453125\n",
            "strain 2.02032470703125\n",
            "strain 2.0316162109375\n",
            "strain 2.032257080078125\n",
            "strain 2.08306884765625\n",
            "strain 2.09649658203125\n",
            "strain 2.141571044921875\n",
            "strain 2.155303955078125\n",
            "strain 2.019500732421875\n",
            "strain 2.034820556640625\n",
            "strain 2.072906494140625\n",
            "strain 1.9518890380859375\n",
            "strain 1.967987060546875\n",
            "strain 1.94085693359375\n",
            "strain 1.916290283203125\n",
            "strain 2.012054443359375\n",
            "strain 2.089599609375\n",
            "strain 2.026824951171875\n",
            "strain 2.019500732421875\n",
            "strain 2.11529541015625\n",
            "strain 1.984527587890625\n",
            "strain 2.022918701171875\n",
            "strain 2.022430419921875\n",
            "strain 2.018768310546875\n",
            "strain 1.99041748046875\n",
            "strain 1.9906005859375\n",
            "strain 2.018218994140625\n",
            "strain 2.089019775390625\n",
            "strain 1.86639404296875\n",
            "strain 2.02392578125\n",
            "strain 2.038238525390625\n",
            "strain 2.006011962890625\n",
            "strain 1.986175537109375\n",
            "strain 2.035980224609375\n",
            "strain 2.061309814453125\n",
            "strain 2.09246826171875\n",
            "strain 1.9916839599609375\n",
            "strain 2.064605712890625\n",
            "strain 2.0526885986328125\n",
            "strain 2.1011962890625\n",
            "strain 2.0673828125\n",
            "strain 2.0882568359375\n",
            "strain 2.012939453125\n",
            "strain 1.994537353515625\n",
            "strain 2.0616455078125\n",
            "strain 2.17083740234375\n",
            "strain 1.9454345703125\n",
            "strain 1.964080810546875\n",
            "strain 2.013641357421875\n",
            "strain 2.027435302734375\n",
            "strain 2.034454345703125\n",
            "strain 2.02398681640625\n",
            "strain 1.9423828125\n",
            "strain 2.05291748046875\n",
            "strain 1.953399658203125\n",
            "strain 1.9846954345703125\n",
            "strain 1.95880126953125\n",
            "strain 2.019256591796875\n",
            "strain 2.057891845703125\n",
            "strain 1.976837158203125\n",
            "strain 2.0588531494140625\n",
            "strain 2.10986328125\n",
            "strain 2.035430908203125\n",
            "strain 2.176513671875\n",
            "strain 2.029083251953125\n",
            "strain 2.0389404296875\n",
            "strain 1.97705078125\n",
            "strain 2.0606689453125\n",
            "strain 1.999481201171875\n",
            "strain 2.07086181640625\n",
            "strain 1.990753173828125\n",
            "strain 2.01708984375\n",
            "strain 2.117523193359375\n",
            "strain 2.06103515625\n",
            "strain 1.926971435546875\n",
            "strain 1.991668701171875\n",
            "strain 1.96929931640625\n",
            "strain 2.018035888671875\n",
            "strain 2.032958984375\n",
            "strain 2.0557861328125\n",
            "strain 1.984619140625\n",
            "strain 2.123687744140625\n",
            "strain 2.081878662109375\n",
            "strain 2.065155029296875\n",
            "strain 2.08782958984375\n",
            "strain 2.043701171875\n",
            "strain 2.0503082275390625\n",
            "strain 2.027374267578125\n",
            "strain 2.038177490234375\n",
            "strain 2.012603759765625\n",
            "strain 1.9556884765625\n",
            "strain 2.076934814453125\n",
            "strain 2.01666259765625\n",
            "strain 1.9637451171875\n",
            "strain 2.011871337890625\n",
            "strain 2.11810302734375\n",
            "strain 2.14886474609375\n",
            "strain 1.96575927734375\n",
            "strain 2.014923095703125\n",
            "strain 2.021728515625\n",
            "strain 2.0856170654296875\n",
            "strain 2.069000244140625\n",
            "strain 2.09130859375\n",
            "strain 2.062957763671875\n",
            "strain 2.04364013671875\n",
            "strain 2.005615234375\n",
            "strain 2.12164306640625\n",
            "strain 1.948089599609375\n",
            "strain 2.015655517578125\n",
            "strain 2.0147552490234375\n",
            "strain 1.999908447265625\n",
            "strain 1.954620361328125\n",
            "strain 2.014678955078125\n",
            "strain 2.08270263671875\n",
            "strain 2.04833984375\n",
            "strain 1.875213623046875\n",
            "strain 1.979705810546875\n",
            "strain 2.012939453125\n",
            "strain 1.95550537109375\n",
            "strain 2.1114501953125\n",
            "strain 1.9962158203125\n",
            "strain 2.0223541259765625\n",
            "strain 1.990142822265625\n",
            "strain 1.9896240234375\n",
            "strain 2.058135986328125\n",
            "strain 2.034423828125\n",
            "strain 1.997955322265625\n",
            "strain 2.017364501953125\n",
            "strain 1.994110107421875\n",
            "strain 1.97113037109375\n",
            "strain 2.02203369140625\n",
            "strain 2.0908660888671875\n",
            "strain 2.0028076171875\n",
            "strain 2.050018310546875\n",
            "strain 2.003936767578125\n",
            "strain 2.061309814453125\n",
            "strain 2.02447509765625\n",
            "strain 2.0344696044921875\n",
            "strain 2.0909423828125\n",
            "strain 2.0767822265625\n",
            "strain 1.996490478515625\n",
            "strain 1.985504150390625\n",
            "strain 2.123748779296875\n",
            "strain 2.07867431640625\n",
            "strain 2.074066162109375\n",
            "strain 1.99346923828125\n",
            "strain 2.025909423828125\n",
            "strain 2.08587646484375\n",
            "strain 2.03997802734375\n",
            "strain 2.02264404296875\n",
            "strain 2.0673828125\n",
            "strain 2.04229736328125\n",
            "strain 2.0380859375\n",
            "strain 2.00408935546875\n",
            "strain 2.0148162841796875\n",
            "strain 2.0189208984375\n",
            "strain 2.0387725830078125\n",
            "strain 2.06414794921875\n",
            "strain 1.99066162109375\n",
            "strain 2.011505126953125\n",
            "strain 1.994659423828125\n",
            "strain 2.056732177734375\n",
            "strain 1.9873046875\n",
            "strain 2.034942626953125\n",
            "strain 2.0419921875\n",
            "strain 2.083648681640625\n",
            "strain 2.042449951171875\n",
            "strain 2.075592041015625\n",
            "strain 2.04876708984375\n",
            "strain 2.031280517578125\n",
            "strain 1.9589691162109375\n",
            "strain 1.9942626953125\n",
            "strain 2.02606201171875\n",
            "strain 1.949554443359375\n",
            "strain 2.0146484375\n",
            "strain 1.93731689453125\n",
            "strain 1.91217041015625\n",
            "strain 1.958038330078125\n",
            "strain 2.03424072265625\n",
            "strain 2.03314208984375\n",
            "strain 1.920257568359375\n",
            "strain 2.042694091796875\n",
            "strain 2.05804443359375\n",
            "strain 2.137420654296875\n",
            "strain 2.071502685546875\n",
            "strain 2.076751708984375\n",
            "strain 2.084930419921875\n",
            "strain 1.983154296875\n",
            "strain 2.022369384765625\n",
            "strain 1.988983154296875\n",
            "strain 2.005950927734375\n",
            "strain 2.157379150390625\n",
            "strain 1.9892578125\n",
            "strain 2.0998992919921875\n",
            "strain 2.0057373046875\n",
            "strain 2.00457763671875\n",
            "strain 1.972381591796875\n",
            "strain 2.1038970947265625\n",
            "strain 1.94744873046875\n",
            "strain 2.005340576171875\n",
            "strain 2.032379150390625\n",
            "strain 2.032623291015625\n",
            "strain 1.981903076171875\n",
            "strain 1.99261474609375\n",
            "strain 1.97528076171875\n",
            "strain 2.030303955078125\n",
            "strain 2.004791259765625\n",
            "strain 2.011688232421875\n",
            "strain 2.0008392333984375\n",
            "strain 1.98785400390625\n",
            "strain 2.0343017578125\n",
            "strain 2.02508544921875\n",
            "strain 2.1024169921875\n",
            "strain 2.0953369140625\n",
            "strain 1.989105224609375\n",
            "strain 2.05230712890625\n",
            "strain 2.065673828125\n",
            "strain 1.9980010986328125\n",
            "strain 2.032623291015625\n",
            "strain 2.144073486328125\n",
            "strain 2.0498046875\n",
            "strain 2.13775634765625\n",
            "strain 2.0555419921875\n",
            "strain 2.059600830078125\n",
            "strain 2.016998291015625\n",
            "strain 2.037261962890625\n",
            "strain 2.0620880126953125\n",
            "strain 1.95428466796875\n",
            "strain 2.078338623046875\n",
            "strain 1.999755859375\n",
            "strain 2.04449462890625\n",
            "strain 2.057342529296875\n",
            "strain 2.032135009765625\n",
            "strain 2.091461181640625\n",
            "strain 2.14862060546875\n",
            "strain 2.05010986328125\n",
            "strain 2.0244140625\n",
            "strain 2.092529296875\n",
            "strain 2.1485595703125\n",
            "strain 2.050811767578125\n",
            "strain 1.891876220703125\n",
            "strain 1.996978759765625\n",
            "strain 2.012664794921875\n",
            "strain 2.009979248046875\n",
            "strain 1.95489501953125\n",
            "strain 2.14093017578125\n",
            "strain 2.07379150390625\n",
            "strain 2.009124755859375\n",
            "strain 1.99176025390625\n",
            "strain 2.136077880859375\n",
            "strain 2.024688720703125\n",
            "strain 2.0440673828125\n",
            "strain 1.95458984375\n",
            "strain 2.028106689453125\n",
            "strain 2.071746826171875\n",
            "strain 1.935150146484375\n",
            "strain 2.05078125\n",
            "strain 1.912689208984375\n",
            "strain 1.99407958984375\n",
            "strain 2.042816162109375\n",
            "strain 2.050811767578125\n",
            "strain 2.125030517578125\n",
            "strain 1.95867919921875\n",
            "strain 1.91375732421875\n",
            "strain 1.97613525390625\n",
            "strain 2.023101806640625\n",
            "strain 1.974273681640625\n",
            "strain 2.011505126953125\n",
            "strain 1.96966552734375\n",
            "strain 2.08050537109375\n",
            "strain 2.026336669921875\n",
            "strain 1.924957275390625\n",
            "strain 2.025634765625\n",
            "strain 1.909942626953125\n",
            "strain 1.997222900390625\n",
            "strain 1.938720703125\n",
            "strain 1.9863770008087158\n",
            "0.1875\n",
            "0.296875\n",
            "0.2578125\n",
            "0.3515625\n",
            "0.328125\n",
            "0.2890625\n",
            "0.3046875\n",
            "0.203125\n",
            "0.2109375\n",
            "0.296875\n",
            "strain 2.032196044921875\n",
            "strain 1.972991943359375\n",
            "strain 2.0511474609375\n",
            "strain 2.046661376953125\n",
            "strain 1.991943359375\n",
            "strain 2.056915283203125\n",
            "strain 2.047454833984375\n",
            "strain 2.010162353515625\n",
            "strain 2.024383544921875\n",
            "strain 2.181488037109375\n",
            "strain 1.958282470703125\n",
            "strain 2.060455322265625\n",
            "strain 1.920196533203125\n",
            "strain 2.098846435546875\n",
            "strain 2.0895843505859375\n",
            "strain 2.018218994140625\n",
            "strain 2.05517578125\n",
            "strain 1.93695068359375\n",
            "strain 2.065185546875\n",
            "strain 2.00537109375\n",
            "strain 1.992919921875\n",
            "strain 2.025970458984375\n",
            "strain 2.1416015625\n",
            "strain 2.070404052734375\n",
            "strain 1.99566650390625\n",
            "strain 1.9261474609375\n",
            "strain 1.97216796875\n",
            "strain 1.839202880859375\n",
            "strain 2.08831787109375\n",
            "strain 2.075592041015625\n",
            "strain 2.0233154296875\n",
            "strain 2.041259765625\n",
            "strain 2.095703125\n",
            "strain 2.086822509765625\n",
            "strain 2.03900146484375\n",
            "strain 1.984649658203125\n",
            "strain 2.071868896484375\n",
            "strain 2.0106353759765625\n",
            "strain 1.987335205078125\n",
            "strain 2.057037353515625\n",
            "strain 2.035919189453125\n",
            "strain 2.029876708984375\n",
            "strain 1.934478759765625\n",
            "strain 2.03717041015625\n",
            "strain 1.97650146484375\n",
            "strain 2.009246826171875\n",
            "strain 2.00872802734375\n",
            "strain 2.001922607421875\n",
            "strain 1.96875\n",
            "strain 2.0079345703125\n",
            "strain 2.067626953125\n",
            "strain 2.057220458984375\n",
            "strain 2.017242431640625\n",
            "strain 1.979156494140625\n",
            "strain 2.01629638671875\n",
            "strain 2.06390380859375\n",
            "strain 2.100921630859375\n",
            "strain 2.01971435546875\n",
            "strain 2.16143798828125\n",
            "strain 1.9205322265625\n",
            "strain 1.935150146484375\n",
            "strain 1.986541748046875\n",
            "strain 1.897552490234375\n",
            "strain 2.021331787109375\n",
            "strain 1.96478271484375\n",
            "strain 2.126922607421875\n",
            "strain 2.022552490234375\n",
            "strain 1.95904541015625\n",
            "strain 1.951080322265625\n",
            "strain 2.084197998046875\n",
            "strain 1.99578857421875\n",
            "strain 1.99249267578125\n",
            "strain 2.017059326171875\n",
            "strain 2.044891357421875\n",
            "strain 1.93017578125\n",
            "strain 2.0467529296875\n",
            "strain 2.042633056640625\n",
            "strain 2.0250701904296875\n",
            "strain 2.03936767578125\n",
            "strain 2.169891357421875\n",
            "strain 2.145904541015625\n",
            "strain 2.0194244384765625\n",
            "strain 2.0526123046875\n",
            "strain 1.997833251953125\n",
            "strain 2.1123046875\n",
            "strain 2.067169189453125\n",
            "strain 2.103546142578125\n",
            "strain 2.095123291015625\n",
            "strain 2.1048583984375\n",
            "strain 2.119384765625\n",
            "strain 2.014617919921875\n",
            "strain 2.045257568359375\n",
            "strain 1.975250244140625\n",
            "strain 2.0545654296875\n",
            "strain 2.0223388671875\n",
            "strain 1.9530029296875\n",
            "strain 1.972259521484375\n",
            "strain 2.016876220703125\n",
            "strain 1.9086456298828125\n",
            "strain 1.981048583984375\n",
            "strain 2.0382080078125\n",
            "strain 2.00860595703125\n",
            "strain 2.05670166015625\n",
            "strain 2.07330322265625\n",
            "strain 2.0087890625\n",
            "strain 2.063262939453125\n",
            "strain 2.0394439697265625\n",
            "strain 2.00054931640625\n",
            "strain 2.014617919921875\n",
            "strain 2.018707275390625\n",
            "strain 2.040924072265625\n",
            "strain 2.09210205078125\n",
            "strain 2.034149169921875\n",
            "strain 2.09295654296875\n",
            "strain 2.003814697265625\n",
            "strain 2.036590576171875\n",
            "strain 2.053619384765625\n",
            "strain 2.091644287109375\n",
            "strain 1.97314453125\n",
            "strain 2.0648040771484375\n",
            "strain 2.006011962890625\n",
            "strain 2.026031494140625\n",
            "strain 2.0306396484375\n",
            "strain 2.019989013671875\n",
            "strain 1.98907470703125\n",
            "strain 1.93896484375\n",
            "strain 1.9234619140625\n",
            "strain 2.145050048828125\n",
            "strain 2.107513427734375\n",
            "strain 1.9991302490234375\n",
            "strain 1.9561767578125\n",
            "strain 2.1002197265625\n",
            "strain 1.989776611328125\n",
            "strain 2.09600830078125\n",
            "strain 2.003570556640625\n",
            "strain 2.0609130859375\n",
            "strain 1.968048095703125\n",
            "strain 2.041748046875\n",
            "strain 2.135833740234375\n",
            "strain 2.069915771484375\n",
            "strain 1.991912841796875\n",
            "strain 2.11260986328125\n",
            "strain 2.062652587890625\n",
            "strain 1.91455078125\n",
            "strain 2.15203857421875\n",
            "strain 1.97100830078125\n",
            "strain 2.0548553466796875\n",
            "strain 2.0311279296875\n",
            "strain 2.009124755859375\n",
            "strain 2.039703369140625\n",
            "strain 2.065032958984375\n",
            "strain 2.04388427734375\n",
            "strain 2.075225830078125\n",
            "strain 2.0294189453125\n",
            "strain 2.0515289306640625\n",
            "strain 2.0064697265625\n",
            "strain 2.038055419921875\n",
            "strain 2.08270263671875\n",
            "strain 1.972320556640625\n",
            "strain 2.03045654296875\n",
            "strain 2.01177978515625\n",
            "strain 1.999481201171875\n",
            "strain 2.033447265625\n",
            "strain 2.0660400390625\n",
            "strain 2.033233642578125\n",
            "strain 2.032867431640625\n",
            "strain 2.081878662109375\n",
            "strain 2.015472412109375\n",
            "strain 1.9705810546875\n",
            "strain 2.068267822265625\n",
            "strain 2.06976318359375\n",
            "strain 2.012481689453125\n",
            "strain 2.082305908203125\n",
            "strain 1.967803955078125\n",
            "strain 2.15155029296875\n",
            "strain 2.066986083984375\n",
            "strain 2.04412841796875\n",
            "strain 2.03558349609375\n",
            "strain 2.0931396484375\n",
            "strain 1.906524658203125\n",
            "strain 2.140716552734375\n",
            "strain 2.05804443359375\n",
            "strain 2.0260467529296875\n",
            "strain 1.987030029296875\n",
            "strain 2.035247802734375\n",
            "strain 1.928070068359375\n",
            "strain 1.9484405517578125\n",
            "strain 1.959014892578125\n",
            "strain 2.04217529296875\n",
            "strain 2.007965087890625\n",
            "strain 1.916015625\n",
            "strain 1.988128662109375\n",
            "strain 2.058013916015625\n",
            "strain 2.02996826171875\n",
            "strain 2.07769775390625\n",
            "strain 2.000885009765625\n",
            "strain 2.03045654296875\n",
            "strain 2.15350341796875\n",
            "strain 2.06829833984375\n",
            "strain 1.9794921875\n",
            "strain 2.04156494140625\n",
            "strain 2.019500732421875\n",
            "strain 2.01116943359375\n",
            "strain 2.0072174072265625\n",
            "strain 1.969024658203125\n",
            "strain 2.005126953125\n",
            "strain 2.013092041015625\n",
            "strain 1.961700439453125\n",
            "strain 2.050323486328125\n",
            "strain 2.078125\n",
            "strain 1.964569091796875\n",
            "strain 1.9626617431640625\n",
            "strain 2.09088134765625\n",
            "strain 1.961639404296875\n",
            "strain 2.0435638427734375\n",
            "strain 2.086517333984375\n",
            "strain 2.0623779296875\n",
            "strain 1.993408203125\n",
            "strain 1.90972900390625\n",
            "strain 1.902130126953125\n",
            "strain 2.053924560546875\n",
            "strain 2.110992431640625\n",
            "strain 1.9072265625\n",
            "strain 1.998016357421875\n",
            "strain 2.15972900390625\n",
            "strain 2.0908203125\n",
            "strain 1.980987548828125\n",
            "strain 1.96844482421875\n",
            "strain 1.995086669921875\n",
            "strain 1.989898681640625\n",
            "strain 1.961273193359375\n",
            "strain 2.01708984375\n",
            "strain 2.008514404296875\n",
            "strain 2.023468017578125\n",
            "strain 2.023284912109375\n",
            "strain 2.0518798828125\n",
            "strain 2.087921142578125\n",
            "strain 2.0721435546875\n",
            "strain 2.0386962890625\n",
            "strain 2.0955810546875\n",
            "strain 1.913726806640625\n",
            "strain 2.039703369140625\n",
            "strain 2.0289306640625\n",
            "strain 2.119415283203125\n",
            "strain 2.12939453125\n",
            "strain 1.937255859375\n",
            "strain 2.101806640625\n",
            "strain 2.02642822265625\n",
            "strain 1.9632568359375\n",
            "strain 2.103607177734375\n",
            "strain 2.082794189453125\n",
            "strain 2.06378173828125\n",
            "strain 2.027099609375\n",
            "strain 2.0281982421875\n",
            "strain 2.063323974609375\n",
            "strain 2.0202484130859375\n",
            "strain 1.90850830078125\n",
            "strain 2.02264404296875\n",
            "strain 2.0311279296875\n",
            "strain 2.011962890625\n",
            "strain 2.009857177734375\n",
            "strain 2.020233154296875\n",
            "strain 2.0215911865234375\n",
            "strain 2.057281494140625\n",
            "strain 2.0604248046875\n",
            "strain 1.954437255859375\n",
            "strain 2.0452423095703125\n",
            "strain 2.064117431640625\n",
            "strain 2.01910400390625\n",
            "strain 1.956634521484375\n",
            "strain 2.052581787109375\n",
            "strain 2.01031494140625\n",
            "strain 2.03936767578125\n",
            "strain 2.058563232421875\n",
            "strain 2.08221435546875\n",
            "strain 2.036285400390625\n",
            "strain 1.9921875\n",
            "strain 2.11865234375\n",
            "strain 2.100982666015625\n",
            "strain 1.962188720703125\n",
            "strain 1.96636962890625\n",
            "strain 2.10284423828125\n",
            "strain 2.02069091796875\n",
            "strain 2.03070068359375\n",
            "strain 2.090606689453125\n",
            "strain 2.107818603515625\n",
            "strain 2.031829833984375\n",
            "strain 1.98968505859375\n",
            "strain 2.033599853515625\n",
            "strain 2.090728759765625\n",
            "strain 2.012054443359375\n",
            "strain 1.99072265625\n",
            "strain 2.0126953125\n",
            "strain 2.0997314453125\n",
            "strain 1.96771240234375\n",
            "strain 2.032135009765625\n",
            "strain 2.072509765625\n",
            "strain 1.981231689453125\n",
            "strain 1.970947265625\n",
            "strain 2.14654541015625\n",
            "strain 2.0384521484375\n",
            "strain 2.02337646484375\n",
            "strain 1.960296630859375\n",
            "strain 2.082733154296875\n",
            "strain 1.967437744140625\n",
            "strain 2.003448486328125\n",
            "strain 2.046661376953125\n",
            "strain 1.90667724609375\n",
            "strain 2.060455322265625\n",
            "strain 1.95867919921875\n",
            "strain 2.013214111328125\n",
            "strain 1.9407958984375\n",
            "strain 2.07122802734375\n",
            "strain 2.035797119140625\n",
            "strain 1.998382568359375\n",
            "strain 2.034912109375\n",
            "strain 2.13848876953125\n",
            "strain 2.080810546875\n",
            "strain 2.095306396484375\n",
            "strain 1.969573974609375\n",
            "strain 1.9764404296875\n",
            "strain 2.096771240234375\n",
            "strain 1.929962158203125\n",
            "strain 2.08782958984375\n",
            "strain 1.954833984375\n",
            "strain 2.044036865234375\n",
            "strain 2.027862548828125\n",
            "strain 2.077056884765625\n",
            "strain 1.99835205078125\n",
            "strain 1.95257568359375\n",
            "strain 1.9549560546875\n",
            "strain 2.013885498046875\n",
            "strain 2.0196075439453125\n",
            "strain 2.037841796875\n",
            "strain 1.99713134765625\n",
            "strain 1.981658935546875\n",
            "strain 2.1187744140625\n",
            "strain 1.9609375\n",
            "strain 2.044647216796875\n",
            "strain 2.13824462890625\n",
            "strain 2.066070556640625\n",
            "strain 1.951019287109375\n",
            "strain 1.969696044921875\n",
            "strain 2.054351806640625\n",
            "strain 2.021881103515625\n",
            "strain 1.9957427978515625\n",
            "strain 2.050537109375\n",
            "strain 2.086029052734375\n",
            "strain 2.015533447265625\n",
            "strain 1.9761962890625\n",
            "strain 2.021148681640625\n",
            "strain 2.014739990234375\n",
            "strain 2.073638916015625\n",
            "strain 2.0185394287109375\n",
            "strain 2.030364990234375\n",
            "strain 1.976959228515625\n",
            "strain 2.124267578125\n",
            "strain 1.873931884765625\n",
            "strain 2.005859375\n",
            "strain 2.005859375\n",
            "strain 1.911529541015625\n",
            "strain 1.94140625\n",
            "strain 1.98138427734375\n",
            "strain 1.996185302734375\n",
            "strain 1.96356201171875\n",
            "strain 2.063201904296875\n",
            "strain 2.009674072265625\n",
            "strain 2.016357421875\n",
            "strain 2.0076904296875\n",
            "strain 2.037750244140625\n",
            "strain 2.15472412109375\n",
            "strain 2.00439453125\n",
            "strain 2.023590087890625\n",
            "strain 2.014404296875\n",
            "strain 2.08709716796875\n",
            "strain 2.090911865234375\n",
            "strain 2.11932373046875\n",
            "strain 1.99853515625\n",
            "strain 2.0455322265625\n",
            "strain 2.01129150390625\n",
            "strain 1.97125244140625\n",
            "strain 2.118865966796875\n",
            "strain 2.020660400390625\n",
            "strain 1.90570068359375\n",
            "strain 1.947601318359375\n",
            "strain 2.014617919921875\n",
            "strain 2.0435333251953125\n",
            "strain 2.02972412109375\n",
            "strain 2.0283355712890625\n",
            "strain 1.947540283203125\n",
            "strain 1.96533203125\n",
            "0.3125\n",
            "0.21875\n",
            "0.2265625\n",
            "0.234375\n",
            "0.234375\n",
            "0.296875\n",
            "0.2890625\n",
            "0.359375\n",
            "0.2109375\n",
            "0.2421875\n",
            "strain 2.00726318359375\n",
            "strain 2.090179443359375\n",
            "strain 2.05645751953125\n",
            "strain 2.087310791015625\n",
            "strain 2.13507080078125\n",
            "strain 2.021331787109375\n",
            "strain 2.039306640625\n",
            "strain 1.891326904296875\n",
            "strain 2.053802490234375\n",
            "strain 1.978302001953125\n",
            "strain 2.03839111328125\n",
            "strain 2.134002685546875\n",
            "strain 2.047393798828125\n",
            "strain 2.02130126953125\n",
            "strain 2.0018310546875\n",
            "strain 1.941680908203125\n",
            "strain 1.990631103515625\n",
            "strain 2.04034423828125\n",
            "strain 1.98095703125\n",
            "strain 1.916900634765625\n",
            "strain 2.004364013671875\n",
            "strain 2.071624755859375\n",
            "strain 1.9097900390625\n",
            "strain 2.12054443359375\n",
            "strain 2.08062744140625\n",
            "strain 2.072906494140625\n",
            "strain 1.955657958984375\n",
            "strain 1.97528076171875\n",
            "strain 1.90643310546875\n",
            "strain 2.04876708984375\n",
            "strain 1.989227294921875\n",
            "strain 2.0012359619140625\n",
            "strain 2.005218505859375\n",
            "strain 1.978515625\n",
            "strain 2.04168701171875\n",
            "strain 2.050933837890625\n",
            "strain 2.130340576171875\n",
            "strain 2.066162109375\n",
            "strain 1.972503662109375\n",
            "strain 1.982391357421875\n",
            "strain 1.98614501953125\n",
            "strain 2.012054443359375\n",
            "strain 2.100341796875\n",
            "strain 2.074462890625\n",
            "strain 2.0279541015625\n",
            "strain 1.953460693359375\n",
            "strain 2.014617919921875\n",
            "strain 1.998443603515625\n",
            "strain 2.05609130859375\n",
            "strain 2.067108154296875\n",
            "strain 1.952606201171875\n",
            "strain 2.056396484375\n",
            "strain 2.03387451171875\n",
            "strain 2.016754150390625\n",
            "strain 1.89593505859375\n",
            "strain 1.963287353515625\n",
            "strain 2.0091552734375\n",
            "strain 2.05364990234375\n",
            "strain 2.08074951171875\n",
            "strain 1.940521240234375\n",
            "strain 2.127960205078125\n",
            "strain 2.01446533203125\n",
            "strain 2.02789306640625\n",
            "strain 1.983306884765625\n",
            "strain 2.037353515625\n",
            "strain 1.953277587890625\n",
            "strain 1.8851318359375\n",
            "strain 2.090545654296875\n",
            "strain 2.082977294921875\n",
            "strain 2.1236572265625\n",
            "strain 2.027099609375\n",
            "strain 2.07806396484375\n",
            "strain 2.05047607421875\n",
            "strain 2.047088623046875\n",
            "strain 2.210784912109375\n",
            "strain 2.058258056640625\n",
            "strain 1.8736419677734375\n",
            "strain 2.103485107421875\n",
            "strain 1.964599609375\n",
            "strain 1.93365478515625\n",
            "strain 1.996185302734375\n",
            "strain 1.961761474609375\n",
            "strain 2.03326416015625\n",
            "strain 2.067779541015625\n",
            "strain 2.0272216796875\n",
            "strain 1.998443603515625\n",
            "strain 2.0502471923828125\n",
            "strain 2.1146392822265625\n",
            "strain 1.91790771484375\n",
            "strain 2.080230712890625\n",
            "strain 2.064117431640625\n",
            "strain 2.04473876953125\n",
            "strain 2.006805419921875\n",
            "strain 2.118743896484375\n",
            "strain 2.0023193359375\n",
            "strain 1.972686767578125\n",
            "strain 2.002471923828125\n",
            "strain 2.08404541015625\n",
            "strain 2.073974609375\n",
            "strain 2.04461669921875\n",
            "strain 2.038970947265625\n",
            "strain 2.11737060546875\n",
            "strain 2.04351806640625\n",
            "strain 2.049224853515625\n",
            "strain 2.043975830078125\n",
            "strain 2.00164794921875\n",
            "strain 1.9527587890625\n",
            "strain 2.05694580078125\n",
            "strain 1.99114990234375\n",
            "strain 2.018798828125\n",
            "strain 1.98858642578125\n",
            "strain 1.98876953125\n",
            "strain 1.944793701171875\n",
            "strain 2.09124755859375\n",
            "strain 1.99853515625\n",
            "strain 2.110748291015625\n",
            "strain 2.006683349609375\n",
            "strain 2.05706787109375\n",
            "strain 2.081298828125\n",
            "strain 1.972137451171875\n",
            "strain 1.990234375\n",
            "strain 1.988372802734375\n",
            "strain 1.950836181640625\n",
            "strain 2.035186767578125\n",
            "strain 1.95770263671875\n",
            "strain 2.044708251953125\n",
            "strain 2.08642578125\n",
            "strain 2.0574951171875\n",
            "strain 2.046783447265625\n",
            "strain 2.01214599609375\n",
            "strain 2.008697509765625\n",
            "strain 1.969482421875\n",
            "strain 2.1036376953125\n",
            "strain 2.097503662109375\n",
            "strain 2.067291259765625\n",
            "strain 1.99053955078125\n",
            "strain 2.001373291015625\n",
            "strain 1.9930877685546875\n",
            "strain 2.1170654296875\n",
            "strain 1.951416015625\n",
            "strain 2.0029296875\n",
            "strain 2.04119873046875\n",
            "strain 2.0411376953125\n",
            "strain 2.050567626953125\n",
            "strain 1.97314453125\n",
            "strain 2.079833984375\n",
            "strain 1.906494140625\n",
            "strain 2.136688232421875\n",
            "strain 2.02117919921875\n",
            "strain 2.023101806640625\n",
            "strain 1.97003173828125\n",
            "strain 1.981231689453125\n",
            "strain 1.993927001953125\n",
            "strain 2.04656982421875\n",
            "strain 2.044708251953125\n",
            "strain 2.044921875\n",
            "strain 1.98834228515625\n",
            "strain 1.989349365234375\n",
            "strain 2.060394287109375\n",
            "strain 2.06329345703125\n",
            "strain 1.93402099609375\n",
            "strain 2.047698974609375\n",
            "strain 2.036651611328125\n",
            "strain 2.007080078125\n",
            "strain 1.975311279296875\n",
            "strain 2.09039306640625\n",
            "strain 1.976593017578125\n",
            "strain 2.0626678466796875\n",
            "strain 1.972137451171875\n",
            "strain 2.09906005859375\n",
            "strain 2.08941650390625\n",
            "strain 2.054901123046875\n",
            "strain 2.047698974609375\n",
            "strain 1.979400634765625\n",
            "strain 1.947418212890625\n",
            "strain 1.983062744140625\n",
            "strain 2.0321044921875\n",
            "strain 1.9566650390625\n",
            "strain 1.95819091796875\n",
            "strain 2.0531005859375\n",
            "strain 2.149444580078125\n",
            "strain 2.058563232421875\n",
            "strain 2.0592041015625\n",
            "strain 1.984710693359375\n",
            "strain 1.999786376953125\n",
            "strain 1.970001220703125\n",
            "strain 1.878692626953125\n",
            "strain 1.9969482421875\n",
            "strain 1.95501708984375\n",
            "strain 2.03948974609375\n",
            "strain 2.0125732421875\n",
            "strain 2.089263916015625\n",
            "strain 1.964599609375\n",
            "strain 2.04803466796875\n",
            "strain 2.029388427734375\n",
            "strain 2.01812744140625\n",
            "strain 2.033599853515625\n",
            "strain 1.988128662109375\n",
            "strain 2.002655029296875\n",
            "strain 2.0182647705078125\n",
            "strain 2.05670166015625\n",
            "strain 1.9590911865234375\n",
            "strain 2.202880859375\n",
            "strain 2.01641845703125\n",
            "strain 1.941680908203125\n",
            "strain 1.991943359375\n",
            "strain 1.96856689453125\n",
            "strain 2.03582763671875\n",
            "strain 1.9776153564453125\n",
            "strain 2.135955810546875\n",
            "strain 2.054443359375\n",
            "strain 1.9942169189453125\n",
            "strain 2.0731201171875\n",
            "strain 2.056793212890625\n",
            "strain 1.990234375\n",
            "strain 2.007232666015625\n",
            "strain 2.143341064453125\n",
            "strain 2.12152099609375\n",
            "strain 2.016357421875\n",
            "strain 2.150848388671875\n",
            "strain 1.983489990234375\n",
            "strain 2.018310546875\n",
            "strain 1.977752685546875\n",
            "strain 2.091461181640625\n",
            "strain 2.045013427734375\n",
            "strain 2.022003173828125\n",
            "strain 2.064361572265625\n",
            "strain 2.015472412109375\n",
            "strain 2.039093017578125\n",
            "strain 2.008026123046875\n",
            "strain 2.059295654296875\n",
            "strain 2.01727294921875\n",
            "strain 2.01153564453125\n",
            "strain 2.054443359375\n",
            "strain 1.95159912109375\n",
            "strain 1.997039794921875\n",
            "strain 2.093902587890625\n",
            "strain 1.908294677734375\n",
            "strain 1.946014404296875\n",
            "strain 2.038909912109375\n",
            "strain 1.9713134765625\n",
            "strain 2.0318603515625\n",
            "strain 2.088043212890625\n",
            "strain 2.11785888671875\n",
            "strain 2.112884521484375\n",
            "strain 2.07421875\n",
            "strain 1.96514892578125\n",
            "strain 2.107757568359375\n",
            "strain 1.978973388671875\n",
            "strain 2.050262451171875\n",
            "strain 2.04315185546875\n",
            "strain 1.969024658203125\n",
            "strain 1.9903564453125\n",
            "strain 1.972808837890625\n",
            "strain 2.059661865234375\n",
            "strain 2.107696533203125\n",
            "strain 1.9677734375\n",
            "strain 1.9859619140625\n",
            "strain 1.966278076171875\n",
            "strain 1.966064453125\n",
            "strain 2.013275146484375\n",
            "strain 1.989013671875\n",
            "strain 1.99658203125\n",
            "strain 2.006378173828125\n",
            "strain 1.988067626953125\n",
            "strain 2.039031982421875\n",
            "strain 2.063873291015625\n",
            "strain 1.9673309326171875\n",
            "strain 1.936614990234375\n",
            "strain 2.07568359375\n",
            "strain 2.192596435546875\n",
            "strain 1.98590087890625\n",
            "strain 2.1255340576171875\n",
            "strain 2.063140869140625\n",
            "strain 2.082672119140625\n",
            "strain 2.05206298828125\n",
            "strain 2.04595947265625\n",
            "strain 1.93316650390625\n",
            "strain 2.160858154296875\n",
            "strain 2.100494384765625\n",
            "strain 2.02154541015625\n",
            "strain 2.021026611328125\n",
            "strain 2.050537109375\n",
            "strain 2.08905029296875\n",
            "strain 1.9632110595703125\n",
            "strain 1.998077392578125\n",
            "strain 1.9993896484375\n",
            "strain 2.087921142578125\n",
            "strain 2.03472900390625\n",
            "strain 2.021087646484375\n",
            "strain 2.1002197265625\n",
            "strain 2.03924560546875\n",
            "strain 1.936676025390625\n",
            "strain 2.0364990234375\n",
            "strain 2.0266265869140625\n",
            "strain 2.087127685546875\n",
            "strain 1.973968505859375\n",
            "strain 2.0675048828125\n",
            "strain 2.04962158203125\n",
            "strain 1.983123779296875\n",
            "strain 2.024749755859375\n",
            "strain 2.114898681640625\n",
            "strain 2.1419677734375\n",
            "strain 2.03509521484375\n",
            "strain 2.01629638671875\n",
            "strain 2.046356201171875\n",
            "strain 1.987701416015625\n",
            "strain 2.013671875\n",
            "strain 1.986480712890625\n",
            "strain 2.077606201171875\n",
            "strain 2.095062255859375\n",
            "strain 1.9736480712890625\n",
            "strain 1.997344970703125\n",
            "strain 1.987884521484375\n",
            "strain 2.080322265625\n",
            "strain 1.99114990234375\n",
            "strain 2.0728759765625\n",
            "strain 2.01422119140625\n",
            "strain 1.9434814453125\n",
            "strain 1.97119140625\n",
            "strain 2.012237548828125\n",
            "strain 2.027069091796875\n",
            "strain 2.0327301025390625\n",
            "strain 2.150543212890625\n",
            "strain 2.03216552734375\n",
            "strain 1.982421875\n",
            "strain 2.052459716796875\n",
            "strain 1.97039794921875\n",
            "strain 2.0715484619140625\n",
            "strain 2.090484619140625\n",
            "strain 2.08929443359375\n",
            "strain 1.9000244140625\n",
            "strain 2.1250152587890625\n",
            "strain 2.017425537109375\n",
            "strain 1.993896484375\n",
            "strain 2.0717010498046875\n",
            "strain 1.9796142578125\n",
            "strain 1.997772216796875\n",
            "strain 1.94049072265625\n",
            "strain 2.05499267578125\n",
            "strain 1.906494140625\n",
            "strain 2.021148681640625\n",
            "strain 2.0774078369140625\n",
            "strain 1.931121826171875\n",
            "strain 1.98370361328125\n",
            "strain 2.03314208984375\n",
            "strain 1.956390380859375\n",
            "strain 1.93634033203125\n",
            "strain 2.05633544921875\n",
            "strain 2.12677001953125\n",
            "strain 2.050933837890625\n",
            "strain 1.94671630859375\n"
          ]
        }
      ],
      "source": [
        "# @title strain ctrain test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "def strain(model, dataloader, optim, scheduler=None): # train function with automatic mixed precision\n",
        "# def strain(model, train_iter, optim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.train()\n",
        "    classifier.train()\n",
        "    # for i, (img, _) in enumerate(dataloader):\n",
        "    for i, (img, y) in enumerate(dataloader):\n",
        "    # for i in range(50):\n",
        "    #     # try: img, _ = next(train_iter)\n",
        "    #     try: img, y = next(train_iter)\n",
        "    #     except StopIteration:\n",
        "    #         train_iter = iter(train_loader)\n",
        "    #         # img, _ = next(train_iter)\n",
        "    #         img, y = next(train_iter)\n",
        "        img = img.to(device).flatten(2).transpose(-2,-1)#.to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "        y = y.to(device)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            # loss = model.loss(img)\n",
        "            y_ = model.classify(img)\n",
        "            # sx = model.context_encoder(img).mean(dim=1)\n",
        "            # # print(sx.shape)\n",
        "            # y_ = classifier(sx)\n",
        "            # # print(y_.shape, y.shape)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        # print(loss)\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # total_norm = 0\n",
        "        # for p in model.parameters(): total_norm += p.grad.data.norm(2).item() ** 2\n",
        "        # total_norm = total_norm**.5\n",
        "        # print('total_norm', total_norm)\n",
        "        # for p in list(filter(lambda p: p.grad is not None, model.parameters())):\n",
        "        #     print(p.grad.data.norm(2).item())\n",
        "        # print(\"max grad norm\", max([p.grad.data.norm(2).item() for p in list(filter(lambda p: p.grad is not None, model.parameters()))]))\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 10) # 0.5\n",
        "\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        # with torch.no_grad():\n",
        "        #     m=0.99999 # 0.99 m = next(momentum_scheduler)\n",
        "        #     norms=[]\n",
        "        #     for param_q, param_k in zip(model.context_encoder.parameters(), model.target_encoder.parameters()):\n",
        "        #         # norm = ((param_k.data-param_q.detach().data)**2).sum()**.5\n",
        "        #         # # # print(param_k.data.shape, norm)\n",
        "        #         # norms.append(norm.item())\n",
        "        #         # # if norm>.01:\n",
        "        #         param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "        #     # print(norms)\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        print(\"strain\",loss.item())\n",
        "        # for param in seq_jepa.context_encoder.cls: print(param.data)\n",
        "        # for param in seq_jepa.predicter.cls: print(param.data)\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "    return train_iter\n",
        "\n",
        "\n",
        "# def ctrain(model, classifier, dataloader, coptim, scheduler=None): # train function with automatic mixed precision\n",
        "def ctrain(model, classifier, train_iter, coptim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.eval()\n",
        "    classifier.train()\n",
        "    # for i, (img, y) in enumerate(dataloader):\n",
        "    for i in range(10):\n",
        "        try: img, y = next(train_iter)\n",
        "        except StopIteration:\n",
        "            train_iter = iter(train_loader)\n",
        "            img, y = next(train_iter)\n",
        "        # print(\"ctrain\",y)\n",
        "        img, y = img.to(device), y.to(device) # [batch, ]\n",
        "        img = img.flatten(2).transpose(-2,-1).to(torch.bfloat16)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            with torch.no_grad():\n",
        "                sx = model(img).detach()\n",
        "            # print(sx[0][0])\n",
        "            y_ = classifier(sx)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "        # print(classifier.classifier.weight[0])\n",
        "        # print(y_.shape, y.shape)\n",
        "        # print(loss)\n",
        "        coptim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 5) # .5\n",
        "        scaler.step(coptim)\n",
        "        scaler.update()\n",
        "        print(\"classify\",loss.item())\n",
        "        try: wandb.log({\"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "    return train_iter\n",
        "\n",
        "\n",
        "# def test(model, dataloader):\n",
        "def test(model, classifier, test_iter):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    # for i, (img, y) in enumerate(dataloader):\n",
        "    for i in range(10):\n",
        "        try: img, y = next(test_iter)\n",
        "        except StopIteration:\n",
        "            test_iter = iter(test_loader)\n",
        "            img, y = next(test_iter)\n",
        "        # print(\"test\",y)\n",
        "\n",
        "        img, y = img.to(device), y.to(device) # [batch, ]\n",
        "        img = img.flatten(2).transpose(-2,-1)#.to(torch.bfloat16)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # sx = model(img)\n",
        "            # y_ = classifier(sx)\n",
        "            y_ = model.classify(img)\n",
        "\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        print(correct/len(y))\n",
        "        try: wandb.log({\"correct\": correct/len(y)})\n",
        "        except NameError: pass\n",
        "    return test_iter\n",
        "\n",
        "train_iter = iter(train_loader)\n",
        "test_iter = iter(test_loader)\n",
        "for i in range(100):\n",
        "    strain(seq_jepa, train_loader, optim)\n",
        "    # ctrain(seq_jepa, classifier, train_loader, coptim)\n",
        "    # test(seq_jepa, test_loader)\n",
        "\n",
        "    # train_iter = strain(seq_jepa, train_iter, optim)\n",
        "    # train_iter = ctrain(seq_jepa, classifier, train_iter, coptim)\n",
        "    test_iter = test(seq_jepa, classifier, test_iter)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# for param in seq_jepa.context_encoder.cls: print(param.data)\n",
        "# print(seq_jepa.context_encoder.cls.is_leaf)\n",
        "# seq=40\n",
        "# src = seq_jepa.context_encoder.pos_encoder(torch.arange(seq, device=device))\n",
        "# for x in src:\n",
        "#     print(x)\n",
        "# print(.999**50)\n"
      ],
      "metadata": {
        "id": "WgN5LlxWsPzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title save/load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder='/content/drive/MyDrive/jepa/'\n",
        "\n",
        "# # modelsd, optimsd = torch.load(folder+'SeqJEPA.pkl', map_location=device).values()\n",
        "# modelsd, optimsd = torch.load('SeqJEPA.pkl', map_location=device).values()\n",
        "# seq_jepa.load_state_dict(modelsd, strict=False)\n",
        "# optim.load_state_dict(optimsd)"
      ],
      "metadata": {
        "id": "JT8AgOx0E_KM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b3d3969-da28-4cc0-c4de-f0a327ea266f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = {'model': seq_jepa.state_dict(), 'optimizer': optim.state_dict()}\n",
        "torch.save(checkpoint, folder+'SeqJEPA.pkl')\n",
        "# torch.save(checkpoint, 'SeqJEPA.pkl')"
      ],
      "metadata": {
        "id": "CpbLPvjiE-pD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2Nd-sGe6Ku4S",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "db867114-47ae-43ce-f2e2-7068c339ff42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbobdole\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250313_025737-i4hp0zgv</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/SeqJEPA/runs/i4hp0zgv' target=\"_blank\">zany-paper-50</a></strong> to <a href='https://wandb.ai/bobdole/SeqJEPA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/SeqJEPA' target=\"_blank\">https://wandb.ai/bobdole/SeqJEPA</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/SeqJEPA/runs/i4hp0zgv' target=\"_blank\">https://wandb.ai/bobdole/SeqJEPA/runs/i4hp0zgv</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "!pip install -q wandb\n",
        "import wandb # https://docs.wandb.ai/quickstart\n",
        "wandb.login(key='487a2109e55dce4e13fc70681781de9f50f27be7')\n",
        "try: run.finish()\n",
        "except NameError: pass\n",
        "run = wandb.init(project=\"SeqJEPA\", config={\"model\": \"res18\",})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## drawer"
      ],
      "metadata": {
        "id": "0vScvFGGSeN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(4,100,3)\n",
        "\n",
        "x=x.transpose(-2,-1) # [batch, dim, seq]\n",
        " # in, out, kernel, stride, pad\n",
        "# net = nn.Conv1d(3,16,7,2,7//2)\n",
        "net = nn.Sequential(nn.Conv1d(3,16,7,2,7//2), nn.MaxPool1d(3, 2, 3//2))\n",
        "# net = nn.Conv1d(3,16,(7,3),2,3//2)\n",
        "out = net(x)\n",
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_YBe6-eZ2Zq",
        "outputId": "09b27f75-ba04-4aaf-f5e4-1b9c1dc8d112"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 16, 25])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test repeat_interleave\n",
        "# x = torch.rand(3,5)\n",
        "x = torch.rand(3,5,7)\n",
        "print(x)\n",
        "# out = x.repeat(2,1) # [2*3,5]\n",
        "# print(out)\n",
        "# x_ = out.reshape(2,3,5)\n",
        "# print(x_)\n",
        "# out = x.repeat_interleave(2,1,1) # [3*2,5]\n",
        "out = x.repeat_interleave(2,dim=0) # [3*2,5]\n",
        "# print(out)\n",
        "# x_ = out.reshape(2,3,5,7)\n",
        "x_ = out.reshape(3,2,5,7)\n",
        "print(x_)\n",
        "\n",
        "\n",
        "# batch=1\n",
        "# seq_len=5\n",
        "# dim=4\n",
        "# positional_emb = torch.rand(batch, seq_len, dim)\n",
        "# print(positional_emb)\n",
        "# num_tok=2\n",
        "# trg_indices=torch.randint(0,seq_len,(M, num_tok))\n",
        "# print(trg_indices)\n",
        "# # pos_embs = positional_emb.gather(1, trg_indices.unsqueeze(0))\n",
        "# pos_embs = positional_emb[torch.arange(batch)[...,None,None],trg_indices.unsqueeze(0)]\n",
        "# # pos_embs = positional_emb[0,trg_indices]\n",
        "# # [batch, M, num_tok, dim]\n",
        "# print(pos_embs.shape)\n",
        "# print(pos_embs)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "R6kiiqw3uIdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test ropes\n",
        "# for i, (img, _) in enumerate(test_loader):\n",
        "#     break\n",
        "# print(img.shape) # [batch, 1, 28, 28]\n",
        "# print(img[0]) # [0,1)\n",
        "d_model=8\n",
        "# rot_emb = RotEmb(d_model, top=torch.pi, base=10)\n",
        "# x = torch.linspace(0,1,20)\n",
        "# out = rot_emb(x)\n",
        "# print(out.shape)\n",
        "# print(out)\n",
        "\n",
        "# # pos_encoder = RoPE(d_model, seq_len=15, base=100)\n",
        "# pos_encoder = RoPE(d_model, seq_len=15, base=10000)\n",
        "x = torch.ones(1, 10, d_model)\n",
        "# # x = torch.ones(1, 10, d_model)\n",
        "# out = pos_encoder(x)\n",
        "# # print(out.shape)\n",
        "# for x in out[0]:\n",
        "#     print(x)\n",
        "\n",
        "\n",
        "# pos_encoder = LearnedRoPE(d_model, seq_len=512)\n",
        "# out = pos_encoder(x)\n",
        "# for o in out[0]:\n",
        "#     print(o)\n",
        "\n",
        "# pos_encoder.weights = nn.Parameter(torch.randn(1, d_model//2))\n",
        "# out = pos_encoder(x)\n",
        "# for o in out[0]:\n",
        "#     print(o)\n",
        "\n",
        "\n",
        "# tensor([-0.0177, -0.9998,  0.8080, -0.5892, -0.7502, -0.6612,  0.3885,  0.9214])\n"
      ],
      "metadata": {
        "id": "8r_GjI_vCMyo",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Transformer Model\n",
        "import torch\n",
        "from torch import nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        # self.embed = nn.Sequential(\n",
        "        #     nn.Linear(in_dim, d_model), #act,\n",
        "        #     # nn.Linear(d_model, d_model), act,\n",
        "        # )\n",
        "        self.embed = nn.Linear(in_dim, d_model) if in_dim != d_model else None\n",
        "        # self.embed = nn.Sequential(nn.Conv1d(in_dim,d_model,7,2,7//2), nn.MaxPool1d(3, 2, 3//2))\n",
        "        self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, d_hid or d_model, dropout, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.d_model = d_model\n",
        "        self.cls = nn.Parameter(torch.zeros(1,1,d_model)) # randn\n",
        "        out_dim = out_dim or d_model\n",
        "        # self.lin = nn.Linear(d_model, out_dim)\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim != d_model else None\n",
        "        self.norm = nn.LayerNorm(out_dim)\n",
        "\n",
        "\n",
        "    def forward(self, src, src_key_padding_mask=None, cls_mask=None, context_indices=None, trg_indices=None): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # if cls_mask != None: src[cls_mask] = self.cls.to(src.dtype)\n",
        "\n",
        "        if self.embed != None: src = self.embed(src) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = src.shape\n",
        "        # src = self.pos_encoder(src)\n",
        "        if context_indices != None:\n",
        "            # print(src.shape, context_indices.shape, self.pos_encoder(context_indices).shape) # [2, 88, 32]) torch.Size([88]) torch.Size([88, 32]\n",
        "            # print(src[0], self.pos_encoder(context_indices)[0])\n",
        "            src = src * self.pos_encoder(context_indices) # context/predictor # src = src + self.positional_emb[:,context_indices]\n",
        "            # print(src[0])\n",
        "        else: src = src * self.pos_encoder(torch.arange(seq, device=device)) # target # src = src + self.positional_emb[:,:seq]\n",
        "            # print(\"trans fwd\", src.shape, self.pos_encoder(src).shape)\n",
        "\n",
        "        if trg_indices != None: # [M, num_trg_toks]\n",
        "            pred_tokens = self.cls * self.pos_encoder(trg_indices) # [M, num_trg_toks, d_model] # pred_tokens = self.cls + self.positional_emb[0,trg_indices]\n",
        "            pred_tokens = pred_tokens.repeat(batch, 1, 1) # [batch*M, num_trg_toks, d_model]\n",
        "            # print(pred_tokens.requires_grad)\n",
        "            src = src.repeat_interleave(trg_indices.shape[0], dim=0) # [batch, seq_len, d_model] -> [batch*M, seq_len, d_model]\n",
        "            src = torch.cat([src, pred_tokens], dim=1) # [batch*M, seq_len+num_trg_toks, d_model]\n",
        "\n",
        "        out = self.transformer_encoder(src, src_key_padding_mask=src_key_padding_mask) # float [seq_len, batch_size, d_model]\n",
        "        if trg_indices != None:\n",
        "            # print(out.shape)\n",
        "            out = out[:,seq:] # [batch*M, num_trg_toks, d_model]\n",
        "        if self.lin != None: out = self.lin(out)\n",
        "        out = self.norm(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "batch, seq_len, d_model = 4,7,64\n",
        "in_dim = 3\n",
        "model = TransformerModel(in_dim, d_model, nhead=8, nlayers=2, dropout=0.).to(device)\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "# print(out.shape)\n",
        "# # print(out)\n"
      ],
      "metadata": {
        "id": "-8i1WLsvH_vn",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title SeqJEPA old\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def multiblock(batch, seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "        mask_len = torch.rand(batch, M) * (max_s - min_s) + min_s # in (min_s, max_s)\n",
        "        mask_pos = torch.rand(batch, M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "        mask_len, mask_pos = mask_len * seq, mask_pos * seq\n",
        "        indices = torch.arange(seq)[None,None,...]\n",
        "        target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [batch, M, seq]\n",
        "        target_mask = target_mask.any(1) # True -> masked # multiblock masking # [batch, seq]\n",
        "        return target_mask\n",
        "\n",
        "\n",
        "\n",
        "class SeqJEPA(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.in_dim = in_dim\n",
        "        # if out_dim is None: self.out_dim = in_dim\n",
        "        if out_dim is None: out_dim = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.d_model = d_model\n",
        "        act = nn.GELU() # ReLU GELU SiLU\n",
        "\n",
        "        dim=8\n",
        "\n",
        "\n",
        "        self.calorie_emb = RotEmb(dim, top=1, base=10) # 0-4\n",
        "        self.steps_emb = RotEmb(dim, top=1, base=10) # 0-5\n",
        "        self.enc0 = nn.Sequential(\n",
        "            nn.Linear(dim*2, d_model), act,\n",
        "            nn.Linear(d_model, d_model), act,\n",
        "        )\n",
        "        self.hour_emb = CircularEmb(dim, torch.pi/2) # circular (0,1) # 0-24\n",
        "        self.temp_emb = RotEmb(dim, top=1/3, base=10) # 18-35\n",
        "        self.heart_emb = RotEmb(dim, top=1/3, base=100) # 50-200\n",
        "        self.enc1 = nn.Sequential(\n",
        "            nn.Linear(dim*3, d_model), act,\n",
        "        )\n",
        "\n",
        "        # self.transformer = TransformerClassifier(d_model, out_dim=out_dim, nhead=8, nlayers=2)\n",
        "        # # self.fc = nn.Linear(d_model, out_dim)\n",
        "        self.context_encoder = TransformerModel(d_model, out_dim=out_dim, nhead=8, nlayers=2, dropout=0.)\n",
        "        self.predicter = TransformerModel(out_dim, nhead=8, nlayers=1, dropout=0.)\n",
        "        self.target_encoder = AveragedModel(self.context_encoder, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    def loss(self, hr, temp, heart): # [batch, 1+T]\n",
        "        # batch = hr.size(0)\n",
        "        res_id, activeKilocalories, steps = hr[:,0], temp[:,0], heart[:,0]\n",
        "        # enc_summary = self.enc0(torch.cat([self.calorie_emb(activeKilocalories), self.steps_emb(steps)], dim=-1)) # [batch, d_model]\n",
        "        enc_summary = self.enc0(torch.cat([self.calorie_emb(torch.log(activeKilocalories.clamp(min=1))), self.steps_emb(torch.log(steps.clamp(min=1)))], dim=-1)) # [batch, d_model]\n",
        "        x = self.enc1(torch.cat([self.hour_emb(hr[:,1:]/24), self.temp_emb(temp[:,1:]), self.heart_emb(heart[:,1:])], dim=-1)) # [batch, T, d_model]\n",
        "        # print('violet fwd', hr.shape, enc_summary.shape, x.shape)\n",
        "\n",
        "        # # mask=(torch.rand(self.seq_len)<.1) # True -> masked # random masking\n",
        "        # mask=(torch.rand_like(x)<.1) # True -> masked # random masking\n",
        "\n",
        "        # patches pad -enc> patch_enc\n",
        "        # rearrange patch in padded/masked, -pred> pred of masked\n",
        "        # mse tgt_enc(img)\n",
        "\n",
        "        # average L2 distance between the predicted patch-level representations sˆy(i) and the target patch-level representation sy(i);\n",
        "        # F.smooth_l1_loss\n",
        "\n",
        "        batch, seq, dim = x.shape\n",
        "\n",
        "        # mask=(torch.rand(batch, seq)<.75) # True -> masked # random masking\n",
        "        # sorted_mask, ids = mask.sort(dim=1)\n",
        "        # # sorted_x = x[torch.arange(batch)[...,None,None],ids]\n",
        "        # sorted_x = x[torch.arange(batch).unsqueeze(-1),ids]\n",
        "        # sorted_mask = torch.cat([torch.zeros((batch, 1), dtype=torch.bool), sorted_mask], dim=1) # True->mask\n",
        "        # sorted_x = torch.cat([enc_summary.unsqueeze(1), sorted_x], dim=1)\n",
        "        # sorted_sx = self.context_encoder(sorted_x, src_key_padding_mask=sorted_mask)\n",
        "        # # torch.zeros_like(sorted_sx)\n",
        "        # sx = sorted_sx[torch.arange(batch).unsqueeze(-1),ids.argsort(1)]\n",
        "        # sy_ = self.predicter(sx, src_key_padding_mask=mask)\n",
        "        # sy = self.target_encoder(x)*~mask\n",
        "\n",
        "\n",
        "        # target_mask = multiblock(batch, seq, min_s=0.15, max_s=0.2, M=4)\n",
        "        # context_mask = ~multiblock(batch, seq, min_s=0.85, max_s=1., M=1)|target_mask\n",
        "        # sx = self.context_encoder(x, src_key_padding_mask=context_mask)\n",
        "        # sy_ = self.predicter(sx, cls_mask=target_mask)*target_mask.unsqueeze(-1)\n",
        "        # sy_ = self.predicter(sx, cls_mask=target_mask)\n",
        "        # # print(sy_.shape, target_mask.shape)\n",
        "        # sy = self.target_encoder(x)*target_mask.unsqueeze(-1)\n",
        "\n",
        "\n",
        "        M=4\n",
        "        target_mask = multiblock(M*batch, seq, min_s=0.15, max_s=0.2, M=1) # mask out targets to be predicted # [4*batch, seq]\n",
        "        context_mask = ~multiblock(batch, seq, min_s=0.85, max_s=1., M=1)|target_mask.reshape(batch,M,seq).any(1) # [batch, seq]\n",
        "\n",
        "\n",
        "        x = torch.cat([enc_summary.unsqueeze(1), x], dim=1) # [batch, 1+T, d_model]\n",
        "        target_mask = torch.cat([torch.zeros((M*batch, 1), dtype=bool), target_mask],dim=1) # [4*batch, 1+T]\n",
        "        context_mask = torch.cat([torch.zeros((batch, 1), dtype=bool), context_mask],dim=1) # [batch, 1+T]\n",
        "\n",
        "        # x = x.repeat(4,1,1)\n",
        "        # context_mask = context_mask.repeat(4,1)\n",
        "        sx = self.context_encoder(x, src_key_padding_mask=context_mask).repeat(M,1,1)\n",
        "        sy_ = self.predicter(sx, cls_mask=target_mask)*target_mask.unsqueeze(-1)\n",
        "        # sy_ = self.predicter(sx, cls_mask=target_mask)\n",
        "        # print(sy_.shape, target_mask.shape)\n",
        "        sy = self.target_encoder(x).repeat(M,1,1)*target_mask.unsqueeze(-1)\n",
        "\n",
        "        loss = F.smooth_l1_loss(sy, sy_)\n",
        "        return loss\n",
        "\n",
        "    # def forward(self, hr, temp, heart): # [batch, 1+T]\n",
        "    #     # batch = hr.size(0)\n",
        "    #     # batch, seq, dim = x.shape\n",
        "    #     res_id, activeKilocalories, steps = hr[:,0], temp[:,0], heart[:,0]\n",
        "    #     enc_summary = self.enc0(torch.cat([self.calorie_emb(activeKilocalories), self.steps_emb(steps)], dim=-1)) # [batch, d_model]\n",
        "    #     x = self.enc1(torch.cat([self.hour_emb(hr[:,1:]/24), self.temp_emb(temp[:,1:]), self.heart_emb(heart[:,1:])], dim=-1)) # [batch, T, d_model]\n",
        "\n",
        "    #     x = torch.cat([enc_summary.unsqueeze(1), x], dim=1)\n",
        "    #     # sx = self.context_encoder(x)\n",
        "    #     sx = self.target_encoder(x)\n",
        "    #     out = sx.mean(dim=1)\n",
        "    #     return out\n",
        "\n",
        "\n",
        "\n",
        "seq_jepa = SeqJEPA(in_dim=16, d_model=32, out_dim=16, num_layers=1).to(device)#.to(torch.float)\n",
        "soptim = torch.optim.AdamW(seq_jepa.parameters(), lr=1e-3)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "O-OU1MaKF7BZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ba31127-d6e5-438a-aede-5c789557ab39",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title SeqJEPA\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "def multiblock(seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "    return target_mask\n",
        "\n",
        "\n",
        "class SeqJEPA(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, num_layers=1, num_classes=10):\n",
        "        super().__init__()\n",
        "        if out_dim is None: out_dim = d_model\n",
        "        act = nn.GELU() # ReLU GELU SiLU\n",
        "        # self.rot_emb = RotEmb(d_model, top=1, base=10)\n",
        "        # self.rot_emb = RotEmb(d_model, top=torch.pi, base=10)\n",
        "        # self.rot_emb = RoPE(d_model, seq_len=1024, base=10)\n",
        "        # self.rot_emb = LearnedRotEmb(dim)\n",
        "        self.encode = nn.Sequential(\n",
        "            nn.Linear(in_dim, d_model), #act,\n",
        "            # nn.Linear(d_model, d_model), act,\n",
        "        )\n",
        "        # self.encode = nn.Sequential(nn.Conv1d(in_dim, d_model,7,2,7//2), nn.MaxPool1d(3, 2, 3//2))\n",
        "        self.context_encoder = TransformerModel(d_model, d_model, out_dim=out_dim, nhead=4, nlayers=2, dropout=0.)\n",
        "        self.predicter = TransformerModel(out_dim, d_model//2, out_dim, nhead=4, nlayers=1, dropout=0.)\n",
        "        self.target_encoder = AveragedModel(self.context_encoder, multi_avg_fn=get_ema_multi_avg_fn(0.85)) # 0.95 0.999\n",
        "        self.target_encoder.requires_grad_(False)\n",
        "        # self.classifier = nn.Linear(out_dim, num_classes)\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]\n",
        "        x = self.encode(x) # [batch, T, d_model]\n",
        "        # x = self.encode(x.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "\n",
        "        batch, seq, dim = x.shape\n",
        "        M=1 # 4\n",
        "        # target_mask = multiblock(seq, min_s=0.15, max_s=0.2, M=M) # mask out targets to be predicted # [M, seq]\n",
        "        target_mask = multiblock(seq, min_s=0.15, max_s=0.2, M=4)#.any(0) # mask out targets to be predicted # [M, seq]\n",
        "        context_mask = ~multiblock(seq, min_s=0.85, max_s=1., M=1)|target_mask.reshape(M,seq).any(0) # [1, seq], True->Mask\n",
        "        target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "        # print(target_mask, context_mask)\n",
        "        sorted_mask, ids = context_mask.int().sort(dim=1, stable=True)\n",
        "        context_indices = ids[~sorted_mask.bool()] # int idx [num_context_toks] , idx of context not masked\n",
        "        sorted_x = x[torch.arange(batch).unsqueeze(-1), context_indices] # [batch, num_context_toks, d_model]\n",
        "        # print(sorted_x.shape, context_indices.shape)[2, 9, 32]) torch.Size([9])\n",
        "        # print(sorted_x[0])\n",
        "        # print(sorted_x.is_leaf)\n",
        "        sorted_sx = self.context_encoder(sorted_x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "        # del sorted_x\n",
        "        # print(sorted_sx[0])\n",
        "\n",
        "        sorted_mask, ids = target_mask.int().sort(dim=1, stable=True)\n",
        "        # print(sorted_mask.shape, ids.shape, ids[~sorted_mask.bool()].shape, sorted_mask.sum(-1))\n",
        "        trg_indices = ids[sorted_mask.bool()].reshape(M,-1) # int idx [M, num_trg_toks] , idx of targets that are masked\n",
        "        # sorted_x = x[torch.arange(batch)[...,None,None], indices] # [batch, M, seq-num_trg_toks, dim]\n",
        "        # sx = sorted_sx[torch.arange(batch).unsqueeze(-1),ids.argsort(1)]\n",
        "        # print(sorted_sx.shape, trg_indices.shape)\n",
        "        sy_ = self.predicter(sorted_sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "        # sx = self.context_encoder(x, src_key_padding_mask=context_mask).repeat(M,1,1)\n",
        "        # # sy_ = self.predicter(sx, cls_mask=target_mask)*target_mask.unsqueeze(-1)\n",
        "        # sy_ = self.predicter(sx, trg_mask=target_mask)*target_mask.unsqueeze(-1)\n",
        "\n",
        "        # print(sy_.shape, target_mask.shape)\n",
        "        # print(self.target_encoder(x).repeat_interleave(M, dim=0).shape, trg_indices.repeat(batch,1).shape)\n",
        "        with torch.no_grad():\n",
        "            sy = self.target_encoder(x).repeat_interleave(M, dim=0)[torch.arange(batch*M).unsqueeze(-1), trg_indices.repeat(batch,1)] # [batch*M, num_trg_toks, out_dim]\n",
        "\n",
        "        # print(sy_[0])\n",
        "        # print(sy.shape, sy_.shape)\n",
        "        # loss = F.smooth_l1_loss(sy, sy_)\n",
        "        loss = F.mse_loss(sy, sy_)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]\n",
        "        batch, seq, dim = x.shape\n",
        "        target_mask = randpatch(seq//self.patch_size, mask_size=16, gamma=.75) # [seq]\n",
        "        context_mask = ~multiblock(seq//self.patch_size, min_s=0.85, max_s=1., M=1)|target_mask # [1, seq], True->Mask\n",
        "        target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "        # print(target_mask, context_mask)\n",
        "        sorted_mask, ids = context_mask.int().sort(dim=1, stable=True)\n",
        "        context_indices = ids[~sorted_mask.bool()]#.unsqueeze(0) # int idx [num_context_toks] , idx of context not masked\n",
        "        sorted_x = x[torch.arange(batch).unsqueeze(-1), context_indices] # [batch, num_context_toks, 3]\n",
        "        print(sorted_x.shape, context_indices.shape)\n",
        "        sorted_sx = self.context_encoder(sorted_x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "\n",
        "        sorted_mask, ids = target_mask.int().sort(dim=-1, stable=True)\n",
        "        trg_indices = ids[sorted_mask.bool()].unsqueeze(0) # int idx [1, num_trg_toks] , idx of targets that are masked\n",
        "        print(sorted_sx.shape, context_indices.shape)\n",
        "        sy_ = self.predicter(sorted_sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "\n",
        "        with torch.no_grad(): sy = self.target_encoder(x.detach())[torch.arange(batch).unsqueeze(-1), trg_indices.repeat(batch,1)] # [batch, num_trg_toks, out_dim]\n",
        "        # print(x[0][0][:8])\n",
        "        # print(sy_[0][0][:8])\n",
        "        # print(sy.shape, sy_.shape)\n",
        "        # loss = F.smooth_l1_loss(sy, sy_)\n",
        "        loss = F.mse_loss(sy.detach(), sy_)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        x = self.encode(x)\n",
        "        # x = self.encode(x.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "        sx = self.target_encoder(x)\n",
        "        out = sx.mean(dim=1)\n",
        "        return out\n",
        "\n",
        "\n",
        "# min_s=0.15, max_s, M\n",
        "# trg.15.2M4 C.85 1\n",
        "\n",
        "seq_jepa = SeqJEPA(in_dim=3, d_model=32, out_dim=16, num_layers=1).to(device)#.to(torch.float)\n",
        "optim = torch.optim.AdamW(seq_jepa.parameters(), lr=1e-4) # 1e-3?\n",
        "print(sum(p.numel() for p in seq_jepa.parameters() if p.requires_grad)) # 59850\n",
        "\n",
        "\n",
        "# x = torch.rand((2,20,3), device=device)\n",
        "# out = seq_jepa.loss(x)\n",
        "# print(out.shape)\n",
        "# print(x.is_leaf) # T\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ijepa vision_transformer.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/models/vision_transformer.py\n",
        "import math\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from src.utils.tensors import (trunc_normal_, repeat_interleave_batch)\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid height and width\n",
        "    return:\n",
        "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid_h = np.arange(grid_size, dtype=float)\n",
        "    grid_w = np.arange(grid_size, dtype=float)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    assert embed_dim % 2 == 0\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid length\n",
        "    return:\n",
        "    pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid = np.arange(grid_size, dtype=float)\n",
        "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=float)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega   # (D/2,)\n",
        "    pos = pos.reshape(-1)   # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
        "    emb = np.concatenate([np.sin(out), np.cos(out)], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x, attn\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x, return_attention=False):\n",
        "        y, attn = self.attn(self.norm1(x))\n",
        "        if return_attention:\n",
        "            return attn\n",
        "        x = x + self.drop_path(y)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvEmbed(nn.Module):\n",
        "    \"\"\"3x3 Convolution stems for ViT following ViTC models\"\"\"\n",
        "    def __init__(self, channels, strides, img_size=224, in_chans=3, batch_norm=True):\n",
        "        super().__init__()\n",
        "        # Build the stems\n",
        "        stem = []\n",
        "        channels = [in_chans] + channels\n",
        "        for i in range(len(channels) - 2):\n",
        "            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3, stride=strides[i], padding=1, bias=(not batch_norm))]\n",
        "            if batch_norm:\n",
        "                stem += [nn.BatchNorm2d(channels[i+1])]\n",
        "            stem += [nn.ReLU(inplace=True)]\n",
        "        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n",
        "        self.stem = nn.Sequential(*stem)\n",
        "\n",
        "        # Comptute the number of patches\n",
        "        stride_prod = int(np.prod(strides))\n",
        "        self.num_patches = (img_size[0] // stride_prod)**2\n",
        "\n",
        "    def forward(self, x):\n",
        "        p = self.stem(x)\n",
        "        return p.flatten(2).transpose(1, 2)\n",
        "\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "def apply_masks(x, masks):\n",
        "    \"\"\":param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
        "    :param masks: list of tensors containing indices of patches in [N] to keep\"\"\"\n",
        "    all_x = []\n",
        "    for m in masks:\n",
        "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [batch,T,dim]\n",
        "        all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "    return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "\n",
        "class VisionTransformerPredictor(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, num_patches, embed_dim=768, predictor_embed_dim=384,\n",
        "        depth=6, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.init_std = init_std\n",
        "        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        # --\n",
        "        self.predictor_pos_embed = nn.Parameter(torch.zeros(1, num_patches, predictor_embed_dim), requires_grad=False)\n",
        "        predictor_pos_embed = get_2d_sincos_pos_embed(self.predictor_pos_embed.shape[-1], int(num_patches**.5), cls_token=False)\n",
        "        self.predictor_pos_embed.data.copy_(torch.from_numpy(predictor_pos_embed).float().unsqueeze(0)) # [1, num_patches, predictor_embed_dim]\n",
        "        # --\n",
        "        self.predictor_blocks = nn.ModuleList([Block(dim=predictor_embed_dim, num_heads=num_heads, drop=drop_rate, drop_path=dpr[i]) for i in range(depth)])\n",
        "        self.predictor_norm = nn.LayerNorm(predictor_embed_dim)\n",
        "        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n",
        "        # ------\n",
        "        # trunc_normal_(self.mask_token, std=self.init_std)\n",
        "\n",
        "    def forward(self, x, masks_x, masks):\n",
        "        assert (masks is not None) and (masks_x is not None), 'Cannot run predictor without mask indices'\n",
        "        if not isinstance(masks_x, list): masks_x = [masks_x]\n",
        "        if not isinstance(masks, list): masks = [masks]\n",
        "\n",
        "        # -- Batch Size\n",
        "        B = len(x) // len(masks_x)\n",
        "\n",
        "        # -- map from encoder-dim to pedictor-dim\n",
        "        x = self.predictor_embed(x) # [batch, num_patches, predictor_embed_dim]\n",
        "\n",
        "        # -- add positional embedding to x tokens\n",
        "        x_pos_embed = self.predictor_pos_embed.repeat(B, 1, 1) # [batch, num_patches, predictor_embed_dim]\n",
        "        x += apply_masks(x_pos_embed, masks_x)\n",
        "\n",
        "        _, N_ctxt, D = x.shape\n",
        "\n",
        "        # -- concat mask tokens to x\n",
        "        pos_embs = self.predictor_pos_embed.repeat(B, 1, 1)\n",
        "        pos_embs = apply_masks(pos_embs, masks)\n",
        "        pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x))\n",
        "        # --\n",
        "        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1)\n",
        "        # --\n",
        "        pred_tokens += pos_embs\n",
        "        x = x.repeat(len(masks), 1, 1)\n",
        "        x = torch.cat([x, pred_tokens], dim=1)\n",
        "\n",
        "        # -- fwd prop\n",
        "        for blk in self.predictor_blocks:\n",
        "            x = blk(x)\n",
        "        x = self.predictor_norm(x)\n",
        "\n",
        "        # -- return preds for mask tokens\n",
        "        x = x[:, N_ctxt:]\n",
        "        x = self.predictor_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, img_size=[224], patch_size=16, in_chans=3, embed_dim=768, predictor_embed_dim=384, depth=12, predictor_depth=12,\n",
        "        num_heads=12, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0, norm_layer=nn.LayerNorm, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_features = self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.init_std = init_std\n",
        "        # --\n",
        "        self.patch_embed = PatchEmbed(img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        # --\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=False)\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "        # --\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "    def forward(self, x, masks=None):\n",
        "        if masks is not None:\n",
        "            if not isinstance(masks, list): masks = [masks]\n",
        "\n",
        "        x = self.patch_embed(x)\n",
        "        B, N, D = x.shape # [batch, T, d_model]\n",
        "\n",
        "        # -- add positional embedding to x\n",
        "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
        "        x = x + pos_embed\n",
        "\n",
        "        if masks is not None: x = apply_masks(x, masks)\n",
        "\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            x = blk(x)\n",
        "\n",
        "        if self.norm is not None: x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "    def interpolate_pos_encoding(self, x, pos_embed):\n",
        "        npatch = x.shape[1] - 1\n",
        "        N = pos_embed.shape[1] - 1\n",
        "        if npatch == N: return pos_embed\n",
        "        class_emb, pos_embed = pos_embed[:, 0], pos_embed[:, 1:]\n",
        "        dim = x.shape[-1]\n",
        "        pos_embed = nn.functional.interpolate(pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2), scale_factor=math.sqrt(npatch / N), mode='bicubic',)\n",
        "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
        "\n",
        "# from functools import partial\n",
        "# def vit_predictor(**kwargs):\n",
        "#     model = VisionTransformerPredictor(\n",
        "#         mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "#     return model\n",
        "\n",
        "def vit(patch_size=16, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, # tiny\n",
        "        # patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, # small\n",
        "        # patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, # base\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "bNjp4xFsGPoa",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title show collated_masks\n",
        "# print(collated_batch, collated_masks_enc, collated_masks_pred)\n",
        "# print(collated_batch) # tensor([0, 1, 2, 3]) batch\n",
        "# print(collated_masks_enc) # nenc*[M*[pred mask blocks patch ind]]\n",
        "# print(collated_masks_pred) # npred * [M * [context]]\n",
        "\n",
        "\n",
        "# print(len(collated_masks_enc[0]), len(collated_masks_pred[0]))\n",
        "# print(collated_masks_enc[0], collated_masks_pred[0])\n",
        "\n",
        "h,w=14,14\n",
        "img = torch.ones(h*w)\n",
        "# img[collated_masks_enc[0]]=0\n",
        "img[collated_masks_pred[0][2]]=0\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    # plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.imshow(npimg)\n",
        "    plt.show()\n",
        "\n",
        "# imshow(img.reshape(h,w))\n",
        "\n",
        "# for masks in collated_masks_pred:\n",
        "#     for mask in masks:\n",
        "#         img = torch.ones(h*w)\n",
        "#         img[mask]=0\n",
        "#         imshow(img.reshape(h,w))\n",
        "\n",
        "\n",
        "for masks in collated_masks_enc:\n",
        "    for mask in masks:\n",
        "        img = torch.ones(h*w)\n",
        "        img[mask]=0\n",
        "        imshow(img.reshape(h,w))\n"
      ],
      "metadata": {
        "id": "tf4T37woBV2V",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aH1UOD4WW8I2",
        "outputId": "6fafea1b-0af2-4bc7-fa26-46b5e56549b4",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vit fwd torch.Size([4, 3, 224, 224])\n",
            "vit fwd torch.Size([4, 196, 768])\n",
            "vit fwd torch.Size([4, 11, 768])\n",
            "predictor fwd1 torch.Size([4, 11, 384]) torch.Size([4, 196, 384])\n"
          ]
        }
      ],
      "source": [
        "# @title ijepa vision_transformer.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/models/vision_transformer.py\n",
        "import math\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# from src.utils.tensors import (trunc_normal_, repeat_interleave_batch)\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid height and width\n",
        "    return:\n",
        "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid_h = np.arange(grid_size, dtype=float)\n",
        "    grid_w = np.arange(grid_size, dtype=float)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    assert embed_dim % 2 == 0\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid length\n",
        "    return:\n",
        "    pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid = np.arange(grid_size, dtype=float)\n",
        "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=float)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega   # (D/2,)\n",
        "    pos = pos.reshape(-1)   # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
        "    emb = np.concatenate([np.sin(out), np.cos(out)], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "def repeat_interleave_batch(x, B, repeat):\n",
        "    N = len(x) // B\n",
        "    x = torch.cat([\n",
        "        torch.cat([x[i*B:(i+1)*B] for _ in range(repeat)], dim=0)\n",
        "        for i in range(N)\n",
        "    ], dim=0)\n",
        "    return x\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x, attn\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x, return_attention=False):\n",
        "        y, attn = self.attn(self.norm1(x))\n",
        "        if return_attention:\n",
        "            return attn\n",
        "        x = x + self.drop_path(y)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvEmbed(nn.Module):\n",
        "    \"\"\"3x3 Convolution stems for ViT following ViTC models\"\"\"\n",
        "    def __init__(self, channels, strides, img_size=224, in_chans=3, batch_norm=True):\n",
        "        super().__init__()\n",
        "        # Build the stems\n",
        "        stem = []\n",
        "        channels = [in_chans] + channels\n",
        "        for i in range(len(channels) - 2):\n",
        "            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3, stride=strides[i], padding=1, bias=(not batch_norm))]\n",
        "            if batch_norm:\n",
        "                stem += [nn.BatchNorm2d(channels[i+1])]\n",
        "            stem += [nn.ReLU(inplace=True)]\n",
        "        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n",
        "        self.stem = nn.Sequential(*stem)\n",
        "\n",
        "        # Comptute the number of patches\n",
        "        stride_prod = int(np.prod(strides))\n",
        "        self.num_patches = (img_size[0] // stride_prod)**2\n",
        "\n",
        "    def forward(self, x):\n",
        "        p = self.stem(x)\n",
        "        return p.flatten(2).transpose(1, 2)\n",
        "\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "def apply_masks(x, masks):\n",
        "    \"\"\":param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
        "    :param masks: list of tensors containing indices of patches in [N] to keep\"\"\"\n",
        "    all_x = []\n",
        "    for m in masks:\n",
        "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [batch,T,dim]\n",
        "        all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "    return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "\n",
        "class VisionTransformerPredictor(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, num_patches, embed_dim=768, predictor_embed_dim=384,\n",
        "        depth=6, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.init_std = init_std\n",
        "        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        # --\n",
        "        self.predictor_pos_embed = nn.Parameter(torch.zeros(1, num_patches, predictor_embed_dim), requires_grad=False)\n",
        "        predictor_pos_embed = get_2d_sincos_pos_embed(self.predictor_pos_embed.shape[-1], int(num_patches**.5), cls_token=False)\n",
        "        self.predictor_pos_embed.data.copy_(torch.from_numpy(predictor_pos_embed).float().unsqueeze(0)) # [1, num_patches, predictor_embed_dim]\n",
        "        # --\n",
        "        self.predictor_blocks = nn.ModuleList([Block(dim=predictor_embed_dim, num_heads=num_heads, drop=drop_rate, drop_path=dpr[i]) for i in range(depth)])\n",
        "        self.predictor_norm = nn.LayerNorm(predictor_embed_dim)\n",
        "        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n",
        "        # ------\n",
        "        # trunc_normal_(self.mask_token, std=self.init_std)\n",
        "\n",
        "    def forward(self, x, masks_x, masks): # [batch, num_context_patches, embed_dim], nenc*[batch, num_context_patches], npred*[batch, num_target_patches]\n",
        "        assert (masks is not None) and (masks_x is not None), 'Cannot run predictor without mask indices'\n",
        "        if not isinstance(masks_x, list): masks_x = [masks_x] # context mask\n",
        "        if not isinstance(masks, list): masks = [masks] # pred mask\n",
        "\n",
        "        # -- Batch Size\n",
        "        B = len(x) // len(masks_x)\n",
        "        # print(\"predictor fwd0\", x.shape) # [3, 121, 768])\n",
        "        # -- map from encoder-dim to pedictor-dim\n",
        "        x = self.predictor_embed(x) # [batch, num_patches, predictor_embed_dim]\n",
        "\n",
        "        # -- add positional embedding to x tokens\n",
        "        x_pos_embed = self.predictor_pos_embed.repeat(B, 1, 1) # [batch, num_patches, predictor_embed_dim]\n",
        "        print(\"predictor fwd1\", x.shape, x_pos_embed.shape) # [3, 121 or 11, 384], [3, 196, 384]\n",
        "        # print(\"predictor fwd masks_x\", masks_x)\n",
        "        x += apply_masks(x_pos_embed, masks_x) # get pos emb of context patches\n",
        "\n",
        "        # print(\"predictor fwd x\", x.shape) # [4, 11, 384])\n",
        "        _, N_ctxt, D = x.shape\n",
        "\n",
        "        # -- concat mask tokens to x\n",
        "        pos_embs = self.predictor_pos_embed.repeat(B, 1, 1) # [batch, num_patches, predictor_embed_dim]\n",
        "        pos_embs = apply_masks(pos_embs, masks) # [16, 104, predictor_embed_dim]\n",
        "        pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x)) # [16, 104, predictor_embed_dim]\n",
        "        # --\n",
        "        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1) # [16, 104, predictor_embed_dim]\n",
        "        # --\n",
        "        # print(\"predictor fwd pred_tokens\", pred_tokens.shape)\n",
        "\n",
        "        pred_tokens += pos_embs\n",
        "        x = x.repeat(len(masks), 1, 1)\n",
        "        x = torch.cat([x, pred_tokens], dim=1)\n",
        "\n",
        "        # -- fwd prop\n",
        "        for blk in self.predictor_blocks:\n",
        "            x = blk(x)\n",
        "        x = self.predictor_norm(x)\n",
        "\n",
        "        # -- return preds for mask tokens\n",
        "        x = x[:, N_ctxt:]\n",
        "        x = self.predictor_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, img_size=[224], patch_size=16, in_chans=3, embed_dim=768, predictor_embed_dim=384, depth=12, predictor_depth=12,\n",
        "        num_heads=12, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0, norm_layer=nn.LayerNorm, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_features = self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.init_std = init_std\n",
        "        # --\n",
        "        self.patch_embed = PatchEmbed(img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        # --\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=False)\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "        # --\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "    def forward(self, x, masks=None): # [batch,3,224,224]\n",
        "        if masks is not None:\n",
        "            if not isinstance(masks, list): masks = [masks]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        x = self.patch_embed(x)\n",
        "        B, N, D = x.shape # [batch, num_patches, embed_dim]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        # -- add positional embedding to x\n",
        "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
        "        x = x + pos_embed\n",
        "\n",
        "        if masks is not None: x = apply_masks(x, masks) # [batch, num_context_patches, embed_dim]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            x = blk(x)\n",
        "\n",
        "        if self.norm is not None: x = self.norm(x)\n",
        "        return x # [batch, num_context_patches, embed_dim]\n",
        "\n",
        "    def interpolate_pos_encoding(self, x, pos_embed):\n",
        "        npatch = x.shape[1] - 1\n",
        "        N = pos_embed.shape[1] - 1\n",
        "        if npatch == N: return pos_embed\n",
        "        class_emb, pos_embed = pos_embed[:, 0], pos_embed[:, 1:]\n",
        "        dim = x.shape[-1]\n",
        "        pos_embed = nn.functional.interpolate(pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2), scale_factor=math.sqrt(npatch / N), mode='bicubic',)\n",
        "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
        "\n",
        "# from functools import partial\n",
        "# def vit_predictor(**kwargs):\n",
        "#     model = VisionTransformerPredictor(\n",
        "#         mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "#     return model\n",
        "\n",
        "def vit(patch_size=16, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, # tiny\n",
        "        # patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, # small\n",
        "        # patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, # base\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "# encoder = vit()\n",
        "encoder = VisionTransformer(\n",
        "        patch_size=16, embed_dim=768, depth=1, num_heads=3, mlp_ratio=1,\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
        "\n",
        "# num_patches = (224/patch_size)^2\n",
        "# predictor = VisionTransformerPredictor(num_patches, embed_dim=768, predictor_embed_dim=384, depth=6, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02, **kwargs)\n",
        "predictor = VisionTransformerPredictor(num_patches=196, embed_dim=768, predictor_embed_dim=384, depth=1, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02)\n",
        "\n",
        "batch = 4\n",
        "img = torch.rand(batch, 3, 224, 224)\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/train.py\n",
        "mask_collator = MaskCollator(\n",
        "        input_size=(224, 224),\n",
        "        patch_size=16,\n",
        "        enc_mask_scale=(0.2, 0.8),\n",
        "        pred_mask_scale=(0.2, 0.8),\n",
        "        aspect_ratio=(0.3, 3.0),\n",
        "        nenc=1,\n",
        "        npred=4,\n",
        "        min_keep=4,\n",
        "        # allow_overlap=True)\n",
        "        allow_overlap=False)\n",
        "# self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
        "\n",
        "\n",
        "\n",
        "# v = mask_collator.step()\n",
        "# should pass the collater a list batch of idx?\n",
        "# collated_batch, collated_masks_enc, collated_masks_pred = mask_collator([batch,3,8])\n",
        "collated_batch, collated_masks_enc, collated_masks_pred = mask_collator([0,1,2,3])\n",
        "# print(v)\n",
        "\n",
        "\n",
        "import copy\n",
        "target_encoder = copy.deepcopy(encoder)\n",
        "\n",
        "\n",
        "def forward_target():\n",
        "    with torch.no_grad():\n",
        "        h = target_encoder(imgs)\n",
        "        h = F.layer_norm(h, (h.size(-1),))  # normalize over feature-dim\n",
        "        B = len(h)\n",
        "        # -- create targets (masked regions of h)\n",
        "        h = apply_masks(h, masks_pred)\n",
        "        h = repeat_interleave_batch(h, B, repeat=len(masks_enc))\n",
        "        return h\n",
        "\n",
        "def forward_context():\n",
        "    z = encoder(imgs, masks_enc)\n",
        "    z = predictor(z, masks_enc, masks_pred)\n",
        "    return z\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    m = next(momentum_scheduler)\n",
        "                    for param_q, param_k in zip(encoder.parameters(), target_encoder.parameters()):\n",
        "                        param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "\n",
        "\n",
        "# # num_context_patches = 121 if allow_overlap=True else 11\n",
        "z = encoder(img, collated_masks_enc) # [batch, num_context_patches, embed_dim]\n",
        "# print(len(collated_masks_enc), len(collated_masks_pred)) # nenc, npred\n",
        "# print(collated_masks_enc[0].shape, collated_masks_pred[0].shape) # , [batch, num_context_patches = 121 or 11], [batch, num_target_patches]\n",
        "out = predictor(z, collated_masks_enc, collated_masks_pred)\n",
        "# # print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test multiblock params\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# dataiter = iter(train_data)\n",
        "img,y = next(dataiter)\n",
        "img = img.unsqueeze(0)\n",
        "b,c,h,w = img.shape\n",
        "img = img.flatten(2).transpose(-2,-1).to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "print(img.shape)\n",
        "batch, seq,_ = img.shape\n",
        "M=4\n",
        "target_mask = multiblock(seq, min_s=0.15, max_s=0.2, M=M) # mask out targets to be predicted # [M, seq]\n",
        "context_mask = ~multiblock(seq, min_s=0.85, max_s=1., M=1)|target_mask.reshape(M,seq).any(0) # [1, seq], True->Mask\n",
        "target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "# print(target_mask, context_mask)\n",
        "\n",
        "\n",
        "# print(target_mask.shape, context_mask.shape)\n",
        "# target_img, context_img = img*target_mask.unsqueeze(-1), img*context_mask.unsqueeze(-1)\n",
        "target_img, context_img = img*~target_mask.unsqueeze(-1), img*~context_mask.unsqueeze(-1)\n",
        "# print(target_img.shape, context_img.shape)\n",
        "target_img, context_img = target_img.transpose(-2,-1).reshape(M,c,h,w), context_img.transpose(-2,-1).reshape(b,c,h,w)\n",
        "target_img, context_img = target_img.float(), context_img.float()\n",
        "\n",
        "# imshow(out.detach().cpu())\n",
        "imshow(target_img[0])\n",
        "imshow(context_img[0])\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "D6lVtbS5OHIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test multiblock mask\n",
        "def multiblock(seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    # mask_len, mask_pos = mask_len * seq, mask_pos * seq\n",
        "    indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "    return target_mask\n",
        "\n",
        "\n",
        "seq=400\n",
        "M=4\n",
        "\n",
        "# target_mask = multiblock(M, seq, min_s=0.15, max_s=0.2, M=1) # mask out targets to be predicted # [4*batch, seq]\n",
        "# context_mask = ~multiblock(1, seq, min_s=0.85, max_s=1., M=1)|target_mask.reshape(M,seq).any(0) # [batch, seq]\n",
        "\n",
        "\n",
        "target_mask = multiblock(seq, min_s=0.15, max_s=0.2, M=4) # mask out targets to be predicted # [4*batch, seq]\n",
        "context_mask = ~multiblock(seq, min_s=0.85, max_s=1., M=1)|target_mask.reshape(M,seq).any(0) # [batch, seq]\n",
        "\n",
        "# target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "\n",
        "# print(target_mask)\n",
        "# print(context_mask)\n",
        "\n",
        "print(target_mask.sum(-1))\n",
        "print(context_mask.sum(-1))\n",
        "\n",
        "\n",
        "# sorted_mask, ids = context_mask.sort(dim=1, stable=True)\n",
        "# indices = ids[~sorted_mask]#.reshape(M,-1) # int idx [num_context_toks] , idx of context not masked\n",
        "# sorted_x = x[torch.arange(batch).unsqueeze(-1), indices] # [batch, seq-num_trg_toks, dim]\n",
        "# print(indices)\n",
        "# print(x)\n",
        "# print(sorted_x)\n",
        "\n",
        "\n",
        "\n",
        "# sorted_mask, ids = target_mask.sort(dim=1, descending=False, stable=True)\n",
        "# # print(sorted_mask, ids)\n",
        "# indices = ids[~sorted_mask].reshape(M,-1) # int idx [M, seq-num_trg_toks] , idx of target not masked\n",
        "# print(indices)\n",
        "# batch=3\n",
        "# dim=2\n",
        "# # indices = indices.expand(batch,-1,-1)\n",
        "# # x = torch.arange(batch*seq).reshape(batch,seq).to(device)\n",
        "# x = torch.rand(batch, seq, dim)\n",
        "# print(x)\n",
        "# sorted_x = x[torch.arange(batch)[...,None,None], indices]\n",
        "# print(sorted_x.shape) # [batch, M, seq-num_trg_toks, dim]\n",
        "# print(sorted_x)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "p8xv1tpKIhim",
        "outputId": "e6613fee-2d32-4c6a-ef96-dee18da3aa6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([73, 73, 73, 73])\n",
            "tensor([248])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ijepa multiblock down\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "import math\n",
        "from multiprocessing import Value\n",
        "import torch\n",
        "_GLOBAL_SEED = 0\n",
        "\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=(224, 224),\n",
        "        patch_size=16,\n",
        "        enc_mask_scale=(0.2, 0.8),\n",
        "        pred_mask_scale=(0.2, 0.8),\n",
        "        aspect_ratio=(0.3, 3.0),\n",
        "        nenc=1,\n",
        "        npred=2,\n",
        "        min_keep=4,\n",
        "        allow_overlap=False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if not isinstance(input_size, tuple):\n",
        "            input_size = (input_size, ) * 2\n",
        "        self.patch_size = patch_size\n",
        "        self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.aspect_ratio = aspect_ratio\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "        self._itr_counter = Value('i', -1)  # collator is shared across worker processes\n",
        "\n",
        "    def step(self):\n",
        "        i = self._itr_counter\n",
        "        with i.get_lock():\n",
        "            i.value += 1\n",
        "            v = i.value\n",
        "        return v\n",
        "\n",
        "    def _sample_block_size(self, generator, scale, aspect_ratio_scale):\n",
        "        _rand = torch.rand(1, generator=generator).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.height * self.width * mask_scale)\n",
        "        # -- Sample block aspect-ratio\n",
        "        min_ar, max_ar = aspect_ratio_scale\n",
        "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
        "        while h >= self.height:\n",
        "            h -= 1\n",
        "        while w >= self.width:\n",
        "            w -= 1\n",
        "\n",
        "        return (h, w)\n",
        "\n",
        "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
        "        h, w = b_size\n",
        "\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # --\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            top = torch.randint(0, self.height - h, (1,))\n",
        "            left = torch.randint(0, self.width - w, (1,))\n",
        "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
        "            mask[top:top+h, left:left+w] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
        "        mask_complement[top:top+h, left:left+w] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        B = len(batch)\n",
        "\n",
        "        collated_batch = torch.utils.data.default_collate(batch)\n",
        "\n",
        "        seed = self.step()\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(seed)\n",
        "        p_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.pred_mask_scale,\n",
        "            aspect_ratio_scale=self.aspect_ratio)\n",
        "        e_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.enc_mask_scale,\n",
        "            aspect_ratio_scale=(1., 1.))\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.height * self.width\n",
        "        min_keep_enc = self.height * self.width\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                print(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "\n",
        "        return collated_batch, collated_masks_enc, collated_masks_pred\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "COvEnBXPYth3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}