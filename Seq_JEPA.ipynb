{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/Seq_JEPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title WISDM me\n",
        "# !wget https://www.cis.fordham.edu/wisdm/includes/datasets/latest/WISDM_ar_latest.tar.gz\n",
        "# !tar -xzf WISDM_ar_latest.tar.gz\n",
        "path = '/content/WISDM_ar_v1.1/WISDM_ar_v1.1_raw.txt'\n",
        "\n",
        "with open(path, 'r') as f:\n",
        "    processedList = f.read().replace(' ', '').replace(',\\n', ';\\n').replace(',;', ';').replace('\\n', '').replace(';','\\n')\n",
        "processedList = [p.split(',') for p in processedList.split('\\n')]\n",
        "\n",
        "# print(df.isna().sum())\n",
        "\n",
        "import pandas as pd\n",
        "columns = ['subject', 'activity','time','x','y','z']\n",
        "dataset = pd.DataFrame(data = processedList, columns = columns)\n",
        "\n",
        "y = dataset['subject'].unique()\n",
        "y.sort()\n",
        "\n",
        "df_train = dataset[dataset['subject'].isin(y[:int(.7*len(y))])]\n",
        "df_test = dataset[dataset['subject'].isin(y[-int(.3*len(y)):])]\n",
        "\n",
        "def make_Xy(dataset):\n",
        "    ans = [y for _, y in dataset.groupby(['subject', 'activity'])]\n",
        "    y_train = [df['activity'].iloc[0] for df in ans]\n",
        "    # y_train = [df['subject'].iloc[0] for df in ans]\n",
        "    X_train = [df.drop(['subject', 'activity','time'], axis=1) for df in ans]\n",
        "    X_train = [df.apply(pd.to_numeric, errors='coerce') for df in X_train] # Convert non-numeric data in dataset to numeric. errors='coerce': replace all non-numeric values with NaN.\n",
        "    X_train = [df.interpolate(method='index', axis=0, limit_direction='both') for df in X_train] # replace NaN by interpolating\n",
        "    # X_train = [df.interpolate(method='index', axis=0, limit_direction='both') for df in ans]\n",
        "    return X_train, y_train\n",
        "\n",
        "X_train, y_train = make_Xy(df_train)\n",
        "X_test, y_test = make_Xy(df_test)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "8OhbtsiD1yGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title papmap2 me\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# https://github.com/EdnaEze/Physical-Activity-Monitoring/blob/main/DSRM-Edna.ipynb\n",
        "\n",
        "activities = {0:'transient', 1:'lying', 2:'sitting', 3:'standing', 4:'walking', 5:'running', 6:'cycling', 7:'Nordic_walking', 9:'watching_TV', 10:'computer_work', 11:'car driving', 12:'ascending_stairs', 13:'descending_stairs', 16:'vacuum_cleaning', 17:'ironing', 18:'folding_laundry', 19:'house_cleaning', 20:'playing_soccer', 24:'rope_jumping'}\n",
        "all_columns = [\"time\", \"activity\", \"heartrate\", 'handTemperature', 'handAcc16_1', 'handAcc16_2', 'handAcc16_3', 'handAcc6_1', 'handAcc6_2', 'handAcc6_3', 'handGyro1', 'handGyro2', 'handGyro3', 'handMagne1', 'handMagne2', 'handMagne3', 'handOrientation1', 'handOrientation2', 'handOrientation3', 'handOrientation4', 'chestTemperature', 'chestAcc16_1', 'chestAcc16_2', 'chestAcc16_3', 'chestAcc6_1', 'chestAcc6_2', 'chestAcc6_3', 'chestGyro1', 'chestGyro2', 'chestGyro3', 'chestMagne1', 'chestMagne2', 'chestMagne3', 'chestOrientation1', 'chestOrientation2', 'chestOrientation3', 'chestOrientation4', 'ankleTemperature', 'ankleAcc16_1', 'ankleAcc16_2', 'ankleAcc16_3', 'ankleAcc6_1', 'ankleAcc6_2', 'ankleAcc6_3', 'ankleGyro1', 'ankleGyro2', 'ankleGyro3', 'ankleMagne1', 'ankleMagne2', 'ankleMagne3', 'ankleOrientation1', 'ankleOrientation2', 'ankleOrientation3', 'ankleOrientation4']\n",
        "\n",
        "dataset = pd.DataFrame()\n",
        "\n",
        "# path = '/content/OpportunityUCIDataset/dataset'\n",
        "path = '/content/PAMAP2_Dataset/Protocol/'\n",
        "\n",
        "usr_lst = os.listdir(path)\n",
        "for file in os.listdir(path):\n",
        "# for file, subject_id in zip(file_names, subject_id):\n",
        "    df = pd.read_table(path+file, header=None, sep='\\s+')\n",
        "    df.columns = all_columns\n",
        "    df['subject'] = file\n",
        "    dataset = pd.concat([dataset, df], ignore_index=True)\n",
        "\n",
        "y = dataset['subject'].unique()\n",
        "y.sort()\n",
        "\n",
        "df_train = dataset[dataset['subject'].isin(y[:int(.7*len(y))])]\n",
        "df_test = dataset[dataset['subject'].isin(y[-int(.3*len(y)):])]\n",
        "\n",
        "def make_Xy(dataset):\n",
        "    anss = [y for _, y in dataset.groupby(['subject', 'activity'])]\n",
        "    ans = []\n",
        "    for x in anss:\n",
        "        if len(x) > 1000: # only keep sequences with more than 1000 samples\n",
        "            ans.append(x)\n",
        "    y_train = [df['activity'].iloc[0] for df in ans]\n",
        "    # y_train = [df['subject'].iloc[0] for df in ans]\n",
        "    # X_train = [df.drop(['subject', 'activity','time'], axis=1) for df in X_train]\n",
        "    X_train = [df.drop(['subject', 'activity','time'], axis=1) for df in ans]\n",
        "    # X_train = [df.interpolate(method='index', axis=0, limit_direction='both') for df in ans]\n",
        "\n",
        "    X_train = [df.apply(pd.to_numeric, errors='coerce') for df in X_train] # Convert non-numeric data in dataset to numeric. errors='coerce': replace all non-numeric values with NaN.\n",
        "    X_train = [df.interpolate(method='index', axis=0, limit_direction='both') for df in X_train] # replace NaN by interpolating\n",
        "\n",
        "    # X_train = [df.interpolate(method='values', axis=0, limit_direction='both') for df in ans]\n",
        "    # data.reset_index(drop=True, inplace=True) # make row ind start from 0\n",
        "    return X_train, y_train\n",
        "\n",
        "X_train, y_train = make_Xy(df_train)\n",
        "X_test, y_test = make_Xy(df_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "y4IxoAdbL9Wy",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title pandasDataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class pandasDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X, self.y = X, y\n",
        "        chars = sorted(list(set(y)))\n",
        "        self.vocab_size = len(chars) #\n",
        "        self.stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "        self.itos = {i:ch for i,ch in enumerate(chars)}\n",
        "        self.y = self.data_process(y) #\n",
        "        self.seq_len = min([len(a) for a in X])\n",
        "        print('seq_len',self.seq_len)\n",
        "\n",
        "    def data_process(self, data): # str\n",
        "        # return torch.tensor([self.stoi.get(c) for c in data]) #\n",
        "        return np.array([self.stoi.get(c) for c in data]) #\n",
        "\n",
        "    def __len__(self): return len(self.X)\n",
        "    # def __getitem__(self, idx): return self.X.iloc[idx].to_numpy(), self.y.iloc[idx]\n",
        "    # def __getitem__(self, idx): return self.X[idx].to_numpy(), self.y[idx]\n",
        "    def __getitem__(self, idx):\n",
        "        i = np.random.randint(0, len(self.X[idx])-self.seq_len+1)\n",
        "        return self.X[idx].to_numpy()[i:i+self.seq_len].astype(float), self.y[idx]\n",
        "\n",
        "train_data = pandasDataset(X_train, y_train)\n",
        "test_data = pandasDataset(X_test, y_test)\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=2)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=2)\n",
        "\n",
        "for X, y in train_loader:\n",
        "    print(X.shape, y.shape)\n",
        "    break\n",
        "\n"
      ],
      "metadata": {
        "id": "0OVelMicgXwi",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(len(X_train)) # 49\n",
        "for df in ans:\n",
        "    print(df['subject'].iloc[0], df['activity'].iloc[0], len(df))\n",
        "# print(y) # ['11', '13', '15', '17', '18', '20', '27', '29', '32', '33', '35', '36', '6']\n",
        "\n",
        "# 1 256 2000+\n"
      ],
      "metadata": {
        "id": "BWj01BGLU_YZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title eigenworm\n",
        "!wget https://timeseriesclassification.com/aeon-toolkit/Worms.zip\n",
        "!unzip har\n",
        "!unzip 'UCI HAR Dataset.zip'\n",
        "!mv 'UCI HAR Dataset' UCI_HAR_Dataset\n",
        "\n",
        "# 131 train and 128 test. We have truncated each series to the shortest usable. Each series has 17984 observations. Each worm is classified as either wild-type (the N2 reference strain) or one of four mutant types: goa-1; unc-1; unc-38 and unc-63.\n"
      ],
      "metadata": {
        "id": "q1v4VIsCxKFj",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ECG Heartbeat Categorization Dataset\n",
        "\n",
        "# https://www.kaggle.com/datasets/shayanfazeli/heartbeat\n",
        "\n",
        "# Install dependencies as needed:\n",
        "# pip install kagglehub[pandas-datasets]\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "\n",
        "# Set the path to the file you'd like to load\n",
        "file_path = \"\"\n",
        "\n",
        "# Load the latest version\n",
        "df = kagglehub.load_dataset(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"shayanfazeli/heartbeat\",\n",
        "  file_path,\n",
        "  # Provide any additional arguments like\n",
        "  # sql_query or pandas_kwargs. See the\n",
        "  # documenation for more information:\n",
        "  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas\n",
        ")\n",
        "\n",
        "print(\"First 5 records:\", df.head())\n"
      ],
      "metadata": {
        "id": "ct1t8hPLyBwd",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Fh9o__m2-j7K"
      },
      "outputs": [],
      "source": [
        "# @title simplex\n",
        "!pip install -q opensimplex\n",
        "import opensimplex\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def simplexmask1d(seq=512, ctx_scale=(.85,1), trg_scale=(.6,.8), B=64, chaos=[1,.5]):\n",
        "    i = np.linspace(0, chaos[0], num=seq) # 2-5\n",
        "    noise = opensimplex.noise2array(i, np.random.randint(1e10, size=B)) # [B, seq]\n",
        "    noise = torch.from_numpy(noise)\n",
        "    # trunc_normal = torch.fmod(torch.randn(2)*.3,1)/2 + .5\n",
        "    # print(trunc_normal)\n",
        "    ctx_mask_scale = torch.rand(1) * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    trg_mask_scale = torch.rand(1) * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "    # ctx_mask_scale = trunc_normal[0] * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    # trg_mask_scale = trunc_normal[1] * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "\n",
        "    ctx_len, trg_len = int(seq*ctx_mask_scale), int(seq*trg_mask_scale)\n",
        "    val, trg_index = torch.topk(noise, trg_len, dim=1, sorted=False)\n",
        "    ctx_len = ctx_len - trg_len\n",
        "\n",
        "    remove_mask = torch.ones((B,seq), dtype=bool) # [B, S]\n",
        "    remove_mask.scatter_(1, trg_index, False).flatten()\n",
        "    ind = torch.arange(seq).unsqueeze(0).repeat(B,1)[remove_mask].reshape(B, -1)\n",
        "\n",
        "    i = np.linspace(0, chaos[1], num=seq) # 2-5\n",
        "    noise = opensimplex.noise2array(i, np.random.randint(1e10, size=B)) # [B, seq]\n",
        "    noise = torch.from_numpy(noise)[remove_mask].reshape(B, -1)\n",
        "    val, ctx_ind = torch.topk(noise, ctx_len, dim=1, sorted=False)\n",
        "    ctx_index = ind[torch.arange(B).unsqueeze(-1), ctx_ind]\n",
        "    return ctx_index, trg_index\n",
        "\n",
        "b=64\n",
        "# ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.7,.8), trg_scale=(.4,.6), B=b, chaos=[3,.5])\n",
        "ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.85,1), trg_scale=(.7,.8), B=b, chaos=[3,.5])\n",
        "# ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.8,.9), trg_scale=(.7,.8), B=b, chaos=[1,.5])\n",
        "mask = torch.zeros(b ,200)\n",
        "mask[torch.arange(b).unsqueeze(-1), trg_index] = 1\n",
        "mask[torch.arange(b).unsqueeze(-1), ctx_index] = .5\n",
        "# mask = mask[None,...]\n",
        "# mask = mask[:,None,None,:]#.repeat(1,3,1,1)\n",
        "print(mask.shape)\n",
        "\n",
        "# def imshow(img):\n",
        "#     npimg = img.numpy()\n",
        "#     plt.rcParams[\"figure.figsize\"] = (8,4)\n",
        "#     # plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "#     plt.pcolormesh(np.transpose(npimg, (1, 2, 0)))\n",
        "#     plt.show()\n",
        "# # imshow(mask[0])\n",
        "# import torchvision\n",
        "# imshow(torchvision.utils.make_grid(mask, nrow=1))\n",
        "\n",
        "# print(index)\n",
        "# print(index.shape)\n",
        "# print(mask)\n",
        "# print(mask.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Xn7WZShwWxF8"
      },
      "outputs": [],
      "source": [
        "# @title ijepa multiblock 1d\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "import torch\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(self, length=200,\n",
        "        enc_mask_scale=(0.2, 0.8), pred_mask_scale=(0.2, 0.8),\n",
        "        nenc=1, npred=2, min_keep=4, allow_overlap=False):\n",
        "        super().__init__()\n",
        "        self.length = length\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "\n",
        "    def _sample_block_size(self, scale):\n",
        "        _rand = torch.rand(1).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.length * mask_scale) # num patches to keep\n",
        "        # -- Sample block aspect-ratio\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        l = max_keep#int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        while l >= self.length: l -= 1 # crop mask to be smaller than img\n",
        "        return l\n",
        "\n",
        "    def _sample_block_mask(self, length, acceptable_regions=None):\n",
        "        l = length\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # --\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            left = torch.randint(0, self.length - l, (1,))\n",
        "            mask = torch.zeros(self.length, dtype=torch.int32)\n",
        "            mask[left:left+l] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones(self.length, dtype=torch.int32)\n",
        "        mask_complement[left:left+l] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, B):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        p_size = self._sample_block_size(scale=self.pred_mask_scale)\n",
        "        e_size = self._sample_block_size(scale=self.enc_mask_scale)\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.length\n",
        "        min_keep_enc = self.length\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                print(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "        return collated_masks_enc, collated_masks_pred\n",
        "\n",
        "batch=64\n",
        "length=200\n",
        "mask_collator = MaskCollator(length=length, enc_mask_scale=(.85, 1.), pred_mask_scale=(.15, .2),\n",
        "        nenc=1, npred=4, min_keep=4,\n",
        "        # allow_overlap=True)\n",
        "        allow_overlap=False)\n",
        "\n",
        "collated_masks_enc, collated_masks_pred = mask_collator(batch)\n",
        "context_indices, trg_indices = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "# print(context_indices.shape, trg_indices.shape)\n",
        "\n",
        "\n",
        "# plt.pcolormesh(mask)\n",
        "b=64\n",
        "mask = torch.zeros(batch ,length)\n",
        "mask[torch.arange(batch).unsqueeze(-1), trg_indices] = 1\n",
        "mask[torch.arange(batch).unsqueeze(-1), context_indices] = .5\n",
        "# mask = mask[None,...]\n",
        "# print(mask.shape)\n",
        "# mask = mask[:,None,None,:]#.repeat(1,3,1,1)\n",
        "# print(mask.shape)\n",
        "\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# def imshow(img):\n",
        "#     npimg = img.numpy()\n",
        "#     plt.rcParams[\"figure.figsize\"] = (8,4)\n",
        "#     # plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "#     plt.pcolormesh(np.transpose(npimg, (1, 2, 0)))\n",
        "#     # plt.imshow(npimg)\n",
        "#     plt.show()\n",
        "# # imshow(mask)\n",
        "# import torchvision\n",
        "# print(torchvision.utils.make_grid(mask, nrow=1).shape)\n",
        "# # imshow(torchvision.utils.make_grid(mask, nrow=8))\n",
        "# imshow(torchvision.utils.make_grid(mask, nrow=1))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ge36SCxOl2Oq"
      },
      "outputs": [],
      "source": [
        "# @title RoPE\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def RoPE(dim, seq_len=512, base=10000):\n",
        "    theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "    pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "    angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "    rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [1, seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "    return rot_emb\n",
        "\n",
        "# class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "#     def __init__(self, dim, seq_len=512, base=10000):\n",
        "#         super().__init__()\n",
        "#         self.dim, self.base = dim, base\n",
        "#         theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "#         pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "#         angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "#         self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         seq_len = x.size(1)\n",
        "#         if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.base)\n",
        "#         return x * self.rot_emb[:seq_len]\n",
        "\n",
        "class RotEmb(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, top=torch.pi, base=10000):\n",
        "        super().__init__()\n",
        "        self.theta = top / (base ** (torch.arange(0, dim, step=2, device=device) / dim))\n",
        "        # self.theta = top / (base ** torch.linspace(0, 1, dim//2, device=device))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (pos.unsqueeze(-1) * self.theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [seq_len, dim]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6T4F651kmGh",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title AttentionBlock\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "import math\n",
        "def StableInit(m): # https://openreview.net/pdf?id=lkRjnNW0gb\n",
        "    if isinstance(m, nn.Linear):\n",
        "        # W ~ N(0, ( 1/ (sqrt(n_in) + sqrt(n_out)) )^2 )\n",
        "        # want std = 1/ (sqrt(n_in) + sqrt(n_out))\n",
        "        # n_in, n_out = module.weight.shape[0], module.weight.shape[1]\n",
        "        n_in, n_out = m.weight.shape\n",
        "        torch.nn.init.normal_(m.weight, std=1/(math.sqrt(n_in)+math.sqrt(n_out)))\n",
        "        if m.bias is not None:\n",
        "            nn.init.zeros_(m.bias)\n",
        "\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class SelfAttn(nn.Module):\n",
        "    def __init__(self, dim, n_heads):\n",
        "        super().__init__()\n",
        "        self.dim, self.n_heads = dim, n_heads\n",
        "        d_head = dim//n_heads\n",
        "        self.qkv = nn.Linear(dim, dim*3, bias=False)\n",
        "        # self.lin = nn.Linear(dim, dim)\n",
        "        self.lin = zero_module(nn.Linear(dim, dim))\n",
        "        # self.rope = RoPE(d_head, seq_len=512, base=10000)\n",
        "        # self.rope = RoPE2D(d_head, h=64, w=64, base=100)\n",
        "        self.scale = d_head**-.5\n",
        "        # torch.nn.init.normal_(self.qkv.weight, std=.02)\n",
        "        # self.qkv.apply(StableInit)\n",
        "\n",
        "    def forward(self, x): # [b,t,d]\n",
        "        q,k,v = self.qkv(x).unflatten(-1, (self.n_heads,-1)).transpose(1,2).chunk(3, dim=-1) # [b, t, n_heads, d_head] -> [b, n_heads, t, d_head]\n",
        "        # q, k = self.rope(q), self.rope(k)\n",
        "        q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2)\n",
        "        context = k.transpose(-2,-1) @ v # [batch, n_heads, d_head, d_head]\n",
        "        x = q @ context # [b, n_heads, t, d_head]\n",
        "        # print('SelfAttn', x.shape)\n",
        "        x = x.transpose(1,2).flatten(2)\n",
        "        return self.lin(x)\n",
        "\n",
        "\n",
        "\n",
        "class SwiGLU(nn.Module): # https://arxiv.org/pdf/2002.05202\n",
        "    def __init__(self, d_model, ff_dim): # d_model * 3*ff_dim params\n",
        "        super().__init__()\n",
        "        self.lin0 = nn.Linear(d_model, 2*ff_dim, bias=False)\n",
        "        self.lin1 = zero_module(nn.Linear(ff_dim, d_model, bias=False))\n",
        "\n",
        "    def forward(self, x): # [b,t,d]\n",
        "        x0, x1 = self.lin0(x).chunk(2, dim=-1)\n",
        "        return self.lin1(x0*F.silu(x1))\n",
        "\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, mult=4, drop=0.):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        # self.norm = nn.RMSNorm(d_model, elementwise_affine=False) # LayerNorm RMSNorm\n",
        "        self.norm1, self.norm2 = nn.LayerNorm(d_model), nn.LayerNorm(d_model)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "        self.attn = SelfAttn(d_model, n_heads)\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        # act = Swwish()\n",
        "        ff_dim=d_model*mult\n",
        "        self.ff = nn.Sequential(\n",
        "            # nn.RMSNorm(d_model), act, nn.Dropout(drop), nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Dropout(drop), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            nn.RMSNorm(d_model), nn.Linear(d_model, ff_dim), act,\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(drop), nn.Linear(d_model, ff_dim), act,\n",
        "            nn.RMSNorm(ff_dim), nn.Dropout(drop), zero_module(nn.Linear(ff_dim, d_model))\n",
        "        )\n",
        "        # self.ff = SwiGLU(d_model, ff_dim)\n",
        "\n",
        "\n",
        "    def forward(self, x): # [batch, seq_len, d_model]\n",
        "        # print('attnblk fwd',x.shape)\n",
        "        x = x + self.drop(self.attn(self.norm1(x)))\n",
        "        x = x + self.ff(x)\n",
        "        # x = x + self.drop(self.ff(self.norm2(x)))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3vUVNJc_sy1a"
      },
      "outputs": [],
      "source": [
        "# @title scheduler\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "import math\n",
        "def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, num_cycles=0.5, last_epoch=-1):\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < num_warmup_steps:\n",
        "            return float(current_step) / float(max(1, num_warmup_steps))\n",
        "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
        "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))\n",
        "    return LambdaLR(optimizer, lr_lambda, last_epoch)\n",
        "\n",
        "# total_steps=100\n",
        "# base_lr, max_lr = 3e-5, 3e-4\n",
        "\n",
        "# import torch\n",
        "# model=torch.nn.Linear(2,3)\n",
        "# optim = torch.optim.AdamW(model.parameters(), lr=base_lr, betas=(0.9, 0.999))\n",
        "\n",
        "# scheduler = get_cosine_schedule_with_warmup(optim, num_warmup_steps=20 , num_training_steps=total_steps) # https://docs.pytorch.org/torchtune/0.2/generated/torchtune.modules.get_cosine_schedule_with_warmup.html\n",
        "# # scheduler = torch.optim.lr_scheduler.OneCycleLR(optim, max_lr=max_lr, total_steps=total_steps, pct_start=0.45, anneal_strategy='cos', cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=div_factor, final_div_factor=100.0, three_phase=True,)\n",
        "\n",
        "# lr_lst=[]\n",
        "# import matplotlib.pyplot as plt\n",
        "# for t in range(total_steps):\n",
        "#     lr=optim.param_groups[0][\"lr\"]\n",
        "#     lr_lst.append(lr)\n",
        "#     scheduler.step()\n",
        "# plt.plot(lr_lst)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-zdjdJixtOu",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title TransformerModel/Predictor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, n_heads=4, d_hid=None, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        # self.pos_emb = nn.Parameter(torch.randn(1, 256, d_model)*.02) # 200\n",
        "        self.pos_emb = RoPE(d_model, seq_len=256, base=10000) # 256\n",
        "\n",
        "        # self.transformer = nn.Sequential(*[AttentionBlock(d_model, n_heads=n_heads) for _ in range(nlayers)])\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, n_heads=n_heads, drop=drop) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.randn(1,1,d_model)*0.02) # randn zeros\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim)# if out_dim != d_model else None\n",
        "\n",
        "        # torch.nn.init.normal_(self.embed.weight, std=.02)\n",
        "        # if self.lin: torch.nn.init.normal_(self.lin.weight, std=.02)\n",
        "\n",
        "\n",
        "    def forward(self, x, context_indices, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = x.shape\n",
        "        # x = x * self.pos_enc(context_indices)\n",
        "        # print(\"Trans pred\",x.shape, self.pos_emb[0,context_indices].shape)\n",
        "        x = x + self.pos_emb[0,context_indices]\n",
        "        # x = x * self.pos_emb[0,context_indices]\n",
        "        # print('pred fwd', self.pos_emb[:,context_indices].shape)\n",
        "\n",
        "        # pred_tokens = self.cls * self.pos_enc(trg_indices) # [M, num_trg_toks, d_model]\n",
        "        pred_tokens = self.cls + self.pos_emb[0,trg_indices]\n",
        "        # pred_tokens = self.cls * self.pos_emb[0,trg_indices]\n",
        "        # print(\"pred fwd\", x.shape, pred_tokens.shape)\n",
        "        x = torch.cat([x, pred_tokens], dim=1) # [batch, seq_len+num_trg_toks, d_model]\n",
        "        out = self.transformer(x)\n",
        "\n",
        "        out = self.norm(out)\n",
        "        out = out[:,seq:] # [batch, num_trg_toks, d_model]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "# class SLSTM(nn.Module):\n",
        "#     def __init__(self, d_model, num_layers=2, batch_first=True):\n",
        "#         super().__init__()\n",
        "#         self.lstm = nn.LSTM(d_model, d_model, num_layers)\n",
        "\n",
        "#     def forward(self, x): # [b,c,t]\n",
        "#         x = x + self.lstm(x.transpose(-2,-1))[0].transpose(-2,-1) # skip=True\n",
        "#         return x\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, drop=0):\n",
        "    def __init__(self, patch_size, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0):\n",
        "        super().__init__()\n",
        "        patch_size=8\n",
        "        act = nn.ReLU() # ReLU SiLU GELU\n",
        "        # act = LearntSwwish(d_model) # SnakeBeta LearntSwwish\n",
        "        # act1 = LearntSwwish(d_model) # SnakeBeta LearntSwwish\n",
        "        # act = Swwish()\n",
        "        self.embed = nn.Sequential(\n",
        "            # # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.MaxPool1d(2,2), #nn.MaxPool1d(3, 2, 3//2),\n",
        "            # # nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.Conv1d(d_model, d_model,3,2,3//2)\n",
        "            nn.Conv1d(in_dim, d_model,7,2,7//2), nn.BatchNorm1d(d_model), act,\n",
        "            nn.Conv1d(d_model, d_model,5,2,5//2), nn.BatchNorm1d(d_model), act,\n",
        "            # nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), act, #nn.MaxPool1d(2,2),\n",
        "            # nn.Conv1d(d_model, d_model,3,2,3//2), nn.BatchNorm1d(d_model), act, #nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2),\n",
        "            # nn.Conv1d(in_dim, d_model, patch_size, patch_size), # like patch\n",
        "            # nn.Conv1d(in_dim, d_model, 1, 1), # like patch\n",
        "\n",
        "            # nn.Conv2d(d_model, d_model,(in_dim,3),2,3//2),\n",
        "            # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.Dropout(drop), nn.BatchNorm1d(d_model), snake,\n",
        "            # SLSTM(d_model),\n",
        "\n",
        "            )\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        # self.pos_emb = nn.Parameter(torch.randn(1, 256, d_model)*.02) # 200\n",
        "        self.pos_emb = RoPE(d_model, seq_len=256, base=10000) # 256\n",
        "        # self.transformer = nn.Sequential(*[AttentionBlock(d_model, n_heads=n_heads) for _ in range(nlayers)])\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, n_heads=n_heads, drop=drop) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "        # if self.lin: torch.nn.init.normal_(self.lin.weight, std=.02)\n",
        "\n",
        "        # self.embed.apply(self.init_conv)\n",
        "        # self.embed.apply(self.init_weights)\n",
        "\n",
        "    # def init_conv(self, m):\n",
        "    #     if isinstance(m, nn.Conv1d):\n",
        "    #         # nn.init.kaiming_uniform_(m.weight, a=math.sqrt(5))\n",
        "    #         nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "    #         if m.bias is not None:\n",
        "    #             # bound = 1 / math.sqrt(m.in_channels * m.kernel_size * m.kernel_size)\n",
        "    #             # nn.init.uniform_(m.bias, -bound, bound)\n",
        "    #             nn.init.zeros_(m.bias)\n",
        "\n",
        "    def init_weights(self, m):\n",
        "        if isinstance(m, (nn.Linear, nn.Conv1d)):\n",
        "            torch.nn.init.normal_(m.weight, std=.02)\n",
        "            if m.bias is not None: nn.init.zeros_(m.bias)\n",
        "\n",
        "\n",
        "    def forward(self, x, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "        # try: print(\"Trans fwd\",x.shape, context_indices.shape)\n",
        "        # except: print(\"Trans fwd noind\",x.shape)\n",
        "        # x = self.pos_enc(x)\n",
        "        x = x + self.pos_emb[:,:x.shape[1]]\n",
        "        # x = x * self.pos_emb[:,:x.shape[1]]\n",
        "        if context_indices != None: x = x[torch.arange(x.shape[0]).unsqueeze(-1), context_indices] # [batch, num_context_toks, d_model]\n",
        "\n",
        "        # print(\"TransformerModel\",x.shape)\n",
        "        x = self.transformer(x)\n",
        "        out = self.norm(x)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out\n",
        "\n",
        "batch, seq_len, d_model = 4,1751,16 # wisdm 3500, ethol conc 1751\n",
        "in_dim = 3\n",
        "patch_size=32\n",
        "model = TransformerModel(patch_size, in_dim, d_model, n_heads=4, nlayers=3, drop=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "# # # print(out)\n",
        "# model = TransformerPredictor(in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1).to(device)\n",
        "# out = model(out)\n",
        "# print(out.shape)\n",
        "# for name, param in model.named_parameters():\n",
        "#     print(name, param.shape, param[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ardu1zJwdHM3",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title SeqJEPA\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class SeqJEPA(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, nlayers=2, n_heads=4, drop=0):\n",
        "        super().__init__()\n",
        "        if out_dim is None: out_dim = d_model\n",
        "        self.patch_size = 8 # 8 32\n",
        "        self.student = TransformerModel(self.patch_size, in_dim, d_model, out_dim=out_dim, n_heads=n_heads, nlayers=nlayers, drop=drop)\n",
        "        self.predicter = TransformerPredictor(out_dim, d_model//2, out_dim, n_heads=4, nlayers=1, drop=drop)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "        # self.transform = RandomResizedCrop1d(3500, scale=(.8,1.))\n",
        "\n",
        "    #     self.apply(self.init_weights)\n",
        "    #     self.apply(self.zero_last_layers)\n",
        "    # def init_weights(self, m):\n",
        "    #     if isinstance(m, (nn.Linear, nn.Conv1d)):\n",
        "    #         torch.nn.init.normal_(m.weight, std=.02)\n",
        "    #         if m.bias is not None: nn.init.zeros_(m.bias)\n",
        "\n",
        "    # def zero_last_layers(self, m):\n",
        "    #     children = list(m.children())\n",
        "    #     if not children: return\n",
        "    #     last = children[-1]\n",
        "    #     if isinstance(last, (nn.Linear, nn.Conv2d)):\n",
        "    #         nn.init.zeros_(last.weight)\n",
        "    #         if last.bias is not None: nn.init.zeros_(last.bias)\n",
        "\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]\n",
        "        batch, seq, dim = x.shape\n",
        "        # print(x.shape)\n",
        "        # target_mask = multiblock(seq//self.patch_size, min_s=.2, max_s=.3, M=4, B=1).any(1).squeeze(1) # best.2.3M4 og.15.2M4# mask out targets to be predicted # [M, seq]\n",
        "        # # target_mask = randpatch(seq//self.patch_size, mask_size=8, gamma=.9).unsqueeze(0) # 8.9 [seq]\n",
        "\n",
        "        # print(target_mask.shape, x.shape)\n",
        "        # context_mask = ~multiblock(seq//self.patch_size, min_s=.85, max_s=1, M=1, B=1).squeeze(1)|target_mask # og .85,1.M1 # [1, seq], True->Mask\n",
        "        # context_mask = torch.zeros((1,seq//self.patch_size), dtype=bool)|target_mask # [1,h,w], True->Mask\n",
        "\n",
        "        # context_indices, trg_indices = simplexmask1d(seq//self.patch_size, ctx_scale=(.85,1), trg_scale=(.7,.8), B=batch, chaos=[3,.5])\n",
        "        # context_indices, trg_indices = simplexmask1d(seq//self.patch_size, ctx_scale=(.8,.9), trg_scale=(.7,.8), B=batch, chaos=[1,.5])\n",
        "        # context_indices, trg_indices = simplexmask1d(seq//self.patch_size, ctx_scale=(.8,1), trg_scale=(.2,.8), B=batch, chaos=[1,.5])\n",
        "        # print(context_indices.shape, trg_indices.shape)\n",
        "\n",
        "        # context_indices = context_indices.repeat(batch,1)\n",
        "        # trg_indices = trg_indices.repeat(batch,1)\n",
        "        # context_mask = ~context_mask|target_mask # [1,]\n",
        "        # context_indices = (~context_mask).nonzero()[:,1].unsqueeze(0).repeat(batch,1)\n",
        "        # # print(trg_indices.shape, context_indices.shape)\n",
        "        # # print(context_mask.shape,target_mask.shape, x.shape)\n",
        "        # target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "        # # # target_mask, context_mask = target_mask.repeat(batch,1), context_mask.repeat(batch,1)\n",
        "        # # x_ = x * F.adaptive_avg_pool1d((~context_mask).float(), x.shape[1]).unsqueeze(-1) # zero masked locations\n",
        "\n",
        "        # context_indices = (~context_mask).nonzero()[:,1].unflatten(0, (batch,-1)) # int idx [num_context_toks] , idx of context not masked\n",
        "        # trg_indices = target_mask.nonzero()[:,1].unflatten(0, (batch,-1)) # int idx [num_trg_toks] , idx of targets that are masked\n",
        "\n",
        "\n",
        "        # mask_collator = MaskCollator(length=seq//self.patch_size, enc_mask_scale=(.85,1), pred_mask_scale=(.15,.2), nenc=1, npred=4, min_keep=4, allow_overlap=False)\n",
        "        mask_collator = MaskCollator(length=seq//self.patch_size, enc_mask_scale=(.85,1), pred_mask_scale=(.2,.25), nenc=1, npred=4, min_keep=4, allow_overlap=False)\n",
        "        collated_masks_enc, collated_masks_pred = mask_collator(batch) # idx of ctx, idx of masked trg\n",
        "        # # collated_masks_enc, collated_masks_pred = mask_collator(1) # idx of ctx, idx of masked trg\n",
        "        context_indices, trg_indices = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "        # # # zero_mask = torch.zeros(batch ,seq//self.patch_size, device=device)\n",
        "        # # # zero_mask[torch.arange(batch).unsqueeze(-1), context_indices] = 1\n",
        "        # # zero_mask = torch.zeros(1 ,seq//self.patch_size, device=device)\n",
        "        # # zero_mask[:, context_indices] = 1\n",
        "        # # x_ = x * F.adaptive_avg_pool1d(zero_mask, x.shape[1]).unsqueeze(-1) # zero masked locations\n",
        "        # # context_indices, trg_indices = context_indices.repeat(batch,1), trg_indices.repeat(batch,1)\n",
        "\n",
        "\n",
        "        # print('x_',x_.shape, context_indices.shape, trg_indices.shape)\n",
        "\n",
        "        sx = self.student(x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "        # print('seq_jepa loss sx',sx.shape)\n",
        "        sy_ = self.predicter(sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "        sy_ = F.layer_norm(sy_, (sy_.size(-1),))\n",
        "        with torch.no_grad():\n",
        "            sy = self.teacher(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "            sy = sy[torch.arange(sy.shape[0]).unsqueeze(-1), trg_indices] # [batch, num_context_toks, d_model] # nan bec len(trg_ind)==0 # print('loss sy',torch.isnan(sy).any())\n",
        "            sy = F.layer_norm(sy, (sy.size(-1),))\n",
        "        loss = F.mse_loss(sy, sy_)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        sx = self.student(x)\n",
        "        out = sx.mean(dim=1)\n",
        "        return out\n",
        "\n",
        "# min_s=0.15, max_s, M\n",
        "# trg.15.2M4 C.85 1\n",
        "\n",
        "# 1e-2,1e-3 < 3e-3,1e-3\n",
        "# patch16 < patch32\n",
        "# NoPE good but sus\n",
        "\n",
        "# ctx/trg sacle min/max, num blk,\n",
        "\n",
        "in_dim = X_train[0].shape[-1] # 3\n",
        "out_dim = train_data.vocab_size # 16\n",
        "d_model=64\n",
        "# seq_jepa = SeqJEPA(in_dim=in_dim, d_model=d_model, out_dim=None, nlayers=1, n_heads=8).to(device)#.to(torch.float)\n",
        "seq_jepa = SeqJEPA(in_dim=in_dim, d_model=d_model, out_dim=None, nlayers=1, n_heads=8, drop=.0).to(device)#.to(torch.float)\n",
        "# optim = torch.optim.AdamW(seq_jepa.parameters(), lr=1e-3) # 1e-3? default 1e-2\n",
        "optim = torch.optim.AdamW(seq_jepa.parameters(), lr=3e-4) # 1e-3? default 1e-2\n",
        "# optim = StableAdamW(seq_jepa.parameters(), lr=1e-3)\n",
        "# optim = Lion(seq_jepa.parameters(), lr=1e-4, weight_decay=1e-1) # lr 1e-3, wd 1e-1\n",
        "# optim = torch.optim.AdamW([{'params': seq_jepa.student.parameters()},\n",
        "#     {'params': seq_jepa.predicter.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # default 1e-2, 5e-2\n",
        "    # {'params': seq_jepa.predicter.parameters(), 'lr': 1e-2}], lr=1e-3, weight_decay=1e-2)\n",
        "\n",
        "# !pip install -q bitsandbytes\n",
        "# import bitsandbytes as bnb\n",
        "# # # optim = bnb.optim.(seq_jepa.parameters(), lr=1e-3, betas=(0.9, 0.99), weight_decay=1e-2)\n",
        "# # optim = bnb.optim.Lion(seq_jepa.parameters(), lr=1e-4, betas=(0.9, 0.99), weight_decay=1e-2) # Lion8bit\n",
        "# optim = bnb.optim.Lion(seq_jepa.parameters(), lr=1e-4, betas=(0.9, 0.99), weight_decay=1e-1) # Lion8bit\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/configs/in1k_vith14_ep300.yaml\n",
        "# d_model 1024,384\n",
        "# depth 12,6/12\n",
        "# wd 5e-2 - 4e-1\n",
        "# adamw 1e-4 - 1e-3 - 1e-6\n",
        "# ema 0.996-1\n",
        "\n",
        "print(sum(p.numel() for p in seq_jepa.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.parameters())) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.predicter.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.student.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.teacher.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# d_model^2 * nlayers\n",
        "\n",
        "x = torch.rand((24, 1700, in_dim), device=device)\n",
        "out = seq_jepa.loss(x)\n",
        "print(out.shape)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(d_model, out_dim).to(device)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3) # 1e-3\n",
        "# optim = torch.optim.AdamW([{'params': seq_jepa.parameters()}, {'params': classifier.parameters(), 'lr': 1e-3}], lr=1e-3)\n",
        "# coptim = torch.optim.AdamW(classifier.parameters(), lr=1e-3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgVbsYsu85eT"
      },
      "outputs": [],
      "source": [
        "for name, param in seq_jepa.named_parameters():\n",
        "    print(name, param.shape, param[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2Nd-sGe6Ku4S"
      },
      "outputs": [],
      "source": [
        "# @title wandb\n",
        "!pip install -q wandb\n",
        "import wandb # https://docs.wandb.ai/quickstart\n",
        "wandb.login(key='487a2109e55dce4e13fc70681781de9f50f27be7')\n",
        "try: run.finish()\n",
        "except NameError: pass\n",
        "run = wandb.init(project=\"ucr\", config={\"model\": \"res18\",}) # violet SeqJEPA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-eTjgAhmp_t",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title strain ctrain test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "# name = 'violet'\n",
        "# train_summary_writer = tf.summary.create_file_writer('logs/'+name+'/train')\n",
        "# test_summary_writer = tf.summary.create_file_writer('logs/'+name+'/test')\n",
        "\n",
        "def strain(model, dataloader, optim, scheduler=None):\n",
        "    model.train()\n",
        "    for i, (x, _) in enumerate(dataloader):\n",
        "        # x = x[...,1:].to(device).to(torch.bfloat16) # for wisdm?\n",
        "        x = x.to(device).to(torch.bfloat16)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            loss = model.loss(x)\n",
        "\n",
        "            # repr_loss, std_loss, cov_loss = model.loss(x)\n",
        "            # loss = model.sim_coeff * repr_loss + model.std_coeff * std_loss + model.cov_coeff * cov_loss\n",
        "\n",
        "        optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # total_norm = 0\n",
        "        # for p in model.parameters(): total_norm += p.grad.data.norm(2).item() ** 2\n",
        "        # total_norm = total_norm**.5\n",
        "        # print('total_norm', total_norm)\n",
        "        # for p in list(filter(lambda p: p.grad is not None, model.parameters())):\n",
        "        #     print(p.grad.data.norm(2).item())\n",
        "        # print(\"max grad norm\", max([p.grad.data.norm(2).item() for p in list(filter(lambda p: p.grad is not None, model.parameters()))]))\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 5) # 0.5\n",
        "\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            m=0.999 # 0.99 m = next(momentum_scheduler)\n",
        "            for param_q, param_k in zip(model.student.parameters(), model.teacher.parameters()):\n",
        "                param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "\n",
        "        if scheduler is not None: scheduler.step()\n",
        "        # print(\"strain\",loss.item())\n",
        "        # for param in seq_jepa.context_encoder.cls: print(param.data)\n",
        "        # for param in seq_jepa.predicter.cls: print(param.data)\n",
        "        try: wandb.log({\"loss\": loss.item(), \"repr/I\": repr_loss.item(), \"std/V\": std_loss.item(), \"cov/C\": cov_loss.item()})\n",
        "        # try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        # with train_summary_writer.as_default(): tf.summary.scalar('strain', loss.item(), step=i)\n",
        "        if i>=500: break\n",
        "\n",
        "\n",
        "def ctrain(model, classifier, dataloader, coptim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.eval()\n",
        "    classifier.train()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        # x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        # x, y = x[...,1:].to(device).to(torch.bfloat16), y[1].to(device) # [batch, ] # (id, activity)\n",
        "        x, y = x.to(device).to(torch.bfloat16), y.to(device) # [batch, ] # (id, activity)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            with torch.no_grad():\n",
        "                sx = model(x).detach()\n",
        "            y_ = classifier(sx)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "        coptim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 5) # .5\n",
        "        scaler.step(coptim)\n",
        "        scaler.update()\n",
        "        # print(\"classify\",loss.item())\n",
        "        try: wandb.log({\"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        # with test_summary_writer.as_default(): tf.summary.scalar('closs', loss.item(), step=i)\n",
        "        if i>=100: break\n",
        "\n",
        "\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    correct = 0\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        # x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        # x, y = x[...,1:].to(device).to(torch.float), y[1].to(device) # [batch, ] # (id, activity)\n",
        "        x, y = x.to(device).to(torch.float), y.to(device) # [batch, ] # (id, activity)\n",
        "        with torch.no_grad():\n",
        "            sx = model(x)\n",
        "            try:\n",
        "                rankme = RankMe(sx).item()\n",
        "                lidar = LiDAR(sx).item()\n",
        "            except NameError: pass\n",
        "            y_ = classifier(sx)\n",
        "        test_loss = F.cross_entropy(y_, y)\n",
        "        correct += (y==y_.argmax(dim=1)).sum().item()\n",
        "        if i>=100: break\n",
        "    # print(correct/len(y))\n",
        "    print(correct/len(dataloader.dataset))\n",
        "    try: wandb.log({\"correct\": correct/len(dataloader.dataset)})\n",
        "    # try: wandb.log({\"correct\": correct/len(y), \"rankme\": rankme, \"lidar\": lidar})\n",
        "    except NameError: pass\n",
        "\n",
        "scheduler = get_cosine_schedule_with_warmup(optim, num_warmup_steps=400, num_training_steps=2000) # https://docs.pytorch.org/torchtune/0.2/generated/torchtune.modules.get_cosine_schedule_with_warmup.html\n",
        "for i in range(2000): #\n",
        "# for i in range(5000): # 5000\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # np.random.shuffle(train_indices); np.random.shuffle(val_indices) # for wisdm\n",
        "    # train_sampler, valid_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(val_indices)\n",
        "    # # batch_size = 64 #512\n",
        "    # train_loader = DataLoader(train_data, sampler=train_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "    # test_loader = DataLoader(train_data, sampler=valid_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True)\n",
        "\n",
        "    strain(seq_jepa, train_loader, optim)\n",
        "    # strain(seq_jepa, train_loader, optim, scheduler)\n",
        "    ctrain(seq_jepa, classifier, train_loader, coptim)\n",
        "    test(seq_jepa, classifier, test_loader)\n",
        "    scheduler.step()\n",
        "    print(i, optim.param_groups[0][\"lr\"])\n",
        "    # strain(violet, train_loader, voptim)\n",
        "    # ctrain(violet, classifier, train_loader, coptim)\n",
        "    # test(violet, classifier, test_loader)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5p6LJ2qPqom"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir logs/gradient_tape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4J2ahp3wmqcg"
      },
      "outputs": [],
      "source": [
        "# @title supervised train test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "def strain(model, classifier, dataloader, optim, coptim, scheduler=None):\n",
        "    model.train()\n",
        "    classifier.train()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        # y = y.to(device)\n",
        "        # x, y = x[...,1:].to(device).to(torch.bfloat16), y[1].to(device) # [batch, ] # (id, activity)\n",
        "        x, y = x.to(device).to(torch.bfloat16), y.to(device) # [batch, ] # (id, activity)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        coptim.zero_grad()\n",
        "        # print(loss)\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # for p in list(filter(lambda p: p.grad is not None, model.parameters())):\n",
        "        #     print(p.grad.data.norm(2).item())\n",
        "        # print(\"max grad norm\", max([p.grad.data.norm(2).item() for p in list(filter(lambda p: p.grad is not None, model.parameters()))]))\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 10) # 0.5\n",
        "\n",
        "        scaler.step(optim)\n",
        "        scaler.step(coptim)\n",
        "        scaler.update()\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        print(\"strain\",loss.item())\n",
        "        # for param in seq_jepa.context_encoder.cls: print(param.data)\n",
        "        # for param in seq_jepa.predicter.cls: print(param.data)\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=50: break\n",
        "\n",
        "\n",
        "# def test(model, dataloader):\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        # x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        # x, y = x[...,1:].to(device).to(torch.float), y[1].to(device) # [batch, ] # (id, activity)\n",
        "        x, y = x.to(device).to(torch.float), y.to(device) # [batch, ] # (id, activity)\n",
        "        with torch.no_grad():\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "        loss = F.cross_entropy(y_, y)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        print(correct/len(y))\n",
        "        try: wandb.log({\"correct\": correct/len(y), \"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "for i in range(1000):\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # np.random.shuffle(train_indices); np.random.shuffle(val_indices)\n",
        "    # train_sampler, valid_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(val_indices)\n",
        "    # # batch_size = 64 #512\n",
        "    # train_loader = DataLoader(train_data, sampler=train_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "    # test_loader = DataLoader(train_data, sampler=valid_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True)\n",
        "\n",
        "    strain(seq_jepa, classifier, train_loader, optim, coptim)\n",
        "    test(seq_jepa, classifier, test_loader)\n",
        "    # strain(violet, train_loader, voptim)\n",
        "    # test(violet, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "JT8AgOx0E_KM"
      },
      "outputs": [],
      "source": [
        "# @title save/load\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# folder='/content/drive/MyDrive/jepa/'\n",
        "\n",
        "# # modelsd, optimsd = torch.load(folder+'SeqJEPA.pkl', map_location=device).values()\n",
        "# modelsd, optimsd = torch.load('SeqJEPA.pkl', map_location=device).values()\n",
        "# seq_jepa.load_state_dict(modelsd, strict=False)\n",
        "# optim.load_state_dict(optimsd)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "modelsd = torch.load(folder+'roberta.pkl', map_location=device)['model']#.values()\n",
        "# print(modelsd)\n",
        "model_mlm.load_state_dict(modelsd, strict=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpbLPvjiE-pD"
      },
      "outputs": [],
      "source": [
        "# checkpoint = {'model': seq_jepa.state_dict(), 'optimizer': optim.state_dict()}\n",
        "# checkpoint = {'model': model_mlm.state_dict()}\n",
        "# torch.save(checkpoint, folder+'roberta.pkl')\n",
        "# torch.save(checkpoint, folder+'SeqJEPA.pkl')\n",
        "# torch.save(checkpoint, 'SeqJEPA.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# storage"
      ],
      "metadata": {
        "id": "3dl1RyOjHdi1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vScvFGGSeN6"
      },
      "source": [
        "## drawer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pQfM2fYvcTh6"
      },
      "outputs": [],
      "source": [
        "# @title masks\n",
        "import torch\n",
        "\n",
        "def multiblock(seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "    return target_mask\n",
        "\n",
        "def multiblock(seq, min_s, max_s, B=64, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    # mask_len = torch.rand(B, 1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_len = torch.rand(1, M) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(B, M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    # indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    indices = torch.arange(seq)[None,None,...] # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [B, M, seq]\n",
        "    return target_mask\n",
        "\n",
        "def multiblock2d(hw=(8,8), scale=(.15,.2), aspect_ratio=(.75,1.5), M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_aspect = torch.rand(1) * (aspect_ratio[1] - aspect_ratio[0]) + aspect_ratio[0] # in (min_s, max_s) # all blocks same size\n",
        "    mask_scale = torch.rand(1) * (scale[1] - scale[0]) + scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    h = (mask_scale/mask_aspect)**.5# h*(h*aspect) = scale\n",
        "    w = h * mask_aspect\n",
        "    h_pos, w_pos = torch.rand(M)*(1-w), torch.rand(M)*(1-h) # in (0, 1 - mask_len)\n",
        "    h_len, h_pos = (h*hw[0]).int(), h_pos*hw[0]\n",
        "    w_len, w_pos = (w*hw[1]).int(), w_pos*hw[1]\n",
        "    h_ind, w_ind = torch.arange(hw[0]).unsqueeze(0), torch.arange(hw[1]).unsqueeze(0) # [1, seq]\n",
        "    h_mask = (h_ind>=h_pos.unsqueeze(-1)) & (h_ind<(h_pos+h_len).unsqueeze(-1)) # [M, seq]\n",
        "    w_mask = (w_ind>=w_pos.unsqueeze(-1)) & (w_ind<(w_pos+w_len).unsqueeze(-1)) # [M, seq]\n",
        "    target_mask = h_mask.unsqueeze(-1) & w_mask.unsqueeze(-2) # [M, seq, seq]\n",
        "    return target_mask\n",
        "\n",
        "def multiblock2d(hw=(8,8), scale=(.15,.2), aspect_ratio=(.75,1.5), B=64, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_aspect = torch.rand(B, 1) * (aspect_ratio[1] - aspect_ratio[0]) + aspect_ratio[0] # in (min_s, max_s) # all blocks same size\n",
        "    mask_scale = torch.rand(B, 1) * (scale[1] - scale[0]) + scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    h = (mask_scale/mask_aspect)**.5# h*(h*aspect) = scale\n",
        "    w = h * mask_aspect\n",
        "    h_pos, w_pos = torch.rand(M)*(1-w), torch.rand(M)*(1-h) # in (0, 1 - mask_len)\n",
        "    h_len, h_pos = (h*hw[0]).int(), h_pos*hw[0]\n",
        "    w_len, w_pos = (w*hw[1]).int(), w_pos*hw[1]\n",
        "    h_ind, w_ind = torch.arange(hw[0]).unsqueeze(0), torch.arange(hw[1]).unsqueeze(0) # [1, seq]\n",
        "    h_mask = (h_ind>=h_pos.unsqueeze(-1)) & (h_ind<(h_pos+h_len).unsqueeze(-1)) # [M, seq]\n",
        "    w_mask = (w_ind>=w_pos.unsqueeze(-1)) & (w_ind<(w_pos+w_len).unsqueeze(-1)) # [M, seq]\n",
        "    target_mask = h_mask.unsqueeze(-1) & w_mask.unsqueeze(-2) # [M, seq, seq]\n",
        "    return target_mask\n",
        "\n",
        "# https://arxiv.org/pdf/2210.07224\n",
        "def randpatch(seq, mask_size=8, gamma=0.9): # num patches of seq, mask patch size, masking ratio\n",
        "    # mask = torch.rand(seq//mask_size)<gamma\n",
        "    length = seq//mask_size\n",
        "    g = torch.normal(gamma, std=.1, size=(1,)).clamp(.5,.9)\n",
        "    # g = gamma\n",
        "    idx = torch.randperm(length)[:int(length*g)]\n",
        "    mask = torch.zeros(length, dtype=bool)\n",
        "    mask[idx] = True\n",
        "    mask = mask.repeat_interleave(mask_size, dim=-1)\n",
        "    return mask # [seq] , True -> mask\n",
        "\n",
        "\n",
        "import torch\n",
        "def apply_masks(x, mask): # [b,t,d], [mask_size] # https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "    mask_keep = mask.unsqueeze(-1).repeat(x.size(0), 1, x.size(-1)) # [batch,T,dim]\n",
        "    return torch.gather(x, dim=1, index=mask_keep) # [batch,mask_size,dim]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9rPxvrrsYI_W"
      },
      "outputs": [],
      "source": [
        "# @title simplex\n",
        "# !pip install -q opensimplex\n",
        "import opensimplex\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def simplexmask(hw=(8,8), scale=(.15,.2)):\n",
        "    ix = iy = np.linspace(0, 1, num=8)\n",
        "    ix, iy = ix+np.random.randint(1e10), iy+np.random.randint(1e10)\n",
        "    y=opensimplex.noise2array(ix, iy)\n",
        "    y = torch.from_numpy(y)\n",
        "    mask_scale = torch.rand(1) * (scale[1] - scale[0]) + scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    yy = y.flatten().sort()[0][int(hw[0]*hw[1]*mask_scale)]\n",
        "    mask = (y<=yy.item())\n",
        "    return mask # T/F [h,w]\n",
        "\n",
        "def simplexmask1d(seq=512, scale=(.15,.2), chaos=2):\n",
        "    i = np.linspace(0, chaos, num=seq) # 2\n",
        "    j = np.random.randint(1e10, size=1)\n",
        "    y=opensimplex.noise2array(i, j) # [1, seq]\n",
        "    # plt.pcolormesh(y)\n",
        "    # plt.show()\n",
        "    y = torch.from_numpy(y)\n",
        "    # print(y.shape)\n",
        "    mask_scale = torch.rand(1) * (scale[1] - scale[0]) + scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    # print(a.shape, int(seq*mask_scale))\n",
        "    val, ind = y.sort()\n",
        "    yy = val[:,int(seq*mask_scale)]\n",
        "    mask = (y<=yy.item())\n",
        "    index = ind[:,:int(seq*mask_scale)]\n",
        "    return index, mask # T/F [h,w]\n",
        "\n",
        "def simplexmask1d(seq=512, ctx_scale=(.85,1), trg_scale=(.6,.8), B=64, chaos=2):\n",
        "    i = np.linspace(0, chaos, num=seq) # 2-5\n",
        "    j = np.random.randint(1e10, size=B)\n",
        "    noise = opensimplex.noise2array(i, j) # [B, seq]\n",
        "    # plt.pcolormesh(noise[:1])\n",
        "    # plt.rcParams[\"figure.figsize\"] = (20,3)\n",
        "    # plt.show()\n",
        "    noise = torch.from_numpy(noise)\n",
        "    ctx_mask_scale = torch.rand(1) * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    trg_mask_scale = torch.rand(1) * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "    val, ind = noise.sort()\n",
        "    trg_index = ind[:,-int(seq*trg_mask_scale):]\n",
        "    ctx_index = ind[:,-int(seq*ctx_mask_scale):-int(seq*trg_mask_scale)] # ctx wraps trg ; most similar to multiblock\n",
        "    # ctx_index = ind[:,:int(seq*ctx_mask_scale)-int(seq*trg_mask_scale)] # ctx hug bottom\n",
        "    # ctx_index = ind[:,-int(seq*ctx_mask_scale)-int(seq*trg_mask_scale):-int(seq*trg_mask_scale)] # ctx wraps trg ; most similar to multiblock\n",
        "    # ctx_index = ind[:,:int(seq*ctx_mask_scale)] # ctx hug bottom\n",
        "    return ctx_index, trg_index\n",
        "\n",
        "\n",
        "# mask = torch.zeros(1 ,200)\n",
        "# mask[:, trg_index[:1]] = 1\n",
        "# mask[:, ctx_index[:1]] = .5\n",
        "# plt.rcParams[\"figure.figsize\"] = (20,1)\n",
        "# plt.pcolormesh(mask)\n",
        "# plt.show()\n",
        "\n",
        "def simplexmask1d(seq=512, ctx_scale=(.85,1), trg_scale=(.6,.8), B=64, chaos=[1,.5]):\n",
        "    i = np.linspace(0, chaos[0], num=seq) # 2-5\n",
        "    noise = opensimplex.noise2array(i, np.random.randint(1e10, size=B)) # [B, seq]\n",
        "    noise = torch.from_numpy(noise)\n",
        "    ctx_mask_scale = torch.rand(1) * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    trg_mask_scale = torch.rand(1) * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "\n",
        "    ctx_len, trg_len = int(seq*ctx_mask_scale), int(seq*trg_mask_scale)\n",
        "    val, trg_index = torch.topk(noise, trg_len, dim=1, sorted=False)\n",
        "    ctx_len = ctx_len - trg_len\n",
        "\n",
        "    remove_mask = torch.ones((B,seq), dtype=bool) # [B, S]\n",
        "    remove_mask.scatter_(1, trg_index, False).flatten()\n",
        "    ind = torch.arange(seq).unsqueeze(0).repeat(B,1)[remove_mask].reshape(B, -1)\n",
        "    print(ind.shape, ind)\n",
        "\n",
        "    i = np.linspace(0, chaos[1], num=seq) # 2-5\n",
        "    noise = opensimplex.noise2array(i, np.random.randint(1e10, size=B)) # [B, seq]\n",
        "    noise = torch.from_numpy(noise)[remove_mask].reshape(B, -1)\n",
        "    val, ctx_ind = torch.topk(noise, ctx_len, dim=1, sorted=False)\n",
        "    print(ctx_ind.shape, ctx_ind)\n",
        "\n",
        "\n",
        "    ctx_index = ind[torch.arange(B).unsqueeze(-1), ctx_ind]\n",
        "    return ctx_index, trg_index\n",
        "\n",
        "\n",
        "# mask = simplexmask(hw=(8,8), scale=(.6,.8))\n",
        "# index, mask = simplexmask1d(seq=100, scale=(.7,.8))\n",
        "# ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.85,1), trg_scale=(.6,.8), B=64, chaos=5)\n",
        "# ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.85,1), trg_scale=(.7,.8), B=64, chaos=2)\n",
        "# ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.7,.9), trg_scale=(.6,.7), B=64, chaos=5)\n",
        "# ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.2,.3), trg_scale=(.4,.5), B=64, chaos=3)\n",
        "# ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.1,.3), trg_scale=(.4,.6), B=64, chaos=3)\n",
        "# # print(trg_index[0], ctx_index[0])\n",
        "\n",
        "ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.85,1), trg_scale=(.7,.8), B=64, chaos=[3,.5])\n",
        "mask = torch.zeros(b ,200)\n",
        "mask[torch.arange(b).unsqueeze(-1), trg_index] = 1\n",
        "mask[torch.arange(b).unsqueeze(-1), ctx_index] = .5\n",
        "mask = mask[None,...]\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "imshow(mask)\n",
        "import torchvision\n",
        "imshow(torchvision.utils.make_grid(mask, nrow=8))\n",
        "\n",
        "# print(index)\n",
        "# print(index.shape)\n",
        "# print(mask)\n",
        "# print(mask.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XwpnHW4wn9S1"
      },
      "outputs": [],
      "source": [
        "# @title data\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "transform = transforms.Compose([transforms.ToTensor(),])\n",
        "\n",
        "# MNIST CIFAR10\n",
        "train_data = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
        "test_data = torchvision.datasets.CIFAR10(root='data', train=False, download=True, transform=transform)\n",
        "batch_size = 128 # 128 1024\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    # print(npimg.shape) # (3, 64, 64)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# dataiter = iter(train_data)\n",
        "# x,y = next(dataiter)\n",
        "# print(x.shape) # [3, 32, 32]\n",
        "# imshow(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AtPUcVu2kSLI"
      },
      "outputs": [],
      "source": [
        "# @title TransformerClassifier\n",
        "import math\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class TransformerClassifier(nn.Module):\n",
        "    # def __init__(self, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, drop = 0.):\n",
        "    def __init__(self, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, drop=0):\n",
        "        super().__init__()\n",
        "        # self.embedding = nn.Embedding(ntoken, d_model)\n",
        "        self.pos_encoder = RoPE(d_model, seq_len=200, base=10000)\n",
        "        # d_hid = d_hid or d_model#*2\n",
        "        # encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, d_hid, drop, batch_first=True)\n",
        "        # self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "\n",
        "        out_dim = out_dim or d_model\n",
        "        self.lin = nn.Linear(d_model, out_dim)\n",
        "        # self.lin = nn.Linear(d_model*2, out_dim)\n",
        "        # self.cls = nn.Parameter(torch.randn(1,1,d_model))\n",
        "        self.attention_pool = nn.Linear(d_model, 1, bias=False)\n",
        "        # self.out = nn.Sequential(\n",
        "        #     nn.Dropout(drop), nn.Linear(d_model, 3), nn.SiLU(),\n",
        "        #     nn.Dropout(drop), nn.Linear(3, out_dim), nn.Sigmoid()\n",
        "        # )\n",
        "\n",
        "    def forward(self, x, src_key_padding_mask = None): # [batch, seq, d_model], [batch, seq] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # batch, seq_len, d_model = x.shape\n",
        "        # x = torch.cat([self.cls.repeat(batch,1,1), x], dim=1)\n",
        "        # src_key_padding_mask = torch.cat([torch.zeros((batch, 1), dtype=torch.bool), src_key_padding_mask], dim=1)\n",
        "        x = self.pos_encoder(x)\n",
        "\n",
        "        x = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
        "        # print(\"fwd\",out.shape) # float [batch, seq_len, d_model]\n",
        "        # out = x.mean(dim=1) # average pool\n",
        "        # out = x.max(dim=1)#[0]\n",
        "\n",
        "        attn = self.attention_pool(x).squeeze(-1) # [batch, seq] # seq_pool\n",
        "        out = (torch.softmax(attn, dim=-1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, seq] @ [batch, seq, dim] -> [batch, dim]\n",
        "        # out = self.out(out) # optional (from GlobalContext) like squeeze excitation\n",
        "\n",
        "        # out = out[:, 0] # first token\n",
        "        # out = torch.cat([out, mean_pool], dim=-1)\n",
        "\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "batch, seq_len, d_model = 4,7,512\n",
        "model = TransformerModel(in_dim, d_model, d_head=4, nlayers=2, dropout=0.).to(device)\n",
        "# model = TransformerClassifier(d_model, nhead=8, nlayers=2, dropout=0.).to(device)\n",
        "x =  torch.rand(batch, seq_len, d_model)\n",
        "src_key_padding_mask = torch.stack([(torch.arange(seq_len) < seq_len - v) for v in torch.randint(seq_len, (batch,))]) # True will be ignored # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "print(src_key_padding_mask)\n",
        "out = model(x, src_key_padding_mask)\n",
        "print(out.shape)\n",
        "\n",
        "# GlobalAveragePooling1D layer, followed by a Dense layer. The final output of the transformer is produced by a softmax layer,\n",
        "# x = GlobalAveragePooling1D()(x)\n",
        "# x = Dropout(0.1)(x)\n",
        "# x = Dense(20, activation=\"relu\")(x)\n",
        "# x = Dropout(0.1)(x)\n",
        "# outputs = Dense(2, activation=\"softmax\")(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-8i1WLsvH_vn"
      },
      "outputs": [],
      "source": [
        "# @title Transformer Model\n",
        "import torch\n",
        "from torch import nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        # self.embed = nn.Sequential(\n",
        "        #     nn.Linear(in_dim, d_model), #act,\n",
        "        #     # nn.Linear(d_model, d_model), act,\n",
        "        # )\n",
        "        self.embed = nn.Linear(in_dim, d_model) if in_dim != d_model else None\n",
        "        # self.embed = nn.Sequential(nn.Conv1d(in_dim,d_model,7,2,7//2), nn.MaxPool1d(3, 2, 3//2))\n",
        "        self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, d_hid or d_model, dropout, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.d_model = d_model\n",
        "        self.cls = nn.Parameter(torch.zeros(1,1,d_model)) # randn\n",
        "        out_dim = out_dim or d_model\n",
        "        # self.lin = nn.Linear(d_model, out_dim)\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim != d_model else None\n",
        "        self.norm = nn.LayerNorm(out_dim)\n",
        "\n",
        "\n",
        "    def forward(self, src, src_key_padding_mask=None, cls_mask=None, context_indices=None, trg_indices=None): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # if cls_mask != None: src[cls_mask] = self.cls.to(src.dtype)\n",
        "\n",
        "        if self.embed != None: src = self.embed(src) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = src.shape\n",
        "        # src = self.pos_encoder(src)\n",
        "        if context_indices != None:\n",
        "            # print(src.shape, context_indices.shape, self.pos_encoder(context_indices).shape) # [2, 88, 32]) torch.Size([88]) torch.Size([88, 32]\n",
        "            # print(src[0], self.pos_encoder(context_indices)[0])\n",
        "            src = src * self.pos_encoder(context_indices) # context/predictor # src = src + self.positional_emb[:,context_indices]\n",
        "            # print(src[0])\n",
        "        else: src = src * self.pos_encoder(torch.arange(seq, device=device)) # target # src = src + self.positional_emb[:,:seq]\n",
        "            # print(\"trans fwd\", src.shape, self.pos_encoder(src).shape)\n",
        "\n",
        "        if trg_indices != None: # [M, num_trg_toks]\n",
        "            pred_tokens = self.cls * self.pos_encoder(trg_indices) # [M, num_trg_toks, d_model] # pred_tokens = self.cls + self.positional_emb[0,trg_indices]\n",
        "            pred_tokens = pred_tokens.repeat(batch, 1, 1) # [batch*M, num_trg_toks, d_model]\n",
        "            # print(pred_tokens.requires_grad)\n",
        "            src = src.repeat_interleave(trg_indices.shape[0], dim=0) # [batch, seq_len, d_model] -> [batch*M, seq_len, d_model]\n",
        "            src = torch.cat([src, pred_tokens], dim=1) # [batch*M, seq_len+num_trg_toks, d_model]\n",
        "\n",
        "        out = self.transformer_encoder(src, src_key_padding_mask=src_key_padding_mask) # float [seq_len, batch_size, d_model]\n",
        "        if trg_indices != None:\n",
        "            # print(out.shape)\n",
        "            out = out[:,seq:] # [batch*M, num_trg_toks, d_model]\n",
        "        if self.lin != None: out = self.lin(out)\n",
        "        out = self.norm(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "batch, seq_len, d_model = 4,7,64\n",
        "in_dim = 3\n",
        "model = TransformerModel(in_dim, d_model, nhead=8, nlayers=2, dropout=0.).to(device)\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "# print(out.shape)\n",
        "# # print(out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## opportunity"
      ],
      "metadata": {
        "id": "670p_tNSKg5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title download opportunity\n",
        "!wget https://archive.ics.uci.edu/static/public/226/opportunity+activity+recognition.zip -O opportunity.zip\n",
        "!unzip opportunity.zip"
      ],
      "metadata": {
        "id": "Sc9DGN_MAziB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# path = '/content/OpportunityUCIDataset/dataset/'\n",
        "path = '/content/OpportunityUCIDataset/dataset/S1-ADL1.dat'\n",
        "path = '/content/OpportunityUCIDataset/dataset/S1-Drill.dat'\n",
        "# df = pd.read_table(path+file, header=None, sep='\\s+')\n",
        "df = pd.read_table(path, header=None, sep='\\s+')\n",
        "print(df)\n",
        "\n",
        "# 51116 rows x 250\n",
        "# 54966 rows x 250\n"
      ],
      "metadata": {
        "id": "LZK6WCoHmyEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title opportunity me\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# https://github.com/IliesChibane/Exploring-the-OPPORTUNITY-Dataset-for-Activity-Recognition/blob/main/preprocess.ipynb\n",
        "\n",
        "# columns = []\n",
        "# with open('/content/OpportunityUCIDataset/dataset/column_names.txt') as f:\n",
        "#     for line in f.read().splitlines():\n",
        "#         print(line)\n",
        "#         # if 'Column' in line: columns.append(line[get_index(line):].split(\";\")[0])\n",
        "\n",
        "# dataset = pd.DataFrame(columns=columns)\n",
        "# path = '/content/OpportunityUCIDataset/dataset/'\n",
        "# for file in os.listdir(path):\n",
        "#     print(file)\n",
        "#     if not file.endswith('.dat'): continue\n",
        "# # for file, subject_id in zip(file_names, subject_id):\n",
        "#     df = pd.read_table(path+file, header=None, sep='\\s+')\n",
        "#     # df.columns = all_columns\n",
        "#     df['subject'] = file[:2]\n",
        "#     dataset = pd.concat([dataset, df], ignore_index=True)\n",
        "\n",
        "\n",
        "# 1: ms, 2-243: data, 244-250: labels\n",
        "\n",
        "\n",
        "# ans = [y for _, y in dataset.groupby(list(dataset)[244-1:250])] # cols 244-250\n",
        "ans = [y for _, y in dataset.groupby(list(dataset)[244-1])] # cols 244-250\n",
        "y=list(set([df['subject'].iloc[0] for df in ans]))\n",
        "y.sort()\n",
        "\n",
        "# dataset_size = len(train_data)\n",
        "# indices = list(range(dataset_size))\n",
        "# split = int(np.floor(0.7 * dataset_size))\n",
        "# np.random.seed(0)\n",
        "# np.random.shuffle(indices)\n",
        "# train_indices, val_indices = indices[:split], indices[split:]\n",
        "\n",
        "df_train = dataset[dataset['subject'].isin(y[:int(.7*len(y))])]\n",
        "df_test = dataset[dataset['subject'].isin(y[-int(.3*len(y)):])]\n",
        "\n",
        "def make_Xy(dataset):\n",
        "    # ans = [y for _, y in dataset.groupby(['subject', 'activity'])]\n",
        "    # ans = [y for _, y in dataset.groupby(list(dataset)[244-1:250])] # cols 244-250\n",
        "    ans = [y for _, y in dataset.groupby(list(dataset)[244-1])] # cols 244-250\n",
        "    y_train = [df[list(dataset)[244]].iloc[0] for df in ans]\n",
        "    # y_train = [df['subject'].iloc[0] for df in ans]\n",
        "    # X_train = [df.drop(['subject', 'activity','time'], axis=1) for df in X_train]\n",
        "    X_train = [df.drop(list(dataset)[244-1:250], axis=1) for df in ans]\n",
        "    X_train = [df.apply(pd.to_numeric, errors='coerce') for df in X_train] # Convert non-numeric data in dataset to numeric. errors='coerce': replace all non-numeric values with NaN.\n",
        "    X_train = [df.interpolate(method='index', axis=0, limit_direction='both') for df in X_train] # replace NaN by interpolating\n",
        "    # X_train = [df.interpolate(method='index', axis=0, limit_direction='both') for df in ans]\n",
        "    return X_train, y_train\n",
        "\n",
        "X_train, y_train = make_Xy(df_train)\n",
        "X_test, y_test = make_Xy(df_test)\n",
        "\n",
        "\n",
        "# data_dir = 'OpportunityUCIDataset/dataset'\n",
        "# files = os.listdir(data_dir)\n",
        "# files = [f for f in files if f.endswith('.dat')]\n",
        "# # Separate the ADL and Drill files\n",
        "# # ADL (Activities of Daily Living): naturalistic, semi-structured routines like making coffee, opening doors, etc.\n",
        "# # Drill: Controlled executions of specific gestures (e.g. open door with right arm) — highly structured, for training gesture classifiers.\n",
        "# dataset = pd.DataFrame(columns=columns)\n",
        "# list_of_files = [f for f in files if 'Drill' not in f]\n",
        "# for _, file in enumerate(list_of_files):\n",
        "#     proc_data = pd.read_table(os.path.join(data_dir, file), header=None, sep='\\s+')\n",
        "#     proc_data.columns = columns\n",
        "#     data_collection = pd.concat([data_collection, proc_data])\n",
        "# data_collection.reset_index(drop=True, inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ans = [y for _, y in dataset.groupby(['subject', 'activityID'])]\n",
        "# y_train = [df['activityID'].iloc[0] for df in ans]\n",
        "# # y_train = [df['subject'].iloc[0] for df in ans]\n",
        "# X_train = [df.drop(['subject', 'activityID'], axis=1) for df in ans]\n",
        "\n",
        "# dataset = dataset.apply(pd.to_numeric, errors='coerce') # Convert non-numeric data in dataset to numeric. errors='coerce': replace all non-numeric values with NaN.\n",
        "# X_train = [df.interpolate(method='index', axis=0, limit_direction='both') for df in dataset]\n",
        "\n",
        "# data.reset_index(drop=True, inplace=True) # make row ind start from 0\n"
      ],
      "metadata": {
        "id": "hJTKRUBGH8lK",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(dataset[244])\n",
        "# print(type(X_train[0]))\n",
        "# for x in X_train:\n",
        "#     print(len(x))\n",
        "# print(ans)\n",
        "# print(len(ans))\n",
        "# print(y)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# plt.plot(X_train[0].iloc[:,0])\n",
        "# plt.plot(dataset.iloc[:,0])\n",
        "# plt.plot(dataset.iloc[0])\n",
        "# for column in dataset.columns[:1]:#+dataset.columns[243:250]:\n",
        "#     dataset[column] = dataset[column] / dataset[column].abs().max()\n",
        "# for i in range(243,250):\n",
        "#     plt.plot(dataset[i], alpha=.3)\n",
        "plt.plot(dataset[243])\n",
        "# # plt.plot(dataset[243])\n",
        "plt.plot(dataset[0])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TpKbecHx8i7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ans = [y for _, y in dataset.groupby(dataset[dataset.columns[list(range(244,250+1))]])]\n",
        "ans = [y for _, y in dataset.groupby(list(dataset)[244-1:250])]\n",
        "# print(list(dataset)[244-1:250+1-1])"
      ],
      "metadata": {
        "id": "67OgkHfj364i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title teco-kit dataloader_opportunity_har.py\n",
        "# https://github.com/teco-kit/timeseries-datasets/blob/main/dataloaders/dataloader_opportunity_har.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "# from dataloaders.dataloader_base import BASE_DATA\n",
        "# TODO the cols ! name\n",
        "# ========================================      Opportunity_HAR_DATA       =============================\n",
        "# class Opportunity_HAR_DATA(BASE_DATA):\n",
        "class Opportunity_HAR_DATA():\n",
        "    \"\"\"\n",
        "    OPPORTUNITY Dataset for Human Activity Recognition from Wearable, Object, and Ambient Sensors\n",
        "    Brief Description of the Dataset:\n",
        "    ---------------------------------\n",
        "    Each .dat file contains a matrix of data in text format.\n",
        "    Each line contains the sensor data sampled at a given time (sample rate: 30Hz).\n",
        "    For more detail . please reffer to the docomentation.html\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        # In this documents in doc/documentation.html, all columns definition coulde be found   (or in the column_names)\n",
        "        # the sensors between 134 and 248 are amounted on devices, so they will not to be considered\n",
        "        # Specifically, the following columns were used for the challenge:\n",
        "        # =============================================================\n",
        "        # 1-37, 38-46, 51-59, 64-72, 77-85, 90-98, 103-134, 244, 250.\n",
        "        # 0 milisconds\n",
        "        self.used_cols = [#1,  2,   3, # Accelerometer RKN^\n",
        "                          #4,  5,   6, # Accelerometer HIP\n",
        "                          #7,  8,   9, # Accelerometer LUA^\n",
        "                          #10, 11,  12, # Accelerometer RUA_\n",
        "                          #13, 14,  15, # Accelerometer LH\n",
        "                          #16, 17,  18, # Accelerometer BACK\n",
        "                          #19, 20,  21, # Accelerometer RKN_\n",
        "                          #22, 23,  24, # Accelerometer RWR\n",
        "                          #25, 26,  27, # Accelerometer RUA^\n",
        "                          #28, 29,  30, # Accelerometer LUA_\n",
        "                          #31, 32,  33, # Accelerometer LWR\n",
        "                          #34, 35,  36, # Accelerometer RH\n",
        "                          37, 38,  39, 40, 41, 42, 43, 44, 45, # InertialMeasurementUnit BACK\n",
        "                          50, 51,  52, 53, 54, 55, 56, 57, 58, # InertialMeasurementUnit RUA\n",
        "                          63, 64,  65, 66, 67, 68, 69, 70, 71, # InertialMeasurementUnit RLA\n",
        "                          76, 77,  78, 79, 80, 81, 82, 83, 84, # InertialMeasurementUnit LUA\n",
        "                          89, 90,  91, 92, 93, 94, 95, 96, 97,  # InertialMeasurementUnit LLA\n",
        "                          102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, # InertialMeasurementUnit L-SHOE\n",
        "                          118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, # InertialMeasurementUnit R-SHOE\n",
        "                          249  # Label\n",
        "                         ]\n",
        "\n",
        "        col_names         = [\"dim_{}\".format(i) for i in range(len(self.used_cols)-1)]\n",
        "        self.col_names    =  col_names + [\"activity_id\"]\n",
        "\n",
        "        self.label_map = [(0,      'Other'),\n",
        "                          (406516, 'Open Door 1'),\n",
        "                          (406517, 'Open Door 2'),\n",
        "                          (404516, 'Close Door 1'),\n",
        "                          (404517, 'Close Door 2'),\n",
        "                          (406520, 'Open Fridge'),\n",
        "                          (404520, 'Close Fridge'),\n",
        "                          (406505, 'Open Dishwasher'),\n",
        "                          (404505, 'Close Dishwasher'),\n",
        "                          (406519, 'Open Drawer 1'),\n",
        "                          (404519, 'Close Drawer 1'),\n",
        "                          (406511, 'Open Drawer 2'),\n",
        "                          (404511, 'Close Drawer 2'),\n",
        "                          (406508, 'Open Drawer 3'),\n",
        "                          (404508, 'Close Drawer 3'),\n",
        "                          (408512, 'Clean Table'),\n",
        "                          (407521, 'Drink from Cup'),\n",
        "                          (405506, 'Toggle Switch')]\n",
        "        self.drop_activities = []\n",
        "        self.train_keys   = [11,12,13,14,15,16,\n",
        "                             21,22,23,      26,\n",
        "                             31,32,33,      36,\n",
        "                             41,42,43,44,45,46]\n",
        "        # 'S1-ADL1.dat', 'S1-ADL2.dat', 'S1-ADL3.dat', 'S1-ADL4.dat',  'S1-ADL5.dat', 'S1-Drill.dat', # subject 1\n",
        "        # 'S2-ADL1.dat', 'S2-ADL2.dat', 'S2-ADL3.dat',                                'S2-Drill.dat', # subject 2\n",
        "        # 'S3-ADL1.dat', 'S3-ADL2.dat', 'S3-ADL3.dat',                                'S3-Drill.dat'  # subject 3\n",
        "        # 'S4-ADL1.dat', 'S4-ADL2.dat', 'S4-ADL3.dat', 'S4-ADL4.dat'   'S4-ADL5.dat', 'S4-Drill.dat'] # subject 4\n",
        "        self.vali_keys    = [ ]\n",
        "        # 'S2-ADL4.dat', 'S2-ADL5.dat','S3-ADL4.dat', 'S3-ADL5.dat'\n",
        "        self.test_keys    = [24,25,34,35]\n",
        "\n",
        "        self.LOCV_keys = [[1],[2],[3],[4]]\n",
        "        self.all_keys = [1,2,3,4]\n",
        "        self.sub_ids_of_each_sub = {}\n",
        "\n",
        "        self.file_encoding = {'S1-ADL1.dat':11, 'S1-ADL2.dat':12, 'S1-ADL3.dat':13, 'S1-ADL4.dat':14, 'S1-ADL5.dat':15, 'S1-Drill.dat':16,\n",
        "                              'S2-ADL1.dat':21, 'S2-ADL2.dat':22, 'S2-ADL3.dat':23, 'S2-ADL4.dat':24, 'S2-ADL5.dat':25, 'S2-Drill.dat':26,\n",
        "                              'S3-ADL1.dat':31, 'S3-ADL2.dat':32, 'S3-ADL3.dat':33, 'S3-ADL4.dat':34, 'S3-ADL5.dat':35, 'S3-Drill.dat':36,\n",
        "                              'S4-ADL1.dat':41, 'S4-ADL2.dat':42, 'S4-ADL3.dat':43, 'S4-ADL4.dat':44, 'S4-ADL5.dat':45, 'S4-Drill.dat':46}\n",
        "\n",
        "        self.labelToId = {int(x[0]): i for i, x in enumerate(self.label_map)}\n",
        "        self.all_labels = list(range(len(self.label_map)))\n",
        "        self.drop_activities = [self.labelToId[i] for i in self.drop_activities]\n",
        "        self.no_drop_activites = [item for item in self.all_labels if item not in self.drop_activities]\n",
        "        super().__init__()\n",
        "\n",
        "    def load_all_the_data(self, root_path):\n",
        "        file_list = os.listdir(root_path)\n",
        "        file_list = [file for file in file_list if file[-3:]==\"dat\"] # in total , it should be 24\n",
        "        assert len(file_list) == 24\n",
        "        df_dict = {}\n",
        "        for file in file_list:\n",
        "            sub_data = pd.read_table(os.path.join(root_path,file), header=None, sep='\\s+')\n",
        "            sub_data = sub_data.iloc[:,self.used_cols]\n",
        "            sub_data.columns = self.col_names\n",
        "            # TODO check missing labels?\n",
        "            sub_data = sub_data.interpolate(method='linear', limit_direction='both')\n",
        "            sub = int(file[1]) # subject number\n",
        "            sub_data['sub_id'] = self.file_encoding[file] # file index\n",
        "            sub_data[\"sub\"] = sub\n",
        "            if sub not in self.sub_ids_of_each_sub.keys():\n",
        "                self.sub_ids_of_each_sub[sub] = []\n",
        "            self.sub_ids_of_each_sub[sub].append(self.file_encoding[file])\n",
        "            df_dict[self.file_encoding[file]] = sub_data\n",
        "\n",
        "        # all data\n",
        "        df_all = pd.concat(df_dict)\n",
        "        df_all = df_all.set_index('sub_id')\n",
        "        # reorder the columns as sensor1, sensor2... sensorn, sub, activity_id\n",
        "        df_all = df_all[self.col_names[:-1]+[\"sub\"]+[\"activity_id\"]]\n",
        "        # label transformation\n",
        "        df_all[\"activity_id\"] = df_all[\"activity_id\"].map(self.labelToId)\n",
        "        data_y = df_all.iloc[:,-1]\n",
        "        data_x = df_all.iloc[:,:-1]\n",
        "        data_x = data_x.reset_index()\n",
        "        # sub_id, sensor1, sensor2... sensorn, sub,\n",
        "        return data_x, data_y\n",
        "\n",
        "\n",
        "# p=Opportunity_HAR_DATA()\n",
        "data_x, data_y = p.load_all_the_data(path)\n"
      ],
      "metadata": {
        "id": "gAy7dHuWjZ4J",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(data_x)\n",
        "print(data_y)\n",
        "plt.plot(data_y)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WhkCi3AUleWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open('/content/OpportunityUCIDataset/dataset/column_names.txt') as f:\n",
        "#     # columns = [line.split()[1] for line in f.readlines()]\n",
        "#     print(f.readlines())\n"
      ],
      "metadata": {
        "id": "5fYXxQ-Sgh8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(df)\n",
        "print(dataset)\n",
        "# print([len(df) for df in X_train])\n",
        "# for df in ans:\n",
        "#     print(df['activityID'].iloc[0], df['subject'].iloc[0], len(df))\n",
        "# X_train = [df.drop(['subject', 'activityID'], axis=1) for df in ans]\n",
        "\n",
        "\n",
        "# print(dataset)\n",
        "# print(X_train[1])\n",
        "# print(X_train[1].reset_index(drop=True))\n",
        "# print(X_train[0].interpolate(method='index', axis=0, limit_direction='both'))\n",
        "# print(X_train[0].interpolate(method='index', axis=0, limit_direction='both').isnull().sum())\n",
        "# print(X_train[0].isnull().sum())\n",
        "# print(y_train)"
      ],
      "metadata": {
        "id": "gaujN3mXNuDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Navidfoumani/Series2Vec opportunity loader\n",
        "# https://github.com/Navidfoumani/Series2Vec/blob/main/Dataset/Benchmarks/Opportunity/Opportunity_Loader.py\n",
        "import os\n",
        "import logging\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "import csv\n",
        "import numpy as np\n",
        "from pandas import Series\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "# Main function for downloading and processing the Opportunity Datasets\n",
        "# Returns the train and test sets\n",
        "\n",
        "# Hardcoded number of sensor channels employed in the OPPORTUNITY challenge\n",
        "NB_SENSOR_CHANNELS = 113\n",
        "test_files = ['S2-ADL4.dat', 'S2-ADL5.dat', 'S3-ADL4.dat', 'S3-ADL5.dat']\n",
        "NORM_MAX_THRESHOLDS = [3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,\n",
        "                       3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,\n",
        "                       3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,\n",
        "                       3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,\n",
        "                       3000,   3000,   3000,   10000,  10000,  10000,  1500,   1500,   1500,\n",
        "                       3000,   3000,   3000,   10000,  10000,  10000,  1500,   1500,   1500,\n",
        "                       3000,   3000,   3000,   10000,  10000,  10000,  1500,   1500,   1500,\n",
        "                       3000,   3000,   3000,   10000,  10000,  10000,  1500,   1500,   1500,\n",
        "                       3000,   3000,   3000,   10000,  10000,  10000,  1500,   1500,   1500,\n",
        "                       250,    25,     200,    5000,   5000,   5000,   5000,   5000,   5000,\n",
        "                       10000,  10000,  10000,  10000,  10000,  10000,  250,    250,    25,\n",
        "                       200,    5000,   5000,   5000,   5000,   5000,   5000,   10000,  10000,\n",
        "                       10000,  10000,  10000,  10000,  250, ]\n",
        "\n",
        "NORM_MIN_THRESHOLDS = [-3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,\n",
        "                       -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,\n",
        "                       -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,\n",
        "                       -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,\n",
        "                       -3000,  -3000,  -3000,  -10000, -10000, -10000, -1000,  -1000,  -1000,\n",
        "                       -3000,  -3000,  -3000,  -10000, -10000, -10000, -1000,  -1000,  -1000,\n",
        "                       -3000,  -3000,  -3000,  -10000, -10000, -10000, -1000,  -1000,  -1000,\n",
        "                       -3000,  -3000,  -3000,  -10000, -10000, -10000, -1000,  -1000,  -1000,\n",
        "                       -3000,  -3000,  -3000,  -10000, -10000, -10000, -1000,  -1000,  -1000,\n",
        "                       -250,   -100,   -200,   -5000,  -5000,  -5000,  -5000,  -5000,  -5000,\n",
        "                       -10000, -10000, -10000, -10000, -10000, -10000, -250,   -250,   -100,\n",
        "                       -200,   -5000,  -5000,  -5000,  -5000,  -5000,  -5000,  -10000, -10000,\n",
        "                       -10000, -10000, -10000, -10000, -250, ]\n",
        "\n",
        "\n",
        "def Opportunity(window_size, step):\n",
        "    # Build data\n",
        "    Data = {}\n",
        "    # Get the current directory path\n",
        "    current_path = os.getcwd()\n",
        "    data_path = os.path.join(current_path, 'Datasets/Opportunity/Opportunity.npy')\n",
        "    if os.path.exists(data_path):\n",
        "        logger.info(\"Loading preprocessed Opportunity data ...\")\n",
        "\n",
        "        Data_npy = np.load(data_path, allow_pickle=True)\n",
        "        Data['train_data'] = Data_npy.item().get('train_data')\n",
        "        Data['train_label'] = Data_npy.item().get('train_label')\n",
        "        Data['test_data'] = Data_npy.item().get('test_data')\n",
        "        Data['test_label'] = Data_npy.item().get('test_label')\n",
        "\n",
        "        logger.info(\"{} samples will be used for training\".format(len(Data['train_label'])))\n",
        "        logger.info(\"{} samples will be used for testing\".format(len(Data['test_label'])))\n",
        "\n",
        "    else:\n",
        "        Downloader(current_path)\n",
        "        train_x, test_x, train_y, test_y = generate_data(current_path, label=\"gestures\")\n",
        "        X_train, y_train = Windowed_majority_labeling(train_x, np.int64(train_y), window_size, step)\n",
        "        X_test, y_test = Windowed_majority_labeling(test_x, np.int64(test_y), window_size, step)\n",
        "\n",
        "        logger.info(\"{} samples will be used for training\".format(len(y_train)))\n",
        "        logger.info(\"{} samples will be used for testing\".format(len(y_test)))\n",
        "\n",
        "        Data['train_data'] = X_train\n",
        "        Data['train_label'] = y_train\n",
        "        Data['test_data'] = X_test\n",
        "        Data['test_label'] = y_test\n",
        "\n",
        "        if not os.path.exists(current_path +'/Datasets/Opportunity/'):\n",
        "            os.makedirs(current_path + '/Datasets/Opportunity/')\n",
        "        np.save(current_path + '/Datasets/Opportunity/Opportunity.npy', Data, allow_pickle=True)\n",
        "    return Data\n",
        "\n",
        "\n",
        "def Downloader(current_path):\n",
        "    # Define the path to check\n",
        "    path_to_check = os.path.join(current_path, 'Opportunity/OpportunityUCIDataset')\n",
        "    # Check if the path exists\n",
        "    if not os.path.exists(path_to_check):\n",
        "        # URL to download the PAMAP2 from\n",
        "        file_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00226/OpportunityUCIDataset.zip'\n",
        "        # Send a GET request to download the file\n",
        "        response = requests.get(file_url, stream=True)\n",
        "\n",
        "        # Check if the request was successful\n",
        "        if response.status_code == 200:\n",
        "            # Create the directory if it doesn't exist\n",
        "            os.makedirs(path_to_check, exist_ok=True)\n",
        "\n",
        "            # Save the downloaded file\n",
        "            file_path = os.path.join(path_to_check, 'PAMAP2_Dataset.zip')\n",
        "            with open(file_path, 'wb') as file:\n",
        "                # Track the progress of the download\n",
        "                total_size = int(response.headers.get('content-length', 0))\n",
        "                block_size = 1024 * 1024 * 100  # 1KB\n",
        "                downloaded_size = 0\n",
        "\n",
        "                for data in response.iter_content(block_size):\n",
        "                    file.write(data)\n",
        "                    downloaded_size += len(data)\n",
        "\n",
        "                    # Calculate the download progress percentage\n",
        "                    progress = (downloaded_size / total_size) * 100\n",
        "\n",
        "                    # Print the progress message\n",
        "                    print(f'OpportunityUCIDataset Download in progress: {progress:.2f}%')\n",
        "\n",
        "            # Extract the contents of the zip file\n",
        "            with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(path_to_check)\n",
        "\n",
        "            # Remove the downloaded zip file\n",
        "            os.remove(file_path)\n",
        "\n",
        "            print('OpportunityUCIDataset Datasets downloaded and extracted successfully.')\n",
        "        else:\n",
        "            print('Failed to download the OpportunityUCIDataset please update the file_url')\n",
        "    else:\n",
        "        print('OpportunityUCIDataset Datasets Raw file already exists.')\n",
        "    return\n",
        "\n",
        "\n",
        "def read_file(current_path):\n",
        "    data = []\n",
        "    labels = []\n",
        "    label_map = [\n",
        "        (0, 'Other'),\n",
        "        (406516, 'Open Door 1'),\n",
        "        (406517, 'Open Door 2'),\n",
        "        (404516, 'Close Door 1'),\n",
        "        (404517, 'Close Door 2'),\n",
        "        (406520, 'Open Fridge'),\n",
        "        (404520, 'Close Fridge'),\n",
        "        (406505, 'Open Dishwasher'),\n",
        "        (404505, 'Close Dishwasher'),\n",
        "        (406519, 'Open Drawer 1'),\n",
        "        (404519, 'Close Drawer 1'),\n",
        "        (406511, 'Open Drawer 2'),\n",
        "        (404511, 'Close Drawer 2'),\n",
        "        (406508, 'Open Drawer 3'),\n",
        "        (404508, 'Close Drawer 3'),\n",
        "        (408512, 'Clean Table'),\n",
        "        (407521, 'Drink from Cup'),\n",
        "        (405506, 'Toggle Switch')\n",
        "    ]\n",
        "    labelToId = {str(x[0]): i for i, x in enumerate(label_map)}\n",
        "    folder_path = os.path.join(current_path, 'Opportunity/OpportunityUCIDataset/OpportunityUCIDataset/dataset')\n",
        "    cols = [38, 39, 40, 41, 42, 43, 44, 45, 46, 51, 52, 53, 54, 55, 56, 57, 58, 59, 64, 65, 66, 67, 68, 69,\n",
        "            70, 71, 72, 77, 78, 79, 80, 81, 82, 83, 84, 85, 90, 91, 92, 93, 94, 95, 96, 97, 98, 103, 104, 105,\n",
        "            106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124,\n",
        "            125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 250]\n",
        "    # Iterate over files in the folder\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.dat'):\n",
        "            # Read the file\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            with open(file_path, 'r') as f:\n",
        "                reader = csv.reader(f, delimiter=' ')\n",
        "                for line in reader:\n",
        "                    elem = []\n",
        "                    for ind in cols:\n",
        "                        elem.append(line[ind-1])\n",
        "                    if sum([x == 'NaN' for x in elem]) < 5:\n",
        "                        data.append([float(x) / 1000 for x in elem[:-1]])\n",
        "                        labels.append(labelToId[elem[-1]])\n",
        "\n",
        "    return data, labels\n",
        "\n",
        "\n",
        "def select_columns_opp(data):\n",
        "    \"\"\"Selection of the 113 columns employed in the OPPORTUNITY challenge\n",
        "\n",
        "    :param data: numpy integer matrix\n",
        "        Sensor data (all features)\n",
        "    :return: numpy integer matrix\n",
        "        Selection of features\n",
        "    \"\"\"\n",
        "\n",
        "    #                     included-excluded\n",
        "    features_delete = np.arange(46, 50)\n",
        "    features_delete = np.concatenate([features_delete, np.arange(59, 63)])\n",
        "    features_delete = np.concatenate([features_delete, np.arange(72, 76)])\n",
        "    features_delete = np.concatenate([features_delete, np.arange(85, 89)])\n",
        "    features_delete = np.concatenate([features_delete, np.arange(98, 102)])\n",
        "    features_delete = np.concatenate([features_delete, np.arange(134, 243)])\n",
        "    features_delete = np.concatenate([features_delete, np.arange(244, 249)])\n",
        "    return np.delete(data, features_delete, 1)\n",
        "\n",
        "\n",
        "def normalize(data, max_list, min_list):\n",
        "    \"\"\"Normalizes all sensor channels\n",
        "\n",
        "    :param data: numpy integer matrix\n",
        "        Sensor data\n",
        "    :param max_list: numpy integer array\n",
        "        Array containing maximums values for every one of the 113 sensor channels\n",
        "    :param min_list: numpy integer array\n",
        "        Array containing minimum values for every one of the 113 sensor channels\n",
        "    :return:\n",
        "        Normalized sensor data\n",
        "    \"\"\"\n",
        "    max_list, min_list = np.array(max_list), np.array(min_list)\n",
        "    diffs = max_list - min_list\n",
        "    for i in np.arange(data.shape[1]):\n",
        "        data[:, i] = (data[:, i]-min_list[i])/diffs[i]\n",
        "    #     Checking the boundaries\n",
        "    data[data > 1] = 0.99\n",
        "    data[data < 0] = 0.00\n",
        "    return data\n",
        "\n",
        "\n",
        "def divide_x_y(data, label):\n",
        "    \"\"\"Segments each sample into features and label\n",
        "\n",
        "    :param data: numpy integer matrix\n",
        "        Sensor data\n",
        "    :param label: string, ['gestures' (default), 'locomotion']\n",
        "        Type of activities to be recognized\n",
        "    :return: numpy integer matrix, numpy integer array\n",
        "        Features encapsulated into a matrix and labels as an array\n",
        "    \"\"\"\n",
        "\n",
        "    data_x = data[:, 1:114]\n",
        "    if label not in ['locomotion', 'gestures']:\n",
        "            raise RuntimeError(\"Invalid label: '%s'\" % label)\n",
        "    if label == 'locomotion':\n",
        "        data_y = data[:, 114]  # Locomotion label\n",
        "    elif label == 'gestures':\n",
        "        data_y = data[:, 115]  # Gestures label\n",
        "\n",
        "    return data_x, data_y\n",
        "\n",
        "\n",
        "def adjust_idx_labels(data_y, label):\n",
        "    \"\"\"Transforms original labels into the range [0, nb_labels-1]\n",
        "\n",
        "    :param data_y: numpy integer array\n",
        "        Sensor labels\n",
        "    :param label: string, ['gestures' (default), 'locomotion']\n",
        "        Type of activities to be recognized\n",
        "    :return: numpy integer array\n",
        "        Modified sensor labels\n",
        "    \"\"\"\n",
        "\n",
        "    if label == 'locomotion':  # Labels for locomotion are adjusted\n",
        "        data_y[data_y == 4] = 3\n",
        "        data_y[data_y == 5] = 4\n",
        "    elif label == 'gestures':  # Labels for gestures are adjusted\n",
        "        data_y[data_y == 406516] = 1\n",
        "        data_y[data_y == 406517] = 2\n",
        "        data_y[data_y == 404516] = 3\n",
        "        data_y[data_y == 404517] = 4\n",
        "        data_y[data_y == 406520] = 5\n",
        "        data_y[data_y == 404520] = 6\n",
        "        data_y[data_y == 406505] = 7\n",
        "        data_y[data_y == 404505] = 8\n",
        "        data_y[data_y == 406519] = 9\n",
        "        data_y[data_y == 404519] = 10\n",
        "        data_y[data_y == 406511] = 11\n",
        "        data_y[data_y == 404511] = 12\n",
        "        data_y[data_y == 406508] = 13\n",
        "        data_y[data_y == 404508] = 14\n",
        "        data_y[data_y == 408512] = 15\n",
        "        data_y[data_y == 407521] = 16\n",
        "        data_y[data_y == 405506] = 17\n",
        "    return data_y\n",
        "\n",
        "\n",
        "def process_dataset_file(data, label):\n",
        "    \"\"\"Function defined as a pipeline to process individual OPPORTUNITY files\n",
        "\n",
        "    :param data: numpy integer matrix\n",
        "        Matrix containing data samples (rows) for every sensor channel (column)\n",
        "    :param label: string, ['gestures' (default), 'locomotion']\n",
        "        Type of activities to be recognized\n",
        "    :return: numpy integer matrix, numy integer array\n",
        "        Processed sensor data, segmented into features (x) and labels (y)\n",
        "    \"\"\"\n",
        "\n",
        "    # Select correct columns\n",
        "    data = select_columns_opp(data)\n",
        "    # Colums are segmentd into features and labels\n",
        "    data_x, data_y = divide_x_y(data, label)\n",
        "    data_y = adjust_idx_labels(data_y, label)\n",
        "    data_y = data_y.astype(int)\n",
        "\n",
        "    # Perform linear interpolation\n",
        "    data_x = np.array([Series(i).interpolate() for i in data_x.T]).T\n",
        "\n",
        "    # Remaining missing data are converted to zero\n",
        "    data_x[np.isnan(data_x)] = 0\n",
        "\n",
        "    # All sensor channels are normalized\n",
        "    data_x = normalize(data_x, NORM_MAX_THRESHOLDS, NORM_MIN_THRESHOLDS)\n",
        "\n",
        "    return data_x, data_y\n",
        "\n",
        "\n",
        "def generate_data(current_path, label):\n",
        "    \"\"\"Function to read the OPPORTUNITY challenge raw data and process all sensor channels\n",
        "\n",
        "    :param current_path: string\n",
        "        Path with original OPPORTUNITY zip file\n",
        "    :param target_filename: string\n",
        "        Processed file\n",
        "    :param label: string, ['gestures' (default), 'locomotion']\n",
        "        Type of activities to be recognized. The OPPORTUNITY dataset includes several annotations to perform\n",
        "        recognition modes of locomotion/postures and recognition of sporadic gestures.\n",
        "    \"\"\"\n",
        "    train_x = np.empty((0, NB_SENSOR_CHANNELS))\n",
        "    train_y = np.empty(0)\n",
        "\n",
        "    test_x = np.empty((0, NB_SENSOR_CHANNELS))\n",
        "    test_y = np.empty(0)\n",
        "\n",
        "    folder_path = os.path.join(current_path, 'Opportunity/OpportunityUCIDataset/OpportunityUCIDataset/dataset')\n",
        "\n",
        "    # Iterate over files in the folder\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.dat'):\n",
        "            # Read the file\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            data = np.loadtxt(file_path)\n",
        "            x, y = process_dataset_file(data, label)\n",
        "            if filename in test_files:\n",
        "                test_x = np.vstack((test_x, x))\n",
        "                test_y = np.concatenate([test_y, y])\n",
        "            else:\n",
        "                train_x = np.vstack((train_x, x))\n",
        "                train_y = np.concatenate([train_y, y])\n",
        "\n",
        "    return train_x, test_x, train_y, test_y\n",
        "\n",
        "\n",
        "def Windowed_majority_labeling(values, labels, window_size, step):\n",
        "\n",
        "    # Initialize empty lists to store windowed samples and labels\n",
        "    windowed_samples = []\n",
        "    window_labels = []\n",
        "\n",
        "    for i in range(0, len(values) - window_size + 1, step):\n",
        "        # Extract the windowed sample\n",
        "        windowed_sample = values[i:i + window_size]\n",
        "\n",
        "        # Assign the majority label to the window\n",
        "        window_label = np.argmax(np.bincount(labels[i:i + window_size]))\n",
        "\n",
        "        # Append the windowed sample and label to the lists\n",
        "        windowed_samples.append(list(windowed_sample))\n",
        "        window_labels.append(window_label)\n",
        "\n",
        "    # Convert the windowed samples and labels to numpy arrays\n",
        "    windowed_samples = np.transpose(np.array(windowed_samples), (0, 2, 1))\n",
        "    window_labels = np.array(window_labels)\n",
        "    return windowed_samples, window_labels\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jnVtofwDCUmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title teco-kit opportunity loader\n",
        "# https://github.com/teco-kit/timeseries-datasets/blob/main/dataloaders/dataloader_opportunity_har.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# from dataloaders.dataloader_base import BASE_DATA\n",
        "# TODO the cols ! name\n",
        "# ========================================      Opportunity_HAR_DATA         =============================\n",
        "# class Opportunity_HAR_DATA(BASE_DATA):\n",
        "class Opportunity_HAR_DATA():\n",
        "    \"\"\"\n",
        "    OPPORTUNITY Dataset for Human Activity Recognition from Wearable, Object, and Ambient Sensors\n",
        "\n",
        "    Brief Description of the Dataset:\n",
        "    ---------------------------------\n",
        "    Each .dat file contains a matrix of data in text format.\n",
        "    Each line contains the sensor data sampled at a given time (sample rate: 30Hz).\n",
        "    For more detail . please reffer to the docomentation.html\n",
        "    \"\"\"\n",
        "    def __init__(self, args):\n",
        "\n",
        "        \"\"\"\n",
        "        root_path : Root directory of the data set\n",
        "        difference (bool) : Whether to calculate the first order derivative of the original data\n",
        "        datanorm_type (str) : Methods of data normalization: \"standardization\", \"minmax\" , \"per_sample_std\", \"per_sample_minmax\"\n",
        "\n",
        "        spectrogram (bool): Whether to convert raw data into frequency representations\n",
        "            scales : Depends on the sampling frequency of the data （sample rate: 30Hz)）\n",
        "            wavelet : Methods of wavelet transformation\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # In this documents in doc/documentation.html, all columns definition coulde be found   (or in the column_names)\n",
        "        # the sensors between 134 and 248 are amounted on devices, so they will not to be considered\n",
        "        # Specifically, the following columns were used for the challenge:\n",
        "        # =============================================================\n",
        "        # 1-37, 38-46, 51-59, 64-72, 77-85, 90-98, 103-134, 244, 250.\n",
        "        # 0 milisconds\n",
        "        self.used_cols = [#1,  2,   3, # Accelerometer RKN^\n",
        "                          #4,  5,   6, # Accelerometer HIP\n",
        "                          #7,  8,   9, # Accelerometer LUA^\n",
        "                          #10, 11,  12, # Accelerometer RUA_\n",
        "                          #13, 14,  15, # Accelerometer LH\n",
        "                          #16, 17,  18, # Accelerometer BACK\n",
        "                          #19, 20,  21, # Accelerometer RKN_\n",
        "                          #22, 23,  24, # Accelerometer RWR\n",
        "                          #25, 26,  27, # Accelerometer RUA^\n",
        "                          #28, 29,  30, # Accelerometer LUA_\n",
        "                          #31, 32,  33, # Accelerometer LWR\n",
        "                          #34, 35,  36, # Accelerometer RH\n",
        "                          37, 38,  39, 40, 41, 42, 43, 44, 45, # InertialMeasurementUnit BACK\n",
        "                          50, 51,  52, 53, 54, 55, 56, 57, 58, # InertialMeasurementUnit RUA\n",
        "                          63, 64,  65, 66, 67, 68, 69, 70, 71, # InertialMeasurementUnit RLA\n",
        "                          76, 77,  78, 79, 80, 81, 82, 83, 84, # InertialMeasurementUnit LUA\n",
        "                          89, 90,  91, 92, 93, 94, 95, 96, 97,  # InertialMeasurementUnit LLA\n",
        "                          102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, # InertialMeasurementUnit L-SHOE\n",
        "                          118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, # InertialMeasurementUnit R-SHOE\n",
        "                          249  # Label\n",
        "                         ]\n",
        "\n",
        "        col_names         = [\"dim_{}\".format(i) for i in range(len(self.used_cols)-1)]\n",
        "        self.col_names    =  col_names + [\"activity_id\"]\n",
        "\n",
        "        self.label_map = [(0,      'Other'),\n",
        "                          (406516, 'Open Door 1'),\n",
        "                          (406517, 'Open Door 2'),\n",
        "                          (404516, 'Close Door 1'),\n",
        "                          (404517, 'Close Door 2'),\n",
        "                          (406520, 'Open Fridge'),\n",
        "                          (404520, 'Close Fridge'),\n",
        "                          (406505, 'Open Dishwasher'),\n",
        "                          (404505, 'Close Dishwasher'),\n",
        "                          (406519, 'Open Drawer 1'),\n",
        "                          (404519, 'Close Drawer 1'),\n",
        "                          (406511, 'Open Drawer 2'),\n",
        "                          (404511, 'Close Drawer 2'),\n",
        "                          (406508, 'Open Drawer 3'),\n",
        "                          (404508, 'Close Drawer 3'),\n",
        "                          (408512, 'Clean Table'),\n",
        "                          (407521, 'Drink from Cup'),\n",
        "                          (405506, 'Toggle Switch')]\n",
        "\n",
        "\n",
        "        self.drop_activities = []\n",
        "\n",
        "        self.train_keys   = [11,12,13,14,15,16,\n",
        "                             21,22,23,      26,\n",
        "                             31,32,33,      36,\n",
        "                             41,42,43,44,45,46]\n",
        "        # 'S1-ADL1.dat', 'S1-ADL2.dat', 'S1-ADL3.dat', 'S1-ADL4.dat',  'S1-ADL5.dat', 'S1-Drill.dat', # subject 1\n",
        "        # 'S2-ADL1.dat', 'S2-ADL2.dat', 'S2-ADL3.dat',                                'S2-Drill.dat', # subject 2\n",
        "        # 'S3-ADL1.dat', 'S3-ADL2.dat', 'S3-ADL3.dat',                                'S3-Drill.dat'  # subject 3\n",
        "        # 'S4-ADL1.dat', 'S4-ADL2.dat', 'S4-ADL3.dat', 'S4-ADL4.dat'   'S4-ADL5.dat', 'S4-Drill.dat'] # subject 4\n",
        "        self.vali_keys    = [ ]\n",
        "        # 'S2-ADL4.dat', 'S2-ADL5.dat','S3-ADL4.dat', 'S3-ADL5.dat'\n",
        "        self.test_keys    = [24,25,34,35]\n",
        "\n",
        "        self.exp_mode     = args.exp_mode\n",
        "        if self.exp_mode == \"LOCV\":\n",
        "            self.split_tag = \"sub\"\n",
        "        else:\n",
        "            self.split_tag = \"sub_id\"\n",
        "\n",
        "\n",
        "        self.LOCV_keys = [[1],[2],[3],[4]]\n",
        "        self.all_keys = [1,2,3,4]\n",
        "        self.sub_ids_of_each_sub = {}\n",
        "\n",
        "        self.file_encoding = {'S1-ADL1.dat':11, 'S1-ADL2.dat':12, 'S1-ADL3.dat':13, 'S1-ADL4.dat':14, 'S1-ADL5.dat':15, 'S1-Drill.dat':16,\n",
        "                              'S2-ADL1.dat':21, 'S2-ADL2.dat':22, 'S2-ADL3.dat':23, 'S2-ADL4.dat':24, 'S2-ADL5.dat':25, 'S2-Drill.dat':26,\n",
        "                              'S3-ADL1.dat':31, 'S3-ADL2.dat':32, 'S3-ADL3.dat':33, 'S3-ADL4.dat':34, 'S3-ADL5.dat':35, 'S3-Drill.dat':36,\n",
        "                              'S4-ADL1.dat':41, 'S4-ADL2.dat':42, 'S4-ADL3.dat':43, 'S4-ADL4.dat':44, 'S4-ADL5.dat':45, 'S4-Drill.dat':46}\n",
        "\n",
        "        self.labelToId = {int(x[0]): i for i, x in enumerate(self.label_map)}\n",
        "        self.all_labels = list(range(len(self.label_map)))\n",
        "\n",
        "        self.drop_activities = [self.labelToId[i] for i in self.drop_activities]\n",
        "        self.no_drop_activites = [item for item in self.all_labels if item not in self.drop_activities]\n",
        "\n",
        "        # super(Opportunity_HAR_DATA, self).__init__(args)\n",
        "\n",
        "\n",
        "\n",
        "    def load_all_the_data(self, root_path):\n",
        "\n",
        "        print(\" ----------------------- load all the data -------------------\")\n",
        "\n",
        "        file_list = os.listdir(root_path)\n",
        "        file_list = [file for file in file_list if file[-3:]==\"dat\"] # in total , it should be 24\n",
        "        assert len(file_list) == 24\n",
        "\n",
        "        df_dict = {}\n",
        "\n",
        "        for file in file_list:\n",
        "            sub_data = pd.read_table(os.path.join(root_path,file), header=None, sep='\\s+')\n",
        "            sub_data =sub_data.iloc[:,self.used_cols]\n",
        "            sub_data.columns = self.col_names\n",
        "\n",
        "            # TODO check missing labels?\n",
        "            sub_data = sub_data.interpolate(method='linear', limit_direction='both')\n",
        "\n",
        "            sub = int(file[1])\n",
        "            sub_data['sub_id'] = self.file_encoding[file]\n",
        "            sub_data[\"sub\"] = sub\n",
        "\n",
        "\n",
        "            if sub not in self.sub_ids_of_each_sub.keys():\n",
        "                self.sub_ids_of_each_sub[sub] = []\n",
        "            self.sub_ids_of_each_sub[sub].append(self.file_encoding[file])\n",
        "\n",
        "            df_dict[self.file_encoding[file]] = sub_data\n",
        "\n",
        "        # all data\n",
        "        df_all = pd.concat(df_dict)\n",
        "        df_all = df_all.set_index('sub_id')\n",
        "        # reorder the columns as sensor1, sensor2... sensorn, sub, activity_id\n",
        "        df_all = df_all[self.col_names[:-1]+[\"sub\"]+[\"activity_id\"]]\n",
        "        # label transformation\n",
        "        df_all[\"activity_id\"] = df_all[\"activity_id\"].map(self.labelToId)\n",
        "        data_y = df_all.iloc[:,-1]\n",
        "        data_x = df_all.iloc[:,:-1]\n",
        "        data_x = data_x.reset_index()\n",
        "        # sub_id, sensor1, sensor2... sensorn, sub,\n",
        "        return data_x, data_y\n",
        "\n",
        "path = '/content/OpportunityUCIDataset/dataset/'\n",
        "Opportunity_HAR_DATA().load_all_the_data(path)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Eb0V4bMbJ06Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## wisdm"
      ],
      "metadata": {
        "id": "qxWJtucwKksn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://archive.ics.uci.edu/static/public/507/wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset.zip\n",
        "!unzip wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset.zip\n",
        "!unzip wisdm-dataset.zip"
      ],
      "metadata": {
        "id": "WXyZyeg585Ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FA00-tf6MJ-s"
      },
      "outputs": [],
      "source": [
        "# @title rohanpandey WISDM\n",
        "# https://github.com/rohanpandey/Analysis--WISDM-Smartphone-and-Smartwatch-Activity/blob/master/dataset_creation.ipynb\n",
        "! wget https://archive.ics.uci.edu/static/public/507/wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset.zip\n",
        "!unzip wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset.zip\n",
        "!unzip wisdm-dataset.zip\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "folder1=glob.glob(\"wisdm-dataset/raw/*\")\n",
        "# print(folder1)\n",
        "column_names = ['ID', 'activity','timestamp','x','y','z']\n",
        "overall_dataframe=pd.DataFrame(columns = column_names)\n",
        "for subfolder in folder1:\n",
        "    parent_dir = \"./processed/\"\n",
        "    path = os.path.join(parent_dir, subfolder.split('\\\\')[-1])\n",
        "    if not os.path.exists(path): os.makedirs(path)\n",
        "    folder2=glob.glob(subfolder+\"/*\")\n",
        "    for subsubfolder in folder2:\n",
        "        activity_dataframe = pd.DataFrame(columns = column_names)\n",
        "        subfolder_path = os.path.join(path, subsubfolder.split('/')[-1])\n",
        "        if not os.path.exists(subfolder_path): os.makedirs(subfolder_path)\n",
        "        files=glob.glob(subsubfolder+\"/*\")\n",
        "        for file in files:\n",
        "            # print(file)\n",
        "            df = pd.read_csv(file, sep=\",\",header=None)\n",
        "            df.columns = ['ID','activity','timestamp','x','y','z']\n",
        "            # activity_dataframe=activity_dataframe.append(df)\n",
        "            activity_dataframe = pd.concat([activity_dataframe, df], ignore_index=True)\n",
        "\n",
        "        activity_dataframe['z']=activity_dataframe['z'].str[:-1]\n",
        "        # activity_dataframe['meter']=subsubfolder.split('/')[-1]\n",
        "        # activity_dataframe['device']=subfolder.split('/')[-1]\n",
        "        # activity_dataframe.to_csv(subfolder_path+'/data.csv',index=False)\n",
        "        overall_dataframe = pd.concat([overall_dataframe, activity_dataframe], ignore_index=True)\n",
        "\n",
        "overall_dataframe.to_csv(\"processed/wisdm-dataset/raw/data.csv\",index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "CNVNCV8CGtR_"
      },
      "outputs": [],
      "source": [
        "# @title wisdm dataloader\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "# /content/processed/wisdm-dataset/raw/phone/accel/data.csv\n",
        "# /content/processed/wisdm-dataset/raw/watch/gyro/data.csv\n",
        "\n",
        "class BufferDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        # df_keep = pd.read_csv(\"data.csv\")#[['ID','activity','timestamp','x','y','z']]\n",
        "        df_keep = pd.read_csv(\"/content/processed/wisdm-dataset/raw/watch/accel/data.csv\")\n",
        "\n",
        "        user_acts = dict(tuple(df_keep.groupby(['ID','activity'])[['timestamp','x','y','z']]))\n",
        "        self.data = [[d.to_numpy(), a] for a, d in user_acts.items()]\n",
        "        self.act_dict = {i: act for i, act in enumerate(df_keep['activity'].unique())}\n",
        "        self.act_invdict = {v: k for k, v in self.act_dict.items()} # {'A': 0, 'B': 1, ...\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, id_act = self.data[idx]\n",
        "        id_act = self.process(id_act)\n",
        "        return torch.tensor(x[:3500]), id_act # 3567\n",
        "\n",
        "    def process(self, id_act):\n",
        "        return int(id_act[0]), self.act_invdict[id_act[1]]\n",
        "\n",
        "    def transform(self, act_list): # rand resize crop, rand mask # https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html\n",
        "        act_list = np.array(act_list)\n",
        "        try: hr, temp, heart= zip(*act_list)\n",
        "        except ValueError: print('err:',act_list)\n",
        "        kind = 'nearest' if len(act_list)>self.seq_len/.7 else 'linear'\n",
        "        temp_interpolator = scipy.interpolate.interp1d(hr, temp, kind=kind) # linear nearest quadratic cubic\n",
        "        heart_interpolator = scipy.interpolate.interp1d(hr, heart, kind=kind) # linear nearest quadratic cubic\n",
        "        hr_ = np.sort(np.random.uniform(hr[0], hr[-1], round(self.seq_len*random.uniform(1,1/.7))))\n",
        "        temp_, heart_ = temp_interpolator(hr_), heart_interpolator(hr_)\n",
        "        act_list = list(zip(hr_, temp_, heart_))\n",
        "\n",
        "        idx = torch.randint(len(act_list)-self.seq_len+1, size=(1,))\n",
        "        # return act_list[idx: idx+self.seq_len]\n",
        "        act_list = act_list[idx: idx+self.seq_len]\n",
        "\n",
        "        # mask=(torch.rand(self.seq_len)<.1) # True -> masked # random masking\n",
        "        # # act_list[mask] = self.pad[0]\n",
        "        # act_list = [self.pad[0] if m else a for m,a in zip(mask, act_list)]\n",
        "        return act_list\n",
        "\n",
        "    def add(self, episode):\n",
        "        self.data.append(episode)\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    # print(npimg.shape) # (3, 64, 64)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# df_keep = pd.read_csv(\"data.csv\")#[['ID','activity','timestamp','x','y','z']]\n",
        "# user_acts = dict(tuple(df_keep.groupby(['ID','activity'])[['timestamp','x','y','z']]))\n",
        "# dataset = [[d.to_numpy(), a] for a, d in user_acts.items()]\n",
        "\n",
        "\n",
        "train_data = BufferDataset() # one line of poem is roughly 50 characters\n",
        "\n",
        "dataset_size = len(train_data)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(0.7 * dataset_size))\n",
        "np.random.seed(0)\n",
        "np.random.shuffle(indices)\n",
        "train_indices, val_indices = indices[:split], indices[split:]\n",
        "\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "valid_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 64 #512\n",
        "# train_loader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "# test_loader = DataLoader(train_data, sampler=valid_sampler, batch_size=batch_size, num_workers=2, drop_last=True)\n",
        "train_loader = DataLoader(train_data, sampler=train_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "test_loader = DataLoader(train_data, sampler=valid_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title from WISDM_ar_v1.1_raw.txt\n",
        "# !wget https://www.cis.fordham.edu/wisdm/includes/datasets/latest/WISDM_ar_latest.tar.gz\n",
        "# !tar -xzf WISDM_ar_latest.tar.gz\n",
        "path = '/content/WISDM_ar_v1.1/WISDM_ar_v1.1_raw.txt'\n",
        "\n",
        "# processedList = []\n",
        "# # https://github.com/aakashratha1006/Human-Activity-Recognition/blob/main/human_activity_recognition.py\n",
        "# # https://github.com/lisatwyw/data-gym/blob/master/demo/WISDM.ipynb\n",
        "# for i, line in enumerate(open(path).readlines()):\n",
        "#     try:\n",
        "#         line = line.split(',')\n",
        "#         last = line[5].split(';')[0]\n",
        "#         last = last.strip()\n",
        "#         if last == '': break;\n",
        "#         temp = [line[0], line[1], line[2], line[3], line[4], last]\n",
        "#         processedList.append(temp)\n",
        "#     except: print('Error at line number: ', i, line)\n",
        "\n",
        "# with open(path, 'r') as f:\n",
        "#     for line in f:\n",
        "#         line = line.strip()\n",
        "#         individual_record_strings = [rec.strip() for rec in line.split(';') if rec.strip()]\n",
        "#         for record_str in individual_record_strings:\n",
        "#             parts = record_str.split(',')\n",
        "#             if len(parts) == 6: processedList.append(parts)\n",
        "#             else: print(line)\n",
        "\n",
        "with open(path, 'r') as f:\n",
        "    processedList = f.read().replace(' ', '').replace(',\\n', ';\\n').replace(',;', ';').replace('\\n', '').replace(';','\\n')\n",
        "processedList = [p.split(',') for p in processedList.split('\\n')]\n",
        "\n",
        "\n",
        "# df = pd.read_table(path, header=None, sep='\\s+')\n",
        "# print(df)\n",
        "# print(df.isna().sum())\n",
        "\n",
        "import pandas as pd\n",
        "columns = ['subject', 'activity','time','x','y','z']\n",
        "dataset = pd.DataFrame(data = processedList, columns = columns)\n",
        "# dataset = pd.DataFrame(data = processedList)\n",
        "\n",
        "ans = [y for _, y in dataset.groupby(['subject', 'activity'])]\n",
        "y=list(set([df['subject'].iloc[0] for df in ans]))\n",
        "y.sort()\n",
        "\n",
        "# dataset_size = len(train_data)\n",
        "# indices = list(range(dataset_size))\n",
        "# split = int(np.floor(0.7 * dataset_size))\n",
        "# np.random.seed(0)\n",
        "# np.random.shuffle(indices)\n",
        "# train_indices, val_indices = indices[:split], indices[split:]\n",
        "\n",
        "df_train = dataset[dataset['subject'].isin(y[:int(.7*len(y))])]\n",
        "df_test = dataset[dataset['subject'].isin(y[-int(.3*len(y)):])]\n",
        "\n",
        "def make_Xy(dataset):\n",
        "    ans = [y for _, y in dataset.groupby(['subject', 'activity'])]\n",
        "    y_train = [df['activity'].iloc[0] for df in ans]\n",
        "    # y_train = [df['subject'].iloc[0] for df in ans]\n",
        "    X_train = [df.drop(['subject', 'activity','time'], axis=1) for df in ans]\n",
        "    X_train = [df.apply(pd.to_numeric, errors='coerce') for df in X_train] # Convert non-numeric data in dataset to numeric. errors='coerce': replace all non-numeric values with NaN.\n",
        "    X_train = [df.interpolate(method='index', axis=0, limit_direction='both') for df in X_train] # replace NaN by interpolating\n",
        "    # X_train = [df.interpolate(method='index', axis=0, limit_direction='both') for df in ans]\n",
        "    return X_train, y_train\n",
        "\n",
        "X_train, y_train = make_Xy(df_train)\n",
        "X_test, y_test = make_Xy(df_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "SuWXahzpQVQ5",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pamap2"
      ],
      "metadata": {
        "id": "sALSYgwsKqr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://dl.acm.org/doi/pdf/10.1145/3460421.3480419\n",
        "# https://github.com/mariusbock/dl-for-har/blob/main/data_processing/preprocess_data.py\n"
      ],
      "metadata": {
        "id": "paWFwK2XXuRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "https://github.com/Navidfoumani/Series2Vec/tree/main/Dataset/Benchmarks\n",
        "# https://link.springer.com/article/10.1007/s10618-024-01043-w\n"
      ],
      "metadata": {
        "id": "U63KjrZwEoUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://archive.ics.uci.edu/dataset/231/pamap2+physical+activity+monitoring\n",
        "# https://archive.ics.uci.edu/ml/datasets/pamap2+physical+activity+monitoring\n",
        "!wget https://archive.ics.uci.edu/static/public/231/pamap2+physical+activity+monitoring.zip -O pamap2.zip\n",
        "!unzip pamap2.zip\n",
        "!unzip PAMAP2_Dataset.zip\n"
      ],
      "metadata": {
        "id": "mBMyuPWwpaHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title pamap2 dataloader\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00231/PAMAP2_Dataset.zip\n",
        "!unzip PAMAP2_Dataset.zip\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def load_pamap2(path='PAMAP2_Dataset', activities=[1,2,3,4,5,6,7,12,13,16,17,24]):\n",
        "    cols = list(range(1, 54))  # 52 features + HR\n",
        "    data = []\n",
        "    for file in os.listdir(path):\n",
        "        if file.endswith(\".dat\"):\n",
        "            df = pd.read_csv(os.path.join(path, file), sep=' ', header=None, usecols=[0] + cols)\n",
        "            df.columns = ['timestamp'] + [f'feat_{i}' for i in range(len(cols))]\n",
        "            df = df.dropna()\n",
        "            df['label'] = pd.read_csv(os.path.join(path, file), sep=' ', header=None, usecols=[0]).values\n",
        "            data.append(df)\n",
        "    full_df = pd.concat(data).dropna()\n",
        "    return full_df\n",
        "\n",
        "# df = load_pamap2()\n",
        "# df = pd.read_csv('/content/PAMAP2_Dataset/Protocol/subject101.dat', sep=' ', header=None, usecols=[0] + cols)\n",
        "df = pd.read_csv('/content/PAMAP2_Dataset/Protocol/subject101.dat', sep=' ', header=None)\n",
        "\n"
      ],
      "metadata": {
        "id": "GYmA5JWdnVdg",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(path, 'r') as f:\n",
        "    for i, line in enumerate(f.read().split('\\n')):\n",
        "        print(line)\n",
        "        if i>3: break\n"
      ],
      "metadata": {
        "id": "V30m0354fPED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeazTakLj_Nn"
      },
      "source": [
        "## visualise mask distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37BYpacmIcw1"
      },
      "outputs": [],
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (4,4)\n",
        "# plt.plot(ttc,ttt)\n",
        "plt.scatter(ttc,ttt)\n",
        "plt.xlabel('context masks')\n",
        "plt.ylabel('target masks')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58nNtUss4YFX"
      },
      "outputs": [],
      "source": [
        "ttc=[]\n",
        "ttt=[]\n",
        "def mean(x): return sum(x)/len(x)\n",
        "\n",
        "for i in range(1000):\n",
        "    # collated_masks_enc, collated_masks_pred = mask_collator(1)\n",
        "    # context_indices, trg_indices = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "    # context_indices, trg_indices = simplexmask1d(seq=200, ctx_scale=(.8,1), trg_scale=(.2,.8), B=1, chaos=[3,.5])\n",
        "    context_indices, trg_indices = simplexmask1d(seq=200, ctx_scale=(.85,1), trg_scale=(.7,.8), B=1, chaos=[1,.5])\n",
        "    ttc.append(context_indices.shape[-1])\n",
        "    ttt.append(trg_indices.shape[-1])\n",
        "\n",
        "print(mean(ttc), mean(ttt))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"figure.figsize\"] = (4,4)\n",
        "plt.hist(ttc, bins=20, alpha=.5, label='context mask')\n",
        "plt.hist(ttt, bins=20, alpha=.5, label='target mask')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## snake, lion"
      ],
      "metadata": {
        "id": "-CbC0ghOGgzf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "n9a9OwgKjTUP"
      },
      "outputs": [],
      "source": [
        "# @title snake\n",
        "# https://github.com/Aria-K-Alethia/BigCodec/blob/main/vq/activations.py\n",
        "# https://github.com/zhenye234/X-Codec-2.0/blob/main/vq/activations.py#L62\n",
        "# Implementation adapted from https://github.com/EdwardDixon/snake under the MIT license.\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# class Snake(nn.Module):\n",
        "#     def __init__(self, in_features, alpha=1.0, alpha_logscale=False):\n",
        "#         super().__init__()\n",
        "#         # self.in_features = in_features\n",
        "#         self.alpha_logscale = alpha_logscale\n",
        "#         if self.alpha_logscale: # log scale alphas initialized to zeros\n",
        "#             self.alpha = nn.Parameter(torch.zeros(in_features) * alpha)\n",
        "#         else: # linear scale alphas initialized to ones\n",
        "#             self.alpha = nn.Parameter(torch.ones(in_features) * alpha)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         alpha = self.alpha.unsqueeze(0).unsqueeze(-1) # line up with x to [B, C, T]\n",
        "#         if self.alpha_logscale:\n",
        "#             alpha = torch.exp(alpha)\n",
        "#         x = x + (1.0 / (alpha + 1e-9)) * torch.pow(torch.sin(x * alpha), 2) # Snake ∶= x + 1/a * sin^2(ax)\n",
        "#         return x\n",
        "\n",
        "\n",
        "class Snake(nn.Module):\n",
        "    def __init__(self, dim, *args, **kwargs):\n",
        "        super().__init__()\n",
        "        # self.alpha = nn.Parameter(torch.zeros(1,dim,1)).exp() # alpha_logscale=True\n",
        "        self.alpha = nn.Parameter(torch.ones(1,dim,1)*1.) # 1.\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + (1.0 / (self.alpha + 1e-9)) * torch.pow(torch.sin(x * self.alpha), 2) # Snake ∶= x + 1/a * sin^2(ax)\n",
        "\n",
        "\n",
        "@torch.jit.script\n",
        "def snake(x, alpha): # [b,c,t], [1,c,1]\n",
        "    return x + (alpha + 1e-9).reciprocal() * torch.sin(alpha * x).pow(2) # [b,c,t]\n",
        "\n",
        "class Snake1d(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.alpha = nn.Parameter(torch.zeros(1,dim,1)).exp()\n",
        "        self.alpha = nn.Parameter(torch.ones(1,dim,1)*1.) # 1.\n",
        "\n",
        "    def forward(self, x):\n",
        "        return snake(x, self.alpha)\n",
        "\n",
        "\n",
        "class SnakeBeta(nn.Module):\n",
        "    def __init__(self, in_features, alpha=1.0, alpha_logscale=False):\n",
        "        super().__init__()\n",
        "        # self.in_features = in_features\n",
        "        self.alpha_logscale = alpha_logscale\n",
        "        if self.alpha_logscale: # log scale alphas initialized to zeros\n",
        "            self.alpha = nn.Parameter(torch.zeros(in_features) * alpha)\n",
        "            self.beta = nn.Parameter(torch.zeros(in_features) * alpha)\n",
        "        else: # linear scale alphas initialized to ones\n",
        "            self.alpha = nn.Parameter(torch.ones(in_features) * alpha)\n",
        "            self.beta = nn.Parameter(torch.ones(in_features) * alpha)\n",
        "\n",
        "    def forward(self, x): # [b,c,t]\n",
        "        alpha = self.alpha.unsqueeze(0).unsqueeze(-1) # line up with x to [B, C, T]\n",
        "        beta = self.beta.unsqueeze(0).unsqueeze(-1)\n",
        "        if self.alpha_logscale:\n",
        "            alpha = torch.exp(alpha)\n",
        "            beta = torch.exp(beta)\n",
        "        x = x + (1. / (beta + 1e-9)) * pow(torch.sin(x * alpha), 2) # SnakeBeta ∶= x + 1/b *sin^2(ax)\n",
        "        return x # [b,c,t]\n",
        "\n",
        "b,c,t = 5,16,7\n",
        "# a1 = Snake(c)\n",
        "a1 = Snake(c, alpha_logscale=True) # 70.4 µs 69.9\n",
        "# a1 = Snake1d(c) # 47.8 µs 48.3\n",
        "# a1 = SnakeBeta(256)\n",
        "x = torch.randn(b,c,t)\n",
        "# x = a1(x)\n",
        "# print(x.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "MXZJBLF3D2Cx"
      },
      "outputs": [],
      "source": [
        "# @title swwish\n",
        "@torch.jit.script\n",
        "def learntswwish(x, alpha): # [b,c,t], [1,c,1]\n",
        "    # print('alpha', alpha.shape, x.shape)\n",
        "    # alpha = alpha.exp()\n",
        "    return .5 * (1 + x - torch.cos(alpha * x)) # [b,c,t]\n",
        "    # return .5 * (1/alpha + x - torch.cos(alpha * x)/alpha) # [b,c,t]\n",
        "    # return .5 * (1/alpha + x - torch.cos(1.25*alpha * x)/alpha) # [b,c,t]\n",
        "\n",
        "class LearntSwwish(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.alpha = nn.Parameter(torch.zeros(1,dim,1))#.exp()\n",
        "        self.alpha = nn.Parameter(torch.ones(1,dim,1)*1.) # 1.\n",
        "        self.alpha = nn.Parameter(torch.zeros(dim))#.exp()\n",
        "        # self.alpha = nn.Parameter(torch.randn(dim).abs()*4)\n",
        "        self.alpha = nn.Parameter(torch.randn(dim,1)*30) #4 20\n",
        "        # self.alpha = nn.Parameter(torch.ones(1,dim)*1.) # 1.\n",
        "\n",
        "    def forward(self, x):\n",
        "        return learntswwish(x, self.alpha)\n",
        "\n",
        "class Swwish(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # return .5 * (1 + x - x.cos())\n",
        "        return .5 * (1 + x - 1.25*x.cos())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DmqUKWp03pKH"
      },
      "outputs": [],
      "source": [
        "# @title lion optim\n",
        "# https://github.com/google/automl/blob/master/lion/lion_pytorch.py\n",
        "import torch\n",
        "from torch.optim.optimizer import Optimizer\n",
        "\n",
        "class Lion(Optimizer):\n",
        "    def __init__(self, params, lr=1e-4, betas=(0.9, 0.99), weight_decay=0.0):\n",
        "        defaults = dict(lr=lr, betas=betas, weight_decay=weight_decay)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        # closure (callable, optional): A closure that reevaluates the model and returns the loss.\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                # Perform stepweight decay\n",
        "                p.data.mul_(1 - group['lr'] * group['weight_decay'])\n",
        "\n",
        "                grad = p.grad\n",
        "                state = self.state[p]\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    # Exponential moving average of gradient values\n",
        "                    state['exp_avg'] = torch.zeros_like(p)\n",
        "                exp_avg = state['exp_avg']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                # Weight update\n",
        "                update = exp_avg * beta1 + grad * (1 - beta1)\n",
        "                p.add_(update.sign_(), alpha=-group['lr'])\n",
        "                exp_avg.mul_(beta2).add_(grad, alpha= 1-beta2) # Decay the momentum running average coefficient\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rbdiTTOg5X5b"
      },
      "outputs": [],
      "source": [
        "# @title lucidrains/lion-pytorch\n",
        "# https://github.com/lucidrains/lion-pytorch/blob/main/lion_pytorch/lion_pytorch.py\n",
        "import torch\n",
        "from torch.optim.optimizer import Optimizer\n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def update_fn(p, grad, exp_avg, lr, wd, beta1, beta2):\n",
        "    # stepweight decay\n",
        "    p.data.mul_(1. - lr * wd)\n",
        "    # weight update\n",
        "    update = exp_avg.clone().mul_(beta1).add(grad, alpha = 1. - beta1).sign_()\n",
        "    p.add_(update, alpha = -lr)\n",
        "    # decay the momentum running average coefficient\n",
        "    exp_avg.mul_(beta2).add_(grad, alpha = 1. - beta2)\n",
        "\n",
        "class Lion(Optimizer):\n",
        "    def __init__(self, params, lr = 1e-4, betas = (0.9, 0.99), weight_decay=0.0, decoupled_weight_decay = False):\n",
        "        assert lr > 0.\n",
        "        assert all([0. <= beta <= 1. for beta in betas])\n",
        "        self._init_lr = lr\n",
        "        self.decoupled_wd = decoupled_weight_decay\n",
        "        defaults = dict(lr = lr, betas = betas, weight_decay = weight_decay)\n",
        "        super().__init__(params, defaults)\n",
        "        self.update_fn = update_fn\n",
        "        # from lion_pytorch.triton import update_fn as triton_update_fn # https://github.com/lucidrains/lion-pytorch/blob/main/lion_pytorch/triton.py\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure = None):\n",
        "        loss = None\n",
        "        if exists(closure):\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "        for group in self.param_groups:\n",
        "            for p in filter(lambda p: exists(p.grad), group['params']):\n",
        "                grad, lr, wd, beta1, beta2, state, decoupled_wd, init_lr = p.grad, group['lr'], group['weight_decay'], *group['betas'], self.state[p], self.decoupled_wd, self._init_lr\n",
        "                # maybe decoupled weight decay\n",
        "                if decoupled_wd:\n",
        "                    wd /= init_lr\n",
        "                # init state - exponential moving average of gradient values\n",
        "                if len(state) == 0:\n",
        "                    state['exp_avg'] = torch.zeros_like(p)\n",
        "                exp_avg = state['exp_avg']\n",
        "                self.(p, grad, exp_avg, lr, wd, beta1, beta2)\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "_3GJBPfEmAXp"
      },
      "outputs": [],
      "source": [
        "# @title StableAdamW\n",
        "# https://github.com/guojiajeremy/StableAdamW/blob/master/stableadamw.py\n",
        "# https://github.com/warner-benjamin/optimi/blob/main/optimi/stableadamw.py\n",
        "# https://github.com/pytorch/pytorch/blob/main/torch/optim/adamw.py\n",
        "\n",
        "import math\n",
        "import torch\n",
        "from torch.optim.optimizer import Optimizer\n",
        "\n",
        "\n",
        "class StableAdamW(Optimizer):\n",
        "    r\"\"\"Implements AdamW algorithm.\n",
        "\n",
        "    The original Adam algorithm was proposed in `Adam: A Method for Stochastic Optimization`_.\n",
        "    The AdamW variant was proposed in `Decoupled Weight Decay Regularization`_.\n",
        "\n",
        "    Arguments:\n",
        "        params (iterable): iterable of parameters to optimize or dicts defining\n",
        "            parameter groups\n",
        "        lr (float, optional): learning rate (default: 1e-3)\n",
        "        betas (Tuple[float, float], optional): coefficients used for computing\n",
        "            running averages of gradient and its square (default: (0.9, 0.999))\n",
        "        eps (float, optional): term added to the denominator to improve\n",
        "            numerical stability (default: 1e-8)\n",
        "        weight_decay (float, optional): weight decay coefficient (default: 1e-2)\n",
        "        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n",
        "            algorithm from the paper `On the Convergence of Adam and Beyond`_\n",
        "            (default: False)\n",
        "\n",
        "    .. _Adam\\: A Method for Stochastic Optimization:\n",
        "        https://arxiv.org/abs/1412.6980\n",
        "    .. _Decoupled Weight Decay Regularization:\n",
        "        https://arxiv.org/abs/1711.05101\n",
        "    .. _On the Convergence of Adam and Beyond:\n",
        "        https://openreview.net/forum?id=ryQu7f-RZ\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
        "                 weight_decay=1e-2, amsgrad=False, clip_threshold: float = 1.0\n",
        "                 ):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "        if not 0.0 <= weight_decay:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
        "                        weight_decay=weight_decay, amsgrad=amsgrad, clip_threshold=clip_threshold\n",
        "                        )\n",
        "        super(StableAdamW, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(StableAdamW, self).__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('amsgrad', False)\n",
        "\n",
        "    def _rms(self, tensor: torch.Tensor) -> float:\n",
        "        return tensor.norm(2) / (tensor.numel() ** 0.5)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                # Perform stepweight decay\n",
        "                p.data.mul_(1 - group['lr'] * group['weight_decay'])\n",
        "\n",
        "                # Perform optimization step\n",
        "                grad = p.grad\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
        "                amsgrad = group['amsgrad']\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    # Exponential moving average of gradient values\n",
        "                    state['exp_avg'] = torch.zeros_like(p)  # , memory_format=torch.preserve_format)\n",
        "                    # Exponential moving average of squared gradient values\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p)  # , memory_format=torch.preserve_format)\n",
        "                    if amsgrad:\n",
        "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
        "                        state['max_exp_avg_sq'] = torch.zeros_like(p)  # , memory_format=torch.preserve_format)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                if amsgrad:\n",
        "                    max_exp_avg_sq = state['max_exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                state['step'] += 1\n",
        "                bias_correction1 = 1 - beta1 ** state['step']\n",
        "                bias_correction2 = 1 - beta2 ** state['step']\n",
        "\n",
        "                # Decay the first and second moment running average coefficient\n",
        "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
        "                if amsgrad:\n",
        "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
        "                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n",
        "                    # Use the max. for normalizing running avg. of gradient\n",
        "                    denom = (max_exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n",
        "\n",
        "                else:\n",
        "                    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n",
        "\n",
        "                lr_scale = grad / denom\n",
        "                lr_scale = max(1.0, self._rms(lr_scale) / group[\"clip_threshold\"])\n",
        "                step_size = group['lr'] / bias_correction1 / (lr_scale)\n",
        "                p.data.addcdiv_(exp_avg, denom, value=-step_size)\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u99QqyJCqp_P"
      },
      "source": [
        "## violet vicreg rankme"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "m4rj4LfPuN1H"
      },
      "outputs": [],
      "source": [
        "# @title TransformerVICReg\n",
        "import torch\n",
        "from torch import nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class TransformerVICReg(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, n_heads=4, d_hid=None, nlayers=1, drop=0):\n",
        "        super().__init__()\n",
        "        act = nn.GELU()\n",
        "        # patch_size=4\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Linear(in_dim, d_model), act\n",
        "            # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(3,2, 3//2),\n",
        "            nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2),\n",
        "            # nn.Conv1d(in_dim, d_model, patch_size, patch_size), # patch\n",
        "            )\n",
        "        self.pos_enc = RoPE(d_model, seq_len=200, base=10000) # 10000\n",
        "        # self.pos_emb = nn.Parameter(torch.randn(1, 200, d_model))\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=200, base=10000).unsqueeze(0), requires_grad=False)\n",
        "\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, n_heads=n_heads) for _ in range(nlayers)])\n",
        "\n",
        "        # out_dim = out_dim or d_model\n",
        "        self.lin = nn.Linear(d_model, out_dim)\n",
        "        self.attn_pool = nn.Linear(d_model, 1, bias=False)\n",
        "\n",
        "        dim_v = d_model * 4\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(out_dim, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v, bias=False),\n",
        "            )\n",
        "\n",
        "    def forward(self, x): # [b,t,d] / [b,c,h,w]\n",
        "        # x = self.embed(x).flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c]\n",
        "        x = self.embed(x.transpose(-2,-1)).transpose(-2,-1) # [b,t,d]\n",
        "        # x = self.embed(x)\n",
        "        x = self.pos_enc(x)\n",
        "        # x = x + self.pos_emb[:,:x.shape[1]]\n",
        "\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        attn = self.attn_pool(x).squeeze(-1) # [batch, seq] # seq_pool\n",
        "        out = (torch.softmax(attn, dim=-1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, seq] @ [batch, seq, dim] -> [batch, dim]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch, ntoken]\n",
        "\n",
        "    def expand(self, x):\n",
        "        sx = self.forward(x)\n",
        "        vx = self.exp(sx)\n",
        "        return vx\n",
        "\n",
        "batch, seq_len = 4,3500\n",
        "in_dim, d_model, out_dim=16,64,16\n",
        "model = TransformerVICReg(in_dim, d_model, out_dim, n_heads=8, nlayers=1, drop=0.).to(device)\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "# x =  torch.rand((batch, in_dim, 32,32), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ObiHp-LSuRBA"
      },
      "outputs": [],
      "source": [
        "# @title Violet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class Violet(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, nlayers=2, n_heads=4):\n",
        "        super().__init__()\n",
        "        out_dim = out_dim or d_model\n",
        "        self.student = TransformerVICReg(in_dim, d_model, out_dim=out_dim, n_heads=n_heads, nlayers=nlayers, drop=0.)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "\n",
        "        # vicreg\n",
        "        self.sim_coeff=25. # 10.0 # 25.0 # λ\n",
        "        self.std_coeff=25. # 1.0 # 25.0 # µ\n",
        "        self.cov_coeff=1. # 25.0 # 1.0 # ν\n",
        "\n",
        "        # self.transform = RandomResizedCrop1d(3500, scale=(.8,1.))\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]c/ [b,c,h,w]\n",
        "        # print(x.shape)\n",
        "        # self.transform(x)\n",
        "        vx = self.student.expand(x) # [batch, num_context_toks, out_dim]\n",
        "        with torch.no_grad(): vy = self.teacher.expand(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "        loss = self.vicreg(vx, vy)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        return self.student(x)\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size, num_features = x.shape\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = self.sim_coeff * repr_loss + self.std_coeff * std_loss + self.cov_coeff * cov_loss\n",
        "        print(\"in vicreg \",self.sim_coeff * repr_loss.item() , self.std_coeff * std_loss.item() , self.cov_coeff * cov_loss.item())\n",
        "        # return loss\n",
        "        return repr_loss, std_loss, cov_loss\n",
        "\n",
        "# lr1e-3\n",
        "# n_heads=8 < 4\n",
        "\n",
        "in_dim=3\n",
        "violet = Violet(in_dim, d_model=64, out_dim=16, nlayers=1, n_heads=8).to(device)\n",
        "voptim = torch.optim.AdamW(violet.parameters(), lr=1e-3) # 1e-3?\n",
        "# voptim = torch.optim.AdamW([{'params': violet.student.transformer.parameters()},\n",
        "#     {'params': violet.student.exp.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # default 1e-2\n",
        "# voptim = torch.optim.AdamW([{'params': violet.student.exp.parameters(), 'lr': 3e-3},\n",
        "#     {'params': [p for n, p in violet.named_parameters() if 'student.exp' not in n]}], lr=1e-3, weight_decay=1e-2) # default 1e-2\n",
        "\n",
        "# print(sum(p.numel() for p in violet.parameters() if p.requires_grad)) # 27584\n",
        "\n",
        "\n",
        "x = torch.rand((2,1000,in_dim), device=device)\n",
        "loss = violet.loss(x)\n",
        "# print(out.shape)\n",
        "print(loss)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(16, 18).to(device) # torch/autograd/graph.py RuntimeError: CUDA error: device-side assert triggered CUDA kernel errors\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IACKDymhit7"
      },
      "outputs": [],
      "source": [
        "# optim.param_groups[0]['lr'] = 1e-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7zghfu_jeOQk"
      },
      "outputs": [],
      "source": [
        "# @title RankMe\n",
        "# RankMe: Assessing the Downstream Performance of Pretrained Self-Supervised Representations by Their Rank jun 2023 https://arxiv.org/pdf/2210.02885\n",
        "import torch\n",
        "\n",
        "# https://github.com/Spidartist/IJEPA_endoscopy/blob/main/src/helper.py#L22\n",
        "def RankMe(Z):\n",
        "    \"\"\"\n",
        "    Calculate the RankMe score (the higher, the better).\n",
        "    RankMe(Z) = exp(-sum_{k=1}^{min(N, K)} p_k * log(p_k)),\n",
        "    where p_k = sigma_k (Z) / ||sigma_k (Z)||_1 + epsilon\n",
        "    where sigma_k is the kth singular value of Z.\n",
        "    where Z is the matrix of embeddings (N × K)\n",
        "    \"\"\"\n",
        "    # compute the singular values of the embeddings\n",
        "    # _u, s, _vh = torch.linalg.svd(Z, full_matrices=False)  # s.shape = (min(N, K),)\n",
        "    # s = torch.linalg.svd(Z, full_matrices=False).S\n",
        "    s = torch.linalg.svdvals(Z)\n",
        "    p = s / torch.sum(s, axis=0) + 1e-7\n",
        "    return torch.exp(-torch.sum(p * torch.log(p)))\n",
        "\n",
        "# Z = torch.randn(5, 3)\n",
        "# o = RankMe(Z)\n",
        "# print(o)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Pj25OrEk14pR"
      },
      "outputs": [],
      "source": [
        "# @title LiDAR stable_ssl next\n",
        "# https://github.com/rbalestr-lab/stable-ssl/blob/main/stable_ssl/monitors.py#L106\n",
        "\n",
        "def LiDAR(embeddings, eps=1e-7, delta=1e-3):\n",
        "    embeddings = embeddings.unflatten(0, (-1,2))\n",
        "    (local_n, q, d), device = embeddings.shape, embeddings.device\n",
        "\n",
        "    class_means = embeddings.mean(dim=1) # mu_x\n",
        "    grand_mean_local = class_means.mean(dim=0) # mu\n",
        "    # print(embeddings.shape, class_means.shape, grand_mean_local.shape) # [50, 2, 64], [50, 64], [64]\n",
        "\n",
        "    # local_Sb = torch.zeros(d, d, device=device)\n",
        "    # local_Sw = torch.zeros(d, d, device=device)\n",
        "\n",
        "\n",
        "    # print(diff_b.shape, diff_w.shape) # [64,1], [64,1]\n",
        "    # print(local_Sb.shape, local_Sw.shape) # [64,64], [64,64]\n",
        "\n",
        "    diff_b = (class_means - grand_mean_local).unsqueeze(-1)\n",
        "    # print(diff_b.shape)\n",
        "    # # local_Sb = (diff_b @ diff_b.T).sum()\n",
        "    local_Sb = (diff_b @ diff_b.transpose(-2,-1)).sum(0)\n",
        "    # # print(embeddings.shape, class_means.shape)\n",
        "    diff_w = (embeddings - class_means.unsqueeze(1)).reshape(-1,d,1)\n",
        "    # # print(diff_w.shape)\n",
        "    # # local_Sw = (diff_w @ diff_w.T).sum()\n",
        "    local_Sw = (diff_w @ diff_w.transpose(-2,-1)).sum(0)\n",
        "\n",
        "    # print(local_Sb.shape, local_Sw.shape)\n",
        "    S_b = local_Sb / (local_n - 1)\n",
        "    S_w = local_Sw / (local_n * (q - 1))\n",
        "    S_w += delta * torch.eye(d, device=device)\n",
        "    # print(S_w.shape, d)\n",
        "\n",
        "    eigvals_w, eigvecs_w = torch.linalg.eigh(S_w)\n",
        "    eigvals_w = torch.clamp(eigvals_w, min=eps)\n",
        "\n",
        "    invsqrt_w = (eigvecs_w * (1.0 / torch.sqrt(eigvals_w))) @ eigvecs_w.transpose(-1, -2)\n",
        "    Sigma_lidar = invsqrt_w @ S_b @ invsqrt_w\n",
        "\n",
        "    # lam, _ = torch.linalg.eigh(Sigma_lidar)\n",
        "    lam = torch.linalg.eigh(Sigma_lidar)[0]\n",
        "    # print(lam)\n",
        "    # lam = torch.clamp(lam, min=0.0)\n",
        "\n",
        "    p = lam / lam.sum() + eps\n",
        "    # print(p)\n",
        "    # p = s / torch.sum(s, axis=0) + 1e-7\n",
        "    return torch.exp(-torch.sum(p * torch.log(p)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tqJqL5Vj2vL"
      },
      "source": [
        "## mae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nDJvKFcMGIa7"
      },
      "outputs": [],
      "source": [
        "# @title mae me enc,dec\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def random_masking(length, mask_ratio, b=64):\n",
        "    noise = torch.rand(b, length)\n",
        "    len_mask = int(length * mask_ratio)\n",
        "    _, msk_ind = torch.topk(selected_probs, k=len_mask, dim=-1, sorted=False) # val, ind -> [b,len_mask]\n",
        "    _, keep_ind = torch.topk(selected_probs, k=length-len_mask, largest=False, dim=-1, sorted=False) # val, ind -> [b,len_keep]\n",
        "    return msk_ind, keep_ind\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, n_heads=4, d_hid=None, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, 200, d_model)*.02)\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=200, base=10000), requires_grad=False)\n",
        "\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, n_heads=n_heads) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.randn(1,1,d_model)*0.02) # randn zeros\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim)# if out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = x.shape\n",
        "        # x = x * self.pos_enc(context_indices)\n",
        "        # print(\"Trans pred\",x.shape, self.pos_emb[0,context_indices].shape)\n",
        "        x = x + self.pos_emb[0,context_indices]\n",
        "        # print('pred fwd', self.pos_emb[:,context_indices].shape)\n",
        "\n",
        "        # pred_tokens = self.cls * self.pos_enc(trg_indices) # [M, num_trg_toks, d_model]\n",
        "        pred_tokens = self.cls + self.pos_emb[0,trg_indices]\n",
        "        # print(\"pred fwd\", x.shape, pred_tokens.shape)\n",
        "        x = torch.cat([x, pred_tokens], dim=1) # [batch, seq_len+num_trg_toks, d_model]\n",
        "        out = self.transformer(x)\n",
        "\n",
        "        out = self.norm(out)\n",
        "        out = out[:,seq:] # [batch, num_trg_toks, d_model]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, drop=0):\n",
        "    def __init__(self, patch_size, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0):\n",
        "        super().__init__()\n",
        "        # patch_size=32\n",
        "        self.embed = nn.Sequential(\n",
        "            # # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.MaxPool1d(2,2), #nn.MaxPool1d(3, 2, 3//2),\n",
        "            # # nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.Conv1d(d_model, d_model,3,2,3//2)\n",
        "            nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2),\n",
        "            # nn.Conv1d(in_dim, d_model, patch_size, patch_size), # like patch\n",
        "\n",
        "            # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.Dropout(drop), nn.BatchNorm1d(d_model), snake,\n",
        "            # lstm\n",
        "\n",
        "            )\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, 200, d_model)*.02)\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=200, base=10000), requires_grad=False)#.unsqueeze(0)\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, n_heads=n_heads) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "        # try: print(\"Trans fwd\",x.shape, context_indices.shape)\n",
        "        # except: print(\"Trans fwd noind\",x.shape)\n",
        "        # x = self.pos_enc(x)\n",
        "        x = x + self.pos_emb[:,:x.shape[1]]\n",
        "        if context_indices != None: x = x[torch.arange(x.shape[0]).unsqueeze(-1), context_indices] # [batch, num_context_toks, d_model]\n",
        "\n",
        "        # print(\"TransformerModel\",x.shape)\n",
        "        x = self.transformer(x)\n",
        "        out = self.norm(x)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "batch, seq_len, d_model = 4,3500,16\n",
        "in_dim = 3\n",
        "patch_size=32\n",
        "model = TransformerModel(patch_size, in_dim, d_model, n_heads=4, nlayers=10, drop=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "# # # print(out)\n",
        "# model = TransformerPredictor(in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1).to(device)\n",
        "# out = model(out)\n",
        "# print(out.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "PE3XAPkCZ2oM"
      },
      "outputs": [],
      "source": [
        "# @title MAE me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class SeqJEPA(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, nlayers=2, n_heads=4):\n",
        "        super().__init__()\n",
        "        if out_dim is None: out_dim = d_model\n",
        "        self.patch_size = 32\n",
        "        self.student = TransformerModel(self.patch_size, in_dim, d_model, out_dim=out_dim, n_heads=n_heads, nlayers=nlayers, drop=0.)\n",
        "        self.predicter = TransformerPredictor(out_dim, d_model//2, out_dim, n_heads=4, nlayers=1, drop=0.)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "        # self.transform = RandomResizedCrop1d(3500, scale=(.8,1.))\n",
        "\n",
        "    def loss(self, x): # [b,t,d]\n",
        "        b,t,d = x.shape\n",
        "        msk_ind, keep_ind = random_masking(length, mask_ratio, b=b)\n",
        "\n",
        "        sx = self.student(x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "        # print('seq_jepa loss sx',sx.shape)\n",
        "        sy_ = self.predicter(sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "        sy_ = F.layer_norm(sy_, (sy_.size(-1),))\n",
        "        with torch.no_grad():\n",
        "            sy = self.teacher(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "            sy = sy[torch.arange(sy.shape[0]).unsqueeze(-1), trg_indices] # [batch, num_context_toks, d_model] # nan bec len(trg_ind)==0 # print('loss sy',torch.isnan(sy).any())\n",
        "            sy = F.layer_norm(sy, (sy.size(-1),))\n",
        "        loss = F.mse_loss(sy, sy_)\n",
        "        return loss\n",
        "\n",
        "\n",
        "msk_ind, keep_ind\n",
        "\n",
        "        sx = self.encoder(x, context_indices=keep_ind) # [batch, num_context_toks, out_dim]\n",
        "        x_ = self.decoder(sx, context_indices=keep_ind, msk_ind) # [batch, num_context_toks, out_dim]\n",
        "\n",
        "        loss = (pred - target) ** 2\n",
        "        loss = loss.mean(dim=-1)  # [N, L], mean loss per patch\n",
        "        loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches\n",
        "\n",
        "pred, target =\n",
        "        # loss = F.mse_loss(pred, target)\n",
        "\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        sx = self.student(x)\n",
        "        out = sx.mean(dim=1)\n",
        "        return out\n",
        "\n",
        "# min_s=0.15, max_s, M\n",
        "# trg.15.2M4 C.85 1\n",
        "\n",
        "# 1e-2,1e-3 < 3e-3,1e-3\n",
        "# patch16 < patch32\n",
        "# NoPE good but sus\n",
        "\n",
        "# ctx/trg sacle min/max, num blk,\n",
        "\n",
        "\n",
        "# seq_jepa = SeqJEPA(in_dim=3, d_model=32, out_dim=16, nlayers=4, n_heads=4).to(device)#.to(torch.float)\n",
        "seq_jepa = SeqJEPA(in_dim=3, d_model=64, out_dim=16, nlayers=1, n_heads=8).to(device)#.to(torch.float)\n",
        "optim = torch.optim.AdamW(seq_jepa.parameters(), lr=1e-3) # 1e-3?\n",
        "# optim = torch.optim.AdamW([{'params': seq_jepa.student.parameters()},\n",
        "#     {'params': seq_jepa.predicter.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # default 1e-2, 5e-2\n",
        "    # {'params': seq_jepa.predicter.parameters(), 'lr': 1e-2}], lr=1e-3, weight_decay=1e-2)\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/configs/in1k_vith14_ep300.yaml\n",
        "# d_model 1024,384\n",
        "# depth 12,6/12\n",
        "# wd 5e-2 - 4e-1\n",
        "# adamw 1e-4 - 1e-3 - 1e-6\n",
        "# ema 0.996-1\n",
        "\n",
        "print(sum(p.numel() for p in seq_jepa.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.parameters())) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.predicter.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.student.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.teacher.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# d_model^2 * nlayers\n",
        "\n",
        "x = torch.rand((24, 3500, in_dim), device=device)\n",
        "out = seq_jepa.loss(x)\n",
        "print(out.shape)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(16, 18).to(device)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n",
        "# optim = torch.optim.AdamW([{'params': seq_jepa.parameters()}, {'params': classifier.parameters(), 'lr': 1e-3}], lr=1e-3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ABygeAwL5N-6"
      },
      "outputs": [],
      "source": [
        "# @title facebookresearch/mae models_mae.py\n",
        "# https://github.com/facebookresearch/mae/blob/main/models_mae.py\n",
        "from functools import partial\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from timm.models.vision_transformer import PatchEmbed, Block\n",
        "\n",
        "\n",
        "class MaskedAutoencoderViT(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3,\n",
        "                 embed_dim=1024, depth=24, num_heads=16,\n",
        "                 decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
        "                 mlp_ratio=4., norm_layer=nn.LayerNorm, norm_pix_loss=False):\n",
        "        super().__init__()\n",
        "\n",
        "        # --------------------------------------------------------------------------\n",
        "        # MAE encoder specifics\n",
        "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        # self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True, qk_scale=None, norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "        # --------------------------------------------------------------------------\n",
        "\n",
        "        # --------------------------------------------------------------------------\n",
        "        # MAE decoder specifics\n",
        "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
        "        # self.decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, decoder_embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
        "\n",
        "        self.decoder_blocks = nn.ModuleList([\n",
        "            Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=True, qk_scale=None, norm_layer=norm_layer)\n",
        "            for i in range(decoder_depth)])\n",
        "\n",
        "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
        "        self.decoder_pred = nn.Linear(decoder_embed_dim, patch_size**2 * in_chans, bias=True) # decoder to patch\n",
        "        # --------------------------------------------------------------------------\n",
        "\n",
        "    def random_masking(self, x, mask_ratio):\n",
        "        \"\"\"\n",
        "        Perform per-sample random masking by per-sample shuffling.\n",
        "        Per-sample shuffling is done by argsort random noise.\n",
        "        x: [N, L, D], sequence\n",
        "        \"\"\"\n",
        "        N, L, D = x.shape  # batch, length, dim\n",
        "        len_keep = int(L * (1 - mask_ratio))\n",
        "\n",
        "        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n",
        "        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
        "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
        "\n",
        "        # keep the first subset\n",
        "        ids_keep = ids_shuffle[:, :len_keep]\n",
        "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
        "\n",
        "        # generate the binary mask: 0 is keep, 1 is remove\n",
        "        mask = torch.ones([N, L], device=x.device)\n",
        "        mask[:, :len_keep] = 0\n",
        "        # unshuffle to get the binary mask\n",
        "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
        "        return x_masked, mask, ids_restore\n",
        "\n",
        "    def forward_encoder(self, x, mask_ratio):\n",
        "        x = self.patch_embed(x)\n",
        "        x = x + self.pos_embed[:, 1:, :]\n",
        "\n",
        "        # masking: length -> length * mask_ratio\n",
        "        x, mask, ids_restore = self.random_masking(x, mask_ratio)\n",
        "\n",
        "        # append cls token\n",
        "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
        "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        for blk in self.blocks: x = blk(x)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        return x, mask, ids_restore\n",
        "\n",
        "    def forward_decoder(self, x, ids_restore):\n",
        "        x = self.decoder_embed(x)\n",
        "\n",
        "        # append mask tokens to sequence\n",
        "        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
        "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
        "        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
        "        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
        "\n",
        "        x = x + self.decoder_pos_embed\n",
        "\n",
        "        for blk in self.decoder_blocks: x = blk(x)\n",
        "        x = self.decoder_norm(x)\n",
        "        x = self.decoder_pred(x)\n",
        "        x = x[:, 1:, :]\n",
        "        return x\n",
        "\n",
        "    def forward_loss(self, imgs, pred, mask):\n",
        "        \"\"\"\n",
        "        imgs: [N, 3, H, W]\n",
        "        pred: [N, L, p*p*3]\n",
        "        mask: [N, L], 0 is keep, 1 is remove,\n",
        "        \"\"\"\n",
        "        target = self.patchify(imgs)\n",
        "        # if self.norm_pix_loss:\n",
        "        #     mean = target.mean(dim=-1, keepdim=True)\n",
        "        #     var = target.var(dim=-1, keepdim=True)\n",
        "        #     target = (target - mean) / (var + 1.e-6)**.5\n",
        "\n",
        "        loss = (pred - target) ** 2\n",
        "        loss = loss.mean(dim=-1)  # [N, L], mean loss per patch\n",
        "        # loss = F.mse_loss(pred, target)\n",
        "\n",
        "        loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches\n",
        "        return loss\n",
        "\n",
        "    def forward(self, imgs, mask_ratio=0.75):\n",
        "        latent, mask, ids_restore = self.forward_encoder(imgs, mask_ratio)\n",
        "        pred = self.forward_decoder(latent, ids_restore)  # [N, L, p*p*3]\n",
        "        loss = self.forward_loss(imgs, pred, mask)\n",
        "        return loss, pred, mask\n",
        "\n",
        "\n",
        "model = MaskedAutoencoderViT(\n",
        "    patch_size=16, embed_dim=768, depth=12, num_heads=12, # B16\n",
        "    # patch_size=16, embed_dim=1024, depth=24, num_heads=16, # L16\n",
        "    # patch_size=14, embed_dim=1280, depth=32, num_heads=16, # H14\n",
        "    decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
        "    mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## uci har + ucr uea"
      ],
      "metadata": {
        "id": "E9aP1IdNGG69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Chaolei98 chinese\n",
        "# https://github.com/Chaolei98/Baseline-with-HAR-datasets/tree/main/Pre-processing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "\n",
        "#索引1为activity_id\n",
        "#索引[4:16)/[21,33)/[38,50)分别为3个IMU的3D-acc1,3D-acc2,3D-gyro,3D-magn(共36种特征)\n",
        "loc = [1] + [*range(4,16)] + [*range(21,33)] + [*range(38,50)]\n",
        "\n",
        "def window(data, label, size, stride):\n",
        "    '''将数组data和label按照滑窗尺寸size和stride进行切割'''\n",
        "    x, y = [], []\n",
        "    for i in range(0, len(label), stride):\n",
        "        if i+size < len(label): #不足一个滑窗大小的数据丢弃\n",
        "\n",
        "            l = set(label[i:i+size])\n",
        "            if len(l) > 1 or label[i] == 0: #当一个滑窗中含有包含多种activity或者activity_id为0（即属于其他动作），丢弃\n",
        "                continue\n",
        "            elif len(l) == 1:\n",
        "                x.append(data[i: i + size, :])\n",
        "                y.append(label[i])\n",
        "\n",
        "    return x, y\n",
        "\n",
        "def generate(window_size, step):\n",
        "    '''生成训练样本X和对应标签Y'''\n",
        "    X, Y = [], []\n",
        "    # 遍历9个subject文件\n",
        "    for i in range(1, 10):\n",
        "        # total = pd.read_csv('./Protocol/subject10' + str(i) + '.dat', header=None, sep=' ', usecols=loc).values\n",
        "        total = pd.read_csv('PAMAP2_Dataset/Protocol/subject10' + str(i) + '.dat', header=None, sep=' ', usecols=loc).values\n",
        "        total = total[~np.isnan(total).any(axis=1), :]  #去除NaN\n",
        "        data = total[:, 1:]\n",
        "        label = total[:, 0].reshape(-1)\n",
        "\n",
        "        # 调用window函数进行滑窗处理\n",
        "        x, y = window(data, label, window_size, step)\n",
        "        X += x\n",
        "        Y += y\n",
        "\n",
        "    # 将索引从0开始依次编号\n",
        "    cate_idx = list(Counter(Y).keys())\n",
        "    cate_idx.sort()\n",
        "    for i in range(len(Y)):\n",
        "        Y[i] = cate_idx.index(Y[i])\n",
        "\n",
        "    return X, Y\n",
        "\n",
        "def category(X, Y):\n",
        "    '''按照种类分类动作'''\n",
        "    result = [[] for i in range(len(list(Counter(Y).keys())))]  #result对应的索引即标签\n",
        "    for step, y in enumerate(Y):\n",
        "        result[y].append(X[step])\n",
        "    return result\n",
        "\n",
        "def split(result, test_size):\n",
        "    '''划分数据集\n",
        "    test_size:测试集样本数量占比'''\n",
        "    x_train, x_test, y_train, y_test = [], [], [], []\n",
        "    for i, data in enumerate(result):\n",
        "        label = [i for n in range(len(data))]\n",
        "        x_train_, x_test_, y_train_, y_test_ = train_test_split(data, label, test_size=test_size, shuffle=True)\n",
        "        x_train.extend(x_train_)\n",
        "        y_train.extend(y_train_)\n",
        "        x_test.extend(x_test_)\n",
        "        y_test.extend(y_test_)\n",
        "    return x_train, y_train, x_test, y_test\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "X, Y = generate(171, 85)\n",
        "result = category(X, Y)\n",
        "x_train, y_train, x_test, y_test = split(result, 0.2)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, shuffle=True)\n",
        "np.save('./x_train', x_train)\n",
        "np.save('./x_test', x_test)\n",
        "np.save('./y_train', y_train)\n",
        "np.save('./y_test', y_test)\n"
      ],
      "metadata": {
        "id": "xbFkO6SE-f7F",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title UCI HAR download\n",
        "# https://archive.ics.uci.edu/static/public/240/human+activity+recognition+using+smartphones.zip\n",
        "!wget https://archive.ics.uci.edu/static/public/240/human+activity+recognition+using+smartphones.zip -O har\n",
        "!unzip har\n",
        "!unzip 'UCI HAR Dataset.zip'\n",
        "!mv 'UCI HAR Dataset' UCI_HAR_Dataset"
      ],
      "metadata": {
        "id": "LypwmdFN18_n",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title UCI HAR\n",
        "# https://github.com/arijitiiest/UCI-Human-Activity-Recognition/blob/master/Data-preprocessing.ipynb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "X_train = pd.read_csv('UCI_HAR_Dataset/train/X_train.txt', delim_whitespace=True, header=None)\n",
        "with open('UCI_HAR_Dataset/features.txt') as f: X_train.columns = [line.split()[1] for line in f.readlines()]\n",
        "y_train = pd.read_csv('UCI_HAR_Dataset/train/y_train.txt', names=['Activity']).squeeze(\"columns\")\n",
        "X_train['subject'] = pd.read_csv('UCI_HAR_Dataset/train/subject_train.txt', header=None).squeeze(\"columns\")\n",
        "# class_dict = {1:'WALKING', 2:'WALKING_UPSTAIRS', 3:'WALKING_DOWNSTAIRS', 4:'SITTING', 5:'STANDING', 6:'LAYING'}\n",
        "# y_train_labels = y_train.map(class_dict)\n",
        "X_train['Activity'] = y_train\n",
        "# train['ActivityName'] = y_train_labels\n",
        "\n",
        "X_test = pd.read_csv('UCI_HAR_Dataset/test/X_test.txt', delim_whitespace=True, header=None)\n",
        "with open('UCI_HAR_Dataset/features.txt') as f: X_test.columns = [line.split()[1] for line in f.readlines()]\n",
        "y_test = pd.read_csv('UCI_HAR_Dataset/test/y_test.txt', names=['Activity']).squeeze(\"columns\")\n",
        "X_test['subject'] = pd.read_csv('UCI_HAR_Dataset/test/subject_test.txt', header=None).squeeze(\"columns\")\n",
        "\n",
        "ans = [y for _, y in X_train.groupby(['subject', 'Activity'])]\n",
        "y_train = [df['Activity'].iloc[0] for df in ans]\n",
        "# y_train = [df['subject'].iloc[0] for df in ans]\n",
        "X_train = [df.drop(['subject', 'Activity'], axis=1) for df in ans]\n"
      ],
      "metadata": {
        "id": "ZEapWaI-1swh",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(X_train)\n",
        "# print(y_train)\n",
        "# print(X_test)\n",
        "# print(y_test)\n",
        "\n",
        "# ans = [y for _, y in X_train.groupby(['subject', 'Activity'])]\n",
        "# print(ans)\n",
        "# print(ans[125])\n",
        "# print(len(ans)) # 126\n",
        "# print([len(a) for a in ans]) # 126\n",
        "# print(min([len(a) for a in ans])) # 126\n",
        "\n",
        "# y_train = [df['Activity'].iloc[0] for df in ans]\n",
        "# # y_train = [df['subject'].iloc[0] for df in ans]\n",
        "# X_train = [df.drop(['subject', 'Activity'], axis=1) for df in ans]\n",
        "\n",
        "# print(y)\n",
        "\n",
        "# print(len(y_train))\n",
        "\n",
        "# print(ans[125].to_numpy().shape)\n",
        "print(X_train[0].shape[-1])\n",
        "\n",
        "# out = []\n",
        "# for (_, group), subdf in X_train.groupby(['subject', 'Activity']):\n",
        "#     pair = list(group)  # [col1, col2]\n",
        "#     count = len(subdf)\n",
        "#     if count == 1: out.append(pair)\n",
        "#     else: out.append([pair] * count)\n",
        "# print(out)\n"
      ],
      "metadata": {
        "id": "a_bJFZOL4Fu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title pandasDataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class pandasDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X, self.y = X, y\n",
        "        chars = sorted(list(set(y)))\n",
        "        self.vocab_size = len(chars) #\n",
        "    #     self.stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "    #     self.itos = {i:ch for i,ch in enumerate(chars)}\n",
        "    #     self.y = self.data_process(y) #\n",
        "        self.seq_len = min([len(a) for a in X])\n",
        "\n",
        "    # def data_process(self, data): # str\n",
        "    #     return torch.tensor([self.stoi.get(c) for c in data]) #\n",
        "\n",
        "    def __len__(self): return len(self.X)\n",
        "    # def __getitem__(self, idx): return self.X.iloc[idx].to_numpy(), self.y.iloc[idx]\n",
        "    # def __getitem__(self, idx): return self.X[idx].to_numpy(), self.y[idx]\n",
        "    def __getitem__(self, idx):\n",
        "        i = np.random.randint(0, len(self.X[idx])-self.seq_len+1)\n",
        "        return self.X[idx].to_numpy()[i:i+self.seq_len], self.y[idx]\n",
        "\n",
        "train_data = pandasDataset(X_train, y_train)\n",
        "test_data = pandasDataset(X_test, y_test)\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=2)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=2)\n",
        "\n",
        "for X, y in train_loader:\n",
        "    print(X.shape, y.shape)\n",
        "    break\n",
        "\n"
      ],
      "metadata": {
        "id": "G6vwmzM4qUXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "E4c4r5Hkry99"
      },
      "outputs": [],
      "source": [
        "# @title tsai\n",
        "# https://timeseriesai.github.io/tsai/\n",
        "!pip install -qU tsai # 3mins\n",
        "import tsai\n",
        "from tsai.data.external import get_UCR_data, get_UCR_multivariate_list\n",
        "\n",
        "# l = get_UCR_multivariate_list()\n",
        "# print(len(l), l)\n",
        "# X_train, y_train, X_valid, y_valid = get_UCR_data(dsid) # tsai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "yRuBXTauj2f4"
      },
      "outputs": [],
      "source": [
        "# @title tslearn\n",
        "!pip install -qU tslearn\n",
        "from tslearn.datasets import UCR_UEA_datasets # https://tslearn.readthedocs.io/en/latest/gen_modules/datasets/tslearn.datasets.UCR_UEA_datasets.html\n",
        "\n",
        "# l = UCR_UEA_datasets().list_datasets() # 30 ['ArticularyWordRecognition', 'AtrialFibrillation', 'BasicMotions', 'CharacterTrajectories', 'Cricket', 'DuckDuckGeese', 'EigenWorms', 'Epilepsy', 'EthanolConcentration', 'ERing', 'FaceDetection', 'FingerMovements', 'HandMovementDirection', 'Handwriting', 'Heartbeat', 'InsectWingbeat', 'JapaneseVowels', 'Libras', 'LSST', 'MotorImagery', 'NATOPS', 'PenDigits', 'PEMS-SF', 'Phoneme', 'RacketSports', 'SelfRegulationSCP1', 'SelfRegulationSCP2', 'SpokenArabicDigits', 'StandWalkJump', 'UWaveGestureLibrary']\n",
        "# l = UCR_UEA_datasets().list_multivariate_datasets() # same ^\n",
        "# l = UCR_UEA_datasets().list_univariate_datasets() # 0 []\n",
        "# print(len(l), l)\n",
        "\n",
        "# for dataset_name in data_loader.list_datasets(): # 30 ['ArticularyWordRecognition', 'AtrialFibrillation', 'BasicMotions', 'CharacterTrajectories', 'Cricket', 'DuckDuckGeese', 'EigenWorms', 'Epilepsy', 'EthanolConcentration', 'ERing', 'FaceDetection', 'FingerMovements', 'HandMovementDirection', 'Handwriting', 'Heartbeat', 'InsectWingbeat', 'JapaneseVowels', 'Libras', 'LSST', 'MotorImagery', 'NATOPS', 'PenDigits', 'PEMS-SF', 'Phoneme', 'RacketSports', 'SelfRegulationSCP1', 'SelfRegulationSCP2', 'SpokenArabicDigits', 'StandWalkJump', 'UWaveGestureLibrary']\n",
        "#     X_train, y_train, X_test, y_test = data_loader.load_dataset(dataset_name)\n",
        "\n",
        "# X_train, y_train, X_test, y_test = UCR_UEA_datasets().load_dataset('AtrialFibrillation') # tslearn\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ep7xumXZke5y"
      },
      "outputs": [],
      "source": [
        "# @title sktime\n",
        "!pip install -q sktime\n",
        "from sktime.datasets import load_UCR_UEA_dataset\n",
        "import numpy as np\n",
        "# https://www.sktime.net/en/v0.32.2/examples/AA_datatypes_and_datasets.html#Section-3.2.3:-time-series-classification-data-sets-from-the-UCR/UEA-time-series-classification-repository\n",
        "\n",
        "X_train, y_train = load_UCR_UEA_dataset('AtrialFibrillation')\n",
        "X_train, y_train = load_UCR_UEA_dataset('AtrialFibrillation', split=\"train\")\n",
        "X_test, y_test = load_UCR_UEA_dataset('AtrialFibrillation', split=\"test\")\n",
        "\n",
        "print(f\"Training data shape: {X_train.shape}\")\n",
        "print(f\"Training labels shape: {y_train.shape}\")\n",
        "print(f\"Testing data shape: {X_test.shape}\")\n",
        "print(f\"Testing labels shape: {y_test.shape}\")\n",
        "\n",
        "# X_train and X_test will typically be Pandas DataFrames or NumPy arrays\n",
        "# y_train and y_test will be NumPy arrays or Pandas Series\n",
        "\n",
        "# print(X_train, y_train)\n",
        "# print(X_train[0], y_train[0])\n",
        "# print(X_train)\n",
        "print(X_train.iloc[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2Gr_588txsd6"
      },
      "outputs": [],
      "source": [
        "# @title test load all datasets\n",
        "l = get_UCR_multivariate_list()\n",
        "print(len(l), l)\n",
        "for i, dataset_name in enumerate(get_UCR_multivariate_list()):\n",
        "# for dataset_name in get_UCR_multivariate_list()[15:20]:\n",
        "    print(dataset_name)\n",
        "    # if dataset_name in ['DuckDuckGeese','FaceDetection','InsectWingbeat','PEMS-SF']:\n",
        "    #     print('skip', dataset_name)\n",
        "    #     continue\n",
        "    # try: X_train, y_train, X_valid, y_valid = get_UCR_data(dataset_name) # tsai\n",
        "    # # InsectWingbeat, PEMS-SF slow\n",
        "\n",
        "    # if dataset_name in ['AtrialFibrillation', 'CharacterTrajectories','DuckDuckGeese','EigenWorms','ERing','InsectWingbeat','JapaneseVowels','SpokenArabicDigits']:\n",
        "    # # EigenWorms slow\n",
        "    #     print('skip', dataset_name)\n",
        "    #     continue\n",
        "    # try: X_train, y_train, X_test, y_test = UCR_UEA_datasets().load_dataset(dataset_name) # tslearn\n",
        "\n",
        "    if dataset_name in ['InsectWingbeat']:\n",
        "        print('skip', dataset_name)\n",
        "        continue\n",
        "    try:\n",
        "        X_train, y_train = load_UCR_UEA_dataset(dataset_name)\n",
        "        X_train, y_train = load_UCR_UEA_dataset(dataset_name, split=\"train\")\n",
        "        X_test, y_test = load_UCR_UEA_dataset(dataset_name, split=\"test\")\n",
        "    # InsectWingbeat slow oom\n",
        "\n",
        "    # who has DuckDuckGeese','FaceDetection','InsectWingbeat\n",
        "\n",
        "\n",
        "    except Exception as e: print(e); continue\n",
        "    print(dataset_name, X_train.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "EgLjldpPdSsQ"
      },
      "outputs": [],
      "source": [
        "# @title time series DataLoader\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "dataset_name = 'EthanolConcentration'\n",
        "# X_train, y_train, X_test, y_test = get_UCR_data(dataset_name) # tsai\n",
        "X_train, y_train, X_test, y_test = UCR_UEA_datasets().load_dataset(dataset_name) # tslearn\n",
        "\n",
        "\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        chars = sorted(list(set(y)))\n",
        "        self.vocab_size = len(chars) #\n",
        "        self.stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "        self.itos = {i:ch for i,ch in enumerate(chars)}\n",
        "        self.X = torch.tensor(X) # (N, 1, T)\n",
        "        self.y = self.data_process(y) #\n",
        "\n",
        "    def data_process(self, data): # str\n",
        "        return torch.tensor([self.stoi.get(c) for c in data]) #\n",
        "    def __len__(self): return len(self.y)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "train_data = TimeSeriesDataset(X_train, y_train)\n",
        "test_data = TimeSeriesDataset(X_test, y_test)\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=2)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=2)\n",
        "\n",
        "# print(X_train, y_train, X_test, y_test)\n",
        "# for x,y in train_loader:\n",
        "# # for x,y in train_data:\n",
        "#     print(x.shape, y.shape) # (261, 1751, 3)\n",
        "#     print(x, y)\n",
        "#     break\n",
        "\n",
        "# print(x.shape[-1])\n",
        "# print(X_train.shape[-1])\n",
        "# print(train_data.vocab_size) # 4\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title aeon\n",
        "# https://www.aeon-toolkit.org/en/stable/examples/datasets/load_data_from_web.html#Time-Series-Classification-Archive\n",
        "# !pip install -q aeon\n",
        "from aeon.datasets import load_classification\n",
        "\n",
        "# download from website https://www.aeon-toolkit.org/en/stable/examples/datasets/data_loading.html\n",
        "\n",
        "from aeon.datasets.tsc_datasets import multivariate, univariate\n",
        "# print(len(univariate), len(multivariate)) # 128, 30\n",
        "# print(univariate) # ['ACSF1', 'Adiac', 'AllGestureWiimoteX', 'AllGestureWiimoteY', 'AllGestureWiimoteZ', 'ArrowHead', 'Beef', 'BeetleFly', 'BirdChicken', 'BME', 'Car', 'CBF', 'Chinatown', 'ChlorineConcentration', 'CinCECGTorso', 'Coffee', 'Computers', 'CricketX', 'CricketY', 'CricketZ', 'Crop', 'DiatomSizeReduction', 'DistalPhalanxOutlineAgeGroup', 'DistalPhalanxOutlineCorrect', 'DistalPhalanxTW', 'DodgerLoopDay', 'DodgerLoopGame', 'DodgerLoopWeekend', 'Earthquakes', 'ECG200', 'ECG5000', 'ECGFiveDays', 'ElectricDevices', 'EOGHorizontalSignal', 'EOGVerticalSignal', 'EthanolLevel', 'FaceAll', 'FaceFour', 'FacesUCR', 'FiftyWords', 'Fish', 'FordA', 'FordB', 'FreezerRegularTrain', 'FreezerSmallTrain', 'Fungi', 'GestureMidAirD1', 'GestureMidAirD2', 'GestureMidAirD3', 'GesturePebbleZ1', 'GesturePebbleZ2', 'GunPoint', 'GunPointAgeSpan', 'GunPointMaleVersusFemale', 'GunPointOldVersusYoung', 'Ham', 'HandOutlines', 'Haptics', 'Herring', 'HouseTwenty', 'InlineSkate', 'InsectEPGRegularTrain', 'InsectEPGSmallTrain', 'InsectWingbeatSound', 'ItalyPowerDemand', 'LargeKitchenAppliances', 'Lightning2', 'Lightning7', 'Mallat', 'Meat', 'MedicalImages', 'MelbournePedestrian', 'MiddlePhalanxOutlineCorrect', 'MiddlePhalanxOutlineAgeGroup', 'MiddlePhalanxTW', 'MixedShapesRegularTrain', 'MixedShapesSmallTrain', 'MoteStrain', 'NonInvasiveFetalECGThorax1', 'NonInvasiveFetalECGThorax2', 'OliveOil', 'OSULeaf', 'PhalangesOutlinesCorrect', 'Phoneme', 'PickupGestureWiimoteZ', 'PigAirwayPressure', 'PigArtPressure', 'PigCVP', 'PLAID', 'Plane', 'PowerCons', 'ProximalPhalanxOutlineCorrect', 'ProximalPhalanxOutlineAgeGroup', 'ProximalPhalanxTW', 'RefrigerationDevices', 'Rock', 'ScreenType', 'SemgHandGenderCh2', 'SemgHandMovementCh2', 'SemgHandSubjectCh2', 'ShakeGestureWiimoteZ', 'ShapeletSim', 'ShapesAll', 'SmallKitchenAppliances', 'SmoothSubspace', 'SonyAIBORobotSurface1', 'SonyAIBORobotSurface2', 'StarLightCurves', 'Strawberry', 'SwedishLeaf', 'Symbols', 'SyntheticControl', 'ToeSegmentation1', 'ToeSegmentation2', 'Trace', 'TwoLeadECG', 'TwoPatterns', 'UMD', 'UWaveGestureLibraryAll', 'UWaveGestureLibraryX', 'UWaveGestureLibraryY', 'UWaveGestureLibraryZ', 'Wafer', 'Wine', 'WordSynonyms', 'Worms', 'WormsTwoClass', 'Yoga']\n",
        "# print(multivariate) # ['ArticularyWordRecognition', 'AtrialFibrillation', 'BasicMotions', 'CharacterTrajectories', 'Cricket', 'DuckDuckGeese', 'EigenWorms', 'Epilepsy', 'EthanolConcentration', 'ERing', 'FaceDetection', 'FingerMovements', 'HandMovementDirection', 'Handwriting', 'Heartbeat', 'InsectWingbeat', 'JapaneseVowels', 'Libras', 'LSST', 'MotorImagery', 'NATOPS', 'PenDigits', 'PEMS-SF', 'PhonemeSpectra', 'RacketSports', 'SelfRegulationSCP1', 'SelfRegulationSCP2', 'SpokenArabicDigits', 'StandWalkJump', 'UWaveGestureLibrary']\n",
        "\n",
        "dataset_name = 'EthanolConcentration'\n",
        "X_train, y_train = load_classification(dataset_name, split=\"train\") # https://www.aeon-toolkit.org/en/latest/api_reference/auto_generated/aeon.datasets.load_classification.html\n",
        "X_test, y_test = load_classification(dataset_name, split=\"test\")\n",
        "# [b,c,t]\n",
        "\n",
        "# print(\"Shape of X = \", X.shape)\n",
        "# print(\"First case = \", X[0][0], \" has label = \", y[0])\n",
        "\n",
        "\n",
        "# # # https://www.aeon-toolkit.org/en/stable/examples/classification/classification.html\n",
        "# from aeon.classification.convolution_based import RocketClassifier\n",
        "# rocket = RocketClassifier(n_kernels=2000)\n",
        "# rocket.fit(X_train, y_train)\n",
        "# y_pred = rocket.predict(X_test)\n",
        "# .87\n",
        "\n",
        "from aeon.classification.hybrid import HIVECOTEV2\n",
        "hc2 = HIVECOTEV2(time_limit_in_minutes=0.2) # 1min .83acc # https://www.aeon-toolkit.org/en/latest/api_reference/auto_generated/aeon.classification.hybrid.HIVECOTEV2.html\n",
        "hc2.fit(X_train, y_train)\n",
        "y_pred = hc2.predict(X_test)\n",
        "\n",
        "import torch.nn.functional as F\n",
        "# test_loss = F.cross_entropy(y_test, y_pred)\n",
        "# accuracy = (y_test==y_pred.argmax(dim=1)).sum().item()\n",
        "# accuracy = (y_test==y_pred.argmax()).sum().item()\n",
        "accuracy = (y_test==y_pred).mean().item()\n",
        "# print(y_test.shape, y_pred.shape)\n",
        "# print(y_test, y_pred)\n",
        "print(accuracy)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# https://www.aeon-toolkit.org/en/stable/examples/classification/deep_learning.html\n",
        "\n",
        "# from aeon.visualisation import plot_boxplot, plot_critical_difference\n",
        "# plot_critical_difference(results, names)\n"
      ],
      "metadata": {
        "id": "sArBDLdE643V",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3B3t5tSmK0Fv"
      },
      "outputs": [],
      "source": [
        "# @title plot data\n",
        "# print(X_train, y_train, X_test, y_test)\n",
        "# for x,y in train_loader:\n",
        "# for x,y in train_data:\n",
        "for i, (x,y) in enumerate(train_data):\n",
        "    # print(x.shape, y.shape)\n",
        "    # print(x, y)\n",
        "    # break\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.rcParams[\"figure.figsize\"] = (8,4)\n",
        "    # plt.plot(x[:,0])\n",
        "    for j in range(x.shape[-1]):\n",
        "        plt.plot(x[:,j])\n",
        "    plt.show()\n",
        "    if i>=3: break\n",
        "\n",
        "# print(x.shape[-1])\n",
        "# print(X_train.shape[-1])\n",
        "# print(train_data.vocab_size)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpBQCgArjj7x"
      },
      "source": [
        "## text classification, roberta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkiHS6yPkaVo"
      },
      "outputs": [],
      "source": [
        "!pip install -qU datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "o4xSm_eyhSK0"
      },
      "outputs": [],
      "source": [
        "# @title yelp data\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataset = load_dataset(\"yelp_polarity\") # yelp_polarity yelp_review_full\n",
        "# # dataset = load_dataset(\"yelp_review_full\") # yelp_polarity yelp_review_full\n",
        "# print(dataset[\"train\"][0]) # {'text': \"Unfortunately, the ... to give Dr. Goldberg 2 stars.\", 'label': 0}\n",
        "# print(len(dataset))\n",
        "# print(len(dataset[\"train\"]))\n",
        "# # train_text = dataset[\"train\"][:10]['text']\n",
        "# train_text = dataset[\"train\"]['text']\n",
        "# # train_tok = [enc.encode(text) for text in train_text]\n",
        "# train_tok = [torch.tensor(enc.encode(text)) for text in train_text]\n",
        "# # print(train_tok)\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    train_text = dataset[\"train\"]['text']\n",
        "    train_tok = [torch.tensor(enc.encode(text)) for text in train_text]\n",
        "\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "# def tokenize(example): return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\")\n",
        "# tokenized = dataset.map(tokenize, batched=True)\n",
        "# tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "\n",
        "# train_loader = DataLoader(tokenized[\"train\"], batch_size=32, shuffle=True)\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "x = pad_sequence(train_tok, batch_first=True, padding_value=0, padding_side='left')\n",
        "print(x)\n",
        "\n",
        "def left_pad(batch, pad_value=0):\n",
        "    # batch: list of 1D tensors\n",
        "    lengths = torch.tensor([len(x) for x in batch])\n",
        "    max_len = lengths.max()\n",
        "\n",
        "    # Preallocate padded tensor\n",
        "    # padded = torch.full((len(batch), max_len), pad_value, dtype=batch[0].dtype)\n",
        "    padded = torch.full((len(batch), max_len), pad_value)\n",
        "\n",
        "    # for i, x in enumerate(batch):\n",
        "    #     padded[i, -x.size(0):] = x  # align to right, pad left\n",
        "    padded[torch.arange(len(batch)).unsqueeze(-1), -lengths:] = batch\n",
        "    return padded, lengths\n",
        "\n",
        "# left_pad(train_tok)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2HZejCTckoQG"
      },
      "outputs": [],
      "source": [
        "# @title tiktoken\n",
        "# https://github.com/openai/tiktoken/tree/main\n",
        "import tiktoken\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\") # gpt2 r50k_base p50k_base p50k_edit cl100k_base o200k_base # https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py\n",
        "# enc = tiktoken.encoding_for_model(\"gpt-4o\") # https://github.com/openai/tiktoken/blob/main/tiktoken/model.py#L24\n",
        "tok = enc.encode(\"hello world\")\n",
        "out = enc.decode(tok)\n",
        "print(tok, out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "i1iatz1SSK3s"
      },
      "outputs": [],
      "source": [
        "# @title tiktoken dataloader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import tiktoken # https://github.com/openai/tiktoken/tree/main\n",
        "\n",
        "\n",
        "# train_text = dataset[\"train\"]['text']\n",
        "# # train_tok = [enc.encode(text) for text in train_text]\n",
        "# train_tok = [torch.tensor(enc.encode(text)) for text in train_text]\n",
        "\n",
        "\n",
        "class CharDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, raw_data, seq_len):\n",
        "        # data = ''.join(raw_data)\n",
        "        # data = raw_data['text']\n",
        "        self.enc = tiktoken.get_encoding(\"gpt2\") # https://github.com/openai/tiktoken/blob/main/tiktoken/core.py\n",
        "        self.vocab_size = self.enc.n_vocab # gpt2:50257\n",
        "        self.data = self.data_process(data) # list of int\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def data_process(self, data): # str 10780437\n",
        "        return torch.tensor(self.enc.encode(data))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)//(self.seq_len+1)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        dix = self.data[idx*(self.seq_len+1) : (idx+1)*(self.seq_len+1)]\n",
        "        x, y = dix[:-1], dix[1:]\n",
        "        return x, y\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
        "\n",
        "train_data = CharDataset(dataset[\"train\"], seq_len) # one line of poem is roughly 50 characters\n",
        "\n",
        "\n",
        "seq_len = 100 # 128\n",
        "train_data = CharDataset(text, seq_len) # one line of poem is roughly 50 characters\n",
        "test_data = CharDataset(test_text, seq_len) # one line of poem is roughly 50 characters\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 64 #512\n",
        "train_loader = DataLoader(train_data, shuffle=True, pin_memory=True, batch_size=batch_size, num_workers=2) # num_workers = 4\n",
        "test_loader = DataLoader(test_data, shuffle=True, pin_memory=True, batch_size=batch_size, num_workers=0)\n",
        "\n",
        "# https://github.com/openai/tiktoken/blob/main/tiktoken/core.py\n",
        "def encode(context):\n",
        "    if type(context) == str: return torch.tensor([train_loader.dataset.enc.encode(context)], device=device)\n",
        "    elif type(context) == list: return train_loader.dataset.enc.encode_batch(context)\n",
        "    else: raise Exception\n",
        "def decode(x): return train_loader.dataset.enc.decode(list(x))\n",
        "# for x,y in train_loader:\n",
        "#     break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qVHe2tXTmacN"
      },
      "outputs": [],
      "source": [
        "# @title hf roberta\n",
        "# https://huggingface.co/docs/transformers/en/model_doc/roberta\n",
        "import torch\n",
        "from transformers import AutoTokenizer, RobertaConfig, RobertaModel\n",
        "from transformers import RobertaForMaskedLM, RobertaForSequenceClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
        "print(tokenizer(\"Hello world\")[\"input_ids\"])\n",
        "print(tokenizer(\" Hello world\")[\"input_ids\"])\n",
        "# {'input_ids': tensor([[    0,   133,   812,     9,  1470,    16, 50264,     4,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
        "\n",
        "config = RobertaConfig()\n",
        "# model = RobertaModel(config)\n",
        "model = RobertaForMaskedLM(config)\n",
        "# model = RobertaForSequenceClassification(config)\n",
        "\n",
        "# inputs = tokenizer(\"The capital of France is <mask>.\", return_tensors=\"pt\")\n",
        "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
        "print(inputs)\n",
        "with torch.no_grad():\n",
        "    # logits = model(**inputs).logits\n",
        "    logits = model(**inputs)\n",
        "\n",
        "# LM: last_hidden_state, pooler_output, hidden_states=None, past_key_values=None, attentions=None, cross_attentions\n",
        "\n",
        "\n",
        "# predicted_class_id = logits.argmax().item()\n",
        "# model.config.id2label[predicted_class_id]\n",
        "\n",
        "# # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n",
        "# num_labels = len(model.config.id2label)\n",
        "# # model = RobertaForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-emotion\", num_labels=num_labels)\n",
        "# model = RobertaForSequenceClassification(config)\n",
        "# labels = torch.tensor([1])\n",
        "# loss = model(**inputs, labels=labels).loss\n",
        "# round(loss.item(), 2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # print(logits.keys())\n",
        "# # retrieve index of <mask>\n",
        "# mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
        "\n",
        "# predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
        "# tokenizer.decode(predicted_token_id)\n",
        "\n",
        "# labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\")[\"input_ids\"]\n",
        "# # mask labels of non-<mask> tokens\n",
        "# labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)\n",
        "\n",
        "# outputs = model(**inputs, labels=labels)\n",
        "# round(outputs.loss.item(), 2)\n",
        "\n",
        "# last_hidden_states = outputs.last_hidden_state\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLiCsK97Tz3H"
      },
      "outputs": [],
      "source": [
        "# print(logits)\n",
        "# # loss = model(**inputs, labels=labels).loss\n",
        "# loss = model(**inputs)\n",
        "\n",
        "# MaskedLMOutput = loss, logits, hidden_states=None, attentions\n",
        "# print(tokenizer(\"Hello world\")[\"input_ids\"])\n",
        "print(tokenizer(\"Hello world\")) # input_ids attention_mask\n",
        "print(tokenizer([\"Hello world\",\"dfg4\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TtKHgsmyvj5G"
      },
      "outputs": [],
      "source": [
        "# @title hf data\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "dataset = load_dataset(\"yelp_polarity\") # yelp_polarity yelp_review_full\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
        "\n",
        "import os\n",
        "def tokenize(examples): return tokenizer(examples[\"text\"], truncation=True, max_length=256)\n",
        "tok_dataset = dataset.map(tokenize, batched=True, num_proc=os.cpu_count(), # Use multiple processes for faster tokenization\n",
        "    remove_columns=[\"text\"] # Remove the original text column\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4yn7hlrBREhk"
      },
      "outputs": [],
      "source": [
        "# @title gemini roberta\n",
        "from transformers import AutoTokenizer, RobertaConfig, RobertaForMaskedLM\n",
        "\n",
        "# config = RobertaConfig() # vocab_size = 50265, hidden_size = 768, num_hidden_layers = 12, num_attention_heads = 12, intermediate_size = 3072, hidden_act = 'gelu', hidden_dropout_prob = 0.1, attention_probs_dropout_prob = 0.1, max_position_embeddings = 512, type_vocab_size = 2, initializer_range = 0.02, layer_norm_eps = 1e-12, pad_token_id = 1, bos_token_id = 0eos_token_id = 2, position_embedding_type = 'absolute'\n",
        "config = RobertaConfig(vocab_size = 50265, hidden_size = 64, num_hidden_layers = 1, num_attention_heads = 8, intermediate_size = 256, hidden_act = 'gelu', hidden_dropout_prob = 0., attention_probs_dropout_prob = 0.)\n",
        "model_mlm = RobertaForMaskedLM(config)\n",
        "\n",
        "\n",
        "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "collator = DataCollatorForLanguageModeling(tokenizer) # Masked Language Model (MLM); .15,.8,.1 # https://huggingface.co/docs/transformers/en/main_classes/data_collator#transformers.DataCollatorForLanguageModeling\n",
        "import torch\n",
        "\n",
        "train_args = TrainingArguments(\n",
        "    # output_dir=MODEL_OUTPUT_DIR_STAGE1, overwrite_output_dir=True,\n",
        "    num_train_epochs=1,\n",
        "    prediction_loss_only=True, # Only compute loss, no predictions during eval\n",
        "    # optim='adamw_torch',\n",
        "    optim='adamw_torch_fused',\n",
        "    learning_rate=3e-4,\n",
        "#     lr_scheduler_type (str or SchedulerType, optional, defaults to \"linear\") — The scheduler type to use. See the documentation of SchedulerType for all possible values.\n",
        "# lr_scheduler_kwargs (‘dict’, optional, defaults to {}) —\n",
        "    # warmup_steps=0.1 * NUM_TRAIN_EPOCHS_STAGE1 * (len(tokenized_dataset_mlm) // PER_DEVICE_BATCH_SIZE), # 10% warmup\n",
        "    warmup_ratio=0.1,\n",
        "    # weight_decay=0.01,\n",
        "    # report_to=\"tensorboard\",\n",
        "    report_to=\"wandb\", # 487a2109e55dce4e13fc70681781de9f50f27be7\n",
        "    fp16=torch.cuda.is_available(), # Enable mixed precision if GPU is available\n",
        ") # https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments\n",
        "\n",
        "# half_precision_backend=\"auto\"\n",
        "# bf16=True\n",
        "\n",
        "# trainer_mlm = Trainer(model=model_mlm, args=train_args, train_dataset=tok_dataset['train'].remove_columns(\"label\"), data_collator=collator)\n",
        "# trainer_mlm.train()\n",
        "\n",
        "# # trainer_mlm.save_model(MODEL_OUTPUT_DIR_STAGE1)\n",
        "\n",
        "# eval_results = trainer_mlm.evaluate()\n",
        "# perplexity = math.exp(eval_results[\"eval_loss\"])\n",
        "# print('perplexity', perplexity)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iISjeDVDugDQ"
      },
      "outputs": [],
      "source": [
        "print(tok_dataset['train']['label'][:4])\n",
        "print(tok_dataset['train']['input_ids'][:4])\n",
        "print(tok_dataset['train']['attention_mask'][:4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "gy1anCqbHN59"
      },
      "outputs": [],
      "source": [
        "# @title chatgpt roberta\n",
        "from transformers import RobertaConfig, RobertaForMaskedLM, RobertaTokenizerFast\n",
        "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")  # small for testing\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
        "def tokenize_function(example): return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
        "\n",
        "config = RobertaConfig(vocab_size=tokenizer.vocab_size, max_position_embeddings=128, num_attention_heads=8, num_hidden_layers=6, hidden_size=512, intermediate_size=2048)\n",
        "model = RobertaForMaskedLM(config)\n",
        "\n",
        "training_args = TrainingArguments(output_dir=\"./roberta-small\", overwrite_output_dir=True, num_train_epochs=5, per_device_train_batch_size=16, evaluation_strategy=\"no\", save_steps=10_000, save_total_limit=2, logging_steps=500)\n",
        "trainer = Trainer(model=model, args=training_args, train_dataset=tokenized_datasets[\"train\"], tokenizer=tokenizer, data_collator=data_collator)\n",
        "\n",
        "trainer.train()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pt98LQu2EC4K"
      },
      "source": [
        "## trash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WGmRZ_ojEFGc"
      },
      "outputs": [],
      "source": [
        "# @title facebookresearch/mae models_mae.py\n",
        "# https://github.com/facebookresearch/mae/blob/main/models_mae.py\n",
        "from functools import partial\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from timm.models.vision_transformer import PatchEmbed, Block\n",
        "from util.pos_embed import get_2d_sincos_pos_embed\n",
        "\n",
        "\n",
        "class MaskedAutoencoderViT(nn.Module):\n",
        "    \"\"\" Masked Autoencoder with VisionTransformer backbone\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3,\n",
        "                 embed_dim=1024, depth=24, num_heads=16,\n",
        "                 decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
        "                 mlp_ratio=4., norm_layer=nn.LayerNorm, norm_pix_loss=False):\n",
        "        super().__init__()\n",
        "\n",
        "        # --------------------------------------------------------------------------\n",
        "        # MAE encoder specifics\n",
        "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True, qk_scale=None, norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "        # --------------------------------------------------------------------------\n",
        "\n",
        "        # --------------------------------------------------------------------------\n",
        "        # MAE decoder specifics\n",
        "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
        "        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, decoder_embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
        "        self.decoder_blocks = nn.ModuleList([\n",
        "            Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=True, qk_scale=None, norm_layer=norm_layer)\n",
        "            for i in range(decoder_depth)])\n",
        "\n",
        "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
        "        self.decoder_pred = nn.Linear(decoder_embed_dim, patch_size**2 * in_chans, bias=True) # decoder to patch\n",
        "        # --------------------------------------------------------------------------\n",
        "        self.norm_pix_loss = norm_pix_loss\n",
        "        self.initialize_weights()\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        # initialization\n",
        "        # initialize (and freeze) pos_embed by sin-cos embedding\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=True)\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "\n",
        "        decoder_pos_embed = get_2d_sincos_pos_embed(self.decoder_pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=True)\n",
        "        self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).float().unsqueeze(0))\n",
        "\n",
        "        # initialize patch_embed like nn.Linear (instead of nn.Conv2d)\n",
        "        w = self.patch_embed.proj.weight.data\n",
        "        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
        "\n",
        "        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n",
        "        torch.nn.init.normal_(self.cls_token, std=.02)\n",
        "        torch.nn.init.normal_(self.mask_token, std=.02)\n",
        "\n",
        "        # initialize nn.Linear and nn.LayerNorm\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            # we use xavier_uniform following official JAX ViT:\n",
        "            torch.nn.init.xavier_uniform_(m.weight)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    def patchify(self, imgs): # [b,3,h,w]\n",
        "        p = self.patch_embed.patch_size[0]\n",
        "        assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0\n",
        "        h = w = imgs.shape[2] // p\n",
        "        x = imgs.reshape(imgs.shape[0], 3, h, p, w, p) # [b,3,h/p,p,w/p,p]\n",
        "        x = torch.einsum('nchpwq->nhwpqc', x) # [b,h/p,w/p,p,p,3]\n",
        "        x = x.reshape(imgs.shape[0], h * w, p**2 * 3) # [b, h/p *w/p, p*p*3]\n",
        "        return x # [b, h/p *w/p, p*p*3] ~ [b,t,d]\n",
        "\n",
        "    def unpatchify(self, x):\n",
        "        \"\"\"\n",
        "        x: (N, L, patch_size**2 *3)\n",
        "        imgs: (N, 3, H, W)\n",
        "        \"\"\"\n",
        "        p = self.patch_embed.patch_size[0]\n",
        "        h = w = int(x.shape[1]**.5)\n",
        "        assert h * w == x.shape[1]\n",
        "\n",
        "        x = x.reshape(shape=(x.shape[0], h, w, p, p, 3))\n",
        "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
        "        imgs = x.reshape(shape=(x.shape[0], 3, h * p, h * p))\n",
        "        return imgs\n",
        "\n",
        "    def random_masking(self, x, mask_ratio):\n",
        "        \"\"\"\n",
        "        Perform per-sample random masking by per-sample shuffling.\n",
        "        Per-sample shuffling is done by argsort random noise.\n",
        "        x: [N, L, D], sequence\n",
        "        \"\"\"\n",
        "        N, L, D = x.shape  # batch, length, dim\n",
        "        len_keep = int(L * (1 - mask_ratio))\n",
        "\n",
        "        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n",
        "\n",
        "        # sort noise for each sample\n",
        "        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
        "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
        "\n",
        "        # keep the first subset\n",
        "        ids_keep = ids_shuffle[:, :len_keep]\n",
        "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
        "\n",
        "        # generate the binary mask: 0 is keep, 1 is remove\n",
        "        mask = torch.ones([N, L], device=x.device)\n",
        "        mask[:, :len_keep] = 0\n",
        "        # unshuffle to get the binary mask\n",
        "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
        "\n",
        "        return x_masked, mask, ids_restore\n",
        "\n",
        "    def forward_encoder(self, x, mask_ratio):\n",
        "        # embed patches\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        # add pos embed w/o cls token\n",
        "        x = x + self.pos_embed[:, 1:, :]\n",
        "\n",
        "        # masking: length -> length * mask_ratio\n",
        "        x, mask, ids_restore = self.random_masking(x, mask_ratio)\n",
        "\n",
        "        # append cls token\n",
        "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
        "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        # apply Transformer blocks\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        return x, mask, ids_restore\n",
        "\n",
        "    def forward_decoder(self, x, ids_restore):\n",
        "        # embed tokens\n",
        "        x = self.decoder_embed(x)\n",
        "\n",
        "        # append mask tokens to sequence\n",
        "        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
        "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
        "        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
        "        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
        "\n",
        "        # add pos embed\n",
        "        x = x + self.decoder_pos_embed\n",
        "\n",
        "        # apply Transformer blocks\n",
        "        for blk in self.decoder_blocks:\n",
        "            x = blk(x)\n",
        "        x = self.decoder_norm(x)\n",
        "\n",
        "        # predictor projection\n",
        "        x = self.decoder_pred(x)\n",
        "\n",
        "        # remove cls token\n",
        "        x = x[:, 1:, :]\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward_loss(self, imgs, pred, mask):\n",
        "        \"\"\"\n",
        "        imgs: [N, 3, H, W]\n",
        "        pred: [N, L, p*p*3]\n",
        "        mask: [N, L], 0 is keep, 1 is remove,\n",
        "        \"\"\"\n",
        "        target = self.patchify(imgs)\n",
        "        if self.norm_pix_loss:\n",
        "            mean = target.mean(dim=-1, keepdim=True)\n",
        "            var = target.var(dim=-1, keepdim=True)\n",
        "            target = (target - mean) / (var + 1.e-6)**.5\n",
        "\n",
        "        loss = (pred - target) ** 2\n",
        "        loss = loss.mean(dim=-1)  # [N, L], mean loss per patch\n",
        "\n",
        "        loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches\n",
        "        return loss\n",
        "\n",
        "    def forward(self, imgs, mask_ratio=0.75):\n",
        "        latent, mask, ids_restore = self.forward_encoder(imgs, mask_ratio)\n",
        "        pred = self.forward_decoder(latent, ids_restore)  # [N, L, p*p*3]\n",
        "        loss = self.forward_loss(imgs, pred, mask)\n",
        "        return loss, pred, mask\n",
        "\n",
        "\n",
        "def mae_vit_base_patch16_dec512d8b(**kwargs):\n",
        "    model = MaskedAutoencoderViT(\n",
        "        patch_size=16, embed_dim=768, depth=12, num_heads=12,\n",
        "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
        "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def mae_vit_large_patch16_dec512d8b(**kwargs):\n",
        "    model = MaskedAutoencoderViT(\n",
        "        patch_size=16, embed_dim=1024, depth=24, num_heads=16,\n",
        "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
        "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def mae_vit_huge_patch14_dec512d8b(**kwargs):\n",
        "    model = MaskedAutoencoderViT(\n",
        "        patch_size=14, embed_dim=1280, depth=32, num_heads=16,\n",
        "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
        "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "# set recommended archs\n",
        "mae_vit_base_patch16 = mae_vit_base_patch16_dec512d8b  # decoder: 512 dim, 8 blocks\n",
        "mae_vit_large_patch16 = mae_vit_large_patch16_dec512d8b  # decoder: 512 dim, 8 blocks\n",
        "mae_vit_huge_patch14 = mae_vit_huge_patch14_dec512d8b  # decoder: 512 dim, 8 blocks\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "670p_tNSKg5i",
        "3dl1RyOjHdi1",
        "0vScvFGGSeN6",
        "oeazTakLj_Nn",
        "-CbC0ghOGgzf",
        "u99QqyJCqp_P",
        "5tqJqL5Vj2vL",
        "E9aP1IdNGG69",
        "rpBQCgArjj7x"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}