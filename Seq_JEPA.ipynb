{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNBcwlX1OJKLg0OOSBBAnVk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/Seq_JEPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "XwpnHW4wn9S1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cf347c4-564c-4820-fbae-6b588dbfccce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# @title data\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# transform = transforms.Compose([transforms.ToTensor(),])\n",
        "\n",
        "# train_data = torchvision.datasets.MNIST(root=\"data\", train=True, download=True,transform=transform) # do not normalise! want img in [0,1)\n",
        "# test_data = torchvision.datasets.MNIST(root=\"data\", train=False, download=True, transform=transform) #opt no download\n",
        "# # batch_size = 32 # 64 512\n",
        "# batch_size = 32 if torch.cuda.is_available() else 32\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "train_data = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
        "test_data = torchvision.datasets.CIFAR10(root='data', train=False, download=True, transform=transform)\n",
        "batch_size = 16 # 4\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "\n",
        "# dataiter = iter(train_data)\n",
        "# x,y = next(dataiter)\n",
        "# print(x.shape) # [3, 32, 32]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title (Learned)RotEmb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, seq_len=512, base=10000):\n",
        "        super().__init__()\n",
        "        self.dim, self.base = dim, base\n",
        "        theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "        pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "        angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "        self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.base)\n",
        "        return x * self.rot_emb[:seq_len]\n",
        "\n",
        "# class LearnedRoPE(nn.Module): # learnt RoPE ; each tok is 1 pos\n",
        "#     def __init__(self, dim):\n",
        "#         super().__init__()\n",
        "#         self.weights = nn.Parameter(torch.randn(1, dim//2))\n",
        "\n",
        "#     def forward(self, x): #\n",
        "#         batch, seq_len, dim = x.shape\n",
        "#         # if rot_emb.shape[0] < seq_len: self.__init__(dim, seq_len)\n",
        "#         pos = torch.arange(seq_len).unsqueeze(1)\n",
        "#         angles = (self.weights * pos * 2*torch.pi).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "#         rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "#         return x * rot_emb.flatten(-2).unsqueeze(0)\n",
        "\n",
        "\n",
        "class LearnedRoPE(nn.Module): # learnt RoPE ; each tok is 1 pos\n",
        "    def __init__(self, dim, seq_len=512):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.weights = nn.Parameter(torch.randn(1, dim//2))\n",
        "        pos = torch.arange(seq_len).unsqueeze(1)\n",
        "        angles = (self.weights * pos * 2*torch.pi)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "        self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "    def forward(self, x): # [batch, seq_len, dim]\n",
        "        seq_len = x.size(1)\n",
        "        if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len)\n",
        "        return x * self.rot_emb[:seq_len]\n",
        "\n",
        "\n",
        "class RotEmb(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, top=torch.pi, base=10000):\n",
        "        super().__init__()\n",
        "        self.theta = top / (base ** (torch.arange(0, dim, step=2, device=device) / dim))\n",
        "        # self.theta = top / (base ** torch.linspace(0, 1, dim//2, device=device))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (pos.unsqueeze(-1) * self.theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [seq_len, dim]\n",
        "\n",
        "class LearnedRotEmb(nn.Module): # pos in R\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.weights = nn.Parameter(torch.randn((1, dim//2), device=device))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (self.weights * pos.unsqueeze(-1) * 2*torch.pi).unsqueeze(-1) # [batch, 1] * [1, dim//2] -> [batch, dim//2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [batch, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [batch, dim]\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "npc_xGtOz7DC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Transformer Model\n",
        "import torch\n",
        "from torch import nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Sequential(\n",
        "            nn.Linear(in_dim, d_model), #act,\n",
        "            # nn.Linear(d_model, d_model), act,\n",
        "        )\n",
        "        # self.embed = nn.Sequential(nn.Conv1d(in_dim,d_model,7,2,7//2), nn.MaxPool1d(3, 2, 3//2))\n",
        "        self.pos_encoder = RotEmb(d_model, top=1, base=1000)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, d_hid or d_model, dropout, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.d_model = d_model\n",
        "        self.cls = nn.Parameter(torch.randn(1,1,d_model))\n",
        "        self.lin = nn.Linear(d_model, out_dim or d_model)\n",
        "\n",
        "\n",
        "    def forward(self, src, src_key_padding_mask=None, cls_mask=None, context_indices=None, trg_indices=None): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # if cls_mask != None: src[cls_mask] = self.cls.to(src.dtype)\n",
        "\n",
        "        src = self.embed(src) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = src.shape\n",
        "        # src = self.pos_encoder(src)\n",
        "        if context_indices != None: src = src * self.pos_encoder(context_indices) # context/predictor # src = src + self.positional_emb[:,context_indices]\n",
        "        else: src = src * self.pos_encoder(torch.arange(seq, device=device)) # target # src = src + self.positional_emb[:,:seq]\n",
        "            # print(\"trans fwd\", src.shape, self.pos_encoder(src).shape)\n",
        "\n",
        "        if trg_indices != None: # [M, num_trg_toks]\n",
        "            pred_tokens = self.cls * self.pos_encoder(trg_indices) # [M, num_trg_toks, d_model] # pred_tokens = self.cls + self.positional_emb[0,trg_indices]\n",
        "            pred_tokens = pred_tokens.repeat(batch, 1, 1) # [batch*M, num_trg_toks, d_model]\n",
        "            src = src.repeat_interleave(trg_indices.shape[0], dim=0) # [batch, seq_len, d_model] -> [batch*M, seq_len, d_model]\n",
        "            src = torch.cat([src, pred_tokens], dim=1) # [batch*M, seq_len+num_trg_toks, d_model]\n",
        "\n",
        "        out = self.transformer_encoder(src, src_key_padding_mask=src_key_padding_mask) # float [seq_len, batch_size, d_model]\n",
        "        if trg_indices != None: out = out[:,seq:] # [batch*M, num_trg_toks, d_model]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "# batch, seq_len, d_model = 4,7,512\n",
        "# model = TransformerModel(d_model, nhead=8, nlayers=2, dropout=0.).to(device)\n",
        "# x =  torch.rand((batch, seq_len, d_model), device=device)\n",
        "# src_key_padding_mask = torch.stack([(torch.arange(seq_len) < seq_len - v) for v in torch.randint(seq_len, (batch,))]).to(device) # True will be ignored # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "# print(src_key_padding_mask)\n",
        "# out = model(x, src_key_padding_mask=src_key_padding_mask)\n",
        "# print(out.shape)\n",
        "# # print(out)\n"
      ],
      "metadata": {
        "id": "-8i1WLsvH_vn"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "8ba31127-d6e5-438a-aede-5c789557ab39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00957aba-af87-4da1-c974-1103a4fc825a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "113472\n",
            "torch.Size([])\n"
          ]
        }
      ],
      "source": [
        "# @title SeqJEPA\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "def multiblock(seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "    return target_mask\n",
        "\n",
        "class SeqJEPA(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, num_layers=1, num_classes=10):\n",
        "        super().__init__()\n",
        "        if out_dim is None: out_dim = d_model\n",
        "        act = nn.GELU() # ReLU GELU SiLU\n",
        "        # self.rot_emb = RotEmb(d_model, top=1, base=10)\n",
        "        # self.rot_emb = RotEmb(d_model, top=torch.pi, base=10)\n",
        "        # self.rot_emb = RoPE(d_model, seq_len=1024, base=10)\n",
        "        # self.rot_emb = LearnedRotEmb(dim)\n",
        "        self.encode = nn.Sequential(\n",
        "            nn.Linear(in_dim, d_model), #act,\n",
        "            # nn.Linear(d_model, d_model), act,\n",
        "        )\n",
        "        # self.encode = nn.Sequential(nn.Conv1d(in_dim, d_model,7,2,7//2), nn.MaxPool1d(3, 2, 3//2))\n",
        "        self.context_encoder = TransformerModel(d_model, d_model, out_dim=out_dim, nhead=4, nlayers=2, dropout=0.)\n",
        "        self.predicter = TransformerModel(out_dim, d_model//2, out_dim, nhead=4, nlayers=1, dropout=0.)\n",
        "        self.target_encoder = AveragedModel(self.context_encoder, multi_avg_fn=get_ema_multi_avg_fn(0.95)) # 0.999\n",
        "        # self.target_encoder.requires_grad_(False)\n",
        "        # self.classifier = nn.Linear(out_dim, num_classes)\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]\n",
        "        x = self.encode(x) # [batch, T, d_model]\n",
        "        # x = self.encode(x.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "\n",
        "        batch, seq, dim = x.shape\n",
        "        M=4\n",
        "        target_mask = multiblock(seq, min_s=0.15, max_s=0.2, M=M) # mask out targets to be predicted # [M, seq]\n",
        "        context_mask = ~multiblock(seq, min_s=0.85, max_s=1., M=1)|target_mask.reshape(M,seq).any(0) # [1, seq], True->Mask\n",
        "        target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "        # print(target_mask, context_mask)\n",
        "        sorted_mask, ids = context_mask.int().sort(dim=1, stable=True)\n",
        "        context_indices = ids[~sorted_mask.bool()] # int idx [num_context_toks] , idx of context not masked\n",
        "        sorted_x = x[torch.arange(batch).unsqueeze(-1), context_indices] # [batch, num_context_toks, d_model]\n",
        "        # print(sorted_x.shape, context_indices.shape)[2, 9, 32]) torch.Size([9])\n",
        "        sorted_sx = self.context_encoder(sorted_x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "        del sorted_x\n",
        "\n",
        "        sorted_mask, ids = target_mask.int().sort(dim=1, stable=True)\n",
        "        # print(sorted_mask.shape, ids.shape, ids[~sorted_mask.bool()].shape, sorted_mask.sum(-1))\n",
        "        trg_indices = ids[sorted_mask.bool()].reshape(M,-1) # int idx [M, num_trg_toks] , idx of targets that are masked\n",
        "        # sorted_x = x[torch.arange(batch)[...,None,None], indices] # [batch, M, seq-num_trg_toks, dim]\n",
        "        # sx = sorted_sx[torch.arange(batch).unsqueeze(-1),ids.argsort(1)]\n",
        "        # print(sorted_sx.shape, trg_indices.shape)\n",
        "        sy_ = self.predicter(sorted_sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "\n",
        "        # sx = self.context_encoder(x, src_key_padding_mask=context_mask).repeat(M,1,1)\n",
        "        # # sy_ = self.predicter(sx, cls_mask=target_mask)*target_mask.unsqueeze(-1)\n",
        "        # sy_ = self.predicter(sx, trg_mask=target_mask)*target_mask.unsqueeze(-1)\n",
        "\n",
        "        # print(sy_.shape, target_mask.shape)\n",
        "        # print(self.target_encoder(x).repeat_interleave(M, dim=0).shape, trg_indices.repeat(batch,1).shape)\n",
        "        sy = self.target_encoder(x).repeat_interleave(M, dim=0)[torch.arange(batch*M).unsqueeze(-1), trg_indices.repeat(batch,1)] # [batch*M, num_trg_toks, out_dim]\n",
        "\n",
        "        # print(sy.shape, sy_.shape)\n",
        "        # loss = F.smooth_l1_loss(sy, sy_)\n",
        "        loss = F.mse_loss(sy, sy_)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        x = self.encode(x)\n",
        "        # x = self.encode(x.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "        sx = self.target_encoder(x)\n",
        "        out = sx.mean(dim=1)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "seq_jepa = SeqJEPA(in_dim=3, d_model=32, out_dim=16, num_layers=1).to(device)#.to(torch.float)\n",
        "optim = torch.optim.AdamW(seq_jepa.parameters(), lr=1e-3)\n",
        "print(sum(p.numel() for p in seq_jepa.parameters() if p.requires_grad)) # 59850\n",
        "\n",
        "\n",
        "x = torch.rand((2,20,3), device=device)\n",
        "out = seq_jepa.loss(x)\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# context_encoder = TransformerModel(d_model, d_model, out_dim=out_dim, nhead=4, nlayers=2, dropout=0.)\n",
        "# self.predicter = TransformerModel(out_dim, d_model//2, out_dim, nhead=4, nlayers=1, dropout=0.)\n",
        "# self.target_encoder\n",
        "print(sum(p.numel() for p in seq_jepa.target_encoder.parameters() if p.requires_grad)) # 59850\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8F8cuvFwMaYk",
        "outputId": "3d85990c-4b33-40e6-c63b-2d99c4411b78"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test multiblock mask\n",
        "def multiblock(seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    # mask_len, mask_pos = mask_len * seq, mask_pos * seq\n",
        "    indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "    return target_mask\n",
        "\n",
        "\n",
        "seq=400\n",
        "M=4\n",
        "\n",
        "# target_mask = multiblock(M, seq, min_s=0.15, max_s=0.2, M=1) # mask out targets to be predicted # [4*batch, seq]\n",
        "# context_mask = ~multiblock(1, seq, min_s=0.85, max_s=1., M=1)|target_mask.reshape(M,seq).any(0) # [batch, seq]\n",
        "\n",
        "\n",
        "target_mask = multiblock(seq, min_s=0.15, max_s=0.2, M=4) # mask out targets to be predicted # [4*batch, seq]\n",
        "context_mask = ~multiblock(seq, min_s=0.85, max_s=1., M=1)|target_mask.reshape(M,seq).any(0) # [batch, seq]\n",
        "\n",
        "# target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "\n",
        "# print(target_mask)\n",
        "# print(context_mask)\n",
        "\n",
        "print(target_mask.sum(-1))\n",
        "print(context_mask.sum(-1))\n",
        "\n",
        "\n",
        "# sorted_mask, ids = context_mask.sort(dim=1, stable=True)\n",
        "# indices = ids[~sorted_mask]#.reshape(M,-1) # int idx [num_context_toks] , idx of context not masked\n",
        "# sorted_x = x[torch.arange(batch).unsqueeze(-1), indices] # [batch, seq-num_trg_toks, dim]\n",
        "# print(indices)\n",
        "# print(x)\n",
        "# print(sorted_x)\n",
        "\n",
        "\n",
        "\n",
        "# sorted_mask, ids = target_mask.sort(dim=1, descending=False, stable=True)\n",
        "# # print(sorted_mask, ids)\n",
        "# indices = ids[~sorted_mask].reshape(M,-1) # int idx [M, seq-num_trg_toks] , idx of target not masked\n",
        "# print(indices)\n",
        "# batch=3\n",
        "# dim=2\n",
        "# # indices = indices.expand(batch,-1,-1)\n",
        "# # x = torch.arange(batch*seq).reshape(batch,seq).to(device)\n",
        "# x = torch.rand(batch, seq, dim)\n",
        "# print(x)\n",
        "# sorted_x = x[torch.arange(batch)[...,None,None], indices]\n",
        "# print(sorted_x.shape) # [batch, M, seq-num_trg_toks, dim]\n",
        "# print(sorted_x)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "p8xv1tpKIhim",
        "outputId": "e6613fee-2d32-4c6a-ef96-dee18da3aa6e"
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([73, 73, 73, 73])\n",
            "tensor([248])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(16).to(device)\n",
        "# coptim = torch.optim.AdamW(classifier.parameters(), lr=1e-3)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "wjK1TVwBh2u8"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(seq_jepa.classifier.weight[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IACKDymhit7",
        "outputId": "48b0ed81-556d-49e3-bdc8-f5fae906555a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.2311,  0.2408, -0.2281,  0.0604,  0.1454, -0.1304, -0.2386, -0.0326,\n",
            "        -0.0012,  0.1645, -0.0006, -0.0675, -0.0903, -0.1517, -0.2304, -0.1201],\n",
            "       device='cuda:0', grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "34de8800-fa4b-4460-803c-3cd8dbfb6c70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56e93c96-004b-4b78-b208-023dbb93df93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "strain 0.7233960628509521\n",
            "strain 0.7047440409660339\n",
            "strain 0.6477567553520203\n",
            "strain 0.5806209444999695\n",
            "strain 0.5063909888267517\n",
            "strain 0.4305904805660248\n",
            "strain 0.39544010162353516\n",
            "strain 0.36414071917533875\n",
            "strain 0.3248467445373535\n",
            "strain 0.2929871082305908\n",
            "strain 0.27568429708480835\n",
            "strain 0.2622128129005432\n",
            "strain 0.23925703763961792\n",
            "strain 0.21338587999343872\n",
            "strain 0.21342161297798157\n",
            "strain 0.21327579021453857\n",
            "strain 0.1973874419927597\n",
            "strain 0.16914187371730804\n",
            "strain 0.16834399104118347\n",
            "strain 0.17400102317333221\n",
            "strain 0.15849579870700836\n",
            "strain 0.1418035477399826\n",
            "strain 0.13174743950366974\n",
            "strain 0.13478507101535797\n",
            "strain 0.1215221956372261\n",
            "strain 0.10934145748615265\n",
            "strain 0.1080884113907814\n",
            "strain 0.10362914204597473\n",
            "strain 0.08588480949401855\n",
            "strain 0.0787363201379776\n",
            "strain 0.07616209983825684\n",
            "strain 0.06065196171402931\n",
            "strain 0.06353863328695297\n",
            "strain 0.06526054441928864\n",
            "strain 0.052797455340623856\n",
            "strain 0.044139690697193146\n",
            "strain 0.0389510840177536\n",
            "strain 0.037552643567323685\n",
            "strain 0.036937110126018524\n",
            "strain 0.027271011844277382\n",
            "strain 0.027730455622076988\n",
            "strain 0.025362635031342506\n",
            "strain 0.021652568131685257\n",
            "strain 0.01840360090136528\n",
            "strain 0.024469222873449326\n",
            "strain 0.017238082364201546\n",
            "strain 0.021867580711841583\n",
            "strain 0.012829078361392021\n",
            "strain 0.013517859391868114\n",
            "strain 0.010792150162160397\n",
            "classify 2.4150390625\n",
            "classify 2.3857421875\n",
            "classify 2.3466796875\n",
            "classify 2.2490234375\n",
            "classify 2.28466796875\n",
            "classify 2.3984375\n",
            "classify 2.31298828125\n",
            "classify 2.2666015625\n",
            "classify 2.35986328125\n",
            "classify 2.2333984375\n",
            "0.125\n",
            "0.125\n",
            "0.25\n",
            "0.0625\n",
            "0.0625\n",
            "0.0625\n",
            "0.0\n",
            "0.0\n",
            "0.1875\n",
            "0.125\n",
            "strain 0.01229874324053526\n",
            "strain 0.011883641593158245\n",
            "strain 0.012150215916335583\n",
            "strain 0.013428729958832264\n",
            "strain 0.011558068916201591\n",
            "strain 0.00973585620522499\n",
            "strain 0.009311391040682793\n",
            "strain 0.0073551698587834835\n",
            "strain 0.008634427562355995\n",
            "strain 0.008594170212745667\n",
            "strain 0.009215518832206726\n",
            "strain 0.007294449023902416\n",
            "strain 0.005179505329579115\n",
            "strain 0.006982496939599514\n",
            "strain 0.0063698492012917995\n",
            "strain 0.0072470842860639095\n",
            "strain 0.006033198907971382\n",
            "strain 0.005402666982263327\n",
            "strain 0.005763688124716282\n",
            "strain 0.0056765018962323666\n",
            "strain 0.004225904121994972\n",
            "strain 0.0046589309349656105\n",
            "strain 0.004498627036809921\n",
            "strain 0.00491719227284193\n",
            "strain 0.004824576433748007\n",
            "strain 0.004041464067995548\n",
            "strain 0.004206393379718065\n",
            "strain 0.005591116845607758\n",
            "strain 0.0035373324062675238\n",
            "strain 0.003980441018939018\n",
            "strain 0.003759578103199601\n",
            "strain 0.003272672649472952\n",
            "strain 0.003719388274475932\n",
            "strain 0.0034404457546770573\n",
            "strain 0.0031950343400239944\n",
            "strain 0.0031915626022964716\n",
            "strain 0.0031623798422515392\n",
            "strain 0.0037738154642283916\n",
            "strain 0.002835372230038047\n",
            "strain 0.0028394085820764303\n",
            "strain 0.003126621711999178\n",
            "strain 0.00247723376378417\n",
            "strain 0.0032543695997446775\n",
            "strain 0.002852531848475337\n",
            "strain 0.002531154779717326\n",
            "strain 0.0029932097531855106\n",
            "strain 0.002455791225656867\n",
            "strain 0.003102017566561699\n",
            "strain 0.002552423160523176\n",
            "strain 0.002376826014369726\n",
            "classify 2.35107421875\n",
            "classify 2.3046875\n",
            "classify 2.38720703125\n",
            "classify 2.3408203125\n",
            "classify 2.2861328125\n",
            "classify 2.35546875\n",
            "classify 2.3671875\n",
            "classify 2.3603515625\n",
            "classify 2.3427734375\n",
            "classify 2.345703125\n",
            "0.3125\n",
            "0.0\n",
            "0.0\n",
            "0.25\n",
            "0.0625\n",
            "0.25\n",
            "0.0625\n",
            "0.125\n",
            "0.125\n",
            "0.0\n",
            "strain 0.0029696549754589796\n",
            "strain 0.002232780447229743\n",
            "strain 0.002404944971203804\n",
            "strain 0.0022504879161715508\n",
            "strain 0.002700152574107051\n",
            "strain 0.0020584696903824806\n",
            "strain 0.0022135514300316572\n",
            "strain 0.0023752818815410137\n",
            "strain 0.0021739769726991653\n",
            "strain 0.001844568643718958\n",
            "strain 0.0023196260444819927\n",
            "strain 0.0019728424958884716\n",
            "strain 0.0028282233979552984\n",
            "strain 0.0020502323750406504\n",
            "strain 0.002094333292916417\n",
            "strain 0.0018895420944318175\n",
            "strain 0.0018067440250888467\n",
            "strain 0.0019929013215005398\n",
            "strain 0.001958389999344945\n",
            "strain 0.001744881272315979\n",
            "strain 0.0015556721482425928\n",
            "strain 0.0017810126300901175\n",
            "strain 0.001401383662596345\n",
            "strain 0.0015636298339813948\n",
            "strain 0.0017193494131788611\n",
            "strain 0.0019022177439182997\n",
            "strain 0.001395415747538209\n",
            "strain 0.001514001633040607\n",
            "strain 0.0016262057470157743\n",
            "strain 0.001734146848320961\n",
            "strain 0.001682201400399208\n",
            "strain 0.0015459742862731218\n",
            "strain 0.0017568878829479218\n",
            "strain 0.001448733382858336\n",
            "strain 0.0017542546847835183\n",
            "strain 0.0015015973476693034\n",
            "strain 0.0014541269047185779\n",
            "strain 0.001825157436542213\n",
            "strain 0.0013848430244252086\n",
            "strain 0.0012781552504748106\n",
            "strain 0.0016001646872609854\n",
            "strain 0.0012620397610589862\n",
            "strain 0.0012254484463483095\n",
            "strain 0.0010697036050260067\n",
            "strain 0.0011412805179134011\n",
            "strain 0.0016308203339576721\n",
            "strain 0.0013792157405987382\n",
            "strain 0.001220160978846252\n",
            "strain 0.0012946048518642783\n",
            "strain 0.0012098985025659204\n",
            "classify 2.33447265625\n",
            "classify 2.31640625\n",
            "classify 2.30126953125\n",
            "classify 2.2822265625\n",
            "classify 2.2822265625\n",
            "classify 2.3291015625\n",
            "classify 2.38671875\n",
            "classify 2.19091796875\n",
            "classify 2.21923828125\n",
            "classify 2.38330078125\n",
            "0.125\n",
            "0.125\n",
            "0.125\n",
            "0.1875\n",
            "0.0625\n",
            "0.1875\n",
            "0.0625\n",
            "0.125\n",
            "0.125\n",
            "0.0\n",
            "strain 0.0012363651767373085\n",
            "strain 0.0015006803441792727\n",
            "strain 0.0012868717312812805\n",
            "strain 0.0015118407318368554\n",
            "strain 0.0014672995312139392\n",
            "strain 0.0012159906327724457\n",
            "strain 0.0011843161191791296\n",
            "strain 0.0010633226484060287\n",
            "strain 0.0012440229766070843\n",
            "strain 0.0011232782853767276\n",
            "strain 0.0011663130717352033\n",
            "strain 0.0011438846122473478\n",
            "strain 0.0009766253642737865\n",
            "strain 0.0012112264521420002\n",
            "strain 0.001414290745742619\n",
            "strain 0.0011206172639504075\n",
            "strain 0.0009916384005919099\n",
            "strain 0.0009636335307732224\n",
            "strain 0.001063707284629345\n",
            "strain 0.0009744444396346807\n",
            "strain 0.0010791567619889975\n",
            "strain 0.0010425575310364366\n",
            "strain 0.0009232464944943786\n",
            "strain 0.0012092090910300612\n",
            "strain 0.0010793424444273114\n",
            "strain 0.0009915499249473214\n",
            "strain 0.001134816906414926\n",
            "strain 0.0012399681145325303\n",
            "strain 0.0009486695635132492\n",
            "strain 0.0006349797477014363\n",
            "strain 0.0007662485004402697\n",
            "strain 0.0007710342179052532\n",
            "strain 0.0008504808647558093\n",
            "strain 0.0010298817651346326\n",
            "strain 0.0007352180546149611\n",
            "strain 0.0012337527005001903\n",
            "strain 0.0010050327982753515\n",
            "strain 0.0008976953686214983\n",
            "strain 0.0009449409553781152\n",
            "strain 0.0007595366914756596\n",
            "strain 0.0006943731568753719\n",
            "strain 0.0007398540619760752\n",
            "strain 0.0008464318234473467\n",
            "strain 0.0008211609092541039\n",
            "strain 0.0010584956035017967\n",
            "strain 0.0009183112997561693\n",
            "strain 0.0009214685414917767\n",
            "strain 0.0007900234195403755\n",
            "strain 0.0009782606502994895\n",
            "strain 0.0007611126638948917\n",
            "classify 2.30419921875\n",
            "classify 2.365234375\n",
            "classify 2.3828125\n",
            "classify 2.32958984375\n",
            "classify 2.373046875\n",
            "classify 2.27392578125\n",
            "classify 2.3193359375\n",
            "classify 2.30712890625\n",
            "classify 2.34814453125\n",
            "classify 2.27685546875\n",
            "0.1875\n",
            "0.1875\n",
            "0.0625\n",
            "0.1875\n",
            "0.125\n",
            "0.125\n",
            "0.25\n",
            "0.0\n",
            "0.125\n",
            "0.125\n",
            "strain 0.0006677618366666138\n",
            "strain 0.0007183235720731318\n",
            "strain 0.0007882328354753554\n",
            "strain 0.0008208758081309497\n",
            "strain 0.0006807117024436593\n",
            "strain 0.0008525825687684119\n",
            "strain 0.0008195774862542748\n",
            "strain 0.0008857458014972508\n",
            "strain 0.0006946905632503331\n",
            "strain 0.0008701374754309654\n",
            "strain 0.0007076855399645865\n",
            "strain 0.0008827870478853583\n",
            "strain 0.0006718944641761482\n",
            "strain 0.0007479102932848036\n",
            "strain 0.0007455956656485796\n",
            "strain 0.0007133071194402874\n",
            "strain 0.0006691279704682529\n",
            "strain 0.0007369548548012972\n",
            "strain 0.0006987405940890312\n",
            "strain 0.0005596594419330359\n",
            "strain 0.0005908836028538644\n",
            "strain 0.0007055038004182279\n",
            "strain 0.0007228169706650078\n",
            "strain 0.0005758083425462246\n",
            "strain 0.0006331838667392731\n",
            "strain 0.0005972645594738424\n",
            "strain 0.000577392871491611\n",
            "strain 0.0006169338012114167\n",
            "strain 0.0006701837992295623\n",
            "strain 0.000785534386523068\n",
            "strain 0.0005339907365851104\n",
            "strain 0.0006187415565364063\n",
            "strain 0.0006781474803574383\n",
            "strain 0.0007368437945842743\n",
            "strain 0.0006012776866555214\n",
            "strain 0.0005988690536469221\n",
            "strain 0.00069249706575647\n",
            "strain 0.0006175253074616194\n",
            "strain 0.0007782779284752905\n",
            "strain 0.0006292767939157784\n",
            "strain 0.0005869761807844043\n",
            "strain 0.0005568405031226575\n",
            "strain 0.0006425221799872816\n",
            "strain 0.0006242319941520691\n",
            "strain 0.0006795897497795522\n",
            "strain 0.0005250998074188828\n",
            "strain 0.0006221355870366096\n",
            "strain 0.000657545228023082\n",
            "strain 0.0005972668877802789\n",
            "strain 0.0005316409515216947\n",
            "classify 2.2236328125\n",
            "classify 2.3212890625\n",
            "classify 2.38671875\n",
            "classify 2.3125\n",
            "classify 2.2578125\n",
            "classify 2.2666015625\n",
            "classify 2.330078125\n",
            "classify 2.265625\n",
            "classify 2.32421875\n",
            "classify 2.3720703125\n",
            "0.1875\n",
            "0.125\n",
            "0.1875\n",
            "0.0\n",
            "0.125\n",
            "0.125\n",
            "0.125\n",
            "0.0625\n",
            "0.0625\n",
            "0.0625\n",
            "strain 0.0005066836019977927\n",
            "strain 0.0005076194647699594\n",
            "strain 0.0004946599947288632\n",
            "strain 0.0006444680038839579\n",
            "strain 0.000641985738184303\n",
            "strain 0.0008002813556231558\n",
            "strain 0.0004535046173259616\n",
            "strain 0.0005983503069728613\n",
            "strain 0.0005744901718571782\n",
            "strain 0.0005057731759734452\n",
            "strain 0.0007605550927110016\n",
            "strain 0.0004695718816947192\n",
            "strain 0.0005756458849646151\n",
            "strain 0.0005113906227052212\n",
            "strain 0.00045571246300823987\n",
            "strain 0.0005389233701862395\n",
            "strain 0.0005195648409426212\n",
            "strain 0.0006571709527634084\n",
            "strain 0.00044610307668335736\n",
            "strain 0.00048766995314508677\n",
            "strain 0.0003742934495676309\n",
            "strain 0.0004520929360296577\n",
            "strain 0.0004589674645103514\n",
            "strain 0.0004556640633381903\n",
            "strain 0.000529985933098942\n",
            "strain 0.0004260175919625908\n",
            "strain 0.0006331222830340266\n",
            "strain 0.0005341634387150407\n",
            "strain 0.0005175852566026151\n",
            "strain 0.0005560024874284863\n",
            "strain 0.0005430122255347669\n",
            "strain 0.00041263882303610444\n",
            "strain 0.00041224865708500147\n",
            "strain 0.0005292095011100173\n",
            "strain 0.0008648679358884692\n",
            "strain 0.0004816373693756759\n",
            "strain 0.0005674608983099461\n",
            "strain 0.0007750481017865241\n",
            "strain 0.0004986388375982642\n",
            "strain 0.00051225780043751\n",
            "strain 0.00047650912893004715\n",
            "strain 0.0004246400494594127\n",
            "strain 0.0004953574389219284\n",
            "strain 0.0005160545697435737\n",
            "strain 0.0006034814286977053\n",
            "strain 0.00043776194797828794\n",
            "strain 0.0005210070521570742\n",
            "strain 0.0004107583954464644\n",
            "strain 0.0005230928072705865\n",
            "strain 0.00044620249536819756\n",
            "classify 2.404296875\n",
            "classify 2.33154296875\n",
            "classify 2.3779296875\n",
            "classify 2.3017578125\n",
            "classify 2.31298828125\n",
            "classify 2.3876953125\n",
            "classify 2.2509765625\n",
            "classify 2.27197265625\n",
            "classify 2.333984375\n",
            "classify 2.29052734375\n",
            "0.0\n",
            "0.0625\n",
            "0.0625\n",
            "0.125\n",
            "0.0\n",
            "0.0625\n",
            "0.0\n",
            "0.1875\n",
            "0.125\n",
            "0.0625\n",
            "strain 0.0003515216812957078\n",
            "strain 0.00038305585621856153\n",
            "strain 0.0004052321019116789\n",
            "strain 0.0003849297354463488\n",
            "strain 0.00038619019323959947\n",
            "strain 0.00039599474985152483\n",
            "strain 0.00036897501558996737\n",
            "strain 0.0004257623222656548\n",
            "strain 0.0004231531929690391\n",
            "strain 0.00030171783873811364\n",
            "strain 0.00038910494185984135\n",
            "strain 0.0003797250974457711\n",
            "strain 0.0003533618582878262\n",
            "strain 0.0004210411570966244\n",
            "strain 0.00033708236878737807\n",
            "strain 0.000522384827490896\n",
            "strain 0.000529147160705179\n",
            "strain 0.0003494442207738757\n",
            "strain 0.0003522908373270184\n",
            "strain 0.00032663598540239036\n",
            "strain 0.0003052232204936445\n",
            "strain 0.0004713799571618438\n",
            "strain 0.0004553103062789887\n",
            "strain 0.00035079556982964277\n",
            "strain 0.00040950559196062386\n",
            "strain 0.0006716624484397471\n",
            "strain 0.0004002252535428852\n",
            "strain 0.0004475231107790023\n",
            "strain 0.00042822593240998685\n",
            "strain 0.00042693523573689163\n",
            "strain 0.0003876267292071134\n",
            "strain 0.0005740520427934825\n",
            "strain 0.00029887654818594456\n",
            "strain 0.00032014850876294076\n",
            "strain 0.00039598593139089644\n",
            "strain 0.00047193816863000393\n",
            "strain 0.0003844729799311608\n",
            "strain 0.00032737539731897414\n",
            "strain 0.00043515980360098183\n",
            "strain 0.000372204085579142\n",
            "strain 0.00033351517049595714\n",
            "strain 0.0003843485319521278\n",
            "strain 0.0003868171479552984\n",
            "strain 0.00040776398964226246\n",
            "strain 0.0003551519475877285\n",
            "strain 0.0003342556592542678\n",
            "strain 0.0003500520542729646\n",
            "strain 0.0004344596527516842\n",
            "strain 0.000359584140824154\n",
            "strain 0.00037770092603750527\n",
            "classify 2.3046875\n",
            "classify 2.3447265625\n",
            "classify 2.2451171875\n",
            "classify 2.2978515625\n",
            "classify 2.3359375\n",
            "classify 2.28125\n",
            "classify 2.3466796875\n",
            "classify 2.3486328125\n",
            "classify 2.298828125\n",
            "classify 2.2998046875\n",
            "0.25\n",
            "0.125\n",
            "0.0\n",
            "0.0625\n",
            "0.0\n",
            "0.0\n",
            "0.25\n",
            "0.0625\n",
            "0.0\n",
            "0.1875\n",
            "strain 0.0003816764510702342\n",
            "strain 0.00041280328878201544\n",
            "strain 0.0002661723119672388\n",
            "strain 0.0003754833887796849\n",
            "strain 0.0002795275649987161\n",
            "strain 0.00033555898698978126\n",
            "strain 0.0003563761420082301\n",
            "strain 0.0003599050105549395\n",
            "strain 0.0002932662609964609\n",
            "strain 0.00033911471837200224\n",
            "strain 0.0004162898985669017\n",
            "strain 0.00046187869156710804\n",
            "strain 0.00043332463246770203\n",
            "strain 0.00041152918129228055\n",
            "strain 0.0002640247403178364\n",
            "strain 0.0003507515066303313\n",
            "strain 0.0005167447961866856\n",
            "strain 0.0003617246984504163\n",
            "strain 0.0004133243637625128\n",
            "strain 0.0004083062813151628\n",
            "strain 0.00030163300107233226\n",
            "strain 0.0003237914352212101\n",
            "strain 0.0003518689190968871\n",
            "strain 0.00029043620452284813\n",
            "strain 0.0003234650648664683\n",
            "strain 0.0003532321425154805\n",
            "strain 0.0003274398040957749\n",
            "strain 0.00036250552511774004\n",
            "strain 0.00033677445026114583\n",
            "strain 0.0003077106666751206\n",
            "strain 0.00032188076875172555\n",
            "strain 0.00033874053042382\n",
            "strain 0.0003410275385249406\n",
            "strain 0.00028671641484834254\n",
            "strain 0.00028535875026136637\n",
            "strain 0.00030644499929621816\n",
            "strain 0.00030300646903924644\n",
            "strain 0.0003106713411398232\n",
            "strain 0.00036387337604537606\n",
            "strain 0.0003859865537378937\n",
            "strain 0.0003109065582975745\n",
            "strain 0.0003221544320695102\n",
            "strain 0.0003111290279775858\n",
            "strain 0.0003644054231699556\n",
            "strain 0.0002526665630284697\n",
            "strain 0.0002656735014170408\n",
            "strain 0.0003991032426711172\n",
            "strain 0.00030114289256744087\n",
            "strain 0.00030939269345253706\n",
            "strain 0.0003331471816636622\n",
            "classify 2.326171875\n",
            "classify 2.3427734375\n",
            "classify 2.3330078125\n",
            "classify 2.3701171875\n",
            "classify 2.2314453125\n",
            "classify 2.3681640625\n",
            "classify 2.2080078125\n",
            "classify 2.2744140625\n",
            "classify 2.3720703125\n",
            "classify 2.3857421875\n",
            "0.0\n",
            "0.0625\n",
            "0.0625\n",
            "0.1875\n",
            "0.0625\n",
            "0.0\n",
            "0.25\n",
            "0.0\n",
            "0.125\n",
            "0.0625\n",
            "strain 0.00024176221631933004\n",
            "strain 0.00026623529265634716\n",
            "strain 0.0003162999637424946\n",
            "strain 0.00030159574816934764\n",
            "strain 0.00027399483951739967\n",
            "strain 0.0002548476913943887\n",
            "strain 0.00029173362418077886\n",
            "strain 0.00036290992284193635\n",
            "strain 0.00037512771086767316\n",
            "strain 0.0003290611202828586\n",
            "strain 0.0002445382997393608\n",
            "strain 0.0002658622106537223\n",
            "strain 0.000335842341883108\n",
            "strain 0.00019400632299948484\n",
            "strain 0.0004102623788639903\n",
            "strain 0.00041276717092841864\n",
            "strain 0.0002649516682140529\n",
            "strain 0.0002512158243916929\n",
            "strain 0.000262334244325757\n",
            "strain 0.00030101972515694797\n",
            "strain 0.00021807829034514725\n",
            "strain 0.00023851334117352962\n",
            "strain 0.00026379222981631756\n",
            "strain 0.00031039517489261925\n",
            "strain 0.00024443428264930844\n",
            "strain 0.00022910628467798233\n",
            "strain 0.00030270643765106797\n",
            "strain 0.00036198971793055534\n",
            "strain 0.0002298232284374535\n",
            "strain 0.00021975647541694343\n",
            "strain 0.00027876364765688777\n",
            "strain 0.0002907774178311229\n",
            "strain 0.00023636003606952727\n",
            "strain 0.00021827984892297536\n",
            "strain 0.00031490091350860894\n",
            "strain 0.00021128443768247962\n",
            "strain 0.00023263551702257246\n",
            "strain 0.0002492071071173996\n",
            "strain 0.0002715754962991923\n",
            "strain 0.0003087923105340451\n",
            "strain 0.00021619057224597782\n",
            "strain 0.0002660599129740149\n",
            "strain 0.00022859280579723418\n",
            "strain 0.0002793298044707626\n",
            "strain 0.0002327447582501918\n",
            "strain 0.0003270842134952545\n",
            "strain 0.00031529823900200427\n",
            "strain 0.00024400517577305436\n",
            "strain 0.00023662886815145612\n",
            "strain 0.00025979947531595826\n",
            "classify 2.2705078125\n",
            "classify 2.21875\n",
            "classify 2.275390625\n",
            "classify 2.3017578125\n",
            "classify 2.263671875\n",
            "classify 2.3359375\n",
            "classify 2.376953125\n",
            "classify 2.447265625\n",
            "classify 2.3515625\n",
            "classify 2.3720703125\n",
            "0.25\n",
            "0.125\n",
            "0.125\n",
            "0.0\n",
            "0.1875\n",
            "0.125\n",
            "0.1875\n",
            "0.0625\n",
            "0.0625\n",
            "0.25\n",
            "strain 0.000323992659104988\n",
            "strain 0.0002261150802951306\n",
            "strain 0.00026916328351944685\n",
            "strain 0.0002409116132184863\n",
            "strain 0.00023387488909065723\n",
            "strain 0.00023613461235072464\n",
            "strain 0.00024506644695065916\n",
            "strain 0.00030056663672439754\n",
            "strain 0.00020678005239460617\n",
            "strain 0.0002812985912896693\n",
            "strain 0.000227171418373473\n",
            "strain 0.00029155975789763033\n",
            "strain 0.0002231783582828939\n",
            "strain 0.0002076141827274114\n",
            "strain 0.00022999735665507615\n",
            "strain 0.0002771880535874516\n",
            "strain 0.00026314210845157504\n",
            "strain 0.00028930953703820705\n",
            "strain 0.0002970208879560232\n",
            "strain 0.00022765956236980855\n",
            "strain 0.00021448037296067923\n",
            "strain 0.00021663987718056887\n",
            "strain 0.0002946104505099356\n",
            "strain 0.00029784825164824724\n",
            "strain 0.00024637862225063145\n",
            "strain 0.00032007426489144564\n",
            "strain 0.00026841918588615954\n",
            "strain 0.00016889865219127387\n",
            "strain 0.00024607847444713116\n",
            "strain 0.0002515346568543464\n",
            "strain 0.00027067787596024573\n",
            "strain 0.00019541023357305676\n",
            "strain 0.000240876674070023\n",
            "strain 0.0002349955029785633\n",
            "strain 0.000272994686383754\n",
            "strain 0.00021315169578883797\n",
            "strain 0.0002195541892433539\n",
            "strain 0.00025624645058996975\n",
            "strain 0.0002712077694013715\n",
            "strain 0.0003150195989292115\n",
            "strain 0.0002318515325896442\n",
            "strain 0.0002042071137111634\n",
            "strain 0.00019744437304325402\n",
            "strain 0.00026863059611059725\n",
            "strain 0.0002448555896990001\n",
            "strain 0.0002503350260667503\n",
            "strain 0.0003396815445739776\n",
            "strain 0.00016623243573121727\n",
            "strain 0.0001420549233444035\n",
            "strain 0.00020672418759204447\n",
            "classify 2.423828125\n",
            "classify 2.2705078125\n",
            "classify 2.3369140625\n",
            "classify 2.296875\n",
            "classify 2.2880859375\n",
            "classify 2.2412109375\n",
            "classify 2.3125\n",
            "classify 2.330078125\n",
            "classify 2.3974609375\n",
            "classify 2.28515625\n",
            "0.0\n",
            "0.0625\n",
            "0.125\n",
            "0.0625\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0625\n",
            "0.0625\n",
            "0.0\n",
            "strain 0.00033026543678715825\n",
            "strain 0.00018672668375074863\n",
            "strain 0.0002231268008472398\n",
            "strain 0.00021426004241220653\n",
            "strain 0.0002906096342485398\n",
            "strain 0.00020762502390425652\n",
            "strain 0.0002673087874427438\n",
            "strain 0.0003381581627763808\n",
            "strain 0.00023050080926623195\n",
            "strain 0.0002189553197240457\n",
            "strain 0.000241351401200518\n",
            "strain 0.00026338189491070807\n",
            "strain 0.00020943296840414405\n",
            "strain 0.0001879601477412507\n",
            "strain 0.00020724219211842865\n",
            "strain 0.00018085757619701326\n",
            "strain 0.00020867300918325782\n",
            "strain 0.00017264304915443063\n",
            "strain 0.00015910643560346216\n",
            "strain 0.0001933322346303612\n",
            "strain 0.00025582811213098466\n",
            "strain 0.00018009066116064787\n",
            "strain 0.0002204982883995399\n",
            "strain 0.00018789185560308397\n",
            "strain 0.000277724233455956\n",
            "strain 0.0001923541713040322\n",
            "strain 0.00020771792333107442\n",
            "strain 0.00024071101506706327\n",
            "strain 0.00026244614855386317\n",
            "strain 0.00021216778259258717\n",
            "strain 0.0001872610446298495\n",
            "strain 0.0003032675595022738\n",
            "strain 0.00021211852435953915\n",
            "strain 0.00019431851978879422\n",
            "strain 0.00021766286226920784\n",
            "strain 0.00026241992600262165\n",
            "strain 0.00014805610408075154\n",
            "strain 0.00023497107031289488\n",
            "strain 0.0002199031732743606\n",
            "strain 0.00019861540931742638\n",
            "strain 0.00025009969249367714\n",
            "strain 0.00022356660338118672\n",
            "strain 0.00021234639280010015\n",
            "strain 0.00020018672512378544\n",
            "strain 0.0003020400763489306\n",
            "strain 0.00016812757530715317\n",
            "strain 0.00015156589506659657\n",
            "strain 0.00021608907263725996\n",
            "strain 0.00023256814165506512\n",
            "strain 0.00017453286272939295\n",
            "classify 2.2900390625\n",
            "classify 2.3818359375\n",
            "classify 2.3369140625\n",
            "classify 2.306640625\n",
            "classify 2.2470703125\n",
            "classify 2.283203125\n",
            "classify 2.32421875\n",
            "classify 2.328125\n",
            "classify 2.42578125\n",
            "classify 2.2998046875\n",
            "0.0625\n",
            "0.25\n",
            "0.125\n",
            "0.125\n",
            "0.0625\n",
            "0.1875\n",
            "0.1875\n",
            "0.0625\n",
            "0.125\n",
            "0.0\n",
            "strain 0.00017262111941818148\n",
            "strain 0.0001661763380980119\n",
            "strain 0.00023344783403445035\n",
            "strain 0.0002109277411364019\n",
            "strain 0.00020147109171375632\n",
            "strain 0.000191508253919892\n",
            "strain 0.00018866464961320162\n",
            "strain 0.00016373595281038433\n",
            "strain 0.0001736578269628808\n",
            "strain 0.00021221168572083116\n",
            "strain 0.00024179879983421415\n",
            "strain 0.00015713792527094483\n",
            "strain 0.0001841720804804936\n",
            "strain 0.0001986157731153071\n",
            "strain 0.00016995679470710456\n",
            "strain 0.00017329440743196756\n",
            "strain 0.0002577238774392754\n",
            "strain 0.00019648653687909245\n",
            "strain 0.0002074194635497406\n",
            "strain 0.00017129680782090873\n",
            "strain 0.0002136863477062434\n",
            "strain 0.00020125355513300747\n",
            "strain 0.00019058510952163488\n",
            "strain 0.0001969937584362924\n",
            "strain 0.0001955372135853395\n",
            "strain 0.00013613310875371099\n",
            "strain 0.0001511531590949744\n",
            "strain 0.00018806183652486652\n",
            "strain 0.00017871531599666923\n",
            "strain 0.00019703533325809985\n",
            "strain 0.0002582749293651432\n",
            "strain 0.00017336253949906677\n",
            "strain 0.00015711618470959365\n",
            "strain 0.000163758872076869\n",
            "strain 0.00019779341528192163\n",
            "strain 0.0001236985408468172\n",
            "strain 0.00028749293414875865\n",
            "strain 0.00013478471373673528\n",
            "strain 0.00013899280747864395\n",
            "strain 0.00016108964337036014\n",
            "strain 0.00013612433394882828\n",
            "strain 0.00017004959227051586\n",
            "strain 0.0001787934306776151\n",
            "strain 0.00018452323274686933\n",
            "strain 0.0001715758116915822\n",
            "strain 0.00016118944040499628\n",
            "strain 0.00017400126671418548\n",
            "strain 0.0002870262833312154\n",
            "strain 0.0001583948906045407\n",
            "strain 0.0002346320397919044\n",
            "classify 2.3291015625\n",
            "classify 2.3349609375\n",
            "classify 2.31640625\n",
            "classify 2.3017578125\n",
            "classify 2.2421875\n",
            "classify 2.2958984375\n",
            "classify 2.3876953125\n",
            "classify 2.28125\n",
            "classify 2.373046875\n",
            "classify 2.2734375\n",
            "0.0\n",
            "0.0\n",
            "0.0625\n",
            "0.1875\n",
            "0.25\n",
            "0.1875\n",
            "0.0\n",
            "0.0625\n",
            "0.1875\n",
            "0.3125\n",
            "strain 0.00016631773905828595\n",
            "strain 0.0002060530532617122\n",
            "strain 0.0001948172430275008\n",
            "strain 0.0002267657546326518\n",
            "strain 0.00016398768639191985\n",
            "strain 0.00016226634033955634\n",
            "strain 0.0002734157023951411\n",
            "strain 0.00015646772226318717\n",
            "strain 0.00014897425717208534\n",
            "strain 0.00020974611106794327\n",
            "strain 0.0002318828337593004\n",
            "strain 0.0001440344494767487\n",
            "strain 0.00015947438078001142\n",
            "strain 0.00020069243328180164\n",
            "strain 0.0002083211875287816\n",
            "strain 0.0002252002595923841\n",
            "strain 0.00014209213259164244\n",
            "strain 0.0001596840302227065\n",
            "strain 0.00015022308798506856\n",
            "strain 0.00020971304911654443\n",
            "strain 0.0001651435741223395\n",
            "strain 0.00016279268311336637\n",
            "strain 0.00017450620362069458\n",
            "strain 0.00019289653573650867\n",
            "strain 0.0002206582430517301\n",
            "strain 0.00017911696340888739\n",
            "strain 0.0001710001815808937\n",
            "strain 0.00019280589185655117\n",
            "strain 0.00013073217996861786\n",
            "strain 0.00027229529223404825\n",
            "strain 0.0001339004229521379\n",
            "strain 0.0001635449007153511\n",
            "strain 0.0002254651190014556\n",
            "strain 0.00014422407548408955\n",
            "strain 0.00016490541747771204\n",
            "strain 0.0001716026454232633\n",
            "strain 0.00020898554066661745\n",
            "strain 0.00026486339629627764\n",
            "strain 0.00014649635704699904\n",
            "strain 0.0001206910383189097\n",
            "strain 0.00016781443264335394\n",
            "strain 0.00013077094627078623\n",
            "strain 0.0002069983893306926\n",
            "strain 0.00014053474296815693\n",
            "strain 0.00016878153837751597\n",
            "strain 0.00015426416939590126\n",
            "strain 0.0001376971194986254\n",
            "strain 0.00018765294225886464\n",
            "strain 0.00019041621999349445\n",
            "strain 0.00014476438809651881\n",
            "classify 2.2705078125\n",
            "classify 2.3330078125\n",
            "classify 2.2421875\n",
            "classify 2.3369140625\n",
            "classify 2.3046875\n",
            "classify 2.287109375\n",
            "classify 2.3017578125\n",
            "classify 2.2255859375\n",
            "classify 2.384765625\n",
            "classify 2.2841796875\n",
            "0.125\n",
            "0.0\n",
            "0.0\n",
            "0.0625\n",
            "0.125\n",
            "0.0625\n",
            "0.1875\n",
            "0.0\n",
            "0.0625\n",
            "0.0625\n",
            "strain 0.00016171438619494438\n",
            "strain 0.00019315813551656902\n",
            "strain 0.00018036116671282798\n",
            "strain 0.00015762123803142458\n",
            "strain 0.0001593830093042925\n",
            "strain 0.0001545557752251625\n",
            "strain 0.00014518710668198764\n",
            "strain 0.0001283630554098636\n",
            "strain 0.00022376379638444632\n",
            "strain 0.00016655729268677533\n",
            "strain 0.00018151605036109686\n",
            "strain 0.00017186901823151857\n",
            "strain 0.00013727904297411442\n",
            "strain 0.00014969296171329916\n",
            "strain 0.0001443138171453029\n",
            "strain 0.00014597890549339354\n",
            "strain 0.00032258356804959476\n",
            "strain 0.00013361850869841874\n",
            "strain 0.00013393693370744586\n",
            "strain 0.00018049990467261523\n",
            "strain 0.00013239600230008364\n",
            "strain 0.00022431252000387758\n",
            "strain 0.00020397824118845165\n",
            "strain 0.00017032583127729595\n",
            "strain 0.0001968166179722175\n",
            "strain 0.0001512891030870378\n",
            "strain 0.00016629198216833174\n",
            "strain 0.00012501551827881485\n",
            "strain 0.00016220945690292865\n",
            "strain 0.0001748905488057062\n",
            "strain 0.00018928333884105086\n",
            "strain 0.00015870043716859072\n",
            "strain 0.00013378667063079774\n",
            "strain 0.0001495449396315962\n",
            "strain 0.00020837954070884734\n",
            "strain 0.00016432339907623827\n",
            "strain 0.00015149213140830398\n",
            "strain 0.0001879306073533371\n",
            "strain 0.0001608445745659992\n",
            "strain 0.00028391036903485656\n",
            "strain 0.00018240553617943078\n",
            "strain 0.00011402503150748089\n",
            "strain 0.0001950125879375264\n",
            "strain 0.00021949388610664755\n",
            "strain 0.00013870128896087408\n",
            "strain 0.0001874703448265791\n",
            "strain 0.00011619782162597403\n",
            "strain 0.00020769635739270598\n",
            "strain 0.00014086153532844037\n",
            "strain 0.00013744336320087314\n",
            "classify 2.373046875\n",
            "classify 2.349609375\n",
            "classify 2.2939453125\n",
            "classify 2.3486328125\n",
            "classify 2.400390625\n",
            "classify 2.3583984375\n",
            "classify 2.2294921875\n",
            "classify 2.380859375\n",
            "classify 2.330078125\n",
            "classify 2.30859375\n",
            "0.0\n",
            "0.0625\n",
            "0.125\n",
            "0.1875\n",
            "0.25\n",
            "0.0625\n",
            "0.3125\n",
            "0.125\n",
            "0.0\n",
            "0.0\n",
            "strain 0.00011049958266085014\n",
            "strain 0.00015809804608579725\n",
            "strain 0.00016034736472647637\n",
            "strain 0.00014837559137959033\n",
            "strain 0.00016895437147468328\n",
            "strain 0.0001753809192450717\n",
            "strain 0.0001618886017240584\n",
            "strain 0.0001452192518627271\n",
            "strain 0.00014992548676673323\n",
            "strain 0.00013998114445712417\n",
            "strain 0.00016392074758186936\n",
            "strain 0.00011931158223887905\n",
            "strain 0.00015435286331921816\n",
            "strain 0.00022922034258954227\n",
            "strain 0.0001243710721610114\n",
            "strain 0.00012267306738067418\n",
            "strain 0.00010928558185696602\n",
            "strain 0.00019467553647700697\n",
            "strain 0.00015294511104002595\n",
            "strain 0.00014764773368369788\n",
            "strain 0.00014705935609526932\n",
            "strain 0.00019260233966633677\n",
            "strain 0.00018257234478369355\n",
            "strain 0.00019134771719109267\n",
            "strain 0.00014578606351278722\n",
            "strain 0.00020891521126031876\n",
            "strain 0.00017796075553633273\n",
            "strain 0.0001516581542091444\n",
            "strain 0.00013475414016284049\n",
            "strain 0.0001527409185655415\n",
            "strain 0.0001149419549619779\n",
            "strain 0.00013438986206892878\n",
            "strain 0.0001488796406192705\n",
            "strain 0.0001113986800191924\n",
            "strain 0.0002086395543301478\n",
            "strain 0.00015503809845540673\n",
            "strain 9.372253407491371e-05\n",
            "strain 0.0001249341876246035\n",
            "strain 0.00014007986465003341\n",
            "strain 0.00011429141886765137\n",
            "strain 0.0001328654179815203\n",
            "strain 0.00014955420920159668\n",
            "strain 0.00018509970686864108\n",
            "strain 0.0001330684608547017\n",
            "strain 0.00011173223174409941\n",
            "strain 0.0001773422845872119\n",
            "strain 0.0001599749957676977\n",
            "strain 0.00016901694471016526\n",
            "strain 0.00012711077579297125\n",
            "strain 0.0001381112087983638\n",
            "classify 2.369140625\n",
            "classify 2.25390625\n",
            "classify 2.28515625\n",
            "classify 2.314453125\n",
            "classify 2.2998046875\n",
            "classify 2.341796875\n",
            "classify 2.3603515625\n",
            "classify 2.310546875\n",
            "classify 2.2919921875\n",
            "classify 2.2431640625\n",
            "0.1875\n",
            "0.0625\n",
            "0.1875\n",
            "0.125\n",
            "0.125\n",
            "0.0\n",
            "0.0625\n",
            "0.125\n",
            "0.0625\n",
            "0.0625\n",
            "strain 0.00014229696535039693\n",
            "strain 0.00012182837235741317\n",
            "strain 0.00013608219160232693\n",
            "strain 0.00013574383046943694\n",
            "strain 0.00012971354590263218\n",
            "strain 0.00021660364291165024\n",
            "strain 0.00013954371388535947\n",
            "strain 0.00017903752450365573\n",
            "strain 0.00016525368846487254\n",
            "strain 0.00013751078222412616\n",
            "strain 0.00013439616304822266\n",
            "strain 0.00014861773524899036\n",
            "strain 0.0001684152812231332\n",
            "strain 0.00013714777014683932\n",
            "strain 0.00012234877794981003\n",
            "strain 0.00017123376892413944\n",
            "strain 0.00012366112787276506\n",
            "strain 0.000158872629981488\n",
            "strain 0.00011569516937015578\n",
            "strain 0.00015272984455805272\n",
            "strain 0.00014369934797286987\n",
            "strain 0.00017835001926869154\n",
            "strain 0.00016981814405880868\n",
            "strain 0.00014078580716159195\n",
            "strain 0.0001513681490905583\n",
            "strain 0.0001669556222623214\n",
            "strain 0.00016045753727667034\n",
            "strain 0.00011170429206686094\n",
            "strain 0.00014437954814638942\n",
            "strain 0.00024445500457659364\n",
            "strain 0.00015335898206103593\n",
            "strain 0.00023765512742102146\n",
            "strain 0.00016392146062571555\n",
            "strain 0.0001524028484709561\n",
            "strain 0.00016980261716526002\n",
            "strain 0.0001204837899422273\n",
            "strain 0.00016429803508799523\n",
            "strain 0.00015673307643737644\n",
            "strain 0.00014587506302632391\n",
            "strain 0.0001754849508870393\n",
            "strain 0.00017215003026649356\n",
            "strain 0.00016610520833637565\n",
            "strain 0.00016715230594854802\n",
            "strain 0.00015299981168936938\n",
            "strain 0.00022369077487383038\n",
            "strain 0.0001212567585753277\n",
            "strain 0.000151737971464172\n",
            "strain 0.0001566206628922373\n",
            "strain 0.00015490890655200928\n",
            "strain 0.00023359204351436347\n",
            "classify 2.28125\n",
            "classify 2.3046875\n",
            "classify 2.34375\n",
            "classify 2.349609375\n",
            "classify 2.4404296875\n",
            "classify 2.3095703125\n",
            "classify 2.29296875\n",
            "classify 2.3369140625\n",
            "classify 2.3408203125\n",
            "classify 2.30078125\n",
            "0.125\n",
            "0.0\n",
            "0.1875\n",
            "0.0625\n",
            "0.1875\n",
            "0.25\n",
            "0.125\n",
            "0.1875\n",
            "0.125\n",
            "0.0625\n",
            "strain 0.00016836621216498315\n",
            "strain 0.00018195161828771234\n",
            "strain 0.0001969102449947968\n",
            "strain 0.0001815572613850236\n",
            "strain 0.0001124311747844331\n",
            "strain 9.739107917994261e-05\n",
            "strain 0.00012573925778269768\n",
            "strain 0.00015703748795203865\n",
            "strain 0.00011560469283722341\n",
            "strain 0.00016743273590691388\n",
            "strain 0.00015449892089236528\n",
            "strain 0.00011111345520475879\n",
            "strain 0.00017406087135896087\n",
            "strain 0.00013718467380385846\n",
            "strain 0.00012561693438328803\n",
            "strain 8.738250471651554e-05\n",
            "strain 0.00012584365322254598\n",
            "strain 0.0001415061269653961\n",
            "strain 0.000127845662063919\n",
            "strain 0.0001343785406788811\n",
            "strain 0.00010772491077659652\n",
            "strain 0.00012885502655990422\n",
            "strain 0.00012895342661067843\n",
            "strain 0.00014364227536134422\n",
            "strain 0.00013583120016846806\n",
            "strain 0.00014678327715955675\n",
            "strain 0.00017165559984277934\n",
            "strain 0.00014432109310291708\n",
            "strain 9.948644583346322e-05\n",
            "strain 0.0001104299517464824\n",
            "strain 0.00021916090918239206\n",
            "strain 0.0001318178983638063\n",
            "strain 0.00013424099597614259\n",
            "strain 0.00016125853289850056\n",
            "strain 0.0002062524581560865\n",
            "strain 0.00011623713362496346\n",
            "strain 9.249644790543243e-05\n",
            "strain 0.0001539986114948988\n",
            "strain 0.00014401640510186553\n",
            "strain 0.00011384087702026591\n",
            "strain 0.0001589824678376317\n",
            "strain 0.00013273701188154519\n",
            "strain 0.00013932243746239692\n",
            "strain 0.00011527271999511868\n",
            "strain 0.00011079682008130476\n",
            "strain 0.00011659177835099399\n",
            "strain 0.00011494264617795125\n",
            "strain 0.00014014740008860826\n",
            "strain 0.00013582114479504526\n",
            "strain 0.00011581517173908651\n",
            "classify 2.4619140625\n",
            "classify 2.3212890625\n",
            "classify 2.38671875\n",
            "classify 2.3349609375\n",
            "classify 2.34375\n",
            "classify 2.3203125\n",
            "classify 2.2646484375\n",
            "classify 2.3525390625\n",
            "classify 2.3564453125\n",
            "classify 2.2568359375\n",
            "0.125\n",
            "0.0\n",
            "0.0625\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.125\n",
            "0.0\n",
            "0.1875\n",
            "0.0625\n",
            "strain 0.0001460160274291411\n",
            "strain 0.00010902737267315388\n",
            "strain 0.00010958284110529348\n",
            "strain 0.00015692321176175028\n",
            "strain 0.00011069361062254757\n",
            "strain 0.00013971161388326436\n",
            "strain 0.00012395973317325115\n",
            "strain 0.00012493407120928168\n",
            "strain 0.0001271542569156736\n",
            "strain 0.00012249272549524903\n",
            "strain 0.0001472353469580412\n",
            "strain 0.00012893660459667444\n",
            "strain 0.00011845215340144932\n",
            "strain 0.00013742643932346255\n",
            "strain 0.0001628013123990968\n",
            "strain 9.694587060948834e-05\n",
            "strain 0.00014516054943669587\n",
            "strain 0.00010729287896538153\n",
            "strain 0.00010393791308160871\n",
            "strain 0.00013231849879957736\n",
            "strain 0.00017291946278419346\n",
            "strain 9.819502884056419e-05\n",
            "strain 0.0001720274449326098\n",
            "strain 0.00010717947589000687\n",
            "strain 0.00011371121945558116\n",
            "strain 0.00011977633403148502\n",
            "strain 0.00016421228065155447\n",
            "strain 0.0001095142142730765\n",
            "strain 0.0001132676043198444\n",
            "strain 0.000151635889778845\n",
            "strain 0.00013668209430761635\n",
            "strain 0.0001363972551189363\n",
            "strain 0.00010916569590335712\n",
            "strain 0.00012706330744549632\n",
            "strain 0.0001620711263967678\n",
            "strain 0.00019396886636968702\n",
            "strain 0.00023357891768682748\n",
            "strain 0.00017741392366588116\n",
            "strain 0.0001375007996102795\n",
            "strain 0.00016698171384632587\n",
            "strain 0.0001386011572321877\n",
            "strain 0.000182918636710383\n",
            "strain 0.0002135983668267727\n",
            "strain 0.00013839485472999513\n",
            "strain 0.00017359168850816786\n",
            "strain 0.00017950723122339696\n",
            "strain 0.00017522565030958503\n",
            "strain 0.00013705893070437014\n",
            "strain 0.0001729366194922477\n",
            "strain 0.00016680183762218803\n",
            "classify 2.3310546875\n",
            "classify 2.3466796875\n",
            "classify 2.419921875\n",
            "classify 2.31640625\n",
            "classify 2.376953125\n",
            "classify 2.265625\n",
            "classify 2.3193359375\n",
            "classify 2.240234375\n",
            "classify 2.3623046875\n",
            "classify 2.3212890625\n",
            "0.0625\n",
            "0.125\n",
            "0.1875\n",
            "0.0\n",
            "0.125\n",
            "0.125\n",
            "0.0625\n",
            "0.0\n",
            "0.125\n",
            "0.25\n",
            "strain 0.00018429936608299613\n",
            "strain 0.00019915758457500488\n",
            "strain 0.00012937070277985185\n",
            "strain 0.00016510917339473963\n",
            "strain 0.00016969244461506605\n",
            "strain 0.000170004423125647\n",
            "strain 0.0001290789368795231\n",
            "strain 0.0001337183202849701\n",
            "strain 0.00012065433111274615\n",
            "strain 0.00012578084715642035\n",
            "strain 0.0001945924013853073\n",
            "strain 0.00015603732026647776\n",
            "strain 0.00011946421000175178\n",
            "strain 0.00014492306218016893\n",
            "strain 0.0001396204170305282\n",
            "strain 0.00012836861424148083\n",
            "strain 0.00016655171930324286\n",
            "strain 0.00014136623940430582\n",
            "strain 0.00012670387513935566\n",
            "strain 0.00013181526446714997\n",
            "strain 0.00016636440705042332\n",
            "strain 0.00016138816135935485\n",
            "strain 0.00013219050015322864\n",
            "strain 0.00010165187268285081\n",
            "strain 0.00014265339996200055\n",
            "strain 0.00012319881352595985\n",
            "strain 0.00011431817983975634\n",
            "strain 0.00014598316920455545\n",
            "strain 0.00011268287198618054\n",
            "strain 0.00014523773279506713\n",
            "strain 0.00011379847273929045\n",
            "strain 8.122815052047372e-05\n",
            "strain 0.00015632118447683752\n",
            "strain 0.00010709201160352677\n",
            "strain 0.0001463979424443096\n",
            "strain 0.00010134136391570792\n",
            "strain 9.527105430606753e-05\n",
            "strain 0.00012734263145830482\n",
            "strain 0.00011500554683152586\n",
            "strain 0.00010442347411299124\n",
            "strain 8.95149350981228e-05\n",
            "strain 0.000127094637718983\n",
            "strain 7.677268149564043e-05\n",
            "strain 0.00016526342369616032\n",
            "strain 0.00010069934069178998\n",
            "strain 0.00012534353299997747\n",
            "strain 0.00012062687164871022\n",
            "strain 0.00010619722161209211\n",
            "strain 0.0001376278669340536\n",
            "strain 0.00017033269978128374\n",
            "classify 2.3408203125\n",
            "classify 2.4345703125\n",
            "classify 2.244140625\n",
            "classify 2.2763671875\n",
            "classify 2.302734375\n",
            "classify 2.2841796875\n",
            "classify 2.34765625\n",
            "classify 2.3349609375\n",
            "classify 2.32421875\n",
            "classify 2.25390625\n",
            "0.0625\n",
            "0.1875\n",
            "0.125\n",
            "0.125\n",
            "0.1875\n",
            "0.1875\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0625\n",
            "strain 0.0001318070135312155\n",
            "strain 8.9326873421669e-05\n",
            "strain 8.70104631758295e-05\n",
            "strain 0.00010548694990575314\n",
            "strain 0.0001222348219016567\n",
            "strain 0.00015018589328974485\n",
            "strain 0.0001082560556824319\n",
            "strain 9.752261394169182e-05\n",
            "strain 0.00014942877169232816\n",
            "strain 0.00015219036140479147\n",
            "strain 0.00013472892169374973\n",
            "strain 7.998343062354252e-05\n",
            "strain 0.00012521944881882519\n",
            "strain 0.00013440790644381195\n",
            "strain 0.00011636377166723832\n",
            "strain 0.00013539803330786526\n",
            "strain 0.00014954089419916272\n",
            "strain 0.00013117610069457442\n",
            "strain 0.00010220362310064957\n",
            "strain 0.00012092780525563285\n",
            "strain 0.00011550656927283853\n",
            "strain 0.00010885280789807439\n",
            "strain 0.00014990328054409474\n",
            "strain 8.377317863050848e-05\n",
            "strain 0.00012364199210423976\n",
            "strain 0.00012499696458689868\n",
            "strain 0.00010505849058972672\n",
            "strain 0.00010250120249111205\n",
            "strain 0.00010436662705615163\n",
            "strain 0.00011114507651655003\n",
            "strain 0.0001327396894339472\n",
            "strain 7.972268213052303e-05\n",
            "strain 0.00010563989781076089\n",
            "strain 7.576629286631942e-05\n",
            "strain 0.00010000733163906261\n",
            "strain 9.118244634009898e-05\n",
            "strain 9.416363172931597e-05\n",
            "strain 0.0001284280006075278\n",
            "strain 0.00013350797235034406\n",
            "strain 9.420428978046402e-05\n",
            "strain 9.77482704911381e-05\n",
            "strain 9.880919242277741e-05\n",
            "strain 0.00010810694220708683\n",
            "strain 0.00013852752454113215\n",
            "strain 9.679399954620749e-05\n",
            "strain 7.499865023419261e-05\n",
            "strain 0.00012756981595885009\n",
            "strain 0.0001617248635739088\n",
            "strain 9.46231375564821e-05\n",
            "strain 9.826578752836213e-05\n",
            "classify 2.4208984375\n",
            "classify 2.234375\n",
            "classify 2.3828125\n",
            "classify 2.37109375\n",
            "classify 2.3828125\n",
            "classify 2.3271484375\n",
            "classify 2.2373046875\n",
            "classify 2.2626953125\n",
            "classify 2.26953125\n",
            "classify 2.291015625\n",
            "0.0625\n",
            "0.0625\n",
            "0.125\n",
            "0.0625\n",
            "0.0\n",
            "0.1875\n",
            "0.125\n",
            "0.25\n",
            "0.375\n",
            "0.0\n",
            "strain 7.505290705012158e-05\n",
            "strain 9.255811164621264e-05\n",
            "strain 9.729871817398816e-05\n",
            "strain 7.69646794651635e-05\n",
            "strain 0.000107406078313943\n",
            "strain 0.00011112608626717702\n",
            "strain 8.657250873511657e-05\n",
            "strain 0.00010359848238294944\n",
            "strain 0.00010174548515351489\n",
            "strain 8.040964894462377e-05\n",
            "strain 0.00011024072591681033\n",
            "strain 9.668399434303865e-05\n",
            "strain 8.955989324022084e-05\n",
            "strain 9.275181218981743e-05\n",
            "strain 7.70547630963847e-05\n",
            "strain 9.649567800806835e-05\n",
            "strain 6.720409146510065e-05\n",
            "strain 0.00011161173461005092\n",
            "strain 0.0001218949182657525\n",
            "strain 8.981485007097945e-05\n",
            "strain 0.00014706631191074848\n",
            "strain 0.00013400288298726082\n",
            "strain 9.102032345253974e-05\n",
            "strain 0.00011934697249671444\n",
            "strain 0.00010990555165335536\n",
            "strain 9.686392149887979e-05\n",
            "strain 9.907015191856772e-05\n",
            "strain 0.00011928232561331242\n",
            "strain 0.00013627103180624545\n",
            "strain 0.00010250365448882803\n",
            "strain 0.00010599974484648556\n",
            "strain 0.0001250124187208712\n",
            "strain 0.00011944456491619349\n",
            "strain 0.00011385750258341432\n",
            "strain 9.832859359448776e-05\n",
            "strain 9.198691986966878e-05\n",
            "strain 9.21890459721908e-05\n",
            "strain 8.739940676605329e-05\n",
            "strain 0.00011749142868211493\n",
            "strain 0.00010137166827917099\n",
            "strain 0.00010006418597185984\n",
            "strain 9.613150177756324e-05\n",
            "strain 0.00012112717377021909\n",
            "strain 0.00013919385673943907\n",
            "strain 8.880198583938181e-05\n",
            "strain 7.767294300720096e-05\n",
            "strain 7.217307575047016e-05\n",
            "strain 9.302407124778256e-05\n",
            "strain 8.716874435776845e-05\n",
            "strain 7.300765719264746e-05\n",
            "classify 2.3056640625\n",
            "classify 2.330078125\n",
            "classify 2.2216796875\n",
            "classify 2.392578125\n",
            "classify 2.322265625\n",
            "classify 2.3193359375\n",
            "classify 2.3916015625\n",
            "classify 2.365234375\n",
            "classify 2.3056640625\n",
            "classify 2.3701171875\n",
            "0.1875\n",
            "0.0625\n",
            "0.125\n",
            "0.125\n",
            "0.25\n",
            "0.1875\n",
            "0.0625\n",
            "0.0625\n",
            "0.0625\n",
            "0.0625\n",
            "strain 0.00010755701077869162\n",
            "strain 8.623059693491086e-05\n",
            "strain 6.326608126983047e-05\n",
            "strain 8.010915917111561e-05\n",
            "strain 0.00015860631538089365\n",
            "strain 0.00010008657409343868\n",
            "strain 0.0001271233631996438\n",
            "strain 0.00010379990271758288\n",
            "strain 0.00010836297587957233\n",
            "strain 7.255441596498713e-05\n",
            "strain 7.705354801146314e-05\n",
            "strain 0.00010016962187364697\n",
            "strain 0.00010652984929038212\n",
            "strain 6.71847301418893e-05\n",
            "strain 9.155106090474874e-05\n",
            "strain 0.000113655834866222\n",
            "strain 8.829684520605952e-05\n",
            "strain 7.591096073156223e-05\n",
            "strain 0.00010752495290944353\n",
            "strain 0.00010351264791097492\n",
            "strain 7.717745029367507e-05\n",
            "strain 9.772361227078363e-05\n",
            "strain 8.505031291861087e-05\n",
            "strain 8.474272908642888e-05\n",
            "strain 8.532343053957447e-05\n",
            "strain 0.00017993588699027896\n",
            "strain 9.237961785402149e-05\n",
            "strain 0.00016168590809684247\n",
            "strain 8.235393761424348e-05\n",
            "strain 0.00021537361317314208\n",
            "strain 0.00011107747559435666\n",
            "strain 0.0001710940123302862\n",
            "strain 0.00012652692385017872\n",
            "strain 0.00014704336354043335\n",
            "strain 8.878076187102124e-05\n",
            "strain 0.00018915606779046357\n",
            "strain 0.00017365494568366557\n",
            "strain 0.00013152058818377554\n",
            "strain 0.00014554831432178617\n",
            "strain 0.00010638534877216443\n",
            "strain 0.000212417624425143\n",
            "strain 0.00012894562678411603\n",
            "strain 0.00015525433991570026\n",
            "strain 0.00014318210014607757\n",
            "strain 9.666185360401869e-05\n",
            "strain 0.00012408033944666386\n",
            "strain 0.00011368729610694572\n",
            "strain 9.714092448120937e-05\n",
            "strain 0.00010647635645000264\n",
            "strain 0.00012441992294043303\n",
            "classify 2.2470703125\n",
            "classify 2.345703125\n",
            "classify 2.4140625\n",
            "classify 2.3076171875\n",
            "classify 2.3466796875\n",
            "classify 2.2900390625\n",
            "classify 2.263671875\n",
            "classify 2.341796875\n",
            "classify 2.314453125\n",
            "classify 2.275390625\n",
            "0.0625\n",
            "0.1875\n",
            "0.125\n",
            "0.125\n",
            "0.1875\n",
            "0.125\n",
            "0.125\n",
            "0.0625\n",
            "0.0625\n",
            "0.0625\n",
            "strain 9.424512973055243e-05\n",
            "strain 9.09980371943675e-05\n",
            "strain 0.00013014720752835274\n",
            "strain 0.0001355530257569626\n",
            "strain 9.56536823650822e-05\n",
            "strain 0.00010974807082675397\n",
            "strain 0.00010902427311521024\n",
            "strain 9.84672224149108e-05\n",
            "strain 0.0001082679518731311\n",
            "strain 0.00011315858137095347\n",
            "strain 7.511291187256575e-05\n",
            "strain 0.0001376923028146848\n",
            "strain 0.00010532049782341346\n",
            "strain 9.328652959084138e-05\n",
            "strain 8.467504085274413e-05\n",
            "strain 0.0001026517347781919\n",
            "strain 0.00012075749691575766\n",
            "strain 0.00012823441647924483\n",
            "strain 9.722417598823085e-05\n",
            "strain 0.00012758806406054646\n",
            "strain 0.00010318859131075442\n",
            "strain 0.00012439719284884632\n",
            "strain 0.00015333176997955889\n",
            "strain 0.00010939165076706558\n",
            "strain 0.0001265847240574658\n",
            "strain 8.821373921819031e-05\n",
            "strain 0.00010187398584093899\n",
            "strain 7.794211705913767e-05\n",
            "strain 0.00011672196706058457\n",
            "strain 8.925292786443606e-05\n",
            "strain 0.00011739690671674907\n",
            "strain 7.83006107667461e-05\n",
            "strain 9.936439892044291e-05\n",
            "strain 9.306486754212528e-05\n",
            "strain 7.724321039859205e-05\n",
            "strain 8.514319051755592e-05\n",
            "strain 0.00013599895464722067\n",
            "strain 9.249649883713573e-05\n",
            "strain 7.417264714604244e-05\n",
            "strain 8.825095574138686e-05\n",
            "strain 0.00011605834151851013\n",
            "strain 9.628773113945499e-05\n",
            "strain 7.099082722561434e-05\n",
            "strain 9.572771523380652e-05\n",
            "strain 0.00011142702715005726\n",
            "strain 0.00013994878099765629\n",
            "strain 0.00011997678666375577\n",
            "strain 9.997004963224754e-05\n",
            "strain 9.396844689035788e-05\n",
            "strain 0.00012850569328293204\n",
            "classify 2.3076171875\n",
            "classify 2.29296875\n",
            "classify 2.3232421875\n",
            "classify 2.259765625\n",
            "classify 2.365234375\n",
            "classify 2.369140625\n",
            "classify 2.330078125\n",
            "classify 2.37109375\n",
            "classify 2.3505859375\n",
            "classify 2.27734375\n",
            "0.1875\n",
            "0.0625\n",
            "0.0625\n",
            "0.125\n",
            "0.0\n",
            "0.125\n",
            "0.125\n",
            "0.0\n",
            "0.0\n",
            "0.1875\n",
            "strain 0.00011458912922535092\n",
            "strain 0.00011721976625267416\n",
            "strain 0.00014716330042574555\n",
            "strain 9.391281491843984e-05\n",
            "strain 0.00014699716120958328\n",
            "strain 0.000154094333993271\n",
            "strain 0.00012622970098163933\n",
            "strain 0.00012106558278901502\n",
            "strain 0.0001001182827167213\n",
            "strain 0.00015043084567878395\n",
            "strain 7.344718324020505e-05\n",
            "strain 0.00012353788770269603\n",
            "strain 0.00015920845908112824\n",
            "strain 9.506849164608866e-05\n",
            "strain 8.402986713917926e-05\n",
            "strain 9.170031262328848e-05\n",
            "strain 7.447980169672519e-05\n",
            "strain 0.00013943527301307768\n",
            "strain 9.765518916537985e-05\n",
            "strain 9.373063949169591e-05\n",
            "strain 8.406280539929867e-05\n",
            "strain 0.00010702521831262857\n",
            "strain 8.699826139491051e-05\n",
            "strain 7.015030132606626e-05\n",
            "strain 0.0001131368480855599\n",
            "strain 0.00011044892016798258\n",
            "strain 6.918360304553062e-05\n",
            "strain 0.00010821824980666861\n",
            "strain 0.00010129153815796599\n",
            "strain 9.089692321140319e-05\n",
            "strain 8.188926585717127e-05\n",
            "strain 0.00010135235061170533\n",
            "strain 6.384804873960093e-05\n",
            "strain 9.302038961322978e-05\n",
            "strain 0.00010461431520525366\n",
            "strain 0.00013887276872992516\n",
            "strain 0.00012041220179526135\n",
            "strain 0.00010697494872147217\n",
            "strain 0.0001133026453317143\n",
            "strain 9.845053864410147e-05\n",
            "strain 9.39784076763317e-05\n",
            "strain 0.00011603650636970997\n",
            "strain 8.62787346704863e-05\n",
            "strain 0.00011644887126749381\n",
            "strain 0.00010108246351592243\n",
            "strain 8.057613740675151e-05\n",
            "strain 8.344644447788596e-05\n",
            "strain 0.0001260146382264793\n",
            "strain 0.0001292666856897995\n",
            "strain 0.00013261870481073856\n",
            "classify 2.38671875\n",
            "classify 2.369140625\n",
            "classify 2.3125\n",
            "classify 2.337890625\n",
            "classify 2.240234375\n",
            "classify 2.3125\n",
            "classify 2.3134765625\n",
            "classify 2.3818359375\n",
            "classify 2.4287109375\n",
            "classify 2.2724609375\n",
            "0.0625\n",
            "0.125\n",
            "0.125\n",
            "0.0625\n",
            "0.0625\n",
            "0.0625\n",
            "0.0625\n",
            "0.25\n",
            "0.0625\n",
            "0.1875\n",
            "strain 0.00018290625303052366\n",
            "strain 8.405819971812889e-05\n",
            "strain 0.00020001015218440443\n",
            "strain 0.00015201748465187848\n",
            "strain 8.623690519016236e-05\n",
            "strain 0.00010918392217718065\n",
            "strain 0.0001165840876637958\n",
            "strain 0.00014022068353369832\n",
            "strain 0.00010384492634329945\n",
            "strain 0.00010207630111835897\n",
            "strain 0.00012178188626421615\n",
            "strain 8.334901212947443e-05\n",
            "strain 0.0001404122303938493\n",
            "strain 7.893422298366204e-05\n",
            "strain 7.979022484505549e-05\n",
            "strain 8.471843466395512e-05\n",
            "strain 0.0001129522715928033\n",
            "strain 7.860602636355907e-05\n",
            "strain 6.718006625305861e-05\n",
            "strain 0.0001014667286654003\n",
            "strain 0.00011086130689363927\n",
            "strain 6.908162322361022e-05\n",
            "strain 0.00013148154539521784\n",
            "strain 9.081027383217588e-05\n",
            "strain 7.050472049741074e-05\n",
            "strain 7.459618063876405e-05\n",
            "strain 7.83500581746921e-05\n",
            "strain 7.208731403807178e-05\n",
            "strain 6.51795489829965e-05\n",
            "strain 8.559072011848912e-05\n",
            "strain 0.0001219268306158483\n",
            "strain 6.473407847806811e-05\n",
            "strain 8.62700617290102e-05\n",
            "strain 0.00011209728108951822\n",
            "strain 9.066574421012774e-05\n",
            "strain 0.00011409994476707652\n",
            "strain 8.21399808046408e-05\n",
            "strain 8.94077675184235e-05\n",
            "strain 0.0001020120907924138\n",
            "strain 0.00010192345507675782\n",
            "strain 8.462976984446868e-05\n",
            "strain 0.00010020069021265954\n",
            "strain 7.220356201287359e-05\n",
            "strain 0.00010980803926941007\n",
            "strain 0.00012498136493377388\n",
            "strain 0.00011489438475109637\n",
            "strain 0.00011420503142289817\n",
            "strain 9.266218694392592e-05\n",
            "strain 0.00014133872173260897\n",
            "strain 0.0001131006283685565\n",
            "classify 2.2763671875\n",
            "classify 2.2509765625\n",
            "classify 2.2978515625\n",
            "classify 2.4169921875\n",
            "classify 2.3427734375\n",
            "classify 2.3642578125\n",
            "classify 2.36328125\n",
            "classify 2.2646484375\n",
            "classify 2.248046875\n",
            "classify 2.3798828125\n",
            "0.0625\n",
            "0.0625\n",
            "0.0625\n",
            "0.0625\n",
            "0.0625\n",
            "0.125\n",
            "0.125\n",
            "0.125\n",
            "0.0625\n",
            "0.0\n",
            "strain 0.00010575893975328654\n",
            "strain 0.00011372064909664914\n",
            "strain 9.349099127575755e-05\n",
            "strain 9.958408918464556e-05\n",
            "strain 0.00010612797632347792\n",
            "strain 9.045653860084713e-05\n",
            "strain 8.78626451594755e-05\n",
            "strain 6.682190723950043e-05\n",
            "strain 9.354025678476319e-05\n",
            "strain 0.00012114255514461547\n",
            "strain 0.00011617010022746399\n",
            "strain 9.799323743209243e-05\n",
            "strain 0.00011792498116847128\n",
            "strain 9.369813051307574e-05\n",
            "strain 0.00010425075743114576\n",
            "strain 0.00011046700092265382\n",
            "strain 5.658756708726287e-05\n",
            "strain 0.00012355948274489492\n",
            "strain 0.00012846040772274137\n",
            "strain 9.727901488076895e-05\n",
            "strain 8.755147428018972e-05\n",
            "strain 7.311975787160918e-05\n",
            "strain 7.586536958115175e-05\n",
            "strain 9.294888150179759e-05\n",
            "strain 8.643919136375189e-05\n",
            "strain 9.07288704183884e-05\n",
            "strain 9.277850040234625e-05\n",
            "strain 6.732414476573467e-05\n",
            "strain 0.00014587061014026403\n",
            "strain 8.806671394268051e-05\n",
            "strain 7.53006970626302e-05\n",
            "strain 0.00011349780834279954\n",
            "strain 8.219848677981645e-05\n",
            "strain 0.00011122438445454463\n",
            "strain 9.791766933631152e-05\n",
            "strain 5.241388498689048e-05\n",
            "strain 0.00011091782653238624\n",
            "strain 7.509958959417418e-05\n",
            "strain 8.55697799124755e-05\n",
            "strain 8.824191172607243e-05\n",
            "strain 8.334428275702521e-05\n",
            "strain 9.079533629119396e-05\n",
            "strain 0.0001282534358324483\n",
            "strain 0.00019660487305372953\n",
            "strain 6.93679103278555e-05\n",
            "strain 0.00017806128016673028\n",
            "strain 0.00017395327449776232\n",
            "strain 7.144237315515056e-05\n",
            "strain 0.00019834941485896707\n",
            "strain 0.0001223515282617882\n",
            "classify 2.2138671875\n",
            "classify 2.291015625\n",
            "classify 2.2939453125\n",
            "classify 2.349609375\n",
            "classify 2.30859375\n",
            "classify 2.375\n",
            "classify 2.3642578125\n",
            "classify 2.216796875\n",
            "classify 2.322265625\n",
            "classify 2.3486328125\n",
            "0.0625\n",
            "0.0625\n",
            "0.0\n",
            "0.25\n",
            "0.1875\n",
            "0.125\n",
            "0.0625\n",
            "0.0625\n",
            "0.1875\n",
            "0.0625\n",
            "strain 0.00011420989903854206\n",
            "strain 0.00010999961523339152\n",
            "strain 0.0001372375263599679\n",
            "strain 0.00015113521658349782\n",
            "strain 7.706105679972097e-05\n",
            "strain 0.00013007648522034287\n",
            "strain 7.619421376148239e-05\n",
            "strain 0.0001612763007869944\n",
            "strain 6.282377580646425e-05\n",
            "strain 0.00014269565872382373\n",
            "strain 6.264578405534849e-05\n",
            "strain 0.00013415783178061247\n",
            "strain 9.511527605354786e-05\n",
            "strain 7.090823783073574e-05\n",
            "strain 0.00014034201740287244\n",
            "strain 4.4887281546834856e-05\n",
            "strain 0.0002294628939125687\n",
            "strain 0.00013536811457015574\n",
            "strain 0.0001457647158531472\n",
            "strain 0.00017381856741849333\n",
            "strain 0.00011810316937044263\n",
            "strain 0.0003002320881932974\n",
            "strain 7.67257297411561e-05\n",
            "strain 0.0002762353396974504\n",
            "strain 0.0002215794229414314\n",
            "strain 0.0001283209421671927\n",
            "strain 0.00020741790649481118\n",
            "strain 9.162525384454057e-05\n",
            "strain 0.0003082949260715395\n",
            "strain 0.00010192555782850832\n",
            "strain 0.00019563861133065075\n",
            "strain 0.0001905971730593592\n",
            "strain 9.733632759889588e-05\n",
            "strain 0.00015083294420037419\n",
            "strain 7.760339212836698e-05\n",
            "strain 0.00012645410606637597\n",
            "strain 0.00011363837984390557\n",
            "strain 0.00016215913638006896\n",
            "strain 0.000195892367628403\n",
            "strain 5.703081842511892e-05\n",
            "strain 0.00017448290600441396\n",
            "strain 7.370980165433139e-05\n",
            "strain 0.0002007143630180508\n",
            "strain 0.00011953039938816801\n",
            "strain 0.00012611932470463216\n",
            "strain 0.00018713955068960786\n",
            "strain 6.130574183771387e-05\n",
            "strain 8.503660501446575e-05\n",
            "strain 7.359006849583238e-05\n",
            "strain 0.00011670753883663565\n",
            "classify 2.3115234375\n",
            "classify 2.3828125\n",
            "classify 2.2919921875\n",
            "classify 2.26953125\n",
            "classify 2.2314453125\n",
            "classify 2.3349609375\n",
            "classify 2.341796875\n",
            "classify 2.2900390625\n",
            "classify 2.279296875\n",
            "classify 2.3583984375\n",
            "0.125\n",
            "0.0625\n",
            "0.0\n",
            "0.0\n",
            "0.25\n",
            "0.1875\n",
            "0.0625\n",
            "0.125\n",
            "0.0625\n",
            "0.0625\n",
            "strain 5.739332482335158e-05\n",
            "strain 7.551215821877122e-05\n",
            "strain 5.829556903336197e-05\n",
            "strain 9.606800449546427e-05\n",
            "strain 7.508094859076664e-05\n",
            "strain 6.167221727082506e-05\n",
            "strain 9.452642552787438e-05\n",
            "strain 9.142432099906728e-05\n",
            "strain 0.00010448867396917194\n",
            "strain 9.856650285655633e-05\n",
            "strain 9.476016566623002e-05\n",
            "strain 7.703246956225485e-05\n",
            "strain 0.00010069707786897197\n",
            "strain 0.00010228597966488451\n",
            "strain 0.00010478063632035628\n",
            "strain 8.734906441532075e-05\n",
            "strain 7.266347529366612e-05\n",
            "strain 7.314672257052734e-05\n",
            "strain 8.884008275344968e-05\n",
            "strain 6.128042150521651e-05\n",
            "strain 0.00011148860357934609\n",
            "strain 0.00010994798503816128\n",
            "strain 6.45090767648071e-05\n",
            "strain 6.854967796243727e-05\n",
            "strain 0.00016472798597533256\n",
            "strain 0.00011894785711774603\n",
            "strain 0.00014595218817703426\n",
            "strain 0.00011575883399927989\n",
            "strain 6.905099871801212e-05\n",
            "strain 0.00011679941962938756\n",
            "strain 0.00011947624443564564\n",
            "strain 0.00010005399963119999\n",
            "strain 0.00010364639456383884\n",
            "strain 8.501100819557905e-05\n",
            "strain 6.31125585641712e-05\n",
            "strain 0.00010215240035904571\n",
            "strain 8.891568722901866e-05\n",
            "strain 7.419387839036062e-05\n",
            "strain 9.865295578492805e-05\n",
            "strain 9.588516695657745e-05\n",
            "strain 0.00011527161404956132\n",
            "strain 6.911446689628065e-05\n",
            "strain 7.19021845725365e-05\n",
            "strain 8.226429054047912e-05\n",
            "strain 9.733634942676872e-05\n",
            "strain 6.498918810393661e-05\n",
            "strain 8.422481187153608e-05\n",
            "strain 7.473773439414799e-05\n",
            "strain 6.980283797020093e-05\n",
            "strain 0.00010706816101446748\n",
            "classify 2.2333984375\n",
            "classify 2.384765625\n",
            "classify 2.2939453125\n",
            "classify 2.3681640625\n",
            "classify 2.3193359375\n",
            "classify 2.3271484375\n",
            "classify 2.306640625\n",
            "classify 2.2373046875\n",
            "classify 2.28515625\n",
            "classify 2.4140625\n",
            "0.125\n",
            "0.0625\n",
            "0.0625\n",
            "0.0\n",
            "0.1875\n",
            "0.125\n",
            "0.0625\n",
            "0.0625\n",
            "0.0625\n",
            "0.125\n",
            "strain 7.236206874949858e-05\n",
            "strain 5.397100903792307e-05\n",
            "strain 9.774190402822569e-05\n",
            "strain 0.00011134453234262764\n",
            "strain 4.533654646365903e-05\n",
            "strain 7.738247950328514e-05\n",
            "strain 6.584872608073056e-05\n",
            "strain 5.080582923255861e-05\n",
            "strain 5.5255401093745604e-05\n",
            "strain 8.604408503742889e-05\n",
            "strain 8.801245712675154e-05\n",
            "strain 0.000103582045994699\n",
            "strain 6.551316619152203e-05\n",
            "strain 7.994914631126449e-05\n",
            "strain 9.146740922005847e-05\n",
            "strain 7.457726314896718e-05\n",
            "strain 6.966456567170098e-05\n",
            "strain 8.811942825559527e-05\n",
            "strain 8.11408826848492e-05\n",
            "strain 0.00011022474063793197\n",
            "strain 8.221743337344378e-05\n",
            "strain 6.932196265552193e-05\n",
            "strain 6.62903839838691e-05\n",
            "strain 5.662580588250421e-05\n",
            "strain 8.541555871488526e-05\n",
            "strain 6.133547140052542e-05\n",
            "strain 9.327282896265388e-05\n",
            "strain 7.914111483842134e-05\n",
            "strain 5.710075856768526e-05\n",
            "strain 7.278074190253392e-05\n",
            "strain 6.315659265965223e-05\n",
            "strain 8.880002133082598e-05\n",
            "strain 4.5726861571893096e-05\n",
            "strain 7.810814713593572e-05\n",
            "strain 7.642061245860532e-05\n",
            "strain 7.737464329693466e-05\n",
            "strain 8.61420194269158e-05\n",
            "strain 7.667788304388523e-05\n",
            "strain 0.00012421593419276178\n",
            "strain 8.836112101562321e-05\n",
            "strain 0.00011219990847166628\n",
            "strain 0.00013238517567515373\n",
            "strain 8.063967106863856e-05\n",
            "strain 6.9790767156519e-05\n",
            "strain 0.00011189391079824418\n",
            "strain 6.601760833291337e-05\n",
            "strain 6.975718133617193e-05\n",
            "strain 8.530729974154383e-05\n",
            "strain 6.445600592996925e-05\n",
            "strain 4.915830140816979e-05\n",
            "classify 2.3359375\n",
            "classify 2.29296875\n",
            "classify 2.3193359375\n",
            "classify 2.29296875\n",
            "classify 2.3779296875\n",
            "classify 2.2744140625\n",
            "classify 2.2490234375\n",
            "classify 2.4091796875\n",
            "classify 2.3408203125\n",
            "classify 2.4267578125\n",
            "0.0\n",
            "0.125\n",
            "0.0\n",
            "0.125\n",
            "0.1875\n",
            "0.0\n",
            "0.0625\n",
            "0.0625\n",
            "0.125\n",
            "0.3125\n",
            "strain 7.36960573703982e-05\n",
            "strain 7.465417729690671e-05\n",
            "strain 6.6780237830244e-05\n",
            "strain 8.030594472074881e-05\n",
            "strain 7.767800707370043e-05\n",
            "strain 6.6369328123983e-05\n",
            "strain 9.201760985888541e-05\n",
            "strain 9.053557005245239e-05\n",
            "strain 9.607891843188554e-05\n",
            "strain 6.86381827108562e-05\n",
            "strain 0.00010515039321035147\n",
            "strain 8.71310694492422e-05\n",
            "strain 6.129410030553117e-05\n",
            "strain 8.03094808361493e-05\n",
            "strain 6.201535143190995e-05\n",
            "strain 6.89120715833269e-05\n",
            "strain 7.865309453336522e-05\n",
            "strain 9.922766184899956e-05\n",
            "strain 7.210157491499558e-05\n",
            "strain 0.0001000261545414105\n",
            "strain 7.645034929737449e-05\n",
            "strain 8.924326539272442e-05\n",
            "strain 0.00010224321886198595\n",
            "strain 9.543643682263792e-05\n",
            "strain 0.00010691999341361225\n",
            "strain 0.00011694301792886108\n",
            "strain 6.89811204210855e-05\n",
            "strain 7.133166218409315e-05\n",
            "strain 8.610592340119183e-05\n",
            "strain 0.00010187632142333314\n",
            "strain 7.872882997617126e-05\n",
            "strain 7.73752253735438e-05\n",
            "strain 7.438339525833726e-05\n",
            "strain 7.094717147992924e-05\n",
            "strain 0.00010717994882725179\n",
            "strain 0.00010469074913999066\n",
            "strain 0.00011636339331744239\n",
            "strain 9.690324804978445e-05\n",
            "strain 4.6448796638287604e-05\n",
            "strain 9.571838018018752e-05\n",
            "strain 7.332599489018321e-05\n",
            "strain 5.305910963215865e-05\n",
            "strain 7.692052167840302e-05\n",
            "strain 7.044055382721126e-05\n",
            "strain 6.036471313564107e-05\n",
            "strain 6.279411172727123e-05\n",
            "strain 7.550867303507403e-05\n",
            "strain 6.292069156188518e-05\n",
            "strain 6.722351827193052e-05\n",
            "strain 6.209113053046167e-05\n",
            "classify 2.3603515625\n",
            "classify 2.3046875\n",
            "classify 2.3251953125\n",
            "classify 2.306640625\n",
            "classify 2.26171875\n",
            "classify 2.31640625\n",
            "classify 2.255859375\n",
            "classify 2.3232421875\n",
            "classify 2.38671875\n",
            "classify 2.236328125\n",
            "0.0625\n",
            "0.0\n",
            "0.0\n",
            "0.1875\n",
            "0.0\n",
            "0.0625\n",
            "0.1875\n",
            "0.0625\n",
            "0.0625\n",
            "0.125\n",
            "strain 5.9398247685749084e-05\n",
            "strain 0.00010220494732493535\n",
            "strain 8.778037590673193e-05\n",
            "strain 7.596716022817418e-05\n",
            "strain 9.103889897232875e-05\n",
            "strain 4.694378367275931e-05\n",
            "strain 7.635954534634948e-05\n",
            "strain 5.3700616263085976e-05\n",
            "strain 8.419741789111868e-05\n",
            "strain 9.740952373249456e-05\n",
            "strain 0.00011778774205595255\n",
            "strain 8.63250024849549e-05\n",
            "strain 6.132656562840566e-05\n",
            "strain 0.00010644915892044082\n",
            "strain 8.812726446194574e-05\n",
            "strain 4.507600169745274e-05\n",
            "strain 7.420108886435628e-05\n",
            "strain 8.414030162384734e-05\n",
            "strain 6.526363722514361e-05\n",
            "strain 7.673088839510456e-05\n",
            "strain 7.266196189448237e-05\n",
            "strain 5.589790816884488e-05\n",
            "strain 5.9605143178487197e-05\n",
            "strain 6.971247785259038e-05\n",
            "strain 6.359138205880299e-05\n",
            "strain 4.506451296037994e-05\n",
            "strain 5.869464803254232e-05\n",
            "strain 0.00010268771438859403\n",
            "strain 5.449019590741955e-05\n",
            "strain 5.4094689403427765e-05\n",
            "strain 6.766349542886019e-05\n",
            "strain 5.231406248640269e-05\n",
            "strain 7.058125629555434e-05\n",
            "strain 0.00010378690785728395\n",
            "strain 0.00012713656178675592\n",
            "strain 7.239697151817381e-05\n",
            "strain 6.376273813657463e-05\n",
            "strain 0.00011130248458357528\n",
            "strain 8.925652946345508e-05\n",
            "strain 9.151252015726641e-05\n",
            "strain 0.0001121727327699773\n",
            "strain 0.00013153075997252017\n",
            "strain 6.569710240000859e-05\n",
            "strain 7.545938569819555e-05\n",
            "strain 8.374006574740633e-05\n",
            "strain 9.604605293134227e-05\n",
            "strain 9.65005747275427e-05\n",
            "strain 8.448672451777384e-05\n",
            "strain 6.378757097991183e-05\n",
            "strain 5.9913669247180223e-05\n",
            "classify 2.3359375\n",
            "classify 2.2802734375\n",
            "classify 2.3349609375\n",
            "classify 2.3486328125\n",
            "classify 2.3359375\n",
            "classify 2.30859375\n",
            "classify 2.3134765625\n",
            "classify 2.423828125\n",
            "classify 2.3876953125\n",
            "classify 2.37109375\n",
            "0.125\n",
            "0.1875\n",
            "0.0625\n",
            "0.125\n",
            "0.0\n",
            "0.125\n",
            "0.125\n",
            "0.25\n",
            "0.1875\n",
            "0.125\n",
            "strain 9.350667824037373e-05\n",
            "strain 4.938764323014766e-05\n",
            "strain 9.134432184509933e-05\n",
            "strain 0.00010452396236360073\n",
            "strain 9.608359687263146e-05\n",
            "strain 0.00011106721649412066\n",
            "strain 0.00011625736078713089\n",
            "strain 9.473827230976894e-05\n",
            "strain 5.984892413835041e-05\n",
            "strain 0.00010756358096841723\n",
            "strain 7.087759877322242e-05\n",
            "strain 8.13433580333367e-05\n",
            "strain 0.00011764957889681682\n",
            "strain 0.00011052915215259418\n",
            "strain 5.953508662059903e-05\n",
            "strain 9.470138320466504e-05\n",
            "strain 0.00013342774764169008\n",
            "strain 0.00010826018842635676\n",
            "strain 6.692021997878328e-05\n",
            "strain 0.00011795740283560008\n",
            "strain 9.413257066626102e-05\n",
            "strain 9.772655175765976e-05\n",
            "strain 0.00014345109229907393\n",
            "strain 8.796321344561875e-05\n",
            "strain 5.970876009087078e-05\n",
            "strain 9.566362132318318e-05\n",
            "strain 8.869923476595432e-05\n",
            "strain 6.782064883736894e-05\n",
            "strain 0.00015311050810851157\n",
            "strain 0.0001066418772097677\n",
            "strain 6.623607623623684e-05\n",
            "strain 7.465824455721304e-05\n",
            "strain 6.0567126638488844e-05\n",
            "strain 8.656213321955875e-05\n",
            "strain 0.00011465507122920826\n",
            "strain 8.922963752411306e-05\n",
            "strain 7.815741992089897e-05\n",
            "strain 4.948937930748798e-05\n",
            "strain 8.809812425170094e-05\n",
            "strain 6.282563845161349e-05\n",
            "strain 7.174441998358816e-05\n",
            "strain 9.675742330728099e-05\n",
            "strain 0.0001558836520416662\n",
            "strain 0.00011434778571128845\n",
            "strain 6.230002327356488e-05\n",
            "strain 0.00016474368749186397\n",
            "strain 0.00013011337432544678\n",
            "strain 6.929851952008903e-05\n",
            "strain 0.00028490895056165755\n",
            "strain 0.0002809285360854119\n",
            "classify 2.279296875\n",
            "classify 2.3359375\n",
            "classify 2.3603515625\n",
            "classify 2.3857421875\n",
            "classify 2.3466796875\n",
            "classify 2.3505859375\n",
            "classify 2.3642578125\n",
            "classify 2.35546875\n",
            "classify 2.2412109375\n",
            "classify 2.306640625\n",
            "0.1875\n",
            "0.0625\n",
            "0.125\n",
            "0.0625\n",
            "0.125\n",
            "0.0625\n",
            "0.0625\n",
            "0.1875\n",
            "0.1875\n",
            "0.125\n",
            "strain 7.957500929478556e-05\n",
            "strain 0.00016929190314840525\n",
            "strain 0.00015346294094342738\n",
            "strain 0.0001134720878326334\n",
            "strain 0.00011012486356776208\n",
            "strain 0.00017437657515984029\n",
            "strain 0.0001282682060264051\n",
            "strain 6.946177745703608e-05\n",
            "strain 0.00014993335935287178\n",
            "strain 0.00014867812569718808\n",
            "strain 7.490062125725672e-05\n",
            "strain 0.0001108259311877191\n",
            "strain 0.000147875864058733\n",
            "strain 4.427033491083421e-05\n",
            "strain 9.01260573300533e-05\n",
            "strain 0.00015909329522401094\n",
            "strain 0.00012766211875714362\n",
            "strain 6.148911052150652e-05\n",
            "strain 0.00013493055303115398\n",
            "strain 0.00011867482680827379\n",
            "strain 6.08120062679518e-05\n",
            "strain 0.00010810192179633304\n",
            "strain 0.00010190559987677261\n",
            "strain 4.469595296541229e-05\n",
            "strain 9.82759811449796e-05\n",
            "strain 9.504608169663697e-05\n",
            "strain 8.420810627285391e-05\n",
            "strain 0.00013137397763784975\n",
            "strain 0.0001203731371788308\n",
            "strain 7.939514762256294e-05\n",
            "strain 0.00020105768635403365\n",
            "strain 0.00030379468807950616\n",
            "strain 0.00012705178232863545\n",
            "strain 4.438751784618944e-05\n",
            "strain 0.00011510903277667239\n",
            "strain 6.313816993497312e-05\n",
            "strain 7.179580279625952e-05\n",
            "strain 9.55722716753371e-05\n",
            "strain 0.00010337524145143107\n",
            "strain 6.18387057329528e-05\n",
            "strain 0.00011217307473998517\n",
            "strain 0.00010184302664129063\n",
            "strain 5.7045112043851987e-05\n",
            "strain 6.190418935148045e-05\n",
            "strain 7.683546573389322e-05\n",
            "strain 7.469855336239561e-05\n",
            "strain 5.102058275952004e-05\n",
            "strain 5.904051795369014e-05\n",
            "strain 6.416823453037068e-05\n",
            "strain 7.13924819137901e-05\n",
            "classify 2.2109375\n",
            "classify 2.2958984375\n",
            "classify 2.216796875\n",
            "classify 2.22265625\n",
            "classify 2.3046875\n",
            "classify 2.33984375\n",
            "classify 2.3671875\n",
            "classify 2.2998046875\n",
            "classify 2.376953125\n",
            "classify 2.3125\n",
            "0.125\n",
            "0.0\n",
            "0.125\n",
            "0.125\n",
            "0.0625\n",
            "0.125\n",
            "0.125\n",
            "0.0\n",
            "0.0625\n",
            "0.125\n",
            "strain 8.685091597726569e-05\n",
            "strain 0.00011261011241003871\n",
            "strain 0.0001090259465854615\n",
            "strain 6.232089799595997e-05\n",
            "strain 6.376763485604897e-05\n",
            "strain 4.82815557916183e-05\n",
            "strain 7.275096868397668e-05\n",
            "strain 7.338130671996623e-05\n",
            "strain 7.80602713348344e-05\n",
            "strain 6.421197758754715e-05\n",
            "strain 8.408891881117597e-05\n",
            "strain 8.457598596578464e-05\n",
            "strain 7.594464113935828e-05\n",
            "strain 0.00011475852079456672\n",
            "strain 8.252342377090827e-05\n",
            "strain 7.67939563957043e-05\n",
            "strain 0.00015103077748790383\n",
            "strain 0.00021155904687475413\n",
            "strain 7.693460065638646e-05\n",
            "strain 7.821650069672614e-05\n",
            "strain 0.00019076121679972857\n",
            "strain 0.00013143180694896728\n",
            "strain 6.580151239177212e-05\n",
            "strain 0.00010522328375373036\n",
            "strain 0.00013069283158984035\n",
            "strain 4.242135401000269e-05\n",
            "strain 8.137185795931146e-05\n",
            "strain 0.0001651061902521178\n",
            "strain 7.920384086901322e-05\n",
            "strain 8.60696382005699e-05\n",
            "strain 0.00027325068367645144\n",
            "strain 0.0001714861864456907\n",
            "strain 4.4023123336955905e-05\n",
            "strain 0.00016498941113241017\n",
            "strain 0.00020844547543674707\n",
            "strain 6.44968677079305e-05\n",
            "strain 7.076580368448049e-05\n",
            "strain 0.00011201901361346245\n",
            "strain 5.885211794520728e-05\n",
            "strain 7.376114081125706e-05\n",
            "strain 0.0001296200352953747\n",
            "strain 0.0001561421959195286\n",
            "strain 5.203651380725205e-05\n",
            "strain 6.80883094901219e-05\n",
            "strain 9.22928738873452e-05\n",
            "strain 8.132965012919158e-05\n",
            "strain 8.663241169415414e-05\n",
            "strain 7.525310502387583e-05\n",
            "strain 7.551290764240548e-05\n",
            "strain 7.062700751703233e-05\n",
            "classify 2.298828125\n",
            "classify 2.345703125\n",
            "classify 2.3193359375\n",
            "classify 2.2958984375\n",
            "classify 2.310546875\n",
            "classify 2.2958984375\n",
            "classify 2.3662109375\n",
            "classify 2.333984375\n",
            "classify 2.294921875\n",
            "classify 2.3603515625\n",
            "0.0\n",
            "0.125\n",
            "0.0625\n",
            "0.0625\n",
            "0.0625\n",
            "0.0625\n",
            "0.125\n",
            "0.0\n",
            "0.0\n",
            "0.0625\n",
            "strain 4.1392315324628726e-05\n",
            "strain 7.15733622200787e-05\n",
            "strain 6.509556988021359e-05\n",
            "strain 7.920354255475104e-05\n",
            "strain 5.641551979351789e-05\n",
            "strain 6.676488555967808e-05\n",
            "strain 7.870844274293631e-05\n",
            "strain 7.245119195431471e-05\n",
            "strain 5.234175841906108e-05\n",
            "strain 6.15964163444005e-05\n",
            "strain 9.76018127403222e-05\n",
            "strain 6.010504876030609e-05\n",
            "strain 5.175263504497707e-05\n",
            "strain 5.9694997617043555e-05\n",
            "strain 6.14415475865826e-05\n",
            "strain 9.524804045213386e-05\n",
            "strain 6.023240348440595e-05\n",
            "strain 0.00010007716628024355\n",
            "strain 7.126403215806931e-05\n",
            "strain 6.622233195230365e-05\n",
            "strain 6.961389590287581e-05\n",
            "strain 6.930517702130601e-05\n",
            "strain 6.668792775599286e-05\n",
            "strain 0.00010884088260354474\n",
            "strain 7.231332710944116e-05\n",
            "strain 6.202320219017565e-05\n",
            "strain 5.343828161130659e-05\n",
            "strain 4.415700459503569e-05\n",
            "strain 6.046515409252606e-05\n",
            "strain 7.961113442433998e-05\n",
            "strain 5.8126628573518246e-05\n",
            "strain 8.994581003207713e-05\n",
            "strain 7.817899313522503e-05\n",
            "strain 6.391991337295622e-05\n",
            "strain 8.195849659387022e-05\n",
            "strain 8.323509246110916e-05\n",
            "strain 5.126580799696967e-05\n",
            "strain 7.557772187283263e-05\n",
            "strain 0.00010377440776210278\n",
            "strain 7.829738751752302e-05\n",
            "strain 7.29028761270456e-05\n",
            "strain 9.087497892323881e-05\n",
            "strain 6.393752846634015e-05\n",
            "strain 7.73198262322694e-05\n",
            "strain 7.063143857521936e-05\n",
            "strain 6.476286944234744e-05\n",
            "strain 5.81975509703625e-05\n",
            "strain 8.630403317511082e-05\n",
            "strain 6.134244904387742e-05\n",
            "strain 0.0001264092279598117\n",
            "classify 2.330078125\n",
            "classify 2.4228515625\n",
            "classify 2.3837890625\n",
            "classify 2.34765625\n",
            "classify 2.3212890625\n",
            "classify 2.3076171875\n",
            "classify 2.271484375\n",
            "classify 2.3046875\n",
            "classify 2.2646484375\n",
            "classify 2.296875\n",
            "0.1875\n",
            "0.3125\n",
            "0.125\n",
            "0.0\n",
            "0.0625\n",
            "0.0625\n",
            "0.1875\n",
            "0.125\n",
            "0.1875\n",
            "0.0625\n",
            "strain 9.946410864358768e-05\n",
            "strain 5.1605995395220816e-05\n",
            "strain 0.00018167054804507643\n",
            "strain 0.00028880685567855835\n",
            "strain 0.00012576417066156864\n",
            "strain 7.063108205329627e-05\n",
            "strain 0.00011547338363016024\n",
            "strain 8.143190643750131e-05\n",
            "strain 4.861260822508484e-05\n",
            "strain 0.0001985024573514238\n",
            "strain 0.0003509332600515336\n",
            "strain 0.00023629995121154934\n",
            "strain 8.077072561718524e-05\n",
            "strain 7.506922702305019e-05\n",
            "strain 0.00013315072283148766\n",
            "strain 0.00013527402188628912\n",
            "strain 6.699309597024694e-05\n",
            "strain 0.00010892121645156294\n",
            "strain 0.00021332134201657027\n",
            "strain 0.00017265531641896814\n",
            "strain 5.603141107712872e-05\n",
            "strain 6.85468185110949e-05\n",
            "strain 0.00011206152703380212\n",
            "strain 6.647624832112342e-05\n",
            "strain 0.00011767059186240658\n",
            "strain 8.778259507380426e-05\n",
            "strain 0.00010192632908001542\n",
            "strain 7.972503226483241e-05\n",
            "strain 0.000112439643999096\n",
            "strain 6.484676850959659e-05\n",
            "strain 4.5058004616294056e-05\n",
            "strain 9.252432937500998e-05\n",
            "strain 8.304484799737111e-05\n",
            "strain 6.389713962562382e-05\n",
            "strain 0.0001268213236471638\n",
            "strain 8.113787043839693e-05\n",
            "strain 9.118075104197487e-05\n",
            "strain 6.509178638225421e-05\n",
            "strain 8.222479664254934e-05\n",
            "strain 0.00010137022036360577\n",
            "strain 7.53426575101912e-05\n",
            "strain 8.828829595586285e-05\n",
            "strain 0.0001436379534425214\n",
            "strain 0.00010282490256940946\n",
            "strain 5.196869824430905e-05\n",
            "strain 6.47367414785549e-05\n",
            "strain 7.663678115932271e-05\n",
            "strain 6.483770994236693e-05\n",
            "strain 6.676968769170344e-05\n",
            "strain 7.265315798576921e-05\n",
            "classify 2.3408203125\n",
            "classify 2.2802734375\n",
            "classify 2.3310546875\n",
            "classify 2.3134765625\n",
            "classify 2.37890625\n",
            "classify 2.376953125\n",
            "classify 2.2822265625\n",
            "classify 2.314453125\n",
            "classify 2.3173828125\n",
            "classify 2.296875\n",
            "0.3125\n",
            "0.125\n",
            "0.25\n",
            "0.125\n",
            "0.0625\n",
            "0.1875\n",
            "0.0\n",
            "0.1875\n",
            "0.0625\n",
            "0.125\n",
            "strain 6.351547199301422e-05\n",
            "strain 7.700569403823465e-05\n",
            "strain 5.07381118950434e-05\n",
            "strain 3.755022044060752e-05\n",
            "strain 0.00010080783249577507\n",
            "strain 0.0001036709436448291\n",
            "strain 6.931823008926585e-05\n",
            "strain 5.886879444005899e-05\n",
            "strain 8.237374277086928e-05\n",
            "strain 6.207582919159904e-05\n",
            "strain 4.971500675310381e-05\n",
            "strain 9.680045332061127e-05\n",
            "strain 0.00014920058310963213\n",
            "strain 0.00010333658428862691\n",
            "strain 3.96511095459573e-05\n",
            "strain 6.929026130819693e-05\n",
            "strain 5.41120971320197e-05\n",
            "strain 4.647930836654268e-05\n",
            "strain 9.559522004565224e-05\n",
            "strain 5.330783824319951e-05\n",
            "strain 8.505988080287352e-05\n",
            "strain 7.062373333610594e-05\n",
            "strain 4.839004031964578e-05\n",
            "strain 9.250032599084079e-05\n",
            "strain 0.0001151792675955221\n",
            "strain 5.413961844169535e-05\n",
            "strain 0.0001476613833801821\n",
            "strain 0.00014037899381946772\n",
            "strain 0.00015330570749938488\n",
            "strain 9.679336653789505e-05\n",
            "strain 7.259762787725776e-05\n",
            "strain 0.00015281613741535693\n",
            "strain 0.00020443074754439294\n",
            "strain 0.000160049123223871\n",
            "strain 8.487846935167909e-05\n",
            "strain 7.089647260727361e-05\n",
            "strain 0.0001543467224109918\n",
            "strain 0.00018981278117280453\n",
            "strain 0.00011581216676859185\n",
            "strain 8.715313742868602e-05\n",
            "strain 6.544690404552966e-05\n",
            "strain 8.834745676722378e-05\n",
            "strain 0.00012579321628436446\n",
            "strain 8.081320993369445e-05\n",
            "strain 6.76377530908212e-05\n",
            "strain 8.112012437777594e-05\n",
            "strain 6.200488860486075e-05\n",
            "strain 9.600107296137139e-05\n",
            "strain 9.463557944400236e-05\n",
            "strain 4.231284401612356e-05\n",
            "classify 2.3310546875\n",
            "classify 2.4130859375\n",
            "classify 2.3056640625\n",
            "classify 2.359375\n",
            "classify 2.291015625\n",
            "classify 2.322265625\n",
            "classify 2.26953125\n",
            "classify 2.30859375\n",
            "classify 2.29296875\n",
            "classify 2.2998046875\n",
            "0.0625\n",
            "0.0625\n",
            "0.125\n",
            "0.3125\n",
            "0.1875\n",
            "0.25\n",
            "0.0625\n",
            "0.0\n",
            "0.0625\n",
            "0.125\n",
            "strain 8.340484782820567e-05\n",
            "strain 6.263124669203535e-05\n",
            "strain 6.841008143965155e-05\n",
            "strain 7.368905062321573e-05\n",
            "strain 6.150460831122473e-05\n",
            "strain 8.044542482821271e-05\n",
            "strain 5.839345249114558e-05\n",
            "strain 6.504654447780922e-05\n",
            "strain 9.411689097760245e-05\n",
            "strain 4.0147220715880394e-05\n",
            "strain 5.202376996749081e-05\n",
            "strain 4.563245965982787e-05\n",
            "strain 4.473814260563813e-05\n",
            "strain 5.238119047135115e-05\n",
            "strain 4.286214243620634e-05\n",
            "strain 8.515702211298048e-05\n",
            "strain 0.00010599845700198784\n",
            "strain 8.317636093124747e-05\n",
            "strain 5.9511789004318416e-05\n",
            "strain 7.98132095951587e-05\n",
            "strain 8.548478945158422e-05\n",
            "strain 6.560288602486253e-05\n",
            "strain 4.632642594515346e-05\n",
            "strain 8.447867003269494e-05\n",
            "strain 8.57920094858855e-05\n",
            "strain 4.2352545278845355e-05\n",
            "strain 3.9570277294842526e-05\n",
            "strain 4.967873974237591e-05\n",
            "strain 7.011640991549939e-05\n",
            "strain 7.970759907038882e-05\n",
            "strain 8.598642307333648e-05\n",
            "strain 7.35198482288979e-05\n",
            "strain 6.000788926030509e-05\n",
            "strain 3.5882185329683125e-05\n",
            "strain 5.355422763386741e-05\n",
            "strain 7.100022048689425e-05\n",
            "strain 5.9809128288179636e-05\n",
            "strain 7.083062519086525e-05\n",
            "strain 5.174016405362636e-05\n",
            "strain 5.908537787036039e-05\n",
            "strain 8.347051334567368e-05\n",
            "strain 0.00015304534463211894\n",
            "strain 0.00014823509263806045\n",
            "strain 3.631448635132983e-05\n",
            "strain 0.00019961365615017712\n",
            "strain 0.00039490556810051203\n",
            "strain 0.00027774577029049397\n",
            "strain 5.8588899264577776e-05\n",
            "strain 0.0001495095930295065\n",
            "strain 0.00024559686426073313\n",
            "classify 2.341796875\n",
            "classify 2.2978515625\n",
            "classify 2.349609375\n",
            "classify 2.38671875\n",
            "classify 2.267578125\n",
            "classify 2.33203125\n",
            "classify 2.310546875\n",
            "classify 2.2900390625\n",
            "classify 2.365234375\n",
            "classify 2.240234375\n",
            "0.0625\n",
            "0.0625\n",
            "0.0625\n",
            "0.25\n",
            "0.0625\n",
            "0.0625\n",
            "0.1875\n",
            "0.1875\n",
            "0.0\n",
            "0.25\n",
            "strain 0.0001767239737091586\n",
            "strain 4.976605123374611e-05\n",
            "strain 0.00013657529780175537\n",
            "strain 0.00031661323737353086\n",
            "strain 0.00019835667626466602\n",
            "strain 5.1473452913342044e-05\n",
            "strain 0.00013796619896311313\n",
            "strain 0.0001920006616273895\n",
            "strain 9.415773092769086e-05\n",
            "strain 7.302597805391997e-05\n",
            "strain 0.00017059038509614766\n",
            "strain 0.00016128882998600602\n",
            "strain 7.981574890436605e-05\n",
            "strain 0.00044346405775286257\n",
            "strain 0.0006518035661429167\n",
            "strain 0.00017618566926103085\n",
            "strain 0.00015825765149202198\n",
            "strain 0.0005432133912108839\n",
            "strain 0.00020748972019646317\n",
            "strain 0.00011625917250057682\n",
            "strain 0.0003493993717711419\n",
            "strain 7.732386438874528e-05\n",
            "strain 0.00038004640373401344\n",
            "strain 0.0008260160684585571\n",
            "strain 0.0001485369575675577\n",
            "strain 0.00047799540334381163\n",
            "strain 0.0007944332901388407\n",
            "strain 0.00010863344505196437\n",
            "strain 0.00021841241687070578\n",
            "strain 0.00012231850996613503\n",
            "strain 0.00018335357890464365\n",
            "strain 0.00022604118566960096\n",
            "strain 4.3631127482512966e-05\n",
            "strain 0.0001870813430286944\n",
            "strain 9.904810576699674e-05\n",
            "strain 0.00011319173063384369\n",
            "strain 7.297249976545572e-05\n",
            "strain 0.0003199091006536037\n",
            "strain 0.000242992551648058\n",
            "strain 7.583003025501966e-05\n",
            "strain 0.0003180148487444967\n",
            "strain 5.445341230370104e-05\n",
            "strain 0.00030107650673016906\n",
            "strain 0.0003397490072529763\n",
            "strain 6.267339631449431e-05\n",
            "strain 0.00014199047291185707\n",
            "strain 0.00011751888087019324\n",
            "strain 8.382573287235573e-05\n",
            "strain 0.0001816434960346669\n",
            "strain 8.251587132690474e-05\n",
            "classify 2.453125\n",
            "classify 2.3115234375\n",
            "classify 2.3017578125\n",
            "classify 2.2998046875\n",
            "classify 2.3037109375\n",
            "classify 2.302734375\n",
            "classify 2.2998046875\n",
            "classify 2.349609375\n",
            "classify 2.26953125\n",
            "classify 2.4140625\n",
            "0.125\n",
            "0.0625\n",
            "0.25\n",
            "0.0625\n",
            "0.0\n",
            "0.125\n",
            "0.1875\n",
            "0.125\n",
            "0.125\n",
            "0.125\n",
            "strain 9.002893057186157e-05\n",
            "strain 0.00022385358170140535\n",
            "strain 7.392191764665768e-05\n",
            "strain 0.00014564032608177513\n",
            "strain 0.00024013097572606057\n",
            "strain 3.852873487630859e-05\n",
            "strain 0.00016393893747590482\n",
            "strain 0.00014923977141734213\n",
            "strain 6.80967204971239e-05\n",
            "strain 0.0002722754143178463\n",
            "strain 0.00010045446106232703\n",
            "strain 0.0001228450273629278\n",
            "strain 0.00014503145939670503\n",
            "strain 8.704889478394762e-05\n",
            "strain 0.00017386967374477535\n",
            "strain 8.220505696954206e-05\n",
            "strain 0.0001235312520293519\n",
            "strain 0.00011283580533927307\n",
            "strain 5.328825136530213e-05\n",
            "strain 8.71033626026474e-05\n",
            "strain 5.4486270528286695e-05\n",
            "strain 7.39995448384434e-05\n",
            "strain 4.911604628432542e-05\n",
            "strain 8.898640226107091e-05\n",
            "strain 5.457870429381728e-05\n",
            "strain 7.738207204965875e-05\n",
            "strain 0.00014981103595346212\n",
            "strain 5.628324288409203e-05\n",
            "strain 0.00013689951447304338\n",
            "strain 0.00011393128806957975\n",
            "strain 5.031592809245922e-05\n",
            "strain 0.00017103934078477323\n",
            "strain 7.010204717516899e-05\n",
            "strain 0.00010498781193746254\n",
            "strain 0.00015294372860807925\n",
            "strain 5.502444400917739e-05\n",
            "strain 0.00018158048624172807\n",
            "strain 8.076826634351164e-05\n",
            "strain 0.00011916055518668145\n",
            "strain 0.0001509432913735509\n",
            "strain 7.194038335001096e-05\n",
            "strain 0.00017665013729128987\n",
            "strain 9.004475577967241e-05\n",
            "strain 7.606479630339891e-05\n",
            "strain 0.0001760992017807439\n",
            "strain 6.61621306790039e-05\n",
            "strain 0.00014063791604712605\n",
            "strain 0.00020403457165230066\n",
            "strain 4.468745464691892e-05\n",
            "strain 0.0001349194790236652\n",
            "classify 2.3427734375\n",
            "classify 2.416015625\n",
            "classify 2.3857421875\n",
            "classify 2.4873046875\n",
            "classify 2.365234375\n",
            "classify 2.34765625\n",
            "classify 2.2802734375\n",
            "classify 2.296875\n",
            "classify 2.3525390625\n",
            "classify 2.3115234375\n",
            "0.125\n",
            "0.1875\n",
            "0.125\n",
            "0.0625\n",
            "0.0625\n",
            "0.0625\n",
            "0.0625\n",
            "0.0\n",
            "0.0625\n",
            "0.25\n"
          ]
        }
      ],
      "source": [
        "# @title strain ctrain test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "# def strain(model, dataloader, optim, scheduler=None): # train function with automatic mixed precision\n",
        "def strain(model, train_iter, optim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.train()\n",
        "    # for i, (img, _) in enumerate(dataloader):\n",
        "    for i in range(50):\n",
        "        try: img, _ = next(train_iter)\n",
        "        except NameError:\n",
        "            train_iter = iter(train_loader)\n",
        "            img, _ = next(train_iter)\n",
        "\n",
        "        img = img.to(device) # [batch, ]\n",
        "        img = img.flatten(2).transpose(-2,-1).to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            loss = model.loss(img)\n",
        "\n",
        "            optim.zero_grad()\n",
        "            scaler.scale(loss).backward()\n",
        "            # torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "            scaler.step(optim)\n",
        "            scaler.update()\n",
        "            model.target_encoder.update_parameters(model.context_encoder)\n",
        "            # update_bn(dataloader, model.target_encoder)\n",
        "            update_bn(train_iter, model.target_encoder)\n",
        "\n",
        "            # if scheduler is not None: scheduler.step()\n",
        "            print(\"strain\",loss.item())\n",
        "            try: wandb.log({\"loss\": loss.item()})\n",
        "            except NameError: pass\n",
        "        # if i >= 50: break\n",
        "    return train_iter\n",
        "\n",
        "\n",
        "# def ctrain(model, classifier, dataloader, coptim, scheduler=None): # train function with automatic mixed precision\n",
        "def ctrain(model, classifier, train_iter, coptim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.train()\n",
        "    # for i, (img, y) in enumerate(dataloader):\n",
        "    for i in range(10):\n",
        "        try: img, y = next(train_iter)\n",
        "        except NameError:\n",
        "            train_iter = iter(train_loader)\n",
        "            img, y = next(train_iter)\n",
        "        # print(\"ctrain\",y)\n",
        "        img, y = img.to(device), y.to(device) # [batch, ]\n",
        "        # img = img.flatten(start_dim=1)\n",
        "        img = img.flatten(2).transpose(-2,-1).to(torch.bfloat16)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            with torch.no_grad():\n",
        "                sx = model(img)\n",
        "            y_ = classifier(sx)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "            # print(classifier.classifier.weight[0])\n",
        "            # print(y_.shape, y.shape)\n",
        "            # print(loss)\n",
        "            coptim.zero_grad()\n",
        "            scaler.scale(loss).backward()\n",
        "            # torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "            scaler.step(coptim)\n",
        "            scaler.update()\n",
        "            print(\"classify\",loss.item())\n",
        "        # if i >= 10: break\n",
        "    return train_iter\n",
        "\n",
        "\n",
        "# def test(model, dataloader):\n",
        "def test(model, test_iter):\n",
        "    model.eval()\n",
        "    # for i, (img, y) in enumerate(dataloader):\n",
        "    for i in range(10):\n",
        "        try: img, y = next(test_iter)\n",
        "        except NameError:\n",
        "            test_iter = iter(test_loader)\n",
        "            img, y = next(test_iter)\n",
        "        # print(\"test\",y)\n",
        "\n",
        "        img, y = img.to(device), y.to(device) # [batch, ]\n",
        "        # img = img.flatten(start_dim=1).to(torch.bfloat16)\n",
        "        img = img.flatten(2).transpose(-2,-1).to(torch.bfloat16)\n",
        "\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            with torch.no_grad():\n",
        "                sx = model(img)\n",
        "                y_ = classifier(sx)\n",
        "            correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "            print(correct/len(y))\n",
        "        # if i >= 10: break\n",
        "    return test_iter\n",
        "\n",
        "train_iter = iter(train_loader)\n",
        "test_iter = iter(test_loader)\n",
        "for i in range(40):\n",
        "    # strain(seq_jepa, train_loader, optim)\n",
        "    # ctrain(seq_jepa, classifier, train_loader, coptim)\n",
        "    # test(seq_jepa, test_loader)\n",
        "\n",
        "    train_iter = strain(seq_jepa, train_iter, optim)\n",
        "    train_iter = ctrain(seq_jepa, classifier, train_iter, coptim)\n",
        "    test_iter = test(seq_jepa, test_iter)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# seq=40\n",
        "# src = seq_jepa.context_encoder.pos_encoder(torch.arange(seq, device=device))\n",
        "# for x in src:\n",
        "#     print(x)\n",
        "print(.999**50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgN5LlxWsPzB",
        "outputId": "76c7850b-a56b-43e3-e276-6eae40a52469"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
            "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.],\n",
            "       device='cuda:0')\n",
            "tensor([8.4147e-01, 5.4030e-01, 5.3317e-01, 8.4601e-01, 3.1098e-01, 9.5042e-01,\n",
            "        1.7689e-01, 9.8423e-01, 9.9833e-02, 9.9500e-01, 5.6204e-02, 9.9842e-01,\n",
            "        3.1618e-02, 9.9950e-01, 1.7782e-02, 9.9984e-01, 9.9998e-03, 9.9995e-01,\n",
            "        5.6234e-03, 9.9998e-01, 3.1623e-03, 9.9999e-01, 1.7783e-03, 1.0000e+00,\n",
            "        1.0000e-03, 1.0000e+00, 5.6234e-04, 1.0000e+00, 3.1623e-04, 1.0000e+00,\n",
            "        1.7783e-04, 1.0000e+00], device='cuda:0')\n",
            "tensor([ 9.0930e-01, -4.1615e-01,  9.0213e-01,  4.3146e-01,  5.9113e-01,\n",
            "         8.0658e-01,  3.4821e-01,  9.3742e-01,  1.9867e-01,  9.8007e-01,\n",
            "         1.1223e-01,  9.9368e-01,  6.3203e-02,  9.9800e-01,  3.5558e-02,\n",
            "         9.9937e-01,  1.9999e-02,  9.9980e-01,  1.1247e-02,  9.9994e-01,\n",
            "         6.3245e-03,  9.9998e-01,  3.5566e-03,  9.9999e-01,  2.0000e-03,\n",
            "         1.0000e+00,  1.1247e-03,  1.0000e+00,  6.3246e-04,  1.0000e+00,\n",
            "         3.5566e-04,  1.0000e+00], device='cuda:0')\n",
            "tensor([ 1.4112e-01, -9.8999e-01,  9.9325e-01, -1.1597e-01,  8.1265e-01,\n",
            "         5.8275e-01,  5.0854e-01,  8.6104e-01,  2.9552e-01,  9.5534e-01,\n",
            "         1.6790e-01,  9.8580e-01,  9.4726e-02,  9.9550e-01,  5.3323e-02,\n",
            "         9.9858e-01,  2.9995e-02,  9.9955e-01,  1.6869e-02,  9.9986e-01,\n",
            "         9.4867e-03,  9.9995e-01,  5.3348e-03,  9.9999e-01,  3.0000e-03,\n",
            "         1.0000e+00,  1.6870e-03,  1.0000e+00,  9.4868e-04,  1.0000e+00,\n",
            "         5.3348e-04,  1.0000e+00], device='cuda:0')\n",
            "tensor([-7.5680e-01, -6.5364e-01,  7.7847e-01, -6.2768e-01,  9.5358e-01,\n",
            "         3.0114e-01,  6.5283e-01,  7.5751e-01,  3.8942e-01,  9.2106e-01,\n",
            "         2.2304e-01,  9.7481e-01,  1.2615e-01,  9.9201e-01,  7.1071e-02,\n",
            "         9.9747e-01,  3.9989e-02,  9.9920e-01,  2.2492e-02,  9.9975e-01,\n",
            "         1.2649e-02,  9.9992e-01,  7.1131e-03,  9.9997e-01,  4.0000e-03,\n",
            "         9.9999e-01,  2.2494e-03,  1.0000e+00,  1.2649e-03,  1.0000e+00,\n",
            "         7.1131e-04,  1.0000e+00], device='cuda:0')\n",
            "tensor([-9.5892e-01,  2.8366e-01,  3.2394e-01, -9.4608e-01,  9.9995e-01,\n",
            "        -1.0342e-02,  7.7653e-01,  6.3008e-01,  4.7943e-01,  8.7758e-01,\n",
            "         2.7748e-01,  9.6073e-01,  1.5746e-01,  9.8753e-01,  8.8797e-02,\n",
            "         9.9605e-01,  4.9979e-02,  9.9875e-01,  2.8113e-02,  9.9960e-01,\n",
            "         1.5811e-02,  9.9988e-01,  8.8913e-03,  9.9996e-01,  5.0000e-03,\n",
            "         9.9999e-01,  2.8117e-03,  1.0000e+00,  1.5811e-03,  1.0000e+00,\n",
            "         8.8914e-04,  1.0000e+00], device='cuda:0')\n",
            "tensor([-0.2794,  0.9602, -0.2304, -0.9731,  0.9471, -0.3208,  0.8757,  0.4828,\n",
            "         0.5646,  0.8253,  0.3310,  0.9436,  0.1886,  0.9821,  0.1065,  0.9943,\n",
            "         0.0600,  0.9982,  0.0337,  0.9994,  0.0190,  0.9998,  0.0107,  0.9999,\n",
            "         0.0060,  1.0000,  0.0034,  1.0000,  0.0019,  1.0000,  0.0011,  1.0000],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.6570,  0.7539, -0.7137, -0.7004,  0.8004, -0.5994,  0.9473,  0.3203,\n",
            "         0.6442,  0.7648,  0.3836,  0.9235,  0.2196,  0.9756,  0.1242,  0.9923,\n",
            "         0.0699,  0.9976,  0.0394,  0.9992,  0.0221,  0.9998,  0.0124,  0.9999,\n",
            "         0.0070,  1.0000,  0.0039,  1.0000,  0.0022,  1.0000,  0.0012,  1.0000],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9894, -0.1455, -0.9773, -0.2120,  0.5743, -0.8186,  0.9890,  0.1476,\n",
            "         0.7174,  0.6967,  0.4349,  0.9005,  0.2503,  0.9682,  0.1418,  0.9899,\n",
            "         0.0799,  0.9968,  0.0450,  0.9990,  0.0253,  0.9997,  0.0142,  0.9999,\n",
            "         0.0080,  1.0000,  0.0045,  1.0000,  0.0025,  1.0000,  0.0014,  1.0000],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.4121, -0.9111, -0.9398,  0.3417,  0.2913, -0.9566,  0.9996, -0.0297,\n",
            "         0.7833,  0.6216,  0.4848,  0.8746,  0.2808,  0.9598,  0.1594,  0.9872,\n",
            "         0.0899,  0.9960,  0.0506,  0.9987,  0.0285,  0.9996,  0.0160,  0.9999,\n",
            "         0.0090,  1.0000,  0.0051,  1.0000,  0.0028,  1.0000,  0.0016,  1.0000],\n",
            "       device='cuda:0')\n",
            "tensor([-0.5440, -0.8391, -0.6129,  0.7901, -0.0207, -0.9998,  0.9786, -0.2060,\n",
            "         0.8415,  0.5403,  0.5332,  0.8460,  0.3110,  0.9504,  0.1769,  0.9842,\n",
            "         0.0998,  0.9950,  0.0562,  0.9984,  0.0316,  0.9995,  0.0178,  0.9998,\n",
            "         0.0100,  0.9999,  0.0056,  1.0000,  0.0032,  1.0000,  0.0018,  1.0000],\n",
            "       device='cuda:0')\n",
            "tensor([-1.0000,  0.0044, -0.0973,  0.9953, -0.3306, -0.9438,  0.9267, -0.3758,\n",
            "         0.8912,  0.4536,  0.5799,  0.8147,  0.3409,  0.9401,  0.1944,  0.9809,\n",
            "         0.1098,  0.9940,  0.0618,  0.9981,  0.0348,  0.9994,  0.0196,  0.9998,\n",
            "         0.0110,  0.9999,  0.0062,  1.0000,  0.0035,  1.0000,  0.0020,  1.0000],\n",
            "       device='cuda:0')\n",
            "tensor([-0.5366,  0.8439,  0.4483,  0.8939, -0.6077, -0.7942,  0.8456, -0.5338,\n",
            "         0.9320,  0.3624,  0.6247,  0.7808,  0.3704,  0.9289,  0.2118,  0.9773,\n",
            "         0.1197,  0.9928,  0.0674,  0.9977,  0.0379,  0.9993,  0.0213,  0.9998,\n",
            "         0.0120,  0.9999,  0.0067,  1.0000,  0.0038,  1.0000,  0.0021,  1.0000],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.4202,  0.9074,  0.8559,  0.5172, -0.8245, -0.5658,  0.7378, -0.6750,\n",
            "         0.9636,  0.2675,  0.6676,  0.7445,  0.3996,  0.9167,  0.2291,  0.9734,\n",
            "         0.1296,  0.9916,  0.0730,  0.9973,  0.0411,  0.9992,  0.0231,  0.9997,\n",
            "         0.0130,  0.9999,  0.0073,  1.0000,  0.0041,  1.0000,  0.0023,  1.0000],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9906,  0.1367,  0.9998, -0.0188, -0.9596, -0.2813,  0.6068, -0.7949,\n",
            "         0.9854,  0.1700,  0.7084,  0.7058,  0.4284,  0.9036,  0.2464,  0.9692,\n",
            "         0.1395,  0.9902,  0.0786,  0.9969,  0.0443,  0.9990,  0.0249,  0.9997,\n",
            "         0.0140,  0.9999,  0.0079,  1.0000,  0.0044,  1.0000,  0.0025,  1.0000],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.6503, -0.7597,  0.8358, -0.5490, -0.9995,  0.0310,  0.4566, -0.8897,\n",
            "         0.9975,  0.0707,  0.7470,  0.6648,  0.4568,  0.8896,  0.2636,  0.9646,\n",
            "         0.1494,  0.9888,  0.0843,  0.9964,  0.0474,  0.9989,  0.0267,  0.9996,\n",
            "         0.0150,  0.9999,  0.0084,  1.0000,  0.0047,  1.0000,  0.0027,  1.0000],\n",
            "       device='cuda:0')\n",
            "tensor([-0.2879, -0.9577,  0.4144, -0.9101, -0.9403,  0.3403,  0.2920, -0.9564,\n",
            "         0.9996, -0.0292,  0.7832,  0.6218,  0.4847,  0.8747,  0.2807,  0.9598,\n",
            "         0.1593,  0.9872,  0.0899,  0.9960,  0.0506,  0.9987,  0.0284,  0.9996,\n",
            "         0.0160,  0.9999,  0.0090,  1.0000,  0.0051,  1.0000,  0.0028,  1.0000],\n",
            "       device='cuda:0')\n",
            "tensor([-0.9614, -0.2752, -0.1346, -0.9909, -0.7879,  0.6159,  0.1182, -0.9930,\n",
            "         0.9917, -0.1288,  0.8169,  0.5768,  0.5121,  0.8589,  0.2977,  0.9547,\n",
            "         0.1692,  0.9856,  0.0955,  0.9954,  0.0537,  0.9986,  0.0302,  0.9995,\n",
            "         0.0170,  0.9999,  0.0096,  1.0000,  0.0054,  1.0000,  0.0030,  1.0000],\n",
            "       device='cuda:0')\n",
            "tensor([-0.7510,  0.6603, -0.6422, -0.7665, -0.5573,  0.8303, -0.0593, -0.9982,\n",
            "         0.9738, -0.2272,  0.8480,  0.5300,  0.5390,  0.8423,  0.3147,  0.9492,\n",
            "         0.1790,  0.9838,  0.1010,  0.9949,  0.0569,  0.9984,  0.0320,  0.9995,\n",
            "         0.0180,  0.9998,  0.0101,  0.9999,  0.0057,  1.0000,  0.0032,  1.0000],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.1499,  0.9887, -0.9520, -0.3061, -0.2714,  0.9625, -0.2349, -0.9720,\n",
            "         0.9463, -0.3233,  0.8765,  0.4815,  0.5653,  0.8249,  0.3315,  0.9435,\n",
            "         0.1889,  0.9820,  0.1066,  0.9943,  0.0600,  0.9982,  0.0338,  0.9994,\n",
            "         0.0190,  0.9998,  0.0107,  0.9999,  0.0060,  1.0000,  0.0034,  1.0000],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9129,  0.4081, -0.9686,  0.2486,  0.0414,  0.9991, -0.4032, -0.9151,\n",
            "         0.9093, -0.4161,  0.9021,  0.4315,  0.5911,  0.8066,  0.3482,  0.9374,\n",
            "         0.1987,  0.9801,  0.1122,  0.9937,  0.0632,  0.9980,  0.0356,  0.9994,\n",
            "         0.0200,  0.9998,  0.0112,  0.9999,  0.0063,  1.0000,  0.0036,  1.0000],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.8367, -0.5477, -0.6869,  0.7268,  0.3500,  0.9367, -0.5587, -0.8294,\n",
            "         0.8632, -0.5048,  0.9250,  0.3801,  0.6163,  0.7875,  0.3648,  0.9311,\n",
            "         0.2085,  0.9780,  0.1178,  0.9930,  0.0664,  0.9978,  0.0373,  0.9993,\n",
            "         0.0210,  0.9998,  0.0118,  0.9999,  0.0066,  1.0000,  0.0037,  1.0000],\n",
            "       device='cuda:0')\n",
            "tensor([-0.0089, -1.0000, -0.1936,  0.9811,  0.6240,  0.7814, -0.6966, -0.7175,\n",
            "         0.8085, -0.5885,  0.9449,  0.3275,  0.6409,  0.7676,  0.3813,  0.9244,\n",
            "         0.2182,  0.9759,  0.1234,  0.9924,  0.0695,  0.9976,  0.0391,  0.9992,\n",
            "         0.0220,  0.9998,  0.0124,  0.9999,  0.0070,  1.0000,  0.0039,  1.0000],\n",
            "       device='cuda:0')\n",
            "tensor([-0.8462, -0.5328,  0.3593,  0.9332,  0.8361,  0.5486, -0.8125, -0.5829,\n",
            "         0.7457, -0.6663,  0.9618,  0.2739,  0.6649,  0.7470,  0.3977,  0.9175,\n",
            "         0.2280,  0.9737,  0.1290,  0.9916,  0.0727,  0.9974,  0.0409,  0.9992,\n",
            "         0.0230,  0.9997,  0.0129,  0.9999,  0.0073,  1.0000,  0.0041,  1.0000],\n",
            "       device='cuda:0')\n",
            "tensor([-0.9056,  0.4242,  0.8015,  0.5980,  0.9652,  0.2614, -0.9028, -0.4300,\n",
            "         0.6755, -0.7374,  0.9756,  0.2194,  0.6882,  0.7256,  0.4139,  0.9103,\n",
            "         0.2377,  0.9713,  0.1346,  0.9909,  0.0758,  0.9971,  0.0427,  0.9991,\n",
            "         0.0240,  0.9997,  0.0135,  0.9999,  0.0076,  1.0000,  0.0043,  1.0000],\n",
            "       device='cuda:0')\n",
            "tensor([-0.1324,  0.9912,  0.9969,  0.0786,  0.9987, -0.0517, -0.9646, -0.2635,\n",
            "         0.5985, -0.8011,  0.9864,  0.1642,  0.7108,  0.7034,  0.4301,  0.9028,\n",
            "         0.2474,  0.9689,  0.1401,  0.9901,  0.0790,  0.9969,  0.0444,  0.9990,\n",
            "         0.0250,  0.9997,  0.0141,  0.9999,  0.0079,  1.0000,  0.0044,  1.0000],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.7626,  0.6469,  0.8853, -0.4651,  0.9331, -0.3597, -0.9961, -0.0887,\n",
            "         0.5155, -0.8569,  0.9941,  0.1085,  0.7326,  0.6806,  0.4461,  0.8950,\n",
            "         0.2571,  0.9664,  0.1457,  0.9893,  0.0821,  0.9966,  0.0462,  0.9989,\n",
            "         0.0260,  0.9997,  0.0146,  0.9999,  0.0082,  1.0000,  0.0046,  1.0000],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9564, -0.2921,  0.5010, -0.8655,  0.7749, -0.6320, -0.9960,  0.0888,\n",
            "         0.4274, -0.9041,  0.9986,  0.0525,  0.7538,  0.6571,  0.4619,  0.8869,\n",
            "         0.2667,  0.9638,  0.1512,  0.9885,  0.0853,  0.9964,  0.0480,  0.9988,\n",
            "         0.0270,  0.9996,  0.0152,  0.9999,  0.0085,  1.0000,  0.0048,  1.0000],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.2709, -0.9626, -0.0376, -0.9993,  0.5400, -0.8417, -0.9646,  0.2636,\n",
            "         0.3350, -0.9422,  1.0000, -0.0038,  0.7742,  0.6330,  0.4776,  0.8786,\n",
            "         0.2764,  0.9611,  0.1568,  0.9876,  0.0884,  0.9961,  0.0498,  0.9988,\n",
            "         0.0280,  0.9996,  0.0157,  0.9999,  0.0089,  1.0000,  0.0050,  1.0000],\n",
            "       device='cuda:0')\n",
            "tensor([-0.6636, -0.7481, -0.5646, -0.8254,  0.2514, -0.9679, -0.9028,  0.4301,\n",
            "         0.2392, -0.9710,  0.9982, -0.0600,  0.7938,  0.6082,  0.4931,  0.8699,\n",
            "         0.2860,  0.9582,  0.1624,  0.9867,  0.0916,  0.9958,  0.0515,  0.9987,\n",
            "         0.0290,  0.9996,  0.0163,  0.9999,  0.0092,  1.0000,  0.0052,  1.0000],\n",
            "       device='cuda:0')\n",
            "tensor([-0.9880,  0.1543, -0.9177, -0.3973, -0.0620, -0.9981, -0.8125,  0.5830,\n",
            "         0.1411, -0.9900,  0.9933, -0.1160,  0.8126,  0.5828,  0.5085,  0.8610,\n",
            "         0.2955,  0.9553,  0.1679,  0.9858,  0.0947,  0.9955,  0.0533,  0.9986,\n",
            "         0.0300,  0.9996,  0.0169,  0.9999,  0.0095,  1.0000,  0.0053,  1.0000],\n",
            "       device='cuda:0')\n",
            "tensor([-0.4040,  0.9147, -0.9882,  0.1532, -0.3693, -0.9293, -0.6965,  0.7175,\n",
            "         0.0416, -0.9991,  0.9852, -0.1716,  0.8307,  0.5568,  0.5238,  0.8519,\n",
            "         0.3051,  0.9523,  0.1734,  0.9848,  0.0979,  0.9952,  0.0551,  0.9985,\n",
            "         0.0310,  0.9995,  0.0174,  0.9998,  0.0098,  1.0000,  0.0055,  1.0000],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.5514,  0.8342, -0.7543,  0.6565, -0.6400, -0.7684, -0.5586,  0.8294,\n",
            "        -0.0584, -0.9983,  0.9740, -0.2267,  0.8479,  0.5302,  0.5388,  0.8424,\n",
            "         0.3146,  0.9492,  0.1790,  0.9839,  0.1010,  0.9949,  0.0569,  0.9984,\n",
            "         0.0320,  0.9995,  0.0180,  0.9998,  0.0101,  0.9999,  0.0057,  1.0000],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9999, -0.0133, -0.2881,  0.9576, -0.8472, -0.5312, -0.4031,  0.9152,\n",
            "        -0.1577, -0.9875,  0.9597, -0.2811,  0.8642,  0.5032,  0.5537,  0.8327,\n",
            "         0.3240,  0.9460,  0.1845,  0.9828,  0.1042,  0.9946,  0.0586,  0.9983,\n",
            "         0.0330,  0.9995,  0.0186,  0.9998,  0.0104,  0.9999,  0.0059,  1.0000],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.5291, -0.8486,  0.2668,  0.9638, -0.9704, -0.2414, -0.2348,  0.9720,\n",
            "        -0.2555, -0.9668,  0.9424, -0.3346,  0.8797,  0.4756,  0.5684,  0.8227,\n",
            "         0.3335,  0.9428,  0.1900,  0.9818,  0.1073,  0.9942,  0.0604,  0.9982,\n",
            "         0.0340,  0.9994,  0.0191,  0.9998,  0.0108,  0.9999,  0.0060,  1.0000],\n",
            "       device='cuda:0')\n",
            "tensor([-0.4282, -0.9037,  0.7395,  0.6731, -0.9974,  0.0723, -0.0592,  0.9982,\n",
            "        -0.3508, -0.9365,  0.9221, -0.3870,  0.8943,  0.4475,  0.5830,  0.8125,\n",
            "         0.3429,  0.9394,  0.1956,  0.9807,  0.1105,  0.9939,  0.0622,  0.9981,\n",
            "         0.0350,  0.9994,  0.0197,  0.9998,  0.0111,  0.9999,  0.0062,  1.0000],\n",
            "       device='cuda:0')\n",
            "tensor([-0.9918, -0.1280,  0.9845,  0.1752, -0.9254,  0.3789,  0.1183,  0.9930,\n",
            "        -0.4425, -0.8968,  0.8989, -0.4382,  0.9080,  0.4190,  0.5973,  0.8020,\n",
            "         0.3523,  0.9359,  0.2011,  0.9796,  0.1136,  0.9935,  0.0640,  0.9980,\n",
            "         0.0360,  0.9994,  0.0202,  0.9998,  0.0114,  0.9999,  0.0064,  1.0000],\n",
            "       device='cuda:0')\n",
            "tensor([-0.6435,  0.7654,  0.9263, -0.3767, -0.7617,  0.6479,  0.2921,  0.9564,\n",
            "        -0.5298, -0.8481,  0.8728, -0.4881,  0.9208,  0.3901,  0.6115,  0.7912,\n",
            "         0.3616,  0.9323,  0.2066,  0.9784,  0.1167,  0.9932,  0.0657,  0.9978,\n",
            "         0.0370,  0.9993,  0.0208,  0.9998,  0.0117,  0.9999,  0.0066,  1.0000],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.2964,  0.9551,  0.5828, -0.8126, -0.5224,  0.8527,  0.4567,  0.8896,\n",
            "        -0.6119, -0.7910,  0.8440, -0.5363,  0.9326,  0.3608,  0.6255,  0.7802,\n",
            "         0.3709,  0.9287,  0.2121,  0.9773,  0.1199,  0.9928,  0.0675,  0.9977,\n",
            "         0.0380,  0.9993,  0.0214,  0.9998,  0.0120,  0.9999,  0.0068,  1.0000],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9638,  0.2666,  0.0598, -0.9982, -0.2314,  0.9729,  0.6069,  0.7948,\n",
            "        -0.6878, -0.7259,  0.8125, -0.5829,  0.9436,  0.3311,  0.6393,  0.7690,\n",
            "         0.3802,  0.9249,  0.2176,  0.9760,  0.1230,  0.9924,  0.0693,  0.9976,\n",
            "         0.0390,  0.9992,  0.0219,  0.9998,  0.0123,  0.9999,  0.0069,  1.0000],\n",
            "       device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title save/load\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# folder='/content/drive/MyDrive/jepa/'\n",
        "\n",
        "# modelsd, optimsd = torch.load(folder+'SeqJEPA.pkl', map_location=device).values()\n",
        "modelsd, optimsd = torch.load('SeqJEPA.pkl', map_location=device).values()\n",
        "seq_jepa.load_state_dict(modelsd, strict=False)\n",
        "optim.load_state_dict(optimsd)"
      ],
      "metadata": {
        "id": "JT8AgOx0E_KM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0315f0bc-13a8-4eb4-9935-caf33710185c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-45af7927d988>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  modelsd, optimsd = torch.load('SeqJEPA.pkl', map_location=device).values()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = {'model': seq_jepa.state_dict(), 'optimizer': optim.state_dict()}\n",
        "# # torch.save(checkpoint, folder+'SeqJEPA.pkl')\n",
        "torch.save(checkpoint, 'SeqJEPA.pkl')"
      ],
      "metadata": {
        "id": "CpbLPvjiE-pD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "2Nd-sGe6Ku4S",
        "outputId": "a1c8b79f-baa9-4ade-da4f-4dae4ba405d6",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▂▂▁▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>5e-05</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">restful-snowflake-1</strong> at: <a href='https://wandb.ai/bobdole/SeqJEPA/runs/aj5x66ez' target=\"_blank\">https://wandb.ai/bobdole/SeqJEPA/runs/aj5x66ez</a><br> View project at: <a href='https://wandb.ai/bobdole/SeqJEPA' target=\"_blank\">https://wandb.ai/bobdole/SeqJEPA</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250304_092225-aj5x66ez/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250304_095155-qtz18mb0</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/SeqJEPA/runs/qtz18mb0' target=\"_blank\">copper-water-2</a></strong> to <a href='https://wandb.ai/bobdole/SeqJEPA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/SeqJEPA' target=\"_blank\">https://wandb.ai/bobdole/SeqJEPA</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/SeqJEPA/runs/qtz18mb0' target=\"_blank\">https://wandb.ai/bobdole/SeqJEPA/runs/qtz18mb0</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "!pip install -q wandb\n",
        "import wandb # https://docs.wandb.ai/quickstart\n",
        "wandb.login(key='487a2109e55dce4e13fc70681781de9f50f27be7')\n",
        "try: run.finish()\n",
        "except NameError: pass\n",
        "run = wandb.init(project=\"SeqJEPA\", config={\"model\": \"res18\",})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(4,100,3)\n",
        "\n",
        "x=x.transpose(-2,-1) # [batch, dim, seq]\n",
        " # in, out, kernel, stride, pad\n",
        "# net = nn.Conv1d(3,16,7,2,7//2)\n",
        "net = nn.Sequential(nn.Conv1d(3,16,7,2,7//2), nn.MaxPool1d(3, 2, 3//2))\n",
        "# net = nn.Conv1d(3,16,(7,3),2,3//2)\n",
        "out = net(x)\n",
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_YBe6-eZ2Zq",
        "outputId": "09b27f75-ba04-4aaf-f5e4-1b9c1dc8d112"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 16, 25])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test repeat_interleave\n",
        "# x = torch.rand(3,5)\n",
        "x = torch.rand(3,5,7)\n",
        "print(x)\n",
        "# out = x.repeat(2,1) # [2*3,5]\n",
        "# print(out)\n",
        "# x_ = out.reshape(2,3,5)\n",
        "# print(x_)\n",
        "# out = x.repeat_interleave(2,1,1) # [3*2,5]\n",
        "out = x.repeat_interleave(2,dim=0) # [3*2,5]\n",
        "# print(out)\n",
        "# x_ = out.reshape(2,3,5,7)\n",
        "x_ = out.reshape(3,2,5,7)\n",
        "print(x_)\n",
        "\n",
        "\n",
        "# batch=1\n",
        "# seq_len=5\n",
        "# dim=4\n",
        "# positional_emb = torch.rand(batch, seq_len, dim)\n",
        "# print(positional_emb)\n",
        "# num_tok=2\n",
        "# trg_indices=torch.randint(0,seq_len,(M, num_tok))\n",
        "# print(trg_indices)\n",
        "# # pos_embs = positional_emb.gather(1, trg_indices.unsqueeze(0))\n",
        "# pos_embs = positional_emb[torch.arange(batch)[...,None,None],trg_indices.unsqueeze(0)]\n",
        "# # pos_embs = positional_emb[0,trg_indices]\n",
        "# # [batch, M, num_tok, dim]\n",
        "# print(pos_embs.shape)\n",
        "# print(pos_embs)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "R6kiiqw3uIdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test ropes\n",
        "# for i, (img, _) in enumerate(test_loader):\n",
        "#     break\n",
        "# print(img.shape) # [batch, 1, 28, 28]\n",
        "# print(img[0]) # [0,1)\n",
        "d_model=8\n",
        "# rot_emb = RotEmb(d_model, top=torch.pi, base=10)\n",
        "# x = torch.linspace(0,1,20)\n",
        "# out = rot_emb(x)\n",
        "# print(out.shape)\n",
        "# print(out)\n",
        "\n",
        "# # pos_encoder = RoPE(d_model, seq_len=15, base=100)\n",
        "# pos_encoder = RoPE(d_model, seq_len=15, base=10000)\n",
        "x = torch.ones(1, 10, d_model)\n",
        "# # x = torch.ones(1, 10, d_model)\n",
        "# out = pos_encoder(x)\n",
        "# # print(out.shape)\n",
        "# for x in out[0]:\n",
        "#     print(x)\n",
        "\n",
        "\n",
        "# pos_encoder = LearnedRoPE(d_model, seq_len=512)\n",
        "# out = pos_encoder(x)\n",
        "# for o in out[0]:\n",
        "#     print(o)\n",
        "\n",
        "# pos_encoder.weights = nn.Parameter(torch.randn(1, d_model//2))\n",
        "# out = pos_encoder(x)\n",
        "# for o in out[0]:\n",
        "#     print(o)\n",
        "\n",
        "\n",
        "# tensor([-0.0177, -0.9998,  0.8080, -0.5892, -0.7502, -0.6612,  0.3885,  0.9214])\n"
      ],
      "metadata": {
        "id": "8r_GjI_vCMyo",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title SeqJEPA\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def multiblock(batch, seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "        mask_len = torch.rand(batch, M) * (max_s - min_s) + min_s # in (min_s, max_s)\n",
        "        mask_pos = torch.rand(batch, M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "        mask_len, mask_pos = mask_len * seq, mask_pos * seq\n",
        "        indices = torch.arange(seq)[None,None,...]\n",
        "        target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [batch, M, seq]\n",
        "        target_mask = target_mask.any(1) # True -> masked # multiblock masking # [batch, seq]\n",
        "        return target_mask\n",
        "\n",
        "\n",
        "\n",
        "class SeqJEPA(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.in_dim = in_dim\n",
        "        # if out_dim is None: self.out_dim = in_dim\n",
        "        if out_dim is None: out_dim = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.d_model = d_model\n",
        "        act = nn.GELU() # ReLU GELU SiLU\n",
        "\n",
        "        dim=8\n",
        "\n",
        "\n",
        "        self.calorie_emb = RotEmb(dim, top=1, base=10) # 0-4\n",
        "        self.steps_emb = RotEmb(dim, top=1, base=10) # 0-5\n",
        "        self.enc0 = nn.Sequential(\n",
        "            nn.Linear(dim*2, d_model), act,\n",
        "            nn.Linear(d_model, d_model), act,\n",
        "        )\n",
        "        self.hour_emb = CircularEmb(dim, torch.pi/2) # circular (0,1) # 0-24\n",
        "        self.temp_emb = RotEmb(dim, top=1/3, base=10) # 18-35\n",
        "        self.heart_emb = RotEmb(dim, top=1/3, base=100) # 50-200\n",
        "        self.enc1 = nn.Sequential(\n",
        "            nn.Linear(dim*3, d_model), act,\n",
        "        )\n",
        "\n",
        "        # self.transformer = TransformerClassifier(d_model, out_dim=out_dim, nhead=8, nlayers=2)\n",
        "        # # self.fc = nn.Linear(d_model, out_dim)\n",
        "        self.context_encoder = TransformerModel(d_model, out_dim=out_dim, nhead=8, nlayers=2, dropout=0.)\n",
        "        self.predicter = TransformerModel(out_dim, nhead=8, nlayers=1, dropout=0.)\n",
        "        self.target_encoder = AveragedModel(self.context_encoder, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    def loss(self, hr, temp, heart): # [batch, 1+T]\n",
        "        # batch = hr.size(0)\n",
        "        res_id, activeKilocalories, steps = hr[:,0], temp[:,0], heart[:,0]\n",
        "        # enc_summary = self.enc0(torch.cat([self.calorie_emb(activeKilocalories), self.steps_emb(steps)], dim=-1)) # [batch, d_model]\n",
        "        enc_summary = self.enc0(torch.cat([self.calorie_emb(torch.log(activeKilocalories.clamp(min=1))), self.steps_emb(torch.log(steps.clamp(min=1)))], dim=-1)) # [batch, d_model]\n",
        "        x = self.enc1(torch.cat([self.hour_emb(hr[:,1:]/24), self.temp_emb(temp[:,1:]), self.heart_emb(heart[:,1:])], dim=-1)) # [batch, T, d_model]\n",
        "        # print('violet fwd', hr.shape, enc_summary.shape, x.shape)\n",
        "\n",
        "        # # mask=(torch.rand(self.seq_len)<.1) # True -> masked # random masking\n",
        "        # mask=(torch.rand_like(x)<.1) # True -> masked # random masking\n",
        "\n",
        "        # patches pad -enc> patch_enc\n",
        "        # rearrange patch in padded/masked, -pred> pred of masked\n",
        "        # mse tgt_enc(img)\n",
        "\n",
        "        # average L2 distance between the predicted patch-level representations sˆy(i) and the target patch-level representation sy(i);\n",
        "        # F.smooth_l1_loss\n",
        "\n",
        "        batch, seq, dim = x.shape\n",
        "\n",
        "        # mask=(torch.rand(batch, seq)<.75) # True -> masked # random masking\n",
        "        # sorted_mask, ids = mask.sort(dim=1)\n",
        "        # # sorted_x = x[torch.arange(batch)[...,None,None],ids]\n",
        "        # sorted_x = x[torch.arange(batch).unsqueeze(-1),ids]\n",
        "        # sorted_mask = torch.cat([torch.zeros((batch, 1), dtype=torch.bool), sorted_mask], dim=1) # True->mask\n",
        "        # sorted_x = torch.cat([enc_summary.unsqueeze(1), sorted_x], dim=1)\n",
        "        # sorted_sx = self.context_encoder(sorted_x, src_key_padding_mask=sorted_mask)\n",
        "        # # torch.zeros_like(sorted_sx)\n",
        "        # sx = sorted_sx[torch.arange(batch).unsqueeze(-1),ids.argsort(1)]\n",
        "        # sy_ = self.predicter(sx, src_key_padding_mask=mask)\n",
        "        # sy = self.target_encoder(x)*~mask\n",
        "\n",
        "\n",
        "        # target_mask = multiblock(batch, seq, min_s=0.15, max_s=0.2, M=4)\n",
        "        # context_mask = ~multiblock(batch, seq, min_s=0.85, max_s=1., M=1)|target_mask\n",
        "        # sx = self.context_encoder(x, src_key_padding_mask=context_mask)\n",
        "        # sy_ = self.predicter(sx, cls_mask=target_mask)*target_mask.unsqueeze(-1)\n",
        "        # sy_ = self.predicter(sx, cls_mask=target_mask)\n",
        "        # # print(sy_.shape, target_mask.shape)\n",
        "        # sy = self.target_encoder(x)*target_mask.unsqueeze(-1)\n",
        "\n",
        "\n",
        "        M=4\n",
        "        target_mask = multiblock(M*batch, seq, min_s=0.15, max_s=0.2, M=1) # mask out targets to be predicted # [4*batch, seq]\n",
        "        context_mask = ~multiblock(batch, seq, min_s=0.85, max_s=1., M=1)|target_mask.reshape(batch,M,seq).any(1) # [batch, seq]\n",
        "\n",
        "\n",
        "        x = torch.cat([enc_summary.unsqueeze(1), x], dim=1) # [batch, 1+T, d_model]\n",
        "        target_mask = torch.cat([torch.zeros((M*batch, 1), dtype=bool), target_mask],dim=1) # [4*batch, 1+T]\n",
        "        context_mask = torch.cat([torch.zeros((batch, 1), dtype=bool), context_mask],dim=1) # [batch, 1+T]\n",
        "\n",
        "        # x = x.repeat(4,1,1)\n",
        "        # context_mask = context_mask.repeat(4,1)\n",
        "        sx = self.context_encoder(x, src_key_padding_mask=context_mask).repeat(M,1,1)\n",
        "        sy_ = self.predicter(sx, cls_mask=target_mask)*target_mask.unsqueeze(-1)\n",
        "        # sy_ = self.predicter(sx, cls_mask=target_mask)\n",
        "        # print(sy_.shape, target_mask.shape)\n",
        "        sy = self.target_encoder(x).repeat(M,1,1)*target_mask.unsqueeze(-1)\n",
        "\n",
        "        loss = F.smooth_l1_loss(sy, sy_)\n",
        "        return loss\n",
        "\n",
        "    # def forward(self, hr, temp, heart): # [batch, 1+T]\n",
        "    #     # batch = hr.size(0)\n",
        "    #     # batch, seq, dim = x.shape\n",
        "    #     res_id, activeKilocalories, steps = hr[:,0], temp[:,0], heart[:,0]\n",
        "    #     enc_summary = self.enc0(torch.cat([self.calorie_emb(activeKilocalories), self.steps_emb(steps)], dim=-1)) # [batch, d_model]\n",
        "    #     x = self.enc1(torch.cat([self.hour_emb(hr[:,1:]/24), self.temp_emb(temp[:,1:]), self.heart_emb(heart[:,1:])], dim=-1)) # [batch, T, d_model]\n",
        "\n",
        "    #     x = torch.cat([enc_summary.unsqueeze(1), x], dim=1)\n",
        "    #     # sx = self.context_encoder(x)\n",
        "    #     sx = self.target_encoder(x)\n",
        "    #     out = sx.mean(dim=1)\n",
        "    #     return out\n",
        "\n",
        "\n",
        "\n",
        "seq_jepa = SeqJEPA(in_dim=16, d_model=32, out_dim=16, num_layers=1).to(device)#.to(torch.float)\n",
        "soptim = torch.optim.AdamW(seq_jepa.parameters(), lr=1e-3)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "O-OU1MaKF7BZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ijepa vision_transformer.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/models/vision_transformer.py\n",
        "import math\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from src.utils.tensors import (trunc_normal_, repeat_interleave_batch)\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid height and width\n",
        "    return:\n",
        "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid_h = np.arange(grid_size, dtype=float)\n",
        "    grid_w = np.arange(grid_size, dtype=float)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    assert embed_dim % 2 == 0\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid length\n",
        "    return:\n",
        "    pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid = np.arange(grid_size, dtype=float)\n",
        "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=float)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega   # (D/2,)\n",
        "    pos = pos.reshape(-1)   # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
        "    emb = np.concatenate([np.sin(out), np.cos(out)], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x, attn\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x, return_attention=False):\n",
        "        y, attn = self.attn(self.norm1(x))\n",
        "        if return_attention:\n",
        "            return attn\n",
        "        x = x + self.drop_path(y)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvEmbed(nn.Module):\n",
        "    \"\"\"3x3 Convolution stems for ViT following ViTC models\"\"\"\n",
        "    def __init__(self, channels, strides, img_size=224, in_chans=3, batch_norm=True):\n",
        "        super().__init__()\n",
        "        # Build the stems\n",
        "        stem = []\n",
        "        channels = [in_chans] + channels\n",
        "        for i in range(len(channels) - 2):\n",
        "            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3, stride=strides[i], padding=1, bias=(not batch_norm))]\n",
        "            if batch_norm:\n",
        "                stem += [nn.BatchNorm2d(channels[i+1])]\n",
        "            stem += [nn.ReLU(inplace=True)]\n",
        "        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n",
        "        self.stem = nn.Sequential(*stem)\n",
        "\n",
        "        # Comptute the number of patches\n",
        "        stride_prod = int(np.prod(strides))\n",
        "        self.num_patches = (img_size[0] // stride_prod)**2\n",
        "\n",
        "    def forward(self, x):\n",
        "        p = self.stem(x)\n",
        "        return p.flatten(2).transpose(1, 2)\n",
        "\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "def apply_masks(x, masks):\n",
        "    \"\"\":param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
        "    :param masks: list of tensors containing indices of patches in [N] to keep\"\"\"\n",
        "    all_x = []\n",
        "    for m in masks:\n",
        "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [batch,T,dim]\n",
        "        all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "    return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "\n",
        "class VisionTransformerPredictor(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, num_patches, embed_dim=768, predictor_embed_dim=384,\n",
        "        depth=6, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.init_std = init_std\n",
        "        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        # --\n",
        "        self.predictor_pos_embed = nn.Parameter(torch.zeros(1, num_patches, predictor_embed_dim), requires_grad=False)\n",
        "        predictor_pos_embed = get_2d_sincos_pos_embed(self.predictor_pos_embed.shape[-1], int(num_patches**.5), cls_token=False)\n",
        "        self.predictor_pos_embed.data.copy_(torch.from_numpy(predictor_pos_embed).float().unsqueeze(0)) # [1, num_patches, predictor_embed_dim]\n",
        "        # --\n",
        "        self.predictor_blocks = nn.ModuleList([Block(dim=predictor_embed_dim, num_heads=num_heads, drop=drop_rate, drop_path=dpr[i]) for i in range(depth)])\n",
        "        self.predictor_norm = nn.LayerNorm(predictor_embed_dim)\n",
        "        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n",
        "        # ------\n",
        "        # trunc_normal_(self.mask_token, std=self.init_std)\n",
        "\n",
        "    def forward(self, x, masks_x, masks):\n",
        "        assert (masks is not None) and (masks_x is not None), 'Cannot run predictor without mask indices'\n",
        "        if not isinstance(masks_x, list): masks_x = [masks_x]\n",
        "        if not isinstance(masks, list): masks = [masks]\n",
        "\n",
        "        # -- Batch Size\n",
        "        B = len(x) // len(masks_x)\n",
        "\n",
        "        # -- map from encoder-dim to pedictor-dim\n",
        "        x = self.predictor_embed(x) # [batch, num_patches, predictor_embed_dim]\n",
        "\n",
        "        # -- add positional embedding to x tokens\n",
        "        x_pos_embed = self.predictor_pos_embed.repeat(B, 1, 1) # [batch, num_patches, predictor_embed_dim]\n",
        "        x += apply_masks(x_pos_embed, masks_x)\n",
        "\n",
        "        _, N_ctxt, D = x.shape\n",
        "\n",
        "        # -- concat mask tokens to x\n",
        "        pos_embs = self.predictor_pos_embed.repeat(B, 1, 1)\n",
        "        pos_embs = apply_masks(pos_embs, masks)\n",
        "        pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x))\n",
        "        # --\n",
        "        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1)\n",
        "        # --\n",
        "        pred_tokens += pos_embs\n",
        "        x = x.repeat(len(masks), 1, 1)\n",
        "        x = torch.cat([x, pred_tokens], dim=1)\n",
        "\n",
        "        # -- fwd prop\n",
        "        for blk in self.predictor_blocks:\n",
        "            x = blk(x)\n",
        "        x = self.predictor_norm(x)\n",
        "\n",
        "        # -- return preds for mask tokens\n",
        "        x = x[:, N_ctxt:]\n",
        "        x = self.predictor_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, img_size=[224], patch_size=16, in_chans=3, embed_dim=768, predictor_embed_dim=384, depth=12, predictor_depth=12,\n",
        "        num_heads=12, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0, norm_layer=nn.LayerNorm, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_features = self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.init_std = init_std\n",
        "        # --\n",
        "        self.patch_embed = PatchEmbed(img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        # --\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=False)\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "        # --\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "    def forward(self, x, masks=None):\n",
        "        if masks is not None:\n",
        "            if not isinstance(masks, list): masks = [masks]\n",
        "\n",
        "        x = self.patch_embed(x)\n",
        "        B, N, D = x.shape # [batch, T, d_model]\n",
        "\n",
        "        # -- add positional embedding to x\n",
        "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
        "        x = x + pos_embed\n",
        "\n",
        "        if masks is not None: x = apply_masks(x, masks)\n",
        "\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            x = blk(x)\n",
        "\n",
        "        if self.norm is not None: x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "    def interpolate_pos_encoding(self, x, pos_embed):\n",
        "        npatch = x.shape[1] - 1\n",
        "        N = pos_embed.shape[1] - 1\n",
        "        if npatch == N: return pos_embed\n",
        "        class_emb, pos_embed = pos_embed[:, 0], pos_embed[:, 1:]\n",
        "        dim = x.shape[-1]\n",
        "        pos_embed = nn.functional.interpolate(pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2), scale_factor=math.sqrt(npatch / N), mode='bicubic',)\n",
        "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
        "\n",
        "# from functools import partial\n",
        "# def vit_predictor(**kwargs):\n",
        "#     model = VisionTransformerPredictor(\n",
        "#         mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "#     return model\n",
        "\n",
        "def vit(patch_size=16, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, # tiny\n",
        "        # patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, # small\n",
        "        # patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, # base\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "bNjp4xFsGPoa",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}