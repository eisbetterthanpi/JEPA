{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0vScvFGGSeN6",
        "8DxjYI9RMQoP"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/Seq_JEPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title rohanpandey WISDM\n",
        "# https://github.com/rohanpandey/Analysis--WISDM-Smartphone-and-Smartwatch-Activity/blob/master/dataset_creation.ipynb\n",
        "! wget https://archive.ics.uci.edu/static/public/507/wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset.zip\n",
        "!unzip wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset.zip\n",
        "!unzip wisdm-dataset.zip\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "folder1=glob.glob(\"wisdm-dataset/raw/*\")\n",
        "# print(folder1)\n",
        "column_names = ['ID', 'activity','timestamp','x','y','z']\n",
        "overall_dataframe=pd.DataFrame(columns = column_names)\n",
        "for subfolder in folder1:\n",
        "    parent_dir = \"./processed/\"\n",
        "    path = os.path.join(parent_dir, subfolder.split('\\\\')[-1])\n",
        "    if not os.path.exists(path): os.makedirs(path)\n",
        "    folder2=glob.glob(subfolder+\"/*\")\n",
        "    for subsubfolder in folder2:\n",
        "        activity_dataframe = pd.DataFrame(columns = column_names)\n",
        "        subfolder_path = os.path.join(path, subsubfolder.split('/')[-1])\n",
        "        if not os.path.exists(subfolder_path): os.makedirs(subfolder_path)\n",
        "        files=glob.glob(subsubfolder+\"/*\")\n",
        "        for file in files:\n",
        "            # print(file)\n",
        "            df = pd.read_csv(file, sep=\",\",header=None)\n",
        "            df.columns = ['ID','activity','timestamp','x','y','z']\n",
        "            # activity_dataframe=activity_dataframe.append(df)\n",
        "            activity_dataframe = pd.concat([activity_dataframe, df], ignore_index=True)\n",
        "\n",
        "        activity_dataframe['z']=activity_dataframe['z'].str[:-1]\n",
        "        # activity_dataframe['meter']=subsubfolder.split('/')[-1]\n",
        "        # activity_dataframe['device']=subfolder.split('/')[-1]\n",
        "        activity_dataframe.to_csv(subfolder_path+'/data.csv',index=False)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "FA00-tf6MJ-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title wisdm dataloader\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "# /content/processed/wisdm-dataset/raw/phone/accel/data.csv\n",
        "# /content/processed/wisdm-dataset/raw/watch/gyro/data.csv\n",
        "\n",
        "class BufferDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        # df_keep = pd.read_csv(\"data.csv\")#[['ID','activity','timestamp','x','y','z']]\n",
        "        df_keep = pd.read_csv(\"/content/processed/wisdm-dataset/raw/watch/accel/data.csv\")\n",
        "\n",
        "        user_acts = dict(tuple(df_keep.groupby(['ID','activity'])[['timestamp','x','y','z']]))\n",
        "        self.data = [[d.to_numpy(), a] for a, d in user_acts.items()]\n",
        "        self.act_dict = {i: act for i, act in enumerate(df_keep['activity'].unique())}\n",
        "        self.act_invdict = {v: k for k, v in self.act_dict.items()} # {'A': 0, 'B': 1, ...\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, id_act = self.data[idx]\n",
        "        id_act = self.process(id_act)\n",
        "        return torch.tensor(x[:3500]), id_act # 3567\n",
        "\n",
        "    def process(self, id_act):\n",
        "        return int(id_act[0]), self.act_invdict[id_act[1]]\n",
        "\n",
        "    def transform(self, act_list): # rand resize crop, rand mask # https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html\n",
        "        act_list = np.array(act_list)\n",
        "        try: hr, temp, heart= zip(*act_list)\n",
        "        except ValueError: print('err:',act_list)\n",
        "        kind = 'nearest' if len(act_list)>self.seq_len/.7 else 'linear'\n",
        "        temp_interpolator = scipy.interpolate.interp1d(hr, temp, kind=kind) # linear nearest quadratic cubic\n",
        "        heart_interpolator = scipy.interpolate.interp1d(hr, heart, kind=kind) # linear nearest quadratic cubic\n",
        "        hr_ = np.sort(np.random.uniform(hr[0], hr[-1], round(self.seq_len*random.uniform(1,1/.7))))\n",
        "        temp_, heart_ = temp_interpolator(hr_), heart_interpolator(hr_)\n",
        "        act_list = list(zip(hr_, temp_, heart_))\n",
        "\n",
        "        idx = torch.randint(len(act_list)-self.seq_len+1, size=(1,))\n",
        "        # return act_list[idx: idx+self.seq_len]\n",
        "        act_list = act_list[idx: idx+self.seq_len]\n",
        "\n",
        "        # mask=(torch.rand(self.seq_len)<.1) # True -> masked # random masking\n",
        "        # # act_list[mask] = self.pad[0]\n",
        "        # act_list = [self.pad[0] if m else a for m,a in zip(mask, act_list)]\n",
        "        return act_list\n",
        "\n",
        "    def add(self, episode):\n",
        "        self.data.append(episode)\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    # print(npimg.shape) # (3, 64, 64)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# df_keep = pd.read_csv(\"data.csv\")#[['ID','activity','timestamp','x','y','z']]\n",
        "# user_acts = dict(tuple(df_keep.groupby(['ID','activity'])[['timestamp','x','y','z']]))\n",
        "# dataset = [[d.to_numpy(), a] for a, d in user_acts.items()]\n",
        "\n",
        "\n",
        "train_data = BufferDataset() # one line of poem is roughly 50 characters\n",
        "\n",
        "dataset_size = len(train_data)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(0.7 * dataset_size))\n",
        "np.random.seed(0)\n",
        "np.random.shuffle(indices)\n",
        "train_indices, val_indices = indices[:split], indices[split:]\n",
        "\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "valid_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 64 #512\n",
        "# train_loader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "# test_loader = DataLoader(train_data, sampler=valid_sampler, batch_size=batch_size, num_workers=2, drop_last=True)\n",
        "train_loader = DataLoader(train_data, sampler=train_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "test_loader = DataLoader(train_data, sampler=valid_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True)\n"
      ],
      "metadata": {
        "id": "CNVNCV8CGtR_",
        "cellView": "form"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title RoPE\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def RoPE(dim, seq_len=512, base=10000):\n",
        "    theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "    pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "    angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "    rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [1, seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "    return rot_emb\n",
        "\n",
        "class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, seq_len=512, base=10000):\n",
        "        super().__init__()\n",
        "        self.dim, self.base = dim, base\n",
        "        theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "        pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "        angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "        self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.base)\n",
        "        return x * self.rot_emb[:seq_len]\n",
        "\n",
        "class RotEmb(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, top=torch.pi, base=10000):\n",
        "        super().__init__()\n",
        "        self.theta = top / (base ** (torch.arange(0, dim, step=2, device=device) / dim))\n",
        "        # self.theta = top / (base ** torch.linspace(0, 1, dim//2, device=device))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (pos.unsqueeze(-1) * self.theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [seq_len, dim]\n"
      ],
      "metadata": {
        "id": "ge36SCxOl2Oq",
        "cellView": "form"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title AttentionBlock\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class SelfAttn(nn.Module):\n",
        "    def __init__(self, dim, n_heads):\n",
        "        super().__init__()\n",
        "        self.dim, self.n_heads = dim, n_heads\n",
        "        d_head = dim//n_heads\n",
        "        self.qkv = nn.Linear(dim, dim*3, bias=False)\n",
        "        # self.lin = nn.Linear(dim, dim)\n",
        "        self.lin = zero_module(nn.Linear(dim, dim))\n",
        "        # self.rope = RoPE(d_head, seq_len=512, base=10000)\n",
        "        # self.rope = RoPE2D(d_head, h=64, w=64, base=100)\n",
        "        self.scale = d_head**-.5\n",
        "\n",
        "    def forward(self, x): # [b,t,d]\n",
        "        q,k,v = self.qkv(x).unflatten(-1, (self.n_heads,-1)).transpose(1,2).chunk(3, dim=-1) # [b, t, n_heads, d_head] -> [b, n_heads, t, d_head]\n",
        "        # q, k = self.rope(q), self.rope(k)\n",
        "        q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2)\n",
        "        context = k.transpose(-2,-1) @ v # [batch, n_heads, d_head, d_head]\n",
        "        x = q @ context # [b, n_heads, t, d_head]\n",
        "        # print('SelfAttn', x.shape)\n",
        "        x = x.transpose(1,2).flatten(2)\n",
        "        return self.lin(x)\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, mult=4, drop=0.):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        # self.norm = nn.RMSNorm(d_model, elementwise_affine=False) # LayerNorm RMSNorm\n",
        "        self.norm1, self.norm2 = nn.LayerNorm(d_model), nn.LayerNorm(d_model)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "        self.attn = SelfAttn(d_model, n_heads)\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        ff_dim=d_model*mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), act, nn.Dropout(drop), nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "            nn.RMSNorm(ff_dim), act, nn.Dropout(drop), zero_module(nn.Linear(ff_dim, d_model))\n",
        "        )\n",
        "\n",
        "    def forward(self, x): # [batch, seq_len, d_model]\n",
        "        # print('attnblk fwd',x.shape)\n",
        "        x = x + self.drop(self.attn(self.norm1(x)))\n",
        "        x = x + self.ff(x)\n",
        "        # x = x + self.drop(self.ff(self.norm2(x)))\n",
        "        return x\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "f6T4F651kmGh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title masks\n",
        "import torch\n",
        "\n",
        "def multiblock(seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "    return target_mask\n",
        "\n",
        "\n",
        "def multiblock2d(hw=(8,8), scale=(.15,.2), aspect_ratio=(.75,1.5), M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_aspect = torch.rand(1) * (aspect_ratio[1] - aspect_ratio[0]) + aspect_ratio[0] # in (min_s, max_s) # all blocks same size\n",
        "    mask_scale = torch.rand(1) * (scale[1] - scale[0]) + scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    h = (mask_scale/mask_aspect)**.5# h*(h*aspect) = scale\n",
        "    w = h * mask_aspect\n",
        "    h_pos, w_pos = torch.rand(M)*(1-w), torch.rand(M)*(1-h) # in (0, 1 - mask_len)\n",
        "    h_len, h_pos = (h*hw[0]).int(), h_pos*hw[0]\n",
        "    w_len, w_pos = (w*hw[1]).int(), w_pos*hw[1]\n",
        "    h_ind, w_ind = torch.arange(hw[0]).unsqueeze(0), torch.arange(hw[1]).unsqueeze(0) # [1, seq]\n",
        "    h_mask = (h_ind>=h_pos.unsqueeze(-1)) & (h_ind<(h_pos+h_len).unsqueeze(-1)) # [M, seq]\n",
        "    w_mask = (w_ind>=w_pos.unsqueeze(-1)) & (w_ind<(w_pos+w_len).unsqueeze(-1)) # [M, seq]\n",
        "    target_mask = h_mask.unsqueeze(-1) & w_mask.unsqueeze(-2) # [M, seq, seq]\n",
        "    return target_mask\n",
        "\n",
        "# https://arxiv.org/pdf/2210.07224\n",
        "def randpatch(seq, mask_size=8, gamma=0.9): # num patches of seq, mask patch size, masking ratio\n",
        "    # mask = torch.rand(seq//mask_size)<gamma\n",
        "    length = seq//mask_size\n",
        "    g = torch.normal(gamma, std=.1, size=(1,)).clamp(.5,.9)\n",
        "    # g = gamma\n",
        "    idx = torch.randperm(length)[:int(length*g)]\n",
        "    mask = torch.zeros(length, dtype=bool)\n",
        "    mask[idx] = True\n",
        "    mask = mask.repeat_interleave(mask_size, dim=-1)\n",
        "    return mask # [seq] , True -> mask\n",
        "\n",
        "\n",
        "import torch\n",
        "def apply_masks(x, mask): # [b,t,d], [mask_size] # https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "    mask_keep = mask.unsqueeze(-1).repeat(x.size(0), 1, x.size(-1)) # [batch,T,dim]\n",
        "    return torch.gather(x, dim=1, index=mask_keep) # [batch,mask_size,dim]\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "pQfM2fYvcTh6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title simplex\n",
        "!pip install -q opensimplex\n",
        "import opensimplex\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def simplexmask(hw=(8,8), scale=(.15,.2)):\n",
        "    ix = iy = np.linspace(0, 1, num=8)\n",
        "    ix, iy = ix+np.random.randint(1e10), iy+np.random.randint(1e10)\n",
        "    y=opensimplex.noise2array(ix, iy)\n",
        "    y = torch.from_numpy(y)\n",
        "    mask_scale = torch.rand(1) * (scale[1] - scale[0]) + scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    yy = y.flatten().sort()[0][int(hw[0]*hw[1]*mask_scale)]\n",
        "    mask = (y<=yy.item())\n",
        "    return mask # T/F [h,w]\n",
        "\n",
        "def simplexmask1d(seq=512, scale=(.15,.2)):\n",
        "    i = np.linspace(0, 2, num=seq)\n",
        "    j = np.array([np.random.randint(1e10)])\n",
        "    y=opensimplex.noise2array(i, j)\n",
        "    # plt.pcolormesh(y)\n",
        "    # plt.show()\n",
        "    y = torch.from_numpy(y)\n",
        "    # print(y.shape)\n",
        "    mask_scale = torch.rand(1) * (scale[1] - scale[0]) + scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    yy = y.flatten().sort()[0][int(seq*mask_scale)]\n",
        "    mask = (y<=yy.item())\n",
        "    return mask # T/F [h,w]\n",
        "\n",
        "# mask = simplexmask(hw=(8,8), scale=(.6,.8))\n",
        "# mask = ~simplexmask(hw=(8,8), scale=(.85,1))\n",
        "# mask = simplexmask1d(seq=100, scale=(.6,.8))\n",
        "# # mask = simplexmask1d(seq=100, scale=(.85,1.))\n",
        "\n",
        "# from matplotlib import pyplot as plt\n",
        "# # # plt.pcolormesh(y)\n",
        "# plt.pcolormesh(mask)\n",
        "# plt.show()\n",
        "\n",
        "# print(mask)\n"
      ],
      "metadata": {
        "id": "Fh9o__m2-j7K",
        "cellView": "form"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title RandomResizedCrop1d\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class RandomResizedCrop1d(nn.Module):\n",
        "    def __init__(self, size, scale=(.8,1.)):\n",
        "        super().__init__()\n",
        "        self.size, self.scale = size, scale\n",
        "\n",
        "    def forward(self, x): # [batch, seq, dim]\n",
        "        x = x.transpose(-2,-1)\n",
        "        crop = torch.rand(1) * (self.scale[1] - self.scale[0]) + self.scale[0]\n",
        "        # print(crop)\n",
        "        crop=.5\n",
        "        pos = torch.rand(1) * (1 - crop)\n",
        "        left = int(pos*x.shape[-1])\n",
        "        right = int((pos+crop)*x.shape[-1])\n",
        "        # print(left,right)\n",
        "        # print(pos, pos+crop)\n",
        "        # x = F.interpolate(x[...,left:right], size=x.shape[-1], mode='linear')\n",
        "        x = F.adaptive_avg_pool1d(x[...,left:right], x.shape[-1]) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "        x = x.transpose(-2,-1)\n",
        "        return x\n",
        "\n",
        "# transform = RandomResizedCrop1d(10)\n",
        "# x = torch.randn(1,10,3)\n",
        "# # print(x.shape)\n",
        "# print(x)\n",
        "# out = transform(x)\n",
        "# # print(out.shape)\n",
        "# # print(out)\n"
      ],
      "metadata": {
        "id": "vL2I-31mwfSz",
        "cellView": "form"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title TransformerModel/Predictor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, n_heads=4, d_hid=None, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        # self.pos_emb = nn.Parameter(torch.randn(1, 200, d_model)*0.02)\n",
        "        self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=200, base=10000), requires_grad=False)\n",
        "\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, n_heads=n_heads) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.randn(1,1,d_model)*0.02) # randn zeros\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim)# if out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = x.shape\n",
        "        # x = x * self.pos_enc(context_indices)\n",
        "        # print(\"Trans pred\",x.shape, self.positional_emb[:,context_indices].shape)\n",
        "        x = x + self.pos_emb[:,context_indices]\n",
        "\n",
        "        # pred_tokens = self.cls * self.pos_enc(trg_indices) # [M, num_trg_toks, d_model]\n",
        "        pred_tokens = self.cls + self.pos_emb[:,trg_indices]\n",
        "        pred_tokens = pred_tokens.repeat(batch, 1, 1) # [batch, num_trg_toks, d_model]\n",
        "        # print(\"pred fwd\", x.shape, pred_tokens.shape)\n",
        "        x = torch.cat([x, pred_tokens], dim=1) # [batch, seq_len+num_trg_toks, d_model]\n",
        "        out = self.transformer(x)\n",
        "\n",
        "        out = self.norm(out)\n",
        "        out = out[:,seq:] # [batch, num_trg_toks, d_model]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, drop=0):\n",
        "    def __init__(self, patch_size, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0):\n",
        "        super().__init__()\n",
        "        # patch_size=32\n",
        "        self.embed = nn.Sequential(\n",
        "            # # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.MaxPool1d(2,2), #nn.MaxPool1d(3, 2, 3//2),\n",
        "            # # nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.Conv1d(d_model, d_model,3,2,3//2)\n",
        "            nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2),\n",
        "            # nn.Conv1d(in_dim, d_model, patch_size, patch_size), # like patch\n",
        "            )\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        # self.pos_emb = nn.Parameter(torch.randn(1, 200, d_model)*0.02)\n",
        "        self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=200, base=10000), requires_grad=False)#.unsqueeze(0)\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, n_heads=n_heads) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "        # x = self.pos_enc(x)\n",
        "        x = x + self.pos_emb[:,:x.shape[1]]\n",
        "        if context_indices != None: x = x[:,context_indices]\n",
        "        # print(\"TransformerModel\",x.shape)\n",
        "\n",
        "        x = self.transformer(x)\n",
        "        out = self.norm(x)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "batch, seq_len, d_model = 4,3500,16\n",
        "in_dim = 3\n",
        "patch_size=32\n",
        "model = TransformerModel(patch_size, in_dim, d_model, n_heads=4, nlayers=10, drop=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "# # # print(out)\n",
        "# model = TransformerPredictor(in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1).to(device)\n",
        "# out = model(out)\n",
        "# print(out.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48596ec0-7526-478e-d38c-6f354c0e6ddd",
        "id": "M-zdjdJixtOu",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34928\n",
            "torch.Size([4, 110, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title SeqJEPA\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class SeqJEPA(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, nlayers=2, n_heads=4):\n",
        "        super().__init__()\n",
        "        if out_dim is None: out_dim = d_model\n",
        "        self.patch_size = 32\n",
        "        self.student = TransformerModel(self.patch_size, in_dim, d_model, out_dim=out_dim, n_heads=n_heads, nlayers=nlayers, drop=0.)\n",
        "        self.predicter = TransformerPredictor(out_dim, 3*d_model//8, out_dim, n_heads=4, nlayers=1, drop=0.)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "        # self.transform = RandomResizedCrop1d(3500, scale=(.8,1.))\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]\n",
        "        batch, seq, dim = x.shape\n",
        "        # x = self.transform(x)\n",
        "        target_mask = multiblock(seq//self.patch_size, min_s=0.2, max_s=0.3, M=4).any(0) # best.2.3M4 og.15.2M4# mask out targets to be predicted # [M, seq]\n",
        "        # # target_mask = randpatch(seq//self.patch_size, mask_size=8, gamma=.9) # 8.9 [seq]\n",
        "        # target_mask = simplexmask1d(seq//self.patch_size, scale=(.7,.8)) # .6.8\n",
        "\n",
        "        context_mask = ~multiblock(seq//self.patch_size, min_s=0.85, max_s=1., M=1)|target_mask # og .85,1.M1 # [1, seq], True->Mask\n",
        "        # context_mask = torch.zeros((1,8,8), dtype=bool)|target_mask # [1,h,w], True->Mask\n",
        "        # context_mask = ~simplexmask1d(seq//self.patch_size, scale=(.85,1)).unsqueeze(0)|target_mask # [1,h,w]\n",
        "\n",
        "        # imshow(target_mask)\n",
        "        # imshow(context_mask)\n",
        "\n",
        "        target_mask, context_mask = target_mask.flatten(), context_mask.flatten() # [8*8]\n",
        "        target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "\n",
        "        context_indices = (~context_mask).nonzero().squeeze(-1) # int idx [num_context_toks] , idx of context not masked\n",
        "        # print('seq_jepa loss context_indices',context_indices)\n",
        "        # print('seq_jepa loss x',x.shape)\n",
        "        sx = self.student(x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "        # print('seq_jepa loss sx',sx.shape)\n",
        "\n",
        "        trg_indices = target_mask.nonzero().squeeze(-1) # int idx [num_trg_toks] , idx of targets that are masked\n",
        "        # print(trg_indices.shape)\n",
        "        sy_ = self.predicter(sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "        sy_ = F.layer_norm(sy_, (sy_.size(-1),))\n",
        "        with torch.no_grad():\n",
        "            sy = self.teacher(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "            sy = sy[:,trg_indices] # nan bec len(trg_ind)==0 # print('loss sy',torch.isnan(sy).any())\n",
        "            sy = F.layer_norm(sy, (sy.size(-1),))\n",
        "        loss = F.mse_loss(sy, sy_)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        sx = self.student(x)\n",
        "        out = sx.mean(dim=1)\n",
        "        return out\n",
        "\n",
        "# min_s=0.15, max_s, M\n",
        "# trg.15.2M4 C.85 1\n",
        "\n",
        "# 1e-2,1e-3 < 3e-3,1e-3\n",
        "# patch16 < patch32\n",
        "\n",
        "\n",
        "seq_jepa = SeqJEPA(in_dim=3, d_model=32, out_dim=16, nlayers=4, n_heads=4).to(device)#.to(torch.float)\n",
        "# seq_jepa = SeqJEPA(in_dim=3, d_model=1024, out_dim=16, nlayers=12, d_head=16).to(device)#.to(torch.float)\n",
        "# optim = torch.optim.AdamW(seq_jepa.parameters(), lr=1e-3) # 1e-3?\n",
        "optim = torch.optim.AdamW([{'params': seq_jepa.student.parameters()},\n",
        "    {'params': seq_jepa.predicter.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # default 1e-2, 5e-2\n",
        "    # {'params': seq_jepa.predicter.parameters(), 'lr': 1e-2}], lr=1e-3, weight_decay=1e-2)\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/configs/in1k_vith14_ep300.yaml\n",
        "# d_model 1024,384\n",
        "# depth 12,6/12\n",
        "# wd 5e-2 - 4e-1\n",
        "# adamw 1e-4 - 1e-3 - 1e-6\n",
        "# ema 0.996-1\n",
        "\n",
        "print(sum(p.numel() for p in seq_jepa.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.parameters())) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.predicter.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.student.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.teacher.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# d_model^2 * nlayers\n",
        "\n",
        "x = torch.rand((2, 3500, in_dim), device=device)\n",
        "out = seq_jepa.loss(x)\n",
        "print(out.shape)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(16, 18).to(device)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n",
        "# optim = torch.optim.AdamW([{'params': seq_jepa.parameters()}, {'params': classifier.parameters(), 'lr': 1e-3}], lr=1e-3)\n",
        "\n"
      ],
      "metadata": {
        "id": "ardu1zJwdHM3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76a0dc14-e1bf-4ab3-d406-08bb3755ab08",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60632\n",
            "torch.Size([])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title TransformerVICReg\n",
        "import torch\n",
        "from torch import nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class TransformerVICReg(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, n_heads=4, d_hid=None, nlayers=1, drop=0):\n",
        "        super().__init__()\n",
        "        act = nn.GELU()\n",
        "        patch_size=4\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Linear(in_dim, d_model), act\n",
        "            # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(3,2, 3//2),\n",
        "            nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2),\n",
        "            # nn.Conv1d(in_dim, d_model, patch_size, patch_size), # patch\n",
        "            )\n",
        "        self.pos_enc = RoPE(d_model, seq_len=200, base=10000)\n",
        "        # self.pos_emb = nn.Parameter(torch.randn(1, 200, d_model))\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=200, base=10000).unsqueeze(0), requires_grad=False)\n",
        "\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, n_heads=n_heads) for _ in range(nlayers)])\n",
        "\n",
        "        # out_dim = out_dim or d_model\n",
        "        self.lin = nn.Linear(d_model, out_dim)\n",
        "        self.attn_pool = nn.Linear(d_model, 1, bias=False)\n",
        "\n",
        "        dim_v = d_model * 4\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(out_dim, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v, bias=False),\n",
        "            )\n",
        "\n",
        "    def forward(self, x): # [b,t,d] / [b,c,h,w]\n",
        "        # x = self.embed(x).flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c]\n",
        "        x = self.embed(x.transpose(-2,-1)).transpose(-2,-1) # [b,t,d]\n",
        "        # x = self.pos_enc(x)\n",
        "        # x = x + self.pos_emb[:,:x.shape[1]]\n",
        "\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        attn = self.attn_pool(x).squeeze(-1) # [batch, seq] # seq_pool\n",
        "        out = (torch.softmax(attn, dim=-1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, seq] @ [batch, seq, dim] -> [batch, dim]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch, ntoken]\n",
        "\n",
        "    def expand(self, x):\n",
        "        sx = self.forward(x)\n",
        "        vx = self.exp(sx)\n",
        "        return vx\n",
        "\n",
        "batch, seq_len, d_model = 4,3500,32\n",
        "in_dim, out_dim=3,16\n",
        "model = TransformerVICReg(in_dim, d_model, out_dim, n_heads=4, nlayers=2, drop=0.).to(device)\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "# x =  torch.rand((batch, in_dim, 32,32), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b05e5bb2-904f-45b1-d12d-029f798c7657",
        "id": "m4rj4LfPuN1H"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Violet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class Violet(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, nlayers=2, n_heads=4):\n",
        "        super().__init__()\n",
        "        out_dim = out_dim or d_model\n",
        "        self.student = TransformerVICReg(in_dim, d_model, out_dim=out_dim, n_heads=n_heads, nlayers=nlayers, drop=0.)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "\n",
        "        # vicreg\n",
        "        self.sim_coeff=25. # 10.0 # 25.0 # λ\n",
        "        self.std_coeff=25. # 1.0 # 25.0 # µ\n",
        "        self.cov_coeff=1. # 25.0 # 1.0 # ν\n",
        "\n",
        "        # self.transform = RandomResizedCrop1d(3500, scale=(.8,1.))\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]c/ [b,c,h,w]\n",
        "        # print(x.shape)\n",
        "        # self.transform(x)\n",
        "        vx = self.student.expand(x) # [batch, num_context_toks, out_dim]\n",
        "        with torch.no_grad(): vy = self.teacher.expand(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "        loss = self.vicreg(vx, vy)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        return self.student(x)\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size, num_features = x.shape\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = self.sim_coeff * repr_loss + self.std_coeff * std_loss + self.cov_coeff * cov_loss\n",
        "        print(\"in vicreg \",self.sim_coeff * repr_loss.item() , self.std_coeff * std_loss.item() , self.cov_coeff * cov_loss.item())\n",
        "        # return loss\n",
        "        return repr_loss, std_loss, cov_loss\n",
        "\n",
        "\n",
        "\n",
        "in_dim=3\n",
        "violet = Violet(in_dim, d_model=32, out_dim=16, nlayers=2, n_heads=4).to(device)\n",
        "# voptim = torch.optim.AdamW(violet.parameters(), lr=1e-3) # 1e-3?\n",
        "# voptim = torch.optim.AdamW([{'params': violet.student.transformer.parameters()},\n",
        "voptim = torch.optim.AdamW([{'params': violet.student.exp.parameters(), 'lr': 3e-3},\n",
        "    {'params': [param for name, param in violet.named_parameters() if param != violet.student.exp.parameters()]}], lr=1e-3, weight_decay=1e-2) # default 1e-2\n",
        "# print(sum(p.numel() for p in violet.parameters() if p.requires_grad)) # 27584\n",
        "\n",
        "\n",
        "x = torch.rand((2,1000,in_dim), device=device)\n",
        "loss = violet.loss(x)\n",
        "# print(out.shape)\n",
        "print(loss)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(16, 18).to(device) # torch/autograd/graph.py RuntimeError: CUDA error: device-side assert triggered CUDA kernel errors\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "706b986c-d3bf-40f9-a777-30f644242f87",
        "id": "ObiHp-LSuRBA"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "some parameters appear in more than one parameter group",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-c0d9bc19e4b2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;31m# voptim = torch.optim.AdamW(violet.parameters(), lr=1e-3) # 1e-3?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;31m# voptim = torch.optim.AdamW([{'params': violet.student.transformer.parameters()},\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m voptim = torch.optim.AdamW([{'params': violet.student.parameters()},\n\u001b[0m\u001b[1;32m     74\u001b[0m     {'params': violet.student.exp.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # default 1e-2\n\u001b[1;32m     75\u001b[0m \u001b[0;31m# print(sum(p.numel() for p in violet.parameters() if p.requires_grad)) # 27584\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, maximize, foreach, capturable, differentiable, fused)\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mfused\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfused\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         )\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfused\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam_group\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_param_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;31m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dynamo_disable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m             )\n\u001b[1;32m    744\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m                 \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36madd_param_group\u001b[0;34m(self, param_group)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparam_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdisjoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_group\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"params\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"some parameters appear in more than one parameter group\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: some parameters appear in more than one parameter group"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(seq_jepa.classifier.weight[0])\n",
        "# for n, p in seq_jepa.target_encoder.named_parameters():\n",
        "#     print(n,p)\n",
        "# optim.param_groups[0]['lr'] = 1e-3"
      ],
      "metadata": {
        "id": "7IACKDymhit7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title RankMe\n",
        "# RankMe: Assessing the Downstream Performance of Pretrained Self-Supervised Representations by Their Rank jun 2023 https://arxiv.org/pdf/2210.02885\n",
        "import torch\n",
        "\n",
        "# https://github.com/Spidartist/IJEPA_endoscopy/blob/main/src/helper.py#L22\n",
        "def RankMe(Z):\n",
        "    \"\"\"\n",
        "    Calculate the RankMe score (the higher, the better).\n",
        "    RankMe(Z) = exp(-sum_{k=1}^{min(N, K)} p_k * log(p_k)),\n",
        "    where p_k = sigma_k (Z) / ||sigma_k (Z)||_1 + epsilon\n",
        "    where sigma_k is the kth singular value of Z.\n",
        "    where Z is the matrix of embeddings (N × K)\n",
        "    \"\"\"\n",
        "    # compute the singular values of the embeddings\n",
        "    # _u, s, _vh = torch.linalg.svd(Z, full_matrices=False)  # s.shape = (min(N, K),)\n",
        "    # s = torch.linalg.svd(Z, full_matrices=False).S\n",
        "    s = torch.linalg.svdvals(Z)\n",
        "    p = s / torch.sum(s, axis=0) + 1e-7\n",
        "    return torch.exp(-torch.sum(p * torch.log(p)))\n",
        "\n",
        "# Z = torch.randn(5, 3)\n",
        "# o = RankMe(Z)\n",
        "# print(o)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "7zghfu_jeOQk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "34de8800-fa4b-4460-803c-3cd8dbfb6c70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8fcb59a7-5d07-492e-d86a-5b216d837b59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "in vicreg  0.0 24.70703125 7.023271564321476e-07\n",
            "strain 24.75\n",
            "in vicreg  0.005919950490351766 24.51171875 2.0208944988553412e-05\n",
            "strain 24.50594139099121\n",
            "in vicreg  0.030841468833386898 24.31640625 0.00021151549299247563\n",
            "strain 24.40605354309082\n",
            "in vicreg  0.09839623235166073 24.0234375 0.0017204340547323227\n",
            "strain 24.100116729736328\n",
            "in vicreg  0.29025208204984665 23.4375 0.018299449235200882\n",
            "strain 23.808551788330078\n",
            "in vicreg  0.9190687909722328 22.4609375 0.1834171712398529\n",
            "strain 23.60248565673828\n",
            "in vicreg  1.450022030621767 21.875 0.43215981125831604\n",
            "strain 23.75718116760254\n",
            "in vicreg  1.152593083679676 22.0703125 0.27904433012008667\n",
            "strain 23.556638717651367\n",
            "in vicreg  0.9629294276237488 22.265625 0.1904018521308899\n",
            "strain 23.403329849243164\n",
            "classify 2.858642578125\n",
            "classify 2.9638671875\n",
            "classify 2.82177734375\n",
            "classify 2.88525390625\n",
            "classify 2.951416015625\n",
            "classify 2.9267578125\n",
            "classify 2.943603515625\n",
            "classify 2.902587890625\n",
            "classify 2.896240234375\n",
            "0.109375\n",
            "0.078125\n",
            "0.125\n",
            "0.046875\n",
            "in vicreg  0.9014333598315716 22.265625 0.16238369047641754\n",
            "strain 23.313817977905273\n",
            "in vicreg  0.563402846455574 22.8515625 0.05672319978475571\n",
            "strain 23.49512481689453\n",
            "in vicreg  0.46603851951658726 23.046875 0.03233069181442261\n",
            "strain 23.498369216918945\n",
            "in vicreg  0.6654712371528149 22.65625 0.06818652153015137\n",
            "strain 23.358657836914062\n",
            "in vicreg  0.8172072470188141 22.4609375 0.0940294936299324\n",
            "strain 23.411237716674805\n",
            "in vicreg  0.928054004907608 22.36328125 0.10736550390720367\n",
            "strain 23.410419464111328\n",
            "in vicreg  1.06859365478158 22.265625 0.13782310485839844\n",
            "strain 23.456417083740234\n",
            "in vicreg  1.1852623894810677 22.0703125 0.17773625254631042\n",
            "strain 23.487998962402344\n",
            "in vicreg  1.3810280710458755 21.875 0.21692045032978058\n",
            "strain 23.47294807434082\n",
            "classify 2.90966796875\n",
            "classify 2.947265625\n",
            "classify 2.851806640625\n",
            "classify 2.906494140625\n",
            "classify 2.873779296875\n",
            "classify 2.90283203125\n",
            "classify 2.887451171875\n",
            "classify 2.93994140625\n",
            "classify 2.890869140625\n",
            "0.09375\n",
            "0.09375\n",
            "0.109375\n",
            "0.0625\n",
            "in vicreg  1.1551463976502419 22.0703125 0.14457957446575165\n",
            "strain 23.424724578857422\n",
            "in vicreg  1.2332042679190636 21.875 0.19085286557674408\n",
            "strain 23.299057006835938\n",
            "in vicreg  1.0706542059779167 22.0703125 0.14139384031295776\n",
            "strain 23.337047576904297\n",
            "in vicreg  0.8311894722282887 22.4609375 0.08211157470941544\n",
            "strain 23.413301467895508\n",
            "in vicreg  0.9044250473380089 22.265625 0.10252548009157181\n",
            "strain 23.25695037841797\n",
            "in vicreg  0.8677569217979908 22.36328125 0.08680765330791473\n",
            "strain 23.329565048217773\n",
            "in vicreg  0.9252316318452358 22.265625 0.09731495380401611\n",
            "strain 23.272546768188477\n",
            "in vicreg  0.8646181784570217 22.36328125 0.07786300778388977\n",
            "strain 23.317481994628906\n",
            "in vicreg  0.9409444406628609 22.265625 0.09010504931211472\n",
            "strain 23.281049728393555\n",
            "classify 2.86962890625\n",
            "classify 2.9130859375\n",
            "classify 2.958740234375\n",
            "classify 2.898681640625\n",
            "classify 2.91943359375\n",
            "classify 2.908935546875\n",
            "classify 2.895751953125\n",
            "classify 2.878173828125\n",
            "classify 2.875\n",
            "0.15625\n",
            "0.09375\n",
            "0.0625\n",
            "0.0625\n",
            "in vicreg  1.2032708153128624 21.875 0.1500903069972992\n",
            "strain 23.228361129760742\n",
            "in vicreg  1.007808931171894 22.16796875 0.10508422553539276\n",
            "strain 23.237892150878906\n",
            "in vicreg  1.0670879855751991 22.0703125 0.10328378528356552\n",
            "strain 23.29537010192871\n",
            "in vicreg  1.2794568203389645 21.875 0.14594532549381256\n",
            "strain 23.30040168762207\n",
            "in vicreg  1.3781487010419369 21.6796875 0.18224689364433289\n",
            "strain 23.185396194458008\n",
            "in vicreg  1.4431634917855263 21.6796875 0.18716312944889069\n",
            "strain 23.255325317382812\n",
            "in vicreg  1.3161765411496162 21.875 0.13430768251419067\n",
            "strain 23.325485229492188\n",
            "in vicreg  1.025129109621048 22.16796875 0.08421728760004044\n",
            "strain 23.234346389770508\n",
            "in vicreg  0.9409943595528603 22.265625 0.08187322318553925\n",
            "strain 23.27286720275879\n",
            "classify 2.944091796875\n",
            "classify 2.87646484375\n",
            "classify 2.906982421875\n",
            "classify 2.925537109375\n",
            "classify 2.929443359375\n",
            "classify 2.925537109375\n",
            "classify 2.947998046875\n",
            "classify 2.898193359375\n",
            "classify 2.89404296875\n",
            "0.109375\n",
            "0.03125\n",
            "0.09375\n",
            "0.109375\n",
            "in vicreg  0.964648462831974 22.265625 0.07303747534751892\n",
            "strain 23.287687301635742\n",
            "in vicreg  0.8476724848151207 22.36328125 0.06118973344564438\n",
            "strain 23.28386116027832\n",
            "in vicreg  0.940418615937233 22.265625 0.07650366425514221\n",
            "strain 23.266921997070312\n",
            "in vicreg  1.0330496355891228 22.16796875 0.08161266148090363\n",
            "strain 23.23966407775879\n",
            "in vicreg  1.19162043556571 21.97265625 0.11610764265060425\n",
            "strain 23.307727813720703\n",
            "in vicreg  1.0185308754444122 22.16796875 0.08764059096574783\n",
            "strain 23.231172561645508\n",
            "in vicreg  1.1787484399974346 21.97265625 0.10788919031620026\n",
            "strain 23.286638259887695\n",
            "in vicreg  1.268910150974989 21.875 0.13010406494140625\n",
            "strain 23.27401351928711\n",
            "in vicreg  1.2375237420201302 21.875 0.1344659924507141\n",
            "strain 23.246990203857422\n",
            "classify 2.935302734375\n",
            "classify 2.949462890625\n",
            "classify 2.86962890625\n",
            "classify 2.885986328125\n",
            "classify 2.8779296875\n",
            "classify 2.908203125\n",
            "classify 2.877197265625\n",
            "classify 2.946533203125\n",
            "classify 2.927001953125\n",
            "0.140625\n",
            "0.0625\n",
            "0.046875\n",
            "0.09375\n",
            "in vicreg  1.2326721101999283 21.875 0.1263420730829239\n",
            "strain 23.2340145111084\n",
            "in vicreg  1.495395228266716 21.6796875 0.16797322034835815\n",
            "strain 23.288368225097656\n",
            "in vicreg  1.17857214063406 21.97265625 0.1077737882733345\n",
            "strain 23.286346435546875\n",
            "in vicreg  1.1115197092294693 22.0703125 0.09651494771242142\n",
            "strain 23.33303451538086\n",
            "in vicreg  0.9321462363004684 22.265625 0.0679754838347435\n",
            "strain 23.2501220703125\n",
            "in vicreg  1.0906878858804703 22.0703125 0.09550654143095016\n",
            "strain 23.311195373535156\n",
            "in vicreg  0.9845142252743244 22.265625 0.07933630794286728\n",
            "strain 23.31385040283203\n",
            "in vicreg  0.8892151527106762 22.265625 0.0617348849773407\n",
            "strain 23.200950622558594\n",
            "in vicreg  0.9727832861244678 22.265625 0.06775331497192383\n",
            "strain 23.290536880493164\n",
            "classify 2.8974609375\n",
            "classify 2.88427734375\n",
            "classify 2.94189453125\n",
            "classify 2.898681640625\n",
            "classify 2.857666015625\n",
            "classify 2.967529296875\n",
            "classify 2.87841796875\n",
            "classify 2.89892578125\n",
            "classify 2.909423828125\n",
            "0.078125\n",
            "0.09375\n",
            "0.0625\n",
            "0.0625\n",
            "in vicreg  1.241093222051859 21.875 0.11282891035079956\n",
            "strain 23.22892189025879\n",
            "in vicreg  0.9244818240404129 22.265625 0.05934285372495651\n",
            "strain 23.23382568359375\n",
            "in vicreg  1.3345610350370407 21.77734375 0.12268564105033875\n",
            "strain 23.207246780395508\n",
            "in vicreg  1.5438064932823181 21.58203125 0.1762976497411728\n",
            "strain 23.345104217529297\n",
            "in vicreg  1.398267038166523 21.6796875 0.12907974421977997\n",
            "strain 23.152347564697266\n",
            "in vicreg  1.2729041278362274 21.875 0.10600989311933517\n",
            "strain 23.25391387939453\n",
            "in vicreg  1.2423360720276833 21.875 0.10375110805034637\n",
            "strain 23.221086502075195\n",
            "in vicreg  1.2362413108348846 21.875 0.11703035980463028\n",
            "strain 23.228271484375\n",
            "in vicreg  1.0567627847194672 22.0703125 0.09149813652038574\n",
            "strain 23.27326011657715\n",
            "classify 2.890380859375\n",
            "classify 2.83837890625\n",
            "classify 2.91748046875\n",
            "classify 2.883544921875\n",
            "classify 2.937744140625\n",
            "classify 2.906494140625\n",
            "classify 2.911376953125\n",
            "classify 2.945556640625\n",
            "classify 2.90478515625\n",
            "0.140625\n",
            "0.046875\n",
            "0.0625\n",
            "0.109375\n",
            "in vicreg  1.0122199542820454 22.16796875 0.08516395837068558\n",
            "strain 23.222383499145508\n",
            "in vicreg  1.02932620793581 22.0703125 0.0766761302947998\n",
            "strain 23.231000900268555\n",
            "in vicreg  1.0227825492620468 22.0703125 0.07374165952205658\n",
            "strain 23.221525192260742\n",
            "in vicreg  1.1575352400541306 21.97265625 0.09277330338954926\n",
            "strain 23.250308990478516\n",
            "in vicreg  1.013654936105013 22.16796875 0.06708398461341858\n",
            "strain 23.205738067626953\n",
            "in vicreg  1.2574020773172379 21.875 0.10950729995965958\n",
            "strain 23.24190902709961\n",
            "in vicreg  1.1384101584553719 22.0703125 0.0854470357298851\n",
            "strain 23.348857879638672\n",
            "in vicreg  1.2007879093289375 21.875 0.09793130308389664\n",
            "strain 23.17371940612793\n",
            "in vicreg  1.1334221810102463 22.0703125 0.08251851052045822\n",
            "strain 23.340940475463867\n",
            "classify 2.89501953125\n",
            "classify 2.868408203125\n",
            "classify 2.91796875\n",
            "classify 2.94140625\n",
            "classify 2.926025390625\n",
            "classify 2.86474609375\n",
            "classify 2.89599609375\n",
            "classify 2.890380859375\n",
            "classify 2.904296875\n",
            "0.078125\n",
            "0.046875\n",
            "0.109375\n",
            "0.09375\n",
            "in vicreg  1.453162543475628 21.6796875 0.13289038836956024\n",
            "strain 23.2110538482666\n",
            "in vicreg  1.3651557266712189 21.6796875 0.11643850058317184\n",
            "strain 23.10659408569336\n",
            "in vicreg  1.1219917796552181 22.0703125 0.08808448910713196\n",
            "strain 23.3350772857666\n",
            "in vicreg  1.2492487207055092 21.875 0.10154502093791962\n",
            "strain 23.225793838500977\n",
            "in vicreg  1.2391063384711742 21.875 0.11791687458753586\n",
            "strain 23.232023239135742\n",
            "in vicreg  0.9382197633385658 22.265625 0.06130605190992355\n",
            "strain 23.24952507019043\n",
            "in vicreg  0.9930831380188465 22.16796875 0.07788337022066116\n",
            "strain 23.195966720581055\n",
            "in vicreg  1.0415200144052505 22.0703125 0.07916557788848877\n",
            "strain 23.245685577392578\n",
            "in vicreg  1.084421295672655 22.0703125 0.09929399192333221\n",
            "strain 23.3087158203125\n",
            "classify 2.896484375\n",
            "classify 2.892822265625\n",
            "classify 2.9052734375\n",
            "classify 2.914306640625\n",
            "classify 2.886962890625\n",
            "classify 2.864990234375\n",
            "classify 2.920166015625\n",
            "classify 2.899658203125\n",
            "classify 2.8955078125\n",
            "0.0625\n",
            "0.0625\n",
            "0.09375\n",
            "0.0625\n",
            "in vicreg  1.4009542763233185 21.6796875 0.1421743482351303\n",
            "strain 23.168128967285156\n",
            "in vicreg  1.1577337980270386 21.97265625 0.09170785546302795\n",
            "strain 23.249441146850586\n",
            "in vicreg  1.2422031722962856 21.875 0.1008588895201683\n",
            "strain 23.218061447143555\n",
            "in vicreg  1.3095643371343613 21.77734375 0.11342272162437439\n",
            "strain 23.17298698425293\n",
            "in vicreg  1.2546045705676079 21.77734375 0.11308232694864273\n",
            "strain 23.117687225341797\n",
            "in vicreg  1.1192676611244678 22.0703125 0.0803188681602478\n",
            "strain 23.324586868286133\n",
            "in vicreg  1.0429289191961288 22.0703125 0.07686721533536911\n",
            "strain 23.244796752929688\n",
            "in vicreg  1.2778357602655888 21.875 0.10898406058549881\n",
            "strain 23.26181983947754\n",
            "in vicreg  1.2075943872332573 21.875 0.10220909863710403\n",
            "strain 23.184803009033203\n",
            "classify 2.919921875\n",
            "classify 2.886962890625\n",
            "classify 2.86767578125\n",
            "classify 2.888427734375\n",
            "classify 2.93408203125\n",
            "classify 2.947265625\n",
            "classify 2.934326171875\n",
            "classify 2.878662109375\n",
            "classify 2.891357421875\n",
            "0.078125\n",
            "0.0625\n",
            "0.0625\n",
            "0.09375\n",
            "in vicreg  1.0435804724693298 22.0703125 0.07461657375097275\n",
            "strain 23.24319839477539\n",
            "in vicreg  1.130809634923935 21.97265625 0.0860796794295311\n",
            "strain 23.216890335083008\n",
            "in vicreg  1.2179557234048843 21.875 0.09789250791072845\n",
            "strain 23.19084930419922\n",
            "in vicreg  1.1607514694333076 21.875 0.09455864876508713\n",
            "strain 23.13031005859375\n",
            "in vicreg  1.1187137104570866 21.875 0.08865559101104736\n",
            "strain 23.082368850708008\n",
            "in vicreg  1.2346627190709114 21.77734375 0.1040494441986084\n",
            "strain 23.088712692260742\n",
            "in vicreg  1.1152110062539577 21.97265625 0.09209908545017242\n",
            "strain 23.20730972290039\n",
            "in vicreg  1.1756841093301773 21.875 0.0942482277750969\n",
            "strain 23.14493179321289\n",
            "in vicreg  1.2049991637468338 21.875 0.10868443548679352\n",
            "strain 23.188684463500977\n",
            "classify 2.87744140625\n",
            "classify 2.920654296875\n",
            "classify 2.88916015625\n",
            "classify 2.897705078125\n",
            "classify 2.930908203125\n",
            "classify 2.8759765625\n",
            "classify 2.9345703125\n",
            "classify 2.91796875\n",
            "classify 2.91455078125\n",
            "0.078125\n",
            "0.078125\n",
            "0.09375\n",
            "0.0625\n",
            "in vicreg  1.3533083721995354 21.6796875 0.1260627657175064\n",
            "strain 23.1043701171875\n",
            "in vicreg  1.3089265674352646 21.6796875 0.11625196039676666\n",
            "strain 23.05017852783203\n",
            "in vicreg  1.0919501073658466 22.0703125 0.08365368843078613\n",
            "strain 23.30060386657715\n",
            "in vicreg  1.0723425075411797 21.97265625 0.09094873815774918\n",
            "strain 23.16329002380371\n",
            "in vicreg  1.1848097667098045 21.875 0.09336087852716446\n",
            "strain 23.15317153930664\n",
            "in vicreg  1.2277092784643173 21.875 0.10132361948490143\n",
            "strain 23.20403289794922\n",
            "in vicreg  1.1557074263691902 21.875 0.0912139043211937\n",
            "strain 23.12192153930664\n",
            "in vicreg  0.9940609335899353 22.0703125 0.07523822039365768\n",
            "strain 23.194297790527344\n",
            "in vicreg  1.1979677714407444 21.875 0.09968892484903336\n",
            "strain 23.172657012939453\n",
            "classify 2.8671875\n",
            "classify 2.88720703125\n",
            "classify 2.923828125\n",
            "classify 2.905029296875\n",
            "classify 2.8984375\n",
            "classify 2.916748046875\n",
            "classify 2.88671875\n",
            "classify 2.919921875\n",
            "classify 2.911376953125\n",
            "0.109375\n",
            "0.0625\n",
            "0.0625\n",
            "0.109375\n",
            "in vicreg  1.1702705174684525 21.875 0.09485828876495361\n",
            "strain 23.14012908935547\n",
            "in vicreg  1.1533502489328384 21.875 0.09344863891601562\n",
            "strain 23.12179946899414\n",
            "in vicreg  1.1568976566195488 21.875 0.09070965647697449\n",
            "strain 23.122608184814453\n",
            "in vicreg  1.3090969063341618 21.6796875 0.11657487601041794\n",
            "strain 23.05067253112793\n",
            "in vicreg  1.2765870429575443 21.6796875 0.11748029291629791\n",
            "strain 23.019067764282227\n",
            "in vicreg  1.455339603126049 21.484375 0.13900409638881683\n",
            "strain 23.094343185424805\n",
            "in vicreg  1.0245945304632187 22.0703125 0.0857715755701065\n",
            "strain 23.235366821289062\n",
            "in vicreg  1.0063854977488518 22.0703125 0.07423599809408188\n",
            "strain 23.20562171936035\n",
            "in vicreg  1.0339383035898209 22.0703125 0.07357896119356155\n",
            "strain 23.23251724243164\n",
            "classify 2.929443359375\n",
            "classify 2.86474609375\n",
            "classify 2.884765625\n",
            "classify 2.938720703125\n",
            "classify 2.87255859375\n",
            "classify 2.926513671875\n",
            "classify 2.96630859375\n",
            "classify 2.87451171875\n",
            "classify 2.87255859375\n",
            "0.09375\n",
            "0.078125\n",
            "0.125\n",
            "0.0625\n",
            "in vicreg  1.1347184889018536 21.875 0.0891469344496727\n",
            "strain 23.098865509033203\n",
            "in vicreg  1.141447015106678 21.875 0.09790900349617004\n",
            "strain 23.114356994628906\n",
            "in vicreg  1.2820173054933548 21.6796875 0.11437272280454636\n",
            "strain 23.02138900756836\n",
            "in vicreg  1.0660639964044094 21.97265625 0.08832117915153503\n",
            "strain 23.154386520385742\n",
            "in vicreg  1.2298878282308578 21.77734375 0.10431117564439774\n",
            "strain 23.084197998046875\n",
            "in vicreg  1.1973614804446697 21.875 0.10205475986003876\n",
            "strain 23.174415588378906\n",
            "in vicreg  1.3788308948278427 21.6796875 0.14019052684307098\n",
            "strain 23.144020080566406\n",
            "in vicreg  1.198834367096424 21.875 0.11429129540920258\n",
            "strain 23.188125610351562\n",
            "in vicreg  1.1512037366628647 21.875 0.09798264503479004\n",
            "strain 23.12418556213379\n",
            "classify 2.871337890625\n",
            "classify 2.90966796875\n",
            "classify 2.857177734375\n",
            "classify 2.920166015625\n",
            "classify 2.94140625\n",
            "classify 2.90673828125\n",
            "classify 2.92626953125\n",
            "classify 2.85791015625\n",
            "classify 2.9287109375\n",
            "0.125\n",
            "0.078125\n",
            "0.078125\n",
            "0.0625\n",
            "in vicreg  1.0128017514944077 22.0703125 0.07086416333913803\n",
            "strain 23.20866584777832\n",
            "in vicreg  1.1392123065888882 21.875 0.09187264740467072\n",
            "strain 23.1060848236084\n",
            "in vicreg  1.2351058423519135 21.77734375 0.10719764232635498\n",
            "strain 23.092302322387695\n",
            "in vicreg  1.2166541069746017 21.77734375 0.1088145449757576\n",
            "strain 23.075468063354492\n",
            "in vicreg  1.0889239609241486 21.97265625 0.09067288041114807\n",
            "strain 23.179597854614258\n",
            "in vicreg  1.1873717419803143 21.875 0.10432718694210052\n",
            "strain 23.166698455810547\n",
            "in vicreg  1.2187638320028782 21.77734375 0.10861270129680634\n",
            "strain 23.077375411987305\n",
            "in vicreg  1.2009600177407265 21.875 0.1014028787612915\n",
            "strain 23.1773624420166\n",
            "in vicreg  1.2214232236146927 21.77734375 0.11226627230644226\n",
            "strain 23.083690643310547\n",
            "classify 2.943115234375\n",
            "classify 2.830322265625\n",
            "classify 2.87890625\n",
            "classify 2.9140625\n",
            "classify 2.9228515625\n",
            "classify 2.928955078125\n",
            "classify 2.9052734375\n",
            "classify 2.845458984375\n",
            "classify 2.937255859375\n",
            "0.109375\n",
            "0.125\n",
            "0.0625\n",
            "0.078125\n",
            "in vicreg  1.0067073628306389 22.0703125 0.07514526695013046\n",
            "strain 23.20685386657715\n",
            "in vicreg  1.168517116457224 21.875 0.1071530431509018\n",
            "strain 23.150671005249023\n",
            "in vicreg  1.1249490082263947 21.875 0.09066499769687653\n",
            "strain 23.090614318847656\n",
            "in vicreg  1.0916908271610737 21.875 0.09115533530712128\n",
            "strain 23.057846069335938\n",
            "in vicreg  1.254376582801342 21.6796875 0.11993153393268585\n",
            "strain 22.999309539794922\n",
            "in vicreg  1.1315962299704552 21.875 0.09095548093318939\n",
            "strain 23.097551345825195\n",
            "in vicreg  1.27683999016881 21.6796875 0.11624316871166229\n",
            "strain 23.018083572387695\n",
            "in vicreg  1.2146923691034317 21.77734375 0.10462502390146255\n",
            "strain 23.069318771362305\n",
            "in vicreg  1.2101836502552032 21.77734375 0.10645513981580734\n",
            "strain 23.066638946533203\n",
            "classify 2.921142578125\n",
            "classify 2.931640625\n",
            "classify 2.847412109375\n",
            "classify 2.84619140625\n",
            "classify 2.883056640625\n",
            "classify 2.916259765625\n",
            "classify 2.89990234375\n",
            "classify 2.9482421875\n",
            "classify 2.913330078125\n",
            "0.109375\n",
            "0.078125\n",
            "0.046875\n",
            "0.109375\n",
            "in vicreg  1.2418430298566818 21.6796875 0.11394788324832916\n",
            "strain 22.980791091918945\n",
            "in vicreg  1.1242940090596676 21.875 0.08729620277881622\n",
            "strain 23.086589813232422\n",
            "in vicreg  1.0793840512633324 21.875 0.08937940746545792\n",
            "strain 23.043764114379883\n",
            "in vicreg  1.0675512254238129 21.97265625 0.08621276915073395\n",
            "strain 23.153762817382812\n",
            "in vicreg  1.041479967534542 21.97265625 0.07961030304431915\n",
            "strain 23.121089935302734\n",
            "in vicreg  1.1025778949260712 21.875 0.09306658804416656\n",
            "strain 23.07064437866211\n",
            "in vicreg  1.2066059745848179 21.875 0.11120468378067017\n",
            "strain 23.19281005859375\n",
            "in vicreg  1.4316682703793049 21.484375 0.1480358988046646\n",
            "strain 23.079702377319336\n",
            "in vicreg  1.1797372251749039 21.875 0.10279981791973114\n",
            "strain 23.15753746032715\n",
            "classify 2.954345703125\n",
            "classify 2.846435546875\n",
            "classify 2.842529296875\n",
            "classify 2.889404296875\n",
            "classify 2.936767578125\n",
            "classify 2.850830078125\n",
            "classify 2.87939453125\n",
            "classify 2.91162109375\n",
            "classify 2.906982421875\n",
            "0.078125\n",
            "0.09375\n",
            "0.09375\n",
            "0.0625\n",
            "in vicreg  1.141112670302391 21.875 0.09130674600601196\n",
            "strain 23.107419967651367\n",
            "in vicreg  1.112919021397829 21.875 0.0928509533405304\n",
            "strain 23.08077049255371\n",
            "in vicreg  1.2870999984443188 21.6796875 0.11839723587036133\n",
            "strain 23.03049659729004\n",
            "in vicreg  1.2439746409654617 21.6796875 0.12744887173175812\n",
            "strain 22.996423721313477\n",
            "in vicreg  1.1546443216502666 21.875 0.09535089135169983\n",
            "strain 23.1249942779541\n",
            "in vicreg  1.202908344566822 21.6796875 0.11107154190540314\n",
            "strain 22.93897819519043\n",
            "in vicreg  1.0773098096251488 21.875 0.08498497307300568\n",
            "strain 23.037296295166016\n",
            "in vicreg  1.183997094631195 21.6796875 0.11838483810424805\n",
            "strain 22.927383422851562\n",
            "in vicreg  1.0011011734604836 22.0703125 0.08134903758764267\n",
            "strain 23.207448959350586\n",
            "classify 2.901123046875\n",
            "classify 2.90869140625\n",
            "classify 2.871826171875\n",
            "classify 2.97265625\n",
            "classify 2.900634765625\n",
            "classify 2.876708984375\n",
            "classify 2.847900390625\n",
            "classify 2.892822265625\n",
            "classify 2.88427734375\n",
            "0.046875\n",
            "0.0625\n",
            "0.125\n",
            "0.125\n",
            "in vicreg  1.1800437234342098 21.77734375 0.1066146194934845\n",
            "strain 23.036659240722656\n",
            "in vicreg  1.1739062145352364 21.6796875 0.10303999483585358\n",
            "strain 22.901947021484375\n",
            "in vicreg  0.9875677525997162 22.0703125 0.07433853298425674\n",
            "strain 23.186906814575195\n",
            "in vicreg  1.1078695766627789 21.875 0.09114167839288712\n",
            "strain 23.074010848999023\n",
            "in vicreg  1.1897707358002663 21.6796875 0.10480425506830215\n",
            "strain 22.919574737548828\n",
            "in vicreg  1.1599553748965263 21.875 0.09578798711299896\n",
            "strain 23.1307430267334\n",
            "in vicreg  1.1732134968042374 21.875 0.10025520622730255\n",
            "strain 23.148469924926758\n",
            "in vicreg  1.2476987205445766 21.6796875 0.12113071233034134\n",
            "strain 22.99382781982422\n",
            "in vicreg  1.3058112002909184 21.6796875 0.12878423929214478\n",
            "strain 23.059595108032227\n",
            "classify 2.884765625\n",
            "classify 2.907470703125\n",
            "classify 2.881103515625\n",
            "classify 2.881591796875\n",
            "classify 2.926025390625\n",
            "classify 2.875732421875\n",
            "classify 2.913330078125\n",
            "classify 2.895263671875\n",
            "classify 2.871337890625\n",
            "0.09375\n",
            "0.046875\n",
            "0.09375\n",
            "0.09375\n",
            "in vicreg  1.2920306995511055 21.6796875 0.12132623791694641\n",
            "strain 23.03835678100586\n",
            "in vicreg  1.2545721605420113 21.6796875 0.11515764147043228\n",
            "strain 22.99472999572754\n",
            "in vicreg  1.2167397886514664 21.6796875 0.11860599368810654\n",
            "strain 22.960346221923828\n",
            "in vicreg  1.1271544732153416 21.875 0.09454023838043213\n",
            "strain 23.096694946289062\n",
            "in vicreg  1.1006387881934643 21.875 0.10948021709918976\n",
            "strain 23.085119247436523\n",
            "in vicreg  1.0853083804249763 21.875 0.09805172681808472\n",
            "strain 23.058359146118164\n",
            "in vicreg  0.9825524874031544 21.97265625 0.07900568842887878\n",
            "strain 23.06155776977539\n",
            "in vicreg  1.0959111154079437 21.875 0.09343292564153671\n",
            "strain 23.06434440612793\n",
            "in vicreg  1.0420382022857666 21.875 0.08800748735666275\n",
            "strain 23.00504493713379\n",
            "classify 2.9189453125\n",
            "classify 2.859375\n",
            "classify 2.92138671875\n",
            "classify 2.88623046875\n",
            "classify 2.918701171875\n",
            "classify 2.892578125\n",
            "classify 2.82958984375\n",
            "classify 2.917236328125\n",
            "classify 2.910888671875\n",
            "0.015625\n",
            "0.0625\n",
            "0.15625\n",
            "0.125\n",
            "in vicreg  1.1335539631545544 21.875 0.0949602872133255\n",
            "strain 23.103515625\n",
            "in vicreg  1.2382201850414276 21.6796875 0.12703002989292145\n",
            "strain 22.990249633789062\n",
            "in vicreg  1.3092175126075745 21.58203125 0.12383347749710083\n",
            "strain 23.05805015563965\n",
            "in vicreg  1.2569475919008255 21.6796875 0.1159621924161911\n",
            "strain 22.99791145324707\n",
            "in vicreg  1.2211990542709827 21.6796875 0.1099262535572052\n",
            "strain 22.956125259399414\n",
            "in vicreg  1.0579543188214302 21.875 0.0818810984492302\n",
            "strain 23.014835357666016\n",
            "in vicreg  1.1394103989005089 21.77734375 0.09685925394296646\n",
            "strain 22.986268997192383\n",
            "in vicreg  1.2879319489002228 21.58203125 0.135971337556839\n",
            "strain 23.04890251159668\n",
            "in vicreg  1.0219959542155266 21.875 0.09932315349578857\n",
            "strain 22.996318817138672\n",
            "classify 2.87890625\n",
            "classify 2.88720703125\n",
            "classify 2.890625\n",
            "classify 2.8935546875\n",
            "classify 2.9150390625\n",
            "classify 2.91845703125\n",
            "classify 2.902587890625\n",
            "classify 2.853515625\n",
            "classify 2.91845703125\n",
            "0.09375\n",
            "0.078125\n",
            "0.09375\n",
            "0.140625\n",
            "in vicreg  1.076982356607914 21.875 0.09077654033899307\n",
            "strain 23.04275894165039\n",
            "in vicreg  1.0653109289705753 21.875 0.0879351943731308\n",
            "strain 23.02824592590332\n",
            "in vicreg  1.225709542632103 21.6796875 0.11413223296403885\n",
            "strain 22.964841842651367\n",
            "in vicreg  1.117411907762289 21.875 0.0976998507976532\n",
            "strain 23.090112686157227\n",
            "in vicreg  1.284356229007244 21.6796875 0.12853290140628815\n",
            "strain 23.03788948059082\n",
            "in vicreg  1.2789552100002766 21.6796875 0.12355533987283707\n",
            "strain 23.027511596679688\n",
            "in vicreg  1.281224936246872 21.484375 0.13148364424705505\n",
            "strain 22.912708282470703\n",
            "in vicreg  1.1010239832103252 21.875 0.10039026290178299\n",
            "strain 23.076414108276367\n",
            "in vicreg  1.0328265838325024 21.875 0.08765215426683426\n",
            "strain 22.995479583740234\n",
            "classify 2.89404296875\n",
            "classify 2.93896484375\n",
            "classify 2.888671875\n",
            "classify 2.87158203125\n",
            "classify 2.85791015625\n",
            "classify 2.8876953125\n",
            "classify 2.870849609375\n",
            "classify 2.921142578125\n",
            "classify 2.887451171875\n",
            "0.125\n",
            "0.046875\n",
            "0.125\n",
            "0.046875\n",
            "in vicreg  0.9960060939192772 21.875 0.08407735079526901\n",
            "strain 22.9550838470459\n",
            "in vicreg  1.0494261980056763 21.875 0.0840282142162323\n",
            "strain 23.008455276489258\n",
            "in vicreg  1.067432202398777 21.875 0.09317288547754288\n",
            "strain 23.03560447692871\n",
            "in vicreg  1.479095220565796 21.2890625 0.1770225167274475\n",
            "strain 22.906118392944336\n",
            "in vicreg  1.0570008307695389 21.875 0.08601824939250946\n",
            "strain 23.01801872253418\n",
            "in vicreg  1.1894991621375084 21.6796875 0.11811966449022293\n",
            "strain 22.932619094848633\n",
            "in vicreg  1.2616949155926704 21.58203125 0.1381126344203949\n",
            "strain 23.024808883666992\n",
            "in vicreg  1.0122370906174183 21.875 0.08143139630556107\n",
            "strain 22.968669891357422\n",
            "in vicreg  1.113540306687355 21.77734375 0.09762908518314362\n",
            "strain 22.961170196533203\n",
            "classify 2.877197265625\n",
            "classify 2.932373046875\n",
            "classify 2.88671875\n",
            "classify 2.92626953125\n",
            "classify 2.8759765625\n",
            "classify 2.853271484375\n",
            "classify 2.89111328125\n",
            "classify 2.8642578125\n",
            "classify 2.93359375\n",
            "0.125\n",
            "0.125\n",
            "0.078125\n",
            "0.03125\n",
            "in vicreg  1.0916860774159431 21.875 0.0992528423666954\n",
            "strain 23.06593894958496\n",
            "in vicreg  1.1324836872518063 21.77734375 0.1080302819609642\n",
            "strain 22.990514755249023\n",
            "in vicreg  1.19146928191185 21.6796875 0.11421220749616623\n",
            "strain 22.930681228637695\n",
            "in vicreg  1.1656101793050766 21.6796875 0.12244470417499542\n",
            "strain 22.913053512573242\n",
            "in vicreg  1.1799843981862068 21.6796875 0.124137744307518\n",
            "strain 22.929122924804688\n",
            "in vicreg  1.2394827790558338 21.58203125 0.1268826574087143\n",
            "strain 22.991365432739258\n",
            "in vicreg  1.0003521107137203 21.875 0.07963346689939499\n",
            "strain 22.954986572265625\n",
            "in vicreg  1.1032875627279282 21.77734375 0.10551127046346664\n",
            "strain 22.958797454833984\n",
            "in vicreg  1.1532852426171303 21.6796875 0.10285624116659164\n",
            "strain 22.881141662597656\n",
            "classify 2.85693359375\n",
            "classify 2.93212890625\n",
            "classify 2.8564453125\n",
            "classify 2.920166015625\n",
            "classify 2.845458984375\n",
            "classify 2.8876953125\n",
            "classify 2.90087890625\n",
            "classify 2.917724609375\n",
            "classify 2.88134765625\n",
            "0.125\n",
            "0.046875\n",
            "0.109375\n",
            "0.09375\n",
            "in vicreg  1.0184257291257381 21.875 0.08794662356376648\n",
            "strain 22.98137092590332\n",
            "in vicreg  1.3485250063240528 21.484375 0.14000573754310608\n",
            "strain 22.9885311126709\n",
            "in vicreg  1.2404348701238632 21.58203125 0.1209559291601181\n",
            "strain 22.986391067504883\n",
            "in vicreg  1.201044674962759 21.6796875 0.11321436613798141\n",
            "strain 22.939258575439453\n",
            "in vicreg  1.2017646804451942 21.6796875 0.11640159040689468\n",
            "strain 22.943166732788086\n",
            "in vicreg  1.111135073006153 21.6796875 0.10762986540794373\n",
            "strain 22.843765258789062\n",
            "in vicreg  0.960924755781889 21.97265625 0.07548477500677109\n",
            "strain 23.036409378051758\n",
            "in vicreg  0.9688755497336388 21.97265625 0.07821501046419144\n",
            "strain 23.047090530395508\n",
            "in vicreg  1.0019008070230484 21.875 0.09704186767339706\n",
            "strain 22.97394371032715\n",
            "classify 2.84033203125\n",
            "classify 2.9521484375\n",
            "classify 2.87548828125\n",
            "classify 2.835693359375\n",
            "classify 2.893310546875\n",
            "classify 2.9306640625\n",
            "classify 2.87939453125\n",
            "classify 2.89892578125\n",
            "classify 2.87646484375\n",
            "0.0625\n",
            "0.0625\n",
            "0.125\n",
            "0.109375\n",
            "in vicreg  1.303732581436634 21.484375 0.14607031643390656\n",
            "strain 22.94980239868164\n",
            "in vicreg  1.1537660844624043 21.6796875 0.11397624015808105\n",
            "strain 22.892742156982422\n",
            "in vicreg  1.2334112077951431 21.58203125 0.13155275583267212\n",
            "strain 22.989965438842773\n",
            "in vicreg  1.1843150481581688 21.6796875 0.12145084142684937\n",
            "strain 22.93076515197754\n",
            "in vicreg  1.2266855686903 21.58203125 0.12538836896419525\n",
            "strain 22.977073669433594\n",
            "in vicreg  1.1714099906384945 21.6796875 0.1108776405453682\n",
            "strain 22.90728759765625\n",
            "in vicreg  1.0773103684186935 21.77734375 0.09375181794166565\n",
            "strain 22.921062469482422\n",
            "in vicreg  1.0804327204823494 21.77734375 0.09309818595647812\n",
            "strain 22.92353057861328\n",
            "in vicreg  1.2707596644759178 21.484375 0.12769055366516113\n",
            "strain 22.89845085144043\n",
            "classify 2.86962890625\n",
            "classify 2.916259765625\n",
            "classify 2.849609375\n",
            "classify 2.915771484375\n",
            "classify 2.9404296875\n",
            "classify 2.8974609375\n",
            "classify 2.911376953125\n",
            "classify 2.872314453125\n",
            "classify 2.848876953125\n",
            "0.078125\n",
            "0.078125\n",
            "0.0625\n",
            "0.109375\n",
            "in vicreg  1.2323427945375443 21.58203125 0.11924746632575989\n",
            "strain 22.976591110229492\n",
            "in vicreg  1.0296452790498734 21.875 0.09489985555410385\n",
            "strain 22.99954605102539\n",
            "in vicreg  1.139545813202858 21.6796875 0.10804381966590881\n",
            "strain 22.872589111328125\n",
            "in vicreg  1.094131451100111 21.6796875 0.09912557899951935\n",
            "strain 22.818256378173828\n",
            "in vicreg  0.9671125560998917 21.875 0.07849633693695068\n",
            "strain 22.920610427856445\n",
            "in vicreg  1.1864897795021534 21.6796875 0.11871717125177383\n",
            "strain 22.930206298828125\n",
            "in vicreg  1.268506981432438 21.484375 0.12811875343322754\n",
            "strain 22.896625518798828\n",
            "in vicreg  1.1689390987157822 21.6796875 0.10833168774843216\n",
            "strain 22.902271270751953\n",
            "in vicreg  1.1966262944042683 21.58203125 0.1228623017668724\n",
            "strain 22.944488525390625\n",
            "classify 2.881103515625\n",
            "classify 2.892578125\n",
            "classify 2.932861328125\n",
            "classify 2.86767578125\n",
            "classify 2.900390625\n",
            "classify 2.849609375\n",
            "classify 2.89404296875\n",
            "classify 2.891357421875\n",
            "classify 2.87451171875\n",
            "0.046875\n",
            "0.09375\n",
            "0.15625\n",
            "0.078125\n",
            "in vicreg  1.2090523727238178 21.58203125 0.12201981991529465\n",
            "strain 22.956073760986328\n",
            "in vicreg  1.154344156384468 21.6796875 0.11646970361471176\n",
            "strain 22.895814895629883\n",
            "in vicreg  1.0859984904527664 21.6796875 0.10389415919780731\n",
            "strain 22.814891815185547\n",
            "in vicreg  1.1376561596989632 21.6796875 0.12676627933979034\n",
            "strain 22.889423370361328\n",
            "in vicreg  1.0509870946407318 21.77734375 0.10677972435951233\n",
            "strain 22.907766342163086\n",
            "in vicreg  1.043473742902279 21.77734375 0.09577798843383789\n",
            "strain 22.889251708984375\n",
            "in vicreg  1.1140404269099236 21.6796875 0.10347776859998703\n",
            "strain 22.842517852783203\n",
            "in vicreg  1.143399253487587 21.6796875 0.10640865564346313\n",
            "strain 22.874807357788086\n",
            "in vicreg  1.2592130340635777 21.484375 0.12906740605831146\n",
            "strain 22.88827896118164\n",
            "classify 2.88623046875\n",
            "classify 2.84619140625\n",
            "classify 2.909423828125\n",
            "classify 2.869873046875\n",
            "classify 2.906494140625\n",
            "classify 2.9541015625\n",
            "classify 2.865966796875\n",
            "classify 2.855224609375\n",
            "classify 2.926513671875\n",
            "0.109375\n",
            "0.078125\n",
            "0.0625\n",
            "0.078125\n",
            "in vicreg  1.1212479323148727 21.6796875 0.10354427248239517\n",
            "strain 22.84979248046875\n",
            "in vicreg  1.1704964563250542 21.6796875 0.11349983513355255\n",
            "strain 22.90899658203125\n",
            "in vicreg  1.2881368398666382 21.484375 0.14261670410633087\n",
            "strain 22.930753707885742\n",
            "in vicreg  1.0932757519185543 21.6796875 0.10295254737138748\n",
            "strain 22.82122802734375\n",
            "in vicreg  1.16229597479105 21.58203125 0.12437291443347931\n",
            "strain 22.91166877746582\n",
            "in vicreg  1.1127403937280178 21.6796875 0.10263209789991379\n",
            "strain 22.84037208557129\n",
            "in vicreg  1.135225035250187 21.6796875 0.10677144676446915\n",
            "strain 22.86699676513672\n",
            "in vicreg  1.155021134763956 21.58203125 0.11273174732923508\n",
            "strain 22.89275360107422\n",
            "in vicreg  0.9918106719851494 21.875 0.09156032651662827\n",
            "strain 22.958370208740234\n",
            "classify 2.9208984375\n",
            "classify 2.906982421875\n",
            "classify 2.8671875\n",
            "classify 2.808349609375\n",
            "classify 2.92724609375\n",
            "classify 2.867919921875\n",
            "classify 2.9189453125\n",
            "classify 2.92529296875\n",
            "classify 2.886474609375\n",
            "0.078125\n",
            "0.0625\n",
            "0.078125\n",
            "0.109375\n",
            "in vicreg  1.141396164894104 21.58203125 0.13551993668079376\n",
            "strain 22.901914596557617\n",
            "in vicreg  1.014147326350212 21.875 0.10842268168926239\n",
            "strain 22.997570037841797\n",
            "in vicreg  1.1714612133800983 21.58203125 0.12164508551359177\n",
            "strain 22.918106079101562\n",
            "in vicreg  1.1266924440860748 21.6796875 0.10920878499746323\n",
            "strain 22.86090087890625\n",
            "in vicreg  1.0547270998358727 21.77734375 0.0941270962357521\n",
            "strain 22.898855209350586\n",
            "in vicreg  1.2088160961866379 21.484375 0.12635701894760132\n",
            "strain 22.835172653198242\n",
            "in vicreg  1.280947308987379 21.484375 0.1446416974067688\n",
            "strain 22.925588607788086\n",
            "in vicreg  1.1943783611059189 21.484375 0.11865317821502686\n",
            "strain 22.813030242919922\n",
            "in vicreg  1.0975152254104614 21.6796875 0.10402701795101166\n",
            "strain 22.826541900634766\n",
            "classify 2.87890625\n",
            "classify 2.88037109375\n",
            "classify 2.89111328125\n",
            "classify 2.875244140625\n",
            "classify 2.923583984375\n",
            "classify 2.898193359375\n",
            "classify 2.870849609375\n",
            "classify 2.856201171875\n",
            "classify 2.91015625\n",
            "0.140625\n",
            "0.109375\n",
            "0.0\n",
            "0.09375\n",
            "in vicreg  1.192847266793251 21.484375 0.11917782574892044\n",
            "strain 22.812026977539062\n",
            "in vicreg  1.116064004600048 21.6796875 0.11393636465072632\n",
            "strain 22.854999542236328\n",
            "in vicreg  1.1464672163128853 21.58203125 0.11124800890684128\n",
            "strain 22.882715225219727\n",
            "in vicreg  1.0498791933059692 21.77734375 0.10610682517290115\n",
            "strain 22.905986785888672\n",
            "in vicreg  1.0640360414981842 21.6796875 0.10769951343536377\n",
            "strain 22.796735763549805\n",
            "in vicreg  1.0059170424938202 21.77734375 0.10060518234968185\n",
            "strain 22.856521606445312\n",
            "in vicreg  1.1992000974714756 21.484375 0.13211074471473694\n",
            "strain 22.831310272216797\n",
            "in vicreg  1.133938692510128 21.58203125 0.117769256234169\n",
            "strain 22.876707077026367\n",
            "in vicreg  1.1935200542211533 21.484375 0.11983492970466614\n",
            "strain 22.8133544921875\n",
            "classify 2.829345703125\n",
            "classify 2.89306640625\n",
            "classify 2.8740234375\n",
            "classify 2.86669921875\n",
            "classify 2.92529296875\n",
            "classify 2.8544921875\n",
            "classify 2.909423828125\n",
            "classify 2.89599609375\n",
            "classify 2.930908203125\n",
            "0.109375\n",
            "0.09375\n",
            "0.078125\n",
            "0.09375\n",
            "in vicreg  1.142734382301569 21.58203125 0.1246437206864357\n",
            "strain 22.892377853393555\n",
            "in vicreg  0.9998561814427376 21.77734375 0.0902661383152008\n",
            "strain 22.84012222290039\n",
            "in vicreg  1.1466365307569504 21.58203125 0.11355429142713547\n",
            "strain 22.885190963745117\n",
            "in vicreg  1.189303770661354 21.484375 0.12263783067464828\n",
            "strain 22.81194305419922\n",
            "in vicreg  1.2448637746274471 21.484375 0.1281241774559021\n",
            "strain 22.872987747192383\n",
            "in vicreg  1.2132646515965462 21.484375 0.12454067915678024\n",
            "strain 22.837804794311523\n",
            "in vicreg  0.9707359597086906 21.875 0.08973270654678345\n",
            "strain 22.935468673706055\n",
            "in vicreg  1.1869989335536957 21.484375 0.12403689324855804\n",
            "strain 22.81103515625\n",
            "in vicreg  1.2095935642719269 21.484375 0.1474098563194275\n",
            "strain 22.85700225830078\n",
            "classify 2.88232421875\n",
            "classify 2.907958984375\n",
            "classify 2.890869140625\n",
            "classify 2.885498046875\n",
            "classify 2.902099609375\n",
            "classify 2.847900390625\n",
            "classify 2.8388671875\n",
            "classify 2.947998046875\n",
            "classify 2.849853515625\n",
            "0.015625\n",
            "0.09375\n",
            "0.125\n",
            "0.109375\n",
            "in vicreg  0.999758392572403 21.77734375 0.08945968747138977\n",
            "strain 22.839218139648438\n",
            "in vicreg  1.088141556829214 21.6796875 0.10560932755470276\n",
            "strain 22.81875228881836\n",
            "in vicreg  1.0460229590535164 21.6796875 0.101290263235569\n",
            "strain 22.77231216430664\n",
            "in vicreg  1.1068018153309822 21.6796875 0.11571022868156433\n",
            "strain 22.847511291503906\n",
            "in vicreg  1.2750526890158653 21.38671875 0.14647634327411652\n",
            "strain 22.79652976989746\n",
            "in vicreg  1.2032534927129745 21.484375 0.13025367259979248\n",
            "strain 22.833505630493164\n",
            "in vicreg  1.1523308232426643 21.484375 0.12643086910247803\n",
            "strain 22.77876091003418\n",
            "in vicreg  1.1952858418226242 21.484375 0.11971139162778854\n",
            "strain 22.81499671936035\n",
            "in vicreg  1.1554669588804245 21.484375 0.11936033517122269\n",
            "strain 22.774826049804688\n",
            "classify 2.914306640625\n",
            "classify 2.90771484375\n",
            "classify 2.837890625\n",
            "classify 2.9130859375\n",
            "classify 2.94189453125\n",
            "classify 2.862060546875\n",
            "classify 2.852294921875\n",
            "classify 2.85888671875\n",
            "classify 2.931640625\n",
            "0.078125\n",
            "0.09375\n",
            "0.125\n",
            "0.078125\n",
            "in vicreg  1.198283676058054 21.484375 0.12421136349439621\n",
            "strain 22.82249641418457\n",
            "in vicreg  1.0225776582956314 21.6796875 0.10059022158384323\n",
            "strain 22.748167037963867\n",
            "in vicreg  0.9690083563327789 21.77734375 0.08582410216331482\n",
            "strain 22.804832458496094\n",
            "in vicreg  1.0794258676469326 21.6796875 0.11267626285552979\n",
            "strain 22.817102432250977\n",
            "in vicreg  1.1329688131809235 21.484375 0.12377922236919403\n",
            "strain 22.75674819946289\n",
            "in vicreg  1.0897133499383926 21.484375 0.11779510229825974\n",
            "strain 22.707509994506836\n",
            "in vicreg  1.0647788643836975 21.6796875 0.11300156265497208\n",
            "strain 22.802780151367188\n",
            "in vicreg  1.0921020992100239 21.6796875 0.12231988459825516\n",
            "strain 22.83942222595215\n",
            "in vicreg  1.187598891556263 21.484375 0.13653317093849182\n",
            "strain 22.824132919311523\n",
            "classify 2.83544921875\n",
            "classify 2.87451171875\n",
            "classify 2.91162109375\n",
            "classify 2.92724609375\n",
            "classify 2.843017578125\n",
            "classify 2.88232421875\n",
            "classify 2.9306640625\n",
            "classify 2.860595703125\n",
            "classify 2.90673828125\n",
            "0.0625\n",
            "0.078125\n",
            "0.0625\n",
            "0.125\n",
            "in vicreg  1.3010666705667973 21.2890625 0.15315783023834229\n",
            "strain 22.704225540161133\n",
            "in vicreg  1.1377496644854546 21.58203125 0.11066287010908127\n",
            "strain 22.873411178588867\n",
            "in vicreg  1.1773292906582355 21.484375 0.12546947598457336\n",
            "strain 22.802799224853516\n",
            "in vicreg  1.062467321753502 21.6796875 0.1014494001865387\n",
            "strain 22.788917541503906\n",
            "in vicreg  1.1601335369050503 21.484375 0.12029433995485306\n",
            "strain 22.780427932739258\n",
            "in vicreg  1.0770099237561226 21.58203125 0.10890943557024002\n",
            "strain 22.8109188079834\n",
            "in vicreg  1.1055221781134605 21.484375 0.11016083508729935\n",
            "strain 22.715682983398438\n",
            "in vicreg  1.1399485170841217 21.484375 0.13310793042182922\n",
            "strain 22.773056030273438\n",
            "in vicreg  1.0778607800602913 21.6796875 0.10766138881444931\n",
            "strain 22.810523986816406\n",
            "classify 2.885986328125\n",
            "classify 2.850830078125\n",
            "classify 2.925048828125\n",
            "classify 2.884521484375\n",
            "classify 2.8916015625\n",
            "classify 2.870361328125\n",
            "classify 2.9091796875\n",
            "classify 2.870849609375\n",
            "classify 2.91064453125\n",
            "0.078125\n",
            "0.0625\n",
            "0.109375\n",
            "0.0625\n",
            "in vicreg  1.1110637336969376 21.484375 0.12848173081874847\n",
            "strain 22.739543914794922\n",
            "in vicreg  1.171772088855505 21.484375 0.12570463120937347\n",
            "strain 22.797475814819336\n",
            "in vicreg  1.198878139257431 21.38671875 0.14220814406871796\n",
            "strain 22.71608543395996\n",
            "in vicreg  1.0108251124620438 21.6796875 0.10077068954706192\n",
            "strain 22.736597061157227\n",
            "in vicreg  1.0268221609294415 21.6796875 0.1024973914027214\n",
            "strain 22.75432014465332\n",
            "in vicreg  1.0905724950134754 21.484375 0.11608358472585678\n",
            "strain 22.706655502319336\n",
            "in vicreg  1.0553345084190369 21.6796875 0.10027022659778595\n",
            "strain 22.780603408813477\n",
            "in vicreg  1.0119713842868805 21.6796875 0.10788219422101974\n",
            "strain 22.74485206604004\n",
            "in vicreg  1.4212917536497116 21.09375 0.1779915690422058\n",
            "strain 22.72428321838379\n",
            "classify 2.9033203125\n",
            "classify 2.85546875\n",
            "classify 2.891845703125\n",
            "classify 2.91748046875\n",
            "classify 2.898193359375\n",
            "classify 2.875\n",
            "classify 2.90380859375\n",
            "classify 2.8798828125\n",
            "classify 2.863525390625\n",
            "0.078125\n",
            "0.125\n",
            "0.046875\n",
            "0.109375\n",
            "in vicreg  1.3048220425844193 21.2890625 0.16019949316978455\n",
            "strain 22.715023040771484\n",
            "in vicreg  1.2954646721482277 21.19140625 0.15724748373031616\n",
            "strain 22.702713012695312\n",
            "in vicreg  1.0108523070812225 21.6796875 0.09710489958524704\n",
            "strain 22.73295783996582\n",
            "in vicreg  0.9239817969501019 21.77734375 0.08641473948955536\n",
            "strain 22.76039695739746\n",
            "in vicreg  1.0297543369233608 21.484375 0.10849202424287796\n",
            "strain 22.638246536254883\n",
            "in vicreg  0.99197868257761 21.6796875 0.09434686601161957\n",
            "strain 22.711326599121094\n",
            "in vicreg  1.0932758450508118 21.484375 0.12864793837070465\n",
            "strain 22.721923828125\n",
            "in vicreg  0.9933805093169212 21.6796875 0.09769905358552933\n",
            "strain 22.71607780456543\n",
            "in vicreg  1.0125825181603432 21.6796875 0.10816449671983719\n",
            "strain 22.745746612548828\n",
            "classify 2.87451171875\n",
            "classify 2.89892578125\n",
            "classify 2.917236328125\n",
            "classify 2.888671875\n",
            "classify 2.879150390625\n",
            "classify 2.886474609375\n",
            "classify 2.855224609375\n",
            "classify 2.92333984375\n",
            "classify 2.894775390625\n",
            "0.078125\n",
            "0.078125\n",
            "0.078125\n",
            "0.0625\n",
            "in vicreg  1.20603758841753 21.38671875 0.14349378645420074\n",
            "strain 22.724531173706055\n",
            "in vicreg  1.2134108692407608 21.38671875 0.13712048530578613\n",
            "strain 22.72553253173828\n",
            "in vicreg  1.2103250250220299 21.38671875 0.13427066802978516\n",
            "strain 22.71959686279297\n",
            "in vicreg  1.089657749980688 21.58203125 0.12235458940267563\n",
            "strain 22.837011337280273\n",
            "in vicreg  1.4331961050629616 21.09375 0.19926197826862335\n",
            "strain 22.75745964050293\n",
            "in vicreg  1.225382462143898 21.38671875 0.13027402758598328\n",
            "strain 22.730655670166016\n",
            "in vicreg  1.0889435186982155 21.484375 0.11409728229045868\n",
            "strain 22.703041076660156\n",
            "in vicreg  0.9463129565119743 21.6796875 0.09290163218975067\n",
            "strain 22.664215087890625\n",
            "in vicreg  1.1042457073926926 21.484375 0.11401522904634476\n",
            "strain 22.71826171875\n",
            "classify 2.867431640625\n",
            "classify 2.8427734375\n",
            "classify 2.885009765625\n",
            "classify 2.853271484375\n",
            "classify 2.87060546875\n",
            "classify 2.91015625\n",
            "classify 2.880859375\n",
            "classify 2.897216796875\n",
            "classify 2.949462890625\n",
            "0.015625\n",
            "0.109375\n",
            "0.078125\n",
            "0.140625\n",
            "in vicreg  0.9275344200432301 21.6796875 0.0815097764134407\n",
            "strain 22.634044647216797\n",
            "in vicreg  0.9997167624533176 21.6796875 0.10401175916194916\n",
            "strain 22.728729248046875\n",
            "in vicreg  1.021641120314598 21.6796875 0.11595482379198074\n",
            "strain 22.762596130371094\n",
            "in vicreg  1.0789974592626095 21.484375 0.14278201758861542\n",
            "strain 22.721778869628906\n",
            "in vicreg  1.165906898677349 21.38671875 0.13332372903823853\n",
            "strain 22.674230575561523\n",
            "in vicreg  1.2109233066439629 21.2890625 0.15845490992069244\n",
            "strain 22.6193790435791\n",
            "in vicreg  1.1594685725867748 21.38671875 0.14177565276622772\n",
            "strain 22.67624282836914\n",
            "in vicreg  1.0987574234604836 21.484375 0.11660601943731308\n",
            "strain 22.715362548828125\n",
            "in vicreg  1.0972900316119194 21.484375 0.11283069103956223\n",
            "strain 22.710121154785156\n",
            "classify 2.838623046875\n",
            "classify 2.8583984375\n",
            "classify 2.873291015625\n",
            "classify 2.85986328125\n",
            "classify 2.837158203125\n",
            "classify 2.90234375\n",
            "classify 2.927978515625\n",
            "classify 2.893310546875\n",
            "classify 2.91845703125\n",
            "0.125\n",
            "0.0625\n",
            "0.125\n",
            "0.0625\n",
            "in vicreg  1.1978648602962494 21.2890625 0.1305573284626007\n",
            "strain 22.57842254638672\n",
            "in vicreg  1.0822925716638565 21.484375 0.12739340960979462\n",
            "strain 22.709686279296875\n",
            "in vicreg  0.9658274240791798 21.6796875 0.10396222770214081\n",
            "strain 22.69478988647461\n",
            "in vicreg  1.179317943751812 21.2890625 0.128945991396904\n",
            "strain 22.558263778686523\n",
            "in vicreg  1.1296136304736137 21.484375 0.11936784535646439\n",
            "strain 22.748981475830078\n",
            "in vicreg  1.0983634740114212 21.484375 0.1271757036447525\n",
            "strain 22.725540161132812\n",
            "in vicreg  1.0716228745877743 21.484375 0.10874948650598526\n",
            "strain 22.68037223815918\n",
            "in vicreg  1.234286092221737 21.2890625 0.14670130610466003\n",
            "strain 22.6309871673584\n",
            "in vicreg  1.0801779106259346 21.484375 0.11058039218187332\n",
            "strain 22.690757751464844\n",
            "classify 2.87646484375\n",
            "classify 2.8818359375\n",
            "classify 2.8798828125\n",
            "classify 2.86328125\n",
            "classify 2.874755859375\n",
            "classify 2.9130859375\n",
            "classify 2.883056640625\n",
            "classify 2.913818359375\n",
            "classify 2.8740234375\n",
            "0.0625\n",
            "0.09375\n",
            "0.046875\n",
            "0.140625\n",
            "in vicreg  1.1282209306955338 21.38671875 0.12885332107543945\n",
            "strain 22.6320743560791\n",
            "in vicreg  1.0524822399020195 21.58203125 0.11225399374961853\n",
            "strain 22.789735794067383\n",
            "in vicreg  1.2351449579000473 21.2890625 0.15239280462265015\n",
            "strain 22.63753890991211\n",
            "in vicreg  1.095549575984478 21.484375 0.13459548354148865\n",
            "strain 22.730146408081055\n",
            "in vicreg  1.0856609791517258 21.484375 0.11971542984247208\n",
            "strain 22.70537567138672\n",
            "in vicreg  1.0327177122235298 21.484375 0.12351532280445099\n",
            "strain 22.656234741210938\n",
            "in vicreg  1.0716333985328674 21.484375 0.11978381127119064\n",
            "strain 22.691415786743164\n",
            "in vicreg  1.1471215635538101 21.38671875 0.12652349472045898\n",
            "strain 22.648645401000977\n",
            "in vicreg  1.0444803163409233 21.484375 0.11040791869163513\n",
            "strain 22.654888153076172\n",
            "classify 2.914794921875\n",
            "classify 2.91064453125\n",
            "classify 2.915771484375\n",
            "classify 2.83251953125\n",
            "classify 2.892333984375\n",
            "classify 2.8740234375\n",
            "classify 2.857177734375\n",
            "classify 2.888671875\n",
            "classify 2.887939453125\n",
            "0.078125\n",
            "0.109375\n",
            "0.09375\n",
            "0.125\n",
            "in vicreg  1.2191912159323692 21.2890625 0.13984960317611694\n",
            "strain 22.609039306640625\n",
            "in vicreg  1.0308988392353058 21.484375 0.10214149206876755\n",
            "strain 22.633041381835938\n",
            "in vicreg  0.9698170237243176 21.6796875 0.10434621572494507\n",
            "strain 22.699161529541016\n",
            "in vicreg  1.2802507728338242 21.19140625 0.1514153927564621\n",
            "strain 22.681665420532227\n",
            "in vicreg  1.2631135061383247 21.2890625 0.14896254241466522\n",
            "strain 22.66207504272461\n",
            "in vicreg  1.2120039202272892 21.2890625 0.15550149977207184\n",
            "strain 22.61750602722168\n",
            "in vicreg  1.172446832060814 21.2890625 0.1433316022157669\n",
            "strain 22.565778732299805\n",
            "in vicreg  1.0685110464692116 21.484375 0.11618230491876602\n",
            "strain 22.684694290161133\n",
            "in vicreg  0.9677035734057426 21.484375 0.12726280093193054\n",
            "strain 22.5949649810791\n",
            "classify 2.841796875\n",
            "classify 2.89013671875\n",
            "classify 2.887451171875\n",
            "classify 2.890625\n",
            "classify 2.90234375\n",
            "classify 2.891845703125\n",
            "classify 2.874755859375\n",
            "classify 2.933837890625\n",
            "classify 2.85009765625\n",
            "0.140625\n",
            "0.046875\n",
            "0.078125\n",
            "0.109375\n",
            "in vicreg  0.903401430696249 21.6796875 0.09208662062883377\n",
            "strain 22.6204891204834\n",
            "in vicreg  1.0625321418046951 21.38671875 0.14280252158641815\n",
            "strain 22.58033561706543\n",
            "in vicreg  1.0441792197525501 21.484375 0.11335170269012451\n",
            "strain 22.65753173828125\n",
            "in vicreg  1.051354594528675 21.484375 0.11124524474143982\n",
            "strain 22.662601470947266\n",
            "in vicreg  1.1352816596627235 21.2890625 0.15163446962833405\n",
            "strain 22.536916732788086\n",
            "in vicreg  1.0873664170503616 21.484375 0.11857970058917999\n",
            "strain 22.70594596862793\n",
            "in vicreg  1.2287894263863564 21.19140625 0.15017931163311005\n",
            "strain 22.628969192504883\n",
            "in vicreg  1.0577866807579994 21.38671875 0.12017498910427094\n",
            "strain 22.552961349487305\n",
            "in vicreg  1.012661773711443 21.484375 0.10196854174137115\n",
            "strain 22.6146297454834\n",
            "classify 2.87646484375\n",
            "classify 2.891357421875\n",
            "classify 2.89599609375\n",
            "classify 2.88330078125\n",
            "classify 2.886474609375\n",
            "classify 2.868896484375\n",
            "classify 2.943603515625\n",
            "classify 2.907470703125\n",
            "classify 2.837646484375\n",
            "0.0625\n",
            "0.015625\n",
            "0.171875\n",
            "0.109375\n",
            "in vicreg  1.100592501461506 21.38671875 0.12276878952980042\n",
            "strain 22.598360061645508\n",
            "in vicreg  1.2325098738074303 21.19140625 0.15173391997814178\n",
            "strain 22.63424301147461\n",
            "in vicreg  1.2191202491521835 21.19140625 0.1646617352962494\n",
            "strain 22.63378143310547\n",
            "in vicreg  1.1872466653585434 21.2890625 0.13245725631713867\n",
            "strain 22.569704055786133\n",
            "in vicreg  1.074003055691719 21.484375 0.11067935079336166\n",
            "strain 22.684682846069336\n",
            "in vicreg  1.0928813368082047 21.38671875 0.1191144734621048\n",
            "strain 22.58699607849121\n",
            "in vicreg  1.2033892795443535 21.19140625 0.1406765878200531\n",
            "strain 22.594066619873047\n",
            "in vicreg  1.0886465199291706 21.2890625 0.12082105875015259\n",
            "strain 22.4594669342041\n",
            "in vicreg  0.9799527935683727 21.484375 0.11240600049495697\n",
            "strain 22.59235954284668\n",
            "classify 2.897216796875\n",
            "classify 2.884033203125\n",
            "classify 2.93212890625\n",
            "classify 2.853515625\n",
            "classify 2.888916015625\n",
            "classify 2.892578125\n",
            "classify 2.837158203125\n",
            "classify 2.832275390625\n",
            "classify 2.92431640625\n",
            "0.109375\n",
            "0.09375\n",
            "0.078125\n",
            "0.125\n",
            "in vicreg  0.9908120147883892 21.484375 0.11695937067270279\n",
            "strain 22.607770919799805\n",
            "in vicreg  0.9973637759685516 21.484375 0.12271983176469803\n",
            "strain 22.620084762573242\n",
            "in vicreg  1.0762238875031471 21.38671875 0.14662586152553558\n",
            "strain 22.597848892211914\n",
            "in vicreg  1.1385421268641949 21.2890625 0.1434677392244339\n",
            "strain 22.53200912475586\n",
            "in vicreg  1.1885618790984154 21.09375 0.1667007952928543\n",
            "strain 22.480262756347656\n",
            "in vicreg  1.045356411486864 21.484375 0.1085503101348877\n",
            "strain 22.653907775878906\n",
            "in vicreg  1.196567341685295 21.19140625 0.14358530938625336\n",
            "strain 22.590152740478516\n",
            "in vicreg  1.122898980975151 21.2890625 0.12722347676753998\n",
            "strain 22.5001220703125\n",
            "in vicreg  1.1671073734760284 21.2890625 0.14613285660743713\n",
            "strain 22.56324005126953\n",
            "classify 2.858642578125\n",
            "classify 2.84228515625\n",
            "classify 2.882080078125\n",
            "classify 2.854736328125\n",
            "classify 2.8974609375\n",
            "classify 2.911865234375\n",
            "classify 2.939697265625\n",
            "classify 2.86865234375\n",
            "classify 2.86279296875\n",
            "0.0625\n",
            "0.09375\n",
            "0.09375\n",
            "0.125\n",
            "in vicreg  1.0367476381361485 21.484375 0.11330702900886536\n",
            "strain 22.650054931640625\n",
            "in vicreg  0.9668098762631416 21.484375 0.11040843278169632\n",
            "strain 22.577219009399414\n",
            "in vicreg  1.0550889186561108 21.38671875 0.1105831116437912\n",
            "strain 22.54067039489746\n",
            "in vicreg  1.0629132390022278 21.38671875 0.11798862367868423\n",
            "strain 22.5559024810791\n",
            "in vicreg  1.1337325908243656 21.2890625 0.14086154103279114\n",
            "strain 22.524593353271484\n",
            "in vicreg  1.1483043432235718 21.19140625 0.15587067604064941\n",
            "strain 22.554174423217773\n",
            "in vicreg  1.083268504589796 21.2890625 0.12239627540111542\n",
            "strain 22.455665588378906\n",
            "in vicreg  1.272955909371376 21.09375 0.16998323798179626\n",
            "strain 22.56793975830078\n",
            "in vicreg  1.0882055386900902 21.2890625 0.12066024541854858\n",
            "strain 22.458866119384766\n",
            "classify 2.9306640625\n",
            "classify 2.855224609375\n",
            "classify 2.822509765625\n",
            "classify 2.910888671875\n",
            "classify 2.866455078125\n",
            "classify 2.842041015625\n",
            "classify 2.8759765625\n",
            "classify 2.882568359375\n",
            "classify 2.892333984375\n",
            "0.125\n",
            "0.03125\n",
            "0.125\n",
            "0.109375\n",
            "in vicreg  1.1899221688508987 21.09375 0.14263860881328583\n",
            "strain 22.457561492919922\n",
            "in vicreg  0.9943823330104351 21.484375 0.1115395575761795\n",
            "strain 22.60592269897461\n",
            "in vicreg  0.9993689134716988 21.484375 0.10702289640903473\n",
            "strain 22.60639190673828\n",
            "in vicreg  1.0735738091170788 21.2890625 0.132928267121315\n",
            "strain 22.45650291442871\n",
            "in vicreg  1.0346585884690285 21.38671875 0.12901677191257477\n",
            "strain 22.53867530822754\n",
            "in vicreg  1.113082841038704 21.2890625 0.12437277287244797\n",
            "strain 22.487455368041992\n",
            "in vicreg  1.118042878806591 21.2890625 0.14436128735542297\n",
            "strain 22.51240348815918\n",
            "in vicreg  1.122654601931572 21.2890625 0.13322433829307556\n",
            "strain 22.505878448486328\n",
            "in vicreg  1.1563452892005444 21.19140625 0.14546047151088715\n",
            "strain 22.55180549621582\n",
            "classify 2.875\n",
            "classify 2.9287109375\n",
            "classify 2.91015625\n",
            "classify 2.825927734375\n",
            "classify 2.876953125\n",
            "classify 2.853271484375\n",
            "classify 2.91064453125\n",
            "classify 2.841796875\n",
            "classify 2.88134765625\n",
            "0.09375\n",
            "0.125\n",
            "0.015625\n",
            "0.125\n",
            "in vicreg  1.1373424902558327 21.19140625 0.14199291169643402\n",
            "strain 22.529335021972656\n",
            "in vicreg  1.083544548600912 21.2890625 0.1296841949224472\n",
            "strain 22.463228225708008\n",
            "in vicreg  1.1883119121193886 21.19140625 0.14106738567352295\n",
            "strain 22.57938003540039\n",
            "in vicreg  1.014730241149664 21.38671875 0.11017508804798126\n",
            "strain 22.49990463256836\n",
            "in vicreg  1.1203542351722717 21.19140625 0.14936110377311707\n",
            "strain 22.51971435546875\n",
            "in vicreg  1.0407079011201859 21.38671875 0.11039247363805771\n",
            "strain 22.526100158691406\n",
            "in vicreg  1.1020082980394363 21.2890625 0.13001221418380737\n",
            "strain 22.48202133178711\n",
            "in vicreg  1.0953737422823906 21.2890625 0.13448891043663025\n",
            "strain 22.479862213134766\n",
            "in vicreg  1.0575411841273308 21.2890625 0.12559053301811218\n",
            "strain 22.43313217163086\n",
            "classify 2.86376953125\n",
            "classify 2.88671875\n",
            "classify 2.867919921875\n",
            "classify 2.85009765625\n",
            "classify 2.883056640625\n",
            "classify 2.909423828125\n",
            "classify 2.87109375\n",
            "classify 2.96630859375\n",
            "classify 2.806884765625\n",
            "0.125\n",
            "0.046875\n",
            "0.125\n",
            "0.078125\n",
            "in vicreg  1.1189323849976063 21.2890625 0.13086523115634918\n",
            "strain 22.499797821044922\n",
            "in vicreg  1.1192088946700096 21.19140625 0.13302360475063324\n",
            "strain 22.502233505249023\n",
            "in vicreg  1.181260496377945 21.09375 0.17059914767742157\n",
            "strain 22.47686004638672\n",
            "in vicreg  1.1000173166394234 21.2890625 0.14641046524047852\n",
            "strain 22.496427536010742\n",
            "in vicreg  1.030480582267046 21.2890625 0.13040059804916382\n",
            "strain 22.41088104248047\n",
            "in vicreg  1.0412261821329594 21.2890625 0.12161314487457275\n",
            "strain 22.412837982177734\n",
            "in vicreg  1.0143162682652473 21.2890625 0.10992825031280518\n",
            "strain 22.374244689941406\n",
            "in vicreg  1.1630600318312645 21.19140625 0.13780388236045837\n",
            "strain 22.55086326599121\n",
            "in vicreg  1.1287711560726166 21.19140625 0.13971233367919922\n",
            "strain 22.518482208251953\n",
            "classify 2.89599609375\n",
            "classify 2.88427734375\n",
            "classify 2.884765625\n",
            "classify 2.85888671875\n",
            "classify 2.906982421875\n",
            "classify 2.847412109375\n",
            "classify 2.863525390625\n",
            "classify 2.872802734375\n",
            "classify 2.8837890625\n",
            "0.109375\n",
            "0.0625\n",
            "0.0625\n",
            "0.125\n",
            "in vicreg  0.963187962770462 21.484375 0.10025295615196228\n",
            "strain 22.563440322875977\n",
            "in vicreg  1.1264709755778313 21.19140625 0.13900165259838104\n",
            "strain 22.515472412109375\n",
            "in vicreg  1.2514781206846237 20.8984375 0.17127472162246704\n",
            "strain 22.297752380371094\n",
            "in vicreg  1.028976682573557 21.2890625 0.11862662434577942\n",
            "strain 22.39760398864746\n",
            "in vicreg  1.2100592255592346 21.09375 0.1682964563369751\n",
            "strain 22.50335693359375\n",
            "in vicreg  1.0674066841602325 21.2890625 0.1402764469385147\n",
            "strain 22.45768165588379\n",
            "in vicreg  1.1135808192193508 21.09375 0.14175504446029663\n",
            "strain 22.380334854125977\n",
            "in vicreg  1.0698449797928333 21.2890625 0.1260221004486084\n",
            "strain 22.44586753845215\n",
            "in vicreg  1.032636035233736 21.2890625 0.13922134041786194\n",
            "strain 22.421857833862305\n",
            "classify 2.87158203125\n",
            "classify 2.8984375\n",
            "classify 2.8544921875\n",
            "classify 2.9208984375\n",
            "classify 2.8818359375\n",
            "classify 2.884521484375\n",
            "classify 2.900146484375\n",
            "classify 2.89697265625\n",
            "classify 2.86328125\n",
            "0.078125\n",
            "0.078125\n",
            "0.09375\n",
            "0.09375\n",
            "in vicreg  1.065976358950138 21.19140625 0.13402582705020905\n",
            "strain 22.450002670288086\n",
            "in vicreg  0.9181722067296505 21.38671875 0.10673688352108002\n",
            "strain 22.39990997314453\n",
            "in vicreg  1.0943826287984848 21.09375 0.1392720341682434\n",
            "strain 22.35865592956543\n",
            "in vicreg  0.9752068668603897 21.38671875 0.1281593292951584\n",
            "strain 22.478364944458008\n",
            "in vicreg  1.0601449757814407 21.2890625 0.12582042813301086\n",
            "strain 22.435964584350586\n",
            "in vicreg  1.1506767012178898 21.09375 0.14717796444892883\n",
            "strain 22.422855377197266\n",
            "in vicreg  1.3113753870129585 20.8984375 0.17545221745967865\n",
            "strain 22.361825942993164\n",
            "in vicreg  1.1155260726809502 21.19140625 0.12998294830322266\n",
            "strain 22.49551010131836\n",
            "in vicreg  1.1702490970492363 21.09375 0.15925362706184387\n",
            "strain 22.454504013061523\n",
            "classify 2.874267578125\n",
            "classify 2.834228515625\n",
            "classify 2.881103515625\n",
            "classify 2.8544921875\n",
            "classify 2.912109375\n",
            "classify 2.84814453125\n",
            "classify 2.884033203125\n",
            "classify 2.90478515625\n",
            "classify 2.906982421875\n",
            "0.109375\n",
            "0.0625\n",
            "0.078125\n",
            "0.140625\n",
            "in vicreg  1.0907838121056557 21.09375 0.13986967504024506\n",
            "strain 22.355653762817383\n",
            "in vicreg  1.0160413570702076 21.2890625 0.11989729106426239\n",
            "strain 22.38593864440918\n",
            "in vicreg  1.0460923425853252 21.19140625 0.120000921189785\n",
            "strain 22.416093826293945\n",
            "in vicreg  0.8976440876722336 21.58203125 0.08867309987545013\n",
            "strain 22.611316680908203\n",
            "in vicreg  1.310126855969429 20.80078125 0.20112822949886322\n",
            "strain 22.261255264282227\n",
            "in vicreg  1.0654866695404053 21.2890625 0.15355952084064484\n",
            "strain 22.469045639038086\n",
            "in vicreg  1.1192312464118004 21.09375 0.14719994366168976\n",
            "strain 22.39143180847168\n",
            "in vicreg  0.8994461037218571 21.484375 0.10628695785999298\n",
            "strain 22.505733489990234\n",
            "in vicreg  1.0166076011955738 21.2890625 0.13299207389354706\n",
            "strain 22.399599075317383\n",
            "classify 2.901123046875\n",
            "classify 2.916748046875\n",
            "classify 2.91259765625\n",
            "classify 2.841552734375\n",
            "classify 2.87890625\n",
            "classify 2.84326171875\n",
            "classify 2.826416015625\n",
            "classify 2.8759765625\n",
            "classify 2.8525390625\n",
            "0.09375\n",
            "0.125\n",
            "0.0625\n",
            "0.09375\n",
            "in vicreg  1.054859720170498 21.19140625 0.13444741070270538\n",
            "strain 22.439306259155273\n",
            "in vicreg  1.042596623301506 21.2890625 0.12638041377067566\n",
            "strain 22.418977737426758\n",
            "in vicreg  1.1726032942533493 21.09375 0.1569083333015442\n",
            "strain 22.454511642456055\n",
            "in vicreg  1.1188317090272903 21.09375 0.15421843528747559\n",
            "strain 22.39805030822754\n",
            "in vicreg  1.140647754073143 21.09375 0.14484165608882904\n",
            "strain 22.410490036010742\n",
            "in vicreg  1.101146824657917 21.09375 0.14007942378520966\n",
            "strain 22.366226196289062\n",
            "in vicreg  1.0012653656303883 21.2890625 0.11422564834356308\n",
            "strain 22.365489959716797\n",
            "in vicreg  1.1096157133579254 21.09375 0.15711642801761627\n",
            "strain 22.39173126220703\n",
            "in vicreg  0.9462441317737103 21.38671875 0.11378451436758041\n",
            "strain 22.435028076171875\n",
            "classify 2.906494140625\n",
            "classify 2.90185546875\n",
            "classify 2.856201171875\n",
            "classify 2.908203125\n",
            "classify 2.840087890625\n",
            "classify 2.843505859375\n",
            "classify 2.89306640625\n",
            "classify 2.888427734375\n",
            "classify 2.854248046875\n",
            "0.140625\n",
            "0.09375\n",
            "0.046875\n",
            "0.109375\n",
            "in vicreg  1.0995790362358093 21.09375 0.14572332799434662\n",
            "strain 22.370302200317383\n",
            "in vicreg  1.067287102341652 21.19140625 0.129263773560524\n",
            "strain 22.446550369262695\n",
            "in vicreg  1.212619710713625 20.8984375 0.17140759527683258\n",
            "strain 22.2590274810791\n",
            "in vicreg  1.2180634774267673 20.8984375 0.17030249536037445\n",
            "strain 22.26336669921875\n",
            "in vicreg  1.0326112620532513 21.2890625 0.12129104882478714\n",
            "strain 22.403902053833008\n",
            "in vicreg  1.0253380984067917 21.2890625 0.12203142791986465\n",
            "strain 22.397369384765625\n",
            "in vicreg  1.058357022702694 21.19140625 0.13831976056098938\n",
            "strain 22.44667625427246\n",
            "in vicreg  1.0120459832251072 21.2890625 0.12705084681510925\n",
            "strain 22.389097213745117\n",
            "in vicreg  1.098563987761736 21.09375 0.15072761476039886\n",
            "strain 22.374292373657227\n",
            "classify 2.845458984375\n",
            "classify 2.86376953125\n",
            "classify 2.85888671875\n",
            "classify 2.8837890625\n",
            "classify 2.876220703125\n",
            "classify 2.849365234375\n",
            "classify 2.882080078125\n",
            "classify 2.934814453125\n",
            "classify 2.87060546875\n",
            "0.03125\n",
            "0.140625\n",
            "0.109375\n",
            "0.109375\n",
            "in vicreg  1.01456418633461 21.2890625 0.12414351105690002\n",
            "strain 22.388708114624023\n",
            "in vicreg  1.1799484491348267 20.8984375 0.16577410697937012\n",
            "strain 22.220722198486328\n",
            "in vicreg  1.0763952508568764 21.09375 0.14369694888591766\n",
            "strain 22.3450927734375\n",
            "in vicreg  0.9698318317532539 21.2890625 0.12620922923088074\n",
            "strain 22.346040725708008\n",
            "in vicreg  1.1440993286669254 20.99609375 0.1537802666425705\n",
            "strain 22.297880172729492\n",
            "in vicreg  1.1533564887940884 20.8984375 0.17296943068504333\n",
            "strain 22.201326370239258\n",
            "in vicreg  1.0139526799321175 21.2890625 0.13303010165691376\n",
            "strain 22.396982192993164\n",
            "in vicreg  1.010134257376194 21.2890625 0.11532270908355713\n",
            "strain 22.375455856323242\n",
            "in vicreg  1.09783373773098 21.09375 0.1489904224872589\n",
            "strain 22.371824264526367\n",
            "classify 2.885498046875\n",
            "classify 2.819580078125\n",
            "classify 2.89306640625\n",
            "classify 2.893798828125\n",
            "classify 2.849609375\n",
            "classify 2.90087890625\n",
            "classify 2.825927734375\n",
            "classify 2.843994140625\n",
            "classify 2.941650390625\n",
            "0.0625\n",
            "0.109375\n",
            "0.078125\n",
            "0.125\n",
            "in vicreg  1.04811554774642 21.09375 0.14017444849014282\n",
            "strain 22.313289642333984\n",
            "in vicreg  1.0312845930457115 21.09375 0.13412293791770935\n",
            "strain 22.290407180786133\n",
            "in vicreg  1.098522823303938 21.09375 0.14937718212604523\n",
            "strain 22.372900009155273\n",
            "in vicreg  1.0087907314300537 21.2890625 0.12187306582927704\n",
            "strain 22.380664825439453\n",
            "in vicreg  1.0568618774414062 21.19140625 0.12625013291835785\n",
            "strain 22.4331111907959\n",
            "in vicreg  1.1794481426477432 20.8984375 0.16149882972240448\n",
            "strain 22.215946197509766\n",
            "in vicreg  1.1437887325882912 21.09375 0.15848897397518158\n",
            "strain 22.427278518676758\n",
            "in vicreg  1.247893087565899 20.8984375 0.1797368824481964\n",
            "strain 22.302629470825195\n",
            "in vicreg  1.2379122897982597 20.8984375 0.1761074960231781\n",
            "strain 22.289020538330078\n",
            "classify 2.84765625\n",
            "classify 2.912109375\n",
            "classify 2.871337890625\n",
            "classify 2.816650390625\n",
            "classify 2.845703125\n",
            "classify 2.859130859375\n",
            "classify 2.906005859375\n",
            "classify 2.85107421875\n",
            "classify 2.92138671875\n",
            "0.03125\n",
            "0.140625\n",
            "0.109375\n",
            "0.09375\n",
            "in vicreg  0.9665089659392834 21.2890625 0.11771996319293976\n",
            "strain 22.334228515625\n",
            "in vicreg  0.9597176685929298 21.19140625 0.11667858064174652\n",
            "strain 22.32639503479004\n",
            "in vicreg  0.9477965533733368 21.2890625 0.13352373242378235\n",
            "strain 22.33131980895996\n",
            "in vicreg  0.8750211447477341 21.38671875 0.10498248040676117\n",
            "strain 22.355003356933594\n",
            "in vicreg  1.0353530757129192 21.09375 0.13145434856414795\n",
            "strain 22.291807174682617\n",
            "in vicreg  1.1285202577710152 20.99609375 0.1509016752243042\n",
            "strain 22.279422760009766\n",
            "in vicreg  1.239819172769785 20.8984375 0.17198970913887024\n",
            "strain 22.286808013916016\n",
            "in vicreg  1.2032398954033852 20.8984375 0.17892716825008392\n",
            "strain 22.257165908813477\n",
            "in vicreg  1.0998300276696682 21.09375 0.15735886991024017\n",
            "strain 22.38218879699707\n",
            "classify 2.8486328125\n",
            "classify 2.86376953125\n",
            "classify 2.855712890625\n",
            "classify 2.916259765625\n",
            "classify 2.880859375\n",
            "classify 2.89111328125\n",
            "classify 2.872802734375\n",
            "classify 2.897216796875\n",
            "classify 2.820068359375\n",
            "0.078125\n",
            "0.125\n",
            "0.09375\n",
            "0.046875\n",
            "in vicreg  1.0213610716164112 21.09375 0.14317797124385834\n",
            "strain 22.28953742980957\n",
            "in vicreg  1.0856347158551216 20.99609375 0.15592312812805176\n",
            "strain 22.241558074951172\n",
            "in vicreg  1.0291680693626404 21.09375 0.13312247395515442\n",
            "strain 22.287290573120117\n",
            "in vicreg  0.9826424531638622 21.19140625 0.11550416797399521\n",
            "strain 22.348146438598633\n",
            "in vicreg  0.9151237085461617 21.2890625 0.10602803528308868\n",
            "strain 22.271150588989258\n",
            "in vicreg  0.9572542272508144 21.2890625 0.11888950318098068\n",
            "strain 22.326143264770508\n",
            "in vicreg  1.2328241020441055 20.80078125 0.18107174336910248\n",
            "strain 22.163896560668945\n",
            "in vicreg  1.1759938672184944 20.8984375 0.18916919827461243\n",
            "strain 22.240163803100586\n",
            "in vicreg  1.1626130901277065 20.8984375 0.15605445206165314\n",
            "strain 22.193666458129883\n",
            "classify 2.864501953125\n",
            "classify 2.85693359375\n",
            "classify 2.89599609375\n",
            "classify 2.87158203125\n",
            "classify 2.892822265625\n",
            "classify 2.836669921875\n",
            "classify 2.8515625\n",
            "classify 2.915283203125\n",
            "classify 2.909912109375\n",
            "0.0625\n",
            "0.109375\n",
            "0.125\n",
            "0.046875\n",
            "in vicreg  1.043260097503662 21.09375 0.1484137922525406\n",
            "strain 22.316675186157227\n",
            "in vicreg  1.0250730440020561 21.09375 0.13172203302383423\n",
            "strain 22.281795501708984\n",
            "in vicreg  0.9574828669428825 21.2890625 0.10529347509145737\n",
            "strain 22.312776565551758\n",
            "in vicreg  1.1411763727664948 20.8984375 0.14774101972579956\n",
            "strain 22.163917541503906\n",
            "in vicreg  1.0879604145884514 21.09375 0.1417132467031479\n",
            "strain 22.35467529296875\n",
            "in vicreg  1.13010099157691 20.99609375 0.1561690717935562\n",
            "strain 22.286270141601562\n",
            "in vicreg  1.1591600254178047 20.8984375 0.16614656150341034\n",
            "strain 22.200307846069336\n",
            "in vicreg  1.1943880468606949 20.703125 0.18896351754665375\n",
            "strain 22.133350372314453\n",
            "in vicreg  0.9821523912250996 21.09375 0.12429322302341461\n",
            "strain 22.2314453125\n",
            "classify 2.85400390625\n",
            "classify 2.855224609375\n",
            "classify 2.851318359375\n",
            "classify 2.86279296875\n",
            "classify 2.92578125\n",
            "classify 2.896240234375\n",
            "classify 2.879638671875\n",
            "classify 2.915771484375\n",
            "classify 2.8701171875\n",
            "0.09375\n",
            "0.15625\n",
            "0.046875\n",
            "0.09375\n",
            "in vicreg  0.9565986692905426 21.19140625 0.12655209004878998\n",
            "strain 22.33315086364746\n",
            "in vicreg  1.0365097783505917 20.99609375 0.14673393964767456\n",
            "strain 22.183244705200195\n",
            "in vicreg  1.018170453608036 21.09375 0.13729514181613922\n",
            "strain 22.280466079711914\n",
            "in vicreg  0.9727111086249352 21.09375 0.1324627548456192\n",
            "strain 22.230175018310547\n",
            "in vicreg  0.96459174528718 21.19140625 0.12164761871099472\n",
            "strain 22.336238861083984\n",
            "in vicreg  1.2139670550823212 20.80078125 0.17951524257659912\n",
            "strain 22.143482208251953\n",
            "in vicreg  1.1432796716690063 20.8984375 0.1650882363319397\n",
            "strain 22.183368682861328\n",
            "in vicreg  1.0366134345531464 21.09375 0.14091290533542633\n",
            "strain 22.302526473999023\n",
            "in vicreg  1.2310073710978031 20.703125 0.188813254237175\n",
            "strain 22.16982078552246\n",
            "classify 2.870849609375\n",
            "classify 2.933837890625\n",
            "classify 2.866943359375\n",
            "classify 2.900634765625\n",
            "classify 2.88525390625\n",
            "classify 2.830078125\n",
            "classify 2.873779296875\n",
            "classify 2.88623046875\n",
            "classify 2.87060546875\n",
            "0.109375\n",
            "0.03125\n",
            "0.09375\n",
            "0.109375\n",
            "in vicreg  1.0603288188576698 20.99609375 0.14313067495822906\n",
            "strain 22.203460693359375\n",
            "in vicreg  1.0431621223688126 20.99609375 0.15656355023384094\n",
            "strain 22.199724197387695\n",
            "in vicreg  0.8019020780920982 21.484375 0.0990334153175354\n",
            "strain 22.400936126708984\n",
            "in vicreg  0.982443057000637 21.09375 0.11926250159740448\n",
            "strain 22.22670555114746\n",
            "in vicreg  1.165509968996048 20.8984375 0.16351430118083954\n",
            "strain 22.204025268554688\n",
            "in vicreg  1.2111751362681389 20.703125 0.19848491251468658\n",
            "strain 22.15966033935547\n",
            "in vicreg  1.2111933901906013 20.703125 0.21457630395889282\n",
            "strain 22.175769805908203\n",
            "in vicreg  1.0795374400913715 20.8984375 0.1716553419828415\n",
            "strain 22.126192092895508\n",
            "in vicreg  0.9452883154153824 21.09375 0.12157199531793594\n",
            "strain 22.19186019897461\n",
            "classify 2.9326171875\n",
            "classify 2.904541015625\n",
            "classify 2.84228515625\n",
            "classify 2.873779296875\n",
            "classify 2.9013671875\n",
            "classify 2.802490234375\n",
            "classify 2.853759765625\n",
            "classify 2.8515625\n",
            "classify 2.94873046875\n",
            "0.109375\n",
            "0.125\n",
            "0.0625\n",
            "0.046875\n",
            "in vicreg  0.9554091840982437 21.09375 0.12246183305978775\n",
            "strain 22.202871322631836\n",
            "in vicreg  0.805429182946682 21.2890625 0.08723068237304688\n",
            "strain 22.14266014099121\n",
            "in vicreg  1.0140443220734596 21.09375 0.12333982437849045\n",
            "strain 22.26238441467285\n",
            "in vicreg  1.0771474801003933 20.8984375 0.14391016960144043\n",
            "strain 22.096057891845703\n",
            "in vicreg  1.194504275918007 20.703125 0.18015173077583313\n",
            "strain 22.12465476989746\n",
            "in vicreg  1.2026436626911163 20.80078125 0.18296654522418976\n",
            "strain 22.135610580444336\n",
            "in vicreg  1.1983046308159828 20.80078125 0.16978953778743744\n",
            "strain 22.11809539794922\n",
            "in vicreg  1.0142557322978973 21.09375 0.12910430133342743\n",
            "strain 22.268360137939453\n",
            "in vicreg  0.988224521279335 21.09375 0.14833863079547882\n",
            "strain 22.26156234741211\n",
            "classify 2.882568359375\n",
            "classify 2.896484375\n",
            "classify 2.90771484375\n",
            "classify 2.849609375\n",
            "classify 2.88623046875\n",
            "classify 2.882080078125\n",
            "classify 2.8447265625\n",
            "classify 2.851318359375\n",
            "classify 2.853271484375\n",
            "0.046875\n",
            "0.09375\n",
            "0.09375\n",
            "0.125\n",
            "in vicreg  1.087975688278675 20.8984375 0.17025126516819\n",
            "strain 22.133228302001953\n",
            "in vicreg  1.0316195897758007 21.09375 0.15330179035663605\n",
            "strain 22.309921264648438\n",
            "in vicreg  1.0580884292721748 20.8984375 0.1567607820034027\n",
            "strain 22.0898494720459\n",
            "in vicreg  0.9971596300601959 20.8984375 0.14499294757843018\n",
            "strain 22.017152786254883\n",
            "in vicreg  1.0287508368492126 20.8984375 0.1719151884317398\n",
            "strain 22.075666427612305\n",
            "in vicreg  1.138942688703537 20.703125 0.19050472974777222\n",
            "strain 22.07944679260254\n",
            "in vicreg  0.8637310937047005 21.2890625 0.11213134974241257\n",
            "strain 22.225862503051758\n",
            "in vicreg  0.9442878887057304 21.09375 0.12068513035774231\n",
            "strain 22.189973831176758\n",
            "in vicreg  1.0722968727350235 20.99609375 0.1397000104188919\n",
            "strain 22.21199607849121\n",
            "classify 2.862548828125\n",
            "classify 2.883544921875\n",
            "classify 2.912109375\n",
            "classify 2.916259765625\n",
            "classify 2.8486328125\n",
            "classify 2.885498046875\n",
            "classify 2.855712890625\n",
            "classify 2.85498046875\n",
            "classify 2.82666015625\n",
            "0.09375\n",
            "0.046875\n",
            "0.109375\n",
            "0.0625\n",
            "in vicreg  1.0560845956206322 20.99609375 0.1503707617521286\n",
            "strain 22.20645523071289\n",
            "in vicreg  1.0992125608026981 20.8984375 0.14678016304969788\n",
            "strain 22.12099266052246\n",
            "in vicreg  1.186453364789486 20.703125 0.17827185988426208\n",
            "strain 22.11472511291504\n",
            "in vicreg  1.0976975783705711 20.8984375 0.16064906120300293\n",
            "strain 22.133346557617188\n",
            "in vicreg  1.1475074104964733 20.80078125 0.2143833190202713\n",
            "strain 22.11189079284668\n",
            "in vicreg  1.0783270001411438 20.8984375 0.18196557462215424\n",
            "strain 22.135292053222656\n",
            "in vicreg  0.9415042586624622 21.09375 0.13064990937709808\n",
            "strain 22.197153091430664\n",
            "in vicreg  0.9954980574548244 20.8984375 0.1520407646894455\n",
            "strain 22.022539138793945\n",
            "in vicreg  0.8742188103497028 21.19140625 0.11072907596826553\n",
            "strain 22.234947204589844\n",
            "classify 2.920654296875\n",
            "classify 2.851806640625\n",
            "classify 2.90625\n",
            "classify 2.865234375\n",
            "classify 2.862060546875\n",
            "classify 2.879150390625\n",
            "classify 2.86572265625\n",
            "classify 2.83984375\n",
            "classify 2.88232421875\n",
            "0.078125\n",
            "0.09375\n",
            "0.0625\n",
            "0.09375\n",
            "in vicreg  1.0585656389594078 20.8984375 0.15479779243469238\n",
            "strain 22.088363647460938\n",
            "in vicreg  0.9641297161579132 21.09375 0.14454615116119385\n",
            "strain 22.23367691040039\n",
            "in vicreg  1.0608414188027382 20.8984375 0.1561550348997116\n",
            "strain 22.091995239257812\n",
            "in vicreg  1.0444894433021545 20.8984375 0.1496022641658783\n",
            "strain 22.069091796875\n",
            "in vicreg  1.0923152789473534 20.8984375 0.16416221857070923\n",
            "strain 22.13147735595703\n",
            "in vicreg  1.0932987555861473 20.8984375 0.1637747585773468\n",
            "strain 22.13207244873047\n",
            "in vicreg  1.1177103966474533 20.703125 0.16925200819969177\n",
            "strain 22.036962509155273\n",
            "in vicreg  1.02181788533926 20.99609375 0.136675626039505\n",
            "strain 22.158493041992188\n",
            "in vicreg  1.1139078065752983 20.703125 0.19008366763591766\n",
            "strain 22.053991317749023\n",
            "classify 2.939697265625\n",
            "classify 2.836181640625\n",
            "classify 2.83154296875\n",
            "classify 2.8427734375\n",
            "classify 2.861083984375\n",
            "classify 2.896728515625\n",
            "classify 2.81884765625\n",
            "classify 2.878662109375\n",
            "classify 2.889404296875\n",
            "0.09375\n",
            "0.078125\n",
            "0.140625\n",
            "0.109375\n",
            "in vicreg  1.0595811530947685 20.8984375 0.14924263954162598\n",
            "strain 22.083824157714844\n",
            "in vicreg  1.0345149785280228 20.8984375 0.17405377328395844\n",
            "strain 22.083568572998047\n",
            "in vicreg  1.0991327464580536 20.703125 0.18313202261924744\n",
            "strain 22.032264709472656\n",
            "in vicreg  1.0723207145929337 20.80078125 0.17359749972820282\n",
            "strain 21.99591827392578\n",
            "in vicreg  0.9372499771416187 20.8984375 0.14574024081230164\n",
            "strain 21.957990646362305\n",
            "in vicreg  0.9940782561898232 20.8984375 0.15216240286827087\n",
            "strain 22.021240234375\n",
            "in vicreg  0.9191572666168213 21.09375 0.1256299465894699\n",
            "strain 22.16978645324707\n",
            "in vicreg  0.9508265182375908 21.09375 0.12716777622699738\n",
            "strain 22.20299530029297\n",
            "in vicreg  1.0724988766014576 20.8984375 0.1512601673603058\n",
            "strain 22.098758697509766\n",
            "classify 2.85107421875\n",
            "classify 2.893798828125\n",
            "classify 2.820556640625\n",
            "classify 2.93115234375\n",
            "classify 2.87939453125\n",
            "classify 2.890869140625\n",
            "classify 2.87646484375\n",
            "classify 2.847900390625\n",
            "classify 2.89794921875\n",
            "0.109375\n",
            "0.109375\n",
            "0.03125\n",
            "0.109375\n",
            "in vicreg  1.1247162707149982 20.703125 0.18037936091423035\n",
            "strain 22.055095672607422\n",
            "in vicreg  1.192170660942793 20.60546875 0.19486109912395477\n",
            "strain 22.01203155517578\n",
            "in vicreg  1.1866046115756035 20.5078125 0.21654079854488373\n",
            "strain 21.903146743774414\n",
            "in vicreg  1.0082720778882504 20.8984375 0.1480216383934021\n",
            "strain 22.031293869018555\n",
            "in vicreg  0.9429290890693665 20.8984375 0.13171416521072388\n",
            "strain 21.949642181396484\n",
            "in vicreg  0.8698949590325356 21.09375 0.12298718094825745\n",
            "strain 22.117883682250977\n",
            "in vicreg  1.0368021205067635 20.80078125 0.16547788679599762\n",
            "strain 21.952280044555664\n",
            "in vicreg  0.8902708068490028 21.09375 0.1327398419380188\n",
            "strain 22.14801025390625\n",
            "in vicreg  0.9286079555749893 21.09375 0.13380104303359985\n",
            "strain 22.187408447265625\n",
            "classify 2.88330078125\n",
            "classify 2.857421875\n",
            "classify 2.880126953125\n",
            "classify 2.84375\n",
            "classify 2.892822265625\n",
            "classify 2.8896484375\n",
            "classify 2.8671875\n",
            "classify 2.905517578125\n",
            "classify 2.83154296875\n",
            "0.0625\n",
            "0.171875\n",
            "0.09375\n",
            "0.0625\n",
            "in vicreg  1.1303490027785301 20.703125 0.1889203041791916\n",
            "strain 22.06926918029785\n",
            "in vicreg  1.1557228863239288 20.703125 0.1841105818748474\n",
            "strain 22.089834213256836\n",
            "in vicreg  1.2368757277727127 20.5078125 0.23421423137187958\n",
            "strain 21.97109031677246\n",
            "in vicreg  1.1001039296388626 20.8984375 0.16903015971183777\n",
            "strain 22.144132614135742\n",
            "in vicreg  1.0306410491466522 20.8984375 0.1447400003671646\n",
            "strain 22.05038070678711\n",
            "in vicreg  1.0391825810074806 20.703125 0.1489628702402115\n",
            "strain 21.93814468383789\n",
            "in vicreg  0.9943322278559208 20.80078125 0.15572385489940643\n",
            "strain 21.900054931640625\n",
            "in vicreg  1.018267311155796 20.703125 0.14991483092308044\n",
            "strain 21.918182373046875\n",
            "in vicreg  0.9082878939807415 20.99609375 0.12198852002620697\n",
            "strain 22.030275344848633\n",
            "classify 2.845458984375\n",
            "classify 2.89111328125\n",
            "classify 2.880859375\n",
            "classify 2.913330078125\n",
            "classify 2.842041015625\n",
            "classify 2.872314453125\n",
            "classify 2.870361328125\n",
            "classify 2.831787109375\n",
            "classify 2.831787109375\n",
            "0.140625\n",
            "0.078125\n",
            "0.09375\n",
            "0.09375\n",
            "in vicreg  0.8692820556461811 21.09375 0.12814772129058838\n",
            "strain 22.12242889404297\n",
            "in vicreg  1.0227877646684647 20.8984375 0.14906887710094452\n",
            "strain 22.046855926513672\n",
            "in vicreg  1.148779969662428 20.60546875 0.18804961442947388\n",
            "strain 21.961830139160156\n",
            "in vicreg  1.1232709512114525 20.703125 0.2094895988702774\n",
            "strain 22.082761764526367\n",
            "in vicreg  1.1321051977574825 20.60546875 0.19734232127666473\n",
            "strain 21.95444679260254\n",
            "in vicreg  1.0172775015234947 20.8984375 0.16942666471004486\n",
            "strain 22.061702728271484\n",
            "in vicreg  0.984926987439394 20.8984375 0.14329829812049866\n",
            "strain 22.003225326538086\n",
            "in vicreg  0.9682014584541321 20.8984375 0.16336052119731903\n",
            "strain 22.006561279296875\n",
            "in vicreg  1.1169408448040485 20.703125 0.1839151829481125\n",
            "strain 22.050857543945312\n",
            "classify 2.89111328125\n",
            "classify 2.827880859375\n",
            "classify 2.917724609375\n",
            "classify 2.86376953125\n",
            "classify 2.9150390625\n",
            "classify 2.79150390625\n",
            "classify 2.828857421875\n",
            "classify 2.87890625\n",
            "classify 2.88134765625\n",
            "0.078125\n",
            "0.125\n",
            "0.140625\n",
            "0.078125\n",
            "in vicreg  1.0288547724485397 20.703125 0.1547514796257019\n",
            "strain 21.933605194091797\n",
            "in vicreg  0.9758751839399338 20.80078125 0.14868439733982086\n",
            "strain 21.87455940246582\n",
            "in vicreg  0.995241105556488 20.8984375 0.15175172686576843\n",
            "strain 22.02199363708496\n",
            "in vicreg  1.0763416066765785 20.60546875 0.1887907087802887\n",
            "strain 21.890132904052734\n",
            "in vicreg  0.9670384228229523 20.8984375 0.13649892807006836\n",
            "strain 21.978538513183594\n",
            "in vicreg  1.0033896192908287 20.703125 0.1703491061925888\n",
            "strain 21.923738479614258\n",
            "in vicreg  1.0547609999775887 20.703125 0.1573556661605835\n",
            "strain 21.96211814880371\n",
            "in vicreg  0.8594685234129429 21.09375 0.1303204894065857\n",
            "strain 22.114788055419922\n",
            "in vicreg  0.9784722700715065 20.8984375 0.19008825719356537\n",
            "strain 22.043560028076172\n",
            "classify 2.86328125\n",
            "classify 2.884033203125\n",
            "classify 2.87939453125\n",
            "classify 2.824462890625\n",
            "classify 2.8916015625\n",
            "classify 2.878173828125\n",
            "classify 2.876708984375\n",
            "classify 2.88330078125\n",
            "classify 2.826416015625\n",
            "0.078125\n",
            "0.140625\n",
            "0.078125\n",
            "0.140625\n",
            "in vicreg  1.1931282468140125 20.5078125 0.21931235492229462\n",
            "strain 21.91244125366211\n",
            "in vicreg  1.0728011839091778 20.703125 0.18441855907440186\n",
            "strain 22.007219314575195\n",
            "in vicreg  1.0452035814523697 20.703125 0.17743563652038574\n",
            "strain 21.972639083862305\n",
            "in vicreg  1.0252236388623714 20.703125 0.19715987145900726\n",
            "strain 21.972383499145508\n",
            "in vicreg  0.9417669847607613 20.8984375 0.13696163892745972\n",
            "strain 21.95372772216797\n",
            "in vicreg  1.0087746195495129 20.703125 0.1518716961145401\n",
            "strain 21.910646438598633\n",
            "in vicreg  0.951631274074316 20.8984375 0.1393950879573822\n",
            "strain 21.966026306152344\n",
            "in vicreg  1.0152741335332394 20.80078125 0.16256771981716156\n",
            "strain 21.927841186523438\n",
            "in vicreg  1.024929154664278 20.703125 0.1590239256620407\n",
            "strain 21.93395233154297\n",
            "classify 2.8369140625\n",
            "classify 2.8359375\n",
            "classify 2.932373046875\n",
            "classify 2.8408203125\n",
            "classify 2.889892578125\n",
            "classify 2.880615234375\n",
            "classify 2.827880859375\n",
            "classify 2.908203125\n",
            "classify 2.906982421875\n",
            "0.109375\n",
            "0.109375\n",
            "0.109375\n",
            "0.0625\n",
            "in vicreg  1.0152881033718586 20.703125 0.15222977101802826\n",
            "strain 21.917516708374023\n",
            "in vicreg  1.1395951732993126 20.60546875 0.19350123405456543\n",
            "strain 21.95809555053711\n",
            "in vicreg  1.0555540211498737 20.703125 0.18941572308540344\n",
            "strain 21.99496841430664\n",
            "in vicreg  1.096536312252283 20.5078125 0.21224720776081085\n",
            "strain 21.80878448486328\n",
            "in vicreg  0.910466630011797 20.99609375 0.1381191462278366\n",
            "strain 22.048585891723633\n",
            "in vicreg  0.988200306892395 20.703125 0.16821952164173126\n",
            "strain 21.90641975402832\n",
            "in vicreg  0.9010085836052895 20.8984375 0.14708565175533295\n",
            "strain 21.923093795776367\n",
            "in vicreg  0.9562330320477486 20.8984375 0.14312304556369781\n",
            "strain 21.97435760498047\n",
            "in vicreg  1.1525527574121952 20.5078125 0.20998667180538177\n",
            "strain 21.862539291381836\n",
            "classify 2.825439453125\n",
            "classify 2.8642578125\n",
            "classify 2.909423828125\n",
            "classify 2.8740234375\n",
            "classify 2.903076171875\n",
            "classify 2.880859375\n",
            "classify 2.860107421875\n",
            "classify 2.853515625\n",
            "classify 2.86474609375\n",
            "0.078125\n",
            "0.078125\n",
            "0.03125\n",
            "0.1875\n",
            "in vicreg  1.0248351842164993 20.703125 0.16111475229263306\n",
            "strain 21.935951232910156\n",
            "in vicreg  1.0884269140660763 20.5078125 0.1752220243215561\n",
            "strain 21.763648986816406\n",
            "in vicreg  1.1511337012052536 20.5078125 0.19923768937587738\n",
            "strain 21.850372314453125\n",
            "in vicreg  1.0316861793398857 20.703125 0.18143197894096375\n",
            "strain 21.963119506835938\n",
            "in vicreg  1.0076696053147316 20.703125 0.1852491796016693\n",
            "strain 21.94291877746582\n",
            "in vicreg  0.9326599538326263 20.703125 0.1546463668346405\n",
            "strain 21.837305068969727\n",
            "in vicreg  0.7107313722372055 21.38671875 0.08662384003400803\n",
            "strain 22.17235565185547\n",
            "in vicreg  0.9818954393267632 20.703125 0.16175764799118042\n",
            "strain 21.893653869628906\n",
            "in vicreg  1.0501203127205372 20.60546875 0.18251146376132965\n",
            "strain 21.857633590698242\n",
            "classify 2.861572265625\n",
            "classify 2.86474609375\n",
            "classify 2.8818359375\n",
            "classify 2.868408203125\n",
            "classify 2.874755859375\n",
            "classify 2.89892578125\n",
            "classify 2.840576171875\n",
            "classify 2.873291015625\n",
            "classify 2.856201171875\n",
            "0.09375\n",
            "0.140625\n",
            "0.09375\n",
            "0.046875\n",
            "in vicreg  1.0140025988221169 20.703125 0.16278906166553497\n",
            "strain 21.926790237426758\n",
            "in vicreg  1.1780849657952785 20.41015625 0.21684612333774567\n",
            "strain 21.76993179321289\n",
            "in vicreg  1.118907891213894 20.5078125 0.20801064372062683\n",
            "strain 21.82691764831543\n",
            "in vicreg  1.228566188365221 20.41015625 0.23229922354221344\n",
            "strain 21.835865020751953\n",
            "in vicreg  1.0361511260271072 20.60546875 0.17933078110218048\n",
            "strain 21.840482711791992\n",
            "in vicreg  0.9095577523112297 20.80078125 0.14311613142490387\n",
            "strain 21.80267333984375\n",
            "in vicreg  0.7802552543580532 21.09375 0.13974568247795105\n",
            "strain 22.045000076293945\n",
            "in vicreg  0.8696068078279495 20.8984375 0.14111725986003876\n",
            "strain 21.885723114013672\n",
            "in vicreg  0.9750132448971272 20.703125 0.168725848197937\n",
            "strain 21.893739700317383\n",
            "classify 2.898193359375\n",
            "classify 2.855224609375\n",
            "classify 2.831787109375\n",
            "classify 2.89990234375\n",
            "classify 2.8896484375\n",
            "classify 2.8779296875\n",
            "classify 2.900634765625\n",
            "classify 2.844970703125\n",
            "classify 2.865966796875\n",
            "0.09375\n",
            "0.078125\n",
            "0.140625\n",
            "0.0625\n",
            "in vicreg  0.9181422181427479 20.8984375 0.14140743017196655\n",
            "strain 21.93454933166504\n",
            "in vicreg  1.1021154932677746 20.5078125 0.20611152052879333\n",
            "strain 21.8082275390625\n",
            "in vicreg  1.0902104899287224 20.60546875 0.19352823495864868\n",
            "strain 21.90873908996582\n",
            "in vicreg  0.9847516193985939 20.80078125 0.15918433666229248\n",
            "strain 21.89393424987793\n",
            "in vicreg  0.9895695373415947 20.80078125 0.15817761421203613\n",
            "strain 21.897747039794922\n",
            "in vicreg  1.1363741010427475 20.5078125 0.1888425499200821\n",
            "strain 21.82521629333496\n",
            "in vicreg  1.2149561196565628 20.3125 0.2367725372314453\n",
            "strain 21.70172882080078\n",
            "in vicreg  1.0397843085229397 20.703125 0.17073719203472137\n",
            "strain 21.960519790649414\n",
            "in vicreg  1.0188636370003223 20.5078125 0.1913336217403412\n",
            "strain 21.71019744873047\n",
            "classify 2.85888671875\n",
            "classify 2.843505859375\n",
            "classify 2.888916015625\n",
            "classify 2.83984375\n",
            "classify 2.868896484375\n",
            "classify 2.8291015625\n",
            "classify 2.879150390625\n",
            "classify 2.90966796875\n",
            "classify 2.862548828125\n",
            "0.09375\n",
            "0.125\n",
            "0.0625\n",
            "0.15625\n",
            "in vicreg  0.8816199377179146 20.80078125 0.13948741555213928\n",
            "strain 21.771108627319336\n",
            "in vicreg  0.8542800322175026 20.8984375 0.14385487139225006\n",
            "strain 21.87313461303711\n",
            "in vicreg  0.9390050545334816 20.703125 0.1609705686569214\n",
            "strain 21.8499755859375\n",
            "in vicreg  0.9630396962165833 20.703125 0.17199036478996277\n",
            "strain 21.885028839111328\n",
            "in vicreg  1.0193550027906895 20.5078125 0.1919586956501007\n",
            "strain 21.711315155029297\n",
            "in vicreg  1.0653572157025337 20.5078125 0.19743521511554718\n",
            "strain 21.762792587280273\n",
            "in vicreg  1.011769287288189 20.703125 0.1660037785768509\n",
            "strain 21.927772521972656\n",
            "in vicreg  0.9625167585909367 20.703125 0.17248255014419556\n",
            "strain 21.885000228881836\n",
            "in vicreg  1.1266179382801056 20.41015625 0.2032308280467987\n",
            "strain 21.70484733581543\n",
            "classify 2.859375\n",
            "classify 2.9296875\n",
            "classify 2.839111328125\n",
            "classify 2.866455078125\n",
            "classify 2.84716796875\n",
            "classify 2.88037109375\n",
            "classify 2.83203125\n",
            "classify 2.88818359375\n",
            "classify 2.88134765625\n",
            "0.078125\n",
            "0.109375\n",
            "0.140625\n",
            "0.046875\n",
            "in vicreg  1.118182484060526 20.41015625 0.2023550271987915\n",
            "strain 21.695537567138672\n",
            "in vicreg  1.0169480927288532 20.5078125 0.1793731451034546\n",
            "strain 21.696321487426758\n",
            "in vicreg  0.8915279991924763 20.8984375 0.1389865279197693\n",
            "strain 21.905513763427734\n",
            "in vicreg  0.9509327821433544 20.703125 0.17205505073070526\n",
            "strain 21.872987747192383\n",
            "in vicreg  0.9966445155441761 20.60546875 0.17125828564167023\n",
            "strain 21.792903900146484\n",
            "in vicreg  0.9729145094752312 20.703125 0.1647438406944275\n",
            "strain 21.887657165527344\n",
            "in vicreg  1.1046186089515686 20.3125 0.22838696837425232\n",
            "strain 21.583005905151367\n",
            "in vicreg  1.130492240190506 20.3125 0.20921070873737335\n",
            "strain 21.589704513549805\n",
            "in vicreg  0.9852332063019276 20.60546875 0.16992685198783875\n",
            "strain 21.780160903930664\n",
            "classify 2.840576171875\n",
            "classify 2.9013671875\n",
            "classify 2.86328125\n",
            "classify 2.917724609375\n",
            "classify 2.825439453125\n",
            "classify 2.8447265625\n",
            "classify 2.906494140625\n",
            "classify 2.884033203125\n",
            "classify 2.865478515625\n",
            "0.109375\n",
            "0.078125\n",
            "0.078125\n",
            "0.046875\n",
            "in vicreg  0.9977458044886589 20.60546875 0.1707330346107483\n",
            "strain 21.79347801208496\n",
            "in vicreg  0.8650133386254311 20.8984375 0.13072597980499268\n",
            "strain 21.870738983154297\n",
            "in vicreg  0.9392296895384789 20.703125 0.16751819849014282\n",
            "strain 21.856748580932617\n",
            "in vicreg  0.9408676996827126 20.703125 0.1630409061908722\n",
            "strain 21.85390853881836\n",
            "in vicreg  1.069028489291668 20.5078125 0.20627421140670776\n",
            "strain 21.77530288696289\n",
            "in vicreg  1.1902340687811375 20.1171875 0.2699262499809265\n",
            "strain 21.585159301757812\n",
            "in vicreg  1.0251136496663094 20.5078125 0.20210988819599152\n",
            "strain 21.727224349975586\n",
            "in vicreg  0.7878982461988926 20.8984375 0.1200605258345604\n",
            "strain 21.782958984375\n",
            "in vicreg  0.9293211624026299 20.60546875 0.17245914041996002\n",
            "strain 21.72677993774414\n",
            "classify 2.830810546875\n",
            "classify 2.82275390625\n",
            "classify 2.89501953125\n",
            "classify 2.890380859375\n",
            "classify 2.838134765625\n",
            "classify 2.893310546875\n",
            "classify 2.88330078125\n",
            "classify 2.88671875\n",
            "classify 2.83544921875\n",
            "0.03125\n",
            "0.140625\n",
            "0.15625\n",
            "0.03125\n",
            "in vicreg  0.8904005400836468 20.703125 0.14033837616443634\n",
            "strain 21.780738830566406\n",
            "in vicreg  1.0141465812921524 20.5078125 0.1862832009792328\n",
            "strain 21.700429916381836\n",
            "in vicreg  1.0985463857650757 20.3125 0.21046097576618195\n",
            "strain 21.55900764465332\n",
            "in vicreg  0.9675366804003716 20.703125 0.16581642627716064\n",
            "strain 21.88335418701172\n",
            "in vicreg  1.036948710680008 20.5078125 0.17373394966125488\n",
            "strain 21.710683822631836\n",
            "in vicreg  1.0207896120846272 20.5078125 0.20679466426372528\n",
            "strain 21.727584838867188\n",
            "in vicreg  1.0673221200704575 20.5078125 0.21353861689567566\n",
            "strain 21.780860900878906\n",
            "in vicreg  1.0831216350197792 20.3125 0.22656549513339996\n",
            "strain 21.559688568115234\n",
            "in vicreg  0.9486278519034386 20.60546875 0.17335255444049835\n",
            "strain 21.746980667114258\n",
            "classify 2.854248046875\n",
            "classify 2.888916015625\n",
            "classify 2.90185546875\n",
            "classify 2.94287109375\n",
            "classify 2.782958984375\n",
            "classify 2.806884765625\n",
            "classify 2.912109375\n",
            "classify 2.851318359375\n",
            "classify 2.880615234375\n",
            "0.078125\n",
            "0.125\n",
            "0.078125\n",
            "0.03125\n",
            "in vicreg  0.9139003232121468 20.703125 0.16582660377025604\n",
            "strain 21.829727172851562\n",
            "in vicreg  0.8864385075867176 20.703125 0.15387257933616638\n",
            "strain 21.790311813354492\n",
            "in vicreg  0.9966186247766018 20.5078125 0.1739719659090042\n",
            "strain 21.670589447021484\n",
            "in vicreg  1.0284543968737125 20.3125 0.20264489948749542\n",
            "strain 21.481098175048828\n",
            "in vicreg  0.8839300833642483 20.703125 0.14503028988838196\n",
            "strain 21.778961181640625\n",
            "in vicreg  1.0181697085499763 20.5078125 0.18102742731571198\n",
            "strain 21.69919776916504\n",
            "in vicreg  0.9127850644290447 20.703125 0.14634037017822266\n",
            "strain 21.809123992919922\n",
            "in vicreg  1.0571548715233803 20.5078125 0.1930185854434967\n",
            "strain 21.750173568725586\n",
            "in vicreg  1.1635595001280308 20.3125 0.22808963060379028\n",
            "strain 21.64164924621582\n",
            "classify 2.840576171875\n",
            "classify 2.89892578125\n",
            "classify 2.88037109375\n",
            "classify 2.912841796875\n",
            "classify 2.8701171875\n",
            "classify 2.849365234375\n",
            "classify 2.849365234375\n",
            "classify 2.8505859375\n",
            "classify 2.853515625\n",
            "0.15625\n",
            "0.03125\n",
            "0.078125\n",
            "0.109375\n",
            "in vicreg  1.310928352177143 19.921875 0.30298319458961487\n",
            "strain 21.488910675048828\n",
            "in vicreg  0.9536365047097206 20.60546875 0.18242178857326508\n",
            "strain 21.761058807373047\n",
            "in vicreg  0.8510404266417027 20.703125 0.15306983888149261\n",
            "strain 21.75411033630371\n",
            "in vicreg  0.8760478347539902 20.703125 0.15559571981430054\n",
            "strain 21.78164291381836\n",
            "in vicreg  0.8138952776789665 20.703125 0.15683913230895996\n",
            "strain 21.720735549926758\n",
            "in vicreg  0.790646206587553 20.8984375 0.1486492156982422\n",
            "strain 21.814294815063477\n",
            "in vicreg  0.9607451967895031 20.5078125 0.17351259291172028\n",
            "strain 21.634258270263672\n",
            "in vicreg  0.9818439371883869 20.5078125 0.18484073877334595\n",
            "strain 21.666685104370117\n",
            "in vicreg  1.1504154652357101 20.1171875 0.2367764115333557\n",
            "strain 21.512191772460938\n",
            "classify 2.849853515625\n",
            "classify 2.861083984375\n",
            "classify 2.887939453125\n",
            "classify 2.861083984375\n",
            "classify 2.8564453125\n",
            "classify 2.89990234375\n",
            "classify 2.874267578125\n",
            "classify 2.847900390625\n",
            "classify 2.86376953125\n",
            "0.140625\n",
            "0.078125\n",
            "0.078125\n",
            "0.078125\n",
            "in vicreg  1.112186349928379 20.3125 0.23462437093257904\n",
            "strain 21.596811294555664\n",
            "in vicreg  1.04873925447464 20.41015625 0.20754314959049225\n",
            "strain 21.63128089904785\n",
            "in vicreg  1.0500380769371986 20.3125 0.19755078852176666\n",
            "strain 21.497589111328125\n",
            "in vicreg  0.8418573066592216 20.80078125 0.13744844496250153\n",
            "strain 21.729307174682617\n",
            "in vicreg  0.9082283824682236 20.60546875 0.15325945615768433\n",
            "strain 21.686487197875977\n",
            "in vicreg  0.9858153760433197 20.41015625 0.21997685730457306\n",
            "strain 21.580791473388672\n",
            "in vicreg  0.9423560462892056 20.703125 0.1595061719417572\n",
            "strain 21.85186195373535\n",
            "in vicreg  0.9418686851859093 20.60546875 0.1633102148771286\n",
            "strain 21.730178833007812\n",
            "in vicreg  0.9957477450370789 20.5078125 0.1912635862827301\n",
            "strain 21.68701171875\n",
            "classify 2.874755859375\n",
            "classify 2.868896484375\n",
            "classify 2.82177734375\n",
            "classify 2.842529296875\n",
            "classify 2.90625\n",
            "classify 2.87353515625\n",
            "classify 2.845703125\n",
            "classify 2.926513671875\n",
            "classify 2.82861328125\n",
            "0.09375\n",
            "0.078125\n",
            "0.140625\n",
            "0.046875\n",
            "in vicreg  1.0070988908410072 20.5078125 0.18495912849903107\n",
            "strain 21.692058563232422\n",
            "in vicreg  1.186634972691536 20.1171875 0.27912938594818115\n",
            "strain 21.590763092041016\n",
            "in vicreg  0.9653126820921898 20.5078125 0.17373093962669373\n",
            "strain 21.6390438079834\n",
            "in vicreg  0.9766971692442894 20.5078125 0.1881284862756729\n",
            "strain 21.664827346801758\n",
            "in vicreg  0.9726060554385185 20.41015625 0.18601156771183014\n",
            "strain 21.533618927001953\n",
            "in vicreg  0.8790802210569382 20.60546875 0.16477195918560028\n",
            "strain 21.668851852416992\n",
            "in vicreg  0.9503298439085484 20.5078125 0.18215923011302948\n",
            "strain 21.632490158081055\n",
            "in vicreg  0.9303178638219833 20.5078125 0.18348170816898346\n",
            "strain 21.613798141479492\n",
            "in vicreg  0.9574639610946178 20.5078125 0.17439110577106476\n",
            "strain 21.631855010986328\n",
            "classify 2.887451171875\n",
            "classify 2.880859375\n",
            "classify 2.858154296875\n",
            "classify 2.845703125\n",
            "classify 2.88232421875\n",
            "classify 2.917724609375\n",
            "classify 2.85791015625\n",
            "classify 2.8642578125\n",
            "classify 2.836181640625\n",
            "0.109375\n",
            "0.046875\n",
            "0.078125\n",
            "0.140625\n",
            "in vicreg  0.9481913410127163 20.5078125 0.1624550223350525\n",
            "strain 21.610645294189453\n",
            "in vicreg  1.110879797488451 20.21484375 0.23848973214626312\n",
            "strain 21.599369049072266\n",
            "in vicreg  1.1275190860033035 20.1171875 0.2376316636800766\n",
            "strain 21.490150451660156\n",
            "in vicreg  1.034976076334715 20.3125 0.2164900153875351\n",
            "strain 21.501466751098633\n",
            "in vicreg  0.9375834837555885 20.5078125 0.1739596426486969\n",
            "strain 21.611543655395508\n",
            "in vicreg  0.8546393364667892 20.5078125 0.15684278309345245\n",
            "strain 21.51148223876953\n",
            "in vicreg  0.8859863504767418 20.5078125 0.18214154243469238\n",
            "strain 21.56812858581543\n",
            "in vicreg  0.9692838415503502 20.3125 0.22747264802455902\n",
            "strain 21.44675636291504\n",
            "in vicreg  0.8847753517329693 20.60546875 0.16265028715133667\n",
            "strain 21.672426223754883\n",
            "classify 2.874755859375\n",
            "classify 2.81689453125\n",
            "classify 2.890380859375\n",
            "classify 2.8193359375\n",
            "classify 2.850341796875\n",
            "classify 2.937744140625\n",
            "classify 2.857421875\n",
            "classify 2.862548828125\n",
            "classify 2.868896484375\n",
            "0.1875\n",
            "0.0625\n",
            "0.078125\n",
            "0.046875\n",
            "in vicreg  0.9861191734671593 20.3125 0.22301408648490906\n",
            "strain 21.45913314819336\n",
            "in vicreg  0.8894247002899647 20.60546875 0.16788330674171448\n",
            "strain 21.682308197021484\n",
            "in vicreg  1.0469868779182434 20.3125 0.21284082531929016\n",
            "strain 21.509828567504883\n",
            "in vicreg  0.9604884311556816 20.5078125 0.1842641532421112\n",
            "strain 21.644752502441406\n",
            "in vicreg  1.0026829317212105 20.3125 0.20640650391578674\n",
            "strain 21.459089279174805\n",
            "in vicreg  1.029963418841362 20.3125 0.2070367932319641\n",
            "strain 21.48699951171875\n",
            "in vicreg  0.9702602401375771 20.41015625 0.19439880549907684\n",
            "strain 21.53965950012207\n",
            "in vicreg  1.0177815333008766 20.3125 0.2009083777666092\n",
            "strain 21.468690872192383\n",
            "in vicreg  0.9640483185648918 20.5078125 0.20326779782772064\n",
            "strain 21.667316436767578\n",
            "classify 2.868408203125\n",
            "classify 2.865478515625\n",
            "classify 2.88525390625\n",
            "classify 2.870361328125\n",
            "classify 2.846923828125\n",
            "classify 2.8994140625\n",
            "classify 2.8720703125\n",
            "classify 2.87353515625\n",
            "classify 2.835693359375\n",
            "0.046875\n",
            "0.09375\n",
            "0.09375\n",
            "0.125\n",
            "in vicreg  0.9774690493941307 20.3125 0.2064065784215927\n",
            "strain 21.433874130249023\n",
            "in vicreg  0.7862766273319721 20.8984375 0.14184096455574036\n",
            "strain 21.803117752075195\n",
            "in vicreg  0.9154887869954109 20.5078125 0.1700165867805481\n",
            "strain 21.585506439208984\n",
            "in vicreg  1.0778225027024746 20.1171875 0.22491520643234253\n",
            "strain 21.427736282348633\n",
            "in vicreg  0.9479645639657974 20.5078125 0.19411256909370422\n",
            "strain 21.64207649230957\n",
            "in vicreg  1.0893503203988075 20.1171875 0.23519295454025269\n",
            "strain 21.449542999267578\n",
            "in vicreg  1.1238984763622284 20.1171875 0.25051698088645935\n",
            "strain 21.499414443969727\n",
            "in vicreg  0.9693990461528301 20.3125 0.19800016283988953\n",
            "strain 21.41739845275879\n",
            "in vicreg  0.8860839530825615 20.5078125 0.17800655961036682\n",
            "strain 21.564090728759766\n",
            "classify 2.86328125\n",
            "classify 2.880615234375\n",
            "classify 2.8974609375\n",
            "classify 2.857177734375\n",
            "classify 2.869873046875\n",
            "classify 2.867431640625\n",
            "classify 2.87060546875\n",
            "classify 2.832763671875\n",
            "classify 2.862548828125\n",
            "0.109375\n",
            "0.125\n",
            "0.078125\n",
            "0.0625\n",
            "in vicreg  0.8300583809614182 20.60546875 0.164963498711586\n",
            "strain 21.62002182006836\n",
            "in vicreg  0.7919352501630783 20.703125 0.13918036222457886\n",
            "strain 21.681116104125977\n",
            "in vicreg  0.9035155177116394 20.41015625 0.19602204859256744\n",
            "strain 21.474536895751953\n",
            "in vicreg  0.9688748046755791 20.3125 0.22137340903282166\n",
            "strain 21.44024658203125\n",
            "in vicreg  1.007509045302868 20.3125 0.19834773242473602\n",
            "strain 21.455856323242188\n",
            "in vicreg  1.1265873908996582 20.1171875 0.24775193631649017\n",
            "strain 21.499338150024414\n",
            "in vicreg  1.0692987591028214 20.1171875 0.22948448359966278\n",
            "strain 21.423782348632812\n",
            "in vicreg  0.9251642972230911 20.41015625 0.18512561917304993\n",
            "strain 21.48529052734375\n",
            "in vicreg  0.9153055027127266 20.5078125 0.1673346757888794\n",
            "strain 21.5826416015625\n",
            "classify 2.877685546875\n",
            "classify 2.8701171875\n",
            "classify 2.84716796875\n",
            "classify 2.880859375\n",
            "classify 2.825439453125\n",
            "classify 2.872314453125\n",
            "classify 2.850830078125\n",
            "classify 2.82373046875\n",
            "classify 2.881103515625\n",
            "0.15625\n",
            "0.125\n",
            "0.03125\n",
            "0.078125\n",
            "in vicreg  0.8697111159563065 20.5078125 0.15881416201591492\n",
            "strain 21.52852439880371\n",
            "in vicreg  0.8996454998850822 20.5078125 0.17412559688091278\n",
            "strain 21.57377052307129\n",
            "in vicreg  1.0929767042398453 20.1171875 0.25115224719047546\n",
            "strain 21.46912956237793\n",
            "in vicreg  0.9678605012595654 20.41015625 0.2042388916015625\n",
            "strain 21.547100067138672\n",
            "in vicreg  0.9806398302316666 20.3125 0.23423905670642853\n",
            "strain 21.464879989624023\n",
            "in vicreg  0.8790994063019753 20.5078125 0.1937091201543808\n",
            "strain 21.57280731201172\n",
            "in vicreg  1.090460829436779 20.1171875 0.2257765382528305\n",
            "strain 21.441238403320312\n",
            "in vicreg  1.0261258110404015 20.1171875 0.22125065326690674\n",
            "strain 21.37237548828125\n",
            "in vicreg  0.8730338886380196 20.5078125 0.17589814960956573\n",
            "strain 21.548931121826172\n",
            "classify 2.917724609375\n",
            "classify 2.811767578125\n",
            "classify 2.86767578125\n",
            "classify 2.812744140625\n",
            "classify 2.8671875\n",
            "classify 2.88671875\n",
            "classify 2.894287109375\n",
            "classify 2.83349609375\n",
            "classify 2.86767578125\n",
            "0.078125\n",
            "0.109375\n",
            "0.1875\n",
            "0.046875\n",
            "in vicreg  0.7917234674096107 20.5078125 0.15878444910049438\n",
            "strain 21.45050811767578\n",
            "in vicreg  0.8940001949667931 20.5078125 0.1810571402311325\n",
            "strain 21.575057983398438\n",
            "in vicreg  0.8927007205784321 20.41015625 0.17200590670108795\n",
            "strain 21.439706802368164\n",
            "in vicreg  1.074468158185482 20.01953125 0.257957398891449\n",
            "strain 21.332426071166992\n",
            "in vicreg  0.9843078441917896 20.3125 0.21309970319271088\n",
            "strain 21.44740867614746\n",
            "in vicreg  1.016564667224884 20.3125 0.223413348197937\n",
            "strain 21.489978790283203\n",
            "in vicreg  0.9942458942532539 20.1171875 0.22339987754821777\n",
            "strain 21.3426456451416\n",
            "in vicreg  0.9541435167193413 20.3125 0.2215459644794464\n",
            "strain 21.425689697265625\n",
            "in vicreg  1.0154671967029572 20.1171875 0.21407663822174072\n",
            "strain 21.354543685913086\n",
            "classify 2.8740234375\n",
            "classify 2.891845703125\n",
            "classify 2.859619140625\n",
            "classify 2.846435546875\n",
            "classify 2.850341796875\n",
            "classify 2.859619140625\n",
            "classify 2.8515625\n",
            "classify 2.89208984375\n",
            "classify 2.86376953125\n",
            "0.15625\n",
            "0.046875\n",
            "0.09375\n",
            "0.125\n",
            "in vicreg  0.8413951843976974 20.5078125 0.16911868751049042\n",
            "strain 21.510513305664062\n",
            "in vicreg  0.9365729056298733 20.3125 0.20424216985702515\n",
            "strain 21.39081573486328\n",
            "in vicreg  0.8122214116156101 20.60546875 0.15036024153232574\n",
            "strain 21.587581634521484\n",
            "in vicreg  0.9401263669133186 20.1171875 0.21610303223133087\n",
            "strain 21.28122901916504\n",
            "in vicreg  0.9814455173909664 20.1171875 0.2316320538520813\n",
            "strain 21.338077545166016\n",
            "in vicreg  0.8794423192739487 20.5078125 0.17449939250946045\n",
            "strain 21.55394172668457\n",
            "in vicreg  0.9787295944988728 20.21484375 0.22429566085338593\n",
            "strain 21.453025817871094\n",
            "in vicreg  0.9112400934100151 20.5078125 0.19134333729743958\n",
            "strain 21.602582931518555\n",
            "in vicreg  1.1514483951032162 19.921875 0.26316037774086\n",
            "strain 21.289609909057617\n",
            "classify 2.85302734375\n",
            "classify 2.880859375\n",
            "classify 2.837646484375\n",
            "classify 2.88232421875\n",
            "classify 2.8642578125\n",
            "classify 2.87744140625\n",
            "classify 2.835205078125\n",
            "classify 2.878662109375\n",
            "classify 2.84521484375\n",
            "0.109375\n",
            "0.078125\n",
            "0.078125\n",
            "0.09375\n",
            "in vicreg  0.9534980170428753 20.3125 0.19704362750053406\n",
            "strain 21.400543212890625\n",
            "in vicreg  0.961156003177166 20.21484375 0.2277056872844696\n",
            "strain 21.438861846923828\n",
            "in vicreg  1.003747619688511 20.1171875 0.24317464232444763\n",
            "strain 21.371923446655273\n",
            "in vicreg  0.8673984557390213 20.3125 0.19429612159729004\n",
            "strain 21.311695098876953\n",
            "in vicreg  0.8712666109204292 20.3125 0.20028278231620789\n",
            "strain 21.321550369262695\n",
            "in vicreg  0.9455153718590736 20.1171875 0.20663850009441376\n",
            "strain 21.27715301513672\n",
            "in vicreg  0.8154628798365593 20.5078125 0.164838969707489\n",
            "strain 21.480300903320312\n",
            "in vicreg  0.9176725521683693 20.21484375 0.20299196243286133\n",
            "strain 21.370664596557617\n",
            "in vicreg  0.8542023599147797 20.41015625 0.17025123536586761\n",
            "strain 21.39945411682129\n",
            "classify 2.87548828125\n",
            "classify 2.818603515625\n",
            "classify 2.865234375\n",
            "classify 2.878662109375\n",
            "classify 2.877197265625\n",
            "classify 2.880126953125\n",
            "classify 2.871337890625\n",
            "classify 2.864013671875\n",
            "classify 2.802978515625\n",
            "0.0625\n",
            "0.15625\n",
            "0.0625\n",
            "0.109375\n",
            "in vicreg  1.043655164539814 19.921875 0.2578114867210388\n",
            "strain 21.17646598815918\n",
            "in vicreg  0.9055670350790024 20.41015625 0.21640467643737793\n",
            "strain 21.496971130371094\n",
            "in vicreg  0.9507106617093086 20.3125 0.201766699552536\n",
            "strain 21.402477264404297\n",
            "in vicreg  1.038060151040554 20.01953125 0.251539409160614\n",
            "strain 21.28959846496582\n",
            "in vicreg  1.0035591199994087 20.1171875 0.2234313189983368\n",
            "strain 21.35198974609375\n",
            "in vicreg  1.0826094076037407 19.921875 0.2681099474430084\n",
            "strain 21.225719451904297\n",
            "in vicreg  0.8885996416211128 20.3125 0.1827726662158966\n",
            "strain 21.321372985839844\n",
            "in vicreg  0.795894768089056 20.5078125 0.14797021448612213\n",
            "strain 21.443864822387695\n",
            "in vicreg  0.8363238535821438 20.41015625 0.19375132024288177\n",
            "strain 21.405075073242188\n",
            "classify 2.85205078125\n",
            "classify 2.8837890625\n",
            "classify 2.868408203125\n",
            "classify 2.83056640625\n",
            "classify 2.87890625\n",
            "classify 2.828125\n",
            "classify 2.9189453125\n",
            "classify 2.827392578125\n",
            "classify 2.883056640625\n",
            "0.109375\n",
            "0.015625\n",
            "0.125\n",
            "0.15625\n",
            "in vicreg  0.9679600596427917 20.1171875 0.22442050278186798\n",
            "strain 21.317380905151367\n",
            "in vicreg  0.8685562759637833 20.3125 0.20049485564231873\n",
            "strain 21.31905174255371\n",
            "in vicreg  1.0248923674225807 20.1171875 0.23086847364902496\n",
            "strain 21.380762100219727\n",
            "in vicreg  0.8679527789354324 20.41015625 0.17342069745063782\n",
            "strain 21.416372299194336\n",
            "in vicreg  1.059094537049532 20.01953125 0.24030135571956635\n",
            "strain 21.299396514892578\n",
            "in vicreg  0.9249638766050339 20.3125 0.20420430600643158\n",
            "strain 21.379167556762695\n",
            "in vicreg  1.0427734814584255 20.01953125 0.25857600569725037\n",
            "strain 21.301349639892578\n",
            "in vicreg  0.9010168723762035 20.3125 0.19882838428020477\n",
            "strain 21.349843978881836\n",
            "in vicreg  0.8480779826641083 20.5078125 0.1900404691696167\n",
            "strain 21.538118362426758\n",
            "classify 2.864990234375\n",
            "classify 2.896484375\n",
            "classify 2.885498046875\n",
            "classify 2.84326171875\n",
            "classify 2.86376953125\n",
            "classify 2.809326171875\n",
            "classify 2.88720703125\n",
            "classify 2.8115234375\n",
            "classify 2.868408203125\n",
            "0.125\n",
            "0.09375\n",
            "0.0625\n",
            "0.09375\n",
            "in vicreg  0.9666625410318375 20.1171875 0.2203487753868103\n",
            "strain 21.31201171875\n",
            "in vicreg  0.9094228968024254 20.21484375 0.21331298351287842\n",
            "strain 21.37273597717285\n",
            "in vicreg  0.954635813832283 20.1171875 0.22638940811157227\n",
            "strain 21.3060245513916\n",
            "in vicreg  0.8857836946845055 20.3125 0.18422846496105194\n",
            "strain 21.32001304626465\n",
            "in vicreg  0.9144204668700695 20.3125 0.18567915260791779\n",
            "strain 21.350099563598633\n",
            "in vicreg  1.0069923475384712 20.1171875 0.22045235335826874\n",
            "strain 21.352445602416992\n",
            "in vicreg  1.0136757045984268 20.1171875 0.23067425191402435\n",
            "strain 21.36935043334961\n",
            "in vicreg  1.0244892910122871 19.921875 0.25306621193885803\n",
            "strain 21.152555465698242\n",
            "in vicreg  1.0190747678279877 20.01953125 0.23031237721443176\n",
            "strain 21.249387741088867\n",
            "classify 2.845703125\n",
            "classify 2.935546875\n",
            "classify 2.859619140625\n",
            "classify 2.830322265625\n",
            "classify 2.812744140625\n",
            "classify 2.872802734375\n",
            "classify 2.907470703125\n",
            "classify 2.820556640625\n",
            "classify 2.8447265625\n",
            "0.140625\n",
            "0.109375\n",
            "0.046875\n",
            "0.109375\n",
            "in vicreg  0.9374278597533703 20.1171875 0.21530631184577942\n",
            "strain 21.277734756469727\n",
            "in vicreg  0.9163474664092064 20.1171875 0.19983018934726715\n",
            "strain 21.241178512573242\n",
            "in vicreg  0.8118391968309879 20.3125 0.1788155883550644\n",
            "strain 21.24065589904785\n",
            "in vicreg  0.7627641782164574 20.5078125 0.17549937963485718\n",
            "strain 21.438262939453125\n",
            "in vicreg  0.8435359224677086 20.3125 0.18189921975135803\n",
            "strain 21.275436401367188\n",
            "in vicreg  0.8899359963834286 20.3125 0.20329983532428741\n",
            "strain 21.343236923217773\n",
            "in vicreg  1.149173453450203 19.7265625 0.3125841021537781\n",
            "strain 21.21175765991211\n",
            "in vicreg  1.054622046649456 19.921875 0.26992642879486084\n",
            "strain 21.199548721313477\n",
            "in vicreg  1.0004178620874882 20.1171875 0.2291274070739746\n",
            "strain 21.35454559326172\n",
            "classify 2.876708984375\n",
            "classify 2.838134765625\n",
            "classify 2.838134765625\n",
            "classify 2.904296875\n",
            "classify 2.834716796875\n",
            "classify 2.896484375\n",
            "classify 2.853515625\n",
            "classify 2.884033203125\n",
            "classify 2.808349609375\n",
            "0.109375\n",
            "0.078125\n",
            "0.078125\n",
            "0.125\n",
            "in vicreg  1.0489467531442642 19.7265625 0.26337483525276184\n",
            "strain 21.062320709228516\n",
            "in vicreg  0.8218403905630112 20.3125 0.17208565771579742\n",
            "strain 21.243925094604492\n",
            "in vicreg  0.8579923771321774 20.1171875 0.19010809063911438\n",
            "strain 21.173099517822266\n",
            "in vicreg  0.7661033421754837 20.41015625 0.15355034172534943\n",
            "strain 21.294654846191406\n",
            "in vicreg  0.807599164545536 20.3125 0.1675063818693161\n",
            "strain 21.225107192993164\n",
            "in vicreg  0.9649187326431274 20.01953125 0.23716208338737488\n",
            "strain 21.20207977294922\n",
            "in vicreg  0.9565180167555809 20.1171875 0.2326599657535553\n",
            "strain 21.314178466796875\n",
            "in vicreg  1.0018198750913143 19.921875 0.26337307691574097\n",
            "strain 21.14019203186035\n",
            "in vicreg  0.9780015796422958 20.01953125 0.25681936740875244\n",
            "strain 21.234819412231445\n",
            "classify 2.84375\n",
            "classify 2.892578125\n",
            "classify 2.84423828125\n",
            "classify 2.881103515625\n",
            "classify 2.876708984375\n",
            "classify 2.880859375\n",
            "classify 2.838134765625\n",
            "classify 2.827392578125\n",
            "classify 2.883544921875\n",
            "0.15625\n",
            "0.03125\n",
            "0.125\n",
            "0.0625\n",
            "in vicreg  1.0023477487266064 19.921875 0.2403443455696106\n",
            "strain 21.117692947387695\n",
            "in vicreg  0.9078185074031353 20.1171875 0.22076493501663208\n",
            "strain 21.253582000732422\n",
            "in vicreg  0.8496960625052452 20.3125 0.1843220442533493\n",
            "strain 21.28401756286621\n",
            "in vicreg  0.916629284620285 20.1171875 0.21366603672504425\n",
            "strain 21.25529670715332\n",
            "in vicreg  0.8592958562076092 20.3125 0.19080114364624023\n",
            "strain 21.30009651184082\n",
            "in vicreg  0.8743727579712868 20.3125 0.20607472956180573\n",
            "strain 21.330448150634766\n",
            "in vicreg  0.9746500290930271 19.921875 0.25509926676750183\n",
            "strain 21.104747772216797\n",
            "in vicreg  1.0839920490980148 19.82421875 0.2770690619945526\n",
            "strain 21.236061096191406\n",
            "in vicreg  0.9188521653413773 20.21484375 0.21032240986824036\n",
            "strain 21.379175186157227\n",
            "classify 2.855712890625\n",
            "classify 2.822509765625\n",
            "classify 2.865234375\n",
            "classify 2.80859375\n",
            "classify 2.889404296875\n",
            "classify 2.892333984375\n",
            "classify 2.885498046875\n",
            "classify 2.830322265625\n",
            "classify 2.911865234375\n",
            "0.078125\n",
            "0.171875\n",
            "0.09375\n",
            "0.078125\n",
            "in vicreg  0.9570199996232986 19.921875 0.26138100028038025\n",
            "strain 21.093400955200195\n",
            "in vicreg  0.958523340523243 19.921875 0.2509785294532776\n",
            "strain 21.084501266479492\n",
            "in vicreg  0.7443358190357685 20.3125 0.17723575234413147\n",
            "strain 21.171571731567383\n",
            "in vicreg  0.8092515170574188 20.1171875 0.20746812224388123\n",
            "strain 21.141719818115234\n",
            "in vicreg  0.8604167029261589 20.1171875 0.20615985989570618\n",
            "strain 21.19157600402832\n",
            "in vicreg  0.8450958877801895 20.21484375 0.18877992033958435\n",
            "strain 21.283876419067383\n",
            "in vicreg  0.9340441785752773 19.921875 0.22768284380435944\n",
            "strain 21.036725997924805\n",
            "in vicreg  0.9045133367180824 20.21484375 0.21196509897708893\n",
            "strain 21.366479873657227\n",
            "in vicreg  0.9678959846496582 20.1171875 0.22033630311489105\n",
            "strain 21.313232421875\n",
            "classify 2.868896484375\n",
            "classify 2.8037109375\n",
            "classify 2.875732421875\n",
            "classify 2.814697265625\n",
            "classify 2.908935546875\n",
            "classify 2.896728515625\n",
            "classify 2.845458984375\n",
            "classify 2.843994140625\n",
            "classify 2.8828125\n",
            "0.109375\n",
            "0.0625\n",
            "0.078125\n",
            "0.140625\n",
            "in vicreg  1.0071353986859322 19.921875 0.23541216552257538\n",
            "strain 21.1175479888916\n",
            "in vicreg  1.0106181725859642 19.921875 0.25984475016593933\n",
            "strain 21.145462036132812\n",
            "in vicreg  1.0497823357582092 19.921875 0.2639511227607727\n",
            "strain 21.188732147216797\n",
            "in vicreg  1.043161191046238 19.921875 0.2852441668510437\n",
            "strain 21.203405380249023\n",
            "in vicreg  0.8260683156549931 20.21484375 0.19839030504226685\n",
            "strain 21.274459838867188\n",
            "in vicreg  0.7701363414525986 20.3125 0.18041439354419708\n",
            "strain 21.200550079345703\n",
            "in vicreg  0.8329953998327255 20.1171875 0.20993493497371674\n",
            "strain 21.167930603027344\n",
            "in vicreg  0.8617091923952103 20.1171875 0.19188031554222107\n",
            "strain 21.178590774536133\n",
            "in vicreg  0.8775252848863602 20.1171875 0.20171797275543213\n",
            "strain 21.204242706298828\n",
            "classify 2.87158203125\n",
            "classify 2.839599609375\n",
            "classify 2.849609375\n",
            "classify 2.8955078125\n",
            "classify 2.87939453125\n",
            "classify 2.824951171875\n",
            "classify 2.8955078125\n",
            "classify 2.83984375\n",
            "classify 2.86083984375\n",
            "0.03125\n",
            "0.078125\n",
            "0.109375\n",
            "0.140625\n",
            "in vicreg  1.0462031699717045 19.921875 0.2517881989479065\n",
            "strain 21.172992706298828\n",
            "in vicreg  1.024758629500866 19.82421875 0.26997900009155273\n",
            "strain 21.16973876953125\n",
            "in vicreg  0.9061452001333237 20.01953125 0.24838963150978088\n",
            "strain 21.1545352935791\n",
            "in vicreg  0.8698442950844765 20.1171875 0.22562135756015778\n",
            "strain 21.22046661376953\n",
            "in vicreg  0.8558533154428005 20.1171875 0.20875632762908936\n",
            "strain 21.18960952758789\n",
            "in vicreg  0.90982336550951 19.921875 0.23890388011932373\n",
            "strain 21.023725509643555\n",
            "in vicreg  0.8603066205978394 20.1171875 0.20102784037590027\n",
            "strain 21.18633270263672\n",
            "in vicreg  0.8379114791750908 20.21484375 0.18930652737617493\n",
            "strain 21.277217864990234\n",
            "in vicreg  0.8810684084892273 20.1171875 0.23651771247386932\n",
            "strain 21.242586135864258\n",
            "classify 2.867431640625\n",
            "classify 2.840087890625\n",
            "classify 2.864013671875\n",
            "classify 2.854248046875\n",
            "classify 2.8681640625\n",
            "classify 2.879150390625\n",
            "classify 2.8671875\n",
            "classify 2.841064453125\n",
            "classify 2.89013671875\n",
            "0.125\n",
            "0.140625\n",
            "0.109375\n",
            "0.015625\n",
            "in vicreg  0.9376039728522301 19.921875 0.2325861006975174\n",
            "strain 21.045188903808594\n",
            "in vicreg  0.891605019569397 20.1171875 0.22231659293174744\n",
            "strain 21.238922119140625\n",
            "in vicreg  1.0067369788885117 19.921875 0.2694573402404785\n",
            "strain 21.151193618774414\n",
            "in vicreg  1.11014973372221 19.7265625 0.3148084580898285\n",
            "strain 21.174957275390625\n",
            "in vicreg  0.8554600179195404 20.1171875 0.20809945464134216\n",
            "strain 21.18855857849121\n",
            "in vicreg  0.8950549177825451 20.01953125 0.2081582099199295\n",
            "strain 21.103214263916016\n",
            "in vicreg  0.890564639121294 19.921875 0.23137129843235016\n",
            "strain 20.99693489074707\n",
            "in vicreg  0.8466634899377823 20.1171875 0.19830647110939026\n",
            "strain 21.169971466064453\n",
            "in vicreg  0.809028185904026 20.1171875 0.1829318106174469\n",
            "strain 21.116960525512695\n",
            "classify 2.851806640625\n",
            "classify 2.851806640625\n",
            "classify 2.84130859375\n",
            "classify 2.87939453125\n",
            "classify 2.841796875\n",
            "classify 2.90625\n",
            "classify 2.811767578125\n",
            "classify 2.8740234375\n",
            "classify 2.8427734375\n",
            "0.078125\n",
            "0.09375\n",
            "0.109375\n",
            "0.109375\n",
            "in vicreg  0.8739530108869076 19.921875 0.22787505388259888\n",
            "strain 20.97682762145996\n",
            "in vicreg  0.8452471345663071 20.1171875 0.19401636719703674\n",
            "strain 21.164262771606445\n",
            "in vicreg  0.8093570359051228 20.3125 0.19471825659275055\n",
            "strain 21.254074096679688\n",
            "in vicreg  0.9988495148718357 19.921875 0.27311503887176514\n",
            "strain 21.14696502685547\n",
            "in vicreg  1.1212543584406376 19.7265625 0.29293370246887207\n",
            "strain 21.164188385009766\n",
            "in vicreg  1.148368138819933 19.53125 0.32694536447525024\n",
            "strain 20.97531509399414\n",
            "in vicreg  1.0276665911078453 19.7265625 0.30212512612342834\n",
            "strain 21.079792022705078\n",
            "in vicreg  0.9091299958527088 19.921875 0.21690113842487335\n",
            "strain 21.00103187561035\n",
            "in vicreg  0.7367708720266819 20.1171875 0.19648407399654388\n",
            "strain 21.05825424194336\n",
            "classify 2.858154296875\n",
            "classify 2.89404296875\n",
            "classify 2.8642578125\n",
            "classify 2.869384765625\n",
            "classify 2.87939453125\n",
            "classify 2.853271484375\n",
            "classify 2.83447265625\n",
            "classify 2.8349609375\n",
            "classify 2.839599609375\n",
            "0.140625\n",
            "0.078125\n",
            "0.09375\n",
            "0.046875\n",
            "in vicreg  0.7280090823769569 20.21484375 0.16149303317070007\n",
            "strain 21.139501571655273\n",
            "in vicreg  0.6817891728132963 20.3125 0.1566031575202942\n",
            "strain 21.08839225769043\n",
            "in vicreg  0.8076329715549946 20.1171875 0.21285763382911682\n",
            "strain 21.145490646362305\n",
            "in vicreg  0.8783970959484577 19.921875 0.22685539722442627\n",
            "strain 20.980253219604492\n",
            "in vicreg  0.9642681106925011 19.82421875 0.25808408856391907\n",
            "strain 21.09735107421875\n",
            "in vicreg  1.0676078498363495 19.62890625 0.31810057163238525\n",
            "strain 21.01070785522461\n",
            "in vicreg  0.9273839183151722 19.921875 0.22340288758277893\n",
            "strain 21.025785446166992\n",
            "in vicreg  0.9747443720698357 19.7265625 0.28601109981536865\n",
            "strain 21.01075553894043\n",
            "in vicreg  0.8054830133914948 20.21484375 0.19935190677642822\n",
            "strain 21.25483512878418\n",
            "classify 2.846435546875\n",
            "classify 2.850341796875\n",
            "classify 2.83447265625\n",
            "classify 2.869384765625\n",
            "classify 2.8203125\n",
            "classify 2.86279296875\n",
            "classify 2.861328125\n",
            "classify 2.879638671875\n",
            "classify 2.862548828125\n",
            "0.0625\n",
            "0.09375\n",
            "0.125\n",
            "0.0625\n",
            "in vicreg  0.8599244058132172 19.921875 0.24081459641456604\n",
            "strain 20.975738525390625\n",
            "in vicreg  0.8074802346527576 20.3125 0.1834569275379181\n",
            "strain 21.240936279296875\n",
            "in vicreg  0.8872393518686295 20.01953125 0.21465398371219635\n",
            "strain 21.10189437866211\n",
            "in vicreg  0.9640581905841827 19.7265625 0.26383161544799805\n",
            "strain 20.977890014648438\n",
            "in vicreg  0.9528182446956635 19.921875 0.24154409766197205\n",
            "strain 21.06936264038086\n",
            "in vicreg  1.005027163773775 19.7265625 0.2734425961971283\n",
            "strain 21.028470993041992\n",
            "in vicreg  0.9733761660754681 19.82421875 0.2531595826148987\n",
            "strain 21.10153579711914\n",
            "in vicreg  0.929221510887146 19.7265625 0.25119873881340027\n",
            "strain 20.930419921875\n",
            "in vicreg  0.9170060977339745 19.7265625 0.24791501462459564\n",
            "strain 20.914920806884766\n",
            "classify 2.805419921875\n",
            "classify 2.854248046875\n",
            "classify 2.872314453125\n",
            "classify 2.91015625\n",
            "classify 2.810546875\n",
            "classify 2.866943359375\n",
            "classify 2.869873046875\n",
            "classify 2.855224609375\n",
            "classify 2.910888671875\n",
            "0.0625\n",
            "0.125\n",
            "0.078125\n",
            "0.15625\n",
            "in vicreg  0.8363526314496994 20.1171875 0.205739364027977\n",
            "strain 21.16709327697754\n",
            "in vicreg  0.8373597636818886 19.921875 0.23540575802326202\n",
            "strain 20.947765350341797\n",
            "in vicreg  0.8190395310521126 20.01953125 0.22516056895256042\n",
            "strain 21.044200897216797\n",
            "in vicreg  0.7582997437566519 20.1171875 0.22428910434246063\n",
            "strain 21.107587814331055\n",
            "in vicreg  0.8587184362113476 19.921875 0.234120175242424\n",
            "strain 20.967838287353516\n",
            "in vicreg  0.8463747799396515 19.921875 0.24345599114894867\n",
            "strain 20.96483039855957\n",
            "in vicreg  0.865032896399498 19.921875 0.21694914996623993\n",
            "strain 20.956981658935547\n",
            "in vicreg  0.9118221700191498 19.921875 0.26211172342300415\n",
            "strain 21.048933029174805\n",
            "in vicreg  1.0220358148217201 19.7265625 0.27360084652900696\n",
            "strain 21.045637130737305\n",
            "classify 2.877197265625\n",
            "classify 2.89501953125\n",
            "classify 2.797119140625\n",
            "classify 2.873291015625\n",
            "classify 2.796142578125\n",
            "classify 2.852783203125\n",
            "classify 2.8203125\n",
            "classify 2.892333984375\n",
            "classify 2.894775390625\n",
            "0.078125\n",
            "0.078125\n",
            "0.09375\n",
            "0.140625\n",
            "in vicreg  1.0035490617156029 19.7265625 0.2566526234149933\n",
            "strain 21.010202407836914\n",
            "in vicreg  0.8729312568902969 20.1171875 0.22262567281723022\n",
            "strain 21.220556259155273\n",
            "in vicreg  0.9295654483139515 19.921875 0.2432967871427536\n",
            "strain 21.047863006591797\n",
            "in vicreg  1.0099314153194427 19.53125 0.2925737500190735\n",
            "strain 20.802505493164062\n",
            "in vicreg  0.8061382919549942 20.01953125 0.21729028224945068\n",
            "strain 21.02342987060547\n",
            "in vicreg  0.7927573285996914 20.1171875 0.20130132138729095\n",
            "strain 21.11905860900879\n",
            "in vicreg  0.8716096170246601 19.82421875 0.24265627562999725\n",
            "strain 20.98926544189453\n",
            "in vicreg  0.846531055867672 19.921875 0.25620967149734497\n",
            "strain 20.977741241455078\n",
            "in vicreg  0.8402775041759014 19.921875 0.23411700129508972\n",
            "strain 20.94939422607422\n",
            "classify 2.870361328125\n",
            "classify 2.87646484375\n",
            "classify 2.8916015625\n",
            "classify 2.891845703125\n",
            "classify 2.836181640625\n",
            "classify 2.813720703125\n",
            "classify 2.865966796875\n",
            "classify 2.82861328125\n",
            "classify 2.8623046875\n",
            "0.0625\n",
            "0.078125\n",
            "0.15625\n",
            "0.0625\n",
            "in vicreg  0.8665062487125397 19.7265625 0.2635580003261566\n",
            "strain 20.880064010620117\n",
            "in vicreg  0.8773794397711754 19.82421875 0.2372199296951294\n",
            "strain 20.989601135253906\n",
            "in vicreg  0.9470149874687195 19.7265625 0.25606268644332886\n",
            "strain 20.953079223632812\n",
            "in vicreg  0.9338552132248878 19.7265625 0.2565060555934906\n",
            "strain 20.94036102294922\n",
            "in vicreg  0.9693479165434837 19.7265625 0.2703585624694824\n",
            "strain 20.98970603942871\n",
            "in vicreg  0.8671448566019535 19.921875 0.2167917788028717\n",
            "strain 20.95893669128418\n",
            "in vicreg  0.7340563461184502 20.21484375 0.17849183082580566\n",
            "strain 21.162548065185547\n",
            "in vicreg  0.8833416737616062 19.82421875 0.25832733511924744\n",
            "strain 21.01666831970215\n",
            "in vicreg  0.9666771627962589 19.62890625 0.29244959354400635\n",
            "strain 20.884126663208008\n",
            "classify 2.809326171875\n",
            "classify 2.838623046875\n",
            "classify 2.854248046875\n",
            "classify 2.914306640625\n",
            "classify 2.894287109375\n",
            "classify 2.87255859375\n",
            "classify 2.84716796875\n",
            "classify 2.819580078125\n",
            "classify 2.8759765625\n",
            "0.09375\n",
            "0.015625\n",
            "0.078125\n",
            "0.171875\n",
            "in vicreg  0.9795194491744041 19.53125 0.30722367763519287\n",
            "strain 20.7867431640625\n",
            "in vicreg  0.8593933656811714 19.7265625 0.2520996630191803\n",
            "strain 20.861494064331055\n",
            "in vicreg  0.7864506915211678 19.921875 0.21619592607021332\n",
            "strain 20.877647399902344\n",
            "in vicreg  0.7627576589584351 20.1171875 0.18323171138763428\n",
            "strain 21.07098960876465\n",
            "in vicreg  0.848126970231533 19.921875 0.229678213596344\n",
            "strain 20.95280647277832\n",
            "in vicreg  0.8865428157150745 19.921875 0.22534073889255524\n",
            "strain 20.98688316345215\n",
            "in vicreg  0.956210307776928 19.7265625 0.2753490209579468\n",
            "strain 20.98155975341797\n",
            "in vicreg  1.0030439123511314 19.7265625 0.28554627299308777\n",
            "strain 21.038589477539062\n",
            "in vicreg  1.1186294257640839 19.3359375 0.3625880479812622\n",
            "strain 20.856218338012695\n",
            "classify 2.889892578125\n",
            "classify 2.852783203125\n",
            "classify 2.8447265625\n",
            "classify 2.87353515625\n",
            "classify 2.78466796875\n",
            "classify 2.81298828125\n",
            "classify 2.88330078125\n",
            "classify 2.875244140625\n",
            "classify 2.8662109375\n",
            "0.125\n",
            "0.0625\n",
            "0.09375\n",
            "0.15625\n",
            "in vicreg  0.8261633105576038 19.921875 0.2284848988056183\n",
            "strain 20.929649353027344\n",
            "in vicreg  0.7690501399338245 20.1171875 0.20321422815322876\n",
            "strain 21.097265243530273\n",
            "in vicreg  0.8217877708375454 19.7265625 0.2549380660057068\n",
            "strain 20.826725006103516\n",
            "in vicreg  0.7870580069720745 19.921875 0.23577755689620972\n",
            "strain 20.89783477783203\n",
            "in vicreg  0.8288655430078506 19.7265625 0.2586177885532379\n",
            "strain 20.837482452392578\n",
            "in vicreg  0.6941811181604862 20.21484375 0.1825697273015976\n",
            "strain 21.126750946044922\n",
            "in vicreg  0.8627701550722122 19.7265625 0.23898783326148987\n",
            "strain 20.851757049560547\n",
            "in vicreg  0.8375251665711403 19.921875 0.22162339091300964\n",
            "strain 20.934146881103516\n",
            "in vicreg  1.0646423324942589 19.43359375 0.33430373668670654\n",
            "strain 20.773944854736328\n",
            "classify 2.816162109375\n",
            "classify 2.805908203125\n",
            "classify 2.873779296875\n",
            "classify 2.87109375\n",
            "classify 2.830810546875\n",
            "classify 2.900390625\n",
            "classify 2.883544921875\n",
            "classify 2.87451171875\n",
            "classify 2.856689453125\n",
            "0.0625\n",
            "0.140625\n",
            "0.109375\n",
            "0.109375\n",
            "in vicreg  0.9524205699563026 19.7265625 0.27429866790771484\n",
            "strain 20.97671890258789\n",
            "in vicreg  0.9199602529406548 19.7265625 0.2676154375076294\n",
            "strain 20.937576293945312\n",
            "in vicreg  0.8980605751276016 19.7265625 0.2597854733467102\n",
            "strain 20.90784454345703\n",
            "in vicreg  1.0502771474421024 19.3359375 0.3316470980644226\n",
            "strain 20.756925582885742\n",
            "in vicreg  0.7234208285808563 20.1171875 0.18893511593341827\n",
            "strain 21.037355422973633\n",
            "in vicreg  0.8431987836956978 19.7265625 0.24443018436431885\n",
            "strain 20.837629318237305\n",
            "in vicreg  0.7662219926714897 20.01953125 0.21162734925746918\n",
            "strain 20.97784996032715\n",
            "in vicreg  0.804353691637516 19.921875 0.26180824637413025\n",
            "strain 20.941162109375\n",
            "in vicreg  0.8405282162129879 19.7265625 0.2480001300573349\n",
            "strain 20.83852767944336\n",
            "classify 2.868408203125\n",
            "classify 2.889404296875\n",
            "classify 2.856201171875\n",
            "classify 2.828857421875\n",
            "classify 2.8544921875\n",
            "classify 2.834228515625\n",
            "classify 2.856689453125\n",
            "classify 2.8486328125\n",
            "classify 2.87890625\n",
            "0.109375\n",
            "0.109375\n",
            "0.0625\n",
            "0.109375\n",
            "in vicreg  0.8591474965214729 19.62890625 0.2627406120300293\n",
            "strain 20.746889114379883\n",
            "in vicreg  0.855041854083538 19.7265625 0.2516900599002838\n",
            "strain 20.856731414794922\n",
            "in vicreg  0.8655063807964325 19.7265625 0.25135326385498047\n",
            "strain 20.866859436035156\n",
            "in vicreg  0.8844336494803429 19.7265625 0.27265262603759766\n",
            "strain 20.907085418701172\n",
            "in vicreg  0.8567491546273232 19.82421875 0.25690463185310364\n",
            "strain 20.9886531829834\n",
            "in vicreg  0.8103620260953903 19.921875 0.24536359310150146\n",
            "strain 20.93072509765625\n",
            "in vicreg  0.9631557390093803 19.53125 0.29143115878105164\n",
            "strain 20.754587173461914\n",
            "in vicreg  0.9318226017057896 19.53125 0.2847796678543091\n",
            "strain 20.716602325439453\n",
            "in vicreg  0.9359696879982948 19.53125 0.272125244140625\n",
            "strain 20.70809555053711\n",
            "classify 2.883056640625\n",
            "classify 2.8369140625\n",
            "classify 2.837646484375\n",
            "classify 2.870361328125\n",
            "classify 2.86083984375\n",
            "classify 2.855712890625\n",
            "classify 2.84716796875\n",
            "classify 2.8359375\n",
            "classify 2.866455078125\n",
            "0.09375\n",
            "0.0625\n",
            "0.1875\n",
            "0.09375\n",
            "in vicreg  0.9123248979449272 19.53125 0.2725332975387573\n",
            "strain 20.684858322143555\n",
            "in vicreg  0.8367510512471199 19.7265625 0.24854877591133118\n",
            "strain 20.83530044555664\n",
            "in vicreg  0.7078683003783226 20.1171875 0.1994297206401825\n",
            "strain 21.032299041748047\n",
            "in vicreg  0.7218156475573778 20.1171875 0.20451121032238007\n",
            "strain 21.051326751708984\n",
            "in vicreg  0.7276857271790504 20.1171875 0.22315828502178192\n",
            "strain 21.075843811035156\n",
            "in vicreg  0.8700801059603691 19.7265625 0.26923003792762756\n",
            "strain 20.889310836791992\n",
            "in vicreg  0.9611546061933041 19.53125 0.30830875039100647\n",
            "strain 20.76946449279785\n",
            "in vicreg  1.0453525930643082 19.43359375 0.3202662765979767\n",
            "strain 20.740619659423828\n",
            "in vicreg  1.024271734058857 19.53125 0.3063254654407501\n",
            "strain 20.830596923828125\n",
            "classify 2.834228515625\n",
            "classify 2.852783203125\n",
            "classify 2.86767578125\n",
            "classify 2.91357421875\n",
            "classify 2.84130859375\n",
            "classify 2.850341796875\n",
            "classify 2.805908203125\n",
            "classify 2.858154296875\n",
            "classify 2.863037109375\n",
            "0.0625\n",
            "0.171875\n",
            "0.0625\n",
            "0.109375\n",
            "in vicreg  0.8175980299711227 19.82421875 0.2427784949541092\n",
            "strain 20.93537712097168\n",
            "in vicreg  0.8321679197251797 19.62890625 0.2844685912132263\n",
            "strain 20.741636276245117\n",
            "in vicreg  0.7842034101486206 19.7265625 0.2548748552799225\n",
            "strain 20.789077758789062\n",
            "in vicreg  0.7351110689342022 19.921875 0.21122010052204132\n",
            "strain 20.821331024169922\n",
            "in vicreg  0.8401436731219292 19.53125 0.2755212187767029\n",
            "strain 20.615663528442383\n",
            "in vicreg  0.8029669523239136 19.7265625 0.24645133316516876\n",
            "strain 20.79941749572754\n",
            "in vicreg  0.7860209792852402 19.921875 0.2162168323993683\n",
            "strain 20.87723731994629\n",
            "in vicreg  0.7911376655101776 19.921875 0.20776847004890442\n",
            "strain 20.8739070892334\n",
            "in vicreg  0.945042259991169 19.53125 0.3061472773551941\n",
            "strain 20.751188278198242\n",
            "classify 2.876708984375\n",
            "classify 2.844970703125\n",
            "classify 2.850830078125\n",
            "classify 2.833251953125\n",
            "classify 2.916259765625\n",
            "classify 2.834228515625\n",
            "classify 2.88916015625\n",
            "classify 2.816162109375\n",
            "classify 2.860595703125\n",
            "0.09375\n",
            "0.125\n",
            "0.0625\n",
            "0.046875\n",
            "in vicreg  0.8929181843996048 19.7265625 0.257153183221817\n",
            "strain 20.900070190429688\n",
            "in vicreg  0.9380361065268517 19.53125 0.3039054870605469\n",
            "strain 20.741941452026367\n",
            "in vicreg  0.9004427120089531 19.53125 0.2942790687084198\n",
            "strain 20.694721221923828\n",
            "in vicreg  0.9321163408458233 19.53125 0.2862842381000519\n",
            "strain 20.718399047851562\n",
            "in vicreg  0.8463148027658463 19.7265625 0.24964872002601624\n",
            "strain 20.845964431762695\n",
            "in vicreg  0.9181760251522064 19.53125 0.3124660551548004\n",
            "strain 20.730642318725586\n",
            "in vicreg  0.8030528202652931 19.7265625 0.2351725846529007\n",
            "strain 20.788225173950195\n",
            "in vicreg  0.8061152882874012 19.7265625 0.24174094200134277\n",
            "strain 20.7978572845459\n",
            "in vicreg  0.7477688603103161 19.921875 0.23256441950798035\n",
            "strain 20.85533332824707\n",
            "classify 2.813720703125\n",
            "classify 2.85595703125\n",
            "classify 2.859619140625\n",
            "classify 2.857666015625\n",
            "classify 2.8701171875\n",
            "classify 2.842041015625\n",
            "classify 2.841552734375\n",
            "classify 2.864990234375\n",
            "classify 2.88427734375\n",
            "0.078125\n",
            "0.1875\n",
            "0.09375\n",
            "0.0625\n",
            "in vicreg  0.760200247168541 19.921875 0.21740812063217163\n",
            "strain 20.85260772705078\n",
            "in vicreg  0.8431457914412022 19.62890625 0.26479578018188477\n",
            "strain 20.732940673828125\n",
            "in vicreg  0.9755870327353477 19.3359375 0.315194308757782\n",
            "strain 20.665782928466797\n",
            "in vicreg  0.9435507468879223 19.53125 0.3122231960296631\n",
            "strain 20.755773544311523\n",
            "in vicreg  0.8190052583813667 19.7265625 0.26296266913414\n",
            "strain 20.831968307495117\n",
            "in vicreg  0.749822985380888 19.921875 0.2121402472257614\n",
            "strain 20.836963653564453\n",
            "in vicreg  0.8063498884439468 19.7265625 0.2194814383983612\n",
            "strain 20.77583122253418\n",
            "in vicreg  0.9318644180893898 19.53125 0.28627878427505493\n",
            "strain 20.718143463134766\n",
            "in vicreg  0.8806725963950157 19.62890625 0.2637481689453125\n",
            "strain 20.769420623779297\n",
            "classify 2.837646484375\n",
            "classify 2.827880859375\n",
            "classify 2.865966796875\n",
            "classify 2.883544921875\n",
            "classify 2.85595703125\n",
            "classify 2.88037109375\n",
            "classify 2.855224609375\n",
            "classify 2.837890625\n",
            "classify 2.8564453125\n",
            "0.171875\n",
            "0.0625\n",
            "0.078125\n",
            "0.0625\n",
            "in vicreg  0.8284639567136765 19.82421875 0.26104357838630676\n",
            "strain 20.964508056640625\n",
            "in vicreg  0.9800944477319717 19.3359375 0.3272262215614319\n",
            "strain 20.682321548461914\n",
            "in vicreg  0.928550586104393 19.53125 0.2949870228767395\n",
            "strain 20.72353744506836\n",
            "in vicreg  0.8668271824717522 19.53125 0.2610519528388977\n",
            "strain 20.627878189086914\n",
            "in vicreg  0.856848806142807 19.53125 0.2870342433452606\n",
            "strain 20.643884658813477\n",
            "in vicreg  0.8624302223324776 19.53125 0.26818037033081055\n",
            "strain 20.630611419677734\n",
            "in vicreg  0.7458175998181105 19.82421875 0.2136768400669098\n",
            "strain 20.83449363708496\n",
            "in vicreg  0.788077712059021 19.62890625 0.26148709654808044\n",
            "strain 20.6745662689209\n",
            "in vicreg  0.774284265935421 19.7265625 0.271261602640152\n",
            "strain 20.79554557800293\n",
            "classify 2.91552734375\n",
            "classify 2.85498046875\n",
            "classify 2.828857421875\n",
            "classify 2.8359375\n",
            "classify 2.846435546875\n",
            "classify 2.87158203125\n",
            "classify 2.86376953125\n",
            "classify 2.82763671875\n",
            "classify 2.862548828125\n",
            "0.140625\n",
            "0.046875\n",
            "0.078125\n",
            "0.109375\n",
            "in vicreg  0.8284337818622589 19.53125 0.2812100648880005\n",
            "strain 20.609643936157227\n",
            "in vicreg  0.8948860689997673 19.3359375 0.31849390268325806\n",
            "strain 20.588380813598633\n",
            "in vicreg  0.7835305295884609 19.7265625 0.24664762616157532\n",
            "strain 20.78017807006836\n",
            "in vicreg  0.7995168678462505 19.7265625 0.24222852289676666\n",
            "strain 20.791746139526367\n",
            "in vicreg  0.8245548233389854 19.7265625 0.2418016940355301\n",
            "strain 20.816356658935547\n",
            "in vicreg  0.849009957164526 19.53125 0.2838086783885956\n",
            "strain 20.6328182220459\n",
            "in vicreg  0.8013403043150902 19.921875 0.23191151022911072\n",
            "strain 20.90825080871582\n",
            "in vicreg  1.1087298393249512 19.140625 0.384474515914917\n",
            "strain 20.61820411682129\n",
            "in vicreg  0.9067310020327568 19.53125 0.29516029357910156\n",
            "strain 20.70189094543457\n",
            "classify 2.832275390625\n",
            "classify 2.905517578125\n",
            "classify 2.8515625\n",
            "classify 2.8447265625\n",
            "classify 2.835205078125\n",
            "classify 2.86572265625\n",
            "classify 2.83349609375\n",
            "classify 2.8837890625\n",
            "classify 2.853271484375\n",
            "0.09375\n",
            "0.078125\n",
            "0.078125\n",
            "0.140625\n",
            "in vicreg  0.8984750136733055 19.43359375 0.29408591985702515\n",
            "strain 20.567562103271484\n",
            "in vicreg  0.800690334290266 19.53125 0.25557100772857666\n",
            "strain 20.55626106262207\n",
            "in vicreg  0.8218241855502129 19.43359375 0.2787744104862213\n",
            "strain 20.47559928894043\n",
            "in vicreg  0.6813299842178822 19.921875 0.20060433447360992\n",
            "strain 20.756933212280273\n",
            "in vicreg  0.7292568683624268 19.7265625 0.24597662687301636\n",
            "strain 20.725234985351562\n",
            "in vicreg  0.8207270875573158 19.53125 0.2661314308643341\n",
            "strain 20.58685874938965\n",
            "in vicreg  0.8087724447250366 19.7265625 0.2517734169960022\n",
            "strain 20.810546875\n",
            "in vicreg  0.8027334697544575 19.7265625 0.2563591003417969\n",
            "strain 20.809093475341797\n",
            "in vicreg  0.960288755595684 19.3359375 0.31741392612457275\n",
            "strain 20.65270233154297\n",
            "classify 2.84033203125\n",
            "classify 2.850830078125\n",
            "classify 2.857177734375\n",
            "classify 2.85693359375\n",
            "classify 2.881591796875\n",
            "classify 2.860595703125\n",
            "classify 2.826171875\n",
            "classify 2.868408203125\n",
            "classify 2.83349609375\n",
            "0.0625\n",
            "0.171875\n",
            "0.09375\n",
            "0.046875\n",
            "in vicreg  0.9918016381561756 19.23828125 0.3383050560951233\n",
            "strain 20.580106735229492\n",
            "in vicreg  0.9289979934692383 19.3359375 0.3172934055328369\n",
            "strain 20.62129020690918\n",
            "in vicreg  0.8195824921131134 19.53125 0.2614912986755371\n",
            "strain 20.581073760986328\n",
            "in vicreg  0.8119813166558743 19.53125 0.30709534883499146\n",
            "strain 20.619075775146484\n",
            "in vicreg  0.7324582431465387 19.7265625 0.23637212812900543\n",
            "strain 20.718830108642578\n",
            "in vicreg  0.7116908207535744 19.7265625 0.21945488452911377\n",
            "strain 20.6811466217041\n",
            "in vicreg  0.7987285032868385 19.53125 0.25918447971343994\n",
            "strain 20.557912826538086\n",
            "in vicreg  0.832260400056839 19.62890625 0.2530130445957184\n",
            "strain 20.71027374267578\n",
            "in vicreg  0.9731341153383255 19.23828125 0.35508954524993896\n",
            "strain 20.578224182128906\n",
            "classify 2.82373046875\n",
            "classify 2.89501953125\n",
            "classify 2.919921875\n",
            "classify 2.849365234375\n",
            "classify 2.868896484375\n",
            "classify 2.842529296875\n",
            "classify 2.830810546875\n",
            "classify 2.855224609375\n",
            "classify 2.8251953125\n",
            "0.078125\n",
            "0.046875\n",
            "0.140625\n",
            "0.125\n",
            "in vicreg  0.9297795593738556 19.3359375 0.34945252537727356\n",
            "strain 20.654232025146484\n",
            "in vicreg  0.8340153843164444 19.62890625 0.25061267614364624\n",
            "strain 20.709627151489258\n",
            "in vicreg  0.748895388096571 19.7265625 0.2482789009809494\n",
            "strain 20.747175216674805\n",
            "in vicreg  0.8444837294518948 19.53125 0.27525997161865234\n",
            "strain 20.61974334716797\n",
            "in vicreg  0.7652860134840012 19.62890625 0.23977956175804138\n",
            "strain 20.63006591796875\n",
            "in vicreg  0.9028792381286621 19.23828125 0.3436318337917328\n",
            "strain 20.496511459350586\n",
            "in vicreg  0.85584856569767 19.3359375 0.2962903678417206\n",
            "strain 20.527137756347656\n",
            "in vicreg  0.7480479776859283 19.7265625 0.23069070279598236\n",
            "strain 20.72873878479004\n",
            "in vicreg  0.7728853728622198 19.7265625 0.2584258019924164\n",
            "strain 20.781312942504883\n",
            "classify 2.88134765625\n",
            "classify 2.821533203125\n",
            "classify 2.86083984375\n",
            "classify 2.84765625\n",
            "classify 2.880615234375\n",
            "classify 2.8369140625\n",
            "classify 2.83203125\n",
            "classify 2.87060546875\n",
            "classify 2.839599609375\n",
            "0.15625\n",
            "0.0625\n",
            "0.109375\n",
            "0.03125\n",
            "in vicreg  0.8965861052274704 19.3359375 0.337382972240448\n",
            "strain 20.60896873474121\n",
            "in vicreg  0.9097162634134293 19.23828125 0.33286821842193604\n",
            "strain 20.492584228515625\n",
            "in vicreg  0.7379841059446335 19.7265625 0.21838054060935974\n",
            "strain 20.706363677978516\n",
            "in vicreg  0.7737291045486927 19.53125 0.2636423110961914\n",
            "strain 20.537372589111328\n",
            "in vicreg  0.8526382967829704 19.3359375 0.2839001715183258\n",
            "strain 20.511537551879883\n",
            "in vicreg  0.8866475895047188 19.3359375 0.30834680795669556\n",
            "strain 20.569995880126953\n",
            "in vicreg  0.766141340136528 19.7265625 0.22790852189064026\n",
            "strain 20.744050979614258\n",
            "in vicreg  0.8542395196855068 19.43359375 0.3012349009513855\n",
            "strain 20.530475616455078\n",
            "in vicreg  0.8146552368998528 19.53125 0.2692915201187134\n",
            "strain 20.583946228027344\n",
            "classify 2.847900390625\n",
            "classify 2.866455078125\n",
            "classify 2.8271484375\n",
            "classify 2.873046875\n",
            "classify 2.80126953125\n",
            "classify 2.882568359375\n",
            "classify 2.82568359375\n",
            "classify 2.869384765625\n",
            "classify 2.8896484375\n",
            "0.140625\n",
            "0.09375\n",
            "0.09375\n",
            "0.0625\n",
            "in vicreg  0.9383596479892731 19.23828125 0.3141792118549347\n",
            "strain 20.502538681030273\n",
            "in vicreg  0.8637949824333191 19.53125 0.29951590299606323\n",
            "strain 20.663311004638672\n",
            "in vicreg  0.9181849658489227 19.3359375 0.33212485909461975\n",
            "strain 20.625308990478516\n",
            "in vicreg  0.8466159924864769 19.3359375 0.3430236279964447\n",
            "strain 20.564640045166016\n",
            "in vicreg  0.7389878388494253 19.53125 0.2417062669992447\n",
            "strain 20.480693817138672\n",
            "in vicreg  0.6835342384874821 19.7265625 0.21371835470199585\n",
            "strain 20.647253036499023\n",
            "in vicreg  0.7242872845381498 19.53125 0.26826971769332886\n",
            "strain 20.492557525634766\n",
            "in vicreg  0.765285175293684 19.62890625 0.23583941161632538\n",
            "strain 20.62612533569336\n",
            "in vicreg  0.7705113850533962 19.53125 0.26997309923171997\n",
            "strain 20.540485382080078\n",
            "classify 2.810546875\n",
            "classify 2.844970703125\n",
            "classify 2.83154296875\n",
            "classify 2.89404296875\n",
            "classify 2.831787109375\n",
            "classify 2.855224609375\n",
            "classify 2.8779296875\n",
            "classify 2.843017578125\n",
            "classify 2.86279296875\n",
            "0.109375\n",
            "0.140625\n",
            "0.109375\n",
            "0.03125\n",
            "in vicreg  0.8753914386034012 19.3359375 0.3176116347312927\n",
            "strain 20.568002700805664\n",
            "in vicreg  0.9519455954432487 19.140625 0.3498416543006897\n",
            "strain 20.426788330078125\n",
            "in vicreg  0.9320656768977642 19.3359375 0.32869982719421387\n",
            "strain 20.635765075683594\n",
            "in vicreg  0.8636590093374252 19.3359375 0.30928727984428406\n",
            "strain 20.54794692993164\n",
            "in vicreg  0.8178850635886192 19.53125 0.3242259919643402\n",
            "strain 20.64211082458496\n",
            "in vicreg  0.7861558347940445 19.62890625 0.2436109185218811\n",
            "strain 20.654766082763672\n",
            "in vicreg  0.7645242381840944 19.53125 0.2602665424346924\n",
            "strain 20.524791717529297\n",
            "in vicreg  0.6581451278179884 19.921875 0.1991463154554367\n",
            "strain 20.73229217529297\n",
            "in vicreg  0.859734695404768 19.3359375 0.3144751489162445\n",
            "strain 20.549211502075195\n",
            "classify 2.87158203125\n",
            "classify 2.817138671875\n",
            "classify 2.816650390625\n",
            "classify 2.896728515625\n",
            "classify 2.84765625\n",
            "classify 2.89990234375\n",
            "classify 2.842041015625\n",
            "classify 2.859130859375\n",
            "classify 2.8173828125\n",
            "0.0625\n",
            "0.125\n",
            "0.078125\n",
            "0.125\n",
            "in vicreg  0.7861880585551262 19.53125 0.2709939777851105\n",
            "strain 20.55718231201172\n",
            "in vicreg  0.8502368815243244 19.3359375 0.2997950315475464\n",
            "strain 20.52503204345703\n",
            "in vicreg  0.8775386027991772 19.3359375 0.28421473503112793\n",
            "strain 20.536752700805664\n",
            "in vicreg  0.8085715584456921 19.53125 0.30908864736557007\n",
            "strain 20.617658615112305\n",
            "in vicreg  0.8502902463078499 19.3359375 0.29960983991622925\n",
            "strain 20.524900436401367\n",
            "in vicreg  0.9907227009534836 19.04296875 0.3847373127937317\n",
            "strain 20.375459671020508\n",
            "in vicreg  0.8022378198802471 19.53125 0.2736818790435791\n",
            "strain 20.57592010498047\n",
            "in vicreg  0.8828108198940754 19.140625 0.3387226164340973\n",
            "strain 20.346532821655273\n",
            "in vicreg  0.7191644515842199 19.53125 0.24785006046295166\n",
            "strain 20.46701431274414\n",
            "classify 2.85888671875\n",
            "classify 2.8466796875\n",
            "classify 2.855224609375\n",
            "classify 2.880126953125\n",
            "classify 2.837158203125\n",
            "classify 2.859619140625\n",
            "classify 2.838134765625\n",
            "classify 2.83544921875\n",
            "classify 2.83740234375\n",
            "0.046875\n",
            "0.109375\n",
            "0.125\n",
            "0.09375\n",
            "in vicreg  0.7319750264286995 19.53125 0.2512391209602356\n",
            "strain 20.48321533203125\n",
            "in vicreg  0.7504408247768879 19.3359375 0.2775523364543915\n",
            "strain 20.402992248535156\n",
            "in vicreg  0.6913953460752964 19.62890625 0.2432381510734558\n",
            "strain 20.559633255004883\n",
            "in vicreg  0.7995994761586189 19.3359375 0.32301706075668335\n",
            "strain 20.497615814208984\n",
            "in vicreg  0.8052004501223564 19.3359375 0.27433910965919495\n",
            "strain 20.454540252685547\n",
            "in vicreg  0.8108282461762428 19.53125 0.27331528067588806\n",
            "strain 20.584144592285156\n",
            "in vicreg  0.8521456271409988 19.3359375 0.3205230236053467\n",
            "strain 20.54766845703125\n",
            "in vicreg  0.9981943294405937 18.9453125 0.39884087443351746\n",
            "strain 20.39703369140625\n",
            "in vicreg  0.774991512298584 19.43359375 0.28260254859924316\n",
            "strain 20.432594299316406\n",
            "classify 2.86083984375\n",
            "classify 2.852783203125\n",
            "classify 2.84326171875\n",
            "classify 2.84521484375\n",
            "classify 2.837158203125\n",
            "classify 2.8681640625\n",
            "classify 2.8486328125\n",
            "classify 2.864501953125\n",
            "classify 2.869873046875\n",
            "0.0625\n",
            "0.0625\n",
            "0.140625\n",
            "0.15625\n",
            "in vicreg  0.8071229793131351 19.53125 0.2539994716644287\n",
            "strain 20.56112289428711\n",
            "in vicreg  0.8921036496758461 19.3359375 0.3344559669494629\n",
            "strain 20.601558685302734\n",
            "in vicreg  0.8187303319573402 19.3359375 0.2661550045013428\n",
            "strain 20.459884643554688\n",
            "in vicreg  0.7033660542219877 19.62890625 0.23639488220214844\n",
            "strain 20.564760208129883\n",
            "in vicreg  0.7180702872574329 19.62890625 0.2361210584640503\n",
            "strain 20.579191207885742\n",
            "in vicreg  0.7594976108521223 19.53125 0.2680603861808777\n",
            "strain 20.527557373046875\n",
            "in vicreg  0.8965766988694668 19.140625 0.36451730132102966\n",
            "strain 20.386093139648438\n",
            "in vicreg  0.8695087395608425 19.3359375 0.3028930723667145\n",
            "strain 20.547401428222656\n",
            "in vicreg  0.9623918682336807 19.04296875 0.37870943546295166\n",
            "strain 20.341100692749023\n",
            "classify 2.845947265625\n",
            "classify 2.81640625\n",
            "classify 2.836669921875\n",
            "classify 2.89013671875\n",
            "classify 2.81982421875\n",
            "classify 2.844482421875\n",
            "classify 2.88818359375\n",
            "classify 2.882568359375\n",
            "classify 2.849609375\n",
            "0.140625\n",
            "0.046875\n",
            "0.078125\n",
            "0.1875\n",
            "in vicreg  0.7965598255395889 19.3359375 0.30443042516708374\n",
            "strain 20.475990295410156\n",
            "in vicreg  0.7750918157398701 19.43359375 0.27023157477378845\n",
            "strain 20.42032241821289\n",
            "in vicreg  0.8144601248204708 19.3359375 0.30708396434783936\n",
            "strain 20.496543884277344\n",
            "in vicreg  0.6802272517234087 19.53125 0.26415395736694336\n",
            "strain 20.444381713867188\n",
            "in vicreg  0.7337545044720173 19.53125 0.2473403513431549\n",
            "strain 20.481096267700195\n",
            "in vicreg  0.727291963994503 19.53125 0.24903567135334015\n",
            "strain 20.47632598876953\n",
            "in vicreg  0.8539541624486446 19.23828125 0.3408627212047577\n",
            "strain 20.44481658935547\n",
            "in vicreg  0.9533537551760674 19.140625 0.39326369762420654\n",
            "strain 20.471616744995117\n",
            "in vicreg  0.8815579116344452 19.3359375 0.3052906095981598\n",
            "strain 20.561847686767578\n",
            "classify 2.86474609375\n",
            "classify 2.86572265625\n",
            "classify 2.8564453125\n",
            "classify 2.820068359375\n",
            "classify 2.86328125\n",
            "classify 2.833984375\n",
            "classify 2.837890625\n",
            "classify 2.844482421875\n",
            "classify 2.857666015625\n",
            "0.046875\n",
            "0.125\n",
            "0.09375\n",
            "0.109375\n",
            "in vicreg  0.8149986155331135 19.3359375 0.29083988070487976\n",
            "strain 20.480838775634766\n",
            "in vicreg  0.7993043400347233 19.3359375 0.30452871322631836\n",
            "strain 20.47883415222168\n",
            "in vicreg  0.7129322737455368 19.43359375 0.2671629786491394\n",
            "strain 20.35509490966797\n",
            "in vicreg  0.6719789933413267 19.7265625 0.22812169790267944\n",
            "strain 20.650100708007812\n",
            "in vicreg  0.7656755391508341 19.3359375 0.3057841956615448\n",
            "strain 20.44645881652832\n",
            "in vicreg  0.8121520280838013 19.3359375 0.31337422132492065\n",
            "strain 20.500526428222656\n",
            "in vicreg  0.9382640011608601 18.9453125 0.3889520466327667\n",
            "strain 20.32721710205078\n",
            "in vicreg  0.8171470835804939 19.3359375 0.31842029094696045\n",
            "strain 20.51056671142578\n",
            "in vicreg  0.8124921470880508 19.3359375 0.26997941732406616\n",
            "strain 20.45747184753418\n",
            "classify 2.851318359375\n",
            "classify 2.84912109375\n",
            "classify 2.832763671875\n",
            "classify 2.916015625\n",
            "classify 2.833984375\n",
            "classify 2.868896484375\n",
            "classify 2.83984375\n",
            "classify 2.822021484375\n",
            "classify 2.855712890625\n",
            "0.09375\n",
            "0.078125\n",
            "0.109375\n",
            "0.078125\n",
            "in vicreg  0.7893988862633705 19.3359375 0.2755359709262848\n",
            "strain 20.43993377685547\n",
            "in vicreg  0.76048425398767 19.3359375 0.29100173711776733\n",
            "strain 20.42648696899414\n",
            "in vicreg  0.7998354732990265 19.3359375 0.29747945070266724\n",
            "strain 20.472314834594727\n",
            "in vicreg  0.8856114000082016 18.9453125 0.36402061581611633\n",
            "strain 20.249631881713867\n",
            "in vicreg  0.7533639669418335 19.3359375 0.30485180020332336\n",
            "strain 20.433216094970703\n",
            "in vicreg  0.7187757175415754 19.53125 0.2609390616416931\n",
            "strain 20.479713439941406\n",
            "in vicreg  0.7721064612269402 19.3359375 0.3085096776485443\n",
            "strain 20.455615997314453\n",
            "in vicreg  0.7371659856289625 19.53125 0.26826828718185425\n",
            "strain 20.505434036254883\n",
            "in vicreg  0.8637405000627041 19.140625 0.3556250333786011\n",
            "strain 20.3443660736084\n",
            "classify 2.828369140625\n",
            "classify 2.819091796875\n",
            "classify 2.855712890625\n",
            "classify 2.87548828125\n",
            "classify 2.8662109375\n",
            "classify 2.838623046875\n",
            "classify 2.83349609375\n",
            "classify 2.9091796875\n",
            "classify 2.80908203125\n",
            "0.078125\n",
            "0.109375\n",
            "0.109375\n",
            "0.109375\n",
            "in vicreg  0.7371818646788597 19.53125 0.2491832971572876\n",
            "strain 20.486366271972656\n",
            "in vicreg  0.851958803832531 19.140625 0.3400704860687256\n",
            "strain 20.31702995300293\n",
            "in vicreg  0.9106475859880447 19.140625 0.32981717586517334\n",
            "strain 20.36546516418457\n",
            "in vicreg  0.9383227676153183 18.84765625 0.40687814354896545\n",
            "strain 20.220199584960938\n",
            "in vicreg  0.8186560124158859 19.04296875 0.34196653962135315\n",
            "strain 20.16062355041504\n",
            "in vicreg  0.6351216696202755 19.62890625 0.21891933679580688\n",
            "strain 20.479042053222656\n",
            "in vicreg  0.6986901164054871 19.43359375 0.2648518681526184\n",
            "strain 20.338542938232422\n",
            "in vicreg  0.7044297643005848 19.3359375 0.2724156677722931\n",
            "strain 20.351844787597656\n",
            "in vicreg  0.6767669226974249 19.53125 0.24777092039585114\n",
            "strain 20.424537658691406\n",
            "classify 2.83935546875\n",
            "classify 2.859619140625\n",
            "classify 2.862060546875\n",
            "classify 2.87744140625\n",
            "classify 2.850830078125\n",
            "classify 2.84912109375\n",
            "classify 2.8681640625\n",
            "classify 2.847412109375\n",
            "classify 2.825439453125\n",
            "0.09375\n",
            "0.125\n",
            "0.0625\n",
            "0.109375\n",
            "in vicreg  0.8135920390486717 19.140625 0.3433307707309723\n",
            "strain 20.281923294067383\n",
            "in vicreg  0.8805803023278713 19.140625 0.3308488130569458\n",
            "strain 20.336429595947266\n",
            "in vicreg  0.8698092773556709 19.140625 0.3514973521232605\n",
            "strain 20.3463077545166\n",
            "in vicreg  0.8865167386829853 19.140625 0.36194419860839844\n",
            "strain 20.37346076965332\n",
            "in vicreg  0.8862188085913658 18.9453125 0.3638876676559448\n",
            "strain 20.250106811523438\n",
            "in vicreg  0.8242921903729439 19.140625 0.32426533102989197\n",
            "strain 20.273555755615234\n",
            "in vicreg  0.671056006103754 19.53125 0.2230425775051117\n",
            "strain 20.394100189208984\n",
            "in vicreg  0.7417627144604921 19.3359375 0.3070083260536194\n",
            "strain 20.423770904541016\n",
            "in vicreg  0.74244593270123 19.3359375 0.2886706292629242\n",
            "strain 20.406118392944336\n",
            "classify 2.889892578125\n",
            "classify 2.835693359375\n",
            "classify 2.817626953125\n",
            "classify 2.8095703125\n",
            "classify 2.85595703125\n",
            "classify 2.86572265625\n",
            "classify 2.796142578125\n",
            "classify 2.856201171875\n",
            "classify 2.89306640625\n",
            "0.109375\n",
            "0.125\n",
            "0.078125\n",
            "0.09375\n",
            "in vicreg  0.8288497105240822 19.04296875 0.3730827271938324\n",
            "strain 20.201932907104492\n",
            "in vicreg  0.6994006223976612 19.43359375 0.26090267300605774\n",
            "strain 20.335302352905273\n",
            "in vicreg  0.7403239607810974 19.140625 0.3184207081794739\n",
            "strain 20.183744430541992\n",
            "in vicreg  0.7773086428642273 19.23828125 0.3072703778743744\n",
            "strain 20.334579467773438\n",
            "in vicreg  0.8711310103535652 18.9453125 0.35746294260025024\n",
            "strain 20.228595733642578\n",
            "in vicreg  0.7544505409896374 19.53125 0.29493284225463867\n",
            "strain 20.54938316345215\n",
            "in vicreg  0.777471624314785 19.3359375 0.2921864688396454\n",
            "strain 20.444658279418945\n",
            "in vicreg  0.8053390309214592 19.3359375 0.3157176077365875\n",
            "strain 20.496057510375977\n",
            "in vicreg  0.7825108245015144 19.23828125 0.28589320182800293\n",
            "strain 20.318403244018555\n",
            "classify 2.84716796875\n",
            "classify 2.8515625\n",
            "classify 2.840576171875\n",
            "classify 2.870849609375\n",
            "classify 2.84228515625\n",
            "classify 2.8134765625\n",
            "classify 2.808837890625\n",
            "classify 2.843505859375\n",
            "classify 2.896240234375\n",
            "0.09375\n",
            "0.109375\n",
            "0.125\n",
            "0.03125\n",
            "in vicreg  0.8720736019313335 18.9453125 0.34466493129730225\n",
            "strain 20.216739654541016\n",
            "in vicreg  0.753121031448245 19.3359375 0.26644185185432434\n",
            "strain 20.394561767578125\n",
            "in vicreg  0.7888448424637318 19.140625 0.31299006938934326\n",
            "strain 20.226835250854492\n",
            "in vicreg  0.8656935766339302 18.9453125 0.4012390077114105\n",
            "strain 20.26693344116211\n",
            "in vicreg  0.7813752628862858 19.3359375 0.2781827747821808\n",
            "strain 20.434558868408203\n",
            "in vicreg  0.7882059551775455 19.140625 0.33266034722328186\n",
            "strain 20.245866775512695\n",
            "in vicreg  0.8402835577726364 18.9453125 0.3705577850341797\n",
            "strain 20.21084213256836\n",
            "in vicreg  0.7734113372862339 19.04296875 0.3380029797554016\n",
            "strain 20.111413955688477\n",
            "in vicreg  0.6816609762609005 19.3359375 0.2767704129219055\n",
            "strain 20.333431243896484\n",
            "classify 2.80419921875\n",
            "classify 2.895263671875\n",
            "classify 2.86669921875\n",
            "classify 2.818359375\n",
            "classify 2.86474609375\n",
            "classify 2.874267578125\n",
            "classify 2.835693359375\n",
            "classify 2.87353515625\n",
            "classify 2.8251953125\n",
            "0.09375\n",
            "0.109375\n",
            "0.078125\n",
            "0.09375\n",
            "in vicreg  0.6546729244291782 19.53125 0.2482222616672516\n",
            "strain 20.402894973754883\n",
            "in vicreg  0.690809078514576 19.3359375 0.25472134351730347\n",
            "strain 20.32052993774414\n",
            "in vicreg  0.8536158129572868 18.9453125 0.3649478256702423\n",
            "strain 20.218564987182617\n",
            "in vicreg  0.8340208791196346 19.04296875 0.34895601868629456\n",
            "strain 20.18297576904297\n",
            "in vicreg  0.780223635956645 19.140625 0.3109152913093567\n",
            "strain 20.21613883972168\n",
            "in vicreg  0.8244527503848076 19.140625 0.3205578029155731\n",
            "strain 20.27001190185547\n",
            "in vicreg  0.876791775226593 18.75 0.40101519227027893\n",
            "strain 20.02780532836914\n",
            "in vicreg  0.8559998124837875 18.75 0.39638832211494446\n",
            "strain 20.00238800048828\n",
            "in vicreg  0.5911628715693951 19.921875 0.19782666862010956\n",
            "strain 20.663990020751953\n",
            "classify 2.835205078125\n",
            "classify 2.822021484375\n",
            "classify 2.8818359375\n",
            "classify 2.895263671875\n",
            "classify 2.8447265625\n",
            "classify 2.841552734375\n",
            "classify 2.80078125\n",
            "classify 2.82373046875\n",
            "classify 2.86962890625\n",
            "0.109375\n",
            "0.0625\n",
            "0.171875\n",
            "0.0625\n",
            "in vicreg  0.7593410089612007 19.04296875 0.3463294208049774\n",
            "strain 20.105669021606445\n",
            "in vicreg  0.772616546601057 19.04296875 0.3275282382965088\n",
            "strain 20.10014533996582\n",
            "in vicreg  0.7376466877758503 19.140625 0.3015322685241699\n",
            "strain 20.1641788482666\n",
            "in vicreg  0.7607902400195599 19.140625 0.3200123906135559\n",
            "strain 20.20580291748047\n",
            "in vicreg  0.7275540381669998 19.140625 0.3232596218585968\n",
            "strain 20.175813674926758\n",
            "in vicreg  0.7616179995238781 19.23828125 0.2899908423423767\n",
            "strain 20.30160903930664\n",
            "in vicreg  0.8168790489435196 19.140625 0.31745362281799316\n",
            "strain 20.25933265686035\n",
            "in vicreg  0.8114529773592949 19.140625 0.29320719838142395\n",
            "strain 20.229660034179688\n",
            "in vicreg  0.8656606078147888 19.04296875 0.33841559290885925\n",
            "strain 20.20407485961914\n",
            "classify 2.864990234375\n",
            "classify 2.824951171875\n",
            "classify 2.841552734375\n",
            "classify 2.82421875\n",
            "classify 2.8759765625\n",
            "classify 2.809326171875\n",
            "classify 2.83349609375\n",
            "classify 2.872802734375\n",
            "classify 2.88134765625\n",
            "0.109375\n",
            "0.09375\n",
            "0.078125\n",
            "0.140625\n",
            "in vicreg  0.8499800227582455 19.04296875 0.358209490776062\n",
            "strain 20.208189010620117\n",
            "in vicreg  0.7338645402342081 19.3359375 0.3070164918899536\n",
            "strain 20.41588020324707\n",
            "in vicreg  0.8850048296153545 18.9453125 0.3864057958126068\n",
            "strain 20.27140998840332\n",
            "in vicreg  0.8515044115483761 19.04296875 0.3461815118789673\n",
            "strain 20.19768714904785\n",
            "in vicreg  0.725206732749939 19.140625 0.2817114591598511\n",
            "strain 20.13191795349121\n",
            "in vicreg  0.8488144725561142 18.5546875 0.4527905583381653\n",
            "strain 19.801605224609375\n",
            "in vicreg  0.6447922438383102 19.3359375 0.2815823554992676\n",
            "strain 20.301374435424805\n",
            "in vicreg  0.6095202639698982 19.3359375 0.24858665466308594\n",
            "strain 20.23310661315918\n",
            "in vicreg  0.6633184384554625 19.3359375 0.2740190625190735\n",
            "strain 20.31233787536621\n",
            "classify 2.857421875\n",
            "classify 2.87255859375\n",
            "classify 2.830322265625\n",
            "classify 2.8447265625\n",
            "classify 2.891845703125\n",
            "classify 2.81689453125\n",
            "classify 2.826171875\n",
            "classify 2.831298828125\n",
            "classify 2.844482421875\n",
            "0.125\n",
            "0.140625\n",
            "0.0625\n",
            "0.109375\n",
            "in vicreg  0.5910143721848726 19.7265625 0.22733815014362335\n",
            "strain 20.5683536529541\n",
            "in vicreg  0.8018545806407928 18.9453125 0.367641806602478\n",
            "strain 20.169496536254883\n",
            "in vicreg  0.9264027699828148 18.75 0.39905139803886414\n",
            "strain 20.075454711914062\n",
            "in vicreg  0.8291471749544144 19.140625 0.32044798135757446\n",
            "strain 20.274595260620117\n",
            "in vicreg  0.9320922195911407 18.65234375 0.4595714509487152\n",
            "strain 20.016664505004883\n",
            "in vicreg  0.8901745080947876 18.75 0.405017614364624\n",
            "strain 20.04519271850586\n",
            "in vicreg  0.6850844714790583 19.3359375 0.2639055550098419\n",
            "strain 20.323991775512695\n",
            "in vicreg  0.7474761921912432 18.9453125 0.33136194944381714\n",
            "strain 20.078838348388672\n",
            "in vicreg  0.6876340601593256 19.140625 0.29359012842178345\n",
            "strain 20.106224060058594\n",
            "classify 2.829833984375\n",
            "classify 2.847412109375\n",
            "classify 2.864990234375\n",
            "classify 2.876708984375\n",
            "classify 2.878173828125\n",
            "classify 2.865966796875\n",
            "classify 2.807861328125\n",
            "classify 2.82275390625\n",
            "classify 2.86376953125\n",
            "0.046875\n",
            "0.0625\n",
            "0.125\n",
            "0.171875\n",
            "in vicreg  0.6319003645330667 19.3359375 0.2604559659957886\n",
            "strain 20.267356872558594\n",
            "in vicreg  0.607289420440793 19.53125 0.23445948958396912\n",
            "strain 20.34174919128418\n",
            "in vicreg  0.7894683629274368 18.9453125 0.3782404661178589\n",
            "strain 20.167709350585938\n",
            "in vicreg  0.759192556142807 19.23828125 0.3066249191761017\n",
            "strain 20.315818786621094\n",
            "in vicreg  0.9045501239597797 18.65234375 0.46852338314056396\n",
            "strain 19.99807357788086\n",
            "in vicreg  0.8703600615262985 18.75 0.3899633586406708\n",
            "strain 20.01032257080078\n",
            "in vicreg  0.8110186085104942 18.9453125 0.34539464116096497\n",
            "strain 20.15641212463379\n",
            "in vicreg  0.6279821507632732 19.53125 0.22637993097305298\n",
            "strain 20.35436248779297\n",
            "in vicreg  0.7776732556521893 19.04296875 0.33125579357147217\n",
            "strain 20.108928680419922\n",
            "classify 2.823974609375\n",
            "classify 2.888427734375\n",
            "classify 2.815185546875\n",
            "classify 2.88330078125\n",
            "classify 2.841064453125\n",
            "classify 2.845703125\n",
            "classify 2.916748046875\n",
            "classify 2.837158203125\n",
            "classify 2.797607421875\n",
            "0.109375\n",
            "0.046875\n",
            "0.1875\n",
            "0.109375\n",
            "in vicreg  0.7422552444040775 19.04296875 0.31587743759155273\n",
            "strain 20.058134078979492\n",
            "in vicreg  0.8528620004653931 18.65234375 0.39871132373809814\n",
            "strain 19.87657356262207\n",
            "in vicreg  0.7792562246322632 18.9453125 0.34305867552757263\n",
            "strain 20.122316360473633\n",
            "in vicreg  0.6373228039592505 19.43359375 0.26523277163505554\n",
            "strain 20.277555465698242\n",
            "in vicreg  0.6859730929136276 19.3359375 0.2959330081939697\n",
            "strain 20.356904983520508\n",
            "in vicreg  0.7774033583700657 19.04296875 0.36952054500579834\n",
            "strain 20.146923065185547\n",
            "in vicreg  0.7935561239719391 18.9453125 0.37718403339385986\n",
            "strain 20.170740127563477\n",
            "in vicreg  0.8613178506493568 18.75 0.40730321407318115\n",
            "strain 20.01862144470215\n",
            "in vicreg  0.6714911665767431 19.3359375 0.262719988822937\n",
            "strain 20.30921173095703\n",
            "classify 2.843994140625\n",
            "classify 2.822021484375\n",
            "classify 2.84228515625\n",
            "classify 2.85986328125\n",
            "classify 2.802490234375\n",
            "classify 2.86279296875\n",
            "classify 2.841552734375\n",
            "classify 2.87109375\n",
            "classify 2.875732421875\n",
            "0.046875\n",
            "0.09375\n",
            "0.125\n",
            "0.140625\n",
            "in vicreg  0.7341169752180576 19.04296875 0.3128408193588257\n",
            "strain 20.046958923339844\n",
            "in vicreg  0.6837563589215279 19.3359375 0.29957324266433716\n",
            "strain 20.35832977294922\n",
            "in vicreg  0.7631177548319101 18.9453125 0.337910532951355\n",
            "strain 20.10102653503418\n",
            "in vicreg  0.8228384889662266 18.75 0.3855665624141693\n",
            "strain 19.958404541015625\n",
            "in vicreg  0.7232361938804388 19.140625 0.3155795931816101\n",
            "strain 20.163816452026367\n",
            "in vicreg  0.8560171350836754 18.75 0.3950141370296478\n",
            "strain 20.00103187561035\n",
            "in vicreg  0.7590196561068296 18.9453125 0.31353139877319336\n",
            "strain 20.072551727294922\n",
            "in vicreg  0.7151768542826176 19.140625 0.3126554489135742\n",
            "strain 20.15283203125\n",
            "in vicreg  0.7359723560512066 19.140625 0.303499698638916\n",
            "strain 20.164470672607422\n",
            "classify 2.88818359375\n",
            "classify 2.82666015625\n",
            "classify 2.8212890625\n",
            "classify 2.826904296875\n",
            "classify 2.863525390625\n",
            "classify 2.878662109375\n",
            "classify 2.84912109375\n",
            "classify 2.854736328125\n",
            "classify 2.82666015625\n",
            "0.0625\n",
            "0.0625\n",
            "0.125\n",
            "0.109375\n",
            "in vicreg  0.8285881951451302 18.75 0.439431756734848\n",
            "strain 20.018020629882812\n",
            "in vicreg  0.7161610759794712 19.140625 0.34632956981658936\n",
            "strain 20.187490463256836\n",
            "in vicreg  0.7089375518262386 19.140625 0.2937085032463074\n",
            "strain 20.12764549255371\n",
            "in vicreg  0.6858654320240021 19.140625 0.3061998784542084\n",
            "strain 20.1170654296875\n",
            "in vicreg  0.7161149755120277 19.23828125 0.2675447165966034\n",
            "strain 20.233657836914062\n",
            "in vicreg  0.7744302041828632 19.04296875 0.3432755768299103\n",
            "strain 20.117704391479492\n",
            "in vicreg  0.8674167096614838 18.75 0.3690297305583954\n",
            "strain 19.986446380615234\n",
            "in vicreg  0.8880789391696453 18.5546875 0.4464375972747803\n",
            "strain 19.834516525268555\n",
            "in vicreg  0.7599773351103067 18.9453125 0.32804572582244873\n",
            "strain 20.088022232055664\n",
            "classify 2.838134765625\n",
            "classify 2.83154296875\n",
            "classify 2.832275390625\n",
            "classify 2.888916015625\n",
            "classify 2.82958984375\n",
            "classify 2.840087890625\n",
            "classify 2.836669921875\n",
            "classify 2.878173828125\n",
            "classify 2.83349609375\n",
            "0.078125\n",
            "0.140625\n",
            "0.078125\n",
            "0.078125\n",
            "in vicreg  0.7625395897775888 18.75 0.3573080599308014\n",
            "strain 19.86984634399414\n",
            "in vicreg  0.6913138553500175 19.140625 0.31229522824287415\n",
            "strain 20.128610610961914\n",
            "in vicreg  0.725267082452774 18.84765625 0.33031168580055237\n",
            "strain 19.930578231811523\n",
            "in vicreg  0.709667382761836 19.140625 0.30022403597831726\n",
            "strain 20.134891510009766\n",
            "in vicreg  0.7758505642414093 18.84765625 0.3771434426307678\n",
            "strain 20.02799415588379\n",
            "in vicreg  0.7606927305459976 18.9453125 0.3576893210411072\n",
            "strain 20.11838150024414\n",
            "in vicreg  0.7201576139777899 19.140625 0.3089298605918884\n",
            "strain 20.15408706665039\n",
            "in vicreg  0.7166345603764057 19.140625 0.31992867588996887\n",
            "strain 20.161563873291016\n",
            "in vicreg  0.7498369086533785 18.9453125 0.35650312900543213\n",
            "strain 20.106338500976562\n",
            "classify 2.852783203125\n",
            "classify 2.853759765625\n",
            "classify 2.830078125\n",
            "classify 2.862060546875\n",
            "classify 2.805908203125\n",
            "classify 2.826904296875\n",
            "classify 2.875732421875\n",
            "classify 2.843994140625\n",
            "classify 2.8466796875\n",
            "0.09375\n",
            "0.078125\n",
            "0.171875\n",
            "0.09375\n",
            "in vicreg  0.7629407569766045 18.9453125 0.36000674962997437\n",
            "strain 20.122947692871094\n",
            "in vicreg  0.738447904586792 19.140625 0.30525022745132446\n",
            "strain 20.168697357177734\n",
            "in vicreg  0.8293179795145988 18.9453125 0.36110159754753113\n",
            "strain 20.190418243408203\n",
            "in vicreg  0.7734375074505806 18.9453125 0.33271074295043945\n",
            "strain 20.10614776611328\n",
            "in vicreg  0.8285701274871826 18.65234375 0.39408910274505615\n",
            "strain 19.847658157348633\n",
            "in vicreg  0.7983295246958733 18.75 0.3762402832508087\n",
            "strain 19.924570083618164\n",
            "in vicreg  0.7347020786255598 18.9453125 0.3307475447654724\n",
            "strain 20.065448760986328\n",
            "in vicreg  0.7573781535029411 18.9453125 0.3183756172657013\n",
            "strain 20.075754165649414\n",
            "in vicreg  0.6817071698606014 18.9453125 0.34955936670303345\n",
            "strain 20.031267166137695\n",
            "classify 2.830322265625\n",
            "classify 2.8134765625\n",
            "classify 2.831298828125\n",
            "classify 2.851318359375\n",
            "classify 2.88232421875\n",
            "classify 2.829833984375\n",
            "classify 2.9169921875\n",
            "classify 2.7890625\n",
            "classify 2.874755859375\n",
            "0.125\n",
            "0.046875\n",
            "0.09375\n",
            "0.0625\n",
            "in vicreg  0.737810181453824 18.9453125 0.3542780876159668\n",
            "strain 20.09208869934082\n",
            "in vicreg  0.665354123339057 19.140625 0.32345953583717346\n",
            "strain 20.113813400268555\n",
            "in vicreg  0.739350588992238 18.75 0.39050406217575073\n",
            "strain 19.87985610961914\n",
            "in vicreg  0.6780833005905151 19.04296875 0.3210640847682953\n",
            "strain 19.999147415161133\n",
            "in vicreg  0.754111260175705 18.75 0.39056268334388733\n",
            "strain 19.89467430114746\n",
            "in vicreg  0.7814956828951836 18.75 0.38554641604423523\n",
            "strain 19.917041778564453\n",
            "in vicreg  0.7253209128975868 18.9453125 0.33013007044792175\n",
            "strain 20.055450439453125\n",
            "in vicreg  0.6439149379730225 19.43359375 0.2540110647678375\n",
            "strain 20.272926330566406\n",
            "in vicreg  0.6637393962591887 19.3359375 0.26744887232780457\n",
            "strain 20.306188583374023\n",
            "classify 2.855712890625\n",
            "classify 2.847900390625\n",
            "classify 2.847412109375\n",
            "classify 2.82763671875\n",
            "classify 2.809814453125\n",
            "classify 2.828857421875\n",
            "classify 2.898193359375\n",
            "classify 2.85498046875\n",
            "classify 2.86328125\n",
            "0.109375\n",
            "0.171875\n",
            "0.078125\n",
            "0.015625\n",
            "in vicreg  0.797741673886776 18.9453125 0.33715498447418213\n",
            "strain 20.13489532470703\n",
            "in vicreg  0.8505610749125481 18.75 0.38021978735923767\n",
            "strain 19.98078155517578\n",
            "in vicreg  0.9610587731003761 18.5546875 0.4671853184700012\n",
            "strain 19.928245544433594\n",
            "in vicreg  0.8018911816179752 18.9453125 0.37666434049606323\n",
            "strain 20.178556442260742\n",
            "in vicreg  0.7837284356355667 18.9453125 0.3332418203353882\n",
            "strain 20.11697006225586\n",
            "in vicreg  0.8649367839097977 18.65234375 0.39889752864837646\n",
            "strain 19.88883399963379\n",
            "in vicreg  0.7716099731624126 18.5546875 0.4292486608028412\n",
            "strain 19.70085906982422\n",
            "in vicreg  0.5734400823712349 19.23828125 0.2728739380836487\n",
            "strain 20.096315383911133\n",
            "in vicreg  0.6314774043858051 18.75 0.34460321068763733\n",
            "strain 19.7260799407959\n",
            "classify 2.801025390625\n",
            "classify 2.87060546875\n",
            "classify 2.848388671875\n",
            "classify 2.879638671875\n",
            "classify 2.843017578125\n",
            "classify 2.903076171875\n",
            "classify 2.837890625\n",
            "classify 2.870361328125\n",
            "classify 2.838623046875\n",
            "0.109375\n",
            "0.09375\n",
            "0.0625\n",
            "0.171875\n",
            "in vicreg  0.5475005600601435 19.3359375 0.2553805708885193\n",
            "strain 20.177881240844727\n",
            "in vicreg  0.6521687842905521 18.84765625 0.35413217544555664\n",
            "strain 19.88129997253418\n",
            "in vicreg  0.6992807611823082 18.75 0.3720911741256714\n",
            "strain 19.82137107849121\n",
            "in vicreg  0.6966417655348778 18.9453125 0.3377726376056671\n",
            "strain 20.034414291381836\n",
            "in vicreg  0.7670477032661438 18.84765625 0.3793315291404724\n",
            "strain 20.021379470825195\n",
            "in vicreg  0.7394729182124138 18.9453125 0.3456791639328003\n",
            "strain 20.08515167236328\n",
            "in vicreg  0.8203206583857536 18.75 0.37768104672431946\n",
            "strain 19.948001861572266\n",
            "in vicreg  0.8360555395483971 18.75 0.3668469190597534\n",
            "strain 19.95290184020996\n",
            "in vicreg  0.8297750726342201 18.75 0.38586410880088806\n",
            "strain 19.965639114379883\n",
            "classify 2.838623046875\n",
            "classify 2.844482421875\n",
            "classify 2.814208984375\n",
            "classify 2.856689453125\n",
            "classify 2.835205078125\n",
            "classify 2.900146484375\n",
            "classify 2.882080078125\n",
            "classify 2.824951171875\n",
            "classify 2.81005859375\n",
            "0.0625\n",
            "0.078125\n",
            "0.15625\n",
            "0.140625\n",
            "in vicreg  0.8010688237845898 18.75 0.37716010212898254\n",
            "strain 19.92823028564453\n",
            "in vicreg  0.7821438834071159 18.84765625 0.36213135719299316\n",
            "strain 20.019275665283203\n",
            "in vicreg  0.7693812251091003 18.75 0.3624834418296814\n",
            "strain 19.881864547729492\n",
            "in vicreg  0.7158862426877022 18.75 0.3319517970085144\n",
            "strain 19.79783821105957\n",
            "in vicreg  0.7055299356579781 18.84765625 0.4038922190666199\n",
            "strain 19.98442268371582\n",
            "in vicreg  0.5852400325238705 19.3359375 0.2875337600708008\n",
            "strain 20.247772216796875\n",
            "in vicreg  0.6448671687394381 18.9453125 0.346016526222229\n",
            "strain 19.990882873535156\n",
            "in vicreg  0.6873193196952343 18.75 0.34566083550453186\n",
            "strain 19.78297996520996\n",
            "in vicreg  0.6440939381718636 19.04296875 0.3195415139198303\n",
            "strain 19.96363639831543\n",
            "classify 2.825927734375\n",
            "classify 2.864013671875\n",
            "classify 2.79150390625\n",
            "classify 2.882568359375\n",
            "classify 2.898681640625\n",
            "classify 2.828125\n",
            "classify 2.860595703125\n",
            "classify 2.822265625\n",
            "classify 2.835205078125\n",
            "0.140625\n",
            "0.109375\n",
            "0.09375\n",
            "0.046875\n",
            "in vicreg  0.6721021607518196 19.140625 0.3025449812412262\n",
            "strain 20.099647521972656\n",
            "in vicreg  0.8677490055561066 18.5546875 0.43894511461257935\n",
            "strain 19.80669403076172\n",
            "in vicreg  0.8893850259482861 18.5546875 0.43364274501800537\n",
            "strain 19.823028564453125\n",
            "in vicreg  0.8523148484528065 18.5546875 0.40944063663482666\n",
            "strain 19.761754989624023\n",
            "in vicreg  0.7334122434258461 18.84765625 0.3694794476032257\n",
            "strain 19.97789192199707\n",
            "in vicreg  0.6778128445148468 18.9453125 0.3195672035217285\n",
            "strain 19.997379302978516\n",
            "in vicreg  0.650452496483922 19.04296875 0.2959732115268707\n",
            "strain 19.94642448425293\n",
            "in vicreg  0.7308463100343943 18.75 0.4045262932777405\n",
            "strain 19.885372161865234\n",
            "in vicreg  0.664827972650528 18.9453125 0.3189006745815277\n",
            "strain 19.983728408813477\n",
            "classify 2.848876953125\n",
            "classify 2.8671875\n",
            "classify 2.839599609375\n",
            "classify 2.84521484375\n",
            "classify 2.83544921875\n",
            "classify 2.8037109375\n",
            "classify 2.857421875\n",
            "classify 2.843505859375\n",
            "classify 2.78466796875\n",
            "0.03125\n",
            "0.09375\n",
            "0.078125\n",
            "0.171875\n",
            "in vicreg  0.6854280363768339 18.9453125 0.3173272907733917\n",
            "strain 20.002756118774414\n",
            "in vicreg  0.6699638441205025 19.04296875 0.31106942892074585\n",
            "strain 19.981033325195312\n",
            "in vicreg  0.7202918641269207 19.04296875 0.3145475685596466\n",
            "strain 20.034839630126953\n",
            "in vicreg  0.8167851716279984 18.65234375 0.4126230478286743\n",
            "strain 19.854408264160156\n",
            "in vicreg  0.9200403466820717 18.359375 0.4713481068611145\n",
            "strain 19.766387939453125\n",
            "in vicreg  0.7959144189953804 18.75 0.37732720375061035\n",
            "strain 19.923240661621094\n",
            "in vicreg  0.7766329683363438 18.65234375 0.3870888650417328\n",
            "strain 19.788721084594727\n",
            "in vicreg  0.7704495452344418 18.5546875 0.4277709722518921\n",
            "strain 19.698219299316406\n",
            "in vicreg  0.7322290912270546 18.75 0.35736775398254395\n",
            "strain 19.839597702026367\n",
            "classify 2.85546875\n",
            "classify 2.876953125\n",
            "classify 2.8427734375\n",
            "classify 2.859375\n",
            "classify 2.811767578125\n",
            "classify 2.8515625\n",
            "classify 2.807373046875\n",
            "classify 2.82177734375\n",
            "classify 2.85888671875\n",
            "0.140625\n",
            "0.0625\n",
            "0.109375\n",
            "0.140625\n",
            "in vicreg  0.5974194500595331 18.9453125 0.28831392526626587\n",
            "strain 19.88573455810547\n",
            "in vicreg  0.6392054259777069 18.75 0.35483789443969727\n",
            "strain 19.744043350219727\n",
            "in vicreg  0.5906658712774515 19.140625 0.27437692880630493\n",
            "strain 19.99004364013672\n",
            "in vicreg  0.6950296461582184 18.75 0.34668463468551636\n",
            "strain 19.791715621948242\n",
            "in vicreg  0.7276477292180061 18.75 0.3593118190765381\n",
            "strain 19.836959838867188\n",
            "in vicreg  0.8168614469468594 18.5546875 0.46132516860961914\n",
            "strain 19.778186798095703\n",
            "in vicreg  0.8056692779064178 18.75 0.36723533272743225\n",
            "strain 19.92290496826172\n",
            "in vicreg  0.798161793500185 18.65234375 0.42357125878334045\n",
            "strain 19.84673309326172\n",
            "in vicreg  0.7282719947397709 18.5546875 0.40769320726394653\n",
            "strain 19.63596534729004\n",
            "classify 2.822265625\n",
            "classify 2.81591796875\n",
            "classify 2.859375\n",
            "classify 2.8681640625\n",
            "classify 2.821533203125\n",
            "classify 2.843017578125\n",
            "classify 2.817626953125\n",
            "classify 2.849365234375\n",
            "classify 2.86328125\n",
            "0.0625\n",
            "0.09375\n",
            "0.125\n",
            "0.09375\n",
            "in vicreg  0.6555691361427307 18.9453125 0.3171038329601288\n",
            "strain 19.972673416137695\n",
            "in vicreg  0.6812009029090405 18.84765625 0.3303290605545044\n",
            "strain 19.88652992248535\n",
            "in vicreg  0.6363296881318092 19.04296875 0.31834664940834045\n",
            "strain 19.954675674438477\n",
            "in vicreg  0.7983624935150146 18.5546875 0.40802228450775146\n",
            "strain 19.706384658813477\n",
            "in vicreg  0.7534443400800228 18.75 0.3796762228012085\n",
            "strain 19.883121490478516\n",
            "in vicreg  0.8471090346574783 18.359375 0.443575918674469\n",
            "strain 19.665685653686523\n",
            "in vicreg  0.7591518573462963 18.5546875 0.3940241038799286\n",
            "strain 19.653175354003906\n",
            "in vicreg  0.6922813132405281 18.5546875 0.40929973125457764\n",
            "strain 19.601581573486328\n",
            "in vicreg  0.6015186198055744 19.140625 0.27683839201927185\n",
            "strain 20.00335693359375\n",
            "classify 2.8505859375\n",
            "classify 2.814453125\n",
            "classify 2.859619140625\n",
            "classify 2.8642578125\n",
            "classify 2.813232421875\n",
            "classify 2.83837890625\n",
            "classify 2.8857421875\n",
            "classify 2.804931640625\n",
            "classify 2.86572265625\n",
            "0.078125\n",
            "0.046875\n",
            "0.109375\n",
            "0.125\n",
            "in vicreg  0.6580731831490993 19.04296875 0.30237331986427307\n",
            "strain 19.960447311401367\n",
            "in vicreg  0.7156660780310631 18.75 0.3633604347705841\n",
            "strain 19.82902717590332\n",
            "in vicreg  0.7426084950566292 18.5546875 0.3851641118526459\n",
            "strain 19.62777328491211\n",
            "in vicreg  0.663379579782486 18.9453125 0.391084760427475\n",
            "strain 20.05446434020996\n",
            "in vicreg  0.7237683050334454 18.75 0.35852330923080444\n",
            "strain 19.832290649414062\n",
            "in vicreg  0.7525337859988213 18.75 0.3525972366333008\n",
            "strain 19.85512924194336\n",
            "in vicreg  0.8012951351702213 18.5546875 0.40716004371643066\n",
            "strain 19.708454132080078\n",
            "in vicreg  0.8264102041721344 18.5546875 0.4430517554283142\n",
            "strain 19.76946258544922\n",
            "in vicreg  0.8035069331526756 18.5546875 0.4011295437812805\n",
            "strain 19.704635620117188\n",
            "classify 2.861328125\n",
            "classify 2.8818359375\n",
            "classify 2.814697265625\n",
            "classify 2.82763671875\n",
            "classify 2.848876953125\n",
            "classify 2.81201171875\n",
            "classify 2.856689453125\n",
            "classify 2.878662109375\n",
            "classify 2.852783203125\n",
            "0.15625\n",
            "0.078125\n",
            "0.09375\n",
            "0.09375\n",
            "in vicreg  0.7084540091454983 18.65234375 0.36914363503456116\n",
            "strain 19.702598571777344\n",
            "in vicreg  0.6953777279704809 18.75 0.36698198318481445\n",
            "strain 19.812358856201172\n",
            "in vicreg  0.6083800923079252 18.9453125 0.3065529763698578\n",
            "strain 19.914932250976562\n",
            "in vicreg  0.5906590260565281 19.140625 0.28211110830307007\n",
            "strain 19.99776840209961\n",
            "in vicreg  0.6940227001905441 18.75 0.3634517788887024\n",
            "strain 19.80747413635254\n",
            "in vicreg  0.768366502597928 18.5546875 0.40857183933258057\n",
            "strain 19.676937103271484\n",
            "in vicreg  0.874345563352108 18.359375 0.5103007555007935\n",
            "strain 19.759647369384766\n",
            "in vicreg  0.8040932938456535 18.5546875 0.4231611490249634\n",
            "strain 19.727252960205078\n",
            "in vicreg  0.743121700361371 18.5546875 0.41680410504341125\n",
            "strain 19.65992546081543\n",
            "classify 2.866455078125\n",
            "classify 2.89404296875\n",
            "classify 2.905029296875\n",
            "classify 2.806884765625\n",
            "classify 2.875244140625\n",
            "classify 2.819580078125\n",
            "classify 2.8427734375\n",
            "classify 2.785888671875\n",
            "classify 2.830810546875\n",
            "0.09375\n",
            "0.15625\n",
            "0.0625\n",
            "0.125\n",
            "in vicreg  0.6790471263229847 18.5546875 0.3568427264690399\n",
            "strain 19.535888671875\n",
            "in vicreg  0.6365963257849216 18.75 0.3385217785835266\n",
            "strain 19.72511863708496\n",
            "in vicreg  0.634317472577095 18.65234375 0.3527960181236267\n",
            "strain 19.61211395263672\n",
            "in vicreg  0.5686445161700249 18.9453125 0.2926782965660095\n",
            "strain 19.861324310302734\n",
            "in vicreg  0.6296020932495594 19.04296875 0.3407513499259949\n",
            "strain 19.970354080200195\n",
            "in vicreg  0.7650035433471203 18.45703125 0.44001567363739014\n",
            "strain 19.705018997192383\n",
            "in vicreg  0.7408817764371634 18.9453125 0.3504665493965149\n",
            "strain 20.091346740722656\n",
            "in vicreg  0.8226211182773113 18.5546875 0.42356449365615845\n",
            "strain 19.746185302734375\n",
            "in vicreg  0.8008651435375214 18.5546875 0.40607741475105286\n",
            "strain 19.70694351196289\n",
            "classify 2.886962890625\n",
            "classify 2.798828125\n",
            "classify 2.814453125\n",
            "classify 2.811767578125\n",
            "classify 2.861328125\n",
            "classify 2.83154296875\n",
            "classify 2.843505859375\n",
            "classify 2.8720703125\n",
            "classify 2.861083984375\n",
            "0.0625\n",
            "0.109375\n",
            "0.078125\n",
            "0.15625\n",
            "in vicreg  0.787101499736309 18.5546875 0.380094975233078\n",
            "strain 19.66719627380371\n",
            "in vicreg  0.6990217603743076 18.75 0.3610868453979492\n",
            "strain 19.810108184814453\n",
            "in vicreg  0.7290276698768139 18.5546875 0.40007540583610535\n",
            "strain 19.62910270690918\n",
            "in vicreg  0.7464555557817221 18.5546875 0.44101864099502563\n",
            "strain 19.687475204467773\n",
            "in vicreg  0.6686615757644176 18.75 0.3467133641242981\n",
            "strain 19.7653751373291\n",
            "in vicreg  0.6073638796806335 18.75 0.36024582386016846\n",
            "strain 19.71761131286621\n",
            "in vicreg  0.6920247338712215 18.5546875 0.38878893852233887\n",
            "strain 19.580812454223633\n",
            "in vicreg  0.6465359125286341 18.75 0.3453783094882965\n",
            "strain 19.741914749145508\n",
            "in vicreg  0.6866263691335917 18.75 0.35095828771591187\n",
            "strain 19.78758430480957\n",
            "classify 2.829833984375\n",
            "classify 2.797119140625\n",
            "classify 2.873046875\n",
            "classify 2.85546875\n",
            "classify 2.8544921875\n",
            "classify 2.86669921875\n",
            "classify 2.85205078125\n",
            "classify 2.845458984375\n",
            "classify 2.82080078125\n",
            "0.109375\n",
            "0.109375\n",
            "0.109375\n",
            "0.140625\n",
            "in vicreg  0.6882071495056152 18.75 0.3313477039337158\n",
            "strain 19.769556045532227\n",
            "in vicreg  0.7777157239615917 18.45703125 0.42347097396850586\n",
            "strain 19.701187133789062\n",
            "in vicreg  0.801337044686079 18.5546875 0.432828426361084\n",
            "strain 19.73416519165039\n",
            "in vicreg  0.8318756707012653 18.359375 0.47153574228286743\n",
            "strain 19.67841148376465\n",
            "in vicreg  0.7070806343108416 18.65234375 0.35170280933380127\n",
            "strain 19.68378448486328\n",
            "in vicreg  0.7159922271966934 18.5546875 0.3765212893486023\n",
            "strain 19.592514038085938\n",
            "in vicreg  0.6475899368524551 18.75 0.3490474820137024\n",
            "strain 19.74663734436035\n",
            "in vicreg  0.6835282314568758 18.45703125 0.41277268528938293\n",
            "strain 19.596302032470703\n",
            "in vicreg  0.612418632954359 18.75 0.3479728102684021\n",
            "strain 19.710391998291016\n",
            "classify 2.8525390625\n",
            "classify 2.821044921875\n",
            "classify 2.817626953125\n",
            "classify 2.844970703125\n",
            "classify 2.82470703125\n",
            "classify 2.86767578125\n",
            "classify 2.81689453125\n",
            "classify 2.875244140625\n",
            "classify 2.82666015625\n",
            "0.125\n",
            "0.125\n",
            "0.078125\n",
            "0.09375\n",
            "in vicreg  0.7057873532176018 18.45703125 0.4178297221660614\n",
            "strain 19.62361717224121\n",
            "in vicreg  0.7288952358067036 18.5546875 0.39147520065307617\n",
            "strain 19.620370864868164\n",
            "in vicreg  0.6754307076334953 18.5546875 0.36480480432510376\n",
            "strain 19.54023551940918\n",
            "in vicreg  0.6939767859876156 18.75 0.3500249683856964\n",
            "strain 19.794002532958984\n",
            "in vicreg  0.7534021511673927 18.5546875 0.42014726996421814\n",
            "strain 19.67354965209961\n",
            "in vicreg  0.7237840909510851 18.5546875 0.40856412053108215\n",
            "strain 19.632347106933594\n",
            "in vicreg  0.7070813793689013 18.75 0.3832181990146637\n",
            "strain 19.840299606323242\n",
            "in vicreg  0.8090628311038017 18.359375 0.4671635627746582\n",
            "strain 19.651226043701172\n",
            "in vicreg  0.6604521069675684 18.75 0.3609755337238312\n",
            "strain 19.771427154541016\n",
            "classify 2.8564453125\n",
            "classify 2.8427734375\n",
            "classify 2.8232421875\n",
            "classify 2.869873046875\n",
            "classify 2.84521484375\n",
            "classify 2.849365234375\n",
            "classify 2.791259765625\n",
            "classify 2.835693359375\n",
            "classify 2.830810546875\n",
            "0.140625\n",
            "0.125\n",
            "0.109375\n",
            "0.0625\n",
            "in vicreg  0.6550811231136322 18.75 0.3612689673900604\n",
            "strain 19.76634979248047\n",
            "in vicreg  0.6649283692240715 18.5546875 0.3878694176673889\n",
            "strain 19.552797317504883\n",
            "in vicreg  0.6301512941718102 18.75 0.3711310029029846\n",
            "strain 19.751283645629883\n",
            "in vicreg  0.7172271143645048 18.5546875 0.3982924520969391\n",
            "strain 19.615520477294922\n",
            "in vicreg  0.729880016297102 18.65234375 0.3599712550640106\n",
            "strain 19.71485137939453\n",
            "in vicreg  0.7308565080165863 18.5546875 0.387484073638916\n",
            "strain 19.61833953857422\n",
            "in vicreg  0.7815520279109478 18.359375 0.4570273756980896\n",
            "strain 19.61357879638672\n",
            "in vicreg  0.8278518915176392 18.1640625 0.4789865016937256\n",
            "strain 19.431838989257812\n",
            "in vicreg  0.6747781299054623 18.5546875 0.36481645703315735\n",
            "strain 19.539594650268555\n",
            "classify 2.815185546875\n",
            "classify 2.818603515625\n",
            "classify 2.892578125\n",
            "classify 2.8095703125\n",
            "classify 2.868896484375\n",
            "classify 2.80712890625\n",
            "classify 2.866455078125\n",
            "classify 2.856201171875\n",
            "classify 2.8388671875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: Exception ignored in: <function _after_fork at 0x7cbb20a8ede0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1664, in _after_fork\n",
            "    thread._stop()<function _after_fork at 0x7cbb20a8ede0>\n",
            "\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1051, in _stop\n",
            "    Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1664, in _after_fork\n",
            "    thread._stop()\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1051, in _stop\n",
            "    def _stop(self):\n",
            "\n",
            ": KeyboardInterruptdef _stop(self):\n",
            "\n",
            "KeyboardInterrupt: \n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-66e80394a779>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mstrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mviolet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mctrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mviolet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mviolet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-66e80394a779>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, classifier, dataloader)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0;31m# x, y = x.to(device), y.to(device) # [batch, ]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [batch, ] # (id, activity)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1457\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1459\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1408\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1410\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1411\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1412\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# @title strain ctrain test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "def strain(model, dataloader, optim, scheduler=None):\n",
        "    model.train()\n",
        "    for i, (x, _) in enumerate(dataloader):\n",
        "        x = x[...,1:].to(device).to(torch.bfloat16)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            # loss = model.loss(x)\n",
        "\n",
        "            repr_loss, std_loss, cov_loss = model.loss(x)\n",
        "            loss = model.sim_coeff * repr_loss + model.std_coeff * std_loss + model.cov_coeff * cov_loss\n",
        "\n",
        "        optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # total_norm = 0\n",
        "        # for p in model.parameters(): total_norm += p.grad.data.norm(2).item() ** 2\n",
        "        # total_norm = total_norm**.5\n",
        "        # print('total_norm', total_norm)\n",
        "        # for p in list(filter(lambda p: p.grad is not None, model.parameters())):\n",
        "        #     print(p.grad.data.norm(2).item())\n",
        "        # print(\"max grad norm\", max([p.grad.data.norm(2).item() for p in list(filter(lambda p: p.grad is not None, model.parameters()))]))\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 5) # 0.5\n",
        "\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            m=0.999 # 0.99 m = next(momentum_scheduler)\n",
        "            for param_q, param_k in zip(model.student.parameters(), model.teacher.parameters()):\n",
        "                param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        print(\"strain\",loss.item())\n",
        "        # for param in seq_jepa.context_encoder.cls: print(param.data)\n",
        "        # for param in seq_jepa.predicter.cls: print(param.data)\n",
        "        try: wandb.log({\"loss\": loss.item(), \"repr/I\": repr_loss.item(), \"std/V\": std_loss.item(), \"cov/C\": cov_loss.item()})\n",
        "        # try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=50: break\n",
        "\n",
        "\n",
        "def ctrain(model, classifier, dataloader, coptim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.eval()\n",
        "    classifier.train()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        # x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        x, y = x[...,1:].to(device).to(torch.bfloat16), y[1].to(device) # [batch, ] # (id, activity)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            with torch.no_grad():\n",
        "                sx = model(x).detach()\n",
        "            # print(sx[0][0])\n",
        "            y_ = classifier(sx)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "        # print(classifier.classifier.weight[0])\n",
        "        # print(y_.shape, y.shape)\n",
        "        # print(loss)\n",
        "        coptim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 5) # .5\n",
        "        scaler.step(coptim)\n",
        "        scaler.update()\n",
        "        print(\"classify\",loss.item())\n",
        "        try: wandb.log({\"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        # x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        x, y = x[...,1:].to(device).to(torch.float), y[1].to(device) # [batch, ] # (id, activity)\n",
        "        with torch.no_grad():\n",
        "            sx = model(x)\n",
        "            rankme = RankMe(sx).item()\n",
        "            y_ = classifier(sx)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        print(correct/len(y))\n",
        "        # try: wandb.log({\"correct\": correct/len(y)})\n",
        "        try: wandb.log({\"correct\": correct/len(y), \"rankme\": rankme})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "for i in range(700):\n",
        "    # train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    # test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    np.random.shuffle(train_indices); np.random.shuffle(val_indices)\n",
        "    train_sampler, valid_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(val_indices)\n",
        "    # batch_size = 64 #512\n",
        "    train_loader = DataLoader(train_data, sampler=train_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "    test_loader = DataLoader(train_data, sampler=valid_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True)\n",
        "\n",
        "    # strain(seq_jepa, train_loader, optim)\n",
        "    # ctrain(seq_jepa, classifier, train_loader, coptim)\n",
        "    # test(seq_jepa, classifier, test_loader)\n",
        "\n",
        "    strain(violet, train_loader, voptim)\n",
        "    ctrain(violet, classifier, train_loader, coptim)\n",
        "    test(violet, classifier, test_loader)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "2Nd-sGe6Ku4S",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 673
        },
        "outputId": "a69b0e4e-62b4-406f-bb56-957f0ac21782"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>█▇██▇▆▆▅▅▅▄▄▅▅▅▅▄▄▄▄▂▃▃▂▂▃▂▃▃▂▂▂▂▂▃▃▁▂▃▂</td></tr><tr><td>correct</td><td>▁▃▂▃▃▄▃▄▂▅▄▆▇▄▆▆▇▇▆▆▇██▆▇▇▇▇▆▇▆▆▆█▇▆▆▆▇▇</td></tr><tr><td>cov/C</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▆▅▆▆▆▇▇█▇█▇████</td></tr><tr><td>loss</td><td>█▇▇▆▆▆▆▆▆▆▅▅▄▄▄▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▁</td></tr><tr><td>rankme</td><td>▁▂▂▂▂▄▄▄▅▄▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇███████████</td></tr><tr><td>repr/I</td><td>▇█▇▇█▇▆▅▅▄▃▃▃▃▂▂▂▁▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>std/V</td><td>██▇▇▇▇▆▅▅▅▄▄▄▄▄▄▄▃▃▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>1.89981</td></tr><tr><td>correct</td><td>0.39062</td></tr><tr><td>cov/C</td><td>3.66092</td></tr><tr><td>loss</td><td>10.8551</td></tr><tr><td>rankme</td><td>10.05311</td></tr><tr><td>repr/I</td><td>0.00652</td></tr><tr><td>std/V</td><td>0.28125</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">crisp-terrain-133</strong> at: <a href='https://wandb.ai/bobdole/SeqJEPA/runs/r5de4v1u' target=\"_blank\">https://wandb.ai/bobdole/SeqJEPA/runs/r5de4v1u</a><br> View project at: <a href='https://wandb.ai/bobdole/SeqJEPA' target=\"_blank\">https://wandb.ai/bobdole/SeqJEPA</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250402_051526-r5de4v1u/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250402_054854-b8t30l71</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/SeqJEPA/runs/b8t30l71' target=\"_blank\">copper-cloud-134</a></strong> to <a href='https://wandb.ai/bobdole/SeqJEPA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/SeqJEPA' target=\"_blank\">https://wandb.ai/bobdole/SeqJEPA</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/SeqJEPA/runs/b8t30l71' target=\"_blank\">https://wandb.ai/bobdole/SeqJEPA/runs/b8t30l71</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "!pip install -q wandb\n",
        "import wandb # https://docs.wandb.ai/quickstart\n",
        "wandb.login(key='487a2109e55dce4e13fc70681781de9f50f27be7')\n",
        "try: run.finish()\n",
        "except NameError: pass\n",
        "run = wandb.init(project=\"SeqJEPA\", config={\"model\": \"res18\",}) # violet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title supervised train test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "def strain(model, classifier, dataloader, optim, coptim, scheduler=None):\n",
        "    model.train()\n",
        "    classifier.train()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        # x = x.to(device).flatten(2).transpose(-2,-1)#.to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "        # y = y.to(device)\n",
        "        x, y = x[...,1:].to(device).to(torch.bfloat16), y[1].to(device) # [batch, ] # (id, activity)\n",
        "\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        # print(loss)\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # for p in list(filter(lambda p: p.grad is not None, model.parameters())):\n",
        "        #     print(p.grad.data.norm(2).item())\n",
        "        # print(\"max grad norm\", max([p.grad.data.norm(2).item() for p in list(filter(lambda p: p.grad is not None, model.parameters()))]))\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 10) # 0.5\n",
        "\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        print(\"strain\",loss.item())\n",
        "        # for param in seq_jepa.context_encoder.cls: print(param.data)\n",
        "        # for param in seq_jepa.predicter.cls: print(param.data)\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=50: break\n",
        "\n",
        "\n",
        "# def test(model, dataloader):\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        # x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        x, y = x[...,1:].to(device).to(torch.float), y[1].to(device) # [batch, ] # (id, activity)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "\n",
        "        loss = F.cross_entropy(y_, y)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        print(correct/len(y))\n",
        "        try: wandb.log({\"correct\": correct/len(y), \"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "for i in range(1000):\n",
        "    # train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    # test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    np.random.shuffle(train_indices); np.random.shuffle(val_indices)\n",
        "    train_sampler, valid_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(val_indices)\n",
        "    # batch_size = 64 #512\n",
        "    train_loader = DataLoader(train_data, sampler=train_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "    test_loader = DataLoader(train_data, sampler=valid_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True)\n",
        "\n",
        "    # strain(seq_jepa, train_loader, optim)\n",
        "    # test(seq_jepa, test_loader)\n",
        "    strain(violet, train_loader, voptim)\n",
        "    test(violet, test_loader)\n"
      ],
      "metadata": {
        "id": "4J2ahp3wmqcg",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for param in seq_jepa.context_encoder.cls: print(param.data)\n",
        "# print(seq_jepa.context_encoder.cls.is_leaf)\n",
        "# seq=40\n",
        "# src = seq_jepa.context_encoder.pos_encoder(torch.arange(seq, device=device))\n",
        "# for x in src:\n",
        "#     print(x)\n",
        "# print(.999**50)"
      ],
      "metadata": {
        "id": "WgN5LlxWsPzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title save/load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder='/content/drive/MyDrive/jepa/'\n",
        "\n",
        "# # modelsd, optimsd = torch.load(folder+'SeqJEPA.pkl', map_location=device).values()\n",
        "# modelsd, optimsd = torch.load('SeqJEPA.pkl', map_location=device).values()\n",
        "# seq_jepa.load_state_dict(modelsd, strict=False)\n",
        "# optim.load_state_dict(optimsd)"
      ],
      "metadata": {
        "id": "JT8AgOx0E_KM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b3d3969-da28-4cc0-c4de-f0a327ea266f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = {'model': seq_jepa.state_dict(), 'optimizer': optim.state_dict()}\n",
        "torch.save(checkpoint, folder+'SeqJEPA.pkl')\n",
        "# torch.save(checkpoint, 'SeqJEPA.pkl')"
      ],
      "metadata": {
        "id": "CpbLPvjiE-pD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## drawer"
      ],
      "metadata": {
        "id": "0vScvFGGSeN6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwpnHW4wn9S1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40413ecd-7f4b-49a2-984b-fe812ec57e26",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:08<00:00, 19.5MB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title data\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "transform = transforms.Compose([transforms.ToTensor(),])\n",
        "\n",
        "# MNIST CIFAR10\n",
        "train_data = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
        "test_data = torchvision.datasets.CIFAR10(root='data', train=False, download=True, transform=transform)\n",
        "batch_size = 128 # 128 1024\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    # print(npimg.shape) # (3, 64, 64)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# dataiter = iter(train_data)\n",
        "# x,y = next(dataiter)\n",
        "# print(x.shape) # [3, 32, 32]\n",
        "# imshow(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title evergarmin buffer dataloader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "import numpy as np\n",
        "import scipy.interpolate\n",
        "\n",
        "class BufferDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, activities, seq_len):\n",
        "        # self.data = [step for episode in self.process(buffer) for step in episode] # 0.00053\n",
        "        self.data = [self.process(activity) for activity in activities] # 0.00053\n",
        "        self.data = [x for x in self.data if x!=None and x!=[]]\n",
        "        self.seq_len = seq_len\n",
        "        self.pad = [(-1,-1,-1)]*(self.seq_len) # pad. need to mask later # torch.full((self.seq_len,), -1)\n",
        "        # normalise\n",
        "\n",
        "    def __len__(self):\n",
        "        # return len(self.data)//self.seq_len\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        act_list = self.data[idx].copy()\n",
        "        summary = list(act_list.pop(0))\n",
        "        # act_list = self.pad[len(act_list):] + act_list # forward padding\n",
        "        act_list = [summary] + self.transform(act_list) # summary, aug(x)\n",
        "        hr, temp, heart= zip(*act_list)\n",
        "        # print(hr, temp, heart)\n",
        "        # return hr, temp, heart\n",
        "        return torch.tensor(hr), torch.tensor(temp), torch.tensor(heart, dtype=torch.float)\n",
        "\n",
        "\n",
        "\n",
        "    def process(self, activity):\n",
        "        res_id = activity[\"res_id\"]\n",
        "        try: activeKilocalories = activity[\"summary\"][\"activeKilocalories\"]\n",
        "        except KeyError: activeKilocalories = 0.\n",
        "        try: steps = activity[\"summary\"][\"steps\"]\n",
        "        except KeyError: steps = 0.\n",
        "        # (res_id, activeKilocalories, steps)\n",
        "        res_id = float(res_id[3:]) # remove 'RES' and turn id into float, in line with other data\n",
        "        act_list = [(res_id, activeKilocalories, steps)]\n",
        "        samples = activity[\"samples\"]\n",
        "\n",
        "        if len(samples)==0: return\n",
        "\n",
        "        for sample in samples:\n",
        "            try:\n",
        "                startTimeInSeconds = sample[\"startTimeInSeconds\"]\n",
        "                hour_decimal = (startTimeInSeconds / 3600) % 24\n",
        "            except KeyError: hour_decimal = 12.\n",
        "            try: airTemperatureCelcius = sample[\"airTemperatureCelcius\"]\n",
        "            except KeyError: airTemperatureCelcius = 28. # singapore average temperature?\n",
        "            try: heartRate = sample[\"heartRate\"]\n",
        "            except KeyError: heartRate = 80.\n",
        "            # (hour_decimal, airTemperatureCelcius, heartRate)\n",
        "            act_list.append((hour_decimal, airTemperatureCelcius, heartRate))\n",
        "        return act_list\n",
        "\n",
        "    def transform(self, act_list): # rand resize crop, rand mask\n",
        "\n",
        "        # https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html\n",
        "        act_list = np.array(act_list)\n",
        "        try: hr, temp, heart= zip(*act_list)\n",
        "        except ValueError: print('err:',act_list)\n",
        "        kind = 'nearest' if len(act_list)>self.seq_len/.7 else 'linear'\n",
        "        temp_interpolator = scipy.interpolate.interp1d(hr, temp, kind=kind) # linear nearest quadratic cubic\n",
        "        heart_interpolator = scipy.interpolate.interp1d(hr, heart, kind=kind) # linear nearest quadratic cubic\n",
        "        hr_ = np.sort(np.random.uniform(hr[0], hr[-1], round(self.seq_len*random.uniform(1,1/.7))))\n",
        "        temp_, heart_ = temp_interpolator(hr_), heart_interpolator(hr_)\n",
        "        act_list = list(zip(hr_, temp_, heart_))\n",
        "\n",
        "        idx = torch.randint(len(act_list)-self.seq_len+1, size=(1,))\n",
        "        # return act_list[idx: idx+self.seq_len]\n",
        "        act_list = act_list[idx: idx+self.seq_len]\n",
        "\n",
        "        # mask=(torch.rand(self.seq_len)<.1) # True -> masked # random masking\n",
        "        # # act_list[mask] = self.pad[0]\n",
        "        # act_list = [self.pad[0] if m else a for m,a in zip(mask, act_list)]\n",
        "        return act_list\n",
        "\n",
        "    def push(self, activities):\n",
        "        # self.data.append(self.process(activity))\n",
        "        self.data.extend([self.process(activity) for activity in activities])\n",
        "\n",
        "\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(30, 14))\n",
        "    print(npimg.shape)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "seq_len = 200 #26/51 # 50\n",
        "train_data = BufferDataset(activities, seq_len)\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 128 #128\n",
        "# train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # [3,batch, T]\n",
        "train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, drop_last=True) # [3,batch, T]\n",
        "\n",
        "\n",
        "for batch, (hr, temp, heart) in enumerate(train_loader):\n",
        "    pass\n",
        "    # print(hr[6])\n",
        "    # for h in hr:\n",
        "    #     print(h)\n",
        "    # break\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "DNCJFn5kNuua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ViT me simple save\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    \"\"\"Zero out the parameters of a module and return it.\"\"\"\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads=None, d_head=8, cond_dim=None, dropout=0.): # .1\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_head = d_head\n",
        "        self.n_heads = d_model // d_head\n",
        "        # self.d_head = d_model // n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or d_model, 2*d_model, bias=False)\n",
        "        # self.k = nn.Sequential(nn.Dropout(dropout), nn.Linear(cond_dim, d_model, bias=False))\n",
        "        # self.lin = nn.Linear(d_model, d_model)\n",
        "        self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        # self.lin = nn.Sequential(nn.Dropout(dropout), zero_module(nn.Linear(d_model, d_model)))\n",
        "        self.drop = nn.Dropout(dropout) # indp before q,k,v; after linout\n",
        "        self.scale = self.d_head ** -.5\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [batch, T, d_model]=[batch, h*w, c], [batch, num_tok, cond_dim], [batch,T]\n",
        "        batch = x.shape[0]\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        Q = self.q(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        # K = self.k(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        K, V = self.kv(cond).view(batch, -1, self.n_heads, 2*self.d_head).transpose(1, 2).chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # linear attention # Softmax(Q) @ (Softmax(K).T @ V)\n",
        "        if mask != None:\n",
        "            mask = mask[:, None, :, None] # [batch,T] -> [batch,1,T,1]\n",
        "            K, V = K.masked_fill(mask, -torch.finfo(x.dtype).max), V.masked_fill(mask, -torch.finfo(x.dtype).max)\n",
        "        Q, K = Q.softmax(dim=-1)*self.scale, K.softmax(dim=-2)\n",
        "        context = K.transpose(-2,-1) @ V # [batch, n_heads, d_head, d_head]\n",
        "        out = Q @ context # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # (quadratic) attention # Softmax(Q @ K.T) @ V\n",
        "        # attn = Q @ K.transpose(-2,-1) * self.scale # [batch, n_heads, T] # [batch, n_heads, T, T/num_tok]\n",
        "        # if mask != None: attn = attn.masked_fill(mask[:, None, :, None], -torch.finfo(attn.dtype).max) # [batch,T]->[batch,1,T,1]\n",
        "        # attention = torch.softmax(attn, dim=-1)\n",
        "        # out = self.drop(attention) @ V # [batch, n_heads, T, d_head]\n",
        "\n",
        "        out = out.transpose(1, 2).flatten(2)\n",
        "        return self.drop(self.lin(out)) # [batch, T, d_model]\n",
        "\n",
        "# if self, dont pass cond_dim in init, dont pass cond in fwd\n",
        "# Softmax(Q @ K.T) @ V ~ Softmax(Q) @ Softmax(K).T @ V\n",
        "\n",
        "# https://github.com/lucidrains/x-transformers/blob/main/x_transformers/x_transformers.py#L1855\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_head=4, cond_dim=None, ff_mult=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm1 = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.norm2 = nn.RMSNorm(d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.cond_dim = cond_dim\n",
        "        self.self = MultiHeadAttention(d_model, d_head=d_head, dropout=0)\n",
        "        if self.cond_dim!=None: self.cross = MultiHeadAttention(d_model, d_head=d_head, cond_dim=cond_dim, dropout=0)\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        ff_dim=d_model*ff_mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), nn.Dropout(dropout), nn.Linear(d_model, ff_dim), act, # ReLU GELU\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "            nn.RMSNorm(d_model), nn.Dropout(dropout), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            # nn.RMSNorm(d_model), act, nn.Dropout(dropout), nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,c,h,w], [batch, num_tok, cond_dim], [batch,T]\n",
        "        # bchw = x.shape\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "\n",
        "        if self.cond_dim==None: cond=None # is self attn\n",
        "        x = x + self.drop(self.self(self.norm1(x)))\n",
        "        if self.cond_dim!=None: x = x + self.cross(self.norm2(x), cond, mask)\n",
        "        x = x + self.ff(x)\n",
        "        # x = x + self.drop(self.ff(x))\n",
        "\n",
        "        # return x.transpose(1,2).reshape(*bchw)\n",
        "        return x\n",
        "\n",
        "\n",
        "# class SimpleViT(nn.Module):\n",
        "#     def __init__(self, in_dim, out_dim, dim, depth, heads, mlp_dim, channels=3, dim_head=8):\n",
        "#         super().__init__()\n",
        "#         self.to_patch_embedding = nn.Sequential( # in, out, kernel, stride, pad\n",
        "#             nn.Conv2d(in_dim, dim, kernel_size=7, stride=2, padding=7//2, bias=False), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "#             # nn.Conv2d(in_dim, dim, kernel_size=7, stride=2, padding=7//2, bias=False), nn.MaxPool2d(2,2)\n",
        "#             )\n",
        "#         # self.pos_embedding = LearnedRoPE2D(dim) # LearnedRoPE2D, RoPE2D\n",
        "#         self.positional_emb = nn.Parameter(torch.zeros(1, 8*8, dim)) # positional_embedding == 'learnable'\n",
        "#         self.transformer = AttentionBlock(d_model=dim, d_head=dim_head)\n",
        "#         self.transformer_blocks = nn.ModuleList([AttentionBlock(d_model=dim, d_head=dim_head) for i in range(depth)])\n",
        "\n",
        "#         self.attention_pool = nn.Linear(dim, 1)\n",
        "#         self.out = nn.Linear(dim, out_dim, bias=False)\n",
        "\n",
        "#     def forward(self, img):\n",
        "#         device = img.device\n",
        "#         x = self.to_patch_embedding(img)\n",
        "#         # x = self.pos_embedding(x)\n",
        "#         x = x.flatten(-2).transpose(-2,-1) # b c h w -> b (h w) c\n",
        "#         x = x + self.positional_emb\n",
        "#         x = self.transformer(x)\n",
        "#         # for blk in self.predictor_blocks: x = blk(x)\n",
        "\n",
        "#         # x = x.flatten(-2).mean(dim=-1) # meal pool\n",
        "#         attn_weights = self.attention_pool(x).squeeze(-1) # [batch, (h,w)] # seq_pool\n",
        "#         x = (attn_weights.softmax(dim = 1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, (h,w)] @ [batch, (h,w), dim]\n",
        "#         # cls_token = repeat(self.class_emb, '1 1 d -> b 1 d', b = b)\n",
        "#         # x = torch.cat((cls_token, x), dim=1)\n",
        "#         # x = x[:, 0] # first token\n",
        "#         return self.out(x)\n",
        "\n",
        "# ijepa: 224*224 -> 14*14 = 196\n",
        "# 32*7\n",
        "# me: 32*32=1024 -> 256\n",
        "\n",
        "import torch\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "def apply_masks(x, masks):\n",
        "    \"\"\":param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
        "    :param masks: list of tensors containing indices of patches in [N] to keep\"\"\"\n",
        "    all_x = []\n",
        "    for m in masks: # [1,T]\n",
        "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [1,T,dim]\n",
        "        all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "    return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "# batch, seq_len, d_model = 2,7,4\n",
        "# x = torch.rand(batch, seq_len, d_model)\n",
        "# print(x)\n",
        "# masks = [torch.randint(0,seq_len,(2,3,))]\n",
        "# print(masks)\n",
        "# out = apply_masks(x, masks)\n",
        "# print(out)\n",
        "# # print(out.shape)\n",
        "\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        # self.positional_emb = nn.Parameter(torch.zeros(1, 32*32//4, d_model)) # positional_embedding == 'learnable'\n",
        "        # encoder_layers = nn.TransformerEncoderLayer(d_model, d_model//d_head, d_hid or d_model, dropout, batch_first=True)\n",
        "        # self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.zeros(1,1,d_model)) # randn zeros\n",
        "        nn.init.trunc_normal_(self.cls, std=.02)\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model)\n",
        "        self.lin = nn.Linear(d_model, out_dim)# if out_dim != d_model else None\n",
        "\n",
        "    def forward(self, src, context_indices, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        src = self.embed(src) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = src.shape\n",
        "        src = src * self.pos_encoder(context_indices)\n",
        "        # src = src + self.positional_emb[:,context_indices]\n",
        "        # src = apply_masks(src, [context_indices])\n",
        "\n",
        "        pred_tokens = self.cls * self.pos_encoder(trg_indices) # [M, num_trg_toks, d_model]\n",
        "        # pred_tokens = self.cls + self.positional_emb[0,trg_indices]\n",
        "        pred_tokens = pred_tokens.repeat(batch, 1, 1) # [batch*M, num_trg_toks, d_model]\n",
        "        # print(pred_tokens.requires_grad)\n",
        "        # print(\"pred fwd\", src.shape, pred_tokens.shape)\n",
        "        # src = src.repeat_interleave(trg_indices.shape[0], dim=0) # [batch, seq_len, d_model] -> [batch*M, seq_len, d_model]\n",
        "        src = torch.cat([src, pred_tokens], dim=1) # [batch*M, seq_len+num_trg_toks, d_model]\n",
        "\n",
        "        out = self.transformer_encoder(src) # float [seq_len, batch_size, d_model]\n",
        "        out = self.norm(out)\n",
        "        out = out[:,seq:] # [batch*M, num_trg_toks, d_model]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.MaxPool1d(2,2), #nn.MaxPool1d(3, 2, 3//2),\n",
        "            nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.Conv1d(d_model, d_model,3,2,3//2)\n",
        "            # nn.Conv1d(in_dim, d_model,2,2,0), # like patch\n",
        "            )\n",
        "        self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        # self.positional_emb = nn.Parameter(torch.zeros(1, 32*32//4, d_model)) # positional_embedding == 'learnable'\n",
        "        # encoder_layers = nn.TransformerEncoderLayer(d_model, d_model//d_head, d_hid or d_model, dropout, batch_first=True)\n",
        "        # self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model)\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, src, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # src = self.embed(src) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        src = self.embed(src.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "\n",
        "        batch, seq, dim = src.shape\n",
        "\n",
        "        # # # src = self.pos_encoder(src)\n",
        "        # if context_indices != None:\n",
        "        # print(src.shape, self.pos_encoder(context_indices).shape)\n",
        "        #     src = src * self.pos_encoder(context_indices) # context/predictor # src = src + self.positional_emb[:,context_indices]\n",
        "        # else: src = src * self.pos_encoder(torch.arange(seq, device=device).unsqueeze(0)) # target # src = src + self.positional_emb[:,:seq]\n",
        "\n",
        "\n",
        "        # src = self.pos_encoder(src)\n",
        "        src = src * self.pos_encoder(torch.arange(seq, device=device)).unsqueeze(0)\n",
        "        # src = src + self.positional_emb\n",
        "        if context_indices != None:\n",
        "            src = apply_masks(src, [context_indices])\n",
        "\n",
        "\n",
        "        out = self.transformer_encoder(src) # float [seq_len, batch_size, d_model]\n",
        "        out = self.norm(out)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "batch, seq_len, d_model = 4,1024,4\n",
        "in_dim = 3\n",
        "model = TransformerModel(in_dim, d_model, d_head=4, nlayers=2, dropout=0.).to(device)\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "# # print(out)\n",
        "\n"
      ],
      "metadata": {
        "id": "GHzr3NVs2EcG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cecf37be-0d0f-4890-83f3-305b0b01c3ea",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 256, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(4,100,3)\n",
        "\n",
        "x=x.transpose(-2,-1) # [batch, dim, seq]\n",
        " # in, out, kernel, stride, pad\n",
        "# net = nn.Conv1d(3,16,7,2,7//2)\n",
        "net = nn.Sequential(nn.Conv1d(3,16,7,2,7//2), nn.MaxPool1d(3, 2, 3//2))\n",
        "# net = nn.Conv1d(3,16,(7,3),2,3//2)\n",
        "out = net(x)\n",
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_YBe6-eZ2Zq",
        "outputId": "09b27f75-ba04-4aaf-f5e4-1b9c1dc8d112"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 16, 25])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test repeat_interleave\n",
        "# x = torch.rand(3,5)\n",
        "x = torch.rand(3,5,7)\n",
        "print(x)\n",
        "# out = x.repeat(2,1) # [2*3,5]\n",
        "# print(out)\n",
        "# x_ = out.reshape(2,3,5)\n",
        "# print(x_)\n",
        "# out = x.repeat_interleave(2,1,1) # [3*2,5]\n",
        "out = x.repeat_interleave(2,dim=0) # [3*2,5]\n",
        "# print(out)\n",
        "# x_ = out.reshape(2,3,5,7)\n",
        "x_ = out.reshape(3,2,5,7)\n",
        "print(x_)\n",
        "\n",
        "\n",
        "# batch=1\n",
        "# seq_len=5\n",
        "# dim=4\n",
        "# positional_emb = torch.rand(batch, seq_len, dim)\n",
        "# print(positional_emb)\n",
        "# num_tok=2\n",
        "# trg_indices=torch.randint(0,seq_len,(M, num_tok))\n",
        "# print(trg_indices)\n",
        "# # pos_embs = positional_emb.gather(1, trg_indices.unsqueeze(0))\n",
        "# pos_embs = positional_emb[torch.arange(batch)[...,None,None],trg_indices.unsqueeze(0)]\n",
        "# # pos_embs = positional_emb[0,trg_indices]\n",
        "# # [batch, M, num_tok, dim]\n",
        "# print(pos_embs.shape)\n",
        "# print(pos_embs)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "R6kiiqw3uIdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test ropes\n",
        "# for i, (img, _) in enumerate(test_loader):\n",
        "#     break\n",
        "# print(img.shape) # [batch, 1, 28, 28]\n",
        "# print(img[0]) # [0,1)\n",
        "d_model=8\n",
        "# rot_emb = RotEmb(d_model, top=torch.pi, base=10)\n",
        "# x = torch.linspace(0,1,20)\n",
        "# out = rot_emb(x)\n",
        "# print(out.shape)\n",
        "# print(out)\n",
        "\n",
        "# # pos_encoder = RoPE(d_model, seq_len=15, base=100)\n",
        "# pos_encoder = RoPE(d_model, seq_len=15, base=10000)\n",
        "x = torch.ones(1, 10, d_model)\n",
        "# # x = torch.ones(1, 10, d_model)\n",
        "# out = pos_encoder(x)\n",
        "# # print(out.shape)\n",
        "# for x in out[0]:\n",
        "#     print(x)\n",
        "\n",
        "\n",
        "# pos_encoder = LearnedRoPE(d_model, seq_len=512)\n",
        "# out = pos_encoder(x)\n",
        "# for o in out[0]:\n",
        "#     print(o)\n",
        "\n",
        "# pos_encoder.weights = nn.Parameter(torch.randn(1, d_model//2))\n",
        "# out = pos_encoder(x)\n",
        "# for o in out[0]:\n",
        "#     print(o)\n",
        "\n",
        "\n",
        "# tensor([-0.0177, -0.9998,  0.8080, -0.5892, -0.7502, -0.6612,  0.3885,  0.9214])\n"
      ],
      "metadata": {
        "id": "8r_GjI_vCMyo",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "d929355f-be2b-47a1-82ff-a7630671f47f",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title RNN pytorch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, num_layers=1):\n",
        "        super().__init__()\n",
        "        if out_dim is None: out_dim = in_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.d_model = d_model\n",
        "        # self.rnn = nn.RNN(d_model, d_model, num_layers, batch_first=True)\n",
        "        self.rnn = nn.GRU(in_dim, d_model, num_layers, batch_first=True)\n",
        "        # self.lstm = nn.LSTM(d_model, d_model, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(d_model, out_dim)\n",
        "\n",
        "        # self.emb = nn.Embedding(in_dim, d_model)\n",
        "        self.mask_vec = nn.Parameter(torch.randn(in_dim))\n",
        "\n",
        "\n",
        "    # def forward(self, x, hc=None): # lstm [batch_size, seq, in_dim]\n",
        "    #     x = self.emb(x)\n",
        "    #     if hc is None:\n",
        "    #         h0 = torch.zeros((self.num_layers, x.size(0), self.d_model), device=device)\n",
        "    #         c0 = torch.zeros((self.num_layers, x.size(0), self.d_model), device=device)\n",
        "    #     else: h0,c0 = hc\n",
        "    #     # print(x.shape, h0.shape,c0.shape)\n",
        "    #     out, (h,c) = self.lstm(x, (h0,c0)) # [batch, seq_len, d_model], ([num_layers, batch, d_model] )\n",
        "    #     # out = out[:, -1, :] # out: (n, 128)\n",
        "    #     out = self.fc(out) # out: (n, 10)\n",
        "    #     return out, (h, c)\n",
        "\n",
        "    def reset(self, batch):\n",
        "        h0 = torch.zeros((self.num_layers, batch, self.d_model), device=device)\n",
        "        return h0\n",
        "\n",
        "    def forward(self, x, h0=None, mask=None): # rnn/gru # [batch, T, in_dim], [num_layers, batch, d_model], [batch, T] True->masked\n",
        "        # x = self.emb(x)\n",
        "        if h0==None: h0 = self.reset(x.shape[0])\n",
        "        # print(x.shape, h0.shape)\n",
        "        if mask!=None: x[mask] = self.mask_vec.to(x.dtype)\n",
        "        out, h = self.rnn(x, h0) #\n",
        "        out = out[:, -1, :] # out: (n, 128)\n",
        "        out = self.fc(out) # out: (n, 10)\n",
        "        return out, h # [batch, out_dim], [num_layers, batch, d_model]\n",
        "\n",
        "\n",
        "# hidden_size = 128\n",
        "# num_layers = 2\n",
        "# input_size = num_classes = 4\n",
        "\n",
        "# model = RNN(input_size, hidden_size, num_classes, num_layers).to(device)\n",
        "# # # print(model)\n",
        "# # print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "# batch=2\n",
        "# seq_len=3\n",
        "# x=torch.rand(batch,seq_len,input_size).to(device)\n",
        "# h=torch.rand(num_layers,batch,hidden_size).to(device)\n",
        "# mask=(torch.rand(batch,seq_len)<.5)#.expand(-1,-1,x.size(-1))\n",
        "# print(mask)\n",
        "# print(x)\n",
        "# out,h = model(x, h, mask)\n",
        "# print(out.shape)\n",
        "# print(h.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title TransformerClassifier\n",
        "import math\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class TransformerClassifier(nn.Module):\n",
        "    # def __init__(self, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, drop = 0.):\n",
        "    def __init__(self, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, drop=0):\n",
        "        super().__init__()\n",
        "        # self.embedding = nn.Embedding(ntoken, d_model)\n",
        "        self.pos_encoder = RoPE(d_model, seq_len=200, base=10000)\n",
        "        # d_hid = d_hid or d_model#*2\n",
        "        # encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, d_hid, drop, batch_first=True)\n",
        "        # self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "\n",
        "        out_dim = out_dim or d_model\n",
        "        self.lin = nn.Linear(d_model, out_dim)\n",
        "        # self.lin = nn.Linear(d_model*2, out_dim)\n",
        "        # self.cls = nn.Parameter(torch.randn(1,1,d_model))\n",
        "        self.attention_pool = nn.Linear(d_model, 1, bias=False)\n",
        "        # self.out = nn.Sequential(\n",
        "        #     nn.Dropout(drop), nn.Linear(d_model, 3), nn.SiLU(),\n",
        "        #     nn.Dropout(drop), nn.Linear(3, out_dim), nn.Sigmoid()\n",
        "        # )\n",
        "\n",
        "    def forward(self, x, src_key_padding_mask = None): # [batch, seq, d_model], [batch, seq] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # batch, seq_len, d_model = x.shape\n",
        "        # x = torch.cat([self.cls.repeat(batch,1,1), x], dim=1)\n",
        "        # src_key_padding_mask = torch.cat([torch.zeros((batch, 1), dtype=torch.bool), src_key_padding_mask], dim=1)\n",
        "        x = self.pos_encoder(x)\n",
        "\n",
        "        x = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
        "        # print(\"fwd\",out.shape) # float [batch, seq_len, d_model]\n",
        "        # out = x.mean(dim=1) # average pool\n",
        "        # out = x.max(dim=1)#[0]\n",
        "\n",
        "        attn = self.attention_pool(x).squeeze(-1) # [batch, seq] # seq_pool\n",
        "        out = (torch.softmax(attn, dim=-1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, seq] @ [batch, seq, dim] -> [batch, dim]\n",
        "        # out = self.out(out) # optional (from GlobalContext) like squeeze excitation\n",
        "\n",
        "        # out = out[:, 0] # first token\n",
        "        # out = torch.cat([out, mean_pool], dim=-1)\n",
        "\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "batch, seq_len, d_model = 4,7,512\n",
        "model = TransformerModel(in_dim, d_model, d_head=4, nlayers=2, dropout=0.).to(device)\n",
        "# model = TransformerClassifier(d_model, nhead=8, nlayers=2, dropout=0.).to(device)\n",
        "x =  torch.rand(batch, seq_len, d_model)\n",
        "src_key_padding_mask = torch.stack([(torch.arange(seq_len) < seq_len - v) for v in torch.randint(seq_len, (batch,))]) # True will be ignored # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "print(src_key_padding_mask)\n",
        "out = model(x, src_key_padding_mask)\n",
        "print(out.shape)\n",
        "\n",
        "# GlobalAveragePooling1D layer, followed by a Dense layer. The final output of the transformer is produced by a softmax layer,\n",
        "# x = GlobalAveragePooling1D()(x)\n",
        "# x = Dropout(0.1)(x)\n",
        "# x = Dense(20, activation=\"relu\")(x)\n",
        "# x = Dropout(0.1)(x)\n",
        "# outputs = Dense(2, activation=\"softmax\")(x)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "AtPUcVu2kSLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Transformer Model\n",
        "import torch\n",
        "from torch import nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        # self.embed = nn.Sequential(\n",
        "        #     nn.Linear(in_dim, d_model), #act,\n",
        "        #     # nn.Linear(d_model, d_model), act,\n",
        "        # )\n",
        "        self.embed = nn.Linear(in_dim, d_model) if in_dim != d_model else None\n",
        "        # self.embed = nn.Sequential(nn.Conv1d(in_dim,d_model,7,2,7//2), nn.MaxPool1d(3, 2, 3//2))\n",
        "        self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, d_hid or d_model, dropout, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.d_model = d_model\n",
        "        self.cls = nn.Parameter(torch.zeros(1,1,d_model)) # randn\n",
        "        out_dim = out_dim or d_model\n",
        "        # self.lin = nn.Linear(d_model, out_dim)\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim != d_model else None\n",
        "        self.norm = nn.LayerNorm(out_dim)\n",
        "\n",
        "\n",
        "    def forward(self, src, src_key_padding_mask=None, cls_mask=None, context_indices=None, trg_indices=None): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # if cls_mask != None: src[cls_mask] = self.cls.to(src.dtype)\n",
        "\n",
        "        if self.embed != None: src = self.embed(src) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = src.shape\n",
        "        # src = self.pos_encoder(src)\n",
        "        if context_indices != None:\n",
        "            # print(src.shape, context_indices.shape, self.pos_encoder(context_indices).shape) # [2, 88, 32]) torch.Size([88]) torch.Size([88, 32]\n",
        "            # print(src[0], self.pos_encoder(context_indices)[0])\n",
        "            src = src * self.pos_encoder(context_indices) # context/predictor # src = src + self.positional_emb[:,context_indices]\n",
        "            # print(src[0])\n",
        "        else: src = src * self.pos_encoder(torch.arange(seq, device=device)) # target # src = src + self.positional_emb[:,:seq]\n",
        "            # print(\"trans fwd\", src.shape, self.pos_encoder(src).shape)\n",
        "\n",
        "        if trg_indices != None: # [M, num_trg_toks]\n",
        "            pred_tokens = self.cls * self.pos_encoder(trg_indices) # [M, num_trg_toks, d_model] # pred_tokens = self.cls + self.positional_emb[0,trg_indices]\n",
        "            pred_tokens = pred_tokens.repeat(batch, 1, 1) # [batch*M, num_trg_toks, d_model]\n",
        "            # print(pred_tokens.requires_grad)\n",
        "            src = src.repeat_interleave(trg_indices.shape[0], dim=0) # [batch, seq_len, d_model] -> [batch*M, seq_len, d_model]\n",
        "            src = torch.cat([src, pred_tokens], dim=1) # [batch*M, seq_len+num_trg_toks, d_model]\n",
        "\n",
        "        out = self.transformer_encoder(src, src_key_padding_mask=src_key_padding_mask) # float [seq_len, batch_size, d_model]\n",
        "        if trg_indices != None:\n",
        "            # print(out.shape)\n",
        "            out = out[:,seq:] # [batch*M, num_trg_toks, d_model]\n",
        "        if self.lin != None: out = self.lin(out)\n",
        "        out = self.norm(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "batch, seq_len, d_model = 4,7,64\n",
        "in_dim = 3\n",
        "model = TransformerModel(in_dim, d_model, nhead=8, nlayers=2, dropout=0.).to(device)\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "# print(out.shape)\n",
        "# # print(out)\n"
      ],
      "metadata": {
        "id": "-8i1WLsvH_vn",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ba31127-d6e5-438a-aede-5c789557ab39",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title SeqJEPA\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "def multiblock(seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "    return target_mask\n",
        "\n",
        "\n",
        "class SeqJEPA(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, num_layers=1, num_classes=10):\n",
        "        super().__init__()\n",
        "        if out_dim is None: out_dim = d_model\n",
        "        act = nn.GELU() # ReLU GELU SiLU\n",
        "        # self.rot_emb = RotEmb(d_model, top=1, base=10)\n",
        "        # self.rot_emb = RotEmb(d_model, top=torch.pi, base=10)\n",
        "        # self.rot_emb = RoPE(d_model, seq_len=1024, base=10)\n",
        "        # self.rot_emb = LearnedRotEmb(dim)\n",
        "        self.encode = nn.Sequential(\n",
        "            nn.Linear(in_dim, d_model), #act,\n",
        "            # nn.Linear(d_model, d_model), act,\n",
        "        )\n",
        "        # self.encode = nn.Sequential(nn.Conv1d(in_dim, d_model,7,2,7//2), nn.MaxPool1d(3, 2, 3//2))\n",
        "        self.context_encoder = TransformerModel(d_model, d_model, out_dim=out_dim, nhead=4, nlayers=2, dropout=0.)\n",
        "        self.predicter = TransformerModel(out_dim, d_model//2, out_dim, nhead=4, nlayers=1, dropout=0.)\n",
        "        self.target_encoder = AveragedModel(self.context_encoder, multi_avg_fn=get_ema_multi_avg_fn(0.85)) # 0.95 0.999\n",
        "        self.target_encoder.requires_grad_(False)\n",
        "        # self.classifier = nn.Linear(out_dim, num_classes)\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]\n",
        "        x = self.encode(x) # [batch, T, d_model]\n",
        "        # x = self.encode(x.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "\n",
        "        batch, seq, dim = x.shape\n",
        "        M=1 # 4\n",
        "        # target_mask = multiblock(seq, min_s=0.15, max_s=0.2, M=M) # mask out targets to be predicted # [M, seq]\n",
        "        target_mask = multiblock(seq, min_s=0.15, max_s=0.2, M=4)#.any(0) # mask out targets to be predicted # [M, seq]\n",
        "        context_mask = ~multiblock(seq, min_s=0.85, max_s=1., M=1)|target_mask.reshape(M,seq).any(0) # [1, seq], True->Mask\n",
        "        target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "        # print(target_mask, context_mask)\n",
        "        sorted_mask, ids = context_mask.int().sort(dim=1, stable=True)\n",
        "        context_indices = ids[~sorted_mask.bool()] # int idx [num_context_toks] , idx of context not masked\n",
        "        sorted_x = x[torch.arange(batch).unsqueeze(-1), context_indices] # [batch, num_context_toks, d_model]\n",
        "        # print(sorted_x.shape, context_indices.shape)[2, 9, 32]) torch.Size([9])\n",
        "        # print(sorted_x[0])\n",
        "        # print(sorted_x.is_leaf)\n",
        "        sorted_sx = self.context_encoder(sorted_x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "        # del sorted_x\n",
        "        # print(sorted_sx[0])\n",
        "\n",
        "        sorted_mask, ids = target_mask.int().sort(dim=1, stable=True)\n",
        "        # print(sorted_mask.shape, ids.shape, ids[~sorted_mask.bool()].shape, sorted_mask.sum(-1))\n",
        "        trg_indices = ids[sorted_mask.bool()].reshape(M,-1) # int idx [M, num_trg_toks] , idx of targets that are masked\n",
        "        # sorted_x = x[torch.arange(batch)[...,None,None], indices] # [batch, M, seq-num_trg_toks, dim]\n",
        "        # sx = sorted_sx[torch.arange(batch).unsqueeze(-1),ids.argsort(1)]\n",
        "        # print(sorted_sx.shape, trg_indices.shape)\n",
        "        sy_ = self.predicter(sorted_sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "        # sx = self.context_encoder(x, src_key_padding_mask=context_mask).repeat(M,1,1)\n",
        "        # # sy_ = self.predicter(sx, cls_mask=target_mask)*target_mask.unsqueeze(-1)\n",
        "        # sy_ = self.predicter(sx, trg_mask=target_mask)*target_mask.unsqueeze(-1)\n",
        "\n",
        "        # print(sy_.shape, target_mask.shape)\n",
        "        # print(self.target_encoder(x).repeat_interleave(M, dim=0).shape, trg_indices.repeat(batch,1).shape)\n",
        "        with torch.no_grad():\n",
        "            sy = self.target_encoder(x).repeat_interleave(M, dim=0)[torch.arange(batch*M).unsqueeze(-1), trg_indices.repeat(batch,1)] # [batch*M, num_trg_toks, out_dim]\n",
        "\n",
        "        # print(sy_[0])\n",
        "        # print(sy.shape, sy_.shape)\n",
        "        # loss = F.smooth_l1_loss(sy, sy_)\n",
        "        loss = F.mse_loss(sy, sy_)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]\n",
        "        batch, seq, dim = x.shape\n",
        "        target_mask = randpatch(seq//self.patch_size, mask_size=16, gamma=.75) # [seq]\n",
        "        context_mask = ~multiblock(seq//self.patch_size, min_s=0.85, max_s=1., M=1)|target_mask # [1, seq], True->Mask\n",
        "        target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "        # print(target_mask, context_mask)\n",
        "        sorted_mask, ids = context_mask.int().sort(dim=1, stable=True)\n",
        "        context_indices = ids[~sorted_mask.bool()]#.unsqueeze(0) # int idx [num_context_toks] , idx of context not masked\n",
        "        sorted_x = x[torch.arange(batch).unsqueeze(-1), context_indices] # [batch, num_context_toks, 3]\n",
        "        print(sorted_x.shape, context_indices.shape)\n",
        "        sorted_sx = self.context_encoder(sorted_x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "\n",
        "        sorted_mask, ids = target_mask.int().sort(dim=-1, stable=True)\n",
        "        trg_indices = ids[sorted_mask.bool()].unsqueeze(0) # int idx [1, num_trg_toks] , idx of targets that are masked\n",
        "        print(sorted_sx.shape, context_indices.shape)\n",
        "        sy_ = self.predicter(sorted_sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "\n",
        "        with torch.no_grad(): sy = self.target_encoder(x.detach())[torch.arange(batch).unsqueeze(-1), trg_indices.repeat(batch,1)] # [batch, num_trg_toks, out_dim]\n",
        "        # print(x[0][0][:8])\n",
        "        # print(sy_[0][0][:8])\n",
        "        # print(sy.shape, sy_.shape)\n",
        "        # loss = F.smooth_l1_loss(sy, sy_)\n",
        "        loss = F.mse_loss(sy.detach(), sy_)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        x = self.encode(x)\n",
        "        # x = self.encode(x.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "        sx = self.target_encoder(x)\n",
        "        out = sx.mean(dim=1)\n",
        "        return out\n",
        "\n",
        "\n",
        "# min_s=0.15, max_s, M\n",
        "# trg.15.2M4 C.85 1\n",
        "\n",
        "seq_jepa = SeqJEPA(in_dim=3, d_model=32, out_dim=16, num_layers=1).to(device)#.to(torch.float)\n",
        "optim = torch.optim.AdamW(seq_jepa.parameters(), lr=1e-4) # 1e-3?\n",
        "print(sum(p.numel() for p in seq_jepa.parameters() if p.requires_grad)) # 59850\n",
        "\n",
        "\n",
        "# x = torch.rand((2,20,3), device=device)\n",
        "# out = seq_jepa.loss(x)\n",
        "# print(out.shape)\n",
        "# print(x.is_leaf) # T\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title SeqJEPA old\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def multiblock(batch, seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "        mask_len = torch.rand(batch, M) * (max_s - min_s) + min_s # in (min_s, max_s)\n",
        "        mask_pos = torch.rand(batch, M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "        mask_len, mask_pos = mask_len * seq, mask_pos * seq\n",
        "        indices = torch.arange(seq)[None,None,...]\n",
        "        target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [batch, M, seq]\n",
        "        target_mask = target_mask.any(1) # True -> masked # multiblock masking # [batch, seq]\n",
        "        return target_mask\n",
        "\n",
        "\n",
        "\n",
        "class SeqJEPA(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.in_dim = in_dim\n",
        "        # if out_dim is None: self.out_dim = in_dim\n",
        "        if out_dim is None: out_dim = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.d_model = d_model\n",
        "        act = nn.GELU() # ReLU GELU SiLU\n",
        "\n",
        "        dim=8\n",
        "\n",
        "\n",
        "        self.calorie_emb = RotEmb(dim, top=1, base=10) # 0-4\n",
        "        self.steps_emb = RotEmb(dim, top=1, base=10) # 0-5\n",
        "        self.enc0 = nn.Sequential(\n",
        "            nn.Linear(dim*2, d_model), act,\n",
        "            nn.Linear(d_model, d_model), act,\n",
        "        )\n",
        "        self.hour_emb = CircularEmb(dim, torch.pi/2) # circular (0,1) # 0-24\n",
        "        self.temp_emb = RotEmb(dim, top=1/3, base=10) # 18-35\n",
        "        self.heart_emb = RotEmb(dim, top=1/3, base=100) # 50-200\n",
        "        self.enc1 = nn.Sequential(\n",
        "            nn.Linear(dim*3, d_model), act,\n",
        "        )\n",
        "\n",
        "        # self.transformer = TransformerClassifier(d_model, out_dim=out_dim, nhead=8, nlayers=2)\n",
        "        # # self.fc = nn.Linear(d_model, out_dim)\n",
        "        self.context_encoder = TransformerModel(d_model, out_dim=out_dim, nhead=8, nlayers=2, dropout=0.)\n",
        "        self.predicter = TransformerModel(out_dim, nhead=8, nlayers=1, dropout=0.)\n",
        "        self.target_encoder = AveragedModel(self.context_encoder, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    def loss(self, hr, temp, heart): # [batch, 1+T]\n",
        "        # batch = hr.size(0)\n",
        "        res_id, activeKilocalories, steps = hr[:,0], temp[:,0], heart[:,0]\n",
        "        # enc_summary = self.enc0(torch.cat([self.calorie_emb(activeKilocalories), self.steps_emb(steps)], dim=-1)) # [batch, d_model]\n",
        "        enc_summary = self.enc0(torch.cat([self.calorie_emb(torch.log(activeKilocalories.clamp(min=1))), self.steps_emb(torch.log(steps.clamp(min=1)))], dim=-1)) # [batch, d_model]\n",
        "        x = self.enc1(torch.cat([self.hour_emb(hr[:,1:]/24), self.temp_emb(temp[:,1:]), self.heart_emb(heart[:,1:])], dim=-1)) # [batch, T, d_model]\n",
        "        # print('violet fwd', hr.shape, enc_summary.shape, x.shape)\n",
        "\n",
        "        # # mask=(torch.rand(self.seq_len)<.1) # True -> masked # random masking\n",
        "        # mask=(torch.rand_like(x)<.1) # True -> masked # random masking\n",
        "\n",
        "        # patches pad -enc> patch_enc\n",
        "        # rearrange patch in padded/masked, -pred> pred of masked\n",
        "        # mse tgt_enc(img)\n",
        "\n",
        "        # average L2 distance between the predicted patch-level representations sˆy(i) and the target patch-level representation sy(i);\n",
        "        # F.smooth_l1_loss\n",
        "\n",
        "        batch, seq, dim = x.shape\n",
        "\n",
        "        # mask=(torch.rand(batch, seq)<.75) # True -> masked # random masking\n",
        "        # sorted_mask, ids = mask.sort(dim=1)\n",
        "        # # sorted_x = x[torch.arange(batch)[...,None,None],ids]\n",
        "        # sorted_x = x[torch.arange(batch).unsqueeze(-1),ids]\n",
        "        # sorted_mask = torch.cat([torch.zeros((batch, 1), dtype=torch.bool), sorted_mask], dim=1) # True->mask\n",
        "        # sorted_x = torch.cat([enc_summary.unsqueeze(1), sorted_x], dim=1)\n",
        "        # sorted_sx = self.context_encoder(sorted_x, src_key_padding_mask=sorted_mask)\n",
        "        # # torch.zeros_like(sorted_sx)\n",
        "        # sx = sorted_sx[torch.arange(batch).unsqueeze(-1),ids.argsort(1)]\n",
        "        # sy_ = self.predicter(sx, src_key_padding_mask=mask)\n",
        "        # sy = self.target_encoder(x)*~mask\n",
        "\n",
        "\n",
        "        # target_mask = multiblock(batch, seq, min_s=0.15, max_s=0.2, M=4)\n",
        "        # context_mask = ~multiblock(batch, seq, min_s=0.85, max_s=1., M=1)|target_mask\n",
        "        # sx = self.context_encoder(x, src_key_padding_mask=context_mask)\n",
        "        # sy_ = self.predicter(sx, cls_mask=target_mask)*target_mask.unsqueeze(-1)\n",
        "        # sy_ = self.predicter(sx, cls_mask=target_mask)\n",
        "        # # print(sy_.shape, target_mask.shape)\n",
        "        # sy = self.target_encoder(x)*target_mask.unsqueeze(-1)\n",
        "\n",
        "\n",
        "        M=4\n",
        "        target_mask = multiblock(M*batch, seq, min_s=0.15, max_s=0.2, M=1) # mask out targets to be predicted # [4*batch, seq]\n",
        "        context_mask = ~multiblock(batch, seq, min_s=0.85, max_s=1., M=1)|target_mask.reshape(batch,M,seq).any(1) # [batch, seq]\n",
        "\n",
        "\n",
        "        x = torch.cat([enc_summary.unsqueeze(1), x], dim=1) # [batch, 1+T, d_model]\n",
        "        target_mask = torch.cat([torch.zeros((M*batch, 1), dtype=bool), target_mask],dim=1) # [4*batch, 1+T]\n",
        "        context_mask = torch.cat([torch.zeros((batch, 1), dtype=bool), context_mask],dim=1) # [batch, 1+T]\n",
        "\n",
        "        # x = x.repeat(4,1,1)\n",
        "        # context_mask = context_mask.repeat(4,1)\n",
        "        sx = self.context_encoder(x, src_key_padding_mask=context_mask).repeat(M,1,1)\n",
        "        sy_ = self.predicter(sx, cls_mask=target_mask)*target_mask.unsqueeze(-1)\n",
        "        # sy_ = self.predicter(sx, cls_mask=target_mask)\n",
        "        # print(sy_.shape, target_mask.shape)\n",
        "        sy = self.target_encoder(x).repeat(M,1,1)*target_mask.unsqueeze(-1)\n",
        "\n",
        "        loss = F.smooth_l1_loss(sy, sy_)\n",
        "        return loss\n",
        "\n",
        "    # def forward(self, hr, temp, heart): # [batch, 1+T]\n",
        "    #     # batch = hr.size(0)\n",
        "    #     # batch, seq, dim = x.shape\n",
        "    #     res_id, activeKilocalories, steps = hr[:,0], temp[:,0], heart[:,0]\n",
        "    #     enc_summary = self.enc0(torch.cat([self.calorie_emb(activeKilocalories), self.steps_emb(steps)], dim=-1)) # [batch, d_model]\n",
        "    #     x = self.enc1(torch.cat([self.hour_emb(hr[:,1:]/24), self.temp_emb(temp[:,1:]), self.heart_emb(heart[:,1:])], dim=-1)) # [batch, T, d_model]\n",
        "\n",
        "    #     x = torch.cat([enc_summary.unsqueeze(1), x], dim=1)\n",
        "    #     # sx = self.context_encoder(x)\n",
        "    #     sx = self.target_encoder(x)\n",
        "    #     out = sx.mean(dim=1)\n",
        "    #     return out\n",
        "\n",
        "\n",
        "\n",
        "seq_jepa = SeqJEPA(in_dim=16, d_model=32, out_dim=16, num_layers=1).to(device)#.to(torch.float)\n",
        "soptim = torch.optim.AdamW(seq_jepa.parameters(), lr=1e-3)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "O-OU1MaKF7BZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ijepa vision_transformer.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/models/vision_transformer.py\n",
        "import math\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from src.utils.tensors import (trunc_normal_, repeat_interleave_batch)\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid height and width\n",
        "    return:\n",
        "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid_h = np.arange(grid_size, dtype=float)\n",
        "    grid_w = np.arange(grid_size, dtype=float)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    assert embed_dim % 2 == 0\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid length\n",
        "    return:\n",
        "    pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid = np.arange(grid_size, dtype=float)\n",
        "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=float)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega   # (D/2,)\n",
        "    pos = pos.reshape(-1)   # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
        "    emb = np.concatenate([np.sin(out), np.cos(out)], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x, attn\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x, return_attention=False):\n",
        "        y, attn = self.attn(self.norm1(x))\n",
        "        if return_attention:\n",
        "            return attn\n",
        "        x = x + self.drop_path(y)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvEmbed(nn.Module):\n",
        "    \"\"\"3x3 Convolution stems for ViT following ViTC models\"\"\"\n",
        "    def __init__(self, channels, strides, img_size=224, in_chans=3, batch_norm=True):\n",
        "        super().__init__()\n",
        "        # Build the stems\n",
        "        stem = []\n",
        "        channels = [in_chans] + channels\n",
        "        for i in range(len(channels) - 2):\n",
        "            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3, stride=strides[i], padding=1, bias=(not batch_norm))]\n",
        "            if batch_norm:\n",
        "                stem += [nn.BatchNorm2d(channels[i+1])]\n",
        "            stem += [nn.ReLU(inplace=True)]\n",
        "        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n",
        "        self.stem = nn.Sequential(*stem)\n",
        "\n",
        "        # Comptute the number of patches\n",
        "        stride_prod = int(np.prod(strides))\n",
        "        self.num_patches = (img_size[0] // stride_prod)**2\n",
        "\n",
        "    def forward(self, x):\n",
        "        p = self.stem(x)\n",
        "        return p.flatten(2).transpose(1, 2)\n",
        "\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "def apply_masks(x, masks):\n",
        "    \"\"\":param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
        "    :param masks: list of tensors containing indices of patches in [N] to keep\"\"\"\n",
        "    all_x = []\n",
        "    for m in masks:\n",
        "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [batch,T,dim]\n",
        "        all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "    return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "\n",
        "class VisionTransformerPredictor(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, num_patches, embed_dim=768, predictor_embed_dim=384,\n",
        "        depth=6, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.init_std = init_std\n",
        "        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        # --\n",
        "        self.predictor_pos_embed = nn.Parameter(torch.zeros(1, num_patches, predictor_embed_dim), requires_grad=False)\n",
        "        predictor_pos_embed = get_2d_sincos_pos_embed(self.predictor_pos_embed.shape[-1], int(num_patches**.5), cls_token=False)\n",
        "        self.predictor_pos_embed.data.copy_(torch.from_numpy(predictor_pos_embed).float().unsqueeze(0)) # [1, num_patches, predictor_embed_dim]\n",
        "        # --\n",
        "        self.predictor_blocks = nn.ModuleList([Block(dim=predictor_embed_dim, num_heads=num_heads, drop=drop_rate, drop_path=dpr[i]) for i in range(depth)])\n",
        "        self.predictor_norm = nn.LayerNorm(predictor_embed_dim)\n",
        "        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n",
        "        # ------\n",
        "        # trunc_normal_(self.mask_token, std=self.init_std)\n",
        "\n",
        "    def forward(self, x, masks_x, masks):\n",
        "        assert (masks is not None) and (masks_x is not None), 'Cannot run predictor without mask indices'\n",
        "        if not isinstance(masks_x, list): masks_x = [masks_x]\n",
        "        if not isinstance(masks, list): masks = [masks]\n",
        "\n",
        "        # -- Batch Size\n",
        "        B = len(x) // len(masks_x)\n",
        "\n",
        "        # -- map from encoder-dim to pedictor-dim\n",
        "        x = self.predictor_embed(x) # [batch, num_patches, predictor_embed_dim]\n",
        "\n",
        "        # -- add positional embedding to x tokens\n",
        "        x_pos_embed = self.predictor_pos_embed.repeat(B, 1, 1) # [batch, num_patches, predictor_embed_dim]\n",
        "        x += apply_masks(x_pos_embed, masks_x)\n",
        "\n",
        "        _, N_ctxt, D = x.shape\n",
        "\n",
        "        # -- concat mask tokens to x\n",
        "        pos_embs = self.predictor_pos_embed.repeat(B, 1, 1)\n",
        "        pos_embs = apply_masks(pos_embs, masks)\n",
        "        pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x))\n",
        "        # --\n",
        "        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1)\n",
        "        # --\n",
        "        pred_tokens += pos_embs\n",
        "        x = x.repeat(len(masks), 1, 1)\n",
        "        x = torch.cat([x, pred_tokens], dim=1)\n",
        "\n",
        "        # -- fwd prop\n",
        "        for blk in self.predictor_blocks:\n",
        "            x = blk(x)\n",
        "        x = self.predictor_norm(x)\n",
        "\n",
        "        # -- return preds for mask tokens\n",
        "        x = x[:, N_ctxt:]\n",
        "        x = self.predictor_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, img_size=[224], patch_size=16, in_chans=3, embed_dim=768, predictor_embed_dim=384, depth=12, predictor_depth=12,\n",
        "        num_heads=12, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0, norm_layer=nn.LayerNorm, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_features = self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.init_std = init_std\n",
        "        # --\n",
        "        self.patch_embed = PatchEmbed(img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        # --\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=False)\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "        # --\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "    def forward(self, x, masks=None):\n",
        "        if masks is not None:\n",
        "            if not isinstance(masks, list): masks = [masks]\n",
        "\n",
        "        x = self.patch_embed(x)\n",
        "        B, N, D = x.shape # [batch, T, d_model]\n",
        "\n",
        "        # -- add positional embedding to x\n",
        "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
        "        x = x + pos_embed\n",
        "\n",
        "        if masks is not None: x = apply_masks(x, masks)\n",
        "\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            x = blk(x)\n",
        "\n",
        "        if self.norm is not None: x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "    def interpolate_pos_encoding(self, x, pos_embed):\n",
        "        npatch = x.shape[1] - 1\n",
        "        N = pos_embed.shape[1] - 1\n",
        "        if npatch == N: return pos_embed\n",
        "        class_emb, pos_embed = pos_embed[:, 0], pos_embed[:, 1:]\n",
        "        dim = x.shape[-1]\n",
        "        pos_embed = nn.functional.interpolate(pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2), scale_factor=math.sqrt(npatch / N), mode='bicubic',)\n",
        "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
        "\n",
        "# from functools import partial\n",
        "# def vit_predictor(**kwargs):\n",
        "#     model = VisionTransformerPredictor(\n",
        "#         mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "#     return model\n",
        "\n",
        "def vit(patch_size=16, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, # tiny\n",
        "        # patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, # small\n",
        "        # patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, # base\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "bNjp4xFsGPoa",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title show collated_masks\n",
        "# print(collated_batch, collated_masks_enc, collated_masks_pred)\n",
        "# print(collated_batch) # tensor([0, 1, 2, 3]) batch\n",
        "# print(collated_masks_enc) # nenc*[M*[pred mask blocks patch ind]]\n",
        "# print(collated_masks_pred) # npred * [M * [context]]\n",
        "\n",
        "\n",
        "# print(len(collated_masks_enc[0]), len(collated_masks_pred[0]))\n",
        "# print(collated_masks_enc[0], collated_masks_pred[0])\n",
        "\n",
        "h,w=14,14\n",
        "img = torch.ones(h*w)\n",
        "# img[collated_masks_enc[0]]=0\n",
        "img[collated_masks_pred[0][2]]=0\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    # plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.imshow(npimg)\n",
        "    plt.show()\n",
        "\n",
        "# imshow(img.reshape(h,w))\n",
        "\n",
        "# for masks in collated_masks_pred:\n",
        "#     for mask in masks:\n",
        "#         img = torch.ones(h*w)\n",
        "#         img[mask]=0\n",
        "#         imshow(img.reshape(h,w))\n",
        "\n",
        "\n",
        "for masks in collated_masks_enc:\n",
        "    for mask in masks:\n",
        "        img = torch.ones(h*w)\n",
        "        img[mask]=0\n",
        "        imshow(img.reshape(h,w))\n"
      ],
      "metadata": {
        "id": "tf4T37woBV2V",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aH1UOD4WW8I2",
        "outputId": "6fafea1b-0af2-4bc7-fa26-46b5e56549b4",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vit fwd torch.Size([4, 3, 224, 224])\n",
            "vit fwd torch.Size([4, 196, 768])\n",
            "vit fwd torch.Size([4, 11, 768])\n",
            "predictor fwd1 torch.Size([4, 11, 384]) torch.Size([4, 196, 384])\n"
          ]
        }
      ],
      "source": [
        "# @title ijepa vision_transformer.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/models/vision_transformer.py\n",
        "import math\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# from src.utils.tensors import (trunc_normal_, repeat_interleave_batch)\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid height and width\n",
        "    return:\n",
        "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid_h = np.arange(grid_size, dtype=float)\n",
        "    grid_w = np.arange(grid_size, dtype=float)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    assert embed_dim % 2 == 0\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid length\n",
        "    return:\n",
        "    pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid = np.arange(grid_size, dtype=float)\n",
        "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=float)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega   # (D/2,)\n",
        "    pos = pos.reshape(-1)   # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
        "    emb = np.concatenate([np.sin(out), np.cos(out)], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "def repeat_interleave_batch(x, B, repeat):\n",
        "    N = len(x) // B\n",
        "    x = torch.cat([\n",
        "        torch.cat([x[i*B:(i+1)*B] for _ in range(repeat)], dim=0)\n",
        "        for i in range(N)\n",
        "    ], dim=0)\n",
        "    return x\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x, attn\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x, return_attention=False):\n",
        "        y, attn = self.attn(self.norm1(x))\n",
        "        if return_attention:\n",
        "            return attn\n",
        "        x = x + self.drop_path(y)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvEmbed(nn.Module):\n",
        "    \"\"\"3x3 Convolution stems for ViT following ViTC models\"\"\"\n",
        "    def __init__(self, channels, strides, img_size=224, in_chans=3, batch_norm=True):\n",
        "        super().__init__()\n",
        "        # Build the stems\n",
        "        stem = []\n",
        "        channels = [in_chans] + channels\n",
        "        for i in range(len(channels) - 2):\n",
        "            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3, stride=strides[i], padding=1, bias=(not batch_norm))]\n",
        "            if batch_norm:\n",
        "                stem += [nn.BatchNorm2d(channels[i+1])]\n",
        "            stem += [nn.ReLU(inplace=True)]\n",
        "        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n",
        "        self.stem = nn.Sequential(*stem)\n",
        "\n",
        "        # Comptute the number of patches\n",
        "        stride_prod = int(np.prod(strides))\n",
        "        self.num_patches = (img_size[0] // stride_prod)**2\n",
        "\n",
        "    def forward(self, x):\n",
        "        p = self.stem(x)\n",
        "        return p.flatten(2).transpose(1, 2)\n",
        "\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "def apply_masks(x, masks):\n",
        "    \"\"\":param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
        "    :param masks: list of tensors containing indices of patches in [N] to keep\"\"\"\n",
        "    all_x = []\n",
        "    for m in masks:\n",
        "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [batch,T,dim]\n",
        "        all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "    return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "\n",
        "class VisionTransformerPredictor(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, num_patches, embed_dim=768, predictor_embed_dim=384,\n",
        "        depth=6, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.init_std = init_std\n",
        "        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        # --\n",
        "        self.predictor_pos_embed = nn.Parameter(torch.zeros(1, num_patches, predictor_embed_dim), requires_grad=False)\n",
        "        predictor_pos_embed = get_2d_sincos_pos_embed(self.predictor_pos_embed.shape[-1], int(num_patches**.5), cls_token=False)\n",
        "        self.predictor_pos_embed.data.copy_(torch.from_numpy(predictor_pos_embed).float().unsqueeze(0)) # [1, num_patches, predictor_embed_dim]\n",
        "        # --\n",
        "        self.predictor_blocks = nn.ModuleList([Block(dim=predictor_embed_dim, num_heads=num_heads, drop=drop_rate, drop_path=dpr[i]) for i in range(depth)])\n",
        "        self.predictor_norm = nn.LayerNorm(predictor_embed_dim)\n",
        "        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n",
        "        # ------\n",
        "        # trunc_normal_(self.mask_token, std=self.init_std)\n",
        "\n",
        "    def forward(self, x, masks_x, masks): # [batch, num_context_patches, embed_dim], nenc*[batch, num_context_patches], npred*[batch, num_target_patches]\n",
        "        assert (masks is not None) and (masks_x is not None), 'Cannot run predictor without mask indices'\n",
        "        if not isinstance(masks_x, list): masks_x = [masks_x] # context mask\n",
        "        if not isinstance(masks, list): masks = [masks] # pred mask\n",
        "\n",
        "        # -- Batch Size\n",
        "        B = len(x) // len(masks_x)\n",
        "        # print(\"predictor fwd0\", x.shape) # [3, 121, 768])\n",
        "        # -- map from encoder-dim to pedictor-dim\n",
        "        x = self.predictor_embed(x) # [batch, num_patches, predictor_embed_dim]\n",
        "\n",
        "        # -- add positional embedding to x tokens\n",
        "        x_pos_embed = self.predictor_pos_embed.repeat(B, 1, 1) # [batch, num_patches, predictor_embed_dim]\n",
        "        print(\"predictor fwd1\", x.shape, x_pos_embed.shape) # [3, 121 or 11, 384], [3, 196, 384]\n",
        "        # print(\"predictor fwd masks_x\", masks_x)\n",
        "        x += apply_masks(x_pos_embed, masks_x) # get pos emb of context patches\n",
        "\n",
        "        # print(\"predictor fwd x\", x.shape) # [4, 11, 384])\n",
        "        _, N_ctxt, D = x.shape\n",
        "\n",
        "        # -- concat mask tokens to x\n",
        "        pos_embs = self.predictor_pos_embed.repeat(B, 1, 1) # [batch, num_patches, predictor_embed_dim]\n",
        "        pos_embs = apply_masks(pos_embs, masks) # [16, 104, predictor_embed_dim]\n",
        "        pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x)) # [16, 104, predictor_embed_dim]\n",
        "        # --\n",
        "        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1) # [16, 104, predictor_embed_dim]\n",
        "        # --\n",
        "        # print(\"predictor fwd pred_tokens\", pred_tokens.shape)\n",
        "\n",
        "        pred_tokens += pos_embs\n",
        "        x = x.repeat(len(masks), 1, 1)\n",
        "        x = torch.cat([x, pred_tokens], dim=1)\n",
        "\n",
        "        # -- fwd prop\n",
        "        for blk in self.predictor_blocks:\n",
        "            x = blk(x)\n",
        "        x = self.predictor_norm(x)\n",
        "\n",
        "        # -- return preds for mask tokens\n",
        "        x = x[:, N_ctxt:]\n",
        "        x = self.predictor_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, img_size=[224], patch_size=16, in_chans=3, embed_dim=768, predictor_embed_dim=384, depth=12, predictor_depth=12,\n",
        "        num_heads=12, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0, norm_layer=nn.LayerNorm, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_features = self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.init_std = init_std\n",
        "        # --\n",
        "        self.patch_embed = PatchEmbed(img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        # --\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=False)\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "        # --\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "    def forward(self, x, masks=None): # [batch,3,224,224]\n",
        "        if masks is not None:\n",
        "            if not isinstance(masks, list): masks = [masks]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        x = self.patch_embed(x)\n",
        "        B, N, D = x.shape # [batch, num_patches, embed_dim]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        # -- add positional embedding to x\n",
        "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
        "        x = x + pos_embed\n",
        "\n",
        "        if masks is not None: x = apply_masks(x, masks) # [batch, num_context_patches, embed_dim]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            x = blk(x)\n",
        "\n",
        "        if self.norm is not None: x = self.norm(x)\n",
        "        return x # [batch, num_context_patches, embed_dim]\n",
        "\n",
        "    def interpolate_pos_encoding(self, x, pos_embed):\n",
        "        npatch = x.shape[1] - 1\n",
        "        N = pos_embed.shape[1] - 1\n",
        "        if npatch == N: return pos_embed\n",
        "        class_emb, pos_embed = pos_embed[:, 0], pos_embed[:, 1:]\n",
        "        dim = x.shape[-1]\n",
        "        pos_embed = nn.functional.interpolate(pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2), scale_factor=math.sqrt(npatch / N), mode='bicubic',)\n",
        "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
        "\n",
        "# from functools import partial\n",
        "# def vit_predictor(**kwargs):\n",
        "#     model = VisionTransformerPredictor(\n",
        "#         mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "#     return model\n",
        "\n",
        "def vit(patch_size=16, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, # tiny\n",
        "        # patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, # small\n",
        "        # patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, # base\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "# encoder = vit()\n",
        "encoder = VisionTransformer(\n",
        "        patch_size=16, embed_dim=768, depth=1, num_heads=3, mlp_ratio=1,\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
        "\n",
        "# num_patches = (224/patch_size)^2\n",
        "# predictor = VisionTransformerPredictor(num_patches, embed_dim=768, predictor_embed_dim=384, depth=6, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02, **kwargs)\n",
        "predictor = VisionTransformerPredictor(num_patches=196, embed_dim=768, predictor_embed_dim=384, depth=1, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02)\n",
        "\n",
        "batch = 4\n",
        "img = torch.rand(batch, 3, 224, 224)\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/train.py\n",
        "mask_collator = MaskCollator(\n",
        "        input_size=(224, 224),\n",
        "        patch_size=16,\n",
        "        enc_mask_scale=(0.2, 0.8),\n",
        "        pred_mask_scale=(0.2, 0.8),\n",
        "        aspect_ratio=(0.3, 3.0),\n",
        "        nenc=1,\n",
        "        npred=4,\n",
        "        min_keep=4,\n",
        "        # allow_overlap=True)\n",
        "        allow_overlap=False)\n",
        "# self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
        "\n",
        "\n",
        "\n",
        "# v = mask_collator.step()\n",
        "# should pass the collater a list batch of idx?\n",
        "# collated_batch, collated_masks_enc, collated_masks_pred = mask_collator([batch,3,8])\n",
        "collated_batch, collated_masks_enc, collated_masks_pred = mask_collator([0,1,2,3])\n",
        "# print(v)\n",
        "\n",
        "\n",
        "import copy\n",
        "target_encoder = copy.deepcopy(encoder)\n",
        "\n",
        "\n",
        "def forward_target():\n",
        "    with torch.no_grad():\n",
        "        h = target_encoder(imgs)\n",
        "        h = F.layer_norm(h, (h.size(-1),))  # normalize over feature-dim\n",
        "        B = len(h)\n",
        "        # -- create targets (masked regions of h)\n",
        "        h = apply_masks(h, masks_pred)\n",
        "        h = repeat_interleave_batch(h, B, repeat=len(masks_enc))\n",
        "        return h\n",
        "\n",
        "def forward_context():\n",
        "    z = encoder(imgs, masks_enc)\n",
        "    z = predictor(z, masks_enc, masks_pred)\n",
        "    return z\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    m = next(momentum_scheduler)\n",
        "                    for param_q, param_k in zip(encoder.parameters(), target_encoder.parameters()):\n",
        "                        param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "\n",
        "\n",
        "# # num_context_patches = 121 if allow_overlap=True else 11\n",
        "z = encoder(img, collated_masks_enc) # [batch, num_context_patches, embed_dim]\n",
        "# print(len(collated_masks_enc), len(collated_masks_pred)) # nenc, npred\n",
        "# print(collated_masks_enc[0].shape, collated_masks_pred[0].shape) # , [batch, num_context_patches = 121 or 11], [batch, num_target_patches]\n",
        "out = predictor(z, collated_masks_enc, collated_masks_pred)\n",
        "# # print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title RandomResizedCrop is same across batch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# dataiter = iter(train_data)\n",
        "dataiter = iter(train_loader)\n",
        "img,y = next(dataiter)\n",
        "# img = img.unsqueeze(0)\n",
        "# # img = F.interpolate(img, (8,8))\n",
        "# b,c,h,w = img.shape\n",
        "# img = img.flatten(2).transpose(-2,-1).to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "print(img.shape)\n",
        "\n",
        "from torchvision.transforms import v2\n",
        "resize_cropper = v2.RandomResizedCrop(size=(32, 32))\n",
        "resized_crops = resize_cropper(img)\n",
        "\n",
        "# imshow(img[0])\n",
        "# imshow(resized_crops[0])\n",
        "# imshow(img[1])\n",
        "# imshow(resized_crops[1])\n",
        "\n",
        "import torchvision\n",
        "imshow(torchvision.utils.make_grid(img[:8].cpu(), nrow=4))\n",
        "imshow(torchvision.utils.make_grid(resized_crops[:8].cpu(), nrow=4))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "id": "NLHOwdqK222R",
        "outputId": "a211c62f-141d-461a-ecfe-70bd061d6194",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 3, 32, 32])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAErCAYAAAB+XuH3AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAesRJREFUeJztvX2UHFd55/9UVXdXd0/P9GhGmhnJerEMJpLBBmODLczuZo2yjpcQiHWS4OMkDvEv/GBlB1tnA2gTYOMAcpKza8JGmIXjNZsTvE7829iJOcE+rACznCP5RWCCMZZlLFuypJmRNNPvr/Xy+8Oh7/N8S13TLY1aGuv5nKNz6vatunXr1q2a0v0+L1YYhiEpiqIoiqIMCPtsd0BRFEVRlPML/fhQFEVRFGWg6MeHoiiKoigDRT8+FEVRFEUZKPrxoSiKoijKQNGPD0VRFEVRBop+fCiKoiiKMlD040NRFEVRlIGiHx+KoiiKogwU/fhQFEVRFGWgnLGPj507d9KFF15I6XSarrrqKnryySfP1KkURVEURVlCnJGPj7/927+lbdu20Wc+8xn6wQ9+QG9961vpuuuuo9nZ2TNxOkVRFEVRlhDWmUgsd9VVV9E73vEO+qu/+isiIgqCgNasWUO33XYbffKTn4w9NggCOnLkCA0PD5NlWYvdNUVRFEVRzgBhGFK5XKZVq1aRbcevbSQW++StVov27t1L27dv7/xm2zZt3ryZdu/eHdm/2WxSs9nslA8fPkyXXHLJYndLURRFUZQBcOjQIVq9enXsPov+8XH8+HHyfZ8mJyfF75OTk/T8889H9t+xYwf9yZ/8SeT3O+64g1zXXezuKYqiKIpyBmg2m3T33XfT8PDwgvsu+sdHv2zfvp22bdvWKZdKJVqzZg25rqsfH4qiKIqyxOjFZGLRPz6WL19OjuPQzMyM+H1mZoampqYi++tHhqIoiqKcXyy6t0sqlaIrrriCdu3a1fktCALatWsXbdq0abFPpyiKoijKEuOMyC7btm2jm2++ma688kp65zvfSV/4wheoWq3Shz70oTNxOkVRFEVRlhBn5OPjN3/zN+nYsWP06U9/mqanp+ltb3sbPfrooxEj1FPhmpv+H1EOg6DrvnFexKhJxXoch92LAVSKdmLaxCUnC9ySbNv0z4G+OmzfpO3IOlkUxyagMsHaseA6ULLjblMJC/tqyjYcGOdudd8X/qJr3Y2/9WuyPw7cL95fS16XxcYuacsp3mjUoX9m31ZL7ttqtjrb69etE3XTM0dEuVg8bs5PkoDN0SDwRZ3Xbpu+1RuiDueom8l0toeGR0RdNp8327mcqMtkh+S+braznXCSos5mMzM6e/HKwi7bsvxnd34u0hLndz90S2fb9+X4iLELA6jD3pm+t31ZWW95XY+LPqbmOkO4ZLEvHCjmJL4zQnzfyCM5bEoSTHsxt4nkteD4iGaxnci95G3K/uA7TrQDc/Tb3/y7rvve/Du/37UOCdm1RO9PyOqgr3BzeX0/7UT7E1d3atEqTj2UxGmEoIj9MwfzOei+8//3918/9T78C2fM4PTWW2+lW2+99Uw1ryiKoijKEkVzuyiKoiiKMlD040NRFEVRlIFy1uN89ItDqEcaUHuLK0csRWK0wSBG20VNOE5T4/hQDkHr5s1E7CisOHsQWU6wcgLsL5IJYyvhJuRUSDpyX24T4tugqzJtMITvWRy7XnVOKwHH2aiZmz7Y0NdWw9hqBFZb1Pm+B2V+DnwczDmx37aF48P2RfsZVo5cvXXSzZMiuoAavtVlP4rOH3FOGNc4W4BoD3m5u83HQnA7j8BHuw5m87FAm/wy4224Yoy4iOT4RC4rrg/hSbaibUbaiRlW1OHxxSXMPCIvo5N27V92xXcj3+5uR4eTqz9zhzj7i96OW/hYmM+iiO10f77j5jOef7HSgMS1E1fX45R8rRiZ0OwcOH/i5uEioCsfiqIoiqIMFP34UBRFURRloCw52cVHP7kgbimv+1IRuugGQnYJu9a91mr3c0p3u5ilsoWWkIW7n2yHCzRSRIjKLh53gwW5JMGWuFuelH2SKNEwaSPismvx5UuKBfvXDXS5JE9eaRCa+jCU0kqbuVWm3LQ8Du87W8e2QQzjcyTSH0AuVXeX+yJL2kH3Ogv+bxDrxh03fyNz3e9a5zA5CSUqdOW0uZtuVJ+gXgmC7n3nRCWr7u7ysdLBAu0ImSwib7HnEvoqZshCq/i9SmiR8ZBlhz2X6BoZht3dpqOjzMer+/J7lN7vs5y+8a79vcoyC+8XJ2XE/H0Iu8s30Xdcj1LygrvFvUd7l9SEXNKHe3Hk2RNu5YufYV5XPhRFURRFGSj68aEoiqIoykDRjw9FURRFUQbKkrP5qDM3SiLUqdA2g6DcPfyxrOrd5gORu8a5Ry2kwsZobEJ/BG3SB5dQvnOMW2U0vLosczddx+4eph1NOmx0/cX4710BPR1sPkT45UhId7MdgK0GusimksZuoSWnlrSjAD0d26GYeUgxthrCNiHiw4c2IL0Sc36S993z5EVXaoXOdqlaEHWppLSfmVi+ip3x1DVhDDkviNGvw4hbLt/uJ9Q4ls0vUROl7u7F+HTJ8+O9ZZsR12heB670DrrLMxsdT56j3WahBWLDu/fr+nr6nKqNx0LtxBnbRFNq9GMPErdvzH7ClX7x7SZOhvDijhnnhe4Bf4Ti7GNOFV35UBRFURRloOjHh6IoiqIoA0U/PhRFURRFGShLzuZj+vAh+QPXzCMaVncbkIj6FnYtROhj1zODCHuL/ugx4bLhooMYWwQH4nyI+AZ4Dnaog2HHoZ0ew3yQDQPrYTyKBNPl4Rxtpn17YCuStNHmpHvABZnWG2JcxIZjjgnNHKnrfg6M8xF/DnMshknGdtvMzqNSrYi6V6df6mw3GnVRt3J8rSgvX8bT1Eu7DSeZol7hcT7iNHy0W/AxrkXAx7L7+eyILt89lUDUjqx7/BAYdahEmyo2f+FI/ozA1KaEgzZd7Ixo08Ur/YVeVN3DvYuYQ5Hr6t2OQcaN6cf+4dRiieCxcXOiH9uRfohNVnCKJiCnE5q+n3b5eyMIFv8Pna58KIqiKIoyUPTjQ1EURVGUgbL0ZJeDL8IvMXGKT9l/7NRDRS8ap3zK7iFz46Mk937C+NXC3rNDxrYCoeDjQmmj26ttcwlAtuMH0D+hykhJRobRh2XziNuy6Jw8B78HsT6OcI6Ii2z3pXFehe7NhdIJUT4xP9vZTqVdUZdJDXW2lw9PiboVY5OiXK2XOttHZl4VdZPjK6lXRHj1mPmBsgu6wAtiQqb34dQe71YPdVyKQ7fKaPZrPre638y4jNY/P1P0qJP8skD4e8vu7pLKiao3vb83WhFfdtGDnts51fP3c87oMMeFJeg1vHofYetjLiv6CokL/dB7Hbqno2S92OjKh6IoiqIoA0U/PhRFURRFGSj68aEoiqIoykBZcjYflEjK8qlKfn2ETY7CNXypk8WbTsSEd4/rTkwzMVLygs3E9icSBpxtR6TLuPDhC3Swa88wZDraYzA3MDjWZrPa8lHXhHDrYffQ8OJ8EVdbWR9jeQS2Ad2194WU47hzcD253pIusoePSvf0YsXYgAwPD4m6qfyFne3x/ApRNzSUE+WXj77Q2T505GVRV6tWqVe47UYkLTzfL8aeiQhsLrqbRsSHuIezom2PaDLGrR3dZ6MpG9h2bLoGtHPp7tCL7pBhzIsiNmw8dMePm759mGqkUtz9uvcD+3NJxdADcXXWSbdPVuYuzxHTjZh2+M4Re52YC0P7izg38jgbEJzbwr4qpg7bORPoyoeiKIqiKANFPz4URVEURRkoS052sSAran9LQ736Ni30Tcbd9mKym56GW1qv9COzYGRScdwCa3m9rnxG3A/jlg9j2nGcrGzHxWXrtmnH6p4V1UnI+RKA91giwe8fRD+NuZcWyEBiVd+KW/7uPq4x3n2RTmBVwC5s9sQxUVeE7LQOe4aaFTkgLxz/aWf71elXRN3GjW+T7VZMu77fFHXlepF6JfD58xTn2hqPWP6G0KAOE+e8EOYLTn3eZtw5+shajW7BQdhdNuRTxAPZMOIqHneOmMi6kb6zYyMuzGdg9X2hiKKxmWJj2omTT+LqUNbF7MEcjHLre2Y+RaIUszAALR8zc3efI8mkNC9wHPanOuYcr5WZ+zXIzG2W5dsP4LhIlmh+XbTo6MqHoiiKoigDpe+Pj+9973v0vve9j1atWkWWZdHDDz8s6sMwpE9/+tO0cuVKymQytHnzZtq/f/9i9VdRFEVRlCVO3x8f1WqV3vrWt9LOnTtPWv/nf/7n9MUvfpG+/OUv0xNPPEFDQ0N03XXXUaPROO3OKoqiKIqy9Onb5uP666+n66+//qR1YRjSF77wBfrjP/5jev/7309ERH/9139Nk5OT9PDDD9MHP/jB0+stUSQDpRBIYzONLkAkxDE/R9wP3d2nInYU/HQ9dyz+4Mg54sJuo7uocPcDvS8mUj2GF5YaOfr04f3qzXrETY6IctOT7qMJEQ5ahgjnWW6btnT5bKENiug92o50zzjroCsesyOwAhlGOuGbMrbTYjqrF5Mx9bVjuc2HPD/PVFtmthhERK2G7I/rmPFKpeUroO2ZLLfHZmXGW6IfiVI6nTEFS7aTy8v7F0dcxkzu6ho374ikZm4FMe6zC7k8Cs/x7rYSEdffmHD8UTOK7u8tvm9kaGLfKd1tPiLvt0jmY74dExo+Eqa9dz9Y2XUMUY5792bnEXWJRfsH5koP/sXcriORkPMX26ky13HPk+8JPJbD7SYaDWkX1YRw83zOOGDbyPvTbsn/yKfTaVHOpMxzmUpC6gmWFRnfdlbEnojZjpzrNh8HDhyg6elp2rx5c+e3fD5PV111Fe3evfukxzSbTSqVSuKfoiiKoiivXxb142N6epqIiCYnZQKqycnJTh2yY8cOyufznX9r1qxZzC4piqIoinKOcda9XbZv307FYrHz79ChQwsfpCiKoijKkmVR43xMTb2WgntmZoZWrjQptWdmZuhtb3vbSY9xXZdc1z1p3UmBuA22MDiIMVQgosAKu1UBoEdGdu7N5qMvM5JYuvvdowSL57TbRlfMglaYYrpiAH7uNThnm7UbYOjzOGlZFsnh8QSoO8O5UXlcA2Jw2EbnRJ98n+msLU/afKA/vzQNALsOprMGTWn/kGrLdrPtsjm/h6nDzTWn0ylREyRMnQtjV27KuABROwZWx0N7w01IYAwDrh9Dm23PlP22VIWnXz0iyqmMuZb8uLTxSDqQBiEGHvPCBq1baPoLxJ/g7cTtGjGTipmzUZOLMKau+3FooxNy27WYd1EkBkhMiPnINYuxQxuPGH0faqSdUu8hwk+P7vE5uO3GQjYfvOwkZF2S2Wq02vKZLc+XRblaq5l9m21RJ+JqxMTgwKFKQCwPbjtiwYudX9ay0VFRd/Dgq6LcZM4dY6PDom7iX/5GExGlbLD3whhEwr4y3h7tVFjUlY/169fT1NQU7dq1q/NbqVSiJ554gjZt2rSYp1IURVEUZYnS98pHpVKhF198sVM+cOAAPfPMMzQ2NkZr166l22+/nT772c/SxRdfTOvXr6dPfepTtGrVKvrABz6wmP1WFEVRFGWJ0vfHx9NPP03/9t/+205527ZtRER0880309e+9jX6+Mc/TtVqlT784Q9ToVCgd7/73fToo49G3IFOlWi2SLYcFOMWh8cGcSlMF4vF8qcFxHXhKiguKTMJIpeSy3xDQyaEeSRELyxfTleM7FAH/78kdy+GcL4pSy6jJxJm3xnqDoYXHnbyohxaZunTB9dWLrskbClzkCP73mqxfUHZSdimXW9O2iLVGzVR9tvGja7dlnJJKm1kRZQnWi1zXL0ll3rbDele3GKhtlNDcjk1nTXudbkhmam2WpV9bdTMsmw6J5/LkIWADiDcchLmj8PupeXIiVgozlOvSFdOqGRlzArqYxZO/iqAZuLceWP7Fil3z/4an5BgAT2yxx5EXW9ttmeMBrugyy53RY6MntkCd0wLUxLEwtvtHhKcCKUVgjr7pNtEURdV6U4r9y2VjWdlYb4APe0+lqWSTB2wbNlYZ3tkRL6n+HvslVdkuoKL3nBR1757nnyHzM7OdrYtUECqFSkBLxsf7Wyn2HuBiChg7bogAXtnTEI7OX1/fPziL/5ibD4Vy7LozjvvpDvvvPO0OqYoiqIoyuuTs+7toiiKoijK+YV+fCiKoiiKMlAW1dV2IMSGTe5aFSmjrinTzS/gThYT3rfbfpHzR5SrPvrDZa+mdJ+1IzFzmQsmGDUkmItzCOOKqegzrB0XOs9DjScSUkdMge1GEPbqsgX2Bkk5VS3LtBuE8hwhF8aHZDs1kiGOrdDYsqTBLbfFbDBqDVlXKst2uKseprFekTPj7LXlccWCsccolOS9rNalLUu9aPYNHOmensyacfcttNWQ96RwYq6z3YScSzz8cgbsQTK57vpxO+J+2PurRYS9jkkXEEmJHgnR3d3GIe4xtcHeSYYl7+7a2q/DvOxQ9yrpzosh3PFAZvOBbp783QhHJSHsdiplnncb7KJqdTPvfB9ddvux+ehONN09c5EFeyJuGyFSzVPU7Z6XiyVph1Qum+d7cmpCnh9G7Njx451ttF90XfN8TUzIdgoFYx+C9inJlOy7x2zFRpdJ25H5gnlmK3VpC+ZmZH8cdi+HwC23Vjbvu+xQTtRh/6IB2BcXXflQFEVRFGWg6MeHoiiKoigDZenJLjHZD8OIT1Z3uSLqkhoT1zCSSZftGVFoumcC5LtGllPjMm1CJDqbHZsAd0yMkumzDIweXHS9Zo5tgHyD8KiYLizj88yjHmRqnC9KN89aI/48PyfhoMwiL6zV5sv88rpSSdO/XHpUNuzJxIWJZqGzHTRlX7kUZcNy7khOjsFc0SyZHitIiSbpmmNbMD6lqrmOKmSfbYCUUW+ZssWWgYmIrKxZag1HZN8SIFlxSa1ekTKQkzR1uTHIFgyuikHbjHutJOdhsy77Hge/tQmQ+7iEFs0U293XNcaTdEEv2Dg32HihpXcZhi/rR9w6Y49DGcY83w7IbU6M+2waoko7CRbZ1pfzkGdpxvtqOVnqnd7cZ4nkPEAphT+LKMeiq22Tvddwvqxdu5a1I58ZzIA7z1xxp1iU0Nf6mjjpNpGUZCanZM6zoawcO+4O7qbk/eERwwtF+X5Bt/9i0Ug0U+waiYgazO3e86WsEpVdOIuf1lZXPhRFURRFGSj68aEoiqIoykDRjw9FURRFUQbKkrP5CNH+guu8NmY+jfVnk+U4m4+4OMqoQ/Mm8ZysOw5cRxLOkWRuc0nQYJPclTOSRVGekmv4eFV1Zi+C7oaonXKbizq4etVYuelJHdGHdiP3qEfaXoxra9w9gJtgBdLmxGkad7u5uYKom6sYfdSGc4xkpHtvuWb6V6rJ+5WYN2NXqcuwyR4LmV4D19o2aLIiGy3a1swe62wPp1eIOguy045wV8GWrKswu5NkIF34Rl3p/mcxyfrEvLSlqUAI6jjm5+e61nH7olQS7BTALggzFnPCuGc2Mkm6vzfiojufCdDl07LweTf3zw7BNdI39hkh1NUg5D4P898Au6z5gnFR9TzZn1VrLu7W9Qj8HRLNPotpGLgdBYRMZ7ZHiQS48sO9nGNu5ehOOzJiUh2EELfeg/dYLmfSGeTzo6KOuwWjrYhIEwFTp1iU6RT4rS4W5fOUYs+B60qX96wr7VXKc9Od7aOvTos6/t5Iwdil09IGRbh8n2J6gjh05UNRFEVRlIGiHx+KoiiKogwU/fhQFEVRFGWgLDmbjyAixzJ/efyUAv9w7uqOCpbF0oPbEROP7jYFUfdnlvYc+uoy3TUNIbgTkELZZnq/Azoi93u30lIHDyOxTcwm6qFc08M4Gs2mtLHg8TnqLdnXwOL9kXok6vI8nkkwf4K60WxJTdoLZXwBn11LswU6b8HopRZo9GMuxvIw7c6Xpf/8zDGjyWJY+HlXXledxeSwIRx0hdlyQFcp7RrdtQ22Gb4nd+bxOkKI28DjbKQL0h5keEjeEy/F4o5k5DnqrA+rLpAxAi5Ys1qUK3UTqnl8Qvb9xAljg/LST16lOLIZozXPz8kQ2MdmTDuYZhxjPGRY+vDMkNSvM+wcqQTGdJA2BRUWThxtCnj6AD/GFCzuvfAa7L0FerrN7DoSNoZMxzli7rXvy/EJWbnpyTlRrcvnoMLSsqMtAg+pvv6iXxB1KYhHEQd/b+F7AeNjCJuPJNqHmDJec7EobY1++tOfdraPQ2ycjZds7NrXdku+b3hcIWyHv2I8D95T7D3uummoQ/ud7jGiaiwmU9uTlX4N3mksbPzsoSOiLjNsbFeCfHz4dB6uv/e0GL2jKx+KoiiKogwU/fhQFEVRFGWgLDnZhZyYLoP7lmXhch0PxQ6HcncpdOeFJVOHLUFlYH0swxxsXViqSgjNSHbAT+F1sXLEK9i022rLZUcLVtL4Un0qJZeQm0w+KdbAfRZcOX02dk5uSNQlLS4HwHWhZ3RkbE9OBZYS6xB6vNU0ffdhqZNCUzeSGxFVDkz5g0fMcnPdk8vxNbbUiq53LXSDZdftwDxssnbaobwOHioaFzbbsK7vk2mHu1ATEXks1HllTrpKXjB1oShnJk02y/DoQVE3Prmqs718hXRNbIPs4TAJws1AWPRo2uauZJhUl14pl6aXjy/vbJfBPbRclkvsPIPokSOHRZ3PZE4Mj71mzRpRfv6nP+5sDw9L92I+JpjBmUs0KGOiaymvT0LY+gyT4kJws6/X5L3ly/wNlEqrTEopSdfNMi7Vs9uVSsp7sGaNkd/Glo3Dcb2HlJfus/JdlEyivGXmUxJcbbn85sOczOVkptbL3npZZ7sA7uBc+T52bFbUcTdcIqJR5l6LcgmXr9FtnM/JCy6ALLIQzoBLNI2GvJc8Ay9eR/nQz0TZYWkicsNSKp0cNyHe0aMcpXYhDZ4BF3Nd+VAURVEUZaDox4eiKIqiKANFPz4URVEURRkoS8/mI9E91XrEVgNELV4Eb0ji6m0WjCxcCE3MbTkSGKqZuX6hzQm3SfHhOB9sCgIRQl02w7XTTFrqsxamlGaa6DykaK81mQsonMPKSFfFJGvXhnH12HX6UIc2IDGRqwUlSBsdRNI/m+086LzDw0avHQKXy/qctAUoV42GjuHMfWaj02zh+eE6WYdsuGHck7LZAJdqy9yDSDj1hDxHiruW2nifzbEeuFViOOY1ay7sbFvgjp4ZMvY86F03fXhGlFuBcbWt1AuizrJ6D6MvQ4hDmgEWVnoUbBFG8stEeYrN9VpNuoseOnigs83TrBMRHT0qXYHLRePuWygWYF8zf5K2fBdxm6oRCMF90UUXyXaOGBfIRkPO9Ynlxs7Fhv8fBuCiX6qa6+R2AUREVWHXIdsZGRkT5bExY8tRKVdEXZ7bP8C70O7jPgs7jpQ8LgXutNxWDd08+bszgNgLOGdz2VF2fmmrVq6YeVDDtAeeHEvbMWOJIQvazO6u1YQUCeyVe/CgfPekICx6kx1br8s52mB1tZJ0R8858p4kWeqH+VJB1LkFY5OSBDfuZcy+igj/fvVu29MruvKhKIqiKMpA0Y8PRVEURVEGypKTXWx0tWVLn+DNJrO/ElGGuaWlwD0zyRwdHVi7C2E5nC9xN1Fm4F2DDnFXKly251kLiYhctiSH8g3PwlkBl9gquKT6bLwCcA0M2bcnykcB9M9zurvTxqyaRyKMUo/ZEdOujJyYG5LSCo+KiWPJl9UxA6/jyqXXHJNo6selrDCUMX3AbocwRzzmfudAiFxe8qEhLrXgUGEUyIC5NXoQ5XDZuFk2XzEhXWSHs1J6qrKleq8tnwOXZY5tQd30jFw2rtTMEi4uyqYzaeoV/pxgglmRWROOC/CmMAkglxsVVevfYCJzlity2boyd0yUJ0aNnFMHV/Z55t7b9qRcwlWOVEo+s1U45/TRQ53tQkHWvXrolc42uramIaLxXNHcA4yQu3KlcbNctUq6XObABZ1PZ3htivdPJN833rAYZIRTzJotx6vEXINfeeVlUTc+brI243MwOyujJvusPsB3XEyEZ4w6GxfhU7gbY+iFtJFAVq1aJepcuJezsyZyagGizPKM13VwiV0zKecIH9npWSnfVJj79fLRYVGXcKS7M3dPx+zKi4GufCiKoiiKMlD6+vjYsWMHveMd76Dh4WGamJigD3zgA7Rv3z6xT6PRoK1bt9L4+DjlcjnasmULzczMdGlRURRFUZTzjb4+Ph5//HHaunUr7dmzh771rW9Ru92mf/fv/h1V2VLOHXfcQY888gg9+OCD9Pjjj9ORI0fohhtuWPSOK4qiKIqyNOnL5uPRRx8V5a997Ws0MTFBe/fupX/9r/81FYtFuvfee+n++++na6+9loiI7rvvPtq4cSPt2bOHrr766tPusNWSeleSuUQlwf7BAW0sZG5iLQwty/U/sNVALZ7r0Gib4DB3shb0leua2ay0PYjYhzCJ0YPA2x7TJz1X6vnpnNTtPKZd1qE/XNrFKMkB2huw/lmYhVOUpWAcRrTC3mw+JlZMirLXBtuWitGEuQ0DEVGjZlwFUdueWiVDab/hzW/vbLee/ZFsp2VCjzu2tH8IfNSAzXVFsk4yd0AnKe1ukmlz/5Jg9xMtZ9i2vD/5sdHO9vhy6TLnZuRce/WgsSmo1aXdgstcbYslGSq6VpNhnT1mD5HPS925XpPumnFwPT2aQdqU/VDaX6C7Onf7DMEF1M0afbvekKHFwVORVubN+I1MyXlYYtmWS0Vpq3HksHHZrdbkuO5/cb8o11hWWQez7LJ3SAXuT6FcEOW2yGpLso7Z7KBtxosvyBXrVRcYm5CJiRWizmcvB7RZ6uf/r/wd1wBX0pkZGd78hRdeMHWzkJk1Y+y/1q6TLswtTBvN5kE/dgvoTiuaxMzhvF04rtk25Z8896yow1DsIpS/JZ9vcf+grg45LLyGsXML4f5wW5IMvENSrnxm0+ne7bZOhdOy+fh5+uKxsdd8xvfu3Uvtdps2b97c2WfDhg20du1a2r1790nbaDabVCqVxD9FURRFUV6/nPLHRxAEdPvtt9M111xDb3nLW4iIaHp6mlKpFI2Ojop9JycnaXp6+qTt7Nixg/L5fOcfJnlSFEVRFOX1xSl/fGzdupWeffZZeuCBB06rA9u3b6disdj5d+jQoYUPUhRFURRlyXJKcT5uvfVW+sY3vkHf+973aPVqoxVOTU1Rq9WiQqEgVj9mZmYiaax/juu65ILdRBwO6KwJFj42QNsI0OUt5sdsg55uMW05Bf1BfZ2HN8f4HNxfHH3ZuTbYgr454GNtZ43elkxBummubbekLUK1IbXUKrPz8DEQSopdZ0THhBTgwiYG4qBwnX6huB7Yhy4cPyZXyriNBxFRm10X6tn8XrZT8l6iYj3MQgpn8zLkdPXllzrbTYj3kMD4/OzeBqAtJ1iK9GVjMiT4snFzzkxGhkF3XdBcWej+GqSXb7Pw78mUPK4F9jKz0yZex9DIqKw7ZvT1l196UdRZJOfz8LA5FkOd1yFlexw8ZgrGU+DTqVIqirpyCUNgs/DdSXiG02Zseehuoui7gM+SZSMyFkI2YewNeBwYIqJc1pzj2KyMHTI9K20aPBHfBexcmJ1AG+y0IrYI7Ng2zFG+2lyrStuRSlmO5fCwuZbDJXnOC9ZebE4Hz28/0R+4rUStJt9T+55/QZSPHTPjh6vh8wXzLjh6VL4nciMQ88LpHoMjLtcDprsIY2wCebsh2CVxG4vDh18RdQ68c4+fMDFKJiZkTBB+230w0Judh1QUbM60LfkuSLA5Ml8oiLq5efm3dePGjaaw+GE++lv5CMOQbr31VnrooYfo29/+Nq1fv17UX3HFFZRMJmnXrl2d3/bt20cHDx6kTZs2LU6PFUVRFEVZ0vS18rF161a6//776R/+4R9oeHi482Wdz+cpk8lQPp+nW265hbZt20ZjY2M0MjJCt912G23atGlRPF0URVEURVn69PXxcc899xAR0S/+4i+K3++77z763d/9XSIiuvvuu8m2bdqyZQs1m0267rrr6Etf+tKidJaIKJgAF0y2bO7CqtoQrOvk2fKq5cvlsTbLGlitgpsgtDO2zCyVo8tswJZss0OyzuGhq6GvTVhOrbAQ4TVwGS6xJfcqhMD2YH0sZO6Z0aUz3gnMPts9bHy0obB7VVwo9hiOzx6VZ4DleL5iGQk/z/ZtQxh9XLb2/e7hu1PMjbnRlPMFJT4+Xpixk8uKGHacS3gOZJi1Sd73RtUsy84fl3N0aPkFnW0Ploxxib3NZJiZaRkAsNY07eJzkEnLEPdJ5jZ84piUGTAbaxz8ukMfZpplyqN5KYFwmYOIqMFcDCtVGVb/xDEzBq2WrBuHdn2WouCVQwdEnccyDaPsU2futaUySkLdZcyRYXn+C6bMkntEWoLnibuvVkDq4squDzHTU+BGeYRJNPUqhGlf80ZzPpJYPbrOE8lXQQqkZJTI8ywr8Lq1a6Eh49J8ePq4rHNQwmchAiKyC9tG6SsmvEIkWzrbTkAm6nmWRTYP82z1qpWifPiIuQfz8zJMfCJhrgseb0rn5d+ZRNY8l35Vylseex82GrKh0dG8KPNnr9cQCf3Q18cHapMnI51O086dO2nnzp2n3ClFURRFUV6/aG4XRVEURVEGin58KIqiKIoyUE7J1fasAqGibaaZp0FHXAFucuuYW2MO3O2IuSAdn5HuW21wVRwZMdpYAtwhW8wWIMA00cy9rFiRevocaLsVZufRBJGPn4MS8pojeiQv9pjOnohOJnqy7bgDQdu2QEvt8fQB2OTgkQG3+SB0zzT7euB+iPYQPB02pqIPPO6WJt3QSszFm4ioyUJ2J2Fuua7RYJPg+ms7pi5Bsq8tsNVotE1fl18gPc1cZv8wOyPtZQpzMoxzm7lnF+akC3OF2S04CXkvKy05R6tls296SNpfTEye3LX+ZMhU6+jGaDbRbsEFF9l83oTSXwFzvcFsuubmpJ1AuyFtQPjcmj8B+7L57EM8c57SPoRX64UXyTDgs8wVdygjbWlWXXBhZ3t/9XlRN7FChj6fYHO20ZDvqTbrXwueg8NHXxXlJnvfXPgG6eYpQgaE3W1yFoTtmkhIu6gEzLUEe2aWjUkX+KPTxm3Zg5QaaM8j8kbEhEyPdDXGBiREuxtWDgN5XTwtxJrV0mV4fFy6Bbda5h797CXplsv/zATgaluuoj2Raadal+PB0x400/L5idp88HZ7H7te0ZUPRVEURVEGin58KIqiKIoyUJac7IIuoClWHMvKDK8TILsMsaW84Yzcl9iSbhoiF+KSZZ1HiYPslXMskmChIl3fijWzBFaF5UIPlgTF0h5GFbRivhlxWbS7N62QHKLN9ONaFdMOutqGvS3fxWWNJZLLvZitkmc7bcPSuI+RZZlslRuGZccLzDgnXSlvpdOyHLD5gxF7szkzD1EqsJjMkLCkW3AdIpW6zI2wBS7WMyzSYxGiAGO0Rj6WKEuxxM/k473CucUi3QZpeY5ly6U8EAeP9Bi5l2xccdaj+zWXyVB+zLD7t/YCKSvgXOdRRVvwnLZ5f1AiInNvGw2IiAvZjMeWGTdLHyTG48eNTFYEOfaCVReIss3eBS64zyaY9JRMyWtcv/5NoszH0raldBCwdlBmQdffOLik5sLzNDEBWZFZ1luM5svd5X0P3qNNkNBY3zESMndTxsvA94RoE8r8HtihvC4+t3MQegHv5fy8kVl9HyJXV02dBRmbMat3EJhjXZCAsyyi8fhyOearwPWXS2NBsPjrFLryoSiKoijKQNGPD0VRFEVRBop+fCiKoiiKMlCWnM1HJPsqE+uwqu1Jde4Es8EoNWToau7O2mhjpli5b4HZeTRh3wpz6auBXsxlxBCy2IZJcJFl4rsFdgvSZRZtKrAYZ7vRXa+1IGurkNdR+xfniwm9Hn9KQRCgSyzafDA7AWiTh2puwL1D90hu8xHitzjTcttgN5EG+6LhnCljX4dZqOgE2HzwbL2FealXF4uo5RY625UKZCktmXY8cA1PoDs2z67sSXsDn+v76JoNtkY2u850WrraZnPSfTQOcfvAjoPXRRMioy2UaAja4dcFcxsmEE+DMAR2N7xVtInhRRdsPNpgQ5BMmvmC5gVNFjJg4yUbRV0+L8eV24vg7Yp78h1MScDGEm1prC7bRCdJhh0DzybsJGTvrrr6HaL8k2d/0tk+cUKGGi+zuT4G7qHr1q4T5QqzwTtyFFzQWejzkZERUTcyLMeZhyX38H3Mxy6QzxN/jXqQgbwC4Ra4fZFlyXuQYWkZyvheiERFMMe+6ZKLRd2ll17e2U5BmAi8l9y2pj8bwN7QlQ9FURRFUQaKfnwoiqIoijJQ9ONDURRFUZSBsuRsPnzQnhpMfztYKIi6w+AjzxVL1LB4u5DVO6L383TqmKbZ577kGI/DkdYRJ+/Zz9u1utZJFrCp4GGBI7vyNNF4WIw9SMwP0aOw4d6+d9E2A+GKKPrk11lqcQf0/HZTpphOMH0/EjeCDU/KlTYemYzU9Llfvg/arm2bfYvMl5+I6MDP9ps6mL+NuoxvkBR2BPJmei1p28KJ2M+wexKAXRTXedGWxoZzhmx+jy2X4dRtiEUQRxB2n6T8OfUDrOsei8GOGoh0wGc2gAsNYtIQcFujaAhubtQlj4Po4eJ9A7dAxIlxhmTMmCBij8HiWMTZfy0g2VvdX03ih0hYjz7ifPDxwnvH43oQyRQFhw9LW40SSzuQSkp7pkjofGaTx1MgEBE1mqacJxkTanxsVJR5TBfbkXPbYv+Ht+Gdz+OQNODdMz9fkP1hYf5XQBj9K99xZWf7+9/7nqg7fkKmT3DTZs5cdNGFom4oZ2yzggDnC8RwEc+Q2nwoiqIoirLE0Y8PRVEURVEGypKTXVA6aLEfmrhcGllujtMguHYA32SRZVhWxkyJ3AW0V7/Sk3Sny9miO0dCXqO7KOtPJFx2zEkgjHJs8ko+dgu5ZPW4TNv247PRtlgWzhZkJXVYdtgcy0D82um7y2S4rM/LDrir4hL3sWMmS2m1LF3oKkwGaoE8MseWTFFmCWBuecwt1ga/uIDJVDgHsMxlDjwHiaVxWeVB1t2htHFHHB5BF9Des2DG7cuX6gOUS2Lkkag8ESOX9NTL+L79/CwnOx9RNC0El0tgFV+6OEaGpneJKOY1Ed1XjEL3zNh9qCwREiwDOY4dhnS/6A1v6GxPrZTh8JtMvsDUAW0PUhSwlBZr69INt8GkHpREPHBBb7CM5JF3EZN28DiflT3oG6ZIOHz4cGfbgay/z/742a7nx7EcGTZuw9WqfN9MHzFpGJLg9p9MyM8B3geUkxYDXflQFEVRFGWg6MeHoiiKoigDRT8+FEVRFEUZKEvP5gNTz8dpmTFhguN8UiMufDEniUigvQqteGAkRi5324uxOYk7f+SU8a5VccTafIgmF8clq1qW4cNboMnaodFL0xCaPsvcYIdzUteMeGAyERttD8KQi91Sg63VZH8adeNSV8awyUzbdV3ZH4cJ/hi2OYi4GzOXR7D54KnNIy6x6A7ud7ejEPvidEFXVxZKen7miDwHhHiPQ+j2MX7cOLXi5lqcPUb0sN4NruLOaTFdHO9P1ObDYOPDxafdAucPYmx0hIlbH/ZncUTvQe/HOmjc0iNZSGUgzx8/1zmBj2PH7L3AjgJd/bnLLoZe4GHRW5BSo9Ew74l6Tb7TSixMPJF0kW02pa1Gi6XtGBsbF3XLRsdEmdvWzEzPiLpMxrjaptMyvDo/PxGRm2Iu36d47+LQlQ9FURRFUQaKfnwoiqIoijJQlpzsgpk2uYtYZPUy4k4bs0QpovhZXWpO1m735d04cAkb+2rFyDdhTEjR2CXlBYKhdjssWlwgc+0i0KrJ6LQJuLlcTslmpZThsqyguFqIsl3cciJfOkclrtFowb7mUUomZH/4cisuvVoxbsrRTL7Ufd/uVZG702buf+hqK4/F80O02LY59sihaVFXgOiNccS5i8re4PhAPZcgej57n1JhnOxi88jH6BIL4yx6KOu4hLZQz6Sc0v291c9bKpIJuw/1OA7uhopzabGIm89xEk3kWQPZLMXcUiFhscg4G++6Hn83PRZeAGUfLgtF6mL2jbqVx8mG8RnSFxtd+VAURVEUZaD09fFxzz330GWXXUYjIyM0MjJCmzZtom9+85ud+kajQVu3bqXx8XHK5XK0ZcsWmpmZiWlRURRFUZTzjb4+PlavXk133XUX7d27l55++mm69tpr6f3vfz/95Cc/ISKiO+64gx555BF68MEH6fHHH6cjR47QDTfccEY6riiKoijK0qQvm4/3ve99ovy5z32O7rnnHtqzZw+tXr2a7r33Xrr//vvp2muvJSKi++67jzZu3Eh79uyhq6++evF6LeiejjE2LHk/Z+huOnISWYzZjsQpotiXuBDu6KbXhwvvInm+Dpw8uMimXTlVsxlj15GBzJ/cnTWALLr9yJp8Vx9C9Xu+DI3MQyc3IYR6hYVb9yBsPM+W2WhgZlqceKZHSUiTyttBUPfl2ntEh47xqY7UsH0x5HOpUMW9uxIbij3GNiBiA8KeoX6e/X4eEWFhETOZFrYjMfU2XiK3TQsgNH7EroOFYo+cIzZVbR9wuySs6t12oy3cUE/d5kOmyYC6WLuOaEvdjkM7JFEP7fAUDZH7Hue6HpMVOW76OPCsY+jzJEsFEWe/s5BtT5xNzGJwyjYfvu/TAw88QNVqlTZt2kR79+6ldrtNmzdv7uyzYcMGWrt2Le3evbtrO81mk0qlkvinKIqiKMrrl74/Pn784x9TLpcj13XpIx/5CD300EN0ySWX0PT0NKVSKRodHRX7T05O0vT09MkbI6IdO3ZQPp/v/FuzZk3fF6EoiqIoytKh74+PX/iFX6BnnnmGnnjiCfroRz9KN998Mz333HOn3IHt27dTsVjs/Dt06NApt6UoiqIoyrlP33E+UqkUvfGNbyQioiuuuIKeeuop+su//Ev6zd/8TWq1WlQoFMTqx8zMDE1NTXVtz3Vdcl23a32E2JDpSO9pvaWu2bs9Rqy/fHzs99iykAojunzvzQoJv7sJQeSS48c1zuYENeneYwZwsjkZ+nd8mUzZ7jCh3E3LdPcpFl6dx98gIkq68ns7Rsolj2m5AdQ2ajKE+rHZo53tYrEo6losVHJES2btRsOpA+J+yetK87jxC9i1+F6MRn2KwV8i9hd9aPoy6nWcZh5v0BSnUfdn12HOE7HriInBER8dqGszZEPMf2ErBgYhPs4fNmUCjBfC0tRH5gDMNX6/wkgo+MWJyVFnKezxmk+d+Ng4ca/guJg2ce1g+g35TMe872AYo/Ft2LugrzQicVV9vH9jbFLOKZuPnxMEATWbTbriiisomUzSrl27OnX79u2jgwcP0qZNm073NIqiKIqivE7oa+Vj+/btdP3119PatWupXC7T/fffT9/97nfpscceo3w+T7fccgtt27aNxsbGaGRkhG677TbatGnTGfR0URRFURRlqdHXx8fs7Cz9zu/8Dh09epTy+Txddtll9Nhjj9Ev/dIvERHR3XffTbZt05YtW6jZbNJ1111HX/rSlxa3x324MsXtG11KZMtckQyzp7jkFLNauVC2yjjvtljVI85jF6vOhBtu5CSnFoq9WpcuqevWgOzCMtnajjyHkzLTGsOiR1buZcxyuS8Prw6DVQavrLm5uc42z0D5WqsxroHc3Y+AyNIvy8AbE5I8Ou3gutgkcZyYZf0F3CpF32OuayH4FIn1Il9gwvLaiOAa2zBKK91aXeC4GLkGd+WyIUoQcZKEhZmX2eI1LtXz+xy9dxD+XWhfveu6/bxCfB7WPzI+kKWZjUGc+2zU0zfG/Tmmb9z9fKFzLiTR9F7Xfd9+2uzr2evjhsl0BYv/x6Kvj4977703tj6dTtPOnTtp586dp9UpRVEURVFev2huF0VRFEVRBop+fCiKoiiKMlD6drU926DeZXNdaoE09SITfUQo6y207UKgk5o8Rx86/anKdn2EkY53oDs1W42Fz9obR2fmRXntmklRXj6cZ2cArZu5GDrgqoihrNssLHoQcc80O/Pw6URE9XpNlGNDhMekeo8L23yShsxmxE6Baf847QNM2W52SCUcUecx7d/Da4rNFnAaD430Y+y+2wLlvuw6eFX8U9v1nHHmBlEbBtib/bcPe43zsNv5sV3LkvcyEGG/5b1MOuhyztzK+/JL7t0Nl6eltxc4znbMtaAtS9yxbbDd4PcBw5DzsOgy9PvJXF1j7B96fb4jdTH7xriKL+RGHvdOibPdwOfgTNt86MqHoiiKoigDRT8+FEVRFEUZKEtOdgn27pHls9QP5czy4svTseXznXZLRqislBtd9hxcH06Vf3rkoUVpRzm32XmPekEqBl35UBRFURRloOjHh6IoiqIoA0U/PhRFURRFGSj68aEoiqIoykDRjw9FURRFUQaKfnwoiqIoijJQ9ONDURRFUZSBsuTifPzeH35alP2YVNCRGCAxqbLtmLpIoHHf67Yr2UkzpBZUJoiH+oXwyzF9DR08ianE8OGRXVkfHAgv3G6Y2BAvv7hP1FWKx0Q5aZnw4hiqmfdgeHhY1LmZIehQsrP5N1/vHt/h2t/+f0X5wuGUKDcCc9aXCk1R12LxoS2SsSiGLRlGefVwtrPtJOTjINKeW3Jgj9TkGMywMqYr55PEjgneHQ1h3Htq8zii/WFniGSTNz8sGM48ppa3s/eBe2KPvPC97N7CHG3ZZlztpBzzTFKGE8/w5yKEuZ40/avVIQT3nOxPLjBz1C/K+VzP1zvby4/LeTfimeMsOV0JIp9TMGT2LboyRkspb67TyckD7bYcZ4898PW2HJ+EZ8YgEcob7cObP2Tj7LRgX5fN7QrUwXW9/JAcE84HfvW9pp2Y+RIB5yiftIsV9XuBbBK9pqk/ve7whiC8O6tbKLw6v5TIvqKIdXFlWffNR79Dp4uufCiKoiiKMlD040NRFEVRlIGy5GSXVFJ+L3lsjSmSzTNyNM8K2v27C5efnEAu05aKxzvbuByfXzZh2vFlXbtlliQbNZkVtVmXy5XJlFmWHV85Ieq4PICXgVflsGtJwBp7o20kiHZLLv3idYUx2YPdTMZsp9OizrJljxLJ3qac70m5xIcskyl2LVnLg315B6BdmCNtJqHxTJpEMsMrTDsadWXDhbrpbzOQdXHiSexyc0Q4FAuqMcfFn0MIlSGe35wzKsnEn+VUabFxHh+VdaWWGfhqHWQFXDZuMskI9EefS19lOQdH5jKiPDWyrrO96m2Xibq5wmxnu1j8iTyHZ/SbyrDsayUn+xO4rDwEUgqTVppl+Ryk4KaECaPvyCePyGXyUQuyvdbCqihnmbyTwrcIy4DbzIMcEJfNGeBySWTex6ouMfue+rQ7IyxWd0J8+OISNvfRrlCsFj9RbV/oyoeiKIqiKANFPz4URVEURRko+vGhKIqiKMpAWXI2H6i9hza3RZB16OrKtTEfdp6ZMa6l5VJF1KVDqZe2ambfdFraCcwcmelsN+ptUdesGzuPalmeo9WUNh8ZZkcxdeE6UbdyzarO9vjyZaLOAtdbPlyo77fqxm3Qb4OLHNi5+J65FrSNSDM7D8uGKRVjWxNHuy3PX2/KvmcTpuxaUhcvMx0atVMfbFnqTWPrkkwmRZ3N+o7uqllwMRx1zb7Ha5BqnvUhKrNyN9xuNf/STIzw293hPGrDxDV0lJb5wai1x3sjYql3QdljNkS1grzvns/GDq4jEcq5Nto0z4zvynPUiuaepGfkfR5feaEo56fM85Z05F0ZGV5hzn/RJbKvcy91tlvDJVE3FkgX78aMef6rYB+SSplzplvy/FYC7MhSxuZjmb9S1E0sN9fhWfL5frn+Y1Eut+dN39Fllz0HHrj6RidQd8JF8kmNcwdfNJuLqK9tb/vG+r0ufNZu58e532td3L7xrrV4XM+n6Bld+VAURVEUZaDox4eiKIqiKANlyckuDpb55xNIDh64gc3NGVe448dPiLpXXj7U2S4X5ZJp2pLtuI6RIEZGsqJurmCkjGYL3OuYW2dktRvWtRotc476gVdEXciuM5uWoRTTwzKiqMOjqsJSXovJQKEnl4XjZBcuCRERJZNmjRtlF3S17ZWoe50sJxKm3SEYgwJzaY6ugsp2uKttAFIcn04+zDx081zGZJdSQ8ouHl+VjdEuFlwytvpaw+0KP9LH+8xcmts+RrKV99JmDx/e5WSi9/vulowM0kiDK2fFjKU9LEcoU5XySa5h6otwD/ySuU5PqqFUtGdFuXLclPGZSQ9xOVTKHJNveGNne1lburJSQ5bnjh3obBeOzos6n7nhDsOEsbPy2QtGpzrb4/mLRd2yvOmfB9JxpVkUZa9h+pADl+Zy3PxtnprLdwjyZz8RTmOr+tFdYt1Xu7v3RqKzht2vK7ZvvSs7sfKNhe7Xp6iRLFY7vaIrH4qiKIqiDJTT+vi46667yLIsuv322zu/NRoN2rp1K42Pj1Mul6MtW7bQzMxM90YURVEURTmvOOWPj6eeeor++3//73TZZTIC4B133EGPPPIIPfjgg/T444/TkSNH6IYbbjjtjiqKoiiK8vrglGw+KpUK3XTTTfTVr36VPvvZz3Z+LxaLdO+999L9999P1157LRER3XfffbRx40bas2cPXX311afd4RT0mCeYRU2tUquL8gv7X+hsl+cLoi5k4YdzKflN5thS77dCUz5xQmq5beY26LjSFiFgWSaDID5Msc3cPh1wAeXaf70iz78sLX0M+bE+hFiu18yxAQjhAdiAOMx2I5OWurPjmJtiO/IGoWqYcHqbcha4OBLYktisPzkXRVDjPltvoR2H3JffIR9tHJiLoQ33C5xpKcNsHLKO3PdIhdmVgDu4F5pxj9hUoBsjM0JJwTjy0PleIHsXQrst39RXIcx/m82DFoTRx/5wjdiBukwC5mwMa2tm3yrcL69m2q004P6E8jor7NhyRc7nY3NmPuM9cArSxsthl4LT8A1vWNPZTibgWUsYe6vxjLQF84ZGRdl1jXu6e+gFUVeaZTYg4EaeaMlysmRsN+qlfxZ1jSOmnHTlOyxLZVFey14NyyA0/XzTDNghmYWBnOO9h1dfvBS05xb9uJX3flx3N/dTPd+5ximtfGzdupXe+9730ubNm8Xve/fupXa7LX7fsGEDrV27lnbv3n3StprNJpVKJfFPURRFUZTXL32vfDzwwAP0gx/8gJ566qlI3fT0NKVSKRodHRW/T05O0vT09Enb27FjB/3Jn/xJv91QFEVRFGWJ0tfKx6FDh+hjH/sYff3rXxdRLU+H7du3U7FY7Pw7dOjQwgcpiqIoirJk6WvlY+/evTQ7O0tvf/vbO7/5vk/f+9736K/+6q/oscceo1arRYVCQax+zMzM0NTU1ElaJHJdl1zXPWldL1hMw0fd2W9L3TdkadqzcE43w8qgmXvo/8xCPjuO/AhzAhNjogXt8JgXNmh6WBa2ERDOvNUy+nWpKP31cxBfYWL58s6235Z2HO2GsYlB/dyDseOxPSJhyIU9BlwHxl7BIAtdOF6S1zWSHJHllNHXk7bUQAMWMv3ocRnG3oJ9/SFzLWNZOSf4PUCfd7T5SDJDAZekbc2JOROOvwkhwa2Y0MwwdCItfALC1vPzR+1KIL4LKwcRuw72PMH/TVBp5mMAUWKoBeHx4xhndi/BCTmys8wkBe0vfFeeo8HGpAKxKhoxtmGYdoDfaycBtjXMdgNj2Ii4PpgRHcKrjyw378OhZdI+5Ed7TOjz/fvkinHalXZsjmXsttDKhpt/pdOyr5kMlNMsVoU06aJxFv/GqUOKBmkydF6yONF3SMQwj9p1vD7sPDh9fXy85z3voR//WOYE+NCHPkQbNmygT3ziE7RmzRpKJpO0a9cu2rJlCxER7du3jw4ePEibNm1avF4riqIoirJk6evjY3h4mN7ylreI34aGhmh8fLzz+y233ELbtm2jsbExGhkZodtuu402bdq0KJ4uiqIoiqIsfRY9vPrdd99Ntm3Tli1bqNls0nXXXUdf+tKXFq19XN4l2ywJJmEpOgXLohnm3pYAl1S/xXzIAnBzgmXrNpNTUq7c1wnN4mcqlHIJXwhGt0pc+uVL5zasv8+dOG7O5+VEXSKQ2SuzLByz7UvJw/bNmmkChIRZyOzrsf4sn4TleCa72BhO3Ze+eQl0G+7CkRMFUR7PyuNW5s1SNXqkjjF7pP0BhK725c5ltjzf9uQYZJgLL4YeDmBOeOyG5SEE9mjG9P1VyHTschkGQ07DdSVFll1Jy0MhyNBGt27WMIaR5udIwRz1I/KNOacXxrsix+GyFMFBBcKAM/fadApkVXAJbTMX4hq65Yp3A45e93zCGXhPhIHZ98TxgqhbPmHmZLsl31SVopRPlq2Y6GxPrJ4QdSsvWt3ZfvonMg3E0ZIcH4dpURHPbOLzV16z44DLLpuGLmTOTTM33TzINReMYMzwOLmtH+mA73umAnGHXbYpPltvbNhxjJnOpbgF8kLzjNK4L9sV3z1xYdBjklZHOFMZgrtx2h8f3/3ud0U5nU7Tzp07aefOnafbtKIoiqIor0M0t4uiKIqiKANFPz4URVEURRkoi27zcaaJsxhAjcrHFOlZYwuwckrqrMV5o62emJWJ8MKm1G99FnoctTn+NedCePWQp15GaRDcI7n9gU2ysloodLYzIbiu1qWtxvJlxtU2nQTNnIdQRxsGT56z3jDnsR24C6wcgstu4ING3aPNRwsUyEZLtiN1T3ldYzljc5FNSrubQlO2U2ND0AD34hHWh4isim6wbLhcyAGwbkW+sz17UM4tj9ugdI+mTkRECWZPg7qvz+w6cE460A44lsoSs5tA9R6dpOvs3mKQbacPxdhh9wiHOZfhIdxlHWQLoAJz+6xDHbfbQjk9lez+DI/kpP1OucrspJLyJBPMjqIGaQ8O/EzaHpXKZjTHJvKibmJimakbkzZd1eqcKHNXaXSb5ne6L0dNmFsWc/XPZ+VYTeTk8xWL8ElFG4vYDsU0dBpYfdhuxNRxt9jIUZbcUwAPeMieYbyXVpw9SKR74cm3FyDeImXx0ZUPRVEURVEGin58KIqiKIoyUJac7OLiYhBzi/VhBdCDJX+XLfkvXz4u6kZyJmJmE7LhHiu+KsqthnFnTYG04rNlaxsysfKVNIxgF1kxZZ+FDkgZ1aJZws3acsF7yJFLuCFzCwZVgZKuWVLGpbyVUxBVlXXBcsBNmbkUWxgdtgGutqnewvLjEmCjKV2IfdEfuXc6Y/qzLCvvz1xd9qdFPCqmlNeWj3Rfqidw6+Y7BKChTebN3JrMyGX8Q+WI87hpByeFcJnFpV/mcgl1CfTaY/OJu6f+y8GsTYhOG+KcZVIPRuxdaGmYt8smF8qPaZZhGrtab6LLoTl4fEief4hF8MwPy3ZGsvKkDpNhQoiImx4xkXbzo3IuV8vGndZNyDZHhuA5TZt3kwWiVSDc7lHQ6p5ZGF3y5TPd+yJ6VGI0P7TgJjTbr7/Im/0iHHZjtIs2RHcuQGb1E3NG+q/X5d8gHsJg5aSMFr5iXP4t49Gy+8k5PGh05UNRFEVRlIGiHx+KoiiKogwU/fhQFEVRFGWgLDmbD9SW4zyJPAhxnGKaaBoyxXJ3slxWZpmcDruHEw8tOYRt5nPpet2194iLLvQn5Rr7lGZb6n+ttnH3C0kK2E3QFXkP0mBvML7CaIethkxPmcqA5pg0dh6JhNS6PZb9NfCkTUUIob1tTE3aBTyuCQYrPrNbSOA3NLvPk3npqvjKnMyW2/bNCBWqsu8+s19xU/GZl/n9xNDizGyBLhyXNjlHmJ1AEwRjzNLM53qCwMCJZ2JF19oAxo65WDu2bCfL7KI8X96DBmSq5doyvkgSYe8B1rlpS6MtO19nNgU+POtoN7ViNMG2Zd0wm7IYajwE+x2HhW23U3J83CGjrx84IEOml4uHO9sbNkyKuvVvXCHKGWMGJJ4tIqL5OeOO3QCX3RQY8PArQRMhHnY7LgT3a8TY6LDxCiD1hH8uGxUMCv7sg9FSpVzubB85fFjUzc7MinKT27VFbEfMPagWy6LKuVju7KbMM4xZmdOQ+uFsoisfiqIoiqIMFP34UBRFURRloOjHh6IoiqIoA2XJ2XyUmpAWnotjkDLea0kNv940+umrR2XsjmrV1B2dPSrqUmCP4VosBThoqZkE08xBayeWmjqZABsP0JaHsszGoixjXAyleAp7SJUNob3djGlnJC/tDQpM++exS147v7SV4DEfPND+G0yXTwYQTt2W37d4bDdsELBbcJzPz2OB/QPTpZcPD4mqfEqGd59nYePnq9LupV4zoerRViXhyHF2uC4Ogq3PQtWPD0t9f4LFIXm1JM8fgj1GIOxK4L6z87tgK5LwpO1Tu2LsXrJj0hYhy+ZlMZDH2TCfk+z/Lgm4zz2a9kQIMeQ1u5ZcWjY6MiJjuNgOs7ey5Hzm74kQY5Cgvs6uxQfbo0LBzIljx6U9Rrtt9m1CDJLsqEznkGbSe0jyOqzQzIOLVst5X67K+cvD1qD9Bc9I0GrJ/vC+EhHxxwuqhC0JPgdeEGMrcjboKxJ773FQYmtZM5WyTG/xwgv7O9vFORkaH9+NPA5VxCaQ2SV5kFfg5ZdfFuWxZaOd7alVK2VXWbsL2wGdWXTlQ1EURVGUgaIfH4qiKIqiDJQlJ7vsmy2IcrtulijzsMTWmIN9TxzrbB88KrOL1lkYcA+zm6blUrnXMku6Pqx18qW0alsuW9eqZt8UyhGYsTPJpBVYZhtiq2VeRS7VZyH0bpGF8B0Zlm65yZRZ+/UCmXWzDeF959mSYXJoRNSlR437ISSRpVpTtpPNL6NecGD5He9Jk8lvWXCD5eHFcxD+fnJYupoV6mbpvBXKzs+cKLAdpYuuk5TtZpkMg6HF+cp0Epat80xumwM3vSb6TorUltKV1RIZZiHMNoTdTtbYNQ/J8QgCU7ZBckhCfwJWH5H/cCLEELBnKOPKvi4bM0vRw5BBNQkS41zBzIkWPFDMc52cJIbGl0WbzT0P/n/mtcx8Tqcg9HqSjZ0t5RFMtcD7EPpSHs6mTflNF0Oag7Zsp1Aw9W1P7stfG40GZIluyPlTq5vyXFE+a1y+yY/IwUr1fptpsfKkoot1HFYYJ62w8kLqUYxEwaXkI69Kd9pK0bw3XMjo7cDc4o+pBe7f/Jp9eL/U6vJvwKoLVnW2MxA24mxLLRxd+VAURVEUZaDox4eiKIqiKANFPz4URVEURRkoS87mgyBle4OFBR8CO4pg9rgop6ZNOFsMQ+4z3Wx8Yrmoc33ptheyNPYehJHmXrlDoKsWjhm7iQTodhn4DkwwzbwC4dXDprElya2WLrGjOWnXUS0XOtvFOanvD2XMsRa41504dkyUuR6Zy0n3VZvbxLSlfp0AnbNnx7xQ2rm0IFp3ldnojEJ/+FnQBXRqmXQ3/tmsSWPdgvjdczXTBw/Cxjdh3wwLYwwessS9HFEi59PA85pQBy6zTL/Ogau25Zs5UffkYJXAdqM6Z56LLLgeJzLmHBnQyH3Qi9vsQh2o81u9x93mevbQEIR7H2JjAM9aqSTnSKls2hmBKNJcQrcjbz1Ig8AfYhvC6rfMeyKThPQNrnn2fEg136jOw748BLY8Bw/rH0Bqh5G8bDfJ7E5qVWmH5DEbEG9I3o8W+NM2mM0H2gVwF+bRUXl/0vjiiIE3i3O75wMp3nIkkn4DXbe7tdSHKQSOz4nj5nk6PitDpifZM+Im5LMWom80azeyKsDeYzx8AhFRG2zF3CFm5wH2XqF37sTD15UPRVEURVEGin58KIqiKIoyUJac7JIoSBfZITJLr2mSy1peVUabc0slU/Dlkm2bjAyTARe6TEa2y10M221cPjRlB9wq2yzSJC65ubBW74Tm1gxBdM+gbZZIc+BmesG6C0T56JGXO9szhw+IuvFxk3nTJtkfCyKVZlmk0GRaLu9WmOtxEtyLMTttr55eNiwlBii7sNCOXoDLxOx8cF3jOel6NsaupdSQfW/65qToGu3CdTSYK7ADUR/thDm2UpfjGrIovA5mNgZ3uxHmNrx+UkqDTmDuwZH5kqir1uV11dncT8AqbC7DMjZX5POTgjnKs8NipF+7j//WpFyzc8oHWbVuOhiCFIcnSbFpyTPT4r4LZbVNDxt38GpTyph8PuXz8vU5NGzeE25KtlkrnpD7jpjIk44jZUPHNa7rnndQ1AUg9w1lzZg4jhyfas0MiANuuBbceB7BOBFxRTblTEZmtE5AFNxeQekiVoaJyfAKiaAjbrgow3RveIEXE+ufyD5LRLPTJruxBZJnmr2fk5GIxRj92bxDIkF32b65ISm1J+CeSDddjJTKI5zKc0RdmM9s9Fpd+VAURVEUZaD09fHxn//zfybLssS/DRs2dOobjQZt3bqVxsfHKZfL0ZYtW2hmZiamRUVRFEVRzjf6Xvl485vfTEePHu38+/73v9+pu+OOO+iRRx6hBx98kB5//HE6cuQI3XDDDYvaYUVRFEVRljZ923wkEgmampqK/F4sFunee++l+++/n6699loiIrrvvvto48aNtGfPHrr66qtPv7dE9NJzP5I/pM0lpCZWiaqgLcPO+p5xWXXR6ZHrZhAaug0xhIPAfLM1I25gRicbHZNunRkWrjuADKoe2CYEzHbCRj2d960l3XCPn5AZeat1c846aPiVqrENSEM46EZT7ttibqB1W9p8+CmjWY+kpOaJrq69utgtg9DZAWipCaZPBmhXwnwp0dVuCLT4C8fNPTo6L0Oo15kGmwc7lzS4R1bbZnwwvPpQxhxbSIDbdsP0NZmS52j5Uk9npiNk+TKj6pqVxk4gm5HtDKWkRnxw1rh9HnXlOI9lzFxPZOX8bTsQMjxhztOErMgjzP35u89QLC5zr20W5X3mbu0psONIwVxLsXsbxmRbxaj1ZMnxSqTNdbfKclcnYd4T6bTU2lNp03B2SI4V3hM7xbPcynQFmaH1ne1qcr+oazSlPU+W2ZVlh8COjblVtsEeLgRjCS9h+t6ADLiFihnnoZwc85GRM2sX8BqYZiBmV3j2wrC7a2m86y+8c5kdRbEENlXsvZpNyTkhMtXCGTx4b/nsXkai1rP+tSHVxIqVMnNtOmP8zOPCqUfsZeCHUGwvflj2vlc+9u/fT6tWraKLLrqIbrrpJjp48DWDqL1791K73abNmzd39t2wYQOtXbuWdu/e3bW9ZrNJpVJJ/FMURVEU5fVLXx8fV111FX3ta1+jRx99lO655x46cOAA/at/9a+oXC7T9PQ0pVIpGh0dFcdMTk7SNLMGRnbs2EH5fL7zb82aNad0IYqiKIqiLA36kl2uv/76zvZll11GV111Fa1bt47+7u/+jjKZTMyR3dm+fTtt27atUy6VSvoBoiiKoiivY04rzsfo6Ci96U1vohdffJF+6Zd+iVqtFhUKBbH6MTMzc1IbkZ/jui65rtu1HqlXpCzjBEZLDSEEdjIjLy+73GirVlPqbQkW/6EJ9gZ1iD0QMNG4BQEoeOrsVktq9i5Lww5yMbWychHKYfmwm0VpH2IJ/Q98zmePyL63jB5Zg7gn+SFzzpERCJkOMSdsFs47BTYxFrOHSIDtSghxCTAeRDdGQSP3GtK2xWZxSHwP4j8w2wTU90MIN7w8b657riznVq1htNUwhGsmeW+XD5uP76GsjA3BbXashoz34KVYfyC1+rArxyA3ZuJPlOakF9lc0eybTMj/CFgtOUcmcsYGpDUsz7Fi2Whn20nKOkrI5+IwS19Qguey0ZLXEoebNvfAd+S4Biwejht5W8lnJsmMYmyIY5FMdjcU8ELZcLlijvUDeS8dpuHbEC7bYWU3Ld8LQ+zeEREl0iZOC+rybsZo+Jlhacd2Yrogz8me03RaPge5nLnvhw/Lc9Rrcpy5yU6pLPteb5pyoSjtDVaPYWqDU5PN+0v1zp6naEOiKGw50K6Dl2Ns94iIQvY3oDAvQ+XzOWo5EWuNDhjHB9MVyPPjD+YXjDMSwN+gJHtuA7B5iYtsgnYdIVh9LDanFeejUqnQz372M1q5ciVdccUVlEwmadeuXZ36ffv20cGDB2nTpk2n3VFFURRFUV4f9LXy8R//43+k973vfbRu3To6cuQIfeYznyHHcejGG2+kfD5Pt9xyC23bto3GxsZoZGSEbrvtNtq0adOiebooiqIoirL06evj49VXX6Ubb7yRTpw4QStWrKB3v/vdtGfPHlqxYgUREd19991k2zZt2bKFms0mXXfddfSlL31pUTs8vgzCHbO12DTIAR6ERfdCo3W0S3IZ32WyQxsyhnoQCt1n4c0hSju5TC5o2bIywzKqNucLos5ZIa8rxdzkGmXZ18AzS2COheFzZX9SrpGTUJ2wLDN2I/kxUdeoQbZeNlOGsrCsz7LaWi25LNtowLJfj662B49KI2Ub3NK4hISyXWrKLFtjiPIwkj3Y3K8KuCLXmDv0sqzUyVrg7rZ8+YrOdhrc7dpNloF3RN5nyzLllw8eEnVzMzJD5qWjRjase3Icf7Lvpc72Gy5cL+rmytItN50392/demlflc2ZczQgA6YNrsC1Q4c724fAqNzuI766HzCpso5hv808bDjymgPIHJtgS8xJmGc+m78eZNyt1OQydjJj7m0iDZKwxTL5wtvT5fIjvllD/MGcww7lOSzHzJ9kVkrWnrdPlJsNJgFnZTsOy0Y7MyOlgpcPolxstuuQjTvJ5AKUZGqN3pfji0Xjyl6rSSl5cnJSlKVcgi2Z67IX8ML1PS6h4bvIXLQHKSyQQrFgtk9I6dRhg5eAvx08Q3KAWWwBLj214A9LioVpR/kG27VYOwHFyN4R2Qc16rh9T5++Pj4eeOCB2Pp0Ok07d+6knTt3nlanFEVRFEV5/aK5XRRFURRFGSj68aEoiqIoykA5LVfbs8Fll10qf2A2H4mG1OGPlqSGb+WNnm0npT7qpY19CIbkToDdQNJhrkzw+TYyZDR82wX9L2fc22YaUof3wIYgxXS8uLTRwzkZmvnNG+X4hEyPxHTcXt1o3Q1wvUtmpAsdjy4egh4ZekwjB20Q9clegzHXanJ80LaFD+0rr7ws6kploy1nIAx6G9xOa8zH0LXl47B20ujtq1bIFPZjeTnu3P0tNyTHLsH3Bbc4PiD7X5Y2H8fnCqL8zz/4YWd7aJl03RzJGtseH+ZWAlzHh5nrew6CAnKbhjS4p2IK8Bxz2XWgzoqXtwXTM+Y59T2w0WENNSF7uwc2H8wLllpgr2Ixs6kGeAH7JO9JwmXtBjAPk+aak0kIN8+7DnMJbZ1Cn6VPd2T4+5BMB1PZFaLOScq5FYRm31Zb9scNjV3FaF6OVbXe3QXTgXeaxd43jaYcq/k5aY8Wx1Fmx1Uuy7j1Y2PS5ozbcUVDpJv+hG3Zn1JRuvqeYPYZtSa6f5urxhQNSIvZsvkQQiGd5m728v5YzNcf7UrwXcTtpNpt2Z82M9hzwPbKA2O+JpvgSbCHQ5tATgjvJn7f48LUnyq68qEoiqIoykDRjw9FURRFUQbKkpNdVkxIlyyPXUFrTmYlzWTlcuYwW6quw9prmSW0S8EyVgAyDDGXP3T1arHIjslASg4p1yyLTly0TtSlM9I9s8r6c+JV6XLJSYJ8lE1LV04e4c5Jyc422bJjqSCXQUN04WXyiQ3rsg53L4Nsov1FLjRYEFUWz+mzeg+WL5sFE3mzBlLTyOi4KC8fM2Ub3OT40m+lUBB1Pz34sijzjJAeyFJj42ZJ+ZI3b4TjTP/WrZbRLKdA6mk1zTK6C1l23dCcE2UxlGgsFl7XQn9R7koKrncWiGZZ5v5nwXpuXPRGpMQi+AYQkpYv9oawau5Blluf65NySoh2A9AxXXAdTyRZpmy3u3zC9yMiSiRMnUXyHngQfTnBXOBDkvIwj6brJKW8l87IcqNuZKHSvJQDJkbNNafg+RkdkXO9VmXhAzC6J7sJDrTjgWt9HKWSeT8XQR7BMne9DUKQKplEcmJWvhuPHZZZvRssOzgmOuZTFCMvR0ICsHqMYeozyRxd8LlknwD/6yRcl8XGFq+Zyz5JkEdqFSkN8gzTQ8Pyb2CKSTZhAPK5L/vO5fQAY0osArryoSiKoijKQNGPD0VRFEVRBop+fCiKoiiKMlCWnM0H6vIJpjWHLmi34PbEzQgwa6HN9DgLdDu0W4hTs1tMYyRfniOTMjYfWXDHHMuPinKKae8n4BuRK36YtRX17JDtEIJLIbcNCMDOpQ52FB4TTEfzeVGXYnYnXltq27YN2XFjsj5KwIYAbDe4i+z6ldIOaCzJ3Nvg3q1au1qUc8PGHqKJoeGZ/c7u7/9I1J04flyUhc0HuOK97fLLO9sWSZsPh9lcvHnDL4g6zMJZrBpt129Knbd8zGjfCXCvy45K2xHuhWrB/eGjhV55NmZfZc+iDZ54/Zj65FlG51ZLzlHuSQlTIGJvJTuMmj0P140h9qUu7rET5fANKe4JuCLLF4zAh5DuQdrMX8cCd1UWbt1ypI2Hm5bvuOK8OWe9Jp/hlRNmX8eRz3MKsv422JB4YGLBXfuTCXnNQ27v/3/l7qxteMfOz8+J8gpm74T2F0UW6vzYtMzu7EHGVx76PDKh+ZSAa44D/3bw+47XxTNDYwgHfDf6zMaCh1MnImqzm9JoyGtMZ2Q5ZGNQAFu1YeYeb0MG8lZdvlMCj73H1NVWURRFUZSljn58KIqiKIoyUPTjQ1EURVGUgbLkbD5C8LLm6eUpIfXQ/ISMm8DD0IYgytayJtbA9IxMDx5iLnpeh3EJuDZmY2hxoxU2itK3f7Yidd923dgbDENsE5f5Zw+NQlwPH0PksjoL4yabcsqRoZkbdRkzpZk0++ZGwe6GhTBvQHrnZgvHrrcA6xZojCjvt1kshDKEIl67cqU5f0Vex9z0K6JcOmE041Ra2uGEzJ5o3Ro5l1ZOyLDXzSbTayH+w4ZfuLizHXhSEw5Co6t6cJU+jCUP64w2Ohmm5SYzYMMAczSdMTEmLBvmBE/HHbkH8t41WbUPc5383uM/ZEUaAjl/fZa/wIaY7SEEbvDZuwCnOseyoK+WvF+1uhnn3LBM/e4yG4cE2C+FoWnHdmQcllQKzhkwPR1yNFiOGbvQl/F3GlXZH2Ih5hNJiMHhm/LQMD53ONdYTYwtTRBAGgiIMxSHxWwMsDcnTkibj2PHTVh0NBObPXKks90C2zQMeR+IbYjlwea3D3Pdh3Dr3DYLpxbfl9uJEcnrzIK9DtqycJOQFMQECZgNSKUm50C5JOeIkzQD1gI7tiRrN+PKsfLgbwd/x+D7eDHQlQ9FURRFUQaKfnwoiqIoijJQlpzsEviwtMiWWstVuQQ3d7wgygm2zJVMySWnJltDbntyOQyXFhNJvg6I4cT5WrRcqkqw5d5ESoZT9yFl5/AyE/Z7zZveJOpqDR6OWi6/Q8RpSrBlyNABFzHmFuZC5tP8kLzmFruWBC7Viw7IOnSN7jXceiTLJGbHZcuHPpzDZ8ub2TQsLTblkmWLua9WSsdkX1kflg/LJVPKSYnGY65w6TSMXdksKU/XpAzEo5Lb4IqHMkeChUZu2/K6eEpVvD0Y1jkhwjjD/WDz1wI5oAm+rgePzXe2PeiP3e49HHMiZc6DUa0t/v+jBVLlcnXJQh905mqLGTpxV+4p3WqCzJAyy+oJmHcp5rLrJOR4+D7Muwpb0gaf4ZCFyq+WpPvj7IyUJ/yWua7hZXLwPPbMppLyHEk3MtAdwJtWuO+jy7uT6P3/rwl2/1JwHGa5PXDggOlrUo5zs2JCsWcSUt5KpuV71Wbvv2ZbvmN9luXWcuScgKTIQrJJgWzHXWjbmLWayTCYggAzQfMyZlJ32XW0QVpqg9TjsbQe+HiXy2bs2k3ZTlRa4ZJnr/nIe0dXPhRFURRFGSj68aEoiqIoykDRjw9FURRFUQbKkrP5aIOLYaVm7DyaoDNbYH/QbBh31nJVaow8tLaP7qpgNxDvx2fqSuAC1bJNXzO2HHobREafudpynY6IqMI0zyGw1UC9X4QzT3a3x3AgvnAajEdaws1TaqdNpjl6oBviPbDjxk7sCO6HKant5lmI94kVE6IuYJpssS5dmIfS0jU56zIXVXDzDJgbXwvCNmPqd25TgDq4w+6Bg+6ZbDxqDTmuuWHZV5vZEaA+y203MAy5A3ot3xd1aB4mvgEa+bPP7xfl6SMmfXkK3QaZa6JUpKNwF3R0p+XuiDhzLLCR4e2gvQzXvn141tAOqcbClFcqcgyyQ2Ye+m1px2Gnebpy2WYVXL499q6q1+R74sRx83w3wK3Sb0ModkaygQZfxqU6A+6qo8vkvsU5M/dtsEFpcBs4NCLA5yCGNHPHHs5I24xyXT5fPCw42tZw2z00OfHhvjs2f/bku5LPEXwuLXRFZtttGAP5zoPj2PhgWPQEvhvZs9cGl3xeduCZTSTw75OpR5sujz3TtTaEoocxSLF3bq+2ev2gKx+KoiiKogwU/fhQFEVRFGWgLDnZpQWRE6t1s2RZqkq3tAQsgS0bN0vsbUjdWK6YY9sFuRzVgCh6+dFRU4DlqDpb5g9gGavFpIQWZD5NwHKhVTfLrY3SvKhrscyx6VVTog4/J0MeVRCWxrmLmAWSgwszY3jIuJrikr/v9+5WiZJWNyxwC3ZdKbsMD7EondBmpWyix7ZBIvLbct/hnMkaGiazos5yjDutO4RpWyM97mzZsJxqiXuLkSVNf3xPLr+jOy13O8XVbh7FFLPP1hoymi6XLtPgmthg7oevskiSRESHDh8V5RzL2JmAaImhY+ZL+VWKhffWgfueSnF3P5CaYBAwarHYlxfQlRTcI3mw1lJZikYjI+yaHTzOPKdBAPJa0N29t1SS7RTL5uFrN+SzlYDx4bKD15LXz+UKx5bPT9bFSKXs3eR1lxyGEt3PvxDc2zcLoQ6QKns/tkBOD5nM4MMLrw1zwmbj7mO6XvY+tKAdVBm4tJKCZybhmWO9FsolLHM4ps4N5UuWy6NtmC98/jgoSctWyWKdDyFzLY+CG/GyB0mcR0cN+ohY3Cu68qEoiqIoykDp++Pj8OHD9Fu/9Vs0Pj5OmUyGLr30Unr66ac79WEY0qc//WlauXIlZTIZ2rx5M+3fvz+mRUVRFEVRzif6+viYn5+na665hpLJJH3zm9+k5557jv7Lf/kvtGzZss4+f/7nf05f/OIX6ctf/jI98cQTNDQ0RNdddx01GgvZvCuKoiiKcj7Ql83Hn/3Zn9GaNWvovvvu6/y2fv36znYYhvSFL3yB/viP/5je//73ExHRX//1X9Pk5CQ9/PDD9MEPfvC0O3xk5rgo8yyLrQa4pWG4ahYiu1yR+vrhIyaT7cyMDLNtge3GhiFjC4A2DCfmmO4LYhx39QogbDO6k3G7geGM1GuHMqYdF7V2eUryfWbz0AatkIf2Bi0Z3e14OGQLtHef2VWgPUgSQntjaOtuYIhyB/pTqxrXxVea0taHhyIOMSwwaKnZjJkTNrg/8xDvSUytifYzbF8bNFmbGRFgHR8PD9zrKocOQ394xlm0d2ChmTGkPdjk8EOTkIGX2yz5oJ/nx5eLcrFk7kG9Iu1KQqv3V4vD5roDz2wyYfoO3tYEGQmIZQugyDTjGUMx5L6Pz6LZudmQz/f8vHnHDOXyoq5cMmNXr4PWDvPn8KvmPVGtybluMVufNkjt6GbJm0UXbztlbNyCJthqWPJ+NVlm6HoTnm9mC9CG+5OA0OdxcFukJDw/OXjekyyEQL0l70G1YQalji7w8D6uMds5DFnO7U4wfEAAqTFyI8Y27JI3v1nUhcyObP/z+0Qdt6Fqg+1gG1zZeYZpC2xrHGarAa/CSCoBbvNBcL94Pgd8N8Ili2TLTuR5hofvFOhr5eMf//Ef6corr6Rf//Vfp4mJCbr88svpq1/9aqf+wIEDND09TZs3b+78ls/n6aqrrqLdu3eftM1ms0mlUkn8UxRFURTl9UtfHx8vvfQS3XPPPXTxxRfTY489Rh/96EfpD/7gD+h//s//SURE09OvrR5MTk6K4yYnJzt1yI4dOyifz3f+rVmz5lSuQ1EURVGUJUJfHx9BENDb3/52+vznP0+XX345ffjDH6bf//3fpy9/+cun3IHt27dTsVjs/Dt06NApt6UoiqIoyrlPXzYfK1eupEsuuUT8tnHjRvrf//t/ExHR1NRrMSdmZmZo5cqVnX1mZmbobW9720nbdF2XXNc9ad3JePnQrCivGDMhqEeGR0Udj01BJDX8F198WdQdetXEMAgwlTnomi/sf7GzjfE6uKA+zHRCIqIkS9vcBsG6BXq/z7zr06vGRV1WxLyQQp0NfW81jQ5dg/TcboqNO4bghnZ5GUMPc196C2wRHLCj8IPebD4wnC+GEOblJmipPFS+jSGVIaZDjRlCY9ponooew2V7oC3z7kbjTfBwx7KmxUJ9t2Fo0J6I+9oHEENd9BzGGONj2Kx/OM6WsNGB8YCx5LFyIrFerFOL/ZIArZuPAIaGcFPy/048On2tDnOUadaZIRnPJRKOns2RJoS8L5ZN/y6gUVFXY5HPU4Hs7AWrwTbLM3P28KvyuSyW+BjIOYBxPnh8nqG8fN+k0sYRILDl+yU5skyUsyNmjgRVua94VVnxz0Ecoeg63B8op5n9CtpbOWxe1sDmow62dA77/3UW4nN4zMgBnyc3If8mrVph4inZYMx34oSx37EifztYSgQYOxvilzhsDDALBX+vRtMMQJm1E6DdI48BgrYj0PcUs8XCusWgr5WPa665hvbtkwY1L7zwAq1bt46IXjM+nZqaol27dnXqS6USPfHEE7Rp06ZF6K6iKIqiKEudvlY+7rjjDnrXu95Fn//85+k3fuM36Mknn6SvfOUr9JWvfIWIXougefvtt9NnP/tZuvjii2n9+vX0qU99ilatWkUf+MAHzkT/FUVRFEVZYvT18fGOd7yDHnroIdq+fTvdeeedtH79evrCF75AN910U2efj3/841StVunDH/4wFQoFeve7302PPvpoJIzzqdICf6Bj8zy8usz4mAI3T59JJLUaLM8lzTLbMCxfwkonNVl2wmpFLplms2ZJdyg9JOps5ua0LD8q6srgqlhgIdXRDZa7SDVacunu+ImCKNfZWnClJONcD7s8W6X0Y2xieF+2RBfxOmVyEs8i+VrnUc7pcbEN9IkmjLPHlgR9kFISLOx3Og1LtiDf8JDzPmRMbjD5JhIeG2UXVh3AmmkYI9/w5V5UpFAO4GuxltvdZTcExQP7yn3qLAuyeYYshDuMaxvCtNu8Hq+rjyyYfCk4lcL1ZrMJ3piUTMlz5NihZdlVajTNdSZBvqnUsa9mfMaXy2d4GZN5R5fJrMMTK9kzZIEbZesVUR5fZs7hJuUzsn+fmXflUncJj4ioxTLOprIrRJ3jmHdusw2SA4QBX8GuM+PKmEyzs+Z9h5lYrV6zVBORL6SV7hmJiUg8DJh6IufyOSvf8Q3IWOxY7GbDXOfqSQL+VqBbbnmu0Nkusm0ionqNSbf4x4K148HzBI+wkJNQviaRCmMB2A6R+yNCyse46BKRlPwWX3bpO7fLr/zKr9Cv/MqvdK23LIvuvPNOuvPOO0+rY4qiKIqivD7R3C6KoiiKogwU/fhQFEVRFGWg9C27nG1QN2vWjR5ZrHRPQ0xEFLBQt5hCefmE0UuTOemiWy9JATlgLoaOA6GaWahdDKXNQ4Y7EAJ7bGxMlIeHje2ITVKfrTLblldmZdrznx2Qdh2Bb/owmpZ2E1PDRsdr+vKaMysmRNl2eMp2sEERrl3gCog6a4/aYQvsL3xwqbO46y/o0C7TLtEF1AOboRSL2d2Gc3JJFMMtB+3uLtbkSPuZgPmIOqCrBszVN+K0CK6tQqOFYRS2JBF35u52AxhimYdpRzdchOvkXuSUvWvENtOl0xlp2JEfNbYIYQBh0JPgQszmaC4vbRxabH5bYPeTrUBIdxbaeyiXk3XMYCQJRihDw+a+N4oyqGKzIdM5OOyag7ocqyn2LliWkhGfyy35DFsZE/I+EUpbtbkX5zrb1ZI8f3NGtjvkmtk3tUa64U6EZo5W6xiKvg9X28g85JVoA8JdQuV95rYJqYiNBbjWs/nsptBdnz0H8Hx7aP/FwrRHfFuZTZUHhloB6w7+Tx+fL56GwaKYZzjeDEjaz1hoo0NdwSq0CVlsdOVDURRFUZSBoh8fiqIoiqIMlCUnu6D7X+xKHlS22fKdBcvxSeZqmk3KZXMffPOmD5tso62IHGAWr557Dvz9iMsBclkNo7y6adOHALJucinBBlkjlQSXZr68C5FAEyxtYQvSZ64YGhXlTI5lio244XI5ILp413XfGBIwHglw2fWY7IFttnhIRpC+LHAFbrB9McKpzXyK0YUY3VctPiYh1LHQpSjtWHzccfkUl5vFfIb/N3DVx47/P4WIXgt95dEbE5Dx1k5J11IuteBtxYiVcXB5kruqExGtWLeWnU+OHUa9bfo8A66UEdOueS7wHWJZcq4Fobnu+WMnRF2SRQVuN6Vrf7k1Y9r05XEWuqczqad9XJ5/eMLkt5pwpaxaluoJ0Tgbn2kZ/bnKosx68Nochhs2mjfy0vCUzK811TBZvkuz8ro8q3fZxRJSXPz8CGPkWf68Y07dFEZg5XMmlO84r8XuAUSYxu7xOWrB88WVH3xmQ/aQoORqR9xgeURRlF2sk26epAi7xo0zSF0omXMJNqaVU0VXPhRFURRFGSj68aEoiqIoykDRjw9FURRFUQbKErT5kFoYz4gZyQKKtglM0gpAm6tVjQtbC2wjWp50qxSaMeh43M6k2ZBhiuNCTmN2XAwPzbGZa+lQFkK4Q5jrkGU0rbVk3Ymi6V8SM5Y2pJ7NM4Gi65sImhy5xO4udHHYIFL7KIGy72Yb7EFsZguA+mzUn4xlnMX5w+wzbAwbn8BsudzVVdomCDc+yPJrZY1tD4aYtnComE0DerLy/mHmXBxyPkdx7EaYa2ka7G7aLcwgasrJmL7LmRQlYPYYoStdW8uOuScQIZyGEvKH+bZ5aJY7su88tHcLw1qDTVeTueT7rhy8bNbsazelAYbfNOfHdw9BJtR6w9zLBPSV22L5gbzG/Ngqec4RU3/s6EFRV7Xzpq+Y3Rlj8LeNjUzQkAPkJc0dbIHrr+NI25p4wpNsvUacY3Y0+yo7P7YTsTkz98EDGx2LzX38X7gD7vs8bDqewuZesHDbeZZxfC4TaGPGn8sFDTsYMa/UeBM89Nc/s661iK58KIqiKIoyUPTjQ1EURVGUgaIfH4qiKIqiDJQlb/Mhwl4voFnxajQhaDP7jFZJatsYX4D3wbJRN+t+fr5rRMeM0/TAH5yHtcZ2MI4Etw3AvuaSRmsezcvYJuTJMM4UjPJWu3Y1Eq4b/eUjBgknp1GF86NvPbPrCPEesD74MB5o68Nje0TC8bPOR2xHInQPaSwOxfHhtiShPM5H3Znbi2BcAq5+R+Zk95Tx2FceHrpSlzZLCUuOT9Y1/WlBinaMmRJHJTPa2a4tgxgcLRO7IvRkX7MJ+ZwW20bTT0Ka+kbSHPtKY17UpWwZvyQIje1GaGMId2P7dMHyi0RdrWLColfmXpHHBcdFmUdUT6Uh5sa46Y8PgT38EGOvGLuOVE6maHCyK00BnrtKRRqVpYbMsUPjF4q6VsBSWBSl3YTf7uP/r7ExmbqDtk88XshCtiM8Vk9kRornEtpBewxehA5ZbGxDW9ZxmxSMXRIxx+DvG+xrXIiU2Edtsew4Ft8eRFc+FEVRFEUZKPrxoSiKoijKQFlyskuIrqRsqQpXlyNhwLlbLtR5kUyg4khR4m5XuJQWxmk7bN/IqjQuLXI3MIjNnODlyDWDuyhbKvdgybbFdnVT8rgESdkj5CHe4ZzChTiMWa6MHtoVDO0dYCjiGA3LZyHLQwijH8SsWaKrq80yD1twfhxnHvLesrHv3J1X9sfhx2FGTnAB5fM5Er6c3edoUtvuUlOQkNdVajMpA541FzKIcldkTBeQcUHGiyFomWys2YKc6y5zDy9lpFzSgPuVICMjllJy3+HR0c625ctz5DzpLuqQ6Xu9XhR1FpPG6r68P+WSuX/lIsiEJF3iqzUzdunsuKhrNc38DQI5jl7hqGznVeZOG75N1KU8cyy+N5NZmbU6KJsxmf+pDHHvJDaaNn0Zer08/yxJpqkr3SOExyvmEQmkt9Dr0WYi+QvMcRHPVnyx+qK2276RU1j8/SKJC72AiF17VzRjiZWhiOBlvUgnjTmdoiiKoijKGUU/PhRFURRFGSj68aEoiqIoykBZcjYfL//0x2e7C4NBSOjSvqDVlqHYT5Uik7NfjpFqX+P4QjssKpUjhwd6PmVhmgvv0mGhkOqcf3rsiX67chZBG4fF4NXTOPaVhXc5R3h+urzwTsp5g658KIqiKIoyUPTjQ1EURVGUgaIfH4qiKIqiDBT9+FAURVEUZaDox4eiKIqiKAPlnPN2+XmEumazH9t6RVEURVHOJj//ux0XafbnWGEvew2QV199ldasWbPwjoqiKIqinHMcOnSIVq9eHbvPOffxEQQBHTlyhMIwpLVr19KhQ4doZGRk4QPPM0qlEq1Zs0bHpws6PvHo+MSj4xOPjk93zuexCcOQyuUyrVq1imw73qrjnJNdbNum1atXU6lUIiKikZGR8+4G9oOOTzw6PvHo+MSj4xOPjk93ztexyefzC+9EanCqKIqiKMqA0Y8PRVEURVEGyjn78eG6Ln3mM58h13XPdlfOSXR84tHxiUfHJx4dn3h0fLqjY9Mb55zBqaIoiqIor2/O2ZUPRVEURVFen+jHh6IoiqIoA0U/PhRFURRFGSj68aEoiqIoykA5Zz8+du7cSRdeeCGl02m66qqr6MknnzzbXRo4O3bsoHe84x00PDxMExMT9IEPfID27dsn9mk0GrR161YaHx+nXC5HW7ZsoZmZmbPU47PLXXfdRZZl0e2339757Xwfn8OHD9Nv/dZv0fj4OGUyGbr00kvp6aef7tSHYUif/vSnaeXKlZTJZGjz5s20f//+s9jjweH7Pn3qU5+i9evXUyaToTe84Q30p3/6pyIvxfk0Pt/73vfofe97H61atYosy6KHH35Y1PcyFnNzc3TTTTfRyMgIjY6O0i233EKVSmWAV3HmiBufdrtNn/jEJ+jSSy+loaEhWrVqFf3O7/wOHTlyRLTxeh6fvgnPQR544IEwlUqF/+N//I/wJz/5Sfj7v//74ejoaDgzM3O2uzZQrrvuuvC+++4Ln3322fCZZ54J//2///fh2rVrw0ql0tnnIx/5SLhmzZpw165d4dNPPx1effXV4bve9a6z2Ouzw5NPPhleeOGF4WWXXRZ+7GMf6/x+Po/P3NxcuG7duvB3f/d3wyeeeCJ86aWXwsceeyx88cUXO/vcddddYT6fDx9++OHwRz/6Ufirv/qr4fr168N6vX4Wez4YPve5z4Xj4+PhN77xjfDAgQPhgw8+GOZyufAv//IvO/ucT+PzT//0T+Ef/dEfhX//938fElH40EMPifpexuKXf/mXw7e+9a3hnj17wv/7f/9v+MY3vjG88cYbB3wlZ4a48SkUCuHmzZvDv/3bvw2ff/75cPfu3eE73/nO8IorrhBtvJ7Hp1/OyY+Pd77zneHWrVs7Zd/3w1WrVoU7duw4i706+8zOzoZEFD7++ONhGL424ZPJZPjggw929vnpT38aElG4e/fus9XNgVMul8OLL744/Na3vhX+m3/zbzofH+f7+HziE58I3/3ud3etD4IgnJqaCv/iL/6i81uhUAhd1w3/1//6X4Po4lnlve99b/h7v/d74rcbbrghvOmmm8IwPL/HB/+49jIWzz33XEhE4VNPPdXZ55vf/GZoWVZ4+PDhgfV9EJzs4wx58sknQyIKX3nllTAMz6/x6YVzTnZptVq0d+9e2rx5c+c327Zp8+bNtHv37rPYs7NPsVgkIqKxsTEiItq7dy+1220xVhs2bKC1a9eeV2O1detWeu973yvGgUjH5x//8R/pyiuvpF//9V+niYkJuvzyy+mrX/1qp/7AgQM0PT0txiefz9NVV111XozPu971Ltq1axe98MILRET0ox/9iL7//e/T9ddfT0Q6PpxexmL37t00OjpKV155ZWefzZs3k23b9MQTTwy8z2ebYrFIlmXR6OgoEen4IOdcYrnjx4+T7/s0OTkpfp+cnKTnn3/+LPXq7BMEAd1+++10zTXX0Fve8hYiIpqenqZUKtWZ3D9ncnKSpqenz0IvB88DDzxAP/jBD+ipp56K1J3v4/PSSy/RPffcQ9u2baP/9J/+Ez311FP0B3/wB5RKpejmm2/ujMHJnrXzYXw++clPUqlUog0bNpDjOOT7Pn3uc5+jm266iYjovB8fTi9jMT09TRMTE6I+kUjQ2NjYeTdejUaDPvGJT9CNN97YSS6n4yM55z4+lJOzdetWevbZZ+n73//+2e7KOcOhQ4foYx/7GH3rW9+idDp9trtzzhEEAV155ZX0+c9/noiILr/8cnr22Wfpy1/+Mt18881nuXdnn7/7u7+jr3/963T//ffTm9/8ZnrmmWfo9ttvp1WrVun4KKdMu92m3/iN36AwDOmee+452905ZznnZJfly5eT4zgRj4SZmRmampo6S706u9x66630jW98g77zne/Q6tWrO79PTU1Rq9WiQqEg9j9fxmrv3r00OztLb3/72ymRSFAikaDHH3+cvvjFL1IikaDJycnzenxWrlxJl1xyifht48aNdPDgQSKizhicr8/aH/7hH9InP/lJ+uAHP0iXXnop/fZv/zbdcccdtGPHDiLS8eH0MhZTU1M0Ozsr6j3Po7m5ufNmvH7+4fHKK6/Qt771rc6qB5GOD3LOfXykUim64ooraNeuXZ3fgiCgXbt20aZNm85izwZPGIZ066230kMPPUTf/va3af369aL+iiuuoGQyKcZq3759dPDgwfNirN7znvfQj3/8Y3rmmWc6/6688kq66aabOtvn8/hcc801EdfsF154gdatW0dEROvXr6epqSkxPqVSiZ544onzYnxqtRrZtnwFOo5DQRAQkY4Pp5ex2LRpExUKBdq7d29nn29/+9sUBAFdddVVA+/zoPn5h8f+/fvp//yf/0Pj4+Oi/nwfnwhn2+L1ZDzwwAOh67rh1772tfC5554LP/zhD4ejo6Ph9PT02e7aQPnoRz8a5vP58Lvf/W549OjRzr9ardbZ5yMf+Ui4du3a8Nvf/nb49NNPh5s2bQo3bdp0Fnt9duHeLmF4fo/Pk08+GSYSifBzn/tcuH///vDrX/96mM1mw7/5m7/p7HPXXXeFo6Oj4T/8wz+E//zP/xy+//3vf926kiI333xzeMEFF3Rcbf/+7/8+XL58efjxj3+8s8/5ND7lcjn84Q9/GP7whz8MiSj8r//1v4Y//OEPO94avYzFL//yL4eXX355+MQTT4Tf//73w4svvvh140oaNz6tViv81V/91XD16tXhM888I97XzWaz08breXz65Zz8+AjDMPxv/+2/hWvXrg1TqVT4zne+M9yzZ8/Z7tLAIaKT/rvvvvs6+9Tr9fA//If/EC5btizMZrPhr/3ar4VHjx49e50+y+DHx/k+Po888kj4lre8JXRdN9ywYUP4la98RdQHQRB+6lOfCicnJ0PXdcP3vOc94b59+85SbwdLqVQKP/axj4Vr164N0+l0eNFFF4V/9Ed/JP5YnE/j853vfOek75ubb745DMPexuLEiRPhjTfeGOZyuXBkZCT80Ic+FJbL5bNwNYtP3PgcOHCg6/v6O9/5TqeN1/P49IsVhiycn6IoiqIoyhnmnLP5UBRFURTl9Y1+fCiKoiiKMlD040NRFEVRlIGiHx+KoiiKogwU/fhQFEVRFGWg6MeHoiiKoigDRT8+FEVRFEUZKPrxoSiKoijKQNGPD0VRFEVRBop+fCiKoiiKMlD040NRFEVRlIGiHx+KoiiKogyU/x8clZxmd6PZFAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test multiblock params\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "dataiter = iter(train_data)\n",
        "img,y = next(dataiter)\n",
        "img = img.unsqueeze(0)\n",
        "b,c,h,w = img.shape\n",
        "img = img.flatten(2).transpose(-2,-1).to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "print(img.shape)\n",
        "batch, seq,_ = img.shape\n",
        "\n",
        "\n",
        "# target_mask = randpatch(seq, mask_size=8, gamma=.9) # 8.9 [seq]\n",
        "target_mask = multiblock(seq, min_s=0.2, max_s=0.3, M=4).any(0) # best.2.3M4 og.15.2M4# mask out targets to be predicted # [M, seq]\n",
        "target_mask = multiblock(seq, min_s=0.2, max_s=0.3, M=4).any(0)\n",
        "\n",
        "context_mask = ~multiblock(seq, min_s=0.85, max_s=1., M=1)|target_mask # [1, seq], True->Mask\n",
        "\n",
        "# print(target_mask, context_mask)\n",
        "\n",
        "print(target_mask.shape, context_mask.shape)\n",
        "# target_img, context_img = img*target_mask.unsqueeze(-1), img*context_mask.unsqueeze(-1)\n",
        "target_img, context_img = img*~target_mask.unsqueeze(-1), img*~context_mask.unsqueeze(-1)\n",
        "# print(target_img.shape, context_img.shape)\n",
        "target_img, context_img = target_img.transpose(-2,-1).reshape(b,c,h,w), context_img.transpose(-2,-1).reshape(b,c,h,w)\n",
        "target_img, context_img = target_img.float(), context_img.float()\n",
        "\n",
        "# imshow(out.detach().cpu())\n",
        "imshow(target_img[0])\n",
        "imshow(context_img[0])\n"
      ],
      "metadata": {
        "id": "D6lVtbS5OHIv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 879
        },
        "outputId": "13b6ad36-0c36-4d1e-daf0-70b098b51a2e",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1024, 3])\n",
            "torch.Size([1024]) torch.Size([1, 1024])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "trg_msks = []\n",
        "ctx_msks = []\n",
        "for i in range(16):\n",
        "    # target_mask = randpatch(seq, mask_size=8, gamma=.9) # 8.9 [seq]\n",
        "    target_mask = multiblock(seq, min_s=0.2, max_s=0.3, M=4).any(0) # best.2.3M4 og.15.2M4# mask out targets to be predicted # [M, seq]\n",
        "    target_mask = multiblock(seq, min_s=0.2, max_s=0.3, M=4).any(0)\n",
        "\n",
        "    context_mask = ~multiblock(seq, min_s=0.85, max_s=1., M=1)|target_mask # [1, seq], True->Mask\n",
        "\n",
        "    # print(target_mask.shape, context_mask.shape)\n",
        "    target_img, context_img = img*~target_mask.unsqueeze(-1), img*~context_mask.unsqueeze(-1)\n",
        "    target_img, context_img = target_img.transpose(-2,-1).reshape(b,c,h,w).float(), context_img.transpose(-2,-1).reshape(b,c,h,w).float()\n",
        "    trg_msks.append(target_img)\n",
        "    ctx_msks.append(context_img)\n",
        "\n",
        "trg_msks = torch.cat(trg_msks, dim=0)\n",
        "ctx_msks = torch.cat(ctx_msks, dim=0)\n",
        "imshow(torchvision.utils.make_grid(trg_msks.cpu(), nrow=4))\n",
        "imshow(torchvision.utils.make_grid(ctx_msks.cpu(), nrow=4))\n"
      ],
      "metadata": {
        "id": "kiHDuPjB0SBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test multiblock mask\n",
        "def multiblock(seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    # mask_len, mask_pos = mask_len * seq, mask_pos * seq\n",
        "    indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "    return target_mask\n",
        "\n",
        "\n",
        "seq=400\n",
        "M=4\n",
        "\n",
        "# target_mask = multiblock(M, seq, min_s=0.15, max_s=0.2, M=1) # mask out targets to be predicted # [4*batch, seq]\n",
        "# context_mask = ~multiblock(1, seq, min_s=0.85, max_s=1., M=1)|target_mask.reshape(M,seq).any(0) # [batch, seq]\n",
        "\n",
        "\n",
        "target_mask = multiblock(seq, min_s=0.15, max_s=0.2, M=4) # mask out targets to be predicted # [4*batch, seq]\n",
        "context_mask = ~multiblock(seq, min_s=0.85, max_s=1., M=1)|target_mask.reshape(M,seq).any(0) # [batch, seq]\n",
        "\n",
        "# target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "\n",
        "# print(target_mask)\n",
        "# print(context_mask)\n",
        "\n",
        "print(target_mask.sum(-1))\n",
        "print(context_mask.sum(-1))\n",
        "\n",
        "\n",
        "# sorted_mask, ids = context_mask.sort(dim=1, stable=True)\n",
        "# indices = ids[~sorted_mask]#.reshape(M,-1) # int idx [num_context_toks] , idx of context not masked\n",
        "# sorted_x = x[torch.arange(batch).unsqueeze(-1), indices] # [batch, seq-num_trg_toks, dim]\n",
        "# print(indices)\n",
        "# print(x)\n",
        "# print(sorted_x)\n",
        "\n",
        "\n",
        "\n",
        "# sorted_mask, ids = target_mask.sort(dim=1, descending=False, stable=True)\n",
        "# # print(sorted_mask, ids)\n",
        "# indices = ids[~sorted_mask].reshape(M,-1) # int idx [M, seq-num_trg_toks] , idx of target not masked\n",
        "# print(indices)\n",
        "# batch=3\n",
        "# dim=2\n",
        "# # indices = indices.expand(batch,-1,-1)\n",
        "# # x = torch.arange(batch*seq).reshape(batch,seq).to(device)\n",
        "# x = torch.rand(batch, seq, dim)\n",
        "# print(x)\n",
        "# sorted_x = x[torch.arange(batch)[...,None,None], indices]\n",
        "# print(sorted_x.shape) # [batch, M, seq-num_trg_toks, dim]\n",
        "# print(sorted_x)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "p8xv1tpKIhim",
        "outputId": "e6613fee-2d32-4c6a-ef96-dee18da3aa6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([73, 73, 73, 73])\n",
            "tensor([248])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ijepa multiblock down\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "import math\n",
        "from multiprocessing import Value\n",
        "import torch\n",
        "_GLOBAL_SEED = 0\n",
        "\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=(224, 224),\n",
        "        patch_size=16,\n",
        "        enc_mask_scale=(0.2, 0.8),\n",
        "        pred_mask_scale=(0.2, 0.8),\n",
        "        aspect_ratio=(0.3, 3.0),\n",
        "        nenc=1,\n",
        "        npred=2,\n",
        "        min_keep=4,\n",
        "        allow_overlap=False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if not isinstance(input_size, tuple):\n",
        "            input_size = (input_size, ) * 2\n",
        "        self.patch_size = patch_size\n",
        "        self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.aspect_ratio = aspect_ratio\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "        self._itr_counter = Value('i', -1)  # collator is shared across worker processes\n",
        "\n",
        "    def step(self):\n",
        "        i = self._itr_counter\n",
        "        with i.get_lock():\n",
        "            i.value += 1\n",
        "            v = i.value\n",
        "        return v\n",
        "\n",
        "    def _sample_block_size(self, generator, scale, aspect_ratio_scale):\n",
        "        _rand = torch.rand(1, generator=generator).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.height * self.width * mask_scale)\n",
        "        # -- Sample block aspect-ratio\n",
        "        min_ar, max_ar = aspect_ratio_scale\n",
        "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
        "        while h >= self.height:\n",
        "            h -= 1\n",
        "        while w >= self.width:\n",
        "            w -= 1\n",
        "\n",
        "        return (h, w)\n",
        "\n",
        "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
        "        h, w = b_size\n",
        "\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # --\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            top = torch.randint(0, self.height - h, (1,))\n",
        "            left = torch.randint(0, self.width - w, (1,))\n",
        "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
        "            mask[top:top+h, left:left+w] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
        "        mask_complement[top:top+h, left:left+w] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        B = len(batch)\n",
        "\n",
        "        collated_batch = torch.utils.data.default_collate(batch)\n",
        "\n",
        "        seed = self.step()\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(seed)\n",
        "        p_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.pred_mask_scale,\n",
        "            aspect_ratio_scale=self.aspect_ratio)\n",
        "        e_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.enc_mask_scale,\n",
        "            aspect_ratio_scale=(1., 1.))\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.height * self.width\n",
        "        min_keep_enc = self.height * self.width\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                print(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "\n",
        "        return collated_batch, collated_masks_enc, collated_masks_pred\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "COvEnBXPYth3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## trash"
      ],
      "metadata": {
        "id": "8DxjYI9RMQoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title (Learned)RotEmb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, seq_len=512, base=10000):\n",
        "        super().__init__()\n",
        "        self.dim, self.base = dim, base\n",
        "        theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "        pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "        angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "        self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.base)\n",
        "        return x * self.rot_emb[:seq_len]\n",
        "\n",
        "# class LearnedRoPE(nn.Module): # learnt RoPE ; each tok is 1 pos\n",
        "#     def __init__(self, dim):\n",
        "#         super().__init__()\n",
        "#         self.weights = nn.Parameter(torch.randn(1, dim//2))\n",
        "\n",
        "#     def forward(self, x): #\n",
        "#         batch, seq_len, dim = x.shape\n",
        "#         # if rot_emb.shape[0] < seq_len: self.__init__(dim, seq_len)\n",
        "#         pos = torch.arange(seq_len).unsqueeze(1)\n",
        "#         angles = (self.weights * pos * 2*torch.pi).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "#         rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "#         return x * rot_emb.flatten(-2).unsqueeze(0)\n",
        "\n",
        "\n",
        "class LearnedRoPE(nn.Module): # learnt RoPE ; each tok is 1 pos\n",
        "    def __init__(self, dim, seq_len=512):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.weights = nn.Parameter(torch.randn(1, dim//2))\n",
        "        pos = torch.arange(seq_len).unsqueeze(1)\n",
        "        angles = (self.weights * pos * 2*torch.pi)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "        self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "    def forward(self, x): # [batch, seq_len, dim]\n",
        "        seq_len = x.size(1)\n",
        "        if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len)\n",
        "        return x * self.rot_emb[:seq_len]\n",
        "\n",
        "\n",
        "class RotEmb(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, top=torch.pi, base=10000):\n",
        "        super().__init__()\n",
        "        self.theta = top / (base ** (torch.arange(0, dim, step=2, device=device) / dim))\n",
        "        # self.theta = top / (base ** torch.linspace(0, 1, dim//2, device=device))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (pos.unsqueeze(-1) * self.theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [seq_len, dim]\n",
        "\n",
        "class LearnedRotEmb(nn.Module): # pos in R\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.weights = nn.Parameter(torch.randn((1, dim//2), device=device))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (self.weights * pos.unsqueeze(-1) * 2*torch.pi).unsqueeze(-1) # [batch, 1] * [1, dim//2] -> [batch, dim//2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [batch, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [batch, dim]\n",
        "\n"
      ],
      "metadata": {
        "id": "npc_xGtOz7DC",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ViT me simple wisdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    \"\"\"Zero out the parameters of a module and return it.\"\"\"\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads=None, d_head=8, cond_dim=None, dropout=0.): # .1\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_head = d_head\n",
        "        self.n_heads = d_model // d_head\n",
        "        # self.d_head = d_model // n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or d_model, 2*d_model, bias=False)\n",
        "        # self.k = nn.Sequential(nn.Dropout(dropout), nn.Linear(cond_dim, d_model, bias=False))\n",
        "        # self.lin = nn.Linear(d_model, d_model)\n",
        "        self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        # self.lin = nn.Sequential(nn.Dropout(dropout), zero_module(nn.Linear(d_model, d_model)))\n",
        "        self.drop = nn.Dropout(dropout) # indp before q,k,v; after linout\n",
        "        self.scale = self.d_head ** -.5\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [batch, T, d_model]=[batch, h*w, c], [batch, num_tok, cond_dim], [batch,T]\n",
        "        batch = x.shape[0]\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        Q = self.q(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        # K = self.k(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        K, V = self.kv(cond).view(batch, -1, self.n_heads, 2*self.d_head).transpose(1, 2).chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # linear attention # Softmax(Q) @ (Softmax(K).T @ V)\n",
        "        if mask != None:\n",
        "            mask = mask[:, None, :, None] # [batch,T] -> [batch,1,T,1]\n",
        "            K, V = K.masked_fill(mask, -torch.finfo(x.dtype).max), V.masked_fill(mask, -torch.finfo(x.dtype).max)\n",
        "        Q, K = Q.softmax(dim=-1)*self.scale, K.softmax(dim=-2)\n",
        "        context = K.transpose(-2,-1) @ V # [batch, n_heads, d_head, d_head]\n",
        "        out = Q @ context # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # (quadratic) attention # Softmax(Q @ K.T) @ V\n",
        "        # attn = Q @ K.transpose(-2,-1) * self.scale # [batch, n_heads, T] # [batch, n_heads, T, T/num_tok]\n",
        "        # if mask != None: attn = attn.masked_fill(mask[:, None, :, None], -torch.finfo(attn.dtype).max) # [batch,T]->[batch,1,T,1]\n",
        "        # attention = torch.softmax(attn, dim=-1)\n",
        "        # out = self.drop(attention) @ V # [batch, n_heads, T, d_head]\n",
        "\n",
        "        out = out.transpose(1, 2).flatten(2)\n",
        "        return self.drop(self.lin(out)) # [batch, T, d_model]\n",
        "\n",
        "# if self, dont pass cond_dim in init, dont pass cond in fwd\n",
        "# Softmax(Q @ K.T) @ V ~ Softmax(Q) @ Softmax(K).T @ V\n",
        "\n",
        "# https://github.com/lucidrains/x-transformers/blob/main/x_transformers/x_transformers.py#L1855\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_head=4, cond_dim=None, ff_mult=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm1 = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.norm2 = nn.RMSNorm(d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.cond_dim = cond_dim\n",
        "        self.self = MultiHeadAttention(d_model, d_head=d_head, dropout=0)\n",
        "        if self.cond_dim!=None: self.cross = MultiHeadAttention(d_model, d_head=d_head, cond_dim=cond_dim, dropout=0)\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        ff_dim=d_model*ff_mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), nn.Dropout(dropout), nn.Linear(d_model, ff_dim), act, # ReLU GELU\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "            nn.RMSNorm(d_model), nn.Dropout(dropout), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            # nn.RMSNorm(d_model), act, nn.Dropout(dropout), nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,c,h,w], [batch, num_tok, cond_dim], [batch,T]\n",
        "        # bchw = x.shape\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "\n",
        "        if self.cond_dim==None: cond=None # is self attn\n",
        "        x = x + self.drop(self.self(self.norm1(x), cond, mask))\n",
        "        if self.cond_dim!=None: x = x + self.cross(self.norm2(x), cond, mask)\n",
        "        x = x + self.ff(x)\n",
        "        # x = x + self.drop(self.ff(x))\n",
        "\n",
        "        # return x.transpose(1,2).reshape(*bchw)\n",
        "        return x\n",
        "\n",
        "import torch\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "def apply_masks(x, masks):\n",
        "    \"\"\":param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
        "    :param masks: list of tensors containing indices of patches in [N] to keep\"\"\"\n",
        "    all_x = []\n",
        "    for m in masks: # [1,T]\n",
        "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [1,T,dim]\n",
        "        all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "    return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        # self.positional_emb = nn.Parameter(torch.zeros(1, 32*32//4, d_model)) # positional_embedding == 'learnable'\n",
        "        # encoder_layers = nn.TransformerEncoderLayer(d_model, d_model//d_head, d_hid or d_model, dropout, batch_first=True)\n",
        "        # self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.zeros(1,1,d_model)) # randn zeros\n",
        "        nn.init.trunc_normal_(self.cls, std=.02)\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model)\n",
        "        self.lin = nn.Linear(d_model, out_dim)# if out_dim != d_model else None\n",
        "\n",
        "    def forward(self, src, context_indices, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        src = self.embed(src) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = src.shape\n",
        "        src = src * self.pos_encoder(context_indices)\n",
        "        # src = src + self.positional_emb[:,context_indices]\n",
        "        # src = apply_masks(src, [context_indices])\n",
        "\n",
        "        pred_tokens = self.cls * self.pos_encoder(trg_indices) # [M, num_trg_toks, d_model]\n",
        "        # pred_tokens = self.cls + self.positional_emb[0,trg_indices]\n",
        "        pred_tokens = pred_tokens.repeat(batch, 1, 1) # [batch*M, num_trg_toks, d_model]\n",
        "        # print(pred_tokens.requires_grad)\n",
        "        # print(\"pred fwd\", src.shape, pred_tokens.shape)\n",
        "        # src = src.repeat_interleave(trg_indices.shape[0], dim=0) # [batch, seq_len, d_model] -> [batch*M, seq_len, d_model]\n",
        "        src = torch.cat([src, pred_tokens], dim=1) # [batch*M, seq_len+num_trg_toks, d_model]\n",
        "\n",
        "        out = self.transformer_encoder(src) # float [seq_len, batch_size, d_model]\n",
        "        out = self.norm(out)\n",
        "        out = out[:,seq:] # [batch*M, num_trg_toks, d_model]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.MaxPool1d(2,2), #nn.MaxPool1d(3, 2, 3//2),\n",
        "            # nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.Conv1d(d_model, d_model,3,2,3//2)\n",
        "            nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2),\n",
        "            # nn.Conv1d(in_dim, d_model,2,2,0), # like patch\n",
        "            )\n",
        "        self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        # self.positional_emb = nn.Parameter(torch.zeros(1, 32*32//4, d_model)) # positional_embedding == 'learnable'\n",
        "        # encoder_layers = nn.TransformerEncoderLayer(d_model, d_model//d_head, d_hid or d_model, dropout, batch_first=True)\n",
        "        # self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model)\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, src, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "\n",
        "        # src = self.embed(src) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        src = self.embed(src.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "\n",
        "        batch, seq, dim = src.shape\n",
        "\n",
        "        # # # src = self.pos_encoder(src)\n",
        "        # if context_indices != None:\n",
        "        # print(src.shape, self.pos_encoder(context_indices).shape)\n",
        "        #     src = src * self.pos_encoder(context_indices) # context/predictor # src = src + self.positional_emb[:,context_indices]\n",
        "        # else: src = src * self.pos_encoder(torch.arange(seq, device=device).unsqueeze(0)) # target # src = src + self.positional_emb[:,:seq]\n",
        "\n",
        "\n",
        "        # src = self.pos_encoder(src)\n",
        "        src = src * self.pos_encoder(torch.arange(seq, device=device)).unsqueeze(0)\n",
        "        # src = src + self.positional_emb\n",
        "        if context_indices != None:\n",
        "            src = apply_masks(src, [context_indices])\n",
        "\n",
        "\n",
        "        out = self.transformer_encoder(src) # float [seq_len, batch_size, d_model]\n",
        "        out = self.norm(out)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "batch, seq_len, d_model = 4,1024,16\n",
        "in_dim = 3\n",
        "model = TransformerModel(in_dim, d_model, d_head=4, nlayers=2, dropout=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "# # print(out)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bnjJHfKj1_g",
        "outputId": "2e3e10eb-bc09-4ef7-c196-8cfe54602b6b",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5104\n",
            "torch.Size([4, 32, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title attention save\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, seq_len=512, base=10000):\n",
        "        super().__init__()\n",
        "        self.dim, self.base = dim, base\n",
        "        theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "        pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "        angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "        self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.base)\n",
        "        return x * self.rot_emb[:seq_len]\n",
        "\n",
        "def zero_module(module):\n",
        "    \"\"\"Zero out the parameters of a module and return it.\"\"\"\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads=None, d_head=8, cond_dim=None, dropout=0.): # .1\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_head = d_head\n",
        "        self.n_heads = d_model // d_head\n",
        "        # self.d_head = d_model // n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or d_model, 2*d_model, bias=False)\n",
        "        # self.k = nn.Sequential(nn.Dropout(dropout), nn.Linear(cond_dim, d_model, bias=False))\n",
        "        # self.lin = nn.Linear(d_model, d_model)\n",
        "        self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        # self.lin = nn.Sequential(nn.Dropout(dropout), zero_module(nn.Linear(d_model, d_model)))\n",
        "        self.drop = nn.Dropout(dropout) # indp before q,k,v; after linout\n",
        "        self.scale = self.d_head ** -.5\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [batch, T, d_model]=[batch, h*w, c], [batch, num_tok, cond_dim], [batch,T]\n",
        "        batch = x.shape[0]\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        Q = self.q(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        # K = self.k(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        K, V = self.kv(cond).view(batch, -1, self.n_heads, 2*self.d_head).transpose(1, 2).chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # linear attention # Softmax(Q) @ (Softmax(K).T @ V)\n",
        "        if mask != None:\n",
        "            mask = mask[:, None, :, None] # [batch,T] -> [batch,1,T,1]\n",
        "            K, V = K.masked_fill(mask, -torch.finfo(x.dtype).max), V.masked_fill(mask, -torch.finfo(x.dtype).max)\n",
        "        Q, K = Q.softmax(dim=-1)*self.scale, K.softmax(dim=-2)\n",
        "        context = K.transpose(-2,-1) @ V # [batch, n_heads, d_head, d_head]\n",
        "        out = Q @ context # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # (quadratic) attention # Softmax(Q @ K.T) @ V\n",
        "        # attn = Q @ K.transpose(-2,-1) * self.scale # [batch, n_heads, T] # [batch, n_heads, T, T/num_tok]\n",
        "        # if mask != None: attn = attn.masked_fill(mask[:, None, :, None], -torch.finfo(attn.dtype).max) # [batch,T]->[batch,1,T,1]\n",
        "        # attention = torch.softmax(attn, dim=-1)\n",
        "        # out = self.drop(attention) @ V # [batch, n_heads, T, d_head]\n",
        "\n",
        "        out = out.transpose(1, 2).flatten(2)\n",
        "        return self.drop(self.lin(out)) # [batch, T, d_model]\n",
        "\n",
        "# if self, dont pass cond_dim in init, dont pass cond in fwd\n",
        "# Softmax(Q @ K.T) @ V ~ Softmax(Q) @ Softmax(K).T @ V\n",
        "\n",
        "# https://github.com/lucidrains/x-transformers/blob/main/x_transformers/x_transformers.py#L1855\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_head=4, cond_dim=None, ff_mult=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm1 = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.norm2 = nn.RMSNorm(d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.cond_dim = cond_dim\n",
        "        self.self = MultiHeadAttention(d_model, d_head=d_head, dropout=0)\n",
        "        if self.cond_dim!=None: self.cross = MultiHeadAttention(d_model, d_head=d_head, cond_dim=cond_dim, dropout=0)\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        ff_dim=d_model*ff_mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), nn.Dropout(dropout), nn.Linear(d_model, ff_dim), act, # ReLU GELU\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "            nn.RMSNorm(d_model), nn.Dropout(dropout), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            # nn.RMSNorm(d_model), act, nn.Dropout(dropout), nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,c,h,w], [batch, num_tok, cond_dim], [batch,T]\n",
        "        # bchw = x.shape\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "\n",
        "        if self.cond_dim==None: cond=None # is self attn\n",
        "        x = x + self.drop(self.self(self.norm1(x)))\n",
        "        if self.cond_dim!=None: x = x + self.cross(self.norm2(x), cond, mask)\n",
        "        x = x + self.ff(x)\n",
        "        # x = x + self.drop(self.ff(x))\n",
        "\n",
        "        # return x.transpose(1,2).reshape(*bchw)\n",
        "        return x\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "qUhlOV8_RsRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title TransformerVICReg save\n",
        "import math\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        for layer in self:\n",
        "            params = inspect.signature(layer.forward).parameters.keys()\n",
        "            layer._fwdparams = ','.join(params)\n",
        "\n",
        "    def forward(self, x, mask=None, cond=None):\n",
        "        for layer in self:\n",
        "            args = [x]\n",
        "            if 'mask' in layer._fwdparams: args.append(mask)\n",
        "            if 'cond' in layer._fwdparams: args.append(cond)\n",
        "            x = layer(*args)\n",
        "        return x\n",
        "\n",
        "class TransformerVICReg(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, drop=0):\n",
        "        super().__init__()\n",
        "        act = nn.GELU()\n",
        "        # self.embed = nn.Sequential(nn.Linear(in_dim, d_model), act)\n",
        "        self.embed = nn.Sequential(\n",
        "            nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2),\n",
        "            )\n",
        "        self.pos_encoder = RoPE(d_model, seq_len=200, base=10000)\n",
        "        # d_hid = d_hid or d_model#*2\n",
        "        # self.encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "        self.encoder = Seq(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "\n",
        "        out_dim = out_dim or d_model\n",
        "        self.lin = nn.Linear(d_model, out_dim)\n",
        "        self.attention_pool = nn.Linear(d_model, 1, bias=False)\n",
        "\n",
        "        dim_v = d_model * 4\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(out_dim, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v, bias=False),\n",
        "            )\n",
        "\n",
        "    def forward(self, x, src_key_padding_mask=None): # [batch, seq, d_model], [batch, seq] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # x = self.embed(x)\n",
        "        x = self.embed(x.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "        # batch, seq_len, d_model = x.shape\n",
        "        # x = torch.cat([self.cls.repeat(batch,1,1), x], dim=1)\n",
        "        # src_key_padding_mask = torch.cat([torch.zeros((batch, 1), dtype=torch.bool), src_key_padding_mask], dim=1)\n",
        "        x = self.pos_encoder(x)\n",
        "\n",
        "        x = self.encoder(x, mask=src_key_padding_mask)\n",
        "        # print(\"fwd\",out.shape) # float [batch, seq_len, d_model]\n",
        "\n",
        "        attn = self.attention_pool(x).squeeze(-1) # [batch, seq] # seq_pool\n",
        "        out = (torch.softmax(attn, dim=-1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, seq] @ [batch, seq, dim] -> [batch, dim]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "    def expand(self, x, src_key_padding_mask=None):\n",
        "        sx = self.forward(x, src_key_padding_mask=src_key_padding_mask)\n",
        "        vx = self.exp(sx)\n",
        "        return vx\n",
        "\n",
        "batch, seq_len, d_model = 4,3500,512\n",
        "in_dim, out_dim=3,16\n",
        "model = TransformerVICReg(in_dim, d_model, out_dim, d_head=4, nlayers=2, drop=0.).to(device)\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "src_key_padding_mask = torch.stack([(torch.arange(seq_len) < seq_len - v) for v in torch.randint(seq_len, (batch,))]) # True will be ignored # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "print(src_key_padding_mask)\n",
        "out = model(x, src_key_padding_mask)\n",
        "print(out.shape)\n"
      ],
      "metadata": {
        "id": "ODpKypTCsfIt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e4dc1cf-22f9-4400-cb37-078985cce64d",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ True,  True,  True,  ..., False, False, False],\n",
            "        [ True,  True,  True,  ..., False, False, False],\n",
            "        [ True,  True,  True,  ..., False, False, False],\n",
            "        [ True,  True,  True,  ..., False, False, False]])\n",
            "torch.Size([4, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Violet save\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class Violet(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, nlayers=2, d_head=4):\n",
        "        super().__init__()\n",
        "        out_dim = out_dim or d_model\n",
        "\n",
        "        # self.rnn = RNN(d_model, d_model, d_model, nlayers)\n",
        "        self.student = TransformerVICReg(in_dim, d_model, out_dim=out_dim, d_head=d_head, nlayers=nlayers, drop=0.)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "\n",
        "        # vicreg\n",
        "        self.sim_coeff=25. # 10.0 # 25.0 # λ\n",
        "        self.std_coeff=25. # 1.0 # 25.0 # µ\n",
        "        self.cov_coeff=1. # 25.0 # 1.0 # ν\n",
        "\n",
        "        self.classifier = nn.Linear(out_dim, 18) # 10 18\n",
        "\n",
        "    def loss(self, x, src_key_padding_mask=None): # [batch, T, 3]\n",
        "        batch, seq, dim = x.shape\n",
        "        # print(x.shape)\n",
        "        vx = self.student.expand(x, src_key_padding_mask=src_key_padding_mask) # [batch, num_context_toks, out_dim]\n",
        "        with torch.no_grad(): vy = self.teacher.expand(x.detach(), src_key_padding_mask=src_key_padding_mask) # [batch, num_trg_toks, out_dim]\n",
        "        loss = self.vicreg(vx, vy)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x, src_key_padding_mask=None): # [batch, T, 3]\n",
        "        sx = self.student(x, src_key_padding_mask=src_key_padding_mask)\n",
        "        return sx\n",
        "\n",
        "    def classify(self, x): # [batch, T, 3]\n",
        "        sx = self.forward(x)\n",
        "        out = self.classifier(sx)\n",
        "        return out\n",
        "\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size, num_features = x.shape\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = self.sim_coeff * repr_loss + self.std_coeff * std_loss + self.cov_coeff * cov_loss\n",
        "        print(\"in vicreg \",self.sim_coeff * repr_loss.item() , self.std_coeff * std_loss.item() , self.cov_coeff * cov_loss.item())\n",
        "        # return loss\n",
        "        return repr_loss, std_loss, cov_loss\n",
        "\n",
        "\n",
        "violet = Violet(in_dim=3, d_model=32, out_dim=16, nlayers=2, d_head=4).to(device)\n",
        "# voptim = torch.optim.AdamW(violet.parameters(), lr=1e-3) # 1e-3?\n",
        "voptim = torch.optim.AdamW([{'params': violet.student.encoder.parameters()},\n",
        "    {'params': violet.student.exp.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # default 1e-2\n",
        "print(sum(p.numel() for p in violet.parameters() if p.requires_grad)) # 27584\n",
        "\n",
        "x = torch.rand((2,1000,3), device=device)\n",
        "# x = torch.rand((2,1,16), device=device)\n",
        "loss = violet.loss(x)\n",
        "# print(out.shape)\n",
        "print(loss)\n"
      ],
      "metadata": {
        "id": "5qwg9dG4sQ3h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11e258f7-70a0-4922-89f5-9c3862a34c44",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "55330\n",
            "in vicreg  3.6479194409180554e-16 24.74782019853592 7.794930811932943e-10\n",
            "(tensor(1.4592e-17, device='cuda:0', grad_fn=<MseLossBackward0>), tensor(0.9899, device='cuda:0', grad_fn=<AddBackward0>), tensor(7.7949e-10, device='cuda:0', grad_fn=<AddBackward0>))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test WISDM\n",
        "\n",
        "# print(activity_dataframe)\n",
        "# print(activity_dataframe['activity'])\n",
        "# activity_dataframe.to_csv('data.csv',index=False)\n",
        "\n",
        "# data = pd.read_csv(\"/data.csv\", index_col =\"Name\")\n",
        "# data = pd.read_csv(\"data.csv\")\n",
        "\n",
        "# a = list(activity_dataframe['timestamp'])\n",
        "# print(a)\n",
        "# print([x-y for x,y in zip(a[1:],a[:-1])])\n",
        "\n",
        "# print(data.loc[0])\n",
        "# p = data.loc[0]\n",
        "# print(len(p))\n",
        "# ID, activity, timestamp, x, y, z, meter, device = data.loc[0]\n",
        "# print(activity)\n",
        "\n",
        "# print(data.loc[1639])\n",
        "\n",
        "# userids = data['ID'].unique()#for id in userids\n",
        "# print(data['activity'].unique())\n",
        "# df_keep = data[['ID','activity','timestamp','x','y','z']]\n",
        "\n",
        "\n",
        "# grouped = df_keep.groupby(['ID'])\n",
        "# grouped = df_keep.groupby(['ID','activity'])\n",
        "# print(len(grouped))\n",
        "# user_acts = dict(tuple(df_keep.groupby(['ID','activity'])))\n",
        "# print(user_acts)\n",
        "# print(len(user_acts))\n",
        "# temp_df = df[df['ID'] == id]\n",
        "\n",
        "# print(len(df_keep))\n",
        "# print(len(data))\n",
        "\n",
        "\n",
        "\n",
        "# act = [[a, d[['timestamp','x','y','z']].to_numpy()] for a, d in user_acts.items()]\n",
        "# act = [[(int(a[0]), ), d[['timestamp','x','y','z']].to_numpy()] for a, d in user_acts.items()]\n",
        "# print(act)\n",
        "# print(act[0])\n",
        "# # print(act[0][1])\n",
        "# print(len(act[1]))\n",
        "# print([len(a[1]) for a in act])\n",
        "# print(min([len(a[1]) for a in act])) # 3567\n",
        "\n",
        "\n",
        "# act_dict = {i: act for i, act in enumerate(data['activity'].unique())}\n",
        "# act_invdict = {v: k for k, v in act_dict.items()}\n",
        "# print(act_invdict)\n",
        "\n",
        "# torch.tensor(act[0][1][:3500])\n",
        "\n",
        "# id_act, x = act[0]\n",
        "# id_act = self.process(id_act)\n",
        "# return id_act,\n",
        "# torch.tensor(x[:3500]) # 3567\n",
        "\n",
        "\n",
        "# /content/processed/wisdm-dataset/raw/phone/accel/data.csv\n",
        "# /content/processed/wisdm-dataset/raw/watch/gyro/data.csv\n",
        "\n",
        "# data0 = pd.read_csv(\"/content/processed/wisdm-dataset/raw/watch/accel/data.csv\")\n",
        "# data1 = pd.read_csv(\"/content/processed/wisdm-dataset/raw/watch/gyro/data.csv\")\n",
        "\n",
        "# user_acts0 = dict(tuple(data0.groupby(['ID','activity'])))\n",
        "# user_acts1 = dict(tuple(data1.groupby(['ID','activity'])))\n",
        "\n",
        "for (a0,d0), (a1,d1) in zip(user_acts0.items(), user_acts1.items()):\n",
        "    print(a0,a1)\n",
        "    print(len(d0),len(d1))\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rV7WLiNT4EAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title har_cnn\n",
        "# https://arxiv.org/pdf/2209.08335v1\n",
        "# https://github.com/Lou1sM/HAR/blob/master/har_cnn.py\n",
        "import sys\n",
        "import json\n",
        "from hmmlearn import hmm\n",
        "from copy import deepcopy\n",
        "import os\n",
        "import math\n",
        "from pdb import set_trace\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import time\n",
        "from torch.utils import data\n",
        "import cl_args\n",
        "from dl_utils.misc import asMinutes,check_dir\n",
        "#from dl_utils.label_funcs import accuracy, mean_f1, debable, translate_labellings, get_num_labels, label_counts, dummy_labels, avoid_minus_ones_lf_wrapper,masked_mode,acc_by_label, get_trans_dict\n",
        "from label_funcs_tmp import accuracy, mean_f1, translate_labellings, get_num_labels, label_counts, dummy_labels, avoid_minus_ones_lf_wrapper,masked_mode,acc_by_label, get_trans_dict\n",
        "from dl_utils.tensor_funcs import noiseify, numpyify, cudify\n",
        "from make_dsets import make_single_dset, make_dsets_by_user\n",
        "from sklearn.metrics import normalized_mutual_info_score,adjusted_rand_score\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from project_config import get_dataset_info_object\n",
        "\n",
        "rari = lambda x,y: round(adjusted_rand_score(x,y),4)\n",
        "rnmi = lambda x,y: round(normalized_mutual_info_score(x,y),4)\n",
        "\n",
        "class EncByLayer(nn.Module):\n",
        "    def __init__(self,x_filters,y_filters,x_strides,y_strides,max_pools,show_shapes):\n",
        "        super().__init__()\n",
        "        self.show_shapes = show_shapes\n",
        "        num_layers = len(x_filters)\n",
        "        assert all(len(x)==num_layers for x in (y_filters,x_strides,y_strides,max_pools))\n",
        "        ncvs = [1]+[4*2**i for i in range(num_layers)]\n",
        "        # conv_layers = []\n",
        "        # for i in range(num_layers):\n",
        "        #     conv_layer = nn.Sequential(\n",
        "        #         nn.Conv2d(ncvs[i],ncvs[i+1],(x_filters[i],y_filters[i]),(x_strides[i],y_strides[i])), nn.BatchNorm2d(ncvs[i+1]) if i<num_layers-1 else nn.Identity(), nn.LeakyReLU(0.3), nn.MaxPool2d(max_pools[i])\n",
        "        #     )\n",
        "        #     conv_layers.append(conv_layer)\n",
        "        # self.conv_layers = nn.ModuleList(conv_layers)\n",
        "\n",
        "        self.conv_layers = nn.Sequential(*[\n",
        "                nn.Conv2d(ncvs[i],ncvs[i+1],(x_filters[i],y_filters[i]),(x_strides[i],y_strides[i])), nn.BatchNorm2d(ncvs[i+1]) if i<num_layers-1 else nn.Identity(), nn.LeakyReLU(0.3), nn.MaxPool2d(max_pools[i])\n",
        "             for i in range(nlayers)])\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        # if self.show_shapes: print(x.shape)\n",
        "        # for conv_layer in self.conv_layers:\n",
        "        x = conv_layer(x)\n",
        "            # if self.show_shapes: print(x.shape)\n",
        "        return x\n",
        "\n",
        "class DecByLayer(nn.Module):\n",
        "    def __init__(self,x_filters,y_filters,x_strides,y_strides,show_shapes):\n",
        "        super().__init__()\n",
        "        self.show_shapes = show_shapes\n",
        "        num_layers = len(x_filters)\n",
        "        assert all(len(x)==num_layers for x in (y_filters,x_strides,y_strides))\n",
        "        ncvs = [4*2**i for i in reversed(range(num_layers))]+[1]\n",
        "        conv_trans_layers = [nn.Sequential(\n",
        "                nn.ConvTranspose2d(ncvs[i],ncvs[i+1],(x_filters[i],y_filters[i]),(x_strides[i],y_strides[i])), nn.BatchNorm2d(ncvs[i+1]), nn.LeakyReLU(0.3),\n",
        "                )\n",
        "            for i in range(num_layers)]\n",
        "        self.conv_trans_layers = nn.ModuleList(conv_trans_layers)\n",
        "\n",
        "    def forward(self,x):\n",
        "        if self.show_shapes: print(x.shape)\n",
        "        for conv_trans_layer in self.conv_trans_layers:\n",
        "            x = conv_trans_layer(x)\n",
        "            if self.show_shapes: print(x.shape)\n",
        "        return x\n",
        "\n",
        "class Var_BS_MLP(nn.Module):\n",
        "    def __init__(self,input_size,hidden_size,output_size):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(nn.Linear(input_size,hidden_size), nn.BatchNorm1d(hidden_size), nn.LeakyReLU(0.3), nn.Linear(hidden_size,output_size))\n",
        "\n",
        "    def forward(self,x):\n",
        "        return self.mlp(x)\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n",
        "    dset_info_object = get_dataset_info_object(args.dset)\n",
        "    num_classes = args.num_classes if args.num_classes != -1 else dset_info_object.num_classes\n",
        "    if args.dset == 'UCI_feat':\n",
        "        enc = nn.Sequential(nn.Linear(561,500),nn.ReLU(),\n",
        "                            nn.Linear(500,500),nn.ReLU(),\n",
        "                            nn.Linear(500,2000),nn.ReLU(),\n",
        "                            nn.Linear(2000,6),nn.ReLU()).cuda()\n",
        "        dec = nn.Sequential(nn.Linear(6,2000),nn.ReLU(),\n",
        "                            nn.Linear(2000,500),nn.ReLU(),\n",
        "                            nn.Linear(500,500),nn.ReLU(),\n",
        "                            nn.Linear(500,561),nn.ReLU()).cuda()\n",
        "        mlp = Var_BS_MLP(6,256,num_classes).cuda()\n",
        "    else:\n",
        "        if args.window_size == 512:\n",
        "            x_filters = (50,40,7,4)\n",
        "            x_strides = (2,2,1,1)\n",
        "            max_pools = ((2,1),(2,1),(2,1),(2,1))\n",
        "        elif args.window_size == 100:\n",
        "            x_filters = (20,20,5,3)\n",
        "            x_strides = (1,1,1,1)\n",
        "            max_pools = ((2,1),(2,1),(2,1),1)\n",
        "        y_filters = (1,1,1,dset_info_object.num_channels)\n",
        "        y_strides = (1,1,1,1)\n",
        "        enc = EncByLayer(x_filters,y_filters,x_strides,y_strides,max_pools,show_shapes=args.show_shapes).cuda()\n",
        "        #if args.is_n2d:\n",
        "        x_filters_trans = (15,10,15,11)\n",
        "        x_strides_trans = (2,3,3,3)\n",
        "        y_filters_trans = (dset_info_object.num_channels,1,1,1)\n",
        "        dec = DecByLayer(x_filters_trans,y_filters_trans,x_strides_trans,y_strides,show_shapes=args.show_shapes).cuda()\n",
        "\n",
        "        optional_umap_like_net_in = Var_BS_MLP(32,256,2).cuda()\n",
        "        optional_umap_like_net_out = Var_BS_MLP(2,256,2).cuda()\n",
        "        if ARGS.is_uln:\n",
        "            enc = nn.Sequential(enc,nn.Flatten(1),Var_BS_MLP(32,256,2).cuda())\n",
        "            dec = nn.Sequential(Var_BS_MLP(32,256,2).cuda(),nn.Unflatten(2,(32,1,1)),dec)\n",
        "        mlp = Var_BS_MLP(2 if ARGS.is_uln else 32,256,num_classes).cuda()\n",
        "    if args.load_pretrained:\n",
        "        enc.load_state_dict(torch.load('enc_pretrained.pt'))\n",
        "    subj_ids = args.subj_ids\n",
        "\n",
        "    metric_dict = {'acc':accuracy,'nmi':rnmi,'ari':rari,'f1':mean_f1}\n",
        "    har = HARLearner(enc=enc,mlp=mlp,dec=dec,num_classes=num_classes,args=args,metric_dict=metric_dict)\n",
        "\n",
        "    start_time = time.time()\n",
        "    already_exists = check_dir(f\"experiments/{args.exp_name}/preds\")\n",
        "    check_dir(f\"experiments/{args.exp_name}/best_preds\")\n",
        "    if args.show_shapes:\n",
        "        dset_train, selected_acts = make_single_dset(args,subj_ids)\n",
        "        num_ftrs = dset_train.x.shape[-1]\n",
        "        print(num_ftrs)\n",
        "        lat = enc(torch.ones((2,1,args.window_size,num_ftrs),device='cuda'))\n",
        "        dec(lat)\n",
        "        sys.exit()\n",
        "    dsets_by_id = make_dsets_by_user(args,subj_ids)\n",
        "    if args.is_n2d:\n",
        "        for subj_id, (dset,sa) in dsets_by_id.items():\n",
        "            print(\"n2ding\", subj_id)\n",
        "            har.n2d_abl(subj_id,dset)\n",
        "    elif not args.subject_independent:\n",
        "        bad_ids = []\n",
        "        for user_id, (dset,sa) in dsets_by_id.items():\n",
        "            n = get_num_labels(dset.y)\n",
        "            if n < dset_info_object.num_classes/2:\n",
        "                print(f\"Excluding user {user_id}, only has {n} different labels, out of {num_classes}\")\n",
        "                bad_ids.append(user_id)\n",
        "        if not args.bad_ids: dsets_by_id = {k:v for k,v in dsets_by_id.items() if k not in bad_ids}\n",
        "        print('reloading clusterings for', [x for x in subj_ids[:args.reload_ids] if x not in bad_ids])\n",
        "        for rid in subj_ids[:args.reload_ids]:\n",
        "            if rid in bad_ids: continue\n",
        "            print('reloading clusterings for', rid)\n",
        "            rdset,sa = dsets_by_id.pop(rid)\n",
        "            best_preds = np.load(f'experiments/{args.exp_name}/best_preds/{rid}.npy')\n",
        "            preds = np.load(f'experiments/{args.exp_name}/preds/{rid}.npy')\n",
        "            har.log_preds_and_scores(rid,preds,best_preds,numpyify(rdset.y))\n",
        "        print('clustering remaining ids', [x for x in subj_ids[args.reload_ids:]], 'from scratch\\n')\n",
        "\n",
        "        print(\"CLUSTERING EACH DSET SEPARATELY\")\n",
        "        for subj_id, (dset,sa) in dsets_by_id.items():\n",
        "            print(\"clustering\", subj_id)\n",
        "            har.pseudo_label_cluster_meta_meta_loop(subj_id,dset)\n",
        "    elif args.subject_independent:\n",
        "        print(\"CLUSTERING AS SINGLE DSET\")\n",
        "        one_big_dset, selected_acts = make_single_dset(args,subj_ids)\n",
        "        har.pseudo_label_cluster_meta_meta_loop('all',one_big_dset)\n",
        "\n",
        "    results_file_path = f'experiments/{args.exp_name}/results.txt'\n",
        "    har.total_time = time.time() - start_time\n",
        "    har.log_final_scores(results_file_path)\n",
        "    har.express_times(results_file_path)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    ARGS, need_umap = cl_args.get_cl_args()\n",
        "    if need_umap: import umap\n",
        "    main(ARGS)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "N-4I7A5hJU7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Lou1sM download_datasets.sh\n",
        "# https://github.com/Lou1sM/HAR/blob/master/download_datasets.sh\n",
        "\n",
        "##!/bin/sh\n",
        "\n",
        "mkdir -p datasets\n",
        "cd datasets\n",
        "\n",
        "#PAMAP\n",
        "# echo -e \"\\n###############\"\n",
        "# echo GETTING AND PREPARING PAMAP DATA\n",
        "# echo -e \"###############\\n\"\n",
        "wget http://archive.ics.uci.edu/ml/machine-learning-databases/00231/PAMAP2_Dataset.zip\n",
        "unzip PAMAP2_Dataset.zip\n",
        "# python ../convert_data_to_np.py PAMAP\n",
        "\n",
        "#UCI\n",
        "# echo -e \"\\n###############\"\n",
        "# echo GETTING AND PREPARING UCI DATA\n",
        "# echo -e \"###############\\n\"\n",
        "mkdir -p UCI2\n",
        "cd UCI2\n",
        "wget http://archive.ics.uci.edu/ml/machine-learning-databases/00341/HAPT%20Data%20Set.zip\n",
        "unzip HAPT\\ Data\\ Set.zip\n",
        "# cd ..\n",
        "# python ../convert_data_to_np.py UCI-raw\n",
        "\n",
        "#mkdir -p capture24\n",
        "#cd capture24/\n",
        "\n",
        "#for i in $(seq -w 151)\n",
        "#do\n",
        "#    curl -JLO \"https://ora.ox.ac.uk/objects/uuid:92650814-a209-4607-9fb5-921eab761c11/download_file?safe_filename=P${i}.csv.gz&type_of_work=Dataset\"\n",
        "#done\n",
        "#\n",
        "#curl -JLO \"https://ora.ox.ac.uk/objects/uuid:92650814-a209-4607-9fb5-921eab761c11/download_file?safe_filename=metadata.csv&type_of_work=Dataset\"\n",
        "#curl -JLO \"https://ora.ox.ac.uk/objects/uuid:92650814-a209-4607-9fb5-921eab761c11/download_file?safe_filename=annotation-label-dictionary.csv&type_of_work=Dataset\"\n",
        "#\n",
        "#\n",
        "#for f in $(ls); do\n",
        "#    if [ ${f: -2} == \"gz\" ]; then\n",
        "#        gunzip $f;\n",
        "#    fi;\n",
        "#done\n",
        "\n",
        "#WISDM-v1\n",
        "# echo -e \"\\n###############\"\n",
        "# echo GETTING AND PREPARING WISDM-v1 DATA\n",
        "# echo -e \"###############\\n\"\n",
        "wget https://www.cis.fordham.edu/wisdm/includes/datasets/latest/WISDM_ar_latest.tar.gz\n",
        "gunzip WISDM_ar_latest.tar.gz\n",
        "tar -xf WISDM_ar_latest.tar\n",
        "\n",
        "# python ../convert_data_to_np.py WISDM-v1\n",
        "\n",
        "#WISDM-watch\n",
        "# echo -e \"\\n###############\"\n",
        "# echo GETTING AND PREPARING WISDM-watch DATA\n",
        "# echo -e \"###############\\n\"\n",
        "wget https://archive.ics.uci.edu/ml/machine-learning-databases/00507/wisdm-dataset.zip\n",
        "unzip wisdm-dataset.zip\n",
        "python ../convert_data_to_np.py WISDM-watch\n",
        "\n",
        "#REALDISP\n",
        "# echo -e \"\\n###############\"\n",
        "# echo GETTING REALDISP DATA\n",
        "# echo -e \"###############\\n\"\n",
        "mkdir -p realdisp\n",
        "cd realdisp\n",
        "mkdir -p RawData\n",
        "wget https://archive.ics.uci.edu/ml/machine-learning-databases/00305/realistic_sensor_displacement.zip\n",
        "unzip realistic_sensor_displacement.zip\n",
        "# cd ..\n",
        "# python ../convert_data_to_np.py REALDISP\n",
        "# cd ..\n",
        "# pwd\n",
        "#HHAR\n",
        "mkdir -p hhar\n",
        "wget http://archive.ics.uci.edu/ml/machine-learning-databases/00344/Activity%20recognition%20exp.zip\n",
        "unzip Activity\\ recognition\\ exp.zip\n",
        "# python ../convert_data_to_np.py HHAR\n",
        "# cd ..\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "cellView": "form",
        "id": "38tWCkK-0Zlu",
        "outputId": "7abe4629-3dcf-4981-810e-9e888be03cb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "leading zeros in decimal integer literals are not permitted; use an 0o prefix for octal integers (<ipython-input-1-94e7ebe34632>, line 10)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-94e7ebe34632>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    wget http://archive.ics.uci.edu/ml/machine-learning-databases/00231/PAMAP2_Dataset.zip\u001b[0m\n\u001b[0m                                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m leading zeros in decimal integer literals are not permitted; use an 0o prefix for octal integers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title PAMAP2\n",
        "# https://arxiv.org/pdf/2209.08335v1\n",
        "# https://archive.ics.uci.edu/dataset/231/pamap2+physical+activity+monitoring\n",
        "# https://archive.ics.uci.edu/static/public/231/pamap2+physical+activity+monitoring.zip\n",
        "# !wget http://archive.ics.uci.edu/ml/machine-learning-databases/00231/PAMAP2_Dataset.zip\n",
        "# !unzip PAMAP2_Dataset.zip\n",
        "\n",
        "import os\n",
        "data_dir = 'PAMAP2_Dataset/Protocol'\n",
        "np_dir = '/content/PAMAP2'\n",
        "if not os.path.isdir(np_dir):\n",
        "    os.makedirs(np_dir)\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def array_from_txt(inpath):\n",
        "    with open(inpath) as f:\n",
        "        d = f.readlines()\n",
        "        array = np.array([[float(x) for x in line.split()] for line in d])\n",
        "    return array\n",
        "\n",
        "def array_expanded(a,expanded_length):\n",
        "    if a.shape[0] >= expanded_length: return a\n",
        "    insert_every = mpf(a.shape[0]/(expanded_length-a.shape[0]))\n",
        "    additional_idxs = (np.arange(expanded_length-a.shape[0])*insert_every).astype(np.int)\n",
        "    values = a[additional_idxs]\n",
        "    expanded = np.insert(a,additional_idxs,values,axis=0)\n",
        "    assert expanded.shape[0] == expanded_length\n",
        "    return expanded\n",
        "\n",
        "def convert(inpath,outpath):\n",
        "    array = array_from_txt(inpath)\n",
        "    timestamps = array[:,0]\n",
        "    labels = array[:,1].astype(int)\n",
        "    # Delete orientation data, which webpage says is 'invalid in this data, and timestamp and label\n",
        "    array = np.delete(array,[0,1,2,13,14,15,16,30,31,32,33,47,48,49,50],1)\n",
        "    np.save(outpath+'_timestamps',timestamps)\n",
        "    np.save(outpath+'_labels',labels)\n",
        "    np.save(outpath,array)\n",
        "\n",
        "for filename in os.listdir(data_dir):\n",
        "    print(data_dir,filename)\n",
        "    # inpath = os.path.join(data_dir,filename)a\n",
        "    inpath = data_dir+'/'+filename\n",
        "    outpath = np_dir+'/'+filename.split('.')[0]\n",
        "    print(outpath)\n",
        "    # outpath = os.path.join(np_dir,filename.split('.')[0])\n",
        "    convert(inpath,outpath)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMvb1-PBQ9u-",
        "outputId": "764e8452-935e-4bcc-aa38-82faddee7da3",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PAMAP2_Dataset/Protocol subject106.dat\n",
            "/content/PAMAP2/subject106\n",
            "PAMAP2_Dataset/Protocol subject101.dat\n",
            "/content/PAMAP2/subject101\n",
            "PAMAP2_Dataset/Protocol subject107.dat\n",
            "/content/PAMAP2/subject107\n",
            "PAMAP2_Dataset/Protocol subject109.dat\n",
            "/content/PAMAP2/subject109\n",
            "PAMAP2_Dataset/Protocol subject103.dat\n",
            "/content/PAMAP2/subject103\n",
            "PAMAP2_Dataset/Protocol subject104.dat\n",
            "/content/PAMAP2/subject104\n",
            "PAMAP2_Dataset/Protocol subject108.dat\n",
            "/content/PAMAP2/subject108\n",
            "PAMAP2_Dataset/Protocol subject105.dat\n",
            "/content/PAMAP2/subject105\n",
            "PAMAP2_Dataset/Protocol subject102.dat\n",
            "/content/PAMAP2/subject102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# def make_pamap_dset_train_val(args,subj_ids):\n",
        "def make_pamap_dset_train_val(subj_ids):\n",
        "    # dset_info_object = PAMAP_INFO.PAMAP_INFO\n",
        "    dset_info_object = PAMAP_INFO\n",
        "    # x_train = np.concatenate([np.load(f'datasets/PAMAP2_Dataset/np_data/subject{s}.npy') for s in subj_ids])\n",
        "    # y_train = np.concatenate([np.load(f'datasets/PAMAP2_Dataset/np_data/subject{s}_labels.npy') for s in subj_ids])\n",
        "    x_train = np.concatenate([np.load(f'PAMAP2/subject{s}.npy') for s in subj_ids])\n",
        "    y_train = np.concatenate([np.load(f'PAMAP2/subject{s}_labels.npy') for s in subj_ids])\n",
        "    x_train = x_train[y_train!=0] # 0 is a transient activity\n",
        "    y_train = y_train[y_train!=0] # 0 is a transient activity\n",
        "    # x_train,y_train,selected_acts = preproc_xys(x_train,y_train,args.step_size,args.window_size,dset_info_object,subj_ids)\n",
        "    x_train,y_train,selected_acts = preproc_xys(x_train,y_train,1,1,dset_info_object,subj_ids)\n",
        "    # dset_train = StepDataset(x_train,y_train,window_size=args.window_size,step_size=args.step_size)\n",
        "    # dset_train = StepDataset(x_train,y_train,window_size=512,step_size=5)\n",
        "    dset_train = StepDataset(x_train,y_train,window_size=1,step_size=1)\n",
        "    return dset_train, selected_acts\n",
        "\n",
        "class HAR_Dataset_Container():\n",
        "    def __init__(self,code_name,dataset_dir_name,possible_subj_ids,num_channels,num_classes,action_name_dict):\n",
        "        self.code_name = code_name\n",
        "        self.dataset_dir_name = dataset_dir_name\n",
        "        self.possible_subj_ids = possible_subj_ids\n",
        "        self.num_channels = num_channels\n",
        "        self.num_classes = num_classes\n",
        "        self.action_name_dict = action_name_dict\n",
        "\n",
        "\n",
        "# PAMAP\n",
        "pamap_ids = [str(x) for x in range(101,110)]\n",
        "pamap_action_name_dict = {1:'lying',2:'sitting',3:'standing',4:'walking',5:'running',6:'cycling',7:'Nordic walking',9:'watching TV',10:'computer work',11:'car driving',12:'ascending stairs',13:'descending stairs',16:'vacuum cleaning',17:'ironing',18:'folding laundry',19:'house cleaning',20:'playing soccer',24:'rope jumping'}\n",
        "PAMAP_INFO = HAR_Dataset_Container(\n",
        "            code_name = 'PAMAP',\n",
        "            dataset_dir_name = 'PAMAP2_Dataset',\n",
        "            possible_subj_ids = pamap_ids,\n",
        "            num_channels = 39,\n",
        "            num_classes = 12,\n",
        "            action_name_dict = pamap_action_name_dict)\n",
        "\n",
        "dset_train, selected_acts = make_pamap_dset_train_val(pamap_ids)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "5TnsfKrMB0yF",
        "outputId": "07859402-b33c-4ede-c389-ef7a59c2f1ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no precomputed datasets, computing from scratch\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "invalid index to scalar variable.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-ddcbc2f0291f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m             action_name_dict = pamap_action_name_dict)\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mdset_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_acts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_pamap_dset_train_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpamap_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-ddcbc2f0291f>\u001b[0m in \u001b[0;36mmake_pamap_dset_train_val\u001b[0;34m(subj_ids)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# 0 is a transient activity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# x_train,y_train,selected_acts = preproc_xys(x_train,y_train,args.step_size,args.window_size,dset_info_object,subj_ids)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mselected_acts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreproc_xys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdset_info_object\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msubj_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;31m# dset_train = StepDataset(x_train,y_train,window_size=args.window_size,step_size=args.step_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# dset_train = StepDataset(x_train,y_train,window_size=512,step_size=5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-a048671b2310>\u001b[0m in \u001b[0;36mpreproc_xys\u001b[0;34m(x, y, step_size, window_size, dset_info_object, subj_ids)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mnum_windows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mstep_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m#mode_labels = np.array([stats.mode(y[w*step_size:w*step_size + window_size]).mode[0] if (y[w*step_size:w*step_size + window_size]==y[w*step_size]).all() else -1 for w in range(num_windows)])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mmode_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstep_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_windows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mselected_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mselected_acts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdset_info_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_name_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mact_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mact_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselected_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-a048671b2310>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mnum_windows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mstep_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m#mode_labels = np.array([stats.mode(y[w*step_size:w*step_size + window_size]).mode[0] if (y[w*step_size:w*step_size + window_size]==y[w*step_size]).all() else -1 for w in range(num_windows)])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mmode_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstep_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_windows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mselected_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mselected_acts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdset_info_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_name_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mact_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mact_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselected_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Lou1sM convert_data_to_np.py\n",
        "# https://github.com/Lou1sM/HAR/blob/master/convert_data_to_np.py\n",
        "from pdb import set_trace\n",
        "from collections import Counter\n",
        "from scipy.fft import fft\n",
        "from scipy import stats\n",
        "from mpmath import mp, mpf\n",
        "#from dl_utils import misc, label_funcs\n",
        "# from dl_utils import misc\n",
        "# import label_funcs_tmp\n",
        "import os\n",
        "from os.path import join\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "def array_from_txt(inpath):\n",
        "    with open(inpath) as f:\n",
        "        d = f.readlines()\n",
        "        array = np.array([[float(x) for x in line.split()] for line in d])\n",
        "    return array\n",
        "\n",
        "def array_expanded(a,expanded_length):\n",
        "    if a.shape[0] >= expanded_length: return a\n",
        "    insert_every = mpf(a.shape[0]/(expanded_length-a.shape[0]))\n",
        "    additional_idxs = (np.arange(expanded_length-a.shape[0])*insert_every).astype(np.int)\n",
        "    values = a[additional_idxs]\n",
        "    expanded = np.insert(a,additional_idxs,values,axis=0)\n",
        "    assert expanded.shape[0] == expanded_length\n",
        "    return expanded\n",
        "\n",
        "def convert(inpath,outpath):\n",
        "    array = array_from_txt(inpath)\n",
        "    timestamps = array[:,0]\n",
        "    labels = array[:,1].astype(np.int)\n",
        "    # Delete orientation data, which webpage says is 'invalid in this data, and timestamp and label\n",
        "    array = np.delete(array,[0,1,2,13,14,15,16,30,31,32,33,47,48,49,50],1)\n",
        "    np.save(outpath+'_timestamps',timestamps)\n",
        "    np.save(outpath+'_labels',labels)\n",
        "    np.save(outpath,array)\n",
        "\n",
        "def expand_and_fill_labels(a,propoer_length):\n",
        "    start_filler = -np.ones(a[0,3])\n",
        "    end_filler = -np.ones(propoer_length-a[-1,4])\n",
        "    nested_lists = [[a[i,2] for _ in range(a[i,4]-a[i,3])] + [-1]*(a[i+1,3]-a[i,4]) for i in range(len(a)-1)] + [[a[-1,2] for _ in range(a[-1,4]-a[-1,3])]]\n",
        "    middle = np.array([item for sublist in nested_lists for item in sublist])\n",
        "    total_label_array = np.concatenate((start_filler,middle,end_filler)).astype(np.int)\n",
        "    return total_label_array\n",
        "\n",
        "def add_dtft(signal):\n",
        "    fft_signal_complex = fft(signal,axis=-1)\n",
        "    fft_signal_modulusses = np.abs(fft_signal_complex)\n",
        "    return np.concatenate((signal,fft_signal_modulusses),axis=-1)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "# if sys.argv[1] == 'PAMAP':\n",
        "data_dir = 'PAMAP2_Dataset/Protocol'\n",
        "np_dir = 'PAMAP2_Dataset/np_data'\n",
        "print(\"\\n#####Preprocessing PAMAP2#####\\n\")\n",
        "if not os.path.isdir(np_dir):\n",
        "    os.makedirs(np_dir)\n",
        "\n",
        "for filename in os.listdir(data_dir):\n",
        "    print(filename)\n",
        "    inpath = join(data_dir,filename)\n",
        "    outpath = join(np_dir,filename.split('.')[0])\n",
        "    convert(inpath,outpath)\n",
        "\n",
        "# elif sys.argv[1] == 'UCI-raw':\n",
        "data_dir = 'UCI2/RawData'\n",
        "np_dir = 'UCI2/np_data'\n",
        "print(\"\\n#####Preprocessing UCI#####\\n\")\n",
        "if not os.path.isdir(np_dir):\n",
        "    os.makedirs(np_dir)\n",
        "\n",
        "raw_label_array = array_from_txt(join(data_dir,'labels.txt')).astype(int)\n",
        "def two_digitify(x): return '0'+str(x) if len(str(x))==1 else str(x)\n",
        "fnames = os.listdir(data_dir)\n",
        "for idx in range(1,31):\n",
        "    print(\"processing user\",idx)\n",
        "    acc_array_list = []\n",
        "    gyro_array_list = []\n",
        "    label_array_list = []\n",
        "    user_idx = two_digitify(idx)\n",
        "    acc_fpaths = sorted([fn for fn in fnames if f'user{user_idx}' in fn and 'acc' in fn])\n",
        "    gyro_fpaths = sorted([fn for fn in fnames if f'user{user_idx}' in fn and 'gyro' in fn])\n",
        "    assert len(acc_fpaths) == len(gyro_fpaths)\n",
        "    for fna,fng in zip(acc_fpaths,gyro_fpaths):\n",
        "        acc_exp_id = int(fna.split('exp')[1][:2])\n",
        "        gyro_exp_id = int(fng.split('exp')[1][:2])\n",
        "        assert acc_exp_id==gyro_exp_id\n",
        "        new_acc_array = array_from_txt(join(data_dir,fna))\n",
        "        new_gyro_array = array_from_txt(join(data_dir,fna))\n",
        "        label_array_block = raw_label_array[raw_label_array[:,0]==acc_exp_id]\n",
        "        filled_label_array_block = expand_and_fill_labels(label_array_block,new_acc_array.shape[0])\n",
        "        assert filled_label_array_block.shape[0] == new_acc_array.shape[0]\n",
        "        assert filled_label_array_block.shape[0] == new_gyro_array.shape[0]\n",
        "        label_array_list.append(filled_label_array_block)\n",
        "        acc_array_list.append(new_acc_array)\n",
        "        gyro_array_list.append(new_gyro_array)\n",
        "    label_array = np.concatenate(label_array_list)\n",
        "    acc_array = np.concatenate(acc_array_list)\n",
        "    gyro_array = np.concatenate(gyro_array_list)\n",
        "    total_array = np.concatenate((acc_array,gyro_array),axis=1)\n",
        "    outpath = join(np_dir,f'user{user_idx}.npy')\n",
        "    np.save(outpath,total_array)\n",
        "    label_outpath = join(np_dir,f'user{user_idx}_labels.npy')\n",
        "    np.save(label_outpath,label_array)\n",
        "\n",
        "# elif sys.argv[1] == 'WISDM-v1':\n",
        "with open('WISDM_ar_v1.1/WISDM_ar_v1.1_raw.txt') as f: text = f.readlines()\n",
        "print(\"\\n#####Preprocessing WISDM-v1#####\\n\")\n",
        "activities_list = ['Jogging','Walking','Upstairs','Downstairs','Standing','Sitting']\n",
        "X_list = []\n",
        "y_list = []\n",
        "users_list = []\n",
        "num_zeros = 0\n",
        "def process_line(line_to_process):\n",
        "    global num_zeros\n",
        "    if float(line_to_process.split(',')[2]) == 0: num_zeros += 1#print(\"Timestamp zero, discarding\")\n",
        "    else:\n",
        "        X_list.append([float(x) for x in line_to_process.split(',')[3:]])\n",
        "        y_list.append(activities_list.index(line_to_process.split(',')[1]))\n",
        "        users_list.append(line_to_process.split(',')[0])\n",
        "for i,raw_line in enumerate(text):\n",
        "    #line = line.replace(';','').replace('\\n','')\n",
        "    if raw_line == '\\n': continue\n",
        "    elif raw_line.endswith(',;\\n'): line = raw_line[:-3]\n",
        "    elif raw_line.endswith(';\\n'): line = raw_line[:-2]\n",
        "    elif raw_line.endswith(',\\n'): line = raw_line[:-2]\n",
        "    else: set_trace()\n",
        "    if len(line.split(',')) == 6:\n",
        "        try: process_line(line)\n",
        "        except: print(f\"Can't process line {i}, even though length 6: {raw_line}\\n\")\n",
        "    else:\n",
        "        print(f\"Bad format at line {i}:\\n{raw_line}\")\n",
        "        try:\n",
        "            line1, line2 = line.split(';')\n",
        "            process_line(line1); process_line(line2)\n",
        "            print(f\"I think this was two lines erroneously put on one line. Processing separately as\\n{line1}\\nand\\n{line2}\")\n",
        "        except: print(\"Can't process this line at all, omitting\")\n",
        "one_big_X_array = np.array(X_list)\n",
        "one_big_y_array = np.array(y_list)\n",
        "one_big_users_array = np.array(users_list)\n",
        "print(one_big_X_array.shape)\n",
        "print(one_big_y_array.shape)\n",
        "print(one_big_users_array.shape)\n",
        "print(f\"Number of zero lines: {num_zeros}\")\n",
        "misc.np_save(one_big_X_array,'wisdm_v1','X.npy')\n",
        "misc.np_save(one_big_y_array,'wisdm_v1','y.npy')\n",
        "misc.np_save(one_big_users_array,'wisdm_v1','users.npy')\n",
        "\n",
        "# elif sys.argv[1] == 'WISDM-watch':\n",
        "p_dir = 'wisdm-dataset/raw/phone'\n",
        "w_dir = 'wisdm-dataset/raw/watch'\n",
        "np_dir = 'wisdm-dataset/np_data'\n",
        "print(\"\\n#####Preprocessing WISDM-watch#####\\n\")\n",
        "if not os.path.isdir(np_dir):\n",
        "    os.makedirs(np_dir)\n",
        "\n",
        "mp.dps = 100 # Avoid floating point errors in label insertion function\n",
        "for user_idx in range(1600,1651):\n",
        "    print('user', user_idx)\n",
        "    phone_acc_path = join(p_dir,'accel',f'data_{user_idx}_accel_phone.txt')\n",
        "    watch_acc_path = join(w_dir,'accel',f'data_{user_idx}_accel_watch.txt')\n",
        "    phone_gyro_path = join(p_dir,'gyro',f'data_{user_idx}_gyro_phone.txt')\n",
        "    watch_gyro_path = join(w_dir,'gyro',f'data_{user_idx}_gyro_watch.txt')\n",
        "\n",
        "    label_codes_list = list('ABCDEFGHIJKLMOPQRS') # Missin 'N' is deliberate\n",
        "    def two_arrays_from_txt(inpath):\n",
        "        with open(inpath) as f:\n",
        "            d = f.readlines()\n",
        "            arr = np.array([[float(x) for x in line.strip(';\\n').split(',')[3:]] for line in d])\n",
        "            label_array = np.array([label_codes_list.index(line.split(',')[1]) for line in d])\n",
        "        return arr, label_array\n",
        "\n",
        "    phone_acc, label_array1 = two_arrays_from_txt(phone_acc_path)\n",
        "    watch_acc, label_array2 = two_arrays_from_txt(watch_acc_path)\n",
        "    phone_gyro, label_array3 = two_arrays_from_txt(phone_gyro_path)\n",
        "    watch_gyro, label_array4 = two_arrays_from_txt(watch_gyro_path)\n",
        "    user_arrays = [phone_acc,watch_acc,phone_gyro,watch_gyro]\n",
        "    label_arrays = [label_array1,label_array2,label_array3,label_array4]\n",
        "    max_len = max([a.shape[0] for a in user_arrays])\n",
        "    equalized_user_arrays = [array_expanded(a,max_len) for a in user_arrays]\n",
        "    equalized_label_arrays = [array_expanded(lab_a,max_len) for lab_a in label_arrays]\n",
        "    total_user_array = np.concatenate(equalized_user_arrays,axis=1)\n",
        "    mode_object = stats.mode(np.stack(equalized_label_arrays,axis=1),axis=1)\n",
        "    mode_labels = mode_object.mode[:,0]\n",
        "    # Print how many windows contained just 1 label, how many 2 etc.\n",
        "    #print('Agreement in labels:',label_funcs_tmp.label_counts(mode_object.count[:,0]))\n",
        "    certains = (mode_object.count == 4)[:,0]\n",
        "    user_fn = f'{user_idx}.npy'\n",
        "    misc.np_save(total_user_array,np_dir,user_fn)\n",
        "    user_labels_fn = f'{user_idx}_labels.npy'\n",
        "    misc.np_save(mode_labels,np_dir,user_labels_fn)\n",
        "    user_certains_fn = f'{user_idx}_certains.npy'\n",
        "    misc.np_save(certains,np_dir,user_certains_fn)\n",
        "\n",
        "# elif sys.argv[1] == 'REALDISP':\n",
        "data_dir = 'realdisp/RawData'\n",
        "np_dir = 'realdisp/np_data'\n",
        "print(\"\\n#####Preprocessing REALDISP#####\\n\")\n",
        "if not os.path.isdir(np_dir):\n",
        "    os.makedirs(np_dir)\n",
        "\n",
        "for filename in os.listdir(data_dir):\n",
        "    print(filename)\n",
        "    if filename == 'dataset manual.pdf': continue\n",
        "    if not filename.split('_')[1].startswith('ideal'):\n",
        "        continue\n",
        "    with open(join(data_dir,filename)) as f: xy = f.readlines()\n",
        "    ar = np.array([[float(item) for item in line.split('\\t')] for line in xy])\n",
        "    x = ar[:,:-1]\n",
        "    y = ar[:,-1].astype(int)\n",
        "\n",
        "    np.save(join(np_dir,filename.split('_')[0]), x)\n",
        "    np.save(join(np_dir,filename.split('_')[0])+'_labels', y)\n",
        "\n",
        "# elif sys.argv[1] == 'Capture24':\n",
        "np_dir = 'capture24/np_data'\n",
        "if not os.path.isdir(np_dir):\n",
        "    os.makedirs(np_dir)\n",
        "name_df = pd.read_csv('capture24/annotation-label-dictionary.csv')\n",
        "#name_conversion_dict = dict(zip(name_df['annotation'],name_df['label:DohertySpecific2018']))\n",
        "name_df = name_df[['annotation','label:DohertySpecific2018']]\n",
        "int_label_converter_df = pd.DataFrame(enumerate(name_df['label:DohertySpecific2018'].unique()),columns=['int_label','label:DohertySpecific2018'])\n",
        "int_label_converter_dict = dict(enumerate(name_df['label:DohertySpecific2018'].unique()))\n",
        "with open('capture24/int_label_converter_df.json','w') as f:\n",
        "    json.dump(int_label_converter_dict,f)\n",
        "name_df = name_df.merge(int_label_converter_df)\n",
        "for fname in os.listdir('capture24'):\n",
        "    if fname.endswith('.gz'): continue\n",
        "    subj_id = fname.split('.')[0]\n",
        "    if not subj_id.startswith('P') and not len(subj_id) == 4: continue # Skip metadata files\n",
        "    print(f\"converting {fname} to np\")\n",
        "    try: df = pd.read_csv(join('capture24',fname))\n",
        "    except: set_trace()\n",
        "    translated_df = df.merge(name_df)\n",
        "    x = translated_df[['x','y','z']].to_numpy()\n",
        "    y = translated_df['int_label'].to_numpy()\n",
        "    np.save(join(np_dir,f'{subj_id}.npy'),x)\n",
        "    np.save(join(np_dir,f'{subj_id}_labels.npy'),y)\n",
        "\n",
        "# elif sys.argv[1] == 'HHAR':\n",
        "data_dir = 'Activity recognition exp'\n",
        "np_dir = 'hhar/np_data'\n",
        "print(\"\\n#####Preprocessing HHAR#####\\n\")\n",
        "if not os.path.isdir(np_dir):\n",
        "    os.makedirs(np_dir)\n",
        "\n",
        "pandaload = lambda path: pd.read_csv(join(data_dir,'Phones_accelerometer.csv')).set_index('Creation_Time').drop(['Index','Arrival_Time','Model','Device'],axis=1).dropna()\n",
        "print('loading dataframes\\n')\n",
        "phone_acc_df = pandaload('Phones_accelerometer.csv')\n",
        "phone_gyro_df = pandaload('Phones_gyroscope.csv')\n",
        "watch_acc_df = pandaload('Watch_accelerometer.csv')\n",
        "watch_gyro_df = pandaload('Watch_gyroscope.csv')\n",
        "activities_list = ['bike', 'sit', 'stand', 'walk', 'stairsup', 'stairsdown']\n",
        "user_list = list('abcdefghi')\n",
        "\n",
        "for user_letter_name in user_list:\n",
        "    print('processing user', user_letter_name)\n",
        "    user_phone_acc = phone_acc_df.loc[phone_acc_df.User==user_letter_name]\n",
        "    user_phone_gyro = phone_gyro_df.loc[phone_gyro_df.User==user_letter_name]\n",
        "    user_watch_acc = watch_acc_df.loc[watch_acc_df.User==user_letter_name]\n",
        "    user_watch_gyro = watch_acc_df.loc[watch_gyro_df.User==user_letter_name]\n",
        "    assert all([user_watch_gyro.shape==d.shape for d in (user_phone_acc,user_phone_gyro,user_watch_acc)])\n",
        "    comb_phone = user_phone_acc.join(user_phone_gyro,how='outer',lsuffix='_acc',rsuffix='_gyro')\n",
        "    comb_watch = user_watch_acc.join(user_watch_gyro,how='outer',lsuffix='_acc',rsuffix='_gyro')\n",
        "    #if not (comb_watch.gt_acc == comb_watch.gt_gyro).all(): set_trace()\n",
        "    #if not (comb_phone.gt_acc == comb_phone.gt_gyro).all(): set_trace()\n",
        "    comb = comb_phone.join(comb_watch,how='outer',lsuffix='_phone',rsuffix='_watch')\n",
        "    duplicate_rows = [x for x,count in Counter(comb.index).items() if count > 1]\n",
        "    if len(duplicate_rows) > 10: set_trace()\n",
        "    elif len(duplicate_rows) > 0:\n",
        "        print( f\"removing {len(duplicate_rows)} duplicate rows\")\n",
        "        comb = comb.drop(duplicate_rows)\n",
        "    if not (comb.gt_acc_phone == comb.gt_acc_watch).all(): set_trace()\n",
        "    user_X_array = comb.drop([c for c in comb.columns if 'User' in c or 'gt' in c],axis=1).to_numpy()\n",
        "    user_y_array = np.array([activities_list.index(a) for a in comb['gt_acc_phone']])\n",
        "    save_path = join(np_dir,f\"{user_list.index(user_letter_name)+1}.npy\")\n",
        "    label_save_path = join(np_dir,f\"{user_list.index(user_letter_name)+1}_labels.npy\")\n",
        "    np.save(save_path,user_X_array,allow_pickle=False)\n",
        "    np.save(label_save_path,user_y_array,allow_pickle=False)\n",
        "    # Make smaller option for testing\n",
        "    np.save(join(np_dir,f\"0.npy\"),user_X_array[::1000],allow_pickle=False)\n",
        "    np.save(join(np_dir,f\"0_labels.npy\"),user_y_array[::1000],allow_pickle=False)\n",
        "\n",
        "else: print('\\nIncorrect or no dataset specified\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGU_rUapvjcE",
        "outputId": "b150f60b-4d01-47b5-b1f4-ea6b20a17027",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Incorrect or no dataset specified\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Lou1sM make_dsets.py\n",
        "# https://github.com/Lou1sM/HAR/blob/master/make_dsets.py\n",
        "import numpy as np\n",
        "# from dl_utils.misc import check_dir, CifarLikeDataset\n",
        "import os\n",
        "import torch\n",
        "# import project_config\n",
        "from scipy import stats\n",
        "from torch.utils import data\n",
        "#from dl_utils import label_funcs\n",
        "# from dl_utils.tensor_funcs import cudify\n",
        "# import label_funcs_tmp\n",
        "from pdb import set_trace\n",
        "from os.path import join\n",
        "\n",
        "\n",
        "class ChunkDataset(data.Dataset):\n",
        "    def __init__(self,x,y):\n",
        "        self.x, self.y = x,y\n",
        "    def __len__(self): return len(self.x)\n",
        "    def __getitem__(self,idx):\n",
        "        batch_x = self.x[idx].unsqueeze(0)\n",
        "        batch_y = self.y[idx]\n",
        "        return batch_x, batch_y, idx\n",
        "\n",
        "class ConcattedDataset(data.Dataset):\n",
        "    \"\"\"Needs datasets to be StepDatasets in order to Concat them.\"\"\"\n",
        "    def __init__(self,xs,ys,window_size,step_size):\n",
        "        self.x, self.y = torch.cat(xs),torch.cat(ys)\n",
        "        self.window_size = window_size\n",
        "        self.step_size = step_size\n",
        "        component_dset_lengths = [((len(x)-self.window_size)//self.step_size + 1) for x in xs]\n",
        "        x_idx_locs = []\n",
        "        block_start_idx = 0\n",
        "        for x in xs:\n",
        "            x_idx_locs += list(range(block_start_idx,block_start_idx+len(x)-window_size+1,step_size))\n",
        "            block_start_idx += len(x)\n",
        "        self.x_idx_locs = np.array(x_idx_locs)\n",
        "        if not len(self.x_idx_locs) == len(self.y): set_trace()\n",
        "\n",
        "    def __len__(self): return len(self.y)\n",
        "    def __getitem__(self,idx):\n",
        "        x_idx = self.x_idx_locs[idx]\n",
        "        batch_x = self.x[x_idx:x_idx + self.window_size].unsqueeze(0)\n",
        "        batch_y = self.y[idx]\n",
        "        return batch_x, batch_y, idx\n",
        "\n",
        "class UCIFeatDataset(data.Dataset):\n",
        "    def __init__(self,x,y,transforms=[]):\n",
        "        self.x, self.y = cudify(x).float(),cudify(y).float()\n",
        "        assert len(self.x) == len(self.y)\n",
        "        for transform in transforms:\n",
        "            self.x = transform(self.x)\n",
        "    def __len__(self): return len(self.x)\n",
        "    def __getitem__(self,idx):\n",
        "        batch_x = self.x[idx]\n",
        "        batch_y = self.y[idx]\n",
        "        return batch_x, batch_y, idx\n",
        "\n",
        "class StepDataset(data.Dataset):\n",
        "    def __init__(self,x,y,window_size,step_size,transforms=[]):\n",
        "        self.x, self.y = cudify(x).float(),cudify(y).float()\n",
        "        self.window_size = window_size\n",
        "        self.step_size = step_size\n",
        "        self.transforms = transforms\n",
        "        self.position = None\n",
        "        self.ensemble_size = None\n",
        "        for transform in transforms:\n",
        "            self.x = transform(self.x)\n",
        "    def __len__(self): return (len(self.x)-self.window_size)//self.step_size + 1\n",
        "    def __getitem__(self,idx):\n",
        "        batch_x = self.x[idx*self.step_size:(idx*self.step_size) + self.window_size].unsqueeze(0)\n",
        "        batch_y = self.y[idx]\n",
        "        return batch_x, batch_y, idx\n",
        "\n",
        "    def put_in_ensemble(self,position,ensemble_size):\n",
        "        self.y += ensemble_size*position\n",
        "        self.position = position\n",
        "        self.ensemble_size = ensemble_size\n",
        "\n",
        "def preproc_xys(x,y,step_size,window_size,dset_info_object,subj_ids):\n",
        "    ids_string = 'all' if set(subj_ids) == set(dset_info_object.possible_subj_ids) else \"-\".join(subj_ids)\n",
        "    precomp_dir = f'datasets/{dset_info_object.dataset_dir_name}/precomputed/{ids_string}step{step_size}_window{window_size}/'\n",
        "    if os.path.isfile(join(precomp_dir,'x.pt')) and os.path.isfile(join(precomp_dir,'y.pt')):\n",
        "        print(\"loading precomputed datasets\")\n",
        "        x = torch.load(join(precomp_dir,'x.pt'))\n",
        "        y = torch.load(join(precomp_dir,'y.pt'))\n",
        "        with open(join(precomp_dir,'selected_acts.txt')) as f: selected_acts = f.readlines()\n",
        "    else:\n",
        "        print(\"no precomputed datasets, computing from scratch\")\n",
        "        xnans = np.isnan(x).any(axis=1)\n",
        "        x = x[~xnans]\n",
        "        y = y[~xnans]\n",
        "        x = x[y!=-1]\n",
        "        y = y[y!=-1]\n",
        "        num_windows = (len(x) - window_size)//step_size + 1\n",
        "        #mode_labels = np.array([stats.mode(y[w*step_size:w*step_size + window_size]).mode[0] if (y[w*step_size:w*step_size + window_size]==y[w*step_size]).all() else -1 for w in range(num_windows)])\n",
        "        mode_labels = np.array([stats.mode(y[w*step_size:w*step_size + window_size]).mode[0] for w in range(num_windows)])\n",
        "        selected_ids = set(mode_labels)\n",
        "        selected_acts = [dset_info_object.action_name_dict[act_id] for act_id in selected_ids]\n",
        "        mode_labels, trans_dict, changed = label_funcs_tmp.compress_labels(mode_labels)\n",
        "        assert len(selected_acts) == len(set(mode_labels))\n",
        "        x = torch.tensor(x).float()\n",
        "        y = torch.tensor(mode_labels).float()\n",
        "        check_dir(precomp_dir)\n",
        "        torch.save(x,join(precomp_dir,'x.pt'))\n",
        "        torch.save(y,join(precomp_dir,'y.pt'))\n",
        "        with open(join(precomp_dir,'selected_acts.txt'),'w') as f:\n",
        "            for a in selected_acts: f.write(a+'\\n')\n",
        "    return x, y, selected_acts\n",
        "\n",
        "def make_pamap_dset_train_val(args,subj_ids):\n",
        "    # dset_info_object = project_config.PAMAP_INFO\n",
        "    dset_info_object = PAMAP_INFO\n",
        "    x_train = np.concatenate([np.load(f'datasets/PAMAP2_Dataset/np_data/subject{s}.npy') for s in subj_ids])\n",
        "    y_train = np.concatenate([np.load(f'datasets/PAMAP2_Dataset/np_data/subject{s}_labels.npy') for s in subj_ids])\n",
        "    x_train = x_train[y_train!=0] # 0 is a transient activity\n",
        "    y_train = y_train[y_train!=0] # 0 is a transient activity\n",
        "    x_train,y_train,selected_acts = preproc_xys(x_train,y_train,args.step_size,args.window_size,dset_info_object,subj_ids)\n",
        "    dset_train = StepDataset(x_train,y_train,window_size=args.window_size,step_size=args.step_size)\n",
        "    return dset_train, selected_acts\n",
        "\n",
        "# def make_uci_dset_train_val(args,subj_ids):\n",
        "#     dset_info_object = project_config.UCI_INFO\n",
        "#     x_train = np.concatenate([np.load(f'datasets/UCI2/np_data/user{s}.npy') for s in subj_ids])\n",
        "#     y_train = np.concatenate([np.load(f'datasets/UCI2/np_data/user{s}_labels.npy') for s in subj_ids])\n",
        "#     x_train = x_train[y_train<7] # Labels still begin at 1 at this point as\n",
        "#     y_train = y_train[y_train<7] # haven't been compressed, so select 1,..,6\n",
        "#     #x_train = x_train[y_train!=-1]\n",
        "#     #y_train = y_train[y_train!=-1]\n",
        "#     #y_val = y_val[y_val!=-1]\n",
        "#     x_train,y_train,selected_acts = preproc_xys(x_train,y_train,args.step_size,args.window_size,dset_info_object,subj_ids)\n",
        "#     dset_train = StepDataset(x_train,y_train,window_size=args.window_size,step_size=args.step_size)\n",
        "#     return dset_train, selected_acts\n",
        "\n",
        "# def make_uci_feat_dset_train_val():\n",
        "#     dset_info_object = project_config.UCI_INFO\n",
        "#     x = np.load(f'datasets/UCI_feat/uci_feat_data.npy')\n",
        "#     y = np.load(f'datasets/UCI_feat/uci_feat_targets.npy')\n",
        "#     selected_acts = dict(enumerate(['walking','upstairs','downstairs','sitting','standing','lying']))\n",
        "#     dset = UCIFeatDataset(x,y)\n",
        "#     #dset.x = dset.data\n",
        "#     #dset.y = dset.targets\n",
        "#     return dset, selected_acts\n",
        "\n",
        "# def make_wisdm_v1_dset_train_val(args,subj_ids):\n",
        "#     dset_info_object = project_config.WISDMv1_INFO\n",
        "#     x = np.load('datasets/wisdm_v1/X.npy')\n",
        "#     y = np.load('datasets/wisdm_v1/y.npy')\n",
        "#     users = np.load('datasets/wisdm_v1/users.npy')\n",
        "#     train_idxs_to_user = np.zeros(users.shape[0]).astype(np.bool)\n",
        "#     for subj_id in subj_ids:\n",
        "#         new_users = users==subj_id\n",
        "#         train_idxs_to_user = np.logical_or(train_idxs_to_user,new_users)\n",
        "#     x_train = x[train_idxs_to_user]\n",
        "#     y_train = y[train_idxs_to_user]\n",
        "#     x_train,y_train,selected_acts = preproc_xys(x_train,y_train,args.step_size,args.window_size,dset_info_object,subj_ids)\n",
        "#     dset_train = StepDataset(x_train,y_train,window_size=args.window_size,step_size=args.step_size)\n",
        "#     return dset_train, selected_acts\n",
        "\n",
        "# def make_wisdm_watch_dset_train_val(args,subj_ids):\n",
        "#     dset_info_object = project_config.WISDMwatch_INFO\n",
        "#     x_train = np.concatenate([np.load(f'datasets/wisdm-dataset/np_data/{s}.npy') for s in subj_ids])\n",
        "#     y_train = np.concatenate([np.load(f'datasets/wisdm-dataset/np_data/{s}_labels.npy') for s in subj_ids])\n",
        "#     certains_train = np.concatenate([np.load(f'datasets/wisdm-dataset/np_data/{s}_certains.npy') for s in subj_ids])\n",
        "#     x_train = x_train[certains_train]\n",
        "#     y_train = y_train[certains_train]\n",
        "#     x_train,y_train,selected_acts = preproc_xys(x_train,y_train,args.step_size,args.window_size,dset_info_object,subj_ids)\n",
        "#     dset_train = StepDataset(x_train,y_train,window_size=args.window_size,step_size=args.step_size)\n",
        "#     return dset_train, selected_acts\n",
        "\n",
        "# def make_realdisp_dset_train_val(args,subj_ids):\n",
        "#     dset_info_object = project_config.REALDISP_INFO\n",
        "#     x_train = np.concatenate([np.load(f'datasets/realdisp/np_data/subject{s}.npy') for s in subj_ids])\n",
        "#     y_train = np.concatenate([np.load(f'datasets/realdisp/np_data/subject{s}_labels.npy') for s in subj_ids])\n",
        "#     x_train = x_train[:,2:] #First two columns are timestamp\n",
        "#     x_train = x_train[y_train!=0] # 0 seems to be a transient activity\n",
        "#     y_train = y_train[y_train!=0] # 0 seems to be a transient activity\n",
        "#     x_train,y_train,selected_acts = preproc_xys(x_train,y_train,args.step_size,args.window_size,dset_info_object,subj_ids)\n",
        "#     dset_train = StepDataset(x_train,y_train,window_size=args.window_size,step_size=args.step_size)\n",
        "#     return dset_train, selected_acts\n",
        "\n",
        "# def make_hhar_dset_train_val(args,subj_ids):\n",
        "#     dset_info_object = project_config.HHAR_INFO\n",
        "#     x_train = np.concatenate([np.load(f'datasets/hhar/np_data/{s}.npy') for s in subj_ids])\n",
        "#     y_train = np.concatenate([np.load(f'datasets/hhar/np_data/{s}_labels.npy') for s in subj_ids])\n",
        "#     x_train,y_train,selected_acts = preproc_xys(x_train,y_train,args.step_size,args.window_size,dset_info_object,subj_ids)\n",
        "#     dset_train = StepDataset(x_train,y_train,window_size=args.window_size,step_size=args.step_size)\n",
        "#     return dset_train, selected_acts\n",
        "\n",
        "def make_capture_dset_train_val(args,subj_ids):\n",
        "    action_name_dict = {0: 'sleep', 1: 'sedentary-screen', 2: 'tasks-moderate', 3: 'sedentary-non-screen', 4: 'walking', 5: 'vehicle', 6: 'bicycling', 7: 'tasks-light', 8: 'sports-continuous', 9: 'sport-interrupted'} # Should also be saved in json file in datasets/capture24\n",
        "    subj_ids = len(subj_ids) - min(2,len(subj_ids)//2)\n",
        "    subj_ids = subj_ids[:subj_ids]\n",
        "    def three_digitify(x): return '00' + str(x) if len(str(x))==1 else '0' + str(x)\n",
        "    x_train = np.concatenate([np.load(f'datasets/capture24/np_data/P{three_digitify(s)}.npy') for s in subj_ids])\n",
        "    y_train = np.concatenate([np.load(f'datasets/capture24/np_data/P{three_digitify(s)}_labels.npy') for s in subj_ids])\n",
        "    x_train,y_train,selected_acts = preproc_xys(x_train,y_train,args.step_size,args.window_size,action_name_dict)\n",
        "    dset_train = StepDataset(x_train,y_train,window_size=args.window_size,step_size=args.step_size)\n",
        "    if len(subj_ids) <= 2: return dset_train, dset_train, selected_acts\n",
        "\n",
        "    # else make val dset\n",
        "    val_ids = subj_ids[subj_ids:]\n",
        "    x_val = np.concatenate([np.load(f'datasets/capture24/np_data/P{three_digitify(s)}.npy') for s in val_ids])\n",
        "    y_val = np.concatenate([np.load(f'datasets/capture24/np_data/P{three_digitify(s)}_labels.npy') for s in val_ids])\n",
        "    x_val,y_val,selected_acts = preproc_xys(x_val,y_val,args.step_size,args.window_size,action_name_dict)\n",
        "    dset_val = StepDataset(x_val,y_val,window_size=args.window_size,step_size=args.step_size)\n",
        "    return dset_train, dset_val, selected_acts\n",
        "\n",
        "def make_single_dset(args,subj_ids):\n",
        "    if args.dset == 'PAMAP':\n",
        "        return make_pamap_dset_train_val(args,subj_ids)\n",
        "    # if args.dset == 'UCI':\n",
        "    #     return make_uci_dset_train_val(args,subj_ids)\n",
        "    # if args.dset == 'UCI_feat':\n",
        "    #     return make_uci_feat_dset_train_val()\n",
        "    # if args.dset == 'WISDM-v1':\n",
        "    #     return make_wisdm_v1_dset_train_val(args,subj_ids)\n",
        "    # if args.dset == 'WISDM-watch':\n",
        "    #     return make_wisdm_watch_dset_train_val(args,subj_ids)\n",
        "    # if args.dset == 'REALDISP':\n",
        "    #     return make_realdisp_dset_train_val(args,subj_ids)\n",
        "    # if args.dset == 'HHAR':\n",
        "    #     return make_hhar_dset_train_val(args,subj_ids)\n",
        "    # if args.dset == 'Capture24':\n",
        "    #     return make_capture_dset_train_val(args,subj_ids)\n",
        "\n",
        "def make_dsets_by_user(args,subj_ids):\n",
        "    dsets_by_id = {}\n",
        "    for subj_id in subj_ids:\n",
        "        dset_subj, selected_acts_subj = make_single_dset(args,[subj_id])\n",
        "        dsets_by_id[subj_id] = dset_subj,selected_acts_subj\n",
        "    return dsets_by_id\n",
        "\n",
        "def chunked_up(x,step_size,window_size):\n",
        "    num_windows = (len(x) - window_size)//step_size + 1\n",
        "    return torch.stack([x[i*step_size:i*step_size+window_size] for i in range(num_windows)])\n",
        "\n",
        "def combine_dsets(dsets):\n",
        "    xs = [d.x for d in dsets]\n",
        "    ys = [d.y for d in dsets]\n",
        "    return ConcattedDataset(xs,ys,dsets[0].window_size,dsets[0].step_size)\n",
        "\n",
        "def combine_dsets_old(dsets):\n",
        "    processed_dset_xs = []\n",
        "    for dset in dsets:\n",
        "        if isinstance(dset,StepDataset):\n",
        "            processed_dset_x = chunked_up(dset.x,dset.step_size,dset.window_size)\n",
        "        elif isinstance(dset,ChunkDataset):\n",
        "            processed_dset_x = dset.x\n",
        "        else:\n",
        "            print(f\"you're trying to combine dsets on a {type(dset)}, but it has to be a dataset\")\n",
        "        processed_dset_xs.append(processed_dset_x)\n",
        "    x = torch.cat(processed_dset_xs)\n",
        "    y = torch.cat([dset.y for dset in dsets])\n",
        "    assert len(x) == len(y)\n",
        "    combined = ChunkDataset(x,y)\n",
        "    return combined\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "nQERarq97cIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Lou1sM project_config.py\n",
        "# https://github.com/Lou1sM/HAR/blob/master/project_config.py\n",
        "import sys\n",
        "import torch\n",
        "\n",
        "class HAR_Dataset_Container():\n",
        "    def __init__(self,code_name,dataset_dir_name,possible_subj_ids,num_channels,num_classes,action_name_dict):\n",
        "        self.code_name = code_name\n",
        "        self.dataset_dir_name = dataset_dir_name\n",
        "        self.possible_subj_ids = possible_subj_ids\n",
        "        self.num_channels = num_channels\n",
        "        self.num_classes = num_classes\n",
        "        self.action_name_dict = action_name_dict\n",
        "\n",
        "\n",
        "# PAMAP\n",
        "pamap_ids = [str(x) for x in range(101,110)]\n",
        "pamap_action_name_dict = {1:'lying',2:'sitting',3:'standing',4:'walking',5:'running',6:'cycling',7:'Nordic walking',9:'watching TV',10:'computer work',11:'car driving',12:'ascending stairs',13:'descending stairs',16:'vacuum cleaning',17:'ironing',18:'folding laundry',19:'house cleaning',20:'playing soccer',24:'rope jumping'}\n",
        "PAMAP_INFO = HAR_Dataset_Container(\n",
        "            code_name = 'PAMAP',\n",
        "            dataset_dir_name = 'PAMAP2_Dataset',\n",
        "            possible_subj_ids = pamap_ids,\n",
        "            num_channels = 39,\n",
        "            num_classes = 12,\n",
        "            action_name_dict = pamap_action_name_dict)\n",
        "\n",
        "# # UCI\n",
        "# def two_digitify(x): return '0' + str(x) if len(str(x))==1 else str(x)\n",
        "# uci_ids = [two_digitify(x) for x in range(1,30)]\n",
        "# uci_action_name_dict = {1:'walking',2:'walking upstairs',3:'walking downstairs',4:'sitting',5:'standing',6:'lying',7:'stand_to_sit',9:'sit_to_stand',10:'sit_to_lit',11:'lie_to_sit',12:'stand_to_lie',13:'lie_to_stand'}\n",
        "# UCI_INFO = HAR_Dataset_Container(\n",
        "#             code_name = 'UCI',\n",
        "#             dataset_dir_name = 'UCI2',\n",
        "#             possible_subj_ids = uci_ids,\n",
        "#             num_channels = 6,\n",
        "#             num_classes = 6,\n",
        "#             action_name_dict = uci_action_name_dict)\n",
        "\n",
        "# # UCI_feat\n",
        "# def two_digitify(x): return '0' + str(x) if len(str(x))==1 else str(x)\n",
        "# uci_feat_action_name_dict = {1:'walking',2:'walking upstairs',3:'walking downstairs',4:'sitting',5:'standing',6:'lying',7:'stand_to_sit',9:'sit_to_stand',10:'sit_to_lit',11:'lie_to_sit',12:'stand_to_lie',13:'lie_to_stand'}\n",
        "# UCI_FEAT_INFO = HAR_Dataset_Container(\n",
        "#             code_name = 'UCI_feat',\n",
        "#             dataset_dir_name = 'UCI2_feat',\n",
        "#             possible_subj_ids = ['0'],\n",
        "#             num_channels = 561,\n",
        "#             num_classes = 6,\n",
        "#             action_name_dict = uci_action_name_dict)\n",
        "\n",
        "# # WISDM-v1\n",
        "# wisdmv1_ids = [str(x) for x in range(1,37)] #Paper says 29 users but ids go up to 36\n",
        "# activities_list = ['Jogging','Walking','Upstairs','Downstairs','Standing','Sitting']\n",
        "# wisdmv1_action_name_dict = dict(zip(range(len(activities_list)),activities_list))\n",
        "# WISDMv1_INFO = HAR_Dataset_Container(\n",
        "#             code_name = 'WISDM-v1',\n",
        "#             dataset_dir_name = 'wisdm_v1',\n",
        "#             possible_subj_ids = wisdmv1_ids,\n",
        "#             num_channels = 3,\n",
        "#             num_classes = 6,\n",
        "#             action_name_dict = wisdmv1_action_name_dict)\n",
        "\n",
        "# # WISDM-watch\n",
        "# wisdmwatch_ids = [str(x) for x in range(1600,1651)]\n",
        "# with open('datasets/wisdm-dataset/activity_key.txt') as f: r=f.readlines()\n",
        "# activities_list = [x.split(' = ')[0] for x in r if ' = ' in x]\n",
        "# wisdmwatch_action_name_dict = dict(zip(range(len(activities_list)),activities_list))\n",
        "# WISDMwatch_INFO = HAR_Dataset_Container(\n",
        "#             code_name = 'WISDM-watch',\n",
        "#             dataset_dir_name = 'wisdm-dataset',\n",
        "#             possible_subj_ids = wisdmwatch_ids,\n",
        "#             num_channels = 12,\n",
        "#             num_classes = 17,\n",
        "#             action_name_dict = wisdmwatch_action_name_dict)\n",
        "\n",
        "# # REALDISP\n",
        "# realdisp_ids = [str(x) for x in range(1,18)]\n",
        "# activities_list = ['Walking','Jogging','Running','Jump up','Jump front & back','Jump sideways','Jump leg/arms open/closed','Jump rope','Trunk twist','Trunk twist','Waist bends forward','Waist rotation','Waist bends','Reach heels backwards','Lateral bend','Lateral bend with arm up','Repetitive forward stretching','Upper trunk and lower body opposite twist','Lateral elevation of arms','Frontal elevation of arms','Frontal hand claps','Frontal crossing of arms','Shoulders high-amplitude rotation','Shoulders low-amplitude rotation','Arms inner rotation','Knees','Heels','Knees bending','Knees','Rotation on the knees','Rowing','Elliptical bike','Cycling']\n",
        "# realdisp_action_name_dict = {i+1:act for i,act in enumerate(activities_list)}\n",
        "# REALDISP_INFO = HAR_Dataset_Container(\n",
        "#             code_name = 'REALDISP',\n",
        "#             dataset_dir_name = 'realdisp',\n",
        "#             possible_subj_ids = realdisp_ids,\n",
        "#             num_channels = 117,\n",
        "#             num_classes = 33,\n",
        "#             action_name_dict = realdisp_action_name_dict)\n",
        "\n",
        "# # HHAR\n",
        "# hhar_ids = [str(x) for x in range(0,10)]\n",
        "# activities_list = ['bike', 'sit', 'stand', 'walk', 'stairsup', 'stairsdown']\n",
        "# hhar_action_name_dict = {i:act for i,act in enumerate(activities_list)}\n",
        "# HHAR_INFO = HAR_Dataset_Container(\n",
        "#             code_name = 'HHAR',\n",
        "#             dataset_dir_name = 'hhar',\n",
        "#             possible_subj_ids = hhar_ids,\n",
        "#             num_channels = 12,\n",
        "#             num_classes = 6,\n",
        "#             action_name_dict = hhar_action_name_dict)\n",
        "\n",
        "# DSET_OBJECTS = [PAMAP_INFO, UCI_INFO, UCI_FEAT_INFO, WISDMv1_INFO, WISDMwatch_INFO,REALDISP_INFO,HHAR_INFO]\n",
        "DSET_OBJECTS = [PAMAP_INFO]\n",
        "\n",
        "\n",
        "def get_dataset_info_object(dset_name):\n",
        "    dsets_by_that_name = [d for d in DSET_OBJECTS if d.code_name == dset_name]\n",
        "    if len(dsets_by_that_name)==0: print(f\"{dset_name} is not a recognized dataset\"); sys.exit()\n",
        "    assert len(dsets_by_that_name)==1\n",
        "    return dsets_by_that_name[0]\n",
        "\n",
        "def get_num_time_points():\n",
        "    for dset_info_object in DSET_OBJECTS:\n",
        "        print(dset_info_object.code_name, sum([torch.load(f'datasets/{dset_info_object.dataset_dir_name}/precomputed/{s}step100_window512/x.pt').shape[0] for s in dset_info_object.possible_subj_ids]))\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "_PZ6amU5BhIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Lou1sM har_cnn.py\n",
        "# https://github.com/Lou1sM/HAR/blob/master/har_cnn.py\n",
        "import argparse\n",
        "import sys\n",
        "# import project_config\n",
        "\n",
        "\n",
        "def get_cl_args():\n",
        "    dset_options = ['PAMAP','UCI','WISDM-v1','WISDM-watch','REALDISP','Capture24']\n",
        "    # dset_options = [di.code_name for di in project_config.DSET_OBJECTS]\n",
        "    training_type_options = ['full','cluster_as_single','cluster_individually','train_frac_gts_as_single','find_similar_users']\n",
        "    parser = argparse.ArgumentParser()\n",
        "    subjs_group = parser.add_mutually_exclusive_group(required=False)\n",
        "    subjs_group.add_argument('--num_subjs',type=int)\n",
        "    subjs_group.add_argument('--subj_ids',type=str,nargs='+',default=['first'])\n",
        "    epochs_group = parser.add_mutually_exclusive_group(required=False)\n",
        "    epochs_group.add_argument('--full_epochs',action='store_true')\n",
        "    epochs_group.add_argument('--short_epochs',action='store_true')\n",
        "    parser.add_argument('--ablate_label_filter',action='store_true')\n",
        "    parser.add_argument('--all_subjs',action='store_true')\n",
        "    parser.add_argument('--bad_ids',action='store_true')\n",
        "    parser.add_argument('--batch_size_train',type=int,default=256)\n",
        "    parser.add_argument('--batch_size_val',type=int,default=1024)\n",
        "    parser.add_argument('--clusterer',type=str,choices=['HMM','GMM'],default='HMM')\n",
        "    parser.add_argument('--compute_cross_metrics',action='store_true')\n",
        "    parser.add_argument('--dec_lr',type=float,default=1e-3)\n",
        "    parser.add_argument('-d','--dset',type=str,default='UCI',choices=dset_options)\n",
        "    parser.add_argument('--enc_lr',type=float,default=1e-3)\n",
        "    parser.add_argument('--exp_name',type=str,default=\"try\")\n",
        "    parser.add_argument('--frac_gt_labels',type=float,default=0.1)\n",
        "    parser.add_argument('--gpu',type=str,default='0')\n",
        "    parser.add_argument('--is_n2d',action='store_true')\n",
        "    parser.add_argument('--is_uln',action='store_true',help='net for dim red. instead of umap')\n",
        "    parser.add_argument('--just_align_time',action='store_true')\n",
        "    parser.add_argument('--load_pretrained',action='store_true')\n",
        "    parser.add_argument('--mlp_lr',type=float,default=1e-3)\n",
        "    parser.add_argument('--no_umap',action='store_true')\n",
        "    parser.add_argument('--noise',type=float,default=1.)\n",
        "    parser.add_argument('--num_classes',type=int,default=-1)\n",
        "    parser.add_argument('--num_meta_epochs',type=int,default=1)\n",
        "    parser.add_argument('--num_meta_meta_epochs',type=int,default=1)\n",
        "    parser.add_argument('--num_pseudo_label_epochs',type=int,default=5)\n",
        "    parser.add_argument('--umap_dim',type=int,default=2)\n",
        "    parser.add_argument('--umap_neighbours',type=int,default=60)\n",
        "    parser.add_argument('--prob_thresh',type=float,default=.95)\n",
        "    parser.add_argument('--reinit',action='store_true')\n",
        "    parser.add_argument('--reload_ids',type=int,default=0)\n",
        "    parser.add_argument('--rlmbda',type=float,default=.1)\n",
        "    parser.add_argument('--show_transitions',action='store_true')\n",
        "    parser.add_argument('--step_size',type=int,default=5)\n",
        "    parser.add_argument('--subject_independent',action='store_true')\n",
        "    parser.add_argument('--test','-t',action='store_true')\n",
        "    parser.add_argument('--train_type',type=str,choices=training_type_options,default='full')\n",
        "    parser.add_argument('--show_shapes',action='store_true',help='print the shapes of hidden layers in enc and dec')\n",
        "    parser.add_argument('--verbose',action='store_true')\n",
        "    parser.add_argument('--window_size',type=int,default=512)\n",
        "    ARGS = parser.parse_args()\n",
        "\n",
        "    need_umap = False\n",
        "    if ARGS.is_uln:\n",
        "        ARGS.no_umap = True\n",
        "    if ARGS.short_epochs:\n",
        "        ARGS.num_meta_meta_epochs = 1\n",
        "        ARGS.num_meta_epochs = 1\n",
        "        ARGS.num_pseudo_label_epochs = 1\n",
        "    elif ARGS.full_epochs:\n",
        "        ARGS.num_meta_meta_epochs = 10\n",
        "        ARGS.num_meta_epochs = 10\n",
        "        ARGS.num_pseudo_label_epochs = 5\n",
        "    if ARGS.test:\n",
        "        ARGS.num_meta_epochs = 1\n",
        "        ARGS.num_meta_meta_epochs = 1\n",
        "        ARGS.num_pseudo_label_epochs = 1\n",
        "    elif not ARGS.no_umap and not ARGS.show_shapes: need_umap = True\n",
        "    print(ARGS)\n",
        "    dset_info_object = project_config.get_dataset_info_object(ARGS.dset)\n",
        "    all_possible_ids = dset_info_object.possible_subj_ids\n",
        "    if ARGS.all_subjs: ARGS.subj_ids=all_possible_ids\n",
        "    elif ARGS.num_subjs is not None: ARGS.subj_ids = all_possible_ids[:ARGS.num_subjs]\n",
        "    elif ARGS.subj_ids == ['first']: ARGS.subj_ids = all_possible_ids[:1]\n",
        "    bad_ids = [x for x in ARGS.subj_ids if x not in all_possible_ids]\n",
        "    if len(bad_ids) > 0 and not (ARGS.test and ARGS.dset=='HHAR'):\n",
        "        print(f\"You have specified non-existent ids: {bad_ids}\\nExistent ids are {all_possible_ids}\"); sys.exit()\n",
        "    return ARGS, need_umap\n",
        "\n",
        "RELEVANT_ARGS = ['ablate_label_filter','clusterer','dset','no_umap','num_meta_epochs','num_meta_meta_epochs','num_pseudo_label_epochs','reinit','step_size','subject_independent']\n",
        "\n",
        "\n",
        "# ARGS, need_umap = cl_args.get_cl_args()\n",
        "ARGS, need_umap = get_cl_args()\n",
        "# if need_umap: import umap\n",
        "# main(ARGS)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "cellView": "form",
        "id": "3UvqfQXfFSu1",
        "outputId": "84e2ee66-1f8e-47bf-88a6-ff9e8a6b9a8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] [--num_subjs NUM_SUBJS | --subj_ids SUBJ_IDS [SUBJ_IDS ...]]\n",
            "                                [--full_epochs | --short_epochs] [--ablate_label_filter]\n",
            "                                [--all_subjs] [--bad_ids] [--batch_size_train BATCH_SIZE_TRAIN]\n",
            "                                [--batch_size_val BATCH_SIZE_VAL] [--clusterer {HMM,GMM}]\n",
            "                                [--compute_cross_metrics] [--dec_lr DEC_LR]\n",
            "                                [-d {PAMAP,UCI,WISDM-v1,WISDM-watch,REALDISP,Capture24}]\n",
            "                                [--enc_lr ENC_LR] [--exp_name EXP_NAME]\n",
            "                                [--frac_gt_labels FRAC_GT_LABELS] [--gpu GPU] [--is_n2d]\n",
            "                                [--is_uln] [--just_align_time] [--load_pretrained]\n",
            "                                [--mlp_lr MLP_LR] [--no_umap] [--noise NOISE]\n",
            "                                [--num_classes NUM_CLASSES] [--num_meta_epochs NUM_META_EPOCHS]\n",
            "                                [--num_meta_meta_epochs NUM_META_META_EPOCHS]\n",
            "                                [--num_pseudo_label_epochs NUM_PSEUDO_LABEL_EPOCHS]\n",
            "                                [--umap_dim UMAP_DIM] [--umap_neighbours UMAP_NEIGHBOURS]\n",
            "                                [--prob_thresh PROB_THRESH] [--reinit] [--reload_ids RELOAD_IDS]\n",
            "                                [--rlmbda RLMBDA] [--show_transitions] [--step_size STEP_SIZE]\n",
            "                                [--subject_independent] [--test]\n",
            "                                [--train_type {full,cluster_as_single,cluster_individually,train_frac_gts_as_single,find_similar_users}]\n",
            "                                [--show_shapes] [--verbose] [--window_size WINDOW_SIZE]\n",
            "colab_kernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-50ea2cb4-a0b3-4dd4-be5b-f719dc2cbc66.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "2",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        }
      ]
    }
  ]
}