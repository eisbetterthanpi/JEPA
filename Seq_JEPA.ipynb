{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/Seq_JEPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## opportunity"
      ],
      "metadata": {
        "id": "670p_tNSKg5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title download opportunity\n",
        "!wget https://archive.ics.uci.edu/static/public/226/opportunity+activity+recognition.zip -O opportunity.zip\n",
        "!unzip opportunity.zip"
      ],
      "metadata": {
        "id": "Sc9DGN_MAziB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# path = '/content/OpportunityUCIDataset/dataset/'\n",
        "path = '/content/OpportunityUCIDataset/dataset/S1-ADL1.dat'\n",
        "path = '/content/OpportunityUCIDataset/dataset/S1-Drill.dat'\n",
        "# df = pd.read_table(path+file, header=None, sep='\\s+')\n",
        "df = pd.read_table(path, header=None, sep='\\s+')\n",
        "print(df)\n",
        "\n",
        "# 51116 rows x 250\n",
        "# 54966 rows x 250\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZK6WCoHmyEf",
        "outputId": "768f3782-9c07-40ec-99c8-01c66f3636fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           0     1       2      3      4      5      6      7       8     9    \\\n",
            "0            0  40.0  1031.0   49.0 -255.0  958.0  250.0 -232.0   969.0  44.0   \n",
            "1           33  49.0  1043.0   42.0 -244.0  954.0  257.0 -242.0   979.0  31.0   \n",
            "2           67  17.0  1049.0   84.0 -243.0  958.0  278.0 -256.0   984.0   2.0   \n",
            "3          100  23.0  1051.0   98.0 -234.0  969.0  295.0 -263.0   972.0 -10.0   \n",
            "4          133  -3.0  1045.0  102.0 -213.0  975.0  273.0 -255.0  1029.0 -44.0   \n",
            "...        ...   ...     ...    ...    ...    ...    ...    ...     ...   ...   \n",
            "54961  1832015   NaN     NaN    NaN    NaN    NaN    NaN    NaN     NaN   NaN   \n",
            "54962  1832048   NaN     NaN    NaN    NaN    NaN    NaN    NaN     NaN   NaN   \n",
            "54963  1832082   NaN     NaN    NaN    NaN    NaN    NaN    NaN     NaN   NaN   \n",
            "54964  1832115   NaN     NaN    NaN    NaN    NaN    NaN    NaN     NaN   NaN   \n",
            "54965  1832148   NaN     NaN    NaN    NaN    NaN    NaN    NaN     NaN   NaN   \n",
            "\n",
            "       ...   240   241   242  243  244  245  246  247  248  249  \n",
            "0      ...  5839  2764  1815    0    0    0    0    0    0    0  \n",
            "1      ...  5840  2765  1815    0    0    0    0    0    0    0  \n",
            "2      ...  5842  2766  1816    0    0    0    0    0    0    0  \n",
            "3      ...  5843  2767  1816    0    0    0    0    0    0    0  \n",
            "4      ...  5841  2766  1814    0    0    0    0    0    0    0  \n",
            "...    ...   ...   ...   ...  ...  ...  ...  ...  ...  ...  ...  \n",
            "54961  ...  3926  1613   869    0    0    0    0    0    0    0  \n",
            "54962  ...  3073  1262   680    0    0    0    0    0    0    0  \n",
            "54963  ...  2202   904   488    0    0    0    0    0    0    0  \n",
            "54964  ...  1371   563   304    0    0    0    0    0    0    0  \n",
            "54965  ...     0     0     0    0    0    0    0    0    0    0  \n",
            "\n",
            "[54966 rows x 250 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title opportunity me\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# https://github.com/IliesChibane/Exploring-the-OPPORTUNITY-Dataset-for-Activity-Recognition/blob/main/preprocess.ipynb\n",
        "\n",
        "# columns = []\n",
        "# with open('/content/OpportunityUCIDataset/dataset/column_names.txt') as f:\n",
        "#     for line in f.read().splitlines():\n",
        "#         print(line)\n",
        "#         # if 'Column' in line: columns.append(line[get_index(line):].split(\";\")[0])\n",
        "\n",
        "# dataset = pd.DataFrame(columns=columns)\n",
        "# path = '/content/OpportunityUCIDataset/dataset/'\n",
        "# for file in os.listdir(path):\n",
        "#     print(file)\n",
        "#     if not file.endswith('.dat'): continue\n",
        "# # for file, subject_id in zip(file_names, subject_id):\n",
        "#     df = pd.read_table(path+file, header=None, sep='\\s+')\n",
        "#     # df.columns = all_columns\n",
        "#     df['subject'] = file[:2]\n",
        "#     dataset = pd.concat([dataset, df], ignore_index=True)\n",
        "\n",
        "\n",
        "# 1: ms, 2-243: data, 244-250: labels\n",
        "\n",
        "\n",
        "# ans = [y for _, y in dataset.groupby(list(dataset)[244-1:250])] # cols 244-250\n",
        "ans = [y for _, y in dataset.groupby(list(dataset)[244-1])] # cols 244-250\n",
        "y=list(set([df['subject'].iloc[0] for df in ans]))\n",
        "y.sort()\n",
        "\n",
        "# dataset_size = len(train_data)\n",
        "# indices = list(range(dataset_size))\n",
        "# split = int(np.floor(0.7 * dataset_size))\n",
        "# np.random.seed(0)\n",
        "# np.random.shuffle(indices)\n",
        "# train_indices, val_indices = indices[:split], indices[split:]\n",
        "\n",
        "df_train = dataset[dataset['subject'].isin(y[:int(.7*len(y))])]\n",
        "df_test = dataset[dataset['subject'].isin(y[-int(.3*len(y)):])]\n",
        "\n",
        "def make_Xy(dataset):\n",
        "    # ans = [y for _, y in dataset.groupby(['subject', 'activity'])]\n",
        "    # ans = [y for _, y in dataset.groupby(list(dataset)[244-1:250])] # cols 244-250\n",
        "    ans = [y for _, y in dataset.groupby(list(dataset)[244-1])] # cols 244-250\n",
        "    y_train = [df[list(dataset)[244]].iloc[0] for df in ans]\n",
        "    # y_train = [df['subject'].iloc[0] for df in ans]\n",
        "    # X_train = [df.drop(['subject', 'activity','time'], axis=1) for df in X_train]\n",
        "    X_train = [df.drop(list(dataset)[244-1:250], axis=1) for df in ans]\n",
        "    X_train = [df.apply(pd.to_numeric, errors='coerce') for df in X_train] # Convert non-numeric data in dataset to numeric. errors='coerce': replace all non-numeric values with NaN.\n",
        "    X_train = [df.interpolate(method='index', axis=0, limit_direction='both') for df in X_train] # replace NaN by interpolating\n",
        "    # X_train = [df.interpolate(method='index', axis=0, limit_direction='both') for df in ans]\n",
        "    return X_train, y_train\n",
        "\n",
        "X_train, y_train = make_Xy(df_train)\n",
        "X_test, y_test = make_Xy(df_test)\n",
        "\n",
        "\n",
        "# data_dir = 'OpportunityUCIDataset/dataset'\n",
        "# files = os.listdir(data_dir)\n",
        "# files = [f for f in files if f.endswith('.dat')]\n",
        "# # Separate the ADL and Drill files\n",
        "# # ADL (Activities of Daily Living): naturalistic, semi-structured routines like making coffee, opening doors, etc.\n",
        "# # Drill: Controlled executions of specific gestures (e.g. open door with right arm) — highly structured, for training gesture classifiers.\n",
        "# dataset = pd.DataFrame(columns=columns)\n",
        "# list_of_files = [f for f in files if 'Drill' not in f]\n",
        "# for _, file in enumerate(list_of_files):\n",
        "#     proc_data = pd.read_table(os.path.join(data_dir, file), header=None, sep='\\s+')\n",
        "#     proc_data.columns = columns\n",
        "#     data_collection = pd.concat([data_collection, proc_data])\n",
        "# data_collection.reset_index(drop=True, inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ans = [y for _, y in dataset.groupby(['subject', 'activityID'])]\n",
        "# y_train = [df['activityID'].iloc[0] for df in ans]\n",
        "# # y_train = [df['subject'].iloc[0] for df in ans]\n",
        "# X_train = [df.drop(['subject', 'activityID'], axis=1) for df in ans]\n",
        "\n",
        "# dataset = dataset.apply(pd.to_numeric, errors='coerce') # Convert non-numeric data in dataset to numeric. errors='coerce': replace all non-numeric values with NaN.\n",
        "# X_train = [df.interpolate(method='index', axis=0, limit_direction='both') for df in dataset]\n",
        "\n",
        "# data.reset_index(drop=True, inplace=True) # make row ind start from 0\n"
      ],
      "metadata": {
        "id": "hJTKRUBGH8lK",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(dataset[244])\n",
        "# print(type(X_train[0]))\n",
        "# for x in X_train:\n",
        "#     print(len(x))\n",
        "# print(ans)\n",
        "# print(len(ans))\n",
        "# print(y)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# plt.plot(X_train[0].iloc[:,0])\n",
        "# plt.plot(dataset.iloc[:,0])\n",
        "# plt.plot(dataset.iloc[0])\n",
        "# for column in dataset.columns[:1]:#+dataset.columns[243:250]:\n",
        "#     dataset[column] = dataset[column] / dataset[column].abs().max()\n",
        "# for i in range(243,250):\n",
        "#     plt.plot(dataset[i], alpha=.3)\n",
        "plt.plot(dataset[243])\n",
        "# # plt.plot(dataset[243])\n",
        "plt.plot(dataset[0])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "TpKbecHx8i7h",
        "outputId": "60163c9f-ce2c-459e-b149-9ae84feb2737"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ans = [y for _, y in dataset.groupby(dataset[dataset.columns[list(range(244,250+1))]])]\n",
        "ans = [y for _, y in dataset.groupby(list(dataset)[244-1:250])]\n",
        "# print(list(dataset)[244-1:250+1-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67OgkHfj364i",
        "outputId": "3a7fe996-5852-40be-fc3c-7736d7d69f89"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[243, 244, 245, 246, 247, 248, 249]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title teco-kit dataloader_opportunity_har.py\n",
        "# https://github.com/teco-kit/timeseries-datasets/blob/main/dataloaders/dataloader_opportunity_har.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "# from dataloaders.dataloader_base import BASE_DATA\n",
        "# TODO the cols ! name\n",
        "# ========================================      Opportunity_HAR_DATA       =============================\n",
        "# class Opportunity_HAR_DATA(BASE_DATA):\n",
        "class Opportunity_HAR_DATA():\n",
        "    \"\"\"\n",
        "    OPPORTUNITY Dataset for Human Activity Recognition from Wearable, Object, and Ambient Sensors\n",
        "    Brief Description of the Dataset:\n",
        "    ---------------------------------\n",
        "    Each .dat file contains a matrix of data in text format.\n",
        "    Each line contains the sensor data sampled at a given time (sample rate: 30Hz).\n",
        "    For more detail . please reffer to the docomentation.html\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        # In this documents in doc/documentation.html, all columns definition coulde be found   (or in the column_names)\n",
        "        # the sensors between 134 and 248 are amounted on devices, so they will not to be considered\n",
        "        # Specifically, the following columns were used for the challenge:\n",
        "        # =============================================================\n",
        "        # 1-37, 38-46, 51-59, 64-72, 77-85, 90-98, 103-134, 244, 250.\n",
        "        # 0 milisconds\n",
        "        self.used_cols = [#1,  2,   3, # Accelerometer RKN^\n",
        "                          #4,  5,   6, # Accelerometer HIP\n",
        "                          #7,  8,   9, # Accelerometer LUA^\n",
        "                          #10, 11,  12, # Accelerometer RUA_\n",
        "                          #13, 14,  15, # Accelerometer LH\n",
        "                          #16, 17,  18, # Accelerometer BACK\n",
        "                          #19, 20,  21, # Accelerometer RKN_\n",
        "                          #22, 23,  24, # Accelerometer RWR\n",
        "                          #25, 26,  27, # Accelerometer RUA^\n",
        "                          #28, 29,  30, # Accelerometer LUA_\n",
        "                          #31, 32,  33, # Accelerometer LWR\n",
        "                          #34, 35,  36, # Accelerometer RH\n",
        "                          37, 38,  39, 40, 41, 42, 43, 44, 45, # InertialMeasurementUnit BACK\n",
        "                          50, 51,  52, 53, 54, 55, 56, 57, 58, # InertialMeasurementUnit RUA\n",
        "                          63, 64,  65, 66, 67, 68, 69, 70, 71, # InertialMeasurementUnit RLA\n",
        "                          76, 77,  78, 79, 80, 81, 82, 83, 84, # InertialMeasurementUnit LUA\n",
        "                          89, 90,  91, 92, 93, 94, 95, 96, 97,  # InertialMeasurementUnit LLA\n",
        "                          102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, # InertialMeasurementUnit L-SHOE\n",
        "                          118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, # InertialMeasurementUnit R-SHOE\n",
        "                          249  # Label\n",
        "                         ]\n",
        "\n",
        "        col_names         = [\"dim_{}\".format(i) for i in range(len(self.used_cols)-1)]\n",
        "        self.col_names    =  col_names + [\"activity_id\"]\n",
        "\n",
        "        self.label_map = [(0,      'Other'),\n",
        "                          (406516, 'Open Door 1'),\n",
        "                          (406517, 'Open Door 2'),\n",
        "                          (404516, 'Close Door 1'),\n",
        "                          (404517, 'Close Door 2'),\n",
        "                          (406520, 'Open Fridge'),\n",
        "                          (404520, 'Close Fridge'),\n",
        "                          (406505, 'Open Dishwasher'),\n",
        "                          (404505, 'Close Dishwasher'),\n",
        "                          (406519, 'Open Drawer 1'),\n",
        "                          (404519, 'Close Drawer 1'),\n",
        "                          (406511, 'Open Drawer 2'),\n",
        "                          (404511, 'Close Drawer 2'),\n",
        "                          (406508, 'Open Drawer 3'),\n",
        "                          (404508, 'Close Drawer 3'),\n",
        "                          (408512, 'Clean Table'),\n",
        "                          (407521, 'Drink from Cup'),\n",
        "                          (405506, 'Toggle Switch')]\n",
        "        self.drop_activities = []\n",
        "        self.train_keys   = [11,12,13,14,15,16,\n",
        "                             21,22,23,      26,\n",
        "                             31,32,33,      36,\n",
        "                             41,42,43,44,45,46]\n",
        "        # 'S1-ADL1.dat', 'S1-ADL2.dat', 'S1-ADL3.dat', 'S1-ADL4.dat',  'S1-ADL5.dat', 'S1-Drill.dat', # subject 1\n",
        "        # 'S2-ADL1.dat', 'S2-ADL2.dat', 'S2-ADL3.dat',                                'S2-Drill.dat', # subject 2\n",
        "        # 'S3-ADL1.dat', 'S3-ADL2.dat', 'S3-ADL3.dat',                                'S3-Drill.dat'  # subject 3\n",
        "        # 'S4-ADL1.dat', 'S4-ADL2.dat', 'S4-ADL3.dat', 'S4-ADL4.dat'   'S4-ADL5.dat', 'S4-Drill.dat'] # subject 4\n",
        "        self.vali_keys    = [ ]\n",
        "        # 'S2-ADL4.dat', 'S2-ADL5.dat','S3-ADL4.dat', 'S3-ADL5.dat'\n",
        "        self.test_keys    = [24,25,34,35]\n",
        "\n",
        "        self.LOCV_keys = [[1],[2],[3],[4]]\n",
        "        self.all_keys = [1,2,3,4]\n",
        "        self.sub_ids_of_each_sub = {}\n",
        "\n",
        "        self.file_encoding = {'S1-ADL1.dat':11, 'S1-ADL2.dat':12, 'S1-ADL3.dat':13, 'S1-ADL4.dat':14, 'S1-ADL5.dat':15, 'S1-Drill.dat':16,\n",
        "                              'S2-ADL1.dat':21, 'S2-ADL2.dat':22, 'S2-ADL3.dat':23, 'S2-ADL4.dat':24, 'S2-ADL5.dat':25, 'S2-Drill.dat':26,\n",
        "                              'S3-ADL1.dat':31, 'S3-ADL2.dat':32, 'S3-ADL3.dat':33, 'S3-ADL4.dat':34, 'S3-ADL5.dat':35, 'S3-Drill.dat':36,\n",
        "                              'S4-ADL1.dat':41, 'S4-ADL2.dat':42, 'S4-ADL3.dat':43, 'S4-ADL4.dat':44, 'S4-ADL5.dat':45, 'S4-Drill.dat':46}\n",
        "\n",
        "        self.labelToId = {int(x[0]): i for i, x in enumerate(self.label_map)}\n",
        "        self.all_labels = list(range(len(self.label_map)))\n",
        "        self.drop_activities = [self.labelToId[i] for i in self.drop_activities]\n",
        "        self.no_drop_activites = [item for item in self.all_labels if item not in self.drop_activities]\n",
        "        super().__init__()\n",
        "\n",
        "    def load_all_the_data(self, root_path):\n",
        "        file_list = os.listdir(root_path)\n",
        "        file_list = [file for file in file_list if file[-3:]==\"dat\"] # in total , it should be 24\n",
        "        assert len(file_list) == 24\n",
        "        df_dict = {}\n",
        "        for file in file_list:\n",
        "            sub_data = pd.read_table(os.path.join(root_path,file), header=None, sep='\\s+')\n",
        "            sub_data = sub_data.iloc[:,self.used_cols]\n",
        "            sub_data.columns = self.col_names\n",
        "            # TODO check missing labels?\n",
        "            sub_data = sub_data.interpolate(method='linear', limit_direction='both')\n",
        "            sub = int(file[1]) # subject number\n",
        "            sub_data['sub_id'] = self.file_encoding[file] # file index\n",
        "            sub_data[\"sub\"] = sub\n",
        "            if sub not in self.sub_ids_of_each_sub.keys():\n",
        "                self.sub_ids_of_each_sub[sub] = []\n",
        "            self.sub_ids_of_each_sub[sub].append(self.file_encoding[file])\n",
        "            df_dict[self.file_encoding[file]] = sub_data\n",
        "\n",
        "        # all data\n",
        "        df_all = pd.concat(df_dict)\n",
        "        df_all = df_all.set_index('sub_id')\n",
        "        # reorder the columns as sensor1, sensor2... sensorn, sub, activity_id\n",
        "        df_all = df_all[self.col_names[:-1]+[\"sub\"]+[\"activity_id\"]]\n",
        "        # label transformation\n",
        "        df_all[\"activity_id\"] = df_all[\"activity_id\"].map(self.labelToId)\n",
        "        data_y = df_all.iloc[:,-1]\n",
        "        data_x = df_all.iloc[:,:-1]\n",
        "        data_x = data_x.reset_index()\n",
        "        # sub_id, sensor1, sensor2... sensorn, sub,\n",
        "        return data_x, data_y\n",
        "\n",
        "\n",
        "# p=Opportunity_HAR_DATA()\n",
        "data_x, data_y = p.load_all_the_data(path)\n"
      ],
      "metadata": {
        "id": "gAy7dHuWjZ4J",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a82f2121-6513-4d78-b85e-a34062c7046a"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ----------------------- load all the data -------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(data_x)\n",
        "print(data_y)\n",
        "plt.plot(data_y)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 665
        },
        "id": "WhkCi3AUleWc",
        "outputId": "e5f2240d-ed1d-4ed4-ecb8-fb08464e10a8"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sub_id\n",
            "22    0\n",
            "22    0\n",
            "22    0\n",
            "22    0\n",
            "22    0\n",
            "     ..\n",
            "41    0\n",
            "41    0\n",
            "41    0\n",
            "41    0\n",
            "41    0\n",
            "Name: activity_id, Length: 869387, dtype: int64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with open('/content/OpportunityUCIDataset/dataset/column_names.txt') as f:\n",
        "#     # columns = [line.split()[1] for line in f.readlines()]\n",
        "#     print(f.readlines())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fYXxQ-Sgh8I",
        "outputId": "81b82892-8977-4945-b8f0-6a41f7857f09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Data columns:\\n', '\\n', 'Column: 1 MILLISEC\\n', 'Column: 2 Accelerometer RKN^ accX; value = round(original_value), unit = milli g\\n', 'Column: 3 Accelerometer RKN^ accY; value = round(original_value), unit = milli g\\n', 'Column: 4 Accelerometer RKN^ accZ; value = round(original_value), unit = milli g\\n', 'Column: 5 Accelerometer HIP accX; value = round(original_value), unit = milli g\\n', 'Column: 6 Accelerometer HIP accY; value = round(original_value), unit = milli g\\n', 'Column: 7 Accelerometer HIP accZ; value = round(original_value), unit = milli g\\n', 'Column: 8 Accelerometer LUA^ accX; value = round(original_value), unit = milli g\\n', 'Column: 9 Accelerometer LUA^ accY; value = round(original_value), unit = milli g\\n', 'Column: 10 Accelerometer LUA^ accZ; value = round(original_value), unit = milli g\\n', 'Column: 11 Accelerometer RUA_ accX; value = round(original_value), unit = milli g\\n', 'Column: 12 Accelerometer RUA_ accY; value = round(original_value), unit = milli g\\n', 'Column: 13 Accelerometer RUA_ accZ; value = round(original_value), unit = milli g\\n', 'Column: 14 Accelerometer LH accX; value = round(original_value), unit = milli g\\n', 'Column: 15 Accelerometer LH accY; value = round(original_value), unit = milli g\\n', 'Column: 16 Accelerometer LH accZ; value = round(original_value), unit = milli g\\n', 'Column: 17 Accelerometer BACK accX; value = round(original_value), unit = milli g\\n', 'Column: 18 Accelerometer BACK accY; value = round(original_value), unit = milli g\\n', 'Column: 19 Accelerometer BACK accZ; value = round(original_value), unit = milli g\\n', 'Column: 20 Accelerometer RKN_ accX; value = round(original_value), unit = milli g\\n', 'Column: 21 Accelerometer RKN_ accY; value = round(original_value), unit = milli g\\n', 'Column: 22 Accelerometer RKN_ accZ; value = round(original_value), unit = milli g\\n', 'Column: 23 Accelerometer RWR accX; value = round(original_value), unit = milli g\\n', 'Column: 24 Accelerometer RWR accY; value = round(original_value), unit = milli g\\n', 'Column: 25 Accelerometer RWR accZ; value = round(original_value), unit = milli g\\n', 'Column: 26 Accelerometer RUA^ accX; value = round(original_value), unit = milli g\\n', 'Column: 27 Accelerometer RUA^ accY; value = round(original_value), unit = milli g\\n', 'Column: 28 Accelerometer RUA^ accZ; value = round(original_value), unit = milli g\\n', 'Column: 29 Accelerometer LUA_ accX; value = round(original_value), unit = milli g\\n', 'Column: 30 Accelerometer LUA_ accY; value = round(original_value), unit = milli g\\n', 'Column: 31 Accelerometer LUA_ accZ; value = round(original_value), unit = milli g\\n', 'Column: 32 Accelerometer LWR accX; value = round(original_value), unit = milli g\\n', 'Column: 33 Accelerometer LWR accY; value = round(original_value), unit = milli g\\n', 'Column: 34 Accelerometer LWR accZ; value = round(original_value), unit = milli g\\n', 'Column: 35 Accelerometer RH accX; value = round(original_value), unit = milli g\\n', 'Column: 36 Accelerometer RH accY; value = round(original_value), unit = milli g\\n', 'Column: 37 Accelerometer RH accZ; value = round(original_value), unit = milli g\\n', 'Column: 38 InertialMeasurementUnit BACK accX; value = round(original_value / 9.8 * 1000), unit = milli g\\n', 'Column: 39 InertialMeasurementUnit BACK accY; value = round(original_value / 9.8 * 1000), unit = milli g\\n', 'Column: 40 InertialMeasurementUnit BACK accZ; value = round(original_value / 9.8 * 1000), unit = milli g\\n', 'Column: 41 InertialMeasurementUnit BACK gyroX; value = round(original_value * 1000), unit = unknown\\n', 'Column: 42 InertialMeasurementUnit BACK gyroY; value = round(original_value * 1000), unit = unknown\\n', 'Column: 43 InertialMeasurementUnit BACK gyroZ; value = round(original_value * 1000), unit = unknown\\n', 'Column: 44 InertialMeasurementUnit BACK magneticX; value = round(original_value * 1000), unit = unknown\\n', 'Column: 45 InertialMeasurementUnit BACK magneticY; value = round(original_value * 1000), unit = unknown\\n', 'Column: 46 InertialMeasurementUnit BACK magneticZ; value = round(original_value * 1000), unit = unknown\\n', 'Column: 47 InertialMeasurementUnit BACK Quaternion1; value = round(original_value * 1000), unit = none\\n', 'Column: 48 InertialMeasurementUnit BACK Quaternion2; value = round(original_value * 1000), unit = none\\n', 'Column: 49 InertialMeasurementUnit BACK Quaternion3; value = round(original_value * 1000), unit = none\\n', 'Column: 50 InertialMeasurementUnit BACK Quaternion4; value = round(original_value * 1000), unit = none\\n', 'Column: 51 InertialMeasurementUnit RUA accX; value = round(original_value / 9.8 * 1000), unit = milli g\\n', 'Column: 52 InertialMeasurementUnit RUA accY; value = round(original_value / 9.8 * 1000), unit = milli g\\n', 'Column: 53 InertialMeasurementUnit RUA accZ; value = round(original_value / 9.8 * 1000), unit = milli g\\n', 'Column: 54 InertialMeasurementUnit RUA gyroX; value = round(original_value * 1000), unit = unknown\\n', 'Column: 55 InertialMeasurementUnit RUA gyroY; value = round(original_value * 1000), unit = unknown\\n', 'Column: 56 InertialMeasurementUnit RUA gyroZ; value = round(original_value * 1000), unit = unknown\\n', 'Column: 57 InertialMeasurementUnit RUA magneticX; value = round(original_value * 1000), unit = unknown\\n', 'Column: 58 InertialMeasurementUnit RUA magneticY; value = round(original_value * 1000), unit = unknown\\n', 'Column: 59 InertialMeasurementUnit RUA magneticZ; value = round(original_value * 1000), unit = unknown\\n', 'Column: 60 InertialMeasurementUnit RUA Quaternion1; value = round(original_value * 1000), unit = none\\n', 'Column: 61 InertialMeasurementUnit RUA Quaternion2; value = round(original_value * 1000), unit = none\\n', 'Column: 62 InertialMeasurementUnit RUA Quaternion3; value = round(original_value * 1000), unit = none\\n', 'Column: 63 InertialMeasurementUnit RUA Quaternion4; value = round(original_value * 1000), unit = none\\n', 'Column: 64 InertialMeasurementUnit RLA accX; value = round(original_value / 9.8 * 1000), unit = milli g\\n', 'Column: 65 InertialMeasurementUnit RLA accY; value = round(original_value / 9.8 * 1000), unit = milli g\\n', 'Column: 66 InertialMeasurementUnit RLA accZ; value = round(original_value / 9.8 * 1000), unit = milli g\\n', 'Column: 67 InertialMeasurementUnit RLA gyroX; value = round(original_value * 1000), unit = unknown\\n', 'Column: 68 InertialMeasurementUnit RLA gyroY; value = round(original_value * 1000), unit = unknown\\n', 'Column: 69 InertialMeasurementUnit RLA gyroZ; value = round(original_value * 1000), unit = unknown\\n', 'Column: 70 InertialMeasurementUnit RLA magneticX; value = round(original_value * 1000), unit = unknown\\n', 'Column: 71 InertialMeasurementUnit RLA magneticY; value = round(original_value * 1000), unit = unknown\\n', 'Column: 72 InertialMeasurementUnit RLA magneticZ; value = round(original_value * 1000), unit = unknown\\n', 'Column: 73 InertialMeasurementUnit RLA Quaternion1; value = round(original_value * 1000), unit = none\\n', 'Column: 74 InertialMeasurementUnit RLA Quaternion2; value = round(original_value * 1000), unit = none\\n', 'Column: 75 InertialMeasurementUnit RLA Quaternion3; value = round(original_value * 1000), unit = none\\n', 'Column: 76 InertialMeasurementUnit RLA Quaternion4; value = round(original_value * 1000), unit = none\\n', 'Column: 77 InertialMeasurementUnit LUA accX; value = round(original_value / 9.8 * 1000), unit = milli g\\n', 'Column: 78 InertialMeasurementUnit LUA accY; value = round(original_value / 9.8 * 1000), unit = milli g\\n', 'Column: 79 InertialMeasurementUnit LUA accZ; value = round(original_value / 9.8 * 1000), unit = milli g\\n', 'Column: 80 InertialMeasurementUnit LUA gyroX; value = round(original_value * 1000), unit = unknown\\n', 'Column: 81 InertialMeasurementUnit LUA gyroY; value = round(original_value * 1000), unit = unknown\\n', 'Column: 82 InertialMeasurementUnit LUA gyroZ; value = round(original_value * 1000), unit = unknown\\n', 'Column: 83 InertialMeasurementUnit LUA magneticX; value = round(original_value * 1000), unit = unknown\\n', 'Column: 84 InertialMeasurementUnit LUA magneticY; value = round(original_value * 1000), unit = unknown\\n', 'Column: 85 InertialMeasurementUnit LUA magneticZ; value = round(original_value * 1000), unit = unknown\\n', 'Column: 86 InertialMeasurementUnit LUA Quaternion1; value = round(original_value * 1000), unit = none\\n', 'Column: 87 InertialMeasurementUnit LUA Quaternion2; value = round(original_value * 1000), unit = none\\n', 'Column: 88 InertialMeasurementUnit LUA Quaternion3; value = round(original_value * 1000), unit = none\\n', 'Column: 89 InertialMeasurementUnit LUA Quaternion4; value = round(original_value * 1000), unit = none\\n', 'Column: 90 InertialMeasurementUnit LLA accX; value = round(original_value / 9.8 * 1000), unit = milli g\\n', 'Column: 91 InertialMeasurementUnit LLA accY; value = round(original_value / 9.8 * 1000), unit = milli g\\n', 'Column: 92 InertialMeasurementUnit LLA accZ; value = round(original_value / 9.8 * 1000), unit = milli g\\n', 'Column: 93 InertialMeasurementUnit LLA gyroX; value = round(original_value * 1000), unit = unknown\\n', 'Column: 94 InertialMeasurementUnit LLA gyroY; value = round(original_value * 1000), unit = unknown\\n', 'Column: 95 InertialMeasurementUnit LLA gyroZ; value = round(original_value * 1000), unit = unknown\\n', 'Column: 96 InertialMeasurementUnit LLA magneticX; value = round(original_value * 1000), unit = unknown\\n', 'Column: 97 InertialMeasurementUnit LLA magneticY; value = round(original_value * 1000), unit = unknown\\n', 'Column: 98 InertialMeasurementUnit LLA magneticZ; value = round(original_value * 1000), unit = unknown\\n', 'Column: 99 InertialMeasurementUnit LLA Quaternion1; value = round(original_value * 1000), unit = none\\n', 'Column: 100 InertialMeasurementUnit LLA Quaternion2; value = round(original_value * 1000), unit = none\\n', 'Column: 101 InertialMeasurementUnit LLA Quaternion3; value = round(original_value * 1000), unit = none\\n', 'Column: 102 InertialMeasurementUnit LLA Quaternion4; value = round(original_value * 1000), unit = none\\n', 'Column: 103 InertialMeasurementUnit L-SHOE EuX; value = round(original_value), unit = degrees\\n', 'Column: 104 InertialMeasurementUnit L-SHOE EuY; value = round(original_value), unit = degrees\\n', 'Column: 105 InertialMeasurementUnit L-SHOE EuZ; value = round(original_value), unit = degrees\\n', 'Column: 106 InertialMeasurementUnit L-SHOE Nav_Ax; value = round(original_value / 9.8 * 1000), unit = milli g\\n', 'Column: 107 InertialMeasurementUnit L-SHOE Nav_Ay; value = round(original_value / 9.8 * 1000), unit = milli g\\n', 'Column: 108 InertialMeasurementUnit L-SHOE Nav_Az; value = round(original_value / 9.8 * 1000), unit = milli g\\n', 'Column: 109 InertialMeasurementUnit L-SHOE Body_Ax; value = round(original_value / 9.8 * 1000), unit = milli g\\n', 'Column: 110 InertialMeasurementUnit L-SHOE Body_Ay; value = round(original_value / 9.8 * 1000), unit = milli g\\n', 'Column: 111 InertialMeasurementUnit L-SHOE Body_Az; value = round(original_value / 9.8 * 1000), unit = milli g\\n', 'Column: 112 InertialMeasurementUnit L-SHOE AngVelBodyFrameX; value = round(original_value * 1000), unit = mm/s\\n', 'Column: 113 InertialMeasurementUnit L-SHOE AngVelBodyFrameY; value = round(original_value * 1000), unit = mm/s\\n', 'Column: 114 InertialMeasurementUnit L-SHOE AngVelBodyFrameZ; value = round(original_value * 1000), unit = mm/s\\n', 'Column: 115 InertialMeasurementUnit L-SHOE AngVelNavFrameX; value = round(original_value * 1000), unit = mm/s\\n', 'Column: 116 InertialMeasurementUnit L-SHOE AngVelNavFrameY; value = round(original_value * 1000), unit = mm/s\\n', 'Column: 117 InertialMeasurementUnit L-SHOE AngVelNavFrameZ; value = round(original_value * 1000), unit = mm/s\\n', 'Column: 118 InertialMeasurementUnit L-SHOE Compass; value = round(original_value), unit = degrees\\n', 'Column: 119 InertialMeasurementUnit R-SHOE EuX; value = round(original_value), unit = degrees\\n', 'Column: 120 InertialMeasurementUnit R-SHOE EuY; value = round(original_value), unit = degrees\\n', 'Column: 121 InertialMeasurementUnit R-SHOE EuZ; value = round(original_value), unit = degrees\\n', 'Column: 122 InertialMeasurementUnit R-SHOE Nav_Ax; value = round(original_value / 9.8 * 1000), unit = milli g\\n', 'Column: 123 InertialMeasurementUnit R-SHOE Nav_Ay; value = round(original_value / 9.8 * 1000), unit = milli g\\n', 'Column: 124 InertialMeasurementUnit R-SHOE Nav_Az; value = round(original_value / 9.8 * 1000), unit = milli g\\n', 'Column: 125 InertialMeasurementUnit R-SHOE Body_Ax; value = round(original_value / 9.8 * 1000), unit = milli g\\n', 'Column: 126 InertialMeasurementUnit R-SHOE Body_Ay; value = round(original_value / 9.8 * 1000), unit = milli g\\n', 'Column: 127 InertialMeasurementUnit R-SHOE Body_Az; value = round(original_value / 9.8 * 1000), unit = milli g\\n', 'Column: 128 InertialMeasurementUnit R-SHOE AngVelBodyFrameX; value = round(original_value * 1000), unit = mm/s\\n', 'Column: 129 InertialMeasurementUnit R-SHOE AngVelBodyFrameY; value = round(original_value * 1000), unit = mm/s\\n', 'Column: 130 InertialMeasurementUnit R-SHOE AngVelBodyFrameZ; value = round(original_value * 1000), unit = mm/s\\n', 'Column: 131 InertialMeasurementUnit R-SHOE AngVelNavFrameX; value = round(original_value * 1000), unit = mm/s\\n', 'Column: 132 InertialMeasurementUnit R-SHOE AngVelNavFrameY; value = round(original_value * 1000), unit = mm/s\\n', 'Column: 133 InertialMeasurementUnit R-SHOE AngVelNavFrameZ; value = round(original_value * 1000), unit = mm/s\\n', 'Column: 134 InertialMeasurementUnit R-SHOE Compass; value = round(original_value), unit = degrees\\n', 'Column: 135 Accelerometer CUP accX; value = round(original_value), unit = milli g\\n', 'Column: 136 Accelerometer CUP accX; value = round(original_value), unit = milli g\\n', 'Column: 137 Accelerometer CUP accX; value = round(original_value), unit = milli g\\n', 'Column: 138 Accelerometer CUP gyroX; value = round(original_value), unit = unknown\\n', 'Column: 139 Accelerometer CUP gyroY; value = round(original_value), unit = unknown\\n', 'Column: 140 Accelerometer SALAMI accX; value = round(original_value), unit = milli g\\n', 'Column: 141 Accelerometer SALAMI accX; value = round(original_value), unit = milli g\\n', 'Column: 142 Accelerometer SALAMI accX; value = round(original_value), unit = milli g\\n', 'Column: 143 Accelerometer SALAMI gyroX; value = round(original_value), unit = unknown\\n', 'Column: 144 Accelerometer SALAMI gyroY; value = round(original_value), unit = unknown\\n', 'Column: 145 Accelerometer WATER accX; value = round(original_value), unit = milli g\\n', 'Column: 146 Accelerometer WATER accX; value = round(original_value), unit = milli g\\n', 'Column: 147 Accelerometer WATER accX; value = round(original_value), unit = milli g\\n', 'Column: 148 Accelerometer WATER gyroX; value = round(original_value), unit = unknown\\n', 'Column: 149 Accelerometer WATER gyroY; value = round(original_value), unit = unknown\\n', 'Column: 150 Accelerometer CHEESE accX; value = round(original_value), unit = milli g\\n', 'Column: 151 Accelerometer CHEESE accX; value = round(original_value), unit = milli g\\n', 'Column: 152 Accelerometer CHEESE accX; value = round(original_value), unit = milli g\\n', 'Column: 153 Accelerometer CHEESE gyroX; value = round(original_value), unit = unknown\\n', 'Column: 154 Accelerometer CHEESE gyroY; value = round(original_value), unit = unknown\\n', 'Column: 155 Accelerometer BREAD accX; value = round(original_value), unit = milli g\\n', 'Column: 156 Accelerometer BREAD accX; value = round(original_value), unit = milli g\\n', 'Column: 157 Accelerometer BREAD accX; value = round(original_value), unit = milli g\\n', 'Column: 158 Accelerometer BREAD gyroX; value = round(original_value), unit = unknown\\n', 'Column: 159 Accelerometer BREAD gyroY; value = round(original_value), unit = unknown\\n', 'Column: 160 Accelerometer KNIFE1 accX; value = round(original_value), unit = milli g\\n', 'Column: 161 Accelerometer KNIFE1 accX; value = round(original_value), unit = milli g\\n', 'Column: 162 Accelerometer KNIFE1 accX; value = round(original_value), unit = milli g\\n', 'Column: 163 Accelerometer KNIFE1 gyroX; value = round(original_value), unit = unknown\\n', 'Column: 164 Accelerometer KNIFE1 gyroY; value = round(original_value), unit = unknown\\n', 'Column: 165 Accelerometer MILK accX; value = round(original_value), unit = milli g\\n', 'Column: 166 Accelerometer MILK accX; value = round(original_value), unit = milli g\\n', 'Column: 167 Accelerometer MILK accX; value = round(original_value), unit = milli g\\n', 'Column: 168 Accelerometer MILK gyroX; value = round(original_value), unit = unknown\\n', 'Column: 169 Accelerometer MILK gyroY; value = round(original_value), unit = unknown\\n', 'Column: 170 Accelerometer SPOON accX; value = round(original_value), unit = milli g\\n', 'Column: 171 Accelerometer SPOON accX; value = round(original_value), unit = milli g\\n', 'Column: 172 Accelerometer SPOON accX; value = round(original_value), unit = milli g\\n', 'Column: 173 Accelerometer SPOON gyroX; value = round(original_value), unit = unknown\\n', 'Column: 174 Accelerometer SPOON gyroY; value = round(original_value), unit = unknown\\n', 'Column: 175 Accelerometer SUGAR accX; value = round(original_value), unit = milli g\\n', 'Column: 176 Accelerometer SUGAR accX; value = round(original_value), unit = milli g\\n', 'Column: 177 Accelerometer SUGAR accX; value = round(original_value), unit = milli g\\n', 'Column: 178 Accelerometer SUGAR gyroX; value = round(original_value), unit = unknown\\n', 'Column: 179 Accelerometer SUGAR gyroY; value = round(original_value), unit = unknown\\n', 'Column: 180 Accelerometer KNIFE2 accX; value = round(original_value), unit = milli g\\n', 'Column: 181 Accelerometer KNIFE2 accX; value = round(original_value), unit = milli g\\n', 'Column: 182 Accelerometer KNIFE2 accX; value = round(original_value), unit = milli g\\n', 'Column: 183 Accelerometer KNIFE2 gyroX; value = round(original_value), unit = unknown\\n', 'Column: 184 Accelerometer KNIFE2 gyroY; value = round(original_value), unit = unknown\\n', 'Column: 185 Accelerometer PLATE accX; value = round(original_value), unit = milli g\\n', 'Column: 186 Accelerometer PLATE accX; value = round(original_value), unit = milli g\\n', 'Column: 187 Accelerometer PLATE accX; value = round(original_value), unit = milli g\\n', 'Column: 188 Accelerometer PLATE gyroX; value = round(original_value), unit = unknown\\n', 'Column: 189 Accelerometer PLATE gyroY; value = round(original_value), unit = unknown\\n', 'Column: 190 Accelerometer GLASS accX; value = round(original_value), unit = milli g\\n', 'Column: 191 Accelerometer GLASS accX; value = round(original_value), unit = milli g\\n', 'Column: 192 Accelerometer GLASS accX; value = round(original_value), unit = milli g\\n', 'Column: 193 Accelerometer GLASS gyroX; value = round(original_value), unit = unknown\\n', 'Column: 194 Accelerometer GLASS gyroY; value = round(original_value), unit = unknown\\n', 'Column: 195 REED SWITCH DISHWASHER S1; value = original_value, unit = logical (0/1)\\n', 'Column: 196 REED SWITCH FRIDGE S3; value = original_value, unit = logical (0/1)\\n', 'Column: 197 REED SWITCH FRIDGE S2; value = original_value, unit = logical (0/1)\\n', 'Column: 198 REED SWITCH FRIDGE S1; value = original_value, unit = logical (0/1)\\n', 'Column: 199 REED SWITCH MIDDLEDRAWER S1; value = original_value, unit = logical (0/1)\\n', 'Column: 200 REED SWITCH MIDDLEDRAWER S2; value = original_value, unit = logical (0/1)\\n', 'Column: 201 REED SWITCH MIDDLEDRAWER S3; value = original_value, unit = logical (0/1)\\n', 'Column: 202 REED SWITCH LOWERDRAWER S3; value = original_value, unit = logical (0/1)\\n', 'Column: 203 REED SWITCH LOWERDRAWER S2; value = original_value, unit = logical (0/1)\\n', 'Column: 204 REED SWITCH UPPERDRAWER; value = original_value, unit = logical (0/1)\\n', 'Column: 205 REED SWITCH DISHWASHER S3; value = original_value, unit = logical (0/1)\\n', 'Column: 206 REED SWITCH LOWERDRAWER S1; value = original_value, unit = logical (0/1)\\n', 'Column: 207 REED SWITCH DISHWASHER S2; value = original_value, unit = logical (0/1)\\n', 'Column: 208 Accelerometer DOOR1 accX; value = round(original_value), unit = milli g\\n', 'Column: 209 Accelerometer DOOR1 accY; value = round(original_value), unit = milli g\\n', 'Column: 210 Accelerometer DOOR1 accZ; value = round(original_value), unit = milli g\\n', 'Column: 211 Accelerometer LAZYCHAIR accX; value = round(original_value), unit = milli g\\n', 'Column: 212 Accelerometer LAZYCHAIR accY; value = round(original_value), unit = milli g\\n', 'Column: 213 Accelerometer LAZYCHAIR accZ; value = round(original_value), unit = milli g\\n', 'Column: 214 Accelerometer DOOR2 accX; value = round(original_value), unit = milli g\\n', 'Column: 215 Accelerometer DOOR2 accY; value = round(original_value), unit = milli g\\n', 'Column: 216 Accelerometer DOOR2 accZ; value = round(original_value), unit = milli g\\n', 'Column: 217 Accelerometer DISHWASHER accX; value = round(original_value), unit = milli g\\n', 'Column: 218 Accelerometer DISHWASHER accY; value = round(original_value), unit = milli g\\n', 'Column: 219 Accelerometer DISHWASHER accZ; value = round(original_value), unit = milli g\\n', 'Column: 220 Accelerometer UPPERDRAWER accX; value = round(original_value), unit = milli g\\n', 'Column: 221 Accelerometer UPPERDRAWER accY; value = round(original_value), unit = milli g\\n', 'Column: 222 Accelerometer UPPERDRAWER accZ; value = round(original_value), unit = milli g\\n', 'Column: 223 Accelerometer LOWERDRAWER accX; value = round(original_value), unit = milli g\\n', 'Column: 224 Accelerometer LOWERDRAWER accY; value = round(original_value), unit = milli g\\n', 'Column: 225 Accelerometer LOWERDRAWER accZ; value = round(original_value), unit = milli g\\n', 'Column: 226 Accelerometer MIDDLEDRAWER accX; value = round(original_value), unit = milli g\\n', 'Column: 227 Accelerometer MIDDLEDRAWER accY; value = round(original_value), unit = milli g\\n', 'Column: 228 Accelerometer MIDDLEDRAWER accZ; value = round(original_value), unit = milli g\\n', 'Column: 229 Accelerometer FRIDGE accX; value = round(original_value), unit = milli g\\n', 'Column: 230 Accelerometer FRIDGE accY; value = round(original_value), unit = milli g\\n', 'Column: 231 Accelerometer FRIDGE accZ; value = round(original_value), unit = milli g\\n', 'Column: 232 LOCATION TAG1 X; value = round(original_value), unit = millimetres\\n', 'Column: 233 LOCATION TAG1 Y; value = round(original_value), unit = millimetres\\n', 'Column: 234 LOCATION TAG1 Z; value = round(original_value), unit = millimetres\\n', 'Column: 235 LOCATION TAG2 X; value = round(original_value), unit = millimetres\\n', 'Column: 236 LOCATION TAG2 Y; value = round(original_value), unit = millimetres\\n', 'Column: 237 LOCATION TAG2 Z; value = round(original_value), unit = millimetres\\n', 'Column: 238 LOCATION TAG3 X; value = round(original_value), unit = millimetres\\n', 'Column: 239 LOCATION TAG3 Y; value = round(original_value), unit = millimetres\\n', 'Column: 240 LOCATION TAG3 Z; value = round(original_value), unit = millimetres\\n', 'Column: 241 LOCATION TAG4 X; value = round(original_value), unit = millimetres\\n', 'Column: 242 LOCATION TAG4 Y; value = round(original_value), unit = millimetres\\n', 'Column: 243 LOCATION TAG4 Z; value = round(original_value), unit = millimetres\\n', '\\n', 'Label columns: \\n', '\\n', 'Column: 244 Locomotion\\n', 'Column: 245 HL_Activity\\n', 'Column: 246 LL_Left_Arm\\n', 'Column: 247 LL_Left_Arm_Object\\n', 'Column: 248 LL_Right_Arm\\n', 'Column: 249 LL_Right_Arm_Object\\n', 'Column: 250 ML_Both_Arms']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(df)\n",
        "print(dataset)\n",
        "# print([len(df) for df in X_train])\n",
        "# for df in ans:\n",
        "#     print(df['activityID'].iloc[0], df['subject'].iloc[0], len(df))\n",
        "# X_train = [df.drop(['subject', 'activityID'], axis=1) for df in ans]\n",
        "\n",
        "\n",
        "# print(dataset)\n",
        "# print(X_train[1])\n",
        "# print(X_train[1].reset_index(drop=True))\n",
        "# print(X_train[0].interpolate(method='index', axis=0, limit_direction='both'))\n",
        "# print(X_train[0].interpolate(method='index', axis=0, limit_direction='both').isnull().sum())\n",
        "# print(X_train[0].isnull().sum())\n",
        "# print(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaujN3mXNuDy",
        "outputId": "79d57a7a-edfe-4469-ecd8-319418ea9ae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              0     1       2      3      4      5      6      7       8  \\\n",
            "0             0  30.0  1020.0  211.0 -176.0  977.0  320.0 -218.0   988.0   \n",
            "1            33  19.0  1023.0  257.0 -185.0  949.0  334.0 -220.0   994.0   \n",
            "2            67  16.0  1031.0  216.0 -196.0  974.0  315.0 -231.0  1003.0   \n",
            "3           100  14.0  1029.0  238.0 -186.0  975.0  324.0 -222.0   996.0   \n",
            "4           133  15.0  1031.0  183.0 -187.0  946.0  312.0 -224.0   989.0   \n",
            "...         ...   ...     ...    ...    ...    ...    ...    ...     ...   \n",
            "869382  1108922   NaN     NaN    NaN    NaN    NaN    NaN    NaN     NaN   \n",
            "869383  1108956   NaN     NaN    NaN    NaN    NaN    NaN    NaN     NaN   \n",
            "869384  1108989   NaN     NaN    NaN    NaN    NaN    NaN    NaN     NaN   \n",
            "869385  1109022   NaN     NaN    NaN    NaN    NaN    NaN    NaN     NaN   \n",
            "869386  1109056   NaN     NaN    NaN    NaN    NaN    NaN    NaN     NaN   \n",
            "\n",
            "           9  ...     241     242  243  244  245  246  247  248  249  \\\n",
            "0       -5.0  ...  2697.0  1514.0    0    0    0    0    0    0    0   \n",
            "1      -19.0  ...  2697.0  1513.0    0    0    0    0    0    0    0   \n",
            "2      -27.0  ...  2698.0  1512.0    0    0    0    0    0    0    0   \n",
            "3       -6.0  ...  2697.0  1510.0    0    0    0    0    0    0    0   \n",
            "4      -36.0  ...  2695.0  1509.0    0    0    0    0    0    0    0   \n",
            "...      ...  ...     ...     ...  ...  ...  ...  ...  ...  ...  ...   \n",
            "869382   NaN  ...     NaN     NaN    0    0    0    0    0    0    0   \n",
            "869383   NaN  ...     NaN     NaN    0    0    0    0    0    0    0   \n",
            "869384   NaN  ...     NaN     NaN    0    0    0    0    0    0    0   \n",
            "869385   NaN  ...     NaN     NaN    0    0    0    0    0    0    0   \n",
            "869386   NaN  ...     NaN     NaN    0    0    0    0    0    0    0   \n",
            "\n",
            "            subject  \n",
            "0       S1-ADL5.dat  \n",
            "1       S1-ADL5.dat  \n",
            "2       S1-ADL5.dat  \n",
            "3       S1-ADL5.dat  \n",
            "4       S1-ADL5.dat  \n",
            "...             ...  \n",
            "869382  S1-ADL3.dat  \n",
            "869383  S1-ADL3.dat  \n",
            "869384  S1-ADL3.dat  \n",
            "869385  S1-ADL3.dat  \n",
            "869386  S1-ADL3.dat  \n",
            "\n",
            "[869387 rows x 251 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Navidfoumani/Series2Vec opportunity loader\n",
        "# https://github.com/Navidfoumani/Series2Vec/blob/main/Dataset/Benchmarks/Opportunity/Opportunity_Loader.py\n",
        "import os\n",
        "import logging\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "import csv\n",
        "import numpy as np\n",
        "from pandas import Series\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "# Main function for downloading and processing the Opportunity Datasets\n",
        "# Returns the train and test sets\n",
        "\n",
        "# Hardcoded number of sensor channels employed in the OPPORTUNITY challenge\n",
        "NB_SENSOR_CHANNELS = 113\n",
        "test_files = ['S2-ADL4.dat', 'S2-ADL5.dat', 'S3-ADL4.dat', 'S3-ADL5.dat']\n",
        "NORM_MAX_THRESHOLDS = [3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,\n",
        "                       3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,\n",
        "                       3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,\n",
        "                       3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,   3000,\n",
        "                       3000,   3000,   3000,   10000,  10000,  10000,  1500,   1500,   1500,\n",
        "                       3000,   3000,   3000,   10000,  10000,  10000,  1500,   1500,   1500,\n",
        "                       3000,   3000,   3000,   10000,  10000,  10000,  1500,   1500,   1500,\n",
        "                       3000,   3000,   3000,   10000,  10000,  10000,  1500,   1500,   1500,\n",
        "                       3000,   3000,   3000,   10000,  10000,  10000,  1500,   1500,   1500,\n",
        "                       250,    25,     200,    5000,   5000,   5000,   5000,   5000,   5000,\n",
        "                       10000,  10000,  10000,  10000,  10000,  10000,  250,    250,    25,\n",
        "                       200,    5000,   5000,   5000,   5000,   5000,   5000,   10000,  10000,\n",
        "                       10000,  10000,  10000,  10000,  250, ]\n",
        "\n",
        "NORM_MIN_THRESHOLDS = [-3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,\n",
        "                       -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,\n",
        "                       -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,\n",
        "                       -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,  -3000,\n",
        "                       -3000,  -3000,  -3000,  -10000, -10000, -10000, -1000,  -1000,  -1000,\n",
        "                       -3000,  -3000,  -3000,  -10000, -10000, -10000, -1000,  -1000,  -1000,\n",
        "                       -3000,  -3000,  -3000,  -10000, -10000, -10000, -1000,  -1000,  -1000,\n",
        "                       -3000,  -3000,  -3000,  -10000, -10000, -10000, -1000,  -1000,  -1000,\n",
        "                       -3000,  -3000,  -3000,  -10000, -10000, -10000, -1000,  -1000,  -1000,\n",
        "                       -250,   -100,   -200,   -5000,  -5000,  -5000,  -5000,  -5000,  -5000,\n",
        "                       -10000, -10000, -10000, -10000, -10000, -10000, -250,   -250,   -100,\n",
        "                       -200,   -5000,  -5000,  -5000,  -5000,  -5000,  -5000,  -10000, -10000,\n",
        "                       -10000, -10000, -10000, -10000, -250, ]\n",
        "\n",
        "\n",
        "def Opportunity(window_size, step):\n",
        "    # Build data\n",
        "    Data = {}\n",
        "    # Get the current directory path\n",
        "    current_path = os.getcwd()\n",
        "    data_path = os.path.join(current_path, 'Datasets/Opportunity/Opportunity.npy')\n",
        "    if os.path.exists(data_path):\n",
        "        logger.info(\"Loading preprocessed Opportunity data ...\")\n",
        "\n",
        "        Data_npy = np.load(data_path, allow_pickle=True)\n",
        "        Data['train_data'] = Data_npy.item().get('train_data')\n",
        "        Data['train_label'] = Data_npy.item().get('train_label')\n",
        "        Data['test_data'] = Data_npy.item().get('test_data')\n",
        "        Data['test_label'] = Data_npy.item().get('test_label')\n",
        "\n",
        "        logger.info(\"{} samples will be used for training\".format(len(Data['train_label'])))\n",
        "        logger.info(\"{} samples will be used for testing\".format(len(Data['test_label'])))\n",
        "\n",
        "    else:\n",
        "        Downloader(current_path)\n",
        "        train_x, test_x, train_y, test_y = generate_data(current_path, label=\"gestures\")\n",
        "        X_train, y_train = Windowed_majority_labeling(train_x, np.int64(train_y), window_size, step)\n",
        "        X_test, y_test = Windowed_majority_labeling(test_x, np.int64(test_y), window_size, step)\n",
        "\n",
        "        logger.info(\"{} samples will be used for training\".format(len(y_train)))\n",
        "        logger.info(\"{} samples will be used for testing\".format(len(y_test)))\n",
        "\n",
        "        Data['train_data'] = X_train\n",
        "        Data['train_label'] = y_train\n",
        "        Data['test_data'] = X_test\n",
        "        Data['test_label'] = y_test\n",
        "\n",
        "        if not os.path.exists(current_path +'/Datasets/Opportunity/'):\n",
        "            os.makedirs(current_path + '/Datasets/Opportunity/')\n",
        "        np.save(current_path + '/Datasets/Opportunity/Opportunity.npy', Data, allow_pickle=True)\n",
        "    return Data\n",
        "\n",
        "\n",
        "def Downloader(current_path):\n",
        "    # Define the path to check\n",
        "    path_to_check = os.path.join(current_path, 'Opportunity/OpportunityUCIDataset')\n",
        "    # Check if the path exists\n",
        "    if not os.path.exists(path_to_check):\n",
        "        # URL to download the PAMAP2 from\n",
        "        file_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00226/OpportunityUCIDataset.zip'\n",
        "        # Send a GET request to download the file\n",
        "        response = requests.get(file_url, stream=True)\n",
        "\n",
        "        # Check if the request was successful\n",
        "        if response.status_code == 200:\n",
        "            # Create the directory if it doesn't exist\n",
        "            os.makedirs(path_to_check, exist_ok=True)\n",
        "\n",
        "            # Save the downloaded file\n",
        "            file_path = os.path.join(path_to_check, 'PAMAP2_Dataset.zip')\n",
        "            with open(file_path, 'wb') as file:\n",
        "                # Track the progress of the download\n",
        "                total_size = int(response.headers.get('content-length', 0))\n",
        "                block_size = 1024 * 1024 * 100  # 1KB\n",
        "                downloaded_size = 0\n",
        "\n",
        "                for data in response.iter_content(block_size):\n",
        "                    file.write(data)\n",
        "                    downloaded_size += len(data)\n",
        "\n",
        "                    # Calculate the download progress percentage\n",
        "                    progress = (downloaded_size / total_size) * 100\n",
        "\n",
        "                    # Print the progress message\n",
        "                    print(f'OpportunityUCIDataset Download in progress: {progress:.2f}%')\n",
        "\n",
        "            # Extract the contents of the zip file\n",
        "            with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(path_to_check)\n",
        "\n",
        "            # Remove the downloaded zip file\n",
        "            os.remove(file_path)\n",
        "\n",
        "            print('OpportunityUCIDataset Datasets downloaded and extracted successfully.')\n",
        "        else:\n",
        "            print('Failed to download the OpportunityUCIDataset please update the file_url')\n",
        "    else:\n",
        "        print('OpportunityUCIDataset Datasets Raw file already exists.')\n",
        "    return\n",
        "\n",
        "\n",
        "def read_file(current_path):\n",
        "    data = []\n",
        "    labels = []\n",
        "    label_map = [\n",
        "        (0, 'Other'),\n",
        "        (406516, 'Open Door 1'),\n",
        "        (406517, 'Open Door 2'),\n",
        "        (404516, 'Close Door 1'),\n",
        "        (404517, 'Close Door 2'),\n",
        "        (406520, 'Open Fridge'),\n",
        "        (404520, 'Close Fridge'),\n",
        "        (406505, 'Open Dishwasher'),\n",
        "        (404505, 'Close Dishwasher'),\n",
        "        (406519, 'Open Drawer 1'),\n",
        "        (404519, 'Close Drawer 1'),\n",
        "        (406511, 'Open Drawer 2'),\n",
        "        (404511, 'Close Drawer 2'),\n",
        "        (406508, 'Open Drawer 3'),\n",
        "        (404508, 'Close Drawer 3'),\n",
        "        (408512, 'Clean Table'),\n",
        "        (407521, 'Drink from Cup'),\n",
        "        (405506, 'Toggle Switch')\n",
        "    ]\n",
        "    labelToId = {str(x[0]): i for i, x in enumerate(label_map)}\n",
        "    folder_path = os.path.join(current_path, 'Opportunity/OpportunityUCIDataset/OpportunityUCIDataset/dataset')\n",
        "    cols = [38, 39, 40, 41, 42, 43, 44, 45, 46, 51, 52, 53, 54, 55, 56, 57, 58, 59, 64, 65, 66, 67, 68, 69,\n",
        "            70, 71, 72, 77, 78, 79, 80, 81, 82, 83, 84, 85, 90, 91, 92, 93, 94, 95, 96, 97, 98, 103, 104, 105,\n",
        "            106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124,\n",
        "            125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 250]\n",
        "    # Iterate over files in the folder\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.dat'):\n",
        "            # Read the file\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            with open(file_path, 'r') as f:\n",
        "                reader = csv.reader(f, delimiter=' ')\n",
        "                for line in reader:\n",
        "                    elem = []\n",
        "                    for ind in cols:\n",
        "                        elem.append(line[ind-1])\n",
        "                    if sum([x == 'NaN' for x in elem]) < 5:\n",
        "                        data.append([float(x) / 1000 for x in elem[:-1]])\n",
        "                        labels.append(labelToId[elem[-1]])\n",
        "\n",
        "    return data, labels\n",
        "\n",
        "\n",
        "def select_columns_opp(data):\n",
        "    \"\"\"Selection of the 113 columns employed in the OPPORTUNITY challenge\n",
        "\n",
        "    :param data: numpy integer matrix\n",
        "        Sensor data (all features)\n",
        "    :return: numpy integer matrix\n",
        "        Selection of features\n",
        "    \"\"\"\n",
        "\n",
        "    #                     included-excluded\n",
        "    features_delete = np.arange(46, 50)\n",
        "    features_delete = np.concatenate([features_delete, np.arange(59, 63)])\n",
        "    features_delete = np.concatenate([features_delete, np.arange(72, 76)])\n",
        "    features_delete = np.concatenate([features_delete, np.arange(85, 89)])\n",
        "    features_delete = np.concatenate([features_delete, np.arange(98, 102)])\n",
        "    features_delete = np.concatenate([features_delete, np.arange(134, 243)])\n",
        "    features_delete = np.concatenate([features_delete, np.arange(244, 249)])\n",
        "    return np.delete(data, features_delete, 1)\n",
        "\n",
        "\n",
        "def normalize(data, max_list, min_list):\n",
        "    \"\"\"Normalizes all sensor channels\n",
        "\n",
        "    :param data: numpy integer matrix\n",
        "        Sensor data\n",
        "    :param max_list: numpy integer array\n",
        "        Array containing maximums values for every one of the 113 sensor channels\n",
        "    :param min_list: numpy integer array\n",
        "        Array containing minimum values for every one of the 113 sensor channels\n",
        "    :return:\n",
        "        Normalized sensor data\n",
        "    \"\"\"\n",
        "    max_list, min_list = np.array(max_list), np.array(min_list)\n",
        "    diffs = max_list - min_list\n",
        "    for i in np.arange(data.shape[1]):\n",
        "        data[:, i] = (data[:, i]-min_list[i])/diffs[i]\n",
        "    #     Checking the boundaries\n",
        "    data[data > 1] = 0.99\n",
        "    data[data < 0] = 0.00\n",
        "    return data\n",
        "\n",
        "\n",
        "def divide_x_y(data, label):\n",
        "    \"\"\"Segments each sample into features and label\n",
        "\n",
        "    :param data: numpy integer matrix\n",
        "        Sensor data\n",
        "    :param label: string, ['gestures' (default), 'locomotion']\n",
        "        Type of activities to be recognized\n",
        "    :return: numpy integer matrix, numpy integer array\n",
        "        Features encapsulated into a matrix and labels as an array\n",
        "    \"\"\"\n",
        "\n",
        "    data_x = data[:, 1:114]\n",
        "    if label not in ['locomotion', 'gestures']:\n",
        "            raise RuntimeError(\"Invalid label: '%s'\" % label)\n",
        "    if label == 'locomotion':\n",
        "        data_y = data[:, 114]  # Locomotion label\n",
        "    elif label == 'gestures':\n",
        "        data_y = data[:, 115]  # Gestures label\n",
        "\n",
        "    return data_x, data_y\n",
        "\n",
        "\n",
        "def adjust_idx_labels(data_y, label):\n",
        "    \"\"\"Transforms original labels into the range [0, nb_labels-1]\n",
        "\n",
        "    :param data_y: numpy integer array\n",
        "        Sensor labels\n",
        "    :param label: string, ['gestures' (default), 'locomotion']\n",
        "        Type of activities to be recognized\n",
        "    :return: numpy integer array\n",
        "        Modified sensor labels\n",
        "    \"\"\"\n",
        "\n",
        "    if label == 'locomotion':  # Labels for locomotion are adjusted\n",
        "        data_y[data_y == 4] = 3\n",
        "        data_y[data_y == 5] = 4\n",
        "    elif label == 'gestures':  # Labels for gestures are adjusted\n",
        "        data_y[data_y == 406516] = 1\n",
        "        data_y[data_y == 406517] = 2\n",
        "        data_y[data_y == 404516] = 3\n",
        "        data_y[data_y == 404517] = 4\n",
        "        data_y[data_y == 406520] = 5\n",
        "        data_y[data_y == 404520] = 6\n",
        "        data_y[data_y == 406505] = 7\n",
        "        data_y[data_y == 404505] = 8\n",
        "        data_y[data_y == 406519] = 9\n",
        "        data_y[data_y == 404519] = 10\n",
        "        data_y[data_y == 406511] = 11\n",
        "        data_y[data_y == 404511] = 12\n",
        "        data_y[data_y == 406508] = 13\n",
        "        data_y[data_y == 404508] = 14\n",
        "        data_y[data_y == 408512] = 15\n",
        "        data_y[data_y == 407521] = 16\n",
        "        data_y[data_y == 405506] = 17\n",
        "    return data_y\n",
        "\n",
        "\n",
        "def process_dataset_file(data, label):\n",
        "    \"\"\"Function defined as a pipeline to process individual OPPORTUNITY files\n",
        "\n",
        "    :param data: numpy integer matrix\n",
        "        Matrix containing data samples (rows) for every sensor channel (column)\n",
        "    :param label: string, ['gestures' (default), 'locomotion']\n",
        "        Type of activities to be recognized\n",
        "    :return: numpy integer matrix, numy integer array\n",
        "        Processed sensor data, segmented into features (x) and labels (y)\n",
        "    \"\"\"\n",
        "\n",
        "    # Select correct columns\n",
        "    data = select_columns_opp(data)\n",
        "    # Colums are segmentd into features and labels\n",
        "    data_x, data_y = divide_x_y(data, label)\n",
        "    data_y = adjust_idx_labels(data_y, label)\n",
        "    data_y = data_y.astype(int)\n",
        "\n",
        "    # Perform linear interpolation\n",
        "    data_x = np.array([Series(i).interpolate() for i in data_x.T]).T\n",
        "\n",
        "    # Remaining missing data are converted to zero\n",
        "    data_x[np.isnan(data_x)] = 0\n",
        "\n",
        "    # All sensor channels are normalized\n",
        "    data_x = normalize(data_x, NORM_MAX_THRESHOLDS, NORM_MIN_THRESHOLDS)\n",
        "\n",
        "    return data_x, data_y\n",
        "\n",
        "\n",
        "def generate_data(current_path, label):\n",
        "    \"\"\"Function to read the OPPORTUNITY challenge raw data and process all sensor channels\n",
        "\n",
        "    :param current_path: string\n",
        "        Path with original OPPORTUNITY zip file\n",
        "    :param target_filename: string\n",
        "        Processed file\n",
        "    :param label: string, ['gestures' (default), 'locomotion']\n",
        "        Type of activities to be recognized. The OPPORTUNITY dataset includes several annotations to perform\n",
        "        recognition modes of locomotion/postures and recognition of sporadic gestures.\n",
        "    \"\"\"\n",
        "    train_x = np.empty((0, NB_SENSOR_CHANNELS))\n",
        "    train_y = np.empty(0)\n",
        "\n",
        "    test_x = np.empty((0, NB_SENSOR_CHANNELS))\n",
        "    test_y = np.empty(0)\n",
        "\n",
        "    folder_path = os.path.join(current_path, 'Opportunity/OpportunityUCIDataset/OpportunityUCIDataset/dataset')\n",
        "\n",
        "    # Iterate over files in the folder\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.dat'):\n",
        "            # Read the file\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            data = np.loadtxt(file_path)\n",
        "            x, y = process_dataset_file(data, label)\n",
        "            if filename in test_files:\n",
        "                test_x = np.vstack((test_x, x))\n",
        "                test_y = np.concatenate([test_y, y])\n",
        "            else:\n",
        "                train_x = np.vstack((train_x, x))\n",
        "                train_y = np.concatenate([train_y, y])\n",
        "\n",
        "    return train_x, test_x, train_y, test_y\n",
        "\n",
        "\n",
        "def Windowed_majority_labeling(values, labels, window_size, step):\n",
        "\n",
        "    # Initialize empty lists to store windowed samples and labels\n",
        "    windowed_samples = []\n",
        "    window_labels = []\n",
        "\n",
        "    for i in range(0, len(values) - window_size + 1, step):\n",
        "        # Extract the windowed sample\n",
        "        windowed_sample = values[i:i + window_size]\n",
        "\n",
        "        # Assign the majority label to the window\n",
        "        window_label = np.argmax(np.bincount(labels[i:i + window_size]))\n",
        "\n",
        "        # Append the windowed sample and label to the lists\n",
        "        windowed_samples.append(list(windowed_sample))\n",
        "        window_labels.append(window_label)\n",
        "\n",
        "    # Convert the windowed samples and labels to numpy arrays\n",
        "    windowed_samples = np.transpose(np.array(windowed_samples), (0, 2, 1))\n",
        "    window_labels = np.array(window_labels)\n",
        "    return windowed_samples, window_labels\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jnVtofwDCUmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title teco-kit opportunity loader\n",
        "# https://github.com/teco-kit/timeseries-datasets/blob/main/dataloaders/dataloader_opportunity_har.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# from dataloaders.dataloader_base import BASE_DATA\n",
        "# TODO the cols ! name\n",
        "# ========================================      Opportunity_HAR_DATA         =============================\n",
        "# class Opportunity_HAR_DATA(BASE_DATA):\n",
        "class Opportunity_HAR_DATA():\n",
        "    \"\"\"\n",
        "    OPPORTUNITY Dataset for Human Activity Recognition from Wearable, Object, and Ambient Sensors\n",
        "\n",
        "    Brief Description of the Dataset:\n",
        "    ---------------------------------\n",
        "    Each .dat file contains a matrix of data in text format.\n",
        "    Each line contains the sensor data sampled at a given time (sample rate: 30Hz).\n",
        "    For more detail . please reffer to the docomentation.html\n",
        "    \"\"\"\n",
        "    def __init__(self, args):\n",
        "\n",
        "        \"\"\"\n",
        "        root_path : Root directory of the data set\n",
        "        difference (bool) : Whether to calculate the first order derivative of the original data\n",
        "        datanorm_type (str) : Methods of data normalization: \"standardization\", \"minmax\" , \"per_sample_std\", \"per_sample_minmax\"\n",
        "\n",
        "        spectrogram (bool): Whether to convert raw data into frequency representations\n",
        "            scales : Depends on the sampling frequency of the data （sample rate: 30Hz)）\n",
        "            wavelet : Methods of wavelet transformation\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # In this documents in doc/documentation.html, all columns definition coulde be found   (or in the column_names)\n",
        "        # the sensors between 134 and 248 are amounted on devices, so they will not to be considered\n",
        "        # Specifically, the following columns were used for the challenge:\n",
        "        # =============================================================\n",
        "        # 1-37, 38-46, 51-59, 64-72, 77-85, 90-98, 103-134, 244, 250.\n",
        "        # 0 milisconds\n",
        "        self.used_cols = [#1,  2,   3, # Accelerometer RKN^\n",
        "                          #4,  5,   6, # Accelerometer HIP\n",
        "                          #7,  8,   9, # Accelerometer LUA^\n",
        "                          #10, 11,  12, # Accelerometer RUA_\n",
        "                          #13, 14,  15, # Accelerometer LH\n",
        "                          #16, 17,  18, # Accelerometer BACK\n",
        "                          #19, 20,  21, # Accelerometer RKN_\n",
        "                          #22, 23,  24, # Accelerometer RWR\n",
        "                          #25, 26,  27, # Accelerometer RUA^\n",
        "                          #28, 29,  30, # Accelerometer LUA_\n",
        "                          #31, 32,  33, # Accelerometer LWR\n",
        "                          #34, 35,  36, # Accelerometer RH\n",
        "                          37, 38,  39, 40, 41, 42, 43, 44, 45, # InertialMeasurementUnit BACK\n",
        "                          50, 51,  52, 53, 54, 55, 56, 57, 58, # InertialMeasurementUnit RUA\n",
        "                          63, 64,  65, 66, 67, 68, 69, 70, 71, # InertialMeasurementUnit RLA\n",
        "                          76, 77,  78, 79, 80, 81, 82, 83, 84, # InertialMeasurementUnit LUA\n",
        "                          89, 90,  91, 92, 93, 94, 95, 96, 97,  # InertialMeasurementUnit LLA\n",
        "                          102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, # InertialMeasurementUnit L-SHOE\n",
        "                          118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, # InertialMeasurementUnit R-SHOE\n",
        "                          249  # Label\n",
        "                         ]\n",
        "\n",
        "        col_names         = [\"dim_{}\".format(i) for i in range(len(self.used_cols)-1)]\n",
        "        self.col_names    =  col_names + [\"activity_id\"]\n",
        "\n",
        "        self.label_map = [(0,      'Other'),\n",
        "                          (406516, 'Open Door 1'),\n",
        "                          (406517, 'Open Door 2'),\n",
        "                          (404516, 'Close Door 1'),\n",
        "                          (404517, 'Close Door 2'),\n",
        "                          (406520, 'Open Fridge'),\n",
        "                          (404520, 'Close Fridge'),\n",
        "                          (406505, 'Open Dishwasher'),\n",
        "                          (404505, 'Close Dishwasher'),\n",
        "                          (406519, 'Open Drawer 1'),\n",
        "                          (404519, 'Close Drawer 1'),\n",
        "                          (406511, 'Open Drawer 2'),\n",
        "                          (404511, 'Close Drawer 2'),\n",
        "                          (406508, 'Open Drawer 3'),\n",
        "                          (404508, 'Close Drawer 3'),\n",
        "                          (408512, 'Clean Table'),\n",
        "                          (407521, 'Drink from Cup'),\n",
        "                          (405506, 'Toggle Switch')]\n",
        "\n",
        "\n",
        "        self.drop_activities = []\n",
        "\n",
        "        self.train_keys   = [11,12,13,14,15,16,\n",
        "                             21,22,23,      26,\n",
        "                             31,32,33,      36,\n",
        "                             41,42,43,44,45,46]\n",
        "        # 'S1-ADL1.dat', 'S1-ADL2.dat', 'S1-ADL3.dat', 'S1-ADL4.dat',  'S1-ADL5.dat', 'S1-Drill.dat', # subject 1\n",
        "        # 'S2-ADL1.dat', 'S2-ADL2.dat', 'S2-ADL3.dat',                                'S2-Drill.dat', # subject 2\n",
        "        # 'S3-ADL1.dat', 'S3-ADL2.dat', 'S3-ADL3.dat',                                'S3-Drill.dat'  # subject 3\n",
        "        # 'S4-ADL1.dat', 'S4-ADL2.dat', 'S4-ADL3.dat', 'S4-ADL4.dat'   'S4-ADL5.dat', 'S4-Drill.dat'] # subject 4\n",
        "        self.vali_keys    = [ ]\n",
        "        # 'S2-ADL4.dat', 'S2-ADL5.dat','S3-ADL4.dat', 'S3-ADL5.dat'\n",
        "        self.test_keys    = [24,25,34,35]\n",
        "\n",
        "        self.exp_mode     = args.exp_mode\n",
        "        if self.exp_mode == \"LOCV\":\n",
        "            self.split_tag = \"sub\"\n",
        "        else:\n",
        "            self.split_tag = \"sub_id\"\n",
        "\n",
        "\n",
        "        self.LOCV_keys = [[1],[2],[3],[4]]\n",
        "        self.all_keys = [1,2,3,4]\n",
        "        self.sub_ids_of_each_sub = {}\n",
        "\n",
        "        self.file_encoding = {'S1-ADL1.dat':11, 'S1-ADL2.dat':12, 'S1-ADL3.dat':13, 'S1-ADL4.dat':14, 'S1-ADL5.dat':15, 'S1-Drill.dat':16,\n",
        "                              'S2-ADL1.dat':21, 'S2-ADL2.dat':22, 'S2-ADL3.dat':23, 'S2-ADL4.dat':24, 'S2-ADL5.dat':25, 'S2-Drill.dat':26,\n",
        "                              'S3-ADL1.dat':31, 'S3-ADL2.dat':32, 'S3-ADL3.dat':33, 'S3-ADL4.dat':34, 'S3-ADL5.dat':35, 'S3-Drill.dat':36,\n",
        "                              'S4-ADL1.dat':41, 'S4-ADL2.dat':42, 'S4-ADL3.dat':43, 'S4-ADL4.dat':44, 'S4-ADL5.dat':45, 'S4-Drill.dat':46}\n",
        "\n",
        "        self.labelToId = {int(x[0]): i for i, x in enumerate(self.label_map)}\n",
        "        self.all_labels = list(range(len(self.label_map)))\n",
        "\n",
        "        self.drop_activities = [self.labelToId[i] for i in self.drop_activities]\n",
        "        self.no_drop_activites = [item for item in self.all_labels if item not in self.drop_activities]\n",
        "\n",
        "        # super(Opportunity_HAR_DATA, self).__init__(args)\n",
        "\n",
        "\n",
        "\n",
        "    def load_all_the_data(self, root_path):\n",
        "\n",
        "        print(\" ----------------------- load all the data -------------------\")\n",
        "\n",
        "        file_list = os.listdir(root_path)\n",
        "        file_list = [file for file in file_list if file[-3:]==\"dat\"] # in total , it should be 24\n",
        "        assert len(file_list) == 24\n",
        "\n",
        "        df_dict = {}\n",
        "\n",
        "        for file in file_list:\n",
        "            sub_data = pd.read_table(os.path.join(root_path,file), header=None, sep='\\s+')\n",
        "            sub_data =sub_data.iloc[:,self.used_cols]\n",
        "            sub_data.columns = self.col_names\n",
        "\n",
        "            # TODO check missing labels?\n",
        "            sub_data = sub_data.interpolate(method='linear', limit_direction='both')\n",
        "\n",
        "            sub = int(file[1])\n",
        "            sub_data['sub_id'] = self.file_encoding[file]\n",
        "            sub_data[\"sub\"] = sub\n",
        "\n",
        "\n",
        "            if sub not in self.sub_ids_of_each_sub.keys():\n",
        "                self.sub_ids_of_each_sub[sub] = []\n",
        "            self.sub_ids_of_each_sub[sub].append(self.file_encoding[file])\n",
        "\n",
        "            df_dict[self.file_encoding[file]] = sub_data\n",
        "\n",
        "        # all data\n",
        "        df_all = pd.concat(df_dict)\n",
        "        df_all = df_all.set_index('sub_id')\n",
        "        # reorder the columns as sensor1, sensor2... sensorn, sub, activity_id\n",
        "        df_all = df_all[self.col_names[:-1]+[\"sub\"]+[\"activity_id\"]]\n",
        "        # label transformation\n",
        "        df_all[\"activity_id\"] = df_all[\"activity_id\"].map(self.labelToId)\n",
        "        data_y = df_all.iloc[:,-1]\n",
        "        data_x = df_all.iloc[:,:-1]\n",
        "        data_x = data_x.reset_index()\n",
        "        # sub_id, sensor1, sensor2... sensorn, sub,\n",
        "        return data_x, data_y\n",
        "\n",
        "path = '/content/OpportunityUCIDataset/dataset/'\n",
        "Opportunity_HAR_DATA().load_all_the_data(path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "cellView": "form",
        "id": "Eb0V4bMbJ06Q",
        "outputId": "7e079b63-777f-4993-f54c-a067359c6557"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Opportunity_HAR_DATA.__init__() missing 1 required positional argument: 'args'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-bb6e2e2ba4be>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/OpportunityUCIDataset/dataset/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m \u001b[0mOpportunity_HAR_DATA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_all_the_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: Opportunity_HAR_DATA.__init__() missing 1 required positional argument: 'args'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## fdsg"
      ],
      "metadata": {
        "id": "G2J5yexjKfqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://dl.acm.org/doi/pdf/10.1145/3460421.3480419\n",
        "# https://github.com/mariusbock/dl-for-har/blob/main/data_processing/preprocess_data.py\n"
      ],
      "metadata": {
        "id": "paWFwK2XXuRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## wisdm"
      ],
      "metadata": {
        "id": "qxWJtucwKksn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://archive.ics.uci.edu/static/public/507/wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset.zip\n",
        "!unzip wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset.zip\n",
        "!unzip wisdm-dataset.zip"
      ],
      "metadata": {
        "id": "WXyZyeg585Ch",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59d03334-3abb-4deb-af08-64232eb928af"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-09 07:47:13--  https://archive.ics.uci.edu/static/public/507/wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset.zip’\n",
            "\n",
            "wisdm+smartphone+an     [   <=>              ] 295.92M  25.5MB/s    in 15s     \n",
            "\n",
            "2025-06-09 07:47:27 (20.4 MB/s) - ‘wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset.zip’ saved [310292805]\n",
            "\n",
            "Archive:  wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset.zip\n",
            " extracting: WISDM-dataset-description.pdf  \n",
            " extracting: wisdm-dataset.zip       \n",
            "Archive:  wisdm-dataset.zip\n",
            "   creating: wisdm-dataset/\n",
            "  inflating: wisdm-dataset/WISDM-dataset-description.pdf  \n",
            "   creating: wisdm-dataset/arffmagic-master/\n",
            "  inflating: wisdm-dataset/arffmagic-master/Makefile  \n",
            "  inflating: wisdm-dataset/arffmagic-master/.DS_Store  \n",
            " extracting: wisdm-dataset/arffmagic-master/README.md  \n",
            "   creating: wisdm-dataset/arffmagic-master/src/\n",
            "  inflating: wisdm-dataset/arffmagic-master/src/arff.cpp  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/comparator.h  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/chunk.h  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/main.cpp  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/attribute.h  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/libmfcc.c  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/raw.h  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/try.h  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/write.h  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/chunk.cpp  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/raw.cpp  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/globals.h  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/write.cpp  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/arff.h  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/except.h  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/read.cpp  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/attribute.cpp  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/libmfcc.h  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/globals.cpp  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/funcmap.cpp  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/read.h  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/funcmap.h  \n",
            "   creating: wisdm-dataset/arffmagic-master/bin/\n",
            "  inflating: wisdm-dataset/arffmagic-master/bin/magic  \n",
            "   creating: wisdm-dataset/arffmagic-master/bin/arffmagic.dSYM/\n",
            "   creating: wisdm-dataset/arffmagic-master/bin/arffmagic.dSYM/Contents/\n",
            "  inflating: wisdm-dataset/arffmagic-master/bin/arffmagic.dSYM/Contents/Info.plist  \n",
            "   creating: wisdm-dataset/arffmagic-master/bin/arffmagic.dSYM/Contents/Resources/\n",
            "   creating: wisdm-dataset/arffmagic-master/bin/arffmagic.dSYM/Contents/Resources/DWARF/\n",
            "  inflating: wisdm-dataset/arffmagic-master/bin/arffmagic.dSYM/Contents/Resources/DWARF/arffmagic  \n",
            "  inflating: wisdm-dataset/arffmagic-master/bin/.DS_Store  \n",
            "  inflating: wisdm-dataset/arffmagic-master/bin/arffmagic  \n",
            "   creating: wisdm-dataset/arffmagic-master/build/\n",
            "  inflating: wisdm-dataset/arffmagic-master/build/libmfcc.o  \n",
            "  inflating: wisdm-dataset/arffmagic-master/build/write.o  \n",
            "  inflating: wisdm-dataset/arffmagic-master/build/attribute.o  \n",
            "  inflating: wisdm-dataset/arffmagic-master/build/arff.o  \n",
            "  inflating: wisdm-dataset/arffmagic-master/build/read.o  \n",
            "  inflating: wisdm-dataset/arffmagic-master/build/raw.o  \n",
            "  inflating: wisdm-dataset/arffmagic-master/build/funcmap.o  \n",
            "  inflating: wisdm-dataset/arffmagic-master/build/main.o  \n",
            "  inflating: wisdm-dataset/arffmagic-master/build/globals.o  \n",
            "  inflating: wisdm-dataset/arffmagic-master/build/chunk.o  \n",
            "  inflating: wisdm-dataset/README.txt  \n",
            "   creating: wisdm-dataset/raw/\n",
            "   creating: wisdm-dataset/raw/phone/\n",
            "   creating: wisdm-dataset/raw/phone/gyro/\n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1631_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1644_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1639_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1621_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1609_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1627_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1628_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1620_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1633_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1638_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1614_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1625_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1612_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1640_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1616_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1647_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1646_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1637_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1608_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1642_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1624_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/.DS_Store  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1643_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1645_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1641_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1626_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1623_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1607_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1629_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1610_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1649_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1648_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1636_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1630_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1600_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1603_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1606_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1604_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1611_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1632_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1618_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1635_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1605_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1602_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1613_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1617_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1619_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1615_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1650_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1634_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1622_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1601_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/.DS_Store  \n",
            "   creating: wisdm-dataset/raw/phone/accel/\n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1635_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1608_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1649_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1647_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1619_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1621_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1630_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1639_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1615_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1611_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1616_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1644_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1648_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1645_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1641_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1603_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1628_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1610_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1634_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1600_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/.DS_Store  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1613_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1624_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1620_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1606_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1633_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1632_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1626_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1643_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1637_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1640_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1627_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1650_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1614_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1617_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1618_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1642_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1612_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1646_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1601_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1605_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1623_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1607_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1638_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1636_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1629_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1604_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1622_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1625_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1602_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1609_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1631_accel_phone.txt  \n",
            "   creating: wisdm-dataset/raw/watch/\n",
            "   creating: wisdm-dataset/raw/watch/gyro/\n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1629_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1633_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1626_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1644_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1625_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1649_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1631_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1632_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1622_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1628_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1602_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1603_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1610_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1609_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1647_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1638_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1642_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1627_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1601_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1646_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1643_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1636_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1635_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1617_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/.DS_Store  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1612_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1620_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1623_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1615_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1621_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1645_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1641_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1616_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1608_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1605_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1607_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1648_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1600_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1637_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1604_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1639_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1611_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1613_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1614_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1640_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1606_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1630_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1618_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1634_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1650_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1619_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1624_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/.DS_Store  \n",
            "   creating: wisdm-dataset/raw/watch/accel/\n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1626_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1631_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1605_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1608_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1600_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1650_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1623_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1612_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1637_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1620_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1633_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1618_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1635_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1619_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1611_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1614_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1601_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1616_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/.DS_Store  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1645_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1606_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1615_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1638_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1636_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1607_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1627_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1625_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1640_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1643_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1602_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1644_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1629_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1642_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1610_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1624_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1603_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1621_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1646_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1628_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1604_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1634_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1613_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1647_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1609_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1630_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1639_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1617_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1648_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1632_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1649_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1641_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1622_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/.DS_Store  \n",
            "   creating: wisdm-dataset/arff_files/\n",
            "   creating: wisdm-dataset/arff_files/phone/\n",
            "   creating: wisdm-dataset/arff_files/phone/gyro/\n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1610_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1612_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1637_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1622_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1604_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1639_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1621_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1623_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1640_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1645_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1602_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1649_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1600_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1644_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1646_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1632_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1608_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1635_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1606_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1629_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1617_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1628_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1636_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1601_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/.DS_Store  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1615_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1611_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1620_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1647_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1631_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1648_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1603_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1627_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1619_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1607_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1643_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1624_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1638_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1618_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1605_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1625_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1626_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1650_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1616_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1609_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1613_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1630_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1634_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1641_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1633_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1642_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/.DS_Store  \n",
            "   creating: wisdm-dataset/arff_files/phone/accel/\n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1633_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1612_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1620_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1628_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1602_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1636_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1604_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1638_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1647_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1607_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1648_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1643_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1627_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1626_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1611_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1600_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1635_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1613_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1617_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1641_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1642_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1637_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1621_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/.DS_Store  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1632_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1629_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1630_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1603_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1631_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1615_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1609_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1644_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1616_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1634_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1622_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1649_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1645_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1606_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1601_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1639_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1610_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1608_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1624_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1618_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1650_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1646_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1625_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1640_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1619_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1605_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1623_accel_phone.arff  \n",
            "   creating: wisdm-dataset/arff_files/watch/\n",
            "   creating: wisdm-dataset/arff_files/watch/gyro/\n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1620_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1604_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1613_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1631_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1635_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1639_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1628_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1603_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1632_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1626_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1618_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1627_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1609_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1646_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1650_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1629_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1621_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1607_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1611_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1637_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1602_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1617_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1619_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/.DS_Store  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1643_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1630_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1641_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1647_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1616_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1648_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1623_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1606_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1634_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1622_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1645_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1642_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1633_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1615_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1640_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1605_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1610_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1624_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1638_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1612_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1600_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1636_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1601_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1644_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1608_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1625_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1649_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/.DS_Store  \n",
            "   creating: wisdm-dataset/arff_files/watch/accel/\n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1623_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1608_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1641_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1635_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1643_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1604_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1633_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1630_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1609_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1649_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1622_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1607_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1634_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1618_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1610_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1650_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1647_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1642_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1646_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1617_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1637_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1626_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1645_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1612_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1615_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/.DS_Store  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1620_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1632_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1613_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1616_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1644_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1628_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1603_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1606_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1625_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1648_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1639_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1602_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1619_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1636_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1638_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1640_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1624_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1600_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1629_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1631_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1611_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1601_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1621_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1627_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1605_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/.DS_Store  \n",
            "  inflating: wisdm-dataset/change_raw_act.pl  \n",
            "  inflating: wisdm-dataset/.activity_key.txt.swp  \n",
            "  inflating: wisdm-dataset/activity_key.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FA00-tf6MJ-s"
      },
      "outputs": [],
      "source": [
        "# @title rohanpandey WISDM\n",
        "# https://github.com/rohanpandey/Analysis--WISDM-Smartphone-and-Smartwatch-Activity/blob/master/dataset_creation.ipynb\n",
        "! wget https://archive.ics.uci.edu/static/public/507/wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset.zip\n",
        "!unzip wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset.zip\n",
        "!unzip wisdm-dataset.zip\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "folder1=glob.glob(\"wisdm-dataset/raw/*\")\n",
        "# print(folder1)\n",
        "column_names = ['ID', 'activity','timestamp','x','y','z']\n",
        "overall_dataframe=pd.DataFrame(columns = column_names)\n",
        "for subfolder in folder1:\n",
        "    parent_dir = \"./processed/\"\n",
        "    path = os.path.join(parent_dir, subfolder.split('\\\\')[-1])\n",
        "    if not os.path.exists(path): os.makedirs(path)\n",
        "    folder2=glob.glob(subfolder+\"/*\")\n",
        "    for subsubfolder in folder2:\n",
        "        activity_dataframe = pd.DataFrame(columns = column_names)\n",
        "        subfolder_path = os.path.join(path, subsubfolder.split('/')[-1])\n",
        "        if not os.path.exists(subfolder_path): os.makedirs(subfolder_path)\n",
        "        files=glob.glob(subsubfolder+\"/*\")\n",
        "        for file in files:\n",
        "            # print(file)\n",
        "            df = pd.read_csv(file, sep=\",\",header=None)\n",
        "            df.columns = ['ID','activity','timestamp','x','y','z']\n",
        "            # activity_dataframe=activity_dataframe.append(df)\n",
        "            activity_dataframe = pd.concat([activity_dataframe, df], ignore_index=True)\n",
        "\n",
        "        activity_dataframe['z']=activity_dataframe['z'].str[:-1]\n",
        "        # activity_dataframe['meter']=subsubfolder.split('/')[-1]\n",
        "        # activity_dataframe['device']=subfolder.split('/')[-1]\n",
        "        # activity_dataframe.to_csv(subfolder_path+'/data.csv',index=False)\n",
        "        overall_dataframe = pd.concat([overall_dataframe, activity_dataframe], ignore_index=True)\n",
        "\n",
        "overall_dataframe.to_csv(\"processed/wisdm-dataset/raw/data.csv\",index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "CNVNCV8CGtR_"
      },
      "outputs": [],
      "source": [
        "# @title wisdm dataloader\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "# /content/processed/wisdm-dataset/raw/phone/accel/data.csv\n",
        "# /content/processed/wisdm-dataset/raw/watch/gyro/data.csv\n",
        "\n",
        "class BufferDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        # df_keep = pd.read_csv(\"data.csv\")#[['ID','activity','timestamp','x','y','z']]\n",
        "        df_keep = pd.read_csv(\"/content/processed/wisdm-dataset/raw/watch/accel/data.csv\")\n",
        "\n",
        "        user_acts = dict(tuple(df_keep.groupby(['ID','activity'])[['timestamp','x','y','z']]))\n",
        "        self.data = [[d.to_numpy(), a] for a, d in user_acts.items()]\n",
        "        self.act_dict = {i: act for i, act in enumerate(df_keep['activity'].unique())}\n",
        "        self.act_invdict = {v: k for k, v in self.act_dict.items()} # {'A': 0, 'B': 1, ...\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, id_act = self.data[idx]\n",
        "        id_act = self.process(id_act)\n",
        "        return torch.tensor(x[:3500]), id_act # 3567\n",
        "\n",
        "    def process(self, id_act):\n",
        "        return int(id_act[0]), self.act_invdict[id_act[1]]\n",
        "\n",
        "    def transform(self, act_list): # rand resize crop, rand mask # https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html\n",
        "        act_list = np.array(act_list)\n",
        "        try: hr, temp, heart= zip(*act_list)\n",
        "        except ValueError: print('err:',act_list)\n",
        "        kind = 'nearest' if len(act_list)>self.seq_len/.7 else 'linear'\n",
        "        temp_interpolator = scipy.interpolate.interp1d(hr, temp, kind=kind) # linear nearest quadratic cubic\n",
        "        heart_interpolator = scipy.interpolate.interp1d(hr, heart, kind=kind) # linear nearest quadratic cubic\n",
        "        hr_ = np.sort(np.random.uniform(hr[0], hr[-1], round(self.seq_len*random.uniform(1,1/.7))))\n",
        "        temp_, heart_ = temp_interpolator(hr_), heart_interpolator(hr_)\n",
        "        act_list = list(zip(hr_, temp_, heart_))\n",
        "\n",
        "        idx = torch.randint(len(act_list)-self.seq_len+1, size=(1,))\n",
        "        # return act_list[idx: idx+self.seq_len]\n",
        "        act_list = act_list[idx: idx+self.seq_len]\n",
        "\n",
        "        # mask=(torch.rand(self.seq_len)<.1) # True -> masked # random masking\n",
        "        # # act_list[mask] = self.pad[0]\n",
        "        # act_list = [self.pad[0] if m else a for m,a in zip(mask, act_list)]\n",
        "        return act_list\n",
        "\n",
        "    def add(self, episode):\n",
        "        self.data.append(episode)\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    # print(npimg.shape) # (3, 64, 64)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# df_keep = pd.read_csv(\"data.csv\")#[['ID','activity','timestamp','x','y','z']]\n",
        "# user_acts = dict(tuple(df_keep.groupby(['ID','activity'])[['timestamp','x','y','z']]))\n",
        "# dataset = [[d.to_numpy(), a] for a, d in user_acts.items()]\n",
        "\n",
        "\n",
        "train_data = BufferDataset() # one line of poem is roughly 50 characters\n",
        "\n",
        "dataset_size = len(train_data)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(0.7 * dataset_size))\n",
        "np.random.seed(0)\n",
        "np.random.shuffle(indices)\n",
        "train_indices, val_indices = indices[:split], indices[split:]\n",
        "\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "valid_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 64 #512\n",
        "# train_loader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "# test_loader = DataLoader(train_data, sampler=valid_sampler, batch_size=batch_size, num_workers=2, drop_last=True)\n",
        "train_loader = DataLoader(train_data, sampler=train_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "test_loader = DataLoader(train_data, sampler=valid_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title from WISDM_ar_v1.1_raw.txt\n",
        "# !wget https://www.cis.fordham.edu/wisdm/includes/datasets/latest/WISDM_ar_latest.tar.gz\n",
        "# !tar -xzf WISDM_ar_latest.tar.gz\n",
        "path = '/content/WISDM_ar_v1.1/WISDM_ar_v1.1_raw.txt'\n",
        "file = open(path)\n",
        "lines = file.readlines()\n",
        "\n",
        "processedList = []\n",
        "# https://github.com/aakashratha1006/Human-Activity-Recognition/blob/main/human_activity_recognition.py\n",
        "# https://github.com/lisatwyw/data-gym/blob/master/demo/WISDM.ipynb\n",
        "for i, line in enumerate(lines):\n",
        "    try:\n",
        "        line = line.split(',')\n",
        "        last = line[5].split(';')[0]\n",
        "        last = last.strip()\n",
        "        if last == '': break;\n",
        "        temp = [line[0], line[1], line[2], line[3], line[4], last]\n",
        "        processedList.append(temp)\n",
        "    except: print('Error at line number: ', i, line)\n",
        "\n",
        "# with open(path, 'r') as f:\n",
        "#     for line in f:\n",
        "#         line = line.strip()\n",
        "#         individual_record_strings = [rec.strip() for rec in line.split(';') if rec.strip()]\n",
        "#         for record_str in individual_record_strings:\n",
        "#             parts = record_str.split(',')\n",
        "#             if len(parts) == 6: processedList.append(parts)\n",
        "#             else: print(line)\n",
        "\n",
        "# df = pd.read_table(path, header=None, sep='\\s+')\n",
        "# print(df)\n",
        "# print(df.isna().sum())\n",
        "\n",
        "import pandas as pd\n",
        "columns = ['subject', 'activity','time','x','y','z']\n",
        "dataset = pd.DataFrame(data = processedList, columns = columns)\n",
        "# dataset = pd.DataFrame(data = processedList)\n",
        "\n",
        "ans = [y for _, y in dataset.groupby(['subject', 'activity'])]\n",
        "y=list(set([df['subject'].iloc[0] for df in ans]))\n",
        "y.sort()\n",
        "\n",
        "# dataset_size = len(train_data)\n",
        "# indices = list(range(dataset_size))\n",
        "# split = int(np.floor(0.7 * dataset_size))\n",
        "# np.random.seed(0)\n",
        "# np.random.shuffle(indices)\n",
        "# train_indices, val_indices = indices[:split], indices[split:]\n",
        "\n",
        "df_train = dataset[dataset['subject'].isin(y[:int(.7*len(y))])]\n",
        "df_test = dataset[dataset['subject'].isin(y[-int(.3*len(y)):])]\n",
        "\n",
        "def make_Xy(dataset):\n",
        "    ans = [y for _, y in dataset.groupby(['subject', 'activity'])]\n",
        "    y_train = [df['activity'].iloc[0] for df in ans]\n",
        "    # y_train = [df['subject'].iloc[0] for df in ans]\n",
        "    # X_train = [df[df.select_dtypes(include=['number'])].interpolate(method='index', axis=0, index=df['time'], limit_direction='both') for df in ans]\n",
        "    # X_train = [df.drop(['subject', 'activity','time'], axis=1) for df in X_train]\n",
        "    X_train = [df.drop(['subject', 'activity','time'], axis=1) for df in ans]\n",
        "    X_train = [df.apply(pd.to_numeric, errors='coerce') for df in X_train] # Convert non-numeric data in dataset to numeric. errors='coerce': replace all non-numeric values with NaN.\n",
        "    X_train = [df.interpolate(method='index', axis=0, limit_direction='both') for df in X_train] # replace NaN by interpolating\n",
        "    # X_train = [df.interpolate(method='index', axis=0, limit_direction='both') for df in ans]\n",
        "    return X_train, y_train\n",
        "\n",
        "X_train, y_train = make_Xy(df_train)\n",
        "X_test, y_test = make_Xy(df_test)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "SuWXahzpQVQ5",
        "outputId": "d47cff13-cd22-4e02-85b4-4b99ef55a782"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error at line number:  281873 ['\\n']\n",
            "Error at line number:  281874 ['\\n']\n",
            "Error at line number:  281875 ['\\n']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Cannot interpolate with all object-dtype columns in the DataFrame. Try setting at least one column to a numeric dtype.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-115-1c8ff6795d16>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_Xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_Xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-115-1c8ff6795d16>\u001b[0m in \u001b[0;36mmake_Xy\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# X_train = [df.drop(['subject', 'activity','time'], axis=1) for df in X_train]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'subject'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'activity'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit_direction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'both'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-115-1c8ff6795d16>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# X_train = [df.drop(['subject', 'activity','time'], axis=1) for df in X_train]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'subject'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'activity'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit_direction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'both'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36minterpolate\u001b[0;34m(self, method, axis, limit, inplace, limit_direction, limit_area, downcast, **kwargs)\u001b[0m\n\u001b[1;32m   8471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8472\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8473\u001b[0;31m             raise TypeError(\n\u001b[0m\u001b[1;32m   8474\u001b[0m                 \u001b[0;34m\"Cannot interpolate with all object-dtype columns \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8475\u001b[0m                 \u001b[0;34m\"in the DataFrame. Try setting at least one \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Cannot interpolate with all object-dtype columns in the DataFrame. Try setting at least one column to a numeric dtype."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ans = [y for _, y in dataset.groupby(['subject', 'activity'])]\n",
        "y_train = [df['activity'].iloc[0] for df in ans]\n",
        "# y_train = [df['subject'].iloc[0] for df in ans]\n",
        "# print(ans[0].select_dtypes(include=['number']))\n",
        "# X_train = [df[df.select_dtypes(include=['number'])].interpolate(method='index', axis=0, index=df['time'], limit_direction='both') for df in ans]\n",
        "X_train = [df.drop(['subject', 'activity','time'], axis=1) for df in ans]\n",
        "print(X_train[0])\n",
        "\n",
        "X_train = [df.astype(float).interpolate(method='index', axis=0, limit_direction='both') for df in ans]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        },
        "id": "I6jKwHhx88FY",
        "outputId": "72de4500-73d3-4eb5-bc50-0d42a9df4968"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            x      y            z\n",
            "337217  -1.73  10.31   0.53119355\n",
            "337218  -1.18  11.92   -0.3405087\n",
            "337219  -2.03   11.5     -1.56634\n",
            "337220  -2.14   9.34   -2.1111538\n",
            "337221  -3.76  11.88   -4.5628166\n",
            "...       ...    ...          ...\n",
            "343411   0.31  10.73  0.040861044\n",
            "343412   2.76  10.95   0.84446156\n",
            "343413    0.8  10.15     1.334794\n",
            "343414  -1.57   8.69    1.4165162\n",
            "343415  -2.56   8.58     1.879608\n",
            "\n",
            "[6199 rows x 3 columns]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "could not convert string to float: 'Walking'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-121-bc96338ebc67>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit_direction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'both'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-121-bc96338ebc67>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit_direction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'both'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   6641\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6642\u001b[0m             \u001b[0;31m# else, only a single dtype is given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6643\u001b[0;31m             \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6644\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor_from_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6645\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"astype\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m         return self.apply(\n\u001b[0m\u001b[1;32m    431\u001b[0m             \u001b[0;34m\"astype\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# type: ignore[call-overload]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype_array_safe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_coerce_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/dtypes/astype.py\u001b[0m in \u001b[0;36mastype_array_safe\u001b[0;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;31m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/dtypes/astype.py\u001b[0m in \u001b[0;36mastype_array\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_astype_nansafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;31m# in pandas we don't store numpy str dtypes, so convert to object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/dtypes/astype.py\u001b[0m in \u001b[0;36m_astype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;31m# Explicit copy, or required since NumPy can't view from / to object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'Walking'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ans = [y for _, y in df_train.groupby(['subject', 'activity'])]\n",
        "# y_train = [df['activity'].iloc[0] for df in ans]\n",
        "# # y_train = [df['subject'].iloc[0] for df in ans]\n",
        "# # X_train = [df.interpolate(method='index', axis=0, limit_direction='both') for df in X_train]\n",
        "# X_train = [df.interpolate(method='index', axis='time', limit_direction='both') for df in ans]\n",
        "# print(X_train)\n",
        "print(df_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOPEPHJq2lRU",
        "outputId": "b47d2e7f-3a5b-4892-b98a-7743154765db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       subject activity            time            x          y            z\n",
            "8489        17  Walking  57869902574000    0.5720546    9.80665  -0.53119355\n",
            "8490        17  Walking  57870012316000         -0.0   10.18802   -0.5720546\n",
            "8491        17  Walking  57870122301000  -0.23154591   9.847511   -0.6946377\n",
            "8492        17  Walking  57870222246000   0.14982383  10.079058  -0.50395286\n",
            "8493        17  Walking  57870332292000   0.14982383  10.147159  -0.61291564\n",
            "...        ...      ...             ...          ...        ...          ...\n",
            "343411      11  Walking   1786872234000         0.31      10.73  0.040861044\n",
            "343412      11  Walking   1786922282000         2.76      10.95   0.84446156\n",
            "343413      11  Walking   1786972392000          0.8      10.15     1.334794\n",
            "343414      11  Walking   1787022227000        -1.57       8.69    1.4165162\n",
            "343415      11  Walking   1787072368000        -2.56       8.58     1.879608\n",
            "\n",
            "[239520 rows x 6 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title pandasDataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class pandasDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X, self.y = X, y\n",
        "        chars = sorted(list(set(y)))\n",
        "        self.vocab_size = len(chars) #\n",
        "        self.stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "        self.itos = {i:ch for i,ch in enumerate(chars)}\n",
        "        self.y = self.data_process(y) #\n",
        "        self.seq_len = min([len(a) for a in X])\n",
        "        print('seq_len',self.seq_len)\n",
        "\n",
        "    def data_process(self, data): # str\n",
        "        # return torch.tensor([self.stoi.get(c) for c in data]) #\n",
        "        return np.array([self.stoi.get(c) for c in data]) #\n",
        "\n",
        "    def __len__(self): return len(self.X)\n",
        "    # def __getitem__(self, idx): return self.X.iloc[idx].to_numpy(), self.y.iloc[idx]\n",
        "    # def __getitem__(self, idx): return self.X[idx].to_numpy(), self.y[idx]\n",
        "    def __getitem__(self, idx):\n",
        "        i = np.random.randint(0, len(self.X[idx])-self.seq_len+1)\n",
        "        return self.X[idx].to_numpy()[i:i+self.seq_len].astype(float), self.y[idx]\n",
        "\n",
        "train_data = pandasDataset(X_train, y_train)\n",
        "test_data = pandasDataset(X_test, y_test)\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=2)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=2)\n",
        "\n",
        "for X, y in train_loader:\n",
        "    print(X.shape, y.shape)\n",
        "    break\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OVelMicgXwi",
        "outputId": "46b1ec8d-4510-4c7d-cb43-3b258ef0dde2",
        "cellView": "form"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seq_len 1\n",
            "seq_len 1\n",
            "torch.Size([64, 1, 244]) torch.Size([64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(len(X_train)) # 49\n",
        "for df in ans:\n",
        "    print(df['subject'].iloc[0], df['activity'].iloc[0], len(df))\n",
        "# y=list(set([int(df['subject'].iloc[0]) for df in ans]))\n",
        "# y=list(set([df['subject'].iloc[0] for df in ans]))\n",
        "# y.sort()\n",
        "# print(y) # ['11', '13', '15', '17', '18', '20', '27', '29', '32', '33', '35', '36', '6']\n",
        "# # print(len(y)) # 13\n",
        "# y[:int(.7*len(y))]\n",
        "\n",
        "# 1 256 2000+\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWj01BGLU_YZ",
        "outputId": "94693953-6684-4036-fbf7-6f22dd831eee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "subject101.dat 0 126460\n",
            "subject101.dat 1 27187\n",
            "subject101.dat 2 23480\n",
            "subject101.dat 3 21717\n",
            "subject101.dat 4 22253\n",
            "subject101.dat 5 21265\n",
            "subject101.dat 6 23575\n",
            "subject101.dat 7 20265\n",
            "subject101.dat 12 15890\n",
            "subject101.dat 13 14899\n",
            "subject101.dat 16 22941\n",
            "subject101.dat 17 23573\n",
            "subject101.dat 24 12912\n",
            "subject102.dat 0 183651\n",
            "subject102.dat 1 23430\n",
            "subject102.dat 2 22345\n",
            "subject102.dat 3 25576\n",
            "subject102.dat 4 32533\n",
            "subject102.dat 5 9238\n",
            "subject102.dat 6 25108\n",
            "subject102.dat 7 29739\n",
            "subject102.dat 12 17342\n",
            "subject102.dat 13 15213\n",
            "subject102.dat 16 20683\n",
            "subject102.dat 17 28880\n",
            "subject102.dat 24 13262\n",
            "subject103.dat 0 78495\n",
            "subject103.dat 1 22044\n",
            "subject103.dat 2 28761\n",
            "subject103.dat 3 20533\n",
            "subject103.dat 4 29036\n",
            "subject103.dat 12 10389\n",
            "subject103.dat 13 15275\n",
            "subject103.dat 16 20325\n",
            "subject103.dat 17 27975\n",
            "subject104.dat 0 98155\n",
            "subject104.dat 1 23047\n",
            "subject104.dat 2 25492\n",
            "subject104.dat 3 24706\n",
            "subject104.dat 4 31932\n",
            "subject104.dat 5 1\n",
            "subject104.dat 6 22699\n",
            "subject104.dat 7 27533\n",
            "subject104.dat 12 16694\n",
            "subject104.dat 13 14285\n",
            "subject104.dat 16 20037\n",
            "subject104.dat 17 24995\n",
            "subject105.dat 0 102341\n",
            "subject105.dat 1 23699\n",
            "subject105.dat 2 26864\n",
            "subject105.dat 3 22132\n",
            "subject105.dat 4 32033\n",
            "subject105.dat 5 24646\n",
            "subject105.dat 6 24577\n",
            "subject105.dat 7 26271\n",
            "subject105.dat 12 14281\n",
            "subject105.dat 13 12727\n",
            "subject105.dat 16 24445\n",
            "subject105.dat 17 33034\n",
            "subject105.dat 24 7733\n",
            "subject106.dat 0 111721\n",
            "subject106.dat 1 23340\n",
            "subject106.dat 2 23041\n",
            "subject106.dat 3 24356\n",
            "subject106.dat 4 25721\n",
            "subject106.dat 5 22825\n",
            "subject106.dat 6 20486\n",
            "subject106.dat 7 26686\n",
            "subject106.dat 12 13291\n",
            "subject106.dat 13 11272\n",
            "subject106.dat 16 21078\n",
            "subject106.dat 17 37744\n",
            "subject106.dat 24 256\n",
            "subject107.dat 0 80823\n",
            "subject107.dat 1 25611\n",
            "subject107.dat 2 12282\n",
            "subject107.dat 3 25751\n",
            "subject107.dat 4 33720\n",
            "subject107.dat 5 3692\n",
            "subject107.dat 6 22680\n",
            "subject107.dat 7 28725\n",
            "subject107.dat 12 17646\n",
            "subject107.dat 13 11618\n",
            "subject107.dat 16 21552\n",
            "subject107.dat 17 29499\n",
            "subject108.dat 0 145929\n",
            "subject108.dat 1 24165\n",
            "subject108.dat 2 22923\n",
            "subject108.dat 3 25160\n",
            "subject108.dat 4 31533\n",
            "subject108.dat 5 16532\n",
            "subject108.dat 6 25475\n",
            "subject108.dat 7 28888\n",
            "subject108.dat 12 11683\n",
            "subject108.dat 13 9655\n",
            "subject108.dat 16 24292\n",
            "subject108.dat 17 32990\n",
            "subject108.dat 24 8806\n",
            "subject109.dat 0 2086\n",
            "subject109.dat 24 6391\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(path, 'r') as f:\n",
        "# #     # print(f.read())\n",
        "    processedList = f.read().replace(' ', '').replace(',\\n', '\\n').replace('\\n', '').replace(';','\\n')\n",
        "processedList = [p.split(',') for p in processedList.split('\\n')]\n",
        "# print(processedList[:100])\n",
        "\n",
        "df = pd.DataFrame(data = processedList)\n",
        "\n",
        "for i,p in enumerate(processedList):\n",
        "    if len(p)>6:\n",
        "        print(i, p)\n",
        "    if i>800000: break\n",
        "\n",
        "# 343419\n",
        "# 844581\n"
      ],
      "metadata": {
        "id": "d8MOKLyVbsnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(path, 'r') as f:\n",
        "    for i, line in enumerate(f.read().split('\\n')):\n",
        "        print(line)\n",
        "        if i>3: break\n"
      ],
      "metadata": {
        "id": "V30m0354fPED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pamap2"
      ],
      "metadata": {
        "id": "sALSYgwsKqr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://archive.ics.uci.edu/dataset/231/pamap2+physical+activity+monitoring\n",
        "# https://archive.ics.uci.edu/ml/datasets/pamap2+physical+activity+monitoring\n",
        "!wget https://archive.ics.uci.edu/static/public/231/pamap2+physical+activity+monitoring.zip -O pamap2.zip\n",
        "!unzip pamap2.zip\n",
        "!unzip PAMAP2_Dataset.zip\n"
      ],
      "metadata": {
        "id": "mBMyuPWwpaHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title pamap2 dataloader\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00231/PAMAP2_Dataset.zip\n",
        "!unzip PAMAP2_Dataset.zip\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def load_pamap2(path='PAMAP2_Dataset', activities=[1,2,3,4,5,6,7,12,13,16,17,24]):\n",
        "    cols = list(range(1, 54))  # 52 features + HR\n",
        "    data = []\n",
        "    for file in os.listdir(path):\n",
        "        if file.endswith(\".dat\"):\n",
        "            df = pd.read_csv(os.path.join(path, file), sep=' ', header=None, usecols=[0] + cols)\n",
        "            df.columns = ['timestamp'] + [f'feat_{i}' for i in range(len(cols))]\n",
        "            df = df.dropna()\n",
        "            df['label'] = pd.read_csv(os.path.join(path, file), sep=' ', header=None, usecols=[0]).values\n",
        "            data.append(df)\n",
        "    full_df = pd.concat(data).dropna()\n",
        "    return full_df\n",
        "\n",
        "# df = load_pamap2()\n",
        "# df = pd.read_csv('/content/PAMAP2_Dataset/Protocol/subject101.dat', sep=' ', header=None, usecols=[0] + cols)\n",
        "df = pd.read_csv('/content/PAMAP2_Dataset/Protocol/subject101.dat', sep=' ', header=None)\n",
        "\n"
      ],
      "metadata": {
        "id": "GYmA5JWdnVdg",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title papmap2 me\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# https://github.com/EdnaEze/Physical-Activity-Monitoring/blob/main/DSRM-Edna.ipynb\n",
        "\n",
        "activities = {0:'transient', 1:'lying', 2:'sitting', 3:'standing', 4:'walking', 5:'running', 6:'cycling', 7:'Nordic_walking', 9:'watching_TV', 10:'computer_work', 11:'car driving', 12:'ascending_stairs', 13:'descending_stairs', 16:'vacuum_cleaning', 17:'ironing', 18:'folding_laundry', 19:'house_cleaning', 20:'playing_soccer', 24:'rope_jumping'}\n",
        "all_columns = [\"time\", \"activity\", \"heartrate\", 'handTemperature', 'handAcc16_1', 'handAcc16_2', 'handAcc16_3', 'handAcc6_1', 'handAcc6_2', 'handAcc6_3', 'handGyro1', 'handGyro2', 'handGyro3', 'handMagne1', 'handMagne2', 'handMagne3', 'handOrientation1', 'handOrientation2', 'handOrientation3', 'handOrientation4', 'chestTemperature', 'chestAcc16_1', 'chestAcc16_2', 'chestAcc16_3', 'chestAcc6_1', 'chestAcc6_2', 'chestAcc6_3', 'chestGyro1', 'chestGyro2', 'chestGyro3', 'chestMagne1', 'chestMagne2', 'chestMagne3', 'chestOrientation1', 'chestOrientation2', 'chestOrientation3', 'chestOrientation4', 'ankleTemperature', 'ankleAcc16_1', 'ankleAcc16_2', 'ankleAcc16_3', 'ankleAcc6_1', 'ankleAcc6_2', 'ankleAcc6_3', 'ankleGyro1', 'ankleGyro2', 'ankleGyro3', 'ankleMagne1', 'ankleMagne2', 'ankleMagne3', 'ankleOrientation1', 'ankleOrientation2', 'ankleOrientation3', 'ankleOrientation4']\n",
        "\n",
        "dataset = pd.DataFrame()\n",
        "\n",
        "# path = '/content/OpportunityUCIDataset/dataset'\n",
        "path = '/content/PAMAP2_Dataset/Protocol/'\n",
        "\n",
        "usr_lst = os.listdir(path)\n",
        "for file in os.listdir(path):\n",
        "# for file, subject_id in zip(file_names, subject_id):\n",
        "    df = pd.read_table(path+file, header=None, sep='\\s+')\n",
        "    df.columns = all_columns\n",
        "    df['subject'] = file\n",
        "    dataset = pd.concat([dataset, df], ignore_index=True)\n",
        "\n",
        "y = dataset['subject'].unique()\n",
        "y.sort()\n",
        "\n",
        "# dataset_size = len(train_data)\n",
        "# indices = list(range(dataset_size))\n",
        "# split = int(np.floor(0.7 * dataset_size))\n",
        "# np.random.seed(0)\n",
        "# np.random.shuffle(indices)\n",
        "# train_indices, val_indices = indices[:split], indices[split:]\n",
        "\n",
        "df_train = dataset[dataset['subject'].isin(y[:int(.7*len(y))])]\n",
        "df_test = dataset[dataset['subject'].isin(y[-int(.3*len(y)):])]\n",
        "\n",
        "def make_Xy(dataset):\n",
        "    anss = [y for _, y in dataset.groupby(['subject', 'activity'])]\n",
        "    ans = []\n",
        "    for x in anss:\n",
        "        if len(x) > 1000: # only keep sequences with more than 1000 samples\n",
        "            ans.append(x)\n",
        "    y_train = [df['activity'].iloc[0] for df in ans]\n",
        "    # y_train = [df['subject'].iloc[0] for df in ans]\n",
        "    # X_train = [df.drop(['subject', 'activity','time'], axis=1) for df in X_train]\n",
        "    X_train = [df.drop(['subject', 'activity','time'], axis=1) for df in ans]\n",
        "    # X_train = [df.interpolate(method='index', axis=0, limit_direction='both') for df in ans]\n",
        "\n",
        "    X_train = [df.apply(pd.to_numeric, errors='coerce') for df in X_train] # Convert non-numeric data in dataset to numeric. errors='coerce': replace all non-numeric values with NaN.\n",
        "    X_train = [df.interpolate(method='index', axis=0, limit_direction='both') for df in X_train] # replace NaN by interpolating\n",
        "\n",
        "    # X_train = [df.interpolate(method='values', axis=0, limit_direction='both') for df in ans]\n",
        "    # data.reset_index(drop=True, inplace=True) # make row ind start from 0\n",
        "    return X_train, y_train\n",
        "\n",
        "X_train, y_train = make_Xy(df_train)\n",
        "X_test, y_test = make_Xy(df_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "y4IxoAdbL9Wy"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(X_train[0])\n",
        "# print([len(x) for x in X_train])\n",
        "# print([y.tolist() for y in y_train])\n",
        "# anss = []\n",
        "# for x in ans:\n",
        "#     if len(x) > 1000:\n",
        "#         anss.append(x)\n",
        "# print([len(x) for x in anss])\n",
        "\n",
        "print(dataset['subject'].unique())\n",
        "# print(dataset['activity'].unique())\n",
        "# print(dataset['activity'].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_E4ldvPUG8k",
        "outputId": "42c89e0d-b984-4178-b9a3-4af647940fe3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['subject102.dat' 'subject109.dat' 'subject105.dat' 'subject101.dat'\n",
            " 'subject108.dat' 'subject104.dat' 'subject103.dat' 'subject107.dat'\n",
            " 'subject106.dat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## dsfbg"
      ],
      "metadata": {
        "id": "1Kfkma7pKvIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "https://github.com/Navidfoumani/Series2Vec/tree/main/Dataset/Benchmarks\n",
        "\n",
        "# https://link.springer.com/article/10.1007/s10618-024-01043-w\n",
        "\n"
      ],
      "metadata": {
        "id": "U63KjrZwEoUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title eigenworm\n",
        "!wget https://timeseriesclassification.com/aeon-toolkit/Worms.zip\n",
        "!unzip har\n",
        "!unzip 'UCI HAR Dataset.zip'\n",
        "!mv 'UCI HAR Dataset' UCI_HAR_Dataset\n",
        "\n",
        "# 131 train and 128 test. We have truncated each series to the shortest usable. Each series has 17984 observations. Each worm is classified as either wild-type (the N2 reference strain) or one of four mutant types: goa-1; unc-1; unc-38 and unc-63.\n"
      ],
      "metadata": {
        "id": "q1v4VIsCxKFj",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ECG Heartbeat Categorization Dataset\n",
        "\n",
        "# https://www.kaggle.com/datasets/shayanfazeli/heartbeat\n",
        "\n",
        "# Install dependencies as needed:\n",
        "# pip install kagglehub[pandas-datasets]\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "\n",
        "# Set the path to the file you'd like to load\n",
        "file_path = \"\"\n",
        "\n",
        "# Load the latest version\n",
        "df = kagglehub.load_dataset(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"shayanfazeli/heartbeat\",\n",
        "  file_path,\n",
        "  # Provide any additional arguments like\n",
        "  # sql_query or pandas_kwargs. See the\n",
        "  # documenation for more information:\n",
        "  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas\n",
        ")\n",
        "\n",
        "print(\"First 5 records:\", df.head())\n"
      ],
      "metadata": {
        "id": "ct1t8hPLyBwd",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fh9o__m2-j7K",
        "outputId": "f412c2b0-c375-4617-e7b7-002f76904460"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/268.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m266.2/268.0 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.0/268.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25htorch.Size([64, 200])\n"
          ]
        }
      ],
      "source": [
        "# @title simplex\n",
        "!pip install -q opensimplex\n",
        "import opensimplex\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def simplexmask1d(seq=512, ctx_scale=(.85,1), trg_scale=(.6,.8), B=64, chaos=[1,.5]):\n",
        "    i = np.linspace(0, chaos[0], num=seq) # 2-5\n",
        "    noise = opensimplex.noise2array(i, np.random.randint(1e10, size=B)) # [B, seq]\n",
        "    noise = torch.from_numpy(noise)\n",
        "    # trunc_normal = torch.fmod(torch.randn(2)*.3,1)/2 + .5\n",
        "    # print(trunc_normal)\n",
        "    ctx_mask_scale = torch.rand(1) * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    trg_mask_scale = torch.rand(1) * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "    # ctx_mask_scale = trunc_normal[0] * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    # trg_mask_scale = trunc_normal[1] * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "\n",
        "    ctx_len, trg_len = int(seq*ctx_mask_scale), int(seq*trg_mask_scale)\n",
        "    val, trg_index = torch.topk(noise, trg_len, dim=1, sorted=False)\n",
        "    ctx_len = ctx_len - trg_len\n",
        "\n",
        "    remove_mask = torch.ones((B,seq), dtype=bool) # [B, S]\n",
        "    remove_mask.scatter_(1, trg_index, False).flatten()\n",
        "    ind = torch.arange(seq).unsqueeze(0).repeat(B,1)[remove_mask].reshape(B, -1)\n",
        "\n",
        "    i = np.linspace(0, chaos[1], num=seq) # 2-5\n",
        "    noise = opensimplex.noise2array(i, np.random.randint(1e10, size=B)) # [B, seq]\n",
        "    noise = torch.from_numpy(noise)[remove_mask].reshape(B, -1)\n",
        "    val, ctx_ind = torch.topk(noise, ctx_len, dim=1, sorted=False)\n",
        "    ctx_index = ind[torch.arange(B).unsqueeze(-1), ctx_ind]\n",
        "    return ctx_index, trg_index\n",
        "\n",
        "b=64\n",
        "# ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.7,.8), trg_scale=(.4,.6), B=b, chaos=[3,.5])\n",
        "ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.85,1), trg_scale=(.7,.8), B=b, chaos=[3,.5])\n",
        "# ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.8,.9), trg_scale=(.7,.8), B=b, chaos=[1,.5])\n",
        "mask = torch.zeros(b ,200)\n",
        "mask[torch.arange(b).unsqueeze(-1), trg_index] = 1\n",
        "mask[torch.arange(b).unsqueeze(-1), ctx_index] = .5\n",
        "# mask = mask[None,...]\n",
        "# mask = mask[:,None,None,:]#.repeat(1,3,1,1)\n",
        "print(mask.shape)\n",
        "\n",
        "# def imshow(img):\n",
        "#     npimg = img.numpy()\n",
        "#     plt.rcParams[\"figure.figsize\"] = (8,4)\n",
        "#     # plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "#     plt.pcolormesh(np.transpose(npimg, (1, 2, 0)))\n",
        "#     plt.show()\n",
        "# # imshow(mask[0])\n",
        "# import torchvision\n",
        "# imshow(torchvision.utils.make_grid(mask, nrow=1))\n",
        "\n",
        "# print(index)\n",
        "# print(index.shape)\n",
        "# print(mask)\n",
        "# print(mask.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Xn7WZShwWxF8"
      },
      "outputs": [],
      "source": [
        "# @title ijepa multiblock 1d\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "import torch\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(self, length=200,\n",
        "        enc_mask_scale=(0.2, 0.8), pred_mask_scale=(0.2, 0.8),\n",
        "        nenc=1, npred=2, min_keep=4, allow_overlap=False):\n",
        "        super().__init__()\n",
        "        self.length = length\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "\n",
        "    def _sample_block_size(self, scale):\n",
        "        _rand = torch.rand(1).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.length * mask_scale) # num patches to keep\n",
        "        # -- Sample block aspect-ratio\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        l = max_keep#int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        while l >= self.length: l -= 1 # crop mask to be smaller than img\n",
        "        return l\n",
        "\n",
        "    def _sample_block_mask(self, length, acceptable_regions=None):\n",
        "        l = length\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # --\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            left = torch.randint(0, self.length - l, (1,))\n",
        "            mask = torch.zeros(self.length, dtype=torch.int32)\n",
        "            mask[left:left+l] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones(self.length, dtype=torch.int32)\n",
        "        mask_complement[left:left+l] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, B):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        p_size = self._sample_block_size(scale=self.pred_mask_scale)\n",
        "        e_size = self._sample_block_size(scale=self.enc_mask_scale)\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.length\n",
        "        min_keep_enc = self.length\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                print(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "        return collated_masks_enc, collated_masks_pred\n",
        "\n",
        "batch=64\n",
        "length=200\n",
        "mask_collator = MaskCollator(length=length, enc_mask_scale=(.85, 1.), pred_mask_scale=(.15, .2),\n",
        "        nenc=1, npred=4, min_keep=4,\n",
        "        # allow_overlap=True)\n",
        "        allow_overlap=False)\n",
        "\n",
        "collated_masks_enc, collated_masks_pred = mask_collator(batch)\n",
        "context_indices, trg_indices = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "# print(context_indices.shape, trg_indices.shape)\n",
        "\n",
        "\n",
        "# plt.pcolormesh(mask)\n",
        "b=64\n",
        "mask = torch.zeros(batch ,length)\n",
        "mask[torch.arange(batch).unsqueeze(-1), trg_indices] = 1\n",
        "mask[torch.arange(batch).unsqueeze(-1), context_indices] = .5\n",
        "# mask = mask[None,...]\n",
        "# print(mask.shape)\n",
        "# mask = mask[:,None,None,:]#.repeat(1,3,1,1)\n",
        "# print(mask.shape)\n",
        "\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# def imshow(img):\n",
        "#     npimg = img.numpy()\n",
        "#     plt.rcParams[\"figure.figsize\"] = (8,4)\n",
        "#     # plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "#     plt.pcolormesh(np.transpose(npimg, (1, 2, 0)))\n",
        "#     # plt.imshow(npimg)\n",
        "#     plt.show()\n",
        "# # imshow(mask)\n",
        "# import torchvision\n",
        "# print(torchvision.utils.make_grid(mask, nrow=1).shape)\n",
        "# # imshow(torchvision.utils.make_grid(mask, nrow=8))\n",
        "# imshow(torchvision.utils.make_grid(mask, nrow=1))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ge36SCxOl2Oq"
      },
      "outputs": [],
      "source": [
        "# @title RoPE\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def RoPE(dim, seq_len=512, base=10000):\n",
        "    theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "    pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "    angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "    rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [1, seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "    return rot_emb\n",
        "\n",
        "# class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "#     def __init__(self, dim, seq_len=512, base=10000):\n",
        "#         super().__init__()\n",
        "#         self.dim, self.base = dim, base\n",
        "#         theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "#         pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "#         angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "#         self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         seq_len = x.size(1)\n",
        "#         if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.base)\n",
        "#         return x * self.rot_emb[:seq_len]\n",
        "\n",
        "class RotEmb(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, top=torch.pi, base=10000):\n",
        "        super().__init__()\n",
        "        self.theta = top / (base ** (torch.arange(0, dim, step=2, device=device) / dim))\n",
        "        # self.theta = top / (base ** torch.linspace(0, 1, dim//2, device=device))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (pos.unsqueeze(-1) * self.theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [seq_len, dim]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6T4F651kmGh",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title AttentionBlock\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "import math\n",
        "def StableInit(m): # https://openreview.net/pdf?id=lkRjnNW0gb\n",
        "    if isinstance(m, nn.Linear):\n",
        "        # W ~ N(0, ( 1/ (sqrt(n_in) + sqrt(n_out)) )^2 )\n",
        "        # want std = 1/ (sqrt(n_in) + sqrt(n_out))\n",
        "        # n_in, n_out = module.weight.shape[0], module.weight.shape[1]\n",
        "        n_in, n_out = m.weight.shape\n",
        "        torch.nn.init.normal_(m.weight, std=1/(math.sqrt(n_in)+math.sqrt(n_out)))\n",
        "        if m.bias is not None:\n",
        "            nn.init.zeros_(m.bias)\n",
        "\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class SelfAttn(nn.Module):\n",
        "    def __init__(self, dim, n_heads):\n",
        "        super().__init__()\n",
        "        self.dim, self.n_heads = dim, n_heads\n",
        "        d_head = dim//n_heads\n",
        "        self.qkv = nn.Linear(dim, dim*3, bias=False)\n",
        "        # self.lin = nn.Linear(dim, dim)\n",
        "        self.lin = zero_module(nn.Linear(dim, dim))\n",
        "        # self.rope = RoPE(d_head, seq_len=512, base=10000)\n",
        "        # self.rope = RoPE2D(d_head, h=64, w=64, base=100)\n",
        "        self.scale = d_head**-.5\n",
        "        # torch.nn.init.normal_(self.qkv.weight, std=.02)\n",
        "        # self.qkv.apply(StableInit)\n",
        "\n",
        "    def forward(self, x): # [b,t,d]\n",
        "        q,k,v = self.qkv(x).unflatten(-1, (self.n_heads,-1)).transpose(1,2).chunk(3, dim=-1) # [b, t, n_heads, d_head] -> [b, n_heads, t, d_head]\n",
        "        # q, k = self.rope(q), self.rope(k)\n",
        "        q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2)\n",
        "        context = k.transpose(-2,-1) @ v # [batch, n_heads, d_head, d_head]\n",
        "        x = q @ context # [b, n_heads, t, d_head]\n",
        "        # print('SelfAttn', x.shape)\n",
        "        x = x.transpose(1,2).flatten(2)\n",
        "        return self.lin(x)\n",
        "\n",
        "\n",
        "\n",
        "class SwiGLU(nn.Module): # https://arxiv.org/pdf/2002.05202\n",
        "    def __init__(self, d_model, ff_dim): # d_model * 3*ff_dim params\n",
        "        super().__init__()\n",
        "        self.lin0 = nn.Linear(d_model, 2*ff_dim, bias=False)\n",
        "        self.lin1 = zero_module(nn.Linear(ff_dim, d_model, bias=False))\n",
        "\n",
        "    def forward(self, x): # [b,t,d]\n",
        "        x0, x1 = self.lin0(x).chunk(2, dim=-1)\n",
        "        return self.lin1(x0*F.silu(x1))\n",
        "\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, mult=4, drop=0.):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        # self.norm = nn.RMSNorm(d_model, elementwise_affine=False) # LayerNorm RMSNorm\n",
        "        self.norm1, self.norm2 = nn.LayerNorm(d_model), nn.LayerNorm(d_model)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "        self.attn = SelfAttn(d_model, n_heads)\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        # act = Swwish()\n",
        "        ff_dim=d_model*mult\n",
        "        self.ff = nn.Sequential(\n",
        "            # nn.RMSNorm(d_model), act, nn.Dropout(drop), nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Dropout(drop), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            nn.RMSNorm(d_model), nn.Linear(d_model, ff_dim), act,\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(drop), nn.Linear(d_model, ff_dim), act,\n",
        "            nn.RMSNorm(ff_dim), nn.Dropout(drop), zero_module(nn.Linear(ff_dim, d_model))\n",
        "        )\n",
        "        # self.ff = SwiGLU(d_model, ff_dim)\n",
        "\n",
        "\n",
        "    def forward(self, x): # [batch, seq_len, d_model]\n",
        "        # print('attnblk fwd',x.shape)\n",
        "        x = x + self.drop(self.attn(self.norm1(x)))\n",
        "        x = x + self.ff(x)\n",
        "        # x = x + self.drop(self.ff(self.norm2(x)))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3vUVNJc_sy1a"
      },
      "outputs": [],
      "source": [
        "# @title scheduler\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "import math\n",
        "def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, num_cycles=0.5, last_epoch=-1):\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < num_warmup_steps:\n",
        "            return float(current_step) / float(max(1, num_warmup_steps))\n",
        "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
        "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))\n",
        "    return LambdaLR(optimizer, lr_lambda, last_epoch)\n",
        "\n",
        "# total_steps=100\n",
        "# base_lr, max_lr = 3e-5, 3e-4\n",
        "\n",
        "# import torch\n",
        "# model=torch.nn.Linear(2,3)\n",
        "# optim = torch.optim.AdamW(model.parameters(), lr=base_lr, betas=(0.9, 0.999))\n",
        "\n",
        "# scheduler = get_cosine_schedule_with_warmup(optim, num_warmup_steps=20 , num_training_steps=total_steps) # https://docs.pytorch.org/torchtune/0.2/generated/torchtune.modules.get_cosine_schedule_with_warmup.html\n",
        "# # scheduler = torch.optim.lr_scheduler.OneCycleLR(optim, max_lr=max_lr, total_steps=total_steps, pct_start=0.45, anneal_strategy='cos', cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=div_factor, final_div_factor=100.0, three_phase=True,)\n",
        "\n",
        "# lr_lst=[]\n",
        "# import matplotlib.pyplot as plt\n",
        "# for t in range(total_steps):\n",
        "#     lr=optim.param_groups[0][\"lr\"]\n",
        "#     lr_lst.append(lr)\n",
        "#     scheduler.step()\n",
        "# plt.plot(lr_lst)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-zdjdJixtOu",
        "outputId": "2b1e4d3c-a207-4a6c-daab-a168e642c29e",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12448\n",
            "torch.Size([4, 219, 16])\n"
          ]
        }
      ],
      "source": [
        "# @title TransformerModel/Predictor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, n_heads=4, d_hid=None, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        # self.pos_emb = nn.Parameter(torch.randn(1, 256, d_model)*.02) # 200\n",
        "        self.pos_emb = RoPE(d_model, seq_len=256, base=10000) # 256\n",
        "\n",
        "        # self.transformer = nn.Sequential(*[AttentionBlock(d_model, n_heads=n_heads) for _ in range(nlayers)])\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, n_heads=n_heads, drop=drop) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.randn(1,1,d_model)*0.02) # randn zeros\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim)# if out_dim != d_model else None\n",
        "\n",
        "        # torch.nn.init.normal_(self.embed.weight, std=.02)\n",
        "        # if self.lin: torch.nn.init.normal_(self.lin.weight, std=.02)\n",
        "\n",
        "\n",
        "    def forward(self, x, context_indices, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = x.shape\n",
        "        # x = x * self.pos_enc(context_indices)\n",
        "        # print(\"Trans pred\",x.shape, self.pos_emb[0,context_indices].shape)\n",
        "        x = x + self.pos_emb[0,context_indices]\n",
        "        # x = x * self.pos_emb[0,context_indices]\n",
        "        # print('pred fwd', self.pos_emb[:,context_indices].shape)\n",
        "\n",
        "        # pred_tokens = self.cls * self.pos_enc(trg_indices) # [M, num_trg_toks, d_model]\n",
        "        pred_tokens = self.cls + self.pos_emb[0,trg_indices]\n",
        "        # pred_tokens = self.cls * self.pos_emb[0,trg_indices]\n",
        "        # print(\"pred fwd\", x.shape, pred_tokens.shape)\n",
        "        x = torch.cat([x, pred_tokens], dim=1) # [batch, seq_len+num_trg_toks, d_model]\n",
        "        out = self.transformer(x)\n",
        "\n",
        "        out = self.norm(out)\n",
        "        out = out[:,seq:] # [batch, num_trg_toks, d_model]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "# class SLSTM(nn.Module):\n",
        "#     def __init__(self, d_model, num_layers=2, batch_first=True):\n",
        "#         super().__init__()\n",
        "#         self.lstm = nn.LSTM(d_model, d_model, num_layers)\n",
        "\n",
        "#     def forward(self, x): # [b,c,t]\n",
        "#         x = x + self.lstm(x.transpose(-2,-1))[0].transpose(-2,-1) # skip=True\n",
        "#         return x\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, drop=0):\n",
        "    def __init__(self, patch_size, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0):\n",
        "        super().__init__()\n",
        "        patch_size=8\n",
        "        act = nn.ReLU() # ReLU SiLU GELU\n",
        "        # act = LearntSwwish(d_model) # SnakeBeta LearntSwwish\n",
        "        # act1 = LearntSwwish(d_model) # SnakeBeta LearntSwwish\n",
        "        # act = Swwish()\n",
        "        self.embed = nn.Sequential(\n",
        "            # # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.MaxPool1d(2,2), #nn.MaxPool1d(3, 2, 3//2),\n",
        "            # # nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.Conv1d(d_model, d_model,3,2,3//2)\n",
        "            nn.Conv1d(in_dim, d_model,7,2,7//2), nn.BatchNorm1d(d_model), act,\n",
        "            nn.Conv1d(d_model, d_model,5,2,5//2), nn.BatchNorm1d(d_model), act,\n",
        "            # nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), act, #nn.MaxPool1d(2,2),\n",
        "            # nn.Conv1d(d_model, d_model,3,2,3//2), nn.BatchNorm1d(d_model), act, #nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2),\n",
        "            # nn.Conv1d(in_dim, d_model, patch_size, patch_size), # like patch\n",
        "            # nn.Conv1d(in_dim, d_model, 1, 1), # like patch\n",
        "\n",
        "            # nn.Conv2d(d_model, d_model,(in_dim,3),2,3//2),\n",
        "            # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.Dropout(drop), nn.BatchNorm1d(d_model), snake,\n",
        "            # SLSTM(d_model),\n",
        "\n",
        "            )\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        # self.pos_emb = nn.Parameter(torch.randn(1, 256, d_model)*.02) # 200\n",
        "        self.pos_emb = RoPE(d_model, seq_len=256, base=10000) # 256\n",
        "        # self.transformer = nn.Sequential(*[AttentionBlock(d_model, n_heads=n_heads) for _ in range(nlayers)])\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, n_heads=n_heads, drop=drop) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "        # if self.lin: torch.nn.init.normal_(self.lin.weight, std=.02)\n",
        "\n",
        "        # self.embed.apply(self.init_conv)\n",
        "        # self.embed.apply(self.init_weights)\n",
        "\n",
        "    # def init_conv(self, m):\n",
        "    #     if isinstance(m, nn.Conv1d):\n",
        "    #         # nn.init.kaiming_uniform_(m.weight, a=math.sqrt(5))\n",
        "    #         nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "    #         if m.bias is not None:\n",
        "    #             # bound = 1 / math.sqrt(m.in_channels * m.kernel_size * m.kernel_size)\n",
        "    #             # nn.init.uniform_(m.bias, -bound, bound)\n",
        "    #             nn.init.zeros_(m.bias)\n",
        "\n",
        "    def init_weights(self, m):\n",
        "        if isinstance(m, (nn.Linear, nn.Conv1d)):\n",
        "            torch.nn.init.normal_(m.weight, std=.02)\n",
        "            if m.bias is not None: nn.init.zeros_(m.bias)\n",
        "\n",
        "\n",
        "    def forward(self, x, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "        # try: print(\"Trans fwd\",x.shape, context_indices.shape)\n",
        "        # except: print(\"Trans fwd noind\",x.shape)\n",
        "        # x = self.pos_enc(x)\n",
        "        x = x + self.pos_emb[:,:x.shape[1]]\n",
        "        # x = x * self.pos_emb[:,:x.shape[1]]\n",
        "        if context_indices != None: x = x[torch.arange(x.shape[0]).unsqueeze(-1), context_indices] # [batch, num_context_toks, d_model]\n",
        "\n",
        "        # print(\"TransformerModel\",x.shape)\n",
        "        x = self.transformer(x)\n",
        "        out = self.norm(x)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out\n",
        "\n",
        "batch, seq_len, d_model = 4,1751,16 # wisdm 3500, ethol conc 1751\n",
        "in_dim = 3\n",
        "patch_size=32\n",
        "model = TransformerModel(patch_size, in_dim, d_model, n_heads=4, nlayers=3, drop=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "# # # print(out)\n",
        "# model = TransformerPredictor(in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1).to(device)\n",
        "# out = model(out)\n",
        "# print(out.shape)\n",
        "# for name, param in model.named_parameters():\n",
        "#     print(name, param.shape, param[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ardu1zJwdHM3",
        "outputId": "ee1e8b7a-95ce-4f35-eb8f-27bcc4531259",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "101760\n",
            "torch.Size([])\n"
          ]
        }
      ],
      "source": [
        "# @title SeqJEPA\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class SeqJEPA(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, nlayers=2, n_heads=4, drop=0):\n",
        "        super().__init__()\n",
        "        if out_dim is None: out_dim = d_model\n",
        "        self.patch_size = 8 # 8 32\n",
        "        self.student = TransformerModel(self.patch_size, in_dim, d_model, out_dim=out_dim, n_heads=n_heads, nlayers=nlayers, drop=drop)\n",
        "        self.predicter = TransformerPredictor(out_dim, d_model//2, out_dim, n_heads=4, nlayers=1, drop=drop)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "        # self.transform = RandomResizedCrop1d(3500, scale=(.8,1.))\n",
        "\n",
        "    #     self.apply(self.init_weights)\n",
        "    #     self.apply(self.zero_last_layers)\n",
        "    # def init_weights(self, m):\n",
        "    #     if isinstance(m, (nn.Linear, nn.Conv1d)):\n",
        "    #         torch.nn.init.normal_(m.weight, std=.02)\n",
        "    #         if m.bias is not None: nn.init.zeros_(m.bias)\n",
        "\n",
        "    # def zero_last_layers(self, m):\n",
        "    #     children = list(m.children())\n",
        "    #     if not children: return\n",
        "    #     last = children[-1]\n",
        "    #     if isinstance(last, (nn.Linear, nn.Conv2d)):\n",
        "    #         nn.init.zeros_(last.weight)\n",
        "    #         if last.bias is not None: nn.init.zeros_(last.bias)\n",
        "\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]\n",
        "        batch, seq, dim = x.shape\n",
        "        # print(x.shape)\n",
        "        # target_mask = multiblock(seq//self.patch_size, min_s=.2, max_s=.3, M=4, B=1).any(1).squeeze(1) # best.2.3M4 og.15.2M4# mask out targets to be predicted # [M, seq]\n",
        "        # # target_mask = randpatch(seq//self.patch_size, mask_size=8, gamma=.9).unsqueeze(0) # 8.9 [seq]\n",
        "\n",
        "        # print(target_mask.shape, x.shape)\n",
        "        # context_mask = ~multiblock(seq//self.patch_size, min_s=.85, max_s=1, M=1, B=1).squeeze(1)|target_mask # og .85,1.M1 # [1, seq], True->Mask\n",
        "        # context_mask = torch.zeros((1,seq//self.patch_size), dtype=bool)|target_mask # [1,h,w], True->Mask\n",
        "\n",
        "        # context_indices, trg_indices = simplexmask1d(seq//self.patch_size, ctx_scale=(.85,1), trg_scale=(.7,.8), B=batch, chaos=[3,.5])\n",
        "        # context_indices, trg_indices = simplexmask1d(seq//self.patch_size, ctx_scale=(.8,.9), trg_scale=(.7,.8), B=batch, chaos=[1,.5])\n",
        "        # context_indices, trg_indices = simplexmask1d(seq//self.patch_size, ctx_scale=(.8,1), trg_scale=(.2,.8), B=batch, chaos=[1,.5])\n",
        "        # print(context_indices.shape, trg_indices.shape)\n",
        "\n",
        "        # context_indices = context_indices.repeat(batch,1)\n",
        "        # trg_indices = trg_indices.repeat(batch,1)\n",
        "        # context_mask = ~context_mask|target_mask # [1,]\n",
        "        # context_indices = (~context_mask).nonzero()[:,1].unsqueeze(0).repeat(batch,1)\n",
        "        # # print(trg_indices.shape, context_indices.shape)\n",
        "        # # print(context_mask.shape,target_mask.shape, x.shape)\n",
        "        # target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "        # # # target_mask, context_mask = target_mask.repeat(batch,1), context_mask.repeat(batch,1)\n",
        "        # # x_ = x * F.adaptive_avg_pool1d((~context_mask).float(), x.shape[1]).unsqueeze(-1) # zero masked locations\n",
        "\n",
        "        # context_indices = (~context_mask).nonzero()[:,1].unflatten(0, (batch,-1)) # int idx [num_context_toks] , idx of context not masked\n",
        "        # trg_indices = target_mask.nonzero()[:,1].unflatten(0, (batch,-1)) # int idx [num_trg_toks] , idx of targets that are masked\n",
        "\n",
        "\n",
        "        # mask_collator = MaskCollator(length=seq//self.patch_size, enc_mask_scale=(.85,1), pred_mask_scale=(.15,.2), nenc=1, npred=4, min_keep=4, allow_overlap=False)\n",
        "        mask_collator = MaskCollator(length=seq//self.patch_size, enc_mask_scale=(.85,1), pred_mask_scale=(.2,.25), nenc=1, npred=4, min_keep=4, allow_overlap=False)\n",
        "        collated_masks_enc, collated_masks_pred = mask_collator(batch) # idx of ctx, idx of masked trg\n",
        "        # # collated_masks_enc, collated_masks_pred = mask_collator(1) # idx of ctx, idx of masked trg\n",
        "        context_indices, trg_indices = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "        # # # zero_mask = torch.zeros(batch ,seq//self.patch_size, device=device)\n",
        "        # # # zero_mask[torch.arange(batch).unsqueeze(-1), context_indices] = 1\n",
        "        # # zero_mask = torch.zeros(1 ,seq//self.patch_size, device=device)\n",
        "        # # zero_mask[:, context_indices] = 1\n",
        "        # # x_ = x * F.adaptive_avg_pool1d(zero_mask, x.shape[1]).unsqueeze(-1) # zero masked locations\n",
        "        # # context_indices, trg_indices = context_indices.repeat(batch,1), trg_indices.repeat(batch,1)\n",
        "\n",
        "\n",
        "        # print('x_',x_.shape, context_indices.shape, trg_indices.shape)\n",
        "\n",
        "        sx = self.student(x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "        # print('seq_jepa loss sx',sx.shape)\n",
        "        sy_ = self.predicter(sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "        sy_ = F.layer_norm(sy_, (sy_.size(-1),))\n",
        "        with torch.no_grad():\n",
        "            sy = self.teacher(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "            sy = sy[torch.arange(sy.shape[0]).unsqueeze(-1), trg_indices] # [batch, num_context_toks, d_model] # nan bec len(trg_ind)==0 # print('loss sy',torch.isnan(sy).any())\n",
        "            sy = F.layer_norm(sy, (sy.size(-1),))\n",
        "        loss = F.mse_loss(sy, sy_)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        sx = self.student(x)\n",
        "        out = sx.mean(dim=1)\n",
        "        return out\n",
        "\n",
        "# min_s=0.15, max_s, M\n",
        "# trg.15.2M4 C.85 1\n",
        "\n",
        "# 1e-2,1e-3 < 3e-3,1e-3\n",
        "# patch16 < patch32\n",
        "# NoPE good but sus\n",
        "\n",
        "# ctx/trg sacle min/max, num blk,\n",
        "\n",
        "in_dim = X_train[0].shape[-1] # 3\n",
        "out_dim = train_data.vocab_size # 16\n",
        "d_model=64\n",
        "# seq_jepa = SeqJEPA(in_dim=in_dim, d_model=d_model, out_dim=None, nlayers=1, n_heads=8).to(device)#.to(torch.float)\n",
        "seq_jepa = SeqJEPA(in_dim=in_dim, d_model=d_model, out_dim=None, nlayers=1, n_heads=8, drop=.0).to(device)#.to(torch.float)\n",
        "# optim = torch.optim.AdamW(seq_jepa.parameters(), lr=1e-3) # 1e-3? default 1e-2\n",
        "optim = torch.optim.AdamW(seq_jepa.parameters(), lr=3e-4) # 1e-3? default 1e-2\n",
        "# optim = StableAdamW(seq_jepa.parameters(), lr=1e-3)\n",
        "# optim = Lion(seq_jepa.parameters(), lr=1e-4, weight_decay=1e-1) # lr 1e-3, wd 1e-1\n",
        "# optim = torch.optim.AdamW([{'params': seq_jepa.student.parameters()},\n",
        "#     {'params': seq_jepa.predicter.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # default 1e-2, 5e-2\n",
        "    # {'params': seq_jepa.predicter.parameters(), 'lr': 1e-2}], lr=1e-3, weight_decay=1e-2)\n",
        "\n",
        "# !pip install -q bitsandbytes\n",
        "# import bitsandbytes as bnb\n",
        "# # # optim = bnb.optim.(seq_jepa.parameters(), lr=1e-3, betas=(0.9, 0.99), weight_decay=1e-2)\n",
        "# # optim = bnb.optim.Lion(seq_jepa.parameters(), lr=1e-4, betas=(0.9, 0.99), weight_decay=1e-2) # Lion8bit\n",
        "# optim = bnb.optim.Lion(seq_jepa.parameters(), lr=1e-4, betas=(0.9, 0.99), weight_decay=1e-1) # Lion8bit\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/configs/in1k_vith14_ep300.yaml\n",
        "# d_model 1024,384\n",
        "# depth 12,6/12\n",
        "# wd 5e-2 - 4e-1\n",
        "# adamw 1e-4 - 1e-3 - 1e-6\n",
        "# ema 0.996-1\n",
        "\n",
        "print(sum(p.numel() for p in seq_jepa.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.parameters())) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.predicter.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.student.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.teacher.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# d_model^2 * nlayers\n",
        "\n",
        "x = torch.rand((24, 1700, in_dim), device=device)\n",
        "out = seq_jepa.loss(x)\n",
        "print(out.shape)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(d_model, out_dim).to(device)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3) # 1e-3\n",
        "# optim = torch.optim.AdamW([{'params': seq_jepa.parameters()}, {'params': classifier.parameters(), 'lr': 1e-3}], lr=1e-3)\n",
        "# coptim = torch.optim.AdamW(classifier.parameters(), lr=1e-3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgVbsYsu85eT"
      },
      "outputs": [],
      "source": [
        "for name, param in seq_jepa.named_parameters():\n",
        "    print(name, param.shape, param[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "2Nd-sGe6Ku4S",
        "outputId": "4bbfbfdc-df30-437c-86f2-40315029a240"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbobdole\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250530_104056-ncdm009c</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/ucr/runs/ncdm009c' target=\"_blank\">dazzling-glade-40</a></strong> to <a href='https://wandb.ai/bobdole/ucr' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/ucr' target=\"_blank\">https://wandb.ai/bobdole/ucr</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/ucr/runs/ncdm009c' target=\"_blank\">https://wandb.ai/bobdole/ucr/runs/ncdm009c</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "!pip install -q wandb\n",
        "import wandb # https://docs.wandb.ai/quickstart\n",
        "wandb.login(key='487a2109e55dce4e13fc70681781de9f50f27be7')\n",
        "try: run.finish()\n",
        "except NameError: pass\n",
        "run = wandb.init(project=\"ucr\", config={\"model\": \"res18\",}) # violet SeqJEPA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 769
        },
        "id": "v-eTjgAhmp_t",
        "outputId": "4a4f0e0f-bb25-489c-d760-990c8e042535",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.08333333333333333\n",
            "0 7.499999999999999e-07\n",
            "0.08333333333333333\n",
            "1 1.4999999999999998e-06\n",
            "0.08333333333333333\n",
            "2 2.2499999999999996e-06\n",
            "0.08333333333333333\n",
            "3 2.9999999999999997e-06\n",
            "0.08333333333333333\n",
            "4 3.7499999999999997e-06\n",
            "0.08333333333333333\n",
            "5 4.499999999999999e-06\n",
            "0.08333333333333333\n",
            "6 5.25e-06\n",
            "0.08333333333333333\n",
            "7 5.999999999999999e-06\n",
            "0.08333333333333333\n",
            "8 6.749999999999999e-06\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-b557b30f7478>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mstrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_jepa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;31m# strain(seq_jepa, train_loader, optim, scheduler)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0mctrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_jepa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_jepa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-76-b557b30f7478>\u001b[0m in \u001b[0;36mctrain\u001b[0;34m(model, classifier, dataloader, coptim, scheduler)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# bfloat16 float16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0msx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0my_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-75-73d9700ae806>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# [batch, T, 3]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0msx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstudent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-74-e21b71b35c10>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, context_indices)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# print(\"TransformerModel\",x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-72-74936f7ff458>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# [batch, seq_len, d_model]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# print('attnblk fwd',x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m# x = x + self.drop(self.ff(self.norm2(x)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-72-74936f7ff458>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqkv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [b, t, n_heads, d_head] -> [b, n_heads, t, d_head]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# q, k = self.rope(q), self.rope(k)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mv\u001b[0m \u001b[0;31m# [batch, n_heads, d_head, d_head]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;31m# [b, n_heads, t, d_head]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# @title strain ctrain test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "# name = 'violet'\n",
        "# train_summary_writer = tf.summary.create_file_writer('logs/'+name+'/train')\n",
        "# test_summary_writer = tf.summary.create_file_writer('logs/'+name+'/test')\n",
        "\n",
        "def strain(model, dataloader, optim, scheduler=None):\n",
        "    model.train()\n",
        "    for i, (x, _) in enumerate(dataloader):\n",
        "        # x = x[...,1:].to(device).to(torch.bfloat16) # for wisdm?\n",
        "        x = x.to(device).to(torch.bfloat16)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            loss = model.loss(x)\n",
        "\n",
        "            # repr_loss, std_loss, cov_loss = model.loss(x)\n",
        "            # loss = model.sim_coeff * repr_loss + model.std_coeff * std_loss + model.cov_coeff * cov_loss\n",
        "\n",
        "        optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # total_norm = 0\n",
        "        # for p in model.parameters(): total_norm += p.grad.data.norm(2).item() ** 2\n",
        "        # total_norm = total_norm**.5\n",
        "        # print('total_norm', total_norm)\n",
        "        # for p in list(filter(lambda p: p.grad is not None, model.parameters())):\n",
        "        #     print(p.grad.data.norm(2).item())\n",
        "        # print(\"max grad norm\", max([p.grad.data.norm(2).item() for p in list(filter(lambda p: p.grad is not None, model.parameters()))]))\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 5) # 0.5\n",
        "\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            m=0.999 # 0.99 m = next(momentum_scheduler)\n",
        "            for param_q, param_k in zip(model.student.parameters(), model.teacher.parameters()):\n",
        "                param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "\n",
        "        if scheduler is not None: scheduler.step()\n",
        "        # print(\"strain\",loss.item())\n",
        "        # for param in seq_jepa.context_encoder.cls: print(param.data)\n",
        "        # for param in seq_jepa.predicter.cls: print(param.data)\n",
        "        try: wandb.log({\"loss\": loss.item(), \"repr/I\": repr_loss.item(), \"std/V\": std_loss.item(), \"cov/C\": cov_loss.item()})\n",
        "        # try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        # with train_summary_writer.as_default(): tf.summary.scalar('strain', loss.item(), step=i)\n",
        "        if i>=500: break\n",
        "\n",
        "\n",
        "def ctrain(model, classifier, dataloader, coptim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.eval()\n",
        "    classifier.train()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        # x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        # x, y = x[...,1:].to(device).to(torch.bfloat16), y[1].to(device) # [batch, ] # (id, activity)\n",
        "        x, y = x.to(device).to(torch.bfloat16), y.to(device) # [batch, ] # (id, activity)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            with torch.no_grad():\n",
        "                sx = model(x).detach()\n",
        "            y_ = classifier(sx)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "        coptim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 5) # .5\n",
        "        scaler.step(coptim)\n",
        "        scaler.update()\n",
        "        # print(\"classify\",loss.item())\n",
        "        try: wandb.log({\"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        # with test_summary_writer.as_default(): tf.summary.scalar('closs', loss.item(), step=i)\n",
        "        if i>=100: break\n",
        "\n",
        "\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    correct = 0\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        # x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        # x, y = x[...,1:].to(device).to(torch.float), y[1].to(device) # [batch, ] # (id, activity)\n",
        "        x, y = x.to(device).to(torch.float), y.to(device) # [batch, ] # (id, activity)\n",
        "        with torch.no_grad():\n",
        "            sx = model(x)\n",
        "            try:\n",
        "                rankme = RankMe(sx).item()\n",
        "                lidar = LiDAR(sx).item()\n",
        "            except NameError: pass\n",
        "            y_ = classifier(sx)\n",
        "        test_loss = F.cross_entropy(y_, y)\n",
        "        correct += (y==y_.argmax(dim=1)).sum().item()\n",
        "        if i>=100: break\n",
        "    # print(correct/len(y))\n",
        "    print(correct/len(dataloader.dataset))\n",
        "    try: wandb.log({\"correct\": correct/len(dataloader.dataset)})\n",
        "    # try: wandb.log({\"correct\": correct/len(y), \"rankme\": rankme, \"lidar\": lidar})\n",
        "    except NameError: pass\n",
        "\n",
        "scheduler = get_cosine_schedule_with_warmup(optim, num_warmup_steps=400, num_training_steps=2000) # https://docs.pytorch.org/torchtune/0.2/generated/torchtune.modules.get_cosine_schedule_with_warmup.html\n",
        "for i in range(2000): #\n",
        "# for i in range(5000): # 5000\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # np.random.shuffle(train_indices); np.random.shuffle(val_indices) # for wisdm\n",
        "    # train_sampler, valid_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(val_indices)\n",
        "    # # batch_size = 64 #512\n",
        "    # train_loader = DataLoader(train_data, sampler=train_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "    # test_loader = DataLoader(train_data, sampler=valid_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True)\n",
        "\n",
        "    strain(seq_jepa, train_loader, optim)\n",
        "    # strain(seq_jepa, train_loader, optim, scheduler)\n",
        "    ctrain(seq_jepa, classifier, train_loader, coptim)\n",
        "    test(seq_jepa, classifier, test_loader)\n",
        "    scheduler.step()\n",
        "    print(i, optim.param_groups[0][\"lr\"])\n",
        "    # strain(violet, train_loader, voptim)\n",
        "    # ctrain(violet, classifier, train_loader, coptim)\n",
        "    # test(violet, classifier, test_loader)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5p6LJ2qPqom"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir logs/gradient_tape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4J2ahp3wmqcg"
      },
      "outputs": [],
      "source": [
        "# @title supervised train test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "def strain(model, classifier, dataloader, optim, coptim, scheduler=None):\n",
        "    model.train()\n",
        "    classifier.train()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        # y = y.to(device)\n",
        "        # x, y = x[...,1:].to(device).to(torch.bfloat16), y[1].to(device) # [batch, ] # (id, activity)\n",
        "        x, y = x.to(device).to(torch.bfloat16), y.to(device) # [batch, ] # (id, activity)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        coptim.zero_grad()\n",
        "        # print(loss)\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # for p in list(filter(lambda p: p.grad is not None, model.parameters())):\n",
        "        #     print(p.grad.data.norm(2).item())\n",
        "        # print(\"max grad norm\", max([p.grad.data.norm(2).item() for p in list(filter(lambda p: p.grad is not None, model.parameters()))]))\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 10) # 0.5\n",
        "\n",
        "        scaler.step(optim)\n",
        "        scaler.step(coptim)\n",
        "        scaler.update()\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        print(\"strain\",loss.item())\n",
        "        # for param in seq_jepa.context_encoder.cls: print(param.data)\n",
        "        # for param in seq_jepa.predicter.cls: print(param.data)\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=50: break\n",
        "\n",
        "\n",
        "# def test(model, dataloader):\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        # x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        # x, y = x[...,1:].to(device).to(torch.float), y[1].to(device) # [batch, ] # (id, activity)\n",
        "        x, y = x.to(device).to(torch.float), y.to(device) # [batch, ] # (id, activity)\n",
        "        with torch.no_grad():\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "        loss = F.cross_entropy(y_, y)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        print(correct/len(y))\n",
        "        try: wandb.log({\"correct\": correct/len(y), \"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "for i in range(1000):\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # np.random.shuffle(train_indices); np.random.shuffle(val_indices)\n",
        "    # train_sampler, valid_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(val_indices)\n",
        "    # # batch_size = 64 #512\n",
        "    # train_loader = DataLoader(train_data, sampler=train_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "    # test_loader = DataLoader(train_data, sampler=valid_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True)\n",
        "\n",
        "    strain(seq_jepa, classifier, train_loader, optim, coptim)\n",
        "    test(seq_jepa, classifier, test_loader)\n",
        "    # strain(violet, train_loader, voptim)\n",
        "    # test(violet, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JT8AgOx0E_KM",
        "outputId": "19308cfe-2f31-472c-bfcc-0066ae14644f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# @title save/load\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# folder='/content/drive/MyDrive/jepa/'\n",
        "\n",
        "# # modelsd, optimsd = torch.load(folder+'SeqJEPA.pkl', map_location=device).values()\n",
        "# modelsd, optimsd = torch.load('SeqJEPA.pkl', map_location=device).values()\n",
        "# seq_jepa.load_state_dict(modelsd, strict=False)\n",
        "# optim.load_state_dict(optimsd)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "modelsd = torch.load(folder+'roberta.pkl', map_location=device)['model']#.values()\n",
        "# print(modelsd)\n",
        "model_mlm.load_state_dict(modelsd, strict=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpbLPvjiE-pD"
      },
      "outputs": [],
      "source": [
        "# checkpoint = {'model': seq_jepa.state_dict(), 'optimizer': optim.state_dict()}\n",
        "# checkpoint = {'model': model_mlm.state_dict()}\n",
        "# torch.save(checkpoint, folder+'roberta.pkl')\n",
        "# torch.save(checkpoint, folder+'SeqJEPA.pkl')\n",
        "# torch.save(checkpoint, 'SeqJEPA.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# storage"
      ],
      "metadata": {
        "id": "3dl1RyOjHdi1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vScvFGGSeN6"
      },
      "source": [
        "## drawer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pQfM2fYvcTh6"
      },
      "outputs": [],
      "source": [
        "# @title masks\n",
        "import torch\n",
        "\n",
        "def multiblock(seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "    return target_mask\n",
        "\n",
        "def multiblock(seq, min_s, max_s, B=64, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    # mask_len = torch.rand(B, 1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_len = torch.rand(1, M) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(B, M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    # indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    indices = torch.arange(seq)[None,None,...] # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [B, M, seq]\n",
        "    return target_mask\n",
        "\n",
        "def multiblock2d(hw=(8,8), scale=(.15,.2), aspect_ratio=(.75,1.5), M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_aspect = torch.rand(1) * (aspect_ratio[1] - aspect_ratio[0]) + aspect_ratio[0] # in (min_s, max_s) # all blocks same size\n",
        "    mask_scale = torch.rand(1) * (scale[1] - scale[0]) + scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    h = (mask_scale/mask_aspect)**.5# h*(h*aspect) = scale\n",
        "    w = h * mask_aspect\n",
        "    h_pos, w_pos = torch.rand(M)*(1-w), torch.rand(M)*(1-h) # in (0, 1 - mask_len)\n",
        "    h_len, h_pos = (h*hw[0]).int(), h_pos*hw[0]\n",
        "    w_len, w_pos = (w*hw[1]).int(), w_pos*hw[1]\n",
        "    h_ind, w_ind = torch.arange(hw[0]).unsqueeze(0), torch.arange(hw[1]).unsqueeze(0) # [1, seq]\n",
        "    h_mask = (h_ind>=h_pos.unsqueeze(-1)) & (h_ind<(h_pos+h_len).unsqueeze(-1)) # [M, seq]\n",
        "    w_mask = (w_ind>=w_pos.unsqueeze(-1)) & (w_ind<(w_pos+w_len).unsqueeze(-1)) # [M, seq]\n",
        "    target_mask = h_mask.unsqueeze(-1) & w_mask.unsqueeze(-2) # [M, seq, seq]\n",
        "    return target_mask\n",
        "\n",
        "def multiblock2d(hw=(8,8), scale=(.15,.2), aspect_ratio=(.75,1.5), B=64, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_aspect = torch.rand(B, 1) * (aspect_ratio[1] - aspect_ratio[0]) + aspect_ratio[0] # in (min_s, max_s) # all blocks same size\n",
        "    mask_scale = torch.rand(B, 1) * (scale[1] - scale[0]) + scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    h = (mask_scale/mask_aspect)**.5# h*(h*aspect) = scale\n",
        "    w = h * mask_aspect\n",
        "    h_pos, w_pos = torch.rand(M)*(1-w), torch.rand(M)*(1-h) # in (0, 1 - mask_len)\n",
        "    h_len, h_pos = (h*hw[0]).int(), h_pos*hw[0]\n",
        "    w_len, w_pos = (w*hw[1]).int(), w_pos*hw[1]\n",
        "    h_ind, w_ind = torch.arange(hw[0]).unsqueeze(0), torch.arange(hw[1]).unsqueeze(0) # [1, seq]\n",
        "    h_mask = (h_ind>=h_pos.unsqueeze(-1)) & (h_ind<(h_pos+h_len).unsqueeze(-1)) # [M, seq]\n",
        "    w_mask = (w_ind>=w_pos.unsqueeze(-1)) & (w_ind<(w_pos+w_len).unsqueeze(-1)) # [M, seq]\n",
        "    target_mask = h_mask.unsqueeze(-1) & w_mask.unsqueeze(-2) # [M, seq, seq]\n",
        "    return target_mask\n",
        "\n",
        "# https://arxiv.org/pdf/2210.07224\n",
        "def randpatch(seq, mask_size=8, gamma=0.9): # num patches of seq, mask patch size, masking ratio\n",
        "    # mask = torch.rand(seq//mask_size)<gamma\n",
        "    length = seq//mask_size\n",
        "    g = torch.normal(gamma, std=.1, size=(1,)).clamp(.5,.9)\n",
        "    # g = gamma\n",
        "    idx = torch.randperm(length)[:int(length*g)]\n",
        "    mask = torch.zeros(length, dtype=bool)\n",
        "    mask[idx] = True\n",
        "    mask = mask.repeat_interleave(mask_size, dim=-1)\n",
        "    return mask # [seq] , True -> mask\n",
        "\n",
        "\n",
        "import torch\n",
        "def apply_masks(x, mask): # [b,t,d], [mask_size] # https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "    mask_keep = mask.unsqueeze(-1).repeat(x.size(0), 1, x.size(-1)) # [batch,T,dim]\n",
        "    return torch.gather(x, dim=1, index=mask_keep) # [batch,mask_size,dim]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9rPxvrrsYI_W"
      },
      "outputs": [],
      "source": [
        "# @title simplex\n",
        "# !pip install -q opensimplex\n",
        "import opensimplex\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def simplexmask(hw=(8,8), scale=(.15,.2)):\n",
        "    ix = iy = np.linspace(0, 1, num=8)\n",
        "    ix, iy = ix+np.random.randint(1e10), iy+np.random.randint(1e10)\n",
        "    y=opensimplex.noise2array(ix, iy)\n",
        "    y = torch.from_numpy(y)\n",
        "    mask_scale = torch.rand(1) * (scale[1] - scale[0]) + scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    yy = y.flatten().sort()[0][int(hw[0]*hw[1]*mask_scale)]\n",
        "    mask = (y<=yy.item())\n",
        "    return mask # T/F [h,w]\n",
        "\n",
        "def simplexmask1d(seq=512, scale=(.15,.2), chaos=2):\n",
        "    i = np.linspace(0, chaos, num=seq) # 2\n",
        "    j = np.random.randint(1e10, size=1)\n",
        "    y=opensimplex.noise2array(i, j) # [1, seq]\n",
        "    # plt.pcolormesh(y)\n",
        "    # plt.show()\n",
        "    y = torch.from_numpy(y)\n",
        "    # print(y.shape)\n",
        "    mask_scale = torch.rand(1) * (scale[1] - scale[0]) + scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    # print(a.shape, int(seq*mask_scale))\n",
        "    val, ind = y.sort()\n",
        "    yy = val[:,int(seq*mask_scale)]\n",
        "    mask = (y<=yy.item())\n",
        "    index = ind[:,:int(seq*mask_scale)]\n",
        "    return index, mask # T/F [h,w]\n",
        "\n",
        "def simplexmask1d(seq=512, ctx_scale=(.85,1), trg_scale=(.6,.8), B=64, chaos=2):\n",
        "    i = np.linspace(0, chaos, num=seq) # 2-5\n",
        "    j = np.random.randint(1e10, size=B)\n",
        "    noise = opensimplex.noise2array(i, j) # [B, seq]\n",
        "    # plt.pcolormesh(noise[:1])\n",
        "    # plt.rcParams[\"figure.figsize\"] = (20,3)\n",
        "    # plt.show()\n",
        "    noise = torch.from_numpy(noise)\n",
        "    ctx_mask_scale = torch.rand(1) * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    trg_mask_scale = torch.rand(1) * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "    val, ind = noise.sort()\n",
        "    trg_index = ind[:,-int(seq*trg_mask_scale):]\n",
        "    ctx_index = ind[:,-int(seq*ctx_mask_scale):-int(seq*trg_mask_scale)] # ctx wraps trg ; most similar to multiblock\n",
        "    # ctx_index = ind[:,:int(seq*ctx_mask_scale)-int(seq*trg_mask_scale)] # ctx hug bottom\n",
        "    # ctx_index = ind[:,-int(seq*ctx_mask_scale)-int(seq*trg_mask_scale):-int(seq*trg_mask_scale)] # ctx wraps trg ; most similar to multiblock\n",
        "    # ctx_index = ind[:,:int(seq*ctx_mask_scale)] # ctx hug bottom\n",
        "    return ctx_index, trg_index\n",
        "\n",
        "\n",
        "# mask = torch.zeros(1 ,200)\n",
        "# mask[:, trg_index[:1]] = 1\n",
        "# mask[:, ctx_index[:1]] = .5\n",
        "# plt.rcParams[\"figure.figsize\"] = (20,1)\n",
        "# plt.pcolormesh(mask)\n",
        "# plt.show()\n",
        "\n",
        "def simplexmask1d(seq=512, ctx_scale=(.85,1), trg_scale=(.6,.8), B=64, chaos=[1,.5]):\n",
        "    i = np.linspace(0, chaos[0], num=seq) # 2-5\n",
        "    noise = opensimplex.noise2array(i, np.random.randint(1e10, size=B)) # [B, seq]\n",
        "    noise = torch.from_numpy(noise)\n",
        "    ctx_mask_scale = torch.rand(1) * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    trg_mask_scale = torch.rand(1) * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "\n",
        "    ctx_len, trg_len = int(seq*ctx_mask_scale), int(seq*trg_mask_scale)\n",
        "    val, trg_index = torch.topk(noise, trg_len, dim=1, sorted=False)\n",
        "    ctx_len = ctx_len - trg_len\n",
        "\n",
        "    remove_mask = torch.ones((B,seq), dtype=bool) # [B, S]\n",
        "    remove_mask.scatter_(1, trg_index, False).flatten()\n",
        "    ind = torch.arange(seq).unsqueeze(0).repeat(B,1)[remove_mask].reshape(B, -1)\n",
        "    print(ind.shape, ind)\n",
        "\n",
        "    i = np.linspace(0, chaos[1], num=seq) # 2-5\n",
        "    noise = opensimplex.noise2array(i, np.random.randint(1e10, size=B)) # [B, seq]\n",
        "    noise = torch.from_numpy(noise)[remove_mask].reshape(B, -1)\n",
        "    val, ctx_ind = torch.topk(noise, ctx_len, dim=1, sorted=False)\n",
        "    print(ctx_ind.shape, ctx_ind)\n",
        "\n",
        "\n",
        "    ctx_index = ind[torch.arange(B).unsqueeze(-1), ctx_ind]\n",
        "    return ctx_index, trg_index\n",
        "\n",
        "\n",
        "# mask = simplexmask(hw=(8,8), scale=(.6,.8))\n",
        "# index, mask = simplexmask1d(seq=100, scale=(.7,.8))\n",
        "# ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.85,1), trg_scale=(.6,.8), B=64, chaos=5)\n",
        "# ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.85,1), trg_scale=(.7,.8), B=64, chaos=2)\n",
        "# ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.7,.9), trg_scale=(.6,.7), B=64, chaos=5)\n",
        "# ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.2,.3), trg_scale=(.4,.5), B=64, chaos=3)\n",
        "# ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.1,.3), trg_scale=(.4,.6), B=64, chaos=3)\n",
        "# # print(trg_index[0], ctx_index[0])\n",
        "\n",
        "ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.85,1), trg_scale=(.7,.8), B=64, chaos=[3,.5])\n",
        "mask = torch.zeros(b ,200)\n",
        "mask[torch.arange(b).unsqueeze(-1), trg_index] = 1\n",
        "mask[torch.arange(b).unsqueeze(-1), ctx_index] = .5\n",
        "mask = mask[None,...]\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "imshow(mask)\n",
        "import torchvision\n",
        "imshow(torchvision.utils.make_grid(mask, nrow=8))\n",
        "\n",
        "# print(index)\n",
        "# print(index.shape)\n",
        "# print(mask)\n",
        "# print(mask.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwpnHW4wn9S1",
        "outputId": "40413ecd-7f4b-49a2-984b-fe812ec57e26"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:08<00:00, 19.5MB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title data\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "transform = transforms.Compose([transforms.ToTensor(),])\n",
        "\n",
        "# MNIST CIFAR10\n",
        "train_data = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
        "test_data = torchvision.datasets.CIFAR10(root='data', train=False, download=True, transform=transform)\n",
        "batch_size = 128 # 128 1024\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    # print(npimg.shape) # (3, 64, 64)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# dataiter = iter(train_data)\n",
        "# x,y = next(dataiter)\n",
        "# print(x.shape) # [3, 32, 32]\n",
        "# imshow(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AtPUcVu2kSLI"
      },
      "outputs": [],
      "source": [
        "# @title TransformerClassifier\n",
        "import math\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class TransformerClassifier(nn.Module):\n",
        "    # def __init__(self, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, drop = 0.):\n",
        "    def __init__(self, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, drop=0):\n",
        "        super().__init__()\n",
        "        # self.embedding = nn.Embedding(ntoken, d_model)\n",
        "        self.pos_encoder = RoPE(d_model, seq_len=200, base=10000)\n",
        "        # d_hid = d_hid or d_model#*2\n",
        "        # encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, d_hid, drop, batch_first=True)\n",
        "        # self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "\n",
        "        out_dim = out_dim or d_model\n",
        "        self.lin = nn.Linear(d_model, out_dim)\n",
        "        # self.lin = nn.Linear(d_model*2, out_dim)\n",
        "        # self.cls = nn.Parameter(torch.randn(1,1,d_model))\n",
        "        self.attention_pool = nn.Linear(d_model, 1, bias=False)\n",
        "        # self.out = nn.Sequential(\n",
        "        #     nn.Dropout(drop), nn.Linear(d_model, 3), nn.SiLU(),\n",
        "        #     nn.Dropout(drop), nn.Linear(3, out_dim), nn.Sigmoid()\n",
        "        # )\n",
        "\n",
        "    def forward(self, x, src_key_padding_mask = None): # [batch, seq, d_model], [batch, seq] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # batch, seq_len, d_model = x.shape\n",
        "        # x = torch.cat([self.cls.repeat(batch,1,1), x], dim=1)\n",
        "        # src_key_padding_mask = torch.cat([torch.zeros((batch, 1), dtype=torch.bool), src_key_padding_mask], dim=1)\n",
        "        x = self.pos_encoder(x)\n",
        "\n",
        "        x = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
        "        # print(\"fwd\",out.shape) # float [batch, seq_len, d_model]\n",
        "        # out = x.mean(dim=1) # average pool\n",
        "        # out = x.max(dim=1)#[0]\n",
        "\n",
        "        attn = self.attention_pool(x).squeeze(-1) # [batch, seq] # seq_pool\n",
        "        out = (torch.softmax(attn, dim=-1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, seq] @ [batch, seq, dim] -> [batch, dim]\n",
        "        # out = self.out(out) # optional (from GlobalContext) like squeeze excitation\n",
        "\n",
        "        # out = out[:, 0] # first token\n",
        "        # out = torch.cat([out, mean_pool], dim=-1)\n",
        "\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "batch, seq_len, d_model = 4,7,512\n",
        "model = TransformerModel(in_dim, d_model, d_head=4, nlayers=2, dropout=0.).to(device)\n",
        "# model = TransformerClassifier(d_model, nhead=8, nlayers=2, dropout=0.).to(device)\n",
        "x =  torch.rand(batch, seq_len, d_model)\n",
        "src_key_padding_mask = torch.stack([(torch.arange(seq_len) < seq_len - v) for v in torch.randint(seq_len, (batch,))]) # True will be ignored # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "print(src_key_padding_mask)\n",
        "out = model(x, src_key_padding_mask)\n",
        "print(out.shape)\n",
        "\n",
        "# GlobalAveragePooling1D layer, followed by a Dense layer. The final output of the transformer is produced by a softmax layer,\n",
        "# x = GlobalAveragePooling1D()(x)\n",
        "# x = Dropout(0.1)(x)\n",
        "# x = Dense(20, activation=\"relu\")(x)\n",
        "# x = Dropout(0.1)(x)\n",
        "# outputs = Dense(2, activation=\"softmax\")(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-8i1WLsvH_vn"
      },
      "outputs": [],
      "source": [
        "# @title Transformer Model\n",
        "import torch\n",
        "from torch import nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        # self.embed = nn.Sequential(\n",
        "        #     nn.Linear(in_dim, d_model), #act,\n",
        "        #     # nn.Linear(d_model, d_model), act,\n",
        "        # )\n",
        "        self.embed = nn.Linear(in_dim, d_model) if in_dim != d_model else None\n",
        "        # self.embed = nn.Sequential(nn.Conv1d(in_dim,d_model,7,2,7//2), nn.MaxPool1d(3, 2, 3//2))\n",
        "        self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, d_hid or d_model, dropout, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.d_model = d_model\n",
        "        self.cls = nn.Parameter(torch.zeros(1,1,d_model)) # randn\n",
        "        out_dim = out_dim or d_model\n",
        "        # self.lin = nn.Linear(d_model, out_dim)\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim != d_model else None\n",
        "        self.norm = nn.LayerNorm(out_dim)\n",
        "\n",
        "\n",
        "    def forward(self, src, src_key_padding_mask=None, cls_mask=None, context_indices=None, trg_indices=None): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # if cls_mask != None: src[cls_mask] = self.cls.to(src.dtype)\n",
        "\n",
        "        if self.embed != None: src = self.embed(src) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = src.shape\n",
        "        # src = self.pos_encoder(src)\n",
        "        if context_indices != None:\n",
        "            # print(src.shape, context_indices.shape, self.pos_encoder(context_indices).shape) # [2, 88, 32]) torch.Size([88]) torch.Size([88, 32]\n",
        "            # print(src[0], self.pos_encoder(context_indices)[0])\n",
        "            src = src * self.pos_encoder(context_indices) # context/predictor # src = src + self.positional_emb[:,context_indices]\n",
        "            # print(src[0])\n",
        "        else: src = src * self.pos_encoder(torch.arange(seq, device=device)) # target # src = src + self.positional_emb[:,:seq]\n",
        "            # print(\"trans fwd\", src.shape, self.pos_encoder(src).shape)\n",
        "\n",
        "        if trg_indices != None: # [M, num_trg_toks]\n",
        "            pred_tokens = self.cls * self.pos_encoder(trg_indices) # [M, num_trg_toks, d_model] # pred_tokens = self.cls + self.positional_emb[0,trg_indices]\n",
        "            pred_tokens = pred_tokens.repeat(batch, 1, 1) # [batch*M, num_trg_toks, d_model]\n",
        "            # print(pred_tokens.requires_grad)\n",
        "            src = src.repeat_interleave(trg_indices.shape[0], dim=0) # [batch, seq_len, d_model] -> [batch*M, seq_len, d_model]\n",
        "            src = torch.cat([src, pred_tokens], dim=1) # [batch*M, seq_len+num_trg_toks, d_model]\n",
        "\n",
        "        out = self.transformer_encoder(src, src_key_padding_mask=src_key_padding_mask) # float [seq_len, batch_size, d_model]\n",
        "        if trg_indices != None:\n",
        "            # print(out.shape)\n",
        "            out = out[:,seq:] # [batch*M, num_trg_toks, d_model]\n",
        "        if self.lin != None: out = self.lin(out)\n",
        "        out = self.norm(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "batch, seq_len, d_model = 4,7,64\n",
        "in_dim = 3\n",
        "model = TransformerModel(in_dim, d_model, nhead=8, nlayers=2, dropout=0.).to(device)\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "# print(out.shape)\n",
        "# # print(out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeazTakLj_Nn"
      },
      "source": [
        "## visualise mask distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37BYpacmIcw1"
      },
      "outputs": [],
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (4,4)\n",
        "# plt.plot(ttc,ttt)\n",
        "plt.scatter(ttc,ttt)\n",
        "plt.xlabel('context masks')\n",
        "plt.ylabel('target masks')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58nNtUss4YFX"
      },
      "outputs": [],
      "source": [
        "ttc=[]\n",
        "ttt=[]\n",
        "def mean(x): return sum(x)/len(x)\n",
        "\n",
        "for i in range(1000):\n",
        "    # collated_masks_enc, collated_masks_pred = mask_collator(1)\n",
        "    # context_indices, trg_indices = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "    # context_indices, trg_indices = simplexmask1d(seq=200, ctx_scale=(.8,1), trg_scale=(.2,.8), B=1, chaos=[3,.5])\n",
        "    context_indices, trg_indices = simplexmask1d(seq=200, ctx_scale=(.85,1), trg_scale=(.7,.8), B=1, chaos=[1,.5])\n",
        "    ttc.append(context_indices.shape[-1])\n",
        "    ttt.append(trg_indices.shape[-1])\n",
        "\n",
        "print(mean(ttc), mean(ttt))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"figure.figsize\"] = (4,4)\n",
        "plt.hist(ttc, bins=20, alpha=.5, label='context mask')\n",
        "plt.hist(ttt, bins=20, alpha=.5, label='target mask')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## snake, lion"
      ],
      "metadata": {
        "id": "-CbC0ghOGgzf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "n9a9OwgKjTUP"
      },
      "outputs": [],
      "source": [
        "# @title snake\n",
        "# https://github.com/Aria-K-Alethia/BigCodec/blob/main/vq/activations.py\n",
        "# https://github.com/zhenye234/X-Codec-2.0/blob/main/vq/activations.py#L62\n",
        "# Implementation adapted from https://github.com/EdwardDixon/snake under the MIT license.\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# class Snake(nn.Module):\n",
        "#     def __init__(self, in_features, alpha=1.0, alpha_logscale=False):\n",
        "#         super().__init__()\n",
        "#         # self.in_features = in_features\n",
        "#         self.alpha_logscale = alpha_logscale\n",
        "#         if self.alpha_logscale: # log scale alphas initialized to zeros\n",
        "#             self.alpha = nn.Parameter(torch.zeros(in_features) * alpha)\n",
        "#         else: # linear scale alphas initialized to ones\n",
        "#             self.alpha = nn.Parameter(torch.ones(in_features) * alpha)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         alpha = self.alpha.unsqueeze(0).unsqueeze(-1) # line up with x to [B, C, T]\n",
        "#         if self.alpha_logscale:\n",
        "#             alpha = torch.exp(alpha)\n",
        "#         x = x + (1.0 / (alpha + 1e-9)) * torch.pow(torch.sin(x * alpha), 2) # Snake ∶= x + 1/a * sin^2(ax)\n",
        "#         return x\n",
        "\n",
        "\n",
        "class Snake(nn.Module):\n",
        "    def __init__(self, dim, *args, **kwargs):\n",
        "        super().__init__()\n",
        "        # self.alpha = nn.Parameter(torch.zeros(1,dim,1)).exp() # alpha_logscale=True\n",
        "        self.alpha = nn.Parameter(torch.ones(1,dim,1)*1.) # 1.\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + (1.0 / (self.alpha + 1e-9)) * torch.pow(torch.sin(x * self.alpha), 2) # Snake ∶= x + 1/a * sin^2(ax)\n",
        "\n",
        "\n",
        "@torch.jit.script\n",
        "def snake(x, alpha): # [b,c,t], [1,c,1]\n",
        "    return x + (alpha + 1e-9).reciprocal() * torch.sin(alpha * x).pow(2) # [b,c,t]\n",
        "\n",
        "class Snake1d(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.alpha = nn.Parameter(torch.zeros(1,dim,1)).exp()\n",
        "        self.alpha = nn.Parameter(torch.ones(1,dim,1)*1.) # 1.\n",
        "\n",
        "    def forward(self, x):\n",
        "        return snake(x, self.alpha)\n",
        "\n",
        "\n",
        "class SnakeBeta(nn.Module):\n",
        "    def __init__(self, in_features, alpha=1.0, alpha_logscale=False):\n",
        "        super().__init__()\n",
        "        # self.in_features = in_features\n",
        "        self.alpha_logscale = alpha_logscale\n",
        "        if self.alpha_logscale: # log scale alphas initialized to zeros\n",
        "            self.alpha = nn.Parameter(torch.zeros(in_features) * alpha)\n",
        "            self.beta = nn.Parameter(torch.zeros(in_features) * alpha)\n",
        "        else: # linear scale alphas initialized to ones\n",
        "            self.alpha = nn.Parameter(torch.ones(in_features) * alpha)\n",
        "            self.beta = nn.Parameter(torch.ones(in_features) * alpha)\n",
        "\n",
        "    def forward(self, x): # [b,c,t]\n",
        "        alpha = self.alpha.unsqueeze(0).unsqueeze(-1) # line up with x to [B, C, T]\n",
        "        beta = self.beta.unsqueeze(0).unsqueeze(-1)\n",
        "        if self.alpha_logscale:\n",
        "            alpha = torch.exp(alpha)\n",
        "            beta = torch.exp(beta)\n",
        "        x = x + (1. / (beta + 1e-9)) * pow(torch.sin(x * alpha), 2) # SnakeBeta ∶= x + 1/b *sin^2(ax)\n",
        "        return x # [b,c,t]\n",
        "\n",
        "b,c,t = 5,16,7\n",
        "# a1 = Snake(c)\n",
        "a1 = Snake(c, alpha_logscale=True) # 70.4 µs 69.9\n",
        "# a1 = Snake1d(c) # 47.8 µs 48.3\n",
        "# a1 = SnakeBeta(256)\n",
        "x = torch.randn(b,c,t)\n",
        "# x = a1(x)\n",
        "# print(x.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "MXZJBLF3D2Cx"
      },
      "outputs": [],
      "source": [
        "# @title swwish\n",
        "@torch.jit.script\n",
        "def learntswwish(x, alpha): # [b,c,t], [1,c,1]\n",
        "    # print('alpha', alpha.shape, x.shape)\n",
        "    # alpha = alpha.exp()\n",
        "    return .5 * (1 + x - torch.cos(alpha * x)) # [b,c,t]\n",
        "    # return .5 * (1/alpha + x - torch.cos(alpha * x)/alpha) # [b,c,t]\n",
        "    # return .5 * (1/alpha + x - torch.cos(1.25*alpha * x)/alpha) # [b,c,t]\n",
        "\n",
        "class LearntSwwish(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.alpha = nn.Parameter(torch.zeros(1,dim,1))#.exp()\n",
        "        self.alpha = nn.Parameter(torch.ones(1,dim,1)*1.) # 1.\n",
        "        self.alpha = nn.Parameter(torch.zeros(dim))#.exp()\n",
        "        # self.alpha = nn.Parameter(torch.randn(dim).abs()*4)\n",
        "        self.alpha = nn.Parameter(torch.randn(dim,1)*30) #4 20\n",
        "        # self.alpha = nn.Parameter(torch.ones(1,dim)*1.) # 1.\n",
        "\n",
        "    def forward(self, x):\n",
        "        return learntswwish(x, self.alpha)\n",
        "\n",
        "class Swwish(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # return .5 * (1 + x - x.cos())\n",
        "        return .5 * (1 + x - 1.25*x.cos())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DmqUKWp03pKH"
      },
      "outputs": [],
      "source": [
        "# @title lion optim\n",
        "# https://github.com/google/automl/blob/master/lion/lion_pytorch.py\n",
        "import torch\n",
        "from torch.optim.optimizer import Optimizer\n",
        "\n",
        "class Lion(Optimizer):\n",
        "    def __init__(self, params, lr=1e-4, betas=(0.9, 0.99), weight_decay=0.0):\n",
        "        defaults = dict(lr=lr, betas=betas, weight_decay=weight_decay)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        # closure (callable, optional): A closure that reevaluates the model and returns the loss.\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                # Perform stepweight decay\n",
        "                p.data.mul_(1 - group['lr'] * group['weight_decay'])\n",
        "\n",
        "                grad = p.grad\n",
        "                state = self.state[p]\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    # Exponential moving average of gradient values\n",
        "                    state['exp_avg'] = torch.zeros_like(p)\n",
        "                exp_avg = state['exp_avg']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                # Weight update\n",
        "                update = exp_avg * beta1 + grad * (1 - beta1)\n",
        "                p.add_(update.sign_(), alpha=-group['lr'])\n",
        "                exp_avg.mul_(beta2).add_(grad, alpha= 1-beta2) # Decay the momentum running average coefficient\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rbdiTTOg5X5b"
      },
      "outputs": [],
      "source": [
        "# @title lucidrains/lion-pytorch\n",
        "# https://github.com/lucidrains/lion-pytorch/blob/main/lion_pytorch/lion_pytorch.py\n",
        "import torch\n",
        "from torch.optim.optimizer import Optimizer\n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def update_fn(p, grad, exp_avg, lr, wd, beta1, beta2):\n",
        "    # stepweight decay\n",
        "    p.data.mul_(1. - lr * wd)\n",
        "    # weight update\n",
        "    update = exp_avg.clone().mul_(beta1).add(grad, alpha = 1. - beta1).sign_()\n",
        "    p.add_(update, alpha = -lr)\n",
        "    # decay the momentum running average coefficient\n",
        "    exp_avg.mul_(beta2).add_(grad, alpha = 1. - beta2)\n",
        "\n",
        "class Lion(Optimizer):\n",
        "    def __init__(self, params, lr = 1e-4, betas = (0.9, 0.99), weight_decay=0.0, decoupled_weight_decay = False):\n",
        "        assert lr > 0.\n",
        "        assert all([0. <= beta <= 1. for beta in betas])\n",
        "        self._init_lr = lr\n",
        "        self.decoupled_wd = decoupled_weight_decay\n",
        "        defaults = dict(lr = lr, betas = betas, weight_decay = weight_decay)\n",
        "        super().__init__(params, defaults)\n",
        "        self.update_fn = update_fn\n",
        "        # from lion_pytorch.triton import update_fn as triton_update_fn # https://github.com/lucidrains/lion-pytorch/blob/main/lion_pytorch/triton.py\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure = None):\n",
        "        loss = None\n",
        "        if exists(closure):\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "        for group in self.param_groups:\n",
        "            for p in filter(lambda p: exists(p.grad), group['params']):\n",
        "                grad, lr, wd, beta1, beta2, state, decoupled_wd, init_lr = p.grad, group['lr'], group['weight_decay'], *group['betas'], self.state[p], self.decoupled_wd, self._init_lr\n",
        "                # maybe decoupled weight decay\n",
        "                if decoupled_wd:\n",
        "                    wd /= init_lr\n",
        "                # init state - exponential moving average of gradient values\n",
        "                if len(state) == 0:\n",
        "                    state['exp_avg'] = torch.zeros_like(p)\n",
        "                exp_avg = state['exp_avg']\n",
        "                self.(p, grad, exp_avg, lr, wd, beta1, beta2)\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "_3GJBPfEmAXp"
      },
      "outputs": [],
      "source": [
        "# @title StableAdamW\n",
        "# https://github.com/guojiajeremy/StableAdamW/blob/master/stableadamw.py\n",
        "# https://github.com/warner-benjamin/optimi/blob/main/optimi/stableadamw.py\n",
        "# https://github.com/pytorch/pytorch/blob/main/torch/optim/adamw.py\n",
        "\n",
        "import math\n",
        "import torch\n",
        "from torch.optim.optimizer import Optimizer\n",
        "\n",
        "\n",
        "class StableAdamW(Optimizer):\n",
        "    r\"\"\"Implements AdamW algorithm.\n",
        "\n",
        "    The original Adam algorithm was proposed in `Adam: A Method for Stochastic Optimization`_.\n",
        "    The AdamW variant was proposed in `Decoupled Weight Decay Regularization`_.\n",
        "\n",
        "    Arguments:\n",
        "        params (iterable): iterable of parameters to optimize or dicts defining\n",
        "            parameter groups\n",
        "        lr (float, optional): learning rate (default: 1e-3)\n",
        "        betas (Tuple[float, float], optional): coefficients used for computing\n",
        "            running averages of gradient and its square (default: (0.9, 0.999))\n",
        "        eps (float, optional): term added to the denominator to improve\n",
        "            numerical stability (default: 1e-8)\n",
        "        weight_decay (float, optional): weight decay coefficient (default: 1e-2)\n",
        "        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n",
        "            algorithm from the paper `On the Convergence of Adam and Beyond`_\n",
        "            (default: False)\n",
        "\n",
        "    .. _Adam\\: A Method for Stochastic Optimization:\n",
        "        https://arxiv.org/abs/1412.6980\n",
        "    .. _Decoupled Weight Decay Regularization:\n",
        "        https://arxiv.org/abs/1711.05101\n",
        "    .. _On the Convergence of Adam and Beyond:\n",
        "        https://openreview.net/forum?id=ryQu7f-RZ\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
        "                 weight_decay=1e-2, amsgrad=False, clip_threshold: float = 1.0\n",
        "                 ):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "        if not 0.0 <= weight_decay:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
        "                        weight_decay=weight_decay, amsgrad=amsgrad, clip_threshold=clip_threshold\n",
        "                        )\n",
        "        super(StableAdamW, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(StableAdamW, self).__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('amsgrad', False)\n",
        "\n",
        "    def _rms(self, tensor: torch.Tensor) -> float:\n",
        "        return tensor.norm(2) / (tensor.numel() ** 0.5)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                # Perform stepweight decay\n",
        "                p.data.mul_(1 - group['lr'] * group['weight_decay'])\n",
        "\n",
        "                # Perform optimization step\n",
        "                grad = p.grad\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
        "                amsgrad = group['amsgrad']\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    # Exponential moving average of gradient values\n",
        "                    state['exp_avg'] = torch.zeros_like(p)  # , memory_format=torch.preserve_format)\n",
        "                    # Exponential moving average of squared gradient values\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p)  # , memory_format=torch.preserve_format)\n",
        "                    if amsgrad:\n",
        "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
        "                        state['max_exp_avg_sq'] = torch.zeros_like(p)  # , memory_format=torch.preserve_format)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                if amsgrad:\n",
        "                    max_exp_avg_sq = state['max_exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                state['step'] += 1\n",
        "                bias_correction1 = 1 - beta1 ** state['step']\n",
        "                bias_correction2 = 1 - beta2 ** state['step']\n",
        "\n",
        "                # Decay the first and second moment running average coefficient\n",
        "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
        "                if amsgrad:\n",
        "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
        "                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n",
        "                    # Use the max. for normalizing running avg. of gradient\n",
        "                    denom = (max_exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n",
        "\n",
        "                else:\n",
        "                    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n",
        "\n",
        "                lr_scale = grad / denom\n",
        "                lr_scale = max(1.0, self._rms(lr_scale) / group[\"clip_threshold\"])\n",
        "                step_size = group['lr'] / bias_correction1 / (lr_scale)\n",
        "                p.data.addcdiv_(exp_avg, denom, value=-step_size)\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u99QqyJCqp_P"
      },
      "source": [
        "## violet vicreg rankme"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "m4rj4LfPuN1H"
      },
      "outputs": [],
      "source": [
        "# @title TransformerVICReg\n",
        "import torch\n",
        "from torch import nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class TransformerVICReg(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, n_heads=4, d_hid=None, nlayers=1, drop=0):\n",
        "        super().__init__()\n",
        "        act = nn.GELU()\n",
        "        # patch_size=4\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Linear(in_dim, d_model), act\n",
        "            # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(3,2, 3//2),\n",
        "            nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2),\n",
        "            # nn.Conv1d(in_dim, d_model, patch_size, patch_size), # patch\n",
        "            )\n",
        "        self.pos_enc = RoPE(d_model, seq_len=200, base=10000) # 10000\n",
        "        # self.pos_emb = nn.Parameter(torch.randn(1, 200, d_model))\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=200, base=10000).unsqueeze(0), requires_grad=False)\n",
        "\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, n_heads=n_heads) for _ in range(nlayers)])\n",
        "\n",
        "        # out_dim = out_dim or d_model\n",
        "        self.lin = nn.Linear(d_model, out_dim)\n",
        "        self.attn_pool = nn.Linear(d_model, 1, bias=False)\n",
        "\n",
        "        dim_v = d_model * 4\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(out_dim, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v, bias=False),\n",
        "            )\n",
        "\n",
        "    def forward(self, x): # [b,t,d] / [b,c,h,w]\n",
        "        # x = self.embed(x).flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c]\n",
        "        x = self.embed(x.transpose(-2,-1)).transpose(-2,-1) # [b,t,d]\n",
        "        # x = self.embed(x)\n",
        "        x = self.pos_enc(x)\n",
        "        # x = x + self.pos_emb[:,:x.shape[1]]\n",
        "\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        attn = self.attn_pool(x).squeeze(-1) # [batch, seq] # seq_pool\n",
        "        out = (torch.softmax(attn, dim=-1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, seq] @ [batch, seq, dim] -> [batch, dim]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch, ntoken]\n",
        "\n",
        "    def expand(self, x):\n",
        "        sx = self.forward(x)\n",
        "        vx = self.exp(sx)\n",
        "        return vx\n",
        "\n",
        "batch, seq_len = 4,3500\n",
        "in_dim, d_model, out_dim=16,64,16\n",
        "model = TransformerVICReg(in_dim, d_model, out_dim, n_heads=8, nlayers=1, drop=0.).to(device)\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "# x =  torch.rand((batch, in_dim, 32,32), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObiHp-LSuRBA",
        "outputId": "c6ce8734-e0a0-47b2-b11d-e0f8e53af582"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "in vicreg  1.9939420276162247e-16 24.746832251548767 1.6621681808715039e-09\n",
            "(tensor(7.9758e-18, device='cuda:0', grad_fn=<MseLossBackward0>), tensor(0.9899, device='cuda:0', grad_fn=<AddBackward0>), tensor(1.6622e-09, device='cuda:0', grad_fn=<AddBackward0>))\n"
          ]
        }
      ],
      "source": [
        "# @title Violet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class Violet(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, nlayers=2, n_heads=4):\n",
        "        super().__init__()\n",
        "        out_dim = out_dim or d_model\n",
        "        self.student = TransformerVICReg(in_dim, d_model, out_dim=out_dim, n_heads=n_heads, nlayers=nlayers, drop=0.)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "\n",
        "        # vicreg\n",
        "        self.sim_coeff=25. # 10.0 # 25.0 # λ\n",
        "        self.std_coeff=25. # 1.0 # 25.0 # µ\n",
        "        self.cov_coeff=1. # 25.0 # 1.0 # ν\n",
        "\n",
        "        # self.transform = RandomResizedCrop1d(3500, scale=(.8,1.))\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]c/ [b,c,h,w]\n",
        "        # print(x.shape)\n",
        "        # self.transform(x)\n",
        "        vx = self.student.expand(x) # [batch, num_context_toks, out_dim]\n",
        "        with torch.no_grad(): vy = self.teacher.expand(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "        loss = self.vicreg(vx, vy)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        return self.student(x)\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size, num_features = x.shape\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = self.sim_coeff * repr_loss + self.std_coeff * std_loss + self.cov_coeff * cov_loss\n",
        "        print(\"in vicreg \",self.sim_coeff * repr_loss.item() , self.std_coeff * std_loss.item() , self.cov_coeff * cov_loss.item())\n",
        "        # return loss\n",
        "        return repr_loss, std_loss, cov_loss\n",
        "\n",
        "# lr1e-3\n",
        "# n_heads=8 < 4\n",
        "\n",
        "in_dim=3\n",
        "violet = Violet(in_dim, d_model=64, out_dim=16, nlayers=1, n_heads=8).to(device)\n",
        "voptim = torch.optim.AdamW(violet.parameters(), lr=1e-3) # 1e-3?\n",
        "# voptim = torch.optim.AdamW([{'params': violet.student.transformer.parameters()},\n",
        "#     {'params': violet.student.exp.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # default 1e-2\n",
        "# voptim = torch.optim.AdamW([{'params': violet.student.exp.parameters(), 'lr': 3e-3},\n",
        "#     {'params': [p for n, p in violet.named_parameters() if 'student.exp' not in n]}], lr=1e-3, weight_decay=1e-2) # default 1e-2\n",
        "\n",
        "# print(sum(p.numel() for p in violet.parameters() if p.requires_grad)) # 27584\n",
        "\n",
        "\n",
        "x = torch.rand((2,1000,in_dim), device=device)\n",
        "loss = violet.loss(x)\n",
        "# print(out.shape)\n",
        "print(loss)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(16, 18).to(device) # torch/autograd/graph.py RuntimeError: CUDA error: device-side assert triggered CUDA kernel errors\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IACKDymhit7"
      },
      "outputs": [],
      "source": [
        "# optim.param_groups[0]['lr'] = 1e-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7zghfu_jeOQk"
      },
      "outputs": [],
      "source": [
        "# @title RankMe\n",
        "# RankMe: Assessing the Downstream Performance of Pretrained Self-Supervised Representations by Their Rank jun 2023 https://arxiv.org/pdf/2210.02885\n",
        "import torch\n",
        "\n",
        "# https://github.com/Spidartist/IJEPA_endoscopy/blob/main/src/helper.py#L22\n",
        "def RankMe(Z):\n",
        "    \"\"\"\n",
        "    Calculate the RankMe score (the higher, the better).\n",
        "    RankMe(Z) = exp(-sum_{k=1}^{min(N, K)} p_k * log(p_k)),\n",
        "    where p_k = sigma_k (Z) / ||sigma_k (Z)||_1 + epsilon\n",
        "    where sigma_k is the kth singular value of Z.\n",
        "    where Z is the matrix of embeddings (N × K)\n",
        "    \"\"\"\n",
        "    # compute the singular values of the embeddings\n",
        "    # _u, s, _vh = torch.linalg.svd(Z, full_matrices=False)  # s.shape = (min(N, K),)\n",
        "    # s = torch.linalg.svd(Z, full_matrices=False).S\n",
        "    s = torch.linalg.svdvals(Z)\n",
        "    p = s / torch.sum(s, axis=0) + 1e-7\n",
        "    return torch.exp(-torch.sum(p * torch.log(p)))\n",
        "\n",
        "# Z = torch.randn(5, 3)\n",
        "# o = RankMe(Z)\n",
        "# print(o)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Pj25OrEk14pR"
      },
      "outputs": [],
      "source": [
        "# @title LiDAR stable_ssl next\n",
        "# https://github.com/rbalestr-lab/stable-ssl/blob/main/stable_ssl/monitors.py#L106\n",
        "\n",
        "def LiDAR(embeddings, eps=1e-7, delta=1e-3):\n",
        "    embeddings = embeddings.unflatten(0, (-1,2))\n",
        "    (local_n, q, d), device = embeddings.shape, embeddings.device\n",
        "\n",
        "    class_means = embeddings.mean(dim=1) # mu_x\n",
        "    grand_mean_local = class_means.mean(dim=0) # mu\n",
        "    # print(embeddings.shape, class_means.shape, grand_mean_local.shape) # [50, 2, 64], [50, 64], [64]\n",
        "\n",
        "    # local_Sb = torch.zeros(d, d, device=device)\n",
        "    # local_Sw = torch.zeros(d, d, device=device)\n",
        "\n",
        "\n",
        "    # print(diff_b.shape, diff_w.shape) # [64,1], [64,1]\n",
        "    # print(local_Sb.shape, local_Sw.shape) # [64,64], [64,64]\n",
        "\n",
        "    diff_b = (class_means - grand_mean_local).unsqueeze(-1)\n",
        "    # print(diff_b.shape)\n",
        "    # # local_Sb = (diff_b @ diff_b.T).sum()\n",
        "    local_Sb = (diff_b @ diff_b.transpose(-2,-1)).sum(0)\n",
        "    # # print(embeddings.shape, class_means.shape)\n",
        "    diff_w = (embeddings - class_means.unsqueeze(1)).reshape(-1,d,1)\n",
        "    # # print(diff_w.shape)\n",
        "    # # local_Sw = (diff_w @ diff_w.T).sum()\n",
        "    local_Sw = (diff_w @ diff_w.transpose(-2,-1)).sum(0)\n",
        "\n",
        "    # print(local_Sb.shape, local_Sw.shape)\n",
        "    S_b = local_Sb / (local_n - 1)\n",
        "    S_w = local_Sw / (local_n * (q - 1))\n",
        "    S_w += delta * torch.eye(d, device=device)\n",
        "    # print(S_w.shape, d)\n",
        "\n",
        "    eigvals_w, eigvecs_w = torch.linalg.eigh(S_w)\n",
        "    eigvals_w = torch.clamp(eigvals_w, min=eps)\n",
        "\n",
        "    invsqrt_w = (eigvecs_w * (1.0 / torch.sqrt(eigvals_w))) @ eigvecs_w.transpose(-1, -2)\n",
        "    Sigma_lidar = invsqrt_w @ S_b @ invsqrt_w\n",
        "\n",
        "    # lam, _ = torch.linalg.eigh(Sigma_lidar)\n",
        "    lam = torch.linalg.eigh(Sigma_lidar)[0]\n",
        "    # print(lam)\n",
        "    # lam = torch.clamp(lam, min=0.0)\n",
        "\n",
        "    p = lam / lam.sum() + eps\n",
        "    # print(p)\n",
        "    # p = s / torch.sum(s, axis=0) + 1e-7\n",
        "    return torch.exp(-torch.sum(p * torch.log(p)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tqJqL5Vj2vL"
      },
      "source": [
        "## mae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDJvKFcMGIa7",
        "outputId": "df899e65-2888-4fa3-b772-11c7af625a0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "38128\n",
            "torch.Size([4, 110, 16])\n"
          ]
        }
      ],
      "source": [
        "# @title mae me enc,dec\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def random_masking(length, mask_ratio, b=64):\n",
        "    noise = torch.rand(b, length)\n",
        "    len_mask = int(length * mask_ratio)\n",
        "    _, msk_ind = torch.topk(selected_probs, k=len_mask, dim=-1, sorted=False) # val, ind -> [b,len_mask]\n",
        "    _, keep_ind = torch.topk(selected_probs, k=length-len_mask, largest=False, dim=-1, sorted=False) # val, ind -> [b,len_keep]\n",
        "    return msk_ind, keep_ind\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, n_heads=4, d_hid=None, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, 200, d_model)*.02)\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=200, base=10000), requires_grad=False)\n",
        "\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, n_heads=n_heads) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.randn(1,1,d_model)*0.02) # randn zeros\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim)# if out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = x.shape\n",
        "        # x = x * self.pos_enc(context_indices)\n",
        "        # print(\"Trans pred\",x.shape, self.pos_emb[0,context_indices].shape)\n",
        "        x = x + self.pos_emb[0,context_indices]\n",
        "        # print('pred fwd', self.pos_emb[:,context_indices].shape)\n",
        "\n",
        "        # pred_tokens = self.cls * self.pos_enc(trg_indices) # [M, num_trg_toks, d_model]\n",
        "        pred_tokens = self.cls + self.pos_emb[0,trg_indices]\n",
        "        # print(\"pred fwd\", x.shape, pred_tokens.shape)\n",
        "        x = torch.cat([x, pred_tokens], dim=1) # [batch, seq_len+num_trg_toks, d_model]\n",
        "        out = self.transformer(x)\n",
        "\n",
        "        out = self.norm(out)\n",
        "        out = out[:,seq:] # [batch, num_trg_toks, d_model]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, drop=0):\n",
        "    def __init__(self, patch_size, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0):\n",
        "        super().__init__()\n",
        "        # patch_size=32\n",
        "        self.embed = nn.Sequential(\n",
        "            # # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.MaxPool1d(2,2), #nn.MaxPool1d(3, 2, 3//2),\n",
        "            # # nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.Conv1d(d_model, d_model,3,2,3//2)\n",
        "            nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2),\n",
        "            # nn.Conv1d(in_dim, d_model, patch_size, patch_size), # like patch\n",
        "\n",
        "            # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.Dropout(drop), nn.BatchNorm1d(d_model), snake,\n",
        "            # lstm\n",
        "\n",
        "            )\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, 200, d_model)*.02)\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=200, base=10000), requires_grad=False)#.unsqueeze(0)\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, n_heads=n_heads) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "        # try: print(\"Trans fwd\",x.shape, context_indices.shape)\n",
        "        # except: print(\"Trans fwd noind\",x.shape)\n",
        "        # x = self.pos_enc(x)\n",
        "        x = x + self.pos_emb[:,:x.shape[1]]\n",
        "        if context_indices != None: x = x[torch.arange(x.shape[0]).unsqueeze(-1), context_indices] # [batch, num_context_toks, d_model]\n",
        "\n",
        "        # print(\"TransformerModel\",x.shape)\n",
        "        x = self.transformer(x)\n",
        "        out = self.norm(x)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "batch, seq_len, d_model = 4,3500,16\n",
        "in_dim = 3\n",
        "patch_size=32\n",
        "model = TransformerModel(patch_size, in_dim, d_model, n_heads=4, nlayers=10, drop=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "# # # print(out)\n",
        "# model = TransformerPredictor(in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1).to(device)\n",
        "# out = model(out)\n",
        "# print(out.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PE3XAPkCZ2oM",
        "outputId": "d9395c50-9307-46bb-c035-e5bc63f2fa23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "109920\n",
            "torch.Size([])\n"
          ]
        }
      ],
      "source": [
        "# @title MAE me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class SeqJEPA(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, nlayers=2, n_heads=4):\n",
        "        super().__init__()\n",
        "        if out_dim is None: out_dim = d_model\n",
        "        self.patch_size = 32\n",
        "        self.student = TransformerModel(self.patch_size, in_dim, d_model, out_dim=out_dim, n_heads=n_heads, nlayers=nlayers, drop=0.)\n",
        "        self.predicter = TransformerPredictor(out_dim, d_model//2, out_dim, n_heads=4, nlayers=1, drop=0.)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "        # self.transform = RandomResizedCrop1d(3500, scale=(.8,1.))\n",
        "\n",
        "    def loss(self, x): # [b,t,d]\n",
        "        b,t,d = x.shape\n",
        "        msk_ind, keep_ind = random_masking(length, mask_ratio, b=b)\n",
        "\n",
        "        sx = self.student(x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "        # print('seq_jepa loss sx',sx.shape)\n",
        "        sy_ = self.predicter(sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "        sy_ = F.layer_norm(sy_, (sy_.size(-1),))\n",
        "        with torch.no_grad():\n",
        "            sy = self.teacher(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "            sy = sy[torch.arange(sy.shape[0]).unsqueeze(-1), trg_indices] # [batch, num_context_toks, d_model] # nan bec len(trg_ind)==0 # print('loss sy',torch.isnan(sy).any())\n",
        "            sy = F.layer_norm(sy, (sy.size(-1),))\n",
        "        loss = F.mse_loss(sy, sy_)\n",
        "        return loss\n",
        "\n",
        "\n",
        "msk_ind, keep_ind\n",
        "\n",
        "        sx = self.encoder(x, context_indices=keep_ind) # [batch, num_context_toks, out_dim]\n",
        "        x_ = self.decoder(sx, context_indices=keep_ind, msk_ind) # [batch, num_context_toks, out_dim]\n",
        "\n",
        "        loss = (pred - target) ** 2\n",
        "        loss = loss.mean(dim=-1)  # [N, L], mean loss per patch\n",
        "        loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches\n",
        "\n",
        "pred, target =\n",
        "        # loss = F.mse_loss(pred, target)\n",
        "\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        sx = self.student(x)\n",
        "        out = sx.mean(dim=1)\n",
        "        return out\n",
        "\n",
        "# min_s=0.15, max_s, M\n",
        "# trg.15.2M4 C.85 1\n",
        "\n",
        "# 1e-2,1e-3 < 3e-3,1e-3\n",
        "# patch16 < patch32\n",
        "# NoPE good but sus\n",
        "\n",
        "# ctx/trg sacle min/max, num blk,\n",
        "\n",
        "\n",
        "# seq_jepa = SeqJEPA(in_dim=3, d_model=32, out_dim=16, nlayers=4, n_heads=4).to(device)#.to(torch.float)\n",
        "seq_jepa = SeqJEPA(in_dim=3, d_model=64, out_dim=16, nlayers=1, n_heads=8).to(device)#.to(torch.float)\n",
        "optim = torch.optim.AdamW(seq_jepa.parameters(), lr=1e-3) # 1e-3?\n",
        "# optim = torch.optim.AdamW([{'params': seq_jepa.student.parameters()},\n",
        "#     {'params': seq_jepa.predicter.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # default 1e-2, 5e-2\n",
        "    # {'params': seq_jepa.predicter.parameters(), 'lr': 1e-2}], lr=1e-3, weight_decay=1e-2)\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/configs/in1k_vith14_ep300.yaml\n",
        "# d_model 1024,384\n",
        "# depth 12,6/12\n",
        "# wd 5e-2 - 4e-1\n",
        "# adamw 1e-4 - 1e-3 - 1e-6\n",
        "# ema 0.996-1\n",
        "\n",
        "print(sum(p.numel() for p in seq_jepa.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.parameters())) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.predicter.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.student.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.teacher.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# d_model^2 * nlayers\n",
        "\n",
        "x = torch.rand((24, 3500, in_dim), device=device)\n",
        "out = seq_jepa.loss(x)\n",
        "print(out.shape)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(16, 18).to(device)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n",
        "# optim = torch.optim.AdamW([{'params': seq_jepa.parameters()}, {'params': classifier.parameters(), 'lr': 1e-3}], lr=1e-3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ABygeAwL5N-6"
      },
      "outputs": [],
      "source": [
        "# @title facebookresearch/mae models_mae.py\n",
        "# https://github.com/facebookresearch/mae/blob/main/models_mae.py\n",
        "from functools import partial\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from timm.models.vision_transformer import PatchEmbed, Block\n",
        "\n",
        "\n",
        "class MaskedAutoencoderViT(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3,\n",
        "                 embed_dim=1024, depth=24, num_heads=16,\n",
        "                 decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
        "                 mlp_ratio=4., norm_layer=nn.LayerNorm, norm_pix_loss=False):\n",
        "        super().__init__()\n",
        "\n",
        "        # --------------------------------------------------------------------------\n",
        "        # MAE encoder specifics\n",
        "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        # self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True, qk_scale=None, norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "        # --------------------------------------------------------------------------\n",
        "\n",
        "        # --------------------------------------------------------------------------\n",
        "        # MAE decoder specifics\n",
        "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
        "        # self.decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, decoder_embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
        "\n",
        "        self.decoder_blocks = nn.ModuleList([\n",
        "            Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=True, qk_scale=None, norm_layer=norm_layer)\n",
        "            for i in range(decoder_depth)])\n",
        "\n",
        "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
        "        self.decoder_pred = nn.Linear(decoder_embed_dim, patch_size**2 * in_chans, bias=True) # decoder to patch\n",
        "        # --------------------------------------------------------------------------\n",
        "\n",
        "    def random_masking(self, x, mask_ratio):\n",
        "        \"\"\"\n",
        "        Perform per-sample random masking by per-sample shuffling.\n",
        "        Per-sample shuffling is done by argsort random noise.\n",
        "        x: [N, L, D], sequence\n",
        "        \"\"\"\n",
        "        N, L, D = x.shape  # batch, length, dim\n",
        "        len_keep = int(L * (1 - mask_ratio))\n",
        "\n",
        "        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n",
        "        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
        "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
        "\n",
        "        # keep the first subset\n",
        "        ids_keep = ids_shuffle[:, :len_keep]\n",
        "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
        "\n",
        "        # generate the binary mask: 0 is keep, 1 is remove\n",
        "        mask = torch.ones([N, L], device=x.device)\n",
        "        mask[:, :len_keep] = 0\n",
        "        # unshuffle to get the binary mask\n",
        "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
        "        return x_masked, mask, ids_restore\n",
        "\n",
        "    def forward_encoder(self, x, mask_ratio):\n",
        "        x = self.patch_embed(x)\n",
        "        x = x + self.pos_embed[:, 1:, :]\n",
        "\n",
        "        # masking: length -> length * mask_ratio\n",
        "        x, mask, ids_restore = self.random_masking(x, mask_ratio)\n",
        "\n",
        "        # append cls token\n",
        "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
        "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        for blk in self.blocks: x = blk(x)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        return x, mask, ids_restore\n",
        "\n",
        "    def forward_decoder(self, x, ids_restore):\n",
        "        x = self.decoder_embed(x)\n",
        "\n",
        "        # append mask tokens to sequence\n",
        "        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
        "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
        "        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
        "        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
        "\n",
        "        x = x + self.decoder_pos_embed\n",
        "\n",
        "        for blk in self.decoder_blocks: x = blk(x)\n",
        "        x = self.decoder_norm(x)\n",
        "        x = self.decoder_pred(x)\n",
        "        x = x[:, 1:, :]\n",
        "        return x\n",
        "\n",
        "    def forward_loss(self, imgs, pred, mask):\n",
        "        \"\"\"\n",
        "        imgs: [N, 3, H, W]\n",
        "        pred: [N, L, p*p*3]\n",
        "        mask: [N, L], 0 is keep, 1 is remove,\n",
        "        \"\"\"\n",
        "        target = self.patchify(imgs)\n",
        "        # if self.norm_pix_loss:\n",
        "        #     mean = target.mean(dim=-1, keepdim=True)\n",
        "        #     var = target.var(dim=-1, keepdim=True)\n",
        "        #     target = (target - mean) / (var + 1.e-6)**.5\n",
        "\n",
        "        loss = (pred - target) ** 2\n",
        "        loss = loss.mean(dim=-1)  # [N, L], mean loss per patch\n",
        "        # loss = F.mse_loss(pred, target)\n",
        "\n",
        "        loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches\n",
        "        return loss\n",
        "\n",
        "    def forward(self, imgs, mask_ratio=0.75):\n",
        "        latent, mask, ids_restore = self.forward_encoder(imgs, mask_ratio)\n",
        "        pred = self.forward_decoder(latent, ids_restore)  # [N, L, p*p*3]\n",
        "        loss = self.forward_loss(imgs, pred, mask)\n",
        "        return loss, pred, mask\n",
        "\n",
        "\n",
        "model = MaskedAutoencoderViT(\n",
        "    patch_size=16, embed_dim=768, depth=12, num_heads=12, # B16\n",
        "    # patch_size=16, embed_dim=1024, depth=24, num_heads=16, # L16\n",
        "    # patch_size=14, embed_dim=1280, depth=32, num_heads=16, # H14\n",
        "    decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
        "    mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## uci har + ucr uea"
      ],
      "metadata": {
        "id": "E9aP1IdNGG69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Chaolei98 chinese\n",
        "# https://github.com/Chaolei98/Baseline-with-HAR-datasets/tree/main/Pre-processing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "\n",
        "#索引1为activity_id\n",
        "#索引[4:16)/[21,33)/[38,50)分别为3个IMU的3D-acc1,3D-acc2,3D-gyro,3D-magn(共36种特征)\n",
        "loc = [1] + [*range(4,16)] + [*range(21,33)] + [*range(38,50)]\n",
        "\n",
        "def window(data, label, size, stride):\n",
        "    '''将数组data和label按照滑窗尺寸size和stride进行切割'''\n",
        "    x, y = [], []\n",
        "    for i in range(0, len(label), stride):\n",
        "        if i+size < len(label): #不足一个滑窗大小的数据丢弃\n",
        "\n",
        "            l = set(label[i:i+size])\n",
        "            if len(l) > 1 or label[i] == 0: #当一个滑窗中含有包含多种activity或者activity_id为0（即属于其他动作），丢弃\n",
        "                continue\n",
        "            elif len(l) == 1:\n",
        "                x.append(data[i: i + size, :])\n",
        "                y.append(label[i])\n",
        "\n",
        "    return x, y\n",
        "\n",
        "def generate(window_size, step):\n",
        "    '''生成训练样本X和对应标签Y'''\n",
        "    X, Y = [], []\n",
        "    # 遍历9个subject文件\n",
        "    for i in range(1, 10):\n",
        "        # total = pd.read_csv('./Protocol/subject10' + str(i) + '.dat', header=None, sep=' ', usecols=loc).values\n",
        "        total = pd.read_csv('PAMAP2_Dataset/Protocol/subject10' + str(i) + '.dat', header=None, sep=' ', usecols=loc).values\n",
        "        total = total[~np.isnan(total).any(axis=1), :]  #去除NaN\n",
        "        data = total[:, 1:]\n",
        "        label = total[:, 0].reshape(-1)\n",
        "\n",
        "        # 调用window函数进行滑窗处理\n",
        "        x, y = window(data, label, window_size, step)\n",
        "        X += x\n",
        "        Y += y\n",
        "\n",
        "    # 将索引从0开始依次编号\n",
        "    cate_idx = list(Counter(Y).keys())\n",
        "    cate_idx.sort()\n",
        "    for i in range(len(Y)):\n",
        "        Y[i] = cate_idx.index(Y[i])\n",
        "\n",
        "    return X, Y\n",
        "\n",
        "def category(X, Y):\n",
        "    '''按照种类分类动作'''\n",
        "    result = [[] for i in range(len(list(Counter(Y).keys())))]  #result对应的索引即标签\n",
        "    for step, y in enumerate(Y):\n",
        "        result[y].append(X[step])\n",
        "    return result\n",
        "\n",
        "def split(result, test_size):\n",
        "    '''划分数据集\n",
        "    test_size:测试集样本数量占比'''\n",
        "    x_train, x_test, y_train, y_test = [], [], [], []\n",
        "    for i, data in enumerate(result):\n",
        "        label = [i for n in range(len(data))]\n",
        "        x_train_, x_test_, y_train_, y_test_ = train_test_split(data, label, test_size=test_size, shuffle=True)\n",
        "        x_train.extend(x_train_)\n",
        "        y_train.extend(y_train_)\n",
        "        x_test.extend(x_test_)\n",
        "        y_test.extend(y_test_)\n",
        "    return x_train, y_train, x_test, y_test\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "X, Y = generate(171, 85)\n",
        "result = category(X, Y)\n",
        "x_train, y_train, x_test, y_test = split(result, 0.2)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, shuffle=True)\n",
        "np.save('./x_train', x_train)\n",
        "np.save('./x_test', x_test)\n",
        "np.save('./y_train', y_train)\n",
        "np.save('./y_test', y_test)\n"
      ],
      "metadata": {
        "id": "xbFkO6SE-f7F",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title UCI HAR download\n",
        "# https://archive.ics.uci.edu/static/public/240/human+activity+recognition+using+smartphones.zip\n",
        "!wget https://archive.ics.uci.edu/static/public/240/human+activity+recognition+using+smartphones.zip -O har\n",
        "!unzip har\n",
        "!unzip 'UCI HAR Dataset.zip'\n",
        "!mv 'UCI HAR Dataset' UCI_HAR_Dataset"
      ],
      "metadata": {
        "id": "LypwmdFN18_n",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title UCI HAR\n",
        "# https://github.com/arijitiiest/UCI-Human-Activity-Recognition/blob/master/Data-preprocessing.ipynb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "X_train = pd.read_csv('UCI_HAR_Dataset/train/X_train.txt', delim_whitespace=True, header=None)\n",
        "with open('UCI_HAR_Dataset/features.txt') as f: X_train.columns = [line.split()[1] for line in f.readlines()]\n",
        "y_train = pd.read_csv('UCI_HAR_Dataset/train/y_train.txt', names=['Activity']).squeeze(\"columns\")\n",
        "X_train['subject'] = pd.read_csv('UCI_HAR_Dataset/train/subject_train.txt', header=None).squeeze(\"columns\")\n",
        "# class_dict = {1:'WALKING', 2:'WALKING_UPSTAIRS', 3:'WALKING_DOWNSTAIRS', 4:'SITTING', 5:'STANDING', 6:'LAYING'}\n",
        "# y_train_labels = y_train.map(class_dict)\n",
        "X_train['Activity'] = y_train\n",
        "# train['ActivityName'] = y_train_labels\n",
        "\n",
        "X_test = pd.read_csv('UCI_HAR_Dataset/test/X_test.txt', delim_whitespace=True, header=None)\n",
        "with open('UCI_HAR_Dataset/features.txt') as f: X_test.columns = [line.split()[1] for line in f.readlines()]\n",
        "y_test = pd.read_csv('UCI_HAR_Dataset/test/y_test.txt', names=['Activity']).squeeze(\"columns\")\n",
        "X_test['subject'] = pd.read_csv('UCI_HAR_Dataset/test/subject_test.txt', header=None).squeeze(\"columns\")\n",
        "\n",
        "ans = [y for _, y in X_train.groupby(['subject', 'Activity'])]\n",
        "y_train = [df['Activity'].iloc[0] for df in ans]\n",
        "# y_train = [df['subject'].iloc[0] for df in ans]\n",
        "X_train = [df.drop(['subject', 'Activity'], axis=1) for df in ans]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEapWaI-1swh",
        "outputId": "2ada0569-8b4a-4e27-bfb8-4c322fb262f9",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-48920998f2a4>:7: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  X_train = pd.read_csv('UCI_HAR_Dataset/train/X_train.txt', delim_whitespace=True, header=None)\n",
            "<ipython-input-2-48920998f2a4>:17: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  X_test = pd.read_csv('UCI_HAR_Dataset/test/X_test.txt', delim_whitespace=True, header=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(X_train)\n",
        "# print(y_train)\n",
        "# print(X_test)\n",
        "# print(y_test)\n",
        "\n",
        "# ans = [y for _, y in X_train.groupby(['subject', 'Activity'])]\n",
        "# print(ans)\n",
        "# print(ans[125])\n",
        "# print(len(ans)) # 126\n",
        "# print([len(a) for a in ans]) # 126\n",
        "# print(min([len(a) for a in ans])) # 126\n",
        "\n",
        "# y_train = [df['Activity'].iloc[0] for df in ans]\n",
        "# # y_train = [df['subject'].iloc[0] for df in ans]\n",
        "# X_train = [df.drop(['subject', 'Activity'], axis=1) for df in ans]\n",
        "\n",
        "# print(y)\n",
        "\n",
        "# print(len(y_train))\n",
        "\n",
        "# print(ans[125].to_numpy().shape)\n",
        "print(X_train[0].shape[-1])\n",
        "\n",
        "# out = []\n",
        "# for (_, group), subdf in X_train.groupby(['subject', 'Activity']):\n",
        "#     pair = list(group)  # [col1, col2]\n",
        "#     count = len(subdf)\n",
        "#     if count == 1: out.append(pair)\n",
        "#     else: out.append([pair] * count)\n",
        "# print(out)\n"
      ],
      "metadata": {
        "id": "a_bJFZOL4Fu-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddb61c83-9d81-49b1-dad1-3f0d1bafeb7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title pandasDataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class pandasDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X, self.y = X, y\n",
        "        chars = sorted(list(set(y)))\n",
        "        self.vocab_size = len(chars) #\n",
        "    #     self.stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "    #     self.itos = {i:ch for i,ch in enumerate(chars)}\n",
        "    #     self.y = self.data_process(y) #\n",
        "        self.seq_len = min([len(a) for a in X])\n",
        "\n",
        "    # def data_process(self, data): # str\n",
        "    #     return torch.tensor([self.stoi.get(c) for c in data]) #\n",
        "\n",
        "    def __len__(self): return len(self.X)\n",
        "    # def __getitem__(self, idx): return self.X.iloc[idx].to_numpy(), self.y.iloc[idx]\n",
        "    # def __getitem__(self, idx): return self.X[idx].to_numpy(), self.y[idx]\n",
        "    def __getitem__(self, idx):\n",
        "        i = np.random.randint(0, len(self.X[idx])-self.seq_len+1)\n",
        "        return self.X[idx].to_numpy()[i:i+self.seq_len], self.y[idx]\n",
        "\n",
        "train_data = pandasDataset(X_train, y_train)\n",
        "test_data = pandasDataset(X_test, y_test)\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=2)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=2)\n",
        "\n",
        "for X, y in train_loader:\n",
        "    print(X.shape, y.shape)\n",
        "    break\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6vwmzM4qUXT",
        "outputId": "f763dd90-1f54-4916-de6d-def340983469"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 36, 561]) torch.Size([64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "E4c4r5Hkry99"
      },
      "outputs": [],
      "source": [
        "# @title tsai\n",
        "# https://timeseriesai.github.io/tsai/\n",
        "!pip install -qU tsai # 3mins\n",
        "import tsai\n",
        "from tsai.data.external import get_UCR_data, get_UCR_multivariate_list\n",
        "\n",
        "# l = get_UCR_multivariate_list()\n",
        "# print(len(l), l)\n",
        "# X_train, y_train, X_valid, y_valid = get_UCR_data(dsid) # tsai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRuBXTauj2f4",
        "outputId": "5fcd710c-a427-427c-a153-2dacb62e7687"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/374.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m368.6/374.4 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# @title tslearn\n",
        "!pip install -qU tslearn\n",
        "from tslearn.datasets import UCR_UEA_datasets # https://tslearn.readthedocs.io/en/latest/gen_modules/datasets/tslearn.datasets.UCR_UEA_datasets.html\n",
        "\n",
        "# l = UCR_UEA_datasets().list_datasets() # 30 ['ArticularyWordRecognition', 'AtrialFibrillation', 'BasicMotions', 'CharacterTrajectories', 'Cricket', 'DuckDuckGeese', 'EigenWorms', 'Epilepsy', 'EthanolConcentration', 'ERing', 'FaceDetection', 'FingerMovements', 'HandMovementDirection', 'Handwriting', 'Heartbeat', 'InsectWingbeat', 'JapaneseVowels', 'Libras', 'LSST', 'MotorImagery', 'NATOPS', 'PenDigits', 'PEMS-SF', 'Phoneme', 'RacketSports', 'SelfRegulationSCP1', 'SelfRegulationSCP2', 'SpokenArabicDigits', 'StandWalkJump', 'UWaveGestureLibrary']\n",
        "# l = UCR_UEA_datasets().list_multivariate_datasets() # same ^\n",
        "# l = UCR_UEA_datasets().list_univariate_datasets() # 0 []\n",
        "# print(len(l), l)\n",
        "\n",
        "# for dataset_name in data_loader.list_datasets(): # 30 ['ArticularyWordRecognition', 'AtrialFibrillation', 'BasicMotions', 'CharacterTrajectories', 'Cricket', 'DuckDuckGeese', 'EigenWorms', 'Epilepsy', 'EthanolConcentration', 'ERing', 'FaceDetection', 'FingerMovements', 'HandMovementDirection', 'Handwriting', 'Heartbeat', 'InsectWingbeat', 'JapaneseVowels', 'Libras', 'LSST', 'MotorImagery', 'NATOPS', 'PenDigits', 'PEMS-SF', 'Phoneme', 'RacketSports', 'SelfRegulationSCP1', 'SelfRegulationSCP2', 'SpokenArabicDigits', 'StandWalkJump', 'UWaveGestureLibrary']\n",
        "#     X_train, y_train, X_test, y_test = data_loader.load_dataset(dataset_name)\n",
        "\n",
        "# X_train, y_train, X_test, y_test = UCR_UEA_datasets().load_dataset('AtrialFibrillation') # tslearn\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ep7xumXZke5y"
      },
      "outputs": [],
      "source": [
        "# @title sktime\n",
        "!pip install -q sktime\n",
        "from sktime.datasets import load_UCR_UEA_dataset\n",
        "import numpy as np\n",
        "# https://www.sktime.net/en/v0.32.2/examples/AA_datatypes_and_datasets.html#Section-3.2.3:-time-series-classification-data-sets-from-the-UCR/UEA-time-series-classification-repository\n",
        "\n",
        "X_train, y_train = load_UCR_UEA_dataset('AtrialFibrillation')\n",
        "X_train, y_train = load_UCR_UEA_dataset('AtrialFibrillation', split=\"train\")\n",
        "X_test, y_test = load_UCR_UEA_dataset('AtrialFibrillation', split=\"test\")\n",
        "\n",
        "print(f\"Training data shape: {X_train.shape}\")\n",
        "print(f\"Training labels shape: {y_train.shape}\")\n",
        "print(f\"Testing data shape: {X_test.shape}\")\n",
        "print(f\"Testing labels shape: {y_test.shape}\")\n",
        "\n",
        "# X_train and X_test will typically be Pandas DataFrames or NumPy arrays\n",
        "# y_train and y_test will be NumPy arrays or Pandas Series\n",
        "\n",
        "# print(X_train, y_train)\n",
        "# print(X_train[0], y_train[0])\n",
        "# print(X_train)\n",
        "print(X_train.iloc[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2Gr_588txsd6"
      },
      "outputs": [],
      "source": [
        "# @title test load all datasets\n",
        "l = get_UCR_multivariate_list()\n",
        "print(len(l), l)\n",
        "for i, dataset_name in enumerate(get_UCR_multivariate_list()):\n",
        "# for dataset_name in get_UCR_multivariate_list()[15:20]:\n",
        "    print(dataset_name)\n",
        "    # if dataset_name in ['DuckDuckGeese','FaceDetection','InsectWingbeat','PEMS-SF']:\n",
        "    #     print('skip', dataset_name)\n",
        "    #     continue\n",
        "    # try: X_train, y_train, X_valid, y_valid = get_UCR_data(dataset_name) # tsai\n",
        "    # # InsectWingbeat, PEMS-SF slow\n",
        "\n",
        "    # if dataset_name in ['AtrialFibrillation', 'CharacterTrajectories','DuckDuckGeese','EigenWorms','ERing','InsectWingbeat','JapaneseVowels','SpokenArabicDigits']:\n",
        "    # # EigenWorms slow\n",
        "    #     print('skip', dataset_name)\n",
        "    #     continue\n",
        "    # try: X_train, y_train, X_test, y_test = UCR_UEA_datasets().load_dataset(dataset_name) # tslearn\n",
        "\n",
        "    if dataset_name in ['InsectWingbeat']:\n",
        "        print('skip', dataset_name)\n",
        "        continue\n",
        "    try:\n",
        "        X_train, y_train = load_UCR_UEA_dataset(dataset_name)\n",
        "        X_train, y_train = load_UCR_UEA_dataset(dataset_name, split=\"train\")\n",
        "        X_test, y_test = load_UCR_UEA_dataset(dataset_name, split=\"test\")\n",
        "    # InsectWingbeat slow oom\n",
        "\n",
        "    # who has DuckDuckGeese','FaceDetection','InsectWingbeat\n",
        "\n",
        "\n",
        "    except Exception as e: print(e); continue\n",
        "    print(dataset_name, X_train.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "EgLjldpPdSsQ"
      },
      "outputs": [],
      "source": [
        "# @title time series DataLoader\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "dataset_name = 'EthanolConcentration'\n",
        "# X_train, y_train, X_test, y_test = get_UCR_data(dataset_name) # tsai\n",
        "X_train, y_train, X_test, y_test = UCR_UEA_datasets().load_dataset(dataset_name) # tslearn\n",
        "\n",
        "\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        chars = sorted(list(set(y)))\n",
        "        self.vocab_size = len(chars) #\n",
        "        self.stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "        self.itos = {i:ch for i,ch in enumerate(chars)}\n",
        "        self.X = torch.tensor(X) # (N, 1, T)\n",
        "        self.y = self.data_process(y) #\n",
        "\n",
        "    def data_process(self, data): # str\n",
        "        return torch.tensor([self.stoi.get(c) for c in data]) #\n",
        "    def __len__(self): return len(self.y)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "train_data = TimeSeriesDataset(X_train, y_train)\n",
        "test_data = TimeSeriesDataset(X_test, y_test)\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=2)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=2)\n",
        "\n",
        "# print(X_train, y_train, X_test, y_test)\n",
        "# for x,y in train_loader:\n",
        "# # for x,y in train_data:\n",
        "#     print(x.shape, y.shape) # (261, 1751, 3)\n",
        "#     print(x, y)\n",
        "#     break\n",
        "\n",
        "# print(x.shape[-1])\n",
        "# print(X_train.shape[-1])\n",
        "# print(train_data.vocab_size) # 4\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title aeon\n",
        "# https://www.aeon-toolkit.org/en/stable/examples/datasets/load_data_from_web.html#Time-Series-Classification-Archive\n",
        "# !pip install -q aeon\n",
        "from aeon.datasets import load_classification\n",
        "\n",
        "# download from website https://www.aeon-toolkit.org/en/stable/examples/datasets/data_loading.html\n",
        "\n",
        "from aeon.datasets.tsc_datasets import multivariate, univariate\n",
        "# print(len(univariate), len(multivariate)) # 128, 30\n",
        "# print(univariate) # ['ACSF1', 'Adiac', 'AllGestureWiimoteX', 'AllGestureWiimoteY', 'AllGestureWiimoteZ', 'ArrowHead', 'Beef', 'BeetleFly', 'BirdChicken', 'BME', 'Car', 'CBF', 'Chinatown', 'ChlorineConcentration', 'CinCECGTorso', 'Coffee', 'Computers', 'CricketX', 'CricketY', 'CricketZ', 'Crop', 'DiatomSizeReduction', 'DistalPhalanxOutlineAgeGroup', 'DistalPhalanxOutlineCorrect', 'DistalPhalanxTW', 'DodgerLoopDay', 'DodgerLoopGame', 'DodgerLoopWeekend', 'Earthquakes', 'ECG200', 'ECG5000', 'ECGFiveDays', 'ElectricDevices', 'EOGHorizontalSignal', 'EOGVerticalSignal', 'EthanolLevel', 'FaceAll', 'FaceFour', 'FacesUCR', 'FiftyWords', 'Fish', 'FordA', 'FordB', 'FreezerRegularTrain', 'FreezerSmallTrain', 'Fungi', 'GestureMidAirD1', 'GestureMidAirD2', 'GestureMidAirD3', 'GesturePebbleZ1', 'GesturePebbleZ2', 'GunPoint', 'GunPointAgeSpan', 'GunPointMaleVersusFemale', 'GunPointOldVersusYoung', 'Ham', 'HandOutlines', 'Haptics', 'Herring', 'HouseTwenty', 'InlineSkate', 'InsectEPGRegularTrain', 'InsectEPGSmallTrain', 'InsectWingbeatSound', 'ItalyPowerDemand', 'LargeKitchenAppliances', 'Lightning2', 'Lightning7', 'Mallat', 'Meat', 'MedicalImages', 'MelbournePedestrian', 'MiddlePhalanxOutlineCorrect', 'MiddlePhalanxOutlineAgeGroup', 'MiddlePhalanxTW', 'MixedShapesRegularTrain', 'MixedShapesSmallTrain', 'MoteStrain', 'NonInvasiveFetalECGThorax1', 'NonInvasiveFetalECGThorax2', 'OliveOil', 'OSULeaf', 'PhalangesOutlinesCorrect', 'Phoneme', 'PickupGestureWiimoteZ', 'PigAirwayPressure', 'PigArtPressure', 'PigCVP', 'PLAID', 'Plane', 'PowerCons', 'ProximalPhalanxOutlineCorrect', 'ProximalPhalanxOutlineAgeGroup', 'ProximalPhalanxTW', 'RefrigerationDevices', 'Rock', 'ScreenType', 'SemgHandGenderCh2', 'SemgHandMovementCh2', 'SemgHandSubjectCh2', 'ShakeGestureWiimoteZ', 'ShapeletSim', 'ShapesAll', 'SmallKitchenAppliances', 'SmoothSubspace', 'SonyAIBORobotSurface1', 'SonyAIBORobotSurface2', 'StarLightCurves', 'Strawberry', 'SwedishLeaf', 'Symbols', 'SyntheticControl', 'ToeSegmentation1', 'ToeSegmentation2', 'Trace', 'TwoLeadECG', 'TwoPatterns', 'UMD', 'UWaveGestureLibraryAll', 'UWaveGestureLibraryX', 'UWaveGestureLibraryY', 'UWaveGestureLibraryZ', 'Wafer', 'Wine', 'WordSynonyms', 'Worms', 'WormsTwoClass', 'Yoga']\n",
        "# print(multivariate) # ['ArticularyWordRecognition', 'AtrialFibrillation', 'BasicMotions', 'CharacterTrajectories', 'Cricket', 'DuckDuckGeese', 'EigenWorms', 'Epilepsy', 'EthanolConcentration', 'ERing', 'FaceDetection', 'FingerMovements', 'HandMovementDirection', 'Handwriting', 'Heartbeat', 'InsectWingbeat', 'JapaneseVowels', 'Libras', 'LSST', 'MotorImagery', 'NATOPS', 'PenDigits', 'PEMS-SF', 'PhonemeSpectra', 'RacketSports', 'SelfRegulationSCP1', 'SelfRegulationSCP2', 'SpokenArabicDigits', 'StandWalkJump', 'UWaveGestureLibrary']\n",
        "\n",
        "dataset_name = 'EthanolConcentration'\n",
        "X_train, y_train = load_classification(dataset_name, split=\"train\") # https://www.aeon-toolkit.org/en/latest/api_reference/auto_generated/aeon.datasets.load_classification.html\n",
        "X_test, y_test = load_classification(dataset_name, split=\"test\")\n",
        "# [b,c,t]\n",
        "\n",
        "# print(\"Shape of X = \", X.shape)\n",
        "# print(\"First case = \", X[0][0], \" has label = \", y[0])\n",
        "\n",
        "\n",
        "# # # https://www.aeon-toolkit.org/en/stable/examples/classification/classification.html\n",
        "# from aeon.classification.convolution_based import RocketClassifier\n",
        "# rocket = RocketClassifier(n_kernels=2000)\n",
        "# rocket.fit(X_train, y_train)\n",
        "# y_pred = rocket.predict(X_test)\n",
        "# .87\n",
        "\n",
        "from aeon.classification.hybrid import HIVECOTEV2\n",
        "hc2 = HIVECOTEV2(time_limit_in_minutes=0.2) # 1min .83acc # https://www.aeon-toolkit.org/en/latest/api_reference/auto_generated/aeon.classification.hybrid.HIVECOTEV2.html\n",
        "hc2.fit(X_train, y_train)\n",
        "y_pred = hc2.predict(X_test)\n",
        "\n",
        "import torch.nn.functional as F\n",
        "# test_loss = F.cross_entropy(y_test, y_pred)\n",
        "# accuracy = (y_test==y_pred.argmax(dim=1)).sum().item()\n",
        "# accuracy = (y_test==y_pred.argmax()).sum().item()\n",
        "accuracy = (y_test==y_pred).mean().item()\n",
        "# print(y_test.shape, y_pred.shape)\n",
        "# print(y_test, y_pred)\n",
        "print(accuracy)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# https://www.aeon-toolkit.org/en/stable/examples/classification/deep_learning.html\n",
        "\n",
        "# from aeon.visualisation import plot_boxplot, plot_critical_difference\n",
        "# plot_critical_difference(results, names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sArBDLdE643V",
        "outputId": "c93a5392-871f-4917-95ff-e1593afbfe64",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.29277566539923955\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3B3t5tSmK0Fv"
      },
      "outputs": [],
      "source": [
        "# @title plot data\n",
        "# print(X_train, y_train, X_test, y_test)\n",
        "# for x,y in train_loader:\n",
        "# for x,y in train_data:\n",
        "for i, (x,y) in enumerate(train_data):\n",
        "    # print(x.shape, y.shape)\n",
        "    # print(x, y)\n",
        "    # break\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.rcParams[\"figure.figsize\"] = (8,4)\n",
        "    # plt.plot(x[:,0])\n",
        "    for j in range(x.shape[-1]):\n",
        "        plt.plot(x[:,j])\n",
        "    plt.show()\n",
        "    if i>=3: break\n",
        "\n",
        "# print(x.shape[-1])\n",
        "# print(X_train.shape[-1])\n",
        "# print(train_data.vocab_size)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpBQCgArjj7x"
      },
      "source": [
        "## text classification, roberta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkiHS6yPkaVo"
      },
      "outputs": [],
      "source": [
        "!pip install -qU datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "o4xSm_eyhSK0",
        "outputId": "23c3cd4c-7330-4292-fbee-0a1d1cef4d38"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'train_tok' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-8fcae1a8af8a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_side\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_tok' is not defined"
          ]
        }
      ],
      "source": [
        "# @title yelp data\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataset = load_dataset(\"yelp_polarity\") # yelp_polarity yelp_review_full\n",
        "# # dataset = load_dataset(\"yelp_review_full\") # yelp_polarity yelp_review_full\n",
        "# print(dataset[\"train\"][0]) # {'text': \"Unfortunately, the ... to give Dr. Goldberg 2 stars.\", 'label': 0}\n",
        "# print(len(dataset))\n",
        "# print(len(dataset[\"train\"]))\n",
        "# # train_text = dataset[\"train\"][:10]['text']\n",
        "# train_text = dataset[\"train\"]['text']\n",
        "# # train_tok = [enc.encode(text) for text in train_text]\n",
        "# train_tok = [torch.tensor(enc.encode(text)) for text in train_text]\n",
        "# # print(train_tok)\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    train_text = dataset[\"train\"]['text']\n",
        "    train_tok = [torch.tensor(enc.encode(text)) for text in train_text]\n",
        "\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "# def tokenize(example): return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\")\n",
        "# tokenized = dataset.map(tokenize, batched=True)\n",
        "# tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "\n",
        "# train_loader = DataLoader(tokenized[\"train\"], batch_size=32, shuffle=True)\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "x = pad_sequence(train_tok, batch_first=True, padding_value=0, padding_side='left')\n",
        "print(x)\n",
        "\n",
        "def left_pad(batch, pad_value=0):\n",
        "    # batch: list of 1D tensors\n",
        "    lengths = torch.tensor([len(x) for x in batch])\n",
        "    max_len = lengths.max()\n",
        "\n",
        "    # Preallocate padded tensor\n",
        "    # padded = torch.full((len(batch), max_len), pad_value, dtype=batch[0].dtype)\n",
        "    padded = torch.full((len(batch), max_len), pad_value)\n",
        "\n",
        "    # for i, x in enumerate(batch):\n",
        "    #     padded[i, -x.size(0):] = x  # align to right, pad left\n",
        "    padded[torch.arange(len(batch)).unsqueeze(-1), -lengths:] = batch\n",
        "    return padded, lengths\n",
        "\n",
        "# left_pad(train_tok)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HZejCTckoQG",
        "outputId": "8a7253cf-65bc-471e-8b90-3c1590594fc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[31373, 995] hello world\n"
          ]
        }
      ],
      "source": [
        "# @title tiktoken\n",
        "# https://github.com/openai/tiktoken/tree/main\n",
        "import tiktoken\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\") # gpt2 r50k_base p50k_base p50k_edit cl100k_base o200k_base # https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py\n",
        "# enc = tiktoken.encoding_for_model(\"gpt-4o\") # https://github.com/openai/tiktoken/blob/main/tiktoken/model.py#L24\n",
        "tok = enc.encode(\"hello world\")\n",
        "out = enc.decode(tok)\n",
        "print(tok, out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "i1iatz1SSK3s"
      },
      "outputs": [],
      "source": [
        "# @title tiktoken dataloader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import tiktoken # https://github.com/openai/tiktoken/tree/main\n",
        "\n",
        "\n",
        "# train_text = dataset[\"train\"]['text']\n",
        "# # train_tok = [enc.encode(text) for text in train_text]\n",
        "# train_tok = [torch.tensor(enc.encode(text)) for text in train_text]\n",
        "\n",
        "\n",
        "class CharDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, raw_data, seq_len):\n",
        "        # data = ''.join(raw_data)\n",
        "        # data = raw_data['text']\n",
        "        self.enc = tiktoken.get_encoding(\"gpt2\") # https://github.com/openai/tiktoken/blob/main/tiktoken/core.py\n",
        "        self.vocab_size = self.enc.n_vocab # gpt2:50257\n",
        "        self.data = self.data_process(data) # list of int\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def data_process(self, data): # str 10780437\n",
        "        return torch.tensor(self.enc.encode(data))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)//(self.seq_len+1)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        dix = self.data[idx*(self.seq_len+1) : (idx+1)*(self.seq_len+1)]\n",
        "        x, y = dix[:-1], dix[1:]\n",
        "        return x, y\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
        "\n",
        "train_data = CharDataset(dataset[\"train\"], seq_len) # one line of poem is roughly 50 characters\n",
        "\n",
        "\n",
        "seq_len = 100 # 128\n",
        "train_data = CharDataset(text, seq_len) # one line of poem is roughly 50 characters\n",
        "test_data = CharDataset(test_text, seq_len) # one line of poem is roughly 50 characters\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 64 #512\n",
        "train_loader = DataLoader(train_data, shuffle=True, pin_memory=True, batch_size=batch_size, num_workers=2) # num_workers = 4\n",
        "test_loader = DataLoader(test_data, shuffle=True, pin_memory=True, batch_size=batch_size, num_workers=0)\n",
        "\n",
        "# https://github.com/openai/tiktoken/blob/main/tiktoken/core.py\n",
        "def encode(context):\n",
        "    if type(context) == str: return torch.tensor([train_loader.dataset.enc.encode(context)], device=device)\n",
        "    elif type(context) == list: return train_loader.dataset.enc.encode_batch(context)\n",
        "    else: raise Exception\n",
        "def decode(x): return train_loader.dataset.enc.decode(list(x))\n",
        "# for x,y in train_loader:\n",
        "#     break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVHe2tXTmacN",
        "outputId": "63efc423-8a46-490e-885b-4cdb9751cc8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 31414, 232, 2]\n",
            "[0, 20920, 232, 2]\n",
            "{'input_ids': tensor([[    0, 31414,     6,   127,  2335,    16, 11962,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.94"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# @title hf roberta\n",
        "# https://huggingface.co/docs/transformers/en/model_doc/roberta\n",
        "import torch\n",
        "from transformers import AutoTokenizer, RobertaConfig, RobertaModel\n",
        "from transformers import RobertaForMaskedLM, RobertaForSequenceClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
        "print(tokenizer(\"Hello world\")[\"input_ids\"])\n",
        "print(tokenizer(\" Hello world\")[\"input_ids\"])\n",
        "# {'input_ids': tensor([[    0,   133,   812,     9,  1470,    16, 50264,     4,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
        "\n",
        "config = RobertaConfig()\n",
        "# model = RobertaModel(config)\n",
        "model = RobertaForMaskedLM(config)\n",
        "# model = RobertaForSequenceClassification(config)\n",
        "\n",
        "# inputs = tokenizer(\"The capital of France is <mask>.\", return_tensors=\"pt\")\n",
        "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
        "print(inputs)\n",
        "with torch.no_grad():\n",
        "    # logits = model(**inputs).logits\n",
        "    logits = model(**inputs)\n",
        "\n",
        "# LM: last_hidden_state, pooler_output, hidden_states=None, past_key_values=None, attentions=None, cross_attentions\n",
        "\n",
        "\n",
        "# predicted_class_id = logits.argmax().item()\n",
        "# model.config.id2label[predicted_class_id]\n",
        "\n",
        "# # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n",
        "# num_labels = len(model.config.id2label)\n",
        "# # model = RobertaForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-emotion\", num_labels=num_labels)\n",
        "# model = RobertaForSequenceClassification(config)\n",
        "# labels = torch.tensor([1])\n",
        "# loss = model(**inputs, labels=labels).loss\n",
        "# round(loss.item(), 2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # print(logits.keys())\n",
        "# # retrieve index of <mask>\n",
        "# mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
        "\n",
        "# predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
        "# tokenizer.decode(predicted_token_id)\n",
        "\n",
        "# labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\")[\"input_ids\"]\n",
        "# # mask labels of non-<mask> tokens\n",
        "# labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)\n",
        "\n",
        "# outputs = model(**inputs, labels=labels)\n",
        "# round(outputs.loss.item(), 2)\n",
        "\n",
        "# last_hidden_states = outputs.last_hidden_state\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLiCsK97Tz3H",
        "outputId": "d839e6f6-f7bf-4d24-fdc8-4a6fe602a2ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': [0, 31414, 232, 2], 'attention_mask': [1, 1, 1, 1]}\n",
            "{'input_ids': [[0, 31414, 232, 2], [0, 36807, 571, 306, 2]], 'attention_mask': [[1, 1, 1, 1], [1, 1, 1, 1, 1]]}\n"
          ]
        }
      ],
      "source": [
        "# print(logits)\n",
        "# # loss = model(**inputs, labels=labels).loss\n",
        "# loss = model(**inputs)\n",
        "\n",
        "# MaskedLMOutput = loss, logits, hidden_states=None, attentions\n",
        "# print(tokenizer(\"Hello world\")[\"input_ids\"])\n",
        "print(tokenizer(\"Hello world\")) # input_ids attention_mask\n",
        "print(tokenizer([\"Hello world\",\"dfg4\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TtKHgsmyvj5G"
      },
      "outputs": [],
      "source": [
        "# @title hf data\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "dataset = load_dataset(\"yelp_polarity\") # yelp_polarity yelp_review_full\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
        "\n",
        "import os\n",
        "def tokenize(examples): return tokenizer(examples[\"text\"], truncation=True, max_length=256)\n",
        "tok_dataset = dataset.map(tokenize, batched=True, num_proc=os.cpu_count(), # Use multiple processes for faster tokenization\n",
        "    remove_columns=[\"text\"] # Remove the original text column\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4yn7hlrBREhk"
      },
      "outputs": [],
      "source": [
        "# @title gemini roberta\n",
        "from transformers import AutoTokenizer, RobertaConfig, RobertaForMaskedLM\n",
        "\n",
        "# config = RobertaConfig() # vocab_size = 50265, hidden_size = 768, num_hidden_layers = 12, num_attention_heads = 12, intermediate_size = 3072, hidden_act = 'gelu', hidden_dropout_prob = 0.1, attention_probs_dropout_prob = 0.1, max_position_embeddings = 512, type_vocab_size = 2, initializer_range = 0.02, layer_norm_eps = 1e-12, pad_token_id = 1, bos_token_id = 0eos_token_id = 2, position_embedding_type = 'absolute'\n",
        "config = RobertaConfig(vocab_size = 50265, hidden_size = 64, num_hidden_layers = 1, num_attention_heads = 8, intermediate_size = 256, hidden_act = 'gelu', hidden_dropout_prob = 0., attention_probs_dropout_prob = 0.)\n",
        "model_mlm = RobertaForMaskedLM(config)\n",
        "\n",
        "\n",
        "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "collator = DataCollatorForLanguageModeling(tokenizer) # Masked Language Model (MLM); .15,.8,.1 # https://huggingface.co/docs/transformers/en/main_classes/data_collator#transformers.DataCollatorForLanguageModeling\n",
        "import torch\n",
        "\n",
        "train_args = TrainingArguments(\n",
        "    # output_dir=MODEL_OUTPUT_DIR_STAGE1, overwrite_output_dir=True,\n",
        "    num_train_epochs=1,\n",
        "    prediction_loss_only=True, # Only compute loss, no predictions during eval\n",
        "    # optim='adamw_torch',\n",
        "    optim='adamw_torch_fused',\n",
        "    learning_rate=3e-4,\n",
        "#     lr_scheduler_type (str or SchedulerType, optional, defaults to \"linear\") — The scheduler type to use. See the documentation of SchedulerType for all possible values.\n",
        "# lr_scheduler_kwargs (‘dict’, optional, defaults to {}) —\n",
        "    # warmup_steps=0.1 * NUM_TRAIN_EPOCHS_STAGE1 * (len(tokenized_dataset_mlm) // PER_DEVICE_BATCH_SIZE), # 10% warmup\n",
        "    warmup_ratio=0.1,\n",
        "    # weight_decay=0.01,\n",
        "    # report_to=\"tensorboard\",\n",
        "    report_to=\"wandb\", # 487a2109e55dce4e13fc70681781de9f50f27be7\n",
        "    fp16=torch.cuda.is_available(), # Enable mixed precision if GPU is available\n",
        ") # https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments\n",
        "\n",
        "# half_precision_backend=\"auto\"\n",
        "# bf16=True\n",
        "\n",
        "# trainer_mlm = Trainer(model=model_mlm, args=train_args, train_dataset=tok_dataset['train'].remove_columns(\"label\"), data_collator=collator)\n",
        "# trainer_mlm.train()\n",
        "\n",
        "# # trainer_mlm.save_model(MODEL_OUTPUT_DIR_STAGE1)\n",
        "\n",
        "# eval_results = trainer_mlm.evaluate()\n",
        "# perplexity = math.exp(eval_results[\"eval_loss\"])\n",
        "# print('perplexity', perplexity)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iISjeDVDugDQ",
        "outputId": "72490fdd-f071-4424-d149-d70e4a77dd4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 1, 0, 0]\n",
            "[[0, 16861, 6, 5, 8413, 9, 145, 925, 4, 18835, 18, 3186, 16, 10, 7230, 9, 5, 676, 38, 348, 56, 19, 98, 171, 97, 3333, 11, 14415, 480, 205, 3299, 6, 6587, 813, 4, 1437, 85, 1302, 14, 39, 813, 1622, 393, 5274, 5, 1028, 4, 1437, 85, 2333, 1239, 132, 722, 9, 6636, 1765, 7, 120, 41, 1948, 4, 1437, 3394, 34, 86, 13, 14, 50, 1072, 7, 432, 19, 24, 116, 1437, 38, 33, 422, 88, 42, 936, 19, 171, 97, 3333, 8, 38, 95, 218, 75, 120, 24, 4, 1437, 370, 33, 558, 1138, 6, 47, 33, 1484, 19, 1131, 782, 6, 596, 965, 75, 1268, 15635, 5, 1028, 116, 1437, 85, 18, 42494, 8, 45, 173, 5, 29223, 1258, 4, 1437, 85, 18, 19, 9917, 14, 38, 619, 14, 38, 33, 7, 492, 925, 4, 18835, 132, 2690, 4, 2], [0, 9325, 225, 164, 7, 925, 4, 18835, 13, 81, 158, 107, 4, 38, 206, 38, 21, 65, 9, 39, 112, 620, 1484, 77, 37, 554, 23, 24294, 21963, 4, 91, 18, 57, 372, 81, 5, 107, 8, 16, 269, 70, 59, 5, 380, 2170, 4, 85, 16, 142, 9, 123, 6, 45, 127, 122, 320, 821, 3892, 925, 4, 1190, 1529, 6, 14, 38, 303, 66, 38, 33, 19961, 1001, 7823, 4, 91, 17384, 70, 1735, 19, 47, 8, 16, 182, 3186, 8, 2969, 4, 91, 630, 75, 1679, 8, 6990, 70, 5, 235, 1142, 4, 12178, 10675, 8, 1072, 7, 28, 1682, 11, 5, 14018, 15, 358, 6659, 9, 110, 1131, 474, 8, 110, 301, 4, 2], [0, 100, 218, 75, 216, 99, 925, 4, 18835, 21, 101, 137, 1437, 1375, 7, 2605, 6, 53, 905, 162, 1137, 47, 6, 4062, 2547, 18463, 2547, 31, 42, 3299, 8, 42, 558, 4, 38, 21, 164, 7, 925, 4, 1436, 137, 37, 314, 8, 18835, 362, 81, 77, 1436, 314, 4, 91, 16, 45, 10, 10837, 3299, 4, 91, 16, 129, 2509, 11, 5, 1029, 12, 14066, 8, 519, 47, 283, 11, 13, 8456, 4885, 5622, 358, 353, 4, 91, 40, 45, 492, 4885, 5622, 8, 115, 540, 59, 1484, 18, 613, 5458, 4, 28386, 7, 120, 110, 1814, 360, 7107, 409, 15288, 20400, 149, 42, 2173, 16, 10, 8018, 4, 178, 7, 146, 3510, 190, 3007, 6, 39, 558, 813, 16, 30740, 4, 1814, 207, 9, 5, 86, 77, 47, 486, 5, 558, 6, 51, 581, 342, 47, 149, 7, 10, 2236, 7107, 6, 14, 8228, 19551, 655, 5274, 50, 2886, 110, 486, 4, 1868, 127, 4194, 408, 8, 1623, 33, 1276, 7, 989, 42, 1524, 71, 7242, 215, 8413, 4, 20, 1445, 558, 34, 41, 6784, 101, 51, 32, 608, 47, 10, 4402, 4, 12192, 162, 10, 1108, 328, 9631, 409, 31, 42, 22053, 8, 5, 1524, 4, 370, 6565, 357, 8, 51, 40, 45, 28, 89, 77, 47, 269, 240, 106, 4, 38, 33, 393, 1299, 19411, 7, 3116, 10, 1099, 1551, 59, 1268, 454, 38, 1145, 42, 31790, 10525, 13, 10, 3299, 54, 16, 70, 59, 5, 418, 4, 2], [0, 100, 437, 2410, 42, 1551, 7, 492, 47, 10, 3885, 62, 137, 47, 192, 42, 12521, 4, 20, 558, 813, 8, 942, 32, 182, 542, 23878, 4, 38, 314, 10, 1579, 19, 1533, 82, 2624, 127, 1087, 6, 8, 117, 65, 655, 373, 162, 124, 4, 38, 56, 7, 1368, 9834, 106, 7, 120, 41, 1948, 59, 127, 1087, 4, 44128, 282, 37457, 282, 32703, 6, 8, 144, 505, 6, 146, 686, 110, 1911, 16, 164, 7, 1719, 925, 4, 18835, 18, 5695, 8, 1925, 173, 4, 91, 5131, 7, 162, 14, 38, 120, 10, 2166, 6, 8, 37, 1467, 38, 21, 10, 1294, 142, 38, 174, 123, 4, 38, 300, 5, 2166, 626, 4, 6811, 6, 38, 303, 66, 127, 474, 1911, 630, 75, 582, 13, 2097, 3693, 5695, 4, 38, 829, 41, 68, 3913, 4, 612, 1087, 13, 5, 1925, 173, 4, 38, 64, 75, 582, 13, 127, 1087, 142, 38, 437, 10, 1294, 8, 218, 75, 33, 143, 1055, 3041, 23, 42, 595, 86, 4, 38, 64, 75, 679, 5, 12521, 1979, 75, 492, 162, 10, 3885, 62, 7, 146, 686, 127, 1911, 74, 1719, 173, 14, 938, 75, 2139, 8, 21, 14657, 2097, 3693, 4, 20, 558, 64, 75, 109, 932, 7, 244, 162, 1719, 5, 1087, 4, 96, 1285, 6, 5, 558, 813, 26, 5, 15, 687, 16, 15, 162, 7, 146, 686, 127, 1911, 4865, 5695, 4, 4967, 4193, 21172, 1068, 328, 2]]\n",
            "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
          ]
        }
      ],
      "source": [
        "print(tok_dataset['train']['label'][:4])\n",
        "print(tok_dataset['train']['input_ids'][:4])\n",
        "print(tok_dataset['train']['attention_mask'][:4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "gy1anCqbHN59"
      },
      "outputs": [],
      "source": [
        "# @title chatgpt roberta\n",
        "from transformers import RobertaConfig, RobertaForMaskedLM, RobertaTokenizerFast\n",
        "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")  # small for testing\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
        "def tokenize_function(example): return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
        "\n",
        "config = RobertaConfig(vocab_size=tokenizer.vocab_size, max_position_embeddings=128, num_attention_heads=8, num_hidden_layers=6, hidden_size=512, intermediate_size=2048)\n",
        "model = RobertaForMaskedLM(config)\n",
        "\n",
        "training_args = TrainingArguments(output_dir=\"./roberta-small\", overwrite_output_dir=True, num_train_epochs=5, per_device_train_batch_size=16, evaluation_strategy=\"no\", save_steps=10_000, save_total_limit=2, logging_steps=500)\n",
        "trainer = Trainer(model=model, args=training_args, train_dataset=tokenized_datasets[\"train\"], tokenizer=tokenizer, data_collator=data_collator)\n",
        "\n",
        "trainer.train()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pt98LQu2EC4K"
      },
      "source": [
        "## trash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WGmRZ_ojEFGc"
      },
      "outputs": [],
      "source": [
        "# @title facebookresearch/mae models_mae.py\n",
        "# https://github.com/facebookresearch/mae/blob/main/models_mae.py\n",
        "from functools import partial\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from timm.models.vision_transformer import PatchEmbed, Block\n",
        "from util.pos_embed import get_2d_sincos_pos_embed\n",
        "\n",
        "\n",
        "class MaskedAutoencoderViT(nn.Module):\n",
        "    \"\"\" Masked Autoencoder with VisionTransformer backbone\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3,\n",
        "                 embed_dim=1024, depth=24, num_heads=16,\n",
        "                 decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
        "                 mlp_ratio=4., norm_layer=nn.LayerNorm, norm_pix_loss=False):\n",
        "        super().__init__()\n",
        "\n",
        "        # --------------------------------------------------------------------------\n",
        "        # MAE encoder specifics\n",
        "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True, qk_scale=None, norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "        # --------------------------------------------------------------------------\n",
        "\n",
        "        # --------------------------------------------------------------------------\n",
        "        # MAE decoder specifics\n",
        "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
        "        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, decoder_embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
        "        self.decoder_blocks = nn.ModuleList([\n",
        "            Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=True, qk_scale=None, norm_layer=norm_layer)\n",
        "            for i in range(decoder_depth)])\n",
        "\n",
        "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
        "        self.decoder_pred = nn.Linear(decoder_embed_dim, patch_size**2 * in_chans, bias=True) # decoder to patch\n",
        "        # --------------------------------------------------------------------------\n",
        "        self.norm_pix_loss = norm_pix_loss\n",
        "        self.initialize_weights()\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        # initialization\n",
        "        # initialize (and freeze) pos_embed by sin-cos embedding\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=True)\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "\n",
        "        decoder_pos_embed = get_2d_sincos_pos_embed(self.decoder_pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=True)\n",
        "        self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).float().unsqueeze(0))\n",
        "\n",
        "        # initialize patch_embed like nn.Linear (instead of nn.Conv2d)\n",
        "        w = self.patch_embed.proj.weight.data\n",
        "        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
        "\n",
        "        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n",
        "        torch.nn.init.normal_(self.cls_token, std=.02)\n",
        "        torch.nn.init.normal_(self.mask_token, std=.02)\n",
        "\n",
        "        # initialize nn.Linear and nn.LayerNorm\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            # we use xavier_uniform following official JAX ViT:\n",
        "            torch.nn.init.xavier_uniform_(m.weight)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    def patchify(self, imgs): # [b,3,h,w]\n",
        "        p = self.patch_embed.patch_size[0]\n",
        "        assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0\n",
        "        h = w = imgs.shape[2] // p\n",
        "        x = imgs.reshape(imgs.shape[0], 3, h, p, w, p) # [b,3,h/p,p,w/p,p]\n",
        "        x = torch.einsum('nchpwq->nhwpqc', x) # [b,h/p,w/p,p,p,3]\n",
        "        x = x.reshape(imgs.shape[0], h * w, p**2 * 3) # [b, h/p *w/p, p*p*3]\n",
        "        return x # [b, h/p *w/p, p*p*3] ~ [b,t,d]\n",
        "\n",
        "    def unpatchify(self, x):\n",
        "        \"\"\"\n",
        "        x: (N, L, patch_size**2 *3)\n",
        "        imgs: (N, 3, H, W)\n",
        "        \"\"\"\n",
        "        p = self.patch_embed.patch_size[0]\n",
        "        h = w = int(x.shape[1]**.5)\n",
        "        assert h * w == x.shape[1]\n",
        "\n",
        "        x = x.reshape(shape=(x.shape[0], h, w, p, p, 3))\n",
        "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
        "        imgs = x.reshape(shape=(x.shape[0], 3, h * p, h * p))\n",
        "        return imgs\n",
        "\n",
        "    def random_masking(self, x, mask_ratio):\n",
        "        \"\"\"\n",
        "        Perform per-sample random masking by per-sample shuffling.\n",
        "        Per-sample shuffling is done by argsort random noise.\n",
        "        x: [N, L, D], sequence\n",
        "        \"\"\"\n",
        "        N, L, D = x.shape  # batch, length, dim\n",
        "        len_keep = int(L * (1 - mask_ratio))\n",
        "\n",
        "        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n",
        "\n",
        "        # sort noise for each sample\n",
        "        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
        "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
        "\n",
        "        # keep the first subset\n",
        "        ids_keep = ids_shuffle[:, :len_keep]\n",
        "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
        "\n",
        "        # generate the binary mask: 0 is keep, 1 is remove\n",
        "        mask = torch.ones([N, L], device=x.device)\n",
        "        mask[:, :len_keep] = 0\n",
        "        # unshuffle to get the binary mask\n",
        "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
        "\n",
        "        return x_masked, mask, ids_restore\n",
        "\n",
        "    def forward_encoder(self, x, mask_ratio):\n",
        "        # embed patches\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        # add pos embed w/o cls token\n",
        "        x = x + self.pos_embed[:, 1:, :]\n",
        "\n",
        "        # masking: length -> length * mask_ratio\n",
        "        x, mask, ids_restore = self.random_masking(x, mask_ratio)\n",
        "\n",
        "        # append cls token\n",
        "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
        "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        # apply Transformer blocks\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        return x, mask, ids_restore\n",
        "\n",
        "    def forward_decoder(self, x, ids_restore):\n",
        "        # embed tokens\n",
        "        x = self.decoder_embed(x)\n",
        "\n",
        "        # append mask tokens to sequence\n",
        "        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
        "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
        "        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
        "        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
        "\n",
        "        # add pos embed\n",
        "        x = x + self.decoder_pos_embed\n",
        "\n",
        "        # apply Transformer blocks\n",
        "        for blk in self.decoder_blocks:\n",
        "            x = blk(x)\n",
        "        x = self.decoder_norm(x)\n",
        "\n",
        "        # predictor projection\n",
        "        x = self.decoder_pred(x)\n",
        "\n",
        "        # remove cls token\n",
        "        x = x[:, 1:, :]\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward_loss(self, imgs, pred, mask):\n",
        "        \"\"\"\n",
        "        imgs: [N, 3, H, W]\n",
        "        pred: [N, L, p*p*3]\n",
        "        mask: [N, L], 0 is keep, 1 is remove,\n",
        "        \"\"\"\n",
        "        target = self.patchify(imgs)\n",
        "        if self.norm_pix_loss:\n",
        "            mean = target.mean(dim=-1, keepdim=True)\n",
        "            var = target.var(dim=-1, keepdim=True)\n",
        "            target = (target - mean) / (var + 1.e-6)**.5\n",
        "\n",
        "        loss = (pred - target) ** 2\n",
        "        loss = loss.mean(dim=-1)  # [N, L], mean loss per patch\n",
        "\n",
        "        loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches\n",
        "        return loss\n",
        "\n",
        "    def forward(self, imgs, mask_ratio=0.75):\n",
        "        latent, mask, ids_restore = self.forward_encoder(imgs, mask_ratio)\n",
        "        pred = self.forward_decoder(latent, ids_restore)  # [N, L, p*p*3]\n",
        "        loss = self.forward_loss(imgs, pred, mask)\n",
        "        return loss, pred, mask\n",
        "\n",
        "\n",
        "def mae_vit_base_patch16_dec512d8b(**kwargs):\n",
        "    model = MaskedAutoencoderViT(\n",
        "        patch_size=16, embed_dim=768, depth=12, num_heads=12,\n",
        "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
        "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def mae_vit_large_patch16_dec512d8b(**kwargs):\n",
        "    model = MaskedAutoencoderViT(\n",
        "        patch_size=16, embed_dim=1024, depth=24, num_heads=16,\n",
        "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
        "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def mae_vit_huge_patch14_dec512d8b(**kwargs):\n",
        "    model = MaskedAutoencoderViT(\n",
        "        patch_size=14, embed_dim=1280, depth=32, num_heads=16,\n",
        "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
        "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "# set recommended archs\n",
        "mae_vit_base_patch16 = mae_vit_base_patch16_dec512d8b  # decoder: 512 dim, 8 blocks\n",
        "mae_vit_large_patch16 = mae_vit_large_patch16_dec512d8b  # decoder: 512 dim, 8 blocks\n",
        "mae_vit_huge_patch14 = mae_vit_huge_patch14_dec512d8b  # decoder: 512 dim, 8 blocks\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "670p_tNSKg5i",
        "3dl1RyOjHdi1",
        "0vScvFGGSeN6",
        "oeazTakLj_Nn",
        "-CbC0ghOGgzf",
        "u99QqyJCqp_P",
        "5tqJqL5Vj2vL",
        "E9aP1IdNGG69",
        "rpBQCgArjj7x"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}