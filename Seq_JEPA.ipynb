{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0vScvFGGSeN6",
        "8DxjYI9RMQoP"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/Seq_JEPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwpnHW4wn9S1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77ff714b-55bd-4179-b3e4-c38b818c4aab",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 62.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title data\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "transform = transforms.Compose([transforms.ToTensor(),])\n",
        "\n",
        "# train_data = torchvision.datasets.MNIST(root=\"data\", train=True, download=True,transform=transform) # do not normalise! want img in [0,1)\n",
        "# test_data = torchvision.datasets.MNIST(root=\"data\", train=False, download=True, transform=transform) #opt no download\n",
        "# # batch_size = 32 # 64 512\n",
        "# batch_size = 32 if torch.cuda.is_available() else 32\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "train_data = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
        "test_data = torchvision.datasets.CIFAR10(root='data', train=False, download=True, transform=transform)\n",
        "batch_size = 128 # 128 1024\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "\n",
        "# dataiter = iter(train_data)\n",
        "# x,y = next(dataiter)\n",
        "# print(x.shape) # [3, 32, 32]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title rohanpandey WISDM\n",
        "# https://github.com/rohanpandey/Analysis--WISDM-Smartphone-and-Smartwatch-Activity/blob/master/dataset_creation.ipynb\n",
        "! wget https://archive.ics.uci.edu/static/public/507/wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset.zip\n",
        "!unzip wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset.zip\n",
        "!unzip wisdm-dataset.zip\n",
        "\n",
        "import glob\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "folder1=glob.glob(\"wisdm-dataset/raw/*\")\n",
        "# print(folder1)\n",
        "column_names = ['ID', 'activity','timestamp','x','y','z']\n",
        "overall_dataframe=pd.DataFrame(columns = column_names)\n",
        "for subfolder in folder1:\n",
        "    parent_dir = \"./processed/\"\n",
        "    path = os.path.join(parent_dir, subfolder.split('\\\\')[-1])\n",
        "    if not os.path.exists(path): os.makedirs(path)\n",
        "    folder2=glob.glob(subfolder+\"/*\")\n",
        "    for subsubfolder in folder2:\n",
        "        activity_dataframe = pd.DataFrame(columns = column_names)\n",
        "        subfolder_path = os.path.join(path, subsubfolder.split('/')[-1])\n",
        "        if not os.path.exists(subfolder_path): os.makedirs(subfolder_path)\n",
        "        files=glob.glob(subsubfolder+\"/*\")\n",
        "        for file in files:\n",
        "            # print(file)\n",
        "            df = pd.read_csv(file, sep=\",\",header=None)\n",
        "            df.columns = ['ID','activity','timestamp','x','y','z']\n",
        "            # activity_dataframe=activity_dataframe.append(df)\n",
        "            activity_dataframe = pd.concat([activity_dataframe, df], ignore_index=True)\n",
        "\n",
        "        activity_dataframe['z']=activity_dataframe['z'].str[:-1]\n",
        "        # activity_dataframe['meter']=subsubfolder.split('/')[-1]\n",
        "        # activity_dataframe['device']=subfolder.split('/')[-1]\n",
        "        activity_dataframe.to_csv(subfolder_path+'/data.csv',index=False)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "FA00-tf6MJ-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title buffer dataloader\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# /content/processed/wisdm-dataset/raw/phone/accel/data.csv\n",
        "# /content/processed/wisdm-dataset/raw/watch/gyro/data.csv\n",
        "\n",
        "class BufferDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        # df_keep = pd.read_csv(\"data.csv\")#[['ID','activity','timestamp','x','y','z']]\n",
        "        df_keep = pd.read_csv(\"/content/processed/wisdm-dataset/raw/watch/accel/data.csv\")\n",
        "\n",
        "        user_acts = dict(tuple(df_keep.groupby(['ID','activity'])[['timestamp','x','y','z']]))\n",
        "        self.data = [[d.to_numpy(), a] for a, d in user_acts.items()]\n",
        "        self.act_dict = {i: act for i, act in enumerate(df_keep['activity'].unique())}\n",
        "        self.act_invdict = {v: k for k, v in self.act_dict.items()} # {'A': 0, 'B': 1, ...\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, id_act = self.data[idx]\n",
        "        id_act = self.process(id_act)\n",
        "        return torch.tensor(x[:3500]), id_act # 3567\n",
        "\n",
        "    def process(self, id_act):\n",
        "        return int(id_act[0]), self.act_invdict[id_act[1]]\n",
        "\n",
        "    def transform(self, act_list): # rand resize crop, rand mask # https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html\n",
        "        act_list = np.array(act_list)\n",
        "        try: hr, temp, heart= zip(*act_list)\n",
        "        except ValueError: print('err:',act_list)\n",
        "        kind = 'nearest' if len(act_list)>self.seq_len/.7 else 'linear'\n",
        "        temp_interpolator = scipy.interpolate.interp1d(hr, temp, kind=kind) # linear nearest quadratic cubic\n",
        "        heart_interpolator = scipy.interpolate.interp1d(hr, heart, kind=kind) # linear nearest quadratic cubic\n",
        "        hr_ = np.sort(np.random.uniform(hr[0], hr[-1], round(self.seq_len*random.uniform(1,1/.7))))\n",
        "        temp_, heart_ = temp_interpolator(hr_), heart_interpolator(hr_)\n",
        "        act_list = list(zip(hr_, temp_, heart_))\n",
        "\n",
        "        idx = torch.randint(len(act_list)-self.seq_len+1, size=(1,))\n",
        "        # return act_list[idx: idx+self.seq_len]\n",
        "        act_list = act_list[idx: idx+self.seq_len]\n",
        "\n",
        "        # mask=(torch.rand(self.seq_len)<.1) # True -> masked # random masking\n",
        "        # # act_list[mask] = self.pad[0]\n",
        "        # act_list = [self.pad[0] if m else a for m,a in zip(mask, act_list)]\n",
        "        return act_list\n",
        "\n",
        "    def add(self, episode):\n",
        "        self.data.append(episode)\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    # print(npimg.shape) # (3, 64, 64)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# df_keep = pd.read_csv(\"data.csv\")#[['ID','activity','timestamp','x','y','z']]\n",
        "# user_acts = dict(tuple(df_keep.groupby(['ID','activity'])[['timestamp','x','y','z']]))\n",
        "# dataset = [[d.to_numpy(), a] for a, d in user_acts.items()]\n",
        "\n",
        "\n",
        "train_data = BufferDataset() # one line of poem is roughly 50 characters\n",
        "\n",
        "dataset_size = len(train_data)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(0.7 * dataset_size))\n",
        "np.random.seed(0)\n",
        "np.random.shuffle(indices)\n",
        "train_indices, val_indices = indices[:split], indices[split:]\n",
        "\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "valid_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 64 #512\n",
        "train_loader = DataLoader(train_data, sampler=train_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "test_loader = DataLoader(train_data, sampler=valid_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True)\n"
      ],
      "metadata": {
        "id": "CNVNCV8CGtR_",
        "cellView": "form"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title (Learned)RotEmb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, seq_len=512, base=10000):\n",
        "        super().__init__()\n",
        "        self.dim, self.base = dim, base\n",
        "        theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "        pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "        angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "        self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.base)\n",
        "        return x * self.rot_emb[:seq_len]\n",
        "\n",
        "# class LearnedRoPE(nn.Module): # learnt RoPE ; each tok is 1 pos\n",
        "#     def __init__(self, dim):\n",
        "#         super().__init__()\n",
        "#         self.weights = nn.Parameter(torch.randn(1, dim//2))\n",
        "\n",
        "#     def forward(self, x): #\n",
        "#         batch, seq_len, dim = x.shape\n",
        "#         # if rot_emb.shape[0] < seq_len: self.__init__(dim, seq_len)\n",
        "#         pos = torch.arange(seq_len).unsqueeze(1)\n",
        "#         angles = (self.weights * pos * 2*torch.pi).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "#         rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "#         return x * rot_emb.flatten(-2).unsqueeze(0)\n",
        "\n",
        "\n",
        "class LearnedRoPE(nn.Module): # learnt RoPE ; each tok is 1 pos\n",
        "    def __init__(self, dim, seq_len=512):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.weights = nn.Parameter(torch.randn(1, dim//2))\n",
        "        pos = torch.arange(seq_len).unsqueeze(1)\n",
        "        angles = (self.weights * pos * 2*torch.pi)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "        self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "    def forward(self, x): # [batch, seq_len, dim]\n",
        "        seq_len = x.size(1)\n",
        "        if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len)\n",
        "        return x * self.rot_emb[:seq_len]\n",
        "\n",
        "\n",
        "class RotEmb(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, top=torch.pi, base=10000):\n",
        "        super().__init__()\n",
        "        self.theta = top / (base ** (torch.arange(0, dim, step=2, device=device) / dim))\n",
        "        # self.theta = top / (base ** torch.linspace(0, 1, dim//2, device=device))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (pos.unsqueeze(-1) * self.theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [seq_len, dim]\n",
        "\n",
        "class LearnedRotEmb(nn.Module): # pos in R\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.weights = nn.Parameter(torch.randn((1, dim//2), device=device))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (self.weights * pos.unsqueeze(-1) * 2*torch.pi).unsqueeze(-1) # [batch, 1] * [1, dim//2] -> [batch, dim//2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [batch, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [batch, dim]\n",
        "\n"
      ],
      "metadata": {
        "id": "npc_xGtOz7DC",
        "cellView": "form"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ViT me simple wisdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    \"\"\"Zero out the parameters of a module and return it.\"\"\"\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads=None, d_head=8, cond_dim=None, dropout=0.): # .1\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_head = d_head\n",
        "        self.n_heads = d_model // d_head\n",
        "        # self.d_head = d_model // n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or d_model, 2*d_model, bias=False)\n",
        "        # self.k = nn.Sequential(nn.Dropout(dropout), nn.Linear(cond_dim, d_model, bias=False))\n",
        "        # self.lin = nn.Linear(d_model, d_model)\n",
        "        self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        # self.lin = nn.Sequential(nn.Dropout(dropout), zero_module(nn.Linear(d_model, d_model)))\n",
        "        self.drop = nn.Dropout(dropout) # indp before q,k,v; after linout\n",
        "        self.scale = self.d_head ** -.5\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [batch, T, d_model]=[batch, h*w, c], [batch, num_tok, cond_dim], [batch,T]\n",
        "        batch = x.shape[0]\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        Q = self.q(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        # K = self.k(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        K, V = self.kv(cond).view(batch, -1, self.n_heads, 2*self.d_head).transpose(1, 2).chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # linear attention # Softmax(Q) @ (Softmax(K).T @ V)\n",
        "        if mask != None:\n",
        "            mask = mask[:, None, :, None] # [batch,T] -> [batch,1,T,1]\n",
        "            K, V = K.masked_fill(mask, -torch.finfo(x.dtype).max), V.masked_fill(mask, -torch.finfo(x.dtype).max)\n",
        "        Q, K = Q.softmax(dim=-1)*self.scale, K.softmax(dim=-2)\n",
        "        context = K.transpose(-2,-1) @ V # [batch, n_heads, d_head, d_head]\n",
        "        out = Q @ context # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # (quadratic) attention # Softmax(Q @ K.T) @ V\n",
        "        # attn = Q @ K.transpose(-2,-1) * self.scale # [batch, n_heads, T] # [batch, n_heads, T, T/num_tok]\n",
        "        # if mask != None: attn = attn.masked_fill(mask[:, None, :, None], -torch.finfo(attn.dtype).max) # [batch,T]->[batch,1,T,1]\n",
        "        # attention = torch.softmax(attn, dim=-1)\n",
        "        # out = self.drop(attention) @ V # [batch, n_heads, T, d_head]\n",
        "\n",
        "        out = out.transpose(1, 2).flatten(2)\n",
        "        return self.drop(self.lin(out)) # [batch, T, d_model]\n",
        "\n",
        "# if self, dont pass cond_dim in init, dont pass cond in fwd\n",
        "# Softmax(Q @ K.T) @ V ~ Softmax(Q) @ Softmax(K).T @ V\n",
        "\n",
        "# https://github.com/lucidrains/x-transformers/blob/main/x_transformers/x_transformers.py#L1855\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_head=4, cond_dim=None, ff_mult=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm1 = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.norm2 = nn.RMSNorm(d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.cond_dim = cond_dim\n",
        "        self.self = MultiHeadAttention(d_model, d_head=d_head, dropout=0)\n",
        "        if self.cond_dim!=None: self.cross = MultiHeadAttention(d_model, d_head=d_head, cond_dim=cond_dim, dropout=0)\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        ff_dim=d_model*ff_mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), nn.Dropout(dropout), nn.Linear(d_model, ff_dim), act, # ReLU GELU\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "            nn.RMSNorm(d_model), nn.Dropout(dropout), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            # nn.RMSNorm(d_model), act, nn.Dropout(dropout), nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,c,h,w], [batch, num_tok, cond_dim], [batch,T]\n",
        "        # bchw = x.shape\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "\n",
        "        if self.cond_dim==None: cond=None # is self attn\n",
        "        x = x + self.drop(self.self(self.norm1(x), cond, mask))\n",
        "        if self.cond_dim!=None: x = x + self.cross(self.norm2(x), cond, mask)\n",
        "        x = x + self.ff(x)\n",
        "        # x = x + self.drop(self.ff(x))\n",
        "\n",
        "        # return x.transpose(1,2).reshape(*bchw)\n",
        "        return x\n",
        "\n",
        "import torch\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "def apply_masks(x, masks):\n",
        "    \"\"\":param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
        "    :param masks: list of tensors containing indices of patches in [N] to keep\"\"\"\n",
        "    all_x = []\n",
        "    for m in masks: # [1,T]\n",
        "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [1,T,dim]\n",
        "        all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "    return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        # self.positional_emb = nn.Parameter(torch.zeros(1, 32*32//4, d_model)) # positional_embedding == 'learnable'\n",
        "        # encoder_layers = nn.TransformerEncoderLayer(d_model, d_model//d_head, d_hid or d_model, dropout, batch_first=True)\n",
        "        # self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.zeros(1,1,d_model)) # randn zeros\n",
        "        nn.init.trunc_normal_(self.cls, std=.02)\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model)\n",
        "        self.lin = nn.Linear(d_model, out_dim)# if out_dim != d_model else None\n",
        "\n",
        "    def forward(self, src, context_indices, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        src = self.embed(src) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = src.shape\n",
        "        src = src * self.pos_encoder(context_indices)\n",
        "        # src = src + self.positional_emb[:,context_indices]\n",
        "        # src = apply_masks(src, [context_indices])\n",
        "\n",
        "        pred_tokens = self.cls * self.pos_encoder(trg_indices) # [M, num_trg_toks, d_model]\n",
        "        # pred_tokens = self.cls + self.positional_emb[0,trg_indices]\n",
        "        pred_tokens = pred_tokens.repeat(batch, 1, 1) # [batch*M, num_trg_toks, d_model]\n",
        "        # print(pred_tokens.requires_grad)\n",
        "        # print(\"pred fwd\", src.shape, pred_tokens.shape)\n",
        "        # src = src.repeat_interleave(trg_indices.shape[0], dim=0) # [batch, seq_len, d_model] -> [batch*M, seq_len, d_model]\n",
        "        src = torch.cat([src, pred_tokens], dim=1) # [batch*M, seq_len+num_trg_toks, d_model]\n",
        "\n",
        "        out = self.transformer_encoder(src) # float [seq_len, batch_size, d_model]\n",
        "        out = self.norm(out)\n",
        "        out = out[:,seq:] # [batch*M, num_trg_toks, d_model]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "\n",
        "        # self.calorie_emb = RotEmb(dim, top=1, base=10) # 0-4\n",
        "        # self.steps_emb = RotEmb(dim, top=1, base=10) # 0-5\n",
        "        # self.enc0 = nn.Sequential(\n",
        "        #     nn.Linear(dim*2, d_model), act,\n",
        "        #     nn.Linear(d_model, d_model), act,\n",
        "        # )\n",
        "        # self.hour_emb = CircularEmb(dim, torch.pi/2) # circular (0,1) # 0-24\n",
        "        # self.temp_emb = RotEmb(dim, top=1/3, base=10) # 18-35\n",
        "        # self.heart_emb = RotEmb(dim, top=1/3, base=100) # 50-200\n",
        "        # self.enc1 = nn.Sequential(\n",
        "        #     nn.Linear(dim*3, d_model), act,\n",
        "        # )\n",
        "\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.MaxPool1d(2,2), #nn.MaxPool1d(3, 2, 3//2),\n",
        "            # nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.Conv1d(d_model, d_model,3,2,3//2)\n",
        "            nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2),\n",
        "            # nn.Conv1d(in_dim, d_model,2,2,0), # like patch\n",
        "            )\n",
        "        self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        # self.positional_emb = nn.Parameter(torch.zeros(1, 32*32//4, d_model)) # positional_embedding == 'learnable'\n",
        "        # encoder_layers = nn.TransformerEncoderLayer(d_model, d_model//d_head, d_hid or d_model, dropout, batch_first=True)\n",
        "        # self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model)\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, src, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "\n",
        "        # src = self.embed(src) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        src = self.embed(src.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "\n",
        "        batch, seq, dim = src.shape\n",
        "\n",
        "        # # # src = self.pos_encoder(src)\n",
        "        # if context_indices != None:\n",
        "        # print(src.shape, self.pos_encoder(context_indices).shape)\n",
        "        #     src = src * self.pos_encoder(context_indices) # context/predictor # src = src + self.positional_emb[:,context_indices]\n",
        "        # else: src = src * self.pos_encoder(torch.arange(seq, device=device).unsqueeze(0)) # target # src = src + self.positional_emb[:,:seq]\n",
        "\n",
        "\n",
        "        # src = self.pos_encoder(src)\n",
        "        src = src * self.pos_encoder(torch.arange(seq, device=device)).unsqueeze(0)\n",
        "        # src = src + self.positional_emb\n",
        "        if context_indices != None:\n",
        "            src = apply_masks(src, [context_indices])\n",
        "\n",
        "\n",
        "        out = self.transformer_encoder(src) # float [seq_len, batch_size, d_model]\n",
        "        out = self.norm(out)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "batch, seq_len, d_model = 4,1024,16\n",
        "in_dim = 3\n",
        "model = TransformerModel(in_dim, d_model, d_head=4, nlayers=2, dropout=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "# # print(out)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bnjJHfKj1_g",
        "outputId": "2e3e10eb-bc09-4ef7-c196-8cfe54602b6b",
        "cellView": "form"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5104\n",
            "torch.Size([4, 32, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "d929355f-be2b-47a1-82ff-a7630671f47f",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title RNN pytorch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, num_layers=1):\n",
        "        super().__init__()\n",
        "        if out_dim is None: out_dim = in_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.d_model = d_model\n",
        "        # self.rnn = nn.RNN(d_model, d_model, num_layers, batch_first=True)\n",
        "        self.rnn = nn.GRU(in_dim, d_model, num_layers, batch_first=True)\n",
        "        # self.lstm = nn.LSTM(d_model, d_model, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(d_model, out_dim)\n",
        "\n",
        "        # self.emb = nn.Embedding(in_dim, d_model)\n",
        "        self.mask_vec = nn.Parameter(torch.randn(in_dim))\n",
        "\n",
        "\n",
        "    # def forward(self, x, hc=None): # lstm [batch_size, seq, in_dim]\n",
        "    #     x = self.emb(x)\n",
        "    #     if hc is None:\n",
        "    #         h0 = torch.zeros((self.num_layers, x.size(0), self.d_model), device=device)\n",
        "    #         c0 = torch.zeros((self.num_layers, x.size(0), self.d_model), device=device)\n",
        "    #     else: h0,c0 = hc\n",
        "    #     # print(x.shape, h0.shape,c0.shape)\n",
        "    #     out, (h,c) = self.lstm(x, (h0,c0)) # [batch, seq_len, d_model], ([num_layers, batch, d_model] )\n",
        "    #     # out = out[:, -1, :] # out: (n, 128)\n",
        "    #     out = self.fc(out) # out: (n, 10)\n",
        "    #     return out, (h, c)\n",
        "\n",
        "    def reset(self, batch):\n",
        "        h0 = torch.zeros((self.num_layers, batch, self.d_model), device=device)\n",
        "        return h0\n",
        "\n",
        "    def forward(self, x, h0=None, mask=None): # rnn/gru # [batch, T, in_dim], [num_layers, batch, d_model], [batch, T] True->masked\n",
        "        # x = self.emb(x)\n",
        "        if h0==None: h0 = self.reset(x.shape[0])\n",
        "        # print(x.shape, h0.shape)\n",
        "        if mask!=None: x[mask] = self.mask_vec.to(x.dtype)\n",
        "        out, h = self.rnn(x, h0) #\n",
        "        out = out[:, -1, :] # out: (n, 128)\n",
        "        out = self.fc(out) # out: (n, 10)\n",
        "        return out, h # [batch, out_dim], [num_layers, batch, d_model]\n",
        "\n",
        "\n",
        "# hidden_size = 128\n",
        "# num_layers = 2\n",
        "# input_size = num_classes = 4\n",
        "\n",
        "# model = RNN(input_size, hidden_size, num_classes, num_layers).to(device)\n",
        "# # # print(model)\n",
        "# # print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "# batch=2\n",
        "# seq_len=3\n",
        "# x=torch.rand(batch,seq_len,input_size).to(device)\n",
        "# h=torch.rand(num_layers,batch,hidden_size).to(device)\n",
        "# mask=(torch.rand(batch,seq_len)<.5)#.expand(-1,-1,x.size(-1))\n",
        "# print(mask)\n",
        "# print(x)\n",
        "# out,h = model(x, h, mask)\n",
        "# print(out.shape)\n",
        "# print(h.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title TransformerVICReg\n",
        "import math\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        for layer in self:\n",
        "            params = inspect.signature(layer.forward).parameters.keys()\n",
        "            layer._fwdparams = ','.join(params)\n",
        "\n",
        "    def forward(self, x, mask=None, cond=None):\n",
        "        for layer in self:\n",
        "            args = [x]\n",
        "            if 'mask' in layer._fwdparams: args.append(mask)\n",
        "            if 'cond' in layer._fwdparams: args.append(cond)\n",
        "            x = layer(*args)\n",
        "        return x\n",
        "\n",
        "class TransformerVICReg(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, drop=0):\n",
        "        super().__init__()\n",
        "        act = nn.GELU()\n",
        "        # self.embed = nn.Sequential(nn.Linear(in_dim, d_model), act)\n",
        "        self.embed = nn.Sequential(\n",
        "            nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2),\n",
        "            )\n",
        "        self.pos_encoder = RoPE(d_model, seq_len=200, base=10000)\n",
        "        # d_hid = d_hid or d_model#*2\n",
        "        # self.encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "        self.encoder = Seq(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "\n",
        "        out_dim = out_dim or d_model\n",
        "        self.lin = nn.Linear(d_model, out_dim)\n",
        "        self.attention_pool = nn.Linear(d_model, 1, bias=False)\n",
        "\n",
        "        dim_v = d_model * 4\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(out_dim, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v, bias=False),\n",
        "            )\n",
        "\n",
        "    def forward(self, x, src_key_padding_mask=None): # [batch, seq, d_model], [batch, seq] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # x = self.embed(x)\n",
        "        x = self.embed(x.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "        # batch, seq_len, d_model = x.shape\n",
        "        # x = torch.cat([self.cls.repeat(batch,1,1), x], dim=1)\n",
        "        # src_key_padding_mask = torch.cat([torch.zeros((batch, 1), dtype=torch.bool), src_key_padding_mask], dim=1)\n",
        "        x = self.pos_encoder(x)\n",
        "\n",
        "        x = self.encoder(x, mask=src_key_padding_mask)\n",
        "        # print(\"fwd\",out.shape) # float [batch, seq_len, d_model]\n",
        "\n",
        "        attn = self.attention_pool(x).squeeze(-1) # [batch, seq] # seq_pool\n",
        "        out = (torch.softmax(attn, dim=-1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, seq] @ [batch, seq, dim] -> [batch, dim]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "    def expand(self, x, src_key_padding_mask=None):\n",
        "        sx = self.forward(x, src_key_padding_mask=src_key_padding_mask)\n",
        "        vx = self.exp(sx)\n",
        "        return vx\n",
        "\n",
        "batch, seq_len, d_model = 4,3500,512\n",
        "in_dim, out_dim=3,16\n",
        "model = TransformerVICReg(in_dim, d_model, out_dim, d_head=4, nlayers=2, drop=0.).to(device)\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "src_key_padding_mask = torch.stack([(torch.arange(seq_len) < seq_len - v) for v in torch.randint(seq_len, (batch,))]) # True will be ignored # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "print(src_key_padding_mask)\n",
        "out = model(x, src_key_padding_mask)\n",
        "print(out.shape)\n"
      ],
      "metadata": {
        "id": "ODpKypTCsfIt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18930604-ebb4-4dd2-c6fc-762a7af57bee",
        "cellView": "form"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ True,  True,  True,  ..., False, False, False],\n",
            "        [ True,  True,  True,  ..., False, False, False],\n",
            "        [ True,  True,  True,  ..., False, False, False],\n",
            "        [ True,  True,  True,  ..., False, False, False]])\n",
            "torch.Size([4, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Violet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class Violet(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, nlayers=2, d_head=4):\n",
        "        super().__init__()\n",
        "        out_dim = out_dim or d_model\n",
        "\n",
        "        # self.rnn = RNN(d_model, d_model, d_model, nlayers)\n",
        "        self.student = TransformerVICReg(in_dim, d_model, out_dim=out_dim, d_head=d_head, nlayers=nlayers, drop=0.)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "\n",
        "        # vicreg\n",
        "        self.sim_coeff=25. # 10.0 # 25.0 # λ\n",
        "        self.std_coeff=25. # 1.0 # 25.0 # µ\n",
        "        self.cov_coeff=1. # 25.0 # 1.0 # ν\n",
        "\n",
        "        self.classifier = nn.Linear(out_dim, 18) # 10 18\n",
        "\n",
        "    def loss(self, x, src_key_padding_mask=None): # [batch, T, 3]\n",
        "        batch, seq, dim = x.shape\n",
        "        # print(x.shape)\n",
        "        vx = self.student.expand(x, src_key_padding_mask=src_key_padding_mask) # [batch, num_context_toks, out_dim]\n",
        "        with torch.no_grad(): vy = self.teacher.expand(x.detach(), src_key_padding_mask=src_key_padding_mask) # [batch, num_trg_toks, out_dim]\n",
        "        loss = self.vicreg(vx, vy)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x, src_key_padding_mask=None): # [batch, T, 3]\n",
        "        sx = self.student(x, src_key_padding_mask=src_key_padding_mask)\n",
        "        return sx\n",
        "\n",
        "    def classify(self, x): # [batch, T, 3]\n",
        "        sx = self.forward(x)\n",
        "        out = self.classifier(sx)\n",
        "        return out\n",
        "\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size, num_features = x.shape\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = self.sim_coeff * repr_loss + self.std_coeff * std_loss + self.cov_coeff * cov_loss\n",
        "        print(\"in vicreg \",self.sim_coeff * repr_loss.item() , self.std_coeff * std_loss.item() , self.cov_coeff * cov_loss.item())\n",
        "        # return loss\n",
        "        return repr_loss, std_loss, cov_loss\n",
        "\n",
        "\n",
        "violet = Violet(in_dim=3, d_model=32, out_dim=16, nlayers=2, d_head=4).to(device)\n",
        "# voptim = torch.optim.AdamW(violet.parameters(), lr=1e-3) # 1e-3?\n",
        "voptim = torch.optim.AdamW([{'params': violet.student.encoder.parameters()},\n",
        "    {'params': violet.student.exp.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # default 1e-2\n",
        "print(sum(p.numel() for p in violet.parameters() if p.requires_grad)) # 27584\n",
        "\n",
        "x = torch.rand((2,1000,3), device=device)\n",
        "# x = torch.rand((2,1,16), device=device)\n",
        "loss = violet.loss(x)\n",
        "# print(out.shape)\n",
        "print(loss)\n"
      ],
      "metadata": {
        "id": "5qwg9dG4sQ3h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a44e721d-1a60-4b79-a914-86cf229cb6fa",
        "cellView": "form"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "55330\n",
            "in vicreg  2.7820201582526456e-16 24.747037887573242 1.4539491832721296e-09\n",
            "(tensor(1.1128e-17, device='cuda:0', grad_fn=<MseLossBackward0>), tensor(0.9899, device='cuda:0', grad_fn=<AddBackward0>), tensor(1.4539e-09, device='cuda:0', grad_fn=<AddBackward0>))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title SeqJEPA mae like\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def symlog(x): return torch.sign(x) * torch.log(torch.abs(x) + 1.0)\n",
        "def symexp(x): return torch.sign(x) * (torch.exp(torch.abs(x)) - 1.0)\n",
        "\n",
        "\n",
        "def multiblock(seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "    return target_mask\n",
        "\n",
        "# https://arxiv.org/pdf/2210.07224\n",
        "def randpatch(seq, mask_size=8, gamma=0.9): # num patches of seq, mask patch size, masking ratio\n",
        "    # mask = torch.rand(seq//mask_size)<gamma\n",
        "    length = seq//mask_size\n",
        "    g = torch.normal(gamma, std=.1, size=(1,)).clamp(.5,.9)\n",
        "    # g = gamma\n",
        "    idx = torch.randperm(length)[:int(length*g)]\n",
        "    mask = torch.zeros(length, dtype=bool)\n",
        "    mask[idx] = True\n",
        "    mask = mask.repeat_interleave(mask_size, dim=-1)\n",
        "    return mask # [seq] , True -> mask\n",
        "\n",
        "class SeqJEPA(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, nlayers=2, d_head=4):\n",
        "        super().__init__()\n",
        "        if out_dim is None: out_dim = d_model\n",
        "        self.patch_size = 32 # 4\n",
        "        self.context_encoder = TransformerModel(in_dim, d_model, out_dim=out_dim, d_head=d_head, nlayers=nlayers, dropout=0.)\n",
        "        # self.predicter = TransformerPredictor(out_dim, d_model//2, out_dim, d_head=d_head, nlayers=1, dropout=0.)\n",
        "        self.predicter = TransformerPredictor(out_dim, d_model//2, out_dim, d_head=d_head, nlayers=nlayers//2, dropout=0.)\n",
        "        import copy\n",
        "        self.target_encoder = copy.deepcopy(self.context_encoder)\n",
        "        self.target_encoder.requires_grad_(False)\n",
        "        self.classifier = nn.Linear(out_dim, 18) # 10 18\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]\n",
        "        batch, seq, dim = x.shape\n",
        "        # print(x.shape)\n",
        "        target_mask = multiblock(seq//self.patch_size, min_s=0.15, max_s=0.2, M=4).any(0) # best.2.3M4 og.15.2M4# mask out targets to be predicted # [M, seq]\n",
        "        # target_mask = multiblock(seq//self.patch_size, min_s=0.2, max_s=0.3, M=4).any(0) # best.2.3M4 og.15.2M4# mask out targets to be predicted # [M, seq]\n",
        "        # target_mask = randpatch(seq//self.patch_size, mask_size=8, gamma=.9) # 8.9 [seq]\n",
        "        context_mask = ~multiblock(seq//self.patch_size, min_s=0.85, max_s=1., M=1)|target_mask # og .85,1.M1 # [1, seq], True->Mask\n",
        "        target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "\n",
        "        context_indices = (~context_mask[0]).nonzero().squeeze(-1) # int idx [num_context_toks] , idx of context not masked\n",
        "        sx = self.context_encoder(x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "\n",
        "        trg_indices = target_mask.nonzero().squeeze(-1) # int idx [1, num_trg_toks] , idx of targets that are masked\n",
        "        sy_ = self.predicter(sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "        with torch.no_grad():\n",
        "            sy = self.target_encoder(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "            sy = apply_masks(sy, [trg_indices])\n",
        "\n",
        "        loss = F.mse_loss(sy.detach(), sy_)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        # sx = self.target_encoder(x)\n",
        "        sx = self.context_encoder(x)\n",
        "        out = sx.mean(dim=1)\n",
        "        return out\n",
        "\n",
        "    def classify(self, x): # [batch, T, 3]\n",
        "        sx = self.forward(x)\n",
        "        out = self.classifier(sx)\n",
        "        return out\n",
        "\n",
        "# min_s=0.15, max_s, M\n",
        "# trg.15.2M4 C.85 1\n",
        "\n",
        "seq_jepa = SeqJEPA(in_dim=3, d_model=32, out_dim=16, nlayers=6, d_head=4).to(device)#.to(torch.float)\n",
        "# optim = torch.optim.AdamW(seq_jepa.parameters(), lr=1e-3) # 1e-3?\n",
        "# optim = torch.optim.AdamW(seq_jepa.parameters(), lr=1e-3, weight_decay=0) # lr1e-3? wd0\n",
        "optim = torch.optim.AdamW([{'params': seq_jepa.context_encoder.parameters()},\n",
        "    # {'params': seq_jepa.predicter.parameters(), 'lr': 1e-2}], lr=1e-3)#, weight_decay=0) default 1e-2\n",
        "    {'params': seq_jepa.predicter.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # default 1e-2\n",
        "\n",
        "print(sum(p.numel() for p in seq_jepa.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.parameters())) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.predicter.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.context_encoder.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.target_encoder.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# d_model^2 * nlayers\n",
        "\n",
        "x = torch.rand((2,1024,3), device=device)\n",
        "out = seq_jepa.loss(x)\n",
        "print(out.shape)\n",
        "# print(x.is_leaf) # T\n"
      ],
      "metadata": {
        "id": "xy4iC46Vnlog",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00d013df-3fed-4cc7-aef2-4a8502d7596c",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "51250\n",
            "torch.Size([])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "# classifier = Classifier(16).to(device)\n",
        "classifier = Classifier(16, 18).to(device)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n",
        "# optim = torch.optim.AdamW([{'params': seq_jepa.parameters()}, {'params': classifier.parameters(), 'lr': 1e-3}], lr=1e-3)\n"
      ],
      "metadata": {
        "id": "wjK1TVwBh2u8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(seq_jepa.classifier.weight[0])\n",
        "# for n, p in seq_jepa.target_encoder.named_parameters():\n",
        "#     print(n,p)\n",
        "# optim.param_groups[0]['lr'] = 1e-3"
      ],
      "metadata": {
        "id": "7IACKDymhit7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34de8800-fa4b-4460-803c-3cd8dbfb6c70",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title strain ctrain test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "def strain(model, dataloader, optim, scheduler=None): # train function with automatic mixed precision\n",
        "# def strain(model, train_iter, optim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.train()\n",
        "    for i, (x, _) in enumerate(dataloader):\n",
        "        # x = x.to(device).flatten(2).transpose(-2,-1)#.to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "        x = x[...,1:].to(device).to(torch.bfloat16)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            # loss = model.loss(x)\n",
        "\n",
        "            repr_loss, std_loss, cov_loss = model.loss(x)\n",
        "            loss = model.sim_coeff * repr_loss + model.std_coeff * std_loss + model.cov_coeff * cov_loss\n",
        "\n",
        "        optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # total_norm = 0\n",
        "        # for p in model.parameters(): total_norm += p.grad.data.norm(2).item() ** 2\n",
        "        # total_norm = total_norm**.5\n",
        "        # print('total_norm', total_norm)\n",
        "        # for p in list(filter(lambda p: p.grad is not None, model.parameters())):\n",
        "        #     print(p.grad.data.norm(2).item())\n",
        "        # print(\"max grad norm\", max([p.grad.data.norm(2).item() for p in list(filter(lambda p: p.grad is not None, model.parameters()))]))\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 5) # 0.5\n",
        "\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            m=0.999 # 0.99 m = next(momentum_scheduler)\n",
        "            norms=[]\n",
        "            # for param_q, param_k in zip(model.context_encoder.parameters(), model.target_encoder.parameters()):\n",
        "            for param_q, param_k in zip(model.student.parameters(), model.teacher.parameters()):\n",
        "                # norm = ((param_k.data-param_q.detach().data)**2).sum()**.5\n",
        "                # # # print(param_k.data.shape, norm)\n",
        "                # norms.append(norm.item())\n",
        "                # # if norm>.01:\n",
        "                param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "            # print(norms)\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        print(\"strain\",loss.item())\n",
        "        # for param in seq_jepa.context_encoder.cls: print(param.data)\n",
        "        # for param in seq_jepa.predicter.cls: print(param.data)\n",
        "        try: wandb.log({\"loss\": loss.item(), \"repr/I\": repr_loss.item(), \"std/V\": std_loss.item(), \"cov/C\": cov_loss.item()})\n",
        "        # try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=50: break\n",
        "\n",
        "\n",
        "def ctrain(model, classifier, dataloader, coptim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.eval()\n",
        "    classifier.train()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        # x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        # x = x.flatten(2).transpose(-2,-1).to(torch.bfloat16)\n",
        "        x, y = x[...,1:].to(device).to(torch.bfloat16), y[1].to(device) # [batch, ] # (id, activity)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            with torch.no_grad():\n",
        "                sx = model(x).detach()\n",
        "            # print(sx[0][0])\n",
        "            y_ = classifier(sx)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "        # print(classifier.classifier.weight[0])\n",
        "        # print(y_.shape, y.shape)\n",
        "        # print(loss)\n",
        "        coptim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 5) # .5\n",
        "        scaler.step(coptim)\n",
        "        scaler.update()\n",
        "        print(\"classify\",loss.item())\n",
        "        try: wandb.log({\"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        # x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        # x = x.flatten(2).transpose(-2,-1)#.to(torch.bfloat16)\n",
        "        x, y = x[...,1:].to(device).to(torch.float), y[1].to(device) # [batch, ] # (id, activity)\n",
        "        with torch.no_grad():\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        print(correct/len(y))\n",
        "        try: wandb.log({\"correct\": correct/len(y)})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "# for i in range(1):\n",
        "for i in range(1000):\n",
        "    # train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    # test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    np.random.shuffle(train_indices); np.random.shuffle(val_indices)\n",
        "    train_sampler, valid_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(val_indices)\n",
        "    # batch_size = 64 #512\n",
        "    train_loader = DataLoader(train_data, sampler=train_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "    test_loader = DataLoader(train_data, sampler=valid_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True)\n",
        "\n",
        "    # strain(seq_jepa, train_loader, optim)\n",
        "    # ctrain(seq_jepa, classifier, train_loader, coptim)\n",
        "    # test(seq_jepa, classifier, test_loader)\n",
        "\n",
        "    strain(violet, train_loader, voptim)\n",
        "    ctrain(violet, classifier, train_loader, coptim)\n",
        "    test(violet, classifier, test_loader)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Nd-sGe6Ku4S",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title wandb\n",
        "!pip install -q wandb\n",
        "import wandb # https://docs.wandb.ai/quickstart\n",
        "wandb.login(key='487a2109e55dce4e13fc70681781de9f50f27be7')\n",
        "try: run.finish()\n",
        "except NameError: pass\n",
        "run = wandb.init(project=\"violet\", config={\"model\": \"res18\",})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# @title supervised train test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "def strain(model, dataloader, optim, scheduler=None): # train function with automatic mixed precision\n",
        "# def strain(model, train_iter, optim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.train()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        # x = x.to(device).flatten(2).transpose(-2,-1)#.to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "        # y = y.to(device)\n",
        "        x, y = x[...,1:].to(device).to(torch.bfloat16), y[1].to(device) # [batch, ] # (id, activity)\n",
        "\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            y_ = model.classify(x)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        # print(loss)\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # for p in list(filter(lambda p: p.grad is not None, model.parameters())):\n",
        "        #     print(p.grad.data.norm(2).item())\n",
        "        # print(\"max grad norm\", max([p.grad.data.norm(2).item() for p in list(filter(lambda p: p.grad is not None, model.parameters()))]))\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 10) # 0.5\n",
        "\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        print(\"strain\",loss.item())\n",
        "        # for param in seq_jepa.context_encoder.cls: print(param.data)\n",
        "        # for param in seq_jepa.predicter.cls: print(param.data)\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=50: break\n",
        "\n",
        "\n",
        "def test(model, dataloader):\n",
        "    model.eval()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        # x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        # img = img.flatten(2).transpose(-2,-1)#.to(torch.bfloat16)\n",
        "        x, y = x[...,1:].to(device).to(torch.float), y[1].to(device) # [batch, ] # (id, activity)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            y_ = model.classify(x)\n",
        "\n",
        "        loss = F.cross_entropy(y_, y)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        print(correct/len(y))\n",
        "        try: wandb.log({\"correct\": correct/len(y), \"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "for i in range(1000):\n",
        "    # train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    # test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    np.random.shuffle(train_indices); np.random.shuffle(val_indices)\n",
        "    train_sampler, valid_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(val_indices)\n",
        "    # batch_size = 64 #512\n",
        "    train_loader = DataLoader(train_data, sampler=train_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "    test_loader = DataLoader(train_data, sampler=valid_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True)\n",
        "\n",
        "    # strain(seq_jepa, train_loader, optim)\n",
        "    # test(seq_jepa, test_loader)\n",
        "    strain(violet, train_loader, voptim)\n",
        "    test(violet, test_loader)\n"
      ],
      "metadata": {
        "id": "4J2ahp3wmqcg",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for param in seq_jepa.context_encoder.cls: print(param.data)\n",
        "# print(seq_jepa.context_encoder.cls.is_leaf)\n",
        "# seq=40\n",
        "# src = seq_jepa.context_encoder.pos_encoder(torch.arange(seq, device=device))\n",
        "# for x in src:\n",
        "#     print(x)\n",
        "# print(.999**50)"
      ],
      "metadata": {
        "id": "WgN5LlxWsPzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title save/load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder='/content/drive/MyDrive/jepa/'\n",
        "\n",
        "# # modelsd, optimsd = torch.load(folder+'SeqJEPA.pkl', map_location=device).values()\n",
        "# modelsd, optimsd = torch.load('SeqJEPA.pkl', map_location=device).values()\n",
        "# seq_jepa.load_state_dict(modelsd, strict=False)\n",
        "# optim.load_state_dict(optimsd)"
      ],
      "metadata": {
        "id": "JT8AgOx0E_KM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b3d3969-da28-4cc0-c4de-f0a327ea266f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = {'model': seq_jepa.state_dict(), 'optimizer': optim.state_dict()}\n",
        "torch.save(checkpoint, folder+'SeqJEPA.pkl')\n",
        "# torch.save(checkpoint, 'SeqJEPA.pkl')"
      ],
      "metadata": {
        "id": "CpbLPvjiE-pD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## drawer"
      ],
      "metadata": {
        "id": "0vScvFGGSeN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title evergarmin buffer dataloader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "import numpy as np\n",
        "import scipy.interpolate\n",
        "\n",
        "class BufferDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, activities, seq_len):\n",
        "        # self.data = [step for episode in self.process(buffer) for step in episode] # 0.00053\n",
        "        self.data = [self.process(activity) for activity in activities] # 0.00053\n",
        "        self.data = [x for x in self.data if x!=None and x!=[]]\n",
        "        self.seq_len = seq_len\n",
        "        self.pad = [(-1,-1,-1)]*(self.seq_len) # pad. need to mask later # torch.full((self.seq_len,), -1)\n",
        "        # normalise\n",
        "\n",
        "    def __len__(self):\n",
        "        # return len(self.data)//self.seq_len\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        act_list = self.data[idx].copy()\n",
        "        summary = list(act_list.pop(0))\n",
        "        # act_list = self.pad[len(act_list):] + act_list # forward padding\n",
        "        act_list = [summary] + self.transform(act_list) # summary, aug(x)\n",
        "        hr, temp, heart= zip(*act_list)\n",
        "        # print(hr, temp, heart)\n",
        "        # return hr, temp, heart\n",
        "        return torch.tensor(hr), torch.tensor(temp), torch.tensor(heart, dtype=torch.float)\n",
        "\n",
        "\n",
        "\n",
        "    def process(self, activity):\n",
        "        res_id = activity[\"res_id\"]\n",
        "        try: activeKilocalories = activity[\"summary\"][\"activeKilocalories\"]\n",
        "        except KeyError: activeKilocalories = 0.\n",
        "        try: steps = activity[\"summary\"][\"steps\"]\n",
        "        except KeyError: steps = 0.\n",
        "        # (res_id, activeKilocalories, steps)\n",
        "        res_id = float(res_id[3:]) # remove 'RES' and turn id into float, in line with other data\n",
        "        act_list = [(res_id, activeKilocalories, steps)]\n",
        "        samples = activity[\"samples\"]\n",
        "\n",
        "        if len(samples)==0: return\n",
        "\n",
        "        for sample in samples:\n",
        "            try:\n",
        "                startTimeInSeconds = sample[\"startTimeInSeconds\"]\n",
        "                hour_decimal = (startTimeInSeconds / 3600) % 24\n",
        "            except KeyError: hour_decimal = 12.\n",
        "            try: airTemperatureCelcius = sample[\"airTemperatureCelcius\"]\n",
        "            except KeyError: airTemperatureCelcius = 28. # singapore average temperature?\n",
        "            try: heartRate = sample[\"heartRate\"]\n",
        "            except KeyError: heartRate = 80.\n",
        "            # (hour_decimal, airTemperatureCelcius, heartRate)\n",
        "            act_list.append((hour_decimal, airTemperatureCelcius, heartRate))\n",
        "        return act_list\n",
        "\n",
        "    def transform(self, act_list): # rand resize crop, rand mask\n",
        "\n",
        "        # https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html\n",
        "        act_list = np.array(act_list)\n",
        "        try: hr, temp, heart= zip(*act_list)\n",
        "        except ValueError: print('err:',act_list)\n",
        "        kind = 'nearest' if len(act_list)>self.seq_len/.7 else 'linear'\n",
        "        temp_interpolator = scipy.interpolate.interp1d(hr, temp, kind=kind) # linear nearest quadratic cubic\n",
        "        heart_interpolator = scipy.interpolate.interp1d(hr, heart, kind=kind) # linear nearest quadratic cubic\n",
        "        hr_ = np.sort(np.random.uniform(hr[0], hr[-1], round(self.seq_len*random.uniform(1,1/.7))))\n",
        "        temp_, heart_ = temp_interpolator(hr_), heart_interpolator(hr_)\n",
        "        act_list = list(zip(hr_, temp_, heart_))\n",
        "\n",
        "        idx = torch.randint(len(act_list)-self.seq_len+1, size=(1,))\n",
        "        # return act_list[idx: idx+self.seq_len]\n",
        "        act_list = act_list[idx: idx+self.seq_len]\n",
        "\n",
        "        # mask=(torch.rand(self.seq_len)<.1) # True -> masked # random masking\n",
        "        # # act_list[mask] = self.pad[0]\n",
        "        # act_list = [self.pad[0] if m else a for m,a in zip(mask, act_list)]\n",
        "        return act_list\n",
        "\n",
        "    def push(self, activities):\n",
        "        # self.data.append(self.process(activity))\n",
        "        self.data.extend([self.process(activity) for activity in activities])\n",
        "\n",
        "\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(30, 14))\n",
        "    print(npimg.shape)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "seq_len = 200 #26/51 # 50\n",
        "train_data = BufferDataset(activities, seq_len)\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 128 #128\n",
        "# train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # [3,batch, T]\n",
        "train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, drop_last=True) # [3,batch, T]\n",
        "\n",
        "\n",
        "for batch, (hr, temp, heart) in enumerate(train_loader):\n",
        "    pass\n",
        "    # print(hr[6])\n",
        "    # for h in hr:\n",
        "    #     print(h)\n",
        "    # break\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "DNCJFn5kNuua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ViT me simple save\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    \"\"\"Zero out the parameters of a module and return it.\"\"\"\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads=None, d_head=8, cond_dim=None, dropout=0.): # .1\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_head = d_head\n",
        "        self.n_heads = d_model // d_head\n",
        "        # self.d_head = d_model // n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or d_model, 2*d_model, bias=False)\n",
        "        # self.k = nn.Sequential(nn.Dropout(dropout), nn.Linear(cond_dim, d_model, bias=False))\n",
        "        # self.lin = nn.Linear(d_model, d_model)\n",
        "        self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        # self.lin = nn.Sequential(nn.Dropout(dropout), zero_module(nn.Linear(d_model, d_model)))\n",
        "        self.drop = nn.Dropout(dropout) # indp before q,k,v; after linout\n",
        "        self.scale = self.d_head ** -.5\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [batch, T, d_model]=[batch, h*w, c], [batch, num_tok, cond_dim], [batch,T]\n",
        "        batch = x.shape[0]\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        Q = self.q(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        # K = self.k(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        K, V = self.kv(cond).view(batch, -1, self.n_heads, 2*self.d_head).transpose(1, 2).chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # linear attention # Softmax(Q) @ (Softmax(K).T @ V)\n",
        "        if mask != None:\n",
        "            mask = mask[:, None, :, None] # [batch,T] -> [batch,1,T,1]\n",
        "            K, V = K.masked_fill(mask, -torch.finfo(x.dtype).max), V.masked_fill(mask, -torch.finfo(x.dtype).max)\n",
        "        Q, K = Q.softmax(dim=-1)*self.scale, K.softmax(dim=-2)\n",
        "        context = K.transpose(-2,-1) @ V # [batch, n_heads, d_head, d_head]\n",
        "        out = Q @ context # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # (quadratic) attention # Softmax(Q @ K.T) @ V\n",
        "        # attn = Q @ K.transpose(-2,-1) * self.scale # [batch, n_heads, T] # [batch, n_heads, T, T/num_tok]\n",
        "        # if mask != None: attn = attn.masked_fill(mask[:, None, :, None], -torch.finfo(attn.dtype).max) # [batch,T]->[batch,1,T,1]\n",
        "        # attention = torch.softmax(attn, dim=-1)\n",
        "        # out = self.drop(attention) @ V # [batch, n_heads, T, d_head]\n",
        "\n",
        "        out = out.transpose(1, 2).flatten(2)\n",
        "        return self.drop(self.lin(out)) # [batch, T, d_model]\n",
        "\n",
        "# if self, dont pass cond_dim in init, dont pass cond in fwd\n",
        "# Softmax(Q @ K.T) @ V ~ Softmax(Q) @ Softmax(K).T @ V\n",
        "\n",
        "# https://github.com/lucidrains/x-transformers/blob/main/x_transformers/x_transformers.py#L1855\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_head=4, cond_dim=None, ff_mult=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm1 = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.norm2 = nn.RMSNorm(d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.cond_dim = cond_dim\n",
        "        self.self = MultiHeadAttention(d_model, d_head=d_head, dropout=0)\n",
        "        if self.cond_dim!=None: self.cross = MultiHeadAttention(d_model, d_head=d_head, cond_dim=cond_dim, dropout=0)\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        ff_dim=d_model*ff_mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), nn.Dropout(dropout), nn.Linear(d_model, ff_dim), act, # ReLU GELU\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "            nn.RMSNorm(d_model), nn.Dropout(dropout), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            # nn.RMSNorm(d_model), act, nn.Dropout(dropout), nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,c,h,w], [batch, num_tok, cond_dim], [batch,T]\n",
        "        # bchw = x.shape\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "\n",
        "        if self.cond_dim==None: cond=None # is self attn\n",
        "        x = x + self.drop(self.self(self.norm1(x)))\n",
        "        if self.cond_dim!=None: x = x + self.cross(self.norm2(x), cond, mask)\n",
        "        x = x + self.ff(x)\n",
        "        # x = x + self.drop(self.ff(x))\n",
        "\n",
        "        # return x.transpose(1,2).reshape(*bchw)\n",
        "        return x\n",
        "\n",
        "\n",
        "# class SimpleViT(nn.Module):\n",
        "#     def __init__(self, in_dim, out_dim, dim, depth, heads, mlp_dim, channels=3, dim_head=8):\n",
        "#         super().__init__()\n",
        "#         self.to_patch_embedding = nn.Sequential( # in, out, kernel, stride, pad\n",
        "#             nn.Conv2d(in_dim, dim, kernel_size=7, stride=2, padding=7//2, bias=False), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "#             # nn.Conv2d(in_dim, dim, kernel_size=7, stride=2, padding=7//2, bias=False), nn.MaxPool2d(2,2)\n",
        "#             )\n",
        "#         # self.pos_embedding = LearnedRoPE2D(dim) # LearnedRoPE2D, RoPE2D\n",
        "#         self.positional_emb = nn.Parameter(torch.zeros(1, 8*8, dim)) # positional_embedding == 'learnable'\n",
        "#         self.transformer = AttentionBlock(d_model=dim, d_head=dim_head)\n",
        "#         self.transformer_blocks = nn.ModuleList([AttentionBlock(d_model=dim, d_head=dim_head) for i in range(depth)])\n",
        "\n",
        "#         self.attention_pool = nn.Linear(dim, 1)\n",
        "#         self.out = nn.Linear(dim, out_dim, bias=False)\n",
        "\n",
        "#     def forward(self, img):\n",
        "#         device = img.device\n",
        "#         x = self.to_patch_embedding(img)\n",
        "#         # x = self.pos_embedding(x)\n",
        "#         x = x.flatten(-2).transpose(-2,-1) # b c h w -> b (h w) c\n",
        "#         x = x + self.positional_emb\n",
        "#         x = self.transformer(x)\n",
        "#         # for blk in self.predictor_blocks: x = blk(x)\n",
        "\n",
        "#         # x = x.flatten(-2).mean(dim=-1) # meal pool\n",
        "#         attn_weights = self.attention_pool(x).squeeze(-1) # [batch, (h,w)] # seq_pool\n",
        "#         x = (attn_weights.softmax(dim = 1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, (h,w)] @ [batch, (h,w), dim]\n",
        "#         # cls_token = repeat(self.class_emb, '1 1 d -> b 1 d', b = b)\n",
        "#         # x = torch.cat((cls_token, x), dim=1)\n",
        "#         # x = x[:, 0] # first token\n",
        "#         return self.out(x)\n",
        "\n",
        "# ijepa: 224*224 -> 14*14 = 196\n",
        "# 32*7\n",
        "# me: 32*32=1024 -> 256\n",
        "\n",
        "import torch\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "def apply_masks(x, masks):\n",
        "    \"\"\":param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
        "    :param masks: list of tensors containing indices of patches in [N] to keep\"\"\"\n",
        "    all_x = []\n",
        "    for m in masks: # [1,T]\n",
        "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [1,T,dim]\n",
        "        all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "    return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "# batch, seq_len, d_model = 2,7,4\n",
        "# x = torch.rand(batch, seq_len, d_model)\n",
        "# print(x)\n",
        "# masks = [torch.randint(0,seq_len,(2,3,))]\n",
        "# print(masks)\n",
        "# out = apply_masks(x, masks)\n",
        "# print(out)\n",
        "# # print(out.shape)\n",
        "\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        # self.positional_emb = nn.Parameter(torch.zeros(1, 32*32//4, d_model)) # positional_embedding == 'learnable'\n",
        "        # encoder_layers = nn.TransformerEncoderLayer(d_model, d_model//d_head, d_hid or d_model, dropout, batch_first=True)\n",
        "        # self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.zeros(1,1,d_model)) # randn zeros\n",
        "        nn.init.trunc_normal_(self.cls, std=.02)\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model)\n",
        "        self.lin = nn.Linear(d_model, out_dim)# if out_dim != d_model else None\n",
        "\n",
        "    def forward(self, src, context_indices, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        src = self.embed(src) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = src.shape\n",
        "        src = src * self.pos_encoder(context_indices)\n",
        "        # src = src + self.positional_emb[:,context_indices]\n",
        "        # src = apply_masks(src, [context_indices])\n",
        "\n",
        "        pred_tokens = self.cls * self.pos_encoder(trg_indices) # [M, num_trg_toks, d_model]\n",
        "        # pred_tokens = self.cls + self.positional_emb[0,trg_indices]\n",
        "        pred_tokens = pred_tokens.repeat(batch, 1, 1) # [batch*M, num_trg_toks, d_model]\n",
        "        # print(pred_tokens.requires_grad)\n",
        "        # print(\"pred fwd\", src.shape, pred_tokens.shape)\n",
        "        # src = src.repeat_interleave(trg_indices.shape[0], dim=0) # [batch, seq_len, d_model] -> [batch*M, seq_len, d_model]\n",
        "        src = torch.cat([src, pred_tokens], dim=1) # [batch*M, seq_len+num_trg_toks, d_model]\n",
        "\n",
        "        out = self.transformer_encoder(src) # float [seq_len, batch_size, d_model]\n",
        "        out = self.norm(out)\n",
        "        out = out[:,seq:] # [batch*M, num_trg_toks, d_model]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.MaxPool1d(2,2), #nn.MaxPool1d(3, 2, 3//2),\n",
        "            nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.Conv1d(d_model, d_model,3,2,3//2)\n",
        "            # nn.Conv1d(in_dim, d_model,2,2,0), # like patch\n",
        "            )\n",
        "        self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        # self.positional_emb = nn.Parameter(torch.zeros(1, 32*32//4, d_model)) # positional_embedding == 'learnable'\n",
        "        # encoder_layers = nn.TransformerEncoderLayer(d_model, d_model//d_head, d_hid or d_model, dropout, batch_first=True)\n",
        "        # self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model)\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, src, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # src = self.embed(src) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        src = self.embed(src.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "\n",
        "        batch, seq, dim = src.shape\n",
        "\n",
        "        # # # src = self.pos_encoder(src)\n",
        "        # if context_indices != None:\n",
        "        # print(src.shape, self.pos_encoder(context_indices).shape)\n",
        "        #     src = src * self.pos_encoder(context_indices) # context/predictor # src = src + self.positional_emb[:,context_indices]\n",
        "        # else: src = src * self.pos_encoder(torch.arange(seq, device=device).unsqueeze(0)) # target # src = src + self.positional_emb[:,:seq]\n",
        "\n",
        "\n",
        "        # src = self.pos_encoder(src)\n",
        "        src = src * self.pos_encoder(torch.arange(seq, device=device)).unsqueeze(0)\n",
        "        # src = src + self.positional_emb\n",
        "        if context_indices != None:\n",
        "            src = apply_masks(src, [context_indices])\n",
        "\n",
        "\n",
        "        out = self.transformer_encoder(src) # float [seq_len, batch_size, d_model]\n",
        "        out = self.norm(out)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "batch, seq_len, d_model = 4,1024,4\n",
        "in_dim = 3\n",
        "model = TransformerModel(in_dim, d_model, d_head=4, nlayers=2, dropout=0.).to(device)\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "# # print(out)\n",
        "\n"
      ],
      "metadata": {
        "id": "GHzr3NVs2EcG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cecf37be-0d0f-4890-83f3-305b0b01c3ea",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 256, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(4,100,3)\n",
        "\n",
        "x=x.transpose(-2,-1) # [batch, dim, seq]\n",
        " # in, out, kernel, stride, pad\n",
        "# net = nn.Conv1d(3,16,7,2,7//2)\n",
        "net = nn.Sequential(nn.Conv1d(3,16,7,2,7//2), nn.MaxPool1d(3, 2, 3//2))\n",
        "# net = nn.Conv1d(3,16,(7,3),2,3//2)\n",
        "out = net(x)\n",
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_YBe6-eZ2Zq",
        "outputId": "09b27f75-ba04-4aaf-f5e4-1b9c1dc8d112"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 16, 25])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test repeat_interleave\n",
        "# x = torch.rand(3,5)\n",
        "x = torch.rand(3,5,7)\n",
        "print(x)\n",
        "# out = x.repeat(2,1) # [2*3,5]\n",
        "# print(out)\n",
        "# x_ = out.reshape(2,3,5)\n",
        "# print(x_)\n",
        "# out = x.repeat_interleave(2,1,1) # [3*2,5]\n",
        "out = x.repeat_interleave(2,dim=0) # [3*2,5]\n",
        "# print(out)\n",
        "# x_ = out.reshape(2,3,5,7)\n",
        "x_ = out.reshape(3,2,5,7)\n",
        "print(x_)\n",
        "\n",
        "\n",
        "# batch=1\n",
        "# seq_len=5\n",
        "# dim=4\n",
        "# positional_emb = torch.rand(batch, seq_len, dim)\n",
        "# print(positional_emb)\n",
        "# num_tok=2\n",
        "# trg_indices=torch.randint(0,seq_len,(M, num_tok))\n",
        "# print(trg_indices)\n",
        "# # pos_embs = positional_emb.gather(1, trg_indices.unsqueeze(0))\n",
        "# pos_embs = positional_emb[torch.arange(batch)[...,None,None],trg_indices.unsqueeze(0)]\n",
        "# # pos_embs = positional_emb[0,trg_indices]\n",
        "# # [batch, M, num_tok, dim]\n",
        "# print(pos_embs.shape)\n",
        "# print(pos_embs)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "R6kiiqw3uIdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test ropes\n",
        "# for i, (img, _) in enumerate(test_loader):\n",
        "#     break\n",
        "# print(img.shape) # [batch, 1, 28, 28]\n",
        "# print(img[0]) # [0,1)\n",
        "d_model=8\n",
        "# rot_emb = RotEmb(d_model, top=torch.pi, base=10)\n",
        "# x = torch.linspace(0,1,20)\n",
        "# out = rot_emb(x)\n",
        "# print(out.shape)\n",
        "# print(out)\n",
        "\n",
        "# # pos_encoder = RoPE(d_model, seq_len=15, base=100)\n",
        "# pos_encoder = RoPE(d_model, seq_len=15, base=10000)\n",
        "x = torch.ones(1, 10, d_model)\n",
        "# # x = torch.ones(1, 10, d_model)\n",
        "# out = pos_encoder(x)\n",
        "# # print(out.shape)\n",
        "# for x in out[0]:\n",
        "#     print(x)\n",
        "\n",
        "\n",
        "# pos_encoder = LearnedRoPE(d_model, seq_len=512)\n",
        "# out = pos_encoder(x)\n",
        "# for o in out[0]:\n",
        "#     print(o)\n",
        "\n",
        "# pos_encoder.weights = nn.Parameter(torch.randn(1, d_model//2))\n",
        "# out = pos_encoder(x)\n",
        "# for o in out[0]:\n",
        "#     print(o)\n",
        "\n",
        "\n",
        "# tensor([-0.0177, -0.9998,  0.8080, -0.5892, -0.7502, -0.6612,  0.3885,  0.9214])\n"
      ],
      "metadata": {
        "id": "8r_GjI_vCMyo",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title TransformerClassifier\n",
        "import math\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class TransformerClassifier(nn.Module):\n",
        "    # def __init__(self, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, drop = 0.):\n",
        "    def __init__(self, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, drop=0):\n",
        "        super().__init__()\n",
        "        # self.embedding = nn.Embedding(ntoken, d_model)\n",
        "        self.pos_encoder = RoPE(d_model, seq_len=200, base=10000)\n",
        "        # d_hid = d_hid or d_model#*2\n",
        "        # encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, d_hid, drop, batch_first=True)\n",
        "        # self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "\n",
        "        out_dim = out_dim or d_model\n",
        "        self.lin = nn.Linear(d_model, out_dim)\n",
        "        # self.lin = nn.Linear(d_model*2, out_dim)\n",
        "        # self.cls = nn.Parameter(torch.randn(1,1,d_model))\n",
        "        self.attention_pool = nn.Linear(d_model, 1, bias=False)\n",
        "        # self.out = nn.Sequential(\n",
        "        #     nn.Dropout(drop), nn.Linear(d_model, 3), nn.SiLU(),\n",
        "        #     nn.Dropout(drop), nn.Linear(3, out_dim), nn.Sigmoid()\n",
        "        # )\n",
        "\n",
        "    def forward(self, x, src_key_padding_mask = None): # [batch, seq, d_model], [batch, seq] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # batch, seq_len, d_model = x.shape\n",
        "        # x = torch.cat([self.cls.repeat(batch,1,1), x], dim=1)\n",
        "        # src_key_padding_mask = torch.cat([torch.zeros((batch, 1), dtype=torch.bool), src_key_padding_mask], dim=1)\n",
        "        x = self.pos_encoder(x)\n",
        "\n",
        "        x = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
        "        # print(\"fwd\",out.shape) # float [batch, seq_len, d_model]\n",
        "        # out = x.mean(dim=1) # average pool\n",
        "        # out = x.max(dim=1)#[0]\n",
        "\n",
        "        attn = self.attention_pool(x).squeeze(-1) # [batch, seq] # seq_pool\n",
        "        out = (torch.softmax(attn, dim=-1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, seq] @ [batch, seq, dim] -> [batch, dim]\n",
        "        # out = self.out(out) # optional (from GlobalContext) like squeeze excitation\n",
        "\n",
        "        # out = out[:, 0] # first token\n",
        "        # out = torch.cat([out, mean_pool], dim=-1)\n",
        "\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "batch, seq_len, d_model = 4,7,512\n",
        "model = TransformerModel(in_dim, d_model, d_head=4, nlayers=2, dropout=0.).to(device)\n",
        "# model = TransformerClassifier(d_model, nhead=8, nlayers=2, dropout=0.).to(device)\n",
        "x =  torch.rand(batch, seq_len, d_model)\n",
        "src_key_padding_mask = torch.stack([(torch.arange(seq_len) < seq_len - v) for v in torch.randint(seq_len, (batch,))]) # True will be ignored # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "print(src_key_padding_mask)\n",
        "out = model(x, src_key_padding_mask)\n",
        "print(out.shape)\n",
        "\n",
        "# GlobalAveragePooling1D layer, followed by a Dense layer. The final output of the transformer is produced by a softmax layer,\n",
        "# x = GlobalAveragePooling1D()(x)\n",
        "# x = Dropout(0.1)(x)\n",
        "# x = Dense(20, activation=\"relu\")(x)\n",
        "# x = Dropout(0.1)(x)\n",
        "# outputs = Dense(2, activation=\"softmax\")(x)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "AtPUcVu2kSLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Transformer Model\n",
        "import torch\n",
        "from torch import nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        # self.embed = nn.Sequential(\n",
        "        #     nn.Linear(in_dim, d_model), #act,\n",
        "        #     # nn.Linear(d_model, d_model), act,\n",
        "        # )\n",
        "        self.embed = nn.Linear(in_dim, d_model) if in_dim != d_model else None\n",
        "        # self.embed = nn.Sequential(nn.Conv1d(in_dim,d_model,7,2,7//2), nn.MaxPool1d(3, 2, 3//2))\n",
        "        self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, d_hid or d_model, dropout, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.d_model = d_model\n",
        "        self.cls = nn.Parameter(torch.zeros(1,1,d_model)) # randn\n",
        "        out_dim = out_dim or d_model\n",
        "        # self.lin = nn.Linear(d_model, out_dim)\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim != d_model else None\n",
        "        self.norm = nn.LayerNorm(out_dim)\n",
        "\n",
        "\n",
        "    def forward(self, src, src_key_padding_mask=None, cls_mask=None, context_indices=None, trg_indices=None): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # if cls_mask != None: src[cls_mask] = self.cls.to(src.dtype)\n",
        "\n",
        "        if self.embed != None: src = self.embed(src) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = src.shape\n",
        "        # src = self.pos_encoder(src)\n",
        "        if context_indices != None:\n",
        "            # print(src.shape, context_indices.shape, self.pos_encoder(context_indices).shape) # [2, 88, 32]) torch.Size([88]) torch.Size([88, 32]\n",
        "            # print(src[0], self.pos_encoder(context_indices)[0])\n",
        "            src = src * self.pos_encoder(context_indices) # context/predictor # src = src + self.positional_emb[:,context_indices]\n",
        "            # print(src[0])\n",
        "        else: src = src * self.pos_encoder(torch.arange(seq, device=device)) # target # src = src + self.positional_emb[:,:seq]\n",
        "            # print(\"trans fwd\", src.shape, self.pos_encoder(src).shape)\n",
        "\n",
        "        if trg_indices != None: # [M, num_trg_toks]\n",
        "            pred_tokens = self.cls * self.pos_encoder(trg_indices) # [M, num_trg_toks, d_model] # pred_tokens = self.cls + self.positional_emb[0,trg_indices]\n",
        "            pred_tokens = pred_tokens.repeat(batch, 1, 1) # [batch*M, num_trg_toks, d_model]\n",
        "            # print(pred_tokens.requires_grad)\n",
        "            src = src.repeat_interleave(trg_indices.shape[0], dim=0) # [batch, seq_len, d_model] -> [batch*M, seq_len, d_model]\n",
        "            src = torch.cat([src, pred_tokens], dim=1) # [batch*M, seq_len+num_trg_toks, d_model]\n",
        "\n",
        "        out = self.transformer_encoder(src, src_key_padding_mask=src_key_padding_mask) # float [seq_len, batch_size, d_model]\n",
        "        if trg_indices != None:\n",
        "            # print(out.shape)\n",
        "            out = out[:,seq:] # [batch*M, num_trg_toks, d_model]\n",
        "        if self.lin != None: out = self.lin(out)\n",
        "        out = self.norm(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "batch, seq_len, d_model = 4,7,64\n",
        "in_dim = 3\n",
        "model = TransformerModel(in_dim, d_model, nhead=8, nlayers=2, dropout=0.).to(device)\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "# print(out.shape)\n",
        "# # print(out)\n"
      ],
      "metadata": {
        "id": "-8i1WLsvH_vn",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ba31127-d6e5-438a-aede-5c789557ab39",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title SeqJEPA\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "def multiblock(seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "    return target_mask\n",
        "\n",
        "\n",
        "class SeqJEPA(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, num_layers=1, num_classes=10):\n",
        "        super().__init__()\n",
        "        if out_dim is None: out_dim = d_model\n",
        "        act = nn.GELU() # ReLU GELU SiLU\n",
        "        # self.rot_emb = RotEmb(d_model, top=1, base=10)\n",
        "        # self.rot_emb = RotEmb(d_model, top=torch.pi, base=10)\n",
        "        # self.rot_emb = RoPE(d_model, seq_len=1024, base=10)\n",
        "        # self.rot_emb = LearnedRotEmb(dim)\n",
        "        self.encode = nn.Sequential(\n",
        "            nn.Linear(in_dim, d_model), #act,\n",
        "            # nn.Linear(d_model, d_model), act,\n",
        "        )\n",
        "        # self.encode = nn.Sequential(nn.Conv1d(in_dim, d_model,7,2,7//2), nn.MaxPool1d(3, 2, 3//2))\n",
        "        self.context_encoder = TransformerModel(d_model, d_model, out_dim=out_dim, nhead=4, nlayers=2, dropout=0.)\n",
        "        self.predicter = TransformerModel(out_dim, d_model//2, out_dim, nhead=4, nlayers=1, dropout=0.)\n",
        "        self.target_encoder = AveragedModel(self.context_encoder, multi_avg_fn=get_ema_multi_avg_fn(0.85)) # 0.95 0.999\n",
        "        self.target_encoder.requires_grad_(False)\n",
        "        # self.classifier = nn.Linear(out_dim, num_classes)\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]\n",
        "        x = self.encode(x) # [batch, T, d_model]\n",
        "        # x = self.encode(x.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "\n",
        "        batch, seq, dim = x.shape\n",
        "        M=1 # 4\n",
        "        # target_mask = multiblock(seq, min_s=0.15, max_s=0.2, M=M) # mask out targets to be predicted # [M, seq]\n",
        "        target_mask = multiblock(seq, min_s=0.15, max_s=0.2, M=4)#.any(0) # mask out targets to be predicted # [M, seq]\n",
        "        context_mask = ~multiblock(seq, min_s=0.85, max_s=1., M=1)|target_mask.reshape(M,seq).any(0) # [1, seq], True->Mask\n",
        "        target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "        # print(target_mask, context_mask)\n",
        "        sorted_mask, ids = context_mask.int().sort(dim=1, stable=True)\n",
        "        context_indices = ids[~sorted_mask.bool()] # int idx [num_context_toks] , idx of context not masked\n",
        "        sorted_x = x[torch.arange(batch).unsqueeze(-1), context_indices] # [batch, num_context_toks, d_model]\n",
        "        # print(sorted_x.shape, context_indices.shape)[2, 9, 32]) torch.Size([9])\n",
        "        # print(sorted_x[0])\n",
        "        # print(sorted_x.is_leaf)\n",
        "        sorted_sx = self.context_encoder(sorted_x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "        # del sorted_x\n",
        "        # print(sorted_sx[0])\n",
        "\n",
        "        sorted_mask, ids = target_mask.int().sort(dim=1, stable=True)\n",
        "        # print(sorted_mask.shape, ids.shape, ids[~sorted_mask.bool()].shape, sorted_mask.sum(-1))\n",
        "        trg_indices = ids[sorted_mask.bool()].reshape(M,-1) # int idx [M, num_trg_toks] , idx of targets that are masked\n",
        "        # sorted_x = x[torch.arange(batch)[...,None,None], indices] # [batch, M, seq-num_trg_toks, dim]\n",
        "        # sx = sorted_sx[torch.arange(batch).unsqueeze(-1),ids.argsort(1)]\n",
        "        # print(sorted_sx.shape, trg_indices.shape)\n",
        "        sy_ = self.predicter(sorted_sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "        # sx = self.context_encoder(x, src_key_padding_mask=context_mask).repeat(M,1,1)\n",
        "        # # sy_ = self.predicter(sx, cls_mask=target_mask)*target_mask.unsqueeze(-1)\n",
        "        # sy_ = self.predicter(sx, trg_mask=target_mask)*target_mask.unsqueeze(-1)\n",
        "\n",
        "        # print(sy_.shape, target_mask.shape)\n",
        "        # print(self.target_encoder(x).repeat_interleave(M, dim=0).shape, trg_indices.repeat(batch,1).shape)\n",
        "        with torch.no_grad():\n",
        "            sy = self.target_encoder(x).repeat_interleave(M, dim=0)[torch.arange(batch*M).unsqueeze(-1), trg_indices.repeat(batch,1)] # [batch*M, num_trg_toks, out_dim]\n",
        "\n",
        "        # print(sy_[0])\n",
        "        # print(sy.shape, sy_.shape)\n",
        "        # loss = F.smooth_l1_loss(sy, sy_)\n",
        "        loss = F.mse_loss(sy, sy_)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]\n",
        "        batch, seq, dim = x.shape\n",
        "        target_mask = randpatch(seq//self.patch_size, mask_size=16, gamma=.75) # [seq]\n",
        "        context_mask = ~multiblock(seq//self.patch_size, min_s=0.85, max_s=1., M=1)|target_mask # [1, seq], True->Mask\n",
        "        target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "        # print(target_mask, context_mask)\n",
        "        sorted_mask, ids = context_mask.int().sort(dim=1, stable=True)\n",
        "        context_indices = ids[~sorted_mask.bool()]#.unsqueeze(0) # int idx [num_context_toks] , idx of context not masked\n",
        "        sorted_x = x[torch.arange(batch).unsqueeze(-1), context_indices] # [batch, num_context_toks, 3]\n",
        "        print(sorted_x.shape, context_indices.shape)\n",
        "        sorted_sx = self.context_encoder(sorted_x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "\n",
        "        sorted_mask, ids = target_mask.int().sort(dim=-1, stable=True)\n",
        "        trg_indices = ids[sorted_mask.bool()].unsqueeze(0) # int idx [1, num_trg_toks] , idx of targets that are masked\n",
        "        print(sorted_sx.shape, context_indices.shape)\n",
        "        sy_ = self.predicter(sorted_sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "\n",
        "        with torch.no_grad(): sy = self.target_encoder(x.detach())[torch.arange(batch).unsqueeze(-1), trg_indices.repeat(batch,1)] # [batch, num_trg_toks, out_dim]\n",
        "        # print(x[0][0][:8])\n",
        "        # print(sy_[0][0][:8])\n",
        "        # print(sy.shape, sy_.shape)\n",
        "        # loss = F.smooth_l1_loss(sy, sy_)\n",
        "        loss = F.mse_loss(sy.detach(), sy_)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        x = self.encode(x)\n",
        "        # x = self.encode(x.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "        sx = self.target_encoder(x)\n",
        "        out = sx.mean(dim=1)\n",
        "        return out\n",
        "\n",
        "\n",
        "# min_s=0.15, max_s, M\n",
        "# trg.15.2M4 C.85 1\n",
        "\n",
        "seq_jepa = SeqJEPA(in_dim=3, d_model=32, out_dim=16, num_layers=1).to(device)#.to(torch.float)\n",
        "optim = torch.optim.AdamW(seq_jepa.parameters(), lr=1e-4) # 1e-3?\n",
        "print(sum(p.numel() for p in seq_jepa.parameters() if p.requires_grad)) # 59850\n",
        "\n",
        "\n",
        "# x = torch.rand((2,20,3), device=device)\n",
        "# out = seq_jepa.loss(x)\n",
        "# print(out.shape)\n",
        "# print(x.is_leaf) # T\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title SeqJEPA old\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def multiblock(batch, seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "        mask_len = torch.rand(batch, M) * (max_s - min_s) + min_s # in (min_s, max_s)\n",
        "        mask_pos = torch.rand(batch, M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "        mask_len, mask_pos = mask_len * seq, mask_pos * seq\n",
        "        indices = torch.arange(seq)[None,None,...]\n",
        "        target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [batch, M, seq]\n",
        "        target_mask = target_mask.any(1) # True -> masked # multiblock masking # [batch, seq]\n",
        "        return target_mask\n",
        "\n",
        "\n",
        "\n",
        "class SeqJEPA(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.in_dim = in_dim\n",
        "        # if out_dim is None: self.out_dim = in_dim\n",
        "        if out_dim is None: out_dim = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.d_model = d_model\n",
        "        act = nn.GELU() # ReLU GELU SiLU\n",
        "\n",
        "        dim=8\n",
        "\n",
        "\n",
        "        self.calorie_emb = RotEmb(dim, top=1, base=10) # 0-4\n",
        "        self.steps_emb = RotEmb(dim, top=1, base=10) # 0-5\n",
        "        self.enc0 = nn.Sequential(\n",
        "            nn.Linear(dim*2, d_model), act,\n",
        "            nn.Linear(d_model, d_model), act,\n",
        "        )\n",
        "        self.hour_emb = CircularEmb(dim, torch.pi/2) # circular (0,1) # 0-24\n",
        "        self.temp_emb = RotEmb(dim, top=1/3, base=10) # 18-35\n",
        "        self.heart_emb = RotEmb(dim, top=1/3, base=100) # 50-200\n",
        "        self.enc1 = nn.Sequential(\n",
        "            nn.Linear(dim*3, d_model), act,\n",
        "        )\n",
        "\n",
        "        # self.transformer = TransformerClassifier(d_model, out_dim=out_dim, nhead=8, nlayers=2)\n",
        "        # # self.fc = nn.Linear(d_model, out_dim)\n",
        "        self.context_encoder = TransformerModel(d_model, out_dim=out_dim, nhead=8, nlayers=2, dropout=0.)\n",
        "        self.predicter = TransformerModel(out_dim, nhead=8, nlayers=1, dropout=0.)\n",
        "        self.target_encoder = AveragedModel(self.context_encoder, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    def loss(self, hr, temp, heart): # [batch, 1+T]\n",
        "        # batch = hr.size(0)\n",
        "        res_id, activeKilocalories, steps = hr[:,0], temp[:,0], heart[:,0]\n",
        "        # enc_summary = self.enc0(torch.cat([self.calorie_emb(activeKilocalories), self.steps_emb(steps)], dim=-1)) # [batch, d_model]\n",
        "        enc_summary = self.enc0(torch.cat([self.calorie_emb(torch.log(activeKilocalories.clamp(min=1))), self.steps_emb(torch.log(steps.clamp(min=1)))], dim=-1)) # [batch, d_model]\n",
        "        x = self.enc1(torch.cat([self.hour_emb(hr[:,1:]/24), self.temp_emb(temp[:,1:]), self.heart_emb(heart[:,1:])], dim=-1)) # [batch, T, d_model]\n",
        "        # print('violet fwd', hr.shape, enc_summary.shape, x.shape)\n",
        "\n",
        "        # # mask=(torch.rand(self.seq_len)<.1) # True -> masked # random masking\n",
        "        # mask=(torch.rand_like(x)<.1) # True -> masked # random masking\n",
        "\n",
        "        # patches pad -enc> patch_enc\n",
        "        # rearrange patch in padded/masked, -pred> pred of masked\n",
        "        # mse tgt_enc(img)\n",
        "\n",
        "        # average L2 distance between the predicted patch-level representations sˆy(i) and the target patch-level representation sy(i);\n",
        "        # F.smooth_l1_loss\n",
        "\n",
        "        batch, seq, dim = x.shape\n",
        "\n",
        "        # mask=(torch.rand(batch, seq)<.75) # True -> masked # random masking\n",
        "        # sorted_mask, ids = mask.sort(dim=1)\n",
        "        # # sorted_x = x[torch.arange(batch)[...,None,None],ids]\n",
        "        # sorted_x = x[torch.arange(batch).unsqueeze(-1),ids]\n",
        "        # sorted_mask = torch.cat([torch.zeros((batch, 1), dtype=torch.bool), sorted_mask], dim=1) # True->mask\n",
        "        # sorted_x = torch.cat([enc_summary.unsqueeze(1), sorted_x], dim=1)\n",
        "        # sorted_sx = self.context_encoder(sorted_x, src_key_padding_mask=sorted_mask)\n",
        "        # # torch.zeros_like(sorted_sx)\n",
        "        # sx = sorted_sx[torch.arange(batch).unsqueeze(-1),ids.argsort(1)]\n",
        "        # sy_ = self.predicter(sx, src_key_padding_mask=mask)\n",
        "        # sy = self.target_encoder(x)*~mask\n",
        "\n",
        "\n",
        "        # target_mask = multiblock(batch, seq, min_s=0.15, max_s=0.2, M=4)\n",
        "        # context_mask = ~multiblock(batch, seq, min_s=0.85, max_s=1., M=1)|target_mask\n",
        "        # sx = self.context_encoder(x, src_key_padding_mask=context_mask)\n",
        "        # sy_ = self.predicter(sx, cls_mask=target_mask)*target_mask.unsqueeze(-1)\n",
        "        # sy_ = self.predicter(sx, cls_mask=target_mask)\n",
        "        # # print(sy_.shape, target_mask.shape)\n",
        "        # sy = self.target_encoder(x)*target_mask.unsqueeze(-1)\n",
        "\n",
        "\n",
        "        M=4\n",
        "        target_mask = multiblock(M*batch, seq, min_s=0.15, max_s=0.2, M=1) # mask out targets to be predicted # [4*batch, seq]\n",
        "        context_mask = ~multiblock(batch, seq, min_s=0.85, max_s=1., M=1)|target_mask.reshape(batch,M,seq).any(1) # [batch, seq]\n",
        "\n",
        "\n",
        "        x = torch.cat([enc_summary.unsqueeze(1), x], dim=1) # [batch, 1+T, d_model]\n",
        "        target_mask = torch.cat([torch.zeros((M*batch, 1), dtype=bool), target_mask],dim=1) # [4*batch, 1+T]\n",
        "        context_mask = torch.cat([torch.zeros((batch, 1), dtype=bool), context_mask],dim=1) # [batch, 1+T]\n",
        "\n",
        "        # x = x.repeat(4,1,1)\n",
        "        # context_mask = context_mask.repeat(4,1)\n",
        "        sx = self.context_encoder(x, src_key_padding_mask=context_mask).repeat(M,1,1)\n",
        "        sy_ = self.predicter(sx, cls_mask=target_mask)*target_mask.unsqueeze(-1)\n",
        "        # sy_ = self.predicter(sx, cls_mask=target_mask)\n",
        "        # print(sy_.shape, target_mask.shape)\n",
        "        sy = self.target_encoder(x).repeat(M,1,1)*target_mask.unsqueeze(-1)\n",
        "\n",
        "        loss = F.smooth_l1_loss(sy, sy_)\n",
        "        return loss\n",
        "\n",
        "    # def forward(self, hr, temp, heart): # [batch, 1+T]\n",
        "    #     # batch = hr.size(0)\n",
        "    #     # batch, seq, dim = x.shape\n",
        "    #     res_id, activeKilocalories, steps = hr[:,0], temp[:,0], heart[:,0]\n",
        "    #     enc_summary = self.enc0(torch.cat([self.calorie_emb(activeKilocalories), self.steps_emb(steps)], dim=-1)) # [batch, d_model]\n",
        "    #     x = self.enc1(torch.cat([self.hour_emb(hr[:,1:]/24), self.temp_emb(temp[:,1:]), self.heart_emb(heart[:,1:])], dim=-1)) # [batch, T, d_model]\n",
        "\n",
        "    #     x = torch.cat([enc_summary.unsqueeze(1), x], dim=1)\n",
        "    #     # sx = self.context_encoder(x)\n",
        "    #     sx = self.target_encoder(x)\n",
        "    #     out = sx.mean(dim=1)\n",
        "    #     return out\n",
        "\n",
        "\n",
        "\n",
        "seq_jepa = SeqJEPA(in_dim=16, d_model=32, out_dim=16, num_layers=1).to(device)#.to(torch.float)\n",
        "soptim = torch.optim.AdamW(seq_jepa.parameters(), lr=1e-3)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "O-OU1MaKF7BZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ijepa vision_transformer.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/models/vision_transformer.py\n",
        "import math\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from src.utils.tensors import (trunc_normal_, repeat_interleave_batch)\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid height and width\n",
        "    return:\n",
        "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid_h = np.arange(grid_size, dtype=float)\n",
        "    grid_w = np.arange(grid_size, dtype=float)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    assert embed_dim % 2 == 0\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid length\n",
        "    return:\n",
        "    pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid = np.arange(grid_size, dtype=float)\n",
        "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=float)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega   # (D/2,)\n",
        "    pos = pos.reshape(-1)   # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
        "    emb = np.concatenate([np.sin(out), np.cos(out)], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x, attn\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x, return_attention=False):\n",
        "        y, attn = self.attn(self.norm1(x))\n",
        "        if return_attention:\n",
        "            return attn\n",
        "        x = x + self.drop_path(y)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvEmbed(nn.Module):\n",
        "    \"\"\"3x3 Convolution stems for ViT following ViTC models\"\"\"\n",
        "    def __init__(self, channels, strides, img_size=224, in_chans=3, batch_norm=True):\n",
        "        super().__init__()\n",
        "        # Build the stems\n",
        "        stem = []\n",
        "        channels = [in_chans] + channels\n",
        "        for i in range(len(channels) - 2):\n",
        "            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3, stride=strides[i], padding=1, bias=(not batch_norm))]\n",
        "            if batch_norm:\n",
        "                stem += [nn.BatchNorm2d(channels[i+1])]\n",
        "            stem += [nn.ReLU(inplace=True)]\n",
        "        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n",
        "        self.stem = nn.Sequential(*stem)\n",
        "\n",
        "        # Comptute the number of patches\n",
        "        stride_prod = int(np.prod(strides))\n",
        "        self.num_patches = (img_size[0] // stride_prod)**2\n",
        "\n",
        "    def forward(self, x):\n",
        "        p = self.stem(x)\n",
        "        return p.flatten(2).transpose(1, 2)\n",
        "\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "def apply_masks(x, masks):\n",
        "    \"\"\":param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
        "    :param masks: list of tensors containing indices of patches in [N] to keep\"\"\"\n",
        "    all_x = []\n",
        "    for m in masks:\n",
        "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [batch,T,dim]\n",
        "        all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "    return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "\n",
        "class VisionTransformerPredictor(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, num_patches, embed_dim=768, predictor_embed_dim=384,\n",
        "        depth=6, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.init_std = init_std\n",
        "        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        # --\n",
        "        self.predictor_pos_embed = nn.Parameter(torch.zeros(1, num_patches, predictor_embed_dim), requires_grad=False)\n",
        "        predictor_pos_embed = get_2d_sincos_pos_embed(self.predictor_pos_embed.shape[-1], int(num_patches**.5), cls_token=False)\n",
        "        self.predictor_pos_embed.data.copy_(torch.from_numpy(predictor_pos_embed).float().unsqueeze(0)) # [1, num_patches, predictor_embed_dim]\n",
        "        # --\n",
        "        self.predictor_blocks = nn.ModuleList([Block(dim=predictor_embed_dim, num_heads=num_heads, drop=drop_rate, drop_path=dpr[i]) for i in range(depth)])\n",
        "        self.predictor_norm = nn.LayerNorm(predictor_embed_dim)\n",
        "        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n",
        "        # ------\n",
        "        # trunc_normal_(self.mask_token, std=self.init_std)\n",
        "\n",
        "    def forward(self, x, masks_x, masks):\n",
        "        assert (masks is not None) and (masks_x is not None), 'Cannot run predictor without mask indices'\n",
        "        if not isinstance(masks_x, list): masks_x = [masks_x]\n",
        "        if not isinstance(masks, list): masks = [masks]\n",
        "\n",
        "        # -- Batch Size\n",
        "        B = len(x) // len(masks_x)\n",
        "\n",
        "        # -- map from encoder-dim to pedictor-dim\n",
        "        x = self.predictor_embed(x) # [batch, num_patches, predictor_embed_dim]\n",
        "\n",
        "        # -- add positional embedding to x tokens\n",
        "        x_pos_embed = self.predictor_pos_embed.repeat(B, 1, 1) # [batch, num_patches, predictor_embed_dim]\n",
        "        x += apply_masks(x_pos_embed, masks_x)\n",
        "\n",
        "        _, N_ctxt, D = x.shape\n",
        "\n",
        "        # -- concat mask tokens to x\n",
        "        pos_embs = self.predictor_pos_embed.repeat(B, 1, 1)\n",
        "        pos_embs = apply_masks(pos_embs, masks)\n",
        "        pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x))\n",
        "        # --\n",
        "        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1)\n",
        "        # --\n",
        "        pred_tokens += pos_embs\n",
        "        x = x.repeat(len(masks), 1, 1)\n",
        "        x = torch.cat([x, pred_tokens], dim=1)\n",
        "\n",
        "        # -- fwd prop\n",
        "        for blk in self.predictor_blocks:\n",
        "            x = blk(x)\n",
        "        x = self.predictor_norm(x)\n",
        "\n",
        "        # -- return preds for mask tokens\n",
        "        x = x[:, N_ctxt:]\n",
        "        x = self.predictor_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, img_size=[224], patch_size=16, in_chans=3, embed_dim=768, predictor_embed_dim=384, depth=12, predictor_depth=12,\n",
        "        num_heads=12, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0, norm_layer=nn.LayerNorm, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_features = self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.init_std = init_std\n",
        "        # --\n",
        "        self.patch_embed = PatchEmbed(img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        # --\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=False)\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "        # --\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "    def forward(self, x, masks=None):\n",
        "        if masks is not None:\n",
        "            if not isinstance(masks, list): masks = [masks]\n",
        "\n",
        "        x = self.patch_embed(x)\n",
        "        B, N, D = x.shape # [batch, T, d_model]\n",
        "\n",
        "        # -- add positional embedding to x\n",
        "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
        "        x = x + pos_embed\n",
        "\n",
        "        if masks is not None: x = apply_masks(x, masks)\n",
        "\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            x = blk(x)\n",
        "\n",
        "        if self.norm is not None: x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "    def interpolate_pos_encoding(self, x, pos_embed):\n",
        "        npatch = x.shape[1] - 1\n",
        "        N = pos_embed.shape[1] - 1\n",
        "        if npatch == N: return pos_embed\n",
        "        class_emb, pos_embed = pos_embed[:, 0], pos_embed[:, 1:]\n",
        "        dim = x.shape[-1]\n",
        "        pos_embed = nn.functional.interpolate(pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2), scale_factor=math.sqrt(npatch / N), mode='bicubic',)\n",
        "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
        "\n",
        "# from functools import partial\n",
        "# def vit_predictor(**kwargs):\n",
        "#     model = VisionTransformerPredictor(\n",
        "#         mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "#     return model\n",
        "\n",
        "def vit(patch_size=16, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, # tiny\n",
        "        # patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, # small\n",
        "        # patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, # base\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "bNjp4xFsGPoa",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title show collated_masks\n",
        "# print(collated_batch, collated_masks_enc, collated_masks_pred)\n",
        "# print(collated_batch) # tensor([0, 1, 2, 3]) batch\n",
        "# print(collated_masks_enc) # nenc*[M*[pred mask blocks patch ind]]\n",
        "# print(collated_masks_pred) # npred * [M * [context]]\n",
        "\n",
        "\n",
        "# print(len(collated_masks_enc[0]), len(collated_masks_pred[0]))\n",
        "# print(collated_masks_enc[0], collated_masks_pred[0])\n",
        "\n",
        "h,w=14,14\n",
        "img = torch.ones(h*w)\n",
        "# img[collated_masks_enc[0]]=0\n",
        "img[collated_masks_pred[0][2]]=0\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    # plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.imshow(npimg)\n",
        "    plt.show()\n",
        "\n",
        "# imshow(img.reshape(h,w))\n",
        "\n",
        "# for masks in collated_masks_pred:\n",
        "#     for mask in masks:\n",
        "#         img = torch.ones(h*w)\n",
        "#         img[mask]=0\n",
        "#         imshow(img.reshape(h,w))\n",
        "\n",
        "\n",
        "for masks in collated_masks_enc:\n",
        "    for mask in masks:\n",
        "        img = torch.ones(h*w)\n",
        "        img[mask]=0\n",
        "        imshow(img.reshape(h,w))\n"
      ],
      "metadata": {
        "id": "tf4T37woBV2V",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aH1UOD4WW8I2",
        "outputId": "6fafea1b-0af2-4bc7-fa26-46b5e56549b4",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vit fwd torch.Size([4, 3, 224, 224])\n",
            "vit fwd torch.Size([4, 196, 768])\n",
            "vit fwd torch.Size([4, 11, 768])\n",
            "predictor fwd1 torch.Size([4, 11, 384]) torch.Size([4, 196, 384])\n"
          ]
        }
      ],
      "source": [
        "# @title ijepa vision_transformer.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/models/vision_transformer.py\n",
        "import math\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# from src.utils.tensors import (trunc_normal_, repeat_interleave_batch)\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid height and width\n",
        "    return:\n",
        "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid_h = np.arange(grid_size, dtype=float)\n",
        "    grid_w = np.arange(grid_size, dtype=float)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    assert embed_dim % 2 == 0\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid length\n",
        "    return:\n",
        "    pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid = np.arange(grid_size, dtype=float)\n",
        "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=float)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega   # (D/2,)\n",
        "    pos = pos.reshape(-1)   # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
        "    emb = np.concatenate([np.sin(out), np.cos(out)], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "def repeat_interleave_batch(x, B, repeat):\n",
        "    N = len(x) // B\n",
        "    x = torch.cat([\n",
        "        torch.cat([x[i*B:(i+1)*B] for _ in range(repeat)], dim=0)\n",
        "        for i in range(N)\n",
        "    ], dim=0)\n",
        "    return x\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x, attn\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x, return_attention=False):\n",
        "        y, attn = self.attn(self.norm1(x))\n",
        "        if return_attention:\n",
        "            return attn\n",
        "        x = x + self.drop_path(y)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvEmbed(nn.Module):\n",
        "    \"\"\"3x3 Convolution stems for ViT following ViTC models\"\"\"\n",
        "    def __init__(self, channels, strides, img_size=224, in_chans=3, batch_norm=True):\n",
        "        super().__init__()\n",
        "        # Build the stems\n",
        "        stem = []\n",
        "        channels = [in_chans] + channels\n",
        "        for i in range(len(channels) - 2):\n",
        "            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3, stride=strides[i], padding=1, bias=(not batch_norm))]\n",
        "            if batch_norm:\n",
        "                stem += [nn.BatchNorm2d(channels[i+1])]\n",
        "            stem += [nn.ReLU(inplace=True)]\n",
        "        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n",
        "        self.stem = nn.Sequential(*stem)\n",
        "\n",
        "        # Comptute the number of patches\n",
        "        stride_prod = int(np.prod(strides))\n",
        "        self.num_patches = (img_size[0] // stride_prod)**2\n",
        "\n",
        "    def forward(self, x):\n",
        "        p = self.stem(x)\n",
        "        return p.flatten(2).transpose(1, 2)\n",
        "\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "def apply_masks(x, masks):\n",
        "    \"\"\":param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
        "    :param masks: list of tensors containing indices of patches in [N] to keep\"\"\"\n",
        "    all_x = []\n",
        "    for m in masks:\n",
        "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [batch,T,dim]\n",
        "        all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "    return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "\n",
        "class VisionTransformerPredictor(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, num_patches, embed_dim=768, predictor_embed_dim=384,\n",
        "        depth=6, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.init_std = init_std\n",
        "        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        # --\n",
        "        self.predictor_pos_embed = nn.Parameter(torch.zeros(1, num_patches, predictor_embed_dim), requires_grad=False)\n",
        "        predictor_pos_embed = get_2d_sincos_pos_embed(self.predictor_pos_embed.shape[-1], int(num_patches**.5), cls_token=False)\n",
        "        self.predictor_pos_embed.data.copy_(torch.from_numpy(predictor_pos_embed).float().unsqueeze(0)) # [1, num_patches, predictor_embed_dim]\n",
        "        # --\n",
        "        self.predictor_blocks = nn.ModuleList([Block(dim=predictor_embed_dim, num_heads=num_heads, drop=drop_rate, drop_path=dpr[i]) for i in range(depth)])\n",
        "        self.predictor_norm = nn.LayerNorm(predictor_embed_dim)\n",
        "        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n",
        "        # ------\n",
        "        # trunc_normal_(self.mask_token, std=self.init_std)\n",
        "\n",
        "    def forward(self, x, masks_x, masks): # [batch, num_context_patches, embed_dim], nenc*[batch, num_context_patches], npred*[batch, num_target_patches]\n",
        "        assert (masks is not None) and (masks_x is not None), 'Cannot run predictor without mask indices'\n",
        "        if not isinstance(masks_x, list): masks_x = [masks_x] # context mask\n",
        "        if not isinstance(masks, list): masks = [masks] # pred mask\n",
        "\n",
        "        # -- Batch Size\n",
        "        B = len(x) // len(masks_x)\n",
        "        # print(\"predictor fwd0\", x.shape) # [3, 121, 768])\n",
        "        # -- map from encoder-dim to pedictor-dim\n",
        "        x = self.predictor_embed(x) # [batch, num_patches, predictor_embed_dim]\n",
        "\n",
        "        # -- add positional embedding to x tokens\n",
        "        x_pos_embed = self.predictor_pos_embed.repeat(B, 1, 1) # [batch, num_patches, predictor_embed_dim]\n",
        "        print(\"predictor fwd1\", x.shape, x_pos_embed.shape) # [3, 121 or 11, 384], [3, 196, 384]\n",
        "        # print(\"predictor fwd masks_x\", masks_x)\n",
        "        x += apply_masks(x_pos_embed, masks_x) # get pos emb of context patches\n",
        "\n",
        "        # print(\"predictor fwd x\", x.shape) # [4, 11, 384])\n",
        "        _, N_ctxt, D = x.shape\n",
        "\n",
        "        # -- concat mask tokens to x\n",
        "        pos_embs = self.predictor_pos_embed.repeat(B, 1, 1) # [batch, num_patches, predictor_embed_dim]\n",
        "        pos_embs = apply_masks(pos_embs, masks) # [16, 104, predictor_embed_dim]\n",
        "        pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x)) # [16, 104, predictor_embed_dim]\n",
        "        # --\n",
        "        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1) # [16, 104, predictor_embed_dim]\n",
        "        # --\n",
        "        # print(\"predictor fwd pred_tokens\", pred_tokens.shape)\n",
        "\n",
        "        pred_tokens += pos_embs\n",
        "        x = x.repeat(len(masks), 1, 1)\n",
        "        x = torch.cat([x, pred_tokens], dim=1)\n",
        "\n",
        "        # -- fwd prop\n",
        "        for blk in self.predictor_blocks:\n",
        "            x = blk(x)\n",
        "        x = self.predictor_norm(x)\n",
        "\n",
        "        # -- return preds for mask tokens\n",
        "        x = x[:, N_ctxt:]\n",
        "        x = self.predictor_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, img_size=[224], patch_size=16, in_chans=3, embed_dim=768, predictor_embed_dim=384, depth=12, predictor_depth=12,\n",
        "        num_heads=12, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0, norm_layer=nn.LayerNorm, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_features = self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.init_std = init_std\n",
        "        # --\n",
        "        self.patch_embed = PatchEmbed(img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        # --\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=False)\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "        # --\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "    def forward(self, x, masks=None): # [batch,3,224,224]\n",
        "        if masks is not None:\n",
        "            if not isinstance(masks, list): masks = [masks]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        x = self.patch_embed(x)\n",
        "        B, N, D = x.shape # [batch, num_patches, embed_dim]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        # -- add positional embedding to x\n",
        "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
        "        x = x + pos_embed\n",
        "\n",
        "        if masks is not None: x = apply_masks(x, masks) # [batch, num_context_patches, embed_dim]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            x = blk(x)\n",
        "\n",
        "        if self.norm is not None: x = self.norm(x)\n",
        "        return x # [batch, num_context_patches, embed_dim]\n",
        "\n",
        "    def interpolate_pos_encoding(self, x, pos_embed):\n",
        "        npatch = x.shape[1] - 1\n",
        "        N = pos_embed.shape[1] - 1\n",
        "        if npatch == N: return pos_embed\n",
        "        class_emb, pos_embed = pos_embed[:, 0], pos_embed[:, 1:]\n",
        "        dim = x.shape[-1]\n",
        "        pos_embed = nn.functional.interpolate(pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2), scale_factor=math.sqrt(npatch / N), mode='bicubic',)\n",
        "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
        "\n",
        "# from functools import partial\n",
        "# def vit_predictor(**kwargs):\n",
        "#     model = VisionTransformerPredictor(\n",
        "#         mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "#     return model\n",
        "\n",
        "def vit(patch_size=16, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, # tiny\n",
        "        # patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, # small\n",
        "        # patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, # base\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "# encoder = vit()\n",
        "encoder = VisionTransformer(\n",
        "        patch_size=16, embed_dim=768, depth=1, num_heads=3, mlp_ratio=1,\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
        "\n",
        "# num_patches = (224/patch_size)^2\n",
        "# predictor = VisionTransformerPredictor(num_patches, embed_dim=768, predictor_embed_dim=384, depth=6, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02, **kwargs)\n",
        "predictor = VisionTransformerPredictor(num_patches=196, embed_dim=768, predictor_embed_dim=384, depth=1, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02)\n",
        "\n",
        "batch = 4\n",
        "img = torch.rand(batch, 3, 224, 224)\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/train.py\n",
        "mask_collator = MaskCollator(\n",
        "        input_size=(224, 224),\n",
        "        patch_size=16,\n",
        "        enc_mask_scale=(0.2, 0.8),\n",
        "        pred_mask_scale=(0.2, 0.8),\n",
        "        aspect_ratio=(0.3, 3.0),\n",
        "        nenc=1,\n",
        "        npred=4,\n",
        "        min_keep=4,\n",
        "        # allow_overlap=True)\n",
        "        allow_overlap=False)\n",
        "# self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
        "\n",
        "\n",
        "\n",
        "# v = mask_collator.step()\n",
        "# should pass the collater a list batch of idx?\n",
        "# collated_batch, collated_masks_enc, collated_masks_pred = mask_collator([batch,3,8])\n",
        "collated_batch, collated_masks_enc, collated_masks_pred = mask_collator([0,1,2,3])\n",
        "# print(v)\n",
        "\n",
        "\n",
        "import copy\n",
        "target_encoder = copy.deepcopy(encoder)\n",
        "\n",
        "\n",
        "def forward_target():\n",
        "    with torch.no_grad():\n",
        "        h = target_encoder(imgs)\n",
        "        h = F.layer_norm(h, (h.size(-1),))  # normalize over feature-dim\n",
        "        B = len(h)\n",
        "        # -- create targets (masked regions of h)\n",
        "        h = apply_masks(h, masks_pred)\n",
        "        h = repeat_interleave_batch(h, B, repeat=len(masks_enc))\n",
        "        return h\n",
        "\n",
        "def forward_context():\n",
        "    z = encoder(imgs, masks_enc)\n",
        "    z = predictor(z, masks_enc, masks_pred)\n",
        "    return z\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    m = next(momentum_scheduler)\n",
        "                    for param_q, param_k in zip(encoder.parameters(), target_encoder.parameters()):\n",
        "                        param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "\n",
        "\n",
        "# # num_context_patches = 121 if allow_overlap=True else 11\n",
        "z = encoder(img, collated_masks_enc) # [batch, num_context_patches, embed_dim]\n",
        "# print(len(collated_masks_enc), len(collated_masks_pred)) # nenc, npred\n",
        "# print(collated_masks_enc[0].shape, collated_masks_pred[0].shape) # , [batch, num_context_patches = 121 or 11], [batch, num_target_patches]\n",
        "out = predictor(z, collated_masks_enc, collated_masks_pred)\n",
        "# # print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test multiblock params\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "dataiter = iter(train_data)\n",
        "img,y = next(dataiter)\n",
        "img = img.unsqueeze(0)\n",
        "b,c,h,w = img.shape\n",
        "img = img.flatten(2).transpose(-2,-1).to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "print(img.shape)\n",
        "batch, seq,_ = img.shape\n",
        "\n",
        "\n",
        "# target_mask = randpatch(seq, mask_size=8, gamma=.9) # 8.9 [seq]\n",
        "target_mask = multiblock(seq, min_s=0.2, max_s=0.3, M=4).any(0) # best.2.3M4 og.15.2M4# mask out targets to be predicted # [M, seq]\n",
        "target_mask = multiblock(seq, min_s=0.2, max_s=0.3, M=4).any(0)\n",
        "\n",
        "context_mask = ~multiblock(seq, min_s=0.85, max_s=1., M=1)|target_mask # [1, seq], True->Mask\n",
        "\n",
        "# print(target_mask, context_mask)\n",
        "\n",
        "print(target_mask.shape, context_mask.shape)\n",
        "# target_img, context_img = img*target_mask.unsqueeze(-1), img*context_mask.unsqueeze(-1)\n",
        "target_img, context_img = img*~target_mask.unsqueeze(-1), img*~context_mask.unsqueeze(-1)\n",
        "# print(target_img.shape, context_img.shape)\n",
        "target_img, context_img = target_img.transpose(-2,-1).reshape(b,c,h,w), context_img.transpose(-2,-1).reshape(b,c,h,w)\n",
        "target_img, context_img = target_img.float(), context_img.float()\n",
        "\n",
        "# imshow(out.detach().cpu())\n",
        "imshow(target_img[0])\n",
        "imshow(context_img[0])\n"
      ],
      "metadata": {
        "id": "D6lVtbS5OHIv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 879
        },
        "outputId": "13b6ad36-0c36-4d1e-daf0-70b098b51a2e",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1024, 3])\n",
            "torch.Size([1024]) torch.Size([1, 1024])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAJGpJREFUeJzt3Xtw1NUB9vFnd7O7SUiyIUBuJVDwAlqEvqWaZmwplZRLZxxU/lDbmWLr6GiDU6U302m19jKhdsZLOyn+UQvtTBFrR3R0RqyihGkLtKTyUnvJAG9asJCg1GSTTbLZ7J73D8dtV0DOCRtOEr6fmd8M2T05Ob89u3my2eVJwBhjBADAeRb0vQAAwIWJAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgRYHvBbxXJpPRsWPHVFpaqkAg4Hs5AABHxhj19fWptrZWweCZn+eMuwA6duyY6urqfC8DAHCOjh49qpkzZ57x+jELoNbWVv3oRz9SV1eXFi1apJ/85Ce66qqrzvp5paWlkqQnH39UxcVFVl8r0Z+wXle0KGw9VpIKwiHrsb9/7V9OcwPAZJRMJvXwww9nv5+fyZgE0JNPPqn169frscceU319vR555BGtWLFCHR0dqqysfN/PfffXbsXFRZpiGUAmk7FeW+EYBlA0GnWaGwAms7O9jDImb0J46KGHdNttt+kLX/iCLr/8cj322GMqLi7Wz3/+87H4cgCACSjvATQ8PKz29nY1Njb+94sEg2psbNTu3btPGZ9MJhWPx3MOAMDkl/cAeuutt5ROp1VVVZVzeVVVlbq6uk4Z39LSolgslj14AwIAXBi8/z+g5uZm9fb2Zo+jR4/6XhIA4DzI+5sQpk+frlAopO7u7pzLu7u7VV1dfcr4aDTKi/cAcAHK+zOgSCSixYsXa8eOHdnLMpmMduzYoYaGhnx/OQDABDUmb8Nev3691q5dq49+9KO66qqr9MgjjyiRSOgLX/jCWHw5AMAENCYBdOONN+rNN9/Ufffdp66uLn34wx/W9u3bT3ljAgDgwjVmTQjr1q3TunXrRv35v/+//4/XhgBgEvP+LjgAwIWJAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXBBAAwIu8B9B3vvMdBQKBnGP+/Pn5/jIAgAmuYCwm/dCHPqSXX375v1+kYEy+DABgAhuTZCgoKFB1dfVYTA0AmCTG5DWggwcPqra2VnPnztXnPvc5HTly5Ixjk8mk4vF4zgEAmPzyHkD19fXavHmztm/fro0bN6qzs1Of+MQn1NfXd9rxLS0tisVi2aOuri7fSwIAjEMBY4wZyy/Q09Oj2bNn66GHHtKtt956yvXJZFLJZDL7cTweV11dne69915Fo9GxXBoAYAwkk0lt2LBBvb29KisrO+O4MX93QHl5uS699FIdOnTotNdHo1GCBgAuQGP+/4D6+/t1+PBh1dTUjPWXAgBMIHkPoK9+9atqa2vTP//5T/3hD3/Q9ddfr1AopJtvvjnfXwoAMIHl/Vdwb7zxhm6++WadPHlSM2bM0Mc//nHt2bNHM2bMyPeXAgBMYHkPoK1bt+Z7SgDAJEQXHADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAvnANo165duvbaa1VbW6tAIKBnnnkm53pjjO677z7V1NSoqKhIjY2NOnjwYL7WCwCYJJwDKJFIaNGiRWptbT3t9Q8++KB+/OMf67HHHtPevXs1ZcoUrVixQkNDQ+e8WADA5FHg+gmrVq3SqlWrTnudMUaPPPKIvvWtb2n16tWSpF/+8peqqqrSM888o5tuuuncVgsAmDTy+hpQZ2enurq61NjYmL0sFoupvr5eu3fvPu3nJJNJxePxnAMAMPnlNYC6urokSVVVVTmXV1VVZa97r5aWFsVisexRV1eXzyUBAMYp7++Ca25uVm9vb/Y4evSo7yUBAM6DvAZQdXW1JKm7uzvn8u7u7ux17xWNRlVWVpZzAAAmv7wG0Jw5c1RdXa0dO3ZkL4vH49q7d68aGhry+aUAABOc87vg+vv7dejQoezHnZ2d2r9/vyoqKjRr1izdfffd+v73v69LLrlEc+bM0be//W3V1tbquuuuy+e6AQATnHMA7du3T5/61KeyH69fv16StHbtWm3evFlf//rXlUgkdPvtt6unp0cf//jHtX37dhUWFuZv1QCACS9gjDG+F/G/4vG4YrGY7r33XkWjUd/LAQA4SiaT2rBhg3p7e9/3dX3v74IDAFyYCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4QQABALxwDqBdu3bp2muvVW1trQKBgJ555pmc62+55RYFAoGcY+XKlflaLwBgknAOoEQioUWLFqm1tfWMY1auXKnjx49njyeeeOKcFgkAmHwKXD9h1apVWrVq1fuOiUajqq6uHvWiAACT35i8BrRz505VVlZq3rx5uvPOO3Xy5Mkzjk0mk4rH4zkHAGDyy3sArVy5Ur/85S+1Y8cO/fCHP1RbW5tWrVqldDp92vEtLS2KxWLZo66uLt9LAgCMQ86/gjubm266KfvvK664QgsXLtRFF12knTt3atmyZaeMb25u1vr167Mfx+NxQggALgBj/jbsuXPnavr06Tp06NBpr49GoyorK8s5AACT35gH0BtvvKGTJ0+qpqZmrL8UAGACcf4VXH9/f86zmc7OTu3fv18VFRWqqKjQAw88oDVr1qi6ulqHDx/W17/+dV188cVasWJFXhcOAJjYnANo3759+tSnPpX9+N3Xb9auXauNGzfqwIED+sUvfqGenh7V1tZq+fLl+t73vqdoNJq/VQMAJjznAFq6dKmMMWe8/sUXXzynBQEALgx0wQEAvCCAAABeEEAAAC8IIACAFwQQAMALAggA4AUBBADwggACAHhBAAEAvCCAAABeEEAAAC8IIACAFwQQAMALAggA4AUBBADwggACAHhBAAEAvCCAAABeEEAAAC8IIACAFwQQAMCLAt8LgH9LGy5zGh8Khp3Gh8P2P+cMDQ87zf2fngHrsSMjxmnuUDBgPTY5POI0t0JuD71AwP42NOm021qUsV+Hw20iSSGHH3FdxkpSvNd+74dH7M9RkjJyu6+MpO33PxwKOc0dK45Yj73skplOcyuTtB564s0e67GJgUGrcTwDAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXozbLrgl/2e2phQXWY01DvVUIyNuPVlBh4KqcMSt48ml3yttHHvMXLrGHNYhSemUW19bKml/m0cjhU5zl5RMsR578u1+p7nTw/brdr1fybGvLRKN2o8ttO8Ok6S0w9qTKfvuMEkaGrK/r5QUu+19NGJ/nmmHzjNJymTcuuOiEft+xJDjz/01M6Zaj42E3Lr63nToUuztT1iPHRgcshrHMyAAgBcEEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADAi3FbxRONhFQYtVteIGhfPzGSdsvcgrB9xcaIY31HJmNfrzOSSjnNnU7br6WwqNhp7pGAWy1Q0KEWKDE06DR3vN++6iWVGnGauyBgv+5I2K3+Rq63ocP4cNjtYV3gsPZU2u02dPkZdyDhVvEUKrCfuyDsVpOVGnJ7LMvhsTx1qtvjbca0MuuxfYk+p7n/E7ev4kmN2J+j7VieAQEAvHAKoJaWFl155ZUqLS1VZWWlrrvuOnV0dOSMGRoaUlNTk6ZNm6aSkhKtWbNG3d3deV00AGDicwqgtrY2NTU1ac+ePXrppZeUSqW0fPlyJRL/bUm955579Nxzz+mpp55SW1ubjh07phtuuCHvCwcATGxOvyzevn17zsebN29WZWWl2tvbtWTJEvX29urxxx/Xli1bdM0110iSNm3apMsuu0x79uzRxz72sfytHAAwoZ3Ta0C9vb2SpIqKCklSe3u7UqmUGhsbs2Pmz5+vWbNmaffu3aedI5lMKh6P5xwAgMlv1AGUyWR099136+qrr9aCBQskSV1dXYpEIiovL88ZW1VVpa6urtPO09LSolgslj3q6upGuyQAwAQy6gBqamrS66+/rq1bt57TApqbm9Xb25s9jh49ek7zAQAmhlH9P6B169bp+eef165duzRz5szs5dXV1RoeHlZPT0/Os6Du7m5VV1efdq5oNKqow58bBgBMDk7PgIwxWrdunbZt26ZXXnlFc+bMybl+8eLFCofD2rFjR/ayjo4OHTlyRA0NDflZMQBgUnB6BtTU1KQtW7bo2WefVWlpafZ1nVgspqKiIsViMd16661av369KioqVFZWprvuuksNDQ28Aw4AkMMpgDZu3ChJWrp0ac7lmzZt0i233CJJevjhhxUMBrVmzRolk0mtWLFCP/3pT/OyWADA5OEUQMacvd+nsLBQra2tam1tHfWiJCkYyCgYSFuuy74LLhhw7Wuz72ALBd1eUkuP2PdqpdN2t8W7CoL2v10dGhpymjvh2tnl0DWWHHbrGksmHcZb3H//V0HI/jYMOXaNhQrs77OSVDFtqvXY/7zd4zS3AvZrd1u1FHD4DNeuPgXt9zMQclu543CFHfoop1eUOs2dTts/3voG3B7Lb/fZdy+WTSmyHjuSpgsOADCOEUAAAC8IIACAFwQQAMALAggA4AUBBADwggACAHhBAAEAvCCAAABeEEAAAC9G9ecYzodQKKRQyG55Ixn7eh3bOd+Vdpg7nXarEgk5VL3EytzqOzIZ+5qS/oRbfcfwsFstUHooaT02EAo7zR0N29eDBMJuVTwu9SpBh72UpIKI2/0wM2JfCRWQ23majP1+RgrcKoeUth8fDLitO+BQxRNyreIpdLsflpdNsR6bTLo93v4dH7Ae25ewf6y9w/5+W1pSYj9r0G7feQYEAPCCAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8GLddcAoWvHNYGEk5dCtl3DqhwmH7Tqho2O3mDBc49E0F3H5WSI3Y93uNpO377iRp2L6WTJI0mHTp7HLrGjPG/jxNethp7pBD71lpSZnT3IGg234mBuz7wIoKI05zp1L2t2Fsin0fmCSlhu1v8xHXLsWI/ePHpN36C0MBt8dEcZF9J+Fb/3nbae7BfvvbcGDA7T5eWFxoPTbi8L0wFbb7JsEzIACAFwQQAMALAggA4AUBBADwggACAHhBAAEAvCCAAABeEEAAAC8IIACAFwQQAMCLcVvFE45EFY5ErcYOJO3rJ4YdxkpSYWGx9diiQvtaC0mKRO0rU/r77atYJCmVsj/PSNit/iZW5naewQH77p5Mxr62R5JGHM7TBN32PhK2P8+REbf9KSiwu29nx4fsK6SGkg7VVJKGHepyQhm3HqaA7CttHFuY5NDCpKBjlVVpidt9fDCZtB6bHHatvrLf+4KgQ72XpCkO34Nc7ifDln1dPAMCAHhBAAEAvCCAAABeEEAAAC8IIACAFwQQAMALAggA4AUBBADwggACAHhBAAEAvCCAAABejNsuuOqaapWWlliNLezpsZ735Ik3ndZRGLa/icIht5szOWTfq5VKufVHyaFSLRRwK+FKu5RwSYoEHPrAom5rSQXtf4YKBdz61yJhh5/PHM5RkoIBt061YIH9hmZG3ObOBO33czjlNvfIiP3crl1ww+kR67HRSJHT3GnHh1tfn30X4Ns9/U5zF4ftu+BmzCh1mjvg0DHYP5CwHjswMGg1jmdAAAAvnAKopaVFV155pUpLS1VZWanrrrtOHR0dOWOWLl2qQCCQc9xxxx15XTQAYOJzCqC2tjY1NTVpz549eumll5RKpbR8+XIlErlPzW677TYdP348ezz44IN5XTQAYOJzetFi+/btOR9v3rxZlZWVam9v15IlS7KXFxcXq7q6Oj8rBABMSuf0GlBvb68kqaKiIufyX/3qV5o+fboWLFig5uZmDQyc+QW6ZDKpeDyecwAAJr9Rvwsuk8no7rvv1tVXX60FCxZkL//sZz+r2bNnq7a2VgcOHNA3vvENdXR06Omnnz7tPC0tLXrggQdGuwwAwAQ16gBqamrS66+/rt/97nc5l99+++3Zf19xxRWqqanRsmXLdPjwYV100UWnzNPc3Kz169dnP47H46qrqxvtsgAAE8SoAmjdunV6/vnntWvXLs2cOfN9x9bX10uSDh06dNoAikajikbd/n8GAGDicwogY4zuuusubdu2TTt37tScOXPO+jn79++XJNXU1IxqgQCAyckpgJqamrRlyxY9++yzKi0tVVdXlyQpFoupqKhIhw8f1pYtW/SZz3xG06ZN04EDB3TPPfdoyZIlWrhw4ZicAABgYnIKoI0bN0p65z+b/q9NmzbplltuUSQS0csvv6xHHnlEiURCdXV1WrNmjb71rW/lbcEAgMnB+Vdw76eurk5tbW3ntKB3FRSXKVxs1wX3gfKp1vMWFxc7raPnrbesxyaH7bupJMml3m045da/lsk49K8FHIrjJAXkVpRVVGj/bv/CwrDT3Om0/dzFRW57PzRo12clScPDbh1pcrwNg/aVXSqZUug0d0GB/doDAbeXjV06DFMpt8ePHHoARxzL3d5++z9O4132v8Chf02SIhGHvkPHV/VTDr2BGYfuvUzGbixdcAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXo/57QGPt4KF/qmTKFKuxs2bXWs87pbTUaR3H/v1v67Ejabc6ltjUadZjwxm3nxVOvnXCemy0IOQ0d9jxx5awwycUF7v9aY5M2r6iqMDxPAfP/Id8T5HOuNUZZYxjFU/Qfu1TCt0qh5Tutx6aTNnXE0lSyGHdwxm3uqlI2L5yKN5nf46SlBxyO8+AQ7tOJOJWNxWK2N+GSYdqHUlyaeEqitrf3iZtNzHPgAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBfjtgvuzX+fUKK4yGpscYF9jpp00mkdfT32HVLllfaddJJUXBKzHjsw5NbxlBiw77IqjNmvQ5KCQbefWzIOHV+hYMRp7oFEn/XYZNKh3E1SKGj/8EilRpzmluNtmHLo+ArI7Txl7AvBwgVu3zJSDvVuxjgUqkkacnhMJAaGHOd2Gx9y+B4UK7fruHxXYbF9B1tmxK1PLxq2f7wNDg5bjx0ZoQsOADCOEUAAAC8IIACAFwQQAMALAggA4AUBBADwggACAHhBAAEAvCCAAABeEEAAAC/GbRVPRWmxpkwpthqb7I9bz5tIJJzWkcnY14Mkh93qcgYG7es+4n32lUCSFAw41BM5VLFIUsqx7iMQyFiPPXnSfi8ladjhNs9k7NchSUVF9ntv5FYjI7ebXAUFYeuxQ0n7yhRJKgiFrMdGikqc5h5M2FdfpR1vw0DI/tvXjBnVTnNXVbnVakWi9vtTUmJfrSNJBRH7ueV2F5dJ298RC+L23zuDYbs6KJ4BAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8IIAAAF4QQAAAL8ZtF1w4bBQJ2/UUFRTY52ikyK2HaXjQvmssHu9zmjvlUKkWDLj1ZE0psuvRk6RMZsRp7oKI293GpfZsMGnfj/fO3PZ7n067FWWFHTrvQgVut0kwaN+/Jklph86ukENvnCRFHfrdIoWlTnMHwvb327JpbrdhNBqxHjs1NtVpbseqPg0O2nWfSVI67dYZ6bIW49gFVzrFfj8rptvP29dv113JMyAAgBdOAbRx40YtXLhQZWVlKisrU0NDg1544YXs9UNDQ2pqatK0adNUUlKiNWvWqLu7O++LBgBMfE4BNHPmTG3YsEHt7e3at2+frrnmGq1evVp//etfJUn33HOPnnvuOT311FNqa2vTsWPHdMMNN4zJwgEAE5vTL12vvfbanI9/8IMfaOPGjdqzZ49mzpypxx9/XFu2bNE111wjSdq0aZMuu+wy7dmzRx/72Mfyt2oAwIQ36teA0um0tm7dqkQioYaGBrW3tyuVSqmxsTE7Zv78+Zo1a5Z27959xnmSyaTi8XjOAQCY/JwD6C9/+YtKSkoUjUZ1xx13aNu2bbr88svV1dWlSCSi8vLynPFVVVXq6uo643wtLS2KxWLZo66uzvkkAAATj3MAzZs3T/v379fevXt15513au3atfrb3/426gU0Nzert7c3exw9enTUcwEAJg7n/wcUiUR08cUXS5IWL16sP/3pT3r00Ud14403anh4WD09PTnPgrq7u1Vdfea/xx6NRhWNRt1XDgCY0M75/wFlMhklk0ktXrxY4XBYO3bsyF7X0dGhI0eOqKGh4Vy/DABgknF6BtTc3KxVq1Zp1qxZ6uvr05YtW7Rz5069+OKLisViuvXWW7V+/XpVVFSorKxMd911lxoaGngHHADgFE4BdOLECX3+85/X8ePHFYvFtHDhQr344ov69Kc/LUl6+OGHFQwGtWbNGiWTSa1YsUI//elPR7WwvoE+ZWRXERMM2Z9GOuP2pG9oyK0axkVycNB67JQStwqUSMS+jiUYcCseMcahQ0hScti+eiSVcqspCQTs9zMcdqu/yWTse00yDlU5kjTieJ5Dw/Z1SWXlDp0pkqZOr7UeG406VvGE7OtyBobsHw+SW/1NyqFWSZJSDre3JPU41HBFIva3iSQlBhPWYwPGrbKrsMh+P4cdHse2j3mnAHr88cff9/rCwkK1traqtbXVZVoAwAWILjgAgBcEEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBfObdhjzZh3Kk0GBuxrOSZqFU/GOFTDBN1qZAoc6nVcq3gkxyoeh9qZ8VTFk3K4X42k7Wt7JCmTcbvNkw7VMKGwfUWNJPX391uPHU65Vb0EgvaVUINJt8eaSxWPqxHHKp5Ewr4ux6XSRpIGhsauisdp7x3W/e7t8e738zMJmLONOM/eeOMN/igdAEwCR48e1cyZM894/bgLoEwmo2PHjqm0tFSBwH/TPB6Pq66uTkePHlVZWZnHFY4tznPyuBDOUeI8J5t8nKcxRn19faqtrVUweObfUoy7X8EFg8H3TcyysrJJvfnv4jwnjwvhHCXOc7I51/OMxWJnHcObEAAAXhBAAAAvJkwARaNR3X///YpGo76XMqY4z8njQjhHifOcbM7neY67NyEAAC4ME+YZEABgciGAAABeEEAAAC8IIACAFxMmgFpbW/XBD35QhYWFqq+v1x//+EffS8qr73znOwoEAjnH/PnzfS/rnOzatUvXXnutamtrFQgE9Mwzz+Rcb4zRfffdp5qaGhUVFamxsVEHDx70s9hzcLbzvOWWW07Z25UrV/pZ7Ci1tLToyiuvVGlpqSorK3Xdddepo6MjZ8zQ0JCampo0bdo0lZSUaM2aNeru7va04tGxOc+lS5eesp933HGHpxWPzsaNG7Vw4cLsfzZtaGjQCy+8kL3+fO3lhAigJ598UuvXr9f999+vP//5z1q0aJFWrFihEydO+F5aXn3oQx/S8ePHs8fvfvc730s6J4lEQosWLVJra+tpr3/wwQf14x//WI899pj27t2rKVOmaMWKFWNaADsWznaekrRy5cqcvX3iiSfO4wrPXVtbm5qamrRnzx699NJLSqVSWr58eU4J5z333KPnnntOTz31lNra2nTs2DHdcMMNHlftzuY8Jem2227L2c8HH3zQ04pHZ+bMmdqwYYPa29u1b98+XXPNNVq9erX++te/SjqPe2kmgKuuuso0NTVlP06n06a2tta0tLR4XFV+3X///WbRokW+lzFmJJlt27ZlP85kMqa6utr86Ec/yl7W09NjotGoeeKJJzysMD/ee57GGLN27VqzevVqL+sZKydOnDCSTFtbmzHmnb0Lh8Pmqaeeyo75+9//biSZ3bt3+1rmOXvveRpjzCc/+Unz5S9/2d+ixsjUqVPNz372s/O6l+P+GdDw8LDa29vV2NiYvSwYDKqxsVG7d+/2uLL8O3jwoGprazV37lx97nOf05EjR3wvacx0dnaqq6srZ19jsZjq6+sn3b5K0s6dO1VZWal58+bpzjvv1MmTJ30v6Zz09vZKkioqKiRJ7e3tSqVSOfs5f/58zZo1a0Lv53vP812/+tWvNH36dC1YsEDNzc0aGBi7Pw0x1tLptLZu3apEIqGGhobzupfjroz0vd566y2l02lVVVXlXF5VVaV//OMfnlaVf/X19dq8ebPmzZun48eP64EHHtAnPvEJvf766yotLfW9vLzr6uqSpNPu67vXTRYrV67UDTfcoDlz5ujw4cP65je/qVWrVmn37t0Khdz+RtF4kMlkdPfdd+vqq6/WggULJL2zn5FIROXl5TljJ/J+nu48Jemzn/2sZs+erdraWh04cEDf+MY31NHRoaefftrjat395S9/UUNDg4aGhlRSUqJt27bp8ssv1/79+8/bXo77ALpQrFq1KvvvhQsXqr6+XrNnz9avf/1r3XrrrR5XhnN10003Zf99xRVXaOHChbrooou0c+dOLVu2zOPKRqepqUmvv/76hH+N8mzOdJ6333579t9XXHGFampqtGzZMh0+fFgXXXTR+V7mqM2bN0/79+9Xb2+vfvOb32jt2rVqa2s7r2sY97+Cmz59ukKh0CnvwOju7lZ1dbWnVY298vJyXXrppTp06JDvpYyJd/fuQttXSZo7d66mT58+Ifd23bp1ev755/Xqq6/m/NmU6upqDQ8Pq6enJ2f8RN3PM53n6dTX10vShNvPSCSiiy++WIsXL1ZLS4sWLVqkRx999Lzu5bgPoEgkosWLF2vHjh3ZyzKZjHbs2KGGhgaPKxtb/f39Onz4sGpqanwvZUzMmTNH1dXVOfsaj8e1d+/eSb2v0jt/9ffkyZMTam+NMVq3bp22bdumV155RXPmzMm5fvHixQqHwzn72dHRoSNHjkyo/TzbeZ7O/v37JWlC7efpZDIZJZPJ87uXeX1LwxjZunWriUajZvPmzeZvf/ubuf322015ebnp6uryvbS8+cpXvmJ27txpOjs7ze9//3vT2Nhopk+fbk6cOOF7aaPW19dnXnvtNfPaa68ZSeahhx4yr732mvnXv/5ljDFmw4YNpry83Dz77LPmwIEDZvXq1WbOnDlmcHDQ88rdvN959vX1ma9+9atm9+7dprOz07z88svmIx/5iLnkkkvM0NCQ76Vbu/POO00sFjM7d+40x48fzx4DAwPZMXfccYeZNWuWeeWVV8y+fftMQ0ODaWho8Lhqd2c7z0OHDpnvfve7Zt++faazs9M8++yzZu7cuWbJkiWeV+7m3nvvNW1tbaazs9McOHDA3HvvvSYQCJjf/va3xpjzt5cTIoCMMeYnP/mJmTVrlolEIuaqq64ye/bs8b2kvLrxxhtNTU2NiUQi5gMf+IC58cYbzaFDh3wv65y8+uqrRtIpx9q1a40x77wV+9vf/rapqqoy0WjULFu2zHR0dPhd9Ci833kODAyY5cuXmxkzZphwOGxmz55tbrvttgn3w9Ppzk+S2bRpU3bM4OCg+dKXvmSmTp1qiouLzfXXX2+OHz/ub9GjcLbzPHLkiFmyZImpqKgw0WjUXHzxxeZrX/ua6e3t9btwR1/84hfN7NmzTSQSMTNmzDDLli3Lho8x528v+XMMAAAvxv1rQACAyYkAAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXvx/x9WZX8YBW6wAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "trg_msks = []\n",
        "ctx_msks = []\n",
        "for i in range(16):\n",
        "    # target_mask = randpatch(seq, mask_size=8, gamma=.9) # 8.9 [seq]\n",
        "    target_mask = multiblock(seq, min_s=0.2, max_s=0.3, M=4).any(0) # best.2.3M4 og.15.2M4# mask out targets to be predicted # [M, seq]\n",
        "    target_mask = multiblock(seq, min_s=0.2, max_s=0.3, M=4).any(0)\n",
        "\n",
        "    context_mask = ~multiblock(seq, min_s=0.85, max_s=1., M=1)|target_mask # [1, seq], True->Mask\n",
        "\n",
        "    # print(target_mask.shape, context_mask.shape)\n",
        "    target_img, context_img = img*~target_mask.unsqueeze(-1), img*~context_mask.unsqueeze(-1)\n",
        "    target_img, context_img = target_img.transpose(-2,-1).reshape(b,c,h,w).float(), context_img.transpose(-2,-1).reshape(b,c,h,w).float()\n",
        "    trg_msks.append(target_img)\n",
        "    ctx_msks.append(context_img)\n",
        "\n",
        "trg_msks = torch.cat(trg_msks, dim=0)\n",
        "ctx_msks = torch.cat(ctx_msks, dim=0)\n",
        "imshow(torchvision.utils.make_grid(trg_msks.cpu(), nrow=4))\n",
        "imshow(torchvision.utils.make_grid(ctx_msks.cpu(), nrow=4))\n"
      ],
      "metadata": {
        "id": "kiHDuPjB0SBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test multiblock mask\n",
        "def multiblock(seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    # mask_len, mask_pos = mask_len * seq, mask_pos * seq\n",
        "    indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "    return target_mask\n",
        "\n",
        "\n",
        "seq=400\n",
        "M=4\n",
        "\n",
        "# target_mask = multiblock(M, seq, min_s=0.15, max_s=0.2, M=1) # mask out targets to be predicted # [4*batch, seq]\n",
        "# context_mask = ~multiblock(1, seq, min_s=0.85, max_s=1., M=1)|target_mask.reshape(M,seq).any(0) # [batch, seq]\n",
        "\n",
        "\n",
        "target_mask = multiblock(seq, min_s=0.15, max_s=0.2, M=4) # mask out targets to be predicted # [4*batch, seq]\n",
        "context_mask = ~multiblock(seq, min_s=0.85, max_s=1., M=1)|target_mask.reshape(M,seq).any(0) # [batch, seq]\n",
        "\n",
        "# target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "\n",
        "# print(target_mask)\n",
        "# print(context_mask)\n",
        "\n",
        "print(target_mask.sum(-1))\n",
        "print(context_mask.sum(-1))\n",
        "\n",
        "\n",
        "# sorted_mask, ids = context_mask.sort(dim=1, stable=True)\n",
        "# indices = ids[~sorted_mask]#.reshape(M,-1) # int idx [num_context_toks] , idx of context not masked\n",
        "# sorted_x = x[torch.arange(batch).unsqueeze(-1), indices] # [batch, seq-num_trg_toks, dim]\n",
        "# print(indices)\n",
        "# print(x)\n",
        "# print(sorted_x)\n",
        "\n",
        "\n",
        "\n",
        "# sorted_mask, ids = target_mask.sort(dim=1, descending=False, stable=True)\n",
        "# # print(sorted_mask, ids)\n",
        "# indices = ids[~sorted_mask].reshape(M,-1) # int idx [M, seq-num_trg_toks] , idx of target not masked\n",
        "# print(indices)\n",
        "# batch=3\n",
        "# dim=2\n",
        "# # indices = indices.expand(batch,-1,-1)\n",
        "# # x = torch.arange(batch*seq).reshape(batch,seq).to(device)\n",
        "# x = torch.rand(batch, seq, dim)\n",
        "# print(x)\n",
        "# sorted_x = x[torch.arange(batch)[...,None,None], indices]\n",
        "# print(sorted_x.shape) # [batch, M, seq-num_trg_toks, dim]\n",
        "# print(sorted_x)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "p8xv1tpKIhim",
        "outputId": "e6613fee-2d32-4c6a-ef96-dee18da3aa6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([73, 73, 73, 73])\n",
            "tensor([248])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ijepa multiblock down\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "import math\n",
        "from multiprocessing import Value\n",
        "import torch\n",
        "_GLOBAL_SEED = 0\n",
        "\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=(224, 224),\n",
        "        patch_size=16,\n",
        "        enc_mask_scale=(0.2, 0.8),\n",
        "        pred_mask_scale=(0.2, 0.8),\n",
        "        aspect_ratio=(0.3, 3.0),\n",
        "        nenc=1,\n",
        "        npred=2,\n",
        "        min_keep=4,\n",
        "        allow_overlap=False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if not isinstance(input_size, tuple):\n",
        "            input_size = (input_size, ) * 2\n",
        "        self.patch_size = patch_size\n",
        "        self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.aspect_ratio = aspect_ratio\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "        self._itr_counter = Value('i', -1)  # collator is shared across worker processes\n",
        "\n",
        "    def step(self):\n",
        "        i = self._itr_counter\n",
        "        with i.get_lock():\n",
        "            i.value += 1\n",
        "            v = i.value\n",
        "        return v\n",
        "\n",
        "    def _sample_block_size(self, generator, scale, aspect_ratio_scale):\n",
        "        _rand = torch.rand(1, generator=generator).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.height * self.width * mask_scale)\n",
        "        # -- Sample block aspect-ratio\n",
        "        min_ar, max_ar = aspect_ratio_scale\n",
        "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
        "        while h >= self.height:\n",
        "            h -= 1\n",
        "        while w >= self.width:\n",
        "            w -= 1\n",
        "\n",
        "        return (h, w)\n",
        "\n",
        "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
        "        h, w = b_size\n",
        "\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # --\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            top = torch.randint(0, self.height - h, (1,))\n",
        "            left = torch.randint(0, self.width - w, (1,))\n",
        "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
        "            mask[top:top+h, left:left+w] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
        "        mask_complement[top:top+h, left:left+w] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        B = len(batch)\n",
        "\n",
        "        collated_batch = torch.utils.data.default_collate(batch)\n",
        "\n",
        "        seed = self.step()\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(seed)\n",
        "        p_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.pred_mask_scale,\n",
        "            aspect_ratio_scale=self.aspect_ratio)\n",
        "        e_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.enc_mask_scale,\n",
        "            aspect_ratio_scale=(1., 1.))\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.height * self.width\n",
        "        min_keep_enc = self.height * self.width\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                print(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "\n",
        "        return collated_batch, collated_masks_enc, collated_masks_pred\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "COvEnBXPYth3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pamap trash"
      ],
      "metadata": {
        "id": "8DxjYI9RMQoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test WISDM\n",
        "\n",
        "# print(activity_dataframe)\n",
        "# print(activity_dataframe['activity'])\n",
        "# activity_dataframe.to_csv('data.csv',index=False)\n",
        "\n",
        "# data = pd.read_csv(\"/data.csv\", index_col =\"Name\")\n",
        "# data = pd.read_csv(\"data.csv\")\n",
        "\n",
        "# a = list(activity_dataframe['timestamp'])\n",
        "# print(a)\n",
        "# print([x-y for x,y in zip(a[1:],a[:-1])])\n",
        "\n",
        "# print(data.loc[0])\n",
        "# p = data.loc[0]\n",
        "# print(len(p))\n",
        "# ID, activity, timestamp, x, y, z, meter, device = data.loc[0]\n",
        "# print(activity)\n",
        "\n",
        "# print(data.loc[1639])\n",
        "\n",
        "# userids = data['ID'].unique()#for id in userids\n",
        "# print(data['activity'].unique())\n",
        "# df_keep = data[['ID','activity','timestamp','x','y','z']]\n",
        "\n",
        "\n",
        "# grouped = df_keep.groupby(['ID'])\n",
        "# grouped = df_keep.groupby(['ID','activity'])\n",
        "# print(len(grouped))\n",
        "# user_acts = dict(tuple(df_keep.groupby(['ID','activity'])))\n",
        "# print(user_acts)\n",
        "# print(len(user_acts))\n",
        "# temp_df = df[df['ID'] == id]\n",
        "\n",
        "# print(len(df_keep))\n",
        "# print(len(data))\n",
        "\n",
        "\n",
        "\n",
        "# act = [[a, d[['timestamp','x','y','z']].to_numpy()] for a, d in user_acts.items()]\n",
        "# act = [[(int(a[0]), ), d[['timestamp','x','y','z']].to_numpy()] for a, d in user_acts.items()]\n",
        "# print(act)\n",
        "# print(act[0])\n",
        "# # print(act[0][1])\n",
        "# print(len(act[1]))\n",
        "# print([len(a[1]) for a in act])\n",
        "# print(min([len(a[1]) for a in act])) # 3567\n",
        "\n",
        "\n",
        "# act_dict = {i: act for i, act in enumerate(data['activity'].unique())}\n",
        "# act_invdict = {v: k for k, v in act_dict.items()}\n",
        "# print(act_invdict)\n",
        "\n",
        "# torch.tensor(act[0][1][:3500])\n",
        "\n",
        "# id_act, x = act[0]\n",
        "# id_act = self.process(id_act)\n",
        "# return id_act,\n",
        "# torch.tensor(x[:3500]) # 3567\n",
        "\n",
        "\n",
        "# /content/processed/wisdm-dataset/raw/phone/accel/data.csv\n",
        "# /content/processed/wisdm-dataset/raw/watch/gyro/data.csv\n",
        "\n",
        "# data0 = pd.read_csv(\"/content/processed/wisdm-dataset/raw/watch/accel/data.csv\")\n",
        "# data1 = pd.read_csv(\"/content/processed/wisdm-dataset/raw/watch/gyro/data.csv\")\n",
        "\n",
        "# user_acts0 = dict(tuple(data0.groupby(['ID','activity'])))\n",
        "# user_acts1 = dict(tuple(data1.groupby(['ID','activity'])))\n",
        "\n",
        "for (a0,d0), (a1,d1) in zip(user_acts0.items(), user_acts1.items()):\n",
        "    print(a0,a1)\n",
        "    print(len(d0),len(d1))\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rV7WLiNT4EAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title har_cnn\n",
        "# https://arxiv.org/pdf/2209.08335v1\n",
        "# https://github.com/Lou1sM/HAR/blob/master/har_cnn.py\n",
        "import sys\n",
        "import json\n",
        "from hmmlearn import hmm\n",
        "from copy import deepcopy\n",
        "import os\n",
        "import math\n",
        "from pdb import set_trace\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import time\n",
        "from torch.utils import data\n",
        "import cl_args\n",
        "from dl_utils.misc import asMinutes,check_dir\n",
        "#from dl_utils.label_funcs import accuracy, mean_f1, debable, translate_labellings, get_num_labels, label_counts, dummy_labels, avoid_minus_ones_lf_wrapper,masked_mode,acc_by_label, get_trans_dict\n",
        "from label_funcs_tmp import accuracy, mean_f1, translate_labellings, get_num_labels, label_counts, dummy_labels, avoid_minus_ones_lf_wrapper,masked_mode,acc_by_label, get_trans_dict\n",
        "from dl_utils.tensor_funcs import noiseify, numpyify, cudify\n",
        "from make_dsets import make_single_dset, make_dsets_by_user\n",
        "from sklearn.metrics import normalized_mutual_info_score,adjusted_rand_score\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from project_config import get_dataset_info_object\n",
        "\n",
        "rari = lambda x,y: round(adjusted_rand_score(x,y),4)\n",
        "rnmi = lambda x,y: round(normalized_mutual_info_score(x,y),4)\n",
        "\n",
        "class EncByLayer(nn.Module):\n",
        "    def __init__(self,x_filters,y_filters,x_strides,y_strides,max_pools,show_shapes):\n",
        "        super().__init__()\n",
        "        self.show_shapes = show_shapes\n",
        "        num_layers = len(x_filters)\n",
        "        assert all(len(x)==num_layers for x in (y_filters,x_strides,y_strides,max_pools))\n",
        "        ncvs = [1]+[4*2**i for i in range(num_layers)]\n",
        "        # conv_layers = []\n",
        "        # for i in range(num_layers):\n",
        "        #     conv_layer = nn.Sequential(\n",
        "        #         nn.Conv2d(ncvs[i],ncvs[i+1],(x_filters[i],y_filters[i]),(x_strides[i],y_strides[i])), nn.BatchNorm2d(ncvs[i+1]) if i<num_layers-1 else nn.Identity(), nn.LeakyReLU(0.3), nn.MaxPool2d(max_pools[i])\n",
        "        #     )\n",
        "        #     conv_layers.append(conv_layer)\n",
        "        # self.conv_layers = nn.ModuleList(conv_layers)\n",
        "\n",
        "        self.conv_layers = nn.Sequential(*[\n",
        "                nn.Conv2d(ncvs[i],ncvs[i+1],(x_filters[i],y_filters[i]),(x_strides[i],y_strides[i])), nn.BatchNorm2d(ncvs[i+1]) if i<num_layers-1 else nn.Identity(), nn.LeakyReLU(0.3), nn.MaxPool2d(max_pools[i])\n",
        "             for i in range(nlayers)])\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        # if self.show_shapes: print(x.shape)\n",
        "        # for conv_layer in self.conv_layers:\n",
        "        x = conv_layer(x)\n",
        "            # if self.show_shapes: print(x.shape)\n",
        "        return x\n",
        "\n",
        "class DecByLayer(nn.Module):\n",
        "    def __init__(self,x_filters,y_filters,x_strides,y_strides,show_shapes):\n",
        "        super().__init__()\n",
        "        self.show_shapes = show_shapes\n",
        "        num_layers = len(x_filters)\n",
        "        assert all(len(x)==num_layers for x in (y_filters,x_strides,y_strides))\n",
        "        ncvs = [4*2**i for i in reversed(range(num_layers))]+[1]\n",
        "        conv_trans_layers = [nn.Sequential(\n",
        "                nn.ConvTranspose2d(ncvs[i],ncvs[i+1],(x_filters[i],y_filters[i]),(x_strides[i],y_strides[i])), nn.BatchNorm2d(ncvs[i+1]), nn.LeakyReLU(0.3),\n",
        "                )\n",
        "            for i in range(num_layers)]\n",
        "        self.conv_trans_layers = nn.ModuleList(conv_trans_layers)\n",
        "\n",
        "    def forward(self,x):\n",
        "        if self.show_shapes: print(x.shape)\n",
        "        for conv_trans_layer in self.conv_trans_layers:\n",
        "            x = conv_trans_layer(x)\n",
        "            if self.show_shapes: print(x.shape)\n",
        "        return x\n",
        "\n",
        "class Var_BS_MLP(nn.Module):\n",
        "    def __init__(self,input_size,hidden_size,output_size):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(nn.Linear(input_size,hidden_size), nn.BatchNorm1d(hidden_size), nn.LeakyReLU(0.3), nn.Linear(hidden_size,output_size))\n",
        "\n",
        "    def forward(self,x):\n",
        "        return self.mlp(x)\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n",
        "    dset_info_object = get_dataset_info_object(args.dset)\n",
        "    num_classes = args.num_classes if args.num_classes != -1 else dset_info_object.num_classes\n",
        "    if args.dset == 'UCI_feat':\n",
        "        enc = nn.Sequential(nn.Linear(561,500),nn.ReLU(),\n",
        "                            nn.Linear(500,500),nn.ReLU(),\n",
        "                            nn.Linear(500,2000),nn.ReLU(),\n",
        "                            nn.Linear(2000,6),nn.ReLU()).cuda()\n",
        "        dec = nn.Sequential(nn.Linear(6,2000),nn.ReLU(),\n",
        "                            nn.Linear(2000,500),nn.ReLU(),\n",
        "                            nn.Linear(500,500),nn.ReLU(),\n",
        "                            nn.Linear(500,561),nn.ReLU()).cuda()\n",
        "        mlp = Var_BS_MLP(6,256,num_classes).cuda()\n",
        "    else:\n",
        "        if args.window_size == 512:\n",
        "            x_filters = (50,40,7,4)\n",
        "            x_strides = (2,2,1,1)\n",
        "            max_pools = ((2,1),(2,1),(2,1),(2,1))\n",
        "        elif args.window_size == 100:\n",
        "            x_filters = (20,20,5,3)\n",
        "            x_strides = (1,1,1,1)\n",
        "            max_pools = ((2,1),(2,1),(2,1),1)\n",
        "        y_filters = (1,1,1,dset_info_object.num_channels)\n",
        "        y_strides = (1,1,1,1)\n",
        "        enc = EncByLayer(x_filters,y_filters,x_strides,y_strides,max_pools,show_shapes=args.show_shapes).cuda()\n",
        "        #if args.is_n2d:\n",
        "        x_filters_trans = (15,10,15,11)\n",
        "        x_strides_trans = (2,3,3,3)\n",
        "        y_filters_trans = (dset_info_object.num_channels,1,1,1)\n",
        "        dec = DecByLayer(x_filters_trans,y_filters_trans,x_strides_trans,y_strides,show_shapes=args.show_shapes).cuda()\n",
        "\n",
        "        optional_umap_like_net_in = Var_BS_MLP(32,256,2).cuda()\n",
        "        optional_umap_like_net_out = Var_BS_MLP(2,256,2).cuda()\n",
        "        if ARGS.is_uln:\n",
        "            enc = nn.Sequential(enc,nn.Flatten(1),Var_BS_MLP(32,256,2).cuda())\n",
        "            dec = nn.Sequential(Var_BS_MLP(32,256,2).cuda(),nn.Unflatten(2,(32,1,1)),dec)\n",
        "        mlp = Var_BS_MLP(2 if ARGS.is_uln else 32,256,num_classes).cuda()\n",
        "    if args.load_pretrained:\n",
        "        enc.load_state_dict(torch.load('enc_pretrained.pt'))\n",
        "    subj_ids = args.subj_ids\n",
        "\n",
        "    metric_dict = {'acc':accuracy,'nmi':rnmi,'ari':rari,'f1':mean_f1}\n",
        "    har = HARLearner(enc=enc,mlp=mlp,dec=dec,num_classes=num_classes,args=args,metric_dict=metric_dict)\n",
        "\n",
        "    start_time = time.time()\n",
        "    already_exists = check_dir(f\"experiments/{args.exp_name}/preds\")\n",
        "    check_dir(f\"experiments/{args.exp_name}/best_preds\")\n",
        "    if args.show_shapes:\n",
        "        dset_train, selected_acts = make_single_dset(args,subj_ids)\n",
        "        num_ftrs = dset_train.x.shape[-1]\n",
        "        print(num_ftrs)\n",
        "        lat = enc(torch.ones((2,1,args.window_size,num_ftrs),device='cuda'))\n",
        "        dec(lat)\n",
        "        sys.exit()\n",
        "    dsets_by_id = make_dsets_by_user(args,subj_ids)\n",
        "    if args.is_n2d:\n",
        "        for subj_id, (dset,sa) in dsets_by_id.items():\n",
        "            print(\"n2ding\", subj_id)\n",
        "            har.n2d_abl(subj_id,dset)\n",
        "    elif not args.subject_independent:\n",
        "        bad_ids = []\n",
        "        for user_id, (dset,sa) in dsets_by_id.items():\n",
        "            n = get_num_labels(dset.y)\n",
        "            if n < dset_info_object.num_classes/2:\n",
        "                print(f\"Excluding user {user_id}, only has {n} different labels, out of {num_classes}\")\n",
        "                bad_ids.append(user_id)\n",
        "        if not args.bad_ids: dsets_by_id = {k:v for k,v in dsets_by_id.items() if k not in bad_ids}\n",
        "        print('reloading clusterings for', [x for x in subj_ids[:args.reload_ids] if x not in bad_ids])\n",
        "        for rid in subj_ids[:args.reload_ids]:\n",
        "            if rid in bad_ids: continue\n",
        "            print('reloading clusterings for', rid)\n",
        "            rdset,sa = dsets_by_id.pop(rid)\n",
        "            best_preds = np.load(f'experiments/{args.exp_name}/best_preds/{rid}.npy')\n",
        "            preds = np.load(f'experiments/{args.exp_name}/preds/{rid}.npy')\n",
        "            har.log_preds_and_scores(rid,preds,best_preds,numpyify(rdset.y))\n",
        "        print('clustering remaining ids', [x for x in subj_ids[args.reload_ids:]], 'from scratch\\n')\n",
        "\n",
        "        print(\"CLUSTERING EACH DSET SEPARATELY\")\n",
        "        for subj_id, (dset,sa) in dsets_by_id.items():\n",
        "            print(\"clustering\", subj_id)\n",
        "            har.pseudo_label_cluster_meta_meta_loop(subj_id,dset)\n",
        "    elif args.subject_independent:\n",
        "        print(\"CLUSTERING AS SINGLE DSET\")\n",
        "        one_big_dset, selected_acts = make_single_dset(args,subj_ids)\n",
        "        har.pseudo_label_cluster_meta_meta_loop('all',one_big_dset)\n",
        "\n",
        "    results_file_path = f'experiments/{args.exp_name}/results.txt'\n",
        "    har.total_time = time.time() - start_time\n",
        "    har.log_final_scores(results_file_path)\n",
        "    har.express_times(results_file_path)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    ARGS, need_umap = cl_args.get_cl_args()\n",
        "    if need_umap: import umap\n",
        "    main(ARGS)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "N-4I7A5hJU7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Lou1sM download_datasets.sh\n",
        "# https://github.com/Lou1sM/HAR/blob/master/download_datasets.sh\n",
        "\n",
        "##!/bin/sh\n",
        "\n",
        "mkdir -p datasets\n",
        "cd datasets\n",
        "\n",
        "#PAMAP\n",
        "# echo -e \"\\n###############\"\n",
        "# echo GETTING AND PREPARING PAMAP DATA\n",
        "# echo -e \"###############\\n\"\n",
        "wget http://archive.ics.uci.edu/ml/machine-learning-databases/00231/PAMAP2_Dataset.zip\n",
        "unzip PAMAP2_Dataset.zip\n",
        "# python ../convert_data_to_np.py PAMAP\n",
        "\n",
        "#UCI\n",
        "# echo -e \"\\n###############\"\n",
        "# echo GETTING AND PREPARING UCI DATA\n",
        "# echo -e \"###############\\n\"\n",
        "mkdir -p UCI2\n",
        "cd UCI2\n",
        "wget http://archive.ics.uci.edu/ml/machine-learning-databases/00341/HAPT%20Data%20Set.zip\n",
        "unzip HAPT\\ Data\\ Set.zip\n",
        "# cd ..\n",
        "# python ../convert_data_to_np.py UCI-raw\n",
        "\n",
        "#mkdir -p capture24\n",
        "#cd capture24/\n",
        "\n",
        "#for i in $(seq -w 151)\n",
        "#do\n",
        "#    curl -JLO \"https://ora.ox.ac.uk/objects/uuid:92650814-a209-4607-9fb5-921eab761c11/download_file?safe_filename=P${i}.csv.gz&type_of_work=Dataset\"\n",
        "#done\n",
        "#\n",
        "#curl -JLO \"https://ora.ox.ac.uk/objects/uuid:92650814-a209-4607-9fb5-921eab761c11/download_file?safe_filename=metadata.csv&type_of_work=Dataset\"\n",
        "#curl -JLO \"https://ora.ox.ac.uk/objects/uuid:92650814-a209-4607-9fb5-921eab761c11/download_file?safe_filename=annotation-label-dictionary.csv&type_of_work=Dataset\"\n",
        "#\n",
        "#\n",
        "#for f in $(ls); do\n",
        "#    if [ ${f: -2} == \"gz\" ]; then\n",
        "#        gunzip $f;\n",
        "#    fi;\n",
        "#done\n",
        "\n",
        "#WISDM-v1\n",
        "# echo -e \"\\n###############\"\n",
        "# echo GETTING AND PREPARING WISDM-v1 DATA\n",
        "# echo -e \"###############\\n\"\n",
        "wget https://www.cis.fordham.edu/wisdm/includes/datasets/latest/WISDM_ar_latest.tar.gz\n",
        "gunzip WISDM_ar_latest.tar.gz\n",
        "tar -xf WISDM_ar_latest.tar\n",
        "\n",
        "# python ../convert_data_to_np.py WISDM-v1\n",
        "\n",
        "#WISDM-watch\n",
        "# echo -e \"\\n###############\"\n",
        "# echo GETTING AND PREPARING WISDM-watch DATA\n",
        "# echo -e \"###############\\n\"\n",
        "wget https://archive.ics.uci.edu/ml/machine-learning-databases/00507/wisdm-dataset.zip\n",
        "unzip wisdm-dataset.zip\n",
        "python ../convert_data_to_np.py WISDM-watch\n",
        "\n",
        "#REALDISP\n",
        "# echo -e \"\\n###############\"\n",
        "# echo GETTING REALDISP DATA\n",
        "# echo -e \"###############\\n\"\n",
        "mkdir -p realdisp\n",
        "cd realdisp\n",
        "mkdir -p RawData\n",
        "wget https://archive.ics.uci.edu/ml/machine-learning-databases/00305/realistic_sensor_displacement.zip\n",
        "unzip realistic_sensor_displacement.zip\n",
        "# cd ..\n",
        "# python ../convert_data_to_np.py REALDISP\n",
        "# cd ..\n",
        "# pwd\n",
        "#HHAR\n",
        "mkdir -p hhar\n",
        "wget http://archive.ics.uci.edu/ml/machine-learning-databases/00344/Activity%20recognition%20exp.zip\n",
        "unzip Activity\\ recognition\\ exp.zip\n",
        "# python ../convert_data_to_np.py HHAR\n",
        "# cd ..\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "cellView": "form",
        "id": "38tWCkK-0Zlu",
        "outputId": "7abe4629-3dcf-4981-810e-9e888be03cb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "leading zeros in decimal integer literals are not permitted; use an 0o prefix for octal integers (<ipython-input-1-94e7ebe34632>, line 10)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-94e7ebe34632>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    wget http://archive.ics.uci.edu/ml/machine-learning-databases/00231/PAMAP2_Dataset.zip\u001b[0m\n\u001b[0m                                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m leading zeros in decimal integer literals are not permitted; use an 0o prefix for octal integers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title PAMAP2\n",
        "# https://arxiv.org/pdf/2209.08335v1\n",
        "# https://archive.ics.uci.edu/dataset/231/pamap2+physical+activity+monitoring\n",
        "# https://archive.ics.uci.edu/static/public/231/pamap2+physical+activity+monitoring.zip\n",
        "# !wget http://archive.ics.uci.edu/ml/machine-learning-databases/00231/PAMAP2_Dataset.zip\n",
        "# !unzip PAMAP2_Dataset.zip\n",
        "\n",
        "import os\n",
        "data_dir = 'PAMAP2_Dataset/Protocol'\n",
        "np_dir = '/content/PAMAP2'\n",
        "if not os.path.isdir(np_dir):\n",
        "    os.makedirs(np_dir)\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def array_from_txt(inpath):\n",
        "    with open(inpath) as f:\n",
        "        d = f.readlines()\n",
        "        array = np.array([[float(x) for x in line.split()] for line in d])\n",
        "    return array\n",
        "\n",
        "def array_expanded(a,expanded_length):\n",
        "    if a.shape[0] >= expanded_length: return a\n",
        "    insert_every = mpf(a.shape[0]/(expanded_length-a.shape[0]))\n",
        "    additional_idxs = (np.arange(expanded_length-a.shape[0])*insert_every).astype(np.int)\n",
        "    values = a[additional_idxs]\n",
        "    expanded = np.insert(a,additional_idxs,values,axis=0)\n",
        "    assert expanded.shape[0] == expanded_length\n",
        "    return expanded\n",
        "\n",
        "def convert(inpath,outpath):\n",
        "    array = array_from_txt(inpath)\n",
        "    timestamps = array[:,0]\n",
        "    labels = array[:,1].astype(int)\n",
        "    # Delete orientation data, which webpage says is 'invalid in this data, and timestamp and label\n",
        "    array = np.delete(array,[0,1,2,13,14,15,16,30,31,32,33,47,48,49,50],1)\n",
        "    np.save(outpath+'_timestamps',timestamps)\n",
        "    np.save(outpath+'_labels',labels)\n",
        "    np.save(outpath,array)\n",
        "\n",
        "for filename in os.listdir(data_dir):\n",
        "    print(data_dir,filename)\n",
        "    # inpath = os.path.join(data_dir,filename)a\n",
        "    inpath = data_dir+'/'+filename\n",
        "    outpath = np_dir+'/'+filename.split('.')[0]\n",
        "    print(outpath)\n",
        "    # outpath = os.path.join(np_dir,filename.split('.')[0])\n",
        "    convert(inpath,outpath)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMvb1-PBQ9u-",
        "outputId": "764e8452-935e-4bcc-aa38-82faddee7da3",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PAMAP2_Dataset/Protocol subject106.dat\n",
            "/content/PAMAP2/subject106\n",
            "PAMAP2_Dataset/Protocol subject101.dat\n",
            "/content/PAMAP2/subject101\n",
            "PAMAP2_Dataset/Protocol subject107.dat\n",
            "/content/PAMAP2/subject107\n",
            "PAMAP2_Dataset/Protocol subject109.dat\n",
            "/content/PAMAP2/subject109\n",
            "PAMAP2_Dataset/Protocol subject103.dat\n",
            "/content/PAMAP2/subject103\n",
            "PAMAP2_Dataset/Protocol subject104.dat\n",
            "/content/PAMAP2/subject104\n",
            "PAMAP2_Dataset/Protocol subject108.dat\n",
            "/content/PAMAP2/subject108\n",
            "PAMAP2_Dataset/Protocol subject105.dat\n",
            "/content/PAMAP2/subject105\n",
            "PAMAP2_Dataset/Protocol subject102.dat\n",
            "/content/PAMAP2/subject102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# def make_pamap_dset_train_val(args,subj_ids):\n",
        "def make_pamap_dset_train_val(subj_ids):\n",
        "    # dset_info_object = PAMAP_INFO.PAMAP_INFO\n",
        "    dset_info_object = PAMAP_INFO\n",
        "    # x_train = np.concatenate([np.load(f'datasets/PAMAP2_Dataset/np_data/subject{s}.npy') for s in subj_ids])\n",
        "    # y_train = np.concatenate([np.load(f'datasets/PAMAP2_Dataset/np_data/subject{s}_labels.npy') for s in subj_ids])\n",
        "    x_train = np.concatenate([np.load(f'PAMAP2/subject{s}.npy') for s in subj_ids])\n",
        "    y_train = np.concatenate([np.load(f'PAMAP2/subject{s}_labels.npy') for s in subj_ids])\n",
        "    x_train = x_train[y_train!=0] # 0 is a transient activity\n",
        "    y_train = y_train[y_train!=0] # 0 is a transient activity\n",
        "    # x_train,y_train,selected_acts = preproc_xys(x_train,y_train,args.step_size,args.window_size,dset_info_object,subj_ids)\n",
        "    x_train,y_train,selected_acts = preproc_xys(x_train,y_train,1,1,dset_info_object,subj_ids)\n",
        "    # dset_train = StepDataset(x_train,y_train,window_size=args.window_size,step_size=args.step_size)\n",
        "    # dset_train = StepDataset(x_train,y_train,window_size=512,step_size=5)\n",
        "    dset_train = StepDataset(x_train,y_train,window_size=1,step_size=1)\n",
        "    return dset_train, selected_acts\n",
        "\n",
        "class HAR_Dataset_Container():\n",
        "    def __init__(self,code_name,dataset_dir_name,possible_subj_ids,num_channels,num_classes,action_name_dict):\n",
        "        self.code_name = code_name\n",
        "        self.dataset_dir_name = dataset_dir_name\n",
        "        self.possible_subj_ids = possible_subj_ids\n",
        "        self.num_channels = num_channels\n",
        "        self.num_classes = num_classes\n",
        "        self.action_name_dict = action_name_dict\n",
        "\n",
        "\n",
        "# PAMAP\n",
        "pamap_ids = [str(x) for x in range(101,110)]\n",
        "pamap_action_name_dict = {1:'lying',2:'sitting',3:'standing',4:'walking',5:'running',6:'cycling',7:'Nordic walking',9:'watching TV',10:'computer work',11:'car driving',12:'ascending stairs',13:'descending stairs',16:'vacuum cleaning',17:'ironing',18:'folding laundry',19:'house cleaning',20:'playing soccer',24:'rope jumping'}\n",
        "PAMAP_INFO = HAR_Dataset_Container(\n",
        "            code_name = 'PAMAP',\n",
        "            dataset_dir_name = 'PAMAP2_Dataset',\n",
        "            possible_subj_ids = pamap_ids,\n",
        "            num_channels = 39,\n",
        "            num_classes = 12,\n",
        "            action_name_dict = pamap_action_name_dict)\n",
        "\n",
        "dset_train, selected_acts = make_pamap_dset_train_val(pamap_ids)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "5TnsfKrMB0yF",
        "outputId": "07859402-b33c-4ede-c389-ef7a59c2f1ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no precomputed datasets, computing from scratch\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "invalid index to scalar variable.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-ddcbc2f0291f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m             action_name_dict = pamap_action_name_dict)\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mdset_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_acts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_pamap_dset_train_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpamap_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-ddcbc2f0291f>\u001b[0m in \u001b[0;36mmake_pamap_dset_train_val\u001b[0;34m(subj_ids)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# 0 is a transient activity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# x_train,y_train,selected_acts = preproc_xys(x_train,y_train,args.step_size,args.window_size,dset_info_object,subj_ids)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mselected_acts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreproc_xys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdset_info_object\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msubj_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;31m# dset_train = StepDataset(x_train,y_train,window_size=args.window_size,step_size=args.step_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# dset_train = StepDataset(x_train,y_train,window_size=512,step_size=5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-a048671b2310>\u001b[0m in \u001b[0;36mpreproc_xys\u001b[0;34m(x, y, step_size, window_size, dset_info_object, subj_ids)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mnum_windows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mstep_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m#mode_labels = np.array([stats.mode(y[w*step_size:w*step_size + window_size]).mode[0] if (y[w*step_size:w*step_size + window_size]==y[w*step_size]).all() else -1 for w in range(num_windows)])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mmode_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstep_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_windows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mselected_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mselected_acts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdset_info_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_name_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mact_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mact_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselected_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-a048671b2310>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mnum_windows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mstep_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m#mode_labels = np.array([stats.mode(y[w*step_size:w*step_size + window_size]).mode[0] if (y[w*step_size:w*step_size + window_size]==y[w*step_size]).all() else -1 for w in range(num_windows)])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mmode_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstep_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_windows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mselected_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mselected_acts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdset_info_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_name_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mact_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mact_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselected_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Lou1sM convert_data_to_np.py\n",
        "# https://github.com/Lou1sM/HAR/blob/master/convert_data_to_np.py\n",
        "from pdb import set_trace\n",
        "from collections import Counter\n",
        "from scipy.fft import fft\n",
        "from scipy import stats\n",
        "from mpmath import mp, mpf\n",
        "#from dl_utils import misc, label_funcs\n",
        "# from dl_utils import misc\n",
        "# import label_funcs_tmp\n",
        "import os\n",
        "from os.path import join\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "def array_from_txt(inpath):\n",
        "    with open(inpath) as f:\n",
        "        d = f.readlines()\n",
        "        array = np.array([[float(x) for x in line.split()] for line in d])\n",
        "    return array\n",
        "\n",
        "def array_expanded(a,expanded_length):\n",
        "    if a.shape[0] >= expanded_length: return a\n",
        "    insert_every = mpf(a.shape[0]/(expanded_length-a.shape[0]))\n",
        "    additional_idxs = (np.arange(expanded_length-a.shape[0])*insert_every).astype(np.int)\n",
        "    values = a[additional_idxs]\n",
        "    expanded = np.insert(a,additional_idxs,values,axis=0)\n",
        "    assert expanded.shape[0] == expanded_length\n",
        "    return expanded\n",
        "\n",
        "def convert(inpath,outpath):\n",
        "    array = array_from_txt(inpath)\n",
        "    timestamps = array[:,0]\n",
        "    labels = array[:,1].astype(np.int)\n",
        "    # Delete orientation data, which webpage says is 'invalid in this data, and timestamp and label\n",
        "    array = np.delete(array,[0,1,2,13,14,15,16,30,31,32,33,47,48,49,50],1)\n",
        "    np.save(outpath+'_timestamps',timestamps)\n",
        "    np.save(outpath+'_labels',labels)\n",
        "    np.save(outpath,array)\n",
        "\n",
        "def expand_and_fill_labels(a,propoer_length):\n",
        "    start_filler = -np.ones(a[0,3])\n",
        "    end_filler = -np.ones(propoer_length-a[-1,4])\n",
        "    nested_lists = [[a[i,2] for _ in range(a[i,4]-a[i,3])] + [-1]*(a[i+1,3]-a[i,4]) for i in range(len(a)-1)] + [[a[-1,2] for _ in range(a[-1,4]-a[-1,3])]]\n",
        "    middle = np.array([item for sublist in nested_lists for item in sublist])\n",
        "    total_label_array = np.concatenate((start_filler,middle,end_filler)).astype(np.int)\n",
        "    return total_label_array\n",
        "\n",
        "def add_dtft(signal):\n",
        "    fft_signal_complex = fft(signal,axis=-1)\n",
        "    fft_signal_modulusses = np.abs(fft_signal_complex)\n",
        "    return np.concatenate((signal,fft_signal_modulusses),axis=-1)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "# if sys.argv[1] == 'PAMAP':\n",
        "data_dir = 'PAMAP2_Dataset/Protocol'\n",
        "np_dir = 'PAMAP2_Dataset/np_data'\n",
        "print(\"\\n#####Preprocessing PAMAP2#####\\n\")\n",
        "if not os.path.isdir(np_dir):\n",
        "    os.makedirs(np_dir)\n",
        "\n",
        "for filename in os.listdir(data_dir):\n",
        "    print(filename)\n",
        "    inpath = join(data_dir,filename)\n",
        "    outpath = join(np_dir,filename.split('.')[0])\n",
        "    convert(inpath,outpath)\n",
        "\n",
        "# elif sys.argv[1] == 'UCI-raw':\n",
        "data_dir = 'UCI2/RawData'\n",
        "np_dir = 'UCI2/np_data'\n",
        "print(\"\\n#####Preprocessing UCI#####\\n\")\n",
        "if not os.path.isdir(np_dir):\n",
        "    os.makedirs(np_dir)\n",
        "\n",
        "raw_label_array = array_from_txt(join(data_dir,'labels.txt')).astype(int)\n",
        "def two_digitify(x): return '0'+str(x) if len(str(x))==1 else str(x)\n",
        "fnames = os.listdir(data_dir)\n",
        "for idx in range(1,31):\n",
        "    print(\"processing user\",idx)\n",
        "    acc_array_list = []\n",
        "    gyro_array_list = []\n",
        "    label_array_list = []\n",
        "    user_idx = two_digitify(idx)\n",
        "    acc_fpaths = sorted([fn for fn in fnames if f'user{user_idx}' in fn and 'acc' in fn])\n",
        "    gyro_fpaths = sorted([fn for fn in fnames if f'user{user_idx}' in fn and 'gyro' in fn])\n",
        "    assert len(acc_fpaths) == len(gyro_fpaths)\n",
        "    for fna,fng in zip(acc_fpaths,gyro_fpaths):\n",
        "        acc_exp_id = int(fna.split('exp')[1][:2])\n",
        "        gyro_exp_id = int(fng.split('exp')[1][:2])\n",
        "        assert acc_exp_id==gyro_exp_id\n",
        "        new_acc_array = array_from_txt(join(data_dir,fna))\n",
        "        new_gyro_array = array_from_txt(join(data_dir,fna))\n",
        "        label_array_block = raw_label_array[raw_label_array[:,0]==acc_exp_id]\n",
        "        filled_label_array_block = expand_and_fill_labels(label_array_block,new_acc_array.shape[0])\n",
        "        assert filled_label_array_block.shape[0] == new_acc_array.shape[0]\n",
        "        assert filled_label_array_block.shape[0] == new_gyro_array.shape[0]\n",
        "        label_array_list.append(filled_label_array_block)\n",
        "        acc_array_list.append(new_acc_array)\n",
        "        gyro_array_list.append(new_gyro_array)\n",
        "    label_array = np.concatenate(label_array_list)\n",
        "    acc_array = np.concatenate(acc_array_list)\n",
        "    gyro_array = np.concatenate(gyro_array_list)\n",
        "    total_array = np.concatenate((acc_array,gyro_array),axis=1)\n",
        "    outpath = join(np_dir,f'user{user_idx}.npy')\n",
        "    np.save(outpath,total_array)\n",
        "    label_outpath = join(np_dir,f'user{user_idx}_labels.npy')\n",
        "    np.save(label_outpath,label_array)\n",
        "\n",
        "# elif sys.argv[1] == 'WISDM-v1':\n",
        "with open('WISDM_ar_v1.1/WISDM_ar_v1.1_raw.txt') as f: text = f.readlines()\n",
        "print(\"\\n#####Preprocessing WISDM-v1#####\\n\")\n",
        "activities_list = ['Jogging','Walking','Upstairs','Downstairs','Standing','Sitting']\n",
        "X_list = []\n",
        "y_list = []\n",
        "users_list = []\n",
        "num_zeros = 0\n",
        "def process_line(line_to_process):\n",
        "    global num_zeros\n",
        "    if float(line_to_process.split(',')[2]) == 0: num_zeros += 1#print(\"Timestamp zero, discarding\")\n",
        "    else:\n",
        "        X_list.append([float(x) for x in line_to_process.split(',')[3:]])\n",
        "        y_list.append(activities_list.index(line_to_process.split(',')[1]))\n",
        "        users_list.append(line_to_process.split(',')[0])\n",
        "for i,raw_line in enumerate(text):\n",
        "    #line = line.replace(';','').replace('\\n','')\n",
        "    if raw_line == '\\n': continue\n",
        "    elif raw_line.endswith(',;\\n'): line = raw_line[:-3]\n",
        "    elif raw_line.endswith(';\\n'): line = raw_line[:-2]\n",
        "    elif raw_line.endswith(',\\n'): line = raw_line[:-2]\n",
        "    else: set_trace()\n",
        "    if len(line.split(',')) == 6:\n",
        "        try: process_line(line)\n",
        "        except: print(f\"Can't process line {i}, even though length 6: {raw_line}\\n\")\n",
        "    else:\n",
        "        print(f\"Bad format at line {i}:\\n{raw_line}\")\n",
        "        try:\n",
        "            line1, line2 = line.split(';')\n",
        "            process_line(line1); process_line(line2)\n",
        "            print(f\"I think this was two lines erroneously put on one line. Processing separately as\\n{line1}\\nand\\n{line2}\")\n",
        "        except: print(\"Can't process this line at all, omitting\")\n",
        "one_big_X_array = np.array(X_list)\n",
        "one_big_y_array = np.array(y_list)\n",
        "one_big_users_array = np.array(users_list)\n",
        "print(one_big_X_array.shape)\n",
        "print(one_big_y_array.shape)\n",
        "print(one_big_users_array.shape)\n",
        "print(f\"Number of zero lines: {num_zeros}\")\n",
        "misc.np_save(one_big_X_array,'wisdm_v1','X.npy')\n",
        "misc.np_save(one_big_y_array,'wisdm_v1','y.npy')\n",
        "misc.np_save(one_big_users_array,'wisdm_v1','users.npy')\n",
        "\n",
        "# elif sys.argv[1] == 'WISDM-watch':\n",
        "p_dir = 'wisdm-dataset/raw/phone'\n",
        "w_dir = 'wisdm-dataset/raw/watch'\n",
        "np_dir = 'wisdm-dataset/np_data'\n",
        "print(\"\\n#####Preprocessing WISDM-watch#####\\n\")\n",
        "if not os.path.isdir(np_dir):\n",
        "    os.makedirs(np_dir)\n",
        "\n",
        "mp.dps = 100 # Avoid floating point errors in label insertion function\n",
        "for user_idx in range(1600,1651):\n",
        "    print('user', user_idx)\n",
        "    phone_acc_path = join(p_dir,'accel',f'data_{user_idx}_accel_phone.txt')\n",
        "    watch_acc_path = join(w_dir,'accel',f'data_{user_idx}_accel_watch.txt')\n",
        "    phone_gyro_path = join(p_dir,'gyro',f'data_{user_idx}_gyro_phone.txt')\n",
        "    watch_gyro_path = join(w_dir,'gyro',f'data_{user_idx}_gyro_watch.txt')\n",
        "\n",
        "    label_codes_list = list('ABCDEFGHIJKLMOPQRS') # Missin 'N' is deliberate\n",
        "    def two_arrays_from_txt(inpath):\n",
        "        with open(inpath) as f:\n",
        "            d = f.readlines()\n",
        "            arr = np.array([[float(x) for x in line.strip(';\\n').split(',')[3:]] for line in d])\n",
        "            label_array = np.array([label_codes_list.index(line.split(',')[1]) for line in d])\n",
        "        return arr, label_array\n",
        "\n",
        "    phone_acc, label_array1 = two_arrays_from_txt(phone_acc_path)\n",
        "    watch_acc, label_array2 = two_arrays_from_txt(watch_acc_path)\n",
        "    phone_gyro, label_array3 = two_arrays_from_txt(phone_gyro_path)\n",
        "    watch_gyro, label_array4 = two_arrays_from_txt(watch_gyro_path)\n",
        "    user_arrays = [phone_acc,watch_acc,phone_gyro,watch_gyro]\n",
        "    label_arrays = [label_array1,label_array2,label_array3,label_array4]\n",
        "    max_len = max([a.shape[0] for a in user_arrays])\n",
        "    equalized_user_arrays = [array_expanded(a,max_len) for a in user_arrays]\n",
        "    equalized_label_arrays = [array_expanded(lab_a,max_len) for lab_a in label_arrays]\n",
        "    total_user_array = np.concatenate(equalized_user_arrays,axis=1)\n",
        "    mode_object = stats.mode(np.stack(equalized_label_arrays,axis=1),axis=1)\n",
        "    mode_labels = mode_object.mode[:,0]\n",
        "    # Print how many windows contained just 1 label, how many 2 etc.\n",
        "    #print('Agreement in labels:',label_funcs_tmp.label_counts(mode_object.count[:,0]))\n",
        "    certains = (mode_object.count == 4)[:,0]\n",
        "    user_fn = f'{user_idx}.npy'\n",
        "    misc.np_save(total_user_array,np_dir,user_fn)\n",
        "    user_labels_fn = f'{user_idx}_labels.npy'\n",
        "    misc.np_save(mode_labels,np_dir,user_labels_fn)\n",
        "    user_certains_fn = f'{user_idx}_certains.npy'\n",
        "    misc.np_save(certains,np_dir,user_certains_fn)\n",
        "\n",
        "# elif sys.argv[1] == 'REALDISP':\n",
        "data_dir = 'realdisp/RawData'\n",
        "np_dir = 'realdisp/np_data'\n",
        "print(\"\\n#####Preprocessing REALDISP#####\\n\")\n",
        "if not os.path.isdir(np_dir):\n",
        "    os.makedirs(np_dir)\n",
        "\n",
        "for filename in os.listdir(data_dir):\n",
        "    print(filename)\n",
        "    if filename == 'dataset manual.pdf': continue\n",
        "    if not filename.split('_')[1].startswith('ideal'):\n",
        "        continue\n",
        "    with open(join(data_dir,filename)) as f: xy = f.readlines()\n",
        "    ar = np.array([[float(item) for item in line.split('\\t')] for line in xy])\n",
        "    x = ar[:,:-1]\n",
        "    y = ar[:,-1].astype(int)\n",
        "\n",
        "    np.save(join(np_dir,filename.split('_')[0]), x)\n",
        "    np.save(join(np_dir,filename.split('_')[0])+'_labels', y)\n",
        "\n",
        "# elif sys.argv[1] == 'Capture24':\n",
        "np_dir = 'capture24/np_data'\n",
        "if not os.path.isdir(np_dir):\n",
        "    os.makedirs(np_dir)\n",
        "name_df = pd.read_csv('capture24/annotation-label-dictionary.csv')\n",
        "#name_conversion_dict = dict(zip(name_df['annotation'],name_df['label:DohertySpecific2018']))\n",
        "name_df = name_df[['annotation','label:DohertySpecific2018']]\n",
        "int_label_converter_df = pd.DataFrame(enumerate(name_df['label:DohertySpecific2018'].unique()),columns=['int_label','label:DohertySpecific2018'])\n",
        "int_label_converter_dict = dict(enumerate(name_df['label:DohertySpecific2018'].unique()))\n",
        "with open('capture24/int_label_converter_df.json','w') as f:\n",
        "    json.dump(int_label_converter_dict,f)\n",
        "name_df = name_df.merge(int_label_converter_df)\n",
        "for fname in os.listdir('capture24'):\n",
        "    if fname.endswith('.gz'): continue\n",
        "    subj_id = fname.split('.')[0]\n",
        "    if not subj_id.startswith('P') and not len(subj_id) == 4: continue # Skip metadata files\n",
        "    print(f\"converting {fname} to np\")\n",
        "    try: df = pd.read_csv(join('capture24',fname))\n",
        "    except: set_trace()\n",
        "    translated_df = df.merge(name_df)\n",
        "    x = translated_df[['x','y','z']].to_numpy()\n",
        "    y = translated_df['int_label'].to_numpy()\n",
        "    np.save(join(np_dir,f'{subj_id}.npy'),x)\n",
        "    np.save(join(np_dir,f'{subj_id}_labels.npy'),y)\n",
        "\n",
        "# elif sys.argv[1] == 'HHAR':\n",
        "data_dir = 'Activity recognition exp'\n",
        "np_dir = 'hhar/np_data'\n",
        "print(\"\\n#####Preprocessing HHAR#####\\n\")\n",
        "if not os.path.isdir(np_dir):\n",
        "    os.makedirs(np_dir)\n",
        "\n",
        "pandaload = lambda path: pd.read_csv(join(data_dir,'Phones_accelerometer.csv')).set_index('Creation_Time').drop(['Index','Arrival_Time','Model','Device'],axis=1).dropna()\n",
        "print('loading dataframes\\n')\n",
        "phone_acc_df = pandaload('Phones_accelerometer.csv')\n",
        "phone_gyro_df = pandaload('Phones_gyroscope.csv')\n",
        "watch_acc_df = pandaload('Watch_accelerometer.csv')\n",
        "watch_gyro_df = pandaload('Watch_gyroscope.csv')\n",
        "activities_list = ['bike', 'sit', 'stand', 'walk', 'stairsup', 'stairsdown']\n",
        "user_list = list('abcdefghi')\n",
        "\n",
        "for user_letter_name in user_list:\n",
        "    print('processing user', user_letter_name)\n",
        "    user_phone_acc = phone_acc_df.loc[phone_acc_df.User==user_letter_name]\n",
        "    user_phone_gyro = phone_gyro_df.loc[phone_gyro_df.User==user_letter_name]\n",
        "    user_watch_acc = watch_acc_df.loc[watch_acc_df.User==user_letter_name]\n",
        "    user_watch_gyro = watch_acc_df.loc[watch_gyro_df.User==user_letter_name]\n",
        "    assert all([user_watch_gyro.shape==d.shape for d in (user_phone_acc,user_phone_gyro,user_watch_acc)])\n",
        "    comb_phone = user_phone_acc.join(user_phone_gyro,how='outer',lsuffix='_acc',rsuffix='_gyro')\n",
        "    comb_watch = user_watch_acc.join(user_watch_gyro,how='outer',lsuffix='_acc',rsuffix='_gyro')\n",
        "    #if not (comb_watch.gt_acc == comb_watch.gt_gyro).all(): set_trace()\n",
        "    #if not (comb_phone.gt_acc == comb_phone.gt_gyro).all(): set_trace()\n",
        "    comb = comb_phone.join(comb_watch,how='outer',lsuffix='_phone',rsuffix='_watch')\n",
        "    duplicate_rows = [x for x,count in Counter(comb.index).items() if count > 1]\n",
        "    if len(duplicate_rows) > 10: set_trace()\n",
        "    elif len(duplicate_rows) > 0:\n",
        "        print( f\"removing {len(duplicate_rows)} duplicate rows\")\n",
        "        comb = comb.drop(duplicate_rows)\n",
        "    if not (comb.gt_acc_phone == comb.gt_acc_watch).all(): set_trace()\n",
        "    user_X_array = comb.drop([c for c in comb.columns if 'User' in c or 'gt' in c],axis=1).to_numpy()\n",
        "    user_y_array = np.array([activities_list.index(a) for a in comb['gt_acc_phone']])\n",
        "    save_path = join(np_dir,f\"{user_list.index(user_letter_name)+1}.npy\")\n",
        "    label_save_path = join(np_dir,f\"{user_list.index(user_letter_name)+1}_labels.npy\")\n",
        "    np.save(save_path,user_X_array,allow_pickle=False)\n",
        "    np.save(label_save_path,user_y_array,allow_pickle=False)\n",
        "    # Make smaller option for testing\n",
        "    np.save(join(np_dir,f\"0.npy\"),user_X_array[::1000],allow_pickle=False)\n",
        "    np.save(join(np_dir,f\"0_labels.npy\"),user_y_array[::1000],allow_pickle=False)\n",
        "\n",
        "else: print('\\nIncorrect or no dataset specified\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGU_rUapvjcE",
        "outputId": "b150f60b-4d01-47b5-b1f4-ea6b20a17027",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Incorrect or no dataset specified\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Lou1sM make_dsets.py\n",
        "# https://github.com/Lou1sM/HAR/blob/master/make_dsets.py\n",
        "import numpy as np\n",
        "# from dl_utils.misc import check_dir, CifarLikeDataset\n",
        "import os\n",
        "import torch\n",
        "# import project_config\n",
        "from scipy import stats\n",
        "from torch.utils import data\n",
        "#from dl_utils import label_funcs\n",
        "# from dl_utils.tensor_funcs import cudify\n",
        "# import label_funcs_tmp\n",
        "from pdb import set_trace\n",
        "from os.path import join\n",
        "\n",
        "\n",
        "class ChunkDataset(data.Dataset):\n",
        "    def __init__(self,x,y):\n",
        "        self.x, self.y = x,y\n",
        "    def __len__(self): return len(self.x)\n",
        "    def __getitem__(self,idx):\n",
        "        batch_x = self.x[idx].unsqueeze(0)\n",
        "        batch_y = self.y[idx]\n",
        "        return batch_x, batch_y, idx\n",
        "\n",
        "class ConcattedDataset(data.Dataset):\n",
        "    \"\"\"Needs datasets to be StepDatasets in order to Concat them.\"\"\"\n",
        "    def __init__(self,xs,ys,window_size,step_size):\n",
        "        self.x, self.y = torch.cat(xs),torch.cat(ys)\n",
        "        self.window_size = window_size\n",
        "        self.step_size = step_size\n",
        "        component_dset_lengths = [((len(x)-self.window_size)//self.step_size + 1) for x in xs]\n",
        "        x_idx_locs = []\n",
        "        block_start_idx = 0\n",
        "        for x in xs:\n",
        "            x_idx_locs += list(range(block_start_idx,block_start_idx+len(x)-window_size+1,step_size))\n",
        "            block_start_idx += len(x)\n",
        "        self.x_idx_locs = np.array(x_idx_locs)\n",
        "        if not len(self.x_idx_locs) == len(self.y): set_trace()\n",
        "\n",
        "    def __len__(self): return len(self.y)\n",
        "    def __getitem__(self,idx):\n",
        "        x_idx = self.x_idx_locs[idx]\n",
        "        batch_x = self.x[x_idx:x_idx + self.window_size].unsqueeze(0)\n",
        "        batch_y = self.y[idx]\n",
        "        return batch_x, batch_y, idx\n",
        "\n",
        "class UCIFeatDataset(data.Dataset):\n",
        "    def __init__(self,x,y,transforms=[]):\n",
        "        self.x, self.y = cudify(x).float(),cudify(y).float()\n",
        "        assert len(self.x) == len(self.y)\n",
        "        for transform in transforms:\n",
        "            self.x = transform(self.x)\n",
        "    def __len__(self): return len(self.x)\n",
        "    def __getitem__(self,idx):\n",
        "        batch_x = self.x[idx]\n",
        "        batch_y = self.y[idx]\n",
        "        return batch_x, batch_y, idx\n",
        "\n",
        "class StepDataset(data.Dataset):\n",
        "    def __init__(self,x,y,window_size,step_size,transforms=[]):\n",
        "        self.x, self.y = cudify(x).float(),cudify(y).float()\n",
        "        self.window_size = window_size\n",
        "        self.step_size = step_size\n",
        "        self.transforms = transforms\n",
        "        self.position = None\n",
        "        self.ensemble_size = None\n",
        "        for transform in transforms:\n",
        "            self.x = transform(self.x)\n",
        "    def __len__(self): return (len(self.x)-self.window_size)//self.step_size + 1\n",
        "    def __getitem__(self,idx):\n",
        "        batch_x = self.x[idx*self.step_size:(idx*self.step_size) + self.window_size].unsqueeze(0)\n",
        "        batch_y = self.y[idx]\n",
        "        return batch_x, batch_y, idx\n",
        "\n",
        "    def put_in_ensemble(self,position,ensemble_size):\n",
        "        self.y += ensemble_size*position\n",
        "        self.position = position\n",
        "        self.ensemble_size = ensemble_size\n",
        "\n",
        "def preproc_xys(x,y,step_size,window_size,dset_info_object,subj_ids):\n",
        "    ids_string = 'all' if set(subj_ids) == set(dset_info_object.possible_subj_ids) else \"-\".join(subj_ids)\n",
        "    precomp_dir = f'datasets/{dset_info_object.dataset_dir_name}/precomputed/{ids_string}step{step_size}_window{window_size}/'\n",
        "    if os.path.isfile(join(precomp_dir,'x.pt')) and os.path.isfile(join(precomp_dir,'y.pt')):\n",
        "        print(\"loading precomputed datasets\")\n",
        "        x = torch.load(join(precomp_dir,'x.pt'))\n",
        "        y = torch.load(join(precomp_dir,'y.pt'))\n",
        "        with open(join(precomp_dir,'selected_acts.txt')) as f: selected_acts = f.readlines()\n",
        "    else:\n",
        "        print(\"no precomputed datasets, computing from scratch\")\n",
        "        xnans = np.isnan(x).any(axis=1)\n",
        "        x = x[~xnans]\n",
        "        y = y[~xnans]\n",
        "        x = x[y!=-1]\n",
        "        y = y[y!=-1]\n",
        "        num_windows = (len(x) - window_size)//step_size + 1\n",
        "        #mode_labels = np.array([stats.mode(y[w*step_size:w*step_size + window_size]).mode[0] if (y[w*step_size:w*step_size + window_size]==y[w*step_size]).all() else -1 for w in range(num_windows)])\n",
        "        mode_labels = np.array([stats.mode(y[w*step_size:w*step_size + window_size]).mode[0] for w in range(num_windows)])\n",
        "        selected_ids = set(mode_labels)\n",
        "        selected_acts = [dset_info_object.action_name_dict[act_id] for act_id in selected_ids]\n",
        "        mode_labels, trans_dict, changed = label_funcs_tmp.compress_labels(mode_labels)\n",
        "        assert len(selected_acts) == len(set(mode_labels))\n",
        "        x = torch.tensor(x).float()\n",
        "        y = torch.tensor(mode_labels).float()\n",
        "        check_dir(precomp_dir)\n",
        "        torch.save(x,join(precomp_dir,'x.pt'))\n",
        "        torch.save(y,join(precomp_dir,'y.pt'))\n",
        "        with open(join(precomp_dir,'selected_acts.txt'),'w') as f:\n",
        "            for a in selected_acts: f.write(a+'\\n')\n",
        "    return x, y, selected_acts\n",
        "\n",
        "def make_pamap_dset_train_val(args,subj_ids):\n",
        "    # dset_info_object = project_config.PAMAP_INFO\n",
        "    dset_info_object = PAMAP_INFO\n",
        "    x_train = np.concatenate([np.load(f'datasets/PAMAP2_Dataset/np_data/subject{s}.npy') for s in subj_ids])\n",
        "    y_train = np.concatenate([np.load(f'datasets/PAMAP2_Dataset/np_data/subject{s}_labels.npy') for s in subj_ids])\n",
        "    x_train = x_train[y_train!=0] # 0 is a transient activity\n",
        "    y_train = y_train[y_train!=0] # 0 is a transient activity\n",
        "    x_train,y_train,selected_acts = preproc_xys(x_train,y_train,args.step_size,args.window_size,dset_info_object,subj_ids)\n",
        "    dset_train = StepDataset(x_train,y_train,window_size=args.window_size,step_size=args.step_size)\n",
        "    return dset_train, selected_acts\n",
        "\n",
        "# def make_uci_dset_train_val(args,subj_ids):\n",
        "#     dset_info_object = project_config.UCI_INFO\n",
        "#     x_train = np.concatenate([np.load(f'datasets/UCI2/np_data/user{s}.npy') for s in subj_ids])\n",
        "#     y_train = np.concatenate([np.load(f'datasets/UCI2/np_data/user{s}_labels.npy') for s in subj_ids])\n",
        "#     x_train = x_train[y_train<7] # Labels still begin at 1 at this point as\n",
        "#     y_train = y_train[y_train<7] # haven't been compressed, so select 1,..,6\n",
        "#     #x_train = x_train[y_train!=-1]\n",
        "#     #y_train = y_train[y_train!=-1]\n",
        "#     #y_val = y_val[y_val!=-1]\n",
        "#     x_train,y_train,selected_acts = preproc_xys(x_train,y_train,args.step_size,args.window_size,dset_info_object,subj_ids)\n",
        "#     dset_train = StepDataset(x_train,y_train,window_size=args.window_size,step_size=args.step_size)\n",
        "#     return dset_train, selected_acts\n",
        "\n",
        "# def make_uci_feat_dset_train_val():\n",
        "#     dset_info_object = project_config.UCI_INFO\n",
        "#     x = np.load(f'datasets/UCI_feat/uci_feat_data.npy')\n",
        "#     y = np.load(f'datasets/UCI_feat/uci_feat_targets.npy')\n",
        "#     selected_acts = dict(enumerate(['walking','upstairs','downstairs','sitting','standing','lying']))\n",
        "#     dset = UCIFeatDataset(x,y)\n",
        "#     #dset.x = dset.data\n",
        "#     #dset.y = dset.targets\n",
        "#     return dset, selected_acts\n",
        "\n",
        "# def make_wisdm_v1_dset_train_val(args,subj_ids):\n",
        "#     dset_info_object = project_config.WISDMv1_INFO\n",
        "#     x = np.load('datasets/wisdm_v1/X.npy')\n",
        "#     y = np.load('datasets/wisdm_v1/y.npy')\n",
        "#     users = np.load('datasets/wisdm_v1/users.npy')\n",
        "#     train_idxs_to_user = np.zeros(users.shape[0]).astype(np.bool)\n",
        "#     for subj_id in subj_ids:\n",
        "#         new_users = users==subj_id\n",
        "#         train_idxs_to_user = np.logical_or(train_idxs_to_user,new_users)\n",
        "#     x_train = x[train_idxs_to_user]\n",
        "#     y_train = y[train_idxs_to_user]\n",
        "#     x_train,y_train,selected_acts = preproc_xys(x_train,y_train,args.step_size,args.window_size,dset_info_object,subj_ids)\n",
        "#     dset_train = StepDataset(x_train,y_train,window_size=args.window_size,step_size=args.step_size)\n",
        "#     return dset_train, selected_acts\n",
        "\n",
        "# def make_wisdm_watch_dset_train_val(args,subj_ids):\n",
        "#     dset_info_object = project_config.WISDMwatch_INFO\n",
        "#     x_train = np.concatenate([np.load(f'datasets/wisdm-dataset/np_data/{s}.npy') for s in subj_ids])\n",
        "#     y_train = np.concatenate([np.load(f'datasets/wisdm-dataset/np_data/{s}_labels.npy') for s in subj_ids])\n",
        "#     certains_train = np.concatenate([np.load(f'datasets/wisdm-dataset/np_data/{s}_certains.npy') for s in subj_ids])\n",
        "#     x_train = x_train[certains_train]\n",
        "#     y_train = y_train[certains_train]\n",
        "#     x_train,y_train,selected_acts = preproc_xys(x_train,y_train,args.step_size,args.window_size,dset_info_object,subj_ids)\n",
        "#     dset_train = StepDataset(x_train,y_train,window_size=args.window_size,step_size=args.step_size)\n",
        "#     return dset_train, selected_acts\n",
        "\n",
        "# def make_realdisp_dset_train_val(args,subj_ids):\n",
        "#     dset_info_object = project_config.REALDISP_INFO\n",
        "#     x_train = np.concatenate([np.load(f'datasets/realdisp/np_data/subject{s}.npy') for s in subj_ids])\n",
        "#     y_train = np.concatenate([np.load(f'datasets/realdisp/np_data/subject{s}_labels.npy') for s in subj_ids])\n",
        "#     x_train = x_train[:,2:] #First two columns are timestamp\n",
        "#     x_train = x_train[y_train!=0] # 0 seems to be a transient activity\n",
        "#     y_train = y_train[y_train!=0] # 0 seems to be a transient activity\n",
        "#     x_train,y_train,selected_acts = preproc_xys(x_train,y_train,args.step_size,args.window_size,dset_info_object,subj_ids)\n",
        "#     dset_train = StepDataset(x_train,y_train,window_size=args.window_size,step_size=args.step_size)\n",
        "#     return dset_train, selected_acts\n",
        "\n",
        "# def make_hhar_dset_train_val(args,subj_ids):\n",
        "#     dset_info_object = project_config.HHAR_INFO\n",
        "#     x_train = np.concatenate([np.load(f'datasets/hhar/np_data/{s}.npy') for s in subj_ids])\n",
        "#     y_train = np.concatenate([np.load(f'datasets/hhar/np_data/{s}_labels.npy') for s in subj_ids])\n",
        "#     x_train,y_train,selected_acts = preproc_xys(x_train,y_train,args.step_size,args.window_size,dset_info_object,subj_ids)\n",
        "#     dset_train = StepDataset(x_train,y_train,window_size=args.window_size,step_size=args.step_size)\n",
        "#     return dset_train, selected_acts\n",
        "\n",
        "def make_capture_dset_train_val(args,subj_ids):\n",
        "    action_name_dict = {0: 'sleep', 1: 'sedentary-screen', 2: 'tasks-moderate', 3: 'sedentary-non-screen', 4: 'walking', 5: 'vehicle', 6: 'bicycling', 7: 'tasks-light', 8: 'sports-continuous', 9: 'sport-interrupted'} # Should also be saved in json file in datasets/capture24\n",
        "    subj_ids = len(subj_ids) - min(2,len(subj_ids)//2)\n",
        "    subj_ids = subj_ids[:subj_ids]\n",
        "    def three_digitify(x): return '00' + str(x) if len(str(x))==1 else '0' + str(x)\n",
        "    x_train = np.concatenate([np.load(f'datasets/capture24/np_data/P{three_digitify(s)}.npy') for s in subj_ids])\n",
        "    y_train = np.concatenate([np.load(f'datasets/capture24/np_data/P{three_digitify(s)}_labels.npy') for s in subj_ids])\n",
        "    x_train,y_train,selected_acts = preproc_xys(x_train,y_train,args.step_size,args.window_size,action_name_dict)\n",
        "    dset_train = StepDataset(x_train,y_train,window_size=args.window_size,step_size=args.step_size)\n",
        "    if len(subj_ids) <= 2: return dset_train, dset_train, selected_acts\n",
        "\n",
        "    # else make val dset\n",
        "    val_ids = subj_ids[subj_ids:]\n",
        "    x_val = np.concatenate([np.load(f'datasets/capture24/np_data/P{three_digitify(s)}.npy') for s in val_ids])\n",
        "    y_val = np.concatenate([np.load(f'datasets/capture24/np_data/P{three_digitify(s)}_labels.npy') for s in val_ids])\n",
        "    x_val,y_val,selected_acts = preproc_xys(x_val,y_val,args.step_size,args.window_size,action_name_dict)\n",
        "    dset_val = StepDataset(x_val,y_val,window_size=args.window_size,step_size=args.step_size)\n",
        "    return dset_train, dset_val, selected_acts\n",
        "\n",
        "def make_single_dset(args,subj_ids):\n",
        "    if args.dset == 'PAMAP':\n",
        "        return make_pamap_dset_train_val(args,subj_ids)\n",
        "    # if args.dset == 'UCI':\n",
        "    #     return make_uci_dset_train_val(args,subj_ids)\n",
        "    # if args.dset == 'UCI_feat':\n",
        "    #     return make_uci_feat_dset_train_val()\n",
        "    # if args.dset == 'WISDM-v1':\n",
        "    #     return make_wisdm_v1_dset_train_val(args,subj_ids)\n",
        "    # if args.dset == 'WISDM-watch':\n",
        "    #     return make_wisdm_watch_dset_train_val(args,subj_ids)\n",
        "    # if args.dset == 'REALDISP':\n",
        "    #     return make_realdisp_dset_train_val(args,subj_ids)\n",
        "    # if args.dset == 'HHAR':\n",
        "    #     return make_hhar_dset_train_val(args,subj_ids)\n",
        "    # if args.dset == 'Capture24':\n",
        "    #     return make_capture_dset_train_val(args,subj_ids)\n",
        "\n",
        "def make_dsets_by_user(args,subj_ids):\n",
        "    dsets_by_id = {}\n",
        "    for subj_id in subj_ids:\n",
        "        dset_subj, selected_acts_subj = make_single_dset(args,[subj_id])\n",
        "        dsets_by_id[subj_id] = dset_subj,selected_acts_subj\n",
        "    return dsets_by_id\n",
        "\n",
        "def chunked_up(x,step_size,window_size):\n",
        "    num_windows = (len(x) - window_size)//step_size + 1\n",
        "    return torch.stack([x[i*step_size:i*step_size+window_size] for i in range(num_windows)])\n",
        "\n",
        "def combine_dsets(dsets):\n",
        "    xs = [d.x for d in dsets]\n",
        "    ys = [d.y for d in dsets]\n",
        "    return ConcattedDataset(xs,ys,dsets[0].window_size,dsets[0].step_size)\n",
        "\n",
        "def combine_dsets_old(dsets):\n",
        "    processed_dset_xs = []\n",
        "    for dset in dsets:\n",
        "        if isinstance(dset,StepDataset):\n",
        "            processed_dset_x = chunked_up(dset.x,dset.step_size,dset.window_size)\n",
        "        elif isinstance(dset,ChunkDataset):\n",
        "            processed_dset_x = dset.x\n",
        "        else:\n",
        "            print(f\"you're trying to combine dsets on a {type(dset)}, but it has to be a dataset\")\n",
        "        processed_dset_xs.append(processed_dset_x)\n",
        "    x = torch.cat(processed_dset_xs)\n",
        "    y = torch.cat([dset.y for dset in dsets])\n",
        "    assert len(x) == len(y)\n",
        "    combined = ChunkDataset(x,y)\n",
        "    return combined\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "nQERarq97cIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Lou1sM project_config.py\n",
        "# https://github.com/Lou1sM/HAR/blob/master/project_config.py\n",
        "import sys\n",
        "import torch\n",
        "\n",
        "class HAR_Dataset_Container():\n",
        "    def __init__(self,code_name,dataset_dir_name,possible_subj_ids,num_channels,num_classes,action_name_dict):\n",
        "        self.code_name = code_name\n",
        "        self.dataset_dir_name = dataset_dir_name\n",
        "        self.possible_subj_ids = possible_subj_ids\n",
        "        self.num_channels = num_channels\n",
        "        self.num_classes = num_classes\n",
        "        self.action_name_dict = action_name_dict\n",
        "\n",
        "\n",
        "# PAMAP\n",
        "pamap_ids = [str(x) for x in range(101,110)]\n",
        "pamap_action_name_dict = {1:'lying',2:'sitting',3:'standing',4:'walking',5:'running',6:'cycling',7:'Nordic walking',9:'watching TV',10:'computer work',11:'car driving',12:'ascending stairs',13:'descending stairs',16:'vacuum cleaning',17:'ironing',18:'folding laundry',19:'house cleaning',20:'playing soccer',24:'rope jumping'}\n",
        "PAMAP_INFO = HAR_Dataset_Container(\n",
        "            code_name = 'PAMAP',\n",
        "            dataset_dir_name = 'PAMAP2_Dataset',\n",
        "            possible_subj_ids = pamap_ids,\n",
        "            num_channels = 39,\n",
        "            num_classes = 12,\n",
        "            action_name_dict = pamap_action_name_dict)\n",
        "\n",
        "# # UCI\n",
        "# def two_digitify(x): return '0' + str(x) if len(str(x))==1 else str(x)\n",
        "# uci_ids = [two_digitify(x) for x in range(1,30)]\n",
        "# uci_action_name_dict = {1:'walking',2:'walking upstairs',3:'walking downstairs',4:'sitting',5:'standing',6:'lying',7:'stand_to_sit',9:'sit_to_stand',10:'sit_to_lit',11:'lie_to_sit',12:'stand_to_lie',13:'lie_to_stand'}\n",
        "# UCI_INFO = HAR_Dataset_Container(\n",
        "#             code_name = 'UCI',\n",
        "#             dataset_dir_name = 'UCI2',\n",
        "#             possible_subj_ids = uci_ids,\n",
        "#             num_channels = 6,\n",
        "#             num_classes = 6,\n",
        "#             action_name_dict = uci_action_name_dict)\n",
        "\n",
        "# # UCI_feat\n",
        "# def two_digitify(x): return '0' + str(x) if len(str(x))==1 else str(x)\n",
        "# uci_feat_action_name_dict = {1:'walking',2:'walking upstairs',3:'walking downstairs',4:'sitting',5:'standing',6:'lying',7:'stand_to_sit',9:'sit_to_stand',10:'sit_to_lit',11:'lie_to_sit',12:'stand_to_lie',13:'lie_to_stand'}\n",
        "# UCI_FEAT_INFO = HAR_Dataset_Container(\n",
        "#             code_name = 'UCI_feat',\n",
        "#             dataset_dir_name = 'UCI2_feat',\n",
        "#             possible_subj_ids = ['0'],\n",
        "#             num_channels = 561,\n",
        "#             num_classes = 6,\n",
        "#             action_name_dict = uci_action_name_dict)\n",
        "\n",
        "# # WISDM-v1\n",
        "# wisdmv1_ids = [str(x) for x in range(1,37)] #Paper says 29 users but ids go up to 36\n",
        "# activities_list = ['Jogging','Walking','Upstairs','Downstairs','Standing','Sitting']\n",
        "# wisdmv1_action_name_dict = dict(zip(range(len(activities_list)),activities_list))\n",
        "# WISDMv1_INFO = HAR_Dataset_Container(\n",
        "#             code_name = 'WISDM-v1',\n",
        "#             dataset_dir_name = 'wisdm_v1',\n",
        "#             possible_subj_ids = wisdmv1_ids,\n",
        "#             num_channels = 3,\n",
        "#             num_classes = 6,\n",
        "#             action_name_dict = wisdmv1_action_name_dict)\n",
        "\n",
        "# # WISDM-watch\n",
        "# wisdmwatch_ids = [str(x) for x in range(1600,1651)]\n",
        "# with open('datasets/wisdm-dataset/activity_key.txt') as f: r=f.readlines()\n",
        "# activities_list = [x.split(' = ')[0] for x in r if ' = ' in x]\n",
        "# wisdmwatch_action_name_dict = dict(zip(range(len(activities_list)),activities_list))\n",
        "# WISDMwatch_INFO = HAR_Dataset_Container(\n",
        "#             code_name = 'WISDM-watch',\n",
        "#             dataset_dir_name = 'wisdm-dataset',\n",
        "#             possible_subj_ids = wisdmwatch_ids,\n",
        "#             num_channels = 12,\n",
        "#             num_classes = 17,\n",
        "#             action_name_dict = wisdmwatch_action_name_dict)\n",
        "\n",
        "# # REALDISP\n",
        "# realdisp_ids = [str(x) for x in range(1,18)]\n",
        "# activities_list = ['Walking','Jogging','Running','Jump up','Jump front & back','Jump sideways','Jump leg/arms open/closed','Jump rope','Trunk twist','Trunk twist','Waist bends forward','Waist rotation','Waist bends','Reach heels backwards','Lateral bend','Lateral bend with arm up','Repetitive forward stretching','Upper trunk and lower body opposite twist','Lateral elevation of arms','Frontal elevation of arms','Frontal hand claps','Frontal crossing of arms','Shoulders high-amplitude rotation','Shoulders low-amplitude rotation','Arms inner rotation','Knees','Heels','Knees bending','Knees','Rotation on the knees','Rowing','Elliptical bike','Cycling']\n",
        "# realdisp_action_name_dict = {i+1:act for i,act in enumerate(activities_list)}\n",
        "# REALDISP_INFO = HAR_Dataset_Container(\n",
        "#             code_name = 'REALDISP',\n",
        "#             dataset_dir_name = 'realdisp',\n",
        "#             possible_subj_ids = realdisp_ids,\n",
        "#             num_channels = 117,\n",
        "#             num_classes = 33,\n",
        "#             action_name_dict = realdisp_action_name_dict)\n",
        "\n",
        "# # HHAR\n",
        "# hhar_ids = [str(x) for x in range(0,10)]\n",
        "# activities_list = ['bike', 'sit', 'stand', 'walk', 'stairsup', 'stairsdown']\n",
        "# hhar_action_name_dict = {i:act for i,act in enumerate(activities_list)}\n",
        "# HHAR_INFO = HAR_Dataset_Container(\n",
        "#             code_name = 'HHAR',\n",
        "#             dataset_dir_name = 'hhar',\n",
        "#             possible_subj_ids = hhar_ids,\n",
        "#             num_channels = 12,\n",
        "#             num_classes = 6,\n",
        "#             action_name_dict = hhar_action_name_dict)\n",
        "\n",
        "# DSET_OBJECTS = [PAMAP_INFO, UCI_INFO, UCI_FEAT_INFO, WISDMv1_INFO, WISDMwatch_INFO,REALDISP_INFO,HHAR_INFO]\n",
        "DSET_OBJECTS = [PAMAP_INFO]\n",
        "\n",
        "\n",
        "def get_dataset_info_object(dset_name):\n",
        "    dsets_by_that_name = [d for d in DSET_OBJECTS if d.code_name == dset_name]\n",
        "    if len(dsets_by_that_name)==0: print(f\"{dset_name} is not a recognized dataset\"); sys.exit()\n",
        "    assert len(dsets_by_that_name)==1\n",
        "    return dsets_by_that_name[0]\n",
        "\n",
        "def get_num_time_points():\n",
        "    for dset_info_object in DSET_OBJECTS:\n",
        "        print(dset_info_object.code_name, sum([torch.load(f'datasets/{dset_info_object.dataset_dir_name}/precomputed/{s}step100_window512/x.pt').shape[0] for s in dset_info_object.possible_subj_ids]))\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "_PZ6amU5BhIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Lou1sM har_cnn.py\n",
        "# https://github.com/Lou1sM/HAR/blob/master/har_cnn.py\n",
        "import argparse\n",
        "import sys\n",
        "# import project_config\n",
        "\n",
        "\n",
        "def get_cl_args():\n",
        "    dset_options = ['PAMAP','UCI','WISDM-v1','WISDM-watch','REALDISP','Capture24']\n",
        "    # dset_options = [di.code_name for di in project_config.DSET_OBJECTS]\n",
        "    training_type_options = ['full','cluster_as_single','cluster_individually','train_frac_gts_as_single','find_similar_users']\n",
        "    parser = argparse.ArgumentParser()\n",
        "    subjs_group = parser.add_mutually_exclusive_group(required=False)\n",
        "    subjs_group.add_argument('--num_subjs',type=int)\n",
        "    subjs_group.add_argument('--subj_ids',type=str,nargs='+',default=['first'])\n",
        "    epochs_group = parser.add_mutually_exclusive_group(required=False)\n",
        "    epochs_group.add_argument('--full_epochs',action='store_true')\n",
        "    epochs_group.add_argument('--short_epochs',action='store_true')\n",
        "    parser.add_argument('--ablate_label_filter',action='store_true')\n",
        "    parser.add_argument('--all_subjs',action='store_true')\n",
        "    parser.add_argument('--bad_ids',action='store_true')\n",
        "    parser.add_argument('--batch_size_train',type=int,default=256)\n",
        "    parser.add_argument('--batch_size_val',type=int,default=1024)\n",
        "    parser.add_argument('--clusterer',type=str,choices=['HMM','GMM'],default='HMM')\n",
        "    parser.add_argument('--compute_cross_metrics',action='store_true')\n",
        "    parser.add_argument('--dec_lr',type=float,default=1e-3)\n",
        "    parser.add_argument('-d','--dset',type=str,default='UCI',choices=dset_options)\n",
        "    parser.add_argument('--enc_lr',type=float,default=1e-3)\n",
        "    parser.add_argument('--exp_name',type=str,default=\"try\")\n",
        "    parser.add_argument('--frac_gt_labels',type=float,default=0.1)\n",
        "    parser.add_argument('--gpu',type=str,default='0')\n",
        "    parser.add_argument('--is_n2d',action='store_true')\n",
        "    parser.add_argument('--is_uln',action='store_true',help='net for dim red. instead of umap')\n",
        "    parser.add_argument('--just_align_time',action='store_true')\n",
        "    parser.add_argument('--load_pretrained',action='store_true')\n",
        "    parser.add_argument('--mlp_lr',type=float,default=1e-3)\n",
        "    parser.add_argument('--no_umap',action='store_true')\n",
        "    parser.add_argument('--noise',type=float,default=1.)\n",
        "    parser.add_argument('--num_classes',type=int,default=-1)\n",
        "    parser.add_argument('--num_meta_epochs',type=int,default=1)\n",
        "    parser.add_argument('--num_meta_meta_epochs',type=int,default=1)\n",
        "    parser.add_argument('--num_pseudo_label_epochs',type=int,default=5)\n",
        "    parser.add_argument('--umap_dim',type=int,default=2)\n",
        "    parser.add_argument('--umap_neighbours',type=int,default=60)\n",
        "    parser.add_argument('--prob_thresh',type=float,default=.95)\n",
        "    parser.add_argument('--reinit',action='store_true')\n",
        "    parser.add_argument('--reload_ids',type=int,default=0)\n",
        "    parser.add_argument('--rlmbda',type=float,default=.1)\n",
        "    parser.add_argument('--show_transitions',action='store_true')\n",
        "    parser.add_argument('--step_size',type=int,default=5)\n",
        "    parser.add_argument('--subject_independent',action='store_true')\n",
        "    parser.add_argument('--test','-t',action='store_true')\n",
        "    parser.add_argument('--train_type',type=str,choices=training_type_options,default='full')\n",
        "    parser.add_argument('--show_shapes',action='store_true',help='print the shapes of hidden layers in enc and dec')\n",
        "    parser.add_argument('--verbose',action='store_true')\n",
        "    parser.add_argument('--window_size',type=int,default=512)\n",
        "    ARGS = parser.parse_args()\n",
        "\n",
        "    need_umap = False\n",
        "    if ARGS.is_uln:\n",
        "        ARGS.no_umap = True\n",
        "    if ARGS.short_epochs:\n",
        "        ARGS.num_meta_meta_epochs = 1\n",
        "        ARGS.num_meta_epochs = 1\n",
        "        ARGS.num_pseudo_label_epochs = 1\n",
        "    elif ARGS.full_epochs:\n",
        "        ARGS.num_meta_meta_epochs = 10\n",
        "        ARGS.num_meta_epochs = 10\n",
        "        ARGS.num_pseudo_label_epochs = 5\n",
        "    if ARGS.test:\n",
        "        ARGS.num_meta_epochs = 1\n",
        "        ARGS.num_meta_meta_epochs = 1\n",
        "        ARGS.num_pseudo_label_epochs = 1\n",
        "    elif not ARGS.no_umap and not ARGS.show_shapes: need_umap = True\n",
        "    print(ARGS)\n",
        "    dset_info_object = project_config.get_dataset_info_object(ARGS.dset)\n",
        "    all_possible_ids = dset_info_object.possible_subj_ids\n",
        "    if ARGS.all_subjs: ARGS.subj_ids=all_possible_ids\n",
        "    elif ARGS.num_subjs is not None: ARGS.subj_ids = all_possible_ids[:ARGS.num_subjs]\n",
        "    elif ARGS.subj_ids == ['first']: ARGS.subj_ids = all_possible_ids[:1]\n",
        "    bad_ids = [x for x in ARGS.subj_ids if x not in all_possible_ids]\n",
        "    if len(bad_ids) > 0 and not (ARGS.test and ARGS.dset=='HHAR'):\n",
        "        print(f\"You have specified non-existent ids: {bad_ids}\\nExistent ids are {all_possible_ids}\"); sys.exit()\n",
        "    return ARGS, need_umap\n",
        "\n",
        "RELEVANT_ARGS = ['ablate_label_filter','clusterer','dset','no_umap','num_meta_epochs','num_meta_meta_epochs','num_pseudo_label_epochs','reinit','step_size','subject_independent']\n",
        "\n",
        "\n",
        "# ARGS, need_umap = cl_args.get_cl_args()\n",
        "ARGS, need_umap = get_cl_args()\n",
        "# if need_umap: import umap\n",
        "# main(ARGS)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "cellView": "form",
        "id": "3UvqfQXfFSu1",
        "outputId": "84e2ee66-1f8e-47bf-88a6-ff9e8a6b9a8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] [--num_subjs NUM_SUBJS | --subj_ids SUBJ_IDS [SUBJ_IDS ...]]\n",
            "                                [--full_epochs | --short_epochs] [--ablate_label_filter]\n",
            "                                [--all_subjs] [--bad_ids] [--batch_size_train BATCH_SIZE_TRAIN]\n",
            "                                [--batch_size_val BATCH_SIZE_VAL] [--clusterer {HMM,GMM}]\n",
            "                                [--compute_cross_metrics] [--dec_lr DEC_LR]\n",
            "                                [-d {PAMAP,UCI,WISDM-v1,WISDM-watch,REALDISP,Capture24}]\n",
            "                                [--enc_lr ENC_LR] [--exp_name EXP_NAME]\n",
            "                                [--frac_gt_labels FRAC_GT_LABELS] [--gpu GPU] [--is_n2d]\n",
            "                                [--is_uln] [--just_align_time] [--load_pretrained]\n",
            "                                [--mlp_lr MLP_LR] [--no_umap] [--noise NOISE]\n",
            "                                [--num_classes NUM_CLASSES] [--num_meta_epochs NUM_META_EPOCHS]\n",
            "                                [--num_meta_meta_epochs NUM_META_META_EPOCHS]\n",
            "                                [--num_pseudo_label_epochs NUM_PSEUDO_LABEL_EPOCHS]\n",
            "                                [--umap_dim UMAP_DIM] [--umap_neighbours UMAP_NEIGHBOURS]\n",
            "                                [--prob_thresh PROB_THRESH] [--reinit] [--reload_ids RELOAD_IDS]\n",
            "                                [--rlmbda RLMBDA] [--show_transitions] [--step_size STEP_SIZE]\n",
            "                                [--subject_independent] [--test]\n",
            "                                [--train_type {full,cluster_as_single,cluster_individually,train_frac_gts_as_single,find_similar_users}]\n",
            "                                [--show_shapes] [--verbose] [--window_size WINDOW_SIZE]\n",
            "colab_kernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-50ea2cb4-a0b3-4dd4-be5b-f719dc2cbc66.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "2",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        }
      ]
    }
  ]
}