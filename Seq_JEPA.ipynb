{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/Seq_JEPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XwpnHW4wn9S1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5a5e4aa-1939-412f-ac09-2cd00bc005b1",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 48.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# @title data\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "transform = transforms.Compose([transforms.ToTensor(),])\n",
        "\n",
        "# train_data = torchvision.datasets.MNIST(root=\"data\", train=True, download=True,transform=transform) # do not normalise! want img in [0,1)\n",
        "# test_data = torchvision.datasets.MNIST(root=\"data\", train=False, download=True, transform=transform) #opt no download\n",
        "# # batch_size = 32 # 64 512\n",
        "# batch_size = 32 if torch.cuda.is_available() else 32\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "train_data = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
        "test_data = torchvision.datasets.CIFAR10(root='data', train=False, download=True, transform=transform)\n",
        "batch_size = 128 # 128 1024\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "\n",
        "# dataiter = iter(train_data)\n",
        "# x,y = next(dataiter)\n",
        "# print(x.shape) # [3, 32, 32]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title (Learned)RotEmb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, seq_len=512, base=10000):\n",
        "        super().__init__()\n",
        "        self.dim, self.base = dim, base\n",
        "        theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "        pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "        angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "        self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.base)\n",
        "        return x * self.rot_emb[:seq_len]\n",
        "\n",
        "# class LearnedRoPE(nn.Module): # learnt RoPE ; each tok is 1 pos\n",
        "#     def __init__(self, dim):\n",
        "#         super().__init__()\n",
        "#         self.weights = nn.Parameter(torch.randn(1, dim//2))\n",
        "\n",
        "#     def forward(self, x): #\n",
        "#         batch, seq_len, dim = x.shape\n",
        "#         # if rot_emb.shape[0] < seq_len: self.__init__(dim, seq_len)\n",
        "#         pos = torch.arange(seq_len).unsqueeze(1)\n",
        "#         angles = (self.weights * pos * 2*torch.pi).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "#         rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "#         return x * rot_emb.flatten(-2).unsqueeze(0)\n",
        "\n",
        "\n",
        "class LearnedRoPE(nn.Module): # learnt RoPE ; each tok is 1 pos\n",
        "    def __init__(self, dim, seq_len=512):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.weights = nn.Parameter(torch.randn(1, dim//2))\n",
        "        pos = torch.arange(seq_len).unsqueeze(1)\n",
        "        angles = (self.weights * pos * 2*torch.pi)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "        self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "    def forward(self, x): # [batch, seq_len, dim]\n",
        "        seq_len = x.size(1)\n",
        "        if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len)\n",
        "        return x * self.rot_emb[:seq_len]\n",
        "\n",
        "\n",
        "class RotEmb(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, top=torch.pi, base=10000):\n",
        "        super().__init__()\n",
        "        self.theta = top / (base ** (torch.arange(0, dim, step=2, device=device) / dim))\n",
        "        # self.theta = top / (base ** torch.linspace(0, 1, dim//2, device=device))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (pos.unsqueeze(-1) * self.theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [seq_len, dim]\n",
        "\n",
        "class LearnedRotEmb(nn.Module): # pos in R\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.weights = nn.Parameter(torch.randn((1, dim//2), device=device))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (self.weights * pos.unsqueeze(-1) * 2*torch.pi).unsqueeze(-1) # [batch, 1] * [1, dim//2] -> [batch, dim//2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [batch, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [batch, dim]\n",
        "\n"
      ],
      "metadata": {
        "id": "npc_xGtOz7DC",
        "cellView": "form"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ViT me simple\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    \"\"\"Zero out the parameters of a module and return it.\"\"\"\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads=None, d_head=8, cond_dim=None, dropout=0.): # .1\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_head = d_head\n",
        "        self.n_heads = d_model // d_head\n",
        "        # self.d_head = d_model // n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or d_model, 2*d_model, bias=False)\n",
        "        # self.k = nn.Sequential(nn.Dropout(dropout), nn.Linear(cond_dim, d_model, bias=False))\n",
        "        # self.lin = nn.Linear(d_model, d_model)\n",
        "        self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        # self.lin = nn.Sequential(nn.Dropout(dropout), zero_module(nn.Linear(d_model, d_model)))\n",
        "        self.drop = nn.Dropout(dropout) # indp before q,k,v; after linout\n",
        "        self.scale = self.d_head ** -.5\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [batch, T, d_model]=[batch, h*w, c], [batch, num_tok, cond_dim], [batch,T]\n",
        "        batch = x.shape[0]\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        Q = self.q(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        # K = self.k(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        K, V = self.kv(cond).view(batch, -1, self.n_heads, 2*self.d_head).transpose(1, 2).chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # linear attention # Softmax(Q) @ (Softmax(K).T @ V)\n",
        "        if mask != None:\n",
        "            mask = mask[:, None, :, None] # [batch,T] -> [batch,1,T,1]\n",
        "            K, V = K.masked_fill(mask, -torch.finfo(x.dtype).max), V.masked_fill(mask, -torch.finfo(x.dtype).max)\n",
        "        Q, K = Q.softmax(dim=-1)*self.scale, K.softmax(dim=-2)\n",
        "        context = K.transpose(-2,-1) @ V # [batch, n_heads, d_head, d_head]\n",
        "        out = Q @ context # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # (quadratic) attention # Softmax(Q @ K.T) @ V\n",
        "        # attn = Q @ K.transpose(-2,-1) * self.scale # [batch, n_heads, T] # [batch, n_heads, T, T/num_tok]\n",
        "        # if mask != None: attn = attn.masked_fill(mask[:, None, :, None], -torch.finfo(attn.dtype).max) # [batch,T]->[batch,1,T,1]\n",
        "        # attention = torch.softmax(attn, dim=-1)\n",
        "        # out = self.drop(attention) @ V # [batch, n_heads, T, d_head]\n",
        "\n",
        "        out = out.transpose(1, 2).flatten(2)\n",
        "        return self.drop(self.lin(out)) # [batch, T, d_model]\n",
        "\n",
        "# if self, dont pass cond_dim in init, dont pass cond in fwd\n",
        "# Softmax(Q @ K.T) @ V ~ Softmax(Q) @ Softmax(K).T @ V\n",
        "\n",
        "# https://github.com/lucidrains/x-transformers/blob/main/x_transformers/x_transformers.py#L1855\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_head=4, cond_dim=None, ff_mult=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm1 = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.norm2 = nn.RMSNorm(d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.cond_dim = cond_dim\n",
        "        self.self = MultiHeadAttention(d_model, d_head=d_head, dropout=0)\n",
        "        if self.cond_dim!=None: self.cross = MultiHeadAttention(d_model, d_head=d_head, cond_dim=cond_dim, dropout=0)\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        ff_dim=d_model*ff_mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), nn.Dropout(dropout), nn.Linear(d_model, ff_dim), act, # ReLU GELU\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "            nn.RMSNorm(d_model), nn.Dropout(dropout), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            # nn.RMSNorm(d_model), act, nn.Dropout(dropout), nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,c,h,w], [batch, num_tok, cond_dim], [batch,T]\n",
        "        # bchw = x.shape\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "\n",
        "        if self.cond_dim==None: cond=None # is self attn\n",
        "        x = x + self.drop(self.self(self.norm1(x)))\n",
        "        if self.cond_dim!=None: x = x + self.cross(self.norm2(x), cond, mask)\n",
        "        x = x + self.ff(x)\n",
        "        # x = x + self.drop(self.ff(x))\n",
        "\n",
        "        # return x.transpose(1,2).reshape(*bchw)\n",
        "        return x\n",
        "\n",
        "\n",
        "# class SimpleViT(nn.Module):\n",
        "#     def __init__(self, in_dim, out_dim, dim, depth, heads, mlp_dim, channels=3, dim_head=8):\n",
        "#         super().__init__()\n",
        "#         self.to_patch_embedding = nn.Sequential( # in, out, kernel, stride, pad\n",
        "#             nn.Conv2d(in_dim, dim, kernel_size=7, stride=2, padding=7//2, bias=False), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "#             # nn.Conv2d(in_dim, dim, kernel_size=7, stride=2, padding=7//2, bias=False), nn.MaxPool2d(2,2)\n",
        "#             )\n",
        "#         # self.pos_embedding = LearnedRoPE2D(dim) # LearnedRoPE2D, RoPE2D\n",
        "#         self.positional_emb = nn.Parameter(torch.zeros(1, 8*8, dim)) # positional_embedding == 'learnable'\n",
        "#         self.transformer = AttentionBlock(d_model=dim, d_head=dim_head)\n",
        "#         self.transformer_blocks = nn.ModuleList([AttentionBlock(d_model=dim, d_head=dim_head) for i in range(depth)])\n",
        "\n",
        "#         self.attention_pool = nn.Linear(dim, 1)\n",
        "#         self.out = nn.Linear(dim, out_dim, bias=False)\n",
        "\n",
        "#     def forward(self, img):\n",
        "#         device = img.device\n",
        "#         x = self.to_patch_embedding(img)\n",
        "#         # x = self.pos_embedding(x)\n",
        "#         x = x.flatten(-2).transpose(-2,-1) # b c h w -> b (h w) c\n",
        "#         x = x + self.positional_emb\n",
        "#         x = self.transformer(x)\n",
        "#         # for blk in self.predictor_blocks: x = blk(x)\n",
        "\n",
        "#         # x = x.flatten(-2).mean(dim=-1) # meal pool\n",
        "#         attn_weights = self.attention_pool(x).squeeze(-1) # [batch, (h,w)] # seq_pool\n",
        "#         x = (attn_weights.softmax(dim = 1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, (h,w)] @ [batch, (h,w), dim]\n",
        "#         # cls_token = repeat(self.class_emb, '1 1 d -> b 1 d', b = b)\n",
        "#         # x = torch.cat((cls_token, x), dim=1)\n",
        "#         # x = x[:, 0] # first token\n",
        "#         return self.out(x)\n",
        "\n",
        "# ijepa: 224*224 -> 14*14 = 196\n",
        "# 32*7\n",
        "# me: 32*32=1024 -> 256\n",
        "\n",
        "import torch\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "def apply_masks(x, masks):\n",
        "    \"\"\":param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
        "    :param masks: list of tensors containing indices of patches in [N] to keep\"\"\"\n",
        "    all_x = []\n",
        "    for m in masks: # [1,T]\n",
        "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [1,T,dim]\n",
        "        all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "    return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "# batch, seq_len, d_model = 2,7,4\n",
        "# x = torch.rand(batch, seq_len, d_model)\n",
        "# print(x)\n",
        "# masks = [torch.randint(0,seq_len,(2,3,))]\n",
        "# print(masks)\n",
        "# out = apply_masks(x, masks)\n",
        "# print(out)\n",
        "# # print(out.shape)\n",
        "\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        # self.positional_emb = nn.Parameter(torch.zeros(1, 32*32//4, d_model)) # positional_embedding == 'learnable'\n",
        "        # encoder_layers = nn.TransformerEncoderLayer(d_model, d_model//d_head, d_hid or d_model, dropout, batch_first=True)\n",
        "        # self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.zeros(1,1,d_model)) # randn zeros\n",
        "        nn.init.trunc_normal_(self.cls, std=.02)\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model)\n",
        "        self.lin = nn.Linear(d_model, out_dim)# if out_dim != d_model else None\n",
        "\n",
        "    def forward(self, src, context_indices, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        src = self.embed(src) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = src.shape\n",
        "        src = src * self.pos_encoder(context_indices)\n",
        "        # src = src + self.positional_emb[:,context_indices]\n",
        "        # src = apply_masks(src, [context_indices])\n",
        "\n",
        "        pred_tokens = self.cls * self.pos_encoder(trg_indices) # [M, num_trg_toks, d_model]\n",
        "        # pred_tokens = self.cls + self.positional_emb[0,trg_indices]\n",
        "        pred_tokens = pred_tokens.repeat(batch, 1, 1) # [batch*M, num_trg_toks, d_model]\n",
        "        # print(pred_tokens.requires_grad)\n",
        "        # print(\"pred fwd\", src.shape, pred_tokens.shape)\n",
        "        # src = src.repeat_interleave(trg_indices.shape[0], dim=0) # [batch, seq_len, d_model] -> [batch*M, seq_len, d_model]\n",
        "        src = torch.cat([src, pred_tokens], dim=1) # [batch*M, seq_len+num_trg_toks, d_model]\n",
        "\n",
        "        out = self.transformer_encoder(src) # float [seq_len, batch_size, d_model]\n",
        "        out = self.norm(out)\n",
        "        out = out[:,seq:] # [batch*M, num_trg_toks, d_model]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.MaxPool1d(2,2), #nn.MaxPool1d(3, 2, 3//2),\n",
        "            nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.Conv1d(d_model, d_model,3,2,3//2)\n",
        "            # nn.Conv1d(in_dim, d_model,2,2,0), # like patch\n",
        "            )\n",
        "        self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        # self.positional_emb = nn.Parameter(torch.zeros(1, 32*32//4, d_model)) # positional_embedding == 'learnable'\n",
        "        # encoder_layers = nn.TransformerEncoderLayer(d_model, d_model//d_head, d_hid or d_model, dropout, batch_first=True)\n",
        "        # self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model)\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, src, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # src = self.embed(src) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        src = self.embed(src.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "\n",
        "        batch, seq, dim = src.shape\n",
        "\n",
        "        # # # src = self.pos_encoder(src)\n",
        "        # if context_indices != None:\n",
        "        # print(src.shape, self.pos_encoder(context_indices).shape)\n",
        "        #     src = src * self.pos_encoder(context_indices) # context/predictor # src = src + self.positional_emb[:,context_indices]\n",
        "        # else: src = src * self.pos_encoder(torch.arange(seq, device=device).unsqueeze(0)) # target # src = src + self.positional_emb[:,:seq]\n",
        "\n",
        "\n",
        "        # src = self.pos_encoder(src)\n",
        "        src = src * self.pos_encoder(torch.arange(seq, device=device)).unsqueeze(0)\n",
        "        # src = src + self.positional_emb\n",
        "        if context_indices != None:\n",
        "            src = apply_masks(src, [context_indices])\n",
        "\n",
        "\n",
        "        out = self.transformer_encoder(src) # float [seq_len, batch_size, d_model]\n",
        "        out = self.norm(out)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "batch, seq_len, d_model = 4,1024,4\n",
        "in_dim = 3\n",
        "model = TransformerModel(in_dim, d_model, d_head=4, nlayers=2, dropout=0.).to(device)\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "# # print(out)\n",
        "\n"
      ],
      "metadata": {
        "id": "GHzr3NVs2EcG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37be224a-68b1-490c-a997-cd9319b46685",
        "cellView": "form"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 256, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title SeqJEPA mae like\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def multiblock(seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "    return target_mask\n",
        "\n",
        "# https://arxiv.org/pdf/2210.07224\n",
        "def randpatch(seq, mask_size=8, gamma=0.9): # num patches of seq, mask patch size, masking ratio\n",
        "    # mask = torch.rand(seq//mask_size)<gamma\n",
        "    length = seq//mask_size\n",
        "    g = torch.normal(gamma, std=.1, size=(1,)).clamp(.5,.9)\n",
        "    # g = gamma\n",
        "    idx = torch.randperm(length)[:int(length*g)]\n",
        "    mask = torch.zeros(length, dtype=bool)\n",
        "    mask[idx] = True\n",
        "    mask = mask.repeat_interleave(mask_size, dim=-1)\n",
        "    return mask # [seq] , True -> mask\n",
        "\n",
        "class SeqJEPA(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, nlayers=2, d_head=4):\n",
        "        super().__init__()\n",
        "        if out_dim is None: out_dim = d_model\n",
        "        self.patch_size = 4\n",
        "        self.context_encoder = TransformerModel(in_dim, d_model, out_dim=out_dim, d_head=d_head, nlayers=nlayers, dropout=0.)\n",
        "        # self.predicter = TransformerPredictor(out_dim, d_model//2, out_dim, d_head=d_head, nlayers=1, dropout=0.)\n",
        "        self.predicter = TransformerPredictor(out_dim, d_model//2, out_dim, d_head=d_head, nlayers=nlayers//2, dropout=0.)\n",
        "        import copy\n",
        "        self.target_encoder = copy.deepcopy(self.context_encoder)\n",
        "        self.target_encoder.requires_grad_(False)\n",
        "        self.classifier = nn.Linear(out_dim, 10)\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]\n",
        "        batch, seq, dim = x.shape\n",
        "        # target_mask = multiblock(seq, min_s=0.15, max_s=0.2, M=4).any(0) # mask out targets to be predicted # [M, seq]\n",
        "        target_mask = randpatch(seq//self.patch_size, mask_size=8, gamma=.9) # [seq]\n",
        "        context_mask = ~multiblock(seq//self.patch_size, min_s=0.85, max_s=1., M=1)|target_mask # [1, seq], True->Mask\n",
        "        target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "\n",
        "        context_indices = (~context_mask[0]).nonzero().squeeze(-1) # int idx [num_context_toks] , idx of context not masked\n",
        "        sx = self.context_encoder(x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "\n",
        "        trg_indices = target_mask.nonzero().squeeze(-1) # int idx [1, num_trg_toks] , idx of targets that are masked\n",
        "        sy_ = self.predicter(sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "\n",
        "        # with torch.no_grad(): sy = self.target_encoder(x.detach())[torch.arange(batch).unsqueeze(-1), trg_indices.repeat(batch,1)] # [batch, num_trg_toks, out_dim]\n",
        "        with torch.no_grad():\n",
        "            sy = self.target_encoder(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "            # print(sy.shape, trg_indices.shape)\n",
        "            sy = apply_masks(sy, [trg_indices])\n",
        "\n",
        "        # print(x[0][0][:8])\n",
        "        # print(sy_[0][0][:8])\n",
        "        # print(sy.shape, sy_.shape)\n",
        "        # loss = F.smooth_l1_loss(sy, sy_)\n",
        "        loss = F.mse_loss(sy.detach(), sy_)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        sx = self.target_encoder(x)\n",
        "        out = sx.mean(dim=1)\n",
        "        return out\n",
        "\n",
        "\n",
        "    def classify(self, x): # [batch, T, 3]\n",
        "        sx = self.target_encoder(x).mean(dim=1)\n",
        "        out = self.classifier(sx)\n",
        "        return out\n",
        "\n",
        "# min_s=0.15, max_s, M\n",
        "# trg.15.2M4 C.85 1\n",
        "\n",
        "seq_jepa = SeqJEPA(in_dim=3, d_model=32, out_dim=16, nlayers=6, d_head=4).to(device)#.to(torch.float)\n",
        "# optim = torch.optim.AdamW(seq_jepa.parameters(), lr=1e-3) # 1e-3?\n",
        "optim = torch.optim.AdamW(seq_jepa.parameters(), lr=1e-3, weight_decay=0) # lr1e-3? wd0\n",
        "print(sum(p.numel() for p in seq_jepa.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.parameters())) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.predicter.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.context_encoder.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.target_encoder.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# d_model^2 * nlayers\n",
        "\n",
        "x = torch.rand((2,1024,3), device=device)\n",
        "out = seq_jepa.loss(x)\n",
        "print(out.shape)\n",
        "# print(x.is_leaf) # T\n"
      ],
      "metadata": {
        "id": "xy4iC46Vnlog",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14e1dcbd-2bc6-49e3-e530-cc54ac760084"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47946\n",
            "torch.Size([])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "length = 10\n",
        "gamma=0.9\n",
        "mask = torch.rand(length)<gamma\n",
        "\n",
        "g = torch.normal(gamma, std=.1, size=(1,)).clamp(.5,.9)\n",
        "# gamma\n",
        "print(torch.randperm(length))\n",
        "idx = torch.randperm(length)[:int(length*g)]\n",
        "print(g, idx)\n",
        "# mask = (torch.arange(length)==idx)\n",
        "mask = torch.zeros(length, dtype=bool)\n",
        "mask[idx] = True\n",
        "print(mask)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eysGC1WyS-Y6",
        "outputId": "bd1d2b04-80ad-4655-d166-2677261c1527"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2, 9, 6, 0, 5, 3, 4, 1, 7, 8])\n",
            "tensor([0.8571]) tensor([3, 5, 8, 6, 7, 9, 4, 0])\n",
            "tensor([ True, False, False,  True,  True,  True,  True,  True,  True,  True])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(16).to(device)\n",
        "# coptim = torch.optim.AdamW(classifier.parameters(), lr=1e-3)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-4)\n",
        "# optim = torch.optim.AdamW([seq_jepa.parameters(), classifier.parameters()], lr=1e-3) # 1e-3?\n",
        "# optim = torch.optim.AdamW([{'params': seq_jepa.parameters()}, {'params': classifier.parameters(), 'lr': 1e-3}], lr=1e-3)\n"
      ],
      "metadata": {
        "id": "wjK1TVwBh2u8"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(seq_jepa.classifier.weight[0])\n",
        "for n, p in seq_jepa.target_encoder.named_parameters():\n",
        "    print(n,p)\n",
        "# optim.param_groups[0]['lr'] = 1e-5"
      ],
      "metadata": {
        "id": "7IACKDymhit7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acfbb1ab-7910-42a3-a6ee-ac69c102fcd6"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embed.0.weight Parameter containing:\n",
            "tensor([[[ 0.4912,  0.0635,  0.0191],\n",
            "         [ 0.0264,  0.0880,  0.1790],\n",
            "         [-0.1070,  0.2856, -0.1303]],\n",
            "\n",
            "        [[-0.0371, -0.0756,  0.1042],\n",
            "         [-0.2323, -0.2423, -0.2600],\n",
            "         [-0.2795, -0.1325, -0.1551]],\n",
            "\n",
            "        [[ 0.0826,  0.0965, -0.0946],\n",
            "         [ 0.0921,  0.0175, -0.3506],\n",
            "         [-0.1154,  0.0167, -0.2066]],\n",
            "\n",
            "        [[-0.1005,  0.0990,  0.1129],\n",
            "         [ 0.3105,  0.2845,  0.2981],\n",
            "         [-0.0184,  0.0749,  0.2273]],\n",
            "\n",
            "        [[ 0.2633,  0.0643,  0.2523],\n",
            "         [ 0.1403,  0.0230,  0.2192],\n",
            "         [-0.1176, -0.0161,  0.0871]],\n",
            "\n",
            "        [[-0.0126, -0.0269,  0.1508],\n",
            "         [-0.1220, -0.1244, -0.2445],\n",
            "         [-0.1591, -0.1193, -0.0567]],\n",
            "\n",
            "        [[ 0.1119,  0.0123, -0.1114],\n",
            "         [ 0.0037, -0.2412, -0.2582],\n",
            "         [-0.3325, -0.2013,  0.0447]],\n",
            "\n",
            "        [[ 0.2078, -0.2885,  0.0662],\n",
            "         [ 0.1380, -0.4215, -0.0928],\n",
            "         [-0.1574, -0.0785, -0.0607]],\n",
            "\n",
            "        [[-0.1063, -0.4842,  0.1258],\n",
            "         [ 0.0942,  0.1627, -0.0101],\n",
            "         [ 0.2271,  0.2276, -0.1830]],\n",
            "\n",
            "        [[ 0.3526,  0.2703, -0.0860],\n",
            "         [-0.2133, -0.1640, -0.2191],\n",
            "         [ 0.0655,  0.3406,  0.1262]],\n",
            "\n",
            "        [[ 0.0979,  0.2947,  0.1264],\n",
            "         [ 0.4014,  0.2329, -0.1296],\n",
            "         [-0.0699,  0.0883, -0.1095]],\n",
            "\n",
            "        [[ 0.0507,  0.0268,  0.2113],\n",
            "         [ 0.0624, -0.3815, -0.4046],\n",
            "         [-0.3622, -0.3624, -0.0155]],\n",
            "\n",
            "        [[ 0.2513,  0.2173, -0.1884],\n",
            "         [-0.0879, -0.2692,  0.1047],\n",
            "         [-0.1106, -0.1550, -0.2224]],\n",
            "\n",
            "        [[-0.0071,  0.0475, -0.3842],\n",
            "         [-0.0344,  0.2268,  0.1562],\n",
            "         [ 0.1891,  0.0776,  0.0519]],\n",
            "\n",
            "        [[ 0.0998,  0.1104,  0.2065],\n",
            "         [-0.2450, -0.1388, -0.2403],\n",
            "         [-0.3379,  0.0370,  0.2284]],\n",
            "\n",
            "        [[ 0.1183, -0.1537, -0.2483],\n",
            "         [ 0.1495,  0.2747, -0.0570],\n",
            "         [ 0.2293,  0.1708, -0.0313]],\n",
            "\n",
            "        [[-0.2185, -0.0634, -0.1829],\n",
            "         [ 0.2408, -0.3079, -0.0820],\n",
            "         [-0.1469,  0.0103, -0.3504]],\n",
            "\n",
            "        [[ 0.3151, -0.0367,  0.2367],\n",
            "         [ 0.1778, -0.2148, -0.2833],\n",
            "         [-0.0288, -0.2092,  0.1158]],\n",
            "\n",
            "        [[-0.0061, -0.0834,  0.2916],\n",
            "         [-0.2739, -0.2338, -0.1542],\n",
            "         [-0.0350,  0.1711, -0.3073]],\n",
            "\n",
            "        [[ 0.2534,  0.0732, -0.0270],\n",
            "         [-0.1606, -0.2873, -0.1810],\n",
            "         [ 0.0623, -0.4515, -0.0155]],\n",
            "\n",
            "        [[-0.0695, -0.0853, -0.3314],\n",
            "         [-0.0789, -0.0481,  0.3493],\n",
            "         [ 0.1390,  0.2991, -0.1386]],\n",
            "\n",
            "        [[ 0.0541,  0.1819,  0.4104],\n",
            "         [ 0.1896, -0.1027,  0.4220],\n",
            "         [-0.2342, -0.0483,  0.1732]],\n",
            "\n",
            "        [[-0.0793,  0.0904, -0.2462],\n",
            "         [ 0.3118,  0.0329, -0.0727],\n",
            "         [ 0.0531,  0.3158,  0.0728]],\n",
            "\n",
            "        [[ 0.0954, -0.3189, -0.3534],\n",
            "         [ 0.2594,  0.2869, -0.0142],\n",
            "         [ 0.2121, -0.0776,  0.1243]],\n",
            "\n",
            "        [[ 0.0418, -0.0702, -0.2483],\n",
            "         [ 0.2691,  0.2217, -0.2662],\n",
            "         [ 0.3354,  0.4152,  0.1860]],\n",
            "\n",
            "        [[ 0.2953,  0.1106, -0.2469],\n",
            "         [ 0.0275,  0.2784,  0.2408],\n",
            "         [-0.0877, -0.0856,  0.0340]],\n",
            "\n",
            "        [[-0.1311,  0.1061, -0.1407],\n",
            "         [-0.2830,  0.0119, -0.1087],\n",
            "         [-0.3972, -0.1863,  0.0942]],\n",
            "\n",
            "        [[-0.0358,  0.2222,  0.1149],\n",
            "         [ 0.0219, -0.4537, -0.0899],\n",
            "         [-0.2613, -0.3611,  0.0502]],\n",
            "\n",
            "        [[ 0.0235,  0.0486, -0.2763],\n",
            "         [ 0.0359,  0.1108, -0.2862],\n",
            "         [-0.1848, -0.2125, -0.2427]],\n",
            "\n",
            "        [[-0.0862, -0.2211,  0.2669],\n",
            "         [-0.2792, -0.2332, -0.0304],\n",
            "         [ 0.0948, -0.2533, -0.2966]],\n",
            "\n",
            "        [[ 0.0615, -0.0287,  0.1015],\n",
            "         [ 0.3265,  0.1312,  0.0849],\n",
            "         [ 0.0228,  0.0991, -0.1261]],\n",
            "\n",
            "        [[ 0.1671, -0.0167,  0.1543],\n",
            "         [-0.1903, -0.2712, -0.3394],\n",
            "         [-0.1070,  0.0057, -0.2718]]], device='cuda:0')\n",
            "embed.0.bias Parameter containing:\n",
            "tensor([-0.0505,  0.2411,  0.1481,  0.1364,  0.1496, -0.0509, -0.2404,  0.2018,\n",
            "         0.1960, -0.0128,  0.1206,  0.1108, -0.2933, -0.2222,  0.3205, -0.1687,\n",
            "        -0.1788, -0.1655,  0.2968, -0.2303,  0.0737, -0.2052, -0.1740, -0.1935,\n",
            "        -0.2765, -0.1938,  0.1945,  0.2517,  0.1401,  0.2197,  0.2274,  0.1354],\n",
            "       device='cuda:0')\n",
            "embed.1.weight Parameter containing:\n",
            "tensor([1.0553, 1.0943, 1.0394, 1.1181, 0.9817, 1.1264, 1.1399, 1.0598, 1.0680,\n",
            "        0.9889, 1.0547, 1.0645, 0.9393, 0.9518, 1.0632, 0.9453, 1.0058, 0.9364,\n",
            "        1.0972, 1.0237, 1.0865, 0.8574, 1.1356, 1.0080, 1.1050, 1.0600, 1.0488,\n",
            "        0.9875, 1.0284, 1.0909, 1.0711, 1.0656], device='cuda:0')\n",
            "embed.1.bias Parameter containing:\n",
            "tensor([-0.0074,  0.0393, -0.0292,  0.0481, -0.0276,  0.0405,  0.0161, -0.0192,\n",
            "         0.0785, -0.0474, -0.0025,  0.0625, -0.0496, -0.1351,  0.0099, -0.1117,\n",
            "        -0.0118,  0.0113,  0.0561,  0.0113,  0.0419, -0.1654,  0.0474, -0.0030,\n",
            "        -0.0009, -0.0047, -0.0064, -0.0160, -0.0363,  0.0151, -0.0052,  0.0416],\n",
            "       device='cuda:0')\n",
            "embed.3.weight Parameter containing:\n",
            "tensor([[[ 0.0427,  0.0313,  0.2234],\n",
            "         [ 0.0587, -0.1002, -0.0956],\n",
            "         [-0.1572, -0.1524, -0.1928],\n",
            "         ...,\n",
            "         [ 0.0831, -0.0588, -0.0542],\n",
            "         [-0.1611, -0.0197,  0.0558],\n",
            "         [ 0.0089, -0.1877, -0.1816]],\n",
            "\n",
            "        [[-0.2090, -0.0345,  0.0334],\n",
            "         [-0.1343, -0.0712, -0.0572],\n",
            "         [-0.2440, -0.0425, -0.1600],\n",
            "         ...,\n",
            "         [-0.0833, -0.1723, -0.1177],\n",
            "         [-0.0405, -0.1213, -0.0352],\n",
            "         [-0.1854, -0.0532, -0.1799]],\n",
            "\n",
            "        [[-0.1477,  0.0295,  0.1739],\n",
            "         [-0.0425,  0.0835, -0.0290],\n",
            "         [ 0.0535,  0.0290, -0.0380],\n",
            "         ...,\n",
            "         [ 0.0270,  0.1091,  0.0481],\n",
            "         [-0.0460, -0.0431,  0.0880],\n",
            "         [-0.0556,  0.0549,  0.0726]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.1691,  0.0913,  0.0773],\n",
            "         [-0.1605, -0.1050, -0.0229],\n",
            "         [ 0.0143, -0.0730, -0.1793],\n",
            "         ...,\n",
            "         [-0.2118, -0.1747, -0.1356],\n",
            "         [ 0.2209,  0.0292,  0.1449],\n",
            "         [-0.0653, -0.1401, -0.1732]],\n",
            "\n",
            "        [[-0.0248, -0.0623,  0.0103],\n",
            "         [ 0.0985,  0.1102,  0.0711],\n",
            "         [-0.0004, -0.0736, -0.0319],\n",
            "         ...,\n",
            "         [ 0.1281,  0.0483,  0.0544],\n",
            "         [-0.0270, -0.1697, -0.0937],\n",
            "         [ 0.0921,  0.1304,  0.1488]],\n",
            "\n",
            "        [[-0.0179, -0.1315, -0.1996],\n",
            "         [ 0.0565,  0.1452,  0.0903],\n",
            "         [ 0.0830,  0.1170,  0.2612],\n",
            "         ...,\n",
            "         [ 0.1338,  0.1109,  0.2188],\n",
            "         [-0.0326, -0.0933, -0.1526],\n",
            "         [ 0.1565,  0.1141,  0.0551]]], device='cuda:0')\n",
            "embed.3.bias Parameter containing:\n",
            "tensor([-0.1483, -0.2161, -0.0139,  0.1423,  0.0370, -0.0344, -0.0135, -0.0413,\n",
            "        -0.0804,  0.0094,  0.1505,  0.0210,  0.1754, -0.1195, -0.0857,  0.1012,\n",
            "        -0.1238,  0.0131, -0.1731, -0.0596, -0.1308, -0.1780, -0.1474,  0.0313,\n",
            "         0.1338,  0.0384,  0.0593,  0.0628, -0.0768,  0.1050,  0.0161, -0.0319],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.0.norm1.weight Parameter containing:\n",
            "tensor([1.0379, 0.8640, 0.8396, 0.9956, 0.9563, 0.9902, 0.9393, 1.0610, 0.8701,\n",
            "        0.8841, 0.7191, 0.9705, 1.0726, 0.9400, 1.0394, 1.0066, 0.9350, 1.0843,\n",
            "        0.8874, 0.9550, 0.9620, 0.9399, 0.9863, 1.0081, 0.9792, 0.9759, 0.9063,\n",
            "        1.0542, 0.9151, 1.0427, 0.9429, 0.8655], device='cuda:0')\n",
            "transformer_encoder.0.norm2.weight Parameter containing:\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.0.self.q.weight Parameter containing:\n",
            "tensor([[-0.0700,  0.1260,  0.0712,  ...,  0.0403,  0.0563, -0.0365],\n",
            "        [ 0.0123,  0.2217, -0.1688,  ...,  0.1586, -0.0345,  0.0502],\n",
            "        [-0.0702, -0.2775, -0.0413,  ..., -0.1520,  0.2408, -0.0656],\n",
            "        ...,\n",
            "        [ 0.1364,  0.0836, -0.1925,  ..., -0.0609, -0.0365, -0.0662],\n",
            "        [-0.0884, -0.0236, -0.0740,  ...,  0.1954, -0.0591,  0.1265],\n",
            "        [-0.2474,  0.0408, -0.0464,  ..., -0.0287, -0.2967, -0.1242]],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.0.self.kv.weight Parameter containing:\n",
            "tensor([[-0.1604, -0.1089,  0.0062,  ...,  0.1009,  0.0666, -0.0707],\n",
            "        [ 0.0397,  0.1128,  0.0073,  ...,  0.0533,  0.1703, -0.0727],\n",
            "        [ 0.0716, -0.0487,  0.2132,  ..., -0.1459,  0.0362,  0.0658],\n",
            "        ...,\n",
            "        [ 0.1275, -0.2440, -0.0387,  ..., -0.0100, -0.0577,  0.0221],\n",
            "        [ 0.0871,  0.1248,  0.1063,  ...,  0.1073,  0.0254,  0.1035],\n",
            "        [-0.0625,  0.0133, -0.0483,  ...,  0.1596,  0.0477, -0.1329]],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.0.self.lin.weight Parameter containing:\n",
            "tensor([[ 0.0158,  0.0005,  0.0826,  ...,  0.0746, -0.0505, -0.0834],\n",
            "        [ 0.0033,  0.0080, -0.1318,  ..., -0.0139,  0.0849,  0.0767],\n",
            "        [-0.1511,  0.0895, -0.0192,  ...,  0.0176, -0.0114,  0.0054],\n",
            "        ...,\n",
            "        [-0.1061,  0.0811,  0.0058,  ..., -0.0143, -0.0113, -0.0288],\n",
            "        [-0.0543, -0.0281, -0.0525,  ..., -0.0067, -0.0513, -0.0629],\n",
            "        [ 0.0808, -0.1029, -0.1509,  ..., -0.0598,  0.0337,  0.0018]],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.0.self.lin.bias Parameter containing:\n",
            "tensor([-0.0055, -0.0085, -0.0148,  0.0504, -0.0618, -0.0151, -0.0034, -0.0702,\n",
            "        -0.0191,  0.0565,  0.0554,  0.0284, -0.0740,  0.0587,  0.0135, -0.0410,\n",
            "        -0.0346, -0.0233, -0.0063,  0.0315,  0.0498, -0.0466,  0.0070, -0.0164,\n",
            "         0.0655, -0.0293,  0.0915,  0.0653,  0.0125, -0.0135,  0.0764,  0.0223],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.0.ff.0.weight Parameter containing:\n",
            "tensor([0.9624, 1.1655, 0.9334, 0.9449, 0.9487, 0.9745, 0.9465, 0.9020, 0.9465,\n",
            "        0.9719, 1.0325, 0.9375, 0.9362, 1.0403, 1.1907, 0.9332, 0.9554, 1.0490,\n",
            "        1.0618, 1.0067, 0.9894, 0.9663, 0.9875, 1.0922, 1.0011, 0.8349, 0.9216,\n",
            "        1.1881, 0.9495, 1.0885, 0.9223, 0.8675], device='cuda:0')\n",
            "transformer_encoder.0.ff.2.weight Parameter containing:\n",
            "tensor([[ 0.2055, -0.0169,  0.0660,  ..., -0.0791,  0.3024,  0.0827],\n",
            "        [-0.0056,  0.0677, -0.0044,  ...,  0.1411, -0.0088,  0.1214],\n",
            "        [ 0.2729, -0.1486, -0.0525,  ...,  0.1527, -0.0803, -0.1177],\n",
            "        ...,\n",
            "        [-0.0362, -0.0554, -0.0698,  ...,  0.2139, -0.1584, -0.2039],\n",
            "        [ 0.1006, -0.2271, -0.1296,  ...,  0.2737, -0.1414, -0.2543],\n",
            "        [ 0.0238, -0.0339,  0.1122,  ..., -0.0847,  0.1253, -0.1465]],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.0.ff.2.bias Parameter containing:\n",
            "tensor([ 0.2732, -0.0676, -0.1332,  0.0680,  0.1410, -0.0163, -0.1290, -0.0567,\n",
            "        -0.0693,  0.0029,  0.0602,  0.1400,  0.0084,  0.0133, -0.0258, -0.0921,\n",
            "         0.2775, -0.2315,  0.1486, -0.1580, -0.0195,  0.1325, -0.0997,  0.0780,\n",
            "        -0.0560, -0.0624, -0.0345,  0.2063,  0.1294,  0.1197, -0.0882,  0.0184],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.0.ff.4.weight Parameter containing:\n",
            "tensor([1.0202, 1.0593, 1.0193, 0.9643, 1.0498, 0.7990, 0.9790, 1.1308, 0.9220,\n",
            "        0.9427, 0.8377, 0.8311, 0.9760, 0.9613, 0.9714, 1.0398, 1.0703, 0.9518,\n",
            "        1.1420, 0.8018, 0.9992, 0.9869, 1.1696, 1.0585, 0.9495, 1.0143, 0.9281,\n",
            "        1.0246, 0.9049, 0.8503, 1.0291, 0.8262], device='cuda:0')\n",
            "transformer_encoder.0.ff.6.weight Parameter containing:\n",
            "tensor([[-0.1872,  0.0617,  0.0119,  ..., -0.1824,  0.0583,  0.0364],\n",
            "        [ 0.1068, -0.0690, -0.1558,  ...,  0.0692, -0.1153, -0.0026],\n",
            "        [ 0.1090, -0.0072, -0.0210,  ..., -0.0043,  0.0729, -0.0035],\n",
            "        ...,\n",
            "        [-0.0805,  0.0591,  0.0645,  ..., -0.0045,  0.0745,  0.0056],\n",
            "        [ 0.2513, -0.0312, -0.1075,  ...,  0.1147, -0.0997, -0.0242],\n",
            "        [ 0.0833, -0.0660, -0.1089,  ...,  0.0333, -0.0562, -0.0108]],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.0.ff.6.bias Parameter containing:\n",
            "tensor([ 0.0413,  0.1213,  0.0752, -0.0638,  0.0716, -0.0340, -0.0035, -0.0284,\n",
            "        -0.0204,  0.0667, -0.0547, -0.1016, -0.0780, -0.0364, -0.0020, -0.1530,\n",
            "         0.0432, -0.1072, -0.0104,  0.0548,  0.0629, -0.0141,  0.0274, -0.0516,\n",
            "        -0.0240,  0.0162,  0.0632,  0.0516, -0.0174,  0.0618, -0.0095, -0.0560],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.1.norm1.weight Parameter containing:\n",
            "tensor([0.9767, 0.8716, 0.7532, 0.8908, 1.0755, 1.0075, 1.0527, 1.1037, 1.0235,\n",
            "        0.8806, 0.9896, 0.9430, 0.8692, 0.8675, 0.8703, 0.9730, 0.8316, 0.9036,\n",
            "        0.8612, 0.9029, 0.8271, 0.9414, 1.0389, 0.9670, 0.9987, 0.9240, 0.9765,\n",
            "        0.9109, 1.0081, 0.9614, 1.0571, 0.9167], device='cuda:0')\n",
            "transformer_encoder.1.norm2.weight Parameter containing:\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.1.self.q.weight Parameter containing:\n",
            "tensor([[-0.2171, -0.0224,  0.0824,  ...,  0.0365, -0.0228,  0.1125],\n",
            "        [ 0.1527,  0.1340,  0.0273,  ...,  0.1444, -0.0818,  0.0596],\n",
            "        [ 0.0059, -0.2384,  0.0415,  ..., -0.1177,  0.0836, -0.0723],\n",
            "        ...,\n",
            "        [-0.0237, -0.1215, -0.1250,  ...,  0.1193, -0.0100,  0.0860],\n",
            "        [-0.0279,  0.1659,  0.1387,  ..., -0.1192,  0.1538, -0.0038],\n",
            "        [-0.2467,  0.1745,  0.1352,  ..., -0.0069,  0.2229, -0.1569]],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.1.self.kv.weight Parameter containing:\n",
            "tensor([[ 0.1838, -0.0920, -0.0911,  ..., -0.1415, -0.0891,  0.0469],\n",
            "        [-0.1188,  0.0094, -0.1202,  ..., -0.1659,  0.1293, -0.1124],\n",
            "        [ 0.1454, -0.0771, -0.0839,  ...,  0.1104,  0.0394,  0.1694],\n",
            "        ...,\n",
            "        [ 0.1584,  0.1017,  0.0063,  ..., -0.1070,  0.0820,  0.1457],\n",
            "        [ 0.0071, -0.1389, -0.1474,  ..., -0.1269, -0.2797, -0.0193],\n",
            "        [-0.2511, -0.0466,  0.1138,  ...,  0.1277,  0.0091, -0.1442]],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.1.self.lin.weight Parameter containing:\n",
            "tensor([[-0.0359,  0.0023, -0.0059,  ...,  0.0166, -0.0605,  0.0082],\n",
            "        [ 0.0397, -0.1095, -0.1031,  ..., -0.0346,  0.0194,  0.0349],\n",
            "        [ 0.0383, -0.0509, -0.0848,  ..., -0.0292,  0.0057,  0.0802],\n",
            "        ...,\n",
            "        [-0.0377, -0.0272,  0.0096,  ..., -0.0079, -0.0885,  0.0880],\n",
            "        [ 0.0126, -0.0517,  0.0138,  ...,  0.0344,  0.0215, -0.0248],\n",
            "        [ 0.0055,  0.0314,  0.0035,  ...,  0.0569,  0.0974, -0.0995]],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.1.self.lin.bias Parameter containing:\n",
            "tensor([ 0.0686,  0.1318,  0.0872, -0.0544,  0.0732,  0.0347, -0.0221, -0.0741,\n",
            "        -0.0169,  0.0546, -0.0191, -0.0656, -0.1034, -0.0516,  0.0169, -0.1814,\n",
            "        -0.0061, -0.1335,  0.0191,  0.0948,  0.0582, -0.0271,  0.0080,  0.0140,\n",
            "         0.0223, -0.0191,  0.0225,  0.0433,  0.0013,  0.0755, -0.0080, -0.0581],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.1.ff.0.weight Parameter containing:\n",
            "tensor([1.0322, 0.9733, 0.9210, 0.9211, 0.9942, 1.0268, 1.0105, 0.9756, 0.9585,\n",
            "        0.9764, 1.0789, 1.0099, 1.0334, 0.9041, 1.0748, 1.0362, 0.9311, 1.0465,\n",
            "        1.0263, 0.8736, 0.8883, 0.9667, 0.9060, 1.1432, 0.8891, 0.7987, 0.9564,\n",
            "        1.2244, 0.7054, 1.1423, 0.9265, 0.9626], device='cuda:0')\n",
            "transformer_encoder.1.ff.2.weight Parameter containing:\n",
            "tensor([[ 0.0844, -0.0097,  0.1540,  ...,  0.1470,  0.0143, -0.1009],\n",
            "        [-0.1169, -0.0193, -0.0307,  ..., -0.2078, -0.1494,  0.0126],\n",
            "        [ 0.0162, -0.1946,  0.1162,  ..., -0.0430, -0.1193,  0.0512],\n",
            "        ...,\n",
            "        [-0.1061, -0.1116,  0.0347,  ..., -0.1223, -0.2254,  0.1444],\n",
            "        [-0.1043, -0.1392, -0.1929,  ..., -0.1579, -0.1770,  0.1760],\n",
            "        [ 0.0802,  0.2044,  0.0656,  ...,  0.1458, -0.0915, -0.0854]],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.1.ff.2.bias Parameter containing:\n",
            "tensor([-2.0835e-01, -1.2927e-01, -4.5661e-02,  6.0139e-02, -6.8861e-02,\n",
            "        -4.3568e-02, -2.1432e-01, -1.4472e-02, -1.0311e-01,  1.1565e-01,\n",
            "        -7.5216e-02, -1.3694e-02, -1.5386e-01,  3.4919e-02, -6.0118e-02,\n",
            "         5.0334e-02,  1.3435e-01, -9.6691e-02,  3.7627e-02, -1.1842e-01,\n",
            "        -6.6959e-02, -9.0079e-02, -1.1247e-01,  7.1847e-02, -1.4448e-01,\n",
            "        -1.1706e-02,  4.0998e-02,  6.8568e-02, -1.1819e-01, -1.7352e-01,\n",
            "        -1.2202e-01, -1.5721e-05], device='cuda:0')\n",
            "transformer_encoder.1.ff.4.weight Parameter containing:\n",
            "tensor([0.7821, 1.0158, 0.8015, 0.9668, 0.7994, 0.7553, 0.7555, 0.7679, 1.0697,\n",
            "        0.9035, 1.1533, 1.1112, 0.8870, 0.9030, 1.0161, 0.8148, 1.0843, 1.0793,\n",
            "        0.8817, 0.9881, 1.0959, 0.8728, 0.8332, 0.9102, 0.9220, 0.7772, 0.8015,\n",
            "        1.0501, 0.9687, 0.9840, 1.0573, 1.0319], device='cuda:0')\n",
            "transformer_encoder.1.ff.6.weight Parameter containing:\n",
            "tensor([[-0.0423, -0.0165, -0.0560,  ...,  0.0034, -0.0661, -0.1391],\n",
            "        [ 0.0527, -0.0222,  0.0164,  ..., -0.0623, -0.0109,  0.0965],\n",
            "        [ 0.0676, -0.0857, -0.0233,  ..., -0.1106, -0.1706,  0.1973],\n",
            "        ...,\n",
            "        [ 0.0747, -0.0518, -0.0120,  ..., -0.0333, -0.0593,  0.0428],\n",
            "        [-0.0104,  0.1228,  0.0039,  ...,  0.0980,  0.1515,  0.1673],\n",
            "        [-0.0533,  0.0392, -0.0221,  ...,  0.0222,  0.0258, -0.0180]],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.1.ff.6.bias Parameter containing:\n",
            "tensor([ 0.1095,  0.0774,  0.1325,  0.0592,  0.0087,  0.0253,  0.0665,  0.0188,\n",
            "         0.0149, -0.0136,  0.0261, -0.0206, -0.0630, -0.0557,  0.0086, -0.1699,\n",
            "         0.0280, -0.0556,  0.0233,  0.0099, -0.0365, -0.0314,  0.0835, -0.0261,\n",
            "         0.0704, -0.0246, -0.0270,  0.0565, -0.0377,  0.0592,  0.0013, -0.0165],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.2.norm1.weight Parameter containing:\n",
            "tensor([0.6924, 0.9748, 0.7780, 0.8434, 1.0244, 1.1553, 0.9913, 1.0149, 0.9120,\n",
            "        1.0263, 0.9277, 0.6785, 0.9666, 0.9323, 0.9440, 0.7452, 0.8904, 1.0184,\n",
            "        0.7643, 0.9268, 0.9637, 0.9543, 0.8716, 0.9570, 0.7625, 1.0888, 0.6311,\n",
            "        0.8857, 0.9726, 0.9261, 0.9979, 0.9036], device='cuda:0')\n",
            "transformer_encoder.2.norm2.weight Parameter containing:\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.2.self.q.weight Parameter containing:\n",
            "tensor([[-0.0108, -0.2012, -0.0325,  ...,  0.0710,  0.0345, -0.0921],\n",
            "        [ 0.1080,  0.0449,  0.1798,  ...,  0.3069,  0.2886,  0.0749],\n",
            "        [ 0.0809, -0.1184,  0.1747,  ...,  0.0646, -0.1184,  0.0057],\n",
            "        ...,\n",
            "        [ 0.0453, -0.0331,  0.0232,  ..., -0.1376, -0.1337,  0.0754],\n",
            "        [ 0.2101, -0.1174,  0.2540,  ..., -0.1569,  0.1129,  0.0079],\n",
            "        [-0.0351, -0.0483, -0.0499,  ...,  0.1692, -0.0575, -0.0882]],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.2.self.kv.weight Parameter containing:\n",
            "tensor([[ 0.0527, -0.2196,  0.2124,  ...,  0.2292, -0.1754, -0.0157],\n",
            "        [ 0.0380, -0.1774,  0.2923,  ...,  0.0874, -0.1714, -0.1794],\n",
            "        [-0.1117, -0.2569,  0.1858,  ...,  0.2294,  0.1274, -0.0888],\n",
            "        ...,\n",
            "        [-0.0965,  0.0846,  0.1894,  ..., -0.0704, -0.1521,  0.0477],\n",
            "        [-0.0268,  0.2317, -0.0937,  ...,  0.0148,  0.1411,  0.1221],\n",
            "        [ 0.1459, -0.0392, -0.0860,  ...,  0.0202,  0.0193,  0.0086]],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.2.self.lin.weight Parameter containing:\n",
            "tensor([[ 0.1061,  0.1450,  0.1317,  ..., -0.0273, -0.0838,  0.0865],\n",
            "        [ 0.1862,  0.0130,  0.1861,  ..., -0.0758, -0.2039, -0.0444],\n",
            "        [ 0.1381,  0.0832,  0.1783,  ..., -0.0863, -0.0942,  0.0087],\n",
            "        ...,\n",
            "        [-0.0011, -0.0434, -0.0242,  ..., -0.0078, -0.0308, -0.0487],\n",
            "        [-0.0704, -0.0705, -0.0774,  ...,  0.0703,  0.1124, -0.0689],\n",
            "        [ 0.0572,  0.0621,  0.1024,  ..., -0.0789, -0.0824,  0.0225]],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.2.self.lin.bias Parameter containing:\n",
            "tensor([ 0.0936,  0.0779,  0.1084,  0.0655,  0.0240,  0.0364,  0.0598, -0.0014,\n",
            "         0.0454,  0.0191,  0.0593, -0.0206, -0.0612, -0.0162,  0.0165, -0.1898,\n",
            "         0.0247, -0.0757,  0.0211, -0.0055, -0.0428, -0.0341,  0.0896, -0.0140,\n",
            "         0.1167, -0.0327, -0.0561,  0.0368, -0.0890,  0.0470, -0.0254, -0.0216],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.2.ff.0.weight Parameter containing:\n",
            "tensor([0.8182, 1.0341, 0.8925, 0.9127, 0.9346, 1.1459, 1.0690, 1.0055, 0.8946,\n",
            "        0.9916, 0.8756, 0.8985, 1.1416, 1.0907, 1.0566, 1.1430, 1.0650, 1.0025,\n",
            "        0.9990, 1.0437, 0.9061, 0.9490, 1.0062, 1.0421, 0.8062, 0.9995, 0.9715,\n",
            "        1.1166, 0.9919, 0.9937, 0.8876, 0.9470], device='cuda:0')\n",
            "transformer_encoder.2.ff.2.weight Parameter containing:\n",
            "tensor([[-0.1467, -0.1022,  0.0424,  ..., -0.1692, -0.0199, -0.1818],\n",
            "        [-0.2990,  0.0748, -0.2306,  ...,  0.0078,  0.1904, -0.1174],\n",
            "        [-0.3182,  0.0450, -0.2036,  ...,  0.0254,  0.1407, -0.0680],\n",
            "        ...,\n",
            "        [-0.0849, -0.0922, -0.1162,  ...,  0.0563, -0.0875,  0.0536],\n",
            "        [ 0.0727,  0.0314, -0.0439,  ..., -0.0732, -0.0310,  0.0977],\n",
            "        [ 0.0239, -0.0919, -0.1927,  ..., -0.0258,  0.0497,  0.0492]],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.2.ff.2.bias Parameter containing:\n",
            "tensor([ 0.0038, -0.2159, -0.2067, -0.2489, -0.2014,  0.2160,  0.0900, -0.0804,\n",
            "        -0.0291,  0.2060, -0.3189, -0.2644,  0.0521, -0.0211,  0.0541,  0.2197,\n",
            "         0.0682, -0.1471,  0.1498, -0.1638,  0.0028, -0.0266,  0.1116, -0.1759,\n",
            "        -0.1224, -0.1470, -0.0958, -0.0517, -0.2186, -0.0367, -0.0038, -0.0258],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.2.ff.4.weight Parameter containing:\n",
            "tensor([0.8093, 0.8405, 0.6978, 0.9164, 0.8231, 0.9010, 0.6345, 0.8032, 0.7855,\n",
            "        0.6345, 0.6697, 0.8182, 0.8344, 0.8753, 1.0695, 0.8969, 0.8451, 0.8109,\n",
            "        0.8475, 0.8421, 0.9148, 0.6992, 1.0135, 0.7603, 0.8858, 0.8147, 0.8545,\n",
            "        0.7750, 0.8181, 0.6728, 0.7994, 0.7881], device='cuda:0')\n",
            "transformer_encoder.2.ff.6.weight Parameter containing:\n",
            "tensor([[ 0.0080,  0.0402, -0.0177,  ...,  0.0687,  0.0638,  0.0580],\n",
            "        [-0.0117,  0.0053,  0.0046,  ...,  0.0105,  0.0464, -0.0144],\n",
            "        [-0.0398,  0.0253,  0.0590,  ...,  0.0211,  0.1044,  0.0133],\n",
            "        ...,\n",
            "        [ 0.0066, -0.0017, -0.0090,  ...,  0.0088,  0.1265, -0.0137],\n",
            "        [ 0.0431, -0.0548, -0.0059,  ..., -0.0520, -0.0214,  0.0030],\n",
            "        [ 0.0106,  0.0368, -0.0114,  ...,  0.0660,  0.0158,  0.0238]],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.2.ff.6.bias Parameter containing:\n",
            "tensor([ 0.1869,  0.0806,  0.1610,  0.0893,  0.0590, -0.0495,  0.0938, -0.0676,\n",
            "        -0.0467, -0.0118,  0.0793, -0.0916,  0.0498,  0.0976, -0.0320, -0.1393,\n",
            "        -0.0109, -0.0155,  0.0842,  0.0480, -0.0510, -0.0550,  0.0770, -0.0097,\n",
            "         0.1796, -0.0361, -0.1166,  0.0652, -0.0064,  0.0767, -0.1127,  0.0238],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.3.norm1.weight Parameter containing:\n",
            "tensor([0.6980, 0.6335, 0.6702, 0.8149, 0.7442, 0.9502, 0.7089, 0.9366, 0.6693,\n",
            "        1.0298, 1.0193, 0.6550, 1.0540, 0.8786, 0.8860, 0.6600, 0.8077, 0.9819,\n",
            "        0.9219, 0.8904, 0.9532, 0.7670, 0.6903, 1.0156, 0.6890, 1.1290, 0.8557,\n",
            "        0.9859, 0.9374, 0.8887, 0.8831, 0.9063], device='cuda:0')\n",
            "transformer_encoder.3.norm2.weight Parameter containing:\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.3.self.q.weight Parameter containing:\n",
            "tensor([[-0.2454, -0.0607, -0.0157,  ..., -0.1267, -0.0121, -0.1266],\n",
            "        [ 0.0961, -0.1062, -0.1648,  ...,  0.0363, -0.1009, -0.0030],\n",
            "        [ 0.0027, -0.0600, -0.0849,  ...,  0.1495, -0.1073, -0.0661],\n",
            "        ...,\n",
            "        [ 0.0440,  0.0458,  0.1160,  ..., -0.1545,  0.0776,  0.2060],\n",
            "        [-0.0435,  0.0063, -0.1476,  ...,  0.1100,  0.0134,  0.0225],\n",
            "        [-0.1408,  0.0815,  0.0176,  ..., -0.1276, -0.0783, -0.2199]],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.3.self.kv.weight Parameter containing:\n",
            "tensor([[-0.2356,  0.1083,  0.0160,  ..., -0.2865,  0.0979,  0.0513],\n",
            "        [ 0.1409, -0.0598,  0.1917,  ...,  0.0264, -0.1792, -0.1220],\n",
            "        [-0.0964, -0.2616,  0.0121,  ..., -0.2825,  0.2211,  0.1781],\n",
            "        ...,\n",
            "        [ 0.0310, -0.0474,  0.0146,  ..., -0.0630,  0.1686, -0.0256],\n",
            "        [ 0.0161, -0.0524,  0.1316,  ...,  0.0203,  0.0265,  0.0625],\n",
            "        [-0.0236,  0.1736, -0.0668,  ...,  0.0272, -0.0590,  0.1482]],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.3.self.lin.weight Parameter containing:\n",
            "tensor([[-0.0274,  0.0263, -0.0065,  ..., -0.0342,  0.0149,  0.0102],\n",
            "        [ 0.0096, -0.0005, -0.0462,  ...,  0.0101,  0.0228, -0.0188],\n",
            "        [-0.0324,  0.0148,  0.0162,  ..., -0.0319, -0.0267,  0.0186],\n",
            "        ...,\n",
            "        [ 0.0155,  0.0008,  0.0464,  ..., -0.0355,  0.0321,  0.0841],\n",
            "        [ 0.0177,  0.0181, -0.0054,  ..., -0.0200,  0.0337,  0.0109],\n",
            "        [-0.0602,  0.0594,  0.0143,  ..., -0.0140, -0.0048, -0.0446]],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.3.self.lin.bias Parameter containing:\n",
            "tensor([ 0.2163,  0.1091,  0.1564,  0.0816,  0.0705, -0.0851,  0.0995, -0.0716,\n",
            "        -0.0582, -0.0032,  0.0726, -0.1131,  0.0324,  0.0897, -0.0536, -0.1495,\n",
            "        -0.0134,  0.0032,  0.1138,  0.0597, -0.0618, -0.0511,  0.0756,  0.0132,\n",
            "         0.1725, -0.0383, -0.1393,  0.0205, -0.0232,  0.0955, -0.1327,  0.0112],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.3.ff.0.weight Parameter containing:\n",
            "tensor([0.9849, 0.9984, 1.0025, 1.1003, 1.0984, 1.1214, 1.0804, 1.0151, 0.9953,\n",
            "        1.0012, 0.9385, 0.9120, 1.0551, 1.0525, 1.2192, 0.9290, 1.0939, 1.0409,\n",
            "        1.0282, 0.9494, 0.9645, 0.7866, 1.0297, 1.1333, 1.0709, 1.2533, 0.8848,\n",
            "        1.1153, 0.9995, 1.1035, 0.9492, 0.9372], device='cuda:0')\n",
            "transformer_encoder.3.ff.2.weight Parameter containing:\n",
            "tensor([[-0.0303, -0.0025,  0.0638,  ...,  0.0211, -0.0666,  0.0796],\n",
            "        [-0.1663,  0.0837,  0.0798,  ...,  0.2042, -0.0807,  0.1123],\n",
            "        [ 0.0189,  0.0147, -0.1663,  ...,  0.2590, -0.0913, -0.1070],\n",
            "        ...,\n",
            "        [ 0.1542,  0.1101, -0.1280,  ...,  0.2407,  0.0968,  0.0885],\n",
            "        [-0.1159, -0.0767,  0.2137,  ..., -0.2305,  0.0101,  0.0424],\n",
            "        [ 0.1602,  0.2675, -0.0850,  ..., -0.0895,  0.2128, -0.0233]],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.3.ff.2.bias Parameter containing:\n",
            "tensor([-0.1830, -0.0020, -0.1341, -0.0292,  0.0716,  0.0656, -0.0749, -0.1845,\n",
            "         0.0316, -0.2812,  0.1482,  0.2832, -0.0159, -0.0811, -0.1141,  0.0869,\n",
            "         0.0898,  0.0132, -0.1256,  0.0436, -0.1268,  0.1165, -0.0193,  0.0936,\n",
            "        -0.1399,  0.1618, -0.0283,  0.0671, -0.1952,  0.0248,  0.0877, -0.0004],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.3.ff.4.weight Parameter containing:\n",
            "tensor([0.7368, 0.4629, 0.9104, 0.8359, 0.6272, 0.8020, 0.8664, 0.6116, 0.7812,\n",
            "        0.8260, 0.7540, 0.9578, 0.7065, 0.7307, 0.8148, 0.6377, 0.7292, 0.8610,\n",
            "        0.5976, 0.8353, 0.8025, 0.6332, 0.6567, 0.7249, 0.6677, 0.7235, 0.9629,\n",
            "        0.8576, 0.8646, 0.6610, 0.8603, 0.8087], device='cuda:0')\n",
            "transformer_encoder.3.ff.6.weight Parameter containing:\n",
            "tensor([[-0.0186,  0.0032,  0.0924,  ...,  0.0362,  0.0487,  0.1241],\n",
            "        [-0.0049,  0.0027,  0.0726,  ...,  0.0039,  0.0596,  0.0238],\n",
            "        [-0.0310, -0.0470,  0.0794,  ...,  0.0764, -0.0237,  0.0947],\n",
            "        ...,\n",
            "        [ 0.0057,  0.0086,  0.1694,  ...,  0.0142, -0.0392,  0.0159],\n",
            "        [ 0.0074,  0.0227, -0.0525,  ..., -0.0034, -0.0590, -0.0891],\n",
            "        [ 0.0455, -0.0221, -0.0030,  ...,  0.0120,  0.1556,  0.0761]],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.3.ff.6.bias Parameter containing:\n",
            "tensor([ 0.1934,  0.1193,  0.1705,  0.0949,  0.0868, -0.0353,  0.1166, -0.0894,\n",
            "        -0.0653,  0.0234,  0.0877, -0.1288,  0.0945,  0.0739, -0.0572, -0.1736,\n",
            "        -0.0478,  0.0080,  0.1258,  0.0904, -0.0676, -0.0913,  0.1124, -0.0012,\n",
            "         0.1870,  0.0020, -0.1716,  0.0762,  0.0208,  0.0894, -0.1384,  0.0654],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.4.norm1.weight Parameter containing:\n",
            "tensor([0.7757, 0.5651, 0.5938, 0.9042, 0.8325, 0.8984, 0.6051, 0.7998, 0.8462,\n",
            "        0.9160, 0.8466, 0.7486, 1.0640, 0.7494, 0.8842, 0.6941, 0.9105, 0.8909,\n",
            "        0.8434, 0.8158, 0.9561, 0.8133, 0.6644, 0.9364, 0.7558, 1.1733, 0.7394,\n",
            "        0.9984, 0.8415, 0.8945, 0.5958, 0.8952], device='cuda:0')\n",
            "transformer_encoder.4.norm2.weight Parameter containing:\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.4.self.q.weight Parameter containing:\n",
            "tensor([[ 0.0067, -0.1266, -0.0681,  ..., -0.1583,  0.1293,  0.1128],\n",
            "        [ 0.0079, -0.0304,  0.0018,  ...,  0.0870,  0.0198, -0.0648],\n",
            "        [ 0.0702,  0.0190,  0.0447,  ...,  0.1964, -0.0156,  0.0294],\n",
            "        ...,\n",
            "        [ 0.0701, -0.1704, -0.0979,  ...,  0.0091, -0.0813, -0.0264],\n",
            "        [ 0.0965,  0.0911,  0.2286,  ...,  0.2716,  0.0176, -0.1683],\n",
            "        [ 0.0040,  0.1436, -0.0432,  ..., -0.0653, -0.1011,  0.2717]],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.4.self.kv.weight Parameter containing:\n",
            "tensor([[ 0.0535, -0.2015, -0.0681,  ...,  0.0880, -0.0900, -0.1009],\n",
            "        [-0.1489, -0.0046,  0.1328,  ..., -0.0765, -0.0178,  0.0610],\n",
            "        [-0.0143, -0.0358,  0.0409,  ..., -0.0934, -0.0365,  0.0906],\n",
            "        ...,\n",
            "        [-0.0576, -0.0754,  0.0486,  ..., -0.0459, -0.2237,  0.1156],\n",
            "        [ 0.0404,  0.1204, -0.0955,  ..., -0.1949, -0.2010,  0.1228],\n",
            "        [ 0.0712,  0.0293,  0.0699,  ...,  0.1945,  0.0929,  0.0713]],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.4.self.lin.weight Parameter containing:\n",
            "tensor([[ 0.0104,  0.0303, -0.0094,  ..., -0.0344, -0.0030,  0.0283],\n",
            "        [-0.0090,  0.0217,  0.0069,  ..., -0.0148,  0.0019, -0.0048],\n",
            "        [-0.0030, -0.0069, -0.0081,  ...,  0.0231,  0.0003, -0.0033],\n",
            "        ...,\n",
            "        [-0.0123, -0.0369, -0.0071,  ...,  0.0023, -0.0060, -0.0085],\n",
            "        [-0.0275,  0.0093,  0.0439,  ...,  0.0238,  0.0203, -0.0335],\n",
            "        [-0.0614, -0.0440,  0.0115,  ...,  0.0248,  0.0709, -0.0475]],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.4.self.lin.bias Parameter containing:\n",
            "tensor([ 0.1834,  0.1416,  0.1393,  0.0556,  0.1030, -0.0376,  0.1260, -0.0963,\n",
            "        -0.0346,  0.0012,  0.1156, -0.1286,  0.0605,  0.0763, -0.0600, -0.1906,\n",
            "        -0.0250, -0.0171,  0.1092,  0.1009, -0.0698, -0.0967,  0.1363,  0.0095,\n",
            "         0.1820, -0.0104, -0.1765,  0.0532,  0.0250,  0.1001, -0.1380,  0.0579],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.4.ff.0.weight Parameter containing:\n",
            "tensor([0.9766, 0.9916, 0.9404, 0.9983, 1.0669, 1.0880, 0.9139, 0.8882, 0.9164,\n",
            "        1.0905, 1.0692, 1.0140, 1.0426, 1.0804, 1.2777, 1.0400, 1.0110, 1.1118,\n",
            "        0.8519, 0.8572, 0.9591, 1.0079, 0.9603, 1.1973, 0.9603, 1.1956, 1.0038,\n",
            "        1.2259, 1.1847, 1.1135, 0.7983, 1.1618], device='cuda:0')\n",
            "transformer_encoder.4.ff.2.weight Parameter containing:\n",
            "tensor([[-0.1813, -0.2456, -0.1658,  ...,  0.1722, -0.0469, -0.0667],\n",
            "        [-0.0098,  0.1709,  0.0182,  ..., -0.1140, -0.0965, -0.0718],\n",
            "        [-0.0016, -0.3282,  0.0098,  ..., -0.0055, -0.1161,  0.0221],\n",
            "        ...,\n",
            "        [ 0.0524,  0.1402, -0.2351,  ..., -0.1105, -0.0776,  0.2128],\n",
            "        [ 0.0497, -0.0517,  0.0565,  ...,  0.0658,  0.1101,  0.0216],\n",
            "        [-0.1879,  0.1507, -0.1229,  ...,  0.0626,  0.1465, -0.1428]],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.4.ff.2.bias Parameter containing:\n",
            "tensor([-0.1729,  0.0426, -0.1187, -0.1543, -0.0104,  0.0025,  0.0306,  0.4353,\n",
            "        -0.1197, -0.0888, -0.0403, -0.1210, -0.2294, -0.0594, -0.0603, -0.0934,\n",
            "        -0.0428, -0.1265,  0.0673,  0.1265, -0.0255, -0.0993,  0.3016,  0.1648,\n",
            "         0.1944, -0.0288, -0.1905, -0.0950, -0.0121,  0.1748, -0.1788, -0.1939],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.4.ff.4.weight Parameter containing:\n",
            "tensor([0.9832, 0.4680, 0.7374, 0.7944, 0.9198, 0.9359, 0.9670, 1.0016, 0.7795,\n",
            "        0.5008, 0.8639, 0.6931, 0.8037, 0.4548, 0.4812, 0.4956, 0.8903, 0.6106,\n",
            "        0.8364, 0.8546, 0.6973, 0.6601, 0.9821, 0.8171, 0.6047, 0.7954, 0.8863,\n",
            "        0.4640, 0.8389, 0.7740, 0.5676, 0.6750], device='cuda:0')\n",
            "transformer_encoder.4.ff.6.weight Parameter containing:\n",
            "tensor([[ 0.0899, -0.0592, -0.0017,  ..., -0.0025, -0.0117,  0.0396],\n",
            "        [ 0.0676, -0.0110,  0.0389,  ...,  0.0120,  0.0357,  0.0173],\n",
            "        [ 0.1011, -0.0370, -0.0377,  ..., -0.0026,  0.0142, -0.0187],\n",
            "        ...,\n",
            "        [ 0.1575,  0.0079, -0.0249,  ..., -0.0063,  0.0150,  0.0426],\n",
            "        [-0.0614,  0.0457, -0.0067,  ..., -0.0134,  0.0167,  0.0497],\n",
            "        [ 0.0257, -0.0268,  0.0672,  ...,  0.0189, -0.0016,  0.0261]],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.4.ff.6.bias Parameter containing:\n",
            "tensor([ 0.2013,  0.1399,  0.1773,  0.1048,  0.1014, -0.0447,  0.1211, -0.1002,\n",
            "        -0.1036,  0.0183,  0.0930, -0.1485,  0.0789,  0.0416, -0.0491, -0.1814,\n",
            "        -0.0698,  0.0137,  0.1481,  0.1018, -0.0941, -0.1098,  0.1326,  0.0041,\n",
            "         0.1993,  0.0157, -0.1841,  0.0850,  0.0254,  0.0985, -0.1505,  0.0969],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.5.norm1.weight Parameter containing:\n",
            "tensor([0.7210, 0.6241, 0.6184, 0.8217, 0.8147, 0.9288, 0.5900, 0.7807, 0.8735,\n",
            "        0.9404, 0.9299, 0.6737, 0.8326, 0.7645, 0.8667, 0.5836, 0.8312, 0.8468,\n",
            "        0.7286, 0.7617, 0.8195, 0.6973, 0.7898, 0.8999, 0.6364, 1.1733, 0.6329,\n",
            "        0.7937, 0.8532, 0.7859, 0.7644, 0.8067], device='cuda:0')\n",
            "transformer_encoder.5.norm2.weight Parameter containing:\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.5.self.q.weight Parameter containing:\n",
            "tensor([[ 0.0590, -0.0427, -0.2639,  ..., -0.0310,  0.1646,  0.0732],\n",
            "        [-0.2574, -0.1637, -0.2047,  ...,  0.0486, -0.0113,  0.1350],\n",
            "        [ 0.1518,  0.0033,  0.0984,  ..., -0.0569, -0.1758,  0.0181],\n",
            "        ...,\n",
            "        [-0.0676, -0.1203,  0.0383,  ..., -0.0263,  0.1611,  0.1033],\n",
            "        [ 0.2031, -0.0931,  0.0288,  ...,  0.0162, -0.0460, -0.1628],\n",
            "        [ 0.1079,  0.0201,  0.0902,  ...,  0.1048,  0.0105, -0.1653]],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.5.self.kv.weight Parameter containing:\n",
            "tensor([[ 0.1027, -0.2863,  0.0643,  ...,  0.2870, -0.1557, -0.0989],\n",
            "        [-0.0766, -0.0211,  0.2845,  ...,  0.2689, -0.0967,  0.0392],\n",
            "        [-0.1088, -0.2571, -0.0688,  ...,  0.2322, -0.0982, -0.0154],\n",
            "        ...,\n",
            "        [ 0.0767, -0.0990,  0.0892,  ...,  0.0500, -0.0653, -0.0622],\n",
            "        [ 0.1437, -0.1538,  0.1195,  ..., -0.0794,  0.0381, -0.0973],\n",
            "        [ 0.1175,  0.1393, -0.0555,  ..., -0.0782,  0.0430,  0.2068]],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.5.self.lin.weight Parameter containing:\n",
            "tensor([[ 0.0089,  0.0094,  0.1128,  ..., -0.0309, -0.0186, -0.0307],\n",
            "        [-0.0336,  0.0062,  0.1295,  ...,  0.0456,  0.0146,  0.0542],\n",
            "        [-0.0148,  0.0315,  0.0188,  ..., -0.0229, -0.0063, -0.0312],\n",
            "        ...,\n",
            "        [ 0.0160, -0.0042,  0.0338,  ..., -0.0151,  0.0203, -0.0068],\n",
            "        [ 0.0258, -0.0402, -0.0325,  ...,  0.0409, -0.0156, -0.0154],\n",
            "        [-0.0322,  0.0222,  0.0890,  ..., -0.0002,  0.0603,  0.0003]],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.5.self.lin.bias Parameter containing:\n",
            "tensor([ 0.2055,  0.1264,  0.1785,  0.0967,  0.0883, -0.0462,  0.1345, -0.1157,\n",
            "        -0.0736,  0.0014,  0.0949, -0.1680,  0.0835,  0.0277, -0.0552, -0.1825,\n",
            "        -0.0819,  0.0231,  0.1674,  0.1155, -0.0819, -0.1207,  0.1508, -0.0158,\n",
            "         0.2094,  0.0266, -0.1901,  0.0587,  0.0309,  0.0931, -0.1497,  0.1163],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.5.ff.0.weight Parameter containing:\n",
            "tensor([0.9035, 0.9322, 0.8971, 0.9029, 1.0342, 1.1465, 0.9263, 1.2560, 0.9530,\n",
            "        1.1764, 1.0074, 1.1752, 0.9203, 1.1863, 1.2259, 0.9305, 1.1472, 1.1057,\n",
            "        0.9163, 0.9104, 0.9334, 0.8802, 1.0140, 1.1700, 0.8481, 1.4154, 1.0086,\n",
            "        1.1611, 1.1281, 1.1393, 0.7557, 1.0584], device='cuda:0')\n",
            "transformer_encoder.5.ff.2.weight Parameter containing:\n",
            "tensor([[-0.2005, -0.2976, -0.0930,  ..., -0.2090,  0.2373,  0.0264],\n",
            "        [-0.2541, -0.0968, -0.1086,  ...,  0.0985, -0.0483,  0.0996],\n",
            "        [ 0.1363,  0.0696, -0.0376,  ..., -0.0194,  0.0751,  0.2067],\n",
            "        ...,\n",
            "        [-0.1634,  0.0733, -0.1145,  ...,  0.0794,  0.0560,  0.0755],\n",
            "        [-0.1976, -0.0670, -0.1324,  ..., -0.0903,  0.0151, -0.0579],\n",
            "        [-0.0815, -0.1576, -0.1517,  ..., -0.0024, -0.1168, -0.0477]],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.5.ff.2.bias Parameter containing:\n",
            "tensor([-1.1473e-01,  1.8965e-02,  7.1243e-05, -2.4306e-01,  1.9336e-01,\n",
            "        -1.7760e-01,  1.9830e-01, -2.2960e-02,  8.3448e-02,  1.7559e-01,\n",
            "        -3.8521e-02, -1.3512e-01, -8.9807e-02,  1.1376e-01,  2.4682e-01,\n",
            "         2.6492e-01,  2.3978e-02,  1.6717e-01,  7.7896e-02,  1.6583e-01,\n",
            "         5.5729e-02,  1.1087e-01,  5.1082e-02,  1.4240e-01,  1.1986e-01,\n",
            "        -2.9634e-02,  2.3069e-02, -1.8135e-01,  1.7876e-01,  3.1209e-02,\n",
            "         4.8281e-02,  1.7602e-01], device='cuda:0')\n",
            "transformer_encoder.5.ff.4.weight Parameter containing:\n",
            "tensor([0.8203, 0.5240, 0.5980, 0.6618, 0.9247, 0.4960, 0.6587, 0.2751, 0.7507,\n",
            "        1.0023, 1.0638, 0.9014, 0.4057, 0.7978, 0.9275, 0.9324, 1.0063, 0.5351,\n",
            "        0.8045, 1.0106, 0.5629, 0.8924, 0.5433, 0.4506, 0.7392, 0.5591, 0.4950,\n",
            "        0.3473, 0.9953, 0.3967, 0.9163, 0.8211], device='cuda:0')\n",
            "transformer_encoder.5.ff.6.weight Parameter containing:\n",
            "tensor([[-0.0629, -0.0098, -0.0094,  ...,  0.0065,  0.0700,  0.0721],\n",
            "        [-0.0677, -0.0178, -0.0178,  ..., -0.0401,  0.0680,  0.0301],\n",
            "        [-0.0498, -0.0344, -0.0226,  ...,  0.0230,  0.0182,  0.0035],\n",
            "        ...,\n",
            "        [-0.0133, -0.0256,  0.0268,  ...,  0.0100,  0.0508,  0.1229],\n",
            "        [ 0.0304,  0.0677,  0.0034,  ...,  0.0173, -0.0385,  0.0260],\n",
            "        [-0.0858,  0.0191, -0.0021,  ...,  0.0046,  0.0501,  0.0115]],\n",
            "       device='cuda:0')\n",
            "transformer_encoder.5.ff.6.bias Parameter containing:\n",
            "tensor([ 0.2103,  0.1523,  0.1820,  0.1155,  0.1129, -0.0357,  0.1281, -0.1209,\n",
            "        -0.1356,  0.0137,  0.0982, -0.1563,  0.0935,  0.0554, -0.0534, -0.1890,\n",
            "        -0.0743,  0.0290,  0.1568,  0.1073, -0.1120, -0.1241,  0.1431,  0.0025,\n",
            "         0.2061,  0.0216, -0.1912,  0.0957,  0.0503,  0.1005, -0.1597,  0.1046],\n",
            "       device='cuda:0')\n",
            "norm.weight Parameter containing:\n",
            "tensor([1.1264, 0.9806, 1.1083, 0.9728, 1.0462, 0.8719, 0.9643, 0.9200, 0.9900,\n",
            "        0.8807, 0.9729, 1.0539, 0.9160, 0.6473, 1.0149, 1.0801, 1.1114, 1.0050,\n",
            "        1.0648, 1.0166, 0.8396, 1.0126, 0.9891, 1.1871, 1.1398, 0.7582, 1.0679,\n",
            "        1.1539, 0.6706, 1.1672, 0.9611, 1.1895], device='cuda:0')\n",
            "lin.weight Parameter containing:\n",
            "tensor([[-1.6213e-01, -2.5125e-01, -6.6360e-02, -7.4400e-02, -1.8682e-01,\n",
            "          1.5509e-01,  1.8380e-02,  1.0166e-01,  2.7586e-01, -1.6618e-01,\n",
            "          5.2827e-03,  1.9701e-01, -1.1926e-01,  3.4763e-02,  5.0001e-02,\n",
            "          4.6832e-02,  1.7816e-01, -9.9975e-02, -1.7787e-01,  6.1417e-02,\n",
            "          7.5020e-02, -1.6389e-01, -2.0594e-01, -1.9932e-01, -1.4069e-01,\n",
            "         -9.1995e-02, -3.0288e-02,  1.3658e-01,  3.7619e-03, -5.4146e-02,\n",
            "          2.1312e-02, -2.9762e-01],\n",
            "        [ 2.1198e-01,  2.9110e-01,  2.3134e-01,  1.1969e-02, -6.5690e-02,\n",
            "         -9.8261e-02,  2.0420e-01, -2.1721e-01, -1.7087e-01, -9.9401e-02,\n",
            "          3.2666e-02, -2.6553e-01,  2.1174e-01, -3.6718e-03, -1.8599e-01,\n",
            "         -1.7257e-01, -2.5449e-01,  3.0447e-01,  1.2275e-01,  1.1514e-01,\n",
            "          8.3001e-02,  1.3877e-02,  1.5865e-01,  2.0749e-01,  2.0162e-01,\n",
            "          9.0522e-02, -1.1860e-01,  6.7361e-02, -7.5433e-02, -1.0608e-01,\n",
            "         -9.5950e-02,  3.0283e-01],\n",
            "        [ 1.0371e-01,  2.1901e-01,  1.4720e-01,  1.7328e-01,  1.8694e-01,\n",
            "         -2.7652e-01,  1.4246e-01, -2.0976e-01, -3.2548e-02, -1.8968e-01,\n",
            "          2.5404e-01, -3.1870e-01,  1.2369e-01, -6.9801e-02, -2.8302e-01,\n",
            "         -2.6215e-01, -1.0939e-01,  4.4153e-02,  1.4369e-01,  2.5129e-01,\n",
            "         -1.8853e-01, -7.1491e-02,  2.6745e-01,  9.1056e-02,  2.2320e-01,\n",
            "          5.2097e-02, -2.9185e-01,  4.1923e-02,  9.4973e-02,  2.4027e-01,\n",
            "         -1.0069e-01,  3.3890e-02],\n",
            "        [-1.2793e-01, -3.2988e-02,  9.5161e-03,  1.6500e-01, -1.5570e-01,\n",
            "          2.4729e-02, -4.1014e-03, -1.1252e-02, -9.8220e-02, -2.9673e-02,\n",
            "          1.4634e-01,  5.8039e-02, -8.0279e-03, -1.5215e-01,  1.6573e-01,\n",
            "          9.4352e-03,  2.4920e-02, -2.8456e-02, -1.3808e-01, -9.1607e-02,\n",
            "          2.0394e-01, -1.6073e-01, -9.3510e-02, -3.4876e-01,  4.7699e-02,\n",
            "         -6.1844e-02,  5.7473e-02,  1.4072e-01,  3.1531e-03,  2.8329e-01,\n",
            "          1.2336e-01, -3.3972e-01],\n",
            "        [ 1.2353e-01, -2.1257e-02, -9.1783e-02,  1.5993e-01, -5.8800e-02,\n",
            "         -1.5355e-01, -1.0460e-01, -1.4449e-01, -1.1183e-01,  1.0619e-01,\n",
            "         -8.0107e-02, -2.4062e-01,  1.2801e-01,  1.9436e-01,  1.0635e-01,\n",
            "         -1.5259e-01, -1.1346e-01, -6.5632e-02, -1.8539e-01,  2.8093e-02,\n",
            "          7.7601e-02, -4.5770e-02, -2.8537e-03, -1.3062e-01,  7.9990e-02,\n",
            "         -8.0555e-02, -1.2695e-01,  7.9092e-02,  1.2247e-02,  2.5015e-01,\n",
            "          2.3064e-02,  1.3626e-02],\n",
            "        [ 9.2299e-02, -1.4189e-01, -1.2908e-01, -1.8014e-01,  6.2325e-02,\n",
            "          4.7123e-02, -1.5248e-01, -3.5852e-02, -8.8684e-02, -9.7882e-03,\n",
            "          2.3941e-02,  1.3983e-01,  1.0877e-01,  1.2655e-01, -1.3191e-01,\n",
            "         -5.9979e-03,  2.4858e-02,  1.5988e-01,  2.2368e-02, -1.6398e-01,\n",
            "         -1.2968e-01,  2.4761e-02, -4.3647e-02,  1.8411e-02,  3.3985e-02,\n",
            "          8.3321e-02, -9.4490e-02, -2.9243e-01, -6.6815e-02, -1.7257e-01,\n",
            "          1.8736e-01,  7.5035e-02],\n",
            "        [-2.5819e-01,  2.4441e-02, -3.8354e-01, -2.9814e-01, -1.2780e-01,\n",
            "         -3.7484e-02, -9.3360e-02, -4.4156e-02,  3.3414e-02,  2.0818e-04,\n",
            "         -1.6465e-01,  5.1408e-02, -2.2379e-01, -1.1382e-02, -1.4588e-01,\n",
            "          1.6114e-01, -6.0874e-02,  1.4251e-01, -1.2430e-01, -2.1619e-01,\n",
            "          1.2661e-01,  1.7274e-01, -1.2485e-01,  1.1510e-01, -1.4265e-01,\n",
            "          2.4573e-02, -2.0649e-02, -2.6761e-01,  2.5178e-02, -3.0708e-01,\n",
            "          2.5507e-01, -1.2983e-01],\n",
            "        [-1.5566e-02, -4.8359e-02,  1.4926e-01, -1.0163e-01,  5.9501e-02,\n",
            "          1.2600e-02, -1.7372e-02, -4.4570e-02, -9.0643e-02, -1.0096e-01,\n",
            "          8.7747e-02,  8.7876e-02,  1.2741e-01,  4.9031e-02,  1.4527e-01,\n",
            "         -1.6953e-01, -7.7216e-02,  1.7162e-01, -5.1705e-04,  1.0400e-01,\n",
            "         -2.1499e-02, -5.2562e-02,  2.4801e-02,  5.0009e-03, -1.4010e-02,\n",
            "          9.1422e-02, -2.0578e-01, -1.9394e-01, -2.0058e-02, -1.9746e-01,\n",
            "         -8.2998e-02,  1.6844e-01],\n",
            "        [ 6.7008e-02, -1.2011e-01, -2.0273e-01,  4.8223e-02,  1.2752e-01,\n",
            "         -4.9817e-02, -1.4882e-01,  1.6016e-01,  3.6615e-02, -7.8062e-02,\n",
            "         -2.0867e-01, -2.6457e-02, -2.7453e-03,  5.9207e-03,  4.5387e-02,\n",
            "          2.1967e-01, -1.5748e-01,  1.8888e-01, -4.9387e-02, -2.5641e-01,\n",
            "          1.0380e-01,  7.5213e-02, -1.8658e-01,  2.4676e-01, -1.0625e-01,\n",
            "         -1.3006e-01,  1.7594e-01, -1.1009e-01,  5.6445e-02, -1.7401e-01,\n",
            "          2.1942e-01, -9.5845e-02],\n",
            "        [-3.0306e-01, -2.7210e-01, -1.3767e-01, -1.5962e-01, -3.3755e-01,\n",
            "          7.6207e-02, -3.0901e-01,  9.2179e-02,  2.0878e-01,  8.6921e-03,\n",
            "          4.2982e-03,  5.5725e-02, -2.0282e-01,  5.7592e-02,  1.4064e-01,\n",
            "          4.2579e-01,  1.4997e-01,  2.4679e-03, -2.5138e-01, -1.3905e-01,\n",
            "          1.1863e-01,  1.4314e-01, -1.5560e-01, -1.8744e-01, -2.3521e-01,\n",
            "         -1.8103e-01,  4.1064e-01, -6.3983e-02, -6.0411e-02, -4.5497e-02,\n",
            "          1.1860e-01, -1.8637e-01],\n",
            "        [ 4.4694e-01,  1.8259e-01,  5.1757e-01,  1.2747e-01,  2.1916e-01,\n",
            "         -1.4901e-02,  8.6302e-02, -2.0402e-01, -2.3862e-01,  7.3566e-02,\n",
            "          2.4245e-01, -3.3610e-01, -1.0533e-01, -1.8834e-02,  1.2233e-02,\n",
            "         -2.1304e-01, -1.9389e-01, -3.2408e-02,  3.3785e-01,  3.1833e-01,\n",
            "         -2.2755e-01, -3.6023e-01,  2.3496e-01, -2.1091e-01,  5.1426e-01,\n",
            "          4.2866e-02, -2.1429e-01,  3.0993e-01,  1.1572e-01,  3.3085e-01,\n",
            "         -3.6244e-01, -1.1966e-02],\n",
            "        [ 5.9029e-02,  3.7972e-03, -1.8420e-01,  6.3540e-02, -2.2563e-01,\n",
            "         -6.2532e-02, -1.5978e-01, -6.8618e-02, -7.0869e-02,  1.4337e-01,\n",
            "          3.3851e-02,  8.4993e-02,  2.6060e-02,  1.9685e-02,  2.7512e-01,\n",
            "          6.5991e-03,  2.3907e-01, -8.1489e-02, -2.0571e-01, -7.2245e-02,\n",
            "          1.5961e-01, -3.0828e-02,  3.5446e-02, -3.2513e-01, -8.3569e-02,\n",
            "          1.9796e-03,  2.4639e-01,  1.5418e-01, -6.7278e-02,  1.8303e-01,\n",
            "          1.4897e-01, -2.7231e-01],\n",
            "        [-1.7962e-01, -1.1401e-01, -3.2541e-02, -3.2778e-01, -5.3643e-02,\n",
            "         -1.4436e-01, -2.3904e-01, -4.1115e-02,  1.0854e-01, -1.5222e-01,\n",
            "         -2.7345e-01, -1.3154e-02, -1.1267e-01, -1.6668e-01,  4.1157e-02,\n",
            "          2.6709e-01, -2.3560e-01,  3.1371e-02, -1.7987e-01, -6.5177e-03,\n",
            "          2.5484e-01,  3.3996e-01,  9.8487e-02,  2.4342e-01, -1.9816e-01,\n",
            "          3.9998e-02,  1.2817e-01, -1.5026e-01,  7.8172e-02, -3.2878e-01,\n",
            "          5.1171e-02,  1.2266e-01],\n",
            "        [ 3.4382e-02,  1.6907e-01, -9.2225e-02, -8.2357e-02,  7.6445e-02,\n",
            "         -1.1086e-01,  2.1008e-01, -2.7530e-01, -1.5466e-01, -1.7580e-01,\n",
            "         -3.3551e-02, -1.7303e-01,  6.1975e-02, -1.2988e-01, -8.8853e-02,\n",
            "          1.9150e-02, -2.8759e-01, -3.9236e-02, -9.0350e-02, -1.2980e-01,\n",
            "          1.2491e-01, -1.5108e-02,  1.6044e-01,  2.6181e-01, -6.0427e-02,\n",
            "          2.5410e-02,  1.0447e-01, -1.9951e-01,  3.9669e-02, -2.0804e-01,\n",
            "         -9.0849e-02,  2.7126e-01],\n",
            "        [-2.5070e-01,  1.0265e-01, -2.3377e-01, -1.1164e-01, -1.9756e-01,\n",
            "         -2.9327e-02, -1.7658e-01,  2.3696e-02, -1.3378e-01, -1.0912e-01,\n",
            "         -2.6217e-02,  8.6248e-02, -1.5687e-01, -1.0184e-01, -2.0420e-01,\n",
            "          5.2399e-02, -2.0882e-01,  2.3155e-01, -1.2314e-01, -2.0627e-01,\n",
            "         -1.3367e-01,  6.2100e-02,  6.4756e-02,  2.7969e-01, -1.7059e-01,\n",
            "          9.4913e-03,  8.2431e-02, -2.3363e-01, -8.0973e-02, -1.8267e-01,\n",
            "          1.6546e-01,  8.7000e-02],\n",
            "        [ 2.1620e-01,  1.1879e-01,  3.8051e-02,  9.5926e-02, -8.5411e-02,\n",
            "         -1.3059e-01,  4.4828e-02, -1.3878e-01, -2.0909e-01, -6.0645e-02,\n",
            "          1.7917e-02, -1.0469e-01, -2.6381e-01,  1.7237e-02, -1.5826e-01,\n",
            "         -4.7800e-02, -1.3739e-01, -1.7400e-01, -1.0004e-01, -6.2042e-02,\n",
            "          1.1941e-01,  9.5651e-02,  1.3100e-01,  2.0170e-01, -8.0311e-02,\n",
            "         -1.0255e-01,  8.2425e-02, -1.2649e-01,  2.7215e-03, -2.1205e-01,\n",
            "          1.9522e-02,  1.8187e-01]], device='cuda:0')\n",
            "lin.bias Parameter containing:\n",
            "tensor([-0.0920,  0.1684,  0.4019, -0.0813,  0.1214, -0.2267, -0.1834,  0.2512,\n",
            "        -0.1353, -0.5021,  0.4391, -0.0439, -0.1860,  0.0995, -0.1775,  0.0461],\n",
            "       device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "34de8800-fa4b-4460-803c-3cd8dbfb6c70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b92f11fa-9e8f-4e90-f775-614dbffb5392"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "71703.59375\n",
            "11942.978515625\n",
            "59687.84375\n",
            "16516.3203125\n",
            "12525.486328125\n",
            "140308.984375\n",
            "33349.7265625\n",
            "8448.01953125\n",
            "3341.463623046875\n",
            "24860.759765625\n",
            "13485.4638671875\n",
            "27461.13671875\n",
            "1190.61181640625\n",
            "5777.78466796875\n",
            "1368.1170654296875\n",
            "7503.44677734375\n",
            "78722.0859375\n",
            "32747.96484375\n",
            "7955.01904296875\n",
            "13216.1875\n",
            "28443.30859375\n",
            "12510.5078125\n",
            "35024.6171875\n",
            "5691.13134765625\n",
            "23201.525390625\n",
            "5289.17626953125\n",
            "6983.81396484375\n",
            "70965.0390625\n",
            "40838.30859375\n",
            "70755.0234375\n",
            "158067.390625\n",
            "32415.84765625\n",
            "strain 1.0817737579345703\n",
            "159903.75\n",
            "1503.9940185546875\n",
            "59050.60546875\n",
            "108760.625\n",
            "142179.828125\n",
            "39661.05078125\n",
            "13200.3525390625\n",
            "4953.958984375\n",
            "36009.37890625\n",
            "77774.4921875\n",
            "47508.36328125\n",
            "13717.955078125\n",
            "65461.671875\n",
            "12212.7265625\n",
            "18111.236328125\n",
            "166507.09375\n",
            "37159.8046875\n",
            "24742.068359375\n",
            "6968.25048828125\n",
            "70177.0234375\n",
            "34615.63671875\n",
            "30070.017578125\n",
            "10846.078125\n",
            "46590.2265625\n",
            "9068.9482421875\n",
            "14936.375\n",
            "95035.1640625\n",
            "24392.4765625\n",
            "6267.20361328125\n",
            "2546.701904296875\n",
            "23475.73828125\n",
            "34310.6796875\n",
            "21977.044921875\n",
            "5170.52978515625\n",
            "22655.224609375\n",
            "4707.95947265625\n",
            "8124.84423828125\n",
            "92620.8203125\n",
            "21371.568359375\n",
            "3144.61279296875\n",
            "1309.9942626953125\n",
            "14026.35546875\n",
            "10113.0556640625\n",
            "20393.845703125\n",
            "3236.02685546875\n",
            "14557.7763671875\n",
            "3115.65283203125\n",
            "2790.9833984375\n",
            "91702.5390625\n",
            "19938.111328125\n",
            "2938.64013671875\n",
            "1190.7381591796875\n",
            "12462.4267578125\n",
            "8018.1865234375\n",
            "19673.89453125\n",
            "1610.4423828125\n",
            "8561.29296875\n",
            "1581.980712890625\n",
            "16103.79296875\n",
            "92107.78125\n",
            "19268.3203125\n",
            "844.5106811523438\n",
            "39.20589065551758\n",
            "4796.76611328125\n",
            "6460.2294921875\n",
            "19291.541015625\n",
            "1191.6929931640625\n",
            "7340.466796875\n",
            "1368.95703125\n",
            "15444.052734375\n",
            "86147.7265625\n",
            "19351.787109375\n",
            "57709.41015625\n",
            "341071.59375\n",
            "62595.03515625\n",
            "390308.53125\n",
            "787264.75\n",
            "59751.9609375\n",
            "247558.765625\n",
            "228888.34375\n",
            "476619.0\n",
            "229449.65625\n",
            "168587.28125\n",
            "16419.99609375\n",
            "85975.984375\n",
            "24377.443359375\n",
            "23020.025390625\n",
            "422423.4375\n",
            "118438.2578125\n",
            "44260.76171875\n",
            "15462.6328125\n",
            "93348.1171875\n",
            "82536.296875\n",
            "134653.734375\n",
            "1982.905029296875\n",
            "10710.5556640625\n",
            "2732.691162109375\n",
            "3668.264404296875\n",
            "407007.34375\n",
            "124892.796875\n",
            "26062.62890625\n",
            "32728.296875\n",
            "76614.84375\n",
            "35649.5703125\n",
            "136262.1875\n",
            "3846.52197265625\n",
            "13698.939453125\n",
            "3074.338134765625\n",
            "5034.7548828125\n",
            "318990.71875\n",
            "135920.46875\n",
            "199074.109375\n",
            "443122.65625\n",
            "92521.1484375\n",
            "strain 2.2220044136047363\n",
            "250797.5\n",
            "2278.790283203125\n",
            "98354.2578125\n",
            "158830.21875\n",
            "275128.15625\n",
            "71289.875\n",
            "36050.5546875\n",
            "16127.3544921875\n",
            "106963.609375\n",
            "79146.234375\n",
            "92647.8671875\n",
            "64304.04296875\n",
            "198701.375\n",
            "40618.90625\n",
            "45787.375\n",
            "299621.46875\n",
            "72325.375\n",
            "37444.921875\n",
            "13696.404296875\n",
            "147201.59375\n",
            "73498.09375\n",
            "59904.9140625\n",
            "20746.845703125\n",
            "99187.9765625\n",
            "15158.5810546875\n",
            "24378.107421875\n",
            "189545.046875\n",
            "48178.1953125\n",
            "17291.9140625\n",
            "6736.24267578125\n",
            "66827.1484375\n",
            "66894.625\n",
            "46154.40625\n",
            "19071.275390625\n",
            "60711.296875\n",
            "11377.552734375\n",
            "13806.35546875\n",
            "162076.0\n",
            "40787.21875\n",
            "9743.873046875\n",
            "3453.76123046875\n",
            "33006.671875\n",
            "27113.060546875\n",
            "39719.42578125\n",
            "9127.9853515625\n",
            "31752.322265625\n",
            "6444.20361328125\n",
            "7565.3193359375\n",
            "137354.109375\n",
            "37547.53125\n",
            "6522.9892578125\n",
            "1389.200927734375\n",
            "27097.044921875\n",
            "21778.619140625\n",
            "36033.19140625\n",
            "1846.767822265625\n",
            "9475.6435546875\n",
            "1873.75634765625\n",
            "34152.07421875\n",
            "164072.59375\n",
            "36788.75390625\n",
            "2323.1044921875\n",
            "122.8667221069336\n",
            "11933.5009765625\n",
            "10267.8623046875\n",
            "36472.11328125\n",
            "1928.037841796875\n",
            "12964.7333984375\n",
            "2493.905517578125\n",
            "37649.28515625\n",
            "163761.765625\n",
            "36476.0703125\n",
            "81910.2265625\n",
            "609208.1875\n",
            "112325.109375\n",
            "2883161.5\n",
            "1388033.375\n",
            "103024.4453125\n",
            "417650.375\n",
            "205517.5625\n",
            "1270905.0\n",
            "399049.9375\n",
            "512710.21875\n",
            "56117.27734375\n",
            "279924.6875\n",
            "81491.75\n",
            "60147.34765625\n",
            "982082.375\n",
            "258206.828125\n",
            "45096.3828125\n",
            "8589.0751953125\n",
            "100761.2265625\n",
            "157079.515625\n",
            "267395.9375\n",
            "6353.21630859375\n",
            "23891.41015625\n",
            "6185.453125\n",
            "43185.55078125\n",
            "750606.8125\n",
            "271488.875\n",
            "64770.2578125\n",
            "81961.4296875\n",
            "180617.3125\n",
            "133432.0\n",
            "294511.59375\n",
            "7535.5087890625\n",
            "26460.119140625\n",
            "6274.58251953125\n",
            "27033.068359375\n",
            "602163.6875\n",
            "298967.6875\n",
            "292144.3125\n",
            "903830.5625\n",
            "191380.8125\n",
            "strain 1.7007168531417847\n",
            "190335.5\n",
            "1519.7999267578125\n",
            "126208.8046875\n",
            "197355.171875\n",
            "305010.4375\n",
            "71811.1875\n",
            "18145.328125\n",
            "9451.9169921875\n",
            "58083.96875\n",
            "126785.3828125\n",
            "74810.5703125\n",
            "30934.0234375\n",
            "136122.78125\n",
            "25999.078125\n",
            "32960.29296875\n",
            "332969.28125\n",
            "74247.5234375\n",
            "57817.29296875\n",
            "8877.455078125\n",
            "131797.859375\n",
            "167770.40625\n",
            "62118.94140625\n",
            "23510.69921875\n",
            "101484.3984375\n",
            "17542.197265625\n",
            "28243.380859375\n",
            "205767.203125\n",
            "50049.17578125\n",
            "18321.83203125\n",
            "5027.93896484375\n",
            "59569.33203125\n",
            "30245.287109375\n",
            "50234.5\n",
            "10217.5693359375\n",
            "55789.6328125\n",
            "10581.712890625\n",
            "21516.5703125\n",
            "202689.71875\n",
            "44474.16015625\n",
            "11808.8251953125\n",
            "6961.67333984375\n",
            "36288.29296875\n",
            "31876.322265625\n",
            "43790.26953125\n",
            "6527.06201171875\n",
            "34074.32421875\n",
            "6618.36376953125\n",
            "6140.2900390625\n",
            "197199.5\n",
            "41593.51953125\n",
            "8731.7880859375\n",
            "3806.45361328125\n",
            "30520.052734375\n",
            "24998.244140625\n",
            "42215.65234375\n",
            "1884.7259521484375\n",
            "14163.2353515625\n",
            "2654.003662109375\n",
            "35395.21484375\n",
            "189734.53125\n",
            "40888.53125\n",
            "2232.5390625\n",
            "58.004364013671875\n",
            "9570.9521484375\n",
            "11831.572265625\n",
            "39952.1875\n",
            "3906.239990234375\n",
            "18614.15625\n",
            "3316.1259765625\n",
            "33603.78515625\n",
            "204486.6875\n",
            "41448.66796875\n",
            "121256.3046875\n",
            "699680.6875\n",
            "125606.375\n",
            "828439.75\n",
            "1573764.5\n",
            "121467.609375\n",
            "532701.75\n",
            "143685.078125\n",
            "1035694.25\n",
            "230177.03125\n",
            "313033.8125\n",
            "24557.6640625\n",
            "156432.5\n",
            "46078.0625\n",
            "35709.515625\n",
            "744286.6875\n",
            "217380.796875\n",
            "55878.8046875\n",
            "4747.96240234375\n",
            "109741.171875\n",
            "95184.7421875\n",
            "222891.484375\n",
            "4640.80859375\n",
            "23376.591796875\n",
            "5643.84033203125\n",
            "15342.0517578125\n",
            "575521.8125\n",
            "224264.984375\n",
            "55353.0625\n",
            "24284.373046875\n",
            "164086.921875\n",
            "83038.1640625\n",
            "237827.890625\n",
            "5117.45361328125\n",
            "22635.974609375\n",
            "5356.8662109375\n",
            "23015.9609375\n",
            "519017.96875\n",
            "243694.609375\n",
            "246482.359375\n",
            "726102.3125\n",
            "150643.1875\n",
            "strain 1.3566266298294067\n",
            "39456.57421875\n",
            "178.190673828125\n",
            "12061.1708984375\n",
            "18666.693359375\n",
            "30264.62890625\n",
            "7582.17724609375\n",
            "1632.6553955078125\n",
            "160.37442016601562\n",
            "6291.21142578125\n",
            "16004.015625\n",
            "8404.50390625\n",
            "3803.873779296875\n",
            "14930.283203125\n",
            "2950.193115234375\n",
            "2295.75439453125\n",
            "29172.783203125\n",
            "7199.82861328125\n",
            "2625.475341796875\n",
            "1038.673828125\n",
            "12625.1533203125\n",
            "9634.4501953125\n",
            "6172.73095703125\n",
            "1228.7698974609375\n",
            "9020.2275390625\n",
            "1756.875\n",
            "1700.140625\n",
            "23640.716796875\n",
            "5480.03759765625\n",
            "3046.979736328125\n",
            "902.81787109375\n",
            "11028.6484375\n",
            "8584.7099609375\n",
            "5025.75537109375\n",
            "1281.798828125\n",
            "4669.50927734375\n",
            "859.7982788085938\n",
            "2288.148193359375\n",
            "16344.892578125\n",
            "4621.86279296875\n",
            "1361.458251953125\n",
            "497.07720947265625\n",
            "5341.4169921875\n",
            "3257.591552734375\n",
            "4209.8779296875\n",
            "566.5380859375\n",
            "3121.02587890625\n",
            "638.6450805664062\n",
            "2547.161865234375\n",
            "14666.818359375\n",
            "4288.91796875\n",
            "1065.7926025390625\n",
            "81.71139526367188\n",
            "5278.0986328125\n",
            "4296.35595703125\n",
            "4243.69140625\n",
            "268.9358215332031\n",
            "1766.1353759765625\n",
            "302.253662109375\n",
            "4611.06787109375\n",
            "17664.4375\n",
            "4151.31982421875\n",
            "411.0548400878906\n",
            "19.60931968688965\n",
            "2699.613525390625\n",
            "1939.14892578125\n",
            "4132.31201171875\n",
            "659.6185913085938\n",
            "4766.48193359375\n",
            "857.38623046875\n",
            "7945.2890625\n",
            "20847.341796875\n",
            "4294.58447265625\n",
            "15927.4423828125\n",
            "78894.546875\n",
            "14015.1357421875\n",
            "1641922.625\n",
            "214124.546875\n",
            "15347.2412109375\n",
            "93538.78125\n",
            "137599.53125\n",
            "322466.59375\n",
            "91569.59375\n",
            "121219.453125\n",
            "16960.216796875\n",
            "80038.84375\n",
            "21197.9296875\n",
            "24484.955078125\n",
            "293725.375\n",
            "70520.2109375\n",
            "7657.447265625\n",
            "3015.118408203125\n",
            "36064.1875\n",
            "49489.6953125\n",
            "73686.46875\n",
            "2719.3857421875\n",
            "9468.673828125\n",
            "1838.3560791015625\n",
            "2181.348876953125\n",
            "160454.140625\n",
            "73482.078125\n",
            "20421.25390625\n",
            "7003.2216796875\n",
            "66479.8046875\n",
            "59275.5625\n",
            "80888.296875\n",
            "3435.054931640625\n",
            "12737.5537109375\n",
            "3056.53173828125\n",
            "2073.572265625\n",
            "187352.28125\n",
            "79874.1484375\n",
            "83690.421875\n",
            "251595.921875\n",
            "50812.609375\n",
            "strain 1.0226404666900635\n",
            "652444.5625\n",
            "1936.1134033203125\n",
            "106718.6640625\n",
            "107690.8203125\n",
            "246432.75\n",
            "40660.52734375\n",
            "10600.958984375\n",
            "6275.8330078125\n",
            "37118.16796875\n",
            "46305.08203125\n",
            "47327.03125\n",
            "14913.4814453125\n",
            "72837.3046875\n",
            "15164.9228515625\n",
            "16680.07421875\n",
            "147371.46875\n",
            "42003.56640625\n",
            "11034.98828125\n",
            "5444.9482421875\n",
            "55441.296875\n",
            "35787.33984375\n",
            "32824.84375\n",
            "14017.4150390625\n",
            "61793.109375\n",
            "13445.955078125\n",
            "18871.00390625\n",
            "113156.2109375\n",
            "27289.494140625\n",
            "5542.875\n",
            "2749.775146484375\n",
            "21160.4609375\n",
            "34369.5546875\n",
            "27041.751953125\n",
            "5912.64453125\n",
            "24533.634765625\n",
            "5325.263671875\n",
            "10518.931640625\n",
            "115154.21875\n",
            "25169.521484375\n",
            "7063.748046875\n",
            "5974.912109375\n",
            "22409.119140625\n",
            "25749.57421875\n",
            "24973.767578125\n",
            "3498.23193359375\n",
            "17105.625\n",
            "3294.270263671875\n",
            "7544.01416015625\n",
            "108936.2734375\n",
            "24518.427734375\n",
            "5944.193359375\n",
            "5739.162109375\n",
            "17921.294921875\n",
            "27851.564453125\n",
            "24848.478515625\n",
            "969.9630126953125\n",
            "7833.64697265625\n",
            "1329.8865966796875\n",
            "26324.77734375\n",
            "141581.453125\n",
            "24531.796875\n",
            "649.259033203125\n",
            "60.359771728515625\n",
            "4526.6328125\n",
            "14040.9228515625\n",
            "23073.291015625\n",
            "2246.83203125\n",
            "11976.2177734375\n",
            "1918.8800048828125\n",
            "18597.236328125\n",
            "118858.40625\n",
            "24827.060546875\n",
            "92044.03125\n",
            "494376.09375\n",
            "72041.390625\n",
            "2883932.25\n",
            "1455429.375\n",
            "82840.6953125\n",
            "531428.8125\n",
            "224542.734375\n",
            "1871996.875\n",
            "420941.8125\n",
            "583812.125\n",
            "56904.0234375\n",
            "311265.46875\n",
            "95696.3203125\n",
            "62822.8515625\n",
            "1204481.5\n",
            "338464.28125\n",
            "42580.62109375\n",
            "21007.982421875\n",
            "173512.734375\n",
            "222455.265625\n",
            "350294.8125\n",
            "10274.3349609375\n",
            "36248.6015625\n",
            "7526.92041015625\n",
            "55239.99609375\n",
            "891966.6875\n",
            "348505.125\n",
            "62834.6328125\n",
            "123534.765625\n",
            "257038.515625\n",
            "222336.484375\n",
            "367775.625\n",
            "6399.4736328125\n",
            "27862.779296875\n",
            "6347.166015625\n",
            "40113.6796875\n",
            "735620.4375\n",
            "373338.09375\n",
            "332464.9375\n",
            "1153040.0\n",
            "247217.59375\n",
            "strain 2.451768636703491\n",
            "40952.609375\n",
            "627.892578125\n",
            "7084.4951171875\n",
            "16455.310546875\n",
            "43729.80859375\n",
            "16233.361328125\n",
            "2815.131103515625\n",
            "5830.09765625\n",
            "9438.2978515625\n",
            "6259.158203125\n",
            "8852.0048828125\n",
            "13557.1611328125\n",
            "45800.5390625\n",
            "5030.96630859375\n",
            "4581.65673828125\n",
            "24438.380859375\n",
            "2696.069580078125\n",
            "1857.669189453125\n",
            "2799.210693359375\n",
            "9151.5810546875\n",
            "4096.86865234375\n",
            "2856.19140625\n",
            "1634.7490234375\n",
            "9714.580078125\n",
            "1206.9654541015625\n",
            "2432.514404296875\n",
            "14133.5810546875\n",
            "2024.455810546875\n",
            "980.1937255859375\n",
            "965.5217895507812\n",
            "4683.1416015625\n",
            "4031.99658203125\n",
            "1984.6326904296875\n",
            "1191.237548828125\n",
            "4244.869140625\n",
            "691.6516723632812\n",
            "1506.30419921875\n",
            "11642.2255859375\n",
            "1470.1951904296875\n",
            "789.6320190429688\n",
            "488.52642822265625\n",
            "3312.268310546875\n",
            "1210.044189453125\n",
            "1395.701416015625\n",
            "686.6504516601562\n",
            "2378.337890625\n",
            "376.0118408203125\n",
            "1873.352783203125\n",
            "9115.0859375\n",
            "1292.9627685546875\n",
            "440.2989501953125\n",
            "189.93478393554688\n",
            "2069.79541015625\n",
            "1269.4046630859375\n",
            "1310.9664306640625\n",
            "174.7540740966797\n",
            "1121.9534912109375\n",
            "176.49061584472656\n",
            "2324.495849609375\n",
            "11881.51171875\n",
            "1267.6427001953125\n",
            "170.0404052734375\n",
            "17.603296279907227\n",
            "920.0881958007812\n",
            "636.44677734375\n",
            "1233.5076904296875\n",
            "266.4085388183594\n",
            "2431.5185546875\n",
            "434.84722900390625\n",
            "3995.968994140625\n",
            "12772.966796875\n",
            "1279.177001953125\n",
            "7930.13330078125\n",
            "29069.630859375\n",
            "3572.815185546875\n",
            "132453.375\n",
            "80919.515625\n",
            "4593.080078125\n",
            "44762.41796875\n",
            "75813.1796875\n",
            "339808.875\n",
            "97142.40625\n",
            "39306.91796875\n",
            "10267.5478515625\n",
            "37202.5078125\n",
            "6554.1826171875\n",
            "8299.037109375\n",
            "108849.9375\n",
            "27821.337890625\n",
            "12729.443359375\n",
            "1485.20068359375\n",
            "35446.859375\n",
            "16185.505859375\n",
            "27670.291015625\n",
            "585.8198852539062\n",
            "4875.69287109375\n",
            "1139.3516845703125\n",
            "2411.434326171875\n",
            "69655.7109375\n",
            "26581.478515625\n",
            "7856.39404296875\n",
            "4796.14013671875\n",
            "21142.609375\n",
            "13288.6728515625\n",
            "28518.84765625\n",
            "835.1034545898438\n",
            "5556.505859375\n",
            "1258.6982421875\n",
            "4249.99951171875\n",
            "74211.296875\n",
            "25915.36328125\n",
            "108640.859375\n",
            "176395.515625\n",
            "36480.09765625\n",
            "strain 1.1260374784469604\n",
            "50945.125\n",
            "556.1151123046875\n",
            "31584.8828125\n",
            "54890.87890625\n",
            "68439.0390625\n",
            "15848.7724609375\n",
            "6067.36669921875\n",
            "2277.416259765625\n",
            "15702.279296875\n",
            "32874.77734375\n",
            "17654.91796875\n",
            "9577.373046875\n",
            "43872.77734375\n",
            "10064.775390625\n",
            "16491.69140625\n",
            "98723.4921875\n",
            "19209.541015625\n",
            "12644.0634765625\n",
            "3167.030517578125\n",
            "40992.15234375\n",
            "24306.8828125\n",
            "15607.228515625\n",
            "12964.9091796875\n",
            "37721.96484375\n",
            "7393.76904296875\n",
            "11896.23828125\n",
            "51665.09765625\n",
            "11260.35546875\n",
            "1995.6270751953125\n",
            "2678.75146484375\n",
            "10147.7392578125\n",
            "20638.35546875\n",
            "11138.798828125\n",
            "1659.2777099609375\n",
            "8173.44873046875\n",
            "1516.078125\n",
            "4748.583984375\n",
            "54834.51953125\n",
            "10833.8271484375\n",
            "1836.6419677734375\n",
            "1022.3031005859375\n",
            "7432.91455078125\n",
            "6020.3642578125\n",
            "10104.4755859375\n",
            "848.427490234375\n",
            "4766.86376953125\n",
            "866.9503784179688\n",
            "6226.88623046875\n",
            "60093.73828125\n",
            "10643.03125\n",
            "1653.57470703125\n",
            "508.26678466796875\n",
            "7038.08935546875\n",
            "5581.212890625\n",
            "11058.654296875\n",
            "369.6097717285156\n",
            "2837.94140625\n",
            "463.8215026855469\n",
            "17146.814453125\n",
            "57578.73046875\n",
            "10469.1240234375\n",
            "351.5068664550781\n",
            "24.582592010498047\n",
            "2048.27734375\n",
            "3583.620361328125\n",
            "10822.1005859375\n",
            "1019.840576171875\n",
            "6362.90478515625\n",
            "1071.4381103515625\n",
            "12292.6875\n",
            "47668.98046875\n",
            "10335.3662109375\n",
            "50292.01953125\n",
            "187235.203125\n",
            "32086.396484375\n",
            "318573.59375\n",
            "411913.84375\n",
            "31813.052734375\n",
            "133103.546875\n",
            "97451.703125\n",
            "286935.875\n",
            "124481.015625\n",
            "109328.3203125\n",
            "16217.140625\n",
            "78453.5390625\n",
            "25479.9296875\n",
            "20124.74609375\n",
            "288317.25\n",
            "67888.2734375\n",
            "8675.2568359375\n",
            "4446.72998046875\n",
            "34817.703125\n",
            "35787.640625\n",
            "73494.9765625\n",
            "4745.97265625\n",
            "27424.115234375\n",
            "7332.9345703125\n",
            "21206.154296875\n",
            "150679.078125\n",
            "64078.83984375\n",
            "15055.2294921875\n",
            "4479.1533203125\n",
            "57071.375\n",
            "30758.98828125\n",
            "67099.0703125\n",
            "2241.261962890625\n",
            "13192.697265625\n",
            "2859.906494140625\n",
            "19549.380859375\n",
            "136627.609375\n",
            "65772.546875\n",
            "121279.4609375\n",
            "251535.109375\n",
            "49550.70703125\n",
            "strain 0.6409209370613098\n",
            "126451.7421875\n",
            "916.610595703125\n",
            "63188.9453125\n",
            "101525.75\n",
            "176044.21875\n",
            "40002.1796875\n",
            "7335.9619140625\n",
            "3315.01513671875\n",
            "27261.8046875\n",
            "104368.609375\n",
            "44904.91015625\n",
            "24121.427734375\n",
            "93713.5859375\n",
            "18915.341796875\n",
            "23185.076171875\n",
            "215817.359375\n",
            "50378.3984375\n",
            "39169.10546875\n",
            "8330.23046875\n",
            "93181.046875\n",
            "88013.9765625\n",
            "41486.63671875\n",
            "22769.9765625\n",
            "70406.8203125\n",
            "14763.7119140625\n",
            "17640.66796875\n",
            "108748.390625\n",
            "30582.3359375\n",
            "7890.92431640625\n",
            "2219.44970703125\n",
            "28985.9296875\n",
            "38277.21875\n",
            "29576.7421875\n",
            "4112.49169921875\n",
            "26331.8828125\n",
            "5636.1552734375\n",
            "10451.2255859375\n",
            "120962.125\n",
            "27276.142578125\n",
            "4722.94921875\n",
            "1902.2535400390625\n",
            "19318.849609375\n",
            "12420.5703125\n",
            "26745.857421875\n",
            "2666.935546875\n",
            "14557.9931640625\n",
            "2970.694091796875\n",
            "3381.605712890625\n",
            "121670.8203125\n",
            "25759.466796875\n",
            "3791.086669921875\n",
            "646.665771484375\n",
            "16046.8193359375\n",
            "8153.47265625\n",
            "25606.701171875\n",
            "2042.745849609375\n",
            "9599.1826171875\n",
            "1786.4208984375\n",
            "25054.5546875\n",
            "132390.625\n",
            "24865.564453125\n",
            "1052.217041015625\n",
            "43.016544342041016\n",
            "5241.3525390625\n",
            "8235.73828125\n",
            "25487.759765625\n",
            "1782.9779052734375\n",
            "10535.8271484375\n",
            "1896.111083984375\n",
            "21673.12890625\n",
            "129908.0625\n",
            "25072.130859375\n",
            "88435.6484375\n",
            "436694.875\n",
            "77592.6640625\n",
            "350638.8125\n",
            "1002334.6875\n",
            "75021.2421875\n",
            "286830.15625\n",
            "163888.6875\n",
            "555189.5625\n",
            "204391.703125\n",
            "190774.140625\n",
            "20096.03125\n",
            "102620.59375\n",
            "28113.115234375\n",
            "23572.630859375\n",
            "379883.5625\n",
            "109358.765625\n",
            "19688.701171875\n",
            "3247.110595703125\n",
            "47928.83203125\n",
            "62866.2578125\n",
            "111375.9765625\n",
            "2592.9306640625\n",
            "12148.65234375\n",
            "2557.718017578125\n",
            "4220.01171875\n",
            "328771.875\n",
            "115398.015625\n",
            "19840.439453125\n",
            "23081.9375\n",
            "74679.5859375\n",
            "50660.8125\n",
            "125440.828125\n",
            "3772.060791015625\n",
            "15228.087890625\n",
            "3734.721435546875\n",
            "3690.998291015625\n",
            "236594.546875\n",
            "127186.7890625\n",
            "138440.578125\n",
            "371548.0625\n",
            "76234.2265625\n",
            "strain 1.548284888267517\n",
            "42843.359375\n",
            "584.6076049804688\n",
            "39212.41796875\n",
            "57763.4375\n",
            "85504.125\n",
            "22290.615234375\n",
            "8085.62646484375\n",
            "6176.81494140625\n",
            "30340.95703125\n",
            "20333.845703125\n",
            "29152.34765625\n",
            "6964.34521484375\n",
            "34654.8359375\n",
            "8548.564453125\n",
            "9042.85546875\n",
            "89062.9609375\n",
            "20776.21875\n",
            "9924.0341796875\n",
            "3517.839111328125\n",
            "45780.4609375\n",
            "22176.775390625\n",
            "17075.15625\n",
            "8494.599609375\n",
            "35028.18359375\n",
            "6380.37939453125\n",
            "9495.0126953125\n",
            "54453.57421875\n",
            "13362.6376953125\n",
            "2414.8583984375\n",
            "2338.09033203125\n",
            "10439.8251953125\n",
            "18036.923828125\n",
            "13182.8017578125\n",
            "2263.84814453125\n",
            "11850.68359375\n",
            "2425.39306640625\n",
            "4597.02197265625\n",
            "55585.9453125\n",
            "12239.5107421875\n",
            "2265.103271484375\n",
            "1553.1842041015625\n",
            "8021.7529296875\n",
            "8659.6533203125\n",
            "12763.13671875\n",
            "1299.378173828125\n",
            "7093.10888671875\n",
            "1370.0413818359375\n",
            "3651.1240234375\n",
            "54956.546875\n",
            "11810.646484375\n",
            "1974.847412109375\n",
            "840.8792114257812\n",
            "6774.85791015625\n",
            "11034.1630859375\n",
            "11725.955078125\n",
            "364.54144287109375\n",
            "2274.37939453125\n",
            "338.2789611816406\n",
            "16110.4365234375\n",
            "64050.4453125\n",
            "11659.521484375\n",
            "284.61322021484375\n",
            "35.720645904541016\n",
            "2214.912841796875\n",
            "5792.02734375\n",
            "11963.0615234375\n",
            "624.501953125\n",
            "3859.129150390625\n",
            "726.4275512695312\n",
            "13115.1962890625\n",
            "57881.26171875\n",
            "11705.162109375\n",
            "45115.421875\n",
            "199979.984375\n",
            "34887.0078125\n",
            "766592.1875\n",
            "479418.75\n",
            "35694.46484375\n",
            "309147.28125\n",
            "213818.96875\n",
            "625755.6875\n",
            "257488.84375\n",
            "145595.625\n",
            "31741.736328125\n",
            "171630.0625\n",
            "32167.173828125\n",
            "32452.224609375\n",
            "359898.09375\n",
            "122100.515625\n",
            "27784.578125\n",
            "7313.67822265625\n",
            "56118.2578125\n",
            "61910.39453125\n",
            "126889.1328125\n",
            "3445.082275390625\n",
            "18254.095703125\n",
            "3558.2685546875\n",
            "10155.7822265625\n",
            "314865.0\n",
            "127991.515625\n",
            "29550.150390625\n",
            "29418.291015625\n",
            "103973.4296875\n",
            "88677.2421875\n",
            "149717.734375\n",
            "2748.302001953125\n",
            "13341.1416015625\n",
            "2758.113037109375\n",
            "12157.9072265625\n",
            "302262.0\n",
            "139365.25\n",
            "197255.296875\n",
            "477158.46875\n",
            "105759.5\n",
            "strain 2.3810782432556152\n",
            "257415.71875\n",
            "1495.5831298828125\n",
            "28458.53125\n",
            "105789.2734375\n",
            "96327.25\n",
            "38916.0703125\n",
            "17520.001953125\n",
            "9561.595703125\n",
            "54860.8359375\n",
            "58367.30859375\n",
            "43311.59765625\n",
            "27028.974609375\n",
            "96269.453125\n",
            "18678.14453125\n",
            "17064.205078125\n",
            "173266.546875\n",
            "41941.78125\n",
            "26192.38671875\n",
            "12430.4111328125\n",
            "74347.0546875\n",
            "101082.7265625\n",
            "28375.494140625\n",
            "6284.9296875\n",
            "36261.296875\n",
            "7911.037109375\n",
            "8003.8876953125\n",
            "84199.375\n",
            "20864.099609375\n",
            "11078.568359375\n",
            "4096.0390625\n",
            "33091.9609375\n",
            "27999.087890625\n",
            "18698.021484375\n",
            "5056.9404296875\n",
            "24016.189453125\n",
            "4737.9150390625\n",
            "13593.4287109375\n",
            "71050.03125\n",
            "15468.521484375\n",
            "4421.3447265625\n",
            "1140.177001953125\n",
            "15789.7861328125\n",
            "5803.5380859375\n",
            "14065.3212890625\n",
            "3217.845458984375\n",
            "13855.828125\n",
            "2510.991943359375\n",
            "9849.9013671875\n",
            "64532.4375\n",
            "13614.7900390625\n",
            "3026.6064453125\n",
            "345.1124572753906\n",
            "12403.8251953125\n",
            "7388.74462890625\n",
            "12871.998046875\n",
            "1108.5155029296875\n",
            "6900.75390625\n",
            "1295.2288818359375\n",
            "2525.895751953125\n",
            "57371.63671875\n",
            "13003.638671875\n",
            "1049.742919921875\n",
            "148.42007446289062\n",
            "5430.6220703125\n",
            "4329.10888671875\n",
            "13254.0283203125\n",
            "1255.0927734375\n",
            "6760.58056640625\n",
            "1238.3436279296875\n",
            "6336.39697265625\n",
            "71044.9609375\n",
            "13336.076171875\n",
            "44362.9453125\n",
            "232217.515625\n",
            "39475.1640625\n",
            "2002514.5\n",
            "560308.625\n",
            "38341.11328125\n",
            "297642.53125\n",
            "124190.7578125\n",
            "1070398.5\n",
            "177511.453125\n",
            "56818.40625\n",
            "22511.470703125\n",
            "76429.046875\n",
            "11307.1240234375\n",
            "14626.0244140625\n",
            "190279.40625\n",
            "38606.62890625\n",
            "19890.478515625\n",
            "7031.24951171875\n",
            "60864.80078125\n",
            "29807.95703125\n",
            "36741.6484375\n",
            "1596.4234619140625\n",
            "6351.3154296875\n",
            "1405.2596435546875\n",
            "4063.824951171875\n",
            "97443.296875\n",
            "37452.875\n",
            "9167.2978515625\n",
            "3895.06640625\n",
            "31261.638671875\n",
            "21495.751953125\n",
            "38050.0\n",
            "2083.089111328125\n",
            "9726.6494140625\n",
            "2147.902587890625\n",
            "2444.09619140625\n",
            "77931.8984375\n",
            "35504.61328125\n",
            "129815.984375\n",
            "223189.328125\n",
            "45606.82421875\n",
            "strain 1.2518806457519531\n",
            "602186.9375\n",
            "3878.16455078125\n",
            "86683.03125\n",
            "256515.5\n",
            "249744.3125\n",
            "90472.8125\n",
            "14345.1728515625\n",
            "18281.845703125\n",
            "65749.0390625\n",
            "68477.546875\n",
            "101532.3671875\n",
            "37683.1171875\n",
            "211539.890625\n",
            "20626.947265625\n",
            "27896.42578125\n",
            "280594.15625\n",
            "64033.640625\n",
            "38778.63671875\n",
            "19786.123046875\n",
            "154409.828125\n",
            "89895.2734375\n",
            "49875.54296875\n",
            "14247.4677734375\n",
            "87670.9140625\n",
            "14312.6396484375\n",
            "20876.458984375\n",
            "166364.890625\n",
            "38244.9140625\n",
            "14063.2822265625\n",
            "6495.3720703125\n",
            "53249.63671875\n",
            "60784.41015625\n",
            "35846.03515625\n",
            "11576.5546875\n",
            "40862.046875\n",
            "7188.716796875\n",
            "12735.595703125\n",
            "111586.515625\n",
            "32096.595703125\n",
            "6702.7705078125\n",
            "3186.73291015625\n",
            "24616.10546875\n",
            "19083.3203125\n",
            "30451.05859375\n",
            "5631.99951171875\n",
            "19774.537109375\n",
            "3885.334228515625\n",
            "8537.03125\n",
            "121763.3515625\n",
            "30200.005859375\n",
            "5415.8828125\n",
            "3399.531494140625\n",
            "23723.251953125\n",
            "18756.708984375\n",
            "29338.60546875\n",
            "1829.341552734375\n",
            "9572.84375\n",
            "1608.455810546875\n",
            "22813.08984375\n",
            "139698.6875\n",
            "29304.318359375\n",
            "1980.625732421875\n",
            "172.39244079589844\n",
            "10753.2822265625\n",
            "9976.8515625\n",
            "27770.455078125\n",
            "2071.189453125\n",
            "16727.70703125\n",
            "2832.5693359375\n",
            "25249.19921875\n",
            "135228.453125\n",
            "29597.4296875\n",
            "82288.515625\n",
            "546997.25\n",
            "95615.40625\n",
            "426927.6875\n",
            "1390317.125\n",
            "92734.3359375\n",
            "242713.734375\n",
            "159116.96875\n",
            "2047573.25\n",
            "552299.1875\n",
            "390070.96875\n",
            "68172.234375\n",
            "341798.78125\n",
            "97868.5390625\n",
            "69061.3125\n",
            "527629.3125\n",
            "109921.96875\n",
            "9772.9990234375\n",
            "3307.903076171875\n",
            "37009.6640625\n",
            "62150.7109375\n",
            "114359.234375\n",
            "4838.25048828125\n",
            "20126.525390625\n",
            "4487.45703125\n",
            "49432.4765625\n",
            "336258.40625\n",
            "114429.5625\n",
            "11185.6767578125\n",
            "10871.7275390625\n",
            "42713.25390625\n",
            "49720.70703125\n",
            "107556.4453125\n",
            "6526.16748046875\n",
            "28419.025390625\n",
            "6677.33251953125\n",
            "35318.03125\n",
            "225575.1875\n",
            "113125.1875\n",
            "166846.65625\n",
            "314270.71875\n",
            "57624.26171875\n",
            "strain 2.48026180267334\n",
            "419977.9375\n",
            "1157.885498046875\n",
            "119841.2109375\n",
            "170619.453125\n",
            "285258.59375\n",
            "49100.31640625\n",
            "20445.541015625\n",
            "3368.13720703125\n",
            "63393.3671875\n",
            "149827.640625\n",
            "55909.4609375\n",
            "26569.201171875\n",
            "106212.875\n",
            "21605.419921875\n",
            "22219.611328125\n",
            "217219.265625\n",
            "44305.26953125\n",
            "40613.3125\n",
            "2403.53759765625\n",
            "95486.2109375\n",
            "113254.3984375\n",
            "40448.59765625\n",
            "27837.755859375\n",
            "88523.84375\n",
            "16400.611328125\n",
            "23587.267578125\n",
            "125983.8125\n",
            "30847.939453125\n",
            "8995.7294921875\n",
            "2671.95166015625\n",
            "29492.720703125\n",
            "31451.796875\n",
            "29773.44921875\n",
            "6433.18310546875\n",
            "35349.03515625\n",
            "6907.6533203125\n",
            "12509.0146484375\n",
            "130691.21875\n",
            "28446.76171875\n",
            "4776.90087890625\n",
            "858.2222900390625\n",
            "18534.12890625\n",
            "24155.646484375\n",
            "26559.462890625\n",
            "3021.322998046875\n",
            "16089.599609375\n",
            "3078.15771484375\n",
            "12430.5419921875\n",
            "136349.359375\n",
            "27406.07421875\n",
            "4562.87451171875\n",
            "892.2821044921875\n",
            "17503.0078125\n",
            "29120.107421875\n",
            "27632.740234375\n",
            "1397.3526611328125\n",
            "8786.59765625\n",
            "1578.5338134765625\n",
            "36052.87109375\n",
            "137780.515625\n",
            "26913.443359375\n",
            "944.2955322265625\n",
            "85.81654357910156\n",
            "4808.1494140625\n",
            "16087.0224609375\n",
            "27673.771484375\n",
            "2020.83642578125\n",
            "13924.1923828125\n",
            "2402.692138671875\n",
            "35295.875\n",
            "143916.921875\n",
            "26793.966796875\n",
            "98478.4375\n",
            "423868.71875\n",
            "76118.265625\n",
            "401879.34375\n",
            "888571.6875\n",
            "72494.578125\n",
            "268246.3125\n",
            "102069.890625\n",
            "466135.21875\n",
            "183027.921875\n",
            "168890.078125\n",
            "17704.81640625\n",
            "110351.375\n",
            "31642.25390625\n",
            "22537.9921875\n",
            "408646.8125\n",
            "121564.0546875\n",
            "26061.44921875\n",
            "5403.49755859375\n",
            "61619.94140625\n",
            "55961.0546875\n",
            "127357.109375\n",
            "5253.8916015625\n",
            "28936.0703125\n",
            "7907.61572265625\n",
            "25760.70703125\n",
            "313735.25\n",
            "121470.59375\n",
            "36254.09375\n",
            "13755.900390625\n",
            "93979.3984375\n",
            "72550.6484375\n",
            "128340.328125\n",
            "2648.75537109375\n",
            "16228.0087890625\n",
            "3748.646728515625\n",
            "30768.875\n",
            "276989.09375\n",
            "128166.484375\n",
            "138372.0\n",
            "405266.125\n",
            "85172.8203125\n",
            "strain 0.47702211141586304\n",
            "83156.359375\n",
            "627.8974609375\n",
            "26706.595703125\n",
            "37232.859375\n",
            "67633.6796875\n",
            "19671.189453125\n",
            "2725.654296875\n",
            "754.9603271484375\n",
            "14939.6943359375\n",
            "44587.125\n",
            "21581.341796875\n",
            "14545.20703125\n",
            "46043.19921875\n",
            "7735.3203125\n",
            "5609.0517578125\n",
            "65583.34375\n",
            "15701.080078125\n",
            "7542.39892578125\n",
            "1444.908935546875\n",
            "26329.732421875\n",
            "28411.2421875\n",
            "12813.6953125\n",
            "3980.192626953125\n",
            "24907.162109375\n",
            "4468.11962890625\n",
            "4197.416015625\n",
            "38308.36328125\n",
            "10535.1357421875\n",
            "3910.631591796875\n",
            "1408.779052734375\n",
            "14850.3369140625\n",
            "13488.6103515625\n",
            "8914.837890625\n",
            "3261.471923828125\n",
            "10202.017578125\n",
            "1989.95947265625\n",
            "3181.139892578125\n",
            "28688.921875\n",
            "8720.341796875\n",
            "2324.11572265625\n",
            "1248.8724365234375\n",
            "8147.47607421875\n",
            "6158.00439453125\n",
            "8991.9697265625\n",
            "1579.944580078125\n",
            "6271.28271484375\n",
            "1417.94873046875\n",
            "2611.630615234375\n",
            "31819.92578125\n",
            "8041.2939453125\n",
            "1793.90771484375\n",
            "505.58758544921875\n",
            "7721.685546875\n",
            "6520.96826171875\n",
            "8454.85546875\n",
            "387.2362365722656\n",
            "2506.210205078125\n",
            "408.18658447265625\n",
            "5054.3720703125\n",
            "30024.7109375\n",
            "7848.3720703125\n",
            "581.0001220703125\n",
            "54.7972297668457\n",
            "3362.415771484375\n",
            "2889.94677734375\n",
            "7757.1904296875\n",
            "782.2067260742188\n",
            "5031.701171875\n",
            "915.1773681640625\n",
            "9379.171875\n",
            "35787.16796875\n",
            "8026.84619140625\n",
            "19101.001953125\n",
            "133419.421875\n",
            "24556.345703125\n",
            "306488.625\n",
            "357334.59375\n",
            "26161.3046875\n",
            "89184.5625\n",
            "50893.57421875\n",
            "371167.0625\n",
            "146762.359375\n",
            "126622.53125\n",
            "15005.0322265625\n",
            "79136.5078125\n",
            "22920.173828125\n",
            "13840.3974609375\n",
            "194295.96875\n",
            "51455.0234375\n",
            "25821.546875\n",
            "3242.78857421875\n",
            "64046.4609375\n",
            "19197.095703125\n",
            "55060.734375\n",
            "1453.3802490234375\n",
            "6882.03759765625\n",
            "1695.475341796875\n",
            "7117.849609375\n",
            "116669.4453125\n",
            "53553.19140625\n",
            "14387.435546875\n",
            "8908.0146484375\n",
            "41346.6171875\n",
            "20850.18359375\n",
            "58688.0546875\n",
            "6102.49755859375\n",
            "30770.6796875\n",
            "7544.63525390625\n",
            "3626.79052734375\n",
            "96004.3203125\n",
            "51175.71484375\n",
            "192499.265625\n",
            "305010.5625\n",
            "64034.73046875\n",
            "strain 0.8177328705787659\n",
            "103689.703125\n",
            "499.1314392089844\n",
            "34547.5703125\n",
            "73696.3125\n",
            "88489.6328125\n",
            "26808.20703125\n",
            "12430.5048828125\n",
            "4697.4775390625\n",
            "35838.80078125\n",
            "50157.2421875\n",
            "38261.3671875\n",
            "8018.4755859375\n",
            "44410.0703125\n",
            "9158.2080078125\n",
            "10504.439453125\n",
            "87345.2265625\n",
            "22742.0625\n",
            "11910.580078125\n",
            "3926.305908203125\n",
            "44147.015625\n",
            "30084.4140625\n",
            "19600.87109375\n",
            "5863.57958984375\n",
            "31175.44140625\n",
            "6393.50146484375\n",
            "8695.921875\n",
            "51967.96875\n",
            "12387.62109375\n",
            "2567.289794921875\n",
            "2103.8076171875\n",
            "10872.00390625\n",
            "17562.10546875\n",
            "11593.2666015625\n",
            "1684.8883056640625\n",
            "10125.65234375\n",
            "1983.411865234375\n",
            "4232.34326171875\n",
            "51301.95703125\n",
            "11048.0107421875\n",
            "2785.483154296875\n",
            "2472.29248046875\n",
            "9816.8837890625\n",
            "6682.89990234375\n",
            "10993.3603515625\n",
            "974.0717163085938\n",
            "6664.88330078125\n",
            "1212.1212158203125\n",
            "2727.501220703125\n",
            "57938.8359375\n",
            "10660.51171875\n",
            "2144.531494140625\n",
            "2774.708984375\n",
            "7051.009765625\n",
            "7556.33935546875\n",
            "10420.697265625\n",
            "782.3213500976562\n",
            "3629.6572265625\n",
            "636.3113403320312\n",
            "8586.3056640625\n",
            "50054.0390625\n",
            "10347.5234375\n",
            "305.7912292480469\n",
            "25.67429542541504\n",
            "1848.4044189453125\n",
            "3164.436767578125\n",
            "9828.228515625\n",
            "544.8296508789062\n",
            "3544.262939453125\n",
            "605.4155883789062\n",
            "8735.2373046875\n",
            "60345.9140625\n",
            "10476.6015625\n",
            "37055.3125\n",
            "205862.640625\n",
            "31971.154296875\n",
            "682064.375\n",
            "501878.4375\n",
            "31807.505859375\n",
            "198814.828125\n",
            "214971.0625\n",
            "720831.0625\n",
            "135412.578125\n",
            "175441.84375\n",
            "21458.50390625\n",
            "94037.0078125\n",
            "28796.966796875\n",
            "31372.787109375\n",
            "352494.75\n",
            "85950.015625\n",
            "8024.77294921875\n",
            "4185.73095703125\n",
            "30129.298828125\n",
            "39781.21875\n",
            "85680.8828125\n",
            "2466.966552734375\n",
            "10600.689453125\n",
            "2369.283935546875\n",
            "8932.5302734375\n",
            "249395.421875\n",
            "88380.8046875\n",
            "17708.83203125\n",
            "11471.0927734375\n",
            "53984.2265625\n",
            "47812.59375\n",
            "93580.75\n",
            "2903.7392578125\n",
            "13636.8935546875\n",
            "3017.919921875\n",
            "6165.63525390625\n",
            "174577.640625\n",
            "95883.0078125\n",
            "102450.40625\n",
            "299346.0625\n",
            "61966.04296875\n",
            "strain 1.9780391454696655\n",
            "239751.4375\n",
            "676.5987548828125\n",
            "51839.84765625\n",
            "63523.89453125\n",
            "164960.4375\n",
            "30055.541015625\n",
            "17159.9140625\n",
            "1824.261474609375\n",
            "47261.0859375\n",
            "75199.15625\n",
            "39859.359375\n",
            "33146.98828125\n",
            "85979.9921875\n",
            "19728.896484375\n",
            "15509.642578125\n",
            "76607.9765625\n",
            "17574.54296875\n",
            "7283.5\n",
            "1369.0360107421875\n",
            "30507.341796875\n",
            "23598.24609375\n",
            "16346.7373046875\n",
            "6129.18701171875\n",
            "33278.48046875\n",
            "5012.88134765625\n",
            "6686.341796875\n",
            "70594.3828125\n",
            "14792.4921875\n",
            "8881.580078125\n",
            "1040.11669921875\n",
            "33400.1875\n",
            "19476.658203125\n",
            "13706.9619140625\n",
            "7343.96875\n",
            "22914.8515625\n",
            "4671.046875\n",
            "10138.0478515625\n",
            "49446.16796875\n",
            "12946.455078125\n",
            "4079.6474609375\n",
            "1111.423828125\n",
            "18370.33984375\n",
            "10639.369140625\n",
            "12604.2978515625\n",
            "1908.566650390625\n",
            "11093.92578125\n",
            "2081.6669921875\n",
            "11463.8935546875\n",
            "49708.546875\n",
            "11834.3251953125\n",
            "2979.452880859375\n",
            "235.0750732421875\n",
            "15769.2861328125\n",
            "11444.3369140625\n",
            "11438.30859375\n",
            "779.6826171875\n",
            "6641.81005859375\n",
            "1119.9591064453125\n",
            "17794.650390625\n",
            "53241.47265625\n",
            "11608.5322265625\n",
            "1212.3370361328125\n",
            "62.3846321105957\n",
            "8527.18359375\n",
            "5458.00048828125\n",
            "11521.671875\n",
            "2476.021728515625\n",
            "15986.7705078125\n",
            "2748.12255859375\n",
            "24197.974609375\n",
            "54165.71484375\n",
            "11968.4609375\n",
            "60734.7578125\n",
            "243338.203125\n",
            "43319.71484375\n",
            "2700749.0\n",
            "634924.5625\n",
            "47913.01953125\n",
            "416703.78125\n",
            "182689.3125\n",
            "880990.375\n",
            "320748.5\n",
            "288607.125\n",
            "42022.37109375\n",
            "213060.359375\n",
            "60351.84765625\n",
            "46845.546875\n",
            "649043.25\n",
            "181362.5625\n",
            "30945.083984375\n",
            "11896.4638671875\n",
            "116158.0234375\n",
            "126085.0078125\n",
            "183004.234375\n",
            "6049.39453125\n",
            "25056.49609375\n",
            "5702.8681640625\n",
            "52519.4609375\n",
            "482185.84375\n",
            "184298.875\n",
            "49772.73046875\n",
            "23962.3671875\n",
            "143092.84375\n",
            "155333.484375\n",
            "188171.3125\n",
            "8920.197265625\n",
            "31799.544921875\n",
            "7708.150390625\n",
            "39397.15625\n",
            "373890.46875\n",
            "194270.15625\n",
            "176972.125\n",
            "634391.4375\n",
            "135425.21875\n",
            "strain 0.8195058703422546\n",
            "64482.4765625\n",
            "430.7409362792969\n",
            "38853.71484375\n",
            "49742.53515625\n",
            "118389.546875\n",
            "20986.572265625\n",
            "6841.56103515625\n",
            "782.7313232421875\n",
            "21751.6328125\n",
            "63288.8046875\n",
            "22844.392578125\n",
            "21713.787109375\n",
            "66206.640625\n",
            "11858.1416015625\n",
            "12903.419921875\n",
            "76490.453125\n",
            "18148.951171875\n",
            "16008.966796875\n",
            "5546.70751953125\n",
            "44556.34765625\n",
            "28407.642578125\n",
            "17344.71484375\n",
            "22879.751953125\n",
            "66615.15625\n",
            "12350.2548828125\n",
            "8922.1884765625\n",
            "40902.5390625\n",
            "9636.2734375\n",
            "2516.04248046875\n",
            "866.9424438476562\n",
            "8873.728515625\n",
            "6952.34765625\n",
            "9288.1650390625\n",
            "3766.3251953125\n",
            "18550.154296875\n",
            "3513.485107421875\n",
            "6098.1044921875\n",
            "38642.27734375\n",
            "8625.11328125\n",
            "1382.1021728515625\n",
            "542.6500854492188\n",
            "5637.76611328125\n",
            "6606.66015625\n",
            "8641.931640625\n",
            "1625.9794921875\n",
            "11296.484375\n",
            "2130.644287109375\n",
            "4772.4482421875\n",
            "34320.05859375\n",
            "8198.3291015625\n",
            "1000.3488159179688\n",
            "120.94778442382812\n",
            "4785.2353515625\n",
            "9603.7880859375\n",
            "8286.568359375\n",
            "2211.12353515625\n",
            "9965.9921875\n",
            "1793.3944091796875\n",
            "12224.955078125\n",
            "44284.58984375\n",
            "7950.85595703125\n",
            "205.17466735839844\n",
            "42.49908447265625\n",
            "1358.174560546875\n",
            "4602.865234375\n",
            "8197.3837890625\n",
            "654.9216918945312\n",
            "5131.63330078125\n",
            "914.7672119140625\n",
            "9647.5029296875\n",
            "36558.98046875\n",
            "7948.80810546875\n",
            "36918.20703125\n",
            "131657.84375\n",
            "23329.21875\n",
            "261171.078125\n",
            "269682.65625\n",
            "22280.796875\n",
            "96391.8359375\n",
            "54680.53125\n",
            "292503.1875\n",
            "119019.046875\n",
            "52996.5078125\n",
            "7887.462890625\n",
            "39175.08984375\n",
            "10531.091796875\n",
            "8294.30859375\n",
            "134345.078125\n",
            "45377.91015625\n",
            "17650.74609375\n",
            "8889.744140625\n",
            "50873.19140625\n",
            "24370.02734375\n",
            "48225.984375\n",
            "2497.29736328125\n",
            "13471.4775390625\n",
            "3087.747314453125\n",
            "11997.0546875\n",
            "126051.6796875\n",
            "47027.5859375\n",
            "16549.443359375\n",
            "8124.04052734375\n",
            "36479.31640625\n",
            "45574.44921875\n",
            "45894.88671875\n",
            "2303.430908203125\n",
            "13174.8681640625\n",
            "3021.062744140625\n",
            "10497.470703125\n",
            "85597.046875\n",
            "49464.421875\n",
            "98552.9765625\n",
            "178493.34375\n",
            "36032.0390625\n",
            "strain 0.4037644565105438\n",
            "123823.71875\n",
            "1069.567626953125\n",
            "50740.7421875\n",
            "69596.421875\n",
            "124054.2265625\n",
            "29953.29296875\n",
            "5404.54541015625\n",
            "3147.5498046875\n",
            "24821.44921875\n",
            "41221.51953125\n",
            "34419.9765625\n",
            "13449.755859375\n",
            "44509.22265625\n",
            "9869.26171875\n",
            "9673.7587890625\n",
            "85676.2578125\n",
            "23082.888671875\n",
            "9353.9228515625\n",
            "1206.6007080078125\n",
            "35217.64453125\n",
            "31796.98046875\n",
            "21189.142578125\n",
            "5771.66015625\n",
            "27259.1640625\n",
            "4137.3818359375\n",
            "5949.673828125\n",
            "78843.8046875\n",
            "18847.021484375\n",
            "6695.9677734375\n",
            "1887.2176513671875\n",
            "26150.24609375\n",
            "25215.525390625\n",
            "16899.6640625\n",
            "3081.413330078125\n",
            "12629.8076171875\n",
            "1929.292236328125\n",
            "9590.3037109375\n",
            "64733.66015625\n",
            "17035.8359375\n",
            "4131.93603515625\n",
            "2492.5498046875\n",
            "17311.94921875\n",
            "9276.671875\n",
            "16570.306640625\n",
            "1842.5819091796875\n",
            "9869.7705078125\n",
            "2130.445556640625\n",
            "11026.5234375\n",
            "60845.890625\n",
            "16212.5283203125\n",
            "3331.368896484375\n",
            "1047.443115234375\n",
            "16360.0380859375\n",
            "13586.7099609375\n",
            "15995.9462890625\n",
            "816.4135131835938\n",
            "5518.1298828125\n",
            "880.1304321289062\n",
            "21018.35546875\n",
            "74916.796875\n",
            "15988.7646484375\n",
            "1703.875\n",
            "53.77827835083008\n",
            "9082.6416015625\n",
            "7239.22265625\n",
            "15792.3486328125\n",
            "3004.041015625\n",
            "18309.32421875\n",
            "3186.490234375\n",
            "26531.8515625\n",
            "74151.7265625\n",
            "16369.63671875\n",
            "48291.0859375\n",
            "313438.25\n",
            "57415.8359375\n",
            "2785407.25\n",
            "865924.3125\n",
            "61948.5859375\n",
            "392195.5625\n",
            "236413.234375\n",
            "899054.8125\n",
            "315880.6875\n",
            "367155.5\n",
            "30191.849609375\n",
            "193651.015625\n",
            "57684.12890625\n",
            "43218.96484375\n",
            "773686.5\n",
            "223391.0625\n",
            "27443.021484375\n",
            "9252.1328125\n",
            "101699.3046875\n",
            "126879.953125\n",
            "227188.515625\n",
            "6794.13916015625\n",
            "27638.869140625\n",
            "6013.7275390625\n",
            "44358.2421875\n",
            "579013.375\n",
            "229893.40625\n",
            "47279.8828125\n",
            "27727.087890625\n",
            "170886.125\n",
            "138895.90625\n",
            "243213.390625\n",
            "6470.1103515625\n",
            "24225.64453125\n",
            "6069.787109375\n",
            "34364.2421875\n",
            "492363.5625\n",
            "240581.078125\n",
            "226203.265625\n",
            "764280.5\n",
            "162688.21875\n",
            "strain 1.3083522319793701\n",
            "29959.150390625\n",
            "864.3554077148438\n",
            "12825.2685546875\n",
            "48147.453125\n",
            "35988.96484375\n",
            "19889.08984375\n",
            "1704.4698486328125\n",
            "1049.6329345703125\n",
            "13344.9599609375\n",
            "44424.64453125\n",
            "21960.818359375\n",
            "6937.8125\n",
            "34152.25\n",
            "5352.85400390625\n",
            "5591.93505859375\n",
            "65528.58203125\n",
            "12777.0126953125\n",
            "8930.7001953125\n",
            "1917.0516357421875\n",
            "22603.494140625\n",
            "13683.603515625\n",
            "11270.8974609375\n",
            "3390.53515625\n",
            "18647.828125\n",
            "3242.01611328125\n",
            "5918.705078125\n",
            "39329.2578125\n",
            "8601.3193359375\n",
            "3863.084716796875\n",
            "733.9793701171875\n",
            "11975.0234375\n",
            "12006.771484375\n",
            "8111.66552734375\n",
            "3673.484130859375\n",
            "13582.9873046875\n",
            "2431.880859375\n",
            "4433.25244140625\n",
            "32348.462890625\n",
            "6968.37646484375\n",
            "1754.556884765625\n",
            "1598.24609375\n",
            "6048.50830078125\n",
            "3912.59130859375\n",
            "6954.41162109375\n",
            "1628.61572265625\n",
            "8543.498046875\n",
            "1564.690673828125\n",
            "4037.055908203125\n",
            "34305.3671875\n",
            "6323.35693359375\n",
            "1289.2724609375\n",
            "747.4193115234375\n",
            "4906.9052734375\n",
            "3381.35302734375\n",
            "6117.45068359375\n",
            "334.0802001953125\n",
            "2099.760986328125\n",
            "407.63226318359375\n",
            "2475.44873046875\n",
            "26667.46875\n",
            "6231.88818359375\n",
            "361.6419677734375\n",
            "44.04220962524414\n",
            "1835.9876708984375\n",
            "2801.235107421875\n",
            "6506.23046875\n",
            "578.0215454101562\n",
            "2637.295654296875\n",
            "456.67431640625\n",
            "3333.7529296875\n",
            "30760.1015625\n",
            "6369.7734375\n",
            "20250.001953125\n",
            "124362.7734375\n",
            "21683.498046875\n",
            "339639.375\n",
            "333698.03125\n",
            "24383.380859375\n",
            "97711.53125\n",
            "95783.6640625\n",
            "379421.5625\n",
            "125404.8671875\n",
            "98510.375\n",
            "21206.595703125\n",
            "90923.4921875\n",
            "23195.23828125\n",
            "27909.419921875\n",
            "241945.046875\n",
            "24838.755859375\n",
            "6231.87158203125\n",
            "3187.84765625\n",
            "19937.1796875\n",
            "24540.80078125\n",
            "26709.3125\n",
            "4451.99267578125\n",
            "20115.958984375\n",
            "5324.40771484375\n",
            "12564.4384765625\n",
            "89316.4453125\n",
            "26415.078125\n",
            "4255.494140625\n",
            "2522.765625\n",
            "17745.1953125\n",
            "12969.9755859375\n",
            "27715.3828125\n",
            "2917.883544921875\n",
            "8938.001953125\n",
            "2005.170166015625\n",
            "14074.8466796875\n",
            "51882.765625\n",
            "27370.265625\n",
            "55348.3984375\n",
            "96699.5859375\n",
            "15073.4248046875\n",
            "strain 1.4604748487472534\n",
            "221678.5\n",
            "2588.92236328125\n",
            "74507.9765625\n",
            "122174.046875\n",
            "254481.1875\n",
            "85796.890625\n",
            "22502.11328125\n",
            "17247.46875\n",
            "55066.0546875\n",
            "92249.8359375\n",
            "74834.5703125\n",
            "47703.25390625\n",
            "215658.96875\n",
            "35257.5546875\n",
            "20073.74609375\n",
            "183187.21875\n",
            "53225.109375\n",
            "22471.033203125\n",
            "9896.0693359375\n",
            "87780.0\n",
            "86484.796875\n",
            "43080.00390625\n",
            "18143.044921875\n",
            "72209.4609375\n",
            "15545.25\n",
            "17435.71875\n",
            "98774.828125\n",
            "24274.771484375\n",
            "3319.485107421875\n",
            "2856.030029296875\n",
            "15040.0087890625\n",
            "39365.58203125\n",
            "20984.734375\n",
            "6133.02685546875\n",
            "23878.6171875\n",
            "3982.76953125\n",
            "9645.087890625\n",
            "97501.828125\n",
            "21830.009765625\n",
            "3288.270751953125\n",
            "1899.4443359375\n",
            "12956.2783203125\n",
            "12344.3193359375\n",
            "19565.39453125\n",
            "2390.360595703125\n",
            "15245.55859375\n",
            "2639.214111328125\n",
            "3110.294189453125\n",
            "108995.9375\n",
            "20512.705078125\n",
            "2896.713623046875\n",
            "1371.472900390625\n",
            "12176.5888671875\n",
            "15580.888671875\n",
            "20009.779296875\n",
            "995.1812133789062\n",
            "6231.44580078125\n",
            "1093.2957763671875\n",
            "19307.125\n",
            "88317.765625\n",
            "20109.939453125\n",
            "467.13116455078125\n",
            "18.28045082092285\n",
            "3144.285888671875\n",
            "8655.146484375\n",
            "19760.208984375\n",
            "1287.134521484375\n",
            "7380.78515625\n",
            "1124.29150390625\n",
            "19668.396484375\n",
            "92669.0859375\n",
            "20070.12890625\n",
            "74197.78125\n",
            "357512.75\n",
            "64089.38671875\n",
            "734889.125\n",
            "831978.5\n",
            "63529.64453125\n",
            "386985.4375\n",
            "232711.78125\n",
            "930381.9375\n",
            "292255.84375\n",
            "202278.921875\n",
            "30242.25\n",
            "137261.578125\n",
            "39590.734375\n",
            "24344.833984375\n",
            "507440.96875\n",
            "133624.234375\n",
            "35813.0625\n",
            "8634.849609375\n",
            "83316.7265625\n",
            "59381.64453125\n",
            "135671.171875\n",
            "4833.80810546875\n",
            "25460.212890625\n",
            "6470.6591796875\n",
            "19882.083984375\n",
            "392205.6875\n",
            "134825.84375\n",
            "21091.916015625\n",
            "27098.826171875\n",
            "101834.03125\n",
            "82932.703125\n",
            "145237.640625\n",
            "3908.230712890625\n",
            "16475.88671875\n",
            "3774.20458984375\n",
            "22974.783203125\n",
            "295201.28125\n",
            "140353.3125\n",
            "201678.671875\n",
            "492193.4375\n",
            "101427.2421875\n",
            "strain 1.4996682405471802\n",
            "334886.6875\n",
            "2347.65283203125\n",
            "105916.7578125\n",
            "190470.921875\n",
            "226310.21875\n",
            "63432.69140625\n",
            "22456.767578125\n",
            "8082.66748046875\n",
            "67230.40625\n",
            "153995.5625\n",
            "71707.578125\n",
            "23472.8984375\n",
            "112248.609375\n",
            "24365.408203125\n",
            "37086.82421875\n",
            "327671.0\n",
            "66483.3515625\n",
            "40329.8984375\n",
            "10608.763671875\n",
            "125842.03125\n",
            "78632.578125\n",
            "53912.20703125\n",
            "39210.08203125\n",
            "129837.5625\n",
            "22665.861328125\n",
            "37954.2109375\n",
            "175658.03125\n",
            "40618.5234375\n",
            "5518.83203125\n",
            "8465.841796875\n",
            "32726.591796875\n",
            "72775.3515625\n",
            "38539.76171875\n",
            "4768.51708984375\n",
            "30147.66796875\n",
            "5671.4189453125\n",
            "13746.0400390625\n",
            "165937.453125\n",
            "37981.6171875\n",
            "5532.69921875\n",
            "2748.38916015625\n",
            "23636.3046875\n",
            "22063.134765625\n",
            "37847.0859375\n",
            "3404.650390625\n",
            "18008.3515625\n",
            "3479.92529296875\n",
            "12468.49609375\n",
            "165066.046875\n",
            "36705.828125\n",
            "5261.89501953125\n",
            "848.8949584960938\n",
            "22657.330078125\n",
            "21105.94921875\n",
            "37841.17578125\n",
            "2038.2264404296875\n",
            "11439.2939453125\n",
            "2226.79541015625\n",
            "51370.23046875\n",
            "200275.171875\n",
            "35734.0859375\n",
            "891.224609375\n",
            "77.77967071533203\n",
            "4845.9130859375\n",
            "11325.439453125\n",
            "34347.515625\n",
            "2742.889404296875\n",
            "14844.619140625\n",
            "2489.7138671875\n",
            "44727.42578125\n",
            "188497.265625\n",
            "35516.4921875\n",
            "140972.0\n",
            "645594.125\n",
            "113365.7734375\n",
            "521223.0\n",
            "1486421.25\n",
            "112463.1484375\n",
            "435755.46875\n",
            "244827.484375\n",
            "1192697.0\n",
            "369522.71875\n",
            "291401.71875\n",
            "48294.453125\n",
            "236077.109375\n",
            "59632.84765625\n",
            "45150.05078125\n",
            "698072.5625\n",
            "215472.703125\n",
            "61728.86328125\n",
            "13497.7275390625\n",
            "123980.140625\n",
            "85877.0390625\n",
            "218914.25\n",
            "12331.4501953125\n",
            "62937.82421875\n",
            "17339.11328125\n",
            "53555.46875\n",
            "554729.6875\n",
            "206263.21875\n",
            "48225.9453125\n",
            "16750.96875\n",
            "146282.625\n",
            "72676.6484375\n",
            "212935.390625\n",
            "4103.640625\n",
            "35455.9609375\n",
            "8232.775390625\n",
            "47855.01953125\n",
            "439162.6875\n",
            "213854.484375\n",
            "240536.546875\n",
            "595617.8125\n",
            "124977.5078125\n",
            "strain 1.7303098440170288\n",
            "51698.6171875\n",
            "472.9820861816406\n",
            "5416.99560546875\n",
            "12385.6416015625\n",
            "21767.5234375\n",
            "5193.0068359375\n",
            "2167.043212890625\n",
            "539.6649169921875\n",
            "6309.5693359375\n",
            "15202.8720703125\n",
            "5820.244140625\n",
            "10747.78515625\n",
            "37238.4140625\n",
            "6508.19482421875\n",
            "6816.724609375\n",
            "26470.484375\n",
            "3121.8466796875\n",
            "1736.21826171875\n",
            "1195.3397216796875\n",
            "5486.451171875\n",
            "8422.2568359375\n",
            "3051.109375\n",
            "5872.3203125\n",
            "21444.310546875\n",
            "3986.406494140625\n",
            "5126.2001953125\n",
            "18427.765625\n",
            "1726.4271240234375\n",
            "1703.6456298828125\n",
            "1031.0509033203125\n",
            "5780.33447265625\n",
            "1209.278564453125\n",
            "1839.2576904296875\n",
            "1405.0543212890625\n",
            "7698.74609375\n",
            "1031.8631591796875\n",
            "2927.156005859375\n",
            "12747.2451171875\n",
            "1428.708740234375\n",
            "807.7527465820312\n",
            "195.8218994140625\n",
            "2692.349609375\n",
            "1292.546142578125\n",
            "1422.486328125\n",
            "786.7232055664062\n",
            "4967.58837890625\n",
            "844.2728271484375\n",
            "2613.01123046875\n",
            "12630.798828125\n",
            "1248.61572265625\n",
            "508.0376892089844\n",
            "97.38026428222656\n",
            "1896.7471923828125\n",
            "1086.16943359375\n",
            "1290.2296142578125\n",
            "657.6486206054688\n",
            "2726.548095703125\n",
            "382.6402282714844\n",
            "2920.508056640625\n",
            "12649.451171875\n",
            "1289.353759765625\n",
            "198.58688354492188\n",
            "33.8489990234375\n",
            "1027.2210693359375\n",
            "469.1507263183594\n",
            "1318.4429931640625\n",
            "381.0410461425781\n",
            "2743.291015625\n",
            "454.47711181640625\n",
            "3587.68896484375\n",
            "12454.7021484375\n",
            "1292.6290283203125\n",
            "8993.3017578125\n",
            "31913.201171875\n",
            "3030.72412109375\n",
            "170271.46875\n",
            "70244.2421875\n",
            "4805.86328125\n",
            "52612.39453125\n",
            "35815.0546875\n",
            "211365.421875\n",
            "79214.6328125\n",
            "39449.16796875\n",
            "10697.3173828125\n",
            "37687.6328125\n",
            "9612.7265625\n",
            "10350.6552734375\n",
            "121711.9296875\n",
            "30564.146484375\n",
            "11873.2998046875\n",
            "4496.17333984375\n",
            "33685.03515625\n",
            "10395.345703125\n",
            "29478.302734375\n",
            "2055.074462890625\n",
            "10597.4130859375\n",
            "2692.541259765625\n",
            "4206.466796875\n",
            "89991.9375\n",
            "29962.5546875\n",
            "7350.55224609375\n",
            "4249.271484375\n",
            "21112.025390625\n",
            "15668.8056640625\n",
            "27134.4765625\n",
            "988.7542114257812\n",
            "5350.9189453125\n",
            "1131.582763671875\n",
            "2945.587890625\n",
            "58992.4140625\n",
            "28129.234375\n",
            "130621.625\n",
            "239349.53125\n",
            "48201.7734375\n",
            "strain 0.4301413893699646\n",
            "172146.078125\n",
            "958.8868408203125\n",
            "50879.1484375\n",
            "61989.01171875\n",
            "145886.5\n",
            "30124.90625\n",
            "2244.928955078125\n",
            "2537.858642578125\n",
            "17365.345703125\n",
            "39372.86328125\n",
            "34344.4140625\n",
            "9810.5791015625\n",
            "50833.3515625\n",
            "10785.1044921875\n",
            "8606.2578125\n",
            "107708.3984375\n",
            "29789.263671875\n",
            "13083.451171875\n",
            "5735.09375\n",
            "45661.5078125\n",
            "29287.6640625\n",
            "25428.177734375\n",
            "8485.3681640625\n",
            "42102.60546875\n",
            "7739.32568359375\n",
            "11248.341796875\n",
            "76006.5\n",
            "21280.7890625\n",
            "6528.71240234375\n",
            "2012.2236328125\n",
            "24230.203125\n",
            "38525.53515625\n",
            "20250.91796875\n",
            "2264.825439453125\n",
            "13717.306640625\n",
            "2527.621826171875\n",
            "6023.86376953125\n",
            "70859.6328125\n",
            "18993.787109375\n",
            "2782.015869140625\n",
            "1366.9107666015625\n",
            "13088.7705078125\n",
            "11913.1025390625\n",
            "18805.91015625\n",
            "1118.8865966796875\n",
            "6247.64990234375\n",
            "1382.5885009765625\n",
            "4364.3212890625\n",
            "64846.84375\n",
            "18092.28125\n",
            "2932.70458984375\n",
            "463.0659484863281\n",
            "13267.9375\n",
            "13446.837890625\n",
            "16152.78515625\n",
            "1353.4755859375\n",
            "6085.369140625\n",
            "1032.9735107421875\n",
            "15167.755859375\n",
            "82304.1484375\n",
            "17474.95703125\n",
            "1089.8890380859375\n",
            "51.053016662597656\n",
            "6314.11962890625\n",
            "9104.095703125\n",
            "17667.552734375\n",
            "1569.040283203125\n",
            "9708.5380859375\n",
            "1595.5394287109375\n",
            "18330.822265625\n",
            "77333.28125\n",
            "17557.91796875\n",
            "42058.66015625\n",
            "309296.46875\n",
            "58011.4296875\n",
            "974732.25\n",
            "819072.8125\n",
            "61727.45703125\n",
            "162165.453125\n",
            "140453.5625\n",
            "565848.875\n",
            "202180.328125\n",
            "237031.8125\n",
            "17008.703125\n",
            "110211.5390625\n",
            "29677.615234375\n",
            "28901.775390625\n",
            "496667.90625\n",
            "136475.4375\n",
            "25395.09765625\n",
            "2921.147705078125\n",
            "80540.2734375\n",
            "76695.3125\n",
            "145712.40625\n",
            "3218.740966796875\n",
            "13080.2744140625\n",
            "3389.962890625\n",
            "17653.51953125\n",
            "449286.375\n",
            "144887.515625\n",
            "20649.130859375\n",
            "28989.328125\n",
            "99825.1328125\n",
            "91380.40625\n",
            "155495.578125\n",
            "4700.7646484375\n",
            "13578.6630859375\n",
            "2850.263671875\n",
            "7887.69384765625\n",
            "282442.15625\n",
            "155155.859375\n",
            "279272.3125\n",
            "556051.625\n",
            "116673.9375\n",
            "strain 2.249877691268921\n",
            "37817.125\n",
            "288.0646057128906\n",
            "5826.17041015625\n",
            "8042.99267578125\n",
            "26409.51171875\n",
            "4101.8427734375\n",
            "1841.8997802734375\n",
            "1549.583984375\n",
            "4308.00732421875\n",
            "4108.95361328125\n",
            "3971.162841796875\n",
            "4619.84716796875\n",
            "20868.19921875\n",
            "2472.93115234375\n",
            "3405.31005859375\n",
            "16559.10546875\n",
            "2875.758056640625\n",
            "676.0336303710938\n",
            "1117.5240478515625\n",
            "3689.128662109375\n",
            "2926.346923828125\n",
            "2468.536865234375\n",
            "1471.2154541015625\n",
            "8480.287109375\n",
            "978.9536743164062\n",
            "1445.9779052734375\n",
            "12091.498046875\n",
            "2573.806884765625\n",
            "846.3672485351562\n",
            "460.09881591796875\n",
            "3632.933837890625\n",
            "4012.445556640625\n",
            "2393.025390625\n",
            "774.0360107421875\n",
            "3135.164794921875\n",
            "379.617431640625\n",
            "940.1549682617188\n",
            "10115.6015625\n",
            "2310.995849609375\n",
            "499.89959716796875\n",
            "327.6277160644531\n",
            "2529.421875\n",
            "1555.85888671875\n",
            "2591.794677734375\n",
            "320.3121032714844\n",
            "1979.5234375\n",
            "400.78765869140625\n",
            "1438.972412109375\n",
            "7936.1767578125\n",
            "2202.335205078125\n",
            "429.7465515136719\n",
            "93.8782958984375\n",
            "2245.7451171875\n",
            "1414.205322265625\n",
            "1991.279296875\n",
            "123.64501190185547\n",
            "829.698486328125\n",
            "154.35548400878906\n",
            "2344.021240234375\n",
            "9250.4931640625\n",
            "2129.456298828125\n",
            "176.80184936523438\n",
            "15.169957160949707\n",
            "985.851318359375\n",
            "1038.0745849609375\n",
            "2143.354736328125\n",
            "323.43170166015625\n",
            "2135.300537109375\n",
            "388.84515380859375\n",
            "3823.54736328125\n",
            "11765.529296875\n",
            "2185.200439453125\n",
            "5827.451171875\n",
            "42574.76171875\n",
            "7294.49365234375\n",
            "318931.59375\n",
            "106005.8203125\n",
            "7083.18310546875\n",
            "49911.5703125\n",
            "99423.1328125\n",
            "163195.734375\n",
            "92164.296875\n",
            "40896.72265625\n",
            "10606.0693359375\n",
            "49060.39453125\n",
            "7575.953125\n",
            "12168.57421875\n",
            "135053.21875\n",
            "25161.76953125\n",
            "8315.923828125\n",
            "3120.998291015625\n",
            "23046.275390625\n",
            "12381.251953125\n",
            "21144.7890625\n",
            "978.1929321289062\n",
            "4356.68798828125\n",
            "992.5718383789062\n",
            "4771.62646484375\n",
            "70831.2109375\n",
            "26229.353515625\n",
            "5860.60205078125\n",
            "5924.6416015625\n",
            "14516.75\n",
            "10766.15234375\n",
            "28585.271484375\n",
            "1575.022216796875\n",
            "6355.3466796875\n",
            "1340.080810546875\n",
            "5979.2490234375\n",
            "114197.796875\n",
            "27766.94140625\n",
            "62818.53125\n",
            "117115.4453125\n",
            "24120.5390625\n",
            "strain 1.318980097770691\n",
            "229715.53125\n",
            "2682.557373046875\n",
            "67507.3828125\n",
            "156030.765625\n",
            "208976.25\n",
            "90935.890625\n",
            "9991.818359375\n",
            "3995.096435546875\n",
            "49026.26953125\n",
            "110747.1796875\n",
            "85241.390625\n",
            "21866.931640625\n",
            "124632.3046875\n",
            "28630.126953125\n",
            "26032.576171875\n",
            "245580.890625\n",
            "63075.5078125\n",
            "21031.265625\n",
            "9698.4052734375\n",
            "109877.8984375\n",
            "83865.6171875\n",
            "52697.71484375\n",
            "15696.5517578125\n",
            "88634.671875\n",
            "15813.515625\n",
            "23315.16015625\n",
            "158855.0625\n",
            "41231.2265625\n",
            "14026.83984375\n",
            "3696.636474609375\n",
            "48611.43359375\n",
            "60921.96484375\n",
            "38883.64453125\n",
            "10074.861328125\n",
            "33899.30859375\n",
            "6544.34033203125\n",
            "15147.51171875\n",
            "136345.5\n",
            "36113.328125\n",
            "7744.29296875\n",
            "3152.062744140625\n",
            "26878.23828125\n",
            "28216.21875\n",
            "35347.3046875\n",
            "3857.156982421875\n",
            "19592.9375\n",
            "4300.10205078125\n",
            "8310.24609375\n",
            "147833.0\n",
            "34125.2734375\n",
            "6763.74169921875\n",
            "4337.65966796875\n",
            "26036.42578125\n",
            "33314.8984375\n",
            "34732.91015625\n",
            "1405.3369140625\n",
            "10081.2265625\n",
            "1974.6104736328125\n",
            "30345.126953125\n",
            "163297.09375\n",
            "33321.37890625\n",
            "1804.17333984375\n",
            "77.70858764648438\n",
            "11435.7197265625\n",
            "13571.2900390625\n",
            "33835.01953125\n",
            "2585.6005859375\n",
            "15206.376953125\n",
            "2568.290283203125\n",
            "32465.97265625\n",
            "154087.625\n",
            "33659.20703125\n",
            "77948.125\n",
            "579284.5625\n",
            "105598.3125\n",
            "994482.5\n",
            "1424959.0\n",
            "105147.5078125\n",
            "528299.75\n",
            "232101.109375\n",
            "1191555.375\n",
            "282978.4375\n",
            "360085.71875\n",
            "21055.212890625\n",
            "180931.53125\n",
            "48608.5390625\n",
            "40781.4921875\n",
            "729008.75\n",
            "219705.3125\n",
            "21669.935546875\n",
            "10469.9658203125\n",
            "94120.515625\n",
            "137539.3125\n",
            "232310.796875\n",
            "5552.3486328125\n",
            "26415.228515625\n",
            "6188.75537109375\n",
            "38370.5078125\n",
            "599536.1875\n",
            "228545.890625\n",
            "40042.765625\n",
            "42541.96484375\n",
            "156345.703125\n",
            "112173.4140625\n",
            "243114.625\n",
            "10838.9375\n",
            "42503.06640625\n",
            "10348.6474609375\n",
            "29602.8671875\n",
            "506267.96875\n",
            "249547.078125\n",
            "247699.5\n",
            "806456.1875\n",
            "171070.90625\n",
            "strain 1.6237159967422485\n",
            "183885.015625\n",
            "726.5118408203125\n",
            "29895.296875\n",
            "65811.953125\n",
            "93422.453125\n",
            "21740.44140625\n",
            "6086.72412109375\n",
            "1817.073974609375\n",
            "21687.263671875\n",
            "58512.0859375\n",
            "24895.6171875\n",
            "10365.470703125\n",
            "47991.46484375\n",
            "8793.037109375\n",
            "11364.982421875\n",
            "108511.4375\n",
            "21905.427734375\n",
            "14734.775390625\n",
            "3557.84423828125\n",
            "38089.71484375\n",
            "41648.16015625\n",
            "17462.837890625\n",
            "6954.830078125\n",
            "33940.83203125\n",
            "6533.98291015625\n",
            "8161.4580078125\n",
            "55396.88671875\n",
            "13628.8955078125\n",
            "4439.89306640625\n",
            "2878.85986328125\n",
            "16701.638671875\n",
            "21490.201171875\n",
            "13277.5185546875\n",
            "2688.96826171875\n",
            "15938.6484375\n",
            "2914.554443359375\n",
            "5846.00439453125\n",
            "54431.421875\n",
            "11400.8388671875\n",
            "2729.275390625\n",
            "1245.4246826171875\n",
            "10570.4912109375\n",
            "7223.203125\n",
            "12417.2685546875\n",
            "1584.9263916015625\n",
            "6453.2275390625\n",
            "1021.1531982421875\n",
            "1843.0614013671875\n",
            "53756.76171875\n",
            "10688.162109375\n",
            "1790.594482421875\n",
            "455.3978271484375\n",
            "7570.50390625\n",
            "6490.04052734375\n",
            "11261.2802734375\n",
            "678.5057983398438\n",
            "4362.4013671875\n",
            "742.1831665039062\n",
            "9121.6015625\n",
            "49669.5\n",
            "10184.6884765625\n",
            "398.5411376953125\n",
            "54.59611892700195\n",
            "2087.56689453125\n",
            "4381.59912109375\n",
            "9550.787109375\n",
            "884.005615234375\n",
            "4615.21240234375\n",
            "788.6868286132812\n",
            "8332.041015625\n",
            "57529.625\n",
            "10059.23828125\n",
            "34340.6953125\n",
            "178030.171875\n",
            "30450.47265625\n",
            "330947.53125\n",
            "438257.96875\n",
            "31720.927734375\n",
            "111234.3515625\n",
            "67310.1796875\n",
            "730584.625\n",
            "139178.078125\n",
            "115080.2109375\n",
            "19171.560546875\n",
            "75265.96875\n",
            "18875.296875\n",
            "14602.6513671875\n",
            "201381.765625\n",
            "52643.5234375\n",
            "9684.0654296875\n",
            "3449.913818359375\n",
            "50760.27734375\n",
            "31160.298828125\n",
            "53044.5546875\n",
            "1605.5894775390625\n",
            "9847.7685546875\n",
            "2267.770751953125\n",
            "10522.67578125\n",
            "125187.40625\n",
            "52553.08984375\n",
            "13910.3203125\n",
            "6658.84326171875\n",
            "49146.03125\n",
            "30480.533203125\n",
            "54803.8671875\n",
            "1373.1654052734375\n",
            "9229.9482421875\n",
            "2065.138427734375\n",
            "11342.1826171875\n",
            "126338.7265625\n",
            "55273.23046875\n",
            "125736.140625\n",
            "225489.46875\n",
            "45580.578125\n",
            "strain 0.8595483303070068\n",
            "109944.4375\n",
            "1970.0487060546875\n",
            "35711.80859375\n",
            "43838.92578125\n",
            "147149.09375\n",
            "33333.703125\n",
            "12500.681640625\n",
            "18228.7890625\n",
            "52027.6953125\n",
            "43297.87109375\n",
            "49235.625\n",
            "30490.7421875\n",
            "123469.5546875\n",
            "24839.32421875\n",
            "19638.751953125\n",
            "69866.421875\n",
            "6117.412109375\n",
            "4280.93896484375\n",
            "6285.59619140625\n",
            "19391.693359375\n",
            "8116.9208984375\n",
            "6222.57958984375\n",
            "8172.93115234375\n",
            "39531.14453125\n",
            "5973.75537109375\n",
            "6689.349609375\n",
            "33513.8671875\n",
            "1976.413330078125\n",
            "1470.9678955078125\n",
            "2899.87744140625\n",
            "5688.74462890625\n",
            "4814.16357421875\n",
            "2652.32861328125\n",
            "3889.17236328125\n",
            "18706.095703125\n",
            "3128.504150390625\n",
            "8907.396484375\n",
            "29626.744140625\n",
            "1645.9305419921875\n",
            "1227.9344482421875\n",
            "1714.723876953125\n",
            "3928.677734375\n",
            "2914.3544921875\n",
            "2712.461181640625\n",
            "1747.9736328125\n",
            "7160.841796875\n",
            "989.1578979492188\n",
            "7090.8193359375\n",
            "28527.775390625\n",
            "1820.614013671875\n",
            "362.8210144042969\n",
            "1019.7552490234375\n",
            "2164.3369140625\n",
            "2830.13330078125\n",
            "2761.71630859375\n",
            "1042.674072265625\n",
            "6084.67431640625\n",
            "1109.361328125\n",
            "6575.890625\n",
            "28323.857421875\n",
            "1668.734375\n",
            "200.1138916015625\n",
            "35.401981353759766\n",
            "1231.0546875\n",
            "1501.8885498046875\n",
            "3506.553466796875\n",
            "2272.03564453125\n",
            "12147.576171875\n",
            "2234.581298828125\n",
            "7979.08642578125\n",
            "27804.76171875\n",
            "1693.5894775390625\n",
            "20502.08203125\n",
            "102513.1796875\n",
            "9914.1044921875\n",
            "701317.125\n",
            "333711.5625\n",
            "15986.7685546875\n",
            "294590.71875\n",
            "198753.734375\n",
            "1369427.0\n",
            "378981.0\n",
            "162823.84375\n",
            "45871.09765625\n",
            "185043.359375\n",
            "44362.88671875\n",
            "32948.84375\n",
            "412625.84375\n",
            "40115.1328125\n",
            "8482.2822265625\n",
            "3695.376220703125\n",
            "22331.22265625\n",
            "21746.048828125\n",
            "41054.765625\n",
            "2633.541015625\n",
            "11743.3505859375\n",
            "2500.341064453125\n",
            "23317.3203125\n",
            "113094.703125\n",
            "41066.46875\n",
            "15711.0556640625\n",
            "30167.740234375\n",
            "35245.0390625\n",
            "29437.8203125\n",
            "52096.35546875\n",
            "3401.110107421875\n",
            "11605.8125\n",
            "2507.700439453125\n",
            "17694.498046875\n",
            "108755.59375\n",
            "44559.9453125\n",
            "109701.4296875\n",
            "200010.59375\n",
            "33589.453125\n",
            "strain 2.510282278060913\n",
            "21832.23046875\n",
            "106.70989227294922\n",
            "13467.28515625\n",
            "6957.6259765625\n",
            "30876.3125\n",
            "2680.0595703125\n",
            "2321.274169921875\n",
            "37.6137809753418\n",
            "5966.66357421875\n",
            "7946.35498046875\n",
            "3384.939697265625\n",
            "1772.886474609375\n",
            "9174.3642578125\n",
            "1685.3974609375\n",
            "1823.6719970703125\n",
            "19182.119140625\n",
            "3379.441162109375\n",
            "612.2218627929688\n",
            "18.58128547668457\n",
            "3730.414794921875\n",
            "6367.69384765625\n",
            "3288.425537109375\n",
            "403.2491149902344\n",
            "5620.24365234375\n",
            "1030.0096435546875\n",
            "790.7951049804688\n",
            "17539.890625\n",
            "3275.92578125\n",
            "2589.94140625\n",
            "47.91807556152344\n",
            "8562.279296875\n",
            "4231.78466796875\n",
            "3075.332763671875\n",
            "1338.8431396484375\n",
            "4994.81201171875\n",
            "919.783447265625\n",
            "3663.347900390625\n",
            "13435.5830078125\n",
            "3088.74267578125\n",
            "1964.421630859375\n",
            "10.441452980041504\n",
            "5726.30859375\n",
            "3301.897705078125\n",
            "2872.682861328125\n",
            "759.7201538085938\n",
            "4235.07177734375\n",
            "773.447021484375\n",
            "3786.922119140625\n",
            "13196.4296875\n",
            "3033.27783203125\n",
            "1427.7578125\n",
            "5.681884765625\n",
            "5457.77734375\n",
            "3871.990966796875\n",
            "3068.0771484375\n",
            "451.5399475097656\n",
            "3028.671875\n",
            "475.0785217285156\n",
            "5785.736328125\n",
            "15257.439453125\n",
            "3002.087158203125\n",
            "426.6493835449219\n",
            "2.827632188796997\n",
            "2662.213623046875\n",
            "1332.32861328125\n",
            "2913.407958984375\n",
            "937.5185546875\n",
            "7016.39404296875\n",
            "1222.019775390625\n",
            "8675.443359375\n",
            "17461.419921875\n",
            "3165.650390625\n",
            "23117.16015625\n",
            "70817.78125\n",
            "11459.583984375\n",
            "492083.03125\n",
            "160105.078125\n",
            "11696.1767578125\n",
            "103341.1484375\n",
            "139637.84375\n",
            "239964.671875\n",
            "126491.0390625\n",
            "51091.2890625\n",
            "7972.32958984375\n",
            "43453.76171875\n",
            "10123.1318359375\n",
            "8653.8076171875\n",
            "124657.46875\n",
            "33899.609375\n",
            "10011.95703125\n",
            "4192.28955078125\n",
            "38305.98828125\n",
            "26580.40625\n",
            "35733.61328125\n",
            "1186.406982421875\n",
            "8084.78515625\n",
            "2050.68798828125\n",
            "9795.2216796875\n",
            "91494.609375\n",
            "35740.453125\n",
            "6798.18603515625\n",
            "6257.1298828125\n",
            "28757.109375\n",
            "41443.8984375\n",
            "36335.84765625\n",
            "4657.6748046875\n",
            "20430.263671875\n",
            "4928.6728515625\n",
            "12063.376953125\n",
            "114759.203125\n",
            "41042.2265625\n",
            "86808.921875\n",
            "179033.953125\n",
            "38521.4453125\n",
            "strain 0.7539457678794861\n",
            "176781.953125\n",
            "1830.9205322265625\n",
            "50770.70703125\n",
            "102892.734375\n",
            "166143.40625\n",
            "56896.78125\n",
            "4692.02099609375\n",
            "4604.09375\n",
            "28339.75\n",
            "67059.265625\n",
            "54950.51171875\n",
            "35165.5859375\n",
            "104428.75\n",
            "12884.0771484375\n",
            "24085.80078125\n",
            "161052.921875\n",
            "41849.48828125\n",
            "16816.13671875\n",
            "8847.87109375\n",
            "65497.859375\n",
            "50135.30859375\n",
            "34588.10546875\n",
            "7425.11083984375\n",
            "47400.8359375\n",
            "7855.4208984375\n",
            "19183.404296875\n",
            "130375.2421875\n",
            "32022.88671875\n",
            "12301.564453125\n",
            "3765.303955078125\n",
            "48890.95703125\n",
            "58605.75390625\n",
            "28258.65234375\n",
            "9003.79296875\n",
            "30077.580078125\n",
            "5492.7685546875\n",
            "5974.572265625\n",
            "99481.828125\n",
            "26553.388671875\n",
            "5064.06494140625\n",
            "2369.330810546875\n",
            "25253.236328125\n",
            "13480.669921875\n",
            "24318.419921875\n",
            "4326.27197265625\n",
            "17107.267578125\n",
            "3380.691162109375\n",
            "7351.12158203125\n",
            "89523.1875\n",
            "24430.4453125\n",
            "5207.275390625\n",
            "737.1784057617188\n",
            "23216.65234375\n",
            "16630.3125\n",
            "23158.66015625\n",
            "1390.88720703125\n",
            "9524.4296875\n",
            "1810.1417236328125\n",
            "16899.6484375\n",
            "102612.4296875\n",
            "23658.283203125\n",
            "2184.141357421875\n",
            "169.42263793945312\n",
            "11878.9072265625\n",
            "12034.8037109375\n",
            "25160.9140625\n",
            "3300.861328125\n",
            "20100.353515625\n",
            "3468.787109375\n",
            "26901.84375\n",
            "103446.96875\n",
            "24112.060546875\n",
            "71711.875\n",
            "441525.28125\n",
            "78676.765625\n",
            "300574.90625\n",
            "1085119.375\n",
            "77491.375\n",
            "134772.90625\n",
            "198862.484375\n",
            "1376815.0\n",
            "333534.59375\n",
            "322800.03125\n",
            "35946.67578125\n",
            "198096.578125\n",
            "52199.375\n",
            "34401.234375\n",
            "459700.375\n",
            "145826.109375\n",
            "13971.2841796875\n",
            "4096.72705078125\n",
            "66117.15625\n",
            "84510.203125\n",
            "147295.25\n",
            "3455.262939453125\n",
            "16086.9384765625\n",
            "3400.647705078125\n",
            "20379.873046875\n",
            "394238.125\n",
            "150273.15625\n",
            "25043.853515625\n",
            "16771.568359375\n",
            "96165.9921875\n",
            "64441.57421875\n",
            "158418.015625\n",
            "2906.122802734375\n",
            "17236.669921875\n",
            "4139.15576171875\n",
            "12511.00390625\n",
            "304812.40625\n",
            "155514.53125\n",
            "165246.828125\n",
            "488829.90625\n",
            "102684.7265625\n",
            "strain 1.5025759935379028\n",
            "961751.5\n",
            "3249.350341796875\n",
            "169647.953125\n",
            "221518.484375\n",
            "453246.40625\n",
            "78223.234375\n",
            "19413.53515625\n",
            "11631.2841796875\n",
            "73477.828125\n",
            "158717.734375\n",
            "91740.34375\n",
            "46933.6171875\n",
            "202021.4375\n",
            "36351.4296875\n",
            "36420.92578125\n",
            "310174.03125\n",
            "64000.62890625\n",
            "41259.359375\n",
            "11847.916015625\n",
            "106464.78125\n",
            "96428.609375\n",
            "54906.73828125\n",
            "16340.802734375\n",
            "92489.09375\n",
            "17743.361328125\n",
            "24534.1953125\n",
            "202987.6875\n",
            "48226.1328125\n",
            "14189.2490234375\n",
            "9849.3203125\n",
            "58041.20703125\n",
            "49021.07421875\n",
            "47863.7578125\n",
            "13391.205078125\n",
            "59061.9609375\n",
            "10629.4521484375\n",
            "22810.640625\n",
            "190110.8125\n",
            "41916.0390625\n",
            "7898.962890625\n",
            "3205.432861328125\n",
            "30412.947265625\n",
            "26089.919921875\n",
            "42779.4375\n",
            "5813.3076171875\n",
            "28248.181640625\n",
            "5372.14013671875\n",
            "7608.93603515625\n",
            "177794.609375\n",
            "38861.55078125\n",
            "7112.11962890625\n",
            "3129.788818359375\n",
            "27653.8671875\n",
            "16616.47265625\n",
            "39444.97265625\n",
            "1942.0511474609375\n",
            "11064.1044921875\n",
            "2071.74072265625\n",
            "24914.388671875\n",
            "167906.34375\n",
            "37609.7421875\n",
            "2166.054931640625\n",
            "175.42820739746094\n",
            "10050.45703125\n",
            "7918.04248046875\n",
            "36070.72265625\n",
            "2656.54541015625\n",
            "15353.1015625\n",
            "2597.20263671875\n",
            "28289.92578125\n",
            "181786.09375\n",
            "37962.80859375\n",
            "110938.1484375\n",
            "683588.8125\n",
            "123407.71875\n",
            "1052074.125\n",
            "1541322.25\n",
            "117878.8984375\n",
            "516582.875\n",
            "212075.71875\n",
            "1095959.625\n",
            "251288.09375\n",
            "316952.375\n",
            "31040.701171875\n",
            "169548.421875\n",
            "47364.4609375\n",
            "48453.7265625\n",
            "670996.0625\n",
            "179558.40625\n",
            "38956.84375\n",
            "5107.3212890625\n",
            "81060.5390625\n",
            "81846.8359375\n",
            "184884.296875\n",
            "4904.77490234375\n",
            "19355.484375\n",
            "4409.140625\n",
            "10934.7568359375\n",
            "525233.375\n",
            "191062.03125\n",
            "57442.96875\n",
            "88834.9375\n",
            "141339.0625\n",
            "80643.2578125\n",
            "212737.34375\n",
            "6406.904296875\n",
            "23437.19140625\n",
            "5322.203125\n",
            "5974.197265625\n",
            "452971.5625\n",
            "216614.234375\n",
            "240410.484375\n",
            "673969.6875\n",
            "141639.421875\n",
            "strain 1.7907963991165161\n",
            "73307.75\n",
            "727.5123901367188\n",
            "38652.8828125\n",
            "65086.6171875\n",
            "77384.8828125\n",
            "22906.04296875\n",
            "6580.6875\n",
            "843.1868896484375\n",
            "20577.759765625\n",
            "53001.75390625\n",
            "23697.240234375\n",
            "17123.779296875\n",
            "53893.54296875\n",
            "9165.7890625\n",
            "13868.5966796875\n",
            "94480.71875\n",
            "20968.056640625\n",
            "12598.392578125\n",
            "1692.0137939453125\n",
            "33461.71484375\n",
            "28113.26171875\n",
            "17722.27734375\n",
            "12054.00390625\n",
            "39842.578125\n",
            "7368.1259765625\n",
            "11887.7646484375\n",
            "56818.765625\n",
            "13943.3466796875\n",
            "3047.269287109375\n",
            "1007.5057983398438\n",
            "12571.4111328125\n",
            "15658.3779296875\n",
            "13564.0859375\n",
            "1833.53125\n",
            "11064.6025390625\n",
            "2004.7447509765625\n",
            "4839.85986328125\n",
            "54013.01953125\n",
            "13171.1640625\n",
            "1983.056884765625\n",
            "1342.8345947265625\n",
            "8576.7177734375\n",
            "6960.8515625\n",
            "13489.267578125\n",
            "1062.9561767578125\n",
            "6374.8193359375\n",
            "1177.0635986328125\n",
            "5277.04541015625\n",
            "52035.65234375\n",
            "12811.6025390625\n",
            "1424.6715087890625\n",
            "682.3592529296875\n",
            "7187.419921875\n",
            "5693.943359375\n",
            "12321.689453125\n",
            "572.4935302734375\n",
            "4054.27099609375\n",
            "670.2371826171875\n",
            "16722.775390625\n",
            "65044.4375\n",
            "12491.96875\n",
            "332.3305358886719\n",
            "24.970043182373047\n",
            "1735.2327880859375\n",
            "3114.4287109375\n",
            "12841.021484375\n",
            "1012.1167602539062\n",
            "5134.25537109375\n",
            "871.2330322265625\n",
            "16494.2734375\n",
            "68241.2109375\n",
            "12465.3349609375\n",
            "48391.90234375\n",
            "212149.90625\n",
            "36930.7890625\n",
            "146924.1875\n",
            "488201.875\n",
            "36795.86328125\n",
            "145926.59375\n",
            "72634.4296875\n",
            "450003.15625\n",
            "149916.921875\n",
            "107812.8671875\n",
            "15638.755859375\n",
            "73195.4375\n",
            "21817.041015625\n",
            "15779.4482421875\n",
            "253073.21875\n",
            "68790.78125\n",
            "21928.49609375\n",
            "5577.12646484375\n",
            "50164.94921875\n",
            "35778.6171875\n",
            "74299.3828125\n",
            "3418.273193359375\n",
            "18728.2734375\n",
            "4958.2958984375\n",
            "17806.32421875\n",
            "179264.015625\n",
            "70083.2109375\n",
            "20028.97265625\n",
            "7263.63037109375\n",
            "55033.58984375\n",
            "48858.0078125\n",
            "76518.25\n",
            "1837.8253173828125\n",
            "11093.40625\n",
            "2372.81298828125\n",
            "20193.134765625\n",
            "155456.203125\n",
            "73557.359375\n",
            "95690.3828125\n",
            "209629.109375\n",
            "42297.10546875\n",
            "strain 0.8523504137992859\n",
            "53995.87890625\n",
            "1168.834716796875\n",
            "31114.83203125\n",
            "67639.1015625\n",
            "90961.5\n",
            "34341.515625\n",
            "15007.88671875\n",
            "10485.4716796875\n",
            "57182.859375\n",
            "44933.05078125\n",
            "39230.6328125\n",
            "38274.328125\n",
            "119820.8984375\n",
            "17861.91796875\n",
            "23269.201171875\n",
            "83506.1640625\n",
            "17674.341796875\n",
            "7828.16650390625\n",
            "4593.00927734375\n",
            "26012.2109375\n",
            "28207.51953125\n",
            "17168.259765625\n",
            "10494.8330078125\n",
            "41437.53125\n",
            "7104.029296875\n",
            "9984.9443359375\n",
            "68523.3671875\n",
            "14343.025390625\n",
            "6548.287109375\n",
            "719.5916748046875\n",
            "27752.384765625\n",
            "20200.755859375\n",
            "13512.697265625\n",
            "12232.02734375\n",
            "34761.53125\n",
            "5912.01806640625\n",
            "3862.091796875\n",
            "43533.17578125\n",
            "10528.794921875\n",
            "3822.880615234375\n",
            "369.9252624511719\n",
            "13585.015625\n",
            "7484.80126953125\n",
            "10438.0966796875\n",
            "4644.701171875\n",
            "14856.4052734375\n",
            "2889.189453125\n",
            "3072.465576171875\n",
            "33190.03515625\n",
            "9112.3505859375\n",
            "2368.1142578125\n",
            "319.3417053222656\n",
            "9493.21875\n",
            "7230.966796875\n",
            "9206.2529296875\n",
            "614.0242919921875\n",
            "3371.272705078125\n",
            "608.7127075195312\n",
            "8185.076171875\n",
            "39609.70703125\n",
            "8902.73046875\n",
            "610.8171997070312\n",
            "42.36323928833008\n",
            "4033.492919921875\n",
            "1920.99365234375\n",
            "8643.0693359375\n",
            "746.1129150390625\n",
            "5875.02880859375\n",
            "1118.5125732421875\n",
            "14179.9296875\n",
            "44887.8203125\n",
            "8877.326171875\n",
            "19725.765625\n",
            "142769.34375\n",
            "26429.740234375\n",
            "410225.0\n",
            "329129.25\n",
            "24729.486328125\n",
            "136437.0625\n",
            "140771.953125\n",
            "251124.484375\n",
            "65197.515625\n",
            "85000.578125\n",
            "9064.94140625\n",
            "51099.56640625\n",
            "9890.9990234375\n",
            "10167.72265625\n",
            "177772.921875\n",
            "66419.1328125\n",
            "14797.484375\n",
            "4055.855224609375\n",
            "54233.89453125\n",
            "27160.123046875\n",
            "70005.3359375\n",
            "1975.75634765625\n",
            "9862.85546875\n",
            "2251.464599609375\n",
            "5087.86474609375\n",
            "183429.796875\n",
            "65606.0546875\n",
            "14313.5146484375\n",
            "11750.58984375\n",
            "39440.40234375\n",
            "27241.130859375\n",
            "69366.1796875\n",
            "4090.566162109375\n",
            "17935.359375\n",
            "4227.89111328125\n",
            "5748.5947265625\n",
            "162597.1875\n",
            "71958.9921875\n",
            "87131.359375\n",
            "241762.15625\n",
            "52977.51171875\n",
            "strain 1.036803126335144\n",
            "241459.875\n",
            "1499.693115234375\n",
            "69540.84375\n",
            "168628.375\n",
            "166600.921875\n",
            "67552.8125\n",
            "7956.17626953125\n",
            "4874.52392578125\n",
            "43177.125\n",
            "87500.3671875\n",
            "72173.4921875\n",
            "12628.330078125\n",
            "81273.96875\n",
            "16140.9482421875\n",
            "13949.91015625\n",
            "235236.671875\n",
            "54792.47265625\n",
            "27256.21875\n",
            "9599.669921875\n",
            "109235.515625\n",
            "86244.359375\n",
            "45220.39453125\n",
            "10125.5546875\n",
            "66946.53125\n",
            "11367.69921875\n",
            "18051.908203125\n",
            "151732.109375\n",
            "35524.484375\n",
            "10711.6806640625\n",
            "5914.22265625\n",
            "40490.5078125\n",
            "51639.79296875\n",
            "30182.560546875\n",
            "6906.17626953125\n",
            "29210.287109375\n",
            "5752.572265625\n",
            "11673.80078125\n",
            "131160.796875\n",
            "30760.0390625\n",
            "6019.2255859375\n",
            "3028.071044921875\n",
            "24129.7421875\n",
            "17776.49609375\n",
            "29519.23046875\n",
            "3973.0625\n",
            "15511.484375\n",
            "3180.572265625\n",
            "3943.43310546875\n",
            "102358.0390625\n",
            "28869.490234375\n",
            "4738.33984375\n",
            "1891.346923828125\n",
            "21656.48828125\n",
            "20999.048828125\n",
            "29143.57421875\n",
            "1191.3607177734375\n",
            "8591.349609375\n",
            "1727.6495361328125\n",
            "23125.798828125\n",
            "135444.609375\n",
            "28061.861328125\n",
            "1575.64501953125\n",
            "158.55564880371094\n",
            "8638.525390625\n",
            "10160.7294921875\n",
            "27914.15234375\n",
            "2186.759033203125\n",
            "12882.962890625\n",
            "2289.776123046875\n",
            "23071.150390625\n",
            "122479.0390625\n",
            "28153.337890625\n",
            "75855.5859375\n",
            "496422.875\n",
            "89070.3359375\n",
            "2355212.5\n",
            "1174662.25\n",
            "85496.0703125\n",
            "210743.5625\n",
            "114100.78125\n",
            "713783.4375\n",
            "211197.6875\n",
            "353317.46875\n",
            "27334.82421875\n",
            "156767.046875\n",
            "46058.51953125\n",
            "42069.03515625\n",
            "730623.625\n",
            "198458.8125\n",
            "18629.06640625\n",
            "11364.7529296875\n",
            "107426.5078125\n",
            "128137.5078125\n",
            "200159.59375\n",
            "5265.7783203125\n",
            "19422.111328125\n",
            "4062.46923828125\n",
            "15062.435546875\n",
            "499056.90625\n",
            "203013.796875\n",
            "32516.302734375\n",
            "18048.912109375\n",
            "151589.265625\n",
            "114545.609375\n",
            "206362.484375\n",
            "3269.484130859375\n",
            "15227.93359375\n",
            "3491.4296875\n",
            "9144.0625\n",
            "448631.0\n",
            "212978.734375\n",
            "211486.09375\n",
            "676911.875\n",
            "141006.546875\n",
            "strain 1.2830318212509155\n",
            "83943.203125\n",
            "643.2413940429688\n",
            "10647.66796875\n",
            "28420.546875\n",
            "34334.53515625\n",
            "14469.21875\n",
            "6610.5654296875\n",
            "3892.958251953125\n",
            "17076.814453125\n",
            "15856.6728515625\n",
            "11028.248046875\n",
            "7281.609375\n",
            "31405.34765625\n",
            "7085.2841796875\n",
            "8177.32861328125\n",
            "40844.59375\n",
            "5496.87744140625\n",
            "3085.25439453125\n",
            "3874.760009765625\n",
            "20209.39453125\n",
            "9309.2294921875\n",
            "4883.05908203125\n",
            "6060.64208984375\n",
            "22268.18359375\n",
            "4286.537109375\n",
            "8604.42578125\n",
            "36296.98828125\n",
            "4016.217529296875\n",
            "3529.544921875\n",
            "2209.2109375\n",
            "14681.84375\n",
            "10081.2998046875\n",
            "4483.845703125\n",
            "3981.962646484375\n",
            "11718.1787109375\n",
            "2014.62060546875\n",
            "2407.3095703125\n",
            "31784.94140625\n",
            "3597.536865234375\n",
            "2195.489501953125\n",
            "710.253173828125\n",
            "8087.2041015625\n",
            "4616.9140625\n",
            "3748.502685546875\n",
            "1132.85302734375\n",
            "4952.72412109375\n",
            "670.9307250976562\n",
            "2154.68212890625\n",
            "31184.978515625\n",
            "3503.9228515625\n",
            "1371.312744140625\n",
            "108.29301452636719\n",
            "6327.18896484375\n",
            "3500.564697265625\n",
            "3571.62890625\n",
            "493.01129150390625\n",
            "3009.083984375\n",
            "546.5723266601562\n",
            "7686.59228515625\n",
            "33481.39453125\n",
            "3527.7734375\n",
            "522.7590942382812\n",
            "37.41452407836914\n",
            "2664.984375\n",
            "1762.9595947265625\n",
            "3718.710693359375\n",
            "547.8779907226562\n",
            "4101.1796875\n",
            "765.6979370117188\n",
            "9563.9140625\n",
            "32864.94921875\n",
            "3578.0107421875\n",
            "33148.77734375\n",
            "82356.0234375\n",
            "9357.8876953125\n",
            "469999.9375\n",
            "169319.671875\n",
            "10360.9267578125\n",
            "125098.6640625\n",
            "131916.71875\n",
            "520093.15625\n",
            "172860.46875\n",
            "106211.1640625\n",
            "26843.69140625\n",
            "98367.0078125\n",
            "33024.59765625\n",
            "18936.263671875\n",
            "334544.625\n",
            "62853.03515625\n",
            "27533.0234375\n",
            "7833.7333984375\n",
            "65118.375\n",
            "29576.34375\n",
            "56457.58984375\n",
            "3259.550048828125\n",
            "14882.8037109375\n",
            "3868.48828125\n",
            "20510.462890625\n",
            "149074.421875\n",
            "60588.24609375\n",
            "16150.4443359375\n",
            "12393.064453125\n",
            "41959.12890625\n",
            "28383.3125\n",
            "54338.63671875\n",
            "3041.405029296875\n",
            "12602.5380859375\n",
            "3105.47412109375\n",
            "19158.544921875\n",
            "122169.1171875\n",
            "58861.015625\n",
            "147575.640625\n",
            "282254.125\n",
            "55475.7265625\n",
            "strain 1.1425328254699707\n",
            "97101.71875\n",
            "720.140869140625\n",
            "25558.98046875\n",
            "69915.078125\n",
            "71340.328125\n",
            "33839.62109375\n",
            "4371.3076171875\n",
            "6052.26318359375\n",
            "24143.65625\n",
            "29828.943359375\n",
            "37207.6328125\n",
            "8387.896484375\n",
            "46256.8984375\n",
            "8778.9814453125\n",
            "11457.2314453125\n",
            "113824.9765625\n",
            "28447.212890625\n",
            "13738.0869140625\n",
            "5537.19287109375\n",
            "65575.53125\n",
            "44247.6015625\n",
            "23340.236328125\n",
            "7240.43798828125\n",
            "37543.625\n",
            "6122.814453125\n",
            "12667.3017578125\n",
            "81976.7265625\n",
            "20519.408203125\n",
            "6767.45263671875\n",
            "3336.260498046875\n",
            "28272.73828125\n",
            "45722.296875\n",
            "19105.29296875\n",
            "7591.974609375\n",
            "24211.98046875\n",
            "4718.6044921875\n",
            "4847.66650390625\n",
            "64968.58203125\n",
            "17019.857421875\n",
            "2364.0830078125\n",
            "1764.317626953125\n",
            "12464.294921875\n",
            "12949.5380859375\n",
            "16410.146484375\n",
            "2649.967041015625\n",
            "10591.3505859375\n",
            "2250.49267578125\n",
            "3927.03125\n",
            "57880.92578125\n",
            "16068.6181640625\n",
            "2194.975830078125\n",
            "312.9559326171875\n",
            "11425.3837890625\n",
            "13704.970703125\n",
            "16274.9091796875\n",
            "829.8868408203125\n",
            "4782.49755859375\n",
            "795.3335571289062\n",
            "16498.87109375\n",
            "71216.2265625\n",
            "15684.0595703125\n",
            "893.1489868164062\n",
            "52.7928581237793\n",
            "4926.38134765625\n",
            "8563.3134765625\n",
            "15607.296875\n",
            "1258.2630615234375\n",
            "9385.0830078125\n",
            "1643.9913330078125\n",
            "16841.775390625\n",
            "64363.00390625\n",
            "15733.556640625\n",
            "37521.1796875\n",
            "283835.8125\n",
            "53013.234375\n",
            "1176101.125\n",
            "704727.0625\n",
            "52745.890625\n",
            "105971.1328125\n",
            "241137.40625\n",
            "429714.34375\n",
            "161253.546875\n",
            "192322.65625\n",
            "13186.1767578125\n",
            "72239.6796875\n",
            "22434.087890625\n",
            "13363.748046875\n",
            "395384.0625\n",
            "121853.75\n",
            "6218.828125\n",
            "4696.23193359375\n",
            "58103.75390625\n",
            "72572.390625\n",
            "127466.4921875\n",
            "3440.193115234375\n",
            "18250.859375\n",
            "4679.8603515625\n",
            "13599.185546875\n",
            "293556.03125\n",
            "122685.875\n",
            "14764.9609375\n",
            "12372.2939453125\n",
            "107548.65625\n",
            "64292.38671875\n",
            "128937.5234375\n",
            "1473.505859375\n",
            "9178.9697265625\n",
            "2102.966064453125\n",
            "16917.37890625\n",
            "282280.84375\n",
            "127747.9921875\n",
            "116106.96875\n",
            "367079.0\n",
            "75197.40625\n",
            "strain 0.9551671147346497\n",
            "164884.375\n",
            "3126.323486328125\n",
            "70772.3125\n",
            "177688.609375\n",
            "182910.640625\n",
            "90893.8828125\n",
            "17489.28515625\n",
            "15948.0029296875\n",
            "67239.9609375\n",
            "58465.55078125\n",
            "110757.75\n",
            "44884.49609375\n",
            "153422.75\n",
            "29917.55078125\n",
            "34703.71484375\n",
            "302588.34375\n",
            "71462.3515625\n",
            "35888.1015625\n",
            "13317.6982421875\n",
            "166326.0\n",
            "96527.6015625\n",
            "59407.4453125\n",
            "22617.080078125\n",
            "98012.671875\n",
            "16518.8515625\n",
            "26821.68359375\n",
            "180207.890625\n",
            "48082.72265625\n",
            "13461.9013671875\n",
            "9222.994140625\n",
            "48213.8125\n",
            "104334.0078125\n",
            "43345.10546875\n",
            "11164.375\n",
            "43522.05859375\n",
            "8620.921875\n",
            "15934.599609375\n",
            "168951.796875\n",
            "42256.6015625\n",
            "7028.681640625\n",
            "3191.150146484375\n",
            "29510.962890625\n",
            "29670.673828125\n",
            "39425.28125\n",
            "4959.27685546875\n",
            "20207.759765625\n",
            "4102.77490234375\n",
            "5353.7431640625\n",
            "180559.90625\n",
            "39881.03125\n",
            "5481.92138671875\n",
            "1289.1630859375\n",
            "25935.30078125\n",
            "29706.00390625\n",
            "39460.05078125\n",
            "1545.827880859375\n",
            "9262.162109375\n",
            "1609.7872314453125\n",
            "38698.8984375\n",
            "194962.03125\n",
            "38706.75390625\n",
            "1554.4130859375\n",
            "120.1834716796875\n",
            "8362.3623046875\n",
            "17834.361328125\n",
            "38357.08984375\n",
            "2297.753662109375\n",
            "16299.560546875\n",
            "2915.97314453125\n",
            "36836.203125\n",
            "194236.65625\n",
            "38761.08984375\n",
            "129681.046875\n",
            "734123.75\n",
            "131312.53125\n",
            "2162759.25\n",
            "1798721.125\n",
            "131192.0625\n",
            "322121.34375\n",
            "264802.03125\n",
            "944625.375\n",
            "201189.140625\n",
            "411815.59375\n",
            "35872.15625\n",
            "185813.15625\n",
            "55817.3515625\n",
            "43333.8046875\n",
            "767489.4375\n",
            "219604.703125\n",
            "48611.296875\n",
            "11031.21875\n",
            "102663.4296875\n",
            "102638.1171875\n",
            "232929.46875\n",
            "5320.7900390625\n",
            "22692.890625\n",
            "5376.51611328125\n",
            "8967.4345703125\n",
            "621497.875\n",
            "231583.921875\n",
            "50903.7734375\n",
            "10058.5732421875\n",
            "154825.3125\n",
            "93574.140625\n",
            "242376.515625\n",
            "5773.49462890625\n",
            "22854.998046875\n",
            "5344.4111328125\n",
            "12073.0546875\n",
            "508340.96875\n",
            "247990.140625\n",
            "272171.0625\n",
            "732405.8125\n",
            "152247.640625\n",
            "strain 1.528218150138855\n",
            "247578.921875\n",
            "4907.251953125\n",
            "103753.5078125\n",
            "173503.421875\n",
            "262119.78125\n",
            "86794.9765625\n",
            "35386.7578125\n",
            "5100.50537109375\n",
            "96752.640625\n",
            "155783.390625\n",
            "98848.9375\n",
            "37376.43359375\n",
            "145133.28125\n",
            "25826.5546875\n",
            "22881.255859375\n",
            "226183.171875\n",
            "55614.07421875\n",
            "19472.36328125\n",
            "6211.0048828125\n",
            "75921.140625\n",
            "75495.7734375\n",
            "53396.2265625\n",
            "16004.5048828125\n",
            "83113.078125\n",
            "16904.623046875\n",
            "20437.3671875\n",
            "145433.96875\n",
            "37145.73046875\n",
            "12637.82421875\n",
            "1891.754638671875\n",
            "49820.83984375\n",
            "49253.765625\n",
            "34609.8359375\n",
            "4999.830078125\n",
            "29296.71484375\n",
            "4669.25341796875\n",
            "22570.802734375\n",
            "132183.671875\n",
            "33732.8984375\n",
            "7449.15478515625\n",
            "12081.154296875\n",
            "26093.015625\n",
            "21479.271484375\n",
            "34416.89453125\n",
            "3202.4677734375\n",
            "22472.595703125\n",
            "4189.36572265625\n",
            "21089.55859375\n",
            "142462.96875\n",
            "32573.654296875\n",
            "5759.4150390625\n",
            "8703.4501953125\n",
            "28083.365234375\n",
            "29390.6875\n",
            "30928.974609375\n",
            "1418.906494140625\n",
            "10596.3212890625\n",
            "1849.61376953125\n",
            "26336.572265625\n",
            "162824.578125\n",
            "32451.060546875\n",
            "2562.95654296875\n",
            "93.9240951538086\n",
            "17342.34765625\n",
            "14103.2802734375\n",
            "34991.58984375\n",
            "4315.466796875\n",
            "28056.666015625\n",
            "4607.166015625\n",
            "32605.3671875\n",
            "152975.234375\n",
            "33426.1640625\n",
            "91600.5078125\n",
            "577701.5625\n",
            "108554.4453125\n",
            "1063595.25\n",
            "1416126.75\n",
            "108880.15625\n",
            "582377.0\n",
            "370336.9375\n",
            "1264016.875\n",
            "274091.0\n",
            "378278.8125\n",
            "20652.013671875\n",
            "168017.734375\n",
            "45850.7734375\n",
            "44224.37109375\n",
            "782791.4375\n",
            "248531.484375\n",
            "31946.23828125\n",
            "11020.8759765625\n",
            "129081.9609375\n",
            "119215.4296875\n",
            "255008.734375\n",
            "5187.08251953125\n",
            "25650.5390625\n",
            "5724.96630859375\n",
            "34200.0234375\n",
            "672856.5\n",
            "256731.09375\n",
            "59392.8359375\n",
            "42675.71484375\n",
            "166441.734375\n",
            "174801.71875\n",
            "264652.0\n",
            "10157.123046875\n",
            "43744.5859375\n",
            "10688.08984375\n",
            "27183.263671875\n",
            "540961.4375\n",
            "276696.96875\n",
            "236463.34375\n",
            "865563.75\n",
            "186480.421875\n",
            "strain 1.3455426692962646\n",
            "444809.8125\n",
            "3328.19287109375\n",
            "82585.6796875\n",
            "120170.9453125\n",
            "290448.34375\n",
            "57859.94921875\n",
            "6156.16064453125\n",
            "7202.466796875\n",
            "32695.63671875\n",
            "60421.08984375\n",
            "62014.703125\n",
            "34501.7421875\n",
            "159668.0625\n",
            "26340.0234375\n",
            "16865.62109375\n",
            "196547.125\n",
            "54782.72265625\n",
            "17327.294921875\n",
            "13249.1640625\n",
            "95478.1796875\n",
            "70313.8046875\n",
            "48207.546875\n",
            "25798.9453125\n",
            "114068.3828125\n",
            "20641.845703125\n",
            "26415.83984375\n",
            "170634.328125\n",
            "39838.41796875\n",
            "16902.423828125\n",
            "6463.45458984375\n",
            "67465.015625\n",
            "52879.60546875\n",
            "34746.7109375\n",
            "21613.0\n",
            "64617.9296875\n",
            "12200.7744140625\n",
            "16423.62109375\n",
            "125502.265625\n",
            "31441.0625\n",
            "7295.97509765625\n",
            "4767.33203125\n",
            "32020.072265625\n",
            "20543.94921875\n",
            "30379.94921875\n",
            "10074.4521484375\n",
            "31166.43359375\n",
            "6017.86669921875\n",
            "12334.5703125\n",
            "104429.6484375\n",
            "27733.5703125\n",
            "5363.21484375\n",
            "2776.479248046875\n",
            "25749.28515625\n",
            "18138.03515625\n",
            "26402.509765625\n",
            "1724.7384033203125\n",
            "9915.6005859375\n",
            "1841.1561279296875\n",
            "20745.55859375\n",
            "124256.6953125\n",
            "27366.005859375\n",
            "2155.22412109375\n",
            "239.93069458007812\n",
            "13178.046875\n",
            "8897.2841796875\n",
            "27661.703125\n",
            "2489.30859375\n",
            "15057.419921875\n",
            "2715.700439453125\n",
            "33503.953125\n",
            "131960.546875\n",
            "27814.806640625\n",
            "84137.1484375\n",
            "491198.90625\n",
            "88099.390625\n",
            "501947.0\n",
            "1166751.5\n",
            "84165.3984375\n",
            "215511.84375\n",
            "217706.0625\n",
            "870461.9375\n",
            "252469.046875\n",
            "255721.046875\n",
            "24337.2578125\n",
            "155602.03125\n",
            "48046.73828125\n",
            "28637.970703125\n",
            "463125.65625\n",
            "137832.390625\n",
            "15117.0986328125\n",
            "2777.496826171875\n",
            "43399.77734375\n",
            "79869.9296875\n",
            "138213.796875\n",
            "3009.75634765625\n",
            "15108.677734375\n",
            "3425.423095703125\n",
            "29264.447265625\n",
            "400441.625\n",
            "144254.578125\n",
            "25676.802734375\n",
            "17716.873046875\n",
            "89643.5234375\n",
            "71885.6640625\n",
            "146629.390625\n",
            "2749.564208984375\n",
            "14446.916015625\n",
            "3448.87548828125\n",
            "20348.591796875\n",
            "290684.09375\n",
            "149794.75\n",
            "199040.484375\n",
            "485945.625\n",
            "101207.296875\n",
            "strain 0.7801133990287781\n",
            "615066.25\n",
            "2720.14990234375\n",
            "97199.4921875\n",
            "279588.6875\n",
            "270352.4375\n",
            "106143.3125\n",
            "11156.984375\n",
            "5893.52734375\n",
            "56030.1875\n",
            "220016.21875\n",
            "116773.03125\n",
            "29794.6953125\n",
            "141933.0625\n",
            "25722.41015625\n",
            "31591.642578125\n",
            "504586.4375\n",
            "104540.7265625\n",
            "60633.16796875\n",
            "12106.23046875\n",
            "162709.6875\n",
            "187851.421875\n",
            "83622.6953125\n",
            "16095.02734375\n",
            "106228.359375\n",
            "20400.1796875\n",
            "30237.0234375\n",
            "262441.96875\n",
            "69561.8828125\n",
            "26075.439453125\n",
            "9150.7578125\n",
            "81967.09375\n",
            "107634.453125\n",
            "64733.78125\n",
            "13823.02734375\n",
            "72396.4375\n",
            "14298.189453125\n",
            "33760.2734375\n",
            "256069.25\n",
            "59249.97265625\n",
            "11711.50390625\n",
            "2220.062255859375\n",
            "50998.2734375\n",
            "31963.1875\n",
            "56249.27734375\n",
            "8224.830078125\n",
            "33314.21484375\n",
            "6592.74853515625\n",
            "8966.53125\n",
            "183907.28125\n",
            "55153.21875\n",
            "8084.61376953125\n",
            "1724.3521728515625\n",
            "40712.984375\n",
            "16445.62109375\n",
            "54776.70703125\n",
            "4240.53564453125\n",
            "23944.078125\n",
            "4660.34765625\n",
            "33227.56640625\n",
            "247249.265625\n",
            "52876.76953125\n",
            "2917.2666015625\n",
            "173.2668914794922\n",
            "14816.6630859375\n",
            "12163.8095703125\n",
            "51991.1171875\n",
            "3060.273193359375\n",
            "18733.388671875\n",
            "3476.846923828125\n",
            "35376.4296875\n",
            "260513.484375\n",
            "52913.2109375\n",
            "164918.953125\n",
            "954827.1875\n",
            "167460.65625\n",
            "694406.1875\n",
            "2249675.75\n",
            "161211.9375\n",
            "449565.3125\n",
            "158807.1875\n",
            "1075896.5\n",
            "276904.5625\n",
            "408828.21875\n",
            "33549.7578125\n",
            "182458.34375\n",
            "51389.33984375\n",
            "46318.41796875\n",
            "899253.625\n",
            "259228.765625\n",
            "29027.55859375\n",
            "8329.9541015625\n",
            "105400.96875\n",
            "160513.171875\n",
            "263024.25\n",
            "5450.55810546875\n",
            "26210.69921875\n",
            "6116.02099609375\n",
            "17165.0234375\n",
            "719269.125\n",
            "269923.21875\n",
            "51771.0859375\n",
            "19364.384765625\n",
            "185604.953125\n",
            "160179.203125\n",
            "290002.15625\n",
            "5878.70751953125\n",
            "26844.232421875\n",
            "6187.8955078125\n",
            "19246.76171875\n",
            "559785.125\n",
            "290121.09375\n",
            "295255.65625\n",
            "870426.8125\n",
            "175575.46875\n",
            "strain 1.2492541074752808\n",
            "35604.3046875\n",
            "562.98193359375\n",
            "11925.4716796875\n",
            "21896.677734375\n",
            "33958.05859375\n",
            "9174.1357421875\n",
            "1599.7470703125\n",
            "686.27685546875\n",
            "6939.07177734375\n",
            "20927.998046875\n",
            "10083.3466796875\n",
            "4195.22900390625\n",
            "17879.216796875\n",
            "2945.715087890625\n",
            "2982.498291015625\n",
            "33620.44921875\n",
            "7100.55810546875\n",
            "3700.396484375\n",
            "888.1574096679688\n",
            "10668.8447265625\n",
            "8344.201171875\n",
            "6516.79638671875\n",
            "2954.050048828125\n",
            "14074.609375\n",
            "2110.25341796875\n",
            "3015.314697265625\n",
            "22575.29296875\n",
            "5172.412109375\n",
            "1939.1239013671875\n",
            "668.2489013671875\n",
            "6308.7734375\n",
            "6285.4033203125\n",
            "4818.935546875\n",
            "1263.7828369140625\n",
            "7122.001953125\n",
            "1405.5948486328125\n",
            "3098.377197265625\n",
            "20218.724609375\n",
            "4443.56689453125\n",
            "700.4762573242188\n",
            "431.5546875\n",
            "2684.971435546875\n",
            "2094.569091796875\n",
            "3867.361572265625\n",
            "738.4381103515625\n",
            "3251.251220703125\n",
            "600.8310546875\n",
            "616.4077758789062\n",
            "23740.205078125\n",
            "4163.1728515625\n",
            "669.8992919921875\n",
            "361.80682373046875\n",
            "2924.237060546875\n",
            "1582.135498046875\n",
            "4198.7333984375\n",
            "422.3855285644531\n",
            "2160.4033203125\n",
            "376.59552001953125\n",
            "1642.260986328125\n",
            "17113.32421875\n",
            "3988.015869140625\n",
            "263.71697998046875\n",
            "13.722834587097168\n",
            "1237.4130859375\n",
            "1141.4097900390625\n",
            "4566.18310546875\n",
            "219.56626892089844\n",
            "1910.4532470703125\n",
            "286.7668151855469\n",
            "2328.80126953125\n",
            "18591.67578125\n",
            "4020.9365234375\n",
            "13129.275390625\n",
            "81685.3671875\n",
            "12395.744140625\n",
            "266281.0\n",
            "236677.328125\n",
            "15550.4951171875\n",
            "110006.609375\n",
            "120938.9453125\n",
            "366248.96875\n",
            "111481.453125\n",
            "59316.203125\n",
            "11947.41796875\n",
            "48321.25390625\n",
            "10296.3115234375\n",
            "19349.25390625\n",
            "194155.578125\n",
            "45249.5234375\n",
            "4821.4287109375\n",
            "4760.8671875\n",
            "21908.826171875\n",
            "21648.494140625\n",
            "42737.82421875\n",
            "3731.84716796875\n",
            "15911.9375\n",
            "3278.07080078125\n",
            "6905.48779296875\n",
            "177996.734375\n",
            "43948.89453125\n",
            "16778.33203125\n",
            "13373.0693359375\n",
            "51900.53515625\n",
            "14735.64453125\n",
            "55499.08984375\n",
            "1613.1490478515625\n",
            "9237.2294921875\n",
            "2269.027099609375\n",
            "4607.17333984375\n",
            "109795.390625\n",
            "48126.2109375\n",
            "84434.46875\n",
            "195398.171875\n",
            "38632.421875\n",
            "strain 1.3982694149017334\n",
            "371361.53125\n",
            "605.2553100585938\n",
            "101118.1328125\n",
            "124673.046875\n",
            "272609.65625\n",
            "44089.7578125\n",
            "4976.54052734375\n",
            "1811.47802734375\n",
            "28462.619140625\n",
            "86696.578125\n",
            "49971.94140625\n",
            "18548.994140625\n",
            "89855.4296875\n",
            "16128.9853515625\n",
            "28391.080078125\n",
            "216875.15625\n",
            "44195.76171875\n",
            "24609.60546875\n",
            "3794.863525390625\n",
            "70215.1328125\n",
            "38306.51171875\n",
            "39032.8828125\n",
            "33443.484375\n",
            "103649.75\n",
            "17806.560546875\n",
            "27761.640625\n",
            "143616.21875\n",
            "32487.111328125\n",
            "6842.9189453125\n",
            "3323.728515625\n",
            "29256.9765625\n",
            "59850.28125\n",
            "31763.041015625\n",
            "4546.388671875\n",
            "25992.44140625\n",
            "4489.65087890625\n",
            "17968.984375\n",
            "153498.953125\n",
            "31522.60546875\n",
            "3713.52197265625\n",
            "4099.44921875\n",
            "16591.783203125\n",
            "21137.572265625\n",
            "31634.291015625\n",
            "3475.17822265625\n",
            "20506.916015625\n",
            "3959.234375\n",
            "19272.7421875\n",
            "121007.59375\n",
            "30953.748046875\n",
            "4026.101806640625\n",
            "2076.773193359375\n",
            "18616.146484375\n",
            "13804.2822265625\n",
            "31232.533203125\n",
            "1330.9857177734375\n",
            "8856.7841796875\n",
            "1630.4677734375\n",
            "48812.88671875\n",
            "155365.75\n",
            "30702.041015625\n",
            "1191.506103515625\n",
            "127.19274139404297\n",
            "5394.51123046875\n",
            "6845.2373046875\n",
            "31501.16796875\n",
            "1558.0445556640625\n",
            "10301.2548828125\n",
            "1621.6973876953125\n",
            "46327.68359375\n",
            "156643.109375\n",
            "30493.33203125\n",
            "149995.796875\n",
            "661044.875\n",
            "117890.5859375\n",
            "2941581.75\n",
            "1809047.0\n",
            "151241.546875\n",
            "599153.4375\n",
            "286415.59375\n",
            "1579717.0\n",
            "440519.4375\n",
            "453047.53125\n",
            "61816.06640625\n",
            "308185.84375\n",
            "91897.125\n",
            "69960.4609375\n",
            "1032511.25\n",
            "306493.625\n",
            "84052.9453125\n",
            "24613.45703125\n",
            "186816.28125\n",
            "212983.859375\n",
            "326879.03125\n",
            "16447.056640625\n",
            "88929.0078125\n",
            "23539.4609375\n",
            "84807.8359375\n",
            "768937.75\n",
            "304410.1875\n",
            "113602.2109375\n",
            "75325.8515625\n",
            "269755.5\n",
            "268919.0625\n",
            "319876.5625\n",
            "7924.7666015625\n",
            "58306.16796875\n",
            "12481.4921875\n",
            "87783.703125\n",
            "659885.0\n",
            "314643.1875\n",
            "341966.125\n",
            "934234.4375\n",
            "190206.28125\n",
            "strain 2.342503786087036\n",
            "123846.5546875\n",
            "898.190185546875\n",
            "22509.666015625\n",
            "60024.74609375\n",
            "91418.5859375\n",
            "34538.01953125\n",
            "8982.923828125\n",
            "7489.38037109375\n",
            "30550.177734375\n",
            "34105.74609375\n",
            "41615.82421875\n",
            "20960.037109375\n",
            "67661.5\n",
            "9947.109375\n",
            "8247.5498046875\n",
            "84743.40625\n",
            "22581.482421875\n",
            "10658.587890625\n",
            "6950.35009765625\n",
            "47239.4453125\n",
            "31351.771484375\n",
            "18458.6484375\n",
            "5900.90576171875\n",
            "27910.697265625\n",
            "5478.26953125\n",
            "6802.49755859375\n",
            "53937.62890625\n",
            "13471.4765625\n",
            "5382.81298828125\n",
            "2505.45703125\n",
            "19365.6328125\n",
            "21895.556640625\n",
            "12363.3818359375\n",
            "3831.391845703125\n",
            "14368.935546875\n",
            "3147.8974609375\n",
            "6811.28955078125\n",
            "44322.828125\n",
            "10621.29296875\n",
            "1814.7386474609375\n",
            "1093.539306640625\n",
            "8653.662109375\n",
            "5963.91748046875\n",
            "9985.31640625\n",
            "1740.402099609375\n",
            "8670.1787109375\n",
            "1632.25439453125\n",
            "4412.1435546875\n",
            "38123.3828125\n",
            "9736.90625\n",
            "1645.67041015625\n",
            "285.03167724609375\n",
            "8194.4736328125\n",
            "4445.31640625\n",
            "10021.3349609375\n",
            "476.1773986816406\n",
            "3049.4296875\n",
            "563.8361206054688\n",
            "3745.826171875\n",
            "47740.26953125\n",
            "9505.8916015625\n",
            "628.4590454101562\n",
            "67.33806610107422\n",
            "3511.999755859375\n",
            "2725.704345703125\n",
            "9780.3330078125\n",
            "895.2813110351562\n",
            "4027.467041015625\n",
            "706.8041381835938\n",
            "5634.26611328125\n",
            "44295.6875\n",
            "9733.3349609375\n",
            "30571.8046875\n",
            "178593.953125\n",
            "31082.3125\n",
            "174339.625\n",
            "420112.21875\n",
            "29157.10546875\n",
            "64349.71484375\n",
            "67253.53125\n",
            "223733.171875\n",
            "61110.0703125\n",
            "95139.265625\n",
            "10218.9677734375\n",
            "43880.05078125\n",
            "11951.62890625\n",
            "9976.8876953125\n",
            "131736.53125\n",
            "40775.46484375\n",
            "12177.529296875\n",
            "1165.3414306640625\n",
            "29393.623046875\n",
            "23226.626953125\n",
            "41042.12890625\n",
            "774.9349975585938\n",
            "4720.68212890625\n",
            "1162.2999267578125\n",
            "8341.404296875\n",
            "112231.3515625\n",
            "43052.52734375\n",
            "6356.39306640625\n",
            "3637.017822265625\n",
            "19313.2734375\n",
            "18013.513671875\n",
            "41806.484375\n",
            "1118.4710693359375\n",
            "3120.20166015625\n",
            "647.7639770507812\n",
            "5201.32763671875\n",
            "78545.65625\n",
            "44342.984375\n",
            "121139.0\n",
            "201290.96875\n",
            "41210.1953125\n",
            "strain 0.7192243337631226\n",
            "87597.9296875\n",
            "1017.0227661132812\n",
            "52539.10546875\n",
            "86949.546875\n",
            "131543.671875\n",
            "38666.58203125\n",
            "7533.7587890625\n",
            "6009.97314453125\n",
            "24898.88671875\n",
            "39200.2578125\n",
            "42847.82421875\n",
            "8919.490234375\n",
            "50182.83984375\n",
            "9562.7333984375\n",
            "12627.5322265625\n",
            "118844.90625\n",
            "29751.390625\n",
            "10197.0244140625\n",
            "2607.148193359375\n",
            "47132.15625\n",
            "34214.01171875\n",
            "27583.626953125\n",
            "9582.9111328125\n",
            "36277.62109375\n",
            "6270.6240234375\n",
            "10835.34375\n",
            "78888.1953125\n",
            "21722.029296875\n",
            "5276.02001953125\n",
            "1061.199951171875\n",
            "23667.2265625\n",
            "33727.78515625\n",
            "19486.76953125\n",
            "4987.04931640625\n",
            "17789.599609375\n",
            "2966.91259765625\n",
            "7455.51416015625\n",
            "70762.03125\n",
            "19148.62890625\n",
            "2751.24951171875\n",
            "2041.3551025390625\n",
            "13979.31640625\n",
            "9919.5732421875\n",
            "18811.93359375\n",
            "2147.0693359375\n",
            "9844.4453125\n",
            "2324.952392578125\n",
            "6946.572265625\n",
            "73898.3671875\n",
            "17992.849609375\n",
            "2508.0791015625\n",
            "1353.261962890625\n",
            "13199.4052734375\n",
            "13501.455078125\n",
            "17928.169921875\n",
            "861.5208740234375\n",
            "6348.5224609375\n",
            "1299.66552734375\n",
            "17525.140625\n",
            "73738.4140625\n",
            "17676.041015625\n",
            "1051.1822509765625\n",
            "27.54452896118164\n",
            "6661.00341796875\n",
            "6935.892578125\n",
            "17490.10546875\n",
            "1671.5260009765625\n",
            "9666.7919921875\n",
            "1734.2528076171875\n",
            "22227.404296875\n",
            "76881.734375\n",
            "17858.08203125\n",
            "29268.291015625\n",
            "319547.625\n",
            "60137.10546875\n",
            "1408728.625\n",
            "819084.3125\n",
            "62162.5703125\n",
            "197175.140625\n",
            "183151.765625\n",
            "620621.625\n",
            "201678.078125\n",
            "282504.15625\n",
            "22530.734375\n",
            "128306.4609375\n",
            "35321.83203125\n",
            "34153.17578125\n",
            "593087.125\n",
            "165367.859375\n",
            "21962.302734375\n",
            "8376.0361328125\n",
            "54392.77734375\n",
            "48637.8515625\n",
            "168556.65625\n",
            "4275.71533203125\n",
            "17478.80859375\n",
            "3766.66015625\n",
            "10643.787109375\n",
            "456316.34375\n",
            "175415.953125\n",
            "42794.859375\n",
            "69671.875\n",
            "114621.171875\n",
            "80430.671875\n",
            "188832.359375\n",
            "5672.73974609375\n",
            "18851.55859375\n",
            "4426.16015625\n",
            "6864.576171875\n",
            "415373.09375\n",
            "196011.546875\n",
            "236001.25\n",
            "624446.375\n",
            "130537.8046875\n",
            "strain 1.8687481880187988\n",
            "59343.6015625\n",
            "360.4467468261719\n",
            "22267.712890625\n",
            "28825.296875\n",
            "56598.46875\n",
            "11606.033203125\n",
            "1304.0152587890625\n",
            "1168.835205078125\n",
            "7196.9990234375\n",
            "15529.826171875\n",
            "12325.16796875\n",
            "6685.07275390625\n",
            "27513.68359375\n",
            "3581.440673828125\n",
            "5379.5078125\n",
            "38499.0390625\n",
            "8227.3828125\n",
            "3863.264892578125\n",
            "1326.8836669921875\n",
            "13068.1484375\n",
            "9724.6279296875\n",
            "7508.91455078125\n",
            "3029.717041015625\n",
            "17152.697265625\n",
            "2591.083740234375\n",
            "2970.7763671875\n",
            "30103.185546875\n",
            "6290.2431640625\n",
            "2842.023681640625\n",
            "592.767822265625\n",
            "10082.341796875\n",
            "10655.1640625\n",
            "6164.18408203125\n",
            "2462.22021484375\n",
            "8413.5615234375\n",
            "1286.7108154296875\n",
            "1935.95751953125\n",
            "23013.62109375\n",
            "5339.55615234375\n",
            "1060.0225830078125\n",
            "356.2113037109375\n",
            "4636.17919921875\n",
            "3579.1279296875\n",
            "5255.79833984375\n",
            "1115.474365234375\n",
            "3744.913818359375\n",
            "733.7699584960938\n",
            "1451.552001953125\n",
            "21763.814453125\n",
            "4949.37939453125\n",
            "846.4249877929688\n",
            "244.9649200439453\n",
            "4089.23876953125\n",
            "3595.896240234375\n",
            "4922.85546875\n",
            "370.4289245605469\n",
            "1868.82373046875\n",
            "296.1360778808594\n",
            "4844.30810546875\n",
            "25426.61328125\n",
            "4819.55029296875\n",
            "383.88079833984375\n",
            "50.13996505737305\n",
            "1976.438232421875\n",
            "1238.757568359375\n",
            "4958.41162109375\n",
            "541.0226440429688\n",
            "3873.90966796875\n",
            "648.064453125\n",
            "6728.70068359375\n",
            "25795.109375\n",
            "4880.46630859375\n",
            "14047.87109375\n",
            "151457.828125\n",
            "16769.814453125\n",
            "277090.9375\n",
            "474611.78125\n",
            "18710.052734375\n",
            "88074.2578125\n",
            "132463.15625\n",
            "516866.875\n",
            "120915.71875\n",
            "115035.3046875\n",
            "20343.01171875\n",
            "80925.2578125\n",
            "15853.69921875\n",
            "16192.05859375\n",
            "216524.8125\n",
            "44921.09765625\n",
            "23359.443359375\n",
            "2278.6201171875\n",
            "57062.28515625\n",
            "26213.439453125\n",
            "54415.06640625\n",
            "1968.669677734375\n",
            "11069.9765625\n",
            "2488.831298828125\n",
            "3556.3505859375\n",
            "137768.5625\n",
            "44404.84765625\n",
            "12169.7353515625\n",
            "3104.248291015625\n",
            "27898.1875\n",
            "19040.861328125\n",
            "43382.2578125\n",
            "2170.35693359375\n",
            "11132.7216796875\n",
            "2565.2529296875\n",
            "2558.329345703125\n",
            "122028.4609375\n",
            "44416.58984375\n",
            "112076.6328125\n",
            "178488.625\n",
            "37456.44140625\n",
            "strain 1.9319381713867188\n",
            "classify 2.3258056640625\n",
            "classify 2.3919677734375\n",
            "classify 2.40185546875\n",
            "classify 2.4759521484375\n",
            "classify 2.50982666015625\n",
            "classify 2.48760986328125\n",
            "classify 2.33984375\n",
            "classify 2.37274169921875\n",
            "classify 2.42340087890625\n",
            "classify 2.4620361328125\n",
            "classify 2.50323486328125\n",
            "0.140625\n",
            "0.1015625\n",
            "0.171875\n",
            "0.1328125\n",
            "0.1484375\n",
            "0.09375\n",
            "0.21875\n",
            "0.125\n",
            "0.1484375\n",
            "0.078125\n",
            "0.1484375\n"
          ]
        }
      ],
      "source": [
        "# @title strain ctrain test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "def strain(model, dataloader, optim, scheduler=None): # train function with automatic mixed precision\n",
        "# def strain(model, train_iter, optim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.train()\n",
        "    for i, (img, _) in enumerate(dataloader):\n",
        "        img = img.to(device).flatten(2).transpose(-2,-1)#.to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            loss = model.loss(img)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # total_norm = 0\n",
        "        # for p in model.parameters(): total_norm += p.grad.data.norm(2).item() ** 2\n",
        "        # total_norm = total_norm**.5\n",
        "        # print('total_norm', total_norm)\n",
        "        # for p in list(filter(lambda p: p.grad is not None, model.parameters())):\n",
        "        #     print(p.grad.data.norm(2).item())\n",
        "        # print(\"max grad norm\", max([p.grad.data.norm(2).item() for p in list(filter(lambda p: p.grad is not None, model.parameters()))]))\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 5) # 0.5\n",
        "\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            m=0.999 # 0.99 m = next(momentum_scheduler)\n",
        "            norms=[]\n",
        "            for param_q, param_k in zip(model.context_encoder.parameters(), model.target_encoder.parameters()):\n",
        "                # norm = ((param_k.data-param_q.detach().data)**2).sum()**.5\n",
        "                # # # print(param_k.data.shape, norm)\n",
        "                # norms.append(norm.item())\n",
        "                # # if norm>.01:\n",
        "                param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "            # print(norms)\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        print(\"strain\",loss.item())\n",
        "        # for param in seq_jepa.context_encoder.cls: print(param.data)\n",
        "        # for param in seq_jepa.predicter.cls: print(param.data)\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=50: break\n",
        "\n",
        "\n",
        "def ctrain(model, classifier, dataloader, coptim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.eval()\n",
        "    classifier.train()\n",
        "    for i, (img, y) in enumerate(dataloader):\n",
        "        img, y = img.to(device), y.to(device) # [batch, ]\n",
        "        img = img.flatten(2).transpose(-2,-1).to(torch.bfloat16)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            with torch.no_grad():\n",
        "                sx = model(img).detach()\n",
        "            # print(sx[0][0])\n",
        "            y_ = classifier(sx)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "        # print(classifier.classifier.weight[0])\n",
        "        # print(y_.shape, y.shape)\n",
        "        # print(loss)\n",
        "        coptim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 5) # .5\n",
        "        scaler.step(coptim)\n",
        "        scaler.update()\n",
        "        print(\"classify\",loss.item())\n",
        "        try: wandb.log({\"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    for i, (img, y) in enumerate(dataloader):\n",
        "        img, y = img.to(device), y.to(device) # [batch, ]\n",
        "        img = img.flatten(2).transpose(-2,-1)#.to(torch.bfloat16)\n",
        "        with torch.no_grad():\n",
        "            sx = model(img)\n",
        "            y_ = classifier(sx)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        print(correct/len(y))\n",
        "        try: wandb.log({\"correct\": correct/len(y)})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "for i in range(100):\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    strain(seq_jepa, train_loader, optim)\n",
        "    ctrain(seq_jepa, classifier, train_loader, coptim)\n",
        "    test(seq_jepa, classifier, test_loader)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# @title supervised train test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "def strain(model, dataloader, optim, scheduler=None): # train function with automatic mixed precision\n",
        "# def strain(model, train_iter, optim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.train()\n",
        "    for i, (img, y) in enumerate(dataloader):\n",
        "        img = img.to(device).flatten(2).transpose(-2,-1)#.to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "        y = y.to(device)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            y_ = model.classify(img)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        # print(loss)\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # for p in list(filter(lambda p: p.grad is not None, model.parameters())):\n",
        "        #     print(p.grad.data.norm(2).item())\n",
        "        # print(\"max grad norm\", max([p.grad.data.norm(2).item() for p in list(filter(lambda p: p.grad is not None, model.parameters()))]))\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 10) # 0.5\n",
        "\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        print(\"strain\",loss.item())\n",
        "        # for param in seq_jepa.context_encoder.cls: print(param.data)\n",
        "        # for param in seq_jepa.predicter.cls: print(param.data)\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=50: break\n",
        "\n",
        "\n",
        "def test(model, dataloader):\n",
        "    model.eval()\n",
        "    for i, (img, y) in enumerate(dataloader):\n",
        "        img, y = img.to(device), y.to(device) # [batch, ]\n",
        "        img = img.flatten(2).transpose(-2,-1)#.to(torch.bfloat16)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            y_ = model.classify(img)\n",
        "\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        print(correct/len(y))\n",
        "        try: wandb.log({\"correct\": correct/len(y)})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "for i in range(100):\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    strain(seq_jepa, train_loader, optim)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "    test(seq_jepa, test_loader)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4J2ahp3wmqcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for param in seq_jepa.context_encoder.cls: print(param.data)\n",
        "# print(seq_jepa.context_encoder.cls.is_leaf)\n",
        "# seq=40\n",
        "# src = seq_jepa.context_encoder.pos_encoder(torch.arange(seq, device=device))\n",
        "# for x in src:\n",
        "#     print(x)\n",
        "# print(.999**50)"
      ],
      "metadata": {
        "id": "WgN5LlxWsPzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "2Nd-sGe6Ku4S",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "outputId": "87635015-bdc0-4541-f3bf-47b29039d97b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>▁▁▁▂▂▂▂▃▃▃▂▂▂▃▂▃▃▂▃▄▄▅▄▄▄▄▅▅▄▅▆▆▇▅▆▅█▅▆▆</td></tr><tr><td>correct</td><td>▅▄▃▃▁▃▃▃█▂▁▂▃▆▅▅▆▃█▃▇▃▄▆▅▅▄▄▄▃▅▄▂█▂▃▃▅▅▂</td></tr><tr><td>loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▄▂▂▄▂▆▃▄▅▅▁▄▄▆▂██▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>2.96921</td></tr><tr><td>correct</td><td>0.09375</td></tr><tr><td>loss</td><td>3.15408</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">pumpkin-square-81</strong> at: <a href='https://wandb.ai/bobdole/SeqJEPA/runs/0pt0rzp8' target=\"_blank\">https://wandb.ai/bobdole/SeqJEPA/runs/0pt0rzp8</a><br> View project at: <a href='https://wandb.ai/bobdole/SeqJEPA' target=\"_blank\">https://wandb.ai/bobdole/SeqJEPA</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250314_030730-0pt0rzp8/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250314_032804-bm18dl7m</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/SeqJEPA/runs/bm18dl7m' target=\"_blank\">key-lime-brulee-82</a></strong> to <a href='https://wandb.ai/bobdole/SeqJEPA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/SeqJEPA' target=\"_blank\">https://wandb.ai/bobdole/SeqJEPA</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/SeqJEPA/runs/bm18dl7m' target=\"_blank\">https://wandb.ai/bobdole/SeqJEPA/runs/bm18dl7m</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "!pip install -q wandb\n",
        "import wandb # https://docs.wandb.ai/quickstart\n",
        "wandb.login(key='487a2109e55dce4e13fc70681781de9f50f27be7')\n",
        "try: run.finish()\n",
        "except NameError: pass\n",
        "run = wandb.init(project=\"SeqJEPA\", config={\"model\": \"res18\",})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title save/load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder='/content/drive/MyDrive/jepa/'\n",
        "\n",
        "# # modelsd, optimsd = torch.load(folder+'SeqJEPA.pkl', map_location=device).values()\n",
        "# modelsd, optimsd = torch.load('SeqJEPA.pkl', map_location=device).values()\n",
        "# seq_jepa.load_state_dict(modelsd, strict=False)\n",
        "# optim.load_state_dict(optimsd)"
      ],
      "metadata": {
        "id": "JT8AgOx0E_KM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b3d3969-da28-4cc0-c4de-f0a327ea266f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = {'model': seq_jepa.state_dict(), 'optimizer': optim.state_dict()}\n",
        "torch.save(checkpoint, folder+'SeqJEPA.pkl')\n",
        "# torch.save(checkpoint, 'SeqJEPA.pkl')"
      ],
      "metadata": {
        "id": "CpbLPvjiE-pD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## drawer"
      ],
      "metadata": {
        "id": "0vScvFGGSeN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(4,100,3)\n",
        "\n",
        "x=x.transpose(-2,-1) # [batch, dim, seq]\n",
        " # in, out, kernel, stride, pad\n",
        "# net = nn.Conv1d(3,16,7,2,7//2)\n",
        "net = nn.Sequential(nn.Conv1d(3,16,7,2,7//2), nn.MaxPool1d(3, 2, 3//2))\n",
        "# net = nn.Conv1d(3,16,(7,3),2,3//2)\n",
        "out = net(x)\n",
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_YBe6-eZ2Zq",
        "outputId": "09b27f75-ba04-4aaf-f5e4-1b9c1dc8d112"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 16, 25])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test repeat_interleave\n",
        "# x = torch.rand(3,5)\n",
        "x = torch.rand(3,5,7)\n",
        "print(x)\n",
        "# out = x.repeat(2,1) # [2*3,5]\n",
        "# print(out)\n",
        "# x_ = out.reshape(2,3,5)\n",
        "# print(x_)\n",
        "# out = x.repeat_interleave(2,1,1) # [3*2,5]\n",
        "out = x.repeat_interleave(2,dim=0) # [3*2,5]\n",
        "# print(out)\n",
        "# x_ = out.reshape(2,3,5,7)\n",
        "x_ = out.reshape(3,2,5,7)\n",
        "print(x_)\n",
        "\n",
        "\n",
        "# batch=1\n",
        "# seq_len=5\n",
        "# dim=4\n",
        "# positional_emb = torch.rand(batch, seq_len, dim)\n",
        "# print(positional_emb)\n",
        "# num_tok=2\n",
        "# trg_indices=torch.randint(0,seq_len,(M, num_tok))\n",
        "# print(trg_indices)\n",
        "# # pos_embs = positional_emb.gather(1, trg_indices.unsqueeze(0))\n",
        "# pos_embs = positional_emb[torch.arange(batch)[...,None,None],trg_indices.unsqueeze(0)]\n",
        "# # pos_embs = positional_emb[0,trg_indices]\n",
        "# # [batch, M, num_tok, dim]\n",
        "# print(pos_embs.shape)\n",
        "# print(pos_embs)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "R6kiiqw3uIdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test ropes\n",
        "# for i, (img, _) in enumerate(test_loader):\n",
        "#     break\n",
        "# print(img.shape) # [batch, 1, 28, 28]\n",
        "# print(img[0]) # [0,1)\n",
        "d_model=8\n",
        "# rot_emb = RotEmb(d_model, top=torch.pi, base=10)\n",
        "# x = torch.linspace(0,1,20)\n",
        "# out = rot_emb(x)\n",
        "# print(out.shape)\n",
        "# print(out)\n",
        "\n",
        "# # pos_encoder = RoPE(d_model, seq_len=15, base=100)\n",
        "# pos_encoder = RoPE(d_model, seq_len=15, base=10000)\n",
        "x = torch.ones(1, 10, d_model)\n",
        "# # x = torch.ones(1, 10, d_model)\n",
        "# out = pos_encoder(x)\n",
        "# # print(out.shape)\n",
        "# for x in out[0]:\n",
        "#     print(x)\n",
        "\n",
        "\n",
        "# pos_encoder = LearnedRoPE(d_model, seq_len=512)\n",
        "# out = pos_encoder(x)\n",
        "# for o in out[0]:\n",
        "#     print(o)\n",
        "\n",
        "# pos_encoder.weights = nn.Parameter(torch.randn(1, d_model//2))\n",
        "# out = pos_encoder(x)\n",
        "# for o in out[0]:\n",
        "#     print(o)\n",
        "\n",
        "\n",
        "# tensor([-0.0177, -0.9998,  0.8080, -0.5892, -0.7502, -0.6612,  0.3885,  0.9214])\n"
      ],
      "metadata": {
        "id": "8r_GjI_vCMyo",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Transformer Model\n",
        "import torch\n",
        "from torch import nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        # self.embed = nn.Sequential(\n",
        "        #     nn.Linear(in_dim, d_model), #act,\n",
        "        #     # nn.Linear(d_model, d_model), act,\n",
        "        # )\n",
        "        self.embed = nn.Linear(in_dim, d_model) if in_dim != d_model else None\n",
        "        # self.embed = nn.Sequential(nn.Conv1d(in_dim,d_model,7,2,7//2), nn.MaxPool1d(3, 2, 3//2))\n",
        "        self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, d_hid or d_model, dropout, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.d_model = d_model\n",
        "        self.cls = nn.Parameter(torch.zeros(1,1,d_model)) # randn\n",
        "        out_dim = out_dim or d_model\n",
        "        # self.lin = nn.Linear(d_model, out_dim)\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim != d_model else None\n",
        "        self.norm = nn.LayerNorm(out_dim)\n",
        "\n",
        "\n",
        "    def forward(self, src, src_key_padding_mask=None, cls_mask=None, context_indices=None, trg_indices=None): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # if cls_mask != None: src[cls_mask] = self.cls.to(src.dtype)\n",
        "\n",
        "        if self.embed != None: src = self.embed(src) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = src.shape\n",
        "        # src = self.pos_encoder(src)\n",
        "        if context_indices != None:\n",
        "            # print(src.shape, context_indices.shape, self.pos_encoder(context_indices).shape) # [2, 88, 32]) torch.Size([88]) torch.Size([88, 32]\n",
        "            # print(src[0], self.pos_encoder(context_indices)[0])\n",
        "            src = src * self.pos_encoder(context_indices) # context/predictor # src = src + self.positional_emb[:,context_indices]\n",
        "            # print(src[0])\n",
        "        else: src = src * self.pos_encoder(torch.arange(seq, device=device)) # target # src = src + self.positional_emb[:,:seq]\n",
        "            # print(\"trans fwd\", src.shape, self.pos_encoder(src).shape)\n",
        "\n",
        "        if trg_indices != None: # [M, num_trg_toks]\n",
        "            pred_tokens = self.cls * self.pos_encoder(trg_indices) # [M, num_trg_toks, d_model] # pred_tokens = self.cls + self.positional_emb[0,trg_indices]\n",
        "            pred_tokens = pred_tokens.repeat(batch, 1, 1) # [batch*M, num_trg_toks, d_model]\n",
        "            # print(pred_tokens.requires_grad)\n",
        "            src = src.repeat_interleave(trg_indices.shape[0], dim=0) # [batch, seq_len, d_model] -> [batch*M, seq_len, d_model]\n",
        "            src = torch.cat([src, pred_tokens], dim=1) # [batch*M, seq_len+num_trg_toks, d_model]\n",
        "\n",
        "        out = self.transformer_encoder(src, src_key_padding_mask=src_key_padding_mask) # float [seq_len, batch_size, d_model]\n",
        "        if trg_indices != None:\n",
        "            # print(out.shape)\n",
        "            out = out[:,seq:] # [batch*M, num_trg_toks, d_model]\n",
        "        if self.lin != None: out = self.lin(out)\n",
        "        out = self.norm(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "batch, seq_len, d_model = 4,7,64\n",
        "in_dim = 3\n",
        "model = TransformerModel(in_dim, d_model, nhead=8, nlayers=2, dropout=0.).to(device)\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "# print(out.shape)\n",
        "# # print(out)\n"
      ],
      "metadata": {
        "id": "-8i1WLsvH_vn",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ba31127-d6e5-438a-aede-5c789557ab39",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title SeqJEPA\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "def multiblock(seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "    return target_mask\n",
        "\n",
        "\n",
        "class SeqJEPA(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, num_layers=1, num_classes=10):\n",
        "        super().__init__()\n",
        "        if out_dim is None: out_dim = d_model\n",
        "        act = nn.GELU() # ReLU GELU SiLU\n",
        "        # self.rot_emb = RotEmb(d_model, top=1, base=10)\n",
        "        # self.rot_emb = RotEmb(d_model, top=torch.pi, base=10)\n",
        "        # self.rot_emb = RoPE(d_model, seq_len=1024, base=10)\n",
        "        # self.rot_emb = LearnedRotEmb(dim)\n",
        "        self.encode = nn.Sequential(\n",
        "            nn.Linear(in_dim, d_model), #act,\n",
        "            # nn.Linear(d_model, d_model), act,\n",
        "        )\n",
        "        # self.encode = nn.Sequential(nn.Conv1d(in_dim, d_model,7,2,7//2), nn.MaxPool1d(3, 2, 3//2))\n",
        "        self.context_encoder = TransformerModel(d_model, d_model, out_dim=out_dim, nhead=4, nlayers=2, dropout=0.)\n",
        "        self.predicter = TransformerModel(out_dim, d_model//2, out_dim, nhead=4, nlayers=1, dropout=0.)\n",
        "        self.target_encoder = AveragedModel(self.context_encoder, multi_avg_fn=get_ema_multi_avg_fn(0.85)) # 0.95 0.999\n",
        "        self.target_encoder.requires_grad_(False)\n",
        "        # self.classifier = nn.Linear(out_dim, num_classes)\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]\n",
        "        x = self.encode(x) # [batch, T, d_model]\n",
        "        # x = self.encode(x.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "\n",
        "        batch, seq, dim = x.shape\n",
        "        M=1 # 4\n",
        "        # target_mask = multiblock(seq, min_s=0.15, max_s=0.2, M=M) # mask out targets to be predicted # [M, seq]\n",
        "        target_mask = multiblock(seq, min_s=0.15, max_s=0.2, M=4)#.any(0) # mask out targets to be predicted # [M, seq]\n",
        "        context_mask = ~multiblock(seq, min_s=0.85, max_s=1., M=1)|target_mask.reshape(M,seq).any(0) # [1, seq], True->Mask\n",
        "        target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "        # print(target_mask, context_mask)\n",
        "        sorted_mask, ids = context_mask.int().sort(dim=1, stable=True)\n",
        "        context_indices = ids[~sorted_mask.bool()] # int idx [num_context_toks] , idx of context not masked\n",
        "        sorted_x = x[torch.arange(batch).unsqueeze(-1), context_indices] # [batch, num_context_toks, d_model]\n",
        "        # print(sorted_x.shape, context_indices.shape)[2, 9, 32]) torch.Size([9])\n",
        "        # print(sorted_x[0])\n",
        "        # print(sorted_x.is_leaf)\n",
        "        sorted_sx = self.context_encoder(sorted_x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "        # del sorted_x\n",
        "        # print(sorted_sx[0])\n",
        "\n",
        "        sorted_mask, ids = target_mask.int().sort(dim=1, stable=True)\n",
        "        # print(sorted_mask.shape, ids.shape, ids[~sorted_mask.bool()].shape, sorted_mask.sum(-1))\n",
        "        trg_indices = ids[sorted_mask.bool()].reshape(M,-1) # int idx [M, num_trg_toks] , idx of targets that are masked\n",
        "        # sorted_x = x[torch.arange(batch)[...,None,None], indices] # [batch, M, seq-num_trg_toks, dim]\n",
        "        # sx = sorted_sx[torch.arange(batch).unsqueeze(-1),ids.argsort(1)]\n",
        "        # print(sorted_sx.shape, trg_indices.shape)\n",
        "        sy_ = self.predicter(sorted_sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "        # sx = self.context_encoder(x, src_key_padding_mask=context_mask).repeat(M,1,1)\n",
        "        # # sy_ = self.predicter(sx, cls_mask=target_mask)*target_mask.unsqueeze(-1)\n",
        "        # sy_ = self.predicter(sx, trg_mask=target_mask)*target_mask.unsqueeze(-1)\n",
        "\n",
        "        # print(sy_.shape, target_mask.shape)\n",
        "        # print(self.target_encoder(x).repeat_interleave(M, dim=0).shape, trg_indices.repeat(batch,1).shape)\n",
        "        with torch.no_grad():\n",
        "            sy = self.target_encoder(x).repeat_interleave(M, dim=0)[torch.arange(batch*M).unsqueeze(-1), trg_indices.repeat(batch,1)] # [batch*M, num_trg_toks, out_dim]\n",
        "\n",
        "        # print(sy_[0])\n",
        "        # print(sy.shape, sy_.shape)\n",
        "        # loss = F.smooth_l1_loss(sy, sy_)\n",
        "        loss = F.mse_loss(sy, sy_)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]\n",
        "        batch, seq, dim = x.shape\n",
        "        target_mask = randpatch(seq//self.patch_size, mask_size=16, gamma=.75) # [seq]\n",
        "        context_mask = ~multiblock(seq//self.patch_size, min_s=0.85, max_s=1., M=1)|target_mask # [1, seq], True->Mask\n",
        "        target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "        # print(target_mask, context_mask)\n",
        "        sorted_mask, ids = context_mask.int().sort(dim=1, stable=True)\n",
        "        context_indices = ids[~sorted_mask.bool()]#.unsqueeze(0) # int idx [num_context_toks] , idx of context not masked\n",
        "        sorted_x = x[torch.arange(batch).unsqueeze(-1), context_indices] # [batch, num_context_toks, 3]\n",
        "        print(sorted_x.shape, context_indices.shape)\n",
        "        sorted_sx = self.context_encoder(sorted_x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "\n",
        "        sorted_mask, ids = target_mask.int().sort(dim=-1, stable=True)\n",
        "        trg_indices = ids[sorted_mask.bool()].unsqueeze(0) # int idx [1, num_trg_toks] , idx of targets that are masked\n",
        "        print(sorted_sx.shape, context_indices.shape)\n",
        "        sy_ = self.predicter(sorted_sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "\n",
        "        with torch.no_grad(): sy = self.target_encoder(x.detach())[torch.arange(batch).unsqueeze(-1), trg_indices.repeat(batch,1)] # [batch, num_trg_toks, out_dim]\n",
        "        # print(x[0][0][:8])\n",
        "        # print(sy_[0][0][:8])\n",
        "        # print(sy.shape, sy_.shape)\n",
        "        # loss = F.smooth_l1_loss(sy, sy_)\n",
        "        loss = F.mse_loss(sy.detach(), sy_)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        x = self.encode(x)\n",
        "        # x = self.encode(x.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "        sx = self.target_encoder(x)\n",
        "        out = sx.mean(dim=1)\n",
        "        return out\n",
        "\n",
        "\n",
        "# min_s=0.15, max_s, M\n",
        "# trg.15.2M4 C.85 1\n",
        "\n",
        "seq_jepa = SeqJEPA(in_dim=3, d_model=32, out_dim=16, num_layers=1).to(device)#.to(torch.float)\n",
        "optim = torch.optim.AdamW(seq_jepa.parameters(), lr=1e-4) # 1e-3?\n",
        "print(sum(p.numel() for p in seq_jepa.parameters() if p.requires_grad)) # 59850\n",
        "\n",
        "\n",
        "# x = torch.rand((2,20,3), device=device)\n",
        "# out = seq_jepa.loss(x)\n",
        "# print(out.shape)\n",
        "# print(x.is_leaf) # T\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title SeqJEPA old\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def multiblock(batch, seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "        mask_len = torch.rand(batch, M) * (max_s - min_s) + min_s # in (min_s, max_s)\n",
        "        mask_pos = torch.rand(batch, M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "        mask_len, mask_pos = mask_len * seq, mask_pos * seq\n",
        "        indices = torch.arange(seq)[None,None,...]\n",
        "        target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [batch, M, seq]\n",
        "        target_mask = target_mask.any(1) # True -> masked # multiblock masking # [batch, seq]\n",
        "        return target_mask\n",
        "\n",
        "\n",
        "\n",
        "class SeqJEPA(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.in_dim = in_dim\n",
        "        # if out_dim is None: self.out_dim = in_dim\n",
        "        if out_dim is None: out_dim = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.d_model = d_model\n",
        "        act = nn.GELU() # ReLU GELU SiLU\n",
        "\n",
        "        dim=8\n",
        "\n",
        "\n",
        "        self.calorie_emb = RotEmb(dim, top=1, base=10) # 0-4\n",
        "        self.steps_emb = RotEmb(dim, top=1, base=10) # 0-5\n",
        "        self.enc0 = nn.Sequential(\n",
        "            nn.Linear(dim*2, d_model), act,\n",
        "            nn.Linear(d_model, d_model), act,\n",
        "        )\n",
        "        self.hour_emb = CircularEmb(dim, torch.pi/2) # circular (0,1) # 0-24\n",
        "        self.temp_emb = RotEmb(dim, top=1/3, base=10) # 18-35\n",
        "        self.heart_emb = RotEmb(dim, top=1/3, base=100) # 50-200\n",
        "        self.enc1 = nn.Sequential(\n",
        "            nn.Linear(dim*3, d_model), act,\n",
        "        )\n",
        "\n",
        "        # self.transformer = TransformerClassifier(d_model, out_dim=out_dim, nhead=8, nlayers=2)\n",
        "        # # self.fc = nn.Linear(d_model, out_dim)\n",
        "        self.context_encoder = TransformerModel(d_model, out_dim=out_dim, nhead=8, nlayers=2, dropout=0.)\n",
        "        self.predicter = TransformerModel(out_dim, nhead=8, nlayers=1, dropout=0.)\n",
        "        self.target_encoder = AveragedModel(self.context_encoder, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    def loss(self, hr, temp, heart): # [batch, 1+T]\n",
        "        # batch = hr.size(0)\n",
        "        res_id, activeKilocalories, steps = hr[:,0], temp[:,0], heart[:,0]\n",
        "        # enc_summary = self.enc0(torch.cat([self.calorie_emb(activeKilocalories), self.steps_emb(steps)], dim=-1)) # [batch, d_model]\n",
        "        enc_summary = self.enc0(torch.cat([self.calorie_emb(torch.log(activeKilocalories.clamp(min=1))), self.steps_emb(torch.log(steps.clamp(min=1)))], dim=-1)) # [batch, d_model]\n",
        "        x = self.enc1(torch.cat([self.hour_emb(hr[:,1:]/24), self.temp_emb(temp[:,1:]), self.heart_emb(heart[:,1:])], dim=-1)) # [batch, T, d_model]\n",
        "        # print('violet fwd', hr.shape, enc_summary.shape, x.shape)\n",
        "\n",
        "        # # mask=(torch.rand(self.seq_len)<.1) # True -> masked # random masking\n",
        "        # mask=(torch.rand_like(x)<.1) # True -> masked # random masking\n",
        "\n",
        "        # patches pad -enc> patch_enc\n",
        "        # rearrange patch in padded/masked, -pred> pred of masked\n",
        "        # mse tgt_enc(img)\n",
        "\n",
        "        # average L2 distance between the predicted patch-level representations sˆy(i) and the target patch-level representation sy(i);\n",
        "        # F.smooth_l1_loss\n",
        "\n",
        "        batch, seq, dim = x.shape\n",
        "\n",
        "        # mask=(torch.rand(batch, seq)<.75) # True -> masked # random masking\n",
        "        # sorted_mask, ids = mask.sort(dim=1)\n",
        "        # # sorted_x = x[torch.arange(batch)[...,None,None],ids]\n",
        "        # sorted_x = x[torch.arange(batch).unsqueeze(-1),ids]\n",
        "        # sorted_mask = torch.cat([torch.zeros((batch, 1), dtype=torch.bool), sorted_mask], dim=1) # True->mask\n",
        "        # sorted_x = torch.cat([enc_summary.unsqueeze(1), sorted_x], dim=1)\n",
        "        # sorted_sx = self.context_encoder(sorted_x, src_key_padding_mask=sorted_mask)\n",
        "        # # torch.zeros_like(sorted_sx)\n",
        "        # sx = sorted_sx[torch.arange(batch).unsqueeze(-1),ids.argsort(1)]\n",
        "        # sy_ = self.predicter(sx, src_key_padding_mask=mask)\n",
        "        # sy = self.target_encoder(x)*~mask\n",
        "\n",
        "\n",
        "        # target_mask = multiblock(batch, seq, min_s=0.15, max_s=0.2, M=4)\n",
        "        # context_mask = ~multiblock(batch, seq, min_s=0.85, max_s=1., M=1)|target_mask\n",
        "        # sx = self.context_encoder(x, src_key_padding_mask=context_mask)\n",
        "        # sy_ = self.predicter(sx, cls_mask=target_mask)*target_mask.unsqueeze(-1)\n",
        "        # sy_ = self.predicter(sx, cls_mask=target_mask)\n",
        "        # # print(sy_.shape, target_mask.shape)\n",
        "        # sy = self.target_encoder(x)*target_mask.unsqueeze(-1)\n",
        "\n",
        "\n",
        "        M=4\n",
        "        target_mask = multiblock(M*batch, seq, min_s=0.15, max_s=0.2, M=1) # mask out targets to be predicted # [4*batch, seq]\n",
        "        context_mask = ~multiblock(batch, seq, min_s=0.85, max_s=1., M=1)|target_mask.reshape(batch,M,seq).any(1) # [batch, seq]\n",
        "\n",
        "\n",
        "        x = torch.cat([enc_summary.unsqueeze(1), x], dim=1) # [batch, 1+T, d_model]\n",
        "        target_mask = torch.cat([torch.zeros((M*batch, 1), dtype=bool), target_mask],dim=1) # [4*batch, 1+T]\n",
        "        context_mask = torch.cat([torch.zeros((batch, 1), dtype=bool), context_mask],dim=1) # [batch, 1+T]\n",
        "\n",
        "        # x = x.repeat(4,1,1)\n",
        "        # context_mask = context_mask.repeat(4,1)\n",
        "        sx = self.context_encoder(x, src_key_padding_mask=context_mask).repeat(M,1,1)\n",
        "        sy_ = self.predicter(sx, cls_mask=target_mask)*target_mask.unsqueeze(-1)\n",
        "        # sy_ = self.predicter(sx, cls_mask=target_mask)\n",
        "        # print(sy_.shape, target_mask.shape)\n",
        "        sy = self.target_encoder(x).repeat(M,1,1)*target_mask.unsqueeze(-1)\n",
        "\n",
        "        loss = F.smooth_l1_loss(sy, sy_)\n",
        "        return loss\n",
        "\n",
        "    # def forward(self, hr, temp, heart): # [batch, 1+T]\n",
        "    #     # batch = hr.size(0)\n",
        "    #     # batch, seq, dim = x.shape\n",
        "    #     res_id, activeKilocalories, steps = hr[:,0], temp[:,0], heart[:,0]\n",
        "    #     enc_summary = self.enc0(torch.cat([self.calorie_emb(activeKilocalories), self.steps_emb(steps)], dim=-1)) # [batch, d_model]\n",
        "    #     x = self.enc1(torch.cat([self.hour_emb(hr[:,1:]/24), self.temp_emb(temp[:,1:]), self.heart_emb(heart[:,1:])], dim=-1)) # [batch, T, d_model]\n",
        "\n",
        "    #     x = torch.cat([enc_summary.unsqueeze(1), x], dim=1)\n",
        "    #     # sx = self.context_encoder(x)\n",
        "    #     sx = self.target_encoder(x)\n",
        "    #     out = sx.mean(dim=1)\n",
        "    #     return out\n",
        "\n",
        "\n",
        "\n",
        "seq_jepa = SeqJEPA(in_dim=16, d_model=32, out_dim=16, num_layers=1).to(device)#.to(torch.float)\n",
        "soptim = torch.optim.AdamW(seq_jepa.parameters(), lr=1e-3)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "O-OU1MaKF7BZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ijepa vision_transformer.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/models/vision_transformer.py\n",
        "import math\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from src.utils.tensors import (trunc_normal_, repeat_interleave_batch)\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid height and width\n",
        "    return:\n",
        "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid_h = np.arange(grid_size, dtype=float)\n",
        "    grid_w = np.arange(grid_size, dtype=float)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    assert embed_dim % 2 == 0\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid length\n",
        "    return:\n",
        "    pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid = np.arange(grid_size, dtype=float)\n",
        "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=float)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega   # (D/2,)\n",
        "    pos = pos.reshape(-1)   # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
        "    emb = np.concatenate([np.sin(out), np.cos(out)], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x, attn\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x, return_attention=False):\n",
        "        y, attn = self.attn(self.norm1(x))\n",
        "        if return_attention:\n",
        "            return attn\n",
        "        x = x + self.drop_path(y)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvEmbed(nn.Module):\n",
        "    \"\"\"3x3 Convolution stems for ViT following ViTC models\"\"\"\n",
        "    def __init__(self, channels, strides, img_size=224, in_chans=3, batch_norm=True):\n",
        "        super().__init__()\n",
        "        # Build the stems\n",
        "        stem = []\n",
        "        channels = [in_chans] + channels\n",
        "        for i in range(len(channels) - 2):\n",
        "            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3, stride=strides[i], padding=1, bias=(not batch_norm))]\n",
        "            if batch_norm:\n",
        "                stem += [nn.BatchNorm2d(channels[i+1])]\n",
        "            stem += [nn.ReLU(inplace=True)]\n",
        "        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n",
        "        self.stem = nn.Sequential(*stem)\n",
        "\n",
        "        # Comptute the number of patches\n",
        "        stride_prod = int(np.prod(strides))\n",
        "        self.num_patches = (img_size[0] // stride_prod)**2\n",
        "\n",
        "    def forward(self, x):\n",
        "        p = self.stem(x)\n",
        "        return p.flatten(2).transpose(1, 2)\n",
        "\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "def apply_masks(x, masks):\n",
        "    \"\"\":param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
        "    :param masks: list of tensors containing indices of patches in [N] to keep\"\"\"\n",
        "    all_x = []\n",
        "    for m in masks:\n",
        "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [batch,T,dim]\n",
        "        all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "    return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "\n",
        "class VisionTransformerPredictor(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, num_patches, embed_dim=768, predictor_embed_dim=384,\n",
        "        depth=6, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.init_std = init_std\n",
        "        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        # --\n",
        "        self.predictor_pos_embed = nn.Parameter(torch.zeros(1, num_patches, predictor_embed_dim), requires_grad=False)\n",
        "        predictor_pos_embed = get_2d_sincos_pos_embed(self.predictor_pos_embed.shape[-1], int(num_patches**.5), cls_token=False)\n",
        "        self.predictor_pos_embed.data.copy_(torch.from_numpy(predictor_pos_embed).float().unsqueeze(0)) # [1, num_patches, predictor_embed_dim]\n",
        "        # --\n",
        "        self.predictor_blocks = nn.ModuleList([Block(dim=predictor_embed_dim, num_heads=num_heads, drop=drop_rate, drop_path=dpr[i]) for i in range(depth)])\n",
        "        self.predictor_norm = nn.LayerNorm(predictor_embed_dim)\n",
        "        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n",
        "        # ------\n",
        "        # trunc_normal_(self.mask_token, std=self.init_std)\n",
        "\n",
        "    def forward(self, x, masks_x, masks):\n",
        "        assert (masks is not None) and (masks_x is not None), 'Cannot run predictor without mask indices'\n",
        "        if not isinstance(masks_x, list): masks_x = [masks_x]\n",
        "        if not isinstance(masks, list): masks = [masks]\n",
        "\n",
        "        # -- Batch Size\n",
        "        B = len(x) // len(masks_x)\n",
        "\n",
        "        # -- map from encoder-dim to pedictor-dim\n",
        "        x = self.predictor_embed(x) # [batch, num_patches, predictor_embed_dim]\n",
        "\n",
        "        # -- add positional embedding to x tokens\n",
        "        x_pos_embed = self.predictor_pos_embed.repeat(B, 1, 1) # [batch, num_patches, predictor_embed_dim]\n",
        "        x += apply_masks(x_pos_embed, masks_x)\n",
        "\n",
        "        _, N_ctxt, D = x.shape\n",
        "\n",
        "        # -- concat mask tokens to x\n",
        "        pos_embs = self.predictor_pos_embed.repeat(B, 1, 1)\n",
        "        pos_embs = apply_masks(pos_embs, masks)\n",
        "        pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x))\n",
        "        # --\n",
        "        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1)\n",
        "        # --\n",
        "        pred_tokens += pos_embs\n",
        "        x = x.repeat(len(masks), 1, 1)\n",
        "        x = torch.cat([x, pred_tokens], dim=1)\n",
        "\n",
        "        # -- fwd prop\n",
        "        for blk in self.predictor_blocks:\n",
        "            x = blk(x)\n",
        "        x = self.predictor_norm(x)\n",
        "\n",
        "        # -- return preds for mask tokens\n",
        "        x = x[:, N_ctxt:]\n",
        "        x = self.predictor_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, img_size=[224], patch_size=16, in_chans=3, embed_dim=768, predictor_embed_dim=384, depth=12, predictor_depth=12,\n",
        "        num_heads=12, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0, norm_layer=nn.LayerNorm, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_features = self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.init_std = init_std\n",
        "        # --\n",
        "        self.patch_embed = PatchEmbed(img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        # --\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=False)\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "        # --\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "    def forward(self, x, masks=None):\n",
        "        if masks is not None:\n",
        "            if not isinstance(masks, list): masks = [masks]\n",
        "\n",
        "        x = self.patch_embed(x)\n",
        "        B, N, D = x.shape # [batch, T, d_model]\n",
        "\n",
        "        # -- add positional embedding to x\n",
        "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
        "        x = x + pos_embed\n",
        "\n",
        "        if masks is not None: x = apply_masks(x, masks)\n",
        "\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            x = blk(x)\n",
        "\n",
        "        if self.norm is not None: x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "    def interpolate_pos_encoding(self, x, pos_embed):\n",
        "        npatch = x.shape[1] - 1\n",
        "        N = pos_embed.shape[1] - 1\n",
        "        if npatch == N: return pos_embed\n",
        "        class_emb, pos_embed = pos_embed[:, 0], pos_embed[:, 1:]\n",
        "        dim = x.shape[-1]\n",
        "        pos_embed = nn.functional.interpolate(pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2), scale_factor=math.sqrt(npatch / N), mode='bicubic',)\n",
        "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
        "\n",
        "# from functools import partial\n",
        "# def vit_predictor(**kwargs):\n",
        "#     model = VisionTransformerPredictor(\n",
        "#         mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "#     return model\n",
        "\n",
        "def vit(patch_size=16, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, # tiny\n",
        "        # patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, # small\n",
        "        # patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, # base\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "bNjp4xFsGPoa",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title show collated_masks\n",
        "# print(collated_batch, collated_masks_enc, collated_masks_pred)\n",
        "# print(collated_batch) # tensor([0, 1, 2, 3]) batch\n",
        "# print(collated_masks_enc) # nenc*[M*[pred mask blocks patch ind]]\n",
        "# print(collated_masks_pred) # npred * [M * [context]]\n",
        "\n",
        "\n",
        "# print(len(collated_masks_enc[0]), len(collated_masks_pred[0]))\n",
        "# print(collated_masks_enc[0], collated_masks_pred[0])\n",
        "\n",
        "h,w=14,14\n",
        "img = torch.ones(h*w)\n",
        "# img[collated_masks_enc[0]]=0\n",
        "img[collated_masks_pred[0][2]]=0\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    # plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.imshow(npimg)\n",
        "    plt.show()\n",
        "\n",
        "# imshow(img.reshape(h,w))\n",
        "\n",
        "# for masks in collated_masks_pred:\n",
        "#     for mask in masks:\n",
        "#         img = torch.ones(h*w)\n",
        "#         img[mask]=0\n",
        "#         imshow(img.reshape(h,w))\n",
        "\n",
        "\n",
        "for masks in collated_masks_enc:\n",
        "    for mask in masks:\n",
        "        img = torch.ones(h*w)\n",
        "        img[mask]=0\n",
        "        imshow(img.reshape(h,w))\n"
      ],
      "metadata": {
        "id": "tf4T37woBV2V",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aH1UOD4WW8I2",
        "outputId": "6fafea1b-0af2-4bc7-fa26-46b5e56549b4",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vit fwd torch.Size([4, 3, 224, 224])\n",
            "vit fwd torch.Size([4, 196, 768])\n",
            "vit fwd torch.Size([4, 11, 768])\n",
            "predictor fwd1 torch.Size([4, 11, 384]) torch.Size([4, 196, 384])\n"
          ]
        }
      ],
      "source": [
        "# @title ijepa vision_transformer.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/models/vision_transformer.py\n",
        "import math\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# from src.utils.tensors import (trunc_normal_, repeat_interleave_batch)\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid height and width\n",
        "    return:\n",
        "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid_h = np.arange(grid_size, dtype=float)\n",
        "    grid_w = np.arange(grid_size, dtype=float)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    assert embed_dim % 2 == 0\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid length\n",
        "    return:\n",
        "    pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid = np.arange(grid_size, dtype=float)\n",
        "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=float)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega   # (D/2,)\n",
        "    pos = pos.reshape(-1)   # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
        "    emb = np.concatenate([np.sin(out), np.cos(out)], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "def repeat_interleave_batch(x, B, repeat):\n",
        "    N = len(x) // B\n",
        "    x = torch.cat([\n",
        "        torch.cat([x[i*B:(i+1)*B] for _ in range(repeat)], dim=0)\n",
        "        for i in range(N)\n",
        "    ], dim=0)\n",
        "    return x\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x, attn\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x, return_attention=False):\n",
        "        y, attn = self.attn(self.norm1(x))\n",
        "        if return_attention:\n",
        "            return attn\n",
        "        x = x + self.drop_path(y)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvEmbed(nn.Module):\n",
        "    \"\"\"3x3 Convolution stems for ViT following ViTC models\"\"\"\n",
        "    def __init__(self, channels, strides, img_size=224, in_chans=3, batch_norm=True):\n",
        "        super().__init__()\n",
        "        # Build the stems\n",
        "        stem = []\n",
        "        channels = [in_chans] + channels\n",
        "        for i in range(len(channels) - 2):\n",
        "            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3, stride=strides[i], padding=1, bias=(not batch_norm))]\n",
        "            if batch_norm:\n",
        "                stem += [nn.BatchNorm2d(channels[i+1])]\n",
        "            stem += [nn.ReLU(inplace=True)]\n",
        "        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n",
        "        self.stem = nn.Sequential(*stem)\n",
        "\n",
        "        # Comptute the number of patches\n",
        "        stride_prod = int(np.prod(strides))\n",
        "        self.num_patches = (img_size[0] // stride_prod)**2\n",
        "\n",
        "    def forward(self, x):\n",
        "        p = self.stem(x)\n",
        "        return p.flatten(2).transpose(1, 2)\n",
        "\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "def apply_masks(x, masks):\n",
        "    \"\"\":param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
        "    :param masks: list of tensors containing indices of patches in [N] to keep\"\"\"\n",
        "    all_x = []\n",
        "    for m in masks:\n",
        "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [batch,T,dim]\n",
        "        all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "    return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "\n",
        "class VisionTransformerPredictor(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, num_patches, embed_dim=768, predictor_embed_dim=384,\n",
        "        depth=6, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.init_std = init_std\n",
        "        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        # --\n",
        "        self.predictor_pos_embed = nn.Parameter(torch.zeros(1, num_patches, predictor_embed_dim), requires_grad=False)\n",
        "        predictor_pos_embed = get_2d_sincos_pos_embed(self.predictor_pos_embed.shape[-1], int(num_patches**.5), cls_token=False)\n",
        "        self.predictor_pos_embed.data.copy_(torch.from_numpy(predictor_pos_embed).float().unsqueeze(0)) # [1, num_patches, predictor_embed_dim]\n",
        "        # --\n",
        "        self.predictor_blocks = nn.ModuleList([Block(dim=predictor_embed_dim, num_heads=num_heads, drop=drop_rate, drop_path=dpr[i]) for i in range(depth)])\n",
        "        self.predictor_norm = nn.LayerNorm(predictor_embed_dim)\n",
        "        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n",
        "        # ------\n",
        "        # trunc_normal_(self.mask_token, std=self.init_std)\n",
        "\n",
        "    def forward(self, x, masks_x, masks): # [batch, num_context_patches, embed_dim], nenc*[batch, num_context_patches], npred*[batch, num_target_patches]\n",
        "        assert (masks is not None) and (masks_x is not None), 'Cannot run predictor without mask indices'\n",
        "        if not isinstance(masks_x, list): masks_x = [masks_x] # context mask\n",
        "        if not isinstance(masks, list): masks = [masks] # pred mask\n",
        "\n",
        "        # -- Batch Size\n",
        "        B = len(x) // len(masks_x)\n",
        "        # print(\"predictor fwd0\", x.shape) # [3, 121, 768])\n",
        "        # -- map from encoder-dim to pedictor-dim\n",
        "        x = self.predictor_embed(x) # [batch, num_patches, predictor_embed_dim]\n",
        "\n",
        "        # -- add positional embedding to x tokens\n",
        "        x_pos_embed = self.predictor_pos_embed.repeat(B, 1, 1) # [batch, num_patches, predictor_embed_dim]\n",
        "        print(\"predictor fwd1\", x.shape, x_pos_embed.shape) # [3, 121 or 11, 384], [3, 196, 384]\n",
        "        # print(\"predictor fwd masks_x\", masks_x)\n",
        "        x += apply_masks(x_pos_embed, masks_x) # get pos emb of context patches\n",
        "\n",
        "        # print(\"predictor fwd x\", x.shape) # [4, 11, 384])\n",
        "        _, N_ctxt, D = x.shape\n",
        "\n",
        "        # -- concat mask tokens to x\n",
        "        pos_embs = self.predictor_pos_embed.repeat(B, 1, 1) # [batch, num_patches, predictor_embed_dim]\n",
        "        pos_embs = apply_masks(pos_embs, masks) # [16, 104, predictor_embed_dim]\n",
        "        pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x)) # [16, 104, predictor_embed_dim]\n",
        "        # --\n",
        "        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1) # [16, 104, predictor_embed_dim]\n",
        "        # --\n",
        "        # print(\"predictor fwd pred_tokens\", pred_tokens.shape)\n",
        "\n",
        "        pred_tokens += pos_embs\n",
        "        x = x.repeat(len(masks), 1, 1)\n",
        "        x = torch.cat([x, pred_tokens], dim=1)\n",
        "\n",
        "        # -- fwd prop\n",
        "        for blk in self.predictor_blocks:\n",
        "            x = blk(x)\n",
        "        x = self.predictor_norm(x)\n",
        "\n",
        "        # -- return preds for mask tokens\n",
        "        x = x[:, N_ctxt:]\n",
        "        x = self.predictor_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, img_size=[224], patch_size=16, in_chans=3, embed_dim=768, predictor_embed_dim=384, depth=12, predictor_depth=12,\n",
        "        num_heads=12, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0, norm_layer=nn.LayerNorm, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_features = self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.init_std = init_std\n",
        "        # --\n",
        "        self.patch_embed = PatchEmbed(img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        # --\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=False)\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "        # --\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "    def forward(self, x, masks=None): # [batch,3,224,224]\n",
        "        if masks is not None:\n",
        "            if not isinstance(masks, list): masks = [masks]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        x = self.patch_embed(x)\n",
        "        B, N, D = x.shape # [batch, num_patches, embed_dim]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        # -- add positional embedding to x\n",
        "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
        "        x = x + pos_embed\n",
        "\n",
        "        if masks is not None: x = apply_masks(x, masks) # [batch, num_context_patches, embed_dim]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            x = blk(x)\n",
        "\n",
        "        if self.norm is not None: x = self.norm(x)\n",
        "        return x # [batch, num_context_patches, embed_dim]\n",
        "\n",
        "    def interpolate_pos_encoding(self, x, pos_embed):\n",
        "        npatch = x.shape[1] - 1\n",
        "        N = pos_embed.shape[1] - 1\n",
        "        if npatch == N: return pos_embed\n",
        "        class_emb, pos_embed = pos_embed[:, 0], pos_embed[:, 1:]\n",
        "        dim = x.shape[-1]\n",
        "        pos_embed = nn.functional.interpolate(pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2), scale_factor=math.sqrt(npatch / N), mode='bicubic',)\n",
        "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
        "\n",
        "# from functools import partial\n",
        "# def vit_predictor(**kwargs):\n",
        "#     model = VisionTransformerPredictor(\n",
        "#         mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "#     return model\n",
        "\n",
        "def vit(patch_size=16, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, # tiny\n",
        "        # patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, # small\n",
        "        # patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, # base\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "# encoder = vit()\n",
        "encoder = VisionTransformer(\n",
        "        patch_size=16, embed_dim=768, depth=1, num_heads=3, mlp_ratio=1,\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
        "\n",
        "# num_patches = (224/patch_size)^2\n",
        "# predictor = VisionTransformerPredictor(num_patches, embed_dim=768, predictor_embed_dim=384, depth=6, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02, **kwargs)\n",
        "predictor = VisionTransformerPredictor(num_patches=196, embed_dim=768, predictor_embed_dim=384, depth=1, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02)\n",
        "\n",
        "batch = 4\n",
        "img = torch.rand(batch, 3, 224, 224)\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/train.py\n",
        "mask_collator = MaskCollator(\n",
        "        input_size=(224, 224),\n",
        "        patch_size=16,\n",
        "        enc_mask_scale=(0.2, 0.8),\n",
        "        pred_mask_scale=(0.2, 0.8),\n",
        "        aspect_ratio=(0.3, 3.0),\n",
        "        nenc=1,\n",
        "        npred=4,\n",
        "        min_keep=4,\n",
        "        # allow_overlap=True)\n",
        "        allow_overlap=False)\n",
        "# self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
        "\n",
        "\n",
        "\n",
        "# v = mask_collator.step()\n",
        "# should pass the collater a list batch of idx?\n",
        "# collated_batch, collated_masks_enc, collated_masks_pred = mask_collator([batch,3,8])\n",
        "collated_batch, collated_masks_enc, collated_masks_pred = mask_collator([0,1,2,3])\n",
        "# print(v)\n",
        "\n",
        "\n",
        "import copy\n",
        "target_encoder = copy.deepcopy(encoder)\n",
        "\n",
        "\n",
        "def forward_target():\n",
        "    with torch.no_grad():\n",
        "        h = target_encoder(imgs)\n",
        "        h = F.layer_norm(h, (h.size(-1),))  # normalize over feature-dim\n",
        "        B = len(h)\n",
        "        # -- create targets (masked regions of h)\n",
        "        h = apply_masks(h, masks_pred)\n",
        "        h = repeat_interleave_batch(h, B, repeat=len(masks_enc))\n",
        "        return h\n",
        "\n",
        "def forward_context():\n",
        "    z = encoder(imgs, masks_enc)\n",
        "    z = predictor(z, masks_enc, masks_pred)\n",
        "    return z\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    m = next(momentum_scheduler)\n",
        "                    for param_q, param_k in zip(encoder.parameters(), target_encoder.parameters()):\n",
        "                        param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "\n",
        "\n",
        "# # num_context_patches = 121 if allow_overlap=True else 11\n",
        "z = encoder(img, collated_masks_enc) # [batch, num_context_patches, embed_dim]\n",
        "# print(len(collated_masks_enc), len(collated_masks_pred)) # nenc, npred\n",
        "# print(collated_masks_enc[0].shape, collated_masks_pred[0].shape) # , [batch, num_context_patches = 121 or 11], [batch, num_target_patches]\n",
        "out = predictor(z, collated_masks_enc, collated_masks_pred)\n",
        "# # print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test multiblock params\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# dataiter = iter(train_data)\n",
        "img,y = next(dataiter)\n",
        "img = img.unsqueeze(0)\n",
        "b,c,h,w = img.shape\n",
        "img = img.flatten(2).transpose(-2,-1).to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "print(img.shape)\n",
        "batch, seq,_ = img.shape\n",
        "M=4\n",
        "target_mask = multiblock(seq, min_s=0.15, max_s=0.2, M=M) # mask out targets to be predicted # [M, seq]\n",
        "context_mask = ~multiblock(seq, min_s=0.85, max_s=1., M=1)|target_mask.reshape(M,seq).any(0) # [1, seq], True->Mask\n",
        "target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "# print(target_mask, context_mask)\n",
        "\n",
        "\n",
        "# print(target_mask.shape, context_mask.shape)\n",
        "# target_img, context_img = img*target_mask.unsqueeze(-1), img*context_mask.unsqueeze(-1)\n",
        "target_img, context_img = img*~target_mask.unsqueeze(-1), img*~context_mask.unsqueeze(-1)\n",
        "# print(target_img.shape, context_img.shape)\n",
        "target_img, context_img = target_img.transpose(-2,-1).reshape(M,c,h,w), context_img.transpose(-2,-1).reshape(b,c,h,w)\n",
        "target_img, context_img = target_img.float(), context_img.float()\n",
        "\n",
        "# imshow(out.detach().cpu())\n",
        "imshow(target_img[0])\n",
        "imshow(context_img[0])\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "D6lVtbS5OHIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test multiblock mask\n",
        "def multiblock(seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    # mask_len, mask_pos = mask_len * seq, mask_pos * seq\n",
        "    indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "    return target_mask\n",
        "\n",
        "\n",
        "seq=400\n",
        "M=4\n",
        "\n",
        "# target_mask = multiblock(M, seq, min_s=0.15, max_s=0.2, M=1) # mask out targets to be predicted # [4*batch, seq]\n",
        "# context_mask = ~multiblock(1, seq, min_s=0.85, max_s=1., M=1)|target_mask.reshape(M,seq).any(0) # [batch, seq]\n",
        "\n",
        "\n",
        "target_mask = multiblock(seq, min_s=0.15, max_s=0.2, M=4) # mask out targets to be predicted # [4*batch, seq]\n",
        "context_mask = ~multiblock(seq, min_s=0.85, max_s=1., M=1)|target_mask.reshape(M,seq).any(0) # [batch, seq]\n",
        "\n",
        "# target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "\n",
        "# print(target_mask)\n",
        "# print(context_mask)\n",
        "\n",
        "print(target_mask.sum(-1))\n",
        "print(context_mask.sum(-1))\n",
        "\n",
        "\n",
        "# sorted_mask, ids = context_mask.sort(dim=1, stable=True)\n",
        "# indices = ids[~sorted_mask]#.reshape(M,-1) # int idx [num_context_toks] , idx of context not masked\n",
        "# sorted_x = x[torch.arange(batch).unsqueeze(-1), indices] # [batch, seq-num_trg_toks, dim]\n",
        "# print(indices)\n",
        "# print(x)\n",
        "# print(sorted_x)\n",
        "\n",
        "\n",
        "\n",
        "# sorted_mask, ids = target_mask.sort(dim=1, descending=False, stable=True)\n",
        "# # print(sorted_mask, ids)\n",
        "# indices = ids[~sorted_mask].reshape(M,-1) # int idx [M, seq-num_trg_toks] , idx of target not masked\n",
        "# print(indices)\n",
        "# batch=3\n",
        "# dim=2\n",
        "# # indices = indices.expand(batch,-1,-1)\n",
        "# # x = torch.arange(batch*seq).reshape(batch,seq).to(device)\n",
        "# x = torch.rand(batch, seq, dim)\n",
        "# print(x)\n",
        "# sorted_x = x[torch.arange(batch)[...,None,None], indices]\n",
        "# print(sorted_x.shape) # [batch, M, seq-num_trg_toks, dim]\n",
        "# print(sorted_x)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "p8xv1tpKIhim",
        "outputId": "e6613fee-2d32-4c6a-ef96-dee18da3aa6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([73, 73, 73, 73])\n",
            "tensor([248])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ijepa multiblock down\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "import math\n",
        "from multiprocessing import Value\n",
        "import torch\n",
        "_GLOBAL_SEED = 0\n",
        "\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=(224, 224),\n",
        "        patch_size=16,\n",
        "        enc_mask_scale=(0.2, 0.8),\n",
        "        pred_mask_scale=(0.2, 0.8),\n",
        "        aspect_ratio=(0.3, 3.0),\n",
        "        nenc=1,\n",
        "        npred=2,\n",
        "        min_keep=4,\n",
        "        allow_overlap=False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if not isinstance(input_size, tuple):\n",
        "            input_size = (input_size, ) * 2\n",
        "        self.patch_size = patch_size\n",
        "        self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.aspect_ratio = aspect_ratio\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "        self._itr_counter = Value('i', -1)  # collator is shared across worker processes\n",
        "\n",
        "    def step(self):\n",
        "        i = self._itr_counter\n",
        "        with i.get_lock():\n",
        "            i.value += 1\n",
        "            v = i.value\n",
        "        return v\n",
        "\n",
        "    def _sample_block_size(self, generator, scale, aspect_ratio_scale):\n",
        "        _rand = torch.rand(1, generator=generator).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.height * self.width * mask_scale)\n",
        "        # -- Sample block aspect-ratio\n",
        "        min_ar, max_ar = aspect_ratio_scale\n",
        "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
        "        while h >= self.height:\n",
        "            h -= 1\n",
        "        while w >= self.width:\n",
        "            w -= 1\n",
        "\n",
        "        return (h, w)\n",
        "\n",
        "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
        "        h, w = b_size\n",
        "\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # --\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            top = torch.randint(0, self.height - h, (1,))\n",
        "            left = torch.randint(0, self.width - w, (1,))\n",
        "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
        "            mask[top:top+h, left:left+w] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
        "        mask_complement[top:top+h, left:left+w] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        B = len(batch)\n",
        "\n",
        "        collated_batch = torch.utils.data.default_collate(batch)\n",
        "\n",
        "        seed = self.step()\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(seed)\n",
        "        p_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.pred_mask_scale,\n",
        "            aspect_ratio_scale=self.aspect_ratio)\n",
        "        e_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.enc_mask_scale,\n",
        "            aspect_ratio_scale=(1., 1.))\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.height * self.width\n",
        "        min_keep_enc = self.height * self.width\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                print(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "\n",
        "        return collated_batch, collated_masks_enc, collated_masks_pred\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "COvEnBXPYth3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}