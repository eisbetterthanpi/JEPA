{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/Seq_JEPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FA00-tf6MJ-s",
        "outputId": "3b22bb08-3a44-4e2b-e756-bb4060074ee0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-08 01:44:03--  https://archive.ics.uci.edu/static/public/507/wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset.zip’\n",
            "\n",
            "wisdm+smartphone+an     [  <=>               ] 295.92M  5.72MB/s    in 24s     \n",
            "\n",
            "2025-04-08 01:44:28 (12.3 MB/s) - ‘wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset.zip’ saved [310292805]\n",
            "\n",
            "Archive:  wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset.zip\n",
            " extracting: WISDM-dataset-description.pdf  \n",
            " extracting: wisdm-dataset.zip       \n",
            "Archive:  wisdm-dataset.zip\n",
            "   creating: wisdm-dataset/\n",
            "  inflating: wisdm-dataset/WISDM-dataset-description.pdf  \n",
            "   creating: wisdm-dataset/arffmagic-master/\n",
            "  inflating: wisdm-dataset/arffmagic-master/Makefile  \n",
            "  inflating: wisdm-dataset/arffmagic-master/.DS_Store  \n",
            " extracting: wisdm-dataset/arffmagic-master/README.md  \n",
            "   creating: wisdm-dataset/arffmagic-master/src/\n",
            "  inflating: wisdm-dataset/arffmagic-master/src/arff.cpp  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/comparator.h  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/chunk.h  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/main.cpp  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/attribute.h  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/libmfcc.c  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/raw.h  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/try.h  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/write.h  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/chunk.cpp  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/raw.cpp  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/globals.h  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/write.cpp  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/arff.h  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/except.h  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/read.cpp  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/attribute.cpp  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/libmfcc.h  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/globals.cpp  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/funcmap.cpp  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/read.h  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/funcmap.h  \n",
            "   creating: wisdm-dataset/arffmagic-master/bin/\n",
            "  inflating: wisdm-dataset/arffmagic-master/bin/magic  \n",
            "   creating: wisdm-dataset/arffmagic-master/bin/arffmagic.dSYM/\n",
            "   creating: wisdm-dataset/arffmagic-master/bin/arffmagic.dSYM/Contents/\n",
            "  inflating: wisdm-dataset/arffmagic-master/bin/arffmagic.dSYM/Contents/Info.plist  \n",
            "   creating: wisdm-dataset/arffmagic-master/bin/arffmagic.dSYM/Contents/Resources/\n",
            "   creating: wisdm-dataset/arffmagic-master/bin/arffmagic.dSYM/Contents/Resources/DWARF/\n",
            "  inflating: wisdm-dataset/arffmagic-master/bin/arffmagic.dSYM/Contents/Resources/DWARF/arffmagic  \n",
            "  inflating: wisdm-dataset/arffmagic-master/bin/.DS_Store  \n",
            "  inflating: wisdm-dataset/arffmagic-master/bin/arffmagic  \n",
            "   creating: wisdm-dataset/arffmagic-master/build/\n",
            "  inflating: wisdm-dataset/arffmagic-master/build/libmfcc.o  \n",
            "  inflating: wisdm-dataset/arffmagic-master/build/write.o  \n",
            "  inflating: wisdm-dataset/arffmagic-master/build/attribute.o  \n",
            "  inflating: wisdm-dataset/arffmagic-master/build/arff.o  \n",
            "  inflating: wisdm-dataset/arffmagic-master/build/read.o  \n",
            "  inflating: wisdm-dataset/arffmagic-master/build/raw.o  \n",
            "  inflating: wisdm-dataset/arffmagic-master/build/funcmap.o  \n",
            "  inflating: wisdm-dataset/arffmagic-master/build/main.o  \n",
            "  inflating: wisdm-dataset/arffmagic-master/build/globals.o  \n",
            "  inflating: wisdm-dataset/arffmagic-master/build/chunk.o  \n",
            "  inflating: wisdm-dataset/README.txt  \n",
            "   creating: wisdm-dataset/raw/\n",
            "   creating: wisdm-dataset/raw/phone/\n",
            "   creating: wisdm-dataset/raw/phone/gyro/\n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1631_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1644_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1639_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1621_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1609_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1627_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1628_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1620_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1633_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1638_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1614_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1625_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1612_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1640_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1616_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1647_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1646_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1637_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1608_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1642_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1624_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/.DS_Store  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1643_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1645_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1641_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1626_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1623_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1607_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1629_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1610_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1649_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1648_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1636_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1630_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1600_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1603_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1606_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1604_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1611_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1632_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1618_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1635_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1605_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1602_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1613_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1617_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1619_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1615_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1650_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1634_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1622_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1601_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/.DS_Store  \n",
            "   creating: wisdm-dataset/raw/phone/accel/\n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1635_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1608_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1649_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1647_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1619_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1621_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1630_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1639_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1615_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1611_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1616_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1644_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1648_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1645_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1641_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1603_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1628_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1610_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1634_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1600_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/.DS_Store  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1613_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1624_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1620_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1606_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1633_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1632_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1626_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1643_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1637_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1640_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1627_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1650_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1614_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1617_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1618_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1642_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1612_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1646_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1601_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1605_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1623_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1607_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1638_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1636_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1629_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1604_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1622_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1625_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1602_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1609_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1631_accel_phone.txt  \n",
            "   creating: wisdm-dataset/raw/watch/\n",
            "   creating: wisdm-dataset/raw/watch/gyro/\n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1629_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1633_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1626_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1644_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1625_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1649_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1631_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1632_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1622_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1628_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1602_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1603_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1610_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1609_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1647_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1638_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1642_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1627_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1601_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1646_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1643_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1636_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1635_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1617_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/.DS_Store  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1612_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1620_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1623_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1615_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1621_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1645_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1641_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1616_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1608_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1605_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1607_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1648_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1600_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1637_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1604_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1639_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1611_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1613_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1614_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1640_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1606_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1630_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1618_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1634_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1650_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1619_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1624_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/.DS_Store  \n",
            "   creating: wisdm-dataset/raw/watch/accel/\n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1626_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1631_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1605_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1608_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1600_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1650_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1623_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1612_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1637_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1620_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1633_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1618_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1635_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1619_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1611_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1614_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1601_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1616_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/.DS_Store  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1645_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1606_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1615_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1638_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1636_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1607_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1627_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1625_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1640_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1643_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1602_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1644_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1629_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1642_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1610_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1624_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1603_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1621_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1646_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1628_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1604_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1634_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1613_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1647_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1609_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1630_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1639_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1617_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1648_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1632_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1649_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1641_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1622_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/.DS_Store  \n",
            "   creating: wisdm-dataset/arff_files/\n",
            "   creating: wisdm-dataset/arff_files/phone/\n",
            "   creating: wisdm-dataset/arff_files/phone/gyro/\n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1610_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1612_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1637_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1622_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1604_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1639_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1621_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1623_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1640_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1645_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1602_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1649_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1600_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1644_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1646_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1632_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1608_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1635_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1606_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1629_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1617_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1628_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1636_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1601_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/.DS_Store  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1615_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1611_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1620_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1647_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1631_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1648_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1603_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1627_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1619_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1607_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1643_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1624_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1638_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1618_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1605_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1625_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1626_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1650_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1616_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1609_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1613_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1630_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1634_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1641_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1633_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1642_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/.DS_Store  \n",
            "   creating: wisdm-dataset/arff_files/phone/accel/\n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1633_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1612_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1620_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1628_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1602_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1636_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1604_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1638_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1647_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1607_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1648_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1643_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1627_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1626_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1611_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1600_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1635_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1613_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1617_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1641_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1642_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1637_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1621_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/.DS_Store  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1632_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1629_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1630_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1603_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1631_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1615_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1609_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1644_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1616_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1634_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1622_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1649_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1645_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1606_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1601_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1639_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1610_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1608_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1624_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1618_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1650_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1646_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1625_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1640_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1619_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1605_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1623_accel_phone.arff  \n",
            "   creating: wisdm-dataset/arff_files/watch/\n",
            "   creating: wisdm-dataset/arff_files/watch/gyro/\n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1620_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1604_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1613_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1631_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1635_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1639_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1628_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1603_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1632_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1626_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1618_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1627_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1609_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1646_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1650_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1629_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1621_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1607_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1611_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1637_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1602_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1617_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1619_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/.DS_Store  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1643_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1630_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1641_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1647_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1616_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1648_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1623_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1606_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1634_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1622_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1645_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1642_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1633_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1615_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1640_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1605_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1610_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1624_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1638_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1612_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1600_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1636_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1601_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1644_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1608_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1625_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1649_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/.DS_Store  \n",
            "   creating: wisdm-dataset/arff_files/watch/accel/\n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1623_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1608_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1641_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1635_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1643_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1604_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1633_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1630_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1609_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1649_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1622_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1607_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1634_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1618_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1610_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1650_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1647_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1642_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1646_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1617_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1637_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1626_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1645_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1612_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1615_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/.DS_Store  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1620_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1632_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1613_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1616_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1644_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1628_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1603_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1606_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1625_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1648_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1639_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1602_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1619_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1636_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1638_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1640_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1624_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1600_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1629_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1631_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1611_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1601_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1621_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1627_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1605_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/.DS_Store  \n",
            "  inflating: wisdm-dataset/change_raw_act.pl  \n",
            "  inflating: wisdm-dataset/.activity_key.txt.swp  \n",
            "  inflating: wisdm-dataset/activity_key.txt  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-e4f219ecf2d3>:30: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  activity_dataframe = pd.concat([activity_dataframe, df], ignore_index=True)\n",
            "<ipython-input-1-e4f219ecf2d3>:30: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  activity_dataframe = pd.concat([activity_dataframe, df], ignore_index=True)\n",
            "<ipython-input-1-e4f219ecf2d3>:30: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  activity_dataframe = pd.concat([activity_dataframe, df], ignore_index=True)\n",
            "<ipython-input-1-e4f219ecf2d3>:30: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  activity_dataframe = pd.concat([activity_dataframe, df], ignore_index=True)\n"
          ]
        }
      ],
      "source": [
        "# @title rohanpandey WISDM\n",
        "# https://github.com/rohanpandey/Analysis--WISDM-Smartphone-and-Smartwatch-Activity/blob/master/dataset_creation.ipynb\n",
        "! wget https://archive.ics.uci.edu/static/public/507/wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset.zip\n",
        "!unzip wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset.zip\n",
        "!unzip wisdm-dataset.zip\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "folder1=glob.glob(\"wisdm-dataset/raw/*\")\n",
        "# print(folder1)\n",
        "column_names = ['ID', 'activity','timestamp','x','y','z']\n",
        "overall_dataframe=pd.DataFrame(columns = column_names)\n",
        "for subfolder in folder1:\n",
        "    parent_dir = \"./processed/\"\n",
        "    path = os.path.join(parent_dir, subfolder.split('\\\\')[-1])\n",
        "    if not os.path.exists(path): os.makedirs(path)\n",
        "    folder2=glob.glob(subfolder+\"/*\")\n",
        "    for subsubfolder in folder2:\n",
        "        activity_dataframe = pd.DataFrame(columns = column_names)\n",
        "        subfolder_path = os.path.join(path, subsubfolder.split('/')[-1])\n",
        "        if not os.path.exists(subfolder_path): os.makedirs(subfolder_path)\n",
        "        files=glob.glob(subsubfolder+\"/*\")\n",
        "        for file in files:\n",
        "            # print(file)\n",
        "            df = pd.read_csv(file, sep=\",\",header=None)\n",
        "            df.columns = ['ID','activity','timestamp','x','y','z']\n",
        "            # activity_dataframe=activity_dataframe.append(df)\n",
        "            activity_dataframe = pd.concat([activity_dataframe, df], ignore_index=True)\n",
        "\n",
        "        activity_dataframe['z']=activity_dataframe['z'].str[:-1]\n",
        "        # activity_dataframe['meter']=subsubfolder.split('/')[-1]\n",
        "        # activity_dataframe['device']=subfolder.split('/')[-1]\n",
        "        activity_dataframe.to_csv(subfolder_path+'/data.csv',index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "CNVNCV8CGtR_"
      },
      "outputs": [],
      "source": [
        "# @title wisdm dataloader\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "# /content/processed/wisdm-dataset/raw/phone/accel/data.csv\n",
        "# /content/processed/wisdm-dataset/raw/watch/gyro/data.csv\n",
        "\n",
        "class BufferDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        # df_keep = pd.read_csv(\"data.csv\")#[['ID','activity','timestamp','x','y','z']]\n",
        "        df_keep = pd.read_csv(\"/content/processed/wisdm-dataset/raw/watch/accel/data.csv\")\n",
        "\n",
        "        user_acts = dict(tuple(df_keep.groupby(['ID','activity'])[['timestamp','x','y','z']]))\n",
        "        self.data = [[d.to_numpy(), a] for a, d in user_acts.items()]\n",
        "        self.act_dict = {i: act for i, act in enumerate(df_keep['activity'].unique())}\n",
        "        self.act_invdict = {v: k for k, v in self.act_dict.items()} # {'A': 0, 'B': 1, ...\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, id_act = self.data[idx]\n",
        "        id_act = self.process(id_act)\n",
        "        return torch.tensor(x[:3500]), id_act # 3567\n",
        "\n",
        "    def process(self, id_act):\n",
        "        return int(id_act[0]), self.act_invdict[id_act[1]]\n",
        "\n",
        "    def transform(self, act_list): # rand resize crop, rand mask # https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html\n",
        "        act_list = np.array(act_list)\n",
        "        try: hr, temp, heart= zip(*act_list)\n",
        "        except ValueError: print('err:',act_list)\n",
        "        kind = 'nearest' if len(act_list)>self.seq_len/.7 else 'linear'\n",
        "        temp_interpolator = scipy.interpolate.interp1d(hr, temp, kind=kind) # linear nearest quadratic cubic\n",
        "        heart_interpolator = scipy.interpolate.interp1d(hr, heart, kind=kind) # linear nearest quadratic cubic\n",
        "        hr_ = np.sort(np.random.uniform(hr[0], hr[-1], round(self.seq_len*random.uniform(1,1/.7))))\n",
        "        temp_, heart_ = temp_interpolator(hr_), heart_interpolator(hr_)\n",
        "        act_list = list(zip(hr_, temp_, heart_))\n",
        "\n",
        "        idx = torch.randint(len(act_list)-self.seq_len+1, size=(1,))\n",
        "        # return act_list[idx: idx+self.seq_len]\n",
        "        act_list = act_list[idx: idx+self.seq_len]\n",
        "\n",
        "        # mask=(torch.rand(self.seq_len)<.1) # True -> masked # random masking\n",
        "        # # act_list[mask] = self.pad[0]\n",
        "        # act_list = [self.pad[0] if m else a for m,a in zip(mask, act_list)]\n",
        "        return act_list\n",
        "\n",
        "    def add(self, episode):\n",
        "        self.data.append(episode)\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    # print(npimg.shape) # (3, 64, 64)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# df_keep = pd.read_csv(\"data.csv\")#[['ID','activity','timestamp','x','y','z']]\n",
        "# user_acts = dict(tuple(df_keep.groupby(['ID','activity'])[['timestamp','x','y','z']]))\n",
        "# dataset = [[d.to_numpy(), a] for a, d in user_acts.items()]\n",
        "\n",
        "\n",
        "train_data = BufferDataset() # one line of poem is roughly 50 characters\n",
        "\n",
        "dataset_size = len(train_data)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(0.7 * dataset_size))\n",
        "np.random.seed(0)\n",
        "np.random.shuffle(indices)\n",
        "train_indices, val_indices = indices[:split], indices[split:]\n",
        "\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "valid_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 64 #512\n",
        "# train_loader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "# test_loader = DataLoader(train_data, sampler=valid_sampler, batch_size=batch_size, num_workers=2, drop_last=True)\n",
        "train_loader = DataLoader(train_data, sampler=train_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "test_loader = DataLoader(train_data, sampler=valid_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "ge36SCxOl2Oq"
      },
      "outputs": [],
      "source": [
        "# @title RoPE\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def RoPE(dim, seq_len=512, base=10000):\n",
        "    theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "    pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "    angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "    rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [1, seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "    return rot_emb\n",
        "\n",
        "# class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "#     def __init__(self, dim, seq_len=512, base=10000):\n",
        "#         super().__init__()\n",
        "#         self.dim, self.base = dim, base\n",
        "#         theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "#         pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "#         angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "#         self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         seq_len = x.size(1)\n",
        "#         if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.base)\n",
        "#         return x * self.rot_emb[:seq_len]\n",
        "\n",
        "class RotEmb(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, top=torch.pi, base=10000):\n",
        "        super().__init__()\n",
        "        self.theta = top / (base ** (torch.arange(0, dim, step=2, device=device) / dim))\n",
        "        # self.theta = top / (base ** torch.linspace(0, 1, dim//2, device=device))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (pos.unsqueeze(-1) * self.theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [seq_len, dim]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "f6T4F651kmGh"
      },
      "outputs": [],
      "source": [
        "# @title AttentionBlock\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class SelfAttn(nn.Module):\n",
        "    def __init__(self, dim, n_heads):\n",
        "        super().__init__()\n",
        "        self.dim, self.n_heads = dim, n_heads\n",
        "        d_head = dim//n_heads\n",
        "        self.qkv = nn.Linear(dim, dim*3, bias=False)\n",
        "        # self.lin = nn.Linear(dim, dim)\n",
        "        self.lin = zero_module(nn.Linear(dim, dim))\n",
        "        # self.rope = RoPE(d_head, seq_len=512, base=10000)\n",
        "        # self.rope = RoPE2D(d_head, h=64, w=64, base=100)\n",
        "        self.scale = d_head**-.5\n",
        "\n",
        "    def forward(self, x): # [b,t,d]\n",
        "        q,k,v = self.qkv(x).unflatten(-1, (self.n_heads,-1)).transpose(1,2).chunk(3, dim=-1) # [b, t, n_heads, d_head] -> [b, n_heads, t, d_head]\n",
        "        # q, k = self.rope(q), self.rope(k)\n",
        "        q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2)\n",
        "        context = k.transpose(-2,-1) @ v # [batch, n_heads, d_head, d_head]\n",
        "        x = q @ context # [b, n_heads, t, d_head]\n",
        "        # print('SelfAttn', x.shape)\n",
        "        x = x.transpose(1,2).flatten(2)\n",
        "        return self.lin(x)\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, mult=4, drop=0.):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        # self.norm = nn.RMSNorm(d_model, elementwise_affine=False) # LayerNorm RMSNorm\n",
        "        self.norm1, self.norm2 = nn.LayerNorm(d_model), nn.LayerNorm(d_model)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "        self.attn = SelfAttn(d_model, n_heads)\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        ff_dim=d_model*mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), act, nn.Dropout(drop), nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "            nn.RMSNorm(ff_dim), act, nn.Dropout(drop), zero_module(nn.Linear(ff_dim, d_model))\n",
        "        )\n",
        "\n",
        "    def forward(self, x): # [batch, seq_len, d_model]\n",
        "        # print('attnblk fwd',x.shape)\n",
        "        x = x + self.drop(self.attn(self.norm1(x)))\n",
        "        x = x + self.ff(x)\n",
        "        # x = x + self.drop(self.ff(self.norm2(x)))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "pQfM2fYvcTh6"
      },
      "outputs": [],
      "source": [
        "# @title masks\n",
        "import torch\n",
        "\n",
        "def multiblock(seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "    return target_mask\n",
        "\n",
        "def multiblock(seq, min_s, max_s, B=64, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    # mask_len = torch.rand(B, 1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_len = torch.rand(1, M) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(B, M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    # indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    indices = torch.arange(seq)[None,None,...] # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [B, M, seq]\n",
        "    return target_mask\n",
        "\n",
        "def multiblock2d(hw=(8,8), scale=(.15,.2), aspect_ratio=(.75,1.5), M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_aspect = torch.rand(1) * (aspect_ratio[1] - aspect_ratio[0]) + aspect_ratio[0] # in (min_s, max_s) # all blocks same size\n",
        "    mask_scale = torch.rand(1) * (scale[1] - scale[0]) + scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    h = (mask_scale/mask_aspect)**.5# h*(h*aspect) = scale\n",
        "    w = h * mask_aspect\n",
        "    h_pos, w_pos = torch.rand(M)*(1-w), torch.rand(M)*(1-h) # in (0, 1 - mask_len)\n",
        "    h_len, h_pos = (h*hw[0]).int(), h_pos*hw[0]\n",
        "    w_len, w_pos = (w*hw[1]).int(), w_pos*hw[1]\n",
        "    h_ind, w_ind = torch.arange(hw[0]).unsqueeze(0), torch.arange(hw[1]).unsqueeze(0) # [1, seq]\n",
        "    h_mask = (h_ind>=h_pos.unsqueeze(-1)) & (h_ind<(h_pos+h_len).unsqueeze(-1)) # [M, seq]\n",
        "    w_mask = (w_ind>=w_pos.unsqueeze(-1)) & (w_ind<(w_pos+w_len).unsqueeze(-1)) # [M, seq]\n",
        "    target_mask = h_mask.unsqueeze(-1) & w_mask.unsqueeze(-2) # [M, seq, seq]\n",
        "    return target_mask\n",
        "\n",
        "def multiblock2d(hw=(8,8), scale=(.15,.2), aspect_ratio=(.75,1.5), B=64, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_aspect = torch.rand(B, 1) * (aspect_ratio[1] - aspect_ratio[0]) + aspect_ratio[0] # in (min_s, max_s) # all blocks same size\n",
        "    mask_scale = torch.rand(B, 1) * (scale[1] - scale[0]) + scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    h = (mask_scale/mask_aspect)**.5# h*(h*aspect) = scale\n",
        "    w = h * mask_aspect\n",
        "    h_pos, w_pos = torch.rand(M)*(1-w), torch.rand(M)*(1-h) # in (0, 1 - mask_len)\n",
        "    h_len, h_pos = (h*hw[0]).int(), h_pos*hw[0]\n",
        "    w_len, w_pos = (w*hw[1]).int(), w_pos*hw[1]\n",
        "    h_ind, w_ind = torch.arange(hw[0]).unsqueeze(0), torch.arange(hw[1]).unsqueeze(0) # [1, seq]\n",
        "    h_mask = (h_ind>=h_pos.unsqueeze(-1)) & (h_ind<(h_pos+h_len).unsqueeze(-1)) # [M, seq]\n",
        "    w_mask = (w_ind>=w_pos.unsqueeze(-1)) & (w_ind<(w_pos+w_len).unsqueeze(-1)) # [M, seq]\n",
        "    target_mask = h_mask.unsqueeze(-1) & w_mask.unsqueeze(-2) # [M, seq, seq]\n",
        "    return target_mask\n",
        "\n",
        "# https://arxiv.org/pdf/2210.07224\n",
        "def randpatch(seq, mask_size=8, gamma=0.9): # num patches of seq, mask patch size, masking ratio\n",
        "    # mask = torch.rand(seq//mask_size)<gamma\n",
        "    length = seq//mask_size\n",
        "    g = torch.normal(gamma, std=.1, size=(1,)).clamp(.5,.9)\n",
        "    # g = gamma\n",
        "    idx = torch.randperm(length)[:int(length*g)]\n",
        "    mask = torch.zeros(length, dtype=bool)\n",
        "    mask[idx] = True\n",
        "    mask = mask.repeat_interleave(mask_size, dim=-1)\n",
        "    return mask # [seq] , True -> mask\n",
        "\n",
        "\n",
        "import torch\n",
        "def apply_masks(x, mask): # [b,t,d], [mask_size] # https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "    mask_keep = mask.unsqueeze(-1).repeat(x.size(0), 1, x.size(-1)) # [batch,T,dim]\n",
        "    return torch.gather(x, dim=1, index=mask_keep) # [batch,mask_size,dim]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xn7WZShwWxF8",
        "outputId": "53ffdfb6-3cc0-448a-edeb-9b0fc9a81c3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 1, 44]) torch.Size([64, 4, 37])\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 0.], device='cuda:0')\n",
            "torch.Size([64, 148])\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# @title ijepa multiblock 1d\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "import math\n",
        "from multiprocessing import Value\n",
        "import torch\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(self, length=200,\n",
        "        enc_mask_scale=(0.2, 0.8), pred_mask_scale=(0.2, 0.8),\n",
        "        nenc=1, npred=2, min_keep=4, allow_overlap=False):\n",
        "        super().__init__()\n",
        "        self.length = length\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "\n",
        "    def _sample_block_size(self, scale):\n",
        "        _rand = torch.rand(1).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.length * mask_scale) # num patches to keep\n",
        "        # -- Sample block aspect-ratio\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        l = max_keep#int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        while l >= self.length: l -= 1 # crop mask to be smaller than img\n",
        "        return l\n",
        "\n",
        "    def _sample_block_mask(self, length, acceptable_regions=None):\n",
        "        l = length\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # --\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            left = torch.randint(0, self.length - l, (1,))\n",
        "            mask = torch.zeros(self.length, dtype=torch.int32)\n",
        "            mask[left:left+l] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones(self.length, dtype=torch.int32)\n",
        "        mask_complement[left:left+l] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, B):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        p_size = self._sample_block_size(scale=self.pred_mask_scale)\n",
        "        e_size = self._sample_block_size(scale=self.enc_mask_scale)\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.length\n",
        "        min_keep_enc = self.length\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                print(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "        return collated_masks_enc, collated_masks_pred\n",
        "\n",
        "batch=64\n",
        "length=200\n",
        "mask_collator = MaskCollator(length=length, enc_mask_scale=(.85, 1.), pred_mask_scale=(.15, .2),\n",
        "        nenc=1, npred=4, min_keep=4,\n",
        "        # allow_overlap=True)\n",
        "        allow_overlap=False)\n",
        "\n",
        "collated_masks_enc, collated_masks_pred = mask_collator(batch)\n",
        "\n",
        "context_indices, trg_indices = torch.stack(collated_masks_enc).transpose(0,1), torch.stack(collated_masks_pred).transpose(0,1)\n",
        "print(context_indices.shape, trg_indices.shape)\n",
        "# print(collated_masks_enc[0])\n",
        "# img = torch.ones(200)\n",
        "# img[collated_masks_enc[0]] = 0\n",
        "# print(img)\n",
        "# print(context_indices)\n",
        "zero_mask = torch.zeros(batch ,length, device=device)\n",
        "zero_mask[torch.arange(batch).unsqueeze(-1), context_indices.squeeze(1)] = 1\n",
        "print(zero_mask[0])\n",
        "\n",
        "zero_mask = torch.zeros(batch ,length, device=device)\n",
        "# zero_mask[torch.arange(batch).unsqueeze(-1), trg_indices[:,0]] = 0\n",
        "# trg_indices = torch.unique(trg_indices.flatten(1), dim=1)\n",
        "trg_indices = trg_indices.flatten(1).unique(dim=1)\n",
        "print(trg_indices.shape)\n",
        "zero_mask[torch.arange(batch).unsqueeze(-1), trg_indices] = 1\n",
        "# for i in range(4):\n",
        "#     zero_mask[torch.arange(batch).unsqueeze(-1), trg_indices[:,i]] = 1\n",
        "print(zero_mask[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fh9o__m2-j7K",
        "outputId": "2bb5b39b-6b7b-4096-e599-5e6e9c7411e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/268.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m266.2/268.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.0/268.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# @title simplex\n",
        "!pip install -q opensimplex\n",
        "import opensimplex\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def simplexmask(hw=(8,8), scale=(.15,.2)):\n",
        "    ix = iy = np.linspace(0, 1, num=8)\n",
        "    ix, iy = ix+np.random.randint(1e10), iy+np.random.randint(1e10)\n",
        "    y=opensimplex.noise2array(ix, iy)\n",
        "    y = torch.from_numpy(y)\n",
        "    mask_scale = torch.rand(1) * (scale[1] - scale[0]) + scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    yy = y.flatten().sort()[0][int(hw[0]*hw[1]*mask_scale)]\n",
        "    mask = (y<=yy.item())\n",
        "    return mask # T/F [h,w]\n",
        "\n",
        "def simplexmask1d(seq=512, scale=(.15,.2)):\n",
        "    i = np.linspace(0, 2, num=seq)\n",
        "    j = np.array([np.random.randint(1e10)])\n",
        "    y=opensimplex.noise2array(i, j)\n",
        "    # plt.pcolormesh(y)\n",
        "    # plt.show()\n",
        "    y = torch.from_numpy(y)\n",
        "    # print(y.shape)\n",
        "    mask_scale = torch.rand(1) * (scale[1] - scale[0]) + scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    yy = y.flatten().sort()[0][int(seq*mask_scale)]\n",
        "    mask = (y<=yy.item())\n",
        "    return mask # T/F [h,w]\n",
        "\n",
        "# mask = simplexmask(hw=(8,8), scale=(.6,.8))\n",
        "# mask = ~simplexmask(hw=(8,8), scale=(.85,1))\n",
        "# mask = simplexmask1d(seq=100, scale=(.6,.8))\n",
        "# # mask = simplexmask1d(seq=100, scale=(.85,1.))\n",
        "\n",
        "# from matplotlib import pyplot as plt\n",
        "# # # plt.pcolormesh(y)\n",
        "# plt.pcolormesh(mask)\n",
        "# plt.show()\n",
        "\n",
        "# print(mask)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "form",
        "id": "vL2I-31mwfSz"
      },
      "outputs": [],
      "source": [
        "# @title RandomResizedCrop1d\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class RandomResizedCrop1d(nn.Module):\n",
        "    def __init__(self, size, scale=(.8,1.)):\n",
        "        super().__init__()\n",
        "        self.size, self.scale = size, scale\n",
        "\n",
        "    def forward(self, x): # [batch, seq, dim]\n",
        "        x = x.transpose(-2,-1)\n",
        "        crop = torch.rand(1) * (self.scale[1] - self.scale[0]) + self.scale[0]\n",
        "        # print(crop)\n",
        "        crop=.5\n",
        "        pos = torch.rand(1) * (1 - crop)\n",
        "        left = int(pos*x.shape[-1])\n",
        "        right = int((pos+crop)*x.shape[-1])\n",
        "        # print(left,right)\n",
        "        # print(pos, pos+crop)\n",
        "        # x = F.interpolate(x[...,left:right], size=x.shape[-1], mode='linear')\n",
        "        x = F.adaptive_avg_pool1d(x[...,left:right], x.shape[-1]) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "        x = x.transpose(-2,-1)\n",
        "        return x\n",
        "\n",
        "# transform = RandomResizedCrop1d(10)\n",
        "# x = torch.randn(1,10,3)\n",
        "# # print(x.shape)\n",
        "# print(x)\n",
        "# out = transform(x)\n",
        "# # print(out.shape)\n",
        "# # print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-zdjdJixtOu",
        "outputId": "542d26bb-eb31-45c7-8251-333d7bc6af3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "38128\n",
            "torch.Size([4, 110, 16])\n"
          ]
        }
      ],
      "source": [
        "# @title TransformerModel/Predictor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, n_heads=4, d_hid=None, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, 200, d_model)*.02)\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=200, base=10000), requires_grad=False)\n",
        "\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, n_heads=n_heads) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.randn(1,1,d_model)*0.02) # randn zeros\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim)# if out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = x.shape\n",
        "        # x = x * self.pos_enc(context_indices)\n",
        "        # print(\"Trans pred\",x.shape, self.pos_emb[0,context_indices].shape)\n",
        "        # x = x + self.pos_emb[:,context_indices]\n",
        "        x = x + self.pos_emb[0,context_indices]\n",
        "        # print('pred fwd', self.pos_emb[:,context_indices].shape)\n",
        "\n",
        "        # pred_tokens = self.cls * self.pos_enc(trg_indices) # [M, num_trg_toks, d_model]\n",
        "        # pred_tokens = self.cls + self.pos_emb[:,trg_indices]\n",
        "        pred_tokens = self.cls + self.pos_emb[0,trg_indices]\n",
        "        # pred_tokens = pred_tokens.repeat(batch, 1, 1) # [batch, num_trg_toks, d_model]\n",
        "        # print(\"pred fwd\", x.shape, pred_tokens.shape)\n",
        "        x = torch.cat([x, pred_tokens], dim=1) # [batch, seq_len+num_trg_toks, d_model]\n",
        "        out = self.transformer(x)\n",
        "\n",
        "        out = self.norm(out)\n",
        "        out = out[:,seq:] # [batch, num_trg_toks, d_model]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, drop=0):\n",
        "    def __init__(self, patch_size, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0):\n",
        "        super().__init__()\n",
        "        # patch_size=32\n",
        "        self.embed = nn.Sequential(\n",
        "            # # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.MaxPool1d(2,2), #nn.MaxPool1d(3, 2, 3//2),\n",
        "            # # nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.Conv1d(d_model, d_model,3,2,3//2)\n",
        "            nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2),\n",
        "            # nn.Conv1d(in_dim, d_model, patch_size, patch_size), # like patch\n",
        "            )\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, 200, d_model)*.02)\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=200, base=10000), requires_grad=False)#.unsqueeze(0)\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, n_heads=n_heads) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "        # try: print(\"Trans fwd\",x.shape, context_indices.shape)\n",
        "        # except: print(\"Trans fwd noind\",x.shape)\n",
        "        # x = self.pos_enc(x)\n",
        "        x = x + self.pos_emb[:,:x.shape[1]]\n",
        "        # if context_indices != None: x = x[:,context_indices]\n",
        "        if context_indices != None: x = x[torch.arange(x.shape[0]).unsqueeze(-1), context_indices] # [batch, num_context_toks, d_model]\n",
        "\n",
        "        # print(\"TransformerModel\",x.shape)\n",
        "        x = self.transformer(x)\n",
        "        out = self.norm(x)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "batch, seq_len, d_model = 4,3500,16\n",
        "in_dim = 3\n",
        "patch_size=32\n",
        "model = TransformerModel(patch_size, in_dim, d_model, n_heads=4, nlayers=10, drop=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "# # # print(out)\n",
        "# model = TransformerPredictor(in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1).to(device)\n",
        "# out = model(out)\n",
        "# print(out.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "ardu1zJwdHM3",
        "outputId": "63efb6c8-b847-481f-ab48-18e740b9e3ec"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-84f1a39818da>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m \u001b[0mseq_jepa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeqJEPA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.to(torch.float)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;31m# seq_jepa = SeqJEPA(in_dim=3, d_model=1024, out_dim=16, nlayers=12, d_head=16).to(device)#.to(torch.float)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0moptim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_jepa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 1e-3?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1327\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     )\n\u001b[0;32m-> 1329\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1330\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "# @title SeqJEPA\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class SeqJEPA(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, nlayers=2, n_heads=4):\n",
        "        super().__init__()\n",
        "        if out_dim is None: out_dim = d_model\n",
        "        self.patch_size = 32\n",
        "        self.student = TransformerModel(self.patch_size, in_dim, d_model, out_dim=out_dim, n_heads=n_heads, nlayers=nlayers, drop=0.)\n",
        "        self.predicter = TransformerPredictor(out_dim, 3*d_model//8, out_dim, n_heads=4, nlayers=1, drop=0.)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "        # self.transform = RandomResizedCrop1d(3500, scale=(.8,1.))\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]\n",
        "        batch, seq, dim = x.shape\n",
        "        # target_mask = multiblock(seq//self.patch_size, min_s=.2, max_s=.3, M=4).any(0) # best.2.3M4 og.15.2M4# mask out targets to be predicted # [M, seq]\n",
        "        # target_mask = multiblock(seq//self.patch_size, min_s=.2, max_s=.3, M=4, B=1).any(1).squeeze(1) # best.2.3M4 og.15.2M4# mask out targets to be predicted # [M, seq]\n",
        "        # # target_mask = randpatch(seq//self.patch_size, mask_size=8, gamma=.9) # 8.9 [seq]\n",
        "        target_mask = simplexmask1d(seq//self.patch_size, scale=(.7,.8)) # .6.8\n",
        "\n",
        "        # context_mask = ~multiblock(seq//self.patch_size, min_s=.85, max_s=1., M=1)|target_mask # og .85,1.M1 # [1, seq], True->Mask\n",
        "        # context_mask = ~multiblock(seq//self.patch_size, min_s=.85, max_s=1., M=1, B=1).squeeze(1)|target_mask # og .85,1.M1 # [1, seq], True->Mask\n",
        "        # context_mask = torch.zeros((1,8,8), dtype=bool)|target_mask # [1,h,w], True->Mask\n",
        "        # context_mask = ~simplexmask1d(seq//self.patch_size, scale=(.85,1)).unsqueeze(0)|target_mask # [1,h,w]\n",
        "        context_mask = ~simplexmask1d(seq//self.patch_size, scale=(.85,1))|target_mask # [1,h,w]\n",
        "        # target_mask = target_mask[:,0]\n",
        "        # print(context_mask.shape,target_mask.shape, x.shape)\n",
        "        target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "        target_mask, context_mask = target_mask.repeat(batch,1), context_mask.repeat(batch,1)\n",
        "\n",
        "        # x_ = x * F.adaptive_avg_pool1d((~context_mask).float(), x.shape[1]).unsqueeze(-1) # zero masked locations\n",
        "\n",
        "        # target_mask, context_mask = target_mask.flatten(), context_mask.flatten() # [8*8]\n",
        "\n",
        "        # # # context_indices = (~context_mask).nonzero()#.squeeze(-1) # int idx [num_context_toks] , idx of context not masked\n",
        "        context_indices = (~context_mask).nonzero()[:,1].unflatten(0, (batch,-1)) # int idx [num_context_toks] , idx of context not masked\n",
        "        # # print('seq_jepa loss context_indices',context_indices)\n",
        "        # # print('seq_jepa loss x',x.shape)\n",
        "        # # # trg_indices = target_mask.nonzero().squeeze(-1) # int idx [num_trg_toks] , idx of targets that are masked\n",
        "        trg_indices = target_mask.nonzero()[:,1].unflatten(0, (batch,-1)) # int idx [num_trg_toks] , idx of targets that are masked\n",
        "        # # print(trg_indices.shape)\n",
        "\n",
        "\n",
        "        # mask_collator = MaskCollator(length=seq//self.patch_size, enc_mask_scale=(.85,1), pred_mask_scale=(.15,.2), nenc=1, npred=4, min_keep=4, allow_overlap=False)\n",
        "        # # collated_masks_enc, collated_masks_pred = mask_collator(batch) # idx of ctx, idx of masked trg\n",
        "        # collated_masks_enc, collated_masks_pred = mask_collator(1) # idx of ctx, idx of masked trg\n",
        "        # context_indices, trg_indices = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "        # # zero_mask = torch.zeros(batch ,seq//self.patch_size, device=device)\n",
        "        # # zero_mask[torch.arange(batch).unsqueeze(-1), context_indices] = 1\n",
        "        # zero_mask = torch.zeros(1 ,seq//self.patch_size, device=device)\n",
        "        # zero_mask[:, context_indices] = 1\n",
        "        # x_ = x * F.adaptive_avg_pool1d(zero_mask, x.shape[1]).unsqueeze(-1) # zero masked locations\n",
        "        # # print(context_indices.shape, trg_indices.shape)\n",
        "        # context_indices, trg_indices = context_indices.repeat(batch,1), trg_indices.repeat(batch,1)\n",
        "\n",
        "\n",
        "        # print('x_',x_.shape, context_indices.shape, trg_indices.shape)\n",
        "\n",
        "        sx = self.student(x_, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "        # print('seq_jepa loss sx',sx.shape)\n",
        "        sy_ = self.predicter(sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "        sy_ = F.layer_norm(sy_, (sy_.size(-1),))\n",
        "        with torch.no_grad():\n",
        "            sy = self.teacher(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "            # sy = sy[:,trg_indices] # nan bec len(trg_ind)==0 # print('loss sy',torch.isnan(sy).any())\n",
        "            sy = sy[torch.arange(sy.shape[0]).unsqueeze(-1), trg_indices] # [batch, num_context_toks, d_model]\n",
        "            sy = F.layer_norm(sy, (sy.size(-1),))\n",
        "        loss = F.mse_loss(sy, sy_)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        sx = self.student(x)\n",
        "        out = sx.mean(dim=1)\n",
        "        return out\n",
        "\n",
        "# min_s=0.15, max_s, M\n",
        "# trg.15.2M4 C.85 1\n",
        "\n",
        "# 1e-2,1e-3 < 3e-3,1e-3\n",
        "# patch16 < patch32\n",
        "# NoPE good but sus\n",
        "\n",
        "\n",
        "seq_jepa = SeqJEPA(in_dim=3, d_model=32, out_dim=16, nlayers=4, n_heads=4).to(device)#.to(torch.float)\n",
        "# seq_jepa = SeqJEPA(in_dim=3, d_model=1024, out_dim=16, nlayers=12, d_head=16).to(device)#.to(torch.float)\n",
        "optim = torch.optim.AdamW(seq_jepa.parameters(), lr=1e-3) # 1e-3?\n",
        "# optim = torch.optim.AdamW([{'params': seq_jepa.student.parameters()},\n",
        "#     {'params': seq_jepa.predicter.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # default 1e-2, 5e-2\n",
        "    # {'params': seq_jepa.predicter.parameters(), 'lr': 1e-2}], lr=1e-3, weight_decay=1e-2)\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/configs/in1k_vith14_ep300.yaml\n",
        "# d_model 1024,384\n",
        "# depth 12,6/12\n",
        "# wd 5e-2 - 4e-1\n",
        "# adamw 1e-4 - 1e-3 - 1e-6\n",
        "# ema 0.996-1\n",
        "\n",
        "print(sum(p.numel() for p in seq_jepa.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.parameters())) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.predicter.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.student.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.teacher.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# d_model^2 * nlayers\n",
        "\n",
        "x = torch.rand((24, 3500, in_dim), device=device)\n",
        "out = seq_jepa.loss(x)\n",
        "print(out.shape)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(16, 18).to(device)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n",
        "# optim = torch.optim.AdamW([{'params': seq_jepa.parameters()}, {'params': classifier.parameters(), 'lr': 1e-3}], lr=1e-3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAYkMGzdCv97",
        "outputId": "c6b806c1-772d-447d-a384-5bb15f4213f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([ 60,  40, 164,  68,  48,  66,  37,  75, 181,  79])\n",
            "torch.Size([1, 10, 16])\n",
            "torch.Size([10, 16])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# mask=torch.rand(1,5)>.5\n",
        "# print(mask)\n",
        "# # nn.AdaptiveAvgPool1d(10)\n",
        "# out = F.adaptive_avg_pool1d(mask.float(), 10)\n",
        "# print(out)\n",
        "\n",
        "posemb = torch.randn(1, 200, d_model)\n",
        "# ind=torch.randint(0, posemb.shape[1], (1, 10))\n",
        "ind=torch.randint(0, posemb.shape[1], (10,))\n",
        "print(ind)\n",
        "print(posemb[:,ind].shape) # [1, 2, 10, d_model]\n",
        "print(posemb[0,ind].shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4rj4LfPuN1H",
        "outputId": "24fa1196-5fe9-4519-c50d-6b788393c491"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 16])\n"
          ]
        }
      ],
      "source": [
        "# @title TransformerVICReg\n",
        "import torch\n",
        "from torch import nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class TransformerVICReg(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, n_heads=4, d_hid=None, nlayers=1, drop=0):\n",
        "        super().__init__()\n",
        "        act = nn.GELU()\n",
        "        patch_size=4\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Linear(in_dim, d_model), act\n",
        "            # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(3,2, 3//2),\n",
        "            nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2),\n",
        "            # nn.Conv1d(in_dim, d_model, patch_size, patch_size), # patch\n",
        "            )\n",
        "        self.pos_enc = RoPE(d_model, seq_len=200, base=10000) # 10000\n",
        "        # self.pos_emb = nn.Parameter(torch.randn(1, 200, d_model))\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=200, base=10000).unsqueeze(0), requires_grad=False)\n",
        "\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, n_heads=n_heads) for _ in range(nlayers)])\n",
        "\n",
        "        # out_dim = out_dim or d_model\n",
        "        self.lin = nn.Linear(d_model, out_dim)\n",
        "        self.attn_pool = nn.Linear(d_model, 1, bias=False)\n",
        "\n",
        "        dim_v = d_model * 4\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(out_dim, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v, bias=False),\n",
        "            )\n",
        "\n",
        "    def forward(self, x): # [b,t,d] / [b,c,h,w]\n",
        "        # x = self.embed(x).flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c]\n",
        "        x = self.embed(x.transpose(-2,-1)).transpose(-2,-1) # [b,t,d]\n",
        "        # x = self.embed(x)\n",
        "        x = self.pos_enc(x)\n",
        "        # x = x + self.pos_emb[:,:x.shape[1]]\n",
        "\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        attn = self.attn_pool(x).squeeze(-1) # [batch, seq] # seq_pool\n",
        "        out = (torch.softmax(attn, dim=-1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, seq] @ [batch, seq, dim] -> [batch, dim]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch, ntoken]\n",
        "\n",
        "    def expand(self, x):\n",
        "        sx = self.forward(x)\n",
        "        vx = self.exp(sx)\n",
        "        return vx\n",
        "\n",
        "batch, seq_len, d_model = 4,3500,32\n",
        "in_dim, out_dim=3,16\n",
        "model = TransformerVICReg(in_dim, d_model, out_dim, n_heads=4, nlayers=2, drop=0.).to(device)\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "# x =  torch.rand((batch, in_dim, 32,32), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObiHp-LSuRBA",
        "outputId": "c6ce8734-e0a0-47b2-b11d-e0f8e53af582"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "in vicreg  1.9939420276162247e-16 24.746832251548767 1.6621681808715039e-09\n",
            "(tensor(7.9758e-18, device='cuda:0', grad_fn=<MseLossBackward0>), tensor(0.9899, device='cuda:0', grad_fn=<AddBackward0>), tensor(1.6622e-09, device='cuda:0', grad_fn=<AddBackward0>))\n"
          ]
        }
      ],
      "source": [
        "# @title Violet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class Violet(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, nlayers=2, n_heads=4):\n",
        "        super().__init__()\n",
        "        out_dim = out_dim or d_model\n",
        "        self.student = TransformerVICReg(in_dim, d_model, out_dim=out_dim, n_heads=n_heads, nlayers=nlayers, drop=0.)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "\n",
        "        # vicreg\n",
        "        self.sim_coeff=25. # 10.0 # 25.0 # λ\n",
        "        self.std_coeff=25. # 1.0 # 25.0 # µ\n",
        "        self.cov_coeff=1. # 25.0 # 1.0 # ν\n",
        "\n",
        "        # self.transform = RandomResizedCrop1d(3500, scale=(.8,1.))\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]c/ [b,c,h,w]\n",
        "        # print(x.shape)\n",
        "        # self.transform(x)\n",
        "        vx = self.student.expand(x) # [batch, num_context_toks, out_dim]\n",
        "        with torch.no_grad(): vy = self.teacher.expand(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "        loss = self.vicreg(vx, vy)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        return self.student(x)\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size, num_features = x.shape\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = self.sim_coeff * repr_loss + self.std_coeff * std_loss + self.cov_coeff * cov_loss\n",
        "        print(\"in vicreg \",self.sim_coeff * repr_loss.item() , self.std_coeff * std_loss.item() , self.cov_coeff * cov_loss.item())\n",
        "        # return loss\n",
        "        return repr_loss, std_loss, cov_loss\n",
        "\n",
        "# lr1e-3\n",
        "# n_heads=8 < 4\n",
        "\n",
        "in_dim=3\n",
        "violet = Violet(in_dim, d_model=32, out_dim=16, nlayers=2, n_heads=4).to(device)\n",
        "voptim = torch.optim.AdamW(violet.parameters(), lr=1e-3) # 1e-3?\n",
        "# voptim = torch.optim.AdamW([{'params': violet.student.transformer.parameters()},\n",
        "#     {'params': violet.student.exp.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # default 1e-2\n",
        "# voptim = torch.optim.AdamW([{'params': violet.student.exp.parameters(), 'lr': 3e-3},\n",
        "#     {'params': [p for n, p in violet.named_parameters() if 'student.exp' not in n]}], lr=1e-3, weight_decay=1e-2) # default 1e-2\n",
        "\n",
        "# print(sum(p.numel() for p in violet.parameters() if p.requires_grad)) # 27584\n",
        "\n",
        "\n",
        "x = torch.rand((2,1000,in_dim), device=device)\n",
        "loss = violet.loss(x)\n",
        "# print(out.shape)\n",
        "print(loss)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(16, 18).to(device) # torch/autograd/graph.py RuntimeError: CUDA error: device-side assert triggered CUDA kernel errors\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IACKDymhit7"
      },
      "outputs": [],
      "source": [
        "# print(seq_jepa.classifier.weight[0])\n",
        "# for n, p in seq_jepa.target_encoder.named_parameters():\n",
        "# for n,p in violet.named_parameters():\n",
        "#     # print('student.exp' in n)\n",
        "#     # print(n,p)\n",
        "#     print(n)\n",
        "# optim.param_groups[0]['lr'] = 1e-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "cellView": "form",
        "id": "7zghfu_jeOQk"
      },
      "outputs": [],
      "source": [
        "# @title RankMe\n",
        "# RankMe: Assessing the Downstream Performance of Pretrained Self-Supervised Representations by Their Rank jun 2023 https://arxiv.org/pdf/2210.02885\n",
        "import torch\n",
        "\n",
        "# https://github.com/Spidartist/IJEPA_endoscopy/blob/main/src/helper.py#L22\n",
        "def RankMe(Z):\n",
        "    \"\"\"\n",
        "    Calculate the RankMe score (the higher, the better).\n",
        "    RankMe(Z) = exp(-sum_{k=1}^{min(N, K)} p_k * log(p_k)),\n",
        "    where p_k = sigma_k (Z) / ||sigma_k (Z)||_1 + epsilon\n",
        "    where sigma_k is the kth singular value of Z.\n",
        "    where Z is the matrix of embeddings (N × K)\n",
        "    \"\"\"\n",
        "    # compute the singular values of the embeddings\n",
        "    # _u, s, _vh = torch.linalg.svd(Z, full_matrices=False)  # s.shape = (min(N, K),)\n",
        "    # s = torch.linalg.svd(Z, full_matrices=False).S\n",
        "    s = torch.linalg.svdvals(Z)\n",
        "    p = s / torch.sum(s, axis=0) + 1e-7\n",
        "    return torch.exp(-torch.sum(p * torch.log(p)))\n",
        "\n",
        "# Z = torch.randn(5, 3)\n",
        "# o = RankMe(Z)\n",
        "# print(o)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "cellView": "form",
        "id": "Pj25OrEk14pR"
      },
      "outputs": [],
      "source": [
        "# @title LiDAR stable_ssl next\n",
        "# https://github.com/rbalestr-lab/stable-ssl/blob/main/stable_ssl/monitors.py#L106\n",
        "\n",
        "def LiDAR(embeddings, eps=1e-7, delta=1e-3):\n",
        "    embeddings = embeddings.unflatten(0, (-1,2))\n",
        "    (local_n, q, d), device = embeddings.shape, embeddings.device\n",
        "\n",
        "    class_means = embeddings.mean(dim=1) # mu_x\n",
        "    grand_mean_local = class_means.mean(dim=0) # mu\n",
        "    # print(embeddings.shape, class_means.shape, grand_mean_local.shape) # [50, 2, 64], [50, 64], [64]\n",
        "\n",
        "    # local_Sb = torch.zeros(d, d, device=device)\n",
        "    # local_Sw = torch.zeros(d, d, device=device)\n",
        "\n",
        "\n",
        "    # print(diff_b.shape, diff_w.shape) # [64,1], [64,1]\n",
        "    # print(local_Sb.shape, local_Sw.shape) # [64,64], [64,64]\n",
        "\n",
        "    diff_b = (class_means - grand_mean_local).unsqueeze(-1)\n",
        "    # print(diff_b.shape)\n",
        "    # # local_Sb = (diff_b @ diff_b.T).sum()\n",
        "    local_Sb = (diff_b @ diff_b.transpose(-2,-1)).sum(0)\n",
        "    # # print(embeddings.shape, class_means.shape)\n",
        "    diff_w = (embeddings - class_means.unsqueeze(1)).reshape(-1,d,1)\n",
        "    # # print(diff_w.shape)\n",
        "    # # local_Sw = (diff_w @ diff_w.T).sum()\n",
        "    local_Sw = (diff_w @ diff_w.transpose(-2,-1)).sum(0)\n",
        "\n",
        "    # print(local_Sb.shape, local_Sw.shape)\n",
        "    S_b = local_Sb / (local_n - 1)\n",
        "    S_w = local_Sw / (local_n * (q - 1))\n",
        "    S_w += delta * torch.eye(d, device=device)\n",
        "    # print(S_w.shape, d)\n",
        "\n",
        "    eigvals_w, eigvecs_w = torch.linalg.eigh(S_w)\n",
        "    eigvals_w = torch.clamp(eigvals_w, min=eps)\n",
        "\n",
        "    invsqrt_w = (eigvecs_w * (1.0 / torch.sqrt(eigvals_w))) @ eigvecs_w.transpose(-1, -2)\n",
        "    Sigma_lidar = invsqrt_w @ S_b @ invsqrt_w\n",
        "\n",
        "    # lam, _ = torch.linalg.eigh(Sigma_lidar)\n",
        "    lam = torch.linalg.eigh(Sigma_lidar)[0]\n",
        "    # print(lam)\n",
        "    # lam = torch.clamp(lam, min=0.0)\n",
        "\n",
        "    p = lam / lam.sum() + eps\n",
        "    # print(p)\n",
        "    # p = s / torch.sum(s, axis=0) + 1e-7\n",
        "    return torch.exp(-torch.sum(p * torch.log(p)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "v-eTjgAhmp_t",
        "outputId": "15533e0f-a37f-4303-de3f-3a7c1ee4c88f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "strain 0.2162536382675171\n",
            "strain 0.24744942784309387\n",
            "strain 0.22166766226291656\n",
            "strain 0.2403697520494461\n",
            "strain 0.25871169567108154\n",
            "strain 0.27637553215026855\n",
            "classify 2.427490234375\n",
            "classify 2.35662841796875\n",
            "classify 2.3033447265625\n",
            "classify 2.3551025390625\n",
            "classify 2.1324462890625\n",
            "classify 2.18634033203125\n",
            "classify 2.13677978515625\n",
            "classify 2.27838134765625\n",
            "classify 2.15277099609375\n",
            "0.25\n",
            "0.21875\n",
            "0.359375\n",
            "0.28125\n",
            "strain 0.25376585125923157\n",
            "strain 0.2300594300031662\n",
            "strain 0.25187939405441284\n",
            "strain 0.23533330857753754\n",
            "strain 0.2581712603569031\n",
            "strain 0.21059231460094452\n",
            "strain 0.20692762732505798\n",
            "strain 0.23913714289665222\n",
            "strain 0.23312145471572876\n",
            "classify 2.3438720703125\n",
            "classify 2.164306640625\n",
            "classify 2.22027587890625\n",
            "classify 2.234375\n",
            "classify 2.41650390625\n",
            "classify 2.23876953125\n",
            "classify 2.1722412109375\n",
            "classify 2.26251220703125\n",
            "classify 2.1719970703125\n",
            "0.296875\n",
            "0.3125\n",
            "0.265625\n",
            "0.359375\n",
            "strain 0.2380589097738266\n",
            "strain 0.19940021634101868\n",
            "strain 0.228471040725708\n",
            "strain 0.2026100903749466\n",
            "strain 0.2363533228635788\n",
            "strain 0.22413207590579987\n",
            "strain 0.2523159682750702\n",
            "strain 0.2532057464122772\n",
            "strain 0.2679672837257385\n",
            "classify 2.3045654296875\n",
            "classify 2.12567138671875\n",
            "classify 2.06976318359375\n",
            "classify 2.333740234375\n",
            "classify 2.27020263671875\n",
            "classify 2.2813720703125\n",
            "classify 2.34210205078125\n",
            "classify 2.30517578125\n",
            "classify 2.29638671875\n",
            "0.203125\n",
            "0.328125\n",
            "0.3125\n",
            "0.234375\n",
            "strain 0.20179647207260132\n",
            "strain 0.24670028686523438\n",
            "strain 0.2290184050798416\n",
            "strain 0.2580704092979431\n",
            "strain 0.24049101769924164\n",
            "strain 0.23228412866592407\n",
            "strain 0.2288278341293335\n",
            "strain 0.2260405570268631\n",
            "strain 0.2581195533275604\n",
            "classify 2.35565185546875\n",
            "classify 2.17071533203125\n",
            "classify 2.44879150390625\n",
            "classify 2.01434326171875\n",
            "classify 2.22723388671875\n",
            "classify 2.403076171875\n",
            "classify 2.29345703125\n",
            "classify 2.22216796875\n",
            "classify 2.2391357421875\n",
            "0.359375\n",
            "0.25\n",
            "0.28125\n",
            "0.234375\n",
            "strain 0.22845393419265747\n",
            "strain 0.25601300597190857\n",
            "strain 0.24615028500556946\n",
            "strain 0.18788954615592957\n",
            "strain 0.2668379247188568\n",
            "strain 0.23702861368656158\n",
            "strain 0.22925344109535217\n",
            "strain 0.21860653162002563\n",
            "strain 0.23835335671901703\n",
            "classify 2.13140869140625\n",
            "classify 2.32403564453125\n",
            "classify 2.33929443359375\n",
            "classify 2.16998291015625\n",
            "classify 2.24530029296875\n",
            "classify 2.28009033203125\n",
            "classify 2.3995361328125\n",
            "classify 2.2945556640625\n",
            "classify 2.27777099609375\n",
            "0.28125\n",
            "0.296875\n",
            "0.28125\n",
            "0.21875\n",
            "strain 0.25184276700019836\n",
            "strain 0.24078576266765594\n",
            "strain 0.2758006751537323\n",
            "strain 0.20493368804454803\n",
            "strain 0.2434965968132019\n",
            "strain 0.2476099282503128\n",
            "strain 0.2605434060096741\n",
            "strain 0.2201804667711258\n",
            "strain 0.2356935441493988\n",
            "classify 2.3477783203125\n",
            "classify 2.2828369140625\n",
            "classify 2.12713623046875\n",
            "classify 2.2012939453125\n",
            "classify 2.31488037109375\n",
            "classify 2.31585693359375\n",
            "classify 2.2969970703125\n",
            "classify 2.23345947265625\n",
            "classify 2.16387939453125\n",
            "0.296875\n",
            "0.328125\n",
            "0.25\n",
            "0.234375\n",
            "strain 0.22341932356357574\n",
            "strain 0.23929786682128906\n",
            "strain 0.2024962306022644\n",
            "strain 0.207274928689003\n",
            "strain 0.2786235809326172\n",
            "strain 0.22654186189174652\n",
            "strain 0.23180675506591797\n",
            "strain 0.26053568720817566\n",
            "strain 0.24054306745529175\n",
            "classify 2.35797119140625\n",
            "classify 2.22698974609375\n",
            "classify 2.41583251953125\n",
            "classify 2.41204833984375\n",
            "classify 2.1683349609375\n",
            "classify 2.13580322265625\n",
            "classify 2.2935791015625\n",
            "classify 2.3223876953125\n",
            "classify 2.17120361328125\n",
            "0.265625\n",
            "0.328125\n",
            "0.25\n",
            "0.25\n",
            "strain 0.24368363618850708\n",
            "strain 0.2677996754646301\n",
            "strain 0.2415967881679535\n",
            "strain 0.22481359541416168\n",
            "strain 0.20227061212062836\n",
            "strain 0.2521649897098541\n",
            "strain 0.2081049531698227\n",
            "strain 0.23875996470451355\n",
            "strain 0.2505953311920166\n",
            "classify 2.33135986328125\n",
            "classify 2.29266357421875\n",
            "classify 2.26995849609375\n",
            "classify 2.3414306640625\n",
            "classify 2.27679443359375\n",
            "classify 2.31060791015625\n",
            "classify 2.322265625\n",
            "classify 2.19134521484375\n",
            "classify 2.07080078125\n",
            "0.203125\n",
            "0.34375\n",
            "0.40625\n",
            "0.234375\n",
            "strain 0.22612297534942627\n",
            "strain 0.24782030284404755\n",
            "strain 0.2885732054710388\n",
            "strain 0.23897235095500946\n",
            "strain 0.22625203430652618\n",
            "strain 0.23765532672405243\n",
            "strain 0.2617351710796356\n",
            "strain 0.21887752413749695\n",
            "strain 0.21863779425621033\n",
            "classify 2.2152099609375\n",
            "classify 2.28399658203125\n",
            "classify 2.28302001953125\n",
            "classify 2.3402099609375\n",
            "classify 2.263427734375\n",
            "classify 2.1910400390625\n",
            "classify 2.142333984375\n",
            "classify 2.40679931640625\n",
            "classify 2.19921875\n",
            "0.3125\n",
            "0.296875\n",
            "0.25\n",
            "0.296875\n",
            "strain 0.23892374336719513\n",
            "strain 0.2476033717393875\n",
            "strain 0.257546067237854\n",
            "strain 0.2149547040462494\n",
            "strain 0.2333669811487198\n",
            "strain 0.23796705901622772\n",
            "strain 0.2397138476371765\n",
            "strain 0.28485602140426636\n",
            "strain 0.2771925926208496\n",
            "classify 2.3883056640625\n",
            "classify 2.30328369140625\n",
            "classify 2.24658203125\n",
            "classify 2.122802734375\n",
            "classify 2.3258056640625\n",
            "classify 2.1822509765625\n",
            "classify 2.18670654296875\n",
            "classify 2.28619384765625\n",
            "classify 2.3060302734375\n",
            "0.265625\n",
            "0.34375\n",
            "0.234375\n",
            "0.234375\n",
            "strain 0.2314331978559494\n",
            "strain 0.2167547196149826\n",
            "strain 0.2569864094257355\n",
            "strain 0.2233189344406128\n",
            "strain 0.2342236191034317\n",
            "strain 0.21620088815689087\n",
            "strain 0.23092181980609894\n",
            "strain 0.258578896522522\n",
            "strain 0.2210443764925003\n",
            "classify 2.301513671875\n",
            "classify 2.16680908203125\n",
            "classify 2.2366943359375\n",
            "classify 2.2972412109375\n",
            "classify 2.288330078125\n",
            "classify 2.23028564453125\n",
            "classify 2.54339599609375\n",
            "classify 2.18389892578125\n",
            "classify 2.21368408203125\n",
            "0.28125\n",
            "0.28125\n",
            "0.203125\n",
            "0.3125\n",
            "strain 0.21335101127624512\n",
            "strain 0.20753240585327148\n",
            "strain 0.23848363757133484\n",
            "strain 0.2707446217536926\n",
            "strain 0.239070326089859\n",
            "strain 0.23962384462356567\n",
            "strain 0.24813145399093628\n",
            "strain 0.22719062864780426\n",
            "strain 0.22918231785297394\n",
            "classify 2.43310546875\n",
            "classify 2.31024169921875\n",
            "classify 2.31793212890625\n",
            "classify 2.189208984375\n",
            "classify 2.14306640625\n",
            "classify 2.28802490234375\n",
            "classify 2.24945068359375\n",
            "classify 2.155029296875\n",
            "classify 2.19195556640625\n",
            "0.265625\n",
            "0.296875\n",
            "0.25\n",
            "0.203125\n",
            "strain 0.22310373187065125\n",
            "strain 0.21693535149097443\n",
            "strain 0.3061923682689667\n",
            "strain 0.2618907392024994\n",
            "strain 0.28342559933662415\n",
            "strain 0.23384995758533478\n",
            "strain 0.2409580647945404\n",
            "strain 0.25475403666496277\n",
            "strain 0.2164016216993332\n",
            "classify 2.34326171875\n",
            "classify 2.2198486328125\n",
            "classify 2.37493896484375\n",
            "classify 2.335205078125\n",
            "classify 2.38482666015625\n",
            "classify 2.33209228515625\n",
            "classify 1.979248046875\n",
            "classify 2.186767578125\n",
            "classify 2.3416748046875\n",
            "0.1875\n",
            "0.296875\n",
            "0.234375\n",
            "0.296875\n",
            "strain 0.2590672969818115\n",
            "strain 0.20555514097213745\n",
            "strain 0.24449753761291504\n",
            "strain 0.27015191316604614\n",
            "strain 0.25189459323883057\n",
            "strain 0.240860715508461\n",
            "strain 0.28534460067749023\n",
            "strain 0.2315867692232132\n",
            "strain 0.25815659761428833\n",
            "classify 2.1380615234375\n",
            "classify 2.3453369140625\n",
            "classify 2.212158203125\n",
            "classify 2.2703857421875\n",
            "classify 2.21728515625\n",
            "classify 2.27459716796875\n",
            "classify 2.3233642578125\n",
            "classify 2.1142578125\n",
            "classify 2.31964111328125\n",
            "0.390625\n",
            "0.25\n",
            "0.25\n",
            "0.34375\n",
            "strain 0.23377789556980133\n",
            "strain 0.24184031784534454\n",
            "strain 0.23882266879081726\n",
            "strain 0.23681403696537018\n",
            "strain 0.2168327271938324\n",
            "strain 0.2593480050563812\n",
            "strain 0.2030588537454605\n",
            "strain 0.225412517786026\n",
            "strain 0.2909300923347473\n",
            "classify 2.4119873046875\n",
            "classify 2.12811279296875\n",
            "classify 2.0562744140625\n",
            "classify 2.32403564453125\n",
            "classify 2.29412841796875\n",
            "classify 2.344970703125\n",
            "classify 2.283203125\n",
            "classify 2.15374755859375\n",
            "classify 2.290771484375\n",
            "0.296875\n",
            "0.328125\n",
            "0.28125\n",
            "0.203125\n",
            "strain 0.2535249590873718\n",
            "strain 0.2189568281173706\n",
            "strain 0.20464685559272766\n",
            "strain 0.20128010213375092\n",
            "strain 0.22166989743709564\n",
            "strain 0.2459307163953781\n",
            "strain 0.31303006410598755\n",
            "strain 0.24132366478443146\n",
            "strain 0.24341367185115814\n",
            "classify 2.2672119140625\n",
            "classify 2.528564453125\n",
            "classify 2.2674560546875\n",
            "classify 2.31072998046875\n",
            "classify 2.16534423828125\n",
            "classify 2.10858154296875\n",
            "classify 2.45013427734375\n",
            "classify 2.109375\n",
            "classify 2.130859375\n",
            "0.21875\n",
            "0.234375\n",
            "0.296875\n",
            "0.328125\n",
            "strain 0.24304784834384918\n",
            "strain 0.23763808608055115\n",
            "strain 0.2460448443889618\n",
            "strain 0.23812782764434814\n",
            "strain 0.21228565275669098\n",
            "strain 0.27230304479599\n",
            "strain 0.24513860046863556\n",
            "strain 0.2085452675819397\n",
            "strain 0.22946874797344208\n",
            "classify 2.2177734375\n",
            "classify 2.3597412109375\n",
            "classify 2.2779541015625\n",
            "classify 2.30108642578125\n",
            "classify 2.29327392578125\n",
            "classify 2.39501953125\n",
            "classify 2.100830078125\n",
            "classify 2.26123046875\n",
            "classify 2.12908935546875\n",
            "0.265625\n",
            "0.265625\n",
            "0.3125\n",
            "0.25\n",
            "strain 0.23026949167251587\n",
            "strain 0.2476155161857605\n",
            "strain 0.27002471685409546\n",
            "strain 0.21720239520072937\n",
            "strain 0.2564344108104706\n",
            "strain 0.2066999077796936\n",
            "strain 0.2278939038515091\n",
            "strain 0.26776477694511414\n",
            "strain 0.26298436522483826\n",
            "classify 2.00714111328125\n",
            "classify 2.35528564453125\n",
            "classify 2.1448974609375\n",
            "classify 2.3382568359375\n",
            "classify 2.10546875\n",
            "classify 2.28802490234375\n",
            "classify 2.455078125\n",
            "classify 2.4139404296875\n",
            "classify 2.207275390625\n",
            "0.375\n",
            "0.234375\n",
            "0.234375\n",
            "0.21875\n",
            "strain 0.29318997263908386\n",
            "strain 0.24493739008903503\n",
            "strain 0.2165893018245697\n",
            "strain 0.23446738719940186\n",
            "strain 0.2726197838783264\n",
            "strain 0.2448035627603531\n",
            "strain 0.2533363103866577\n",
            "strain 0.2068490982055664\n",
            "strain 0.23862530291080475\n",
            "classify 2.38128662109375\n",
            "classify 2.1923828125\n",
            "classify 2.2701416015625\n",
            "classify 2.0836181640625\n",
            "classify 2.225341796875\n",
            "classify 2.32489013671875\n",
            "classify 2.22900390625\n",
            "classify 2.29443359375\n",
            "classify 2.12005615234375\n",
            "0.25\n",
            "0.296875\n",
            "0.25\n",
            "0.328125\n",
            "strain 0.23640955984592438\n",
            "strain 0.22768643498420715\n",
            "strain 0.2404637634754181\n",
            "strain 0.2294144630432129\n",
            "strain 0.21235376596450806\n",
            "strain 0.2153465896844864\n",
            "strain 0.2616991102695465\n",
            "strain 0.23729996383190155\n",
            "strain 0.22708651423454285\n",
            "classify 2.16552734375\n",
            "classify 2.29644775390625\n",
            "classify 2.25164794921875\n",
            "classify 2.218017578125\n",
            "classify 2.3267822265625\n",
            "classify 2.1807861328125\n",
            "classify 2.4459228515625\n",
            "classify 2.1729736328125\n",
            "classify 2.275634765625\n",
            "0.25\n",
            "0.296875\n",
            "0.25\n",
            "0.265625\n",
            "strain 0.2600730359554291\n",
            "strain 0.23693042993545532\n",
            "strain 0.19439150393009186\n",
            "strain 0.2698884904384613\n",
            "strain 0.2610073685646057\n",
            "strain 0.2680119276046753\n",
            "strain 0.24354486167430878\n",
            "strain 0.22799332439899445\n",
            "strain 0.21289609372615814\n",
            "classify 2.17181396484375\n",
            "classify 2.1982421875\n",
            "classify 2.24273681640625\n",
            "classify 2.1966552734375\n",
            "classify 2.3687744140625\n",
            "classify 2.18603515625\n",
            "classify 2.205322265625\n",
            "classify 2.22979736328125\n",
            "classify 2.33892822265625\n",
            "0.328125\n",
            "0.1875\n",
            "0.296875\n",
            "0.25\n",
            "strain 0.2604503631591797\n",
            "strain 0.22346164286136627\n",
            "strain 0.23336568474769592\n",
            "strain 0.2236628532409668\n",
            "strain 0.22958321869373322\n",
            "strain 0.23561449348926544\n",
            "strain 0.29341861605644226\n",
            "strain 0.23497559130191803\n",
            "strain 0.24138188362121582\n",
            "classify 2.2120361328125\n",
            "classify 2.259765625\n",
            "classify 2.2484130859375\n",
            "classify 2.23406982421875\n",
            "classify 2.31292724609375\n",
            "classify 2.0777587890625\n",
            "classify 2.32403564453125\n",
            "classify 2.23931884765625\n",
            "classify 2.18133544921875\n",
            "0.203125\n",
            "0.25\n",
            "0.265625\n",
            "0.34375\n",
            "strain 0.22050447762012482\n",
            "strain 0.2627951204776764\n",
            "strain 0.22277472913265228\n",
            "strain 0.2496016025543213\n",
            "strain 0.21318790316581726\n",
            "strain 0.2523680031299591\n",
            "strain 0.2075604498386383\n",
            "strain 0.22185489535331726\n",
            "strain 0.2635382413864136\n",
            "classify 2.1380615234375\n",
            "classify 2.2918701171875\n",
            "classify 2.463134765625\n",
            "classify 2.2763671875\n",
            "classify 2.2069091796875\n",
            "classify 2.39422607421875\n",
            "classify 2.241455078125\n",
            "classify 2.1536865234375\n",
            "classify 2.1566162109375\n",
            "0.328125\n",
            "0.28125\n",
            "0.21875\n",
            "0.25\n",
            "strain 0.20651775598526\n",
            "strain 0.22011291980743408\n",
            "strain 0.2527627646923065\n",
            "strain 0.22842037677764893\n",
            "strain 0.2882779538631439\n",
            "strain 0.22964639961719513\n",
            "strain 0.20778369903564453\n",
            "strain 0.2423367202281952\n",
            "strain 0.23479869961738586\n",
            "classify 2.378662109375\n",
            "classify 2.29937744140625\n",
            "classify 2.19921875\n",
            "classify 2.27178955078125\n",
            "classify 2.074462890625\n",
            "classify 2.42828369140625\n",
            "classify 2.3135986328125\n",
            "classify 2.18646240234375\n",
            "classify 2.3150634765625\n",
            "0.234375\n",
            "0.28125\n",
            "0.21875\n",
            "0.296875\n",
            "strain 0.23298175632953644\n",
            "strain 0.2645988464355469\n",
            "strain 0.25129517912864685\n",
            "strain 0.23038016259670258\n",
            "strain 0.2058635652065277\n",
            "strain 0.2665475010871887\n",
            "strain 0.238748237490654\n",
            "strain 0.2798319160938263\n",
            "strain 0.24192272126674652\n",
            "classify 2.0802001953125\n",
            "classify 2.366943359375\n",
            "classify 2.27783203125\n",
            "classify 2.2216796875\n",
            "classify 2.1884765625\n",
            "classify 2.11895751953125\n",
            "classify 2.29534912109375\n",
            "classify 2.28485107421875\n",
            "classify 2.2626953125\n",
            "0.28125\n",
            "0.296875\n",
            "0.296875\n",
            "0.21875\n",
            "strain 0.2262183427810669\n",
            "strain 0.21604187786579132\n",
            "strain 0.24586999416351318\n",
            "strain 0.2287309318780899\n",
            "strain 0.22512230277061462\n",
            "strain 0.22802583873271942\n",
            "strain 0.22542424499988556\n",
            "strain 0.26802858710289\n",
            "strain 0.2288525253534317\n",
            "classify 2.28839111328125\n",
            "classify 2.29693603515625\n",
            "classify 2.27490234375\n",
            "classify 2.388671875\n",
            "classify 2.19879150390625\n",
            "classify 2.30255126953125\n",
            "classify 2.1739501953125\n",
            "classify 2.062255859375\n",
            "classify 2.420166015625\n",
            "0.25\n",
            "0.265625\n",
            "0.25\n",
            "0.359375\n",
            "strain 0.24433977901935577\n",
            "strain 0.24176648259162903\n",
            "strain 0.22433800995349884\n",
            "strain 0.2160380780696869\n",
            "strain 0.21991342306137085\n",
            "strain 0.23968133330345154\n",
            "strain 0.22979304194450378\n",
            "strain 0.23305779695510864\n",
            "strain 0.22762656211853027\n",
            "classify 2.301513671875\n",
            "classify 2.21051025390625\n",
            "classify 2.2550048828125\n",
            "classify 2.503662109375\n",
            "classify 2.1890869140625\n",
            "classify 2.23388671875\n",
            "classify 2.30426025390625\n",
            "classify 2.19573974609375\n",
            "classify 2.13409423828125\n",
            "0.21875\n",
            "0.328125\n",
            "0.265625\n",
            "0.265625\n",
            "strain 0.21234630048274994\n",
            "strain 0.22960147261619568\n",
            "strain 0.22012560069561005\n",
            "strain 0.24485838413238525\n",
            "strain 0.23721802234649658\n",
            "strain 0.2312631756067276\n",
            "strain 0.23764073848724365\n",
            "strain 0.22335316240787506\n",
            "strain 0.23591533303260803\n",
            "classify 2.28436279296875\n",
            "classify 2.26336669921875\n",
            "classify 2.44891357421875\n",
            "classify 2.1856689453125\n",
            "classify 2.16217041015625\n",
            "classify 2.2965087890625\n",
            "classify 2.2066650390625\n",
            "classify 2.13653564453125\n",
            "classify 2.380859375\n",
            "0.25\n",
            "0.265625\n",
            "0.3125\n",
            "0.34375\n",
            "strain 0.28371748328208923\n",
            "strain 0.23027770221233368\n",
            "strain 0.2425975501537323\n",
            "strain 0.19848276674747467\n",
            "strain 0.2565571069717407\n",
            "strain 0.2790455222129822\n",
            "strain 0.22874829173088074\n",
            "strain 0.23015514016151428\n",
            "strain 0.2147996574640274\n",
            "classify 2.38751220703125\n",
            "classify 2.14306640625\n",
            "classify 2.2095947265625\n",
            "classify 2.1187744140625\n",
            "classify 2.18212890625\n",
            "classify 2.23028564453125\n",
            "classify 2.1336669921875\n",
            "classify 2.36151123046875\n",
            "classify 2.38446044921875\n",
            "0.34375\n",
            "0.3125\n",
            "0.1875\n",
            "0.234375\n",
            "strain 0.23117420077323914\n",
            "strain 0.2227480709552765\n",
            "strain 0.22452101111412048\n",
            "strain 0.24108600616455078\n",
            "strain 0.24106985330581665\n",
            "strain 0.2117413729429245\n",
            "strain 0.20470096170902252\n",
            "strain 0.2516075670719147\n",
            "strain 0.312690407037735\n",
            "classify 2.28497314453125\n",
            "classify 2.2783203125\n",
            "classify 2.38079833984375\n",
            "classify 2.09185791015625\n",
            "classify 2.362548828125\n",
            "classify 2.18695068359375\n",
            "classify 2.29669189453125\n",
            "classify 2.17852783203125\n",
            "classify 2.22503662109375\n",
            "0.34375\n",
            "0.265625\n",
            "0.25\n",
            "0.265625\n",
            "strain 0.24931548535823822\n",
            "strain 0.2228589504957199\n",
            "strain 0.23383353650569916\n",
            "strain 0.24481050670146942\n",
            "strain 0.2522938847541809\n",
            "strain 0.22250424325466156\n",
            "strain 0.22989635169506073\n",
            "strain 0.257362961769104\n",
            "strain 0.2518269121646881\n",
            "classify 2.2109375\n",
            "classify 2.17242431640625\n",
            "classify 2.38385009765625\n",
            "classify 2.34326171875\n",
            "classify 2.240478515625\n",
            "classify 2.3251953125\n",
            "classify 2.2109375\n",
            "classify 2.1893310546875\n",
            "classify 2.31829833984375\n",
            "0.265625\n",
            "0.28125\n",
            "0.21875\n",
            "0.296875\n",
            "strain 0.22206753492355347\n",
            "strain 0.21967191994190216\n",
            "strain 0.2300439029932022\n",
            "strain 0.25578075647354126\n",
            "strain 0.253079891204834\n",
            "strain 0.24192708730697632\n",
            "strain 0.2062303125858307\n",
            "strain 0.24142156541347504\n",
            "strain 0.24290773272514343\n",
            "classify 2.254638671875\n",
            "classify 2.1146240234375\n",
            "classify 2.1995849609375\n",
            "classify 2.175537109375\n",
            "classify 2.15167236328125\n",
            "classify 2.3309326171875\n",
            "classify 2.3148193359375\n",
            "classify 2.4290771484375\n",
            "classify 2.32818603515625\n",
            "0.234375\n",
            "0.25\n",
            "0.265625\n",
            "0.3125\n",
            "strain 0.24582993984222412\n",
            "strain 0.24982202053070068\n",
            "strain 0.2284718006849289\n",
            "strain 0.22187720239162445\n",
            "strain 0.22885139286518097\n",
            "strain 0.2862586975097656\n",
            "strain 0.20331667363643646\n",
            "strain 0.2563265264034271\n",
            "strain 0.2456868588924408\n",
            "classify 2.4241943359375\n",
            "classify 2.33355712890625\n",
            "classify 2.2515869140625\n",
            "classify 2.24853515625\n",
            "classify 2.24371337890625\n",
            "classify 2.12481689453125\n",
            "classify 2.27972412109375\n",
            "classify 2.28533935546875\n",
            "classify 2.30279541015625\n",
            "0.25\n",
            "0.296875\n",
            "0.203125\n",
            "0.21875\n",
            "strain 0.25365012884140015\n",
            "strain 0.2390219122171402\n",
            "strain 0.25388166308403015\n",
            "strain 0.21673789620399475\n",
            "strain 0.23298029601573944\n",
            "strain 0.23528042435646057\n",
            "strain 0.24024367332458496\n",
            "strain 0.20499928295612335\n",
            "strain 0.23348292708396912\n",
            "classify 2.268310546875\n",
            "classify 2.3310546875\n",
            "classify 2.2489013671875\n",
            "classify 2.19989013671875\n",
            "classify 2.1575927734375\n",
            "classify 2.4434814453125\n",
            "classify 2.20465087890625\n",
            "classify 2.38330078125\n",
            "classify 2.11199951171875\n",
            "0.234375\n",
            "0.28125\n",
            "0.28125\n",
            "0.3125\n",
            "strain 0.26609018445014954\n",
            "strain 0.22337789833545685\n",
            "strain 0.21861477196216583\n",
            "strain 0.2365567982196808\n",
            "strain 0.21149054169654846\n",
            "strain 0.24495157599449158\n",
            "strain 0.2237066924571991\n",
            "strain 0.21215003728866577\n",
            "strain 0.22460761666297913\n",
            "classify 2.37554931640625\n",
            "classify 2.26300048828125\n",
            "classify 2.2427978515625\n",
            "classify 2.2833251953125\n",
            "classify 2.17510986328125\n",
            "classify 2.15911865234375\n",
            "classify 2.37786865234375\n",
            "classify 2.133056640625\n",
            "classify 2.34564208984375\n",
            "0.3125\n",
            "0.28125\n",
            "0.265625\n",
            "0.21875\n",
            "strain 0.23849710822105408\n",
            "strain 0.2630712687969208\n",
            "strain 0.23559698462486267\n",
            "strain 0.22637222707271576\n",
            "strain 0.2485252320766449\n",
            "strain 0.25579506158828735\n",
            "strain 0.23029199242591858\n",
            "strain 0.24511633813381195\n",
            "strain 0.20911799371242523\n",
            "classify 2.22528076171875\n",
            "classify 2.3175048828125\n",
            "classify 2.15997314453125\n",
            "classify 2.2265625\n",
            "classify 2.30029296875\n",
            "classify 2.08135986328125\n",
            "classify 2.1949462890625\n",
            "classify 2.53826904296875\n",
            "classify 2.2039794921875\n",
            "0.28125\n",
            "0.296875\n",
            "0.28125\n",
            "0.234375\n",
            "strain 0.22407354414463043\n",
            "strain 0.22424310445785522\n",
            "strain 0.24808213114738464\n",
            "strain 0.2181807905435562\n",
            "strain 0.2293250560760498\n",
            "strain 0.2491839975118637\n",
            "strain 0.2363821566104889\n",
            "strain 0.2185845822095871\n",
            "strain 0.23249270021915436\n",
            "classify 2.3328857421875\n",
            "classify 2.41387939453125\n",
            "classify 2.39447021484375\n",
            "classify 2.1873779296875\n",
            "classify 2.33477783203125\n",
            "classify 2.2208251953125\n",
            "classify 2.0677490234375\n",
            "classify 2.1654052734375\n",
            "classify 2.27685546875\n",
            "0.25\n",
            "0.21875\n",
            "0.25\n",
            "0.359375\n",
            "strain 0.20653478801250458\n",
            "strain 0.24773699045181274\n",
            "strain 0.2542453408241272\n",
            "strain 0.21071985363960266\n",
            "strain 0.24321234226226807\n",
            "strain 0.21938572824001312\n",
            "strain 0.20514029264450073\n",
            "strain 0.22005890309810638\n",
            "strain 0.23194703459739685\n",
            "classify 2.15350341796875\n",
            "classify 2.30731201171875\n",
            "classify 2.12945556640625\n",
            "classify 2.3328857421875\n",
            "classify 2.2103271484375\n",
            "classify 2.16314697265625\n",
            "classify 2.3477783203125\n",
            "classify 2.36724853515625\n",
            "classify 2.168212890625\n",
            "0.296875\n",
            "0.34375\n",
            "0.265625\n",
            "0.296875\n",
            "strain 0.22474411129951477\n",
            "strain 0.2130253165960312\n",
            "strain 0.21674203872680664\n",
            "strain 0.22685611248016357\n",
            "strain 0.2364099770784378\n",
            "strain 0.23879800736904144\n",
            "strain 0.22330063581466675\n",
            "strain 0.2253146469593048\n",
            "strain 0.2326374053955078\n",
            "classify 2.26727294921875\n",
            "classify 2.1671142578125\n",
            "classify 2.21380615234375\n",
            "classify 2.15966796875\n",
            "classify 2.16943359375\n",
            "classify 2.3074951171875\n",
            "classify 2.4776611328125\n",
            "classify 2.302734375\n",
            "classify 2.2913818359375\n",
            "0.25\n",
            "0.359375\n",
            "0.234375\n",
            "0.21875\n",
            "strain 0.22909307479858398\n",
            "strain 0.2404939830303192\n",
            "strain 0.2527531087398529\n",
            "strain 0.2365771383047104\n",
            "strain 0.23153240978717804\n",
            "strain 0.23002822697162628\n",
            "strain 0.2205522060394287\n",
            "strain 0.23057958483695984\n",
            "strain 0.22750863432884216\n",
            "classify 2.39581298828125\n",
            "classify 2.33270263671875\n",
            "classify 2.288330078125\n",
            "classify 2.33416748046875\n",
            "classify 2.11407470703125\n",
            "classify 2.11651611328125\n",
            "classify 2.237060546875\n",
            "classify 2.1011962890625\n",
            "classify 2.334228515625\n",
            "0.265625\n",
            "0.265625\n",
            "0.296875\n",
            "0.3125\n",
            "strain 0.21953968703746796\n",
            "strain 0.20595110952854156\n",
            "strain 0.2254316508769989\n",
            "strain 0.21321366727352142\n",
            "strain 0.22745157778263092\n",
            "strain 0.2687940001487732\n",
            "strain 0.22011667490005493\n",
            "strain 0.256166011095047\n",
            "strain 0.22622458636760712\n",
            "classify 2.21563720703125\n",
            "classify 2.16387939453125\n",
            "classify 2.07684326171875\n",
            "classify 2.35003662109375\n",
            "classify 2.17120361328125\n",
            "classify 2.40643310546875\n",
            "classify 2.44647216796875\n",
            "classify 2.31964111328125\n",
            "classify 2.19677734375\n",
            "0.3125\n",
            "0.296875\n",
            "0.296875\n",
            "0.296875\n",
            "strain 0.24857084453105927\n",
            "strain 0.23225811123847961\n",
            "strain 0.2252604365348816\n",
            "strain 0.23120975494384766\n",
            "strain 0.2828879654407501\n",
            "strain 0.2105075716972351\n",
            "strain 0.21251888573169708\n",
            "strain 0.24924679100513458\n",
            "strain 0.21956229209899902\n",
            "classify 2.339599609375\n",
            "classify 2.35931396484375\n",
            "classify 2.3260498046875\n",
            "classify 2.0604248046875\n",
            "classify 2.4552001953125\n",
            "classify 2.30865478515625\n",
            "classify 2.1104736328125\n",
            "classify 2.16607666015625\n",
            "classify 2.3853759765625\n",
            "0.328125\n",
            "0.28125\n",
            "0.265625\n",
            "0.140625\n",
            "strain 0.25386252999305725\n",
            "strain 0.21573689579963684\n",
            "strain 0.2222418487071991\n",
            "strain 0.2219334840774536\n",
            "strain 0.21930362284183502\n",
            "strain 0.2132752388715744\n",
            "strain 0.24250121414661407\n",
            "strain 0.21367903053760529\n",
            "strain 0.22106799483299255\n",
            "classify 2.2391357421875\n",
            "classify 2.0958251953125\n",
            "classify 2.3087158203125\n",
            "classify 2.29193115234375\n",
            "classify 2.20733642578125\n",
            "classify 2.3499755859375\n",
            "classify 2.2445068359375\n",
            "classify 2.44720458984375\n",
            "classify 2.23846435546875\n",
            "0.3125\n",
            "0.21875\n",
            "0.265625\n",
            "0.3125\n",
            "strain 0.23460520803928375\n",
            "strain 0.2306196540594101\n",
            "strain 0.23759354650974274\n",
            "strain 0.24076150357723236\n",
            "strain 0.19678163528442383\n",
            "strain 0.23498904705047607\n",
            "strain 0.22278963029384613\n",
            "strain 0.20059198141098022\n",
            "strain 0.2608106732368469\n",
            "classify 2.33233642578125\n",
            "classify 2.1507568359375\n",
            "classify 2.33544921875\n",
            "classify 2.3018798828125\n",
            "classify 2.38275146484375\n",
            "classify 1.99713134765625\n",
            "classify 2.36260986328125\n",
            "classify 2.20074462890625\n",
            "classify 2.2581787109375\n",
            "0.296875\n",
            "0.25\n",
            "0.3125\n",
            "0.21875\n",
            "strain 0.23275521397590637\n",
            "strain 0.22922348976135254\n",
            "strain 0.24025684595108032\n",
            "strain 0.23730342090129852\n",
            "strain 0.2266402691602707\n",
            "strain 0.20714464783668518\n",
            "strain 0.253076434135437\n",
            "strain 0.22385568916797638\n",
            "strain 0.23891586065292358\n",
            "classify 2.14874267578125\n",
            "classify 2.26641845703125\n",
            "classify 2.2806396484375\n",
            "classify 2.26129150390625\n",
            "classify 2.2802734375\n",
            "classify 2.3299560546875\n",
            "classify 2.22650146484375\n",
            "classify 2.26513671875\n",
            "classify 2.21575927734375\n",
            "0.265625\n",
            "0.28125\n",
            "0.28125\n",
            "0.296875\n",
            "strain 0.239262193441391\n",
            "strain 0.22977255284786224\n",
            "strain 0.22099661827087402\n",
            "strain 0.22727800905704498\n",
            "strain 0.22450847923755646\n",
            "strain 0.23002389073371887\n",
            "strain 0.2649071514606476\n",
            "strain 0.20705783367156982\n",
            "strain 0.21589195728302002\n",
            "classify 2.3116455078125\n",
            "classify 2.38165283203125\n",
            "classify 2.126953125\n",
            "classify 2.1259765625\n",
            "classify 2.3399658203125\n",
            "classify 2.03240966796875\n",
            "classify 2.21978759765625\n",
            "classify 2.286376953125\n",
            "classify 2.5296630859375\n",
            "0.3125\n",
            "0.3125\n",
            "0.28125\n",
            "0.265625\n",
            "strain 0.2143314778804779\n",
            "strain 0.24942226707935333\n",
            "strain 0.23932798206806183\n",
            "strain 0.24076764285564423\n",
            "strain 0.22259069979190826\n",
            "strain 0.2739759683609009\n",
            "strain 0.23592312633991241\n",
            "strain 0.23691432178020477\n",
            "strain 0.20551730692386627\n",
            "classify 2.34954833984375\n",
            "classify 2.11956787109375\n",
            "classify 2.3165283203125\n",
            "classify 2.340576171875\n",
            "classify 2.342529296875\n",
            "classify 2.17938232421875\n",
            "classify 2.25164794921875\n",
            "classify 2.272705078125\n",
            "classify 2.1868896484375\n",
            "0.25\n",
            "0.328125\n",
            "0.328125\n",
            "0.25\n",
            "strain 0.23065651953220367\n",
            "strain 0.23794104158878326\n",
            "strain 0.220409095287323\n",
            "strain 0.23333986103534698\n",
            "strain 0.25397658348083496\n",
            "strain 0.24613749980926514\n",
            "strain 0.25714415311813354\n",
            "strain 0.2264324575662613\n",
            "strain 0.23218074440956116\n",
            "classify 2.3416748046875\n",
            "classify 2.2987060546875\n",
            "classify 2.472900390625\n",
            "classify 2.10858154296875\n",
            "classify 2.2467041015625\n",
            "classify 2.3511962890625\n",
            "classify 2.18157958984375\n",
            "classify 2.24505615234375\n",
            "classify 2.310791015625\n",
            "0.203125\n",
            "0.25\n",
            "0.296875\n",
            "0.3125\n",
            "strain 0.21959348022937775\n",
            "strain 0.23073244094848633\n",
            "strain 0.248018279671669\n",
            "strain 0.214790478348732\n",
            "strain 0.24176423251628876\n",
            "strain 0.25032952427864075\n",
            "strain 0.22208057343959808\n",
            "strain 0.2554306387901306\n",
            "strain 0.2377162128686905\n",
            "classify 2.11163330078125\n",
            "classify 2.12738037109375\n",
            "classify 2.2818603515625\n",
            "classify 2.3427734375\n",
            "classify 2.2230224609375\n",
            "classify 2.1925048828125\n",
            "classify 2.42535400390625\n",
            "classify 2.23663330078125\n",
            "classify 2.33184814453125\n",
            "0.203125\n",
            "0.328125\n",
            "0.3125\n",
            "0.234375\n",
            "strain 0.25114160776138306\n",
            "strain 0.24516960978507996\n",
            "strain 0.2512788772583008\n",
            "strain 0.248758003115654\n",
            "strain 0.23842652142047882\n",
            "strain 0.25654178857803345\n",
            "strain 0.21167488396167755\n",
            "strain 0.2525371015071869\n",
            "strain 0.2525824010372162\n",
            "classify 2.15484619140625\n",
            "classify 2.32379150390625\n",
            "classify 2.3131103515625\n",
            "classify 2.4208984375\n",
            "classify 2.28472900390625\n",
            "classify 2.36016845703125\n",
            "classify 2.15802001953125\n",
            "classify 2.1961669921875\n",
            "classify 2.13525390625\n",
            "0.328125\n",
            "0.25\n",
            "0.234375\n",
            "0.359375\n",
            "strain 0.23779946565628052\n",
            "strain 0.24666424095630646\n",
            "strain 0.22801314294338226\n",
            "strain 0.22090007364749908\n",
            "strain 0.22968292236328125\n",
            "strain 0.2363215833902359\n",
            "strain 0.24853919446468353\n",
            "strain 0.23161403834819794\n",
            "strain 0.2581401765346527\n",
            "classify 2.171142578125\n",
            "classify 2.20751953125\n",
            "classify 2.35784912109375\n",
            "classify 2.30078125\n",
            "classify 2.287353515625\n",
            "classify 2.18865966796875\n",
            "classify 2.34002685546875\n",
            "classify 2.34136962890625\n",
            "classify 2.24005126953125\n",
            "0.25\n",
            "0.28125\n",
            "0.34375\n",
            "0.265625\n",
            "strain 0.22009456157684326\n",
            "strain 0.23699866235256195\n",
            "strain 0.2387673556804657\n",
            "strain 0.2629688084125519\n",
            "strain 0.24990087747573853\n",
            "strain 0.21910902857780457\n",
            "strain 0.2266543060541153\n",
            "strain 0.22562281787395477\n",
            "strain 0.23753197491168976\n",
            "classify 2.30841064453125\n",
            "classify 2.30975341796875\n",
            "classify 2.25408935546875\n",
            "classify 2.23541259765625\n",
            "classify 2.29217529296875\n",
            "classify 2.30279541015625\n",
            "classify 2.1015625\n",
            "classify 2.1470947265625\n",
            "classify 2.35491943359375\n",
            "0.3125\n",
            "0.25\n",
            "0.28125\n",
            "0.328125\n",
            "strain 0.2448573261499405\n",
            "strain 0.21245263516902924\n",
            "strain 0.23370249569416046\n",
            "strain 0.24188049137592316\n",
            "strain 0.22084279358386993\n",
            "strain 0.23242194950580597\n",
            "strain 0.2216547578573227\n",
            "strain 0.24506287276744843\n",
            "strain 0.24047529697418213\n",
            "classify 2.22711181640625\n",
            "classify 2.28546142578125\n",
            "classify 2.31304931640625\n",
            "classify 2.5318603515625\n",
            "classify 2.2032470703125\n",
            "classify 2.1536865234375\n",
            "classify 2.28338623046875\n",
            "classify 2.0977783203125\n",
            "classify 2.34796142578125\n",
            "0.21875\n",
            "0.265625\n",
            "0.25\n",
            "0.234375\n",
            "strain 0.22896014153957367\n",
            "strain 0.2421005368232727\n",
            "strain 0.24665479362010956\n",
            "strain 0.27440813183784485\n",
            "strain 0.25340133905410767\n",
            "strain 0.24270272254943848\n",
            "strain 0.2447919100522995\n",
            "strain 0.2636753022670746\n",
            "strain 0.21270661056041718\n",
            "classify 2.14306640625\n",
            "classify 2.2098388671875\n",
            "classify 2.1439208984375\n",
            "classify 2.38787841796875\n",
            "classify 2.37481689453125\n",
            "classify 2.294921875\n",
            "classify 2.31195068359375\n",
            "classify 2.19683837890625\n",
            "classify 2.29071044921875\n",
            "0.265625\n",
            "0.25\n",
            "0.34375\n",
            "0.25\n",
            "strain 0.24020108580589294\n",
            "strain 0.24147982895374298\n",
            "strain 0.2362075001001358\n",
            "strain 0.2540186643600464\n",
            "strain 0.24684995412826538\n",
            "strain 0.2263765186071396\n",
            "strain 0.2277640402317047\n",
            "strain 0.22373953461647034\n",
            "strain 0.21859246492385864\n",
            "classify 2.32904052734375\n",
            "classify 2.180419921875\n",
            "classify 2.34332275390625\n",
            "classify 2.2393798828125\n",
            "classify 2.36151123046875\n",
            "classify 2.14495849609375\n",
            "classify 2.2371826171875\n",
            "classify 2.12322998046875\n",
            "classify 2.2093505859375\n",
            "0.25\n",
            "0.296875\n",
            "0.28125\n",
            "0.28125\n",
            "strain 0.21782852709293365\n",
            "strain 0.2322876900434494\n",
            "strain 0.22269859910011292\n",
            "strain 0.25681808590888977\n",
            "strain 0.240736722946167\n",
            "strain 0.30190175771713257\n",
            "strain 0.23883293569087982\n",
            "strain 0.22912684082984924\n",
            "strain 0.24990352988243103\n",
            "classify 2.24530029296875\n",
            "classify 2.14813232421875\n",
            "classify 2.23785400390625\n",
            "classify 2.24884033203125\n",
            "classify 2.07525634765625\n",
            "classify 2.36163330078125\n",
            "classify 2.21923828125\n",
            "classify 2.3895263671875\n",
            "classify 2.32843017578125\n",
            "0.21875\n",
            "0.3125\n",
            "0.234375\n",
            "0.28125\n",
            "strain 0.25688114762306213\n",
            "strain 0.24542252719402313\n",
            "strain 0.26396575570106506\n",
            "strain 0.21585829555988312\n",
            "strain 0.23794753849506378\n",
            "strain 0.22371453046798706\n",
            "strain 0.2500712275505066\n",
            "strain 0.2623332142829895\n",
            "strain 0.22212538123130798\n",
            "classify 2.18682861328125\n",
            "classify 2.2578125\n",
            "classify 2.13043212890625\n",
            "classify 2.19158935546875\n",
            "classify 2.22686767578125\n",
            "classify 2.1617431640625\n",
            "classify 2.20184326171875\n",
            "classify 2.27911376953125\n",
            "classify 2.46612548828125\n",
            "0.28125\n",
            "0.34375\n",
            "0.28125\n",
            "0.34375\n",
            "strain 0.22712475061416626\n",
            "strain 0.2327747792005539\n",
            "strain 0.2610696852207184\n",
            "strain 0.22958619892597198\n",
            "strain 0.24710115790367126\n",
            "strain 0.22879710793495178\n",
            "strain 0.23559477925300598\n",
            "strain 0.2305891215801239\n",
            "strain 0.230025053024292\n",
            "classify 2.125\n",
            "classify 2.38946533203125\n",
            "classify 2.42034912109375\n",
            "classify 2.51904296875\n",
            "classify 2.1927490234375\n",
            "classify 2.19256591796875\n",
            "classify 2.30181884765625\n",
            "classify 2.25421142578125\n",
            "classify 2.03387451171875\n",
            "0.28125\n",
            "0.3125\n",
            "0.3125\n",
            "0.21875\n",
            "strain 0.23218129575252533\n",
            "strain 0.24837762117385864\n",
            "strain 0.26690229773521423\n",
            "strain 0.22511211037635803\n",
            "strain 0.2303159236907959\n",
            "strain 0.21987095475196838\n",
            "strain 0.2726344168186188\n",
            "strain 0.22684812545776367\n",
            "strain 0.2214486300945282\n",
            "classify 2.21771240234375\n",
            "classify 2.1331787109375\n",
            "classify 2.1920166015625\n",
            "classify 2.23162841796875\n",
            "classify 2.4124755859375\n",
            "classify 2.267333984375\n",
            "classify 2.3843994140625\n",
            "classify 2.38812255859375\n",
            "classify 2.16412353515625\n",
            "0.25\n",
            "0.265625\n",
            "0.3125\n",
            "0.21875\n",
            "strain 0.21105225384235382\n",
            "strain 0.23006922006607056\n",
            "strain 0.21528229117393494\n",
            "strain 0.21390938758850098\n",
            "strain 0.306898295879364\n",
            "strain 0.24018412828445435\n",
            "strain 0.25238335132598877\n",
            "strain 0.2187623381614685\n",
            "strain 0.22545644640922546\n",
            "classify 2.10595703125\n",
            "classify 2.52215576171875\n",
            "classify 2.42718505859375\n",
            "classify 2.1708984375\n",
            "classify 2.2457275390625\n",
            "classify 2.13006591796875\n",
            "classify 2.2735595703125\n",
            "classify 2.18670654296875\n",
            "classify 2.1673583984375\n",
            "0.328125\n",
            "0.328125\n",
            "0.328125\n",
            "0.21875\n",
            "strain 0.23438407480716705\n",
            "strain 0.219312846660614\n",
            "strain 0.22823026776313782\n",
            "strain 0.20728269219398499\n",
            "strain 0.2389656901359558\n",
            "strain 0.2539514899253845\n",
            "strain 0.22578249871730804\n",
            "strain 0.2482025921344757\n",
            "strain 0.249110147356987\n",
            "classify 2.2154541015625\n",
            "classify 2.128662109375\n",
            "classify 2.1097412109375\n",
            "classify 2.381591796875\n",
            "classify 2.453857421875\n",
            "classify 2.38787841796875\n",
            "classify 2.244873046875\n",
            "classify 2.092041015625\n",
            "classify 2.2596435546875\n",
            "0.25\n",
            "0.21875\n",
            "0.328125\n",
            "0.25\n",
            "strain 0.23710808157920837\n",
            "strain 0.23450887203216553\n",
            "strain 0.2205263376235962\n",
            "strain 0.21670319139957428\n",
            "strain 0.2252018302679062\n",
            "strain 0.2735104560852051\n",
            "strain 0.22545912861824036\n",
            "strain 0.28062206506729126\n",
            "strain 0.2475520372390747\n",
            "classify 2.33599853515625\n",
            "classify 2.13519287109375\n",
            "classify 2.2437744140625\n",
            "classify 2.3726806640625\n",
            "classify 2.21368408203125\n",
            "classify 2.09259033203125\n",
            "classify 2.3050537109375\n",
            "classify 2.21636962890625\n",
            "classify 2.4405517578125\n",
            "0.359375\n",
            "0.234375\n",
            "0.3125\n",
            "0.203125\n",
            "strain 0.21957285702228546\n",
            "strain 0.24552470445632935\n",
            "strain 0.23058444261550903\n",
            "strain 0.2434515804052353\n",
            "strain 0.2654529809951782\n",
            "strain 0.24274688959121704\n",
            "strain 0.22327309846878052\n",
            "strain 0.19723859429359436\n",
            "strain 0.23424994945526123\n",
            "classify 2.267578125\n",
            "classify 2.241943359375\n",
            "classify 2.28228759765625\n",
            "classify 2.08795166015625\n",
            "classify 2.35211181640625\n",
            "classify 2.1900634765625\n",
            "classify 2.41265869140625\n",
            "classify 2.3260498046875\n",
            "classify 2.22943115234375\n",
            "0.28125\n",
            "0.234375\n",
            "0.28125\n",
            "0.28125\n",
            "strain 0.23238296806812286\n",
            "strain 0.24228088557720184\n",
            "strain 0.23868563771247864\n",
            "strain 0.23616205155849457\n",
            "strain 0.22055363655090332\n",
            "strain 0.22722512483596802\n",
            "strain 0.2299044281244278\n",
            "strain 0.21366877853870392\n",
            "strain 0.2508021593093872\n",
            "classify 2.1282958984375\n",
            "classify 2.2418212890625\n",
            "classify 2.4627685546875\n",
            "classify 2.2618408203125\n",
            "classify 2.21881103515625\n",
            "classify 2.2598876953125\n",
            "classify 2.36346435546875\n",
            "classify 2.2884521484375\n",
            "classify 2.2366943359375\n",
            "0.265625\n",
            "0.28125\n",
            "0.3125\n",
            "0.296875\n",
            "strain 0.2219248116016388\n",
            "strain 0.21748687326908112\n",
            "strain 0.25668200850486755\n",
            "strain 0.1995917558670044\n",
            "strain 0.23618781566619873\n",
            "strain 0.24876873195171356\n",
            "strain 0.21974684298038483\n",
            "strain 0.2687548100948334\n",
            "strain 0.21790094673633575\n",
            "classify 2.20361328125\n",
            "classify 2.31671142578125\n",
            "classify 2.09136962890625\n",
            "classify 2.421875\n",
            "classify 2.16021728515625\n",
            "classify 2.39453125\n",
            "classify 2.1480712890625\n",
            "classify 2.21051025390625\n",
            "classify 2.24005126953125\n",
            "0.3125\n",
            "0.234375\n",
            "0.28125\n",
            "0.328125\n",
            "strain 0.26250582933425903\n",
            "strain 0.20875723659992218\n",
            "strain 0.2529401481151581\n",
            "strain 0.2662939131259918\n",
            "strain 0.22520969808101654\n",
            "strain 0.23616454005241394\n",
            "strain 0.24929240345954895\n",
            "strain 0.2441907674074173\n",
            "strain 0.2434704750776291\n",
            "classify 2.36328125\n",
            "classify 2.34490966796875\n",
            "classify 2.22076416015625\n",
            "classify 2.14569091796875\n",
            "classify 2.16973876953125\n",
            "classify 2.05029296875\n",
            "classify 2.0838623046875\n",
            "classify 2.43292236328125\n",
            "classify 2.3125\n",
            "0.203125\n",
            "0.3125\n",
            "0.28125\n",
            "0.25\n",
            "strain 0.24575771391391754\n",
            "strain 0.2510112226009369\n",
            "strain 0.22579334676265717\n",
            "strain 0.2851088345050812\n",
            "strain 0.21769723296165466\n",
            "strain 0.22675281763076782\n",
            "strain 0.26973840594291687\n",
            "strain 0.22715744376182556\n",
            "strain 0.27119123935699463\n",
            "classify 2.09503173828125\n",
            "classify 2.39422607421875\n",
            "classify 2.42791748046875\n",
            "classify 2.1236572265625\n",
            "classify 2.3055419921875\n",
            "classify 2.22119140625\n",
            "classify 2.26220703125\n",
            "classify 2.267822265625\n",
            "classify 2.216796875\n",
            "0.296875\n",
            "0.28125\n",
            "0.34375\n",
            "0.25\n",
            "strain 0.27131929993629456\n",
            "strain 0.22225788235664368\n",
            "strain 0.208383709192276\n",
            "strain 0.21890074014663696\n",
            "strain 0.2578354775905609\n",
            "strain 0.20209208130836487\n",
            "strain 0.21328848600387573\n",
            "strain 0.1993284821510315\n",
            "strain 0.23006288707256317\n",
            "classify 2.2078857421875\n",
            "classify 2.33599853515625\n",
            "classify 2.22528076171875\n",
            "classify 2.43505859375\n",
            "classify 2.28875732421875\n",
            "classify 2.18255615234375\n",
            "classify 2.238037109375\n",
            "classify 2.3076171875\n",
            "classify 2.1815185546875\n",
            "0.296875\n",
            "0.296875\n",
            "0.15625\n",
            "0.375\n",
            "strain 0.23704640567302704\n",
            "strain 0.23269370198249817\n",
            "strain 0.20171114802360535\n",
            "strain 0.2259058654308319\n",
            "strain 0.21588028967380524\n",
            "strain 0.24167290329933167\n",
            "strain 0.22727929055690765\n",
            "strain 0.24685445427894592\n",
            "strain 0.19789136946201324\n",
            "classify 2.24517822265625\n",
            "classify 2.53411865234375\n",
            "classify 2.1966552734375\n",
            "classify 2.42083740234375\n",
            "classify 2.17291259765625\n",
            "classify 2.37091064453125\n",
            "classify 2.21343994140625\n",
            "classify 2.26800537109375\n",
            "classify 2.23175048828125\n",
            "0.265625\n",
            "0.234375\n",
            "0.28125\n",
            "0.25\n",
            "strain 0.25073230266571045\n",
            "strain 0.20269230008125305\n",
            "strain 0.2241750806570053\n",
            "strain 0.2108660489320755\n",
            "strain 0.2216646373271942\n",
            "strain 0.23960433900356293\n",
            "strain 0.23014205694198608\n",
            "strain 0.23644636571407318\n",
            "strain 0.2509675621986389\n",
            "classify 2.210693359375\n",
            "classify 2.30828857421875\n",
            "classify 2.123291015625\n",
            "classify 2.29931640625\n",
            "classify 2.26263427734375\n",
            "classify 2.343994140625\n",
            "classify 2.3858642578125\n",
            "classify 2.306396484375\n",
            "classify 2.18243408203125\n",
            "0.28125\n",
            "0.21875\n",
            "0.21875\n",
            "0.3125\n",
            "strain 0.22576281428337097\n",
            "strain 0.2627669870853424\n",
            "strain 0.22151389718055725\n",
            "strain 0.24940942227840424\n",
            "strain 0.22522100806236267\n",
            "strain 0.24696555733680725\n",
            "strain 0.21042762696743011\n",
            "strain 0.22304539382457733\n",
            "strain 0.222673699259758\n",
            "classify 2.35491943359375\n",
            "classify 2.244140625\n",
            "classify 2.19244384765625\n",
            "classify 2.03662109375\n",
            "classify 2.39349365234375\n",
            "classify 2.2677001953125\n",
            "classify 2.20684814453125\n",
            "classify 2.0933837890625\n",
            "classify 2.36663818359375\n",
            "0.21875\n",
            "0.25\n",
            "0.28125\n",
            "0.34375\n",
            "strain 0.22451667487621307\n",
            "strain 0.2364409863948822\n",
            "strain 0.20504139363765717\n",
            "strain 0.22612468898296356\n",
            "strain 0.2007482796907425\n",
            "strain 0.2350907027721405\n",
            "strain 0.22553806006908417\n",
            "strain 0.23649971187114716\n",
            "strain 0.21651381254196167\n",
            "classify 2.278076171875\n",
            "classify 2.1531982421875\n",
            "classify 2.2279052734375\n",
            "classify 2.16412353515625\n",
            "classify 2.21319580078125\n",
            "classify 2.25811767578125\n",
            "classify 2.35137939453125\n",
            "classify 2.222412109375\n",
            "classify 2.40216064453125\n",
            "0.296875\n",
            "0.25\n",
            "0.3125\n",
            "0.296875\n",
            "strain 0.2572188675403595\n",
            "strain 0.22925512492656708\n",
            "strain 0.22866442799568176\n",
            "strain 0.2336755245923996\n",
            "strain 0.22602200508117676\n",
            "strain 0.241627499461174\n",
            "strain 0.22077146172523499\n",
            "strain 0.24616441130638123\n",
            "strain 0.21523980796337128\n",
            "classify 2.19378662109375\n",
            "classify 2.4215087890625\n",
            "classify 2.168212890625\n",
            "classify 2.1754150390625\n",
            "classify 2.3287353515625\n",
            "classify 2.17510986328125\n",
            "classify 2.18463134765625\n",
            "classify 2.38177490234375\n",
            "classify 2.37347412109375\n",
            "0.25\n",
            "0.296875\n",
            "0.234375\n",
            "0.28125\n",
            "strain 0.23966015875339508\n",
            "strain 0.26665014028549194\n",
            "strain 0.23908469080924988\n",
            "strain 0.2293604612350464\n",
            "strain 0.22531656920909882\n",
            "strain 0.24922733008861542\n",
            "strain 0.2515563368797302\n",
            "strain 0.21174007654190063\n",
            "strain 0.28096887469291687\n",
            "classify 2.1693115234375\n",
            "classify 2.3026123046875\n",
            "classify 2.2525634765625\n",
            "classify 2.2569580078125\n",
            "classify 2.33599853515625\n",
            "classify 2.30908203125\n",
            "classify 2.3343505859375\n",
            "classify 2.0706787109375\n",
            "classify 2.2310791015625\n",
            "0.34375\n",
            "0.21875\n",
            "0.265625\n",
            "0.25\n",
            "strain 0.23694604635238647\n",
            "strain 0.24093130230903625\n",
            "strain 0.2456289380788803\n",
            "strain 0.25614726543426514\n",
            "strain 0.2307826727628708\n",
            "strain 0.22999431192874908\n",
            "strain 0.22953155636787415\n",
            "strain 0.22960996627807617\n",
            "strain 0.2579830586910248\n",
            "classify 2.23870849609375\n",
            "classify 2.32330322265625\n",
            "classify 2.1861572265625\n",
            "classify 2.36444091796875\n",
            "classify 2.478759765625\n",
            "classify 2.282470703125\n",
            "classify 2.2623291015625\n",
            "classify 2.2716064453125\n",
            "classify 2.025634765625\n",
            "0.34375\n",
            "0.3125\n",
            "0.109375\n",
            "0.296875\n",
            "strain 0.25038495659828186\n",
            "strain 0.22804327309131622\n",
            "strain 0.22401562333106995\n",
            "strain 0.24522678554058075\n",
            "strain 0.24602149426937103\n",
            "strain 0.24110865592956543\n",
            "strain 0.22238263487815857\n",
            "strain 0.20107898116111755\n",
            "strain 0.22126859426498413\n",
            "classify 2.39776611328125\n",
            "classify 2.2313232421875\n",
            "classify 2.4774169921875\n",
            "classify 2.1953125\n",
            "classify 2.237548828125\n",
            "classify 2.09271240234375\n",
            "classify 2.13824462890625\n",
            "classify 2.32373046875\n",
            "classify 2.22705078125\n",
            "0.34375\n",
            "0.21875\n",
            "0.265625\n",
            "0.28125\n",
            "strain 0.23229117691516876\n",
            "strain 0.23731936514377594\n",
            "strain 0.23647798597812653\n",
            "strain 0.21983057260513306\n",
            "strain 0.23144885897636414\n",
            "strain 0.20247116684913635\n",
            "strain 0.2984994351863861\n",
            "strain 0.2365153282880783\n",
            "strain 0.23373155295848846\n",
            "classify 2.27618408203125\n",
            "classify 2.2667236328125\n",
            "classify 2.29071044921875\n",
            "classify 2.15106201171875\n",
            "classify 2.36907958984375\n",
            "classify 2.22650146484375\n",
            "classify 2.2694091796875\n",
            "classify 2.0428466796875\n",
            "classify 2.28564453125\n",
            "0.265625\n",
            "0.296875\n",
            "0.328125\n",
            "0.28125\n",
            "strain 0.20460425317287445\n",
            "strain 0.22724522650241852\n",
            "strain 0.24965880811214447\n",
            "strain 0.2304353415966034\n",
            "strain 0.24114349484443665\n",
            "strain 0.22640590369701385\n",
            "strain 0.23143228888511658\n",
            "strain 0.23359818756580353\n",
            "strain 0.2373170107603073\n",
            "classify 2.2176513671875\n",
            "classify 2.23406982421875\n",
            "classify 2.10791015625\n",
            "classify 2.3077392578125\n",
            "classify 2.3736572265625\n",
            "classify 2.5291748046875\n",
            "classify 2.11785888671875\n",
            "classify 2.29107666015625\n",
            "classify 2.18499755859375\n",
            "0.359375\n",
            "0.171875\n",
            "0.234375\n",
            "0.3125\n",
            "strain 0.2087366282939911\n",
            "strain 0.23171177506446838\n",
            "strain 0.22275617718696594\n",
            "strain 0.20286628603935242\n",
            "strain 0.22481916844844818\n",
            "strain 0.22306232154369354\n",
            "strain 0.2662910521030426\n",
            "strain 0.24125812947750092\n",
            "strain 0.21488524973392487\n",
            "classify 2.27252197265625\n",
            "classify 2.2666015625\n",
            "classify 2.22930908203125\n",
            "classify 2.33160400390625\n",
            "classify 2.2474365234375\n",
            "classify 2.1824951171875\n",
            "classify 2.29803466796875\n",
            "classify 2.17242431640625\n",
            "classify 2.219970703125\n",
            "0.265625\n",
            "0.3125\n",
            "0.265625\n",
            "0.203125\n",
            "strain 0.22035618126392365\n",
            "strain 0.24059370160102844\n",
            "strain 0.23336707055568695\n",
            "strain 0.22335104644298553\n",
            "strain 0.23241205513477325\n",
            "strain 0.238491028547287\n",
            "strain 0.22499528527259827\n",
            "strain 0.19095541536808014\n",
            "strain 0.2551940381526947\n",
            "classify 2.216064453125\n",
            "classify 2.35711669921875\n",
            "classify 2.36053466796875\n",
            "classify 2.1383056640625\n",
            "classify 2.21905517578125\n",
            "classify 2.304443359375\n",
            "classify 2.1533203125\n",
            "classify 2.326416015625\n",
            "classify 2.22784423828125\n",
            "0.25\n",
            "0.296875\n",
            "0.3125\n",
            "0.265625\n",
            "strain 0.2500804662704468\n",
            "strain 0.2235061526298523\n",
            "strain 0.22873640060424805\n",
            "strain 0.2288639396429062\n",
            "strain 0.24048130214214325\n",
            "strain 0.21904872357845306\n",
            "strain 0.23432715237140656\n",
            "strain 0.22877028584480286\n",
            "strain 0.2564539313316345\n",
            "classify 2.27294921875\n",
            "classify 2.2469482421875\n",
            "classify 2.13787841796875\n",
            "classify 2.2969970703125\n",
            "classify 2.3070068359375\n",
            "classify 2.266357421875\n",
            "classify 2.23919677734375\n",
            "classify 2.208740234375\n",
            "classify 2.2783203125\n",
            "0.359375\n",
            "0.28125\n",
            "0.203125\n",
            "0.328125\n",
            "strain 0.21539641916751862\n",
            "strain 0.22858472168445587\n",
            "strain 0.2169598937034607\n",
            "strain 0.2364492118358612\n",
            "strain 0.23615172505378723\n",
            "strain 0.20761767029762268\n",
            "strain 0.24852469563484192\n",
            "strain 0.2400563508272171\n",
            "strain 0.20266327261924744\n",
            "classify 2.2843017578125\n",
            "classify 2.2310791015625\n",
            "classify 2.19647216796875\n",
            "classify 2.09197998046875\n",
            "classify 2.4541015625\n",
            "classify 2.2357177734375\n",
            "classify 2.19976806640625\n",
            "classify 2.24530029296875\n",
            "classify 2.24224853515625\n",
            "0.25\n",
            "0.25\n",
            "0.359375\n",
            "0.25\n",
            "strain 0.272292822599411\n",
            "strain 0.22566983103752136\n",
            "strain 0.21776090562343597\n",
            "strain 0.21618720889091492\n",
            "strain 0.23197519779205322\n",
            "strain 0.2139684557914734\n",
            "strain 0.2688977122306824\n",
            "strain 0.22587533295154572\n",
            "strain 0.22509457170963287\n",
            "classify 2.40081787109375\n",
            "classify 2.266357421875\n",
            "classify 1.94879150390625\n",
            "classify 2.24609375\n",
            "classify 2.17633056640625\n",
            "classify 2.37451171875\n",
            "classify 2.26885986328125\n",
            "classify 2.30169677734375\n",
            "classify 2.1802978515625\n",
            "0.28125\n",
            "0.328125\n",
            "0.28125\n",
            "0.359375\n",
            "strain 0.2195715606212616\n",
            "strain 0.22419998049736023\n",
            "strain 0.23449520766735077\n",
            "strain 0.19770647585391998\n",
            "strain 0.2540811598300934\n",
            "strain 0.22865945100784302\n",
            "strain 0.22254113852977753\n",
            "strain 0.24275806546211243\n",
            "strain 0.25051963329315186\n",
            "classify 2.3017578125\n",
            "classify 1.98663330078125\n",
            "classify 2.20318603515625\n",
            "classify 2.4207763671875\n",
            "classify 2.17816162109375\n",
            "classify 2.43212890625\n",
            "classify 2.164794921875\n",
            "classify 2.3541259765625\n",
            "classify 2.31622314453125\n",
            "0.28125\n",
            "0.328125\n",
            "0.296875\n",
            "0.296875\n",
            "strain 0.2366235852241516\n",
            "strain 0.25412338972091675\n",
            "strain 0.24183763563632965\n",
            "strain 0.2172621488571167\n",
            "strain 0.233411505818367\n",
            "strain 0.24974623322486877\n",
            "strain 0.22873778641223907\n",
            "strain 0.20660196244716644\n",
            "strain 0.2063436061143875\n",
            "classify 2.31390380859375\n",
            "classify 2.26507568359375\n",
            "classify 2.29241943359375\n",
            "classify 2.40960693359375\n",
            "classify 2.21697998046875\n",
            "classify 2.22515869140625\n",
            "classify 2.43994140625\n",
            "classify 2.1864013671875\n",
            "classify 2.0452880859375\n",
            "0.25\n",
            "0.375\n",
            "0.1875\n",
            "0.328125\n",
            "strain 0.23741869628429413\n",
            "strain 0.23961427807807922\n",
            "strain 0.20199239253997803\n",
            "strain 0.2387271225452423\n",
            "strain 0.22348712384700775\n",
            "strain 0.24764880537986755\n",
            "strain 0.20371778309345245\n",
            "strain 0.22908033430576324\n",
            "strain 0.24665293097496033\n",
            "classify 2.24554443359375\n",
            "classify 2.24212646484375\n",
            "classify 2.22515869140625\n",
            "classify 2.3714599609375\n",
            "classify 2.121826171875\n",
            "classify 2.221435546875\n",
            "classify 2.4622802734375\n",
            "classify 2.24176025390625\n",
            "classify 2.2247314453125\n",
            "0.328125\n",
            "0.28125\n",
            "0.296875\n",
            "0.3125\n",
            "strain 0.2233208566904068\n",
            "strain 0.2281883955001831\n",
            "strain 0.24893492460250854\n",
            "strain 0.22038482129573822\n",
            "strain 0.21133971214294434\n",
            "strain 0.24588844180107117\n",
            "strain 0.21767432987689972\n",
            "strain 0.2401665449142456\n",
            "strain 0.22723975777626038\n",
            "classify 2.1607666015625\n",
            "classify 2.304931640625\n",
            "classify 2.20068359375\n",
            "classify 2.4012451171875\n",
            "classify 2.26898193359375\n",
            "classify 2.4207763671875\n",
            "classify 2.21514892578125\n",
            "classify 2.18682861328125\n",
            "classify 2.11260986328125\n",
            "0.3125\n",
            "0.3125\n",
            "0.203125\n",
            "0.234375\n",
            "strain 0.20213711261749268\n",
            "strain 0.24405692517757416\n",
            "strain 0.24441473186016083\n",
            "strain 0.27389833331108093\n",
            "strain 0.2168055921792984\n",
            "strain 0.23738859593868256\n",
            "strain 0.23886461555957794\n",
            "strain 0.24728995561599731\n",
            "strain 0.2477923333644867\n",
            "classify 2.2767333984375\n",
            "classify 2.4417724609375\n",
            "classify 2.13323974609375\n",
            "classify 2.41473388671875\n",
            "classify 2.21063232421875\n",
            "classify 2.30535888671875\n",
            "classify 2.1373291015625\n",
            "classify 2.26983642578125\n",
            "classify 2.1749267578125\n",
            "0.21875\n",
            "0.359375\n",
            "0.21875\n",
            "0.3125\n",
            "strain 0.2288961261510849\n",
            "strain 0.24637041985988617\n",
            "strain 0.227128267288208\n",
            "strain 0.21402354538440704\n",
            "strain 0.21141387522220612\n",
            "strain 0.24880608916282654\n",
            "strain 0.22778083384037018\n",
            "strain 0.27802160382270813\n",
            "strain 0.2441999316215515\n",
            "classify 2.11468505859375\n",
            "classify 2.30889892578125\n",
            "classify 2.1915283203125\n",
            "classify 2.44451904296875\n",
            "classify 2.23565673828125\n",
            "classify 2.21282958984375\n",
            "classify 2.2015380859375\n",
            "classify 2.23016357421875\n",
            "classify 2.37969970703125\n",
            "0.28125\n",
            "0.359375\n",
            "0.25\n",
            "0.1875\n",
            "strain 0.23164726793766022\n",
            "strain 0.24387575685977936\n",
            "strain 0.2442382127046585\n",
            "strain 0.24948439002037048\n",
            "strain 0.21465501189231873\n",
            "strain 0.22401101887226105\n",
            "strain 0.24464109539985657\n",
            "strain 0.22651991248130798\n",
            "strain 0.20285172760486603\n",
            "classify 2.3778076171875\n",
            "classify 2.224365234375\n",
            "classify 2.29193115234375\n",
            "classify 2.2235107421875\n",
            "classify 2.248779296875\n",
            "classify 2.08941650390625\n",
            "classify 2.21258544921875\n",
            "classify 2.26666259765625\n",
            "classify 2.279052734375\n",
            "0.3125\n",
            "0.296875\n",
            "0.265625\n",
            "0.21875\n",
            "strain 0.2727169990539551\n",
            "strain 0.2372455894947052\n",
            "strain 0.2471315711736679\n",
            "strain 0.20391945540905\n",
            "strain 0.23829209804534912\n",
            "strain 0.21202456951141357\n",
            "strain 0.22372063994407654\n",
            "strain 0.23161821067333221\n",
            "strain 0.2196369171142578\n",
            "classify 2.283203125\n",
            "classify 2.2698974609375\n",
            "classify 2.29571533203125\n",
            "classify 2.16607666015625\n",
            "classify 2.518798828125\n",
            "classify 1.926513671875\n",
            "classify 2.29803466796875\n",
            "classify 2.4185791015625\n",
            "classify 2.22943115234375\n",
            "0.25\n",
            "0.375\n",
            "0.265625\n",
            "0.328125\n",
            "strain 0.22797073423862457\n",
            "strain 0.2490880936384201\n",
            "strain 0.23695620894432068\n",
            "strain 0.22741104662418365\n",
            "strain 0.27564361691474915\n",
            "strain 0.22470755875110626\n",
            "strain 0.2267770916223526\n",
            "strain 0.22107093036174774\n",
            "strain 0.22747211158275604\n",
            "classify 2.145263671875\n",
            "classify 2.37091064453125\n",
            "classify 2.1602783203125\n",
            "classify 2.3521728515625\n",
            "classify 2.3173828125\n",
            "classify 2.41925048828125\n",
            "classify 2.1588134765625\n",
            "classify 2.17889404296875\n",
            "classify 2.20758056640625\n",
            "0.265625\n",
            "0.3125\n",
            "0.28125\n",
            "0.296875\n",
            "strain 0.22798125445842743\n",
            "strain 0.23841185867786407\n",
            "strain 0.2078828364610672\n",
            "strain 0.23512150347232819\n",
            "strain 0.23685961961746216\n",
            "strain 0.19405816495418549\n",
            "strain 0.24455247819423676\n",
            "strain 0.21774417161941528\n",
            "strain 0.25918594002723694\n",
            "classify 2.1707763671875\n",
            "classify 2.2064208984375\n",
            "classify 2.2774658203125\n",
            "classify 2.24700927734375\n",
            "classify 2.31268310546875\n",
            "classify 2.298583984375\n",
            "classify 2.3804931640625\n",
            "classify 2.29010009765625\n",
            "classify 2.31817626953125\n",
            "0.328125\n",
            "0.203125\n",
            "0.234375\n",
            "0.3125\n",
            "strain 0.25818750262260437\n",
            "strain 0.2604289650917053\n",
            "strain 0.23295843601226807\n",
            "strain 0.20959879457950592\n",
            "strain 0.2410658746957779\n",
            "strain 0.2854768633842468\n",
            "strain 0.24134929478168488\n",
            "strain 0.23869118094444275\n",
            "strain 0.19478349387645721\n",
            "classify 2.2357177734375\n",
            "classify 2.29315185546875\n",
            "classify 2.14947509765625\n",
            "classify 2.28125\n",
            "classify 2.3505859375\n",
            "classify 2.36920166015625\n",
            "classify 1.99957275390625\n",
            "classify 2.2652587890625\n",
            "classify 2.37359619140625\n",
            "0.265625\n",
            "0.25\n",
            "0.234375\n",
            "0.34375\n",
            "strain 0.2551266551017761\n",
            "strain 0.2280140519142151\n",
            "strain 0.2251519411802292\n",
            "strain 0.259861022233963\n",
            "strain 0.21065732836723328\n",
            "strain 0.23537738621234894\n",
            "strain 0.2144647240638733\n",
            "strain 0.20705723762512207\n",
            "strain 0.23145124316215515\n",
            "classify 2.19598388671875\n",
            "classify 2.2099609375\n",
            "classify 2.21173095703125\n",
            "classify 2.3333740234375\n",
            "classify 2.220458984375\n",
            "classify 2.3187255859375\n",
            "classify 2.2337646484375\n",
            "classify 2.188232421875\n",
            "classify 2.32073974609375\n",
            "0.28125\n",
            "0.234375\n",
            "0.25\n",
            "0.34375\n",
            "strain 0.22533965110778809\n",
            "strain 0.21006809175014496\n",
            "strain 0.24012356996536255\n",
            "strain 0.23240415751934052\n",
            "strain 0.22711458802223206\n",
            "strain 0.24315617978572845\n",
            "strain 0.24179670214653015\n",
            "strain 0.2592286467552185\n",
            "strain 0.20095153152942657\n",
            "classify 2.3258056640625\n",
            "classify 2.28961181640625\n",
            "classify 2.3929443359375\n",
            "classify 2.305419921875\n",
            "classify 2.26495361328125\n",
            "classify 2.28582763671875\n",
            "classify 2.28118896484375\n",
            "classify 2.2806396484375\n",
            "classify 2.2261962890625\n",
            "0.296875\n",
            "0.21875\n",
            "0.296875\n",
            "0.25\n",
            "strain 0.21549715101718903\n",
            "strain 0.2216406613588333\n",
            "strain 0.2475782334804535\n",
            "strain 0.24219025671482086\n",
            "strain 0.22898095846176147\n",
            "strain 0.2093438357114792\n",
            "strain 0.2969244122505188\n",
            "strain 0.22751107811927795\n",
            "strain 0.2184196263551712\n",
            "classify 2.40106201171875\n",
            "classify 2.25897216796875\n",
            "classify 2.263671875\n",
            "classify 2.38836669921875\n",
            "classify 2.34442138671875\n",
            "classify 2.195068359375\n",
            "classify 2.268798828125\n",
            "classify 2.21484375\n",
            "classify 2.1527099609375\n",
            "0.28125\n",
            "0.203125\n",
            "0.28125\n",
            "0.296875\n",
            "strain 0.20862269401550293\n",
            "strain 0.24181881546974182\n",
            "strain 0.23558446764945984\n",
            "strain 0.24512605369091034\n",
            "strain 0.22204731404781342\n",
            "strain 0.2297752946615219\n",
            "strain 0.24089989066123962\n",
            "strain 0.21221056580543518\n",
            "strain 0.20210281014442444\n",
            "classify 2.13592529296875\n",
            "classify 2.35260009765625\n",
            "classify 2.33062744140625\n",
            "classify 2.24017333984375\n",
            "classify 2.165283203125\n",
            "classify 2.33978271484375\n",
            "classify 2.30859375\n",
            "classify 2.20159912109375\n",
            "classify 2.18829345703125\n",
            "0.21875\n",
            "0.296875\n",
            "0.328125\n",
            "0.40625\n",
            "strain 0.20987649261951447\n",
            "strain 0.2243131846189499\n",
            "strain 0.2326345443725586\n",
            "strain 0.2372705191373825\n",
            "strain 0.2394247204065323\n",
            "strain 0.21875262260437012\n",
            "strain 0.19676360487937927\n",
            "strain 0.24070288240909576\n",
            "strain 0.2434343844652176\n",
            "classify 2.14276123046875\n",
            "classify 2.41802978515625\n",
            "classify 2.32647705078125\n",
            "classify 2.2869873046875\n",
            "classify 2.27288818359375\n",
            "classify 2.267578125\n",
            "classify 2.08074951171875\n",
            "classify 2.100341796875\n",
            "classify 2.462890625\n",
            "0.3125\n",
            "0.25\n",
            "0.296875\n",
            "0.234375\n",
            "strain 0.22218291461467743\n",
            "strain 0.20941336452960968\n",
            "strain 0.23391523957252502\n",
            "strain 0.26517072319984436\n",
            "strain 0.2346794307231903\n",
            "strain 0.21414078772068024\n",
            "strain 0.2663390338420868\n",
            "strain 0.21279610693454742\n",
            "strain 0.2081853300333023\n",
            "classify 2.17510986328125\n",
            "classify 2.2625732421875\n",
            "classify 2.337646484375\n",
            "classify 2.19970703125\n",
            "classify 2.4140625\n",
            "classify 2.35943603515625\n",
            "classify 2.2779541015625\n",
            "classify 2.14959716796875\n",
            "classify 2.257080078125\n",
            "0.21875\n",
            "0.265625\n",
            "0.375\n",
            "0.21875\n",
            "strain 0.21406874060630798\n",
            "strain 0.25176602602005005\n",
            "strain 0.2121003121137619\n",
            "strain 0.24482983350753784\n",
            "strain 0.22255197167396545\n",
            "strain 0.24422171711921692\n",
            "strain 0.22175759077072144\n",
            "strain 0.22388745844364166\n",
            "strain 0.24164970219135284\n",
            "classify 2.23736572265625\n",
            "classify 2.25250244140625\n",
            "classify 2.2279052734375\n",
            "classify 2.185791015625\n",
            "classify 2.411376953125\n",
            "classify 2.4173583984375\n",
            "classify 2.14947509765625\n",
            "classify 2.1759033203125\n",
            "classify 2.1387939453125\n",
            "0.296875\n",
            "0.3125\n",
            "0.25\n",
            "0.3125\n",
            "strain 0.22273264825344086\n",
            "strain 0.2222367376089096\n",
            "strain 0.20497862994670868\n",
            "strain 0.2521091103553772\n",
            "strain 0.20780184864997864\n",
            "strain 0.219725102186203\n",
            "strain 0.24390599131584167\n",
            "strain 0.2602667510509491\n",
            "strain 0.22091317176818848\n",
            "classify 2.25421142578125\n",
            "classify 2.15966796875\n",
            "classify 2.342529296875\n",
            "classify 2.36688232421875\n",
            "classify 2.23956298828125\n",
            "classify 2.172119140625\n",
            "classify 2.26495361328125\n",
            "classify 2.15740966796875\n",
            "classify 2.431640625\n",
            "0.203125\n",
            "0.25\n",
            "0.265625\n",
            "0.265625\n",
            "strain 0.2313985526561737\n",
            "strain 0.22367754578590393\n",
            "strain 0.21483738720417023\n",
            "strain 0.24035681784152985\n",
            "strain 0.23949283361434937\n",
            "strain 0.22747959196567535\n",
            "strain 0.20373211801052094\n",
            "strain 0.19789664447307587\n",
            "strain 0.25600600242614746\n",
            "classify 2.25439453125\n",
            "classify 2.1632080078125\n",
            "classify 2.272216796875\n",
            "classify 2.19842529296875\n",
            "classify 2.16156005859375\n",
            "classify 2.3028564453125\n",
            "classify 2.27154541015625\n",
            "classify 2.404052734375\n",
            "classify 2.299560546875\n",
            "0.296875\n",
            "0.25\n",
            "0.21875\n",
            "0.3125\n",
            "strain 0.26225343346595764\n",
            "strain 0.2667679488658905\n",
            "strain 0.2438044399023056\n",
            "strain 0.22578848898410797\n",
            "strain 0.2264944463968277\n",
            "strain 0.21902602910995483\n",
            "strain 0.20075224339962006\n",
            "strain 0.23716188967227936\n",
            "strain 0.2468891739845276\n",
            "classify 2.17596435546875\n",
            "classify 2.13323974609375\n",
            "classify 2.40966796875\n",
            "classify 2.3873291015625\n",
            "classify 2.1439208984375\n",
            "classify 2.1640625\n",
            "classify 2.5040283203125\n",
            "classify 2.18670654296875\n",
            "classify 2.1282958984375\n",
            "0.34375\n",
            "0.21875\n",
            "0.296875\n",
            "0.3125\n",
            "strain 0.22002744674682617\n",
            "strain 0.23076634109020233\n",
            "strain 0.2250087410211563\n",
            "strain 0.1900119036436081\n",
            "strain 0.23027954995632172\n",
            "strain 0.2423458844423294\n",
            "strain 0.23376615345478058\n",
            "strain 0.20695585012435913\n",
            "strain 0.22402822971343994\n",
            "classify 2.39141845703125\n",
            "classify 2.31475830078125\n",
            "classify 2.27423095703125\n",
            "classify 2.16729736328125\n",
            "classify 2.49945068359375\n",
            "classify 2.21685791015625\n",
            "classify 2.25079345703125\n",
            "classify 2.12530517578125\n",
            "classify 2.1767578125\n",
            "0.359375\n",
            "0.265625\n",
            "0.203125\n",
            "0.3125\n",
            "strain 0.2409772276878357\n",
            "strain 0.20615659654140472\n",
            "strain 0.252851665019989\n",
            "strain 0.20936498045921326\n",
            "strain 0.24287039041519165\n",
            "strain 0.2151334583759308\n",
            "strain 0.23388127982616425\n",
            "strain 0.27501875162124634\n",
            "strain 0.2059948891401291\n",
            "classify 2.28857421875\n",
            "classify 2.0975341796875\n",
            "classify 2.37884521484375\n",
            "classify 2.359619140625\n",
            "classify 2.22515869140625\n",
            "classify 2.2664794921875\n",
            "classify 2.374267578125\n",
            "classify 2.23760986328125\n",
            "classify 2.31890869140625\n",
            "0.171875\n",
            "0.328125\n",
            "0.328125\n",
            "0.21875\n",
            "strain 0.25384604930877686\n",
            "strain 0.2727014720439911\n",
            "strain 0.21727001667022705\n",
            "strain 0.2105260044336319\n",
            "strain 0.219772070646286\n",
            "strain 0.24270087480545044\n",
            "strain 0.22129076719284058\n",
            "strain 0.23103907704353333\n",
            "strain 0.23890356719493866\n",
            "classify 2.12225341796875\n",
            "classify 2.36761474609375\n",
            "classify 2.3424072265625\n",
            "classify 2.33538818359375\n",
            "classify 2.53277587890625\n",
            "classify 2.0994873046875\n",
            "classify 2.27239990234375\n",
            "classify 2.16351318359375\n",
            "classify 2.2322998046875\n",
            "0.375\n",
            "0.296875\n",
            "0.28125\n",
            "0.15625\n",
            "strain 0.2108524590730667\n",
            "strain 0.22830761969089508\n",
            "strain 0.2490999549627304\n",
            "strain 0.20057976245880127\n",
            "strain 0.2111600935459137\n",
            "strain 0.21620994806289673\n",
            "strain 0.24466557800769806\n",
            "strain 0.2300558090209961\n",
            "strain 0.21748292446136475\n",
            "classify 2.39569091796875\n",
            "classify 2.19146728515625\n",
            "classify 2.22723388671875\n",
            "classify 2.24969482421875\n",
            "classify 2.3797607421875\n",
            "classify 2.23272705078125\n",
            "classify 2.209716796875\n",
            "classify 2.24676513671875\n",
            "classify 2.26446533203125\n",
            "0.375\n",
            "0.234375\n",
            "0.265625\n",
            "0.25\n",
            "strain 0.22539369761943817\n",
            "strain 0.2719534635543823\n",
            "strain 0.22697234153747559\n",
            "strain 0.22678792476654053\n",
            "strain 0.20313750207424164\n",
            "strain 0.23584048449993134\n",
            "strain 0.23723915219306946\n",
            "strain 0.21705663204193115\n",
            "strain 0.26987749338150024\n",
            "classify 2.51617431640625\n",
            "classify 2.319091796875\n",
            "classify 2.338134765625\n",
            "classify 2.1142578125\n",
            "classify 2.16693115234375\n",
            "classify 2.2161865234375\n",
            "classify 2.1265869140625\n",
            "classify 2.3758544921875\n",
            "classify 2.24755859375\n",
            "0.34375\n",
            "0.25\n",
            "0.296875\n",
            "0.234375\n",
            "strain 0.2465222179889679\n",
            "strain 0.21452786028385162\n",
            "strain 0.22140434384346008\n",
            "strain 0.2277580350637436\n",
            "strain 0.24681474268436432\n",
            "strain 0.27138906717300415\n",
            "strain 0.21140392124652863\n",
            "strain 0.2579352855682373\n",
            "strain 0.23007386922836304\n",
            "classify 2.297119140625\n",
            "classify 2.4166259765625\n",
            "classify 2.1773681640625\n",
            "classify 2.5360107421875\n",
            "classify 2.34130859375\n",
            "classify 2.11029052734375\n",
            "classify 2.1728515625\n",
            "classify 2.314697265625\n",
            "classify 2.18548583984375\n",
            "0.1875\n",
            "0.203125\n",
            "0.265625\n",
            "0.296875\n",
            "strain 0.23449978232383728\n",
            "strain 0.19942617416381836\n",
            "strain 0.22543714940547943\n",
            "strain 0.2956755459308624\n",
            "strain 0.20781993865966797\n",
            "strain 0.22491896152496338\n",
            "strain 0.21507549285888672\n",
            "strain 0.218270942568779\n",
            "strain 0.2684798538684845\n",
            "classify 2.191650390625\n",
            "classify 2.063720703125\n",
            "classify 2.07012939453125\n",
            "classify 2.09710693359375\n",
            "classify 2.271484375\n",
            "classify 2.4263916015625\n",
            "classify 2.33636474609375\n",
            "classify 2.289794921875\n",
            "classify 2.2906494140625\n",
            "0.296875\n",
            "0.34375\n",
            "0.265625\n",
            "0.265625\n",
            "strain 0.23454312980175018\n",
            "strain 0.22458992898464203\n",
            "strain 0.20819424092769623\n",
            "strain 0.24753263592720032\n",
            "strain 0.23565101623535156\n",
            "strain 0.22779366374015808\n",
            "strain 0.23299053311347961\n",
            "strain 0.21684376895427704\n",
            "strain 0.22064827382564545\n",
            "classify 2.28515625\n",
            "classify 2.29473876953125\n",
            "classify 2.42041015625\n",
            "classify 2.213134765625\n",
            "classify 2.1912841796875\n",
            "classify 2.26824951171875\n",
            "classify 2.07574462890625\n",
            "classify 2.240234375\n",
            "classify 2.2457275390625\n",
            "0.21875\n",
            "0.40625\n",
            "0.15625\n",
            "0.265625\n",
            "strain 0.23394498229026794\n",
            "strain 0.19499430060386658\n",
            "strain 0.24630022048950195\n",
            "strain 0.24744415283203125\n",
            "strain 0.2312811315059662\n",
            "strain 0.22587884962558746\n",
            "strain 0.22863446176052094\n",
            "strain 0.23532265424728394\n",
            "strain 0.22315841913223267\n",
            "classify 2.32818603515625\n",
            "classify 2.350830078125\n",
            "classify 2.144287109375\n",
            "classify 2.4932861328125\n",
            "classify 2.3638916015625\n",
            "classify 2.1739501953125\n",
            "classify 2.22509765625\n",
            "classify 2.11474609375\n",
            "classify 2.2520751953125\n",
            "0.28125\n",
            "0.3125\n",
            "0.234375\n",
            "0.25\n",
            "strain 0.2315990924835205\n",
            "strain 0.2243490070104599\n",
            "strain 0.23328833281993866\n",
            "strain 0.22417940199375153\n",
            "strain 0.21145182847976685\n",
            "strain 0.2513176500797272\n",
            "strain 0.21187379956245422\n",
            "strain 0.26572278141975403\n",
            "strain 0.2052849978208542\n",
            "classify 2.1513671875\n",
            "classify 2.24688720703125\n",
            "classify 2.1961669921875\n",
            "classify 2.296142578125\n",
            "classify 2.38970947265625\n",
            "classify 2.25286865234375\n",
            "classify 2.2132568359375\n",
            "classify 2.2684326171875\n",
            "classify 2.27471923828125\n",
            "0.296875\n",
            "0.328125\n",
            "0.234375\n",
            "0.28125\n",
            "strain 0.24960356950759888\n",
            "strain 0.248310849070549\n",
            "strain 0.2296694666147232\n",
            "strain 0.21082651615142822\n",
            "strain 0.2426806092262268\n",
            "strain 0.217998668551445\n",
            "strain 0.22586387395858765\n",
            "strain 0.21130813658237457\n",
            "strain 0.2240467071533203\n",
            "classify 2.2862548828125\n",
            "classify 2.22235107421875\n",
            "classify 2.3231201171875\n",
            "classify 2.39111328125\n",
            "classify 2.2044677734375\n",
            "classify 2.25567626953125\n",
            "classify 2.045166015625\n",
            "classify 2.1396484375\n",
            "classify 2.24676513671875\n",
            "0.265625\n",
            "0.3125\n",
            "0.171875\n",
            "0.265625\n",
            "strain 0.21048350632190704\n",
            "strain 0.23862220346927643\n",
            "strain 0.20599760115146637\n",
            "strain 0.24430175125598907\n",
            "strain 0.22373288869857788\n",
            "strain 0.2054513841867447\n",
            "strain 0.22942979633808136\n",
            "strain 0.2542540431022644\n",
            "strain 0.23464536666870117\n",
            "classify 2.5888671875\n",
            "classify 2.115478515625\n",
            "classify 2.3359375\n",
            "classify 2.260986328125\n",
            "classify 2.21014404296875\n",
            "classify 2.05377197265625\n",
            "classify 2.3072509765625\n",
            "classify 2.218017578125\n",
            "classify 2.35162353515625\n",
            "0.28125\n",
            "0.234375\n",
            "0.21875\n",
            "0.25\n",
            "strain 0.22620254755020142\n",
            "strain 0.23744633793830872\n",
            "strain 0.2352428436279297\n",
            "strain 0.21095030009746552\n",
            "strain 0.24806886911392212\n",
            "strain 0.2434641420841217\n",
            "strain 0.261324405670166\n",
            "strain 0.26790666580200195\n",
            "strain 0.2697977125644684\n",
            "classify 2.21966552734375\n",
            "classify 2.2557373046875\n",
            "classify 2.286376953125\n",
            "classify 2.16510009765625\n",
            "classify 2.2969970703125\n",
            "classify 2.15765380859375\n",
            "classify 2.2568359375\n",
            "classify 2.28350830078125\n",
            "classify 2.29937744140625\n",
            "0.3125\n",
            "0.25\n",
            "0.234375\n",
            "0.28125\n",
            "strain 0.23297077417373657\n",
            "strain 0.2225535809993744\n",
            "strain 0.23368453979492188\n",
            "strain 0.27136075496673584\n",
            "strain 0.2525327205657959\n",
            "strain 0.2501083314418793\n",
            "strain 0.21689288318157196\n",
            "strain 0.22410468757152557\n",
            "strain 0.2583930492401123\n",
            "classify 2.1649169921875\n",
            "classify 2.23651123046875\n",
            "classify 2.3018798828125\n",
            "classify 2.15509033203125\n",
            "classify 2.48052978515625\n",
            "classify 2.17877197265625\n",
            "classify 2.21978759765625\n",
            "classify 2.29400634765625\n",
            "classify 2.2447509765625\n",
            "0.296875\n",
            "0.328125\n",
            "0.28125\n",
            "0.25\n",
            "strain 0.21153342723846436\n",
            "strain 0.23134347796440125\n",
            "strain 0.2764776051044464\n",
            "strain 0.21075744926929474\n",
            "strain 0.25877079367637634\n",
            "strain 0.2410079389810562\n",
            "strain 0.2484208345413208\n",
            "strain 0.2327781766653061\n",
            "strain 0.2335556298494339\n",
            "classify 2.26373291015625\n",
            "classify 2.2742919921875\n",
            "classify 2.3199462890625\n",
            "classify 2.193359375\n",
            "classify 2.17742919921875\n",
            "classify 2.30841064453125\n",
            "classify 2.46051025390625\n",
            "classify 2.13531494140625\n",
            "classify 2.265380859375\n",
            "0.328125\n",
            "0.234375\n",
            "0.25\n",
            "0.25\n",
            "strain 0.23202356696128845\n",
            "strain 0.2634207308292389\n",
            "strain 0.21160125732421875\n",
            "strain 0.20819152891635895\n",
            "strain 0.24337203800678253\n",
            "strain 0.2314804196357727\n",
            "strain 0.2068079262971878\n",
            "strain 0.21604527533054352\n",
            "strain 0.21651840209960938\n",
            "classify 2.2796630859375\n",
            "classify 2.0130615234375\n",
            "classify 2.3597412109375\n",
            "classify 2.293701171875\n",
            "classify 2.23431396484375\n",
            "classify 2.20880126953125\n",
            "classify 2.3323974609375\n",
            "classify 2.253662109375\n",
            "classify 2.3045654296875\n",
            "0.25\n",
            "0.375\n",
            "0.296875\n",
            "0.234375\n",
            "strain 0.2253410518169403\n",
            "strain 0.23098565638065338\n",
            "strain 0.22433024644851685\n",
            "strain 0.20250728726387024\n",
            "strain 0.23818959295749664\n",
            "strain 0.2411191165447235\n",
            "strain 0.25573205947875977\n",
            "strain 0.21399767696857452\n",
            "strain 0.23376360535621643\n",
            "classify 2.31988525390625\n",
            "classify 2.25665283203125\n",
            "classify 2.19732666015625\n",
            "classify 2.2574462890625\n",
            "classify 2.270263671875\n",
            "classify 2.1920166015625\n",
            "classify 2.1749267578125\n",
            "classify 2.51025390625\n",
            "classify 2.2440185546875\n",
            "0.234375\n",
            "0.265625\n",
            "0.328125\n",
            "0.265625\n",
            "strain 0.21481746435165405\n",
            "strain 0.26056087017059326\n",
            "strain 0.23677481710910797\n",
            "strain 0.21475116908550262\n",
            "strain 0.23616452515125275\n",
            "strain 0.2402098923921585\n",
            "strain 0.22468683123588562\n",
            "strain 0.22180746495723724\n",
            "strain 0.2250330001115799\n",
            "classify 2.448974609375\n",
            "classify 2.1614990234375\n",
            "classify 2.1268310546875\n",
            "classify 2.2818603515625\n",
            "classify 2.19970703125\n",
            "classify 2.33465576171875\n",
            "classify 2.1539306640625\n",
            "classify 2.14605712890625\n",
            "classify 2.489990234375\n",
            "0.234375\n",
            "0.28125\n",
            "0.234375\n",
            "0.234375\n",
            "strain 0.24186396598815918\n",
            "strain 0.22510093450546265\n",
            "strain 0.25373491644859314\n",
            "strain 0.2526014745235443\n",
            "strain 0.2320897877216339\n",
            "strain 0.23333661258220673\n",
            "strain 0.21000181138515472\n",
            "strain 0.23973995447158813\n",
            "strain 0.20626823604106903\n",
            "classify 2.30224609375\n",
            "classify 2.3653564453125\n",
            "classify 2.2742919921875\n",
            "classify 2.200439453125\n",
            "classify 2.401123046875\n",
            "classify 2.20458984375\n",
            "classify 2.39227294921875\n",
            "classify 2.14208984375\n",
            "classify 2.15576171875\n",
            "0.25\n",
            "0.28125\n",
            "0.328125\n",
            "0.3125\n",
            "strain 0.2688461244106293\n",
            "strain 0.23843955993652344\n",
            "strain 0.2581419050693512\n",
            "strain 0.24514973163604736\n",
            "strain 0.21327154338359833\n",
            "strain 0.22806845605373383\n",
            "strain 0.20420756936073303\n",
            "strain 0.24941898882389069\n",
            "strain 0.20056378841400146\n",
            "classify 2.2476806640625\n",
            "classify 2.1925048828125\n",
            "classify 2.3494873046875\n",
            "classify 2.131591796875\n",
            "classify 2.18841552734375\n",
            "classify 2.305908203125\n",
            "classify 2.2578125\n",
            "classify 2.2769775390625\n",
            "classify 2.48626708984375\n",
            "0.234375\n",
            "0.1875\n",
            "0.34375\n",
            "0.40625\n",
            "strain 0.2558435797691345\n",
            "strain 0.21564507484436035\n",
            "strain 0.2266457974910736\n",
            "strain 0.27031210064888\n",
            "strain 0.24762582778930664\n",
            "strain 0.23672187328338623\n",
            "strain 0.21833181381225586\n",
            "strain 0.20115329325199127\n",
            "strain 0.20457686483860016\n",
            "classify 2.15216064453125\n",
            "classify 2.0301513671875\n",
            "classify 2.22271728515625\n",
            "classify 2.13214111328125\n",
            "classify 2.4622802734375\n",
            "classify 2.3487548828125\n",
            "classify 2.40716552734375\n",
            "classify 2.15673828125\n",
            "classify 2.328125\n",
            "0.328125\n",
            "0.234375\n",
            "0.296875\n",
            "0.234375\n",
            "strain 0.23179365694522858\n",
            "strain 0.196918785572052\n",
            "strain 0.2294992357492447\n",
            "strain 0.24094709753990173\n",
            "strain 0.2298365980386734\n",
            "strain 0.27405133843421936\n",
            "strain 0.22905562818050385\n",
            "strain 0.24487856030464172\n",
            "strain 0.2540733218193054\n",
            "classify 2.31341552734375\n",
            "classify 2.36932373046875\n",
            "classify 2.22283935546875\n",
            "classify 2.003173828125\n",
            "classify 2.239990234375\n",
            "classify 2.4114990234375\n",
            "classify 2.2109375\n",
            "classify 2.25140380859375\n",
            "classify 2.21478271484375\n",
            "0.265625\n",
            "0.234375\n",
            "0.375\n",
            "0.296875\n",
            "strain 0.2410760223865509\n",
            "strain 0.23297391831874847\n",
            "strain 0.21595236659049988\n",
            "strain 0.22771747410297394\n",
            "strain 0.26817941665649414\n",
            "strain 0.24432258307933807\n",
            "strain 0.24442920088768005\n",
            "strain 0.21469050645828247\n",
            "strain 0.21978187561035156\n",
            "classify 2.2589111328125\n",
            "classify 2.14886474609375\n",
            "classify 2.3338623046875\n",
            "classify 2.131103515625\n",
            "classify 2.20904541015625\n",
            "classify 2.38800048828125\n",
            "classify 2.2413330078125\n",
            "classify 2.2218017578125\n",
            "classify 2.1802978515625\n",
            "0.3125\n",
            "0.25\n",
            "0.359375\n",
            "0.28125\n",
            "strain 0.2347715049982071\n",
            "strain 0.28958073258399963\n",
            "strain 0.2616233825683594\n",
            "strain 0.23060233891010284\n",
            "strain 0.25883474946022034\n",
            "strain 0.2143925577402115\n",
            "strain 0.2514607608318329\n",
            "strain 0.21365807950496674\n",
            "strain 0.20446030795574188\n",
            "classify 2.3070068359375\n",
            "classify 2.14801025390625\n",
            "classify 2.2406005859375\n",
            "classify 2.22021484375\n",
            "classify 2.2298583984375\n",
            "classify 2.32135009765625\n",
            "classify 2.2734375\n",
            "classify 2.3282470703125\n",
            "classify 2.38446044921875\n",
            "0.296875\n",
            "0.296875\n",
            "0.328125\n",
            "0.1875\n",
            "strain 0.2255803346633911\n",
            "strain 0.2581554055213928\n",
            "strain 0.21174375712871552\n",
            "strain 0.23289664089679718\n",
            "strain 0.2197912037372589\n",
            "strain 0.27611610293388367\n",
            "strain 0.2541463077068329\n",
            "strain 0.20757126808166504\n",
            "strain 0.22446313500404358\n",
            "classify 2.40850830078125\n",
            "classify 2.15264892578125\n",
            "classify 2.34014892578125\n",
            "classify 2.291259765625\n",
            "classify 2.3818359375\n",
            "classify 2.21038818359375\n",
            "classify 2.3580322265625\n",
            "classify 2.008544921875\n",
            "classify 2.19732666015625\n",
            "0.203125\n",
            "0.3125\n",
            "0.203125\n",
            "0.234375\n",
            "strain 0.23290477693080902\n",
            "strain 0.2316177934408188\n",
            "strain 0.2138054072856903\n",
            "strain 0.2196008712053299\n",
            "strain 0.24384023249149323\n",
            "strain 0.2550000548362732\n",
            "strain 0.24041856825351715\n",
            "strain 0.21207578480243683\n",
            "strain 0.23019471764564514\n",
            "classify 2.39630126953125\n",
            "classify 2.349853515625\n",
            "classify 2.23968505859375\n",
            "classify 2.30224609375\n",
            "classify 2.31494140625\n",
            "classify 2.36785888671875\n",
            "classify 2.1365966796875\n",
            "classify 2.2525634765625\n",
            "classify 2.06378173828125\n",
            "0.359375\n",
            "0.234375\n",
            "0.296875\n",
            "0.28125\n",
            "strain 0.24947868287563324\n",
            "strain 0.21959972381591797\n",
            "strain 0.17952725291252136\n",
            "strain 0.22824421525001526\n",
            "strain 0.24241431057453156\n",
            "strain 0.21447370946407318\n",
            "strain 0.24624520540237427\n",
            "strain 0.2618037760257721\n",
            "strain 0.25408607721328735\n",
            "classify 2.40985107421875\n",
            "classify 2.19305419921875\n",
            "classify 2.38006591796875\n",
            "classify 2.3848876953125\n",
            "classify 2.21502685546875\n",
            "classify 2.1500244140625\n",
            "classify 2.26971435546875\n",
            "classify 2.14617919921875\n",
            "classify 2.3544921875\n",
            "0.3125\n",
            "0.25\n",
            "0.34375\n",
            "0.265625\n",
            "strain 0.2355758100748062\n",
            "strain 0.214917853474617\n",
            "strain 0.23747950792312622\n",
            "strain 0.24519464373588562\n",
            "strain 0.3043479919433594\n",
            "strain 0.22940993309020996\n",
            "strain 0.25372448563575745\n",
            "strain 0.2281014621257782\n",
            "strain 0.251528263092041\n",
            "classify 2.33062744140625\n",
            "classify 2.33624267578125\n",
            "classify 2.2265625\n",
            "classify 2.2684326171875\n",
            "classify 2.100830078125\n",
            "classify 2.33038330078125\n",
            "classify 2.2318115234375\n",
            "classify 2.10150146484375\n",
            "classify 2.51019287109375\n",
            "0.328125\n",
            "0.234375\n",
            "0.34375\n",
            "0.234375\n",
            "strain 0.21090678870677948\n",
            "strain 0.2288622111082077\n",
            "strain 0.26519525051116943\n",
            "strain 0.25101906061172485\n",
            "strain 0.20836509764194489\n",
            "strain 0.24895593523979187\n",
            "strain 0.2440672218799591\n",
            "strain 0.23592062294483185\n",
            "strain 0.22346638143062592\n",
            "classify 2.220947265625\n",
            "classify 2.30084228515625\n",
            "classify 2.18621826171875\n",
            "classify 2.381103515625\n",
            "classify 2.26123046875\n",
            "classify 2.193359375\n",
            "classify 2.32110595703125\n",
            "classify 2.2047119140625\n",
            "classify 2.06402587890625\n",
            "0.265625\n",
            "0.328125\n",
            "0.34375\n",
            "0.265625\n",
            "strain 0.24872975051403046\n",
            "strain 0.21858705580234528\n",
            "strain 0.2178739756345749\n",
            "strain 0.23906099796295166\n",
            "strain 0.19784319400787354\n",
            "strain 0.25145140290260315\n",
            "strain 0.20049263536930084\n",
            "strain 0.24428318440914154\n",
            "strain 0.27241241931915283\n",
            "classify 2.3941650390625\n",
            "classify 2.187255859375\n",
            "classify 2.134033203125\n",
            "classify 2.11505126953125\n",
            "classify 2.25390625\n",
            "classify 2.3619384765625\n",
            "classify 2.24749755859375\n",
            "classify 2.32177734375\n",
            "classify 2.17071533203125\n",
            "0.265625\n",
            "0.296875\n",
            "0.328125\n",
            "0.34375\n",
            "strain 0.2210046648979187\n",
            "strain 0.26150014996528625\n",
            "strain 0.18763799965381622\n",
            "strain 0.23832225799560547\n",
            "strain 0.25661489367485046\n",
            "strain 0.2555913031101227\n",
            "strain 0.24540668725967407\n",
            "strain 0.2238595336675644\n",
            "strain 0.2160043567419052\n",
            "classify 2.34600830078125\n",
            "classify 2.291015625\n",
            "classify 2.39111328125\n",
            "classify 2.39190673828125\n",
            "classify 2.2828369140625\n",
            "classify 2.1876220703125\n",
            "classify 2.14251708984375\n",
            "classify 2.20989990234375\n",
            "classify 2.21539306640625\n",
            "0.3125\n",
            "0.3125\n",
            "0.203125\n",
            "0.234375\n",
            "strain 0.22732342779636383\n",
            "strain 0.22874797880649567\n",
            "strain 0.2395932972431183\n",
            "strain 0.2283739149570465\n",
            "strain 0.23014624416828156\n",
            "strain 0.21826381981372833\n",
            "strain 0.21626806259155273\n",
            "strain 0.2360525280237198\n",
            "strain 0.2102218121290207\n",
            "classify 2.2135009765625\n",
            "classify 2.299560546875\n",
            "classify 2.33856201171875\n",
            "classify 2.1873779296875\n",
            "classify 2.2855224609375\n",
            "classify 2.2000732421875\n",
            "classify 2.2032470703125\n",
            "classify 2.39801025390625\n",
            "classify 2.32373046875\n",
            "0.234375\n",
            "0.25\n",
            "0.296875\n",
            "0.28125\n",
            "strain 0.22831879556179047\n",
            "strain 0.24465009570121765\n",
            "strain 0.21352243423461914\n",
            "strain 0.21443454921245575\n",
            "strain 0.23278531432151794\n",
            "strain 0.20840924978256226\n",
            "strain 0.23477457463741302\n",
            "strain 0.22322000563144684\n",
            "strain 0.2425108551979065\n",
            "classify 2.08209228515625\n",
            "classify 2.39141845703125\n",
            "classify 2.2681884765625\n",
            "classify 2.335205078125\n",
            "classify 2.4464111328125\n",
            "classify 2.38653564453125\n",
            "classify 2.1549072265625\n",
            "classify 2.375244140625\n",
            "classify 2.145751953125\n",
            "0.21875\n",
            "0.203125\n",
            "0.28125\n",
            "0.328125\n",
            "strain 0.28145408630371094\n",
            "strain 0.2200571894645691\n",
            "strain 0.20978471636772156\n",
            "strain 0.21938957273960114\n",
            "strain 0.22141088545322418\n",
            "strain 0.19454510509967804\n",
            "strain 0.281594455242157\n",
            "strain 0.24535192549228668\n",
            "strain 0.21301476657390594\n",
            "classify 2.13720703125\n",
            "classify 2.39202880859375\n",
            "classify 2.49871826171875\n",
            "classify 2.35430908203125\n",
            "classify 2.29144287109375\n",
            "classify 2.1923828125\n",
            "classify 2.07659912109375\n",
            "classify 2.3189697265625\n",
            "classify 2.13189697265625\n",
            "0.296875\n",
            "0.265625\n",
            "0.359375\n",
            "0.296875\n",
            "strain 0.22324548661708832\n",
            "strain 0.28235378861427307\n",
            "strain 0.22704362869262695\n",
            "strain 0.2326534390449524\n",
            "strain 0.22512491047382355\n",
            "strain 0.22773025929927826\n",
            "strain 0.24988169968128204\n",
            "strain 0.1984759122133255\n",
            "strain 0.25331684947013855\n",
            "classify 2.259521484375\n",
            "classify 2.27532958984375\n",
            "classify 2.493408203125\n",
            "classify 2.22625732421875\n",
            "classify 2.23687744140625\n",
            "classify 2.298583984375\n",
            "classify 2.314697265625\n",
            "classify 2.110107421875\n",
            "classify 2.158935546875\n",
            "0.296875\n",
            "0.21875\n",
            "0.25\n",
            "0.28125\n",
            "strain 0.21968108415603638\n",
            "strain 0.2128935307264328\n",
            "strain 0.20088805258274078\n",
            "strain 0.22596828639507294\n",
            "strain 0.24172894656658173\n",
            "strain 0.23182626068592072\n",
            "strain 0.2548282742500305\n",
            "strain 0.228977769613266\n",
            "strain 0.2243834137916565\n",
            "classify 2.35357666015625\n",
            "classify 2.1336669921875\n",
            "classify 2.1722412109375\n",
            "classify 2.38916015625\n",
            "classify 2.2705078125\n",
            "classify 2.1644287109375\n",
            "classify 2.2193603515625\n",
            "classify 2.29034423828125\n",
            "classify 2.2747802734375\n",
            "0.203125\n",
            "0.25\n",
            "0.265625\n",
            "0.34375\n",
            "strain 0.20586422085762024\n",
            "strain 0.21249698102474213\n",
            "strain 0.2259225994348526\n",
            "strain 0.23533548414707184\n",
            "strain 0.2334696352481842\n",
            "strain 0.23015698790550232\n",
            "strain 0.230406254529953\n",
            "strain 0.24317248165607452\n",
            "strain 0.24113835394382477\n",
            "classify 2.094970703125\n",
            "classify 2.30291748046875\n",
            "classify 2.4134521484375\n",
            "classify 2.25897216796875\n",
            "classify 2.2039794921875\n",
            "classify 2.14019775390625\n",
            "classify 2.2547607421875\n",
            "classify 2.327392578125\n",
            "classify 2.20843505859375\n",
            "0.3125\n",
            "0.390625\n",
            "0.234375\n",
            "0.203125\n",
            "strain 0.2419172078371048\n",
            "strain 0.2183380126953125\n",
            "strain 0.23319625854492188\n",
            "strain 0.19889159500598907\n",
            "strain 0.23585453629493713\n",
            "strain 0.21850301325321198\n",
            "strain 0.20980039238929749\n",
            "strain 0.24159418046474457\n",
            "strain 0.22984808683395386\n",
            "classify 2.3482666015625\n",
            "classify 2.2083740234375\n",
            "classify 2.46728515625\n",
            "classify 2.16845703125\n",
            "classify 2.2314453125\n",
            "classify 2.32208251953125\n",
            "classify 2.119384765625\n",
            "classify 2.242919921875\n",
            "classify 2.31488037109375\n",
            "0.265625\n",
            "0.203125\n",
            "0.421875\n",
            "0.265625\n",
            "strain 0.21191750466823578\n",
            "strain 0.2280336171388626\n",
            "strain 0.2150035798549652\n",
            "strain 0.22259670495986938\n",
            "strain 0.2540009915828705\n",
            "strain 0.21176491677761078\n",
            "strain 0.25355246663093567\n",
            "strain 0.22988566756248474\n",
            "strain 0.21738694608211517\n",
            "classify 2.21392822265625\n",
            "classify 2.2762451171875\n",
            "classify 2.26300048828125\n",
            "classify 2.13543701171875\n",
            "classify 2.1278076171875\n",
            "classify 2.34393310546875\n",
            "classify 2.4688720703125\n",
            "classify 2.331298828125\n",
            "classify 2.22564697265625\n",
            "0.25\n",
            "0.296875\n",
            "0.234375\n",
            "0.328125\n",
            "strain 0.21873989701271057\n",
            "strain 0.2182309776544571\n",
            "strain 0.24730919301509857\n",
            "strain 0.20876888930797577\n",
            "strain 0.20110547542572021\n",
            "strain 0.22142474353313446\n",
            "strain 0.21317178010940552\n",
            "strain 0.1907651424407959\n",
            "strain 0.2645759582519531\n",
            "classify 2.214111328125\n",
            "classify 2.30767822265625\n",
            "classify 2.2381591796875\n",
            "classify 2.0797119140625\n",
            "classify 2.19476318359375\n",
            "classify 2.28302001953125\n",
            "classify 2.21392822265625\n",
            "classify 2.1949462890625\n",
            "classify 2.52044677734375\n",
            "0.296875\n",
            "0.34375\n",
            "0.265625\n",
            "0.1875\n",
            "strain 0.23919081687927246\n",
            "strain 0.26617345213890076\n",
            "strain 0.23937875032424927\n",
            "strain 0.23103754222393036\n",
            "strain 0.24144358932971954\n",
            "strain 0.23611488938331604\n",
            "strain 0.24050821363925934\n",
            "strain 0.22321273386478424\n",
            "strain 0.21685999631881714\n",
            "classify 2.36572265625\n",
            "classify 2.2711181640625\n",
            "classify 2.11956787109375\n",
            "classify 2.239990234375\n",
            "classify 2.29156494140625\n",
            "classify 2.1397705078125\n",
            "classify 2.20697021484375\n",
            "classify 2.27655029296875\n",
            "classify 2.4495849609375\n",
            "0.3125\n",
            "0.3125\n",
            "0.3125\n",
            "0.265625\n",
            "strain 0.2242925465106964\n",
            "strain 0.21330559253692627\n",
            "strain 0.2369292825460434\n",
            "strain 0.27798908948898315\n",
            "strain 0.22643496096134186\n",
            "strain 0.21950794756412506\n",
            "strain 0.2253383845090866\n",
            "strain 0.21673305332660675\n",
            "strain 0.25257110595703125\n",
            "classify 2.401611328125\n",
            "classify 2.26251220703125\n",
            "classify 2.13946533203125\n",
            "classify 2.13946533203125\n",
            "classify 2.2645263671875\n",
            "classify 2.18084716796875\n",
            "classify 2.51904296875\n",
            "classify 2.14532470703125\n",
            "classify 2.2021484375\n",
            "0.234375\n",
            "0.3125\n",
            "0.328125\n",
            "0.265625\n",
            "strain 0.2602423131465912\n",
            "strain 0.20373818278312683\n",
            "strain 0.23727338016033173\n",
            "strain 0.2307382971048355\n",
            "strain 0.23508326709270477\n",
            "strain 0.24659200012683868\n",
            "strain 0.26011717319488525\n",
            "strain 0.2151382565498352\n",
            "strain 0.24493589997291565\n",
            "classify 2.20220947265625\n",
            "classify 2.3616943359375\n",
            "classify 2.2763671875\n",
            "classify 2.079833984375\n",
            "classify 2.515869140625\n",
            "classify 2.2686767578125\n",
            "classify 2.02239990234375\n",
            "classify 2.1248779296875\n",
            "classify 2.35552978515625\n",
            "0.28125\n",
            "0.28125\n",
            "0.265625\n",
            "0.328125\n",
            "strain 0.21342632174491882\n",
            "strain 0.22092010080814362\n",
            "strain 0.22464311122894287\n",
            "strain 0.25990012288093567\n",
            "strain 0.24687440693378448\n",
            "strain 0.2109038233757019\n",
            "strain 0.23800916969776154\n",
            "strain 0.24524983763694763\n",
            "strain 0.23561866581439972\n",
            "classify 2.1737060546875\n",
            "classify 2.3251953125\n",
            "classify 2.34259033203125\n",
            "classify 2.1712646484375\n",
            "classify 2.234619140625\n",
            "classify 2.2764892578125\n",
            "classify 2.41949462890625\n",
            "classify 2.3428955078125\n",
            "classify 2.29608154296875\n",
            "0.296875\n",
            "0.328125\n",
            "0.234375\n",
            "0.21875\n",
            "strain 0.25578877329826355\n",
            "strain 0.21931548416614532\n",
            "strain 0.23047266900539398\n",
            "strain 0.2125771939754486\n",
            "strain 0.21841301023960114\n",
            "strain 0.24680691957473755\n",
            "strain 0.21702653169631958\n",
            "strain 0.2294483184814453\n",
            "strain 0.25924330949783325\n",
            "classify 2.1234130859375\n",
            "classify 2.15435791015625\n",
            "classify 2.21490478515625\n",
            "classify 2.400146484375\n",
            "classify 2.21234130859375\n",
            "classify 2.23779296875\n",
            "classify 2.32415771484375\n",
            "classify 2.34979248046875\n",
            "classify 2.37835693359375\n",
            "0.328125\n",
            "0.28125\n",
            "0.171875\n",
            "0.25\n",
            "strain 0.21403267979621887\n",
            "strain 0.23439113795757294\n",
            "strain 0.24309386312961578\n",
            "strain 0.21914348006248474\n",
            "strain 0.22921815514564514\n",
            "strain 0.24646949768066406\n",
            "strain 0.22715669870376587\n",
            "strain 0.2374701201915741\n",
            "strain 0.22591370344161987\n",
            "classify 2.12518310546875\n",
            "classify 2.31097412109375\n",
            "classify 2.124755859375\n",
            "classify 2.27581787109375\n",
            "classify 2.3363037109375\n",
            "classify 2.3411865234375\n",
            "classify 2.40447998046875\n",
            "classify 2.2701416015625\n",
            "classify 2.16363525390625\n",
            "0.328125\n",
            "0.28125\n",
            "0.34375\n",
            "0.203125\n",
            "strain 0.2031007707118988\n",
            "strain 0.23661814630031586\n",
            "strain 0.2129916399717331\n",
            "strain 0.20495307445526123\n",
            "strain 0.25665974617004395\n",
            "strain 0.21836966276168823\n",
            "strain 0.22732846438884735\n",
            "strain 0.24190765619277954\n",
            "strain 0.22509928047657013\n",
            "classify 2.39141845703125\n",
            "classify 2.27435302734375\n",
            "classify 2.05279541015625\n",
            "classify 2.27301025390625\n",
            "classify 2.20306396484375\n",
            "classify 2.31341552734375\n",
            "classify 2.249755859375\n",
            "classify 2.32708740234375\n",
            "classify 2.32135009765625\n",
            "0.28125\n",
            "0.1875\n",
            "0.28125\n",
            "0.296875\n",
            "strain 0.24243934452533722\n",
            "strain 0.2338419407606125\n",
            "strain 0.23043231666088104\n",
            "strain 0.21989436447620392\n",
            "strain 0.22967860102653503\n",
            "strain 0.21056602895259857\n",
            "strain 0.23833048343658447\n",
            "strain 0.24218881130218506\n",
            "strain 0.2082771509885788\n",
            "classify 2.35052490234375\n",
            "classify 2.26641845703125\n",
            "classify 2.250244140625\n",
            "classify 2.1793212890625\n",
            "classify 2.42388916015625\n",
            "classify 2.2562255859375\n",
            "classify 2.14697265625\n",
            "classify 2.275390625\n",
            "classify 2.30804443359375\n",
            "0.328125\n",
            "0.296875\n",
            "0.328125\n",
            "0.21875\n",
            "strain 0.2221301645040512\n",
            "strain 0.211081400513649\n",
            "strain 0.2183668315410614\n",
            "strain 0.22948703169822693\n",
            "strain 0.2153278887271881\n",
            "strain 0.2715199887752533\n",
            "strain 0.28613150119781494\n",
            "strain 0.2215057611465454\n",
            "strain 0.25030040740966797\n",
            "classify 2.00970458984375\n",
            "classify 2.37841796875\n",
            "classify 2.27789306640625\n",
            "classify 2.2685546875\n",
            "classify 2.33392333984375\n",
            "classify 2.15704345703125\n",
            "classify 2.44903564453125\n",
            "classify 2.26641845703125\n",
            "classify 2.11102294921875\n",
            "0.265625\n",
            "0.296875\n",
            "0.296875\n",
            "0.25\n",
            "strain 0.26233601570129395\n",
            "strain 0.21953348815441132\n",
            "strain 0.22825616598129272\n",
            "strain 0.1991252303123474\n",
            "strain 0.21878628432750702\n",
            "strain 0.23139223456382751\n",
            "strain 0.25126028060913086\n",
            "strain 0.2212388962507248\n",
            "strain 0.24328652024269104\n",
            "classify 2.34173583984375\n",
            "classify 2.2420654296875\n",
            "classify 2.31243896484375\n",
            "classify 2.1776123046875\n",
            "classify 2.4603271484375\n",
            "classify 2.4119873046875\n",
            "classify 2.266357421875\n",
            "classify 2.12286376953125\n",
            "classify 2.1702880859375\n",
            "0.265625\n",
            "0.34375\n",
            "0.25\n",
            "0.21875\n",
            "strain 0.23971112072467804\n",
            "strain 0.24248722195625305\n",
            "strain 0.19489140808582306\n",
            "strain 0.24102114140987396\n",
            "strain 0.2812207341194153\n",
            "strain 0.2363184094429016\n",
            "strain 0.24743902683258057\n",
            "strain 0.20777632296085358\n",
            "strain 0.23473477363586426\n",
            "classify 2.26910400390625\n",
            "classify 2.3553466796875\n",
            "classify 2.19500732421875\n",
            "classify 2.2325439453125\n",
            "classify 2.211669921875\n",
            "classify 2.27197265625\n",
            "classify 2.48785400390625\n",
            "classify 2.1361083984375\n",
            "classify 2.27923583984375\n",
            "0.25\n",
            "0.203125\n",
            "0.28125\n",
            "0.34375\n",
            "strain 0.24227403104305267\n",
            "strain 0.23105978965759277\n",
            "strain 0.23345032334327698\n",
            "strain 0.24325844645500183\n",
            "strain 0.22783608734607697\n",
            "strain 0.2382577806711197\n",
            "strain 0.24557770788669586\n",
            "strain 0.2362937033176422\n",
            "strain 0.22823019325733185\n",
            "classify 2.1845703125\n",
            "classify 2.30133056640625\n",
            "classify 2.2103271484375\n",
            "classify 2.23382568359375\n",
            "classify 2.2066650390625\n",
            "classify 2.22149658203125\n",
            "classify 2.3126220703125\n",
            "classify 2.297607421875\n",
            "classify 2.3206787109375\n",
            "0.265625\n",
            "0.265625\n",
            "0.203125\n",
            "0.3125\n",
            "strain 0.22414346039295197\n",
            "strain 0.25815245509147644\n",
            "strain 0.19761504232883453\n",
            "strain 0.22411184012889862\n",
            "strain 0.25975289940834045\n",
            "strain 0.23539812862873077\n",
            "strain 0.22457996010780334\n",
            "strain 0.25534623861312866\n",
            "strain 0.2107347697019577\n",
            "classify 2.26141357421875\n",
            "classify 2.3043212890625\n",
            "classify 2.4549560546875\n",
            "classify 2.349365234375\n",
            "classify 2.1080322265625\n",
            "classify 2.294189453125\n",
            "classify 2.40673828125\n",
            "classify 2.213623046875\n",
            "classify 2.06585693359375\n",
            "0.296875\n",
            "0.265625\n",
            "0.265625\n",
            "0.28125\n",
            "strain 0.24178245663642883\n",
            "strain 0.24502837657928467\n",
            "strain 0.21812079846858978\n",
            "strain 0.20753417909145355\n",
            "strain 0.2527773976325989\n",
            "strain 0.21271781623363495\n",
            "strain 0.23913447558879852\n",
            "strain 0.23534797132015228\n",
            "strain 0.21665480732917786\n",
            "classify 2.1748046875\n",
            "classify 2.207275390625\n",
            "classify 2.1756591796875\n",
            "classify 2.5750732421875\n",
            "classify 2.2078857421875\n",
            "classify 2.39617919921875\n",
            "classify 2.2559814453125\n",
            "classify 2.19793701171875\n",
            "classify 2.27276611328125\n",
            "0.25\n",
            "0.25\n",
            "0.25\n",
            "0.359375\n",
            "strain 0.23194661736488342\n",
            "strain 0.19740650057792664\n",
            "strain 0.2521548271179199\n",
            "strain 0.2969211935997009\n",
            "strain 0.23188304901123047\n",
            "strain 0.19866712391376495\n",
            "strain 0.22442492842674255\n",
            "strain 0.22091098129749298\n",
            "strain 0.21605856716632843\n",
            "classify 2.19036865234375\n",
            "classify 2.15997314453125\n",
            "classify 2.3153076171875\n",
            "classify 2.25628662109375\n",
            "classify 2.1058349609375\n",
            "classify 2.322021484375\n",
            "classify 2.3726806640625\n",
            "classify 2.38946533203125\n",
            "classify 2.30218505859375\n",
            "0.296875\n",
            "0.25\n",
            "0.265625\n",
            "0.265625\n",
            "strain 0.22292813658714294\n",
            "strain 0.24041782319545746\n",
            "strain 0.2369309514760971\n",
            "strain 0.22828145325183868\n",
            "strain 0.2568472623825073\n",
            "strain 0.23955895006656647\n",
            "strain 0.20023173093795776\n",
            "strain 0.2277601808309555\n",
            "strain 0.21507608890533447\n",
            "classify 2.20745849609375\n",
            "classify 2.3599853515625\n",
            "classify 2.4268798828125\n",
            "classify 2.2666015625\n",
            "classify 2.21923828125\n",
            "classify 2.23468017578125\n",
            "classify 2.43328857421875\n",
            "classify 2.01751708984375\n",
            "classify 2.0452880859375\n",
            "0.203125\n",
            "0.296875\n",
            "0.25\n",
            "0.328125\n",
            "strain 0.2464640736579895\n",
            "strain 0.22444124519824982\n",
            "strain 0.20606179535388947\n",
            "strain 0.2791133522987366\n",
            "strain 0.21832291781902313\n",
            "strain 0.2320619821548462\n",
            "strain 0.20285429060459137\n",
            "strain 0.2367146760225296\n",
            "strain 0.22223754227161407\n",
            "classify 2.15765380859375\n",
            "classify 2.12689208984375\n",
            "classify 2.2294921875\n",
            "classify 2.27655029296875\n",
            "classify 2.33837890625\n",
            "classify 2.3748779296875\n",
            "classify 2.27642822265625\n",
            "classify 2.295654296875\n",
            "classify 2.249755859375\n",
            "0.265625\n",
            "0.296875\n",
            "0.328125\n",
            "0.265625\n",
            "strain 0.23649239540100098\n",
            "strain 0.23973704874515533\n",
            "strain 0.19749276340007782\n",
            "strain 0.20920613408088684\n",
            "strain 0.2352735549211502\n",
            "strain 0.2187822461128235\n",
            "strain 0.25416457653045654\n",
            "strain 0.24434179067611694\n",
            "strain 0.22535665333271027\n",
            "classify 2.27813720703125\n",
            "classify 2.36810302734375\n",
            "classify 2.2095947265625\n",
            "classify 2.255126953125\n",
            "classify 2.2437744140625\n",
            "classify 2.3138427734375\n",
            "classify 2.214111328125\n",
            "classify 2.2349853515625\n",
            "classify 2.300537109375\n",
            "0.25\n",
            "0.34375\n",
            "0.296875\n",
            "0.140625\n",
            "strain 0.23466582596302032\n",
            "strain 0.22542861104011536\n",
            "strain 0.20687158405780792\n",
            "strain 0.2602480351924896\n",
            "strain 0.21258404850959778\n",
            "strain 0.22617577016353607\n",
            "strain 0.25039413571357727\n",
            "strain 0.24670155346393585\n",
            "strain 0.2553965747356415\n",
            "classify 2.34112548828125\n",
            "classify 2.25653076171875\n",
            "classify 2.23095703125\n",
            "classify 2.22625732421875\n",
            "classify 2.27252197265625\n",
            "classify 2.2686767578125\n",
            "classify 2.27081298828125\n",
            "classify 2.20989990234375\n",
            "classify 2.24530029296875\n",
            "0.234375\n",
            "0.296875\n",
            "0.25\n",
            "0.28125\n",
            "strain 0.24188168346881866\n",
            "strain 0.21544566750526428\n",
            "strain 0.21810851991176605\n",
            "strain 0.2506951689720154\n",
            "strain 0.2414172887802124\n",
            "strain 0.2570074796676636\n",
            "strain 0.24306227266788483\n",
            "strain 0.21583375334739685\n",
            "strain 0.2371656596660614\n",
            "classify 2.2314453125\n",
            "classify 2.37353515625\n",
            "classify 2.12774658203125\n",
            "classify 2.29693603515625\n",
            "classify 2.4888916015625\n",
            "classify 2.23529052734375\n",
            "classify 2.342529296875\n",
            "classify 2.19036865234375\n",
            "classify 2.0882568359375\n",
            "0.296875\n",
            "0.203125\n",
            "0.296875\n",
            "0.3125\n",
            "strain 0.22246971726417542\n",
            "strain 0.22801721096038818\n",
            "strain 0.2223377227783203\n",
            "strain 0.2315407693386078\n",
            "strain 0.27021363377571106\n",
            "strain 0.2411128729581833\n",
            "strain 0.269481360912323\n",
            "strain 0.21442650258541107\n",
            "strain 0.19523034989833832\n",
            "classify 2.1671142578125\n",
            "classify 2.32781982421875\n",
            "classify 2.3778076171875\n",
            "classify 2.34716796875\n",
            "classify 2.17803955078125\n",
            "classify 2.16455078125\n",
            "classify 2.29107666015625\n",
            "classify 2.225341796875\n",
            "classify 2.4561767578125\n",
            "0.234375\n",
            "0.1875\n",
            "0.265625\n",
            "0.34375\n",
            "strain 0.21526843309402466\n",
            "strain 0.22959403693675995\n",
            "strain 0.2059488743543625\n",
            "strain 0.2612084746360779\n",
            "strain 0.22339414060115814\n",
            "strain 0.2413368970155716\n",
            "strain 0.24208226799964905\n",
            "strain 0.1909133642911911\n",
            "strain 0.25799840688705444\n",
            "classify 2.322998046875\n",
            "classify 2.28338623046875\n",
            "classify 2.43182373046875\n",
            "classify 2.330810546875\n",
            "classify 2.29815673828125\n",
            "classify 2.1669921875\n",
            "classify 2.22137451171875\n",
            "classify 2.09259033203125\n",
            "classify 2.1627197265625\n",
            "0.28125\n",
            "0.296875\n",
            "0.25\n",
            "0.296875\n",
            "strain 0.242136612534523\n",
            "strain 0.2470778226852417\n",
            "strain 0.21008062362670898\n",
            "strain 0.22303536534309387\n",
            "strain 0.20828434824943542\n",
            "strain 0.2601545453071594\n",
            "strain 0.21696476638317108\n",
            "strain 0.22285450994968414\n",
            "strain 0.256745845079422\n",
            "classify 2.20196533203125\n",
            "classify 2.15338134765625\n",
            "classify 2.39141845703125\n",
            "classify 2.354248046875\n",
            "classify 2.38720703125\n",
            "classify 2.32281494140625\n",
            "classify 2.161865234375\n",
            "classify 2.1778564453125\n",
            "classify 2.3369140625\n",
            "0.28125\n",
            "0.25\n",
            "0.203125\n",
            "0.390625\n",
            "strain 0.2187209129333496\n",
            "strain 0.2307123988866806\n",
            "strain 0.22936928272247314\n",
            "strain 0.23844872415065765\n",
            "strain 0.25807201862335205\n",
            "strain 0.22334519028663635\n",
            "strain 0.21189802885055542\n",
            "strain 0.24090290069580078\n",
            "strain 0.24130848050117493\n",
            "classify 2.3260498046875\n",
            "classify 2.32000732421875\n",
            "classify 2.150634765625\n",
            "classify 2.25140380859375\n",
            "classify 2.14453125\n",
            "classify 2.24932861328125\n",
            "classify 2.2833251953125\n",
            "classify 2.17437744140625\n",
            "classify 2.51898193359375\n",
            "0.28125\n",
            "0.3125\n",
            "0.25\n",
            "0.21875\n",
            "strain 0.27051660418510437\n",
            "strain 0.23182889819145203\n",
            "strain 0.2418082356452942\n",
            "strain 0.23164360225200653\n",
            "strain 0.19411002099514008\n",
            "strain 0.23677073419094086\n",
            "strain 0.23957479000091553\n",
            "strain 0.250234991312027\n",
            "strain 0.2201213240623474\n",
            "classify 2.37115478515625\n",
            "classify 2.2874755859375\n",
            "classify 2.4022216796875\n",
            "classify 2.24908447265625\n",
            "classify 2.11602783203125\n",
            "classify 2.3765869140625\n",
            "classify 2.31622314453125\n",
            "classify 2.33038330078125\n",
            "classify 2.23974609375\n",
            "0.328125\n",
            "0.359375\n",
            "0.25\n",
            "0.21875\n",
            "strain 0.22326479852199554\n",
            "strain 0.24316444993019104\n",
            "strain 0.27634426951408386\n",
            "strain 0.2260320782661438\n",
            "strain 0.22964368760585785\n",
            "strain 0.23639510571956635\n",
            "strain 0.23196284472942352\n",
            "strain 0.22289596498012543\n",
            "strain 0.24051178991794586\n",
            "classify 2.36395263671875\n",
            "classify 2.1697998046875\n",
            "classify 2.267333984375\n",
            "classify 2.28814697265625\n",
            "classify 2.240966796875\n",
            "classify 2.36236572265625\n",
            "classify 2.16729736328125\n",
            "classify 2.30560302734375\n",
            "classify 2.3135986328125\n",
            "0.171875\n",
            "0.265625\n",
            "0.328125\n",
            "0.28125\n",
            "strain 0.2141309380531311\n",
            "strain 0.2341431975364685\n",
            "strain 0.23762591183185577\n",
            "strain 0.21921849250793457\n",
            "strain 0.25957900285720825\n",
            "strain 0.22124233841896057\n",
            "strain 0.27588459849357605\n",
            "strain 0.23344026505947113\n",
            "strain 0.2213071584701538\n",
            "classify 2.21173095703125\n",
            "classify 2.3670654296875\n",
            "classify 2.2283935546875\n",
            "classify 2.2774658203125\n",
            "classify 2.393798828125\n",
            "classify 2.24432373046875\n",
            "classify 2.13763427734375\n",
            "classify 2.212890625\n",
            "classify 2.1993408203125\n",
            "0.1875\n",
            "0.328125\n",
            "0.25\n",
            "0.34375\n",
            "strain 0.19432146847248077\n",
            "strain 0.264471173286438\n",
            "strain 0.2062210738658905\n",
            "strain 0.22342848777770996\n",
            "strain 0.2290855199098587\n",
            "strain 0.24308790266513824\n",
            "strain 0.2232341319322586\n",
            "strain 0.21763770282268524\n",
            "strain 0.23833602666854858\n",
            "classify 2.32928466796875\n",
            "classify 2.3748779296875\n",
            "classify 2.34979248046875\n",
            "classify 2.257568359375\n",
            "classify 2.18212890625\n",
            "classify 2.08428955078125\n",
            "classify 2.1993408203125\n",
            "classify 2.14556884765625\n",
            "classify 2.2530517578125\n",
            "0.21875\n",
            "0.28125\n",
            "0.328125\n",
            "0.265625\n",
            "strain 0.21109126508235931\n",
            "strain 0.25084781646728516\n",
            "strain 0.19592788815498352\n",
            "strain 0.21522310376167297\n",
            "strain 0.231955885887146\n",
            "strain 0.23627308011054993\n",
            "strain 0.22249920666217804\n",
            "strain 0.2281331866979599\n",
            "strain 0.19609983265399933\n",
            "classify 2.21893310546875\n",
            "classify 2.0772705078125\n",
            "classify 2.1138916015625\n",
            "classify 2.270751953125\n",
            "classify 2.46270751953125\n",
            "classify 2.21160888671875\n",
            "classify 2.433349609375\n",
            "classify 2.095947265625\n",
            "classify 2.19659423828125\n",
            "0.25\n",
            "0.265625\n",
            "0.328125\n",
            "0.296875\n",
            "strain 0.24322117865085602\n",
            "strain 0.22314317524433136\n",
            "strain 0.19800393283367157\n",
            "strain 0.22702841460704803\n",
            "strain 0.19747807085514069\n",
            "strain 0.22953170537948608\n",
            "strain 0.2524004578590393\n",
            "strain 0.20384353399276733\n",
            "strain 0.2272086888551712\n",
            "classify 2.37158203125\n",
            "classify 2.26605224609375\n",
            "classify 2.25\n",
            "classify 2.24822998046875\n",
            "classify 2.234619140625\n",
            "classify 2.123779296875\n",
            "classify 2.4158935546875\n",
            "classify 2.24737548828125\n",
            "classify 2.2581787109375\n",
            "0.3125\n",
            "0.21875\n",
            "0.265625\n",
            "0.265625\n",
            "strain 0.2387990802526474\n",
            "strain 0.22122329473495483\n",
            "strain 0.21254229545593262\n",
            "strain 0.2475215196609497\n",
            "strain 0.24990805983543396\n",
            "strain 0.2356875240802765\n",
            "strain 0.209207683801651\n",
            "strain 0.2282285988330841\n",
            "strain 0.24186286330223083\n",
            "classify 2.02606201171875\n",
            "classify 2.2291259765625\n",
            "classify 2.28460693359375\n",
            "classify 2.18634033203125\n",
            "classify 2.26068115234375\n",
            "classify 2.2255859375\n",
            "classify 2.3699951171875\n",
            "classify 2.2484130859375\n",
            "classify 2.33038330078125\n",
            "0.453125\n",
            "0.1875\n",
            "0.25\n",
            "0.234375\n",
            "strain 0.24248936772346497\n",
            "strain 0.22342132031917572\n",
            "strain 0.24932095408439636\n",
            "strain 0.24153722822666168\n",
            "strain 0.23473407328128815\n",
            "strain 0.23401649296283722\n",
            "strain 0.23298200964927673\n",
            "strain 0.2067321240901947\n",
            "strain 0.24516019225120544\n",
            "classify 2.3746337890625\n",
            "classify 2.24456787109375\n",
            "classify 2.2440185546875\n",
            "classify 2.086669921875\n",
            "classify 2.34539794921875\n",
            "classify 2.23345947265625\n",
            "classify 2.29010009765625\n",
            "classify 2.2327880859375\n",
            "classify 2.174560546875\n",
            "0.21875\n",
            "0.375\n",
            "0.265625\n",
            "0.234375\n",
            "strain 0.23179295659065247\n",
            "strain 0.23469440639019012\n",
            "strain 0.25192299485206604\n",
            "strain 0.20831963419914246\n",
            "strain 0.27835550904273987\n",
            "strain 0.21146203577518463\n",
            "strain 0.25922274589538574\n",
            "strain 0.24321219325065613\n",
            "strain 0.1992470622062683\n",
            "classify 2.16357421875\n",
            "classify 2.3294677734375\n",
            "classify 2.233642578125\n",
            "classify 2.28607177734375\n",
            "classify 2.28955078125\n",
            "classify 2.1341552734375\n",
            "classify 2.28057861328125\n",
            "classify 2.2979736328125\n",
            "classify 2.25250244140625\n",
            "0.3125\n",
            "0.15625\n",
            "0.328125\n",
            "0.3125\n",
            "strain 0.21961751580238342\n",
            "strain 0.246408149600029\n",
            "strain 0.23334604501724243\n",
            "strain 0.21222692728042603\n",
            "strain 0.22423475980758667\n",
            "strain 0.23326276242733002\n",
            "strain 0.2231268286705017\n",
            "strain 0.25054487586021423\n",
            "strain 0.2082708328962326\n",
            "classify 2.2353515625\n",
            "classify 2.3515625\n",
            "classify 2.21343994140625\n",
            "classify 2.2850341796875\n",
            "classify 2.28863525390625\n",
            "classify 2.2611083984375\n",
            "classify 2.168212890625\n",
            "classify 2.29510498046875\n",
            "classify 2.232666015625\n",
            "0.25\n",
            "0.359375\n",
            "0.28125\n",
            "0.203125\n",
            "strain 0.21384096145629883\n",
            "strain 0.23747026920318604\n",
            "strain 0.21971143782138824\n",
            "strain 0.2548656463623047\n",
            "strain 0.21498726308345795\n",
            "strain 0.2131136953830719\n",
            "strain 0.25748616456985474\n",
            "strain 0.22694817185401917\n",
            "strain 0.24368630349636078\n",
            "classify 2.3634033203125\n",
            "classify 2.24029541015625\n",
            "classify 2.06988525390625\n",
            "classify 2.3223876953125\n",
            "classify 2.38372802734375\n",
            "classify 2.35845947265625\n",
            "classify 2.1844482421875\n",
            "classify 2.1815185546875\n",
            "classify 2.1531982421875\n",
            "0.28125\n",
            "0.28125\n",
            "0.234375\n",
            "0.3125\n",
            "strain 0.2366446554660797\n",
            "strain 0.23692288994789124\n",
            "strain 0.25480324029922485\n",
            "strain 0.2368946075439453\n",
            "strain 0.228922501206398\n",
            "strain 0.22455786168575287\n",
            "strain 0.21605223417282104\n",
            "strain 0.2024313509464264\n",
            "strain 0.22282570600509644\n",
            "classify 2.14886474609375\n",
            "classify 2.133056640625\n",
            "classify 2.312255859375\n",
            "classify 2.2796630859375\n",
            "classify 2.26666259765625\n",
            "classify 2.25506591796875\n",
            "classify 2.19354248046875\n",
            "classify 2.45062255859375\n",
            "classify 2.36956787109375\n",
            "0.28125\n",
            "0.296875\n",
            "0.203125\n",
            "0.359375\n",
            "strain 0.21825173497200012\n",
            "strain 0.2200762778520584\n",
            "strain 0.22464729845523834\n",
            "strain 0.23721811175346375\n",
            "strain 0.22710014879703522\n",
            "strain 0.228760227560997\n",
            "strain 0.23968501389026642\n",
            "strain 0.20333363115787506\n",
            "strain 0.2506788372993469\n",
            "classify 2.1693115234375\n",
            "classify 2.32281494140625\n",
            "classify 2.108154296875\n",
            "classify 2.25\n",
            "classify 2.614501953125\n",
            "classify 2.12744140625\n",
            "classify 2.3387451171875\n",
            "classify 2.29083251953125\n",
            "classify 2.27764892578125\n",
            "0.296875\n",
            "0.28125\n",
            "0.265625\n",
            "0.25\n",
            "strain 0.2468774914741516\n",
            "strain 0.23384101688861847\n",
            "strain 0.22153539955615997\n",
            "strain 0.24452432990074158\n",
            "strain 0.2248830646276474\n",
            "strain 0.20148241519927979\n",
            "strain 0.21043360233306885\n",
            "strain 0.2290438562631607\n",
            "strain 0.21667329967021942\n",
            "classify 2.31396484375\n",
            "classify 2.093505859375\n",
            "classify 2.29443359375\n",
            "classify 2.38739013671875\n",
            "classify 2.251953125\n",
            "classify 2.24462890625\n",
            "classify 2.3812255859375\n",
            "classify 2.10369873046875\n",
            "classify 2.3809814453125\n",
            "0.3125\n",
            "0.28125\n",
            "0.25\n",
            "0.25\n",
            "strain 0.26288264989852905\n",
            "strain 0.21502980589866638\n",
            "strain 0.23321311175823212\n",
            "strain 0.2244977355003357\n",
            "strain 0.22401049733161926\n",
            "strain 0.2279086410999298\n",
            "strain 0.19660259783267975\n",
            "strain 0.2216312438249588\n",
            "strain 0.2577876150608063\n",
            "classify 2.0699462890625\n",
            "classify 2.2205810546875\n",
            "classify 2.33795166015625\n",
            "classify 2.45208740234375\n",
            "classify 2.15240478515625\n",
            "classify 2.16265869140625\n",
            "classify 2.21923828125\n",
            "classify 2.3765869140625\n",
            "classify 2.19879150390625\n",
            "0.296875\n",
            "0.265625\n",
            "0.265625\n",
            "0.296875\n",
            "strain 0.2022438496351242\n",
            "strain 0.2578434646129608\n",
            "strain 0.2161269187927246\n",
            "strain 0.23820148408412933\n",
            "strain 0.21744826436042786\n",
            "strain 0.21914182603359222\n",
            "strain 0.24612990021705627\n",
            "strain 0.21737423539161682\n",
            "strain 0.22396320104599\n",
            "classify 2.355712890625\n",
            "classify 2.46527099609375\n",
            "classify 2.2645263671875\n",
            "classify 2.28857421875\n",
            "classify 2.213623046875\n",
            "classify 2.06463623046875\n",
            "classify 2.0831298828125\n",
            "classify 2.212890625\n",
            "classify 2.11383056640625\n",
            "0.25\n",
            "0.328125\n",
            "0.359375\n",
            "0.1875\n",
            "strain 0.2234911024570465\n",
            "strain 0.24180324375629425\n",
            "strain 0.21771959960460663\n",
            "strain 0.2224237620830536\n",
            "strain 0.20994509756565094\n",
            "strain 0.2273068130016327\n",
            "strain 0.2208641618490219\n",
            "strain 0.26138320565223694\n",
            "strain 0.2006436139345169\n",
            "classify 2.324462890625\n",
            "classify 2.38970947265625\n",
            "classify 2.370361328125\n",
            "classify 2.38287353515625\n",
            "classify 2.150390625\n",
            "classify 2.2568359375\n",
            "classify 2.14276123046875\n",
            "classify 2.34075927734375\n",
            "classify 2.1317138671875\n",
            "0.265625\n",
            "0.234375\n",
            "0.265625\n",
            "0.34375\n",
            "strain 0.24244707822799683\n",
            "strain 0.21473149955272675\n",
            "strain 0.23737144470214844\n",
            "strain 0.2521768808364868\n",
            "strain 0.26332640647888184\n",
            "strain 0.23129914700984955\n",
            "strain 0.21200470626354218\n",
            "strain 0.24158793687820435\n",
            "strain 0.25444459915161133\n",
            "classify 2.173583984375\n",
            "classify 2.07806396484375\n",
            "classify 2.45794677734375\n",
            "classify 2.37322998046875\n",
            "classify 2.116943359375\n",
            "classify 2.10687255859375\n",
            "classify 2.368408203125\n",
            "classify 2.2435302734375\n",
            "classify 2.3228759765625\n",
            "0.21875\n",
            "0.25\n",
            "0.375\n",
            "0.328125\n",
            "strain 0.2047881931066513\n",
            "strain 0.25830313563346863\n",
            "strain 0.25352391600608826\n",
            "strain 0.23299510776996613\n",
            "strain 0.20640723407268524\n",
            "strain 0.2643216550350189\n",
            "strain 0.23214048147201538\n",
            "strain 0.2587432861328125\n",
            "strain 0.21346673369407654\n",
            "classify 2.2713623046875\n",
            "classify 2.22991943359375\n",
            "classify 2.26019287109375\n",
            "classify 2.314697265625\n",
            "classify 2.22161865234375\n",
            "classify 2.24432373046875\n",
            "classify 2.3359375\n",
            "classify 2.32220458984375\n",
            "classify 2.2518310546875\n",
            "0.375\n",
            "0.28125\n",
            "0.28125\n",
            "0.21875\n",
            "strain 0.28270918130874634\n",
            "strain 0.22866103053092957\n",
            "strain 0.23709231615066528\n",
            "strain 0.20539510250091553\n",
            "strain 0.22340458631515503\n",
            "strain 0.22494183480739594\n",
            "strain 0.2335229367017746\n",
            "strain 0.22672110795974731\n",
            "strain 0.2632582187652588\n",
            "classify 2.185302734375\n",
            "classify 2.14324951171875\n",
            "classify 2.336669921875\n",
            "classify 2.1964111328125\n",
            "classify 2.261962890625\n",
            "classify 2.2386474609375\n",
            "classify 2.16943359375\n",
            "classify 2.26593017578125\n",
            "classify 2.158935546875\n",
            "0.25\n",
            "0.3125\n",
            "0.296875\n",
            "0.359375\n",
            "strain 0.21344290673732758\n",
            "strain 0.22663944959640503\n",
            "strain 0.2235388159751892\n",
            "strain 0.2230926752090454\n",
            "strain 0.21971265971660614\n",
            "strain 0.2315273880958557\n",
            "strain 0.23145154118537903\n",
            "strain 0.22174353897571564\n",
            "strain 0.21245445311069489\n",
            "classify 2.31353759765625\n",
            "classify 2.47601318359375\n",
            "classify 2.19537353515625\n",
            "classify 2.4017333984375\n",
            "classify 2.1551513671875\n",
            "classify 2.1900634765625\n",
            "classify 2.2254638671875\n",
            "classify 2.27679443359375\n",
            "classify 2.14324951171875\n",
            "0.28125\n",
            "0.328125\n",
            "0.3125\n",
            "0.25\n",
            "strain 0.2399611473083496\n",
            "strain 0.21968822181224823\n",
            "strain 0.21127565205097198\n",
            "strain 0.22400836646556854\n",
            "strain 0.23019562661647797\n",
            "strain 0.2208477258682251\n",
            "strain 0.22066141664981842\n",
            "strain 0.22688324749469757\n",
            "strain 0.2552679181098938\n",
            "classify 2.24407958984375\n",
            "classify 2.1990966796875\n",
            "classify 2.15704345703125\n",
            "classify 2.189697265625\n",
            "classify 2.2451171875\n",
            "classify 2.15338134765625\n",
            "classify 2.4805908203125\n",
            "classify 2.45245361328125\n",
            "classify 2.20489501953125\n",
            "0.21875\n",
            "0.328125\n",
            "0.234375\n",
            "0.296875\n",
            "strain 0.22236265242099762\n",
            "strain 0.21608591079711914\n",
            "strain 0.2194094955921173\n",
            "strain 0.2652904689311981\n",
            "strain 0.20184455811977386\n",
            "strain 0.24375401437282562\n",
            "strain 0.26685187220573425\n",
            "strain 0.22636529803276062\n",
            "strain 0.22472529113292694\n",
            "classify 2.23321533203125\n",
            "classify 2.3111572265625\n",
            "classify 2.27734375\n",
            "classify 2.328857421875\n",
            "classify 2.3330078125\n",
            "classify 2.3729248046875\n",
            "classify 2.05584716796875\n",
            "classify 2.11688232421875\n",
            "classify 2.2186279296875\n",
            "0.25\n",
            "0.25\n",
            "0.296875\n",
            "0.375\n",
            "strain 0.2556736469268799\n",
            "strain 0.2256658524274826\n",
            "strain 0.21556870639324188\n",
            "strain 0.23529455065727234\n",
            "strain 0.238095223903656\n",
            "strain 0.26254940032958984\n",
            "strain 0.21790829300880432\n",
            "strain 0.24446988105773926\n",
            "strain 0.2122548371553421\n",
            "classify 2.24176025390625\n",
            "classify 2.314208984375\n",
            "classify 2.30853271484375\n",
            "classify 2.40911865234375\n",
            "classify 2.29315185546875\n",
            "classify 2.21728515625\n",
            "classify 2.36932373046875\n",
            "classify 2.20941162109375\n",
            "classify 2.12493896484375\n",
            "0.296875\n",
            "0.359375\n",
            "0.15625\n",
            "0.328125\n",
            "strain 0.22005906701087952\n",
            "strain 0.2350512146949768\n",
            "strain 0.22742319107055664\n",
            "strain 0.2653558552265167\n",
            "strain 0.2251729965209961\n",
            "strain 0.22193598747253418\n",
            "strain 0.2122069150209427\n",
            "strain 0.23115047812461853\n",
            "strain 0.20642106235027313\n",
            "classify 2.13525390625\n",
            "classify 2.31109619140625\n",
            "classify 2.27203369140625\n",
            "classify 2.39483642578125\n",
            "classify 2.1297607421875\n",
            "classify 2.2713623046875\n",
            "classify 2.4442138671875\n",
            "classify 2.16845703125\n",
            "classify 2.27008056640625\n",
            "0.265625\n",
            "0.28125\n",
            "0.328125\n",
            "0.234375\n",
            "strain 0.24542704224586487\n",
            "strain 0.19868044555187225\n",
            "strain 0.20567139983177185\n",
            "strain 0.26430177688598633\n",
            "strain 0.2634212076663971\n",
            "strain 0.2319343388080597\n",
            "strain 0.23638398945331573\n",
            "strain 0.23691117763519287\n",
            "strain 0.2097880095243454\n",
            "classify 2.3597412109375\n",
            "classify 2.1026611328125\n",
            "classify 2.16461181640625\n",
            "classify 2.2137451171875\n",
            "classify 2.2161865234375\n",
            "classify 2.44122314453125\n",
            "classify 2.3277587890625\n",
            "classify 2.096923828125\n",
            "classify 2.3397216796875\n",
            "0.265625\n",
            "0.28125\n",
            "0.3125\n",
            "0.265625\n",
            "strain 0.2260584831237793\n",
            "strain 0.25459524989128113\n",
            "strain 0.2211461216211319\n",
            "strain 0.2190946787595749\n",
            "strain 0.22625702619552612\n",
            "strain 0.2684459388256073\n",
            "strain 0.22295014560222626\n",
            "strain 0.2224649339914322\n",
            "strain 0.25956541299819946\n",
            "classify 2.1495361328125\n",
            "classify 2.31396484375\n",
            "classify 2.14630126953125\n",
            "classify 2.16326904296875\n",
            "classify 2.2418212890625\n",
            "classify 2.4075927734375\n",
            "classify 2.5382080078125\n",
            "classify 2.24322509765625\n",
            "classify 2.29443359375\n",
            "0.203125\n",
            "0.1875\n",
            "0.234375\n",
            "0.390625\n",
            "strain 0.21112662553787231\n",
            "strain 0.27642473578453064\n",
            "strain 0.25892773270606995\n",
            "strain 0.24713672697544098\n",
            "strain 0.2190481275320053\n",
            "strain 0.1990615576505661\n",
            "strain 0.20926085114479065\n",
            "strain 0.2263294756412506\n",
            "strain 0.2123783528804779\n",
            "classify 2.26068115234375\n",
            "classify 2.29058837890625\n",
            "classify 2.2388916015625\n",
            "classify 2.3857421875\n",
            "classify 2.2332763671875\n",
            "classify 2.27056884765625\n",
            "classify 2.397216796875\n",
            "classify 2.23944091796875\n",
            "classify 2.07611083984375\n",
            "0.265625\n",
            "0.234375\n",
            "0.28125\n",
            "0.296875\n",
            "strain 0.21624314785003662\n",
            "strain 0.21782997250556946\n",
            "strain 0.2274412214756012\n",
            "strain 0.21327939629554749\n",
            "strain 0.22113192081451416\n",
            "strain 0.2141449749469757\n",
            "strain 0.30301082134246826\n",
            "strain 0.20562340319156647\n",
            "strain 0.21773242950439453\n",
            "classify 2.29962158203125\n",
            "classify 2.1396484375\n",
            "classify 2.26788330078125\n",
            "classify 2.3446044921875\n",
            "classify 2.1744384765625\n",
            "classify 2.2843017578125\n",
            "classify 2.20465087890625\n",
            "classify 2.22259521484375\n",
            "classify 2.24298095703125\n",
            "0.40625\n",
            "0.15625\n",
            "0.25\n",
            "0.28125\n",
            "strain 0.2448328733444214\n",
            "strain 0.22419066727161407\n",
            "strain 0.2089720219373703\n",
            "strain 0.21775580942630768\n",
            "strain 0.2253856211900711\n",
            "strain 0.2390432208776474\n",
            "strain 0.20881204307079315\n",
            "strain 0.214331716299057\n",
            "strain 0.21090927720069885\n",
            "classify 2.2425537109375\n",
            "classify 2.3096923828125\n",
            "classify 2.217041015625\n",
            "classify 2.3426513671875\n",
            "classify 2.07611083984375\n",
            "classify 2.15655517578125\n",
            "classify 2.2723388671875\n",
            "classify 2.31170654296875\n",
            "classify 2.2891845703125\n",
            "0.265625\n",
            "0.328125\n",
            "0.234375\n",
            "0.28125\n",
            "strain 0.2218194603919983\n",
            "strain 0.22566905617713928\n",
            "strain 0.24589556455612183\n",
            "strain 0.20841975510120392\n",
            "strain 0.2057245969772339\n",
            "strain 0.2181558758020401\n",
            "strain 0.22004275023937225\n",
            "strain 0.19180145859718323\n",
            "strain 0.21055950224399567\n",
            "classify 2.2862548828125\n",
            "classify 2.25836181640625\n",
            "classify 2.291259765625\n",
            "classify 2.1357421875\n",
            "classify 2.2552490234375\n",
            "classify 2.28369140625\n",
            "classify 2.334716796875\n",
            "classify 2.4781494140625\n",
            "classify 2.315673828125\n",
            "0.25\n",
            "0.25\n",
            "0.28125\n",
            "0.296875\n",
            "strain 0.239584818482399\n",
            "strain 0.21427330374717712\n",
            "strain 0.2112424224615097\n",
            "strain 0.2129630148410797\n",
            "strain 0.19927631318569183\n",
            "strain 0.23102185130119324\n",
            "strain 0.22521546483039856\n",
            "strain 0.2804310619831085\n",
            "strain 0.25067639350891113\n",
            "classify 2.32861328125\n",
            "classify 2.27691650390625\n",
            "classify 2.287353515625\n",
            "classify 2.1781005859375\n",
            "classify 2.48858642578125\n",
            "classify 2.14990234375\n",
            "classify 2.24530029296875\n",
            "classify 2.27789306640625\n",
            "classify 2.2835693359375\n",
            "0.21875\n",
            "0.21875\n",
            "0.3125\n",
            "0.3125\n",
            "strain 0.2425175905227661\n",
            "strain 0.2294469028711319\n",
            "strain 0.24620500206947327\n",
            "strain 0.21634624898433685\n",
            "strain 0.23074285686016083\n",
            "strain 0.24120353162288666\n",
            "strain 0.2164672315120697\n",
            "strain 0.2220129370689392\n",
            "strain 0.20113153755664825\n",
            "classify 2.255859375\n",
            "classify 2.18817138671875\n",
            "classify 2.2103271484375\n",
            "classify 2.10223388671875\n",
            "classify 2.33685302734375\n",
            "classify 2.3746337890625\n",
            "classify 2.18902587890625\n",
            "classify 2.2305908203125\n",
            "classify 2.4058837890625\n",
            "0.28125\n",
            "0.375\n",
            "0.1875\n",
            "0.28125\n",
            "strain 0.21454966068267822\n",
            "strain 0.2586410939693451\n",
            "strain 0.23089249432086945\n",
            "strain 0.22758477926254272\n",
            "strain 0.24349774420261383\n",
            "strain 0.22678443789482117\n",
            "strain 0.2180691957473755\n",
            "strain 0.2111743986606598\n",
            "strain 0.20608259737491608\n",
            "classify 2.1910400390625\n",
            "classify 2.3841552734375\n",
            "classify 2.18157958984375\n",
            "classify 2.327392578125\n",
            "classify 2.34722900390625\n",
            "classify 2.205810546875\n",
            "classify 2.2965087890625\n",
            "classify 2.35443115234375\n",
            "classify 2.1328125\n",
            "0.21875\n",
            "0.203125\n",
            "0.375\n",
            "0.265625\n",
            "strain 0.19987650215625763\n",
            "strain 0.2494644969701767\n",
            "strain 0.2089506834745407\n",
            "strain 0.23126846551895142\n",
            "strain 0.22007246315479279\n",
            "strain 0.22301989793777466\n",
            "strain 0.2260221391916275\n",
            "strain 0.23494738340377808\n",
            "strain 0.22256237268447876\n",
            "classify 2.19598388671875\n",
            "classify 2.228271484375\n",
            "classify 2.38287353515625\n",
            "classify 2.23870849609375\n",
            "classify 2.3507080078125\n",
            "classify 2.2459716796875\n",
            "classify 2.34991455078125\n",
            "classify 2.26092529296875\n",
            "classify 2.24176025390625\n",
            "0.328125\n",
            "0.3125\n",
            "0.171875\n",
            "0.203125\n",
            "strain 0.2171488255262375\n",
            "strain 0.2660198509693146\n",
            "strain 0.198526531457901\n",
            "strain 0.2524789273738861\n",
            "strain 0.25273337960243225\n",
            "strain 0.20903821289539337\n",
            "strain 0.20841920375823975\n",
            "strain 0.24116991460323334\n",
            "strain 0.21751978993415833\n",
            "classify 2.34429931640625\n",
            "classify 2.23126220703125\n",
            "classify 2.43890380859375\n",
            "classify 2.23541259765625\n",
            "classify 2.30059814453125\n",
            "classify 2.26409912109375\n",
            "classify 2.23895263671875\n",
            "classify 2.27044677734375\n",
            "classify 2.3033447265625\n",
            "0.25\n",
            "0.34375\n",
            "0.25\n",
            "0.234375\n",
            "strain 0.21595990657806396\n",
            "strain 0.24120508134365082\n",
            "strain 0.23783116042613983\n",
            "strain 0.23272858560085297\n",
            "strain 0.23331835865974426\n",
            "strain 0.2582490146160126\n",
            "strain 0.19935302436351776\n",
            "strain 0.24888502061367035\n",
            "strain 0.19762472808361053\n",
            "classify 2.19244384765625\n",
            "classify 2.30096435546875\n",
            "classify 2.17181396484375\n",
            "classify 2.43310546875\n",
            "classify 2.15338134765625\n",
            "classify 2.2623291015625\n",
            "classify 2.12969970703125\n",
            "classify 2.20233154296875\n",
            "classify 2.3443603515625\n",
            "0.28125\n",
            "0.28125\n",
            "0.328125\n",
            "0.203125\n",
            "strain 0.20280860364437103\n",
            "strain 0.20950289070606232\n",
            "strain 0.2870037853717804\n",
            "strain 0.24289865791797638\n",
            "strain 0.22022923827171326\n",
            "strain 0.24383008480072021\n",
            "strain 0.2470235675573349\n",
            "strain 0.22987349331378937\n",
            "strain 0.21735316514968872\n",
            "classify 2.255615234375\n",
            "classify 2.32568359375\n",
            "classify 2.221435546875\n",
            "classify 2.40264892578125\n",
            "classify 2.08782958984375\n",
            "classify 2.25531005859375\n",
            "classify 2.1724853515625\n",
            "classify 2.38824462890625\n",
            "classify 2.04620361328125\n",
            "0.296875\n",
            "0.21875\n",
            "0.234375\n",
            "0.390625\n",
            "strain 0.22355954349040985\n",
            "strain 0.24240021407604218\n",
            "strain 0.22831860184669495\n",
            "strain 0.25417909026145935\n",
            "strain 0.2036658078432083\n",
            "strain 0.2715318500995636\n",
            "strain 0.2141551524400711\n",
            "strain 0.21439743041992188\n",
            "strain 0.2222280353307724\n",
            "classify 2.12298583984375\n",
            "classify 2.33721923828125\n",
            "classify 2.33465576171875\n",
            "classify 2.39947509765625\n",
            "classify 2.24615478515625\n",
            "classify 2.09088134765625\n",
            "classify 2.3330078125\n",
            "classify 2.28924560546875\n",
            "classify 2.23492431640625\n",
            "0.296875\n",
            "0.28125\n",
            "0.296875\n",
            "0.265625\n",
            "strain 0.20184728503227234\n",
            "strain 0.2163798063993454\n",
            "strain 0.26859211921691895\n",
            "strain 0.2498471587896347\n",
            "strain 0.21445021033287048\n",
            "strain 0.230242058634758\n",
            "strain 0.24711675941944122\n",
            "strain 0.23484861850738525\n",
            "strain 0.22809693217277527\n",
            "classify 2.19921875\n",
            "classify 2.26824951171875\n",
            "classify 2.2608642578125\n",
            "classify 2.28912353515625\n",
            "classify 2.10791015625\n",
            "classify 2.28668212890625\n",
            "classify 2.31842041015625\n",
            "classify 2.3067626953125\n",
            "classify 2.387939453125\n",
            "0.296875\n",
            "0.359375\n",
            "0.1875\n",
            "0.328125\n",
            "strain 0.2670294940471649\n",
            "strain 0.23478911817073822\n",
            "strain 0.24148167669773102\n",
            "strain 0.22352731227874756\n",
            "strain 0.1963757425546646\n",
            "strain 0.20703279972076416\n",
            "strain 0.21717528998851776\n",
            "strain 0.25094133615493774\n",
            "strain 0.24557434022426605\n",
            "classify 2.279296875\n",
            "classify 2.2298583984375\n",
            "classify 2.21844482421875\n",
            "classify 2.19415283203125\n",
            "classify 2.467041015625\n",
            "classify 2.18927001953125\n",
            "classify 2.2498779296875\n",
            "classify 2.2713623046875\n",
            "classify 2.307861328125\n",
            "0.234375\n",
            "0.296875\n",
            "0.28125\n",
            "0.296875\n",
            "strain 0.22317245602607727\n",
            "strain 0.23713943362236023\n",
            "strain 0.22091318666934967\n",
            "strain 0.26712965965270996\n",
            "strain 0.2421845942735672\n",
            "strain 0.2398768812417984\n",
            "strain 0.22947241365909576\n",
            "strain 0.25849947333335876\n",
            "strain 0.228624626994133\n",
            "classify 2.1614990234375\n",
            "classify 2.1688232421875\n",
            "classify 2.20355224609375\n",
            "classify 2.25347900390625\n",
            "classify 2.28759765625\n",
            "classify 2.17132568359375\n",
            "classify 2.424560546875\n",
            "classify 2.577880859375\n",
            "classify 2.22607421875\n",
            "0.28125\n",
            "0.28125\n",
            "0.3125\n",
            "0.21875\n",
            "strain 0.20978528261184692\n",
            "strain 0.22061441838741302\n",
            "strain 0.21900080144405365\n",
            "strain 0.23220272362232208\n",
            "strain 0.249110147356987\n",
            "strain 0.24790890514850616\n",
            "strain 0.20611518621444702\n",
            "strain 0.2074342519044876\n",
            "strain 0.21925638616085052\n",
            "classify 2.08197021484375\n",
            "classify 2.22418212890625\n",
            "classify 2.26910400390625\n",
            "classify 2.37579345703125\n",
            "classify 2.232177734375\n",
            "classify 2.37353515625\n",
            "classify 2.250244140625\n",
            "classify 2.44561767578125\n",
            "classify 2.20135498046875\n",
            "0.3125\n",
            "0.359375\n",
            "0.296875\n",
            "0.203125\n",
            "strain 0.21700377762317657\n",
            "strain 0.19970941543579102\n",
            "strain 0.2107950896024704\n",
            "strain 0.21738582849502563\n",
            "strain 0.24635039269924164\n",
            "strain 0.26149338483810425\n",
            "strain 0.24437381327152252\n",
            "strain 0.21967560052871704\n",
            "strain 0.22139766812324524\n",
            "classify 2.30975341796875\n",
            "classify 2.30279541015625\n",
            "classify 2.07720947265625\n",
            "classify 2.265869140625\n",
            "classify 2.35205078125\n",
            "classify 2.3343505859375\n",
            "classify 2.239990234375\n",
            "classify 2.261474609375\n",
            "classify 2.1300048828125\n",
            "0.25\n",
            "0.328125\n",
            "0.296875\n",
            "0.234375\n",
            "strain 0.23550936579704285\n",
            "strain 0.24748602509498596\n",
            "strain 0.23238135874271393\n",
            "strain 0.24174366891384125\n",
            "strain 0.22641205787658691\n",
            "strain 0.21284164488315582\n",
            "strain 0.24068763852119446\n",
            "strain 0.23844937980175018\n",
            "strain 0.25402432680130005\n",
            "classify 2.18487548828125\n",
            "classify 2.3199462890625\n",
            "classify 2.268310546875\n",
            "classify 2.5592041015625\n",
            "classify 2.3421630859375\n",
            "classify 2.260498046875\n",
            "classify 2.12127685546875\n",
            "classify 2.28466796875\n",
            "classify 2.20465087890625\n",
            "0.3125\n",
            "0.1875\n",
            "0.265625\n",
            "0.28125\n",
            "strain 0.23932059109210968\n",
            "strain 0.2248857021331787\n",
            "strain 0.21226166188716888\n",
            "strain 0.24326269328594208\n",
            "strain 0.2541749179363251\n",
            "strain 0.2453998625278473\n",
            "strain 0.2180841565132141\n",
            "strain 0.2270757406949997\n",
            "strain 0.2225324809551239\n",
            "classify 2.34429931640625\n",
            "classify 2.0194091796875\n",
            "classify 2.21197509765625\n",
            "classify 2.19403076171875\n",
            "classify 2.26971435546875\n",
            "classify 2.30767822265625\n",
            "classify 2.565673828125\n",
            "classify 2.22723388671875\n",
            "classify 2.06756591796875\n",
            "0.234375\n",
            "0.21875\n",
            "0.28125\n",
            "0.234375\n",
            "strain 0.24129483103752136\n",
            "strain 0.2185772955417633\n",
            "strain 0.20811690390110016\n",
            "strain 0.21867698431015015\n",
            "strain 0.24343957006931305\n",
            "strain 0.21766841411590576\n",
            "strain 0.24020853638648987\n",
            "strain 0.22152793407440186\n",
            "strain 0.23315614461898804\n",
            "classify 2.231689453125\n",
            "classify 2.2945556640625\n",
            "classify 2.28594970703125\n",
            "classify 2.3724365234375\n",
            "classify 2.357421875\n",
            "classify 2.180419921875\n",
            "classify 2.09442138671875\n",
            "classify 2.35479736328125\n",
            "classify 2.1875\n",
            "0.25\n",
            "0.21875\n",
            "0.359375\n",
            "0.265625\n",
            "strain 0.24834570288658142\n",
            "strain 0.26650574803352356\n",
            "strain 0.22334560751914978\n",
            "strain 0.24400261044502258\n",
            "strain 0.2088329941034317\n",
            "strain 0.2476937621831894\n",
            "strain 0.2722950279712677\n",
            "strain 0.2025545984506607\n",
            "strain 0.2035779356956482\n",
            "classify 2.23309326171875\n",
            "classify 2.23779296875\n",
            "classify 2.37286376953125\n",
            "classify 2.093505859375\n",
            "classify 2.44268798828125\n",
            "classify 2.16766357421875\n",
            "classify 2.212646484375\n",
            "classify 2.33673095703125\n",
            "classify 2.2823486328125\n",
            "0.3125\n",
            "0.265625\n",
            "0.25\n",
            "0.265625\n",
            "strain 0.22805103659629822\n",
            "strain 0.21144209802150726\n",
            "strain 0.2421450912952423\n",
            "strain 0.2790490686893463\n",
            "strain 0.2683514952659607\n",
            "strain 0.2506609261035919\n",
            "strain 0.21343474090099335\n",
            "strain 0.22634531557559967\n",
            "strain 0.23518390953540802\n",
            "classify 2.27667236328125\n",
            "classify 2.28033447265625\n",
            "classify 2.4652099609375\n",
            "classify 2.17840576171875\n",
            "classify 2.3350830078125\n",
            "classify 2.37249755859375\n",
            "classify 2.0850830078125\n",
            "classify 2.18365478515625\n",
            "classify 2.04656982421875\n",
            "0.296875\n",
            "0.328125\n",
            "0.25\n",
            "0.34375\n",
            "strain 0.2258894145488739\n",
            "strain 0.2574828565120697\n",
            "strain 0.2375069260597229\n",
            "strain 0.23254619538784027\n",
            "strain 0.25812193751335144\n",
            "strain 0.24961422383785248\n",
            "strain 0.2368689477443695\n",
            "strain 0.2314889132976532\n",
            "strain 0.2194679230451584\n",
            "classify 2.104736328125\n",
            "classify 2.273193359375\n",
            "classify 2.17022705078125\n",
            "classify 2.27618408203125\n",
            "classify 2.26434326171875\n",
            "classify 2.2152099609375\n",
            "classify 2.2794189453125\n",
            "classify 2.10272216796875\n",
            "classify 2.4168701171875\n",
            "0.265625\n",
            "0.171875\n",
            "0.28125\n",
            "0.328125\n",
            "strain 0.22749648988246918\n",
            "strain 0.24950352311134338\n",
            "strain 0.2316618263721466\n",
            "strain 0.2319500744342804\n",
            "strain 0.21533136069774628\n",
            "strain 0.21965141594409943\n",
            "strain 0.20805801451206207\n",
            "strain 0.25990357995033264\n",
            "strain 0.27030327916145325\n",
            "classify 2.32745361328125\n",
            "classify 2.19342041015625\n",
            "classify 2.1844482421875\n",
            "classify 2.3985595703125\n",
            "classify 2.16741943359375\n",
            "classify 2.14276123046875\n",
            "classify 2.076416015625\n",
            "classify 2.2996826171875\n",
            "classify 2.25872802734375\n",
            "0.296875\n",
            "0.375\n",
            "0.265625\n",
            "0.171875\n",
            "strain 0.23016734421253204\n",
            "strain 0.24131393432617188\n",
            "strain 0.25506410002708435\n",
            "strain 0.21679668128490448\n",
            "strain 0.21948330104351044\n",
            "strain 0.22840872406959534\n",
            "strain 0.22400498390197754\n",
            "strain 0.22090114653110504\n",
            "strain 0.2397679090499878\n",
            "classify 2.36090087890625\n",
            "classify 2.183837890625\n",
            "classify 2.2095947265625\n",
            "classify 2.24273681640625\n",
            "classify 2.33966064453125\n",
            "classify 2.150634765625\n",
            "classify 2.24591064453125\n",
            "classify 2.2200927734375\n",
            "classify 2.1776123046875\n",
            "0.34375\n",
            "0.28125\n",
            "0.28125\n",
            "0.234375\n",
            "strain 0.20089662075042725\n",
            "strain 0.2425065040588379\n",
            "strain 0.21717052161693573\n",
            "strain 0.22716806828975677\n",
            "strain 0.19092650711536407\n",
            "strain 0.23358577489852905\n",
            "strain 0.24544893205165863\n",
            "strain 0.2257642149925232\n",
            "strain 0.2284003645181656\n",
            "classify 2.32177734375\n",
            "classify 2.30078125\n",
            "classify 2.2882080078125\n",
            "classify 2.20806884765625\n",
            "classify 2.29327392578125\n",
            "classify 2.01507568359375\n",
            "classify 2.32916259765625\n",
            "classify 2.07666015625\n",
            "classify 2.4080810546875\n",
            "0.28125\n",
            "0.328125\n",
            "0.234375\n",
            "0.25\n",
            "strain 0.2157626748085022\n",
            "strain 0.2312512844800949\n",
            "strain 0.2402745634317398\n",
            "strain 0.26022103428840637\n",
            "strain 0.22781838476657867\n",
            "strain 0.2202652245759964\n",
            "strain 0.2135525643825531\n",
            "strain 0.21379238367080688\n",
            "strain 0.23367540538311005\n",
            "classify 2.29254150390625\n",
            "classify 2.30596923828125\n",
            "classify 2.29974365234375\n",
            "classify 2.2088623046875\n",
            "classify 2.1209716796875\n",
            "classify 2.26617431640625\n",
            "classify 2.18914794921875\n",
            "classify 2.25244140625\n",
            "classify 2.363525390625\n",
            "0.265625\n",
            "0.25\n",
            "0.28125\n",
            "0.203125\n",
            "strain 0.21721717715263367\n",
            "strain 0.22101955115795135\n",
            "strain 0.239463672041893\n",
            "strain 0.20459941029548645\n",
            "strain 0.22677738964557648\n",
            "strain 0.25213825702667236\n",
            "strain 0.21404775977134705\n",
            "strain 0.21014848351478577\n",
            "strain 0.2093314230442047\n",
            "classify 2.1778564453125\n",
            "classify 2.23486328125\n",
            "classify 2.24493408203125\n",
            "classify 2.05926513671875\n",
            "classify 2.3311767578125\n",
            "classify 2.264892578125\n",
            "classify 2.27703857421875\n",
            "classify 2.31683349609375\n",
            "classify 2.3070068359375\n",
            "0.1875\n",
            "0.296875\n",
            "0.328125\n",
            "0.328125\n",
            "strain 0.22960977256298065\n",
            "strain 0.22833238542079926\n",
            "strain 0.24824489653110504\n",
            "strain 0.2257024645805359\n",
            "strain 0.21488036215305328\n",
            "strain 0.2081768959760666\n",
            "strain 0.2105417400598526\n",
            "strain 0.23489293456077576\n",
            "strain 0.1999446302652359\n",
            "classify 2.25128173828125\n",
            "classify 2.26416015625\n",
            "classify 2.3134765625\n",
            "classify 2.34344482421875\n",
            "classify 2.0343017578125\n",
            "classify 2.29974365234375\n",
            "classify 2.2613525390625\n",
            "classify 2.25177001953125\n",
            "classify 2.43853759765625\n",
            "0.3125\n",
            "0.3125\n",
            "0.203125\n",
            "0.28125\n",
            "strain 0.21644474565982819\n",
            "strain 0.22361840307712555\n",
            "strain 0.23525863885879517\n",
            "strain 0.22512982785701752\n",
            "strain 0.23119090497493744\n",
            "strain 0.19171142578125\n",
            "strain 0.2862538993358612\n",
            "strain 0.24093665182590485\n",
            "strain 0.24084873497486115\n",
            "classify 2.110595703125\n",
            "classify 2.1375732421875\n",
            "classify 2.21826171875\n",
            "classify 2.27423095703125\n",
            "classify 2.3060302734375\n",
            "classify 2.295654296875\n",
            "classify 2.33636474609375\n",
            "classify 2.37158203125\n",
            "classify 2.238525390625\n",
            "0.25\n",
            "0.1875\n",
            "0.296875\n",
            "0.296875\n",
            "strain 0.22393406927585602\n",
            "strain 0.2387283742427826\n",
            "strain 0.22817625105381012\n",
            "strain 0.26294320821762085\n",
            "strain 0.21977441012859344\n",
            "strain 0.21815401315689087\n",
            "strain 0.2241935282945633\n",
            "strain 0.2238609939813614\n",
            "strain 0.22713850438594818\n",
            "classify 2.2012939453125\n",
            "classify 2.16534423828125\n",
            "classify 2.38104248046875\n",
            "classify 2.25274658203125\n",
            "classify 2.32781982421875\n",
            "classify 2.2659912109375\n",
            "classify 2.0654296875\n",
            "classify 2.34954833984375\n",
            "classify 2.1851806640625\n",
            "0.25\n",
            "0.265625\n",
            "0.40625\n",
            "0.203125\n",
            "strain 0.2802842855453491\n",
            "strain 0.21531140804290771\n",
            "strain 0.21413147449493408\n",
            "strain 0.23194965720176697\n",
            "strain 0.22307567298412323\n",
            "strain 0.23593035340309143\n",
            "strain 0.19885574281215668\n",
            "strain 0.22917801141738892\n",
            "strain 0.23060716688632965\n",
            "classify 2.25921630859375\n",
            "classify 2.333984375\n",
            "classify 2.1798095703125\n",
            "classify 2.3721923828125\n",
            "classify 2.257568359375\n",
            "classify 2.175048828125\n",
            "classify 2.26898193359375\n",
            "classify 2.20123291015625\n",
            "classify 2.4124755859375\n",
            "0.328125\n",
            "0.265625\n",
            "0.25\n",
            "0.28125\n",
            "strain 0.23131613433361053\n",
            "strain 0.22877520322799683\n",
            "strain 0.22718213498592377\n",
            "strain 0.21925528347492218\n",
            "strain 0.23387177288532257\n",
            "strain 0.19912385940551758\n",
            "strain 0.20514369010925293\n",
            "strain 0.24125933647155762\n",
            "strain 0.21289324760437012\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _after_fork at 0x7f476d742de0>Exception ignored in: \n",
            "<function _after_fork at 0x7f476d742de0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1645, in _after_fork\n",
            "Traceback (most recent call last):\n",
            "      File \"/usr/lib/python3.11/threading.py\", line 1664, in _after_fork\n",
            "    thread._stop()\n",
            "threads.update(_dangling)\n",
            "  File \"/usr/lib/python3.11/_weakrefset.py\", line 66, in __iter__\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1051, in _stop\n",
            "    item = itemref()\n",
            "     def _stop(self):\n",
            "  ^        \n",
            "^^KeyboardInterrupt^^^^: \n",
            "^^\n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-f0bc6b2bbc31>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mstrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_jepa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mctrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_jepa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_jepa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-f0bc6b2bbc31>\u001b[0m in \u001b[0;36mctrain\u001b[0;34m(model, classifier, dataloader, coptim, scheduler)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;31m# x, y = x.to(device), y.to(device) # [batch, ]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [batch, ] # (id, activity)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1457\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1459\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1408\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1410\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1411\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1412\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# @title strain ctrain test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "def strain(model, dataloader, optim, scheduler=None):\n",
        "    model.train()\n",
        "    for i, (x, _) in enumerate(dataloader):\n",
        "        x = x[...,1:].to(device).to(torch.bfloat16)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            loss = model.loss(x)\n",
        "\n",
        "            # repr_loss, std_loss, cov_loss = model.loss(x)\n",
        "            # loss = model.sim_coeff * repr_loss + model.std_coeff * std_loss + model.cov_coeff * cov_loss\n",
        "\n",
        "        optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # total_norm = 0\n",
        "        # for p in model.parameters(): total_norm += p.grad.data.norm(2).item() ** 2\n",
        "        # total_norm = total_norm**.5\n",
        "        # print('total_norm', total_norm)\n",
        "        # for p in list(filter(lambda p: p.grad is not None, model.parameters())):\n",
        "        #     print(p.grad.data.norm(2).item())\n",
        "        # print(\"max grad norm\", max([p.grad.data.norm(2).item() for p in list(filter(lambda p: p.grad is not None, model.parameters()))]))\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 5) # 0.5\n",
        "\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            m=0.999 # 0.99 m = next(momentum_scheduler)\n",
        "            for param_q, param_k in zip(model.student.parameters(), model.teacher.parameters()):\n",
        "                param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        print(\"strain\",loss.item())\n",
        "        # for param in seq_jepa.context_encoder.cls: print(param.data)\n",
        "        # for param in seq_jepa.predicter.cls: print(param.data)\n",
        "        try: wandb.log({\"loss\": loss.item(), \"repr/I\": repr_loss.item(), \"std/V\": std_loss.item(), \"cov/C\": cov_loss.item()})\n",
        "        # try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=50: break\n",
        "\n",
        "\n",
        "def ctrain(model, classifier, dataloader, coptim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.eval()\n",
        "    classifier.train()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        # x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        x, y = x[...,1:].to(device).to(torch.bfloat16), y[1].to(device) # [batch, ] # (id, activity)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            with torch.no_grad():\n",
        "                sx = model(x).detach()\n",
        "            # print(sx[0][0])\n",
        "            y_ = classifier(sx)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "        # print(classifier.classifier.weight[0])\n",
        "        # print(y_.shape, y.shape)\n",
        "        # print(loss)\n",
        "        coptim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 5) # .5\n",
        "        scaler.step(coptim)\n",
        "        scaler.update()\n",
        "        print(\"classify\",loss.item())\n",
        "        try: wandb.log({\"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        # x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        x, y = x[...,1:].to(device).to(torch.float), y[1].to(device) # [batch, ] # (id, activity)\n",
        "        with torch.no_grad():\n",
        "            sx = model(x)\n",
        "            rankme = RankMe(sx).item()\n",
        "            lidar = LiDAR(sx).item()\n",
        "            y_ = classifier(sx)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        print(correct/len(y))\n",
        "        # try: wandb.log({\"correct\": correct/len(y)})\n",
        "        try: wandb.log({\"correct\": correct/len(y), \"rankme\": rankme, \"lidar\": lidar})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "for i in range(1200): # 1200\n",
        "    # train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    # test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    np.random.shuffle(train_indices); np.random.shuffle(val_indices)\n",
        "    train_sampler, valid_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(val_indices)\n",
        "    # batch_size = 64 #512\n",
        "    train_loader = DataLoader(train_data, sampler=train_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "    test_loader = DataLoader(train_data, sampler=valid_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True)\n",
        "\n",
        "    strain(seq_jepa, train_loader, optim)\n",
        "    ctrain(seq_jepa, classifier, train_loader, coptim)\n",
        "    test(seq_jepa, classifier, test_loader)\n",
        "\n",
        "    # strain(violet, train_loader, voptim)\n",
        "    # ctrain(violet, classifier, train_loader, coptim)\n",
        "    # test(violet, classifier, test_loader)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "id": "2Nd-sGe6Ku4S",
        "outputId": "e4824d79-866a-4c66-983d-aeff83422740"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>██▇▆▆▅▅▅▄▅▃▄▃▄▄▄▃▃▂▃▄▃▂▂▃▃▁▂▂▂▁▂▂▁▃▁▃▁▂▂</td></tr><tr><td>correct</td><td>▁▁▁▂▂▂▂▃▃▂▃▄▅▄▅▅▄▆▆▇▅▅▇█▆▇▇▆▆▆▇██▅▇▇▇▇█▇</td></tr><tr><td>lidar</td><td>▂▁▅▁▃▄▅▃▅▇▆▇▂▄▇▇▅▇▅▆▃▆▆▅▇▅▅▅▅▄█▇▅▇▆▅▆▅▃▄</td></tr><tr><td>rankme</td><td>▁▁▃▃▄▅▅▆▇▆▆▆▇▇▇▇▇██▇▇▇▇▇▇▇▇▇██▇▇▇█▇███▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>1.90576</td></tr><tr><td>correct</td><td>0.40625</td></tr><tr><td>lidar</td><td>9.02756</td></tr><tr><td>rankme</td><td>8.63001</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">smart-surf-148</strong> at: <a href='https://wandb.ai/bobdole/SeqJEPA/runs/dv7acztm' target=\"_blank\">https://wandb.ai/bobdole/SeqJEPA/runs/dv7acztm</a><br> View project at: <a href='https://wandb.ai/bobdole/SeqJEPA' target=\"_blank\">https://wandb.ai/bobdole/SeqJEPA</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250408_023108-dv7acztm/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250408_034155-ublu4b3w</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/SeqJEPA/runs/ublu4b3w' target=\"_blank\">pretty-dew-149</a></strong> to <a href='https://wandb.ai/bobdole/SeqJEPA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/SeqJEPA' target=\"_blank\">https://wandb.ai/bobdole/SeqJEPA</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/SeqJEPA/runs/ublu4b3w' target=\"_blank\">https://wandb.ai/bobdole/SeqJEPA/runs/ublu4b3w</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "!pip install -q wandb\n",
        "import wandb # https://docs.wandb.ai/quickstart\n",
        "wandb.login(key='487a2109e55dce4e13fc70681781de9f50f27be7')\n",
        "try: run.finish()\n",
        "except NameError: pass\n",
        "run = wandb.init(project=\"SeqJEPA\", config={\"model\": \"res18\",}) # violet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4J2ahp3wmqcg"
      },
      "outputs": [],
      "source": [
        "# @title supervised train test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "def strain(model, classifier, dataloader, optim, coptim, scheduler=None):\n",
        "    model.train()\n",
        "    classifier.train()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        # x = x.to(device).flatten(2).transpose(-2,-1)#.to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "        # y = y.to(device)\n",
        "        x, y = x[...,1:].to(device).to(torch.bfloat16), y[1].to(device) # [batch, ] # (id, activity)\n",
        "\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        # print(loss)\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # for p in list(filter(lambda p: p.grad is not None, model.parameters())):\n",
        "        #     print(p.grad.data.norm(2).item())\n",
        "        # print(\"max grad norm\", max([p.grad.data.norm(2).item() for p in list(filter(lambda p: p.grad is not None, model.parameters()))]))\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 10) # 0.5\n",
        "\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        print(\"strain\",loss.item())\n",
        "        # for param in seq_jepa.context_encoder.cls: print(param.data)\n",
        "        # for param in seq_jepa.predicter.cls: print(param.data)\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=50: break\n",
        "\n",
        "\n",
        "# def test(model, dataloader):\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        # x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        x, y = x[...,1:].to(device).to(torch.float), y[1].to(device) # [batch, ] # (id, activity)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "\n",
        "        loss = F.cross_entropy(y_, y)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        print(correct/len(y))\n",
        "        try: wandb.log({\"correct\": correct/len(y), \"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "for i in range(1000):\n",
        "    # train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    # test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    np.random.shuffle(train_indices); np.random.shuffle(val_indices)\n",
        "    train_sampler, valid_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(val_indices)\n",
        "    # batch_size = 64 #512\n",
        "    train_loader = DataLoader(train_data, sampler=train_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "    test_loader = DataLoader(train_data, sampler=valid_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True)\n",
        "\n",
        "    # strain(seq_jepa, train_loader, optim)\n",
        "    # test(seq_jepa, test_loader)\n",
        "    strain(violet, train_loader, voptim)\n",
        "    test(violet, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgN5LlxWsPzB"
      },
      "outputs": [],
      "source": [
        "# for param in seq_jepa.context_encoder.cls: print(param.data)\n",
        "# print(seq_jepa.context_encoder.cls.is_leaf)\n",
        "# seq=40\n",
        "# src = seq_jepa.context_encoder.pos_encoder(torch.arange(seq, device=device))\n",
        "# for x in src:\n",
        "#     print(x)\n",
        "# print(.999**50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JT8AgOx0E_KM",
        "outputId": "6b3d3969-da28-4cc0-c4de-f0a327ea266f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# @title save/load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder='/content/drive/MyDrive/jepa/'\n",
        "\n",
        "# # modelsd, optimsd = torch.load(folder+'SeqJEPA.pkl', map_location=device).values()\n",
        "# modelsd, optimsd = torch.load('SeqJEPA.pkl', map_location=device).values()\n",
        "# seq_jepa.load_state_dict(modelsd, strict=False)\n",
        "# optim.load_state_dict(optimsd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpbLPvjiE-pD"
      },
      "outputs": [],
      "source": [
        "checkpoint = {'model': seq_jepa.state_dict(), 'optimizer': optim.state_dict()}\n",
        "torch.save(checkpoint, folder+'SeqJEPA.pkl')\n",
        "# torch.save(checkpoint, 'SeqJEPA.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vScvFGGSeN6"
      },
      "source": [
        "## drawer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwpnHW4wn9S1",
        "outputId": "40413ecd-7f4b-49a2-984b-fe812ec57e26"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:08<00:00, 19.5MB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title data\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "transform = transforms.Compose([transforms.ToTensor(),])\n",
        "\n",
        "# MNIST CIFAR10\n",
        "train_data = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
        "test_data = torchvision.datasets.CIFAR10(root='data', train=False, download=True, transform=transform)\n",
        "batch_size = 128 # 128 1024\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    # print(npimg.shape) # (3, 64, 64)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# dataiter = iter(train_data)\n",
        "# x,y = next(dataiter)\n",
        "# print(x.shape) # [3, 32, 32]\n",
        "# imshow(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DNCJFn5kNuua"
      },
      "outputs": [],
      "source": [
        "# @title evergarmin buffer dataloader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "import numpy as np\n",
        "import scipy.interpolate\n",
        "\n",
        "class BufferDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, activities, seq_len):\n",
        "        # self.data = [step for episode in self.process(buffer) for step in episode] # 0.00053\n",
        "        self.data = [self.process(activity) for activity in activities] # 0.00053\n",
        "        self.data = [x for x in self.data if x!=None and x!=[]]\n",
        "        self.seq_len = seq_len\n",
        "        self.pad = [(-1,-1,-1)]*(self.seq_len) # pad. need to mask later # torch.full((self.seq_len,), -1)\n",
        "        # normalise\n",
        "\n",
        "    def __len__(self):\n",
        "        # return len(self.data)//self.seq_len\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        act_list = self.data[idx].copy()\n",
        "        summary = list(act_list.pop(0))\n",
        "        # act_list = self.pad[len(act_list):] + act_list # forward padding\n",
        "        act_list = [summary] + self.transform(act_list) # summary, aug(x)\n",
        "        hr, temp, heart= zip(*act_list)\n",
        "        # print(hr, temp, heart)\n",
        "        # return hr, temp, heart\n",
        "        return torch.tensor(hr), torch.tensor(temp), torch.tensor(heart, dtype=torch.float)\n",
        "\n",
        "\n",
        "\n",
        "    def process(self, activity):\n",
        "        res_id = activity[\"res_id\"]\n",
        "        try: activeKilocalories = activity[\"summary\"][\"activeKilocalories\"]\n",
        "        except KeyError: activeKilocalories = 0.\n",
        "        try: steps = activity[\"summary\"][\"steps\"]\n",
        "        except KeyError: steps = 0.\n",
        "        # (res_id, activeKilocalories, steps)\n",
        "        res_id = float(res_id[3:]) # remove 'RES' and turn id into float, in line with other data\n",
        "        act_list = [(res_id, activeKilocalories, steps)]\n",
        "        samples = activity[\"samples\"]\n",
        "\n",
        "        if len(samples)==0: return\n",
        "\n",
        "        for sample in samples:\n",
        "            try:\n",
        "                startTimeInSeconds = sample[\"startTimeInSeconds\"]\n",
        "                hour_decimal = (startTimeInSeconds / 3600) % 24\n",
        "            except KeyError: hour_decimal = 12.\n",
        "            try: airTemperatureCelcius = sample[\"airTemperatureCelcius\"]\n",
        "            except KeyError: airTemperatureCelcius = 28. # singapore average temperature?\n",
        "            try: heartRate = sample[\"heartRate\"]\n",
        "            except KeyError: heartRate = 80.\n",
        "            # (hour_decimal, airTemperatureCelcius, heartRate)\n",
        "            act_list.append((hour_decimal, airTemperatureCelcius, heartRate))\n",
        "        return act_list\n",
        "\n",
        "    def transform(self, act_list): # rand resize crop, rand mask\n",
        "\n",
        "        # https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html\n",
        "        act_list = np.array(act_list)\n",
        "        try: hr, temp, heart= zip(*act_list)\n",
        "        except ValueError: print('err:',act_list)\n",
        "        kind = 'nearest' if len(act_list)>self.seq_len/.7 else 'linear'\n",
        "        temp_interpolator = scipy.interpolate.interp1d(hr, temp, kind=kind) # linear nearest quadratic cubic\n",
        "        heart_interpolator = scipy.interpolate.interp1d(hr, heart, kind=kind) # linear nearest quadratic cubic\n",
        "        hr_ = np.sort(np.random.uniform(hr[0], hr[-1], round(self.seq_len*random.uniform(1,1/.7))))\n",
        "        temp_, heart_ = temp_interpolator(hr_), heart_interpolator(hr_)\n",
        "        act_list = list(zip(hr_, temp_, heart_))\n",
        "\n",
        "        idx = torch.randint(len(act_list)-self.seq_len+1, size=(1,))\n",
        "        # return act_list[idx: idx+self.seq_len]\n",
        "        act_list = act_list[idx: idx+self.seq_len]\n",
        "\n",
        "        # mask=(torch.rand(self.seq_len)<.1) # True -> masked # random masking\n",
        "        # # act_list[mask] = self.pad[0]\n",
        "        # act_list = [self.pad[0] if m else a for m,a in zip(mask, act_list)]\n",
        "        return act_list\n",
        "\n",
        "    def push(self, activities):\n",
        "        # self.data.append(self.process(activity))\n",
        "        self.data.extend([self.process(activity) for activity in activities])\n",
        "\n",
        "\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(30, 14))\n",
        "    print(npimg.shape)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "seq_len = 200 #26/51 # 50\n",
        "train_data = BufferDataset(activities, seq_len)\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 128 #128\n",
        "# train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # [3,batch, T]\n",
        "train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, drop_last=True) # [3,batch, T]\n",
        "\n",
        "\n",
        "for batch, (hr, temp, heart) in enumerate(train_loader):\n",
        "    pass\n",
        "    # print(hr[6])\n",
        "    # for h in hr:\n",
        "    #     print(h)\n",
        "    # break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHzr3NVs2EcG",
        "outputId": "cecf37be-0d0f-4890-83f3-305b0b01c3ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 256, 4])\n"
          ]
        }
      ],
      "source": [
        "# @title ViT me simple save\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    \"\"\"Zero out the parameters of a module and return it.\"\"\"\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads=None, d_head=8, cond_dim=None, dropout=0.): # .1\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_head = d_head\n",
        "        self.n_heads = d_model // d_head\n",
        "        # self.d_head = d_model // n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or d_model, 2*d_model, bias=False)\n",
        "        # self.k = nn.Sequential(nn.Dropout(dropout), nn.Linear(cond_dim, d_model, bias=False))\n",
        "        # self.lin = nn.Linear(d_model, d_model)\n",
        "        self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        # self.lin = nn.Sequential(nn.Dropout(dropout), zero_module(nn.Linear(d_model, d_model)))\n",
        "        self.drop = nn.Dropout(dropout) # indp before q,k,v; after linout\n",
        "        self.scale = self.d_head ** -.5\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [batch, T, d_model]=[batch, h*w, c], [batch, num_tok, cond_dim], [batch,T]\n",
        "        batch = x.shape[0]\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        Q = self.q(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        # K = self.k(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        K, V = self.kv(cond).view(batch, -1, self.n_heads, 2*self.d_head).transpose(1, 2).chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # linear attention # Softmax(Q) @ (Softmax(K).T @ V)\n",
        "        if mask != None:\n",
        "            mask = mask[:, None, :, None] # [batch,T] -> [batch,1,T,1]\n",
        "            K, V = K.masked_fill(mask, -torch.finfo(x.dtype).max), V.masked_fill(mask, -torch.finfo(x.dtype).max)\n",
        "        Q, K = Q.softmax(dim=-1)*self.scale, K.softmax(dim=-2)\n",
        "        context = K.transpose(-2,-1) @ V # [batch, n_heads, d_head, d_head]\n",
        "        out = Q @ context # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # (quadratic) attention # Softmax(Q @ K.T) @ V\n",
        "        # attn = Q @ K.transpose(-2,-1) * self.scale # [batch, n_heads, T] # [batch, n_heads, T, T/num_tok]\n",
        "        # if mask != None: attn = attn.masked_fill(mask[:, None, :, None], -torch.finfo(attn.dtype).max) # [batch,T]->[batch,1,T,1]\n",
        "        # attention = torch.softmax(attn, dim=-1)\n",
        "        # out = self.drop(attention) @ V # [batch, n_heads, T, d_head]\n",
        "\n",
        "        out = out.transpose(1, 2).flatten(2)\n",
        "        return self.drop(self.lin(out)) # [batch, T, d_model]\n",
        "\n",
        "# if self, dont pass cond_dim in init, dont pass cond in fwd\n",
        "# Softmax(Q @ K.T) @ V ~ Softmax(Q) @ Softmax(K).T @ V\n",
        "\n",
        "# https://github.com/lucidrains/x-transformers/blob/main/x_transformers/x_transformers.py#L1855\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_head=4, cond_dim=None, ff_mult=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm1 = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.norm2 = nn.RMSNorm(d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.cond_dim = cond_dim\n",
        "        self.self = MultiHeadAttention(d_model, d_head=d_head, dropout=0)\n",
        "        if self.cond_dim!=None: self.cross = MultiHeadAttention(d_model, d_head=d_head, cond_dim=cond_dim, dropout=0)\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        ff_dim=d_model*ff_mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), nn.Dropout(dropout), nn.Linear(d_model, ff_dim), act, # ReLU GELU\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "            nn.RMSNorm(d_model), nn.Dropout(dropout), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            # nn.RMSNorm(d_model), act, nn.Dropout(dropout), nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,c,h,w], [batch, num_tok, cond_dim], [batch,T]\n",
        "        # bchw = x.shape\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "\n",
        "        if self.cond_dim==None: cond=None # is self attn\n",
        "        x = x + self.drop(self.self(self.norm1(x)))\n",
        "        if self.cond_dim!=None: x = x + self.cross(self.norm2(x), cond, mask)\n",
        "        x = x + self.ff(x)\n",
        "        # x = x + self.drop(self.ff(x))\n",
        "\n",
        "        # return x.transpose(1,2).reshape(*bchw)\n",
        "        return x\n",
        "\n",
        "\n",
        "# class SimpleViT(nn.Module):\n",
        "#     def __init__(self, in_dim, out_dim, dim, depth, heads, mlp_dim, channels=3, dim_head=8):\n",
        "#         super().__init__()\n",
        "#         self.to_patch_embedding = nn.Sequential( # in, out, kernel, stride, pad\n",
        "#             nn.Conv2d(in_dim, dim, kernel_size=7, stride=2, padding=7//2, bias=False), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "#             # nn.Conv2d(in_dim, dim, kernel_size=7, stride=2, padding=7//2, bias=False), nn.MaxPool2d(2,2)\n",
        "#             )\n",
        "#         # self.pos_embedding = LearnedRoPE2D(dim) # LearnedRoPE2D, RoPE2D\n",
        "#         self.positional_emb = nn.Parameter(torch.zeros(1, 8*8, dim)) # positional_embedding == 'learnable'\n",
        "#         self.transformer = AttentionBlock(d_model=dim, d_head=dim_head)\n",
        "#         self.transformer_blocks = nn.ModuleList([AttentionBlock(d_model=dim, d_head=dim_head) for i in range(depth)])\n",
        "\n",
        "#         self.attention_pool = nn.Linear(dim, 1)\n",
        "#         self.out = nn.Linear(dim, out_dim, bias=False)\n",
        "\n",
        "#     def forward(self, img):\n",
        "#         device = img.device\n",
        "#         x = self.to_patch_embedding(img)\n",
        "#         # x = self.pos_embedding(x)\n",
        "#         x = x.flatten(-2).transpose(-2,-1) # b c h w -> b (h w) c\n",
        "#         x = x + self.positional_emb\n",
        "#         x = self.transformer(x)\n",
        "#         # for blk in self.predictor_blocks: x = blk(x)\n",
        "\n",
        "#         # x = x.flatten(-2).mean(dim=-1) # meal pool\n",
        "#         attn_weights = self.attention_pool(x).squeeze(-1) # [batch, (h,w)] # seq_pool\n",
        "#         x = (attn_weights.softmax(dim = 1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, (h,w)] @ [batch, (h,w), dim]\n",
        "#         # cls_token = repeat(self.class_emb, '1 1 d -> b 1 d', b = b)\n",
        "#         # x = torch.cat((cls_token, x), dim=1)\n",
        "#         # x = x[:, 0] # first token\n",
        "#         return self.out(x)\n",
        "\n",
        "# ijepa: 224*224 -> 14*14 = 196\n",
        "# 32*7\n",
        "# me: 32*32=1024 -> 256\n",
        "\n",
        "import torch\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "def apply_masks(x, masks):\n",
        "    \"\"\":param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
        "    :param masks: list of tensors containing indices of patches in [N] to keep\"\"\"\n",
        "    all_x = []\n",
        "    for m in masks: # [1,T]\n",
        "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [1,T,dim]\n",
        "        all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "    return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "# batch, seq_len, d_model = 2,7,4\n",
        "# x = torch.rand(batch, seq_len, d_model)\n",
        "# print(x)\n",
        "# masks = [torch.randint(0,seq_len,(2,3,))]\n",
        "# print(masks)\n",
        "# out = apply_masks(x, masks)\n",
        "# print(out)\n",
        "# # print(out.shape)\n",
        "\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        # self.positional_emb = nn.Parameter(torch.zeros(1, 32*32//4, d_model)) # positional_embedding == 'learnable'\n",
        "        # encoder_layers = nn.TransformerEncoderLayer(d_model, d_model//d_head, d_hid or d_model, dropout, batch_first=True)\n",
        "        # self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.zeros(1,1,d_model)) # randn zeros\n",
        "        nn.init.trunc_normal_(self.cls, std=.02)\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model)\n",
        "        self.lin = nn.Linear(d_model, out_dim)# if out_dim != d_model else None\n",
        "\n",
        "    def forward(self, src, context_indices, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        src = self.embed(src) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = src.shape\n",
        "        src = src * self.pos_encoder(context_indices)\n",
        "        # src = src + self.positional_emb[:,context_indices]\n",
        "        # src = apply_masks(src, [context_indices])\n",
        "\n",
        "        pred_tokens = self.cls * self.pos_encoder(trg_indices) # [M, num_trg_toks, d_model]\n",
        "        # pred_tokens = self.cls + self.positional_emb[0,trg_indices]\n",
        "        pred_tokens = pred_tokens.repeat(batch, 1, 1) # [batch*M, num_trg_toks, d_model]\n",
        "        # print(pred_tokens.requires_grad)\n",
        "        # print(\"pred fwd\", src.shape, pred_tokens.shape)\n",
        "        # src = src.repeat_interleave(trg_indices.shape[0], dim=0) # [batch, seq_len, d_model] -> [batch*M, seq_len, d_model]\n",
        "        src = torch.cat([src, pred_tokens], dim=1) # [batch*M, seq_len+num_trg_toks, d_model]\n",
        "\n",
        "        out = self.transformer_encoder(src) # float [seq_len, batch_size, d_model]\n",
        "        out = self.norm(out)\n",
        "        out = out[:,seq:] # [batch*M, num_trg_toks, d_model]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.MaxPool1d(2,2), #nn.MaxPool1d(3, 2, 3//2),\n",
        "            nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.Conv1d(d_model, d_model,3,2,3//2)\n",
        "            # nn.Conv1d(in_dim, d_model,2,2,0), # like patch\n",
        "            )\n",
        "        self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        # self.positional_emb = nn.Parameter(torch.zeros(1, 32*32//4, d_model)) # positional_embedding == 'learnable'\n",
        "        # encoder_layers = nn.TransformerEncoderLayer(d_model, d_model//d_head, d_hid or d_model, dropout, batch_first=True)\n",
        "        # self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model)\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, src, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # src = self.embed(src) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        src = self.embed(src.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "\n",
        "        batch, seq, dim = src.shape\n",
        "\n",
        "        # # # src = self.pos_encoder(src)\n",
        "        # if context_indices != None:\n",
        "        # print(src.shape, self.pos_encoder(context_indices).shape)\n",
        "        #     src = src * self.pos_encoder(context_indices) # context/predictor # src = src + self.positional_emb[:,context_indices]\n",
        "        # else: src = src * self.pos_encoder(torch.arange(seq, device=device).unsqueeze(0)) # target # src = src + self.positional_emb[:,:seq]\n",
        "\n",
        "\n",
        "        # src = self.pos_encoder(src)\n",
        "        src = src * self.pos_encoder(torch.arange(seq, device=device)).unsqueeze(0)\n",
        "        # src = src + self.positional_emb\n",
        "        if context_indices != None:\n",
        "            src = apply_masks(src, [context_indices])\n",
        "\n",
        "\n",
        "        out = self.transformer_encoder(src) # float [seq_len, batch_size, d_model]\n",
        "        out = self.norm(out)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "batch, seq_len, d_model = 4,1024,4\n",
        "in_dim = 3\n",
        "model = TransformerModel(in_dim, d_model, d_head=4, nlayers=2, dropout=0.).to(device)\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "# # print(out)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_YBe6-eZ2Zq",
        "outputId": "09b27f75-ba04-4aaf-f5e4-1b9c1dc8d112"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 16, 25])\n"
          ]
        }
      ],
      "source": [
        "x = torch.rand(4,100,3)\n",
        "\n",
        "x=x.transpose(-2,-1) # [batch, dim, seq]\n",
        " # in, out, kernel, stride, pad\n",
        "# net = nn.Conv1d(3,16,7,2,7//2)\n",
        "net = nn.Sequential(nn.Conv1d(3,16,7,2,7//2), nn.MaxPool1d(3, 2, 3//2))\n",
        "# net = nn.Conv1d(3,16,(7,3),2,3//2)\n",
        "out = net(x)\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "R6kiiqw3uIdV"
      },
      "outputs": [],
      "source": [
        "# @title test repeat_interleave\n",
        "# x = torch.rand(3,5)\n",
        "x = torch.rand(3,5,7)\n",
        "print(x)\n",
        "# out = x.repeat(2,1) # [2*3,5]\n",
        "# print(out)\n",
        "# x_ = out.reshape(2,3,5)\n",
        "# print(x_)\n",
        "# out = x.repeat_interleave(2,1,1) # [3*2,5]\n",
        "out = x.repeat_interleave(2,dim=0) # [3*2,5]\n",
        "# print(out)\n",
        "# x_ = out.reshape(2,3,5,7)\n",
        "x_ = out.reshape(3,2,5,7)\n",
        "print(x_)\n",
        "\n",
        "\n",
        "# batch=1\n",
        "# seq_len=5\n",
        "# dim=4\n",
        "# positional_emb = torch.rand(batch, seq_len, dim)\n",
        "# print(positional_emb)\n",
        "# num_tok=2\n",
        "# trg_indices=torch.randint(0,seq_len,(M, num_tok))\n",
        "# print(trg_indices)\n",
        "# # pos_embs = positional_emb.gather(1, trg_indices.unsqueeze(0))\n",
        "# pos_embs = positional_emb[torch.arange(batch)[...,None,None],trg_indices.unsqueeze(0)]\n",
        "# # pos_embs = positional_emb[0,trg_indices]\n",
        "# # [batch, M, num_tok, dim]\n",
        "# print(pos_embs.shape)\n",
        "# print(pos_embs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8r_GjI_vCMyo"
      },
      "outputs": [],
      "source": [
        "# @title test ropes\n",
        "# for i, (img, _) in enumerate(test_loader):\n",
        "#     break\n",
        "# print(img.shape) # [batch, 1, 28, 28]\n",
        "# print(img[0]) # [0,1)\n",
        "d_model=8\n",
        "# rot_emb = RotEmb(d_model, top=torch.pi, base=10)\n",
        "# x = torch.linspace(0,1,20)\n",
        "# out = rot_emb(x)\n",
        "# print(out.shape)\n",
        "# print(out)\n",
        "\n",
        "# # pos_encoder = RoPE(d_model, seq_len=15, base=100)\n",
        "# pos_encoder = RoPE(d_model, seq_len=15, base=10000)\n",
        "x = torch.ones(1, 10, d_model)\n",
        "# # x = torch.ones(1, 10, d_model)\n",
        "# out = pos_encoder(x)\n",
        "# # print(out.shape)\n",
        "# for x in out[0]:\n",
        "#     print(x)\n",
        "\n",
        "\n",
        "# pos_encoder = LearnedRoPE(d_model, seq_len=512)\n",
        "# out = pos_encoder(x)\n",
        "# for o in out[0]:\n",
        "#     print(o)\n",
        "\n",
        "# pos_encoder.weights = nn.Parameter(torch.randn(1, d_model//2))\n",
        "# out = pos_encoder(x)\n",
        "# for o in out[0]:\n",
        "#     print(o)\n",
        "\n",
        "\n",
        "# tensor([-0.0177, -0.9998,  0.8080, -0.5892, -0.7502, -0.6612,  0.3885,  0.9214])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "d929355f-be2b-47a1-82ff-a7630671f47f"
      },
      "outputs": [],
      "source": [
        "# @title RNN pytorch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, num_layers=1):\n",
        "        super().__init__()\n",
        "        if out_dim is None: out_dim = in_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.d_model = d_model\n",
        "        # self.rnn = nn.RNN(d_model, d_model, num_layers, batch_first=True)\n",
        "        self.rnn = nn.GRU(in_dim, d_model, num_layers, batch_first=True)\n",
        "        # self.lstm = nn.LSTM(d_model, d_model, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(d_model, out_dim)\n",
        "\n",
        "        # self.emb = nn.Embedding(in_dim, d_model)\n",
        "        self.mask_vec = nn.Parameter(torch.randn(in_dim))\n",
        "\n",
        "\n",
        "    # def forward(self, x, hc=None): # lstm [batch_size, seq, in_dim]\n",
        "    #     x = self.emb(x)\n",
        "    #     if hc is None:\n",
        "    #         h0 = torch.zeros((self.num_layers, x.size(0), self.d_model), device=device)\n",
        "    #         c0 = torch.zeros((self.num_layers, x.size(0), self.d_model), device=device)\n",
        "    #     else: h0,c0 = hc\n",
        "    #     # print(x.shape, h0.shape,c0.shape)\n",
        "    #     out, (h,c) = self.lstm(x, (h0,c0)) # [batch, seq_len, d_model], ([num_layers, batch, d_model] )\n",
        "    #     # out = out[:, -1, :] # out: (n, 128)\n",
        "    #     out = self.fc(out) # out: (n, 10)\n",
        "    #     return out, (h, c)\n",
        "\n",
        "    def reset(self, batch):\n",
        "        h0 = torch.zeros((self.num_layers, batch, self.d_model), device=device)\n",
        "        return h0\n",
        "\n",
        "    def forward(self, x, h0=None, mask=None): # rnn/gru # [batch, T, in_dim], [num_layers, batch, d_model], [batch, T] True->masked\n",
        "        # x = self.emb(x)\n",
        "        if h0==None: h0 = self.reset(x.shape[0])\n",
        "        # print(x.shape, h0.shape)\n",
        "        if mask!=None: x[mask] = self.mask_vec.to(x.dtype)\n",
        "        out, h = self.rnn(x, h0) #\n",
        "        out = out[:, -1, :] # out: (n, 128)\n",
        "        out = self.fc(out) # out: (n, 10)\n",
        "        return out, h # [batch, out_dim], [num_layers, batch, d_model]\n",
        "\n",
        "\n",
        "# hidden_size = 128\n",
        "# num_layers = 2\n",
        "# input_size = num_classes = 4\n",
        "\n",
        "# model = RNN(input_size, hidden_size, num_classes, num_layers).to(device)\n",
        "# # # print(model)\n",
        "# # print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "# batch=2\n",
        "# seq_len=3\n",
        "# x=torch.rand(batch,seq_len,input_size).to(device)\n",
        "# h=torch.rand(num_layers,batch,hidden_size).to(device)\n",
        "# mask=(torch.rand(batch,seq_len)<.5)#.expand(-1,-1,x.size(-1))\n",
        "# print(mask)\n",
        "# print(x)\n",
        "# out,h = model(x, h, mask)\n",
        "# print(out.shape)\n",
        "# print(h.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AtPUcVu2kSLI"
      },
      "outputs": [],
      "source": [
        "# @title TransformerClassifier\n",
        "import math\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class TransformerClassifier(nn.Module):\n",
        "    # def __init__(self, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, drop = 0.):\n",
        "    def __init__(self, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, drop=0):\n",
        "        super().__init__()\n",
        "        # self.embedding = nn.Embedding(ntoken, d_model)\n",
        "        self.pos_encoder = RoPE(d_model, seq_len=200, base=10000)\n",
        "        # d_hid = d_hid or d_model#*2\n",
        "        # encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, d_hid, drop, batch_first=True)\n",
        "        # self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "\n",
        "        out_dim = out_dim or d_model\n",
        "        self.lin = nn.Linear(d_model, out_dim)\n",
        "        # self.lin = nn.Linear(d_model*2, out_dim)\n",
        "        # self.cls = nn.Parameter(torch.randn(1,1,d_model))\n",
        "        self.attention_pool = nn.Linear(d_model, 1, bias=False)\n",
        "        # self.out = nn.Sequential(\n",
        "        #     nn.Dropout(drop), nn.Linear(d_model, 3), nn.SiLU(),\n",
        "        #     nn.Dropout(drop), nn.Linear(3, out_dim), nn.Sigmoid()\n",
        "        # )\n",
        "\n",
        "    def forward(self, x, src_key_padding_mask = None): # [batch, seq, d_model], [batch, seq] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # batch, seq_len, d_model = x.shape\n",
        "        # x = torch.cat([self.cls.repeat(batch,1,1), x], dim=1)\n",
        "        # src_key_padding_mask = torch.cat([torch.zeros((batch, 1), dtype=torch.bool), src_key_padding_mask], dim=1)\n",
        "        x = self.pos_encoder(x)\n",
        "\n",
        "        x = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
        "        # print(\"fwd\",out.shape) # float [batch, seq_len, d_model]\n",
        "        # out = x.mean(dim=1) # average pool\n",
        "        # out = x.max(dim=1)#[0]\n",
        "\n",
        "        attn = self.attention_pool(x).squeeze(-1) # [batch, seq] # seq_pool\n",
        "        out = (torch.softmax(attn, dim=-1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, seq] @ [batch, seq, dim] -> [batch, dim]\n",
        "        # out = self.out(out) # optional (from GlobalContext) like squeeze excitation\n",
        "\n",
        "        # out = out[:, 0] # first token\n",
        "        # out = torch.cat([out, mean_pool], dim=-1)\n",
        "\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "batch, seq_len, d_model = 4,7,512\n",
        "model = TransformerModel(in_dim, d_model, d_head=4, nlayers=2, dropout=0.).to(device)\n",
        "# model = TransformerClassifier(d_model, nhead=8, nlayers=2, dropout=0.).to(device)\n",
        "x =  torch.rand(batch, seq_len, d_model)\n",
        "src_key_padding_mask = torch.stack([(torch.arange(seq_len) < seq_len - v) for v in torch.randint(seq_len, (batch,))]) # True will be ignored # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "print(src_key_padding_mask)\n",
        "out = model(x, src_key_padding_mask)\n",
        "print(out.shape)\n",
        "\n",
        "# GlobalAveragePooling1D layer, followed by a Dense layer. The final output of the transformer is produced by a softmax layer,\n",
        "# x = GlobalAveragePooling1D()(x)\n",
        "# x = Dropout(0.1)(x)\n",
        "# x = Dense(20, activation=\"relu\")(x)\n",
        "# x = Dropout(0.1)(x)\n",
        "# outputs = Dense(2, activation=\"softmax\")(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-8i1WLsvH_vn"
      },
      "outputs": [],
      "source": [
        "# @title Transformer Model\n",
        "import torch\n",
        "from torch import nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        # self.embed = nn.Sequential(\n",
        "        #     nn.Linear(in_dim, d_model), #act,\n",
        "        #     # nn.Linear(d_model, d_model), act,\n",
        "        # )\n",
        "        self.embed = nn.Linear(in_dim, d_model) if in_dim != d_model else None\n",
        "        # self.embed = nn.Sequential(nn.Conv1d(in_dim,d_model,7,2,7//2), nn.MaxPool1d(3, 2, 3//2))\n",
        "        self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, d_hid or d_model, dropout, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.d_model = d_model\n",
        "        self.cls = nn.Parameter(torch.zeros(1,1,d_model)) # randn\n",
        "        out_dim = out_dim or d_model\n",
        "        # self.lin = nn.Linear(d_model, out_dim)\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim != d_model else None\n",
        "        self.norm = nn.LayerNorm(out_dim)\n",
        "\n",
        "\n",
        "    def forward(self, src, src_key_padding_mask=None, cls_mask=None, context_indices=None, trg_indices=None): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # if cls_mask != None: src[cls_mask] = self.cls.to(src.dtype)\n",
        "\n",
        "        if self.embed != None: src = self.embed(src) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = src.shape\n",
        "        # src = self.pos_encoder(src)\n",
        "        if context_indices != None:\n",
        "            # print(src.shape, context_indices.shape, self.pos_encoder(context_indices).shape) # [2, 88, 32]) torch.Size([88]) torch.Size([88, 32]\n",
        "            # print(src[0], self.pos_encoder(context_indices)[0])\n",
        "            src = src * self.pos_encoder(context_indices) # context/predictor # src = src + self.positional_emb[:,context_indices]\n",
        "            # print(src[0])\n",
        "        else: src = src * self.pos_encoder(torch.arange(seq, device=device)) # target # src = src + self.positional_emb[:,:seq]\n",
        "            # print(\"trans fwd\", src.shape, self.pos_encoder(src).shape)\n",
        "\n",
        "        if trg_indices != None: # [M, num_trg_toks]\n",
        "            pred_tokens = self.cls * self.pos_encoder(trg_indices) # [M, num_trg_toks, d_model] # pred_tokens = self.cls + self.positional_emb[0,trg_indices]\n",
        "            pred_tokens = pred_tokens.repeat(batch, 1, 1) # [batch*M, num_trg_toks, d_model]\n",
        "            # print(pred_tokens.requires_grad)\n",
        "            src = src.repeat_interleave(trg_indices.shape[0], dim=0) # [batch, seq_len, d_model] -> [batch*M, seq_len, d_model]\n",
        "            src = torch.cat([src, pred_tokens], dim=1) # [batch*M, seq_len+num_trg_toks, d_model]\n",
        "\n",
        "        out = self.transformer_encoder(src, src_key_padding_mask=src_key_padding_mask) # float [seq_len, batch_size, d_model]\n",
        "        if trg_indices != None:\n",
        "            # print(out.shape)\n",
        "            out = out[:,seq:] # [batch*M, num_trg_toks, d_model]\n",
        "        if self.lin != None: out = self.lin(out)\n",
        "        out = self.norm(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "batch, seq_len, d_model = 4,7,64\n",
        "in_dim = 3\n",
        "model = TransformerModel(in_dim, d_model, nhead=8, nlayers=2, dropout=0.).to(device)\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "# print(out.shape)\n",
        "# # print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8ba31127-d6e5-438a-aede-5c789557ab39"
      },
      "outputs": [],
      "source": [
        "# @title SeqJEPA\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "def multiblock(seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "    return target_mask\n",
        "\n",
        "\n",
        "class SeqJEPA(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, num_layers=1, num_classes=10):\n",
        "        super().__init__()\n",
        "        if out_dim is None: out_dim = d_model\n",
        "        act = nn.GELU() # ReLU GELU SiLU\n",
        "        # self.rot_emb = RotEmb(d_model, top=1, base=10)\n",
        "        # self.rot_emb = RotEmb(d_model, top=torch.pi, base=10)\n",
        "        # self.rot_emb = RoPE(d_model, seq_len=1024, base=10)\n",
        "        # self.rot_emb = LearnedRotEmb(dim)\n",
        "        self.encode = nn.Sequential(\n",
        "            nn.Linear(in_dim, d_model), #act,\n",
        "            # nn.Linear(d_model, d_model), act,\n",
        "        )\n",
        "        # self.encode = nn.Sequential(nn.Conv1d(in_dim, d_model,7,2,7//2), nn.MaxPool1d(3, 2, 3//2))\n",
        "        self.context_encoder = TransformerModel(d_model, d_model, out_dim=out_dim, nhead=4, nlayers=2, dropout=0.)\n",
        "        self.predicter = TransformerModel(out_dim, d_model//2, out_dim, nhead=4, nlayers=1, dropout=0.)\n",
        "        self.target_encoder = AveragedModel(self.context_encoder, multi_avg_fn=get_ema_multi_avg_fn(0.85)) # 0.95 0.999\n",
        "        self.target_encoder.requires_grad_(False)\n",
        "        # self.classifier = nn.Linear(out_dim, num_classes)\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]\n",
        "        x = self.encode(x) # [batch, T, d_model]\n",
        "        # x = self.encode(x.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "\n",
        "        batch, seq, dim = x.shape\n",
        "        M=1 # 4\n",
        "        # target_mask = multiblock(seq, min_s=0.15, max_s=0.2, M=M) # mask out targets to be predicted # [M, seq]\n",
        "        target_mask = multiblock(seq, min_s=0.15, max_s=0.2, M=4)#.any(0) # mask out targets to be predicted # [M, seq]\n",
        "        context_mask = ~multiblock(seq, min_s=0.85, max_s=1., M=1)|target_mask.reshape(M,seq).any(0) # [1, seq], True->Mask\n",
        "        target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "        # print(target_mask, context_mask)\n",
        "        sorted_mask, ids = context_mask.int().sort(dim=1, stable=True)\n",
        "        context_indices = ids[~sorted_mask.bool()] # int idx [num_context_toks] , idx of context not masked\n",
        "        sorted_x = x[torch.arange(batch).unsqueeze(-1), context_indices] # [batch, num_context_toks, d_model]\n",
        "        # print(sorted_x.shape, context_indices.shape)[2, 9, 32]) torch.Size([9])\n",
        "        # print(sorted_x[0])\n",
        "        # print(sorted_x.is_leaf)\n",
        "        sorted_sx = self.context_encoder(sorted_x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "        # del sorted_x\n",
        "        # print(sorted_sx[0])\n",
        "\n",
        "        sorted_mask, ids = target_mask.int().sort(dim=1, stable=True)\n",
        "        # print(sorted_mask.shape, ids.shape, ids[~sorted_mask.bool()].shape, sorted_mask.sum(-1))\n",
        "        trg_indices = ids[sorted_mask.bool()].reshape(M,-1) # int idx [M, num_trg_toks] , idx of targets that are masked\n",
        "        # sorted_x = x[torch.arange(batch)[...,None,None], indices] # [batch, M, seq-num_trg_toks, dim]\n",
        "        # sx = sorted_sx[torch.arange(batch).unsqueeze(-1),ids.argsort(1)]\n",
        "        # print(sorted_sx.shape, trg_indices.shape)\n",
        "        sy_ = self.predicter(sorted_sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "        # sx = self.context_encoder(x, src_key_padding_mask=context_mask).repeat(M,1,1)\n",
        "        # # sy_ = self.predicter(sx, cls_mask=target_mask)*target_mask.unsqueeze(-1)\n",
        "        # sy_ = self.predicter(sx, trg_mask=target_mask)*target_mask.unsqueeze(-1)\n",
        "\n",
        "        # print(sy_.shape, target_mask.shape)\n",
        "        # print(self.target_encoder(x).repeat_interleave(M, dim=0).shape, trg_indices.repeat(batch,1).shape)\n",
        "        with torch.no_grad():\n",
        "            sy = self.target_encoder(x).repeat_interleave(M, dim=0)[torch.arange(batch*M).unsqueeze(-1), trg_indices.repeat(batch,1)] # [batch*M, num_trg_toks, out_dim]\n",
        "\n",
        "        # print(sy_[0])\n",
        "        # print(sy.shape, sy_.shape)\n",
        "        # loss = F.smooth_l1_loss(sy, sy_)\n",
        "        loss = F.mse_loss(sy, sy_)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]\n",
        "        batch, seq, dim = x.shape\n",
        "        target_mask = randpatch(seq//self.patch_size, mask_size=16, gamma=.75) # [seq]\n",
        "        context_mask = ~multiblock(seq//self.patch_size, min_s=0.85, max_s=1., M=1)|target_mask # [1, seq], True->Mask\n",
        "        target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "        # print(target_mask, context_mask)\n",
        "        sorted_mask, ids = context_mask.int().sort(dim=1, stable=True)\n",
        "        context_indices = ids[~sorted_mask.bool()]#.unsqueeze(0) # int idx [num_context_toks] , idx of context not masked\n",
        "        sorted_x = x[torch.arange(batch).unsqueeze(-1), context_indices] # [batch, num_context_toks, 3]\n",
        "        print(sorted_x.shape, context_indices.shape)\n",
        "        sorted_sx = self.context_encoder(sorted_x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "\n",
        "        sorted_mask, ids = target_mask.int().sort(dim=-1, stable=True)\n",
        "        trg_indices = ids[sorted_mask.bool()].unsqueeze(0) # int idx [1, num_trg_toks] , idx of targets that are masked\n",
        "        print(sorted_sx.shape, context_indices.shape)\n",
        "        sy_ = self.predicter(sorted_sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "\n",
        "        with torch.no_grad(): sy = self.target_encoder(x.detach())[torch.arange(batch).unsqueeze(-1), trg_indices.repeat(batch,1)] # [batch, num_trg_toks, out_dim]\n",
        "        # print(x[0][0][:8])\n",
        "        # print(sy_[0][0][:8])\n",
        "        # print(sy.shape, sy_.shape)\n",
        "        # loss = F.smooth_l1_loss(sy, sy_)\n",
        "        loss = F.mse_loss(sy.detach(), sy_)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        x = self.encode(x)\n",
        "        # x = self.encode(x.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "        sx = self.target_encoder(x)\n",
        "        out = sx.mean(dim=1)\n",
        "        return out\n",
        "\n",
        "\n",
        "# min_s=0.15, max_s, M\n",
        "# trg.15.2M4 C.85 1\n",
        "\n",
        "seq_jepa = SeqJEPA(in_dim=3, d_model=32, out_dim=16, num_layers=1).to(device)#.to(torch.float)\n",
        "optim = torch.optim.AdamW(seq_jepa.parameters(), lr=1e-4) # 1e-3?\n",
        "print(sum(p.numel() for p in seq_jepa.parameters() if p.requires_grad)) # 59850\n",
        "\n",
        "\n",
        "# x = torch.rand((2,20,3), device=device)\n",
        "# out = seq_jepa.loss(x)\n",
        "# print(out.shape)\n",
        "# print(x.is_leaf) # T\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "O-OU1MaKF7BZ"
      },
      "outputs": [],
      "source": [
        "# @title SeqJEPA old\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def multiblock(batch, seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "        mask_len = torch.rand(batch, M) * (max_s - min_s) + min_s # in (min_s, max_s)\n",
        "        mask_pos = torch.rand(batch, M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "        mask_len, mask_pos = mask_len * seq, mask_pos * seq\n",
        "        indices = torch.arange(seq)[None,None,...]\n",
        "        target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [batch, M, seq]\n",
        "        target_mask = target_mask.any(1) # True -> masked # multiblock masking # [batch, seq]\n",
        "        return target_mask\n",
        "\n",
        "\n",
        "\n",
        "class SeqJEPA(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.in_dim = in_dim\n",
        "        # if out_dim is None: self.out_dim = in_dim\n",
        "        if out_dim is None: out_dim = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.d_model = d_model\n",
        "        act = nn.GELU() # ReLU GELU SiLU\n",
        "\n",
        "        dim=8\n",
        "\n",
        "\n",
        "        self.calorie_emb = RotEmb(dim, top=1, base=10) # 0-4\n",
        "        self.steps_emb = RotEmb(dim, top=1, base=10) # 0-5\n",
        "        self.enc0 = nn.Sequential(\n",
        "            nn.Linear(dim*2, d_model), act,\n",
        "            nn.Linear(d_model, d_model), act,\n",
        "        )\n",
        "        self.hour_emb = CircularEmb(dim, torch.pi/2) # circular (0,1) # 0-24\n",
        "        self.temp_emb = RotEmb(dim, top=1/3, base=10) # 18-35\n",
        "        self.heart_emb = RotEmb(dim, top=1/3, base=100) # 50-200\n",
        "        self.enc1 = nn.Sequential(\n",
        "            nn.Linear(dim*3, d_model), act,\n",
        "        )\n",
        "\n",
        "        # self.transformer = TransformerClassifier(d_model, out_dim=out_dim, nhead=8, nlayers=2)\n",
        "        # # self.fc = nn.Linear(d_model, out_dim)\n",
        "        self.context_encoder = TransformerModel(d_model, out_dim=out_dim, nhead=8, nlayers=2, dropout=0.)\n",
        "        self.predicter = TransformerModel(out_dim, nhead=8, nlayers=1, dropout=0.)\n",
        "        self.target_encoder = AveragedModel(self.context_encoder, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    def loss(self, hr, temp, heart): # [batch, 1+T]\n",
        "        # batch = hr.size(0)\n",
        "        res_id, activeKilocalories, steps = hr[:,0], temp[:,0], heart[:,0]\n",
        "        # enc_summary = self.enc0(torch.cat([self.calorie_emb(activeKilocalories), self.steps_emb(steps)], dim=-1)) # [batch, d_model]\n",
        "        enc_summary = self.enc0(torch.cat([self.calorie_emb(torch.log(activeKilocalories.clamp(min=1))), self.steps_emb(torch.log(steps.clamp(min=1)))], dim=-1)) # [batch, d_model]\n",
        "        x = self.enc1(torch.cat([self.hour_emb(hr[:,1:]/24), self.temp_emb(temp[:,1:]), self.heart_emb(heart[:,1:])], dim=-1)) # [batch, T, d_model]\n",
        "        # print('violet fwd', hr.shape, enc_summary.shape, x.shape)\n",
        "\n",
        "        # # mask=(torch.rand(self.seq_len)<.1) # True -> masked # random masking\n",
        "        # mask=(torch.rand_like(x)<.1) # True -> masked # random masking\n",
        "\n",
        "        # patches pad -enc> patch_enc\n",
        "        # rearrange patch in padded/masked, -pred> pred of masked\n",
        "        # mse tgt_enc(img)\n",
        "\n",
        "        # average L2 distance between the predicted patch-level representations sˆy(i) and the target patch-level representation sy(i);\n",
        "        # F.smooth_l1_loss\n",
        "\n",
        "        batch, seq, dim = x.shape\n",
        "\n",
        "        # mask=(torch.rand(batch, seq)<.75) # True -> masked # random masking\n",
        "        # sorted_mask, ids = mask.sort(dim=1)\n",
        "        # # sorted_x = x[torch.arange(batch)[...,None,None],ids]\n",
        "        # sorted_x = x[torch.arange(batch).unsqueeze(-1),ids]\n",
        "        # sorted_mask = torch.cat([torch.zeros((batch, 1), dtype=torch.bool), sorted_mask], dim=1) # True->mask\n",
        "        # sorted_x = torch.cat([enc_summary.unsqueeze(1), sorted_x], dim=1)\n",
        "        # sorted_sx = self.context_encoder(sorted_x, src_key_padding_mask=sorted_mask)\n",
        "        # # torch.zeros_like(sorted_sx)\n",
        "        # sx = sorted_sx[torch.arange(batch).unsqueeze(-1),ids.argsort(1)]\n",
        "        # sy_ = self.predicter(sx, src_key_padding_mask=mask)\n",
        "        # sy = self.target_encoder(x)*~mask\n",
        "\n",
        "\n",
        "        # target_mask = multiblock(batch, seq, min_s=0.15, max_s=0.2, M=4)\n",
        "        # context_mask = ~multiblock(batch, seq, min_s=0.85, max_s=1., M=1)|target_mask\n",
        "        # sx = self.context_encoder(x, src_key_padding_mask=context_mask)\n",
        "        # sy_ = self.predicter(sx, cls_mask=target_mask)*target_mask.unsqueeze(-1)\n",
        "        # sy_ = self.predicter(sx, cls_mask=target_mask)\n",
        "        # # print(sy_.shape, target_mask.shape)\n",
        "        # sy = self.target_encoder(x)*target_mask.unsqueeze(-1)\n",
        "\n",
        "\n",
        "        M=4\n",
        "        target_mask = multiblock(M*batch, seq, min_s=0.15, max_s=0.2, M=1) # mask out targets to be predicted # [4*batch, seq]\n",
        "        context_mask = ~multiblock(batch, seq, min_s=0.85, max_s=1., M=1)|target_mask.reshape(batch,M,seq).any(1) # [batch, seq]\n",
        "\n",
        "\n",
        "        x = torch.cat([enc_summary.unsqueeze(1), x], dim=1) # [batch, 1+T, d_model]\n",
        "        target_mask = torch.cat([torch.zeros((M*batch, 1), dtype=bool), target_mask],dim=1) # [4*batch, 1+T]\n",
        "        context_mask = torch.cat([torch.zeros((batch, 1), dtype=bool), context_mask],dim=1) # [batch, 1+T]\n",
        "\n",
        "        # x = x.repeat(4,1,1)\n",
        "        # context_mask = context_mask.repeat(4,1)\n",
        "        sx = self.context_encoder(x, src_key_padding_mask=context_mask).repeat(M,1,1)\n",
        "        sy_ = self.predicter(sx, cls_mask=target_mask)*target_mask.unsqueeze(-1)\n",
        "        # sy_ = self.predicter(sx, cls_mask=target_mask)\n",
        "        # print(sy_.shape, target_mask.shape)\n",
        "        sy = self.target_encoder(x).repeat(M,1,1)*target_mask.unsqueeze(-1)\n",
        "\n",
        "        loss = F.smooth_l1_loss(sy, sy_)\n",
        "        return loss\n",
        "\n",
        "    # def forward(self, hr, temp, heart): # [batch, 1+T]\n",
        "    #     # batch = hr.size(0)\n",
        "    #     # batch, seq, dim = x.shape\n",
        "    #     res_id, activeKilocalories, steps = hr[:,0], temp[:,0], heart[:,0]\n",
        "    #     enc_summary = self.enc0(torch.cat([self.calorie_emb(activeKilocalories), self.steps_emb(steps)], dim=-1)) # [batch, d_model]\n",
        "    #     x = self.enc1(torch.cat([self.hour_emb(hr[:,1:]/24), self.temp_emb(temp[:,1:]), self.heart_emb(heart[:,1:])], dim=-1)) # [batch, T, d_model]\n",
        "\n",
        "    #     x = torch.cat([enc_summary.unsqueeze(1), x], dim=1)\n",
        "    #     # sx = self.context_encoder(x)\n",
        "    #     sx = self.target_encoder(x)\n",
        "    #     out = sx.mean(dim=1)\n",
        "    #     return out\n",
        "\n",
        "\n",
        "\n",
        "seq_jepa = SeqJEPA(in_dim=16, d_model=32, out_dim=16, num_layers=1).to(device)#.to(torch.float)\n",
        "soptim = torch.optim.AdamW(seq_jepa.parameters(), lr=1e-3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bNjp4xFsGPoa"
      },
      "outputs": [],
      "source": [
        "# @title ijepa vision_transformer.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/models/vision_transformer.py\n",
        "import math\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from src.utils.tensors import (trunc_normal_, repeat_interleave_batch)\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid height and width\n",
        "    return:\n",
        "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid_h = np.arange(grid_size, dtype=float)\n",
        "    grid_w = np.arange(grid_size, dtype=float)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    assert embed_dim % 2 == 0\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid length\n",
        "    return:\n",
        "    pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid = np.arange(grid_size, dtype=float)\n",
        "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=float)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega   # (D/2,)\n",
        "    pos = pos.reshape(-1)   # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
        "    emb = np.concatenate([np.sin(out), np.cos(out)], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x, attn\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x, return_attention=False):\n",
        "        y, attn = self.attn(self.norm1(x))\n",
        "        if return_attention:\n",
        "            return attn\n",
        "        x = x + self.drop_path(y)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvEmbed(nn.Module):\n",
        "    \"\"\"3x3 Convolution stems for ViT following ViTC models\"\"\"\n",
        "    def __init__(self, channels, strides, img_size=224, in_chans=3, batch_norm=True):\n",
        "        super().__init__()\n",
        "        # Build the stems\n",
        "        stem = []\n",
        "        channels = [in_chans] + channels\n",
        "        for i in range(len(channels) - 2):\n",
        "            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3, stride=strides[i], padding=1, bias=(not batch_norm))]\n",
        "            if batch_norm:\n",
        "                stem += [nn.BatchNorm2d(channels[i+1])]\n",
        "            stem += [nn.ReLU(inplace=True)]\n",
        "        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n",
        "        self.stem = nn.Sequential(*stem)\n",
        "\n",
        "        # Comptute the number of patches\n",
        "        stride_prod = int(np.prod(strides))\n",
        "        self.num_patches = (img_size[0] // stride_prod)**2\n",
        "\n",
        "    def forward(self, x):\n",
        "        p = self.stem(x)\n",
        "        return p.flatten(2).transpose(1, 2)\n",
        "\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "def apply_masks(x, masks):\n",
        "    \"\"\":param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
        "    :param masks: list of tensors containing indices of patches in [N] to keep\"\"\"\n",
        "    all_x = []\n",
        "    for m in masks:\n",
        "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [batch,T,dim]\n",
        "        all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "    return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "\n",
        "class VisionTransformerPredictor(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, num_patches, embed_dim=768, predictor_embed_dim=384,\n",
        "        depth=6, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.init_std = init_std\n",
        "        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        # --\n",
        "        self.predictor_pos_embed = nn.Parameter(torch.zeros(1, num_patches, predictor_embed_dim), requires_grad=False)\n",
        "        predictor_pos_embed = get_2d_sincos_pos_embed(self.predictor_pos_embed.shape[-1], int(num_patches**.5), cls_token=False)\n",
        "        self.predictor_pos_embed.data.copy_(torch.from_numpy(predictor_pos_embed).float().unsqueeze(0)) # [1, num_patches, predictor_embed_dim]\n",
        "        # --\n",
        "        self.predictor_blocks = nn.ModuleList([Block(dim=predictor_embed_dim, num_heads=num_heads, drop=drop_rate, drop_path=dpr[i]) for i in range(depth)])\n",
        "        self.predictor_norm = nn.LayerNorm(predictor_embed_dim)\n",
        "        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n",
        "        # ------\n",
        "        # trunc_normal_(self.mask_token, std=self.init_std)\n",
        "\n",
        "    def forward(self, x, masks_x, masks):\n",
        "        assert (masks is not None) and (masks_x is not None), 'Cannot run predictor without mask indices'\n",
        "        if not isinstance(masks_x, list): masks_x = [masks_x]\n",
        "        if not isinstance(masks, list): masks = [masks]\n",
        "\n",
        "        # -- Batch Size\n",
        "        B = len(x) // len(masks_x)\n",
        "\n",
        "        # -- map from encoder-dim to pedictor-dim\n",
        "        x = self.predictor_embed(x) # [batch, num_patches, predictor_embed_dim]\n",
        "\n",
        "        # -- add positional embedding to x tokens\n",
        "        x_pos_embed = self.predictor_pos_embed.repeat(B, 1, 1) # [batch, num_patches, predictor_embed_dim]\n",
        "        x += apply_masks(x_pos_embed, masks_x)\n",
        "\n",
        "        _, N_ctxt, D = x.shape\n",
        "\n",
        "        # -- concat mask tokens to x\n",
        "        pos_embs = self.predictor_pos_embed.repeat(B, 1, 1)\n",
        "        pos_embs = apply_masks(pos_embs, masks)\n",
        "        pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x))\n",
        "        # --\n",
        "        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1)\n",
        "        # --\n",
        "        pred_tokens += pos_embs\n",
        "        x = x.repeat(len(masks), 1, 1)\n",
        "        x = torch.cat([x, pred_tokens], dim=1)\n",
        "\n",
        "        # -- fwd prop\n",
        "        for blk in self.predictor_blocks:\n",
        "            x = blk(x)\n",
        "        x = self.predictor_norm(x)\n",
        "\n",
        "        # -- return preds for mask tokens\n",
        "        x = x[:, N_ctxt:]\n",
        "        x = self.predictor_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, img_size=[224], patch_size=16, in_chans=3, embed_dim=768, predictor_embed_dim=384, depth=12, predictor_depth=12,\n",
        "        num_heads=12, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0, norm_layer=nn.LayerNorm, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_features = self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.init_std = init_std\n",
        "        # --\n",
        "        self.patch_embed = PatchEmbed(img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        # --\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=False)\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "        # --\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "    def forward(self, x, masks=None):\n",
        "        if masks is not None:\n",
        "            if not isinstance(masks, list): masks = [masks]\n",
        "\n",
        "        x = self.patch_embed(x)\n",
        "        B, N, D = x.shape # [batch, T, d_model]\n",
        "\n",
        "        # -- add positional embedding to x\n",
        "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
        "        x = x + pos_embed\n",
        "\n",
        "        if masks is not None: x = apply_masks(x, masks)\n",
        "\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            x = blk(x)\n",
        "\n",
        "        if self.norm is not None: x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "    def interpolate_pos_encoding(self, x, pos_embed):\n",
        "        npatch = x.shape[1] - 1\n",
        "        N = pos_embed.shape[1] - 1\n",
        "        if npatch == N: return pos_embed\n",
        "        class_emb, pos_embed = pos_embed[:, 0], pos_embed[:, 1:]\n",
        "        dim = x.shape[-1]\n",
        "        pos_embed = nn.functional.interpolate(pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2), scale_factor=math.sqrt(npatch / N), mode='bicubic',)\n",
        "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
        "\n",
        "# from functools import partial\n",
        "# def vit_predictor(**kwargs):\n",
        "#     model = VisionTransformerPredictor(\n",
        "#         mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "#     return model\n",
        "\n",
        "def vit(patch_size=16, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, # tiny\n",
        "        # patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, # small\n",
        "        # patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, # base\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tf4T37woBV2V"
      },
      "outputs": [],
      "source": [
        "# @title show collated_masks\n",
        "# print(collated_batch, collated_masks_enc, collated_masks_pred)\n",
        "# print(collated_batch) # tensor([0, 1, 2, 3]) batch\n",
        "# print(collated_masks_enc) # nenc*[M*[pred mask blocks patch ind]]\n",
        "# print(collated_masks_pred) # npred * [M * [context]]\n",
        "\n",
        "\n",
        "# print(len(collated_masks_enc[0]), len(collated_masks_pred[0]))\n",
        "# print(collated_masks_enc[0], collated_masks_pred[0])\n",
        "\n",
        "h,w=14,14\n",
        "img = torch.ones(h*w)\n",
        "# img[collated_masks_enc[0]]=0\n",
        "img[collated_masks_pred[0][2]]=0\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    # plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.imshow(npimg)\n",
        "    plt.show()\n",
        "\n",
        "# imshow(img.reshape(h,w))\n",
        "\n",
        "# for masks in collated_masks_pred:\n",
        "#     for mask in masks:\n",
        "#         img = torch.ones(h*w)\n",
        "#         img[mask]=0\n",
        "#         imshow(img.reshape(h,w))\n",
        "\n",
        "\n",
        "for masks in collated_masks_enc:\n",
        "    for mask in masks:\n",
        "        img = torch.ones(h*w)\n",
        "        img[mask]=0\n",
        "        imshow(img.reshape(h,w))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aH1UOD4WW8I2",
        "outputId": "6fafea1b-0af2-4bc7-fa26-46b5e56549b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vit fwd torch.Size([4, 3, 224, 224])\n",
            "vit fwd torch.Size([4, 196, 768])\n",
            "vit fwd torch.Size([4, 11, 768])\n",
            "predictor fwd1 torch.Size([4, 11, 384]) torch.Size([4, 196, 384])\n"
          ]
        }
      ],
      "source": [
        "# @title ijepa vision_transformer.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/models/vision_transformer.py\n",
        "import math\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# from src.utils.tensors import (trunc_normal_, repeat_interleave_batch)\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid height and width\n",
        "    return:\n",
        "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid_h = np.arange(grid_size, dtype=float)\n",
        "    grid_w = np.arange(grid_size, dtype=float)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    assert embed_dim % 2 == 0\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid length\n",
        "    return:\n",
        "    pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid = np.arange(grid_size, dtype=float)\n",
        "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=float)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega   # (D/2,)\n",
        "    pos = pos.reshape(-1)   # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
        "    emb = np.concatenate([np.sin(out), np.cos(out)], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "def repeat_interleave_batch(x, B, repeat):\n",
        "    N = len(x) // B\n",
        "    x = torch.cat([\n",
        "        torch.cat([x[i*B:(i+1)*B] for _ in range(repeat)], dim=0)\n",
        "        for i in range(N)\n",
        "    ], dim=0)\n",
        "    return x\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x, attn\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x, return_attention=False):\n",
        "        y, attn = self.attn(self.norm1(x))\n",
        "        if return_attention:\n",
        "            return attn\n",
        "        x = x + self.drop_path(y)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvEmbed(nn.Module):\n",
        "    \"\"\"3x3 Convolution stems for ViT following ViTC models\"\"\"\n",
        "    def __init__(self, channels, strides, img_size=224, in_chans=3, batch_norm=True):\n",
        "        super().__init__()\n",
        "        # Build the stems\n",
        "        stem = []\n",
        "        channels = [in_chans] + channels\n",
        "        for i in range(len(channels) - 2):\n",
        "            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3, stride=strides[i], padding=1, bias=(not batch_norm))]\n",
        "            if batch_norm:\n",
        "                stem += [nn.BatchNorm2d(channels[i+1])]\n",
        "            stem += [nn.ReLU(inplace=True)]\n",
        "        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n",
        "        self.stem = nn.Sequential(*stem)\n",
        "\n",
        "        # Comptute the number of patches\n",
        "        stride_prod = int(np.prod(strides))\n",
        "        self.num_patches = (img_size[0] // stride_prod)**2\n",
        "\n",
        "    def forward(self, x):\n",
        "        p = self.stem(x)\n",
        "        return p.flatten(2).transpose(1, 2)\n",
        "\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "def apply_masks(x, masks):\n",
        "    \"\"\":param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
        "    :param masks: list of tensors containing indices of patches in [N] to keep\"\"\"\n",
        "    all_x = []\n",
        "    for m in masks:\n",
        "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [batch,T,dim]\n",
        "        all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "    return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "\n",
        "class VisionTransformerPredictor(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, num_patches, embed_dim=768, predictor_embed_dim=384,\n",
        "        depth=6, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.init_std = init_std\n",
        "        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        # --\n",
        "        self.predictor_pos_embed = nn.Parameter(torch.zeros(1, num_patches, predictor_embed_dim), requires_grad=False)\n",
        "        predictor_pos_embed = get_2d_sincos_pos_embed(self.predictor_pos_embed.shape[-1], int(num_patches**.5), cls_token=False)\n",
        "        self.predictor_pos_embed.data.copy_(torch.from_numpy(predictor_pos_embed).float().unsqueeze(0)) # [1, num_patches, predictor_embed_dim]\n",
        "        # --\n",
        "        self.predictor_blocks = nn.ModuleList([Block(dim=predictor_embed_dim, num_heads=num_heads, drop=drop_rate, drop_path=dpr[i]) for i in range(depth)])\n",
        "        self.predictor_norm = nn.LayerNorm(predictor_embed_dim)\n",
        "        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n",
        "        # ------\n",
        "        # trunc_normal_(self.mask_token, std=self.init_std)\n",
        "\n",
        "    def forward(self, x, masks_x, masks): # [batch, num_context_patches, embed_dim], nenc*[batch, num_context_patches], npred*[batch, num_target_patches]\n",
        "        assert (masks is not None) and (masks_x is not None), 'Cannot run predictor without mask indices'\n",
        "        if not isinstance(masks_x, list): masks_x = [masks_x] # context mask\n",
        "        if not isinstance(masks, list): masks = [masks] # pred mask\n",
        "\n",
        "        # -- Batch Size\n",
        "        B = len(x) // len(masks_x)\n",
        "        # print(\"predictor fwd0\", x.shape) # [3, 121, 768])\n",
        "        # -- map from encoder-dim to pedictor-dim\n",
        "        x = self.predictor_embed(x) # [batch, num_patches, predictor_embed_dim]\n",
        "\n",
        "        # -- add positional embedding to x tokens\n",
        "        x_pos_embed = self.predictor_pos_embed.repeat(B, 1, 1) # [batch, num_patches, predictor_embed_dim]\n",
        "        print(\"predictor fwd1\", x.shape, x_pos_embed.shape) # [3, 121 or 11, 384], [3, 196, 384]\n",
        "        # print(\"predictor fwd masks_x\", masks_x)\n",
        "        x += apply_masks(x_pos_embed, masks_x) # get pos emb of context patches\n",
        "\n",
        "        # print(\"predictor fwd x\", x.shape) # [4, 11, 384])\n",
        "        _, N_ctxt, D = x.shape\n",
        "\n",
        "        # -- concat mask tokens to x\n",
        "        pos_embs = self.predictor_pos_embed.repeat(B, 1, 1) # [batch, num_patches, predictor_embed_dim]\n",
        "        pos_embs = apply_masks(pos_embs, masks) # [16, 104, predictor_embed_dim]\n",
        "        pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x)) # [16, 104, predictor_embed_dim]\n",
        "        # --\n",
        "        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1) # [16, 104, predictor_embed_dim]\n",
        "        # --\n",
        "        # print(\"predictor fwd pred_tokens\", pred_tokens.shape)\n",
        "\n",
        "        pred_tokens += pos_embs\n",
        "        x = x.repeat(len(masks), 1, 1)\n",
        "        x = torch.cat([x, pred_tokens], dim=1)\n",
        "\n",
        "        # -- fwd prop\n",
        "        for blk in self.predictor_blocks:\n",
        "            x = blk(x)\n",
        "        x = self.predictor_norm(x)\n",
        "\n",
        "        # -- return preds for mask tokens\n",
        "        x = x[:, N_ctxt:]\n",
        "        x = self.predictor_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, img_size=[224], patch_size=16, in_chans=3, embed_dim=768, predictor_embed_dim=384, depth=12, predictor_depth=12,\n",
        "        num_heads=12, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0, norm_layer=nn.LayerNorm, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_features = self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.init_std = init_std\n",
        "        # --\n",
        "        self.patch_embed = PatchEmbed(img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        # --\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=False)\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "        # --\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "    def forward(self, x, masks=None): # [batch,3,224,224]\n",
        "        if masks is not None:\n",
        "            if not isinstance(masks, list): masks = [masks]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        x = self.patch_embed(x)\n",
        "        B, N, D = x.shape # [batch, num_patches, embed_dim]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        # -- add positional embedding to x\n",
        "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
        "        x = x + pos_embed\n",
        "\n",
        "        if masks is not None: x = apply_masks(x, masks) # [batch, num_context_patches, embed_dim]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            x = blk(x)\n",
        "\n",
        "        if self.norm is not None: x = self.norm(x)\n",
        "        return x # [batch, num_context_patches, embed_dim]\n",
        "\n",
        "    def interpolate_pos_encoding(self, x, pos_embed):\n",
        "        npatch = x.shape[1] - 1\n",
        "        N = pos_embed.shape[1] - 1\n",
        "        if npatch == N: return pos_embed\n",
        "        class_emb, pos_embed = pos_embed[:, 0], pos_embed[:, 1:]\n",
        "        dim = x.shape[-1]\n",
        "        pos_embed = nn.functional.interpolate(pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2), scale_factor=math.sqrt(npatch / N), mode='bicubic',)\n",
        "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
        "\n",
        "# from functools import partial\n",
        "# def vit_predictor(**kwargs):\n",
        "#     model = VisionTransformerPredictor(\n",
        "#         mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "#     return model\n",
        "\n",
        "def vit(patch_size=16, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, # tiny\n",
        "        # patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, # small\n",
        "        # patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, # base\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "# encoder = vit()\n",
        "encoder = VisionTransformer(\n",
        "        patch_size=16, embed_dim=768, depth=1, num_heads=3, mlp_ratio=1,\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
        "\n",
        "# num_patches = (224/patch_size)^2\n",
        "# predictor = VisionTransformerPredictor(num_patches, embed_dim=768, predictor_embed_dim=384, depth=6, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02, **kwargs)\n",
        "predictor = VisionTransformerPredictor(num_patches=196, embed_dim=768, predictor_embed_dim=384, depth=1, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02)\n",
        "\n",
        "batch = 4\n",
        "img = torch.rand(batch, 3, 224, 224)\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/train.py\n",
        "mask_collator = MaskCollator(\n",
        "        input_size=(224, 224),\n",
        "        patch_size=16,\n",
        "        enc_mask_scale=(0.2, 0.8),\n",
        "        pred_mask_scale=(0.2, 0.8),\n",
        "        aspect_ratio=(0.3, 3.0),\n",
        "        nenc=1,\n",
        "        npred=4,\n",
        "        min_keep=4,\n",
        "        # allow_overlap=True)\n",
        "        allow_overlap=False)\n",
        "# self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
        "\n",
        "\n",
        "\n",
        "# v = mask_collator.step()\n",
        "# should pass the collater a list batch of idx?\n",
        "# collated_batch, collated_masks_enc, collated_masks_pred = mask_collator([batch,3,8])\n",
        "collated_batch, collated_masks_enc, collated_masks_pred = mask_collator([0,1,2,3])\n",
        "# print(v)\n",
        "\n",
        "\n",
        "import copy\n",
        "target_encoder = copy.deepcopy(encoder)\n",
        "\n",
        "\n",
        "def forward_target():\n",
        "    with torch.no_grad():\n",
        "        h = target_encoder(imgs)\n",
        "        h = F.layer_norm(h, (h.size(-1),))  # normalize over feature-dim\n",
        "        B = len(h)\n",
        "        # -- create targets (masked regions of h)\n",
        "        h = apply_masks(h, masks_pred)\n",
        "        h = repeat_interleave_batch(h, B, repeat=len(masks_enc))\n",
        "        return h\n",
        "\n",
        "def forward_context():\n",
        "    z = encoder(imgs, masks_enc)\n",
        "    z = predictor(z, masks_enc, masks_pred)\n",
        "    return z\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    m = next(momentum_scheduler)\n",
        "                    for param_q, param_k in zip(encoder.parameters(), target_encoder.parameters()):\n",
        "                        param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "\n",
        "\n",
        "# # num_context_patches = 121 if allow_overlap=True else 11\n",
        "z = encoder(img, collated_masks_enc) # [batch, num_context_patches, embed_dim]\n",
        "# print(len(collated_masks_enc), len(collated_masks_pred)) # nenc, npred\n",
        "# print(collated_masks_enc[0].shape, collated_masks_pred[0].shape) # , [batch, num_context_patches = 121 or 11], [batch, num_target_patches]\n",
        "out = predictor(z, collated_masks_enc, collated_masks_pred)\n",
        "# # print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "id": "NLHOwdqK222R",
        "outputId": "a211c62f-141d-461a-ecfe-70bd061d6194"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([128, 3, 32, 32])\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# @title RandomResizedCrop is same across batch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# dataiter = iter(train_data)\n",
        "dataiter = iter(train_loader)\n",
        "img,y = next(dataiter)\n",
        "# img = img.unsqueeze(0)\n",
        "# # img = F.interpolate(img, (8,8))\n",
        "# b,c,h,w = img.shape\n",
        "# img = img.flatten(2).transpose(-2,-1).to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "print(img.shape)\n",
        "\n",
        "from torchvision.transforms import v2\n",
        "resize_cropper = v2.RandomResizedCrop(size=(32, 32))\n",
        "resized_crops = resize_cropper(img)\n",
        "\n",
        "# imshow(img[0])\n",
        "# imshow(resized_crops[0])\n",
        "# imshow(img[1])\n",
        "# imshow(resized_crops[1])\n",
        "\n",
        "import torchvision\n",
        "imshow(torchvision.utils.make_grid(img[:8].cpu(), nrow=4))\n",
        "imshow(torchvision.utils.make_grid(resized_crops[:8].cpu(), nrow=4))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 879
        },
        "id": "D6lVtbS5OHIv",
        "outputId": "13b6ad36-0c36-4d1e-daf0-70b098b51a2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 1024, 3])\n",
            "torch.Size([1024]) torch.Size([1, 1024])\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# @title test multiblock params\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "dataiter = iter(train_data)\n",
        "img,y = next(dataiter)\n",
        "img = img.unsqueeze(0)\n",
        "b,c,h,w = img.shape\n",
        "img = img.flatten(2).transpose(-2,-1).to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "print(img.shape)\n",
        "batch, seq,_ = img.shape\n",
        "\n",
        "\n",
        "# target_mask = randpatch(seq, mask_size=8, gamma=.9) # 8.9 [seq]\n",
        "target_mask = multiblock(seq, min_s=0.2, max_s=0.3, M=4).any(0) # best.2.3M4 og.15.2M4# mask out targets to be predicted # [M, seq]\n",
        "target_mask = multiblock(seq, min_s=0.2, max_s=0.3, M=4).any(0)\n",
        "\n",
        "context_mask = ~multiblock(seq, min_s=0.85, max_s=1., M=1)|target_mask # [1, seq], True->Mask\n",
        "\n",
        "# print(target_mask, context_mask)\n",
        "\n",
        "print(target_mask.shape, context_mask.shape)\n",
        "# target_img, context_img = img*target_mask.unsqueeze(-1), img*context_mask.unsqueeze(-1)\n",
        "target_img, context_img = img*~target_mask.unsqueeze(-1), img*~context_mask.unsqueeze(-1)\n",
        "# print(target_img.shape, context_img.shape)\n",
        "target_img, context_img = target_img.transpose(-2,-1).reshape(b,c,h,w), context_img.transpose(-2,-1).reshape(b,c,h,w)\n",
        "target_img, context_img = target_img.float(), context_img.float()\n",
        "\n",
        "# imshow(out.detach().cpu())\n",
        "imshow(target_img[0])\n",
        "imshow(context_img[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiHDuPjB0SBt"
      },
      "outputs": [],
      "source": [
        "\n",
        "trg_msks = []\n",
        "ctx_msks = []\n",
        "for i in range(16):\n",
        "    # target_mask = randpatch(seq, mask_size=8, gamma=.9) # 8.9 [seq]\n",
        "    target_mask = multiblock(seq, min_s=0.2, max_s=0.3, M=4).any(0) # best.2.3M4 og.15.2M4# mask out targets to be predicted # [M, seq]\n",
        "    target_mask = multiblock(seq, min_s=0.2, max_s=0.3, M=4).any(0)\n",
        "\n",
        "    context_mask = ~multiblock(seq, min_s=0.85, max_s=1., M=1)|target_mask # [1, seq], True->Mask\n",
        "\n",
        "    # print(target_mask.shape, context_mask.shape)\n",
        "    target_img, context_img = img*~target_mask.unsqueeze(-1), img*~context_mask.unsqueeze(-1)\n",
        "    target_img, context_img = target_img.transpose(-2,-1).reshape(b,c,h,w).float(), context_img.transpose(-2,-1).reshape(b,c,h,w).float()\n",
        "    trg_msks.append(target_img)\n",
        "    ctx_msks.append(context_img)\n",
        "\n",
        "trg_msks = torch.cat(trg_msks, dim=0)\n",
        "ctx_msks = torch.cat(ctx_msks, dim=0)\n",
        "imshow(torchvision.utils.make_grid(trg_msks.cpu(), nrow=4))\n",
        "imshow(torchvision.utils.make_grid(ctx_msks.cpu(), nrow=4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8xv1tpKIhim",
        "outputId": "e6613fee-2d32-4c6a-ef96-dee18da3aa6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([73, 73, 73, 73])\n",
            "tensor([248])\n"
          ]
        }
      ],
      "source": [
        "# @title test multiblock mask\n",
        "def multiblock(seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    # mask_len, mask_pos = mask_len * seq, mask_pos * seq\n",
        "    indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "    return target_mask\n",
        "\n",
        "\n",
        "seq=400\n",
        "M=4\n",
        "\n",
        "# target_mask = multiblock(M, seq, min_s=0.15, max_s=0.2, M=1) # mask out targets to be predicted # [4*batch, seq]\n",
        "# context_mask = ~multiblock(1, seq, min_s=0.85, max_s=1., M=1)|target_mask.reshape(M,seq).any(0) # [batch, seq]\n",
        "\n",
        "\n",
        "target_mask = multiblock(seq, min_s=0.15, max_s=0.2, M=4) # mask out targets to be predicted # [4*batch, seq]\n",
        "context_mask = ~multiblock(seq, min_s=0.85, max_s=1., M=1)|target_mask.reshape(M,seq).any(0) # [batch, seq]\n",
        "\n",
        "# target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "\n",
        "# print(target_mask)\n",
        "# print(context_mask)\n",
        "\n",
        "print(target_mask.sum(-1))\n",
        "print(context_mask.sum(-1))\n",
        "\n",
        "\n",
        "# sorted_mask, ids = context_mask.sort(dim=1, stable=True)\n",
        "# indices = ids[~sorted_mask]#.reshape(M,-1) # int idx [num_context_toks] , idx of context not masked\n",
        "# sorted_x = x[torch.arange(batch).unsqueeze(-1), indices] # [batch, seq-num_trg_toks, dim]\n",
        "# print(indices)\n",
        "# print(x)\n",
        "# print(sorted_x)\n",
        "\n",
        "\n",
        "\n",
        "# sorted_mask, ids = target_mask.sort(dim=1, descending=False, stable=True)\n",
        "# # print(sorted_mask, ids)\n",
        "# indices = ids[~sorted_mask].reshape(M,-1) # int idx [M, seq-num_trg_toks] , idx of target not masked\n",
        "# print(indices)\n",
        "# batch=3\n",
        "# dim=2\n",
        "# # indices = indices.expand(batch,-1,-1)\n",
        "# # x = torch.arange(batch*seq).reshape(batch,seq).to(device)\n",
        "# x = torch.rand(batch, seq, dim)\n",
        "# print(x)\n",
        "# sorted_x = x[torch.arange(batch)[...,None,None], indices]\n",
        "# print(sorted_x.shape) # [batch, M, seq-num_trg_toks, dim]\n",
        "# print(sorted_x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hxzeZzdYm9C",
        "outputId": "2e61c465-c5dc-445b-a985-f6b442bfbd96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1.])\n"
          ]
        }
      ],
      "source": [
        "img = torch.ones(200)\n",
        "img[collated_masks_enc[0][0]] = 0\n",
        "print(img)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "1KUcAQEgKreY",
        "outputId": "288bb97f-e8a9-4ae5-c00c-3ec9563d6e02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "        [0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
            "        [0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
            "        [0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1.],\n",
            "        [0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1.],\n",
            "        [0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1.],\n",
            "        [0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "        [0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# @title ijepa multiblock next\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "import math\n",
        "from multiprocessing import Value\n",
        "import torch\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(self, input_size=(224, 224),\n",
        "        enc_mask_scale=(0.2, 0.8), pred_mask_scale=(0.2, 0.8), aspect_ratio=(0.3, 3.0),\n",
        "        nenc=1, npred=2, min_keep=4, allow_overlap=False):\n",
        "        super().__init__()\n",
        "        if not isinstance(input_size, tuple): input_size = (input_size,) * 2\n",
        "        self.height, self.width = input_size\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.aspect_ratio = aspect_ratio\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "\n",
        "    def _sample_block_size(self, scale, aspect_ratio_scale):\n",
        "        _rand = torch.rand(1).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.height * self.width * mask_scale) # num patches to keep\n",
        "        # -- Sample block aspect-ratio\n",
        "        min_ar, max_ar = aspect_ratio_scale\n",
        "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
        "        while h >= self.height: h -= 1 # crop mask to be smaller than img\n",
        "        while w >= self.width: w -= 1\n",
        "        return (h, w)\n",
        "\n",
        "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
        "        h, w = b_size\n",
        "\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # --\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            top = torch.randint(0, self.height - h, (1,))\n",
        "            left = torch.randint(0, self.width - w, (1,))\n",
        "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
        "            mask[top:top+h, left:left+w] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
        "        mask_complement[top:top+h, left:left+w] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, B):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        p_size = self._sample_block_size(scale=self.pred_mask_scale, aspect_ratio_scale=self.aspect_ratio)\n",
        "        e_size = self._sample_block_size(scale=self.enc_mask_scale, aspect_ratio_scale=(1., 1.))\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.height * self.width\n",
        "        min_keep_enc = self.height * self.width\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                print(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "        return collated_masks_enc, collated_masks_pred\n",
        "\n",
        "mask_collator = MaskCollator(input_size=(224, 224), enc_mask_scale=(.85, 1.), pred_mask_scale=(.15, .2),\n",
        "        aspect_ratio=(.75, 1.5), nenc=1, npred=4, min_keep=4,\n",
        "        # allow_overlap=True)\n",
        "        allow_overlap=False)\n",
        "\n",
        "collated_masks_enc, collated_masks_pred = mask_collator(4)\n",
        "\n",
        "img = torch.ones((224//16)**2)\n",
        "img[collated_masks_enc[0]] = 0\n",
        "img = img.reshape(224//16, 224//16)\n",
        "# imshow(img)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    # plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "print(img)\n",
        "display(imshow(img))\n",
        "# imshow()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwBmCMXVgPZj",
        "outputId": "ce487f85-3f64-4a60-e3da-3cc957b71178"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[torch.Size([4, 63])] [torch.Size([4, 35]), torch.Size([4, 35]), torch.Size([4, 35]), torch.Size([4, 35])]\n"
          ]
        }
      ],
      "source": [
        "print([c.shape for c in collated_masks_enc], [c.shape for c in collated_masks_pred])\n",
        "\n",
        "# print(collated_masks_enc[0], collated_masks_pred[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "COvEnBXPYth3"
      },
      "outputs": [],
      "source": [
        "# @title ijepa multiblock down\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "import math\n",
        "from multiprocessing import Value\n",
        "import torch\n",
        "_GLOBAL_SEED = 0\n",
        "\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(self, input_size=(224, 224), patch_size=16,\n",
        "        enc_mask_scale=(0.2, 0.8), pred_mask_scale=(0.2, 0.8), aspect_ratio=(0.3, 3.0),\n",
        "        nenc=1, npred=2, min_keep=4, allow_overlap=False):\n",
        "        super().__init__()\n",
        "        if not isinstance(input_size, tuple): input_size = (input_size,) * 2\n",
        "        self.patch_size = patch_size\n",
        "        self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.aspect_ratio = aspect_ratio\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "        self._itr_counter = Value('i', -1)  # collator is shared across worker processes\n",
        "\n",
        "    def step(self):\n",
        "        i = self._itr_counter\n",
        "        with i.get_lock():\n",
        "            i.value += 1\n",
        "            v = i.value\n",
        "        return v\n",
        "\n",
        "    def _sample_block_size(self, generator, scale, aspect_ratio_scale):\n",
        "        _rand = torch.rand(1, generator=generator).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.height * self.width * mask_scale)\n",
        "        # -- Sample block aspect-ratio\n",
        "        min_ar, max_ar = aspect_ratio_scale\n",
        "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
        "        while h >= self.height:\n",
        "            h -= 1\n",
        "        while w >= self.width:\n",
        "            w -= 1\n",
        "        return (h, w)\n",
        "\n",
        "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
        "        h, w = b_size\n",
        "\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # --\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            top = torch.randint(0, self.height - h, (1,))\n",
        "            left = torch.randint(0, self.width - w, (1,))\n",
        "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
        "            mask[top:top+h, left:left+w] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
        "        mask_complement[top:top+h, left:left+w] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        B = len(batch)\n",
        "\n",
        "        collated_batch = torch.utils.data.default_collate(batch)\n",
        "\n",
        "        seed = self.step()\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(seed)\n",
        "        p_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.pred_mask_scale,\n",
        "            aspect_ratio_scale=self.aspect_ratio)\n",
        "        e_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.enc_mask_scale,\n",
        "            aspect_ratio_scale=(1., 1.))\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.height * self.width\n",
        "        min_keep_enc = self.height * self.width\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                print(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "        return collated_batch, collated_masks_enc, collated_masks_pred\n",
        "\n",
        "# mask_collator = MaskCollator(\n",
        "#         input_size=(224, 224),\n",
        "#         patch_size=16,\n",
        "#         enc_mask_scale=(0.2, 0.8),\n",
        "#         pred_mask_scale=(0.2, 0.8),\n",
        "#         aspect_ratio=(0.3, 3.0),\n",
        "#         nenc=1,\n",
        "#         npred=4,\n",
        "#         min_keep=4,\n",
        "#         # allow_overlap=True)\n",
        "#         allow_overlap=False)\n",
        "# self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
        "\n",
        "mask_collator = MaskCollator(\n",
        "        input_size=(1, 224),\n",
        "        patch_size=1,\n",
        "        enc_mask_scale=(0.2, 0.8),\n",
        "        pred_mask_scale=(0.15, 0.2),\n",
        "        # aspect_ratio=(1, 1000),\n",
        "        aspect_ratio=(.001, 1000),\n",
        "        nenc=1,\n",
        "        npred=4,\n",
        "        min_keep=4,\n",
        "        # allow_overlap=True)\n",
        "        allow_overlap=False)\n",
        "\n",
        "# v = mask_collator.step()\n",
        "# should pass the collater a list batch of idx?\n",
        "# collated_batch, collated_masks_enc, collated_masks_pred = mask_collator([batch,3,8])\n",
        "collated_batch, collated_masks_enc, collated_masks_pred = mask_collator([0,1,2,3])\n",
        "# print(v)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DxjYI9RMQoP"
      },
      "source": [
        "## trash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "npc_xGtOz7DC"
      },
      "outputs": [],
      "source": [
        "# @title (Learned)RotEmb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, seq_len=512, base=10000):\n",
        "        super().__init__()\n",
        "        self.dim, self.base = dim, base\n",
        "        theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "        pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "        angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "        self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.base)\n",
        "        return x * self.rot_emb[:seq_len]\n",
        "\n",
        "# class LearnedRoPE(nn.Module): # learnt RoPE ; each tok is 1 pos\n",
        "#     def __init__(self, dim):\n",
        "#         super().__init__()\n",
        "#         self.weights = nn.Parameter(torch.randn(1, dim//2))\n",
        "\n",
        "#     def forward(self, x): #\n",
        "#         batch, seq_len, dim = x.shape\n",
        "#         # if rot_emb.shape[0] < seq_len: self.__init__(dim, seq_len)\n",
        "#         pos = torch.arange(seq_len).unsqueeze(1)\n",
        "#         angles = (self.weights * pos * 2*torch.pi).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "#         rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "#         return x * rot_emb.flatten(-2).unsqueeze(0)\n",
        "\n",
        "\n",
        "class LearnedRoPE(nn.Module): # learnt RoPE ; each tok is 1 pos\n",
        "    def __init__(self, dim, seq_len=512):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.weights = nn.Parameter(torch.randn(1, dim//2))\n",
        "        pos = torch.arange(seq_len).unsqueeze(1)\n",
        "        angles = (self.weights * pos * 2*torch.pi)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "        self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "    def forward(self, x): # [batch, seq_len, dim]\n",
        "        seq_len = x.size(1)\n",
        "        if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len)\n",
        "        return x * self.rot_emb[:seq_len]\n",
        "\n",
        "\n",
        "class RotEmb(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, top=torch.pi, base=10000):\n",
        "        super().__init__()\n",
        "        self.theta = top / (base ** (torch.arange(0, dim, step=2, device=device) / dim))\n",
        "        # self.theta = top / (base ** torch.linspace(0, 1, dim//2, device=device))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (pos.unsqueeze(-1) * self.theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [seq_len, dim]\n",
        "\n",
        "class LearnedRotEmb(nn.Module): # pos in R\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.weights = nn.Parameter(torch.randn((1, dim//2), device=device))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (self.weights * pos.unsqueeze(-1) * 2*torch.pi).unsqueeze(-1) # [batch, 1] * [1, dim//2] -> [batch, dim//2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [batch, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [batch, dim]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bnjJHfKj1_g",
        "outputId": "2e3e10eb-bc09-4ef7-c196-8cfe54602b6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5104\n",
            "torch.Size([4, 32, 16])\n"
          ]
        }
      ],
      "source": [
        "# @title ViT me simple wisdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    \"\"\"Zero out the parameters of a module and return it.\"\"\"\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads=None, d_head=8, cond_dim=None, dropout=0.): # .1\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_head = d_head\n",
        "        self.n_heads = d_model // d_head\n",
        "        # self.d_head = d_model // n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or d_model, 2*d_model, bias=False)\n",
        "        # self.k = nn.Sequential(nn.Dropout(dropout), nn.Linear(cond_dim, d_model, bias=False))\n",
        "        # self.lin = nn.Linear(d_model, d_model)\n",
        "        self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        # self.lin = nn.Sequential(nn.Dropout(dropout), zero_module(nn.Linear(d_model, d_model)))\n",
        "        self.drop = nn.Dropout(dropout) # indp before q,k,v; after linout\n",
        "        self.scale = self.d_head ** -.5\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [batch, T, d_model]=[batch, h*w, c], [batch, num_tok, cond_dim], [batch,T]\n",
        "        batch = x.shape[0]\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        Q = self.q(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        # K = self.k(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        K, V = self.kv(cond).view(batch, -1, self.n_heads, 2*self.d_head).transpose(1, 2).chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # linear attention # Softmax(Q) @ (Softmax(K).T @ V)\n",
        "        if mask != None:\n",
        "            mask = mask[:, None, :, None] # [batch,T] -> [batch,1,T,1]\n",
        "            K, V = K.masked_fill(mask, -torch.finfo(x.dtype).max), V.masked_fill(mask, -torch.finfo(x.dtype).max)\n",
        "        Q, K = Q.softmax(dim=-1)*self.scale, K.softmax(dim=-2)\n",
        "        context = K.transpose(-2,-1) @ V # [batch, n_heads, d_head, d_head]\n",
        "        out = Q @ context # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # (quadratic) attention # Softmax(Q @ K.T) @ V\n",
        "        # attn = Q @ K.transpose(-2,-1) * self.scale # [batch, n_heads, T] # [batch, n_heads, T, T/num_tok]\n",
        "        # if mask != None: attn = attn.masked_fill(mask[:, None, :, None], -torch.finfo(attn.dtype).max) # [batch,T]->[batch,1,T,1]\n",
        "        # attention = torch.softmax(attn, dim=-1)\n",
        "        # out = self.drop(attention) @ V # [batch, n_heads, T, d_head]\n",
        "\n",
        "        out = out.transpose(1, 2).flatten(2)\n",
        "        return self.drop(self.lin(out)) # [batch, T, d_model]\n",
        "\n",
        "# if self, dont pass cond_dim in init, dont pass cond in fwd\n",
        "# Softmax(Q @ K.T) @ V ~ Softmax(Q) @ Softmax(K).T @ V\n",
        "\n",
        "# https://github.com/lucidrains/x-transformers/blob/main/x_transformers/x_transformers.py#L1855\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_head=4, cond_dim=None, ff_mult=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm1 = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.norm2 = nn.RMSNorm(d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.cond_dim = cond_dim\n",
        "        self.self = MultiHeadAttention(d_model, d_head=d_head, dropout=0)\n",
        "        if self.cond_dim!=None: self.cross = MultiHeadAttention(d_model, d_head=d_head, cond_dim=cond_dim, dropout=0)\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        ff_dim=d_model*ff_mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), nn.Dropout(dropout), nn.Linear(d_model, ff_dim), act, # ReLU GELU\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "            nn.RMSNorm(d_model), nn.Dropout(dropout), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            # nn.RMSNorm(d_model), act, nn.Dropout(dropout), nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,c,h,w], [batch, num_tok, cond_dim], [batch,T]\n",
        "        # bchw = x.shape\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "\n",
        "        if self.cond_dim==None: cond=None # is self attn\n",
        "        x = x + self.drop(self.self(self.norm1(x), cond, mask))\n",
        "        if self.cond_dim!=None: x = x + self.cross(self.norm2(x), cond, mask)\n",
        "        x = x + self.ff(x)\n",
        "        # x = x + self.drop(self.ff(x))\n",
        "\n",
        "        # return x.transpose(1,2).reshape(*bchw)\n",
        "        return x\n",
        "\n",
        "import torch\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "def apply_masks(x, masks):\n",
        "    \"\"\":param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
        "    :param masks: list of tensors containing indices of patches in [N] to keep\"\"\"\n",
        "    all_x = []\n",
        "    for m in masks: # [1,T]\n",
        "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [1,T,dim]\n",
        "        all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "    return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        # self.positional_emb = nn.Parameter(torch.zeros(1, 32*32//4, d_model)) # positional_embedding == 'learnable'\n",
        "        # encoder_layers = nn.TransformerEncoderLayer(d_model, d_model//d_head, d_hid or d_model, dropout, batch_first=True)\n",
        "        # self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.zeros(1,1,d_model)) # randn zeros\n",
        "        nn.init.trunc_normal_(self.cls, std=.02)\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model)\n",
        "        self.lin = nn.Linear(d_model, out_dim)# if out_dim != d_model else None\n",
        "\n",
        "    def forward(self, src, context_indices, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        src = self.embed(src) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = src.shape\n",
        "        src = src * self.pos_encoder(context_indices)\n",
        "        # src = src + self.positional_emb[:,context_indices]\n",
        "        # src = apply_masks(src, [context_indices])\n",
        "\n",
        "        pred_tokens = self.cls * self.pos_encoder(trg_indices) # [M, num_trg_toks, d_model]\n",
        "        # pred_tokens = self.cls + self.positional_emb[0,trg_indices]\n",
        "        pred_tokens = pred_tokens.repeat(batch, 1, 1) # [batch*M, num_trg_toks, d_model]\n",
        "        # print(pred_tokens.requires_grad)\n",
        "        # print(\"pred fwd\", src.shape, pred_tokens.shape)\n",
        "        # src = src.repeat_interleave(trg_indices.shape[0], dim=0) # [batch, seq_len, d_model] -> [batch*M, seq_len, d_model]\n",
        "        src = torch.cat([src, pred_tokens], dim=1) # [batch*M, seq_len+num_trg_toks, d_model]\n",
        "\n",
        "        out = self.transformer_encoder(src) # float [seq_len, batch_size, d_model]\n",
        "        out = self.norm(out)\n",
        "        out = out[:,seq:] # [batch*M, num_trg_toks, d_model]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.MaxPool1d(2,2), #nn.MaxPool1d(3, 2, 3//2),\n",
        "            # nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.Conv1d(d_model, d_model,3,2,3//2)\n",
        "            nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2),\n",
        "            # nn.Conv1d(in_dim, d_model,2,2,0), # like patch\n",
        "            )\n",
        "        self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        # self.positional_emb = nn.Parameter(torch.zeros(1, 32*32//4, d_model)) # positional_embedding == 'learnable'\n",
        "        # encoder_layers = nn.TransformerEncoderLayer(d_model, d_model//d_head, d_hid or d_model, dropout, batch_first=True)\n",
        "        # self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model)\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, src, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "\n",
        "        # src = self.embed(src) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        src = self.embed(src.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "\n",
        "        batch, seq, dim = src.shape\n",
        "\n",
        "        # # # src = self.pos_encoder(src)\n",
        "        # if context_indices != None:\n",
        "        # print(src.shape, self.pos_encoder(context_indices).shape)\n",
        "        #     src = src * self.pos_encoder(context_indices) # context/predictor # src = src + self.positional_emb[:,context_indices]\n",
        "        # else: src = src * self.pos_encoder(torch.arange(seq, device=device).unsqueeze(0)) # target # src = src + self.positional_emb[:,:seq]\n",
        "\n",
        "\n",
        "        # src = self.pos_encoder(src)\n",
        "        src = src * self.pos_encoder(torch.arange(seq, device=device)).unsqueeze(0)\n",
        "        # src = src + self.positional_emb\n",
        "        if context_indices != None:\n",
        "            src = apply_masks(src, [context_indices])\n",
        "\n",
        "\n",
        "        out = self.transformer_encoder(src) # float [seq_len, batch_size, d_model]\n",
        "        out = self.norm(out)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "batch, seq_len, d_model = 4,1024,16\n",
        "in_dim = 3\n",
        "model = TransformerModel(in_dim, d_model, d_head=4, nlayers=2, dropout=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "# # print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qUhlOV8_RsRi"
      },
      "outputs": [],
      "source": [
        "# @title attention save\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, seq_len=512, base=10000):\n",
        "        super().__init__()\n",
        "        self.dim, self.base = dim, base\n",
        "        theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "        pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "        angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "        self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.base)\n",
        "        return x * self.rot_emb[:seq_len]\n",
        "\n",
        "def zero_module(module):\n",
        "    \"\"\"Zero out the parameters of a module and return it.\"\"\"\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads=None, d_head=8, cond_dim=None, dropout=0.): # .1\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_head = d_head\n",
        "        self.n_heads = d_model // d_head\n",
        "        # self.d_head = d_model // n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or d_model, 2*d_model, bias=False)\n",
        "        # self.k = nn.Sequential(nn.Dropout(dropout), nn.Linear(cond_dim, d_model, bias=False))\n",
        "        # self.lin = nn.Linear(d_model, d_model)\n",
        "        self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        # self.lin = nn.Sequential(nn.Dropout(dropout), zero_module(nn.Linear(d_model, d_model)))\n",
        "        self.drop = nn.Dropout(dropout) # indp before q,k,v; after linout\n",
        "        self.scale = self.d_head ** -.5\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [batch, T, d_model]=[batch, h*w, c], [batch, num_tok, cond_dim], [batch,T]\n",
        "        batch = x.shape[0]\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        Q = self.q(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        # K = self.k(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        K, V = self.kv(cond).view(batch, -1, self.n_heads, 2*self.d_head).transpose(1, 2).chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # linear attention # Softmax(Q) @ (Softmax(K).T @ V)\n",
        "        if mask != None:\n",
        "            mask = mask[:, None, :, None] # [batch,T] -> [batch,1,T,1]\n",
        "            K, V = K.masked_fill(mask, -torch.finfo(x.dtype).max), V.masked_fill(mask, -torch.finfo(x.dtype).max)\n",
        "        Q, K = Q.softmax(dim=-1)*self.scale, K.softmax(dim=-2)\n",
        "        context = K.transpose(-2,-1) @ V # [batch, n_heads, d_head, d_head]\n",
        "        out = Q @ context # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # (quadratic) attention # Softmax(Q @ K.T) @ V\n",
        "        # attn = Q @ K.transpose(-2,-1) * self.scale # [batch, n_heads, T] # [batch, n_heads, T, T/num_tok]\n",
        "        # if mask != None: attn = attn.masked_fill(mask[:, None, :, None], -torch.finfo(attn.dtype).max) # [batch,T]->[batch,1,T,1]\n",
        "        # attention = torch.softmax(attn, dim=-1)\n",
        "        # out = self.drop(attention) @ V # [batch, n_heads, T, d_head]\n",
        "\n",
        "        out = out.transpose(1, 2).flatten(2)\n",
        "        return self.drop(self.lin(out)) # [batch, T, d_model]\n",
        "\n",
        "# if self, dont pass cond_dim in init, dont pass cond in fwd\n",
        "# Softmax(Q @ K.T) @ V ~ Softmax(Q) @ Softmax(K).T @ V\n",
        "\n",
        "# https://github.com/lucidrains/x-transformers/blob/main/x_transformers/x_transformers.py#L1855\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_head=4, cond_dim=None, ff_mult=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm1 = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.norm2 = nn.RMSNorm(d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.cond_dim = cond_dim\n",
        "        self.self = MultiHeadAttention(d_model, d_head=d_head, dropout=0)\n",
        "        if self.cond_dim!=None: self.cross = MultiHeadAttention(d_model, d_head=d_head, cond_dim=cond_dim, dropout=0)\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        ff_dim=d_model*ff_mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), nn.Dropout(dropout), nn.Linear(d_model, ff_dim), act, # ReLU GELU\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "            nn.RMSNorm(d_model), nn.Dropout(dropout), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            # nn.RMSNorm(d_model), act, nn.Dropout(dropout), nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,c,h,w], [batch, num_tok, cond_dim], [batch,T]\n",
        "        # bchw = x.shape\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "\n",
        "        if self.cond_dim==None: cond=None # is self attn\n",
        "        x = x + self.drop(self.self(self.norm1(x)))\n",
        "        if self.cond_dim!=None: x = x + self.cross(self.norm2(x), cond, mask)\n",
        "        x = x + self.ff(x)\n",
        "        # x = x + self.drop(self.ff(x))\n",
        "\n",
        "        # return x.transpose(1,2).reshape(*bchw)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODpKypTCsfIt",
        "outputId": "1e4dc1cf-22f9-4400-cb37-078985cce64d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ True,  True,  True,  ..., False, False, False],\n",
            "        [ True,  True,  True,  ..., False, False, False],\n",
            "        [ True,  True,  True,  ..., False, False, False],\n",
            "        [ True,  True,  True,  ..., False, False, False]])\n",
            "torch.Size([4, 16])\n"
          ]
        }
      ],
      "source": [
        "# @title TransformerVICReg save\n",
        "import math\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        for layer in self:\n",
        "            params = inspect.signature(layer.forward).parameters.keys()\n",
        "            layer._fwdparams = ','.join(params)\n",
        "\n",
        "    def forward(self, x, mask=None, cond=None):\n",
        "        for layer in self:\n",
        "            args = [x]\n",
        "            if 'mask' in layer._fwdparams: args.append(mask)\n",
        "            if 'cond' in layer._fwdparams: args.append(cond)\n",
        "            x = layer(*args)\n",
        "        return x\n",
        "\n",
        "class TransformerVICReg(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, drop=0):\n",
        "        super().__init__()\n",
        "        act = nn.GELU()\n",
        "        # self.embed = nn.Sequential(nn.Linear(in_dim, d_model), act)\n",
        "        self.embed = nn.Sequential(\n",
        "            nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2),\n",
        "            )\n",
        "        self.pos_encoder = RoPE(d_model, seq_len=200, base=10000)\n",
        "        # d_hid = d_hid or d_model#*2\n",
        "        # self.encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "        self.encoder = Seq(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "\n",
        "        out_dim = out_dim or d_model\n",
        "        self.lin = nn.Linear(d_model, out_dim)\n",
        "        self.attention_pool = nn.Linear(d_model, 1, bias=False)\n",
        "\n",
        "        dim_v = d_model * 4\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(out_dim, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v, bias=False),\n",
        "            )\n",
        "\n",
        "    def forward(self, x, src_key_padding_mask=None): # [batch, seq, d_model], [batch, seq] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # x = self.embed(x)\n",
        "        x = self.embed(x.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "        # batch, seq_len, d_model = x.shape\n",
        "        # x = torch.cat([self.cls.repeat(batch,1,1), x], dim=1)\n",
        "        # src_key_padding_mask = torch.cat([torch.zeros((batch, 1), dtype=torch.bool), src_key_padding_mask], dim=1)\n",
        "        x = self.pos_encoder(x)\n",
        "\n",
        "        x = self.encoder(x, mask=src_key_padding_mask)\n",
        "        # print(\"fwd\",out.shape) # float [batch, seq_len, d_model]\n",
        "\n",
        "        attn = self.attention_pool(x).squeeze(-1) # [batch, seq] # seq_pool\n",
        "        out = (torch.softmax(attn, dim=-1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, seq] @ [batch, seq, dim] -> [batch, dim]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "    def expand(self, x, src_key_padding_mask=None):\n",
        "        sx = self.forward(x, src_key_padding_mask=src_key_padding_mask)\n",
        "        vx = self.exp(sx)\n",
        "        return vx\n",
        "\n",
        "batch, seq_len, d_model = 4,3500,512\n",
        "in_dim, out_dim=3,16\n",
        "model = TransformerVICReg(in_dim, d_model, out_dim, d_head=4, nlayers=2, drop=0.).to(device)\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "src_key_padding_mask = torch.stack([(torch.arange(seq_len) < seq_len - v) for v in torch.randint(seq_len, (batch,))]) # True will be ignored # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "print(src_key_padding_mask)\n",
        "out = model(x, src_key_padding_mask)\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qwg9dG4sQ3h",
        "outputId": "11e258f7-70a0-4922-89f5-9c3862a34c44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "55330\n",
            "in vicreg  3.6479194409180554e-16 24.74782019853592 7.794930811932943e-10\n",
            "(tensor(1.4592e-17, device='cuda:0', grad_fn=<MseLossBackward0>), tensor(0.9899, device='cuda:0', grad_fn=<AddBackward0>), tensor(7.7949e-10, device='cuda:0', grad_fn=<AddBackward0>))\n"
          ]
        }
      ],
      "source": [
        "# @title Violet save\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class Violet(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, nlayers=2, d_head=4):\n",
        "        super().__init__()\n",
        "        out_dim = out_dim or d_model\n",
        "\n",
        "        # self.rnn = RNN(d_model, d_model, d_model, nlayers)\n",
        "        self.student = TransformerVICReg(in_dim, d_model, out_dim=out_dim, d_head=d_head, nlayers=nlayers, drop=0.)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "\n",
        "        # vicreg\n",
        "        self.sim_coeff=25. # 10.0 # 25.0 # λ\n",
        "        self.std_coeff=25. # 1.0 # 25.0 # µ\n",
        "        self.cov_coeff=1. # 25.0 # 1.0 # ν\n",
        "\n",
        "        self.classifier = nn.Linear(out_dim, 18) # 10 18\n",
        "\n",
        "    def loss(self, x, src_key_padding_mask=None): # [batch, T, 3]\n",
        "        batch, seq, dim = x.shape\n",
        "        # print(x.shape)\n",
        "        vx = self.student.expand(x, src_key_padding_mask=src_key_padding_mask) # [batch, num_context_toks, out_dim]\n",
        "        with torch.no_grad(): vy = self.teacher.expand(x.detach(), src_key_padding_mask=src_key_padding_mask) # [batch, num_trg_toks, out_dim]\n",
        "        loss = self.vicreg(vx, vy)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x, src_key_padding_mask=None): # [batch, T, 3]\n",
        "        sx = self.student(x, src_key_padding_mask=src_key_padding_mask)\n",
        "        return sx\n",
        "\n",
        "    def classify(self, x): # [batch, T, 3]\n",
        "        sx = self.forward(x)\n",
        "        out = self.classifier(sx)\n",
        "        return out\n",
        "\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size, num_features = x.shape\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = self.sim_coeff * repr_loss + self.std_coeff * std_loss + self.cov_coeff * cov_loss\n",
        "        print(\"in vicreg \",self.sim_coeff * repr_loss.item() , self.std_coeff * std_loss.item() , self.cov_coeff * cov_loss.item())\n",
        "        # return loss\n",
        "        return repr_loss, std_loss, cov_loss\n",
        "\n",
        "\n",
        "violet = Violet(in_dim=3, d_model=32, out_dim=16, nlayers=2, d_head=4).to(device)\n",
        "# voptim = torch.optim.AdamW(violet.parameters(), lr=1e-3) # 1e-3?\n",
        "voptim = torch.optim.AdamW([{'params': violet.student.encoder.parameters()},\n",
        "    {'params': violet.student.exp.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # default 1e-2\n",
        "print(sum(p.numel() for p in violet.parameters() if p.requires_grad)) # 27584\n",
        "\n",
        "x = torch.rand((2,1000,3), device=device)\n",
        "# x = torch.rand((2,1,16), device=device)\n",
        "loss = violet.loss(x)\n",
        "# print(out.shape)\n",
        "print(loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rV7WLiNT4EAV"
      },
      "outputs": [],
      "source": [
        "# @title test WISDM\n",
        "\n",
        "# print(activity_dataframe)\n",
        "# print(activity_dataframe['activity'])\n",
        "# activity_dataframe.to_csv('data.csv',index=False)\n",
        "\n",
        "# data = pd.read_csv(\"/data.csv\", index_col =\"Name\")\n",
        "# data = pd.read_csv(\"data.csv\")\n",
        "\n",
        "# a = list(activity_dataframe['timestamp'])\n",
        "# print(a)\n",
        "# print([x-y for x,y in zip(a[1:],a[:-1])])\n",
        "\n",
        "# print(data.loc[0])\n",
        "# p = data.loc[0]\n",
        "# print(len(p))\n",
        "# ID, activity, timestamp, x, y, z, meter, device = data.loc[0]\n",
        "# print(activity)\n",
        "\n",
        "# print(data.loc[1639])\n",
        "\n",
        "# userids = data['ID'].unique()#for id in userids\n",
        "# print(data['activity'].unique())\n",
        "# df_keep = data[['ID','activity','timestamp','x','y','z']]\n",
        "\n",
        "\n",
        "# grouped = df_keep.groupby(['ID'])\n",
        "# grouped = df_keep.groupby(['ID','activity'])\n",
        "# print(len(grouped))\n",
        "# user_acts = dict(tuple(df_keep.groupby(['ID','activity'])))\n",
        "# print(user_acts)\n",
        "# print(len(user_acts))\n",
        "# temp_df = df[df['ID'] == id]\n",
        "\n",
        "# print(len(df_keep))\n",
        "# print(len(data))\n",
        "\n",
        "\n",
        "\n",
        "# act = [[a, d[['timestamp','x','y','z']].to_numpy()] for a, d in user_acts.items()]\n",
        "# act = [[(int(a[0]), ), d[['timestamp','x','y','z']].to_numpy()] for a, d in user_acts.items()]\n",
        "# print(act)\n",
        "# print(act[0])\n",
        "# # print(act[0][1])\n",
        "# print(len(act[1]))\n",
        "# print([len(a[1]) for a in act])\n",
        "# print(min([len(a[1]) for a in act])) # 3567\n",
        "\n",
        "\n",
        "# act_dict = {i: act for i, act in enumerate(data['activity'].unique())}\n",
        "# act_invdict = {v: k for k, v in act_dict.items()}\n",
        "# print(act_invdict)\n",
        "\n",
        "# torch.tensor(act[0][1][:3500])\n",
        "\n",
        "# id_act, x = act[0]\n",
        "# id_act = self.process(id_act)\n",
        "# return id_act,\n",
        "# torch.tensor(x[:3500]) # 3567\n",
        "\n",
        "\n",
        "# /content/processed/wisdm-dataset/raw/phone/accel/data.csv\n",
        "# /content/processed/wisdm-dataset/raw/watch/gyro/data.csv\n",
        "\n",
        "# data0 = pd.read_csv(\"/content/processed/wisdm-dataset/raw/watch/accel/data.csv\")\n",
        "# data1 = pd.read_csv(\"/content/processed/wisdm-dataset/raw/watch/gyro/data.csv\")\n",
        "\n",
        "# user_acts0 = dict(tuple(data0.groupby(['ID','activity'])))\n",
        "# user_acts1 = dict(tuple(data1.groupby(['ID','activity'])))\n",
        "\n",
        "for (a0,d0), (a1,d1) in zip(user_acts0.items(), user_acts1.items()):\n",
        "    print(a0,a1)\n",
        "    print(len(d0),len(d1))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "N-4I7A5hJU7b"
      },
      "outputs": [],
      "source": [
        "# @title har_cnn\n",
        "# https://arxiv.org/pdf/2209.08335v1\n",
        "# https://github.com/Lou1sM/HAR/blob/master/har_cnn.py\n",
        "import sys\n",
        "import json\n",
        "from hmmlearn import hmm\n",
        "from copy import deepcopy\n",
        "import os\n",
        "import math\n",
        "from pdb import set_trace\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import time\n",
        "from torch.utils import data\n",
        "import cl_args\n",
        "from dl_utils.misc import asMinutes,check_dir\n",
        "#from dl_utils.label_funcs import accuracy, mean_f1, debable, translate_labellings, get_num_labels, label_counts, dummy_labels, avoid_minus_ones_lf_wrapper,masked_mode,acc_by_label, get_trans_dict\n",
        "from label_funcs_tmp import accuracy, mean_f1, translate_labellings, get_num_labels, label_counts, dummy_labels, avoid_minus_ones_lf_wrapper,masked_mode,acc_by_label, get_trans_dict\n",
        "from dl_utils.tensor_funcs import noiseify, numpyify, cudify\n",
        "from make_dsets import make_single_dset, make_dsets_by_user\n",
        "from sklearn.metrics import normalized_mutual_info_score,adjusted_rand_score\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from project_config import get_dataset_info_object\n",
        "\n",
        "rari = lambda x,y: round(adjusted_rand_score(x,y),4)\n",
        "rnmi = lambda x,y: round(normalized_mutual_info_score(x,y),4)\n",
        "\n",
        "class EncByLayer(nn.Module):\n",
        "    def __init__(self,x_filters,y_filters,x_strides,y_strides,max_pools,show_shapes):\n",
        "        super().__init__()\n",
        "        self.show_shapes = show_shapes\n",
        "        num_layers = len(x_filters)\n",
        "        assert all(len(x)==num_layers for x in (y_filters,x_strides,y_strides,max_pools))\n",
        "        ncvs = [1]+[4*2**i for i in range(num_layers)]\n",
        "        # conv_layers = []\n",
        "        # for i in range(num_layers):\n",
        "        #     conv_layer = nn.Sequential(\n",
        "        #         nn.Conv2d(ncvs[i],ncvs[i+1],(x_filters[i],y_filters[i]),(x_strides[i],y_strides[i])), nn.BatchNorm2d(ncvs[i+1]) if i<num_layers-1 else nn.Identity(), nn.LeakyReLU(0.3), nn.MaxPool2d(max_pools[i])\n",
        "        #     )\n",
        "        #     conv_layers.append(conv_layer)\n",
        "        # self.conv_layers = nn.ModuleList(conv_layers)\n",
        "\n",
        "        self.conv_layers = nn.Sequential(*[\n",
        "                nn.Conv2d(ncvs[i],ncvs[i+1],(x_filters[i],y_filters[i]),(x_strides[i],y_strides[i])), nn.BatchNorm2d(ncvs[i+1]) if i<num_layers-1 else nn.Identity(), nn.LeakyReLU(0.3), nn.MaxPool2d(max_pools[i])\n",
        "             for i in range(nlayers)])\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        # if self.show_shapes: print(x.shape)\n",
        "        # for conv_layer in self.conv_layers:\n",
        "        x = conv_layer(x)\n",
        "            # if self.show_shapes: print(x.shape)\n",
        "        return x\n",
        "\n",
        "class DecByLayer(nn.Module):\n",
        "    def __init__(self,x_filters,y_filters,x_strides,y_strides,show_shapes):\n",
        "        super().__init__()\n",
        "        self.show_shapes = show_shapes\n",
        "        num_layers = len(x_filters)\n",
        "        assert all(len(x)==num_layers for x in (y_filters,x_strides,y_strides))\n",
        "        ncvs = [4*2**i for i in reversed(range(num_layers))]+[1]\n",
        "        conv_trans_layers = [nn.Sequential(\n",
        "                nn.ConvTranspose2d(ncvs[i],ncvs[i+1],(x_filters[i],y_filters[i]),(x_strides[i],y_strides[i])), nn.BatchNorm2d(ncvs[i+1]), nn.LeakyReLU(0.3),\n",
        "                )\n",
        "            for i in range(num_layers)]\n",
        "        self.conv_trans_layers = nn.ModuleList(conv_trans_layers)\n",
        "\n",
        "    def forward(self,x):\n",
        "        if self.show_shapes: print(x.shape)\n",
        "        for conv_trans_layer in self.conv_trans_layers:\n",
        "            x = conv_trans_layer(x)\n",
        "            if self.show_shapes: print(x.shape)\n",
        "        return x\n",
        "\n",
        "class Var_BS_MLP(nn.Module):\n",
        "    def __init__(self,input_size,hidden_size,output_size):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(nn.Linear(input_size,hidden_size), nn.BatchNorm1d(hidden_size), nn.LeakyReLU(0.3), nn.Linear(hidden_size,output_size))\n",
        "\n",
        "    def forward(self,x):\n",
        "        return self.mlp(x)\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n",
        "    dset_info_object = get_dataset_info_object(args.dset)\n",
        "    num_classes = args.num_classes if args.num_classes != -1 else dset_info_object.num_classes\n",
        "    if args.dset == 'UCI_feat':\n",
        "        enc = nn.Sequential(nn.Linear(561,500),nn.ReLU(),\n",
        "                            nn.Linear(500,500),nn.ReLU(),\n",
        "                            nn.Linear(500,2000),nn.ReLU(),\n",
        "                            nn.Linear(2000,6),nn.ReLU()).cuda()\n",
        "        dec = nn.Sequential(nn.Linear(6,2000),nn.ReLU(),\n",
        "                            nn.Linear(2000,500),nn.ReLU(),\n",
        "                            nn.Linear(500,500),nn.ReLU(),\n",
        "                            nn.Linear(500,561),nn.ReLU()).cuda()\n",
        "        mlp = Var_BS_MLP(6,256,num_classes).cuda()\n",
        "    else:\n",
        "        if args.window_size == 512:\n",
        "            x_filters = (50,40,7,4)\n",
        "            x_strides = (2,2,1,1)\n",
        "            max_pools = ((2,1),(2,1),(2,1),(2,1))\n",
        "        elif args.window_size == 100:\n",
        "            x_filters = (20,20,5,3)\n",
        "            x_strides = (1,1,1,1)\n",
        "            max_pools = ((2,1),(2,1),(2,1),1)\n",
        "        y_filters = (1,1,1,dset_info_object.num_channels)\n",
        "        y_strides = (1,1,1,1)\n",
        "        enc = EncByLayer(x_filters,y_filters,x_strides,y_strides,max_pools,show_shapes=args.show_shapes).cuda()\n",
        "        #if args.is_n2d:\n",
        "        x_filters_trans = (15,10,15,11)\n",
        "        x_strides_trans = (2,3,3,3)\n",
        "        y_filters_trans = (dset_info_object.num_channels,1,1,1)\n",
        "        dec = DecByLayer(x_filters_trans,y_filters_trans,x_strides_trans,y_strides,show_shapes=args.show_shapes).cuda()\n",
        "\n",
        "        optional_umap_like_net_in = Var_BS_MLP(32,256,2).cuda()\n",
        "        optional_umap_like_net_out = Var_BS_MLP(2,256,2).cuda()\n",
        "        if ARGS.is_uln:\n",
        "            enc = nn.Sequential(enc,nn.Flatten(1),Var_BS_MLP(32,256,2).cuda())\n",
        "            dec = nn.Sequential(Var_BS_MLP(32,256,2).cuda(),nn.Unflatten(2,(32,1,1)),dec)\n",
        "        mlp = Var_BS_MLP(2 if ARGS.is_uln else 32,256,num_classes).cuda()\n",
        "    if args.load_pretrained:\n",
        "        enc.load_state_dict(torch.load('enc_pretrained.pt'))\n",
        "    subj_ids = args.subj_ids\n",
        "\n",
        "    metric_dict = {'acc':accuracy,'nmi':rnmi,'ari':rari,'f1':mean_f1}\n",
        "    har = HARLearner(enc=enc,mlp=mlp,dec=dec,num_classes=num_classes,args=args,metric_dict=metric_dict)\n",
        "\n",
        "    start_time = time.time()\n",
        "    already_exists = check_dir(f\"experiments/{args.exp_name}/preds\")\n",
        "    check_dir(f\"experiments/{args.exp_name}/best_preds\")\n",
        "    if args.show_shapes:\n",
        "        dset_train, selected_acts = make_single_dset(args,subj_ids)\n",
        "        num_ftrs = dset_train.x.shape[-1]\n",
        "        print(num_ftrs)\n",
        "        lat = enc(torch.ones((2,1,args.window_size,num_ftrs),device='cuda'))\n",
        "        dec(lat)\n",
        "        sys.exit()\n",
        "    dsets_by_id = make_dsets_by_user(args,subj_ids)\n",
        "    if args.is_n2d:\n",
        "        for subj_id, (dset,sa) in dsets_by_id.items():\n",
        "            print(\"n2ding\", subj_id)\n",
        "            har.n2d_abl(subj_id,dset)\n",
        "    elif not args.subject_independent:\n",
        "        bad_ids = []\n",
        "        for user_id, (dset,sa) in dsets_by_id.items():\n",
        "            n = get_num_labels(dset.y)\n",
        "            if n < dset_info_object.num_classes/2:\n",
        "                print(f\"Excluding user {user_id}, only has {n} different labels, out of {num_classes}\")\n",
        "                bad_ids.append(user_id)\n",
        "        if not args.bad_ids: dsets_by_id = {k:v for k,v in dsets_by_id.items() if k not in bad_ids}\n",
        "        print('reloading clusterings for', [x for x in subj_ids[:args.reload_ids] if x not in bad_ids])\n",
        "        for rid in subj_ids[:args.reload_ids]:\n",
        "            if rid in bad_ids: continue\n",
        "            print('reloading clusterings for', rid)\n",
        "            rdset,sa = dsets_by_id.pop(rid)\n",
        "            best_preds = np.load(f'experiments/{args.exp_name}/best_preds/{rid}.npy')\n",
        "            preds = np.load(f'experiments/{args.exp_name}/preds/{rid}.npy')\n",
        "            har.log_preds_and_scores(rid,preds,best_preds,numpyify(rdset.y))\n",
        "        print('clustering remaining ids', [x for x in subj_ids[args.reload_ids:]], 'from scratch\\n')\n",
        "\n",
        "        print(\"CLUSTERING EACH DSET SEPARATELY\")\n",
        "        for subj_id, (dset,sa) in dsets_by_id.items():\n",
        "            print(\"clustering\", subj_id)\n",
        "            har.pseudo_label_cluster_meta_meta_loop(subj_id,dset)\n",
        "    elif args.subject_independent:\n",
        "        print(\"CLUSTERING AS SINGLE DSET\")\n",
        "        one_big_dset, selected_acts = make_single_dset(args,subj_ids)\n",
        "        har.pseudo_label_cluster_meta_meta_loop('all',one_big_dset)\n",
        "\n",
        "    results_file_path = f'experiments/{args.exp_name}/results.txt'\n",
        "    har.total_time = time.time() - start_time\n",
        "    har.log_final_scores(results_file_path)\n",
        "    har.express_times(results_file_path)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    ARGS, need_umap = cl_args.get_cl_args()\n",
        "    if need_umap: import umap\n",
        "    main(ARGS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "38tWCkK-0Zlu",
        "outputId": "7abe4629-3dcf-4981-810e-9e888be03cb9"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "leading zeros in decimal integer literals are not permitted; use an 0o prefix for octal integers (<ipython-input-1-94e7ebe34632>, line 10)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-94e7ebe34632>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    wget http://archive.ics.uci.edu/ml/machine-learning-databases/00231/PAMAP2_Dataset.zip\u001b[0m\n\u001b[0m                                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m leading zeros in decimal integer literals are not permitted; use an 0o prefix for octal integers\n"
          ]
        }
      ],
      "source": [
        "# @title Lou1sM download_datasets.sh\n",
        "# https://github.com/Lou1sM/HAR/blob/master/download_datasets.sh\n",
        "\n",
        "##!/bin/sh\n",
        "\n",
        "mkdir -p datasets\n",
        "cd datasets\n",
        "\n",
        "#PAMAP\n",
        "# echo -e \"\\n###############\"\n",
        "# echo GETTING AND PREPARING PAMAP DATA\n",
        "# echo -e \"###############\\n\"\n",
        "wget http://archive.ics.uci.edu/ml/machine-learning-databases/00231/PAMAP2_Dataset.zip\n",
        "unzip PAMAP2_Dataset.zip\n",
        "# python ../convert_data_to_np.py PAMAP\n",
        "\n",
        "#UCI\n",
        "# echo -e \"\\n###############\"\n",
        "# echo GETTING AND PREPARING UCI DATA\n",
        "# echo -e \"###############\\n\"\n",
        "mkdir -p UCI2\n",
        "cd UCI2\n",
        "wget http://archive.ics.uci.edu/ml/machine-learning-databases/00341/HAPT%20Data%20Set.zip\n",
        "unzip HAPT\\ Data\\ Set.zip\n",
        "# cd ..\n",
        "# python ../convert_data_to_np.py UCI-raw\n",
        "\n",
        "#mkdir -p capture24\n",
        "#cd capture24/\n",
        "\n",
        "#for i in $(seq -w 151)\n",
        "#do\n",
        "#    curl -JLO \"https://ora.ox.ac.uk/objects/uuid:92650814-a209-4607-9fb5-921eab761c11/download_file?safe_filename=P${i}.csv.gz&type_of_work=Dataset\"\n",
        "#done\n",
        "#\n",
        "#curl -JLO \"https://ora.ox.ac.uk/objects/uuid:92650814-a209-4607-9fb5-921eab761c11/download_file?safe_filename=metadata.csv&type_of_work=Dataset\"\n",
        "#curl -JLO \"https://ora.ox.ac.uk/objects/uuid:92650814-a209-4607-9fb5-921eab761c11/download_file?safe_filename=annotation-label-dictionary.csv&type_of_work=Dataset\"\n",
        "#\n",
        "#\n",
        "#for f in $(ls); do\n",
        "#    if [ ${f: -2} == \"gz\" ]; then\n",
        "#        gunzip $f;\n",
        "#    fi;\n",
        "#done\n",
        "\n",
        "#WISDM-v1\n",
        "# echo -e \"\\n###############\"\n",
        "# echo GETTING AND PREPARING WISDM-v1 DATA\n",
        "# echo -e \"###############\\n\"\n",
        "wget https://www.cis.fordham.edu/wisdm/includes/datasets/latest/WISDM_ar_latest.tar.gz\n",
        "gunzip WISDM_ar_latest.tar.gz\n",
        "tar -xf WISDM_ar_latest.tar\n",
        "\n",
        "# python ../convert_data_to_np.py WISDM-v1\n",
        "\n",
        "#WISDM-watch\n",
        "# echo -e \"\\n###############\"\n",
        "# echo GETTING AND PREPARING WISDM-watch DATA\n",
        "# echo -e \"###############\\n\"\n",
        "wget https://archive.ics.uci.edu/ml/machine-learning-databases/00507/wisdm-dataset.zip\n",
        "unzip wisdm-dataset.zip\n",
        "python ../convert_data_to_np.py WISDM-watch\n",
        "\n",
        "#REALDISP\n",
        "# echo -e \"\\n###############\"\n",
        "# echo GETTING REALDISP DATA\n",
        "# echo -e \"###############\\n\"\n",
        "mkdir -p realdisp\n",
        "cd realdisp\n",
        "mkdir -p RawData\n",
        "wget https://archive.ics.uci.edu/ml/machine-learning-databases/00305/realistic_sensor_displacement.zip\n",
        "unzip realistic_sensor_displacement.zip\n",
        "# cd ..\n",
        "# python ../convert_data_to_np.py REALDISP\n",
        "# cd ..\n",
        "# pwd\n",
        "#HHAR\n",
        "mkdir -p hhar\n",
        "wget http://archive.ics.uci.edu/ml/machine-learning-databases/00344/Activity%20recognition%20exp.zip\n",
        "unzip Activity\\ recognition\\ exp.zip\n",
        "# python ../convert_data_to_np.py HHAR\n",
        "# cd ..\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMvb1-PBQ9u-",
        "outputId": "764e8452-935e-4bcc-aa38-82faddee7da3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PAMAP2_Dataset/Protocol subject106.dat\n",
            "/content/PAMAP2/subject106\n",
            "PAMAP2_Dataset/Protocol subject101.dat\n",
            "/content/PAMAP2/subject101\n",
            "PAMAP2_Dataset/Protocol subject107.dat\n",
            "/content/PAMAP2/subject107\n",
            "PAMAP2_Dataset/Protocol subject109.dat\n",
            "/content/PAMAP2/subject109\n",
            "PAMAP2_Dataset/Protocol subject103.dat\n",
            "/content/PAMAP2/subject103\n",
            "PAMAP2_Dataset/Protocol subject104.dat\n",
            "/content/PAMAP2/subject104\n",
            "PAMAP2_Dataset/Protocol subject108.dat\n",
            "/content/PAMAP2/subject108\n",
            "PAMAP2_Dataset/Protocol subject105.dat\n",
            "/content/PAMAP2/subject105\n",
            "PAMAP2_Dataset/Protocol subject102.dat\n",
            "/content/PAMAP2/subject102\n"
          ]
        }
      ],
      "source": [
        "# @title PAMAP2\n",
        "# https://arxiv.org/pdf/2209.08335v1\n",
        "# https://archive.ics.uci.edu/dataset/231/pamap2+physical+activity+monitoring\n",
        "# https://archive.ics.uci.edu/static/public/231/pamap2+physical+activity+monitoring.zip\n",
        "# !wget http://archive.ics.uci.edu/ml/machine-learning-databases/00231/PAMAP2_Dataset.zip\n",
        "# !unzip PAMAP2_Dataset.zip\n",
        "\n",
        "import os\n",
        "data_dir = 'PAMAP2_Dataset/Protocol'\n",
        "np_dir = '/content/PAMAP2'\n",
        "if not os.path.isdir(np_dir):\n",
        "    os.makedirs(np_dir)\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def array_from_txt(inpath):\n",
        "    with open(inpath) as f:\n",
        "        d = f.readlines()\n",
        "        array = np.array([[float(x) for x in line.split()] for line in d])\n",
        "    return array\n",
        "\n",
        "def array_expanded(a,expanded_length):\n",
        "    if a.shape[0] >= expanded_length: return a\n",
        "    insert_every = mpf(a.shape[0]/(expanded_length-a.shape[0]))\n",
        "    additional_idxs = (np.arange(expanded_length-a.shape[0])*insert_every).astype(np.int)\n",
        "    values = a[additional_idxs]\n",
        "    expanded = np.insert(a,additional_idxs,values,axis=0)\n",
        "    assert expanded.shape[0] == expanded_length\n",
        "    return expanded\n",
        "\n",
        "def convert(inpath,outpath):\n",
        "    array = array_from_txt(inpath)\n",
        "    timestamps = array[:,0]\n",
        "    labels = array[:,1].astype(int)\n",
        "    # Delete orientation data, which webpage says is 'invalid in this data, and timestamp and label\n",
        "    array = np.delete(array,[0,1,2,13,14,15,16,30,31,32,33,47,48,49,50],1)\n",
        "    np.save(outpath+'_timestamps',timestamps)\n",
        "    np.save(outpath+'_labels',labels)\n",
        "    np.save(outpath,array)\n",
        "\n",
        "for filename in os.listdir(data_dir):\n",
        "    print(data_dir,filename)\n",
        "    # inpath = os.path.join(data_dir,filename)a\n",
        "    inpath = data_dir+'/'+filename\n",
        "    outpath = np_dir+'/'+filename.split('.')[0]\n",
        "    print(outpath)\n",
        "    # outpath = os.path.join(np_dir,filename.split('.')[0])\n",
        "    convert(inpath,outpath)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "5TnsfKrMB0yF",
        "outputId": "07859402-b33c-4ede-c389-ef7a59c2f1ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "no precomputed datasets, computing from scratch\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "invalid index to scalar variable.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-ddcbc2f0291f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m             action_name_dict = pamap_action_name_dict)\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mdset_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_acts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_pamap_dset_train_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpamap_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-ddcbc2f0291f>\u001b[0m in \u001b[0;36mmake_pamap_dset_train_val\u001b[0;34m(subj_ids)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# 0 is a transient activity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# x_train,y_train,selected_acts = preproc_xys(x_train,y_train,args.step_size,args.window_size,dset_info_object,subj_ids)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mselected_acts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreproc_xys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdset_info_object\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msubj_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;31m# dset_train = StepDataset(x_train,y_train,window_size=args.window_size,step_size=args.step_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# dset_train = StepDataset(x_train,y_train,window_size=512,step_size=5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-a048671b2310>\u001b[0m in \u001b[0;36mpreproc_xys\u001b[0;34m(x, y, step_size, window_size, dset_info_object, subj_ids)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mnum_windows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mstep_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m#mode_labels = np.array([stats.mode(y[w*step_size:w*step_size + window_size]).mode[0] if (y[w*step_size:w*step_size + window_size]==y[w*step_size]).all() else -1 for w in range(num_windows)])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mmode_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstep_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_windows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mselected_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mselected_acts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdset_info_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_name_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mact_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mact_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselected_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-a048671b2310>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mnum_windows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mstep_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m#mode_labels = np.array([stats.mode(y[w*step_size:w*step_size + window_size]).mode[0] if (y[w*step_size:w*step_size + window_size]==y[w*step_size]).all() else -1 for w in range(num_windows)])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mmode_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstep_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_windows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mselected_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mselected_acts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdset_info_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_name_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mact_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mact_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselected_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
          ]
        }
      ],
      "source": [
        "\n",
        "# def make_pamap_dset_train_val(args,subj_ids):\n",
        "def make_pamap_dset_train_val(subj_ids):\n",
        "    # dset_info_object = PAMAP_INFO.PAMAP_INFO\n",
        "    dset_info_object = PAMAP_INFO\n",
        "    # x_train = np.concatenate([np.load(f'datasets/PAMAP2_Dataset/np_data/subject{s}.npy') for s in subj_ids])\n",
        "    # y_train = np.concatenate([np.load(f'datasets/PAMAP2_Dataset/np_data/subject{s}_labels.npy') for s in subj_ids])\n",
        "    x_train = np.concatenate([np.load(f'PAMAP2/subject{s}.npy') for s in subj_ids])\n",
        "    y_train = np.concatenate([np.load(f'PAMAP2/subject{s}_labels.npy') for s in subj_ids])\n",
        "    x_train = x_train[y_train!=0] # 0 is a transient activity\n",
        "    y_train = y_train[y_train!=0] # 0 is a transient activity\n",
        "    # x_train,y_train,selected_acts = preproc_xys(x_train,y_train,args.step_size,args.window_size,dset_info_object,subj_ids)\n",
        "    x_train,y_train,selected_acts = preproc_xys(x_train,y_train,1,1,dset_info_object,subj_ids)\n",
        "    # dset_train = StepDataset(x_train,y_train,window_size=args.window_size,step_size=args.step_size)\n",
        "    # dset_train = StepDataset(x_train,y_train,window_size=512,step_size=5)\n",
        "    dset_train = StepDataset(x_train,y_train,window_size=1,step_size=1)\n",
        "    return dset_train, selected_acts\n",
        "\n",
        "class HAR_Dataset_Container():\n",
        "    def __init__(self,code_name,dataset_dir_name,possible_subj_ids,num_channels,num_classes,action_name_dict):\n",
        "        self.code_name = code_name\n",
        "        self.dataset_dir_name = dataset_dir_name\n",
        "        self.possible_subj_ids = possible_subj_ids\n",
        "        self.num_channels = num_channels\n",
        "        self.num_classes = num_classes\n",
        "        self.action_name_dict = action_name_dict\n",
        "\n",
        "\n",
        "# PAMAP\n",
        "pamap_ids = [str(x) for x in range(101,110)]\n",
        "pamap_action_name_dict = {1:'lying',2:'sitting',3:'standing',4:'walking',5:'running',6:'cycling',7:'Nordic walking',9:'watching TV',10:'computer work',11:'car driving',12:'ascending stairs',13:'descending stairs',16:'vacuum cleaning',17:'ironing',18:'folding laundry',19:'house cleaning',20:'playing soccer',24:'rope jumping'}\n",
        "PAMAP_INFO = HAR_Dataset_Container(\n",
        "            code_name = 'PAMAP',\n",
        "            dataset_dir_name = 'PAMAP2_Dataset',\n",
        "            possible_subj_ids = pamap_ids,\n",
        "            num_channels = 39,\n",
        "            num_classes = 12,\n",
        "            action_name_dict = pamap_action_name_dict)\n",
        "\n",
        "dset_train, selected_acts = make_pamap_dset_train_val(pamap_ids)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGU_rUapvjcE",
        "outputId": "b150f60b-4d01-47b5-b1f4-ea6b20a17027"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Incorrect or no dataset specified\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# @title Lou1sM convert_data_to_np.py\n",
        "# https://github.com/Lou1sM/HAR/blob/master/convert_data_to_np.py\n",
        "from pdb import set_trace\n",
        "from collections import Counter\n",
        "from scipy.fft import fft\n",
        "from scipy import stats\n",
        "from mpmath import mp, mpf\n",
        "#from dl_utils import misc, label_funcs\n",
        "# from dl_utils import misc\n",
        "# import label_funcs_tmp\n",
        "import os\n",
        "from os.path import join\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "def array_from_txt(inpath):\n",
        "    with open(inpath) as f:\n",
        "        d = f.readlines()\n",
        "        array = np.array([[float(x) for x in line.split()] for line in d])\n",
        "    return array\n",
        "\n",
        "def array_expanded(a,expanded_length):\n",
        "    if a.shape[0] >= expanded_length: return a\n",
        "    insert_every = mpf(a.shape[0]/(expanded_length-a.shape[0]))\n",
        "    additional_idxs = (np.arange(expanded_length-a.shape[0])*insert_every).astype(np.int)\n",
        "    values = a[additional_idxs]\n",
        "    expanded = np.insert(a,additional_idxs,values,axis=0)\n",
        "    assert expanded.shape[0] == expanded_length\n",
        "    return expanded\n",
        "\n",
        "def convert(inpath,outpath):\n",
        "    array = array_from_txt(inpath)\n",
        "    timestamps = array[:,0]\n",
        "    labels = array[:,1].astype(np.int)\n",
        "    # Delete orientation data, which webpage says is 'invalid in this data, and timestamp and label\n",
        "    array = np.delete(array,[0,1,2,13,14,15,16,30,31,32,33,47,48,49,50],1)\n",
        "    np.save(outpath+'_timestamps',timestamps)\n",
        "    np.save(outpath+'_labels',labels)\n",
        "    np.save(outpath,array)\n",
        "\n",
        "def expand_and_fill_labels(a,propoer_length):\n",
        "    start_filler = -np.ones(a[0,3])\n",
        "    end_filler = -np.ones(propoer_length-a[-1,4])\n",
        "    nested_lists = [[a[i,2] for _ in range(a[i,4]-a[i,3])] + [-1]*(a[i+1,3]-a[i,4]) for i in range(len(a)-1)] + [[a[-1,2] for _ in range(a[-1,4]-a[-1,3])]]\n",
        "    middle = np.array([item for sublist in nested_lists for item in sublist])\n",
        "    total_label_array = np.concatenate((start_filler,middle,end_filler)).astype(np.int)\n",
        "    return total_label_array\n",
        "\n",
        "def add_dtft(signal):\n",
        "    fft_signal_complex = fft(signal,axis=-1)\n",
        "    fft_signal_modulusses = np.abs(fft_signal_complex)\n",
        "    return np.concatenate((signal,fft_signal_modulusses),axis=-1)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "# if sys.argv[1] == 'PAMAP':\n",
        "data_dir = 'PAMAP2_Dataset/Protocol'\n",
        "np_dir = 'PAMAP2_Dataset/np_data'\n",
        "print(\"\\n#####Preprocessing PAMAP2#####\\n\")\n",
        "if not os.path.isdir(np_dir):\n",
        "    os.makedirs(np_dir)\n",
        "\n",
        "for filename in os.listdir(data_dir):\n",
        "    print(filename)\n",
        "    inpath = join(data_dir,filename)\n",
        "    outpath = join(np_dir,filename.split('.')[0])\n",
        "    convert(inpath,outpath)\n",
        "\n",
        "# elif sys.argv[1] == 'UCI-raw':\n",
        "data_dir = 'UCI2/RawData'\n",
        "np_dir = 'UCI2/np_data'\n",
        "print(\"\\n#####Preprocessing UCI#####\\n\")\n",
        "if not os.path.isdir(np_dir):\n",
        "    os.makedirs(np_dir)\n",
        "\n",
        "raw_label_array = array_from_txt(join(data_dir,'labels.txt')).astype(int)\n",
        "def two_digitify(x): return '0'+str(x) if len(str(x))==1 else str(x)\n",
        "fnames = os.listdir(data_dir)\n",
        "for idx in range(1,31):\n",
        "    print(\"processing user\",idx)\n",
        "    acc_array_list = []\n",
        "    gyro_array_list = []\n",
        "    label_array_list = []\n",
        "    user_idx = two_digitify(idx)\n",
        "    acc_fpaths = sorted([fn for fn in fnames if f'user{user_idx}' in fn and 'acc' in fn])\n",
        "    gyro_fpaths = sorted([fn for fn in fnames if f'user{user_idx}' in fn and 'gyro' in fn])\n",
        "    assert len(acc_fpaths) == len(gyro_fpaths)\n",
        "    for fna,fng in zip(acc_fpaths,gyro_fpaths):\n",
        "        acc_exp_id = int(fna.split('exp')[1][:2])\n",
        "        gyro_exp_id = int(fng.split('exp')[1][:2])\n",
        "        assert acc_exp_id==gyro_exp_id\n",
        "        new_acc_array = array_from_txt(join(data_dir,fna))\n",
        "        new_gyro_array = array_from_txt(join(data_dir,fna))\n",
        "        label_array_block = raw_label_array[raw_label_array[:,0]==acc_exp_id]\n",
        "        filled_label_array_block = expand_and_fill_labels(label_array_block,new_acc_array.shape[0])\n",
        "        assert filled_label_array_block.shape[0] == new_acc_array.shape[0]\n",
        "        assert filled_label_array_block.shape[0] == new_gyro_array.shape[0]\n",
        "        label_array_list.append(filled_label_array_block)\n",
        "        acc_array_list.append(new_acc_array)\n",
        "        gyro_array_list.append(new_gyro_array)\n",
        "    label_array = np.concatenate(label_array_list)\n",
        "    acc_array = np.concatenate(acc_array_list)\n",
        "    gyro_array = np.concatenate(gyro_array_list)\n",
        "    total_array = np.concatenate((acc_array,gyro_array),axis=1)\n",
        "    outpath = join(np_dir,f'user{user_idx}.npy')\n",
        "    np.save(outpath,total_array)\n",
        "    label_outpath = join(np_dir,f'user{user_idx}_labels.npy')\n",
        "    np.save(label_outpath,label_array)\n",
        "\n",
        "# elif sys.argv[1] == 'WISDM-v1':\n",
        "with open('WISDM_ar_v1.1/WISDM_ar_v1.1_raw.txt') as f: text = f.readlines()\n",
        "print(\"\\n#####Preprocessing WISDM-v1#####\\n\")\n",
        "activities_list = ['Jogging','Walking','Upstairs','Downstairs','Standing','Sitting']\n",
        "X_list = []\n",
        "y_list = []\n",
        "users_list = []\n",
        "num_zeros = 0\n",
        "def process_line(line_to_process):\n",
        "    global num_zeros\n",
        "    if float(line_to_process.split(',')[2]) == 0: num_zeros += 1#print(\"Timestamp zero, discarding\")\n",
        "    else:\n",
        "        X_list.append([float(x) for x in line_to_process.split(',')[3:]])\n",
        "        y_list.append(activities_list.index(line_to_process.split(',')[1]))\n",
        "        users_list.append(line_to_process.split(',')[0])\n",
        "for i,raw_line in enumerate(text):\n",
        "    #line = line.replace(';','').replace('\\n','')\n",
        "    if raw_line == '\\n': continue\n",
        "    elif raw_line.endswith(',;\\n'): line = raw_line[:-3]\n",
        "    elif raw_line.endswith(';\\n'): line = raw_line[:-2]\n",
        "    elif raw_line.endswith(',\\n'): line = raw_line[:-2]\n",
        "    else: set_trace()\n",
        "    if len(line.split(',')) == 6:\n",
        "        try: process_line(line)\n",
        "        except: print(f\"Can't process line {i}, even though length 6: {raw_line}\\n\")\n",
        "    else:\n",
        "        print(f\"Bad format at line {i}:\\n{raw_line}\")\n",
        "        try:\n",
        "            line1, line2 = line.split(';')\n",
        "            process_line(line1); process_line(line2)\n",
        "            print(f\"I think this was two lines erroneously put on one line. Processing separately as\\n{line1}\\nand\\n{line2}\")\n",
        "        except: print(\"Can't process this line at all, omitting\")\n",
        "one_big_X_array = np.array(X_list)\n",
        "one_big_y_array = np.array(y_list)\n",
        "one_big_users_array = np.array(users_list)\n",
        "print(one_big_X_array.shape)\n",
        "print(one_big_y_array.shape)\n",
        "print(one_big_users_array.shape)\n",
        "print(f\"Number of zero lines: {num_zeros}\")\n",
        "misc.np_save(one_big_X_array,'wisdm_v1','X.npy')\n",
        "misc.np_save(one_big_y_array,'wisdm_v1','y.npy')\n",
        "misc.np_save(one_big_users_array,'wisdm_v1','users.npy')\n",
        "\n",
        "# elif sys.argv[1] == 'WISDM-watch':\n",
        "p_dir = 'wisdm-dataset/raw/phone'\n",
        "w_dir = 'wisdm-dataset/raw/watch'\n",
        "np_dir = 'wisdm-dataset/np_data'\n",
        "print(\"\\n#####Preprocessing WISDM-watch#####\\n\")\n",
        "if not os.path.isdir(np_dir):\n",
        "    os.makedirs(np_dir)\n",
        "\n",
        "mp.dps = 100 # Avoid floating point errors in label insertion function\n",
        "for user_idx in range(1600,1651):\n",
        "    print('user', user_idx)\n",
        "    phone_acc_path = join(p_dir,'accel',f'data_{user_idx}_accel_phone.txt')\n",
        "    watch_acc_path = join(w_dir,'accel',f'data_{user_idx}_accel_watch.txt')\n",
        "    phone_gyro_path = join(p_dir,'gyro',f'data_{user_idx}_gyro_phone.txt')\n",
        "    watch_gyro_path = join(w_dir,'gyro',f'data_{user_idx}_gyro_watch.txt')\n",
        "\n",
        "    label_codes_list = list('ABCDEFGHIJKLMOPQRS') # Missin 'N' is deliberate\n",
        "    def two_arrays_from_txt(inpath):\n",
        "        with open(inpath) as f:\n",
        "            d = f.readlines()\n",
        "            arr = np.array([[float(x) for x in line.strip(';\\n').split(',')[3:]] for line in d])\n",
        "            label_array = np.array([label_codes_list.index(line.split(',')[1]) for line in d])\n",
        "        return arr, label_array\n",
        "\n",
        "    phone_acc, label_array1 = two_arrays_from_txt(phone_acc_path)\n",
        "    watch_acc, label_array2 = two_arrays_from_txt(watch_acc_path)\n",
        "    phone_gyro, label_array3 = two_arrays_from_txt(phone_gyro_path)\n",
        "    watch_gyro, label_array4 = two_arrays_from_txt(watch_gyro_path)\n",
        "    user_arrays = [phone_acc,watch_acc,phone_gyro,watch_gyro]\n",
        "    label_arrays = [label_array1,label_array2,label_array3,label_array4]\n",
        "    max_len = max([a.shape[0] for a in user_arrays])\n",
        "    equalized_user_arrays = [array_expanded(a,max_len) for a in user_arrays]\n",
        "    equalized_label_arrays = [array_expanded(lab_a,max_len) for lab_a in label_arrays]\n",
        "    total_user_array = np.concatenate(equalized_user_arrays,axis=1)\n",
        "    mode_object = stats.mode(np.stack(equalized_label_arrays,axis=1),axis=1)\n",
        "    mode_labels = mode_object.mode[:,0]\n",
        "    # Print how many windows contained just 1 label, how many 2 etc.\n",
        "    #print('Agreement in labels:',label_funcs_tmp.label_counts(mode_object.count[:,0]))\n",
        "    certains = (mode_object.count == 4)[:,0]\n",
        "    user_fn = f'{user_idx}.npy'\n",
        "    misc.np_save(total_user_array,np_dir,user_fn)\n",
        "    user_labels_fn = f'{user_idx}_labels.npy'\n",
        "    misc.np_save(mode_labels,np_dir,user_labels_fn)\n",
        "    user_certains_fn = f'{user_idx}_certains.npy'\n",
        "    misc.np_save(certains,np_dir,user_certains_fn)\n",
        "\n",
        "# elif sys.argv[1] == 'REALDISP':\n",
        "data_dir = 'realdisp/RawData'\n",
        "np_dir = 'realdisp/np_data'\n",
        "print(\"\\n#####Preprocessing REALDISP#####\\n\")\n",
        "if not os.path.isdir(np_dir):\n",
        "    os.makedirs(np_dir)\n",
        "\n",
        "for filename in os.listdir(data_dir):\n",
        "    print(filename)\n",
        "    if filename == 'dataset manual.pdf': continue\n",
        "    if not filename.split('_')[1].startswith('ideal'):\n",
        "        continue\n",
        "    with open(join(data_dir,filename)) as f: xy = f.readlines()\n",
        "    ar = np.array([[float(item) for item in line.split('\\t')] for line in xy])\n",
        "    x = ar[:,:-1]\n",
        "    y = ar[:,-1].astype(int)\n",
        "\n",
        "    np.save(join(np_dir,filename.split('_')[0]), x)\n",
        "    np.save(join(np_dir,filename.split('_')[0])+'_labels', y)\n",
        "\n",
        "# elif sys.argv[1] == 'Capture24':\n",
        "np_dir = 'capture24/np_data'\n",
        "if not os.path.isdir(np_dir):\n",
        "    os.makedirs(np_dir)\n",
        "name_df = pd.read_csv('capture24/annotation-label-dictionary.csv')\n",
        "#name_conversion_dict = dict(zip(name_df['annotation'],name_df['label:DohertySpecific2018']))\n",
        "name_df = name_df[['annotation','label:DohertySpecific2018']]\n",
        "int_label_converter_df = pd.DataFrame(enumerate(name_df['label:DohertySpecific2018'].unique()),columns=['int_label','label:DohertySpecific2018'])\n",
        "int_label_converter_dict = dict(enumerate(name_df['label:DohertySpecific2018'].unique()))\n",
        "with open('capture24/int_label_converter_df.json','w') as f:\n",
        "    json.dump(int_label_converter_dict,f)\n",
        "name_df = name_df.merge(int_label_converter_df)\n",
        "for fname in os.listdir('capture24'):\n",
        "    if fname.endswith('.gz'): continue\n",
        "    subj_id = fname.split('.')[0]\n",
        "    if not subj_id.startswith('P') and not len(subj_id) == 4: continue # Skip metadata files\n",
        "    print(f\"converting {fname} to np\")\n",
        "    try: df = pd.read_csv(join('capture24',fname))\n",
        "    except: set_trace()\n",
        "    translated_df = df.merge(name_df)\n",
        "    x = translated_df[['x','y','z']].to_numpy()\n",
        "    y = translated_df['int_label'].to_numpy()\n",
        "    np.save(join(np_dir,f'{subj_id}.npy'),x)\n",
        "    np.save(join(np_dir,f'{subj_id}_labels.npy'),y)\n",
        "\n",
        "# elif sys.argv[1] == 'HHAR':\n",
        "data_dir = 'Activity recognition exp'\n",
        "np_dir = 'hhar/np_data'\n",
        "print(\"\\n#####Preprocessing HHAR#####\\n\")\n",
        "if not os.path.isdir(np_dir):\n",
        "    os.makedirs(np_dir)\n",
        "\n",
        "pandaload = lambda path: pd.read_csv(join(data_dir,'Phones_accelerometer.csv')).set_index('Creation_Time').drop(['Index','Arrival_Time','Model','Device'],axis=1).dropna()\n",
        "print('loading dataframes\\n')\n",
        "phone_acc_df = pandaload('Phones_accelerometer.csv')\n",
        "phone_gyro_df = pandaload('Phones_gyroscope.csv')\n",
        "watch_acc_df = pandaload('Watch_accelerometer.csv')\n",
        "watch_gyro_df = pandaload('Watch_gyroscope.csv')\n",
        "activities_list = ['bike', 'sit', 'stand', 'walk', 'stairsup', 'stairsdown']\n",
        "user_list = list('abcdefghi')\n",
        "\n",
        "for user_letter_name in user_list:\n",
        "    print('processing user', user_letter_name)\n",
        "    user_phone_acc = phone_acc_df.loc[phone_acc_df.User==user_letter_name]\n",
        "    user_phone_gyro = phone_gyro_df.loc[phone_gyro_df.User==user_letter_name]\n",
        "    user_watch_acc = watch_acc_df.loc[watch_acc_df.User==user_letter_name]\n",
        "    user_watch_gyro = watch_acc_df.loc[watch_gyro_df.User==user_letter_name]\n",
        "    assert all([user_watch_gyro.shape==d.shape for d in (user_phone_acc,user_phone_gyro,user_watch_acc)])\n",
        "    comb_phone = user_phone_acc.join(user_phone_gyro,how='outer',lsuffix='_acc',rsuffix='_gyro')\n",
        "    comb_watch = user_watch_acc.join(user_watch_gyro,how='outer',lsuffix='_acc',rsuffix='_gyro')\n",
        "    #if not (comb_watch.gt_acc == comb_watch.gt_gyro).all(): set_trace()\n",
        "    #if not (comb_phone.gt_acc == comb_phone.gt_gyro).all(): set_trace()\n",
        "    comb = comb_phone.join(comb_watch,how='outer',lsuffix='_phone',rsuffix='_watch')\n",
        "    duplicate_rows = [x for x,count in Counter(comb.index).items() if count > 1]\n",
        "    if len(duplicate_rows) > 10: set_trace()\n",
        "    elif len(duplicate_rows) > 0:\n",
        "        print( f\"removing {len(duplicate_rows)} duplicate rows\")\n",
        "        comb = comb.drop(duplicate_rows)\n",
        "    if not (comb.gt_acc_phone == comb.gt_acc_watch).all(): set_trace()\n",
        "    user_X_array = comb.drop([c for c in comb.columns if 'User' in c or 'gt' in c],axis=1).to_numpy()\n",
        "    user_y_array = np.array([activities_list.index(a) for a in comb['gt_acc_phone']])\n",
        "    save_path = join(np_dir,f\"{user_list.index(user_letter_name)+1}.npy\")\n",
        "    label_save_path = join(np_dir,f\"{user_list.index(user_letter_name)+1}_labels.npy\")\n",
        "    np.save(save_path,user_X_array,allow_pickle=False)\n",
        "    np.save(label_save_path,user_y_array,allow_pickle=False)\n",
        "    # Make smaller option for testing\n",
        "    np.save(join(np_dir,f\"0.npy\"),user_X_array[::1000],allow_pickle=False)\n",
        "    np.save(join(np_dir,f\"0_labels.npy\"),user_y_array[::1000],allow_pickle=False)\n",
        "\n",
        "else: print('\\nIncorrect or no dataset specified\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nQERarq97cIq"
      },
      "outputs": [],
      "source": [
        "# @title Lou1sM make_dsets.py\n",
        "# https://github.com/Lou1sM/HAR/blob/master/make_dsets.py\n",
        "import numpy as np\n",
        "# from dl_utils.misc import check_dir, CifarLikeDataset\n",
        "import os\n",
        "import torch\n",
        "# import project_config\n",
        "from scipy import stats\n",
        "from torch.utils import data\n",
        "#from dl_utils import label_funcs\n",
        "# from dl_utils.tensor_funcs import cudify\n",
        "# import label_funcs_tmp\n",
        "from pdb import set_trace\n",
        "from os.path import join\n",
        "\n",
        "\n",
        "class ChunkDataset(data.Dataset):\n",
        "    def __init__(self,x,y):\n",
        "        self.x, self.y = x,y\n",
        "    def __len__(self): return len(self.x)\n",
        "    def __getitem__(self,idx):\n",
        "        batch_x = self.x[idx].unsqueeze(0)\n",
        "        batch_y = self.y[idx]\n",
        "        return batch_x, batch_y, idx\n",
        "\n",
        "class ConcattedDataset(data.Dataset):\n",
        "    \"\"\"Needs datasets to be StepDatasets in order to Concat them.\"\"\"\n",
        "    def __init__(self,xs,ys,window_size,step_size):\n",
        "        self.x, self.y = torch.cat(xs),torch.cat(ys)\n",
        "        self.window_size = window_size\n",
        "        self.step_size = step_size\n",
        "        component_dset_lengths = [((len(x)-self.window_size)//self.step_size + 1) for x in xs]\n",
        "        x_idx_locs = []\n",
        "        block_start_idx = 0\n",
        "        for x in xs:\n",
        "            x_idx_locs += list(range(block_start_idx,block_start_idx+len(x)-window_size+1,step_size))\n",
        "            block_start_idx += len(x)\n",
        "        self.x_idx_locs = np.array(x_idx_locs)\n",
        "        if not len(self.x_idx_locs) == len(self.y): set_trace()\n",
        "\n",
        "    def __len__(self): return len(self.y)\n",
        "    def __getitem__(self,idx):\n",
        "        x_idx = self.x_idx_locs[idx]\n",
        "        batch_x = self.x[x_idx:x_idx + self.window_size].unsqueeze(0)\n",
        "        batch_y = self.y[idx]\n",
        "        return batch_x, batch_y, idx\n",
        "\n",
        "class UCIFeatDataset(data.Dataset):\n",
        "    def __init__(self,x,y,transforms=[]):\n",
        "        self.x, self.y = cudify(x).float(),cudify(y).float()\n",
        "        assert len(self.x) == len(self.y)\n",
        "        for transform in transforms:\n",
        "            self.x = transform(self.x)\n",
        "    def __len__(self): return len(self.x)\n",
        "    def __getitem__(self,idx):\n",
        "        batch_x = self.x[idx]\n",
        "        batch_y = self.y[idx]\n",
        "        return batch_x, batch_y, idx\n",
        "\n",
        "class StepDataset(data.Dataset):\n",
        "    def __init__(self,x,y,window_size,step_size,transforms=[]):\n",
        "        self.x, self.y = cudify(x).float(),cudify(y).float()\n",
        "        self.window_size = window_size\n",
        "        self.step_size = step_size\n",
        "        self.transforms = transforms\n",
        "        self.position = None\n",
        "        self.ensemble_size = None\n",
        "        for transform in transforms:\n",
        "            self.x = transform(self.x)\n",
        "    def __len__(self): return (len(self.x)-self.window_size)//self.step_size + 1\n",
        "    def __getitem__(self,idx):\n",
        "        batch_x = self.x[idx*self.step_size:(idx*self.step_size) + self.window_size].unsqueeze(0)\n",
        "        batch_y = self.y[idx]\n",
        "        return batch_x, batch_y, idx\n",
        "\n",
        "    def put_in_ensemble(self,position,ensemble_size):\n",
        "        self.y += ensemble_size*position\n",
        "        self.position = position\n",
        "        self.ensemble_size = ensemble_size\n",
        "\n",
        "def preproc_xys(x,y,step_size,window_size,dset_info_object,subj_ids):\n",
        "    ids_string = 'all' if set(subj_ids) == set(dset_info_object.possible_subj_ids) else \"-\".join(subj_ids)\n",
        "    precomp_dir = f'datasets/{dset_info_object.dataset_dir_name}/precomputed/{ids_string}step{step_size}_window{window_size}/'\n",
        "    if os.path.isfile(join(precomp_dir,'x.pt')) and os.path.isfile(join(precomp_dir,'y.pt')):\n",
        "        print(\"loading precomputed datasets\")\n",
        "        x = torch.load(join(precomp_dir,'x.pt'))\n",
        "        y = torch.load(join(precomp_dir,'y.pt'))\n",
        "        with open(join(precomp_dir,'selected_acts.txt')) as f: selected_acts = f.readlines()\n",
        "    else:\n",
        "        print(\"no precomputed datasets, computing from scratch\")\n",
        "        xnans = np.isnan(x).any(axis=1)\n",
        "        x = x[~xnans]\n",
        "        y = y[~xnans]\n",
        "        x = x[y!=-1]\n",
        "        y = y[y!=-1]\n",
        "        num_windows = (len(x) - window_size)//step_size + 1\n",
        "        #mode_labels = np.array([stats.mode(y[w*step_size:w*step_size + window_size]).mode[0] if (y[w*step_size:w*step_size + window_size]==y[w*step_size]).all() else -1 for w in range(num_windows)])\n",
        "        mode_labels = np.array([stats.mode(y[w*step_size:w*step_size + window_size]).mode[0] for w in range(num_windows)])\n",
        "        selected_ids = set(mode_labels)\n",
        "        selected_acts = [dset_info_object.action_name_dict[act_id] for act_id in selected_ids]\n",
        "        mode_labels, trans_dict, changed = label_funcs_tmp.compress_labels(mode_labels)\n",
        "        assert len(selected_acts) == len(set(mode_labels))\n",
        "        x = torch.tensor(x).float()\n",
        "        y = torch.tensor(mode_labels).float()\n",
        "        check_dir(precomp_dir)\n",
        "        torch.save(x,join(precomp_dir,'x.pt'))\n",
        "        torch.save(y,join(precomp_dir,'y.pt'))\n",
        "        with open(join(precomp_dir,'selected_acts.txt'),'w') as f:\n",
        "            for a in selected_acts: f.write(a+'\\n')\n",
        "    return x, y, selected_acts\n",
        "\n",
        "def make_pamap_dset_train_val(args,subj_ids):\n",
        "    # dset_info_object = project_config.PAMAP_INFO\n",
        "    dset_info_object = PAMAP_INFO\n",
        "    x_train = np.concatenate([np.load(f'datasets/PAMAP2_Dataset/np_data/subject{s}.npy') for s in subj_ids])\n",
        "    y_train = np.concatenate([np.load(f'datasets/PAMAP2_Dataset/np_data/subject{s}_labels.npy') for s in subj_ids])\n",
        "    x_train = x_train[y_train!=0] # 0 is a transient activity\n",
        "    y_train = y_train[y_train!=0] # 0 is a transient activity\n",
        "    x_train,y_train,selected_acts = preproc_xys(x_train,y_train,args.step_size,args.window_size,dset_info_object,subj_ids)\n",
        "    dset_train = StepDataset(x_train,y_train,window_size=args.window_size,step_size=args.step_size)\n",
        "    return dset_train, selected_acts\n",
        "\n",
        "# def make_uci_dset_train_val(args,subj_ids):\n",
        "#     dset_info_object = project_config.UCI_INFO\n",
        "#     x_train = np.concatenate([np.load(f'datasets/UCI2/np_data/user{s}.npy') for s in subj_ids])\n",
        "#     y_train = np.concatenate([np.load(f'datasets/UCI2/np_data/user{s}_labels.npy') for s in subj_ids])\n",
        "#     x_train = x_train[y_train<7] # Labels still begin at 1 at this point as\n",
        "#     y_train = y_train[y_train<7] # haven't been compressed, so select 1,..,6\n",
        "#     #x_train = x_train[y_train!=-1]\n",
        "#     #y_train = y_train[y_train!=-1]\n",
        "#     #y_val = y_val[y_val!=-1]\n",
        "#     x_train,y_train,selected_acts = preproc_xys(x_train,y_train,args.step_size,args.window_size,dset_info_object,subj_ids)\n",
        "#     dset_train = StepDataset(x_train,y_train,window_size=args.window_size,step_size=args.step_size)\n",
        "#     return dset_train, selected_acts\n",
        "\n",
        "# def make_uci_feat_dset_train_val():\n",
        "#     dset_info_object = project_config.UCI_INFO\n",
        "#     x = np.load(f'datasets/UCI_feat/uci_feat_data.npy')\n",
        "#     y = np.load(f'datasets/UCI_feat/uci_feat_targets.npy')\n",
        "#     selected_acts = dict(enumerate(['walking','upstairs','downstairs','sitting','standing','lying']))\n",
        "#     dset = UCIFeatDataset(x,y)\n",
        "#     #dset.x = dset.data\n",
        "#     #dset.y = dset.targets\n",
        "#     return dset, selected_acts\n",
        "\n",
        "# def make_wisdm_v1_dset_train_val(args,subj_ids):\n",
        "#     dset_info_object = project_config.WISDMv1_INFO\n",
        "#     x = np.load('datasets/wisdm_v1/X.npy')\n",
        "#     y = np.load('datasets/wisdm_v1/y.npy')\n",
        "#     users = np.load('datasets/wisdm_v1/users.npy')\n",
        "#     train_idxs_to_user = np.zeros(users.shape[0]).astype(np.bool)\n",
        "#     for subj_id in subj_ids:\n",
        "#         new_users = users==subj_id\n",
        "#         train_idxs_to_user = np.logical_or(train_idxs_to_user,new_users)\n",
        "#     x_train = x[train_idxs_to_user]\n",
        "#     y_train = y[train_idxs_to_user]\n",
        "#     x_train,y_train,selected_acts = preproc_xys(x_train,y_train,args.step_size,args.window_size,dset_info_object,subj_ids)\n",
        "#     dset_train = StepDataset(x_train,y_train,window_size=args.window_size,step_size=args.step_size)\n",
        "#     return dset_train, selected_acts\n",
        "\n",
        "# def make_wisdm_watch_dset_train_val(args,subj_ids):\n",
        "#     dset_info_object = project_config.WISDMwatch_INFO\n",
        "#     x_train = np.concatenate([np.load(f'datasets/wisdm-dataset/np_data/{s}.npy') for s in subj_ids])\n",
        "#     y_train = np.concatenate([np.load(f'datasets/wisdm-dataset/np_data/{s}_labels.npy') for s in subj_ids])\n",
        "#     certains_train = np.concatenate([np.load(f'datasets/wisdm-dataset/np_data/{s}_certains.npy') for s in subj_ids])\n",
        "#     x_train = x_train[certains_train]\n",
        "#     y_train = y_train[certains_train]\n",
        "#     x_train,y_train,selected_acts = preproc_xys(x_train,y_train,args.step_size,args.window_size,dset_info_object,subj_ids)\n",
        "#     dset_train = StepDataset(x_train,y_train,window_size=args.window_size,step_size=args.step_size)\n",
        "#     return dset_train, selected_acts\n",
        "\n",
        "# def make_realdisp_dset_train_val(args,subj_ids):\n",
        "#     dset_info_object = project_config.REALDISP_INFO\n",
        "#     x_train = np.concatenate([np.load(f'datasets/realdisp/np_data/subject{s}.npy') for s in subj_ids])\n",
        "#     y_train = np.concatenate([np.load(f'datasets/realdisp/np_data/subject{s}_labels.npy') for s in subj_ids])\n",
        "#     x_train = x_train[:,2:] #First two columns are timestamp\n",
        "#     x_train = x_train[y_train!=0] # 0 seems to be a transient activity\n",
        "#     y_train = y_train[y_train!=0] # 0 seems to be a transient activity\n",
        "#     x_train,y_train,selected_acts = preproc_xys(x_train,y_train,args.step_size,args.window_size,dset_info_object,subj_ids)\n",
        "#     dset_train = StepDataset(x_train,y_train,window_size=args.window_size,step_size=args.step_size)\n",
        "#     return dset_train, selected_acts\n",
        "\n",
        "# def make_hhar_dset_train_val(args,subj_ids):\n",
        "#     dset_info_object = project_config.HHAR_INFO\n",
        "#     x_train = np.concatenate([np.load(f'datasets/hhar/np_data/{s}.npy') for s in subj_ids])\n",
        "#     y_train = np.concatenate([np.load(f'datasets/hhar/np_data/{s}_labels.npy') for s in subj_ids])\n",
        "#     x_train,y_train,selected_acts = preproc_xys(x_train,y_train,args.step_size,args.window_size,dset_info_object,subj_ids)\n",
        "#     dset_train = StepDataset(x_train,y_train,window_size=args.window_size,step_size=args.step_size)\n",
        "#     return dset_train, selected_acts\n",
        "\n",
        "def make_capture_dset_train_val(args,subj_ids):\n",
        "    action_name_dict = {0: 'sleep', 1: 'sedentary-screen', 2: 'tasks-moderate', 3: 'sedentary-non-screen', 4: 'walking', 5: 'vehicle', 6: 'bicycling', 7: 'tasks-light', 8: 'sports-continuous', 9: 'sport-interrupted'} # Should also be saved in json file in datasets/capture24\n",
        "    subj_ids = len(subj_ids) - min(2,len(subj_ids)//2)\n",
        "    subj_ids = subj_ids[:subj_ids]\n",
        "    def three_digitify(x): return '00' + str(x) if len(str(x))==1 else '0' + str(x)\n",
        "    x_train = np.concatenate([np.load(f'datasets/capture24/np_data/P{three_digitify(s)}.npy') for s in subj_ids])\n",
        "    y_train = np.concatenate([np.load(f'datasets/capture24/np_data/P{three_digitify(s)}_labels.npy') for s in subj_ids])\n",
        "    x_train,y_train,selected_acts = preproc_xys(x_train,y_train,args.step_size,args.window_size,action_name_dict)\n",
        "    dset_train = StepDataset(x_train,y_train,window_size=args.window_size,step_size=args.step_size)\n",
        "    if len(subj_ids) <= 2: return dset_train, dset_train, selected_acts\n",
        "\n",
        "    # else make val dset\n",
        "    val_ids = subj_ids[subj_ids:]\n",
        "    x_val = np.concatenate([np.load(f'datasets/capture24/np_data/P{three_digitify(s)}.npy') for s in val_ids])\n",
        "    y_val = np.concatenate([np.load(f'datasets/capture24/np_data/P{three_digitify(s)}_labels.npy') for s in val_ids])\n",
        "    x_val,y_val,selected_acts = preproc_xys(x_val,y_val,args.step_size,args.window_size,action_name_dict)\n",
        "    dset_val = StepDataset(x_val,y_val,window_size=args.window_size,step_size=args.step_size)\n",
        "    return dset_train, dset_val, selected_acts\n",
        "\n",
        "def make_single_dset(args,subj_ids):\n",
        "    if args.dset == 'PAMAP':\n",
        "        return make_pamap_dset_train_val(args,subj_ids)\n",
        "    # if args.dset == 'UCI':\n",
        "    #     return make_uci_dset_train_val(args,subj_ids)\n",
        "    # if args.dset == 'UCI_feat':\n",
        "    #     return make_uci_feat_dset_train_val()\n",
        "    # if args.dset == 'WISDM-v1':\n",
        "    #     return make_wisdm_v1_dset_train_val(args,subj_ids)\n",
        "    # if args.dset == 'WISDM-watch':\n",
        "    #     return make_wisdm_watch_dset_train_val(args,subj_ids)\n",
        "    # if args.dset == 'REALDISP':\n",
        "    #     return make_realdisp_dset_train_val(args,subj_ids)\n",
        "    # if args.dset == 'HHAR':\n",
        "    #     return make_hhar_dset_train_val(args,subj_ids)\n",
        "    # if args.dset == 'Capture24':\n",
        "    #     return make_capture_dset_train_val(args,subj_ids)\n",
        "\n",
        "def make_dsets_by_user(args,subj_ids):\n",
        "    dsets_by_id = {}\n",
        "    for subj_id in subj_ids:\n",
        "        dset_subj, selected_acts_subj = make_single_dset(args,[subj_id])\n",
        "        dsets_by_id[subj_id] = dset_subj,selected_acts_subj\n",
        "    return dsets_by_id\n",
        "\n",
        "def chunked_up(x,step_size,window_size):\n",
        "    num_windows = (len(x) - window_size)//step_size + 1\n",
        "    return torch.stack([x[i*step_size:i*step_size+window_size] for i in range(num_windows)])\n",
        "\n",
        "def combine_dsets(dsets):\n",
        "    xs = [d.x for d in dsets]\n",
        "    ys = [d.y for d in dsets]\n",
        "    return ConcattedDataset(xs,ys,dsets[0].window_size,dsets[0].step_size)\n",
        "\n",
        "def combine_dsets_old(dsets):\n",
        "    processed_dset_xs = []\n",
        "    for dset in dsets:\n",
        "        if isinstance(dset,StepDataset):\n",
        "            processed_dset_x = chunked_up(dset.x,dset.step_size,dset.window_size)\n",
        "        elif isinstance(dset,ChunkDataset):\n",
        "            processed_dset_x = dset.x\n",
        "        else:\n",
        "            print(f\"you're trying to combine dsets on a {type(dset)}, but it has to be a dataset\")\n",
        "        processed_dset_xs.append(processed_dset_x)\n",
        "    x = torch.cat(processed_dset_xs)\n",
        "    y = torch.cat([dset.y for dset in dsets])\n",
        "    assert len(x) == len(y)\n",
        "    combined = ChunkDataset(x,y)\n",
        "    return combined\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_PZ6amU5BhIa"
      },
      "outputs": [],
      "source": [
        "# @title Lou1sM project_config.py\n",
        "# https://github.com/Lou1sM/HAR/blob/master/project_config.py\n",
        "import sys\n",
        "import torch\n",
        "\n",
        "class HAR_Dataset_Container():\n",
        "    def __init__(self,code_name,dataset_dir_name,possible_subj_ids,num_channels,num_classes,action_name_dict):\n",
        "        self.code_name = code_name\n",
        "        self.dataset_dir_name = dataset_dir_name\n",
        "        self.possible_subj_ids = possible_subj_ids\n",
        "        self.num_channels = num_channels\n",
        "        self.num_classes = num_classes\n",
        "        self.action_name_dict = action_name_dict\n",
        "\n",
        "\n",
        "# PAMAP\n",
        "pamap_ids = [str(x) for x in range(101,110)]\n",
        "pamap_action_name_dict = {1:'lying',2:'sitting',3:'standing',4:'walking',5:'running',6:'cycling',7:'Nordic walking',9:'watching TV',10:'computer work',11:'car driving',12:'ascending stairs',13:'descending stairs',16:'vacuum cleaning',17:'ironing',18:'folding laundry',19:'house cleaning',20:'playing soccer',24:'rope jumping'}\n",
        "PAMAP_INFO = HAR_Dataset_Container(\n",
        "            code_name = 'PAMAP',\n",
        "            dataset_dir_name = 'PAMAP2_Dataset',\n",
        "            possible_subj_ids = pamap_ids,\n",
        "            num_channels = 39,\n",
        "            num_classes = 12,\n",
        "            action_name_dict = pamap_action_name_dict)\n",
        "\n",
        "# # UCI\n",
        "# def two_digitify(x): return '0' + str(x) if len(str(x))==1 else str(x)\n",
        "# uci_ids = [two_digitify(x) for x in range(1,30)]\n",
        "# uci_action_name_dict = {1:'walking',2:'walking upstairs',3:'walking downstairs',4:'sitting',5:'standing',6:'lying',7:'stand_to_sit',9:'sit_to_stand',10:'sit_to_lit',11:'lie_to_sit',12:'stand_to_lie',13:'lie_to_stand'}\n",
        "# UCI_INFO = HAR_Dataset_Container(\n",
        "#             code_name = 'UCI',\n",
        "#             dataset_dir_name = 'UCI2',\n",
        "#             possible_subj_ids = uci_ids,\n",
        "#             num_channels = 6,\n",
        "#             num_classes = 6,\n",
        "#             action_name_dict = uci_action_name_dict)\n",
        "\n",
        "# # UCI_feat\n",
        "# def two_digitify(x): return '0' + str(x) if len(str(x))==1 else str(x)\n",
        "# uci_feat_action_name_dict = {1:'walking',2:'walking upstairs',3:'walking downstairs',4:'sitting',5:'standing',6:'lying',7:'stand_to_sit',9:'sit_to_stand',10:'sit_to_lit',11:'lie_to_sit',12:'stand_to_lie',13:'lie_to_stand'}\n",
        "# UCI_FEAT_INFO = HAR_Dataset_Container(\n",
        "#             code_name = 'UCI_feat',\n",
        "#             dataset_dir_name = 'UCI2_feat',\n",
        "#             possible_subj_ids = ['0'],\n",
        "#             num_channels = 561,\n",
        "#             num_classes = 6,\n",
        "#             action_name_dict = uci_action_name_dict)\n",
        "\n",
        "# # WISDM-v1\n",
        "# wisdmv1_ids = [str(x) for x in range(1,37)] #Paper says 29 users but ids go up to 36\n",
        "# activities_list = ['Jogging','Walking','Upstairs','Downstairs','Standing','Sitting']\n",
        "# wisdmv1_action_name_dict = dict(zip(range(len(activities_list)),activities_list))\n",
        "# WISDMv1_INFO = HAR_Dataset_Container(\n",
        "#             code_name = 'WISDM-v1',\n",
        "#             dataset_dir_name = 'wisdm_v1',\n",
        "#             possible_subj_ids = wisdmv1_ids,\n",
        "#             num_channels = 3,\n",
        "#             num_classes = 6,\n",
        "#             action_name_dict = wisdmv1_action_name_dict)\n",
        "\n",
        "# # WISDM-watch\n",
        "# wisdmwatch_ids = [str(x) for x in range(1600,1651)]\n",
        "# with open('datasets/wisdm-dataset/activity_key.txt') as f: r=f.readlines()\n",
        "# activities_list = [x.split(' = ')[0] for x in r if ' = ' in x]\n",
        "# wisdmwatch_action_name_dict = dict(zip(range(len(activities_list)),activities_list))\n",
        "# WISDMwatch_INFO = HAR_Dataset_Container(\n",
        "#             code_name = 'WISDM-watch',\n",
        "#             dataset_dir_name = 'wisdm-dataset',\n",
        "#             possible_subj_ids = wisdmwatch_ids,\n",
        "#             num_channels = 12,\n",
        "#             num_classes = 17,\n",
        "#             action_name_dict = wisdmwatch_action_name_dict)\n",
        "\n",
        "# # REALDISP\n",
        "# realdisp_ids = [str(x) for x in range(1,18)]\n",
        "# activities_list = ['Walking','Jogging','Running','Jump up','Jump front & back','Jump sideways','Jump leg/arms open/closed','Jump rope','Trunk twist','Trunk twist','Waist bends forward','Waist rotation','Waist bends','Reach heels backwards','Lateral bend','Lateral bend with arm up','Repetitive forward stretching','Upper trunk and lower body opposite twist','Lateral elevation of arms','Frontal elevation of arms','Frontal hand claps','Frontal crossing of arms','Shoulders high-amplitude rotation','Shoulders low-amplitude rotation','Arms inner rotation','Knees','Heels','Knees bending','Knees','Rotation on the knees','Rowing','Elliptical bike','Cycling']\n",
        "# realdisp_action_name_dict = {i+1:act for i,act in enumerate(activities_list)}\n",
        "# REALDISP_INFO = HAR_Dataset_Container(\n",
        "#             code_name = 'REALDISP',\n",
        "#             dataset_dir_name = 'realdisp',\n",
        "#             possible_subj_ids = realdisp_ids,\n",
        "#             num_channels = 117,\n",
        "#             num_classes = 33,\n",
        "#             action_name_dict = realdisp_action_name_dict)\n",
        "\n",
        "# # HHAR\n",
        "# hhar_ids = [str(x) for x in range(0,10)]\n",
        "# activities_list = ['bike', 'sit', 'stand', 'walk', 'stairsup', 'stairsdown']\n",
        "# hhar_action_name_dict = {i:act for i,act in enumerate(activities_list)}\n",
        "# HHAR_INFO = HAR_Dataset_Container(\n",
        "#             code_name = 'HHAR',\n",
        "#             dataset_dir_name = 'hhar',\n",
        "#             possible_subj_ids = hhar_ids,\n",
        "#             num_channels = 12,\n",
        "#             num_classes = 6,\n",
        "#             action_name_dict = hhar_action_name_dict)\n",
        "\n",
        "# DSET_OBJECTS = [PAMAP_INFO, UCI_INFO, UCI_FEAT_INFO, WISDMv1_INFO, WISDMwatch_INFO,REALDISP_INFO,HHAR_INFO]\n",
        "DSET_OBJECTS = [PAMAP_INFO]\n",
        "\n",
        "\n",
        "def get_dataset_info_object(dset_name):\n",
        "    dsets_by_that_name = [d for d in DSET_OBJECTS if d.code_name == dset_name]\n",
        "    if len(dsets_by_that_name)==0: print(f\"{dset_name} is not a recognized dataset\"); sys.exit()\n",
        "    assert len(dsets_by_that_name)==1\n",
        "    return dsets_by_that_name[0]\n",
        "\n",
        "def get_num_time_points():\n",
        "    for dset_info_object in DSET_OBJECTS:\n",
        "        print(dset_info_object.code_name, sum([torch.load(f'datasets/{dset_info_object.dataset_dir_name}/precomputed/{s}step100_window512/x.pt').shape[0] for s in dset_info_object.possible_subj_ids]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "3UvqfQXfFSu1",
        "outputId": "84e2ee66-1f8e-47bf-88a6-ff9e8a6b9a8d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "usage: colab_kernel_launcher.py [-h] [--num_subjs NUM_SUBJS | --subj_ids SUBJ_IDS [SUBJ_IDS ...]]\n",
            "                                [--full_epochs | --short_epochs] [--ablate_label_filter]\n",
            "                                [--all_subjs] [--bad_ids] [--batch_size_train BATCH_SIZE_TRAIN]\n",
            "                                [--batch_size_val BATCH_SIZE_VAL] [--clusterer {HMM,GMM}]\n",
            "                                [--compute_cross_metrics] [--dec_lr DEC_LR]\n",
            "                                [-d {PAMAP,UCI,WISDM-v1,WISDM-watch,REALDISP,Capture24}]\n",
            "                                [--enc_lr ENC_LR] [--exp_name EXP_NAME]\n",
            "                                [--frac_gt_labels FRAC_GT_LABELS] [--gpu GPU] [--is_n2d]\n",
            "                                [--is_uln] [--just_align_time] [--load_pretrained]\n",
            "                                [--mlp_lr MLP_LR] [--no_umap] [--noise NOISE]\n",
            "                                [--num_classes NUM_CLASSES] [--num_meta_epochs NUM_META_EPOCHS]\n",
            "                                [--num_meta_meta_epochs NUM_META_META_EPOCHS]\n",
            "                                [--num_pseudo_label_epochs NUM_PSEUDO_LABEL_EPOCHS]\n",
            "                                [--umap_dim UMAP_DIM] [--umap_neighbours UMAP_NEIGHBOURS]\n",
            "                                [--prob_thresh PROB_THRESH] [--reinit] [--reload_ids RELOAD_IDS]\n",
            "                                [--rlmbda RLMBDA] [--show_transitions] [--step_size STEP_SIZE]\n",
            "                                [--subject_independent] [--test]\n",
            "                                [--train_type {full,cluster_as_single,cluster_individually,train_frac_gts_as_single,find_similar_users}]\n",
            "                                [--show_shapes] [--verbose] [--window_size WINDOW_SIZE]\n",
            "colab_kernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-50ea2cb4-a0b3-4dd4-be5b-f719dc2cbc66.json\n"
          ]
        },
        {
          "ename": "SystemExit",
          "evalue": "2",
          "output_type": "error",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        }
      ],
      "source": [
        "# @title Lou1sM har_cnn.py\n",
        "# https://github.com/Lou1sM/HAR/blob/master/har_cnn.py\n",
        "import argparse\n",
        "import sys\n",
        "# import project_config\n",
        "\n",
        "\n",
        "def get_cl_args():\n",
        "    dset_options = ['PAMAP','UCI','WISDM-v1','WISDM-watch','REALDISP','Capture24']\n",
        "    # dset_options = [di.code_name for di in project_config.DSET_OBJECTS]\n",
        "    training_type_options = ['full','cluster_as_single','cluster_individually','train_frac_gts_as_single','find_similar_users']\n",
        "    parser = argparse.ArgumentParser()\n",
        "    subjs_group = parser.add_mutually_exclusive_group(required=False)\n",
        "    subjs_group.add_argument('--num_subjs',type=int)\n",
        "    subjs_group.add_argument('--subj_ids',type=str,nargs='+',default=['first'])\n",
        "    epochs_group = parser.add_mutually_exclusive_group(required=False)\n",
        "    epochs_group.add_argument('--full_epochs',action='store_true')\n",
        "    epochs_group.add_argument('--short_epochs',action='store_true')\n",
        "    parser.add_argument('--ablate_label_filter',action='store_true')\n",
        "    parser.add_argument('--all_subjs',action='store_true')\n",
        "    parser.add_argument('--bad_ids',action='store_true')\n",
        "    parser.add_argument('--batch_size_train',type=int,default=256)\n",
        "    parser.add_argument('--batch_size_val',type=int,default=1024)\n",
        "    parser.add_argument('--clusterer',type=str,choices=['HMM','GMM'],default='HMM')\n",
        "    parser.add_argument('--compute_cross_metrics',action='store_true')\n",
        "    parser.add_argument('--dec_lr',type=float,default=1e-3)\n",
        "    parser.add_argument('-d','--dset',type=str,default='UCI',choices=dset_options)\n",
        "    parser.add_argument('--enc_lr',type=float,default=1e-3)\n",
        "    parser.add_argument('--exp_name',type=str,default=\"try\")\n",
        "    parser.add_argument('--frac_gt_labels',type=float,default=0.1)\n",
        "    parser.add_argument('--gpu',type=str,default='0')\n",
        "    parser.add_argument('--is_n2d',action='store_true')\n",
        "    parser.add_argument('--is_uln',action='store_true',help='net for dim red. instead of umap')\n",
        "    parser.add_argument('--just_align_time',action='store_true')\n",
        "    parser.add_argument('--load_pretrained',action='store_true')\n",
        "    parser.add_argument('--mlp_lr',type=float,default=1e-3)\n",
        "    parser.add_argument('--no_umap',action='store_true')\n",
        "    parser.add_argument('--noise',type=float,default=1.)\n",
        "    parser.add_argument('--num_classes',type=int,default=-1)\n",
        "    parser.add_argument('--num_meta_epochs',type=int,default=1)\n",
        "    parser.add_argument('--num_meta_meta_epochs',type=int,default=1)\n",
        "    parser.add_argument('--num_pseudo_label_epochs',type=int,default=5)\n",
        "    parser.add_argument('--umap_dim',type=int,default=2)\n",
        "    parser.add_argument('--umap_neighbours',type=int,default=60)\n",
        "    parser.add_argument('--prob_thresh',type=float,default=.95)\n",
        "    parser.add_argument('--reinit',action='store_true')\n",
        "    parser.add_argument('--reload_ids',type=int,default=0)\n",
        "    parser.add_argument('--rlmbda',type=float,default=.1)\n",
        "    parser.add_argument('--show_transitions',action='store_true')\n",
        "    parser.add_argument('--step_size',type=int,default=5)\n",
        "    parser.add_argument('--subject_independent',action='store_true')\n",
        "    parser.add_argument('--test','-t',action='store_true')\n",
        "    parser.add_argument('--train_type',type=str,choices=training_type_options,default='full')\n",
        "    parser.add_argument('--show_shapes',action='store_true',help='print the shapes of hidden layers in enc and dec')\n",
        "    parser.add_argument('--verbose',action='store_true')\n",
        "    parser.add_argument('--window_size',type=int,default=512)\n",
        "    ARGS = parser.parse_args()\n",
        "\n",
        "    need_umap = False\n",
        "    if ARGS.is_uln:\n",
        "        ARGS.no_umap = True\n",
        "    if ARGS.short_epochs:\n",
        "        ARGS.num_meta_meta_epochs = 1\n",
        "        ARGS.num_meta_epochs = 1\n",
        "        ARGS.num_pseudo_label_epochs = 1\n",
        "    elif ARGS.full_epochs:\n",
        "        ARGS.num_meta_meta_epochs = 10\n",
        "        ARGS.num_meta_epochs = 10\n",
        "        ARGS.num_pseudo_label_epochs = 5\n",
        "    if ARGS.test:\n",
        "        ARGS.num_meta_epochs = 1\n",
        "        ARGS.num_meta_meta_epochs = 1\n",
        "        ARGS.num_pseudo_label_epochs = 1\n",
        "    elif not ARGS.no_umap and not ARGS.show_shapes: need_umap = True\n",
        "    print(ARGS)\n",
        "    dset_info_object = project_config.get_dataset_info_object(ARGS.dset)\n",
        "    all_possible_ids = dset_info_object.possible_subj_ids\n",
        "    if ARGS.all_subjs: ARGS.subj_ids=all_possible_ids\n",
        "    elif ARGS.num_subjs is not None: ARGS.subj_ids = all_possible_ids[:ARGS.num_subjs]\n",
        "    elif ARGS.subj_ids == ['first']: ARGS.subj_ids = all_possible_ids[:1]\n",
        "    bad_ids = [x for x in ARGS.subj_ids if x not in all_possible_ids]\n",
        "    if len(bad_ids) > 0 and not (ARGS.test and ARGS.dset=='HHAR'):\n",
        "        print(f\"You have specified non-existent ids: {bad_ids}\\nExistent ids are {all_possible_ids}\"); sys.exit()\n",
        "    return ARGS, need_umap\n",
        "\n",
        "RELEVANT_ARGS = ['ablate_label_filter','clusterer','dset','no_umap','num_meta_epochs','num_meta_meta_epochs','num_pseudo_label_epochs','reinit','step_size','subject_independent']\n",
        "\n",
        "\n",
        "# ARGS, need_umap = cl_args.get_cl_args()\n",
        "ARGS, need_umap = get_cl_args()\n",
        "# if need_umap: import umap\n",
        "# main(ARGS)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "0vScvFGGSeN6",
        "8DxjYI9RMQoP"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}