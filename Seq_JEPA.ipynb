{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/Seq_JEPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FA00-tf6MJ-s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "276afb23-f48f-48d5-be12-bb284707065b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-16 07:12:58--  https://archive.ics.uci.edu/static/public/507/wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset.zip’\n",
            "\n",
            "wisdm+smartphone+an     [          <=>       ] 295.92M  12.8MB/s    in 31s     \n",
            "\n",
            "2025-04-16 07:13:35 (9.41 MB/s) - ‘wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset.zip’ saved [310292805]\n",
            "\n",
            "Archive:  wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset.zip\n",
            " extracting: WISDM-dataset-description.pdf  \n",
            " extracting: wisdm-dataset.zip       \n",
            "Archive:  wisdm-dataset.zip\n",
            "   creating: wisdm-dataset/\n",
            "  inflating: wisdm-dataset/WISDM-dataset-description.pdf  \n",
            "   creating: wisdm-dataset/arffmagic-master/\n",
            "  inflating: wisdm-dataset/arffmagic-master/Makefile  \n",
            "  inflating: wisdm-dataset/arffmagic-master/.DS_Store  \n",
            " extracting: wisdm-dataset/arffmagic-master/README.md  \n",
            "   creating: wisdm-dataset/arffmagic-master/src/\n",
            "  inflating: wisdm-dataset/arffmagic-master/src/arff.cpp  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/comparator.h  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/chunk.h  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/main.cpp  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/attribute.h  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/libmfcc.c  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/raw.h  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/try.h  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/write.h  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/chunk.cpp  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/raw.cpp  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/globals.h  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/write.cpp  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/arff.h  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/except.h  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/read.cpp  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/attribute.cpp  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/libmfcc.h  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/globals.cpp  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/funcmap.cpp  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/read.h  \n",
            "  inflating: wisdm-dataset/arffmagic-master/src/funcmap.h  \n",
            "   creating: wisdm-dataset/arffmagic-master/bin/\n",
            "  inflating: wisdm-dataset/arffmagic-master/bin/magic  \n",
            "   creating: wisdm-dataset/arffmagic-master/bin/arffmagic.dSYM/\n",
            "   creating: wisdm-dataset/arffmagic-master/bin/arffmagic.dSYM/Contents/\n",
            "  inflating: wisdm-dataset/arffmagic-master/bin/arffmagic.dSYM/Contents/Info.plist  \n",
            "   creating: wisdm-dataset/arffmagic-master/bin/arffmagic.dSYM/Contents/Resources/\n",
            "   creating: wisdm-dataset/arffmagic-master/bin/arffmagic.dSYM/Contents/Resources/DWARF/\n",
            "  inflating: wisdm-dataset/arffmagic-master/bin/arffmagic.dSYM/Contents/Resources/DWARF/arffmagic  \n",
            "  inflating: wisdm-dataset/arffmagic-master/bin/.DS_Store  \n",
            "  inflating: wisdm-dataset/arffmagic-master/bin/arffmagic  \n",
            "   creating: wisdm-dataset/arffmagic-master/build/\n",
            "  inflating: wisdm-dataset/arffmagic-master/build/libmfcc.o  \n",
            "  inflating: wisdm-dataset/arffmagic-master/build/write.o  \n",
            "  inflating: wisdm-dataset/arffmagic-master/build/attribute.o  \n",
            "  inflating: wisdm-dataset/arffmagic-master/build/arff.o  \n",
            "  inflating: wisdm-dataset/arffmagic-master/build/read.o  \n",
            "  inflating: wisdm-dataset/arffmagic-master/build/raw.o  \n",
            "  inflating: wisdm-dataset/arffmagic-master/build/funcmap.o  \n",
            "  inflating: wisdm-dataset/arffmagic-master/build/main.o  \n",
            "  inflating: wisdm-dataset/arffmagic-master/build/globals.o  \n",
            "  inflating: wisdm-dataset/arffmagic-master/build/chunk.o  \n",
            "  inflating: wisdm-dataset/README.txt  \n",
            "   creating: wisdm-dataset/raw/\n",
            "   creating: wisdm-dataset/raw/phone/\n",
            "   creating: wisdm-dataset/raw/phone/gyro/\n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1631_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1644_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1639_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1621_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1609_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1627_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1628_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1620_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1633_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1638_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1614_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1625_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1612_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1640_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1616_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1647_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1646_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1637_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1608_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1642_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1624_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/.DS_Store  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1643_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1645_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1641_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1626_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1623_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1607_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1629_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1610_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1649_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1648_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1636_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1630_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1600_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1603_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1606_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1604_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1611_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1632_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1618_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1635_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1605_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1602_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1613_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1617_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1619_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1615_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1650_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1634_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1622_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/gyro/data_1601_gyro_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/.DS_Store  \n",
            "   creating: wisdm-dataset/raw/phone/accel/\n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1635_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1608_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1649_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1647_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1619_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1621_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1630_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1639_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1615_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1611_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1616_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1644_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1648_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1645_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1641_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1603_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1628_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1610_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1634_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1600_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/.DS_Store  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1613_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1624_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1620_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1606_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1633_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1632_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1626_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1643_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1637_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1640_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1627_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1650_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1614_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1617_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1618_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1642_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1612_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1646_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1601_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1605_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1623_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1607_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1638_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1636_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1629_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1604_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1622_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1625_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1602_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1609_accel_phone.txt  \n",
            "  inflating: wisdm-dataset/raw/phone/accel/data_1631_accel_phone.txt  \n",
            "   creating: wisdm-dataset/raw/watch/\n",
            "   creating: wisdm-dataset/raw/watch/gyro/\n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1629_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1633_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1626_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1644_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1625_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1649_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1631_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1632_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1622_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1628_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1602_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1603_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1610_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1609_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1647_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1638_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1642_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1627_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1601_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1646_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1643_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1636_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1635_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1617_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/.DS_Store  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1612_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1620_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1623_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1615_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1621_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1645_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1641_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1616_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1608_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1605_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1607_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1648_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1600_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1637_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1604_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1639_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1611_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1613_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1614_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1640_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1606_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1630_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1618_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1634_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1650_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1619_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/gyro/data_1624_gyro_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/.DS_Store  \n",
            "   creating: wisdm-dataset/raw/watch/accel/\n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1626_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1631_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1605_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1608_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1600_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1650_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1623_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1612_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1637_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1620_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1633_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1618_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1635_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1619_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1611_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1614_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1601_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1616_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/.DS_Store  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1645_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1606_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1615_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1638_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1636_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1607_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1627_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1625_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1640_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1643_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1602_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1644_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1629_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1642_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1610_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1624_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1603_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1621_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1646_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1628_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1604_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1634_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1613_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1647_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1609_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1630_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1639_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1617_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1648_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1632_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1649_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1641_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/watch/accel/data_1622_accel_watch.txt  \n",
            "  inflating: wisdm-dataset/raw/.DS_Store  \n",
            "   creating: wisdm-dataset/arff_files/\n",
            "   creating: wisdm-dataset/arff_files/phone/\n",
            "   creating: wisdm-dataset/arff_files/phone/gyro/\n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1610_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1612_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1637_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1622_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1604_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1639_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1621_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1623_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1640_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1645_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1602_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1649_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1600_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1644_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1646_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1632_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1608_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1635_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1606_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1629_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1617_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1628_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1636_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1601_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/.DS_Store  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1615_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1611_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1620_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1647_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1631_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1648_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1603_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1627_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1619_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1607_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1643_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1624_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1638_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1618_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1605_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1625_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1626_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1650_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1616_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1609_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1613_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1630_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1634_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1641_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1633_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/gyro/data_1642_gyro_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/.DS_Store  \n",
            "   creating: wisdm-dataset/arff_files/phone/accel/\n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1633_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1612_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1620_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1628_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1602_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1636_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1604_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1638_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1647_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1607_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1648_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1643_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1627_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1626_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1611_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1600_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1635_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1613_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1617_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1641_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1642_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1637_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1621_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/.DS_Store  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1632_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1629_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1630_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1603_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1631_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1615_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1609_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1644_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1616_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1634_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1622_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1649_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1645_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1606_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1601_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1639_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1610_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1608_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1624_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1618_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1650_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1646_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1625_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1640_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1619_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1605_accel_phone.arff  \n",
            "  inflating: wisdm-dataset/arff_files/phone/accel/data_1623_accel_phone.arff  \n",
            "   creating: wisdm-dataset/arff_files/watch/\n",
            "   creating: wisdm-dataset/arff_files/watch/gyro/\n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1620_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1604_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1613_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1631_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1635_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1639_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1628_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1603_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1632_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1626_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1618_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1627_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1609_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1646_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1650_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1629_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1621_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1607_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1611_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1637_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1602_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1617_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1619_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/.DS_Store  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1643_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1630_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1641_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1647_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1616_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1648_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1623_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1606_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1634_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1622_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1645_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1642_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1633_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1615_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1640_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1605_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1610_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1624_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1638_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1612_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1600_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1636_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1601_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1644_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1608_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1625_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/gyro/data_1649_gyro_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/.DS_Store  \n",
            "   creating: wisdm-dataset/arff_files/watch/accel/\n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1623_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1608_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1641_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1635_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1643_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1604_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1633_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1630_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1609_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1649_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1622_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1607_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1634_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1618_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1610_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1650_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1647_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1642_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1646_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1617_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1637_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1626_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1645_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1612_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1615_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/.DS_Store  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1620_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1632_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1613_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1616_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1644_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1628_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1603_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1606_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1625_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1648_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1639_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1602_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1619_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1636_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1638_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1640_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1624_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1600_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1629_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1631_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1611_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1601_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1621_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1627_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/watch/accel/data_1605_accel_watch.arff  \n",
            "  inflating: wisdm-dataset/arff_files/.DS_Store  \n",
            "  inflating: wisdm-dataset/change_raw_act.pl  \n",
            "  inflating: wisdm-dataset/.activity_key.txt.swp  \n",
            "  inflating: wisdm-dataset/activity_key.txt  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-e4f219ecf2d3>:30: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  activity_dataframe = pd.concat([activity_dataframe, df], ignore_index=True)\n",
            "<ipython-input-1-e4f219ecf2d3>:30: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  activity_dataframe = pd.concat([activity_dataframe, df], ignore_index=True)\n",
            "<ipython-input-1-e4f219ecf2d3>:30: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  activity_dataframe = pd.concat([activity_dataframe, df], ignore_index=True)\n",
            "<ipython-input-1-e4f219ecf2d3>:30: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  activity_dataframe = pd.concat([activity_dataframe, df], ignore_index=True)\n"
          ]
        }
      ],
      "source": [
        "# @title rohanpandey WISDM\n",
        "# https://github.com/rohanpandey/Analysis--WISDM-Smartphone-and-Smartwatch-Activity/blob/master/dataset_creation.ipynb\n",
        "! wget https://archive.ics.uci.edu/static/public/507/wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset.zip\n",
        "!unzip wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset.zip\n",
        "!unzip wisdm-dataset.zip\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "folder1=glob.glob(\"wisdm-dataset/raw/*\")\n",
        "# print(folder1)\n",
        "column_names = ['ID', 'activity','timestamp','x','y','z']\n",
        "overall_dataframe=pd.DataFrame(columns = column_names)\n",
        "for subfolder in folder1:\n",
        "    parent_dir = \"./processed/\"\n",
        "    path = os.path.join(parent_dir, subfolder.split('\\\\')[-1])\n",
        "    if not os.path.exists(path): os.makedirs(path)\n",
        "    folder2=glob.glob(subfolder+\"/*\")\n",
        "    for subsubfolder in folder2:\n",
        "        activity_dataframe = pd.DataFrame(columns = column_names)\n",
        "        subfolder_path = os.path.join(path, subsubfolder.split('/')[-1])\n",
        "        if not os.path.exists(subfolder_path): os.makedirs(subfolder_path)\n",
        "        files=glob.glob(subsubfolder+\"/*\")\n",
        "        for file in files:\n",
        "            # print(file)\n",
        "            df = pd.read_csv(file, sep=\",\",header=None)\n",
        "            df.columns = ['ID','activity','timestamp','x','y','z']\n",
        "            # activity_dataframe=activity_dataframe.append(df)\n",
        "            activity_dataframe = pd.concat([activity_dataframe, df], ignore_index=True)\n",
        "\n",
        "        activity_dataframe['z']=activity_dataframe['z'].str[:-1]\n",
        "        # activity_dataframe['meter']=subsubfolder.split('/')[-1]\n",
        "        # activity_dataframe['device']=subfolder.split('/')[-1]\n",
        "        activity_dataframe.to_csv(subfolder_path+'/data.csv',index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "CNVNCV8CGtR_"
      },
      "outputs": [],
      "source": [
        "# @title wisdm dataloader\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "# /content/processed/wisdm-dataset/raw/phone/accel/data.csv\n",
        "# /content/processed/wisdm-dataset/raw/watch/gyro/data.csv\n",
        "\n",
        "class BufferDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        # df_keep = pd.read_csv(\"data.csv\")#[['ID','activity','timestamp','x','y','z']]\n",
        "        df_keep = pd.read_csv(\"/content/processed/wisdm-dataset/raw/watch/accel/data.csv\")\n",
        "\n",
        "        user_acts = dict(tuple(df_keep.groupby(['ID','activity'])[['timestamp','x','y','z']]))\n",
        "        self.data = [[d.to_numpy(), a] for a, d in user_acts.items()]\n",
        "        self.act_dict = {i: act for i, act in enumerate(df_keep['activity'].unique())}\n",
        "        self.act_invdict = {v: k for k, v in self.act_dict.items()} # {'A': 0, 'B': 1, ...\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, id_act = self.data[idx]\n",
        "        id_act = self.process(id_act)\n",
        "        return torch.tensor(x[:3500]), id_act # 3567\n",
        "\n",
        "    def process(self, id_act):\n",
        "        return int(id_act[0]), self.act_invdict[id_act[1]]\n",
        "\n",
        "    def transform(self, act_list): # rand resize crop, rand mask # https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html\n",
        "        act_list = np.array(act_list)\n",
        "        try: hr, temp, heart= zip(*act_list)\n",
        "        except ValueError: print('err:',act_list)\n",
        "        kind = 'nearest' if len(act_list)>self.seq_len/.7 else 'linear'\n",
        "        temp_interpolator = scipy.interpolate.interp1d(hr, temp, kind=kind) # linear nearest quadratic cubic\n",
        "        heart_interpolator = scipy.interpolate.interp1d(hr, heart, kind=kind) # linear nearest quadratic cubic\n",
        "        hr_ = np.sort(np.random.uniform(hr[0], hr[-1], round(self.seq_len*random.uniform(1,1/.7))))\n",
        "        temp_, heart_ = temp_interpolator(hr_), heart_interpolator(hr_)\n",
        "        act_list = list(zip(hr_, temp_, heart_))\n",
        "\n",
        "        idx = torch.randint(len(act_list)-self.seq_len+1, size=(1,))\n",
        "        # return act_list[idx: idx+self.seq_len]\n",
        "        act_list = act_list[idx: idx+self.seq_len]\n",
        "\n",
        "        # mask=(torch.rand(self.seq_len)<.1) # True -> masked # random masking\n",
        "        # # act_list[mask] = self.pad[0]\n",
        "        # act_list = [self.pad[0] if m else a for m,a in zip(mask, act_list)]\n",
        "        return act_list\n",
        "\n",
        "    def add(self, episode):\n",
        "        self.data.append(episode)\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    # print(npimg.shape) # (3, 64, 64)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# df_keep = pd.read_csv(\"data.csv\")#[['ID','activity','timestamp','x','y','z']]\n",
        "# user_acts = dict(tuple(df_keep.groupby(['ID','activity'])[['timestamp','x','y','z']]))\n",
        "# dataset = [[d.to_numpy(), a] for a, d in user_acts.items()]\n",
        "\n",
        "\n",
        "train_data = BufferDataset() # one line of poem is roughly 50 characters\n",
        "\n",
        "dataset_size = len(train_data)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(0.7 * dataset_size))\n",
        "np.random.seed(0)\n",
        "np.random.shuffle(indices)\n",
        "train_indices, val_indices = indices[:split], indices[split:]\n",
        "\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "valid_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 64 #512\n",
        "# train_loader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "# test_loader = DataLoader(train_data, sampler=valid_sampler, batch_size=batch_size, num_workers=2, drop_last=True)\n",
        "train_loader = DataLoader(train_data, sampler=train_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "test_loader = DataLoader(train_data, sampler=valid_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ge36SCxOl2Oq"
      },
      "outputs": [],
      "source": [
        "# @title RoPE\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def RoPE(dim, seq_len=512, base=10000):\n",
        "    theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "    pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "    angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "    rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [1, seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "    return rot_emb\n",
        "\n",
        "# class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "#     def __init__(self, dim, seq_len=512, base=10000):\n",
        "#         super().__init__()\n",
        "#         self.dim, self.base = dim, base\n",
        "#         theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "#         pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "#         angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "#         self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         seq_len = x.size(1)\n",
        "#         if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.base)\n",
        "#         return x * self.rot_emb[:seq_len]\n",
        "\n",
        "class RotEmb(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, top=torch.pi, base=10000):\n",
        "        super().__init__()\n",
        "        self.theta = top / (base ** (torch.arange(0, dim, step=2, device=device) / dim))\n",
        "        # self.theta = top / (base ** torch.linspace(0, 1, dim//2, device=device))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (pos.unsqueeze(-1) * self.theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [seq_len, dim]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "f6T4F651kmGh"
      },
      "outputs": [],
      "source": [
        "# @title AttentionBlock\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class SelfAttn(nn.Module):\n",
        "    def __init__(self, dim, n_heads):\n",
        "        super().__init__()\n",
        "        self.dim, self.n_heads = dim, n_heads\n",
        "        d_head = dim//n_heads\n",
        "        self.qkv = nn.Linear(dim, dim*3, bias=False)\n",
        "        # self.lin = nn.Linear(dim, dim)\n",
        "        self.lin = zero_module(nn.Linear(dim, dim))\n",
        "        # self.rope = RoPE(d_head, seq_len=512, base=10000)\n",
        "        # self.rope = RoPE2D(d_head, h=64, w=64, base=100)\n",
        "        self.scale = d_head**-.5\n",
        "\n",
        "    def forward(self, x): # [b,t,d]\n",
        "        q,k,v = self.qkv(x).unflatten(-1, (self.n_heads,-1)).transpose(1,2).chunk(3, dim=-1) # [b, t, n_heads, d_head] -> [b, n_heads, t, d_head]\n",
        "        # q, k = self.rope(q), self.rope(k)\n",
        "        q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2)\n",
        "        context = k.transpose(-2,-1) @ v # [batch, n_heads, d_head, d_head]\n",
        "        x = q @ context # [b, n_heads, t, d_head]\n",
        "        # print('SelfAttn', x.shape)\n",
        "        x = x.transpose(1,2).flatten(2)\n",
        "        return self.lin(x)\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, mult=4, drop=0.):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        # self.norm = nn.RMSNorm(d_model, elementwise_affine=False) # LayerNorm RMSNorm\n",
        "        self.norm1, self.norm2 = nn.LayerNorm(d_model), nn.LayerNorm(d_model)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "        self.attn = SelfAttn(d_model, n_heads)\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        ff_dim=d_model*mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), act, nn.Dropout(drop), nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "            nn.RMSNorm(ff_dim), act, nn.Dropout(drop), zero_module(nn.Linear(ff_dim, d_model))\n",
        "        )\n",
        "\n",
        "    def forward(self, x): # [batch, seq_len, d_model]\n",
        "        # print('attnblk fwd',x.shape)\n",
        "        x = x + self.drop(self.attn(self.norm1(x)))\n",
        "        x = x + self.ff(x)\n",
        "        # x = x + self.drop(self.ff(self.norm2(x)))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pQfM2fYvcTh6"
      },
      "outputs": [],
      "source": [
        "# @title masks\n",
        "import torch\n",
        "\n",
        "def multiblock(seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "    return target_mask\n",
        "\n",
        "def multiblock(seq, min_s, max_s, B=64, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    # mask_len = torch.rand(B, 1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_len = torch.rand(1, M) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(B, M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    # indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    indices = torch.arange(seq)[None,None,...] # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [B, M, seq]\n",
        "    return target_mask\n",
        "\n",
        "def multiblock2d(hw=(8,8), scale=(.15,.2), aspect_ratio=(.75,1.5), M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_aspect = torch.rand(1) * (aspect_ratio[1] - aspect_ratio[0]) + aspect_ratio[0] # in (min_s, max_s) # all blocks same size\n",
        "    mask_scale = torch.rand(1) * (scale[1] - scale[0]) + scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    h = (mask_scale/mask_aspect)**.5# h*(h*aspect) = scale\n",
        "    w = h * mask_aspect\n",
        "    h_pos, w_pos = torch.rand(M)*(1-w), torch.rand(M)*(1-h) # in (0, 1 - mask_len)\n",
        "    h_len, h_pos = (h*hw[0]).int(), h_pos*hw[0]\n",
        "    w_len, w_pos = (w*hw[1]).int(), w_pos*hw[1]\n",
        "    h_ind, w_ind = torch.arange(hw[0]).unsqueeze(0), torch.arange(hw[1]).unsqueeze(0) # [1, seq]\n",
        "    h_mask = (h_ind>=h_pos.unsqueeze(-1)) & (h_ind<(h_pos+h_len).unsqueeze(-1)) # [M, seq]\n",
        "    w_mask = (w_ind>=w_pos.unsqueeze(-1)) & (w_ind<(w_pos+w_len).unsqueeze(-1)) # [M, seq]\n",
        "    target_mask = h_mask.unsqueeze(-1) & w_mask.unsqueeze(-2) # [M, seq, seq]\n",
        "    return target_mask\n",
        "\n",
        "def multiblock2d(hw=(8,8), scale=(.15,.2), aspect_ratio=(.75,1.5), B=64, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_aspect = torch.rand(B, 1) * (aspect_ratio[1] - aspect_ratio[0]) + aspect_ratio[0] # in (min_s, max_s) # all blocks same size\n",
        "    mask_scale = torch.rand(B, 1) * (scale[1] - scale[0]) + scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    h = (mask_scale/mask_aspect)**.5# h*(h*aspect) = scale\n",
        "    w = h * mask_aspect\n",
        "    h_pos, w_pos = torch.rand(M)*(1-w), torch.rand(M)*(1-h) # in (0, 1 - mask_len)\n",
        "    h_len, h_pos = (h*hw[0]).int(), h_pos*hw[0]\n",
        "    w_len, w_pos = (w*hw[1]).int(), w_pos*hw[1]\n",
        "    h_ind, w_ind = torch.arange(hw[0]).unsqueeze(0), torch.arange(hw[1]).unsqueeze(0) # [1, seq]\n",
        "    h_mask = (h_ind>=h_pos.unsqueeze(-1)) & (h_ind<(h_pos+h_len).unsqueeze(-1)) # [M, seq]\n",
        "    w_mask = (w_ind>=w_pos.unsqueeze(-1)) & (w_ind<(w_pos+w_len).unsqueeze(-1)) # [M, seq]\n",
        "    target_mask = h_mask.unsqueeze(-1) & w_mask.unsqueeze(-2) # [M, seq, seq]\n",
        "    return target_mask\n",
        "\n",
        "# https://arxiv.org/pdf/2210.07224\n",
        "def randpatch(seq, mask_size=8, gamma=0.9): # num patches of seq, mask patch size, masking ratio\n",
        "    # mask = torch.rand(seq//mask_size)<gamma\n",
        "    length = seq//mask_size\n",
        "    g = torch.normal(gamma, std=.1, size=(1,)).clamp(.5,.9)\n",
        "    # g = gamma\n",
        "    idx = torch.randperm(length)[:int(length*g)]\n",
        "    mask = torch.zeros(length, dtype=bool)\n",
        "    mask[idx] = True\n",
        "    mask = mask.repeat_interleave(mask_size, dim=-1)\n",
        "    return mask # [seq] , True -> mask\n",
        "\n",
        "\n",
        "import torch\n",
        "def apply_masks(x, mask): # [b,t,d], [mask_size] # https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "    mask_keep = mask.unsqueeze(-1).repeat(x.size(0), 1, x.size(-1)) # [batch,T,dim]\n",
        "    return torch.gather(x, dim=1, index=mask_keep) # [batch,mask_size,dim]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fh9o__m2-j7K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "outputId": "ff86ae23-5de4-45d7-f8fd-ba8af33a45b5",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/268.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m266.2/268.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.0/268.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title simplex\n",
        "# !pip install -q opensimplex\n",
        "import opensimplex\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def simplexmask1d(seq=512, ctx_scale=(.85,1), trg_scale=(.6,.8), B=64, chaos=[1,.5]):\n",
        "    i = np.linspace(0, chaos[0], num=seq) # 2-5\n",
        "    noise = opensimplex.noise2array(i, np.random.randint(1e10, size=B)) # [B, seq]\n",
        "    noise = torch.from_numpy(noise)\n",
        "    ctx_mask_scale = torch.rand(1) * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    trg_mask_scale = torch.rand(1) * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "\n",
        "    ctx_len, trg_len = int(seq*ctx_mask_scale), int(seq*trg_mask_scale)\n",
        "    val, trg_index = torch.topk(noise, trg_len, dim=1, sorted=False)\n",
        "    ctx_len = ctx_len - trg_len\n",
        "\n",
        "    remove_mask = torch.ones((B,seq), dtype=bool) # [B, S]\n",
        "    remove_mask.scatter_(1, trg_index, False).flatten()\n",
        "    ind = torch.arange(seq).unsqueeze(0).repeat(B,1)[remove_mask].reshape(B, -1)\n",
        "\n",
        "    i = np.linspace(0, chaos[1], num=seq) # 2-5\n",
        "    noise = opensimplex.noise2array(i, np.random.randint(1e10, size=B)) # [B, seq]\n",
        "    noise = torch.from_numpy(noise)[remove_mask].reshape(B, -1)\n",
        "    val, ctx_ind = torch.topk(noise, ctx_len, dim=1, sorted=False)\n",
        "    ctx_index = ind[torch.arange(B).unsqueeze(-1), ctx_ind]\n",
        "    return ctx_index, trg_index\n",
        "\n",
        "b=64\n",
        "ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.7,.8), trg_scale=(.4,.6), B=b, chaos=[3,.5])\n",
        "mask = torch.zeros(b ,200)\n",
        "mask[torch.arange(b).unsqueeze(-1), trg_index] = 1\n",
        "mask[torch.arange(b).unsqueeze(-1), ctx_index] = .5\n",
        "mask = mask[None,...]\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "imshow(mask)\n",
        "# import torchvision\n",
        "# imshow(torchvision.utils.make_grid(mask, nrow=8))\n",
        "\n",
        "# print(index)\n",
        "# print(index.shape)\n",
        "# print(mask)\n",
        "# print(mask.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Xn7WZShwWxF8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "outputId": "b39a2d86-4119-433a-a671-96dbc5a3dbb0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title ijepa multiblock 1d\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "import math\n",
        "from multiprocessing import Value\n",
        "import torch\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(self, length=200,\n",
        "        enc_mask_scale=(0.2, 0.8), pred_mask_scale=(0.2, 0.8),\n",
        "        nenc=1, npred=2, min_keep=4, allow_overlap=False):\n",
        "        super().__init__()\n",
        "        self.length = length\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "\n",
        "    def _sample_block_size(self, scale):\n",
        "        _rand = torch.rand(1).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.length * mask_scale) # num patches to keep\n",
        "        # -- Sample block aspect-ratio\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        l = max_keep#int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        while l >= self.length: l -= 1 # crop mask to be smaller than img\n",
        "        return l\n",
        "\n",
        "    def _sample_block_mask(self, length, acceptable_regions=None):\n",
        "        l = length\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # --\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            left = torch.randint(0, self.length - l, (1,))\n",
        "            mask = torch.zeros(self.length, dtype=torch.int32)\n",
        "            mask[left:left+l] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones(self.length, dtype=torch.int32)\n",
        "        mask_complement[left:left+l] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, B):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        p_size = self._sample_block_size(scale=self.pred_mask_scale)\n",
        "        e_size = self._sample_block_size(scale=self.enc_mask_scale)\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.length\n",
        "        min_keep_enc = self.length\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                print(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "        return collated_masks_enc, collated_masks_pred\n",
        "\n",
        "batch=64\n",
        "length=200\n",
        "mask_collator = MaskCollator(length=length, enc_mask_scale=(.85, 1.), pred_mask_scale=(.15, .2),\n",
        "        nenc=1, npred=4, min_keep=4,\n",
        "        # allow_overlap=True)\n",
        "        allow_overlap=False)\n",
        "\n",
        "collated_masks_enc, collated_masks_pred = mask_collator(batch)\n",
        "context_indices, trg_indices = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "# print(context_indices.shape, trg_indices.shape)\n",
        "\n",
        "\n",
        "# trg_mask = torch.zeros(1 ,200)\n",
        "# trg_mask[:, trg_indices[:1]] = 1\n",
        "# ctx_mask = torch.zeros(1 ,200)\n",
        "# ctx_mask[:, context_indices[:1]] = 1\n",
        "# plt.rcParams[\"figure.figsize\"] = (20,1)\n",
        "# plt.pcolormesh(trg_mask)\n",
        "# plt.show()\n",
        "# plt.pcolormesh(ctx_mask)\n",
        "# plt.show()\n",
        "\n",
        "b=64\n",
        "mask = torch.zeros(batch ,length)\n",
        "mask[torch.arange(batch).unsqueeze(-1), trg_indices] = 1\n",
        "mask[torch.arange(batch).unsqueeze(-1), context_indices] = .5\n",
        "mask = mask[None,...]\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "imshow(mask)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(context_indices.shape, trg_indices.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sh7RGERyZ_Ma",
        "outputId": "eba62f59-e09c-4bc0-f88d-35ffa9c8d5af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 1, 69]) torch.Size([64, 124])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-zdjdJixtOu",
        "outputId": "7647dfa5-4ac8-44c8-ffbb-59f18c1b9e2c",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "38128\n",
            "torch.Size([4, 110, 16])\n"
          ]
        }
      ],
      "source": [
        "# @title TransformerModel/Predictor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, n_heads=4, d_hid=None, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, 200, d_model)*.02)\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=200, base=10000), requires_grad=False)\n",
        "\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, n_heads=n_heads) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.randn(1,1,d_model)*0.02) # randn zeros\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim)# if out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = x.shape\n",
        "        # x = x * self.pos_enc(context_indices)\n",
        "        # print(\"Trans pred\",x.shape, self.pos_emb[0,context_indices].shape)\n",
        "        x = x + self.pos_emb[0,context_indices]\n",
        "        # print('pred fwd', self.pos_emb[:,context_indices].shape)\n",
        "\n",
        "        # pred_tokens = self.cls * self.pos_enc(trg_indices) # [M, num_trg_toks, d_model]\n",
        "        pred_tokens = self.cls + self.pos_emb[0,trg_indices]\n",
        "        # print(\"pred fwd\", x.shape, pred_tokens.shape)\n",
        "        x = torch.cat([x, pred_tokens], dim=1) # [batch, seq_len+num_trg_toks, d_model]\n",
        "        out = self.transformer(x)\n",
        "\n",
        "        out = self.norm(out)\n",
        "        out = out[:,seq:] # [batch, num_trg_toks, d_model]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, drop=0):\n",
        "    def __init__(self, patch_size, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0):\n",
        "        super().__init__()\n",
        "        # patch_size=32\n",
        "        self.embed = nn.Sequential(\n",
        "            # # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.MaxPool1d(2,2), #nn.MaxPool1d(3, 2, 3//2),\n",
        "            # # nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.Conv1d(d_model, d_model,3,2,3//2)\n",
        "            nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2),\n",
        "            # nn.Conv1d(in_dim, d_model, patch_size, patch_size), # like patch\n",
        "            )\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, 200, d_model)*.02)\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=200, base=10000), requires_grad=False)#.unsqueeze(0)\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, n_heads=n_heads) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "        # try: print(\"Trans fwd\",x.shape, context_indices.shape)\n",
        "        # except: print(\"Trans fwd noind\",x.shape)\n",
        "        # x = self.pos_enc(x)\n",
        "        x = x + self.pos_emb[:,:x.shape[1]]\n",
        "        if context_indices != None: x = x[torch.arange(x.shape[0]).unsqueeze(-1), context_indices] # [batch, num_context_toks, d_model]\n",
        "\n",
        "        # print(\"TransformerModel\",x.shape)\n",
        "        x = self.transformer(x)\n",
        "        out = self.norm(x)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "batch, seq_len, d_model = 4,3500,16\n",
        "in_dim = 3\n",
        "patch_size=32\n",
        "model = TransformerModel(patch_size, in_dim, d_model, n_heads=4, nlayers=10, drop=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "# # # print(out)\n",
        "# model = TransformerPredictor(in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1).to(device)\n",
        "# out = model(out)\n",
        "# print(out.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ardu1zJwdHM3",
        "outputId": "54324b46-fccf-43fe-fff8-bc2e62ddd0d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "109920\n",
            "torch.Size([])\n"
          ]
        }
      ],
      "source": [
        "# @title SeqJEPA\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class SeqJEPA(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, nlayers=2, n_heads=4):\n",
        "        super().__init__()\n",
        "        if out_dim is None: out_dim = d_model\n",
        "        self.patch_size = 32\n",
        "        self.student = TransformerModel(self.patch_size, in_dim, d_model, out_dim=out_dim, n_heads=n_heads, nlayers=nlayers, drop=0.)\n",
        "        self.predicter = TransformerPredictor(out_dim, d_model//2, out_dim, n_heads=4, nlayers=1, drop=0.)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "        # self.transform = RandomResizedCrop1d(3500, scale=(.8,1.))\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]\n",
        "        batch, seq, dim = x.shape\n",
        "        # target_mask = multiblock(seq//self.patch_size, min_s=.2, max_s=.3, M=4, B=1).any(1).squeeze(1) # best.2.3M4 og.15.2M4# mask out targets to be predicted # [M, seq]\n",
        "        # # target_mask = randpatch(seq//self.patch_size, mask_size=8, gamma=.9).unsqueeze(0) # 8.9 [seq]\n",
        "\n",
        "        # print(target_mask.shape, x.shape)\n",
        "        # context_mask = ~multiblock(seq//self.patch_size, min_s=.85, max_s=1, M=1, B=1).squeeze(1)|target_mask # og .85,1.M1 # [1, seq], True->Mask\n",
        "        # context_mask = torch.zeros((1,seq//self.patch_size), dtype=bool)|target_mask # [1,h,w], True->Mask\n",
        "\n",
        "        # context_indices, trg_indices = simplexmask1d(seq//self.patch_size, ctx_scale=(.7,.8), trg_scale=(.5,.7), B=batch, chaos=[3,.5])\n",
        "        context_indices, trg_indices = simplexmask1d(seq//self.patch_size, ctx_scale=(.85,1), trg_scale=(.5,.8), B=batch, chaos=[3,.5])\n",
        "\n",
        "        # trg_indices = trg_indices.repeat(batch,1)\n",
        "        # context_mask = ~context_mask|target_mask # [1,]\n",
        "        # context_indices = (~context_mask).nonzero()[:,1].unsqueeze(0).repeat(batch,1)\n",
        "        # # print(trg_indices.shape, context_indices.shape)\n",
        "        # # print(context_mask.shape,target_mask.shape, x.shape)\n",
        "        # target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "        # # # target_mask, context_mask = target_mask.repeat(batch,1), context_mask.repeat(batch,1)\n",
        "        # # x_ = x * F.adaptive_avg_pool1d((~context_mask).float(), x.shape[1]).unsqueeze(-1) # zero masked locations\n",
        "\n",
        "        # context_indices = (~context_mask).nonzero()[:,1].unflatten(0, (batch,-1)) # int idx [num_context_toks] , idx of context not masked\n",
        "        # trg_indices = target_mask.nonzero()[:,1].unflatten(0, (batch,-1)) # int idx [num_trg_toks] , idx of targets that are masked\n",
        "\n",
        "\n",
        "        # mask_collator = MaskCollator(length=seq//self.patch_size, enc_mask_scale=(.85,1), pred_mask_scale=(.15,.2), nenc=1, npred=4, min_keep=4, allow_overlap=False)\n",
        "        # collated_masks_enc, collated_masks_pred = mask_collator(batch) # idx of ctx, idx of masked trg\n",
        "        # # collated_masks_enc, collated_masks_pred = mask_collator(1) # idx of ctx, idx of masked trg\n",
        "        # context_indices, trg_indices = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "        # # # zero_mask = torch.zeros(batch ,seq//self.patch_size, device=device)\n",
        "        # # # zero_mask[torch.arange(batch).unsqueeze(-1), context_indices] = 1\n",
        "        # # zero_mask = torch.zeros(1 ,seq//self.patch_size, device=device)\n",
        "        # # zero_mask[:, context_indices] = 1\n",
        "        # # x_ = x * F.adaptive_avg_pool1d(zero_mask, x.shape[1]).unsqueeze(-1) # zero masked locations\n",
        "        # # print(context_indices.shape, trg_indices.shape)\n",
        "        # # context_indices, trg_indices = context_indices.repeat(batch,1), trg_indices.repeat(batch,1)\n",
        "\n",
        "\n",
        "        # print('x_',x_.shape, context_indices.shape, trg_indices.shape)\n",
        "\n",
        "        sx = self.student(x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "        # print('seq_jepa loss sx',sx.shape)\n",
        "        sy_ = self.predicter(sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "        sy_ = F.layer_norm(sy_, (sy_.size(-1),))\n",
        "        with torch.no_grad():\n",
        "            sy = self.teacher(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "            sy = sy[torch.arange(sy.shape[0]).unsqueeze(-1), trg_indices] # [batch, num_context_toks, d_model] # nan bec len(trg_ind)==0 # print('loss sy',torch.isnan(sy).any())\n",
        "            sy = F.layer_norm(sy, (sy.size(-1),))\n",
        "        loss = F.mse_loss(sy, sy_)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        sx = self.student(x)\n",
        "        out = sx.mean(dim=1)\n",
        "        return out\n",
        "\n",
        "# min_s=0.15, max_s, M\n",
        "# trg.15.2M4 C.85 1\n",
        "\n",
        "# 1e-2,1e-3 < 3e-3,1e-3\n",
        "# patch16 < patch32\n",
        "# NoPE good but sus\n",
        "\n",
        "\n",
        "# seq_jepa = SeqJEPA(in_dim=3, d_model=32, out_dim=16, nlayers=4, n_heads=4).to(device)#.to(torch.float)\n",
        "seq_jepa = SeqJEPA(in_dim=3, d_model=64, out_dim=16, nlayers=1, n_heads=8).to(device)#.to(torch.float)\n",
        "optim = torch.optim.AdamW(seq_jepa.parameters(), lr=1e-3) # 1e-3?\n",
        "# optim = torch.optim.AdamW([{'params': seq_jepa.student.parameters()},\n",
        "#     {'params': seq_jepa.predicter.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # default 1e-2, 5e-2\n",
        "    # {'params': seq_jepa.predicter.parameters(), 'lr': 1e-2}], lr=1e-3, weight_decay=1e-2)\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/configs/in1k_vith14_ep300.yaml\n",
        "# d_model 1024,384\n",
        "# depth 12,6/12\n",
        "# wd 5e-2 - 4e-1\n",
        "# adamw 1e-4 - 1e-3 - 1e-6\n",
        "# ema 0.996-1\n",
        "\n",
        "print(sum(p.numel() for p in seq_jepa.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.parameters())) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.predicter.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.student.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.teacher.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# d_model^2 * nlayers\n",
        "\n",
        "x = torch.rand((24, 3500, in_dim), device=device)\n",
        "out = seq_jepa.loss(x)\n",
        "print(out.shape)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(16, 18).to(device)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n",
        "# optim = torch.optim.AdamW([{'params': seq_jepa.parameters()}, {'params': classifier.parameters(), 'lr': 1e-3}], lr=1e-3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4rj4LfPuN1H",
        "outputId": "24fa1196-5fe9-4519-c50d-6b788393c491"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 16])\n"
          ]
        }
      ],
      "source": [
        "# @title TransformerVICReg\n",
        "import torch\n",
        "from torch import nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class TransformerVICReg(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, n_heads=4, d_hid=None, nlayers=1, drop=0):\n",
        "        super().__init__()\n",
        "        act = nn.GELU()\n",
        "        patch_size=4\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Linear(in_dim, d_model), act\n",
        "            # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(3,2, 3//2),\n",
        "            nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2),\n",
        "            # nn.Conv1d(in_dim, d_model, patch_size, patch_size), # patch\n",
        "            )\n",
        "        self.pos_enc = RoPE(d_model, seq_len=200, base=10000) # 10000\n",
        "        # self.pos_emb = nn.Parameter(torch.randn(1, 200, d_model))\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=200, base=10000).unsqueeze(0), requires_grad=False)\n",
        "\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, n_heads=n_heads) for _ in range(nlayers)])\n",
        "\n",
        "        # out_dim = out_dim or d_model\n",
        "        self.lin = nn.Linear(d_model, out_dim)\n",
        "        self.attn_pool = nn.Linear(d_model, 1, bias=False)\n",
        "\n",
        "        dim_v = d_model * 4\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(out_dim, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v, bias=False),\n",
        "            )\n",
        "\n",
        "    def forward(self, x): # [b,t,d] / [b,c,h,w]\n",
        "        # x = self.embed(x).flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c]\n",
        "        x = self.embed(x.transpose(-2,-1)).transpose(-2,-1) # [b,t,d]\n",
        "        # x = self.embed(x)\n",
        "        x = self.pos_enc(x)\n",
        "        # x = x + self.pos_emb[:,:x.shape[1]]\n",
        "\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        attn = self.attn_pool(x).squeeze(-1) # [batch, seq] # seq_pool\n",
        "        out = (torch.softmax(attn, dim=-1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, seq] @ [batch, seq, dim] -> [batch, dim]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch, ntoken]\n",
        "\n",
        "    def expand(self, x):\n",
        "        sx = self.forward(x)\n",
        "        vx = self.exp(sx)\n",
        "        return vx\n",
        "\n",
        "batch, seq_len, d_model = 4,3500,32\n",
        "in_dim, out_dim=3,16\n",
        "model = TransformerVICReg(in_dim, d_model, out_dim, n_heads=4, nlayers=2, drop=0.).to(device)\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "# x =  torch.rand((batch, in_dim, 32,32), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObiHp-LSuRBA",
        "outputId": "c6ce8734-e0a0-47b2-b11d-e0f8e53af582"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "in vicreg  1.9939420276162247e-16 24.746832251548767 1.6621681808715039e-09\n",
            "(tensor(7.9758e-18, device='cuda:0', grad_fn=<MseLossBackward0>), tensor(0.9899, device='cuda:0', grad_fn=<AddBackward0>), tensor(1.6622e-09, device='cuda:0', grad_fn=<AddBackward0>))\n"
          ]
        }
      ],
      "source": [
        "# @title Violet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class Violet(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, nlayers=2, n_heads=4):\n",
        "        super().__init__()\n",
        "        out_dim = out_dim or d_model\n",
        "        self.student = TransformerVICReg(in_dim, d_model, out_dim=out_dim, n_heads=n_heads, nlayers=nlayers, drop=0.)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "\n",
        "        # vicreg\n",
        "        self.sim_coeff=25. # 10.0 # 25.0 # λ\n",
        "        self.std_coeff=25. # 1.0 # 25.0 # µ\n",
        "        self.cov_coeff=1. # 25.0 # 1.0 # ν\n",
        "\n",
        "        # self.transform = RandomResizedCrop1d(3500, scale=(.8,1.))\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]c/ [b,c,h,w]\n",
        "        # print(x.shape)\n",
        "        # self.transform(x)\n",
        "        vx = self.student.expand(x) # [batch, num_context_toks, out_dim]\n",
        "        with torch.no_grad(): vy = self.teacher.expand(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "        loss = self.vicreg(vx, vy)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        return self.student(x)\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size, num_features = x.shape\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = self.sim_coeff * repr_loss + self.std_coeff * std_loss + self.cov_coeff * cov_loss\n",
        "        print(\"in vicreg \",self.sim_coeff * repr_loss.item() , self.std_coeff * std_loss.item() , self.cov_coeff * cov_loss.item())\n",
        "        # return loss\n",
        "        return repr_loss, std_loss, cov_loss\n",
        "\n",
        "# lr1e-3\n",
        "# n_heads=8 < 4\n",
        "\n",
        "in_dim=3\n",
        "violet = Violet(in_dim, d_model=32, out_dim=16, nlayers=2, n_heads=4).to(device)\n",
        "voptim = torch.optim.AdamW(violet.parameters(), lr=1e-3) # 1e-3?\n",
        "# voptim = torch.optim.AdamW([{'params': violet.student.transformer.parameters()},\n",
        "#     {'params': violet.student.exp.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # default 1e-2\n",
        "# voptim = torch.optim.AdamW([{'params': violet.student.exp.parameters(), 'lr': 3e-3},\n",
        "#     {'params': [p for n, p in violet.named_parameters() if 'student.exp' not in n]}], lr=1e-3, weight_decay=1e-2) # default 1e-2\n",
        "\n",
        "# print(sum(p.numel() for p in violet.parameters() if p.requires_grad)) # 27584\n",
        "\n",
        "\n",
        "x = torch.rand((2,1000,in_dim), device=device)\n",
        "loss = violet.loss(x)\n",
        "# print(out.shape)\n",
        "print(loss)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(16, 18).to(device) # torch/autograd/graph.py RuntimeError: CUDA error: device-side assert triggered CUDA kernel errors\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IACKDymhit7"
      },
      "outputs": [],
      "source": [
        "# optim.param_groups[0]['lr'] = 1e-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7zghfu_jeOQk"
      },
      "outputs": [],
      "source": [
        "# @title RankMe\n",
        "# RankMe: Assessing the Downstream Performance of Pretrained Self-Supervised Representations by Their Rank jun 2023 https://arxiv.org/pdf/2210.02885\n",
        "import torch\n",
        "\n",
        "# https://github.com/Spidartist/IJEPA_endoscopy/blob/main/src/helper.py#L22\n",
        "def RankMe(Z):\n",
        "    \"\"\"\n",
        "    Calculate the RankMe score (the higher, the better).\n",
        "    RankMe(Z) = exp(-sum_{k=1}^{min(N, K)} p_k * log(p_k)),\n",
        "    where p_k = sigma_k (Z) / ||sigma_k (Z)||_1 + epsilon\n",
        "    where sigma_k is the kth singular value of Z.\n",
        "    where Z is the matrix of embeddings (N × K)\n",
        "    \"\"\"\n",
        "    # compute the singular values of the embeddings\n",
        "    # _u, s, _vh = torch.linalg.svd(Z, full_matrices=False)  # s.shape = (min(N, K),)\n",
        "    # s = torch.linalg.svd(Z, full_matrices=False).S\n",
        "    s = torch.linalg.svdvals(Z)\n",
        "    p = s / torch.sum(s, axis=0) + 1e-7\n",
        "    return torch.exp(-torch.sum(p * torch.log(p)))\n",
        "\n",
        "# Z = torch.randn(5, 3)\n",
        "# o = RankMe(Z)\n",
        "# print(o)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Pj25OrEk14pR"
      },
      "outputs": [],
      "source": [
        "# @title LiDAR stable_ssl next\n",
        "# https://github.com/rbalestr-lab/stable-ssl/blob/main/stable_ssl/monitors.py#L106\n",
        "\n",
        "def LiDAR(embeddings, eps=1e-7, delta=1e-3):\n",
        "    embeddings = embeddings.unflatten(0, (-1,2))\n",
        "    (local_n, q, d), device = embeddings.shape, embeddings.device\n",
        "\n",
        "    class_means = embeddings.mean(dim=1) # mu_x\n",
        "    grand_mean_local = class_means.mean(dim=0) # mu\n",
        "    # print(embeddings.shape, class_means.shape, grand_mean_local.shape) # [50, 2, 64], [50, 64], [64]\n",
        "\n",
        "    # local_Sb = torch.zeros(d, d, device=device)\n",
        "    # local_Sw = torch.zeros(d, d, device=device)\n",
        "\n",
        "\n",
        "    # print(diff_b.shape, diff_w.shape) # [64,1], [64,1]\n",
        "    # print(local_Sb.shape, local_Sw.shape) # [64,64], [64,64]\n",
        "\n",
        "    diff_b = (class_means - grand_mean_local).unsqueeze(-1)\n",
        "    # print(diff_b.shape)\n",
        "    # # local_Sb = (diff_b @ diff_b.T).sum()\n",
        "    local_Sb = (diff_b @ diff_b.transpose(-2,-1)).sum(0)\n",
        "    # # print(embeddings.shape, class_means.shape)\n",
        "    diff_w = (embeddings - class_means.unsqueeze(1)).reshape(-1,d,1)\n",
        "    # # print(diff_w.shape)\n",
        "    # # local_Sw = (diff_w @ diff_w.T).sum()\n",
        "    local_Sw = (diff_w @ diff_w.transpose(-2,-1)).sum(0)\n",
        "\n",
        "    # print(local_Sb.shape, local_Sw.shape)\n",
        "    S_b = local_Sb / (local_n - 1)\n",
        "    S_w = local_Sw / (local_n * (q - 1))\n",
        "    S_w += delta * torch.eye(d, device=device)\n",
        "    # print(S_w.shape, d)\n",
        "\n",
        "    eigvals_w, eigvecs_w = torch.linalg.eigh(S_w)\n",
        "    eigvals_w = torch.clamp(eigvals_w, min=eps)\n",
        "\n",
        "    invsqrt_w = (eigvecs_w * (1.0 / torch.sqrt(eigvals_w))) @ eigvecs_w.transpose(-1, -2)\n",
        "    Sigma_lidar = invsqrt_w @ S_b @ invsqrt_w\n",
        "\n",
        "    # lam, _ = torch.linalg.eigh(Sigma_lidar)\n",
        "    lam = torch.linalg.eigh(Sigma_lidar)[0]\n",
        "    # print(lam)\n",
        "    # lam = torch.clamp(lam, min=0.0)\n",
        "\n",
        "    p = lam / lam.sum() + eps\n",
        "    # print(p)\n",
        "    # p = s / torch.sum(s, axis=0) + 1e-7\n",
        "    return torch.exp(-torch.sum(p * torch.log(p)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "id": "2Nd-sGe6Ku4S",
        "outputId": "86b2bfdf-b354-45d4-ddf8-a7523260103e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>█▇▅▅▅▄▅▅▄▄▅▄▄▄▄▃▃▃▃▃▂▂▃▂▃▃▃▃▃▁▂▃▂▂▁▁▂▃▁▄</td></tr><tr><td>correct</td><td>▁▁▁▃▃▃▂▄▅▄▅▄▅▄▅▅▄▅▅▄▅▅▆▅▅▅▆▅▇▅▆▅▆▆▇▆▇██▆</td></tr><tr><td>lidar</td><td>▃▁▄▇▅▄▂▄▇▅▅▆▆▄▅▅█▄▅▃▇▃▃▁▆▁▆▂▃▅▄▇▅▁▂▂▇▅▃▆</td></tr><tr><td>rankme</td><td>▁▂▂▂▃▃▃▃▄▄▄▅▅▅▅▅▅▆▅▆▆▆▆▆▆▇▇▇▇███████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>1.67563</td></tr><tr><td>correct</td><td>0.46875</td></tr><tr><td>lidar</td><td>9.37988</td></tr><tr><td>rankme</td><td>11.91343</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">visionary-resonance-169</strong> at: <a href='https://wandb.ai/bobdole/SeqJEPA/runs/icm9rc12' target=\"_blank\">https://wandb.ai/bobdole/SeqJEPA/runs/icm9rc12</a><br> View project at: <a href='https://wandb.ai/bobdole/SeqJEPA' target=\"_blank\">https://wandb.ai/bobdole/SeqJEPA</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250416_090111-icm9rc12/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250416_100112-kfhtrk4v</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/SeqJEPA/runs/kfhtrk4v' target=\"_blank\">prime-wave-170</a></strong> to <a href='https://wandb.ai/bobdole/SeqJEPA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/SeqJEPA' target=\"_blank\">https://wandb.ai/bobdole/SeqJEPA</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/SeqJEPA/runs/kfhtrk4v' target=\"_blank\">https://wandb.ai/bobdole/SeqJEPA/runs/kfhtrk4v</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "!pip install -q wandb\n",
        "import wandb # https://docs.wandb.ai/quickstart\n",
        "wandb.login(key='487a2109e55dce4e13fc70681781de9f50f27be7')\n",
        "try: run.finish()\n",
        "except NameError: pass\n",
        "run = wandb.init(project=\"SeqJEPA\", config={\"model\": \"res18\",}) # violet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-eTjgAhmp_t",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e8d1ed6-3611-4955-c79a-357aa3c38049"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "strain 2.0255367755889893\n",
            "strain 1.0404815673828125\n",
            "strain 0.9497750401496887\n",
            "strain 0.8780708909034729\n",
            "strain 0.8245757222175598\n",
            "strain 0.8287532329559326\n",
            "strain 0.8094506859779358\n",
            "strain 0.7626240253448486\n",
            "strain 0.751689076423645\n",
            "classify 2.9339599609375\n",
            "classify 2.889892578125\n",
            "classify 2.9334716796875\n",
            "classify 2.961669921875\n",
            "classify 2.9134521484375\n",
            "classify 2.933837890625\n",
            "classify 2.8939208984375\n",
            "classify 2.945556640625\n",
            "classify 2.9127197265625\n",
            "0.078125\n",
            "0.078125\n",
            "0.078125\n",
            "0.125\n",
            "strain 0.7161738276481628\n",
            "strain 0.7054580450057983\n",
            "strain 0.6618542075157166\n",
            "strain 0.6538353562355042\n",
            "strain 0.6327382922172546\n",
            "strain 0.6618990302085876\n",
            "strain 0.6335053443908691\n",
            "strain 0.5719990730285645\n",
            "strain 0.5696120858192444\n",
            "classify 2.931396484375\n",
            "classify 2.981689453125\n",
            "classify 2.973876953125\n",
            "classify 2.942626953125\n",
            "classify 2.966552734375\n",
            "classify 2.9635009765625\n",
            "classify 2.921630859375\n",
            "classify 2.82275390625\n",
            "classify 2.911376953125\n",
            "0.078125\n",
            "0.078125\n",
            "0.03125\n",
            "0.03125\n",
            "strain 0.562479555606842\n",
            "strain 0.5331909656524658\n",
            "strain 0.6446142196655273\n",
            "strain 0.5683517456054688\n",
            "strain 0.5949717164039612\n",
            "strain 0.5648274421691895\n",
            "strain 0.5527896881103516\n",
            "strain 0.5140185952186584\n",
            "strain 0.4948420226573944\n",
            "classify 2.820556640625\n",
            "classify 3.0081787109375\n",
            "classify 2.9093017578125\n",
            "classify 3.0081787109375\n",
            "classify 2.976318359375\n",
            "classify 2.9385986328125\n",
            "classify 2.9219970703125\n",
            "classify 3.033447265625\n",
            "classify 2.9130859375\n",
            "0.046875\n",
            "0.09375\n",
            "0.125\n",
            "0.046875\n",
            "strain 0.4563806653022766\n",
            "strain 0.5113539695739746\n",
            "strain 0.47261688113212585\n",
            "strain 0.5141768455505371\n",
            "strain 0.4732700288295746\n",
            "strain 0.4535224139690399\n",
            "strain 0.47808799147605896\n",
            "strain 0.4717126786708832\n",
            "strain 0.521043598651886\n",
            "classify 2.9019775390625\n",
            "classify 2.947021484375\n",
            "classify 2.919677734375\n",
            "classify 3.08740234375\n",
            "classify 2.9549560546875\n",
            "classify 2.9556884765625\n",
            "classify 2.9542236328125\n",
            "classify 3.04296875\n",
            "classify 2.900146484375\n",
            "0.03125\n",
            "0.03125\n",
            "0.046875\n",
            "0.046875\n",
            "strain 0.47576403617858887\n",
            "strain 0.4559369683265686\n",
            "strain 0.4467572271823883\n",
            "strain 0.4454258382320404\n",
            "strain 0.442065566778183\n",
            "strain 0.4636036157608032\n",
            "strain 0.43734651803970337\n",
            "strain 0.478270947933197\n",
            "strain 0.4642370939254761\n",
            "classify 3.0283203125\n",
            "classify 2.948974609375\n",
            "classify 2.9644775390625\n",
            "classify 2.9501953125\n",
            "classify 2.887939453125\n",
            "classify 2.930908203125\n",
            "classify 2.98486328125\n",
            "classify 3.00830078125\n",
            "classify 2.9774169921875\n",
            "0.0625\n",
            "0.015625\n",
            "0.046875\n",
            "0.0\n",
            "strain 0.45668113231658936\n",
            "strain 0.46768391132354736\n",
            "strain 0.5157738924026489\n",
            "strain 0.42225009202957153\n",
            "strain 0.4646449685096741\n",
            "strain 0.41369929909706116\n",
            "strain 0.43478402495384216\n",
            "strain 0.42897340655326843\n",
            "strain 0.3900556266307831\n",
            "classify 2.990234375\n",
            "classify 3.01416015625\n",
            "classify 2.9945068359375\n",
            "classify 2.97412109375\n",
            "classify 2.9200439453125\n",
            "classify 2.8306884765625\n",
            "classify 3.018310546875\n",
            "classify 3.120361328125\n",
            "classify 2.9453125\n",
            "0.0\n",
            "0.0625\n",
            "0.03125\n",
            "0.015625\n",
            "strain 0.39389848709106445\n",
            "strain 0.4149908721446991\n",
            "strain 0.43484729528427124\n",
            "strain 0.45797303318977356\n",
            "strain 0.4380582869052887\n",
            "strain 0.3984646201133728\n",
            "strain 0.46389079093933105\n",
            "strain 0.44144266843795776\n",
            "strain 0.42515069246292114\n"
          ]
        }
      ],
      "source": [
        "# @title strain ctrain test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "def strain(model, dataloader, optim, scheduler=None):\n",
        "    model.train()\n",
        "    for i, (x, _) in enumerate(dataloader):\n",
        "        x = x[...,1:].to(device).to(torch.bfloat16)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            loss = model.loss(x)\n",
        "\n",
        "            # repr_loss, std_loss, cov_loss = model.loss(x)\n",
        "            # loss = model.sim_coeff * repr_loss + model.std_coeff * std_loss + model.cov_coeff * cov_loss\n",
        "\n",
        "        optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # total_norm = 0\n",
        "        # for p in model.parameters(): total_norm += p.grad.data.norm(2).item() ** 2\n",
        "        # total_norm = total_norm**.5\n",
        "        # print('total_norm', total_norm)\n",
        "        # for p in list(filter(lambda p: p.grad is not None, model.parameters())):\n",
        "        #     print(p.grad.data.norm(2).item())\n",
        "        # print(\"max grad norm\", max([p.grad.data.norm(2).item() for p in list(filter(lambda p: p.grad is not None, model.parameters()))]))\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 5) # 0.5\n",
        "\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            m=0.999 # 0.99 m = next(momentum_scheduler)\n",
        "            for param_q, param_k in zip(model.student.parameters(), model.teacher.parameters()):\n",
        "                param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        print(\"strain\",loss.item())\n",
        "        # for param in seq_jepa.context_encoder.cls: print(param.data)\n",
        "        # for param in seq_jepa.predicter.cls: print(param.data)\n",
        "        try: wandb.log({\"loss\": loss.item(), \"repr/I\": repr_loss.item(), \"std/V\": std_loss.item(), \"cov/C\": cov_loss.item()})\n",
        "        # try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=50: break\n",
        "\n",
        "\n",
        "def ctrain(model, classifier, dataloader, coptim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.eval()\n",
        "    classifier.train()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        # x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        x, y = x[...,1:].to(device).to(torch.bfloat16), y[1].to(device) # [batch, ] # (id, activity)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            with torch.no_grad():\n",
        "                sx = model(x).detach()\n",
        "            # print(sx[0][0])\n",
        "            y_ = classifier(sx)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "        # print(classifier.classifier.weight[0])\n",
        "        # print(y_.shape, y.shape)\n",
        "        # print(loss)\n",
        "        coptim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 5) # .5\n",
        "        scaler.step(coptim)\n",
        "        scaler.update()\n",
        "        print(\"classify\",loss.item())\n",
        "        try: wandb.log({\"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        # x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        x, y = x[...,1:].to(device).to(torch.float), y[1].to(device) # [batch, ] # (id, activity)\n",
        "        with torch.no_grad():\n",
        "            sx = model(x)\n",
        "            rankme = RankMe(sx).item()\n",
        "            lidar = LiDAR(sx).item()\n",
        "            y_ = classifier(sx)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        print(correct/len(y))\n",
        "        # try: wandb.log({\"correct\": correct/len(y)})\n",
        "        try: wandb.log({\"correct\": correct/len(y), \"rankme\": rankme, \"lidar\": lidar})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "for i in range(1600): #\n",
        "    # train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    # test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    np.random.shuffle(train_indices); np.random.shuffle(val_indices)\n",
        "    train_sampler, valid_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(val_indices)\n",
        "    # batch_size = 64 #512\n",
        "    train_loader = DataLoader(train_data, sampler=train_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "    test_loader = DataLoader(train_data, sampler=valid_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True)\n",
        "\n",
        "    strain(seq_jepa, train_loader, optim)\n",
        "    ctrain(seq_jepa, classifier, train_loader, coptim)\n",
        "    test(seq_jepa, classifier, test_loader)\n",
        "\n",
        "    # strain(violet, train_loader, voptim)\n",
        "    # ctrain(violet, classifier, train_loader, coptim)\n",
        "    # test(violet, classifier, test_loader)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4J2ahp3wmqcg"
      },
      "outputs": [],
      "source": [
        "# @title supervised train test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "def strain(model, classifier, dataloader, optim, coptim, scheduler=None):\n",
        "    model.train()\n",
        "    classifier.train()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        # x = x.to(device).flatten(2).transpose(-2,-1)#.to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "        # y = y.to(device)\n",
        "        x, y = x[...,1:].to(device).to(torch.bfloat16), y[1].to(device) # [batch, ] # (id, activity)\n",
        "\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        # print(loss)\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # for p in list(filter(lambda p: p.grad is not None, model.parameters())):\n",
        "        #     print(p.grad.data.norm(2).item())\n",
        "        # print(\"max grad norm\", max([p.grad.data.norm(2).item() for p in list(filter(lambda p: p.grad is not None, model.parameters()))]))\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 10) # 0.5\n",
        "\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        print(\"strain\",loss.item())\n",
        "        # for param in seq_jepa.context_encoder.cls: print(param.data)\n",
        "        # for param in seq_jepa.predicter.cls: print(param.data)\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=50: break\n",
        "\n",
        "\n",
        "# def test(model, dataloader):\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        # x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        x, y = x[...,1:].to(device).to(torch.float), y[1].to(device) # [batch, ] # (id, activity)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "\n",
        "        loss = F.cross_entropy(y_, y)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        print(correct/len(y))\n",
        "        try: wandb.log({\"correct\": correct/len(y), \"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "for i in range(1000):\n",
        "    # train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    # test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    np.random.shuffle(train_indices); np.random.shuffle(val_indices)\n",
        "    train_sampler, valid_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(val_indices)\n",
        "    # batch_size = 64 #512\n",
        "    train_loader = DataLoader(train_data, sampler=train_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "    test_loader = DataLoader(train_data, sampler=valid_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True)\n",
        "\n",
        "    # strain(seq_jepa, train_loader, optim)\n",
        "    # test(seq_jepa, test_loader)\n",
        "    strain(violet, train_loader, voptim)\n",
        "    test(violet, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JT8AgOx0E_KM",
        "outputId": "6b3d3969-da28-4cc0-c4de-f0a327ea266f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# @title save/load\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# folder='/content/drive/MyDrive/jepa/'\n",
        "\n",
        "# # modelsd, optimsd = torch.load(folder+'SeqJEPA.pkl', map_location=device).values()\n",
        "# modelsd, optimsd = torch.load('SeqJEPA.pkl', map_location=device).values()\n",
        "# seq_jepa.load_state_dict(modelsd, strict=False)\n",
        "# optim.load_state_dict(optimsd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpbLPvjiE-pD"
      },
      "outputs": [],
      "source": [
        "checkpoint = {'model': seq_jepa.state_dict(), 'optimizer': optim.state_dict()}\n",
        "# torch.save(checkpoint, folder+'SeqJEPA.pkl')\n",
        "torch.save(checkpoint, 'SeqJEPA.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vScvFGGSeN6"
      },
      "source": [
        "## drawer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title simplex\n",
        "# !pip install -q opensimplex\n",
        "import opensimplex\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def simplexmask(hw=(8,8), scale=(.15,.2)):\n",
        "    ix = iy = np.linspace(0, 1, num=8)\n",
        "    ix, iy = ix+np.random.randint(1e10), iy+np.random.randint(1e10)\n",
        "    y=opensimplex.noise2array(ix, iy)\n",
        "    y = torch.from_numpy(y)\n",
        "    mask_scale = torch.rand(1) * (scale[1] - scale[0]) + scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    yy = y.flatten().sort()[0][int(hw[0]*hw[1]*mask_scale)]\n",
        "    mask = (y<=yy.item())\n",
        "    return mask # T/F [h,w]\n",
        "\n",
        "def simplexmask1d(seq=512, scale=(.15,.2), chaos=2):\n",
        "    i = np.linspace(0, chaos, num=seq) # 2\n",
        "    j = np.random.randint(1e10, size=1)\n",
        "    y=opensimplex.noise2array(i, j) # [1, seq]\n",
        "    # plt.pcolormesh(y)\n",
        "    # plt.show()\n",
        "    y = torch.from_numpy(y)\n",
        "    # print(y.shape)\n",
        "    mask_scale = torch.rand(1) * (scale[1] - scale[0]) + scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    # print(a.shape, int(seq*mask_scale))\n",
        "    val, ind = y.sort()\n",
        "    yy = val[:,int(seq*mask_scale)]\n",
        "    mask = (y<=yy.item())\n",
        "    index = ind[:,:int(seq*mask_scale)]\n",
        "    return index, mask # T/F [h,w]\n",
        "\n",
        "def simplexmask1d(seq=512, ctx_scale=(.85,1), trg_scale=(.6,.8), B=64, chaos=2):\n",
        "    i = np.linspace(0, chaos, num=seq) # 2-5\n",
        "    j = np.random.randint(1e10, size=B)\n",
        "    noise = opensimplex.noise2array(i, j) # [B, seq]\n",
        "    # plt.pcolormesh(noise[:1])\n",
        "    # plt.rcParams[\"figure.figsize\"] = (20,3)\n",
        "    # plt.show()\n",
        "    noise = torch.from_numpy(noise)\n",
        "    ctx_mask_scale = torch.rand(1) * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    trg_mask_scale = torch.rand(1) * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "    val, ind = noise.sort()\n",
        "    trg_index = ind[:,-int(seq*trg_mask_scale):]\n",
        "    ctx_index = ind[:,-int(seq*ctx_mask_scale):-int(seq*trg_mask_scale)] # ctx wraps trg ; most similar to multiblock\n",
        "    # ctx_index = ind[:,:int(seq*ctx_mask_scale)-int(seq*trg_mask_scale)] # ctx hug bottom\n",
        "    # ctx_index = ind[:,-int(seq*ctx_mask_scale)-int(seq*trg_mask_scale):-int(seq*trg_mask_scale)] # ctx wraps trg ; most similar to multiblock\n",
        "    # ctx_index = ind[:,:int(seq*ctx_mask_scale)] # ctx hug bottom\n",
        "    return ctx_index, trg_index\n",
        "\n",
        "\n",
        "# mask = torch.zeros(1 ,200)\n",
        "# mask[:, trg_index[:1]] = 1\n",
        "# mask[:, ctx_index[:1]] = .5\n",
        "# plt.rcParams[\"figure.figsize\"] = (20,1)\n",
        "# plt.pcolormesh(mask)\n",
        "# plt.show()\n",
        "\n",
        "def simplexmask1d(seq=512, ctx_scale=(.85,1), trg_scale=(.6,.8), B=64, chaos=[1,.5]):\n",
        "    i = np.linspace(0, chaos[0], num=seq) # 2-5\n",
        "    noise = opensimplex.noise2array(i, np.random.randint(1e10, size=B)) # [B, seq]\n",
        "    noise = torch.from_numpy(noise)\n",
        "    ctx_mask_scale = torch.rand(1) * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    trg_mask_scale = torch.rand(1) * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "\n",
        "    ctx_len, trg_len = int(seq*ctx_mask_scale), int(seq*trg_mask_scale)\n",
        "    val, trg_index = torch.topk(noise, trg_len, dim=1, sorted=False)\n",
        "    ctx_len = ctx_len - trg_len\n",
        "\n",
        "    remove_mask = torch.ones((B,seq), dtype=bool) # [B, S]\n",
        "    remove_mask.scatter_(1, trg_index, False).flatten()\n",
        "    ind = torch.arange(seq).unsqueeze(0).repeat(B,1)[remove_mask].reshape(B, -1)\n",
        "    print(ind.shape, ind)\n",
        "\n",
        "    i = np.linspace(0, chaos[1], num=seq) # 2-5\n",
        "    noise = opensimplex.noise2array(i, np.random.randint(1e10, size=B)) # [B, seq]\n",
        "    noise = torch.from_numpy(noise)[remove_mask].reshape(B, -1)\n",
        "    val, ctx_ind = torch.topk(noise, ctx_len, dim=1, sorted=False)\n",
        "    print(ctx_ind.shape, ctx_ind)\n",
        "\n",
        "\n",
        "    ctx_index = ind[torch.arange(B).unsqueeze(-1), ctx_ind]\n",
        "    return ctx_index, trg_index\n",
        "\n",
        "\n",
        "# mask = simplexmask(hw=(8,8), scale=(.6,.8))\n",
        "# index, mask = simplexmask1d(seq=100, scale=(.7,.8))\n",
        "# ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.85,1), trg_scale=(.6,.8), B=64, chaos=5)\n",
        "# ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.85,1), trg_scale=(.7,.8), B=64, chaos=2)\n",
        "# ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.7,.9), trg_scale=(.6,.7), B=64, chaos=5)\n",
        "# ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.2,.3), trg_scale=(.4,.5), B=64, chaos=3)\n",
        "# ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.1,.3), trg_scale=(.4,.6), B=64, chaos=3)\n",
        "# # print(trg_index[0], ctx_index[0])\n",
        "\n",
        "ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.85,1), trg_scale=(.7,.8), B=64, chaos=[3,.5])\n",
        "mask = torch.zeros(b ,200)\n",
        "mask[torch.arange(b).unsqueeze(-1), trg_index] = 1\n",
        "mask[torch.arange(b).unsqueeze(-1), ctx_index] = .5\n",
        "mask = mask[None,...]\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "imshow(mask)\n",
        "import torchvision\n",
        "imshow(torchvision.utils.make_grid(mask, nrow=8))\n",
        "\n",
        "# print(index)\n",
        "# print(index.shape)\n",
        "# print(mask)\n",
        "# print(mask.shape)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "9rPxvrrsYI_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwpnHW4wn9S1",
        "outputId": "40413ecd-7f4b-49a2-984b-fe812ec57e26"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:08<00:00, 19.5MB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title data\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "transform = transforms.Compose([transforms.ToTensor(),])\n",
        "\n",
        "# MNIST CIFAR10\n",
        "train_data = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
        "test_data = torchvision.datasets.CIFAR10(root='data', train=False, download=True, transform=transform)\n",
        "batch_size = 128 # 128 1024\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    # print(npimg.shape) # (3, 64, 64)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# dataiter = iter(train_data)\n",
        "# x,y = next(dataiter)\n",
        "# print(x.shape) # [3, 32, 32]\n",
        "# imshow(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DNCJFn5kNuua"
      },
      "outputs": [],
      "source": [
        "# @title evergarmin buffer dataloader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "import numpy as np\n",
        "import scipy.interpolate\n",
        "\n",
        "class BufferDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, activities, seq_len):\n",
        "        # self.data = [step for episode in self.process(buffer) for step in episode] # 0.00053\n",
        "        self.data = [self.process(activity) for activity in activities] # 0.00053\n",
        "        self.data = [x for x in self.data if x!=None and x!=[]]\n",
        "        self.seq_len = seq_len\n",
        "        self.pad = [(-1,-1,-1)]*(self.seq_len) # pad. need to mask later # torch.full((self.seq_len,), -1)\n",
        "        # normalise\n",
        "\n",
        "    def __len__(self):\n",
        "        # return len(self.data)//self.seq_len\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        act_list = self.data[idx].copy()\n",
        "        summary = list(act_list.pop(0))\n",
        "        # act_list = self.pad[len(act_list):] + act_list # forward padding\n",
        "        act_list = [summary] + self.transform(act_list) # summary, aug(x)\n",
        "        hr, temp, heart= zip(*act_list)\n",
        "        # print(hr, temp, heart)\n",
        "        # return hr, temp, heart\n",
        "        return torch.tensor(hr), torch.tensor(temp), torch.tensor(heart, dtype=torch.float)\n",
        "\n",
        "\n",
        "\n",
        "    def process(self, activity):\n",
        "        res_id = activity[\"res_id\"]\n",
        "        try: activeKilocalories = activity[\"summary\"][\"activeKilocalories\"]\n",
        "        except KeyError: activeKilocalories = 0.\n",
        "        try: steps = activity[\"summary\"][\"steps\"]\n",
        "        except KeyError: steps = 0.\n",
        "        # (res_id, activeKilocalories, steps)\n",
        "        res_id = float(res_id[3:]) # remove 'RES' and turn id into float, in line with other data\n",
        "        act_list = [(res_id, activeKilocalories, steps)]\n",
        "        samples = activity[\"samples\"]\n",
        "\n",
        "        if len(samples)==0: return\n",
        "\n",
        "        for sample in samples:\n",
        "            try:\n",
        "                startTimeInSeconds = sample[\"startTimeInSeconds\"]\n",
        "                hour_decimal = (startTimeInSeconds / 3600) % 24\n",
        "            except KeyError: hour_decimal = 12.\n",
        "            try: airTemperatureCelcius = sample[\"airTemperatureCelcius\"]\n",
        "            except KeyError: airTemperatureCelcius = 28. # singapore average temperature?\n",
        "            try: heartRate = sample[\"heartRate\"]\n",
        "            except KeyError: heartRate = 80.\n",
        "            # (hour_decimal, airTemperatureCelcius, heartRate)\n",
        "            act_list.append((hour_decimal, airTemperatureCelcius, heartRate))\n",
        "        return act_list\n",
        "\n",
        "    def transform(self, act_list): # rand resize crop, rand mask\n",
        "\n",
        "        # https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html\n",
        "        act_list = np.array(act_list)\n",
        "        try: hr, temp, heart= zip(*act_list)\n",
        "        except ValueError: print('err:',act_list)\n",
        "        kind = 'nearest' if len(act_list)>self.seq_len/.7 else 'linear'\n",
        "        temp_interpolator = scipy.interpolate.interp1d(hr, temp, kind=kind) # linear nearest quadratic cubic\n",
        "        heart_interpolator = scipy.interpolate.interp1d(hr, heart, kind=kind) # linear nearest quadratic cubic\n",
        "        hr_ = np.sort(np.random.uniform(hr[0], hr[-1], round(self.seq_len*random.uniform(1,1/.7))))\n",
        "        temp_, heart_ = temp_interpolator(hr_), heart_interpolator(hr_)\n",
        "        act_list = list(zip(hr_, temp_, heart_))\n",
        "\n",
        "        idx = torch.randint(len(act_list)-self.seq_len+1, size=(1,))\n",
        "        # return act_list[idx: idx+self.seq_len]\n",
        "        act_list = act_list[idx: idx+self.seq_len]\n",
        "\n",
        "        # mask=(torch.rand(self.seq_len)<.1) # True -> masked # random masking\n",
        "        # # act_list[mask] = self.pad[0]\n",
        "        # act_list = [self.pad[0] if m else a for m,a in zip(mask, act_list)]\n",
        "        return act_list\n",
        "\n",
        "    def push(self, activities):\n",
        "        # self.data.append(self.process(activity))\n",
        "        self.data.extend([self.process(activity) for activity in activities])\n",
        "\n",
        "\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(30, 14))\n",
        "    print(npimg.shape)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "seq_len = 200 #26/51 # 50\n",
        "train_data = BufferDataset(activities, seq_len)\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 128 #128\n",
        "# train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # [3,batch, T]\n",
        "train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, drop_last=True) # [3,batch, T]\n",
        "\n",
        "\n",
        "for batch, (hr, temp, heart) in enumerate(train_loader):\n",
        "    pass\n",
        "    # print(hr[6])\n",
        "    # for h in hr:\n",
        "    #     print(h)\n",
        "    # break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHzr3NVs2EcG",
        "outputId": "cecf37be-0d0f-4890-83f3-305b0b01c3ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 256, 4])\n"
          ]
        }
      ],
      "source": [
        "# @title ViT me simple save\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    \"\"\"Zero out the parameters of a module and return it.\"\"\"\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads=None, d_head=8, cond_dim=None, dropout=0.): # .1\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_head = d_head\n",
        "        self.n_heads = d_model // d_head\n",
        "        # self.d_head = d_model // n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or d_model, 2*d_model, bias=False)\n",
        "        # self.k = nn.Sequential(nn.Dropout(dropout), nn.Linear(cond_dim, d_model, bias=False))\n",
        "        # self.lin = nn.Linear(d_model, d_model)\n",
        "        self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        # self.lin = nn.Sequential(nn.Dropout(dropout), zero_module(nn.Linear(d_model, d_model)))\n",
        "        self.drop = nn.Dropout(dropout) # indp before q,k,v; after linout\n",
        "        self.scale = self.d_head ** -.5\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [batch, T, d_model]=[batch, h*w, c], [batch, num_tok, cond_dim], [batch,T]\n",
        "        batch = x.shape[0]\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        Q = self.q(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        # K = self.k(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        K, V = self.kv(cond).view(batch, -1, self.n_heads, 2*self.d_head).transpose(1, 2).chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # linear attention # Softmax(Q) @ (Softmax(K).T @ V)\n",
        "        if mask != None:\n",
        "            mask = mask[:, None, :, None] # [batch,T] -> [batch,1,T,1]\n",
        "            K, V = K.masked_fill(mask, -torch.finfo(x.dtype).max), V.masked_fill(mask, -torch.finfo(x.dtype).max)\n",
        "        Q, K = Q.softmax(dim=-1)*self.scale, K.softmax(dim=-2)\n",
        "        context = K.transpose(-2,-1) @ V # [batch, n_heads, d_head, d_head]\n",
        "        out = Q @ context # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # (quadratic) attention # Softmax(Q @ K.T) @ V\n",
        "        # attn = Q @ K.transpose(-2,-1) * self.scale # [batch, n_heads, T] # [batch, n_heads, T, T/num_tok]\n",
        "        # if mask != None: attn = attn.masked_fill(mask[:, None, :, None], -torch.finfo(attn.dtype).max) # [batch,T]->[batch,1,T,1]\n",
        "        # attention = torch.softmax(attn, dim=-1)\n",
        "        # out = self.drop(attention) @ V # [batch, n_heads, T, d_head]\n",
        "\n",
        "        out = out.transpose(1, 2).flatten(2)\n",
        "        return self.drop(self.lin(out)) # [batch, T, d_model]\n",
        "\n",
        "# if self, dont pass cond_dim in init, dont pass cond in fwd\n",
        "# Softmax(Q @ K.T) @ V ~ Softmax(Q) @ Softmax(K).T @ V\n",
        "\n",
        "# https://github.com/lucidrains/x-transformers/blob/main/x_transformers/x_transformers.py#L1855\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_head=4, cond_dim=None, ff_mult=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm1 = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.norm2 = nn.RMSNorm(d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.cond_dim = cond_dim\n",
        "        self.self = MultiHeadAttention(d_model, d_head=d_head, dropout=0)\n",
        "        if self.cond_dim!=None: self.cross = MultiHeadAttention(d_model, d_head=d_head, cond_dim=cond_dim, dropout=0)\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        ff_dim=d_model*ff_mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), nn.Dropout(dropout), nn.Linear(d_model, ff_dim), act, # ReLU GELU\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "            nn.RMSNorm(d_model), nn.Dropout(dropout), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            # nn.RMSNorm(d_model), act, nn.Dropout(dropout), nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,c,h,w], [batch, num_tok, cond_dim], [batch,T]\n",
        "        # bchw = x.shape\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "\n",
        "        if self.cond_dim==None: cond=None # is self attn\n",
        "        x = x + self.drop(self.self(self.norm1(x)))\n",
        "        if self.cond_dim!=None: x = x + self.cross(self.norm2(x), cond, mask)\n",
        "        x = x + self.ff(x)\n",
        "        # x = x + self.drop(self.ff(x))\n",
        "\n",
        "        # return x.transpose(1,2).reshape(*bchw)\n",
        "        return x\n",
        "\n",
        "\n",
        "# class SimpleViT(nn.Module):\n",
        "#     def __init__(self, in_dim, out_dim, dim, depth, heads, mlp_dim, channels=3, dim_head=8):\n",
        "#         super().__init__()\n",
        "#         self.to_patch_embedding = nn.Sequential( # in, out, kernel, stride, pad\n",
        "#             nn.Conv2d(in_dim, dim, kernel_size=7, stride=2, padding=7//2, bias=False), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "#             # nn.Conv2d(in_dim, dim, kernel_size=7, stride=2, padding=7//2, bias=False), nn.MaxPool2d(2,2)\n",
        "#             )\n",
        "#         # self.pos_embedding = LearnedRoPE2D(dim) # LearnedRoPE2D, RoPE2D\n",
        "#         self.positional_emb = nn.Parameter(torch.zeros(1, 8*8, dim)) # positional_embedding == 'learnable'\n",
        "#         self.transformer = AttentionBlock(d_model=dim, d_head=dim_head)\n",
        "#         self.transformer_blocks = nn.ModuleList([AttentionBlock(d_model=dim, d_head=dim_head) for i in range(depth)])\n",
        "\n",
        "#         self.attention_pool = nn.Linear(dim, 1)\n",
        "#         self.out = nn.Linear(dim, out_dim, bias=False)\n",
        "\n",
        "#     def forward(self, img):\n",
        "#         device = img.device\n",
        "#         x = self.to_patch_embedding(img)\n",
        "#         # x = self.pos_embedding(x)\n",
        "#         x = x.flatten(-2).transpose(-2,-1) # b c h w -> b (h w) c\n",
        "#         x = x + self.positional_emb\n",
        "#         x = self.transformer(x)\n",
        "#         # for blk in self.predictor_blocks: x = blk(x)\n",
        "\n",
        "#         # x = x.flatten(-2).mean(dim=-1) # meal pool\n",
        "#         attn_weights = self.attention_pool(x).squeeze(-1) # [batch, (h,w)] # seq_pool\n",
        "#         x = (attn_weights.softmax(dim = 1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, (h,w)] @ [batch, (h,w), dim]\n",
        "#         # cls_token = repeat(self.class_emb, '1 1 d -> b 1 d', b = b)\n",
        "#         # x = torch.cat((cls_token, x), dim=1)\n",
        "#         # x = x[:, 0] # first token\n",
        "#         return self.out(x)\n",
        "\n",
        "# ijepa: 224*224 -> 14*14 = 196\n",
        "# 32*7\n",
        "# me: 32*32=1024 -> 256\n",
        "\n",
        "import torch\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "def apply_masks(x, masks):\n",
        "    \"\"\":param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
        "    :param masks: list of tensors containing indices of patches in [N] to keep\"\"\"\n",
        "    all_x = []\n",
        "    for m in masks: # [1,T]\n",
        "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [1,T,dim]\n",
        "        all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "    return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "# batch, seq_len, d_model = 2,7,4\n",
        "# x = torch.rand(batch, seq_len, d_model)\n",
        "# print(x)\n",
        "# masks = [torch.randint(0,seq_len,(2,3,))]\n",
        "# print(masks)\n",
        "# out = apply_masks(x, masks)\n",
        "# print(out)\n",
        "# # print(out.shape)\n",
        "\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        # self.positional_emb = nn.Parameter(torch.zeros(1, 32*32//4, d_model)) # positional_embedding == 'learnable'\n",
        "        # encoder_layers = nn.TransformerEncoderLayer(d_model, d_model//d_head, d_hid or d_model, dropout, batch_first=True)\n",
        "        # self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.zeros(1,1,d_model)) # randn zeros\n",
        "        nn.init.trunc_normal_(self.cls, std=.02)\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model)\n",
        "        self.lin = nn.Linear(d_model, out_dim)# if out_dim != d_model else None\n",
        "\n",
        "    def forward(self, src, context_indices, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        src = self.embed(src) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = src.shape\n",
        "        src = src * self.pos_encoder(context_indices)\n",
        "        # src = src + self.positional_emb[:,context_indices]\n",
        "        # src = apply_masks(src, [context_indices])\n",
        "\n",
        "        pred_tokens = self.cls * self.pos_encoder(trg_indices) # [M, num_trg_toks, d_model]\n",
        "        # pred_tokens = self.cls + self.positional_emb[0,trg_indices]\n",
        "        pred_tokens = pred_tokens.repeat(batch, 1, 1) # [batch*M, num_trg_toks, d_model]\n",
        "        # print(pred_tokens.requires_grad)\n",
        "        # print(\"pred fwd\", src.shape, pred_tokens.shape)\n",
        "        # src = src.repeat_interleave(trg_indices.shape[0], dim=0) # [batch, seq_len, d_model] -> [batch*M, seq_len, d_model]\n",
        "        src = torch.cat([src, pred_tokens], dim=1) # [batch*M, seq_len+num_trg_toks, d_model]\n",
        "\n",
        "        out = self.transformer_encoder(src) # float [seq_len, batch_size, d_model]\n",
        "        out = self.norm(out)\n",
        "        out = out[:,seq:] # [batch*M, num_trg_toks, d_model]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.MaxPool1d(2,2), #nn.MaxPool1d(3, 2, 3//2),\n",
        "            nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.Conv1d(d_model, d_model,3,2,3//2)\n",
        "            # nn.Conv1d(in_dim, d_model,2,2,0), # like patch\n",
        "            )\n",
        "        self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        # self.positional_emb = nn.Parameter(torch.zeros(1, 32*32//4, d_model)) # positional_embedding == 'learnable'\n",
        "        # encoder_layers = nn.TransformerEncoderLayer(d_model, d_model//d_head, d_hid or d_model, dropout, batch_first=True)\n",
        "        # self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model)\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, src, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # src = self.embed(src) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        src = self.embed(src.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "\n",
        "        batch, seq, dim = src.shape\n",
        "\n",
        "        # # # src = self.pos_encoder(src)\n",
        "        # if context_indices != None:\n",
        "        # print(src.shape, self.pos_encoder(context_indices).shape)\n",
        "        #     src = src * self.pos_encoder(context_indices) # context/predictor # src = src + self.positional_emb[:,context_indices]\n",
        "        # else: src = src * self.pos_encoder(torch.arange(seq, device=device).unsqueeze(0)) # target # src = src + self.positional_emb[:,:seq]\n",
        "\n",
        "\n",
        "        # src = self.pos_encoder(src)\n",
        "        src = src * self.pos_encoder(torch.arange(seq, device=device)).unsqueeze(0)\n",
        "        # src = src + self.positional_emb\n",
        "        if context_indices != None:\n",
        "            src = apply_masks(src, [context_indices])\n",
        "\n",
        "\n",
        "        out = self.transformer_encoder(src) # float [seq_len, batch_size, d_model]\n",
        "        out = self.norm(out)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "batch, seq_len, d_model = 4,1024,4\n",
        "in_dim = 3\n",
        "model = TransformerModel(in_dim, d_model, d_head=4, nlayers=2, dropout=0.).to(device)\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "# # print(out)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_YBe6-eZ2Zq",
        "outputId": "09b27f75-ba04-4aaf-f5e4-1b9c1dc8d112"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 16, 25])\n"
          ]
        }
      ],
      "source": [
        "x = torch.rand(4,100,3)\n",
        "\n",
        "x=x.transpose(-2,-1) # [batch, dim, seq]\n",
        " # in, out, kernel, stride, pad\n",
        "# net = nn.Conv1d(3,16,7,2,7//2)\n",
        "net = nn.Sequential(nn.Conv1d(3,16,7,2,7//2), nn.MaxPool1d(3, 2, 3//2))\n",
        "# net = nn.Conv1d(3,16,(7,3),2,3//2)\n",
        "out = net(x)\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "R6kiiqw3uIdV"
      },
      "outputs": [],
      "source": [
        "# @title test repeat_interleave\n",
        "# x = torch.rand(3,5)\n",
        "x = torch.rand(3,5,7)\n",
        "print(x)\n",
        "# out = x.repeat(2,1) # [2*3,5]\n",
        "# print(out)\n",
        "# x_ = out.reshape(2,3,5)\n",
        "# print(x_)\n",
        "# out = x.repeat_interleave(2,1,1) # [3*2,5]\n",
        "out = x.repeat_interleave(2,dim=0) # [3*2,5]\n",
        "# print(out)\n",
        "# x_ = out.reshape(2,3,5,7)\n",
        "x_ = out.reshape(3,2,5,7)\n",
        "print(x_)\n",
        "\n",
        "\n",
        "# batch=1\n",
        "# seq_len=5\n",
        "# dim=4\n",
        "# positional_emb = torch.rand(batch, seq_len, dim)\n",
        "# print(positional_emb)\n",
        "# num_tok=2\n",
        "# trg_indices=torch.randint(0,seq_len,(M, num_tok))\n",
        "# print(trg_indices)\n",
        "# # pos_embs = positional_emb.gather(1, trg_indices.unsqueeze(0))\n",
        "# pos_embs = positional_emb[torch.arange(batch)[...,None,None],trg_indices.unsqueeze(0)]\n",
        "# # pos_embs = positional_emb[0,trg_indices]\n",
        "# # [batch, M, num_tok, dim]\n",
        "# print(pos_embs.shape)\n",
        "# print(pos_embs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8r_GjI_vCMyo"
      },
      "outputs": [],
      "source": [
        "# @title test ropes\n",
        "# for i, (img, _) in enumerate(test_loader):\n",
        "#     break\n",
        "# print(img.shape) # [batch, 1, 28, 28]\n",
        "# print(img[0]) # [0,1)\n",
        "d_model=8\n",
        "# rot_emb = RotEmb(d_model, top=torch.pi, base=10)\n",
        "# x = torch.linspace(0,1,20)\n",
        "# out = rot_emb(x)\n",
        "# print(out.shape)\n",
        "# print(out)\n",
        "\n",
        "# # pos_encoder = RoPE(d_model, seq_len=15, base=100)\n",
        "# pos_encoder = RoPE(d_model, seq_len=15, base=10000)\n",
        "x = torch.ones(1, 10, d_model)\n",
        "# # x = torch.ones(1, 10, d_model)\n",
        "# out = pos_encoder(x)\n",
        "# # print(out.shape)\n",
        "# for x in out[0]:\n",
        "#     print(x)\n",
        "\n",
        "\n",
        "# pos_encoder = LearnedRoPE(d_model, seq_len=512)\n",
        "# out = pos_encoder(x)\n",
        "# for o in out[0]:\n",
        "#     print(o)\n",
        "\n",
        "# pos_encoder.weights = nn.Parameter(torch.randn(1, d_model//2))\n",
        "# out = pos_encoder(x)\n",
        "# for o in out[0]:\n",
        "#     print(o)\n",
        "\n",
        "\n",
        "# tensor([-0.0177, -0.9998,  0.8080, -0.5892, -0.7502, -0.6612,  0.3885,  0.9214])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "d929355f-be2b-47a1-82ff-a7630671f47f"
      },
      "outputs": [],
      "source": [
        "# @title RNN pytorch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, num_layers=1):\n",
        "        super().__init__()\n",
        "        if out_dim is None: out_dim = in_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.d_model = d_model\n",
        "        # self.rnn = nn.RNN(d_model, d_model, num_layers, batch_first=True)\n",
        "        self.rnn = nn.GRU(in_dim, d_model, num_layers, batch_first=True)\n",
        "        # self.lstm = nn.LSTM(d_model, d_model, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(d_model, out_dim)\n",
        "\n",
        "        # self.emb = nn.Embedding(in_dim, d_model)\n",
        "        self.mask_vec = nn.Parameter(torch.randn(in_dim))\n",
        "\n",
        "\n",
        "    # def forward(self, x, hc=None): # lstm [batch_size, seq, in_dim]\n",
        "    #     x = self.emb(x)\n",
        "    #     if hc is None:\n",
        "    #         h0 = torch.zeros((self.num_layers, x.size(0), self.d_model), device=device)\n",
        "    #         c0 = torch.zeros((self.num_layers, x.size(0), self.d_model), device=device)\n",
        "    #     else: h0,c0 = hc\n",
        "    #     # print(x.shape, h0.shape,c0.shape)\n",
        "    #     out, (h,c) = self.lstm(x, (h0,c0)) # [batch, seq_len, d_model], ([num_layers, batch, d_model] )\n",
        "    #     # out = out[:, -1, :] # out: (n, 128)\n",
        "    #     out = self.fc(out) # out: (n, 10)\n",
        "    #     return out, (h, c)\n",
        "\n",
        "    def reset(self, batch):\n",
        "        h0 = torch.zeros((self.num_layers, batch, self.d_model), device=device)\n",
        "        return h0\n",
        "\n",
        "    def forward(self, x, h0=None, mask=None): # rnn/gru # [batch, T, in_dim], [num_layers, batch, d_model], [batch, T] True->masked\n",
        "        # x = self.emb(x)\n",
        "        if h0==None: h0 = self.reset(x.shape[0])\n",
        "        # print(x.shape, h0.shape)\n",
        "        if mask!=None: x[mask] = self.mask_vec.to(x.dtype)\n",
        "        out, h = self.rnn(x, h0) #\n",
        "        out = out[:, -1, :] # out: (n, 128)\n",
        "        out = self.fc(out) # out: (n, 10)\n",
        "        return out, h # [batch, out_dim], [num_layers, batch, d_model]\n",
        "\n",
        "\n",
        "# hidden_size = 128\n",
        "# num_layers = 2\n",
        "# input_size = num_classes = 4\n",
        "\n",
        "# model = RNN(input_size, hidden_size, num_classes, num_layers).to(device)\n",
        "# # # print(model)\n",
        "# # print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "# batch=2\n",
        "# seq_len=3\n",
        "# x=torch.rand(batch,seq_len,input_size).to(device)\n",
        "# h=torch.rand(num_layers,batch,hidden_size).to(device)\n",
        "# mask=(torch.rand(batch,seq_len)<.5)#.expand(-1,-1,x.size(-1))\n",
        "# print(mask)\n",
        "# print(x)\n",
        "# out,h = model(x, h, mask)\n",
        "# print(out.shape)\n",
        "# print(h.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AtPUcVu2kSLI"
      },
      "outputs": [],
      "source": [
        "# @title TransformerClassifier\n",
        "import math\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class TransformerClassifier(nn.Module):\n",
        "    # def __init__(self, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, drop = 0.):\n",
        "    def __init__(self, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, drop=0):\n",
        "        super().__init__()\n",
        "        # self.embedding = nn.Embedding(ntoken, d_model)\n",
        "        self.pos_encoder = RoPE(d_model, seq_len=200, base=10000)\n",
        "        # d_hid = d_hid or d_model#*2\n",
        "        # encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, d_hid, drop, batch_first=True)\n",
        "        # self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "\n",
        "        out_dim = out_dim or d_model\n",
        "        self.lin = nn.Linear(d_model, out_dim)\n",
        "        # self.lin = nn.Linear(d_model*2, out_dim)\n",
        "        # self.cls = nn.Parameter(torch.randn(1,1,d_model))\n",
        "        self.attention_pool = nn.Linear(d_model, 1, bias=False)\n",
        "        # self.out = nn.Sequential(\n",
        "        #     nn.Dropout(drop), nn.Linear(d_model, 3), nn.SiLU(),\n",
        "        #     nn.Dropout(drop), nn.Linear(3, out_dim), nn.Sigmoid()\n",
        "        # )\n",
        "\n",
        "    def forward(self, x, src_key_padding_mask = None): # [batch, seq, d_model], [batch, seq] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # batch, seq_len, d_model = x.shape\n",
        "        # x = torch.cat([self.cls.repeat(batch,1,1), x], dim=1)\n",
        "        # src_key_padding_mask = torch.cat([torch.zeros((batch, 1), dtype=torch.bool), src_key_padding_mask], dim=1)\n",
        "        x = self.pos_encoder(x)\n",
        "\n",
        "        x = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
        "        # print(\"fwd\",out.shape) # float [batch, seq_len, d_model]\n",
        "        # out = x.mean(dim=1) # average pool\n",
        "        # out = x.max(dim=1)#[0]\n",
        "\n",
        "        attn = self.attention_pool(x).squeeze(-1) # [batch, seq] # seq_pool\n",
        "        out = (torch.softmax(attn, dim=-1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, seq] @ [batch, seq, dim] -> [batch, dim]\n",
        "        # out = self.out(out) # optional (from GlobalContext) like squeeze excitation\n",
        "\n",
        "        # out = out[:, 0] # first token\n",
        "        # out = torch.cat([out, mean_pool], dim=-1)\n",
        "\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "batch, seq_len, d_model = 4,7,512\n",
        "model = TransformerModel(in_dim, d_model, d_head=4, nlayers=2, dropout=0.).to(device)\n",
        "# model = TransformerClassifier(d_model, nhead=8, nlayers=2, dropout=0.).to(device)\n",
        "x =  torch.rand(batch, seq_len, d_model)\n",
        "src_key_padding_mask = torch.stack([(torch.arange(seq_len) < seq_len - v) for v in torch.randint(seq_len, (batch,))]) # True will be ignored # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "print(src_key_padding_mask)\n",
        "out = model(x, src_key_padding_mask)\n",
        "print(out.shape)\n",
        "\n",
        "# GlobalAveragePooling1D layer, followed by a Dense layer. The final output of the transformer is produced by a softmax layer,\n",
        "# x = GlobalAveragePooling1D()(x)\n",
        "# x = Dropout(0.1)(x)\n",
        "# x = Dense(20, activation=\"relu\")(x)\n",
        "# x = Dropout(0.1)(x)\n",
        "# outputs = Dense(2, activation=\"softmax\")(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-8i1WLsvH_vn"
      },
      "outputs": [],
      "source": [
        "# @title Transformer Model\n",
        "import torch\n",
        "from torch import nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        # self.embed = nn.Sequential(\n",
        "        #     nn.Linear(in_dim, d_model), #act,\n",
        "        #     # nn.Linear(d_model, d_model), act,\n",
        "        # )\n",
        "        self.embed = nn.Linear(in_dim, d_model) if in_dim != d_model else None\n",
        "        # self.embed = nn.Sequential(nn.Conv1d(in_dim,d_model,7,2,7//2), nn.MaxPool1d(3, 2, 3//2))\n",
        "        self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, d_hid or d_model, dropout, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.d_model = d_model\n",
        "        self.cls = nn.Parameter(torch.zeros(1,1,d_model)) # randn\n",
        "        out_dim = out_dim or d_model\n",
        "        # self.lin = nn.Linear(d_model, out_dim)\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim != d_model else None\n",
        "        self.norm = nn.LayerNorm(out_dim)\n",
        "\n",
        "\n",
        "    def forward(self, src, src_key_padding_mask=None, cls_mask=None, context_indices=None, trg_indices=None): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # if cls_mask != None: src[cls_mask] = self.cls.to(src.dtype)\n",
        "\n",
        "        if self.embed != None: src = self.embed(src) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = src.shape\n",
        "        # src = self.pos_encoder(src)\n",
        "        if context_indices != None:\n",
        "            # print(src.shape, context_indices.shape, self.pos_encoder(context_indices).shape) # [2, 88, 32]) torch.Size([88]) torch.Size([88, 32]\n",
        "            # print(src[0], self.pos_encoder(context_indices)[0])\n",
        "            src = src * self.pos_encoder(context_indices) # context/predictor # src = src + self.positional_emb[:,context_indices]\n",
        "            # print(src[0])\n",
        "        else: src = src * self.pos_encoder(torch.arange(seq, device=device)) # target # src = src + self.positional_emb[:,:seq]\n",
        "            # print(\"trans fwd\", src.shape, self.pos_encoder(src).shape)\n",
        "\n",
        "        if trg_indices != None: # [M, num_trg_toks]\n",
        "            pred_tokens = self.cls * self.pos_encoder(trg_indices) # [M, num_trg_toks, d_model] # pred_tokens = self.cls + self.positional_emb[0,trg_indices]\n",
        "            pred_tokens = pred_tokens.repeat(batch, 1, 1) # [batch*M, num_trg_toks, d_model]\n",
        "            # print(pred_tokens.requires_grad)\n",
        "            src = src.repeat_interleave(trg_indices.shape[0], dim=0) # [batch, seq_len, d_model] -> [batch*M, seq_len, d_model]\n",
        "            src = torch.cat([src, pred_tokens], dim=1) # [batch*M, seq_len+num_trg_toks, d_model]\n",
        "\n",
        "        out = self.transformer_encoder(src, src_key_padding_mask=src_key_padding_mask) # float [seq_len, batch_size, d_model]\n",
        "        if trg_indices != None:\n",
        "            # print(out.shape)\n",
        "            out = out[:,seq:] # [batch*M, num_trg_toks, d_model]\n",
        "        if self.lin != None: out = self.lin(out)\n",
        "        out = self.norm(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "batch, seq_len, d_model = 4,7,64\n",
        "in_dim = 3\n",
        "model = TransformerModel(in_dim, d_model, nhead=8, nlayers=2, dropout=0.).to(device)\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "# print(out.shape)\n",
        "# # print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8ba31127-d6e5-438a-aede-5c789557ab39"
      },
      "outputs": [],
      "source": [
        "# @title SeqJEPA\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "def multiblock(seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "    return target_mask\n",
        "\n",
        "\n",
        "class SeqJEPA(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, num_layers=1, num_classes=10):\n",
        "        super().__init__()\n",
        "        if out_dim is None: out_dim = d_model\n",
        "        act = nn.GELU() # ReLU GELU SiLU\n",
        "        # self.rot_emb = RotEmb(d_model, top=1, base=10)\n",
        "        # self.rot_emb = RotEmb(d_model, top=torch.pi, base=10)\n",
        "        # self.rot_emb = RoPE(d_model, seq_len=1024, base=10)\n",
        "        # self.rot_emb = LearnedRotEmb(dim)\n",
        "        self.encode = nn.Sequential(\n",
        "            nn.Linear(in_dim, d_model), #act,\n",
        "            # nn.Linear(d_model, d_model), act,\n",
        "        )\n",
        "        # self.encode = nn.Sequential(nn.Conv1d(in_dim, d_model,7,2,7//2), nn.MaxPool1d(3, 2, 3//2))\n",
        "        self.context_encoder = TransformerModel(d_model, d_model, out_dim=out_dim, nhead=4, nlayers=2, dropout=0.)\n",
        "        self.predicter = TransformerModel(out_dim, d_model//2, out_dim, nhead=4, nlayers=1, dropout=0.)\n",
        "        self.target_encoder = AveragedModel(self.context_encoder, multi_avg_fn=get_ema_multi_avg_fn(0.85)) # 0.95 0.999\n",
        "        self.target_encoder.requires_grad_(False)\n",
        "        # self.classifier = nn.Linear(out_dim, num_classes)\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]\n",
        "        x = self.encode(x) # [batch, T, d_model]\n",
        "        # x = self.encode(x.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "\n",
        "        batch, seq, dim = x.shape\n",
        "        M=1 # 4\n",
        "        # target_mask = multiblock(seq, min_s=0.15, max_s=0.2, M=M) # mask out targets to be predicted # [M, seq]\n",
        "        target_mask = multiblock(seq, min_s=0.15, max_s=0.2, M=4)#.any(0) # mask out targets to be predicted # [M, seq]\n",
        "        context_mask = ~multiblock(seq, min_s=0.85, max_s=1., M=1)|target_mask.reshape(M,seq).any(0) # [1, seq], True->Mask\n",
        "        target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "        # print(target_mask, context_mask)\n",
        "        sorted_mask, ids = context_mask.int().sort(dim=1, stable=True)\n",
        "        context_indices = ids[~sorted_mask.bool()] # int idx [num_context_toks] , idx of context not masked\n",
        "        sorted_x = x[torch.arange(batch).unsqueeze(-1), context_indices] # [batch, num_context_toks, d_model]\n",
        "        # print(sorted_x.shape, context_indices.shape)[2, 9, 32]) torch.Size([9])\n",
        "        # print(sorted_x[0])\n",
        "        # print(sorted_x.is_leaf)\n",
        "        sorted_sx = self.context_encoder(sorted_x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "        # del sorted_x\n",
        "        # print(sorted_sx[0])\n",
        "\n",
        "        sorted_mask, ids = target_mask.int().sort(dim=1, stable=True)\n",
        "        # print(sorted_mask.shape, ids.shape, ids[~sorted_mask.bool()].shape, sorted_mask.sum(-1))\n",
        "        trg_indices = ids[sorted_mask.bool()].reshape(M,-1) # int idx [M, num_trg_toks] , idx of targets that are masked\n",
        "        # sorted_x = x[torch.arange(batch)[...,None,None], indices] # [batch, M, seq-num_trg_toks, dim]\n",
        "        # sx = sorted_sx[torch.arange(batch).unsqueeze(-1),ids.argsort(1)]\n",
        "        # print(sorted_sx.shape, trg_indices.shape)\n",
        "        sy_ = self.predicter(sorted_sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "        # sx = self.context_encoder(x, src_key_padding_mask=context_mask).repeat(M,1,1)\n",
        "        # # sy_ = self.predicter(sx, cls_mask=target_mask)*target_mask.unsqueeze(-1)\n",
        "        # sy_ = self.predicter(sx, trg_mask=target_mask)*target_mask.unsqueeze(-1)\n",
        "\n",
        "        # print(sy_.shape, target_mask.shape)\n",
        "        # print(self.target_encoder(x).repeat_interleave(M, dim=0).shape, trg_indices.repeat(batch,1).shape)\n",
        "        with torch.no_grad():\n",
        "            sy = self.target_encoder(x).repeat_interleave(M, dim=0)[torch.arange(batch*M).unsqueeze(-1), trg_indices.repeat(batch,1)] # [batch*M, num_trg_toks, out_dim]\n",
        "\n",
        "        # print(sy_[0])\n",
        "        # print(sy.shape, sy_.shape)\n",
        "        # loss = F.smooth_l1_loss(sy, sy_)\n",
        "        loss = F.mse_loss(sy, sy_)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]\n",
        "        batch, seq, dim = x.shape\n",
        "        target_mask = randpatch(seq//self.patch_size, mask_size=16, gamma=.75) # [seq]\n",
        "        context_mask = ~multiblock(seq//self.patch_size, min_s=0.85, max_s=1., M=1)|target_mask # [1, seq], True->Mask\n",
        "        target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "        # print(target_mask, context_mask)\n",
        "        sorted_mask, ids = context_mask.int().sort(dim=1, stable=True)\n",
        "        context_indices = ids[~sorted_mask.bool()]#.unsqueeze(0) # int idx [num_context_toks] , idx of context not masked\n",
        "        sorted_x = x[torch.arange(batch).unsqueeze(-1), context_indices] # [batch, num_context_toks, 3]\n",
        "        print(sorted_x.shape, context_indices.shape)\n",
        "        sorted_sx = self.context_encoder(sorted_x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "\n",
        "        sorted_mask, ids = target_mask.int().sort(dim=-1, stable=True)\n",
        "        trg_indices = ids[sorted_mask.bool()].unsqueeze(0) # int idx [1, num_trg_toks] , idx of targets that are masked\n",
        "        print(sorted_sx.shape, context_indices.shape)\n",
        "        sy_ = self.predicter(sorted_sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "\n",
        "        with torch.no_grad(): sy = self.target_encoder(x.detach())[torch.arange(batch).unsqueeze(-1), trg_indices.repeat(batch,1)] # [batch, num_trg_toks, out_dim]\n",
        "        # print(x[0][0][:8])\n",
        "        # print(sy_[0][0][:8])\n",
        "        # print(sy.shape, sy_.shape)\n",
        "        # loss = F.smooth_l1_loss(sy, sy_)\n",
        "        loss = F.mse_loss(sy.detach(), sy_)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        x = self.encode(x)\n",
        "        # x = self.encode(x.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "        sx = self.target_encoder(x)\n",
        "        out = sx.mean(dim=1)\n",
        "        return out\n",
        "\n",
        "\n",
        "# min_s=0.15, max_s, M\n",
        "# trg.15.2M4 C.85 1\n",
        "\n",
        "seq_jepa = SeqJEPA(in_dim=3, d_model=32, out_dim=16, num_layers=1).to(device)#.to(torch.float)\n",
        "optim = torch.optim.AdamW(seq_jepa.parameters(), lr=1e-4) # 1e-3?\n",
        "print(sum(p.numel() for p in seq_jepa.parameters() if p.requires_grad)) # 59850\n",
        "\n",
        "\n",
        "# x = torch.rand((2,20,3), device=device)\n",
        "# out = seq_jepa.loss(x)\n",
        "# print(out.shape)\n",
        "# print(x.is_leaf) # T\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "O-OU1MaKF7BZ"
      },
      "outputs": [],
      "source": [
        "# @title SeqJEPA old\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def multiblock(batch, seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "        mask_len = torch.rand(batch, M) * (max_s - min_s) + min_s # in (min_s, max_s)\n",
        "        mask_pos = torch.rand(batch, M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "        mask_len, mask_pos = mask_len * seq, mask_pos * seq\n",
        "        indices = torch.arange(seq)[None,None,...]\n",
        "        target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [batch, M, seq]\n",
        "        target_mask = target_mask.any(1) # True -> masked # multiblock masking # [batch, seq]\n",
        "        return target_mask\n",
        "\n",
        "\n",
        "\n",
        "class SeqJEPA(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.in_dim = in_dim\n",
        "        # if out_dim is None: self.out_dim = in_dim\n",
        "        if out_dim is None: out_dim = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.d_model = d_model\n",
        "        act = nn.GELU() # ReLU GELU SiLU\n",
        "\n",
        "        dim=8\n",
        "\n",
        "\n",
        "        self.calorie_emb = RotEmb(dim, top=1, base=10) # 0-4\n",
        "        self.steps_emb = RotEmb(dim, top=1, base=10) # 0-5\n",
        "        self.enc0 = nn.Sequential(\n",
        "            nn.Linear(dim*2, d_model), act,\n",
        "            nn.Linear(d_model, d_model), act,\n",
        "        )\n",
        "        self.hour_emb = CircularEmb(dim, torch.pi/2) # circular (0,1) # 0-24\n",
        "        self.temp_emb = RotEmb(dim, top=1/3, base=10) # 18-35\n",
        "        self.heart_emb = RotEmb(dim, top=1/3, base=100) # 50-200\n",
        "        self.enc1 = nn.Sequential(\n",
        "            nn.Linear(dim*3, d_model), act,\n",
        "        )\n",
        "\n",
        "        # self.transformer = TransformerClassifier(d_model, out_dim=out_dim, nhead=8, nlayers=2)\n",
        "        # # self.fc = nn.Linear(d_model, out_dim)\n",
        "        self.context_encoder = TransformerModel(d_model, out_dim=out_dim, nhead=8, nlayers=2, dropout=0.)\n",
        "        self.predicter = TransformerModel(out_dim, nhead=8, nlayers=1, dropout=0.)\n",
        "        self.target_encoder = AveragedModel(self.context_encoder, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    def loss(self, hr, temp, heart): # [batch, 1+T]\n",
        "        # batch = hr.size(0)\n",
        "        res_id, activeKilocalories, steps = hr[:,0], temp[:,0], heart[:,0]\n",
        "        # enc_summary = self.enc0(torch.cat([self.calorie_emb(activeKilocalories), self.steps_emb(steps)], dim=-1)) # [batch, d_model]\n",
        "        enc_summary = self.enc0(torch.cat([self.calorie_emb(torch.log(activeKilocalories.clamp(min=1))), self.steps_emb(torch.log(steps.clamp(min=1)))], dim=-1)) # [batch, d_model]\n",
        "        x = self.enc1(torch.cat([self.hour_emb(hr[:,1:]/24), self.temp_emb(temp[:,1:]), self.heart_emb(heart[:,1:])], dim=-1)) # [batch, T, d_model]\n",
        "        # print('violet fwd', hr.shape, enc_summary.shape, x.shape)\n",
        "\n",
        "        # # mask=(torch.rand(self.seq_len)<.1) # True -> masked # random masking\n",
        "        # mask=(torch.rand_like(x)<.1) # True -> masked # random masking\n",
        "\n",
        "        # patches pad -enc> patch_enc\n",
        "        # rearrange patch in padded/masked, -pred> pred of masked\n",
        "        # mse tgt_enc(img)\n",
        "\n",
        "        # average L2 distance between the predicted patch-level representations sˆy(i) and the target patch-level representation sy(i);\n",
        "        # F.smooth_l1_loss\n",
        "\n",
        "        batch, seq, dim = x.shape\n",
        "\n",
        "        # mask=(torch.rand(batch, seq)<.75) # True -> masked # random masking\n",
        "        # sorted_mask, ids = mask.sort(dim=1)\n",
        "        # # sorted_x = x[torch.arange(batch)[...,None,None],ids]\n",
        "        # sorted_x = x[torch.arange(batch).unsqueeze(-1),ids]\n",
        "        # sorted_mask = torch.cat([torch.zeros((batch, 1), dtype=torch.bool), sorted_mask], dim=1) # True->mask\n",
        "        # sorted_x = torch.cat([enc_summary.unsqueeze(1), sorted_x], dim=1)\n",
        "        # sorted_sx = self.context_encoder(sorted_x, src_key_padding_mask=sorted_mask)\n",
        "        # # torch.zeros_like(sorted_sx)\n",
        "        # sx = sorted_sx[torch.arange(batch).unsqueeze(-1),ids.argsort(1)]\n",
        "        # sy_ = self.predicter(sx, src_key_padding_mask=mask)\n",
        "        # sy = self.target_encoder(x)*~mask\n",
        "\n",
        "\n",
        "        # target_mask = multiblock(batch, seq, min_s=0.15, max_s=0.2, M=4)\n",
        "        # context_mask = ~multiblock(batch, seq, min_s=0.85, max_s=1., M=1)|target_mask\n",
        "        # sx = self.context_encoder(x, src_key_padding_mask=context_mask)\n",
        "        # sy_ = self.predicter(sx, cls_mask=target_mask)*target_mask.unsqueeze(-1)\n",
        "        # sy_ = self.predicter(sx, cls_mask=target_mask)\n",
        "        # # print(sy_.shape, target_mask.shape)\n",
        "        # sy = self.target_encoder(x)*target_mask.unsqueeze(-1)\n",
        "\n",
        "\n",
        "        M=4\n",
        "        target_mask = multiblock(M*batch, seq, min_s=0.15, max_s=0.2, M=1) # mask out targets to be predicted # [4*batch, seq]\n",
        "        context_mask = ~multiblock(batch, seq, min_s=0.85, max_s=1., M=1)|target_mask.reshape(batch,M,seq).any(1) # [batch, seq]\n",
        "\n",
        "\n",
        "        x = torch.cat([enc_summary.unsqueeze(1), x], dim=1) # [batch, 1+T, d_model]\n",
        "        target_mask = torch.cat([torch.zeros((M*batch, 1), dtype=bool), target_mask],dim=1) # [4*batch, 1+T]\n",
        "        context_mask = torch.cat([torch.zeros((batch, 1), dtype=bool), context_mask],dim=1) # [batch, 1+T]\n",
        "\n",
        "        # x = x.repeat(4,1,1)\n",
        "        # context_mask = context_mask.repeat(4,1)\n",
        "        sx = self.context_encoder(x, src_key_padding_mask=context_mask).repeat(M,1,1)\n",
        "        sy_ = self.predicter(sx, cls_mask=target_mask)*target_mask.unsqueeze(-1)\n",
        "        # sy_ = self.predicter(sx, cls_mask=target_mask)\n",
        "        # print(sy_.shape, target_mask.shape)\n",
        "        sy = self.target_encoder(x).repeat(M,1,1)*target_mask.unsqueeze(-1)\n",
        "\n",
        "        loss = F.smooth_l1_loss(sy, sy_)\n",
        "        return loss\n",
        "\n",
        "    # def forward(self, hr, temp, heart): # [batch, 1+T]\n",
        "    #     # batch = hr.size(0)\n",
        "    #     # batch, seq, dim = x.shape\n",
        "    #     res_id, activeKilocalories, steps = hr[:,0], temp[:,0], heart[:,0]\n",
        "    #     enc_summary = self.enc0(torch.cat([self.calorie_emb(activeKilocalories), self.steps_emb(steps)], dim=-1)) # [batch, d_model]\n",
        "    #     x = self.enc1(torch.cat([self.hour_emb(hr[:,1:]/24), self.temp_emb(temp[:,1:]), self.heart_emb(heart[:,1:])], dim=-1)) # [batch, T, d_model]\n",
        "\n",
        "    #     x = torch.cat([enc_summary.unsqueeze(1), x], dim=1)\n",
        "    #     # sx = self.context_encoder(x)\n",
        "    #     sx = self.target_encoder(x)\n",
        "    #     out = sx.mean(dim=1)\n",
        "    #     return out\n",
        "\n",
        "\n",
        "\n",
        "seq_jepa = SeqJEPA(in_dim=16, d_model=32, out_dim=16, num_layers=1).to(device)#.to(torch.float)\n",
        "soptim = torch.optim.AdamW(seq_jepa.parameters(), lr=1e-3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bNjp4xFsGPoa"
      },
      "outputs": [],
      "source": [
        "# @title ijepa vision_transformer.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/models/vision_transformer.py\n",
        "import math\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from src.utils.tensors import (trunc_normal_, repeat_interleave_batch)\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid height and width\n",
        "    return:\n",
        "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid_h = np.arange(grid_size, dtype=float)\n",
        "    grid_w = np.arange(grid_size, dtype=float)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    assert embed_dim % 2 == 0\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid length\n",
        "    return:\n",
        "    pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid = np.arange(grid_size, dtype=float)\n",
        "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=float)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega   # (D/2,)\n",
        "    pos = pos.reshape(-1)   # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
        "    emb = np.concatenate([np.sin(out), np.cos(out)], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x, attn\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x, return_attention=False):\n",
        "        y, attn = self.attn(self.norm1(x))\n",
        "        if return_attention:\n",
        "            return attn\n",
        "        x = x + self.drop_path(y)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvEmbed(nn.Module):\n",
        "    \"\"\"3x3 Convolution stems for ViT following ViTC models\"\"\"\n",
        "    def __init__(self, channels, strides, img_size=224, in_chans=3, batch_norm=True):\n",
        "        super().__init__()\n",
        "        # Build the stems\n",
        "        stem = []\n",
        "        channels = [in_chans] + channels\n",
        "        for i in range(len(channels) - 2):\n",
        "            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3, stride=strides[i], padding=1, bias=(not batch_norm))]\n",
        "            if batch_norm:\n",
        "                stem += [nn.BatchNorm2d(channels[i+1])]\n",
        "            stem += [nn.ReLU(inplace=True)]\n",
        "        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n",
        "        self.stem = nn.Sequential(*stem)\n",
        "\n",
        "        # Comptute the number of patches\n",
        "        stride_prod = int(np.prod(strides))\n",
        "        self.num_patches = (img_size[0] // stride_prod)**2\n",
        "\n",
        "    def forward(self, x):\n",
        "        p = self.stem(x)\n",
        "        return p.flatten(2).transpose(1, 2)\n",
        "\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "def apply_masks(x, masks):\n",
        "    \"\"\":param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
        "    :param masks: list of tensors containing indices of patches in [N] to keep\"\"\"\n",
        "    all_x = []\n",
        "    for m in masks:\n",
        "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [batch,T,dim]\n",
        "        all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "    return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "\n",
        "class VisionTransformerPredictor(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, num_patches, embed_dim=768, predictor_embed_dim=384,\n",
        "        depth=6, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.init_std = init_std\n",
        "        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        # --\n",
        "        self.predictor_pos_embed = nn.Parameter(torch.zeros(1, num_patches, predictor_embed_dim), requires_grad=False)\n",
        "        predictor_pos_embed = get_2d_sincos_pos_embed(self.predictor_pos_embed.shape[-1], int(num_patches**.5), cls_token=False)\n",
        "        self.predictor_pos_embed.data.copy_(torch.from_numpy(predictor_pos_embed).float().unsqueeze(0)) # [1, num_patches, predictor_embed_dim]\n",
        "        # --\n",
        "        self.predictor_blocks = nn.ModuleList([Block(dim=predictor_embed_dim, num_heads=num_heads, drop=drop_rate, drop_path=dpr[i]) for i in range(depth)])\n",
        "        self.predictor_norm = nn.LayerNorm(predictor_embed_dim)\n",
        "        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n",
        "        # ------\n",
        "        # trunc_normal_(self.mask_token, std=self.init_std)\n",
        "\n",
        "    def forward(self, x, masks_x, masks):\n",
        "        assert (masks is not None) and (masks_x is not None), 'Cannot run predictor without mask indices'\n",
        "        if not isinstance(masks_x, list): masks_x = [masks_x]\n",
        "        if not isinstance(masks, list): masks = [masks]\n",
        "\n",
        "        # -- Batch Size\n",
        "        B = len(x) // len(masks_x)\n",
        "\n",
        "        # -- map from encoder-dim to pedictor-dim\n",
        "        x = self.predictor_embed(x) # [batch, num_patches, predictor_embed_dim]\n",
        "\n",
        "        # -- add positional embedding to x tokens\n",
        "        x_pos_embed = self.predictor_pos_embed.repeat(B, 1, 1) # [batch, num_patches, predictor_embed_dim]\n",
        "        x += apply_masks(x_pos_embed, masks_x)\n",
        "\n",
        "        _, N_ctxt, D = x.shape\n",
        "\n",
        "        # -- concat mask tokens to x\n",
        "        pos_embs = self.predictor_pos_embed.repeat(B, 1, 1)\n",
        "        pos_embs = apply_masks(pos_embs, masks)\n",
        "        pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x))\n",
        "        # --\n",
        "        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1)\n",
        "        # --\n",
        "        pred_tokens += pos_embs\n",
        "        x = x.repeat(len(masks), 1, 1)\n",
        "        x = torch.cat([x, pred_tokens], dim=1)\n",
        "\n",
        "        # -- fwd prop\n",
        "        for blk in self.predictor_blocks:\n",
        "            x = blk(x)\n",
        "        x = self.predictor_norm(x)\n",
        "\n",
        "        # -- return preds for mask tokens\n",
        "        x = x[:, N_ctxt:]\n",
        "        x = self.predictor_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, img_size=[224], patch_size=16, in_chans=3, embed_dim=768, predictor_embed_dim=384, depth=12, predictor_depth=12,\n",
        "        num_heads=12, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0, norm_layer=nn.LayerNorm, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_features = self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.init_std = init_std\n",
        "        # --\n",
        "        self.patch_embed = PatchEmbed(img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        # --\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=False)\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "        # --\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "    def forward(self, x, masks=None):\n",
        "        if masks is not None:\n",
        "            if not isinstance(masks, list): masks = [masks]\n",
        "\n",
        "        x = self.patch_embed(x)\n",
        "        B, N, D = x.shape # [batch, T, d_model]\n",
        "\n",
        "        # -- add positional embedding to x\n",
        "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
        "        x = x + pos_embed\n",
        "\n",
        "        if masks is not None: x = apply_masks(x, masks)\n",
        "\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            x = blk(x)\n",
        "\n",
        "        if self.norm is not None: x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "    def interpolate_pos_encoding(self, x, pos_embed):\n",
        "        npatch = x.shape[1] - 1\n",
        "        N = pos_embed.shape[1] - 1\n",
        "        if npatch == N: return pos_embed\n",
        "        class_emb, pos_embed = pos_embed[:, 0], pos_embed[:, 1:]\n",
        "        dim = x.shape[-1]\n",
        "        pos_embed = nn.functional.interpolate(pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2), scale_factor=math.sqrt(npatch / N), mode='bicubic',)\n",
        "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
        "\n",
        "# from functools import partial\n",
        "# def vit_predictor(**kwargs):\n",
        "#     model = VisionTransformerPredictor(\n",
        "#         mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "#     return model\n",
        "\n",
        "def vit(patch_size=16, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, # tiny\n",
        "        # patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, # small\n",
        "        # patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, # base\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tf4T37woBV2V"
      },
      "outputs": [],
      "source": [
        "# @title show collated_masks\n",
        "# print(collated_batch, collated_masks_enc, collated_masks_pred)\n",
        "# print(collated_batch) # tensor([0, 1, 2, 3]) batch\n",
        "# print(collated_masks_enc) # nenc*[M*[pred mask blocks patch ind]]\n",
        "# print(collated_masks_pred) # npred * [M * [context]]\n",
        "\n",
        "\n",
        "# print(len(collated_masks_enc[0]), len(collated_masks_pred[0]))\n",
        "# print(collated_masks_enc[0], collated_masks_pred[0])\n",
        "\n",
        "h,w=14,14\n",
        "img = torch.ones(h*w)\n",
        "# img[collated_masks_enc[0]]=0\n",
        "img[collated_masks_pred[0][2]]=0\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    # plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.imshow(npimg)\n",
        "    plt.show()\n",
        "\n",
        "# imshow(img.reshape(h,w))\n",
        "\n",
        "# for masks in collated_masks_pred:\n",
        "#     for mask in masks:\n",
        "#         img = torch.ones(h*w)\n",
        "#         img[mask]=0\n",
        "#         imshow(img.reshape(h,w))\n",
        "\n",
        "\n",
        "for masks in collated_masks_enc:\n",
        "    for mask in masks:\n",
        "        img = torch.ones(h*w)\n",
        "        img[mask]=0\n",
        "        imshow(img.reshape(h,w))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aH1UOD4WW8I2",
        "outputId": "6fafea1b-0af2-4bc7-fa26-46b5e56549b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vit fwd torch.Size([4, 3, 224, 224])\n",
            "vit fwd torch.Size([4, 196, 768])\n",
            "vit fwd torch.Size([4, 11, 768])\n",
            "predictor fwd1 torch.Size([4, 11, 384]) torch.Size([4, 196, 384])\n"
          ]
        }
      ],
      "source": [
        "# @title ijepa vision_transformer.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/models/vision_transformer.py\n",
        "import math\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# from src.utils.tensors import (trunc_normal_, repeat_interleave_batch)\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid height and width\n",
        "    return:\n",
        "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid_h = np.arange(grid_size, dtype=float)\n",
        "    grid_w = np.arange(grid_size, dtype=float)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    assert embed_dim % 2 == 0\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid length\n",
        "    return:\n",
        "    pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid = np.arange(grid_size, dtype=float)\n",
        "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=float)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega   # (D/2,)\n",
        "    pos = pos.reshape(-1)   # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
        "    emb = np.concatenate([np.sin(out), np.cos(out)], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "def repeat_interleave_batch(x, B, repeat):\n",
        "    N = len(x) // B\n",
        "    x = torch.cat([\n",
        "        torch.cat([x[i*B:(i+1)*B] for _ in range(repeat)], dim=0)\n",
        "        for i in range(N)\n",
        "    ], dim=0)\n",
        "    return x\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x, attn\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x, return_attention=False):\n",
        "        y, attn = self.attn(self.norm1(x))\n",
        "        if return_attention:\n",
        "            return attn\n",
        "        x = x + self.drop_path(y)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvEmbed(nn.Module):\n",
        "    \"\"\"3x3 Convolution stems for ViT following ViTC models\"\"\"\n",
        "    def __init__(self, channels, strides, img_size=224, in_chans=3, batch_norm=True):\n",
        "        super().__init__()\n",
        "        # Build the stems\n",
        "        stem = []\n",
        "        channels = [in_chans] + channels\n",
        "        for i in range(len(channels) - 2):\n",
        "            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3, stride=strides[i], padding=1, bias=(not batch_norm))]\n",
        "            if batch_norm:\n",
        "                stem += [nn.BatchNorm2d(channels[i+1])]\n",
        "            stem += [nn.ReLU(inplace=True)]\n",
        "        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n",
        "        self.stem = nn.Sequential(*stem)\n",
        "\n",
        "        # Comptute the number of patches\n",
        "        stride_prod = int(np.prod(strides))\n",
        "        self.num_patches = (img_size[0] // stride_prod)**2\n",
        "\n",
        "    def forward(self, x):\n",
        "        p = self.stem(x)\n",
        "        return p.flatten(2).transpose(1, 2)\n",
        "\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "def apply_masks(x, masks):\n",
        "    \"\"\":param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
        "    :param masks: list of tensors containing indices of patches in [N] to keep\"\"\"\n",
        "    all_x = []\n",
        "    for m in masks:\n",
        "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [batch,T,dim]\n",
        "        all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "    return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "\n",
        "class VisionTransformerPredictor(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, num_patches, embed_dim=768, predictor_embed_dim=384,\n",
        "        depth=6, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.init_std = init_std\n",
        "        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        # --\n",
        "        self.predictor_pos_embed = nn.Parameter(torch.zeros(1, num_patches, predictor_embed_dim), requires_grad=False)\n",
        "        predictor_pos_embed = get_2d_sincos_pos_embed(self.predictor_pos_embed.shape[-1], int(num_patches**.5), cls_token=False)\n",
        "        self.predictor_pos_embed.data.copy_(torch.from_numpy(predictor_pos_embed).float().unsqueeze(0)) # [1, num_patches, predictor_embed_dim]\n",
        "        # --\n",
        "        self.predictor_blocks = nn.ModuleList([Block(dim=predictor_embed_dim, num_heads=num_heads, drop=drop_rate, drop_path=dpr[i]) for i in range(depth)])\n",
        "        self.predictor_norm = nn.LayerNorm(predictor_embed_dim)\n",
        "        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n",
        "        # ------\n",
        "        # trunc_normal_(self.mask_token, std=self.init_std)\n",
        "\n",
        "    def forward(self, x, masks_x, masks): # [batch, num_context_patches, embed_dim], nenc*[batch, num_context_patches], npred*[batch, num_target_patches]\n",
        "        assert (masks is not None) and (masks_x is not None), 'Cannot run predictor without mask indices'\n",
        "        if not isinstance(masks_x, list): masks_x = [masks_x] # context mask\n",
        "        if not isinstance(masks, list): masks = [masks] # pred mask\n",
        "\n",
        "        # -- Batch Size\n",
        "        B = len(x) // len(masks_x)\n",
        "        # print(\"predictor fwd0\", x.shape) # [3, 121, 768])\n",
        "        # -- map from encoder-dim to pedictor-dim\n",
        "        x = self.predictor_embed(x) # [batch, num_patches, predictor_embed_dim]\n",
        "\n",
        "        # -- add positional embedding to x tokens\n",
        "        x_pos_embed = self.predictor_pos_embed.repeat(B, 1, 1) # [batch, num_patches, predictor_embed_dim]\n",
        "        print(\"predictor fwd1\", x.shape, x_pos_embed.shape) # [3, 121 or 11, 384], [3, 196, 384]\n",
        "        # print(\"predictor fwd masks_x\", masks_x)\n",
        "        x += apply_masks(x_pos_embed, masks_x) # get pos emb of context patches\n",
        "\n",
        "        # print(\"predictor fwd x\", x.shape) # [4, 11, 384])\n",
        "        _, N_ctxt, D = x.shape\n",
        "\n",
        "        # -- concat mask tokens to x\n",
        "        pos_embs = self.predictor_pos_embed.repeat(B, 1, 1) # [batch, num_patches, predictor_embed_dim]\n",
        "        pos_embs = apply_masks(pos_embs, masks) # [16, 104, predictor_embed_dim]\n",
        "        pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x)) # [16, 104, predictor_embed_dim]\n",
        "        # --\n",
        "        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1) # [16, 104, predictor_embed_dim]\n",
        "        # --\n",
        "        # print(\"predictor fwd pred_tokens\", pred_tokens.shape)\n",
        "\n",
        "        pred_tokens += pos_embs\n",
        "        x = x.repeat(len(masks), 1, 1)\n",
        "        x = torch.cat([x, pred_tokens], dim=1)\n",
        "\n",
        "        # -- fwd prop\n",
        "        for blk in self.predictor_blocks:\n",
        "            x = blk(x)\n",
        "        x = self.predictor_norm(x)\n",
        "\n",
        "        # -- return preds for mask tokens\n",
        "        x = x[:, N_ctxt:]\n",
        "        x = self.predictor_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, img_size=[224], patch_size=16, in_chans=3, embed_dim=768, predictor_embed_dim=384, depth=12, predictor_depth=12,\n",
        "        num_heads=12, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0, norm_layer=nn.LayerNorm, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_features = self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.init_std = init_std\n",
        "        # --\n",
        "        self.patch_embed = PatchEmbed(img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        # --\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=False)\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "        # --\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "    def forward(self, x, masks=None): # [batch,3,224,224]\n",
        "        if masks is not None:\n",
        "            if not isinstance(masks, list): masks = [masks]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        x = self.patch_embed(x)\n",
        "        B, N, D = x.shape # [batch, num_patches, embed_dim]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        # -- add positional embedding to x\n",
        "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
        "        x = x + pos_embed\n",
        "\n",
        "        if masks is not None: x = apply_masks(x, masks) # [batch, num_context_patches, embed_dim]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            x = blk(x)\n",
        "\n",
        "        if self.norm is not None: x = self.norm(x)\n",
        "        return x # [batch, num_context_patches, embed_dim]\n",
        "\n",
        "    def interpolate_pos_encoding(self, x, pos_embed):\n",
        "        npatch = x.shape[1] - 1\n",
        "        N = pos_embed.shape[1] - 1\n",
        "        if npatch == N: return pos_embed\n",
        "        class_emb, pos_embed = pos_embed[:, 0], pos_embed[:, 1:]\n",
        "        dim = x.shape[-1]\n",
        "        pos_embed = nn.functional.interpolate(pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2), scale_factor=math.sqrt(npatch / N), mode='bicubic',)\n",
        "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
        "\n",
        "# from functools import partial\n",
        "# def vit_predictor(**kwargs):\n",
        "#     model = VisionTransformerPredictor(\n",
        "#         mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "#     return model\n",
        "\n",
        "def vit(patch_size=16, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, # tiny\n",
        "        # patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, # small\n",
        "        # patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, # base\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "# encoder = vit()\n",
        "encoder = VisionTransformer(\n",
        "        patch_size=16, embed_dim=768, depth=1, num_heads=3, mlp_ratio=1,\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
        "\n",
        "# num_patches = (224/patch_size)^2\n",
        "# predictor = VisionTransformerPredictor(num_patches, embed_dim=768, predictor_embed_dim=384, depth=6, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02, **kwargs)\n",
        "predictor = VisionTransformerPredictor(num_patches=196, embed_dim=768, predictor_embed_dim=384, depth=1, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02)\n",
        "\n",
        "batch = 4\n",
        "img = torch.rand(batch, 3, 224, 224)\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/train.py\n",
        "mask_collator = MaskCollator(\n",
        "        input_size=(224, 224),\n",
        "        patch_size=16,\n",
        "        enc_mask_scale=(0.2, 0.8),\n",
        "        pred_mask_scale=(0.2, 0.8),\n",
        "        aspect_ratio=(0.3, 3.0),\n",
        "        nenc=1,\n",
        "        npred=4,\n",
        "        min_keep=4,\n",
        "        # allow_overlap=True)\n",
        "        allow_overlap=False)\n",
        "# self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
        "\n",
        "\n",
        "\n",
        "# v = mask_collator.step()\n",
        "# should pass the collater a list batch of idx?\n",
        "# collated_batch, collated_masks_enc, collated_masks_pred = mask_collator([batch,3,8])\n",
        "collated_batch, collated_masks_enc, collated_masks_pred = mask_collator([0,1,2,3])\n",
        "# print(v)\n",
        "\n",
        "\n",
        "import copy\n",
        "target_encoder = copy.deepcopy(encoder)\n",
        "\n",
        "\n",
        "def forward_target():\n",
        "    with torch.no_grad():\n",
        "        h = target_encoder(imgs)\n",
        "        h = F.layer_norm(h, (h.size(-1),))  # normalize over feature-dim\n",
        "        B = len(h)\n",
        "        # -- create targets (masked regions of h)\n",
        "        h = apply_masks(h, masks_pred)\n",
        "        h = repeat_interleave_batch(h, B, repeat=len(masks_enc))\n",
        "        return h\n",
        "\n",
        "def forward_context():\n",
        "    z = encoder(imgs, masks_enc)\n",
        "    z = predictor(z, masks_enc, masks_pred)\n",
        "    return z\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    m = next(momentum_scheduler)\n",
        "                    for param_q, param_k in zip(encoder.parameters(), target_encoder.parameters()):\n",
        "                        param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "\n",
        "\n",
        "# # num_context_patches = 121 if allow_overlap=True else 11\n",
        "z = encoder(img, collated_masks_enc) # [batch, num_context_patches, embed_dim]\n",
        "# print(len(collated_masks_enc), len(collated_masks_pred)) # nenc, npred\n",
        "# print(collated_masks_enc[0].shape, collated_masks_pred[0].shape) # , [batch, num_context_patches = 121 or 11], [batch, num_target_patches]\n",
        "out = predictor(z, collated_masks_enc, collated_masks_pred)\n",
        "# # print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "id": "NLHOwdqK222R",
        "outputId": "a211c62f-141d-461a-ecfe-70bd061d6194"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([128, 3, 32, 32])\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# @title RandomResizedCrop is same across batch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# dataiter = iter(train_data)\n",
        "dataiter = iter(train_loader)\n",
        "img,y = next(dataiter)\n",
        "# img = img.unsqueeze(0)\n",
        "# # img = F.interpolate(img, (8,8))\n",
        "# b,c,h,w = img.shape\n",
        "# img = img.flatten(2).transpose(-2,-1).to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "print(img.shape)\n",
        "\n",
        "from torchvision.transforms import v2\n",
        "resize_cropper = v2.RandomResizedCrop(size=(32, 32))\n",
        "resized_crops = resize_cropper(img)\n",
        "\n",
        "# imshow(img[0])\n",
        "# imshow(resized_crops[0])\n",
        "# imshow(img[1])\n",
        "# imshow(resized_crops[1])\n",
        "\n",
        "import torchvision\n",
        "imshow(torchvision.utils.make_grid(img[:8].cpu(), nrow=4))\n",
        "imshow(torchvision.utils.make_grid(resized_crops[:8].cpu(), nrow=4))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 879
        },
        "id": "D6lVtbS5OHIv",
        "outputId": "13b6ad36-0c36-4d1e-daf0-70b098b51a2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 1024, 3])\n",
            "torch.Size([1024]) torch.Size([1, 1024])\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# @title test multiblock params\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "dataiter = iter(train_data)\n",
        "img,y = next(dataiter)\n",
        "img = img.unsqueeze(0)\n",
        "b,c,h,w = img.shape\n",
        "img = img.flatten(2).transpose(-2,-1).to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "print(img.shape)\n",
        "batch, seq,_ = img.shape\n",
        "\n",
        "\n",
        "# target_mask = randpatch(seq, mask_size=8, gamma=.9) # 8.9 [seq]\n",
        "target_mask = multiblock(seq, min_s=0.2, max_s=0.3, M=4).any(0) # best.2.3M4 og.15.2M4# mask out targets to be predicted # [M, seq]\n",
        "target_mask = multiblock(seq, min_s=0.2, max_s=0.3, M=4).any(0)\n",
        "\n",
        "context_mask = ~multiblock(seq, min_s=0.85, max_s=1., M=1)|target_mask # [1, seq], True->Mask\n",
        "\n",
        "# print(target_mask, context_mask)\n",
        "\n",
        "print(target_mask.shape, context_mask.shape)\n",
        "# target_img, context_img = img*target_mask.unsqueeze(-1), img*context_mask.unsqueeze(-1)\n",
        "target_img, context_img = img*~target_mask.unsqueeze(-1), img*~context_mask.unsqueeze(-1)\n",
        "# print(target_img.shape, context_img.shape)\n",
        "target_img, context_img = target_img.transpose(-2,-1).reshape(b,c,h,w), context_img.transpose(-2,-1).reshape(b,c,h,w)\n",
        "target_img, context_img = target_img.float(), context_img.float()\n",
        "\n",
        "# imshow(out.detach().cpu())\n",
        "imshow(target_img[0])\n",
        "imshow(context_img[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiHDuPjB0SBt"
      },
      "outputs": [],
      "source": [
        "\n",
        "trg_msks = []\n",
        "ctx_msks = []\n",
        "for i in range(16):\n",
        "    # target_mask = randpatch(seq, mask_size=8, gamma=.9) # 8.9 [seq]\n",
        "    target_mask = multiblock(seq, min_s=0.2, max_s=0.3, M=4).any(0) # best.2.3M4 og.15.2M4# mask out targets to be predicted # [M, seq]\n",
        "    target_mask = multiblock(seq, min_s=0.2, max_s=0.3, M=4).any(0)\n",
        "\n",
        "    context_mask = ~multiblock(seq, min_s=0.85, max_s=1., M=1)|target_mask # [1, seq], True->Mask\n",
        "\n",
        "    # print(target_mask.shape, context_mask.shape)\n",
        "    target_img, context_img = img*~target_mask.unsqueeze(-1), img*~context_mask.unsqueeze(-1)\n",
        "    target_img, context_img = target_img.transpose(-2,-1).reshape(b,c,h,w).float(), context_img.transpose(-2,-1).reshape(b,c,h,w).float()\n",
        "    trg_msks.append(target_img)\n",
        "    ctx_msks.append(context_img)\n",
        "\n",
        "trg_msks = torch.cat(trg_msks, dim=0)\n",
        "ctx_msks = torch.cat(ctx_msks, dim=0)\n",
        "imshow(torchvision.utils.make_grid(trg_msks.cpu(), nrow=4))\n",
        "imshow(torchvision.utils.make_grid(ctx_msks.cpu(), nrow=4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8xv1tpKIhim",
        "outputId": "e6613fee-2d32-4c6a-ef96-dee18da3aa6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([73, 73, 73, 73])\n",
            "tensor([248])\n"
          ]
        }
      ],
      "source": [
        "# @title test multiblock mask\n",
        "def multiblock(seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    # mask_len, mask_pos = mask_len * seq, mask_pos * seq\n",
        "    indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "    return target_mask\n",
        "\n",
        "\n",
        "seq=400\n",
        "M=4\n",
        "\n",
        "# target_mask = multiblock(M, seq, min_s=0.15, max_s=0.2, M=1) # mask out targets to be predicted # [4*batch, seq]\n",
        "# context_mask = ~multiblock(1, seq, min_s=0.85, max_s=1., M=1)|target_mask.reshape(M,seq).any(0) # [batch, seq]\n",
        "\n",
        "\n",
        "target_mask = multiblock(seq, min_s=0.15, max_s=0.2, M=4) # mask out targets to be predicted # [4*batch, seq]\n",
        "context_mask = ~multiblock(seq, min_s=0.85, max_s=1., M=1)|target_mask.reshape(M,seq).any(0) # [batch, seq]\n",
        "\n",
        "# target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "\n",
        "# print(target_mask)\n",
        "# print(context_mask)\n",
        "\n",
        "print(target_mask.sum(-1))\n",
        "print(context_mask.sum(-1))\n",
        "\n",
        "\n",
        "# sorted_mask, ids = context_mask.sort(dim=1, stable=True)\n",
        "# indices = ids[~sorted_mask]#.reshape(M,-1) # int idx [num_context_toks] , idx of context not masked\n",
        "# sorted_x = x[torch.arange(batch).unsqueeze(-1), indices] # [batch, seq-num_trg_toks, dim]\n",
        "# print(indices)\n",
        "# print(x)\n",
        "# print(sorted_x)\n",
        "\n",
        "\n",
        "\n",
        "# sorted_mask, ids = target_mask.sort(dim=1, descending=False, stable=True)\n",
        "# # print(sorted_mask, ids)\n",
        "# indices = ids[~sorted_mask].reshape(M,-1) # int idx [M, seq-num_trg_toks] , idx of target not masked\n",
        "# print(indices)\n",
        "# batch=3\n",
        "# dim=2\n",
        "# # indices = indices.expand(batch,-1,-1)\n",
        "# # x = torch.arange(batch*seq).reshape(batch,seq).to(device)\n",
        "# x = torch.rand(batch, seq, dim)\n",
        "# print(x)\n",
        "# sorted_x = x[torch.arange(batch)[...,None,None], indices]\n",
        "# print(sorted_x.shape) # [batch, M, seq-num_trg_toks, dim]\n",
        "# print(sorted_x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hxzeZzdYm9C",
        "outputId": "2e61c465-c5dc-445b-a985-f6b442bfbd96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1.])\n"
          ]
        }
      ],
      "source": [
        "img = torch.ones(200)\n",
        "img[collated_masks_enc[0][0]] = 0\n",
        "print(img)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "1KUcAQEgKreY",
        "outputId": "288bb97f-e8a9-4ae5-c00c-3ec9563d6e02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "        [0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
            "        [0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
            "        [0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1.],\n",
            "        [0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1.],\n",
            "        [0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1.],\n",
            "        [0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "        [0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# @title ijepa multiblock next\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "import math\n",
        "from multiprocessing import Value\n",
        "import torch\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(self, input_size=(224, 224),\n",
        "        enc_mask_scale=(0.2, 0.8), pred_mask_scale=(0.2, 0.8), aspect_ratio=(0.3, 3.0),\n",
        "        nenc=1, npred=2, min_keep=4, allow_overlap=False):\n",
        "        super().__init__()\n",
        "        if not isinstance(input_size, tuple): input_size = (input_size,) * 2\n",
        "        self.height, self.width = input_size\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.aspect_ratio = aspect_ratio\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "\n",
        "    def _sample_block_size(self, scale, aspect_ratio_scale):\n",
        "        _rand = torch.rand(1).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.height * self.width * mask_scale) # num patches to keep\n",
        "        # -- Sample block aspect-ratio\n",
        "        min_ar, max_ar = aspect_ratio_scale\n",
        "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
        "        while h >= self.height: h -= 1 # crop mask to be smaller than img\n",
        "        while w >= self.width: w -= 1\n",
        "        return (h, w)\n",
        "\n",
        "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
        "        h, w = b_size\n",
        "\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # --\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            top = torch.randint(0, self.height - h, (1,))\n",
        "            left = torch.randint(0, self.width - w, (1,))\n",
        "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
        "            mask[top:top+h, left:left+w] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
        "        mask_complement[top:top+h, left:left+w] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, B):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        p_size = self._sample_block_size(scale=self.pred_mask_scale, aspect_ratio_scale=self.aspect_ratio)\n",
        "        e_size = self._sample_block_size(scale=self.enc_mask_scale, aspect_ratio_scale=(1., 1.))\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.height * self.width\n",
        "        min_keep_enc = self.height * self.width\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                print(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "        return collated_masks_enc, collated_masks_pred\n",
        "\n",
        "mask_collator = MaskCollator(input_size=(224, 224), enc_mask_scale=(.85, 1.), pred_mask_scale=(.15, .2),\n",
        "        aspect_ratio=(.75, 1.5), nenc=1, npred=4, min_keep=4,\n",
        "        # allow_overlap=True)\n",
        "        allow_overlap=False)\n",
        "\n",
        "collated_masks_enc, collated_masks_pred = mask_collator(4)\n",
        "\n",
        "img = torch.ones((224//16)**2)\n",
        "img[collated_masks_enc[0]] = 0\n",
        "img = img.reshape(224//16, 224//16)\n",
        "# imshow(img)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    # plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "print(img)\n",
        "display(imshow(img))\n",
        "# imshow()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwBmCMXVgPZj",
        "outputId": "ce487f85-3f64-4a60-e3da-3cc957b71178"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[torch.Size([4, 63])] [torch.Size([4, 35]), torch.Size([4, 35]), torch.Size([4, 35]), torch.Size([4, 35])]\n"
          ]
        }
      ],
      "source": [
        "print([c.shape for c in collated_masks_enc], [c.shape for c in collated_masks_pred])\n",
        "\n",
        "# print(collated_masks_enc[0], collated_masks_pred[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "COvEnBXPYth3"
      },
      "outputs": [],
      "source": [
        "# @title ijepa multiblock down\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "import math\n",
        "from multiprocessing import Value\n",
        "import torch\n",
        "_GLOBAL_SEED = 0\n",
        "\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(self, input_size=(224, 224), patch_size=16,\n",
        "        enc_mask_scale=(0.2, 0.8), pred_mask_scale=(0.2, 0.8), aspect_ratio=(0.3, 3.0),\n",
        "        nenc=1, npred=2, min_keep=4, allow_overlap=False):\n",
        "        super().__init__()\n",
        "        if not isinstance(input_size, tuple): input_size = (input_size,) * 2\n",
        "        self.patch_size = patch_size\n",
        "        self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.aspect_ratio = aspect_ratio\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "        self._itr_counter = Value('i', -1)  # collator is shared across worker processes\n",
        "\n",
        "    def step(self):\n",
        "        i = self._itr_counter\n",
        "        with i.get_lock():\n",
        "            i.value += 1\n",
        "            v = i.value\n",
        "        return v\n",
        "\n",
        "    def _sample_block_size(self, generator, scale, aspect_ratio_scale):\n",
        "        _rand = torch.rand(1, generator=generator).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.height * self.width * mask_scale)\n",
        "        # -- Sample block aspect-ratio\n",
        "        min_ar, max_ar = aspect_ratio_scale\n",
        "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
        "        while h >= self.height:\n",
        "            h -= 1\n",
        "        while w >= self.width:\n",
        "            w -= 1\n",
        "        return (h, w)\n",
        "\n",
        "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
        "        h, w = b_size\n",
        "\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # --\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            top = torch.randint(0, self.height - h, (1,))\n",
        "            left = torch.randint(0, self.width - w, (1,))\n",
        "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
        "            mask[top:top+h, left:left+w] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
        "        mask_complement[top:top+h, left:left+w] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        B = len(batch)\n",
        "\n",
        "        collated_batch = torch.utils.data.default_collate(batch)\n",
        "\n",
        "        seed = self.step()\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(seed)\n",
        "        p_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.pred_mask_scale,\n",
        "            aspect_ratio_scale=self.aspect_ratio)\n",
        "        e_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.enc_mask_scale,\n",
        "            aspect_ratio_scale=(1., 1.))\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.height * self.width\n",
        "        min_keep_enc = self.height * self.width\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                print(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "        return collated_batch, collated_masks_enc, collated_masks_pred\n",
        "\n",
        "# mask_collator = MaskCollator(\n",
        "#         input_size=(224, 224),\n",
        "#         patch_size=16,\n",
        "#         enc_mask_scale=(0.2, 0.8),\n",
        "#         pred_mask_scale=(0.2, 0.8),\n",
        "#         aspect_ratio=(0.3, 3.0),\n",
        "#         nenc=1,\n",
        "#         npred=4,\n",
        "#         min_keep=4,\n",
        "#         # allow_overlap=True)\n",
        "#         allow_overlap=False)\n",
        "# self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
        "\n",
        "mask_collator = MaskCollator(\n",
        "        input_size=(1, 224),\n",
        "        patch_size=1,\n",
        "        enc_mask_scale=(0.2, 0.8),\n",
        "        pred_mask_scale=(0.15, 0.2),\n",
        "        # aspect_ratio=(1, 1000),\n",
        "        aspect_ratio=(.001, 1000),\n",
        "        nenc=1,\n",
        "        npred=4,\n",
        "        min_keep=4,\n",
        "        # allow_overlap=True)\n",
        "        allow_overlap=False)\n",
        "\n",
        "# v = mask_collator.step()\n",
        "# should pass the collater a list batch of idx?\n",
        "# collated_batch, collated_masks_enc, collated_masks_pred = mask_collator([batch,3,8])\n",
        "collated_batch, collated_masks_enc, collated_masks_pred = mask_collator([0,1,2,3])\n",
        "# print(v)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DxjYI9RMQoP"
      },
      "source": [
        "## trash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "npc_xGtOz7DC"
      },
      "outputs": [],
      "source": [
        "# @title (Learned)RotEmb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, seq_len=512, base=10000):\n",
        "        super().__init__()\n",
        "        self.dim, self.base = dim, base\n",
        "        theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "        pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "        angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "        self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.base)\n",
        "        return x * self.rot_emb[:seq_len]\n",
        "\n",
        "# class LearnedRoPE(nn.Module): # learnt RoPE ; each tok is 1 pos\n",
        "#     def __init__(self, dim):\n",
        "#         super().__init__()\n",
        "#         self.weights = nn.Parameter(torch.randn(1, dim//2))\n",
        "\n",
        "#     def forward(self, x): #\n",
        "#         batch, seq_len, dim = x.shape\n",
        "#         # if rot_emb.shape[0] < seq_len: self.__init__(dim, seq_len)\n",
        "#         pos = torch.arange(seq_len).unsqueeze(1)\n",
        "#         angles = (self.weights * pos * 2*torch.pi).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "#         rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "#         return x * rot_emb.flatten(-2).unsqueeze(0)\n",
        "\n",
        "\n",
        "class LearnedRoPE(nn.Module): # learnt RoPE ; each tok is 1 pos\n",
        "    def __init__(self, dim, seq_len=512):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.weights = nn.Parameter(torch.randn(1, dim//2))\n",
        "        pos = torch.arange(seq_len).unsqueeze(1)\n",
        "        angles = (self.weights * pos * 2*torch.pi)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "        self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "    def forward(self, x): # [batch, seq_len, dim]\n",
        "        seq_len = x.size(1)\n",
        "        if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len)\n",
        "        return x * self.rot_emb[:seq_len]\n",
        "\n",
        "\n",
        "class RotEmb(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, top=torch.pi, base=10000):\n",
        "        super().__init__()\n",
        "        self.theta = top / (base ** (torch.arange(0, dim, step=2, device=device) / dim))\n",
        "        # self.theta = top / (base ** torch.linspace(0, 1, dim//2, device=device))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (pos.unsqueeze(-1) * self.theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [seq_len, dim]\n",
        "\n",
        "class LearnedRotEmb(nn.Module): # pos in R\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.weights = nn.Parameter(torch.randn((1, dim//2), device=device))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (self.weights * pos.unsqueeze(-1) * 2*torch.pi).unsqueeze(-1) # [batch, 1] * [1, dim//2] -> [batch, dim//2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [batch, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [batch, dim]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bnjJHfKj1_g",
        "outputId": "2e3e10eb-bc09-4ef7-c196-8cfe54602b6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5104\n",
            "torch.Size([4, 32, 16])\n"
          ]
        }
      ],
      "source": [
        "# @title ViT me simple wisdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    \"\"\"Zero out the parameters of a module and return it.\"\"\"\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads=None, d_head=8, cond_dim=None, dropout=0.): # .1\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_head = d_head\n",
        "        self.n_heads = d_model // d_head\n",
        "        # self.d_head = d_model // n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or d_model, 2*d_model, bias=False)\n",
        "        # self.k = nn.Sequential(nn.Dropout(dropout), nn.Linear(cond_dim, d_model, bias=False))\n",
        "        # self.lin = nn.Linear(d_model, d_model)\n",
        "        self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        # self.lin = nn.Sequential(nn.Dropout(dropout), zero_module(nn.Linear(d_model, d_model)))\n",
        "        self.drop = nn.Dropout(dropout) # indp before q,k,v; after linout\n",
        "        self.scale = self.d_head ** -.5\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [batch, T, d_model]=[batch, h*w, c], [batch, num_tok, cond_dim], [batch,T]\n",
        "        batch = x.shape[0]\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        Q = self.q(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        # K = self.k(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        K, V = self.kv(cond).view(batch, -1, self.n_heads, 2*self.d_head).transpose(1, 2).chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # linear attention # Softmax(Q) @ (Softmax(K).T @ V)\n",
        "        if mask != None:\n",
        "            mask = mask[:, None, :, None] # [batch,T] -> [batch,1,T,1]\n",
        "            K, V = K.masked_fill(mask, -torch.finfo(x.dtype).max), V.masked_fill(mask, -torch.finfo(x.dtype).max)\n",
        "        Q, K = Q.softmax(dim=-1)*self.scale, K.softmax(dim=-2)\n",
        "        context = K.transpose(-2,-1) @ V # [batch, n_heads, d_head, d_head]\n",
        "        out = Q @ context # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # (quadratic) attention # Softmax(Q @ K.T) @ V\n",
        "        # attn = Q @ K.transpose(-2,-1) * self.scale # [batch, n_heads, T] # [batch, n_heads, T, T/num_tok]\n",
        "        # if mask != None: attn = attn.masked_fill(mask[:, None, :, None], -torch.finfo(attn.dtype).max) # [batch,T]->[batch,1,T,1]\n",
        "        # attention = torch.softmax(attn, dim=-1)\n",
        "        # out = self.drop(attention) @ V # [batch, n_heads, T, d_head]\n",
        "\n",
        "        out = out.transpose(1, 2).flatten(2)\n",
        "        return self.drop(self.lin(out)) # [batch, T, d_model]\n",
        "\n",
        "# if self, dont pass cond_dim in init, dont pass cond in fwd\n",
        "# Softmax(Q @ K.T) @ V ~ Softmax(Q) @ Softmax(K).T @ V\n",
        "\n",
        "# https://github.com/lucidrains/x-transformers/blob/main/x_transformers/x_transformers.py#L1855\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_head=4, cond_dim=None, ff_mult=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm1 = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.norm2 = nn.RMSNorm(d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.cond_dim = cond_dim\n",
        "        self.self = MultiHeadAttention(d_model, d_head=d_head, dropout=0)\n",
        "        if self.cond_dim!=None: self.cross = MultiHeadAttention(d_model, d_head=d_head, cond_dim=cond_dim, dropout=0)\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        ff_dim=d_model*ff_mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), nn.Dropout(dropout), nn.Linear(d_model, ff_dim), act, # ReLU GELU\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "            nn.RMSNorm(d_model), nn.Dropout(dropout), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            # nn.RMSNorm(d_model), act, nn.Dropout(dropout), nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,c,h,w], [batch, num_tok, cond_dim], [batch,T]\n",
        "        # bchw = x.shape\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "\n",
        "        if self.cond_dim==None: cond=None # is self attn\n",
        "        x = x + self.drop(self.self(self.norm1(x), cond, mask))\n",
        "        if self.cond_dim!=None: x = x + self.cross(self.norm2(x), cond, mask)\n",
        "        x = x + self.ff(x)\n",
        "        # x = x + self.drop(self.ff(x))\n",
        "\n",
        "        # return x.transpose(1,2).reshape(*bchw)\n",
        "        return x\n",
        "\n",
        "import torch\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "def apply_masks(x, masks):\n",
        "    \"\"\":param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
        "    :param masks: list of tensors containing indices of patches in [N] to keep\"\"\"\n",
        "    all_x = []\n",
        "    for m in masks: # [1,T]\n",
        "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [1,T,dim]\n",
        "        all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "    return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        # self.positional_emb = nn.Parameter(torch.zeros(1, 32*32//4, d_model)) # positional_embedding == 'learnable'\n",
        "        # encoder_layers = nn.TransformerEncoderLayer(d_model, d_model//d_head, d_hid or d_model, dropout, batch_first=True)\n",
        "        # self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.zeros(1,1,d_model)) # randn zeros\n",
        "        nn.init.trunc_normal_(self.cls, std=.02)\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model)\n",
        "        self.lin = nn.Linear(d_model, out_dim)# if out_dim != d_model else None\n",
        "\n",
        "    def forward(self, src, context_indices, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        src = self.embed(src) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = src.shape\n",
        "        src = src * self.pos_encoder(context_indices)\n",
        "        # src = src + self.positional_emb[:,context_indices]\n",
        "        # src = apply_masks(src, [context_indices])\n",
        "\n",
        "        pred_tokens = self.cls * self.pos_encoder(trg_indices) # [M, num_trg_toks, d_model]\n",
        "        # pred_tokens = self.cls + self.positional_emb[0,trg_indices]\n",
        "        pred_tokens = pred_tokens.repeat(batch, 1, 1) # [batch*M, num_trg_toks, d_model]\n",
        "        # print(pred_tokens.requires_grad)\n",
        "        # print(\"pred fwd\", src.shape, pred_tokens.shape)\n",
        "        # src = src.repeat_interleave(trg_indices.shape[0], dim=0) # [batch, seq_len, d_model] -> [batch*M, seq_len, d_model]\n",
        "        src = torch.cat([src, pred_tokens], dim=1) # [batch*M, seq_len+num_trg_toks, d_model]\n",
        "\n",
        "        out = self.transformer_encoder(src) # float [seq_len, batch_size, d_model]\n",
        "        out = self.norm(out)\n",
        "        out = out[:,seq:] # [batch*M, num_trg_toks, d_model]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.MaxPool1d(2,2), #nn.MaxPool1d(3, 2, 3//2),\n",
        "            # nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.Conv1d(d_model, d_model,3,2,3//2)\n",
        "            nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2),\n",
        "            # nn.Conv1d(in_dim, d_model,2,2,0), # like patch\n",
        "            )\n",
        "        self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        # self.positional_emb = nn.Parameter(torch.zeros(1, 32*32//4, d_model)) # positional_embedding == 'learnable'\n",
        "        # encoder_layers = nn.TransformerEncoderLayer(d_model, d_model//d_head, d_hid or d_model, dropout, batch_first=True)\n",
        "        # self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model)\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, src, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "\n",
        "        # src = self.embed(src) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        src = self.embed(src.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "\n",
        "        batch, seq, dim = src.shape\n",
        "\n",
        "        # # # src = self.pos_encoder(src)\n",
        "        # if context_indices != None:\n",
        "        # print(src.shape, self.pos_encoder(context_indices).shape)\n",
        "        #     src = src * self.pos_encoder(context_indices) # context/predictor # src = src + self.positional_emb[:,context_indices]\n",
        "        # else: src = src * self.pos_encoder(torch.arange(seq, device=device).unsqueeze(0)) # target # src = src + self.positional_emb[:,:seq]\n",
        "\n",
        "\n",
        "        # src = self.pos_encoder(src)\n",
        "        src = src * self.pos_encoder(torch.arange(seq, device=device)).unsqueeze(0)\n",
        "        # src = src + self.positional_emb\n",
        "        if context_indices != None:\n",
        "            src = apply_masks(src, [context_indices])\n",
        "\n",
        "\n",
        "        out = self.transformer_encoder(src) # float [seq_len, batch_size, d_model]\n",
        "        out = self.norm(out)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "batch, seq_len, d_model = 4,1024,16\n",
        "in_dim = 3\n",
        "model = TransformerModel(in_dim, d_model, d_head=4, nlayers=2, dropout=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "# # print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qUhlOV8_RsRi"
      },
      "outputs": [],
      "source": [
        "# @title attention save\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, seq_len=512, base=10000):\n",
        "        super().__init__()\n",
        "        self.dim, self.base = dim, base\n",
        "        theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "        pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "        angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "        self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.base)\n",
        "        return x * self.rot_emb[:seq_len]\n",
        "\n",
        "def zero_module(module):\n",
        "    \"\"\"Zero out the parameters of a module and return it.\"\"\"\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads=None, d_head=8, cond_dim=None, dropout=0.): # .1\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_head = d_head\n",
        "        self.n_heads = d_model // d_head\n",
        "        # self.d_head = d_model // n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or d_model, 2*d_model, bias=False)\n",
        "        # self.k = nn.Sequential(nn.Dropout(dropout), nn.Linear(cond_dim, d_model, bias=False))\n",
        "        # self.lin = nn.Linear(d_model, d_model)\n",
        "        self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        # self.lin = nn.Sequential(nn.Dropout(dropout), zero_module(nn.Linear(d_model, d_model)))\n",
        "        self.drop = nn.Dropout(dropout) # indp before q,k,v; after linout\n",
        "        self.scale = self.d_head ** -.5\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [batch, T, d_model]=[batch, h*w, c], [batch, num_tok, cond_dim], [batch,T]\n",
        "        batch = x.shape[0]\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        Q = self.q(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        # K = self.k(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        K, V = self.kv(cond).view(batch, -1, self.n_heads, 2*self.d_head).transpose(1, 2).chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # linear attention # Softmax(Q) @ (Softmax(K).T @ V)\n",
        "        if mask != None:\n",
        "            mask = mask[:, None, :, None] # [batch,T] -> [batch,1,T,1]\n",
        "            K, V = K.masked_fill(mask, -torch.finfo(x.dtype).max), V.masked_fill(mask, -torch.finfo(x.dtype).max)\n",
        "        Q, K = Q.softmax(dim=-1)*self.scale, K.softmax(dim=-2)\n",
        "        context = K.transpose(-2,-1) @ V # [batch, n_heads, d_head, d_head]\n",
        "        out = Q @ context # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # (quadratic) attention # Softmax(Q @ K.T) @ V\n",
        "        # attn = Q @ K.transpose(-2,-1) * self.scale # [batch, n_heads, T] # [batch, n_heads, T, T/num_tok]\n",
        "        # if mask != None: attn = attn.masked_fill(mask[:, None, :, None], -torch.finfo(attn.dtype).max) # [batch,T]->[batch,1,T,1]\n",
        "        # attention = torch.softmax(attn, dim=-1)\n",
        "        # out = self.drop(attention) @ V # [batch, n_heads, T, d_head]\n",
        "\n",
        "        out = out.transpose(1, 2).flatten(2)\n",
        "        return self.drop(self.lin(out)) # [batch, T, d_model]\n",
        "\n",
        "# if self, dont pass cond_dim in init, dont pass cond in fwd\n",
        "# Softmax(Q @ K.T) @ V ~ Softmax(Q) @ Softmax(K).T @ V\n",
        "\n",
        "# https://github.com/lucidrains/x-transformers/blob/main/x_transformers/x_transformers.py#L1855\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_head=4, cond_dim=None, ff_mult=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm1 = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.norm2 = nn.RMSNorm(d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.cond_dim = cond_dim\n",
        "        self.self = MultiHeadAttention(d_model, d_head=d_head, dropout=0)\n",
        "        if self.cond_dim!=None: self.cross = MultiHeadAttention(d_model, d_head=d_head, cond_dim=cond_dim, dropout=0)\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        ff_dim=d_model*ff_mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), nn.Dropout(dropout), nn.Linear(d_model, ff_dim), act, # ReLU GELU\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "            nn.RMSNorm(d_model), nn.Dropout(dropout), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            # nn.RMSNorm(d_model), act, nn.Dropout(dropout), nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,c,h,w], [batch, num_tok, cond_dim], [batch,T]\n",
        "        # bchw = x.shape\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "\n",
        "        if self.cond_dim==None: cond=None # is self attn\n",
        "        x = x + self.drop(self.self(self.norm1(x)))\n",
        "        if self.cond_dim!=None: x = x + self.cross(self.norm2(x), cond, mask)\n",
        "        x = x + self.ff(x)\n",
        "        # x = x + self.drop(self.ff(x))\n",
        "\n",
        "        # return x.transpose(1,2).reshape(*bchw)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODpKypTCsfIt",
        "outputId": "1e4dc1cf-22f9-4400-cb37-078985cce64d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ True,  True,  True,  ..., False, False, False],\n",
            "        [ True,  True,  True,  ..., False, False, False],\n",
            "        [ True,  True,  True,  ..., False, False, False],\n",
            "        [ True,  True,  True,  ..., False, False, False]])\n",
            "torch.Size([4, 16])\n"
          ]
        }
      ],
      "source": [
        "# @title TransformerVICReg save\n",
        "import math\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        for layer in self:\n",
        "            params = inspect.signature(layer.forward).parameters.keys()\n",
        "            layer._fwdparams = ','.join(params)\n",
        "\n",
        "    def forward(self, x, mask=None, cond=None):\n",
        "        for layer in self:\n",
        "            args = [x]\n",
        "            if 'mask' in layer._fwdparams: args.append(mask)\n",
        "            if 'cond' in layer._fwdparams: args.append(cond)\n",
        "            x = layer(*args)\n",
        "        return x\n",
        "\n",
        "class TransformerVICReg(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, drop=0):\n",
        "        super().__init__()\n",
        "        act = nn.GELU()\n",
        "        # self.embed = nn.Sequential(nn.Linear(in_dim, d_model), act)\n",
        "        self.embed = nn.Sequential(\n",
        "            nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2),\n",
        "            )\n",
        "        self.pos_encoder = RoPE(d_model, seq_len=200, base=10000)\n",
        "        # d_hid = d_hid or d_model#*2\n",
        "        # self.encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "        self.encoder = Seq(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "\n",
        "        out_dim = out_dim or d_model\n",
        "        self.lin = nn.Linear(d_model, out_dim)\n",
        "        self.attention_pool = nn.Linear(d_model, 1, bias=False)\n",
        "\n",
        "        dim_v = d_model * 4\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(out_dim, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v, bias=False),\n",
        "            )\n",
        "\n",
        "    def forward(self, x, src_key_padding_mask=None): # [batch, seq, d_model], [batch, seq] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # x = self.embed(x)\n",
        "        x = self.embed(x.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "        # batch, seq_len, d_model = x.shape\n",
        "        # x = torch.cat([self.cls.repeat(batch,1,1), x], dim=1)\n",
        "        # src_key_padding_mask = torch.cat([torch.zeros((batch, 1), dtype=torch.bool), src_key_padding_mask], dim=1)\n",
        "        x = self.pos_encoder(x)\n",
        "\n",
        "        x = self.encoder(x, mask=src_key_padding_mask)\n",
        "        # print(\"fwd\",out.shape) # float [batch, seq_len, d_model]\n",
        "\n",
        "        attn = self.attention_pool(x).squeeze(-1) # [batch, seq] # seq_pool\n",
        "        out = (torch.softmax(attn, dim=-1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, seq] @ [batch, seq, dim] -> [batch, dim]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "    def expand(self, x, src_key_padding_mask=None):\n",
        "        sx = self.forward(x, src_key_padding_mask=src_key_padding_mask)\n",
        "        vx = self.exp(sx)\n",
        "        return vx\n",
        "\n",
        "batch, seq_len, d_model = 4,3500,512\n",
        "in_dim, out_dim=3,16\n",
        "model = TransformerVICReg(in_dim, d_model, out_dim, d_head=4, nlayers=2, drop=0.).to(device)\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "src_key_padding_mask = torch.stack([(torch.arange(seq_len) < seq_len - v) for v in torch.randint(seq_len, (batch,))]) # True will be ignored # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "print(src_key_padding_mask)\n",
        "out = model(x, src_key_padding_mask)\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qwg9dG4sQ3h",
        "outputId": "11e258f7-70a0-4922-89f5-9c3862a34c44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "55330\n",
            "in vicreg  3.6479194409180554e-16 24.74782019853592 7.794930811932943e-10\n",
            "(tensor(1.4592e-17, device='cuda:0', grad_fn=<MseLossBackward0>), tensor(0.9899, device='cuda:0', grad_fn=<AddBackward0>), tensor(7.7949e-10, device='cuda:0', grad_fn=<AddBackward0>))\n"
          ]
        }
      ],
      "source": [
        "# @title Violet save\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class Violet(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, nlayers=2, d_head=4):\n",
        "        super().__init__()\n",
        "        out_dim = out_dim or d_model\n",
        "\n",
        "        # self.rnn = RNN(d_model, d_model, d_model, nlayers)\n",
        "        self.student = TransformerVICReg(in_dim, d_model, out_dim=out_dim, d_head=d_head, nlayers=nlayers, drop=0.)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "\n",
        "        # vicreg\n",
        "        self.sim_coeff=25. # 10.0 # 25.0 # λ\n",
        "        self.std_coeff=25. # 1.0 # 25.0 # µ\n",
        "        self.cov_coeff=1. # 25.0 # 1.0 # ν\n",
        "\n",
        "        self.classifier = nn.Linear(out_dim, 18) # 10 18\n",
        "\n",
        "    def loss(self, x, src_key_padding_mask=None): # [batch, T, 3]\n",
        "        batch, seq, dim = x.shape\n",
        "        # print(x.shape)\n",
        "        vx = self.student.expand(x, src_key_padding_mask=src_key_padding_mask) # [batch, num_context_toks, out_dim]\n",
        "        with torch.no_grad(): vy = self.teacher.expand(x.detach(), src_key_padding_mask=src_key_padding_mask) # [batch, num_trg_toks, out_dim]\n",
        "        loss = self.vicreg(vx, vy)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x, src_key_padding_mask=None): # [batch, T, 3]\n",
        "        sx = self.student(x, src_key_padding_mask=src_key_padding_mask)\n",
        "        return sx\n",
        "\n",
        "    def classify(self, x): # [batch, T, 3]\n",
        "        sx = self.forward(x)\n",
        "        out = self.classifier(sx)\n",
        "        return out\n",
        "\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size, num_features = x.shape\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = self.sim_coeff * repr_loss + self.std_coeff * std_loss + self.cov_coeff * cov_loss\n",
        "        print(\"in vicreg \",self.sim_coeff * repr_loss.item() , self.std_coeff * std_loss.item() , self.cov_coeff * cov_loss.item())\n",
        "        # return loss\n",
        "        return repr_loss, std_loss, cov_loss\n",
        "\n",
        "\n",
        "violet = Violet(in_dim=3, d_model=32, out_dim=16, nlayers=2, d_head=4).to(device)\n",
        "# voptim = torch.optim.AdamW(violet.parameters(), lr=1e-3) # 1e-3?\n",
        "voptim = torch.optim.AdamW([{'params': violet.student.encoder.parameters()},\n",
        "    {'params': violet.student.exp.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # default 1e-2\n",
        "print(sum(p.numel() for p in violet.parameters() if p.requires_grad)) # 27584\n",
        "\n",
        "x = torch.rand((2,1000,3), device=device)\n",
        "# x = torch.rand((2,1,16), device=device)\n",
        "loss = violet.loss(x)\n",
        "# print(out.shape)\n",
        "print(loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rV7WLiNT4EAV"
      },
      "outputs": [],
      "source": [
        "# @title test WISDM\n",
        "\n",
        "# print(activity_dataframe)\n",
        "# print(activity_dataframe['activity'])\n",
        "# activity_dataframe.to_csv('data.csv',index=False)\n",
        "\n",
        "# data = pd.read_csv(\"/data.csv\", index_col =\"Name\")\n",
        "# data = pd.read_csv(\"data.csv\")\n",
        "\n",
        "# a = list(activity_dataframe['timestamp'])\n",
        "# print(a)\n",
        "# print([x-y for x,y in zip(a[1:],a[:-1])])\n",
        "\n",
        "# print(data.loc[0])\n",
        "# p = data.loc[0]\n",
        "# print(len(p))\n",
        "# ID, activity, timestamp, x, y, z, meter, device = data.loc[0]\n",
        "# print(activity)\n",
        "\n",
        "# print(data.loc[1639])\n",
        "\n",
        "# userids = data['ID'].unique()#for id in userids\n",
        "# print(data['activity'].unique())\n",
        "# df_keep = data[['ID','activity','timestamp','x','y','z']]\n",
        "\n",
        "\n",
        "# grouped = df_keep.groupby(['ID'])\n",
        "# grouped = df_keep.groupby(['ID','activity'])\n",
        "# print(len(grouped))\n",
        "# user_acts = dict(tuple(df_keep.groupby(['ID','activity'])))\n",
        "# print(user_acts)\n",
        "# print(len(user_acts))\n",
        "# temp_df = df[df['ID'] == id]\n",
        "\n",
        "# print(len(df_keep))\n",
        "# print(len(data))\n",
        "\n",
        "\n",
        "\n",
        "# act = [[a, d[['timestamp','x','y','z']].to_numpy()] for a, d in user_acts.items()]\n",
        "# act = [[(int(a[0]), ), d[['timestamp','x','y','z']].to_numpy()] for a, d in user_acts.items()]\n",
        "# print(act)\n",
        "# print(act[0])\n",
        "# # print(act[0][1])\n",
        "# print(len(act[1]))\n",
        "# print([len(a[1]) for a in act])\n",
        "# print(min([len(a[1]) for a in act])) # 3567\n",
        "\n",
        "\n",
        "# act_dict = {i: act for i, act in enumerate(data['activity'].unique())}\n",
        "# act_invdict = {v: k for k, v in act_dict.items()}\n",
        "# print(act_invdict)\n",
        "\n",
        "# torch.tensor(act[0][1][:3500])\n",
        "\n",
        "# id_act, x = act[0]\n",
        "# id_act = self.process(id_act)\n",
        "# return id_act,\n",
        "# torch.tensor(x[:3500]) # 3567\n",
        "\n",
        "\n",
        "# /content/processed/wisdm-dataset/raw/phone/accel/data.csv\n",
        "# /content/processed/wisdm-dataset/raw/watch/gyro/data.csv\n",
        "\n",
        "# data0 = pd.read_csv(\"/content/processed/wisdm-dataset/raw/watch/accel/data.csv\")\n",
        "# data1 = pd.read_csv(\"/content/processed/wisdm-dataset/raw/watch/gyro/data.csv\")\n",
        "\n",
        "# user_acts0 = dict(tuple(data0.groupby(['ID','activity'])))\n",
        "# user_acts1 = dict(tuple(data1.groupby(['ID','activity'])))\n",
        "\n",
        "for (a0,d0), (a1,d1) in zip(user_acts0.items(), user_acts1.items()):\n",
        "    print(a0,a1)\n",
        "    print(len(d0),len(d1))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "N-4I7A5hJU7b"
      },
      "outputs": [],
      "source": [
        "# @title har_cnn\n",
        "# https://arxiv.org/pdf/2209.08335v1\n",
        "# https://github.com/Lou1sM/HAR/blob/master/har_cnn.py\n",
        "import sys\n",
        "import json\n",
        "from hmmlearn import hmm\n",
        "from copy import deepcopy\n",
        "import os\n",
        "import math\n",
        "from pdb import set_trace\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import time\n",
        "from torch.utils import data\n",
        "import cl_args\n",
        "from dl_utils.misc import asMinutes,check_dir\n",
        "#from dl_utils.label_funcs import accuracy, mean_f1, debable, translate_labellings, get_num_labels, label_counts, dummy_labels, avoid_minus_ones_lf_wrapper,masked_mode,acc_by_label, get_trans_dict\n",
        "from label_funcs_tmp import accuracy, mean_f1, translate_labellings, get_num_labels, label_counts, dummy_labels, avoid_minus_ones_lf_wrapper,masked_mode,acc_by_label, get_trans_dict\n",
        "from dl_utils.tensor_funcs import noiseify, numpyify, cudify\n",
        "from make_dsets import make_single_dset, make_dsets_by_user\n",
        "from sklearn.metrics import normalized_mutual_info_score,adjusted_rand_score\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from project_config import get_dataset_info_object\n",
        "\n",
        "rari = lambda x,y: round(adjusted_rand_score(x,y),4)\n",
        "rnmi = lambda x,y: round(normalized_mutual_info_score(x,y),4)\n",
        "\n",
        "class EncByLayer(nn.Module):\n",
        "    def __init__(self,x_filters,y_filters,x_strides,y_strides,max_pools,show_shapes):\n",
        "        super().__init__()\n",
        "        self.show_shapes = show_shapes\n",
        "        num_layers = len(x_filters)\n",
        "        assert all(len(x)==num_layers for x in (y_filters,x_strides,y_strides,max_pools))\n",
        "        ncvs = [1]+[4*2**i for i in range(num_layers)]\n",
        "        # conv_layers = []\n",
        "        # for i in range(num_layers):\n",
        "        #     conv_layer = nn.Sequential(\n",
        "        #         nn.Conv2d(ncvs[i],ncvs[i+1],(x_filters[i],y_filters[i]),(x_strides[i],y_strides[i])), nn.BatchNorm2d(ncvs[i+1]) if i<num_layers-1 else nn.Identity(), nn.LeakyReLU(0.3), nn.MaxPool2d(max_pools[i])\n",
        "        #     )\n",
        "        #     conv_layers.append(conv_layer)\n",
        "        # self.conv_layers = nn.ModuleList(conv_layers)\n",
        "\n",
        "        self.conv_layers = nn.Sequential(*[\n",
        "                nn.Conv2d(ncvs[i],ncvs[i+1],(x_filters[i],y_filters[i]),(x_strides[i],y_strides[i])), nn.BatchNorm2d(ncvs[i+1]) if i<num_layers-1 else nn.Identity(), nn.LeakyReLU(0.3), nn.MaxPool2d(max_pools[i])\n",
        "             for i in range(nlayers)])\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        # if self.show_shapes: print(x.shape)\n",
        "        # for conv_layer in self.conv_layers:\n",
        "        x = conv_layer(x)\n",
        "            # if self.show_shapes: print(x.shape)\n",
        "        return x\n",
        "\n",
        "class DecByLayer(nn.Module):\n",
        "    def __init__(self,x_filters,y_filters,x_strides,y_strides,show_shapes):\n",
        "        super().__init__()\n",
        "        self.show_shapes = show_shapes\n",
        "        num_layers = len(x_filters)\n",
        "        assert all(len(x)==num_layers for x in (y_filters,x_strides,y_strides))\n",
        "        ncvs = [4*2**i for i in reversed(range(num_layers))]+[1]\n",
        "        conv_trans_layers = [nn.Sequential(\n",
        "                nn.ConvTranspose2d(ncvs[i],ncvs[i+1],(x_filters[i],y_filters[i]),(x_strides[i],y_strides[i])), nn.BatchNorm2d(ncvs[i+1]), nn.LeakyReLU(0.3),\n",
        "                )\n",
        "            for i in range(num_layers)]\n",
        "        self.conv_trans_layers = nn.ModuleList(conv_trans_layers)\n",
        "\n",
        "    def forward(self,x):\n",
        "        if self.show_shapes: print(x.shape)\n",
        "        for conv_trans_layer in self.conv_trans_layers:\n",
        "            x = conv_trans_layer(x)\n",
        "            if self.show_shapes: print(x.shape)\n",
        "        return x\n",
        "\n",
        "class Var_BS_MLP(nn.Module):\n",
        "    def __init__(self,input_size,hidden_size,output_size):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(nn.Linear(input_size,hidden_size), nn.BatchNorm1d(hidden_size), nn.LeakyReLU(0.3), nn.Linear(hidden_size,output_size))\n",
        "\n",
        "    def forward(self,x):\n",
        "        return self.mlp(x)\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n",
        "    dset_info_object = get_dataset_info_object(args.dset)\n",
        "    num_classes = args.num_classes if args.num_classes != -1 else dset_info_object.num_classes\n",
        "    if args.dset == 'UCI_feat':\n",
        "        enc = nn.Sequential(nn.Linear(561,500),nn.ReLU(),\n",
        "                            nn.Linear(500,500),nn.ReLU(),\n",
        "                            nn.Linear(500,2000),nn.ReLU(),\n",
        "                            nn.Linear(2000,6),nn.ReLU()).cuda()\n",
        "        dec = nn.Sequential(nn.Linear(6,2000),nn.ReLU(),\n",
        "                            nn.Linear(2000,500),nn.ReLU(),\n",
        "                            nn.Linear(500,500),nn.ReLU(),\n",
        "                            nn.Linear(500,561),nn.ReLU()).cuda()\n",
        "        mlp = Var_BS_MLP(6,256,num_classes).cuda()\n",
        "    else:\n",
        "        if args.window_size == 512:\n",
        "            x_filters = (50,40,7,4)\n",
        "            x_strides = (2,2,1,1)\n",
        "            max_pools = ((2,1),(2,1),(2,1),(2,1))\n",
        "        elif args.window_size == 100:\n",
        "            x_filters = (20,20,5,3)\n",
        "            x_strides = (1,1,1,1)\n",
        "            max_pools = ((2,1),(2,1),(2,1),1)\n",
        "        y_filters = (1,1,1,dset_info_object.num_channels)\n",
        "        y_strides = (1,1,1,1)\n",
        "        enc = EncByLayer(x_filters,y_filters,x_strides,y_strides,max_pools,show_shapes=args.show_shapes).cuda()\n",
        "        #if args.is_n2d:\n",
        "        x_filters_trans = (15,10,15,11)\n",
        "        x_strides_trans = (2,3,3,3)\n",
        "        y_filters_trans = (dset_info_object.num_channels,1,1,1)\n",
        "        dec = DecByLayer(x_filters_trans,y_filters_trans,x_strides_trans,y_strides,show_shapes=args.show_shapes).cuda()\n",
        "\n",
        "        optional_umap_like_net_in = Var_BS_MLP(32,256,2).cuda()\n",
        "        optional_umap_like_net_out = Var_BS_MLP(2,256,2).cuda()\n",
        "        if ARGS.is_uln:\n",
        "            enc = nn.Sequential(enc,nn.Flatten(1),Var_BS_MLP(32,256,2).cuda())\n",
        "            dec = nn.Sequential(Var_BS_MLP(32,256,2).cuda(),nn.Unflatten(2,(32,1,1)),dec)\n",
        "        mlp = Var_BS_MLP(2 if ARGS.is_uln else 32,256,num_classes).cuda()\n",
        "    if args.load_pretrained:\n",
        "        enc.load_state_dict(torch.load('enc_pretrained.pt'))\n",
        "    subj_ids = args.subj_ids\n",
        "\n",
        "    metric_dict = {'acc':accuracy,'nmi':rnmi,'ari':rari,'f1':mean_f1}\n",
        "    har = HARLearner(enc=enc,mlp=mlp,dec=dec,num_classes=num_classes,args=args,metric_dict=metric_dict)\n",
        "\n",
        "    start_time = time.time()\n",
        "    already_exists = check_dir(f\"experiments/{args.exp_name}/preds\")\n",
        "    check_dir(f\"experiments/{args.exp_name}/best_preds\")\n",
        "    if args.show_shapes:\n",
        "        dset_train, selected_acts = make_single_dset(args,subj_ids)\n",
        "        num_ftrs = dset_train.x.shape[-1]\n",
        "        print(num_ftrs)\n",
        "        lat = enc(torch.ones((2,1,args.window_size,num_ftrs),device='cuda'))\n",
        "        dec(lat)\n",
        "        sys.exit()\n",
        "    dsets_by_id = make_dsets_by_user(args,subj_ids)\n",
        "    if args.is_n2d:\n",
        "        for subj_id, (dset,sa) in dsets_by_id.items():\n",
        "            print(\"n2ding\", subj_id)\n",
        "            har.n2d_abl(subj_id,dset)\n",
        "    elif not args.subject_independent:\n",
        "        bad_ids = []\n",
        "        for user_id, (dset,sa) in dsets_by_id.items():\n",
        "            n = get_num_labels(dset.y)\n",
        "            if n < dset_info_object.num_classes/2:\n",
        "                print(f\"Excluding user {user_id}, only has {n} different labels, out of {num_classes}\")\n",
        "                bad_ids.append(user_id)\n",
        "        if not args.bad_ids: dsets_by_id = {k:v for k,v in dsets_by_id.items() if k not in bad_ids}\n",
        "        print('reloading clusterings for', [x for x in subj_ids[:args.reload_ids] if x not in bad_ids])\n",
        "        for rid in subj_ids[:args.reload_ids]:\n",
        "            if rid in bad_ids: continue\n",
        "            print('reloading clusterings for', rid)\n",
        "            rdset,sa = dsets_by_id.pop(rid)\n",
        "            best_preds = np.load(f'experiments/{args.exp_name}/best_preds/{rid}.npy')\n",
        "            preds = np.load(f'experiments/{args.exp_name}/preds/{rid}.npy')\n",
        "            har.log_preds_and_scores(rid,preds,best_preds,numpyify(rdset.y))\n",
        "        print('clustering remaining ids', [x for x in subj_ids[args.reload_ids:]], 'from scratch\\n')\n",
        "\n",
        "        print(\"CLUSTERING EACH DSET SEPARATELY\")\n",
        "        for subj_id, (dset,sa) in dsets_by_id.items():\n",
        "            print(\"clustering\", subj_id)\n",
        "            har.pseudo_label_cluster_meta_meta_loop(subj_id,dset)\n",
        "    elif args.subject_independent:\n",
        "        print(\"CLUSTERING AS SINGLE DSET\")\n",
        "        one_big_dset, selected_acts = make_single_dset(args,subj_ids)\n",
        "        har.pseudo_label_cluster_meta_meta_loop('all',one_big_dset)\n",
        "\n",
        "    results_file_path = f'experiments/{args.exp_name}/results.txt'\n",
        "    har.total_time = time.time() - start_time\n",
        "    har.log_final_scores(results_file_path)\n",
        "    har.express_times(results_file_path)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    ARGS, need_umap = cl_args.get_cl_args()\n",
        "    if need_umap: import umap\n",
        "    main(ARGS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "38tWCkK-0Zlu",
        "outputId": "7abe4629-3dcf-4981-810e-9e888be03cb9"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "leading zeros in decimal integer literals are not permitted; use an 0o prefix for octal integers (<ipython-input-1-94e7ebe34632>, line 10)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-94e7ebe34632>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    wget http://archive.ics.uci.edu/ml/machine-learning-databases/00231/PAMAP2_Dataset.zip\u001b[0m\n\u001b[0m                                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m leading zeros in decimal integer literals are not permitted; use an 0o prefix for octal integers\n"
          ]
        }
      ],
      "source": [
        "# @title Lou1sM download_datasets.sh\n",
        "# https://github.com/Lou1sM/HAR/blob/master/download_datasets.sh\n",
        "\n",
        "##!/bin/sh\n",
        "\n",
        "mkdir -p datasets\n",
        "cd datasets\n",
        "\n",
        "#PAMAP\n",
        "# echo -e \"\\n###############\"\n",
        "# echo GETTING AND PREPARING PAMAP DATA\n",
        "# echo -e \"###############\\n\"\n",
        "wget http://archive.ics.uci.edu/ml/machine-learning-databases/00231/PAMAP2_Dataset.zip\n",
        "unzip PAMAP2_Dataset.zip\n",
        "# python ../convert_data_to_np.py PAMAP\n",
        "\n",
        "#UCI\n",
        "# echo -e \"\\n###############\"\n",
        "# echo GETTING AND PREPARING UCI DATA\n",
        "# echo -e \"###############\\n\"\n",
        "mkdir -p UCI2\n",
        "cd UCI2\n",
        "wget http://archive.ics.uci.edu/ml/machine-learning-databases/00341/HAPT%20Data%20Set.zip\n",
        "unzip HAPT\\ Data\\ Set.zip\n",
        "# cd ..\n",
        "# python ../convert_data_to_np.py UCI-raw\n",
        "\n",
        "#mkdir -p capture24\n",
        "#cd capture24/\n",
        "\n",
        "#for i in $(seq -w 151)\n",
        "#do\n",
        "#    curl -JLO \"https://ora.ox.ac.uk/objects/uuid:92650814-a209-4607-9fb5-921eab761c11/download_file?safe_filename=P${i}.csv.gz&type_of_work=Dataset\"\n",
        "#done\n",
        "#\n",
        "#curl -JLO \"https://ora.ox.ac.uk/objects/uuid:92650814-a209-4607-9fb5-921eab761c11/download_file?safe_filename=metadata.csv&type_of_work=Dataset\"\n",
        "#curl -JLO \"https://ora.ox.ac.uk/objects/uuid:92650814-a209-4607-9fb5-921eab761c11/download_file?safe_filename=annotation-label-dictionary.csv&type_of_work=Dataset\"\n",
        "#\n",
        "#\n",
        "#for f in $(ls); do\n",
        "#    if [ ${f: -2} == \"gz\" ]; then\n",
        "#        gunzip $f;\n",
        "#    fi;\n",
        "#done\n",
        "\n",
        "#WISDM-v1\n",
        "# echo -e \"\\n###############\"\n",
        "# echo GETTING AND PREPARING WISDM-v1 DATA\n",
        "# echo -e \"###############\\n\"\n",
        "wget https://www.cis.fordham.edu/wisdm/includes/datasets/latest/WISDM_ar_latest.tar.gz\n",
        "gunzip WISDM_ar_latest.tar.gz\n",
        "tar -xf WISDM_ar_latest.tar\n",
        "\n",
        "# python ../convert_data_to_np.py WISDM-v1\n",
        "\n",
        "#WISDM-watch\n",
        "# echo -e \"\\n###############\"\n",
        "# echo GETTING AND PREPARING WISDM-watch DATA\n",
        "# echo -e \"###############\\n\"\n",
        "wget https://archive.ics.uci.edu/ml/machine-learning-databases/00507/wisdm-dataset.zip\n",
        "unzip wisdm-dataset.zip\n",
        "python ../convert_data_to_np.py WISDM-watch\n",
        "\n",
        "#REALDISP\n",
        "# echo -e \"\\n###############\"\n",
        "# echo GETTING REALDISP DATA\n",
        "# echo -e \"###############\\n\"\n",
        "mkdir -p realdisp\n",
        "cd realdisp\n",
        "mkdir -p RawData\n",
        "wget https://archive.ics.uci.edu/ml/machine-learning-databases/00305/realistic_sensor_displacement.zip\n",
        "unzip realistic_sensor_displacement.zip\n",
        "# cd ..\n",
        "# python ../convert_data_to_np.py REALDISP\n",
        "# cd ..\n",
        "# pwd\n",
        "#HHAR\n",
        "mkdir -p hhar\n",
        "wget http://archive.ics.uci.edu/ml/machine-learning-databases/00344/Activity%20recognition%20exp.zip\n",
        "unzip Activity\\ recognition\\ exp.zip\n",
        "# python ../convert_data_to_np.py HHAR\n",
        "# cd ..\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMvb1-PBQ9u-",
        "outputId": "764e8452-935e-4bcc-aa38-82faddee7da3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PAMAP2_Dataset/Protocol subject106.dat\n",
            "/content/PAMAP2/subject106\n",
            "PAMAP2_Dataset/Protocol subject101.dat\n",
            "/content/PAMAP2/subject101\n",
            "PAMAP2_Dataset/Protocol subject107.dat\n",
            "/content/PAMAP2/subject107\n",
            "PAMAP2_Dataset/Protocol subject109.dat\n",
            "/content/PAMAP2/subject109\n",
            "PAMAP2_Dataset/Protocol subject103.dat\n",
            "/content/PAMAP2/subject103\n",
            "PAMAP2_Dataset/Protocol subject104.dat\n",
            "/content/PAMAP2/subject104\n",
            "PAMAP2_Dataset/Protocol subject108.dat\n",
            "/content/PAMAP2/subject108\n",
            "PAMAP2_Dataset/Protocol subject105.dat\n",
            "/content/PAMAP2/subject105\n",
            "PAMAP2_Dataset/Protocol subject102.dat\n",
            "/content/PAMAP2/subject102\n"
          ]
        }
      ],
      "source": [
        "# @title PAMAP2\n",
        "# https://arxiv.org/pdf/2209.08335v1\n",
        "# https://archive.ics.uci.edu/dataset/231/pamap2+physical+activity+monitoring\n",
        "# https://archive.ics.uci.edu/static/public/231/pamap2+physical+activity+monitoring.zip\n",
        "# !wget http://archive.ics.uci.edu/ml/machine-learning-databases/00231/PAMAP2_Dataset.zip\n",
        "# !unzip PAMAP2_Dataset.zip\n",
        "\n",
        "import os\n",
        "data_dir = 'PAMAP2_Dataset/Protocol'\n",
        "np_dir = '/content/PAMAP2'\n",
        "if not os.path.isdir(np_dir):\n",
        "    os.makedirs(np_dir)\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def array_from_txt(inpath):\n",
        "    with open(inpath) as f:\n",
        "        d = f.readlines()\n",
        "        array = np.array([[float(x) for x in line.split()] for line in d])\n",
        "    return array\n",
        "\n",
        "def array_expanded(a,expanded_length):\n",
        "    if a.shape[0] >= expanded_length: return a\n",
        "    insert_every = mpf(a.shape[0]/(expanded_length-a.shape[0]))\n",
        "    additional_idxs = (np.arange(expanded_length-a.shape[0])*insert_every).astype(np.int)\n",
        "    values = a[additional_idxs]\n",
        "    expanded = np.insert(a,additional_idxs,values,axis=0)\n",
        "    assert expanded.shape[0] == expanded_length\n",
        "    return expanded\n",
        "\n",
        "def convert(inpath,outpath):\n",
        "    array = array_from_txt(inpath)\n",
        "    timestamps = array[:,0]\n",
        "    labels = array[:,1].astype(int)\n",
        "    # Delete orientation data, which webpage says is 'invalid in this data, and timestamp and label\n",
        "    array = np.delete(array,[0,1,2,13,14,15,16,30,31,32,33,47,48,49,50],1)\n",
        "    np.save(outpath+'_timestamps',timestamps)\n",
        "    np.save(outpath+'_labels',labels)\n",
        "    np.save(outpath,array)\n",
        "\n",
        "for filename in os.listdir(data_dir):\n",
        "    print(data_dir,filename)\n",
        "    # inpath = os.path.join(data_dir,filename)a\n",
        "    inpath = data_dir+'/'+filename\n",
        "    outpath = np_dir+'/'+filename.split('.')[0]\n",
        "    print(outpath)\n",
        "    # outpath = os.path.join(np_dir,filename.split('.')[0])\n",
        "    convert(inpath,outpath)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "5TnsfKrMB0yF",
        "outputId": "07859402-b33c-4ede-c389-ef7a59c2f1ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "no precomputed datasets, computing from scratch\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "invalid index to scalar variable.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-ddcbc2f0291f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m             action_name_dict = pamap_action_name_dict)\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mdset_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_acts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_pamap_dset_train_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpamap_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-ddcbc2f0291f>\u001b[0m in \u001b[0;36mmake_pamap_dset_train_val\u001b[0;34m(subj_ids)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# 0 is a transient activity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# x_train,y_train,selected_acts = preproc_xys(x_train,y_train,args.step_size,args.window_size,dset_info_object,subj_ids)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mselected_acts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreproc_xys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdset_info_object\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msubj_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;31m# dset_train = StepDataset(x_train,y_train,window_size=args.window_size,step_size=args.step_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# dset_train = StepDataset(x_train,y_train,window_size=512,step_size=5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-a048671b2310>\u001b[0m in \u001b[0;36mpreproc_xys\u001b[0;34m(x, y, step_size, window_size, dset_info_object, subj_ids)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mnum_windows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mstep_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m#mode_labels = np.array([stats.mode(y[w*step_size:w*step_size + window_size]).mode[0] if (y[w*step_size:w*step_size + window_size]==y[w*step_size]).all() else -1 for w in range(num_windows)])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mmode_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstep_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_windows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mselected_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mselected_acts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdset_info_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_name_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mact_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mact_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselected_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-a048671b2310>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mnum_windows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mstep_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m#mode_labels = np.array([stats.mode(y[w*step_size:w*step_size + window_size]).mode[0] if (y[w*step_size:w*step_size + window_size]==y[w*step_size]).all() else -1 for w in range(num_windows)])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mmode_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstep_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_windows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mselected_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mselected_acts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdset_info_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_name_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mact_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mact_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselected_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
          ]
        }
      ],
      "source": [
        "\n",
        "# def make_pamap_dset_train_val(args,subj_ids):\n",
        "def make_pamap_dset_train_val(subj_ids):\n",
        "    # dset_info_object = PAMAP_INFO.PAMAP_INFO\n",
        "    dset_info_object = PAMAP_INFO\n",
        "    # x_train = np.concatenate([np.load(f'datasets/PAMAP2_Dataset/np_data/subject{s}.npy') for s in subj_ids])\n",
        "    # y_train = np.concatenate([np.load(f'datasets/PAMAP2_Dataset/np_data/subject{s}_labels.npy') for s in subj_ids])\n",
        "    x_train = np.concatenate([np.load(f'PAMAP2/subject{s}.npy') for s in subj_ids])\n",
        "    y_train = np.concatenate([np.load(f'PAMAP2/subject{s}_labels.npy') for s in subj_ids])\n",
        "    x_train = x_train[y_train!=0] # 0 is a transient activity\n",
        "    y_train = y_train[y_train!=0] # 0 is a transient activity\n",
        "    # x_train,y_train,selected_acts = preproc_xys(x_train,y_train,args.step_size,args.window_size,dset_info_object,subj_ids)\n",
        "    x_train,y_train,selected_acts = preproc_xys(x_train,y_train,1,1,dset_info_object,subj_ids)\n",
        "    # dset_train = StepDataset(x_train,y_train,window_size=args.window_size,step_size=args.step_size)\n",
        "    # dset_train = StepDataset(x_train,y_train,window_size=512,step_size=5)\n",
        "    dset_train = StepDataset(x_train,y_train,window_size=1,step_size=1)\n",
        "    return dset_train, selected_acts\n",
        "\n",
        "class HAR_Dataset_Container():\n",
        "    def __init__(self,code_name,dataset_dir_name,possible_subj_ids,num_channels,num_classes,action_name_dict):\n",
        "        self.code_name = code_name\n",
        "        self.dataset_dir_name = dataset_dir_name\n",
        "        self.possible_subj_ids = possible_subj_ids\n",
        "        self.num_channels = num_channels\n",
        "        self.num_classes = num_classes\n",
        "        self.action_name_dict = action_name_dict\n",
        "\n",
        "\n",
        "# PAMAP\n",
        "pamap_ids = [str(x) for x in range(101,110)]\n",
        "pamap_action_name_dict = {1:'lying',2:'sitting',3:'standing',4:'walking',5:'running',6:'cycling',7:'Nordic walking',9:'watching TV',10:'computer work',11:'car driving',12:'ascending stairs',13:'descending stairs',16:'vacuum cleaning',17:'ironing',18:'folding laundry',19:'house cleaning',20:'playing soccer',24:'rope jumping'}\n",
        "PAMAP_INFO = HAR_Dataset_Container(\n",
        "            code_name = 'PAMAP',\n",
        "            dataset_dir_name = 'PAMAP2_Dataset',\n",
        "            possible_subj_ids = pamap_ids,\n",
        "            num_channels = 39,\n",
        "            num_classes = 12,\n",
        "            action_name_dict = pamap_action_name_dict)\n",
        "\n",
        "dset_train, selected_acts = make_pamap_dset_train_val(pamap_ids)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGU_rUapvjcE",
        "outputId": "b150f60b-4d01-47b5-b1f4-ea6b20a17027"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Incorrect or no dataset specified\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# @title Lou1sM convert_data_to_np.py\n",
        "# https://github.com/Lou1sM/HAR/blob/master/convert_data_to_np.py\n",
        "from pdb import set_trace\n",
        "from collections import Counter\n",
        "from scipy.fft import fft\n",
        "from scipy import stats\n",
        "from mpmath import mp, mpf\n",
        "#from dl_utils import misc, label_funcs\n",
        "# from dl_utils import misc\n",
        "# import label_funcs_tmp\n",
        "import os\n",
        "from os.path import join\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "def array_from_txt(inpath):\n",
        "    with open(inpath) as f:\n",
        "        d = f.readlines()\n",
        "        array = np.array([[float(x) for x in line.split()] for line in d])\n",
        "    return array\n",
        "\n",
        "def array_expanded(a,expanded_length):\n",
        "    if a.shape[0] >= expanded_length: return a\n",
        "    insert_every = mpf(a.shape[0]/(expanded_length-a.shape[0]))\n",
        "    additional_idxs = (np.arange(expanded_length-a.shape[0])*insert_every).astype(np.int)\n",
        "    values = a[additional_idxs]\n",
        "    expanded = np.insert(a,additional_idxs,values,axis=0)\n",
        "    assert expanded.shape[0] == expanded_length\n",
        "    return expanded\n",
        "\n",
        "def convert(inpath,outpath):\n",
        "    array = array_from_txt(inpath)\n",
        "    timestamps = array[:,0]\n",
        "    labels = array[:,1].astype(np.int)\n",
        "    # Delete orientation data, which webpage says is 'invalid in this data, and timestamp and label\n",
        "    array = np.delete(array,[0,1,2,13,14,15,16,30,31,32,33,47,48,49,50],1)\n",
        "    np.save(outpath+'_timestamps',timestamps)\n",
        "    np.save(outpath+'_labels',labels)\n",
        "    np.save(outpath,array)\n",
        "\n",
        "def expand_and_fill_labels(a,propoer_length):\n",
        "    start_filler = -np.ones(a[0,3])\n",
        "    end_filler = -np.ones(propoer_length-a[-1,4])\n",
        "    nested_lists = [[a[i,2] for _ in range(a[i,4]-a[i,3])] + [-1]*(a[i+1,3]-a[i,4]) for i in range(len(a)-1)] + [[a[-1,2] for _ in range(a[-1,4]-a[-1,3])]]\n",
        "    middle = np.array([item for sublist in nested_lists for item in sublist])\n",
        "    total_label_array = np.concatenate((start_filler,middle,end_filler)).astype(np.int)\n",
        "    return total_label_array\n",
        "\n",
        "def add_dtft(signal):\n",
        "    fft_signal_complex = fft(signal,axis=-1)\n",
        "    fft_signal_modulusses = np.abs(fft_signal_complex)\n",
        "    return np.concatenate((signal,fft_signal_modulusses),axis=-1)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "# if sys.argv[1] == 'PAMAP':\n",
        "data_dir = 'PAMAP2_Dataset/Protocol'\n",
        "np_dir = 'PAMAP2_Dataset/np_data'\n",
        "print(\"\\n#####Preprocessing PAMAP2#####\\n\")\n",
        "if not os.path.isdir(np_dir):\n",
        "    os.makedirs(np_dir)\n",
        "\n",
        "for filename in os.listdir(data_dir):\n",
        "    print(filename)\n",
        "    inpath = join(data_dir,filename)\n",
        "    outpath = join(np_dir,filename.split('.')[0])\n",
        "    convert(inpath,outpath)\n",
        "\n",
        "# elif sys.argv[1] == 'UCI-raw':\n",
        "data_dir = 'UCI2/RawData'\n",
        "np_dir = 'UCI2/np_data'\n",
        "print(\"\\n#####Preprocessing UCI#####\\n\")\n",
        "if not os.path.isdir(np_dir):\n",
        "    os.makedirs(np_dir)\n",
        "\n",
        "raw_label_array = array_from_txt(join(data_dir,'labels.txt')).astype(int)\n",
        "def two_digitify(x): return '0'+str(x) if len(str(x))==1 else str(x)\n",
        "fnames = os.listdir(data_dir)\n",
        "for idx in range(1,31):\n",
        "    print(\"processing user\",idx)\n",
        "    acc_array_list = []\n",
        "    gyro_array_list = []\n",
        "    label_array_list = []\n",
        "    user_idx = two_digitify(idx)\n",
        "    acc_fpaths = sorted([fn for fn in fnames if f'user{user_idx}' in fn and 'acc' in fn])\n",
        "    gyro_fpaths = sorted([fn for fn in fnames if f'user{user_idx}' in fn and 'gyro' in fn])\n",
        "    assert len(acc_fpaths) == len(gyro_fpaths)\n",
        "    for fna,fng in zip(acc_fpaths,gyro_fpaths):\n",
        "        acc_exp_id = int(fna.split('exp')[1][:2])\n",
        "        gyro_exp_id = int(fng.split('exp')[1][:2])\n",
        "        assert acc_exp_id==gyro_exp_id\n",
        "        new_acc_array = array_from_txt(join(data_dir,fna))\n",
        "        new_gyro_array = array_from_txt(join(data_dir,fna))\n",
        "        label_array_block = raw_label_array[raw_label_array[:,0]==acc_exp_id]\n",
        "        filled_label_array_block = expand_and_fill_labels(label_array_block,new_acc_array.shape[0])\n",
        "        assert filled_label_array_block.shape[0] == new_acc_array.shape[0]\n",
        "        assert filled_label_array_block.shape[0] == new_gyro_array.shape[0]\n",
        "        label_array_list.append(filled_label_array_block)\n",
        "        acc_array_list.append(new_acc_array)\n",
        "        gyro_array_list.append(new_gyro_array)\n",
        "    label_array = np.concatenate(label_array_list)\n",
        "    acc_array = np.concatenate(acc_array_list)\n",
        "    gyro_array = np.concatenate(gyro_array_list)\n",
        "    total_array = np.concatenate((acc_array,gyro_array),axis=1)\n",
        "    outpath = join(np_dir,f'user{user_idx}.npy')\n",
        "    np.save(outpath,total_array)\n",
        "    label_outpath = join(np_dir,f'user{user_idx}_labels.npy')\n",
        "    np.save(label_outpath,label_array)\n",
        "\n",
        "# elif sys.argv[1] == 'WISDM-v1':\n",
        "with open('WISDM_ar_v1.1/WISDM_ar_v1.1_raw.txt') as f: text = f.readlines()\n",
        "print(\"\\n#####Preprocessing WISDM-v1#####\\n\")\n",
        "activities_list = ['Jogging','Walking','Upstairs','Downstairs','Standing','Sitting']\n",
        "X_list = []\n",
        "y_list = []\n",
        "users_list = []\n",
        "num_zeros = 0\n",
        "def process_line(line_to_process):\n",
        "    global num_zeros\n",
        "    if float(line_to_process.split(',')[2]) == 0: num_zeros += 1#print(\"Timestamp zero, discarding\")\n",
        "    else:\n",
        "        X_list.append([float(x) for x in line_to_process.split(',')[3:]])\n",
        "        y_list.append(activities_list.index(line_to_process.split(',')[1]))\n",
        "        users_list.append(line_to_process.split(',')[0])\n",
        "for i,raw_line in enumerate(text):\n",
        "    #line = line.replace(';','').replace('\\n','')\n",
        "    if raw_line == '\\n': continue\n",
        "    elif raw_line.endswith(',;\\n'): line = raw_line[:-3]\n",
        "    elif raw_line.endswith(';\\n'): line = raw_line[:-2]\n",
        "    elif raw_line.endswith(',\\n'): line = raw_line[:-2]\n",
        "    else: set_trace()\n",
        "    if len(line.split(',')) == 6:\n",
        "        try: process_line(line)\n",
        "        except: print(f\"Can't process line {i}, even though length 6: {raw_line}\\n\")\n",
        "    else:\n",
        "        print(f\"Bad format at line {i}:\\n{raw_line}\")\n",
        "        try:\n",
        "            line1, line2 = line.split(';')\n",
        "            process_line(line1); process_line(line2)\n",
        "            print(f\"I think this was two lines erroneously put on one line. Processing separately as\\n{line1}\\nand\\n{line2}\")\n",
        "        except: print(\"Can't process this line at all, omitting\")\n",
        "one_big_X_array = np.array(X_list)\n",
        "one_big_y_array = np.array(y_list)\n",
        "one_big_users_array = np.array(users_list)\n",
        "print(one_big_X_array.shape)\n",
        "print(one_big_y_array.shape)\n",
        "print(one_big_users_array.shape)\n",
        "print(f\"Number of zero lines: {num_zeros}\")\n",
        "misc.np_save(one_big_X_array,'wisdm_v1','X.npy')\n",
        "misc.np_save(one_big_y_array,'wisdm_v1','y.npy')\n",
        "misc.np_save(one_big_users_array,'wisdm_v1','users.npy')\n",
        "\n",
        "# elif sys.argv[1] == 'WISDM-watch':\n",
        "p_dir = 'wisdm-dataset/raw/phone'\n",
        "w_dir = 'wisdm-dataset/raw/watch'\n",
        "np_dir = 'wisdm-dataset/np_data'\n",
        "print(\"\\n#####Preprocessing WISDM-watch#####\\n\")\n",
        "if not os.path.isdir(np_dir):\n",
        "    os.makedirs(np_dir)\n",
        "\n",
        "mp.dps = 100 # Avoid floating point errors in label insertion function\n",
        "for user_idx in range(1600,1651):\n",
        "    print('user', user_idx)\n",
        "    phone_acc_path = join(p_dir,'accel',f'data_{user_idx}_accel_phone.txt')\n",
        "    watch_acc_path = join(w_dir,'accel',f'data_{user_idx}_accel_watch.txt')\n",
        "    phone_gyro_path = join(p_dir,'gyro',f'data_{user_idx}_gyro_phone.txt')\n",
        "    watch_gyro_path = join(w_dir,'gyro',f'data_{user_idx}_gyro_watch.txt')\n",
        "\n",
        "    label_codes_list = list('ABCDEFGHIJKLMOPQRS') # Missin 'N' is deliberate\n",
        "    def two_arrays_from_txt(inpath):\n",
        "        with open(inpath) as f:\n",
        "            d = f.readlines()\n",
        "            arr = np.array([[float(x) for x in line.strip(';\\n').split(',')[3:]] for line in d])\n",
        "            label_array = np.array([label_codes_list.index(line.split(',')[1]) for line in d])\n",
        "        return arr, label_array\n",
        "\n",
        "    phone_acc, label_array1 = two_arrays_from_txt(phone_acc_path)\n",
        "    watch_acc, label_array2 = two_arrays_from_txt(watch_acc_path)\n",
        "    phone_gyro, label_array3 = two_arrays_from_txt(phone_gyro_path)\n",
        "    watch_gyro, label_array4 = two_arrays_from_txt(watch_gyro_path)\n",
        "    user_arrays = [phone_acc,watch_acc,phone_gyro,watch_gyro]\n",
        "    label_arrays = [label_array1,label_array2,label_array3,label_array4]\n",
        "    max_len = max([a.shape[0] for a in user_arrays])\n",
        "    equalized_user_arrays = [array_expanded(a,max_len) for a in user_arrays]\n",
        "    equalized_label_arrays = [array_expanded(lab_a,max_len) for lab_a in label_arrays]\n",
        "    total_user_array = np.concatenate(equalized_user_arrays,axis=1)\n",
        "    mode_object = stats.mode(np.stack(equalized_label_arrays,axis=1),axis=1)\n",
        "    mode_labels = mode_object.mode[:,0]\n",
        "    # Print how many windows contained just 1 label, how many 2 etc.\n",
        "    #print('Agreement in labels:',label_funcs_tmp.label_counts(mode_object.count[:,0]))\n",
        "    certains = (mode_object.count == 4)[:,0]\n",
        "    user_fn = f'{user_idx}.npy'\n",
        "    misc.np_save(total_user_array,np_dir,user_fn)\n",
        "    user_labels_fn = f'{user_idx}_labels.npy'\n",
        "    misc.np_save(mode_labels,np_dir,user_labels_fn)\n",
        "    user_certains_fn = f'{user_idx}_certains.npy'\n",
        "    misc.np_save(certains,np_dir,user_certains_fn)\n",
        "\n",
        "# elif sys.argv[1] == 'REALDISP':\n",
        "data_dir = 'realdisp/RawData'\n",
        "np_dir = 'realdisp/np_data'\n",
        "print(\"\\n#####Preprocessing REALDISP#####\\n\")\n",
        "if not os.path.isdir(np_dir):\n",
        "    os.makedirs(np_dir)\n",
        "\n",
        "for filename in os.listdir(data_dir):\n",
        "    print(filename)\n",
        "    if filename == 'dataset manual.pdf': continue\n",
        "    if not filename.split('_')[1].startswith('ideal'):\n",
        "        continue\n",
        "    with open(join(data_dir,filename)) as f: xy = f.readlines()\n",
        "    ar = np.array([[float(item) for item in line.split('\\t')] for line in xy])\n",
        "    x = ar[:,:-1]\n",
        "    y = ar[:,-1].astype(int)\n",
        "\n",
        "    np.save(join(np_dir,filename.split('_')[0]), x)\n",
        "    np.save(join(np_dir,filename.split('_')[0])+'_labels', y)\n",
        "\n",
        "# elif sys.argv[1] == 'Capture24':\n",
        "np_dir = 'capture24/np_data'\n",
        "if not os.path.isdir(np_dir):\n",
        "    os.makedirs(np_dir)\n",
        "name_df = pd.read_csv('capture24/annotation-label-dictionary.csv')\n",
        "#name_conversion_dict = dict(zip(name_df['annotation'],name_df['label:DohertySpecific2018']))\n",
        "name_df = name_df[['annotation','label:DohertySpecific2018']]\n",
        "int_label_converter_df = pd.DataFrame(enumerate(name_df['label:DohertySpecific2018'].unique()),columns=['int_label','label:DohertySpecific2018'])\n",
        "int_label_converter_dict = dict(enumerate(name_df['label:DohertySpecific2018'].unique()))\n",
        "with open('capture24/int_label_converter_df.json','w') as f:\n",
        "    json.dump(int_label_converter_dict,f)\n",
        "name_df = name_df.merge(int_label_converter_df)\n",
        "for fname in os.listdir('capture24'):\n",
        "    if fname.endswith('.gz'): continue\n",
        "    subj_id = fname.split('.')[0]\n",
        "    if not subj_id.startswith('P') and not len(subj_id) == 4: continue # Skip metadata files\n",
        "    print(f\"converting {fname} to np\")\n",
        "    try: df = pd.read_csv(join('capture24',fname))\n",
        "    except: set_trace()\n",
        "    translated_df = df.merge(name_df)\n",
        "    x = translated_df[['x','y','z']].to_numpy()\n",
        "    y = translated_df['int_label'].to_numpy()\n",
        "    np.save(join(np_dir,f'{subj_id}.npy'),x)\n",
        "    np.save(join(np_dir,f'{subj_id}_labels.npy'),y)\n",
        "\n",
        "# elif sys.argv[1] == 'HHAR':\n",
        "data_dir = 'Activity recognition exp'\n",
        "np_dir = 'hhar/np_data'\n",
        "print(\"\\n#####Preprocessing HHAR#####\\n\")\n",
        "if not os.path.isdir(np_dir):\n",
        "    os.makedirs(np_dir)\n",
        "\n",
        "pandaload = lambda path: pd.read_csv(join(data_dir,'Phones_accelerometer.csv')).set_index('Creation_Time').drop(['Index','Arrival_Time','Model','Device'],axis=1).dropna()\n",
        "print('loading dataframes\\n')\n",
        "phone_acc_df = pandaload('Phones_accelerometer.csv')\n",
        "phone_gyro_df = pandaload('Phones_gyroscope.csv')\n",
        "watch_acc_df = pandaload('Watch_accelerometer.csv')\n",
        "watch_gyro_df = pandaload('Watch_gyroscope.csv')\n",
        "activities_list = ['bike', 'sit', 'stand', 'walk', 'stairsup', 'stairsdown']\n",
        "user_list = list('abcdefghi')\n",
        "\n",
        "for user_letter_name in user_list:\n",
        "    print('processing user', user_letter_name)\n",
        "    user_phone_acc = phone_acc_df.loc[phone_acc_df.User==user_letter_name]\n",
        "    user_phone_gyro = phone_gyro_df.loc[phone_gyro_df.User==user_letter_name]\n",
        "    user_watch_acc = watch_acc_df.loc[watch_acc_df.User==user_letter_name]\n",
        "    user_watch_gyro = watch_acc_df.loc[watch_gyro_df.User==user_letter_name]\n",
        "    assert all([user_watch_gyro.shape==d.shape for d in (user_phone_acc,user_phone_gyro,user_watch_acc)])\n",
        "    comb_phone = user_phone_acc.join(user_phone_gyro,how='outer',lsuffix='_acc',rsuffix='_gyro')\n",
        "    comb_watch = user_watch_acc.join(user_watch_gyro,how='outer',lsuffix='_acc',rsuffix='_gyro')\n",
        "    #if not (comb_watch.gt_acc == comb_watch.gt_gyro).all(): set_trace()\n",
        "    #if not (comb_phone.gt_acc == comb_phone.gt_gyro).all(): set_trace()\n",
        "    comb = comb_phone.join(comb_watch,how='outer',lsuffix='_phone',rsuffix='_watch')\n",
        "    duplicate_rows = [x for x,count in Counter(comb.index).items() if count > 1]\n",
        "    if len(duplicate_rows) > 10: set_trace()\n",
        "    elif len(duplicate_rows) > 0:\n",
        "        print( f\"removing {len(duplicate_rows)} duplicate rows\")\n",
        "        comb = comb.drop(duplicate_rows)\n",
        "    if not (comb.gt_acc_phone == comb.gt_acc_watch).all(): set_trace()\n",
        "    user_X_array = comb.drop([c for c in comb.columns if 'User' in c or 'gt' in c],axis=1).to_numpy()\n",
        "    user_y_array = np.array([activities_list.index(a) for a in comb['gt_acc_phone']])\n",
        "    save_path = join(np_dir,f\"{user_list.index(user_letter_name)+1}.npy\")\n",
        "    label_save_path = join(np_dir,f\"{user_list.index(user_letter_name)+1}_labels.npy\")\n",
        "    np.save(save_path,user_X_array,allow_pickle=False)\n",
        "    np.save(label_save_path,user_y_array,allow_pickle=False)\n",
        "    # Make smaller option for testing\n",
        "    np.save(join(np_dir,f\"0.npy\"),user_X_array[::1000],allow_pickle=False)\n",
        "    np.save(join(np_dir,f\"0_labels.npy\"),user_y_array[::1000],allow_pickle=False)\n",
        "\n",
        "else: print('\\nIncorrect or no dataset specified\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nQERarq97cIq"
      },
      "outputs": [],
      "source": [
        "# @title Lou1sM make_dsets.py\n",
        "# https://github.com/Lou1sM/HAR/blob/master/make_dsets.py\n",
        "import numpy as np\n",
        "# from dl_utils.misc import check_dir, CifarLikeDataset\n",
        "import os\n",
        "import torch\n",
        "# import project_config\n",
        "from scipy import stats\n",
        "from torch.utils import data\n",
        "#from dl_utils import label_funcs\n",
        "# from dl_utils.tensor_funcs import cudify\n",
        "# import label_funcs_tmp\n",
        "from pdb import set_trace\n",
        "from os.path import join\n",
        "\n",
        "\n",
        "class ChunkDataset(data.Dataset):\n",
        "    def __init__(self,x,y):\n",
        "        self.x, self.y = x,y\n",
        "    def __len__(self): return len(self.x)\n",
        "    def __getitem__(self,idx):\n",
        "        batch_x = self.x[idx].unsqueeze(0)\n",
        "        batch_y = self.y[idx]\n",
        "        return batch_x, batch_y, idx\n",
        "\n",
        "class ConcattedDataset(data.Dataset):\n",
        "    \"\"\"Needs datasets to be StepDatasets in order to Concat them.\"\"\"\n",
        "    def __init__(self,xs,ys,window_size,step_size):\n",
        "        self.x, self.y = torch.cat(xs),torch.cat(ys)\n",
        "        self.window_size = window_size\n",
        "        self.step_size = step_size\n",
        "        component_dset_lengths = [((len(x)-self.window_size)//self.step_size + 1) for x in xs]\n",
        "        x_idx_locs = []\n",
        "        block_start_idx = 0\n",
        "        for x in xs:\n",
        "            x_idx_locs += list(range(block_start_idx,block_start_idx+len(x)-window_size+1,step_size))\n",
        "            block_start_idx += len(x)\n",
        "        self.x_idx_locs = np.array(x_idx_locs)\n",
        "        if not len(self.x_idx_locs) == len(self.y): set_trace()\n",
        "\n",
        "    def __len__(self): return len(self.y)\n",
        "    def __getitem__(self,idx):\n",
        "        x_idx = self.x_idx_locs[idx]\n",
        "        batch_x = self.x[x_idx:x_idx + self.window_size].unsqueeze(0)\n",
        "        batch_y = self.y[idx]\n",
        "        return batch_x, batch_y, idx\n",
        "\n",
        "class UCIFeatDataset(data.Dataset):\n",
        "    def __init__(self,x,y,transforms=[]):\n",
        "        self.x, self.y = cudify(x).float(),cudify(y).float()\n",
        "        assert len(self.x) == len(self.y)\n",
        "        for transform in transforms:\n",
        "            self.x = transform(self.x)\n",
        "    def __len__(self): return len(self.x)\n",
        "    def __getitem__(self,idx):\n",
        "        batch_x = self.x[idx]\n",
        "        batch_y = self.y[idx]\n",
        "        return batch_x, batch_y, idx\n",
        "\n",
        "class StepDataset(data.Dataset):\n",
        "    def __init__(self,x,y,window_size,step_size,transforms=[]):\n",
        "        self.x, self.y = cudify(x).float(),cudify(y).float()\n",
        "        self.window_size = window_size\n",
        "        self.step_size = step_size\n",
        "        self.transforms = transforms\n",
        "        self.position = None\n",
        "        self.ensemble_size = None\n",
        "        for transform in transforms:\n",
        "            self.x = transform(self.x)\n",
        "    def __len__(self): return (len(self.x)-self.window_size)//self.step_size + 1\n",
        "    def __getitem__(self,idx):\n",
        "        batch_x = self.x[idx*self.step_size:(idx*self.step_size) + self.window_size].unsqueeze(0)\n",
        "        batch_y = self.y[idx]\n",
        "        return batch_x, batch_y, idx\n",
        "\n",
        "    def put_in_ensemble(self,position,ensemble_size):\n",
        "        self.y += ensemble_size*position\n",
        "        self.position = position\n",
        "        self.ensemble_size = ensemble_size\n",
        "\n",
        "def preproc_xys(x,y,step_size,window_size,dset_info_object,subj_ids):\n",
        "    ids_string = 'all' if set(subj_ids) == set(dset_info_object.possible_subj_ids) else \"-\".join(subj_ids)\n",
        "    precomp_dir = f'datasets/{dset_info_object.dataset_dir_name}/precomputed/{ids_string}step{step_size}_window{window_size}/'\n",
        "    if os.path.isfile(join(precomp_dir,'x.pt')) and os.path.isfile(join(precomp_dir,'y.pt')):\n",
        "        print(\"loading precomputed datasets\")\n",
        "        x = torch.load(join(precomp_dir,'x.pt'))\n",
        "        y = torch.load(join(precomp_dir,'y.pt'))\n",
        "        with open(join(precomp_dir,'selected_acts.txt')) as f: selected_acts = f.readlines()\n",
        "    else:\n",
        "        print(\"no precomputed datasets, computing from scratch\")\n",
        "        xnans = np.isnan(x).any(axis=1)\n",
        "        x = x[~xnans]\n",
        "        y = y[~xnans]\n",
        "        x = x[y!=-1]\n",
        "        y = y[y!=-1]\n",
        "        num_windows = (len(x) - window_size)//step_size + 1\n",
        "        #mode_labels = np.array([stats.mode(y[w*step_size:w*step_size + window_size]).mode[0] if (y[w*step_size:w*step_size + window_size]==y[w*step_size]).all() else -1 for w in range(num_windows)])\n",
        "        mode_labels = np.array([stats.mode(y[w*step_size:w*step_size + window_size]).mode[0] for w in range(num_windows)])\n",
        "        selected_ids = set(mode_labels)\n",
        "        selected_acts = [dset_info_object.action_name_dict[act_id] for act_id in selected_ids]\n",
        "        mode_labels, trans_dict, changed = label_funcs_tmp.compress_labels(mode_labels)\n",
        "        assert len(selected_acts) == len(set(mode_labels))\n",
        "        x = torch.tensor(x).float()\n",
        "        y = torch.tensor(mode_labels).float()\n",
        "        check_dir(precomp_dir)\n",
        "        torch.save(x,join(precomp_dir,'x.pt'))\n",
        "        torch.save(y,join(precomp_dir,'y.pt'))\n",
        "        with open(join(precomp_dir,'selected_acts.txt'),'w') as f:\n",
        "            for a in selected_acts: f.write(a+'\\n')\n",
        "    return x, y, selected_acts\n",
        "\n",
        "def make_pamap_dset_train_val(args,subj_ids):\n",
        "    # dset_info_object = project_config.PAMAP_INFO\n",
        "    dset_info_object = PAMAP_INFO\n",
        "    x_train = np.concatenate([np.load(f'datasets/PAMAP2_Dataset/np_data/subject{s}.npy') for s in subj_ids])\n",
        "    y_train = np.concatenate([np.load(f'datasets/PAMAP2_Dataset/np_data/subject{s}_labels.npy') for s in subj_ids])\n",
        "    x_train = x_train[y_train!=0] # 0 is a transient activity\n",
        "    y_train = y_train[y_train!=0] # 0 is a transient activity\n",
        "    x_train,y_train,selected_acts = preproc_xys(x_train,y_train,args.step_size,args.window_size,dset_info_object,subj_ids)\n",
        "    dset_train = StepDataset(x_train,y_train,window_size=args.window_size,step_size=args.step_size)\n",
        "    return dset_train, selected_acts\n",
        "\n",
        "# def make_uci_dset_train_val(args,subj_ids):\n",
        "#     dset_info_object = project_config.UCI_INFO\n",
        "#     x_train = np.concatenate([np.load(f'datasets/UCI2/np_data/user{s}.npy') for s in subj_ids])\n",
        "#     y_train = np.concatenate([np.load(f'datasets/UCI2/np_data/user{s}_labels.npy') for s in subj_ids])\n",
        "#     x_train = x_train[y_train<7] # Labels still begin at 1 at this point as\n",
        "#     y_train = y_train[y_train<7] # haven't been compressed, so select 1,..,6\n",
        "#     #x_train = x_train[y_train!=-1]\n",
        "#     #y_train = y_train[y_train!=-1]\n",
        "#     #y_val = y_val[y_val!=-1]\n",
        "#     x_train,y_train,selected_acts = preproc_xys(x_train,y_train,args.step_size,args.window_size,dset_info_object,subj_ids)\n",
        "#     dset_train = StepDataset(x_train,y_train,window_size=args.window_size,step_size=args.step_size)\n",
        "#     return dset_train, selected_acts\n",
        "\n",
        "# def make_uci_feat_dset_train_val():\n",
        "#     dset_info_object = project_config.UCI_INFO\n",
        "#     x = np.load(f'datasets/UCI_feat/uci_feat_data.npy')\n",
        "#     y = np.load(f'datasets/UCI_feat/uci_feat_targets.npy')\n",
        "#     selected_acts = dict(enumerate(['walking','upstairs','downstairs','sitting','standing','lying']))\n",
        "#     dset = UCIFeatDataset(x,y)\n",
        "#     #dset.x = dset.data\n",
        "#     #dset.y = dset.targets\n",
        "#     return dset, selected_acts\n",
        "\n",
        "# def make_wisdm_v1_dset_train_val(args,subj_ids):\n",
        "#     dset_info_object = project_config.WISDMv1_INFO\n",
        "#     x = np.load('datasets/wisdm_v1/X.npy')\n",
        "#     y = np.load('datasets/wisdm_v1/y.npy')\n",
        "#     users = np.load('datasets/wisdm_v1/users.npy')\n",
        "#     train_idxs_to_user = np.zeros(users.shape[0]).astype(np.bool)\n",
        "#     for subj_id in subj_ids:\n",
        "#         new_users = users==subj_id\n",
        "#         train_idxs_to_user = np.logical_or(train_idxs_to_user,new_users)\n",
        "#     x_train = x[train_idxs_to_user]\n",
        "#     y_train = y[train_idxs_to_user]\n",
        "#     x_train,y_train,selected_acts = preproc_xys(x_train,y_train,args.step_size,args.window_size,dset_info_object,subj_ids)\n",
        "#     dset_train = StepDataset(x_train,y_train,window_size=args.window_size,step_size=args.step_size)\n",
        "#     return dset_train, selected_acts\n",
        "\n",
        "# def make_wisdm_watch_dset_train_val(args,subj_ids):\n",
        "#     dset_info_object = project_config.WISDMwatch_INFO\n",
        "#     x_train = np.concatenate([np.load(f'datasets/wisdm-dataset/np_data/{s}.npy') for s in subj_ids])\n",
        "#     y_train = np.concatenate([np.load(f'datasets/wisdm-dataset/np_data/{s}_labels.npy') for s in subj_ids])\n",
        "#     certains_train = np.concatenate([np.load(f'datasets/wisdm-dataset/np_data/{s}_certains.npy') for s in subj_ids])\n",
        "#     x_train = x_train[certains_train]\n",
        "#     y_train = y_train[certains_train]\n",
        "#     x_train,y_train,selected_acts = preproc_xys(x_train,y_train,args.step_size,args.window_size,dset_info_object,subj_ids)\n",
        "#     dset_train = StepDataset(x_train,y_train,window_size=args.window_size,step_size=args.step_size)\n",
        "#     return dset_train, selected_acts\n",
        "\n",
        "# def make_realdisp_dset_train_val(args,subj_ids):\n",
        "#     dset_info_object = project_config.REALDISP_INFO\n",
        "#     x_train = np.concatenate([np.load(f'datasets/realdisp/np_data/subject{s}.npy') for s in subj_ids])\n",
        "#     y_train = np.concatenate([np.load(f'datasets/realdisp/np_data/subject{s}_labels.npy') for s in subj_ids])\n",
        "#     x_train = x_train[:,2:] #First two columns are timestamp\n",
        "#     x_train = x_train[y_train!=0] # 0 seems to be a transient activity\n",
        "#     y_train = y_train[y_train!=0] # 0 seems to be a transient activity\n",
        "#     x_train,y_train,selected_acts = preproc_xys(x_train,y_train,args.step_size,args.window_size,dset_info_object,subj_ids)\n",
        "#     dset_train = StepDataset(x_train,y_train,window_size=args.window_size,step_size=args.step_size)\n",
        "#     return dset_train, selected_acts\n",
        "\n",
        "# def make_hhar_dset_train_val(args,subj_ids):\n",
        "#     dset_info_object = project_config.HHAR_INFO\n",
        "#     x_train = np.concatenate([np.load(f'datasets/hhar/np_data/{s}.npy') for s in subj_ids])\n",
        "#     y_train = np.concatenate([np.load(f'datasets/hhar/np_data/{s}_labels.npy') for s in subj_ids])\n",
        "#     x_train,y_train,selected_acts = preproc_xys(x_train,y_train,args.step_size,args.window_size,dset_info_object,subj_ids)\n",
        "#     dset_train = StepDataset(x_train,y_train,window_size=args.window_size,step_size=args.step_size)\n",
        "#     return dset_train, selected_acts\n",
        "\n",
        "def make_capture_dset_train_val(args,subj_ids):\n",
        "    action_name_dict = {0: 'sleep', 1: 'sedentary-screen', 2: 'tasks-moderate', 3: 'sedentary-non-screen', 4: 'walking', 5: 'vehicle', 6: 'bicycling', 7: 'tasks-light', 8: 'sports-continuous', 9: 'sport-interrupted'} # Should also be saved in json file in datasets/capture24\n",
        "    subj_ids = len(subj_ids) - min(2,len(subj_ids)//2)\n",
        "    subj_ids = subj_ids[:subj_ids]\n",
        "    def three_digitify(x): return '00' + str(x) if len(str(x))==1 else '0' + str(x)\n",
        "    x_train = np.concatenate([np.load(f'datasets/capture24/np_data/P{three_digitify(s)}.npy') for s in subj_ids])\n",
        "    y_train = np.concatenate([np.load(f'datasets/capture24/np_data/P{three_digitify(s)}_labels.npy') for s in subj_ids])\n",
        "    x_train,y_train,selected_acts = preproc_xys(x_train,y_train,args.step_size,args.window_size,action_name_dict)\n",
        "    dset_train = StepDataset(x_train,y_train,window_size=args.window_size,step_size=args.step_size)\n",
        "    if len(subj_ids) <= 2: return dset_train, dset_train, selected_acts\n",
        "\n",
        "    # else make val dset\n",
        "    val_ids = subj_ids[subj_ids:]\n",
        "    x_val = np.concatenate([np.load(f'datasets/capture24/np_data/P{three_digitify(s)}.npy') for s in val_ids])\n",
        "    y_val = np.concatenate([np.load(f'datasets/capture24/np_data/P{three_digitify(s)}_labels.npy') for s in val_ids])\n",
        "    x_val,y_val,selected_acts = preproc_xys(x_val,y_val,args.step_size,args.window_size,action_name_dict)\n",
        "    dset_val = StepDataset(x_val,y_val,window_size=args.window_size,step_size=args.step_size)\n",
        "    return dset_train, dset_val, selected_acts\n",
        "\n",
        "def make_single_dset(args,subj_ids):\n",
        "    if args.dset == 'PAMAP':\n",
        "        return make_pamap_dset_train_val(args,subj_ids)\n",
        "    # if args.dset == 'UCI':\n",
        "    #     return make_uci_dset_train_val(args,subj_ids)\n",
        "    # if args.dset == 'UCI_feat':\n",
        "    #     return make_uci_feat_dset_train_val()\n",
        "    # if args.dset == 'WISDM-v1':\n",
        "    #     return make_wisdm_v1_dset_train_val(args,subj_ids)\n",
        "    # if args.dset == 'WISDM-watch':\n",
        "    #     return make_wisdm_watch_dset_train_val(args,subj_ids)\n",
        "    # if args.dset == 'REALDISP':\n",
        "    #     return make_realdisp_dset_train_val(args,subj_ids)\n",
        "    # if args.dset == 'HHAR':\n",
        "    #     return make_hhar_dset_train_val(args,subj_ids)\n",
        "    # if args.dset == 'Capture24':\n",
        "    #     return make_capture_dset_train_val(args,subj_ids)\n",
        "\n",
        "def make_dsets_by_user(args,subj_ids):\n",
        "    dsets_by_id = {}\n",
        "    for subj_id in subj_ids:\n",
        "        dset_subj, selected_acts_subj = make_single_dset(args,[subj_id])\n",
        "        dsets_by_id[subj_id] = dset_subj,selected_acts_subj\n",
        "    return dsets_by_id\n",
        "\n",
        "def chunked_up(x,step_size,window_size):\n",
        "    num_windows = (len(x) - window_size)//step_size + 1\n",
        "    return torch.stack([x[i*step_size:i*step_size+window_size] for i in range(num_windows)])\n",
        "\n",
        "def combine_dsets(dsets):\n",
        "    xs = [d.x for d in dsets]\n",
        "    ys = [d.y for d in dsets]\n",
        "    return ConcattedDataset(xs,ys,dsets[0].window_size,dsets[0].step_size)\n",
        "\n",
        "def combine_dsets_old(dsets):\n",
        "    processed_dset_xs = []\n",
        "    for dset in dsets:\n",
        "        if isinstance(dset,StepDataset):\n",
        "            processed_dset_x = chunked_up(dset.x,dset.step_size,dset.window_size)\n",
        "        elif isinstance(dset,ChunkDataset):\n",
        "            processed_dset_x = dset.x\n",
        "        else:\n",
        "            print(f\"you're trying to combine dsets on a {type(dset)}, but it has to be a dataset\")\n",
        "        processed_dset_xs.append(processed_dset_x)\n",
        "    x = torch.cat(processed_dset_xs)\n",
        "    y = torch.cat([dset.y for dset in dsets])\n",
        "    assert len(x) == len(y)\n",
        "    combined = ChunkDataset(x,y)\n",
        "    return combined\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_PZ6amU5BhIa"
      },
      "outputs": [],
      "source": [
        "# @title Lou1sM project_config.py\n",
        "# https://github.com/Lou1sM/HAR/blob/master/project_config.py\n",
        "import sys\n",
        "import torch\n",
        "\n",
        "class HAR_Dataset_Container():\n",
        "    def __init__(self,code_name,dataset_dir_name,possible_subj_ids,num_channels,num_classes,action_name_dict):\n",
        "        self.code_name = code_name\n",
        "        self.dataset_dir_name = dataset_dir_name\n",
        "        self.possible_subj_ids = possible_subj_ids\n",
        "        self.num_channels = num_channels\n",
        "        self.num_classes = num_classes\n",
        "        self.action_name_dict = action_name_dict\n",
        "\n",
        "\n",
        "# PAMAP\n",
        "pamap_ids = [str(x) for x in range(101,110)]\n",
        "pamap_action_name_dict = {1:'lying',2:'sitting',3:'standing',4:'walking',5:'running',6:'cycling',7:'Nordic walking',9:'watching TV',10:'computer work',11:'car driving',12:'ascending stairs',13:'descending stairs',16:'vacuum cleaning',17:'ironing',18:'folding laundry',19:'house cleaning',20:'playing soccer',24:'rope jumping'}\n",
        "PAMAP_INFO = HAR_Dataset_Container(\n",
        "            code_name = 'PAMAP',\n",
        "            dataset_dir_name = 'PAMAP2_Dataset',\n",
        "            possible_subj_ids = pamap_ids,\n",
        "            num_channels = 39,\n",
        "            num_classes = 12,\n",
        "            action_name_dict = pamap_action_name_dict)\n",
        "\n",
        "# # UCI\n",
        "# def two_digitify(x): return '0' + str(x) if len(str(x))==1 else str(x)\n",
        "# uci_ids = [two_digitify(x) for x in range(1,30)]\n",
        "# uci_action_name_dict = {1:'walking',2:'walking upstairs',3:'walking downstairs',4:'sitting',5:'standing',6:'lying',7:'stand_to_sit',9:'sit_to_stand',10:'sit_to_lit',11:'lie_to_sit',12:'stand_to_lie',13:'lie_to_stand'}\n",
        "# UCI_INFO = HAR_Dataset_Container(\n",
        "#             code_name = 'UCI',\n",
        "#             dataset_dir_name = 'UCI2',\n",
        "#             possible_subj_ids = uci_ids,\n",
        "#             num_channels = 6,\n",
        "#             num_classes = 6,\n",
        "#             action_name_dict = uci_action_name_dict)\n",
        "\n",
        "# # UCI_feat\n",
        "# def two_digitify(x): return '0' + str(x) if len(str(x))==1 else str(x)\n",
        "# uci_feat_action_name_dict = {1:'walking',2:'walking upstairs',3:'walking downstairs',4:'sitting',5:'standing',6:'lying',7:'stand_to_sit',9:'sit_to_stand',10:'sit_to_lit',11:'lie_to_sit',12:'stand_to_lie',13:'lie_to_stand'}\n",
        "# UCI_FEAT_INFO = HAR_Dataset_Container(\n",
        "#             code_name = 'UCI_feat',\n",
        "#             dataset_dir_name = 'UCI2_feat',\n",
        "#             possible_subj_ids = ['0'],\n",
        "#             num_channels = 561,\n",
        "#             num_classes = 6,\n",
        "#             action_name_dict = uci_action_name_dict)\n",
        "\n",
        "# # WISDM-v1\n",
        "# wisdmv1_ids = [str(x) for x in range(1,37)] #Paper says 29 users but ids go up to 36\n",
        "# activities_list = ['Jogging','Walking','Upstairs','Downstairs','Standing','Sitting']\n",
        "# wisdmv1_action_name_dict = dict(zip(range(len(activities_list)),activities_list))\n",
        "# WISDMv1_INFO = HAR_Dataset_Container(\n",
        "#             code_name = 'WISDM-v1',\n",
        "#             dataset_dir_name = 'wisdm_v1',\n",
        "#             possible_subj_ids = wisdmv1_ids,\n",
        "#             num_channels = 3,\n",
        "#             num_classes = 6,\n",
        "#             action_name_dict = wisdmv1_action_name_dict)\n",
        "\n",
        "# # WISDM-watch\n",
        "# wisdmwatch_ids = [str(x) for x in range(1600,1651)]\n",
        "# with open('datasets/wisdm-dataset/activity_key.txt') as f: r=f.readlines()\n",
        "# activities_list = [x.split(' = ')[0] for x in r if ' = ' in x]\n",
        "# wisdmwatch_action_name_dict = dict(zip(range(len(activities_list)),activities_list))\n",
        "# WISDMwatch_INFO = HAR_Dataset_Container(\n",
        "#             code_name = 'WISDM-watch',\n",
        "#             dataset_dir_name = 'wisdm-dataset',\n",
        "#             possible_subj_ids = wisdmwatch_ids,\n",
        "#             num_channels = 12,\n",
        "#             num_classes = 17,\n",
        "#             action_name_dict = wisdmwatch_action_name_dict)\n",
        "\n",
        "# # REALDISP\n",
        "# realdisp_ids = [str(x) for x in range(1,18)]\n",
        "# activities_list = ['Walking','Jogging','Running','Jump up','Jump front & back','Jump sideways','Jump leg/arms open/closed','Jump rope','Trunk twist','Trunk twist','Waist bends forward','Waist rotation','Waist bends','Reach heels backwards','Lateral bend','Lateral bend with arm up','Repetitive forward stretching','Upper trunk and lower body opposite twist','Lateral elevation of arms','Frontal elevation of arms','Frontal hand claps','Frontal crossing of arms','Shoulders high-amplitude rotation','Shoulders low-amplitude rotation','Arms inner rotation','Knees','Heels','Knees bending','Knees','Rotation on the knees','Rowing','Elliptical bike','Cycling']\n",
        "# realdisp_action_name_dict = {i+1:act for i,act in enumerate(activities_list)}\n",
        "# REALDISP_INFO = HAR_Dataset_Container(\n",
        "#             code_name = 'REALDISP',\n",
        "#             dataset_dir_name = 'realdisp',\n",
        "#             possible_subj_ids = realdisp_ids,\n",
        "#             num_channels = 117,\n",
        "#             num_classes = 33,\n",
        "#             action_name_dict = realdisp_action_name_dict)\n",
        "\n",
        "# # HHAR\n",
        "# hhar_ids = [str(x) for x in range(0,10)]\n",
        "# activities_list = ['bike', 'sit', 'stand', 'walk', 'stairsup', 'stairsdown']\n",
        "# hhar_action_name_dict = {i:act for i,act in enumerate(activities_list)}\n",
        "# HHAR_INFO = HAR_Dataset_Container(\n",
        "#             code_name = 'HHAR',\n",
        "#             dataset_dir_name = 'hhar',\n",
        "#             possible_subj_ids = hhar_ids,\n",
        "#             num_channels = 12,\n",
        "#             num_classes = 6,\n",
        "#             action_name_dict = hhar_action_name_dict)\n",
        "\n",
        "# DSET_OBJECTS = [PAMAP_INFO, UCI_INFO, UCI_FEAT_INFO, WISDMv1_INFO, WISDMwatch_INFO,REALDISP_INFO,HHAR_INFO]\n",
        "DSET_OBJECTS = [PAMAP_INFO]\n",
        "\n",
        "\n",
        "def get_dataset_info_object(dset_name):\n",
        "    dsets_by_that_name = [d for d in DSET_OBJECTS if d.code_name == dset_name]\n",
        "    if len(dsets_by_that_name)==0: print(f\"{dset_name} is not a recognized dataset\"); sys.exit()\n",
        "    assert len(dsets_by_that_name)==1\n",
        "    return dsets_by_that_name[0]\n",
        "\n",
        "def get_num_time_points():\n",
        "    for dset_info_object in DSET_OBJECTS:\n",
        "        print(dset_info_object.code_name, sum([torch.load(f'datasets/{dset_info_object.dataset_dir_name}/precomputed/{s}step100_window512/x.pt').shape[0] for s in dset_info_object.possible_subj_ids]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "3UvqfQXfFSu1",
        "outputId": "84e2ee66-1f8e-47bf-88a6-ff9e8a6b9a8d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "usage: colab_kernel_launcher.py [-h] [--num_subjs NUM_SUBJS | --subj_ids SUBJ_IDS [SUBJ_IDS ...]]\n",
            "                                [--full_epochs | --short_epochs] [--ablate_label_filter]\n",
            "                                [--all_subjs] [--bad_ids] [--batch_size_train BATCH_SIZE_TRAIN]\n",
            "                                [--batch_size_val BATCH_SIZE_VAL] [--clusterer {HMM,GMM}]\n",
            "                                [--compute_cross_metrics] [--dec_lr DEC_LR]\n",
            "                                [-d {PAMAP,UCI,WISDM-v1,WISDM-watch,REALDISP,Capture24}]\n",
            "                                [--enc_lr ENC_LR] [--exp_name EXP_NAME]\n",
            "                                [--frac_gt_labels FRAC_GT_LABELS] [--gpu GPU] [--is_n2d]\n",
            "                                [--is_uln] [--just_align_time] [--load_pretrained]\n",
            "                                [--mlp_lr MLP_LR] [--no_umap] [--noise NOISE]\n",
            "                                [--num_classes NUM_CLASSES] [--num_meta_epochs NUM_META_EPOCHS]\n",
            "                                [--num_meta_meta_epochs NUM_META_META_EPOCHS]\n",
            "                                [--num_pseudo_label_epochs NUM_PSEUDO_LABEL_EPOCHS]\n",
            "                                [--umap_dim UMAP_DIM] [--umap_neighbours UMAP_NEIGHBOURS]\n",
            "                                [--prob_thresh PROB_THRESH] [--reinit] [--reload_ids RELOAD_IDS]\n",
            "                                [--rlmbda RLMBDA] [--show_transitions] [--step_size STEP_SIZE]\n",
            "                                [--subject_independent] [--test]\n",
            "                                [--train_type {full,cluster_as_single,cluster_individually,train_frac_gts_as_single,find_similar_users}]\n",
            "                                [--show_shapes] [--verbose] [--window_size WINDOW_SIZE]\n",
            "colab_kernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-50ea2cb4-a0b3-4dd4-be5b-f719dc2cbc66.json\n"
          ]
        },
        {
          "ename": "SystemExit",
          "evalue": "2",
          "output_type": "error",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        }
      ],
      "source": [
        "# @title Lou1sM har_cnn.py\n",
        "# https://github.com/Lou1sM/HAR/blob/master/har_cnn.py\n",
        "import argparse\n",
        "import sys\n",
        "# import project_config\n",
        "\n",
        "\n",
        "def get_cl_args():\n",
        "    dset_options = ['PAMAP','UCI','WISDM-v1','WISDM-watch','REALDISP','Capture24']\n",
        "    # dset_options = [di.code_name for di in project_config.DSET_OBJECTS]\n",
        "    training_type_options = ['full','cluster_as_single','cluster_individually','train_frac_gts_as_single','find_similar_users']\n",
        "    parser = argparse.ArgumentParser()\n",
        "    subjs_group = parser.add_mutually_exclusive_group(required=False)\n",
        "    subjs_group.add_argument('--num_subjs',type=int)\n",
        "    subjs_group.add_argument('--subj_ids',type=str,nargs='+',default=['first'])\n",
        "    epochs_group = parser.add_mutually_exclusive_group(required=False)\n",
        "    epochs_group.add_argument('--full_epochs',action='store_true')\n",
        "    epochs_group.add_argument('--short_epochs',action='store_true')\n",
        "    parser.add_argument('--ablate_label_filter',action='store_true')\n",
        "    parser.add_argument('--all_subjs',action='store_true')\n",
        "    parser.add_argument('--bad_ids',action='store_true')\n",
        "    parser.add_argument('--batch_size_train',type=int,default=256)\n",
        "    parser.add_argument('--batch_size_val',type=int,default=1024)\n",
        "    parser.add_argument('--clusterer',type=str,choices=['HMM','GMM'],default='HMM')\n",
        "    parser.add_argument('--compute_cross_metrics',action='store_true')\n",
        "    parser.add_argument('--dec_lr',type=float,default=1e-3)\n",
        "    parser.add_argument('-d','--dset',type=str,default='UCI',choices=dset_options)\n",
        "    parser.add_argument('--enc_lr',type=float,default=1e-3)\n",
        "    parser.add_argument('--exp_name',type=str,default=\"try\")\n",
        "    parser.add_argument('--frac_gt_labels',type=float,default=0.1)\n",
        "    parser.add_argument('--gpu',type=str,default='0')\n",
        "    parser.add_argument('--is_n2d',action='store_true')\n",
        "    parser.add_argument('--is_uln',action='store_true',help='net for dim red. instead of umap')\n",
        "    parser.add_argument('--just_align_time',action='store_true')\n",
        "    parser.add_argument('--load_pretrained',action='store_true')\n",
        "    parser.add_argument('--mlp_lr',type=float,default=1e-3)\n",
        "    parser.add_argument('--no_umap',action='store_true')\n",
        "    parser.add_argument('--noise',type=float,default=1.)\n",
        "    parser.add_argument('--num_classes',type=int,default=-1)\n",
        "    parser.add_argument('--num_meta_epochs',type=int,default=1)\n",
        "    parser.add_argument('--num_meta_meta_epochs',type=int,default=1)\n",
        "    parser.add_argument('--num_pseudo_label_epochs',type=int,default=5)\n",
        "    parser.add_argument('--umap_dim',type=int,default=2)\n",
        "    parser.add_argument('--umap_neighbours',type=int,default=60)\n",
        "    parser.add_argument('--prob_thresh',type=float,default=.95)\n",
        "    parser.add_argument('--reinit',action='store_true')\n",
        "    parser.add_argument('--reload_ids',type=int,default=0)\n",
        "    parser.add_argument('--rlmbda',type=float,default=.1)\n",
        "    parser.add_argument('--show_transitions',action='store_true')\n",
        "    parser.add_argument('--step_size',type=int,default=5)\n",
        "    parser.add_argument('--subject_independent',action='store_true')\n",
        "    parser.add_argument('--test','-t',action='store_true')\n",
        "    parser.add_argument('--train_type',type=str,choices=training_type_options,default='full')\n",
        "    parser.add_argument('--show_shapes',action='store_true',help='print the shapes of hidden layers in enc and dec')\n",
        "    parser.add_argument('--verbose',action='store_true')\n",
        "    parser.add_argument('--window_size',type=int,default=512)\n",
        "    ARGS = parser.parse_args()\n",
        "\n",
        "    need_umap = False\n",
        "    if ARGS.is_uln:\n",
        "        ARGS.no_umap = True\n",
        "    if ARGS.short_epochs:\n",
        "        ARGS.num_meta_meta_epochs = 1\n",
        "        ARGS.num_meta_epochs = 1\n",
        "        ARGS.num_pseudo_label_epochs = 1\n",
        "    elif ARGS.full_epochs:\n",
        "        ARGS.num_meta_meta_epochs = 10\n",
        "        ARGS.num_meta_epochs = 10\n",
        "        ARGS.num_pseudo_label_epochs = 5\n",
        "    if ARGS.test:\n",
        "        ARGS.num_meta_epochs = 1\n",
        "        ARGS.num_meta_meta_epochs = 1\n",
        "        ARGS.num_pseudo_label_epochs = 1\n",
        "    elif not ARGS.no_umap and not ARGS.show_shapes: need_umap = True\n",
        "    print(ARGS)\n",
        "    dset_info_object = project_config.get_dataset_info_object(ARGS.dset)\n",
        "    all_possible_ids = dset_info_object.possible_subj_ids\n",
        "    if ARGS.all_subjs: ARGS.subj_ids=all_possible_ids\n",
        "    elif ARGS.num_subjs is not None: ARGS.subj_ids = all_possible_ids[:ARGS.num_subjs]\n",
        "    elif ARGS.subj_ids == ['first']: ARGS.subj_ids = all_possible_ids[:1]\n",
        "    bad_ids = [x for x in ARGS.subj_ids if x not in all_possible_ids]\n",
        "    if len(bad_ids) > 0 and not (ARGS.test and ARGS.dset=='HHAR'):\n",
        "        print(f\"You have specified non-existent ids: {bad_ids}\\nExistent ids are {all_possible_ids}\"); sys.exit()\n",
        "    return ARGS, need_umap\n",
        "\n",
        "RELEVANT_ARGS = ['ablate_label_filter','clusterer','dset','no_umap','num_meta_epochs','num_meta_meta_epochs','num_pseudo_label_epochs','reinit','step_size','subject_independent']\n",
        "\n",
        "\n",
        "# ARGS, need_umap = cl_args.get_cl_args()\n",
        "ARGS, need_umap = get_cl_args()\n",
        "# if need_umap: import umap\n",
        "# main(ARGS)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "0vScvFGGSeN6",
        "8DxjYI9RMQoP"
      ],
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}