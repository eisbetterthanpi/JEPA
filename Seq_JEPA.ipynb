{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/Seq_JEPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FA00-tf6MJ-s"
      },
      "outputs": [],
      "source": [
        "# @title rohanpandey WISDM\n",
        "# https://github.com/rohanpandey/Analysis--WISDM-Smartphone-and-Smartwatch-Activity/blob/master/dataset_creation.ipynb\n",
        "! wget https://archive.ics.uci.edu/static/public/507/wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset.zip\n",
        "!unzip wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset.zip\n",
        "!unzip wisdm-dataset.zip\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "folder1=glob.glob(\"wisdm-dataset/raw/*\")\n",
        "# print(folder1)\n",
        "column_names = ['ID', 'activity','timestamp','x','y','z']\n",
        "overall_dataframe=pd.DataFrame(columns = column_names)\n",
        "for subfolder in folder1:\n",
        "    parent_dir = \"./processed/\"\n",
        "    path = os.path.join(parent_dir, subfolder.split('\\\\')[-1])\n",
        "    if not os.path.exists(path): os.makedirs(path)\n",
        "    folder2=glob.glob(subfolder+\"/*\")\n",
        "    for subsubfolder in folder2:\n",
        "        activity_dataframe = pd.DataFrame(columns = column_names)\n",
        "        subfolder_path = os.path.join(path, subsubfolder.split('/')[-1])\n",
        "        if not os.path.exists(subfolder_path): os.makedirs(subfolder_path)\n",
        "        files=glob.glob(subsubfolder+\"/*\")\n",
        "        for file in files:\n",
        "            # print(file)\n",
        "            df = pd.read_csv(file, sep=\",\",header=None)\n",
        "            df.columns = ['ID','activity','timestamp','x','y','z']\n",
        "            # activity_dataframe=activity_dataframe.append(df)\n",
        "            activity_dataframe = pd.concat([activity_dataframe, df], ignore_index=True)\n",
        "\n",
        "        activity_dataframe['z']=activity_dataframe['z'].str[:-1]\n",
        "        # activity_dataframe['meter']=subsubfolder.split('/')[-1]\n",
        "        # activity_dataframe['device']=subfolder.split('/')[-1]\n",
        "        activity_dataframe.to_csv(subfolder_path+'/data.csv',index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "CNVNCV8CGtR_"
      },
      "outputs": [],
      "source": [
        "# @title wisdm dataloader\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "# /content/processed/wisdm-dataset/raw/phone/accel/data.csv\n",
        "# /content/processed/wisdm-dataset/raw/watch/gyro/data.csv\n",
        "\n",
        "class BufferDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        # df_keep = pd.read_csv(\"data.csv\")#[['ID','activity','timestamp','x','y','z']]\n",
        "        df_keep = pd.read_csv(\"/content/processed/wisdm-dataset/raw/watch/accel/data.csv\")\n",
        "\n",
        "        user_acts = dict(tuple(df_keep.groupby(['ID','activity'])[['timestamp','x','y','z']]))\n",
        "        self.data = [[d.to_numpy(), a] for a, d in user_acts.items()]\n",
        "        self.act_dict = {i: act for i, act in enumerate(df_keep['activity'].unique())}\n",
        "        self.act_invdict = {v: k for k, v in self.act_dict.items()} # {'A': 0, 'B': 1, ...\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, id_act = self.data[idx]\n",
        "        id_act = self.process(id_act)\n",
        "        return torch.tensor(x[:3500]), id_act # 3567\n",
        "\n",
        "    def process(self, id_act):\n",
        "        return int(id_act[0]), self.act_invdict[id_act[1]]\n",
        "\n",
        "    def transform(self, act_list): # rand resize crop, rand mask # https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html\n",
        "        act_list = np.array(act_list)\n",
        "        try: hr, temp, heart= zip(*act_list)\n",
        "        except ValueError: print('err:',act_list)\n",
        "        kind = 'nearest' if len(act_list)>self.seq_len/.7 else 'linear'\n",
        "        temp_interpolator = scipy.interpolate.interp1d(hr, temp, kind=kind) # linear nearest quadratic cubic\n",
        "        heart_interpolator = scipy.interpolate.interp1d(hr, heart, kind=kind) # linear nearest quadratic cubic\n",
        "        hr_ = np.sort(np.random.uniform(hr[0], hr[-1], round(self.seq_len*random.uniform(1,1/.7))))\n",
        "        temp_, heart_ = temp_interpolator(hr_), heart_interpolator(hr_)\n",
        "        act_list = list(zip(hr_, temp_, heart_))\n",
        "\n",
        "        idx = torch.randint(len(act_list)-self.seq_len+1, size=(1,))\n",
        "        # return act_list[idx: idx+self.seq_len]\n",
        "        act_list = act_list[idx: idx+self.seq_len]\n",
        "\n",
        "        # mask=(torch.rand(self.seq_len)<.1) # True -> masked # random masking\n",
        "        # # act_list[mask] = self.pad[0]\n",
        "        # act_list = [self.pad[0] if m else a for m,a in zip(mask, act_list)]\n",
        "        return act_list\n",
        "\n",
        "    def add(self, episode):\n",
        "        self.data.append(episode)\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    # print(npimg.shape) # (3, 64, 64)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# df_keep = pd.read_csv(\"data.csv\")#[['ID','activity','timestamp','x','y','z']]\n",
        "# user_acts = dict(tuple(df_keep.groupby(['ID','activity'])[['timestamp','x','y','z']]))\n",
        "# dataset = [[d.to_numpy(), a] for a, d in user_acts.items()]\n",
        "\n",
        "\n",
        "train_data = BufferDataset() # one line of poem is roughly 50 characters\n",
        "\n",
        "dataset_size = len(train_data)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(0.7 * dataset_size))\n",
        "np.random.seed(0)\n",
        "np.random.shuffle(indices)\n",
        "train_indices, val_indices = indices[:split], indices[split:]\n",
        "\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "valid_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 64 #512\n",
        "# train_loader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "# test_loader = DataLoader(train_data, sampler=valid_sampler, batch_size=batch_size, num_workers=2, drop_last=True)\n",
        "train_loader = DataLoader(train_data, sampler=train_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "test_loader = DataLoader(train_data, sampler=valid_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "f6T4F651kmGh"
      },
      "outputs": [],
      "source": [
        "# @title AttentionBlock\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class SelfAttn(nn.Module):\n",
        "    def __init__(self, dim, n_heads):\n",
        "        super().__init__()\n",
        "        self.dim, self.n_heads = dim, n_heads\n",
        "        d_head = dim//n_heads\n",
        "        self.qkv = nn.Linear(dim, dim*3, bias=False)\n",
        "        # self.lin = nn.Linear(dim, dim)\n",
        "        self.lin = zero_module(nn.Linear(dim, dim))\n",
        "        # self.rope = RoPE(d_head, seq_len=512, base=10000)\n",
        "        # self.rope = RoPE2D(d_head, h=64, w=64, base=100)\n",
        "        self.scale = d_head**-.5\n",
        "\n",
        "    def forward(self, x): # [b,t,d]\n",
        "        q,k,v = self.qkv(x).unflatten(-1, (self.n_heads,-1)).transpose(1,2).chunk(3, dim=-1) # [b, t, n_heads, d_head] -> [b, n_heads, t, d_head]\n",
        "        # q, k = self.rope(q), self.rope(k)\n",
        "        q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2)\n",
        "        context = k.transpose(-2,-1) @ v # [batch, n_heads, d_head, d_head]\n",
        "        x = q @ context # [b, n_heads, t, d_head]\n",
        "        # print('SelfAttn', x.shape)\n",
        "        x = x.transpose(1,2).flatten(2)\n",
        "        return self.lin(x)\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, mult=4, drop=0.):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        # self.norm = nn.RMSNorm(d_model, elementwise_affine=False) # LayerNorm RMSNorm\n",
        "        self.norm1, self.norm2 = nn.LayerNorm(d_model), nn.LayerNorm(d_model)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "        self.attn = SelfAttn(d_model, n_heads)\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        ff_dim=d_model*mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), act, nn.Dropout(drop), nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "            nn.RMSNorm(ff_dim), act, nn.Dropout(drop), zero_module(nn.Linear(ff_dim, d_model))\n",
        "        )\n",
        "\n",
        "    def forward(self, x): # [batch, seq_len, d_model]\n",
        "        # print('attnblk fwd',x.shape)\n",
        "        x = x + self.drop(self.attn(self.norm1(x)))\n",
        "        x = x + self.ff(x)\n",
        "        # x = x + self.drop(self.ff(self.norm2(x)))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Fh9o__m2-j7K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "421743bc-79dd-480d-fd76-4870e4cd3bc8",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title simplex\n",
        "# !pip install -q opensimplex\n",
        "import opensimplex\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def simplexmask1d(seq=512, ctx_scale=(.85,1), trg_scale=(.6,.8), B=64, chaos=[1,.5]):\n",
        "    i = np.linspace(0, chaos[0], num=seq) # 2-5\n",
        "    noise = opensimplex.noise2array(i, np.random.randint(1e10, size=B)) # [B, seq]\n",
        "    noise = torch.from_numpy(noise)\n",
        "    # trunc_normal = torch.fmod(torch.randn(2)*.3,1)/2 + .5\n",
        "    # print(trunc_normal)\n",
        "    ctx_mask_scale = torch.rand(1) * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    trg_mask_scale = torch.rand(1) * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "    # ctx_mask_scale = trunc_normal[0] * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    # trg_mask_scale = trunc_normal[1] * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "\n",
        "    ctx_len, trg_len = int(seq*ctx_mask_scale), int(seq*trg_mask_scale)\n",
        "    val, trg_index = torch.topk(noise, trg_len, dim=1, sorted=False)\n",
        "    ctx_len = ctx_len - trg_len\n",
        "\n",
        "    remove_mask = torch.ones((B,seq), dtype=bool) # [B, S]\n",
        "    remove_mask.scatter_(1, trg_index, False).flatten()\n",
        "    ind = torch.arange(seq).unsqueeze(0).repeat(B,1)[remove_mask].reshape(B, -1)\n",
        "\n",
        "    i = np.linspace(0, chaos[1], num=seq) # 2-5\n",
        "    noise = opensimplex.noise2array(i, np.random.randint(1e10, size=B)) # [B, seq]\n",
        "    noise = torch.from_numpy(noise)[remove_mask].reshape(B, -1)\n",
        "    val, ctx_ind = torch.topk(noise, ctx_len, dim=1, sorted=False)\n",
        "    ctx_index = ind[torch.arange(B).unsqueeze(-1), ctx_ind]\n",
        "    return ctx_index, trg_index\n",
        "\n",
        "b=64\n",
        "# ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.7,.8), trg_scale=(.4,.6), B=b, chaos=[3,.5])\n",
        "# ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.85,1), trg_scale=(.7,.8), B=b, chaos=[1,.5])\n",
        "ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.8,.9), trg_scale=(.7,.8), B=b, chaos=[1,.5])\n",
        "mask = torch.zeros(b ,200)\n",
        "mask[torch.arange(b).unsqueeze(-1), trg_index] = 1\n",
        "mask[torch.arange(b).unsqueeze(-1), ctx_index] = .5\n",
        "mask = mask[None,...]\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "# imshow(mask)\n",
        "import torchvision\n",
        "imshow(torchvision.utils.make_grid(mask, nrow=8))\n",
        "\n",
        "# print(index)\n",
        "# print(index.shape)\n",
        "# print(mask)\n",
        "# print(mask.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trunc_normal = torch.fmod(torch.randn(100),1)/2 + .5\n",
        "print(trunc_normal)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3mc56_TRvSE",
        "outputId": "10c814a1-ae73-4497-d69c-f8ca6764a5ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4075, 0.8762, 0.7514, 0.2137, 0.7609, 0.4488, 0.9301, 0.6824, 0.2306,\n",
            "        0.0416, 0.5345, 0.2072, 0.2726, 0.8464, 0.1821, 0.6435, 0.1008, 0.5005,\n",
            "        0.6204, 0.9338, 0.6580, 0.5576, 0.4063, 0.3929, 0.5876, 0.2481, 0.6494,\n",
            "        0.1944, 0.9617, 0.8610, 0.5882, 0.1805, 0.8120, 0.6265, 0.1962, 0.7703,\n",
            "        0.7108, 0.4484, 0.6399, 0.5808, 0.9070, 0.8521, 0.8258, 0.4551, 0.0527,\n",
            "        0.3494, 0.1667, 0.4829, 0.2349, 0.5762, 0.3556, 0.2047, 0.2669, 0.5557,\n",
            "        0.6850, 0.4638, 0.7767, 0.4705, 0.4847, 0.2944, 0.5512, 0.0012, 0.7335,\n",
            "        0.3080, 0.5374, 0.4232, 0.6948, 0.9091, 0.3728, 0.8499, 0.5635, 0.8143,\n",
            "        0.6646, 0.2978, 0.6630, 0.5456, 0.2028, 0.0696, 0.7285, 0.4377, 0.7755,\n",
            "        0.7153, 0.5771, 0.0754, 0.1795, 0.7382, 0.1254, 0.0416, 0.1422, 0.5820,\n",
            "        0.9830, 0.5813, 0.2172, 0.8218, 0.9785, 0.9753, 0.3840, 0.2212, 0.2657,\n",
            "        0.5348])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Xn7WZShwWxF8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "8356b8fa-ef70-4ea4-8e62-81449808a9af"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title ijepa multiblock 1d\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "import torch\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(self, length=200,\n",
        "        enc_mask_scale=(0.2, 0.8), pred_mask_scale=(0.2, 0.8),\n",
        "        nenc=1, npred=2, min_keep=4, allow_overlap=False):\n",
        "        super().__init__()\n",
        "        self.length = length\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "\n",
        "    def _sample_block_size(self, scale):\n",
        "        _rand = torch.rand(1).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.length * mask_scale) # num patches to keep\n",
        "        # -- Sample block aspect-ratio\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        l = max_keep#int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        while l >= self.length: l -= 1 # crop mask to be smaller than img\n",
        "        return l\n",
        "\n",
        "    def _sample_block_mask(self, length, acceptable_regions=None):\n",
        "        l = length\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # --\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            left = torch.randint(0, self.length - l, (1,))\n",
        "            mask = torch.zeros(self.length, dtype=torch.int32)\n",
        "            mask[left:left+l] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones(self.length, dtype=torch.int32)\n",
        "        mask_complement[left:left+l] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, B):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        p_size = self._sample_block_size(scale=self.pred_mask_scale)\n",
        "        e_size = self._sample_block_size(scale=self.enc_mask_scale)\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.length\n",
        "        min_keep_enc = self.length\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                print(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "        return collated_masks_enc, collated_masks_pred\n",
        "\n",
        "batch=64\n",
        "length=200\n",
        "mask_collator = MaskCollator(length=length, enc_mask_scale=(.85, 1.), pred_mask_scale=(.15, .2),\n",
        "        nenc=1, npred=4, min_keep=4,\n",
        "        # allow_overlap=True)\n",
        "        allow_overlap=False)\n",
        "\n",
        "collated_masks_enc, collated_masks_pred = mask_collator(batch)\n",
        "context_indices, trg_indices = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "# print(context_indices.shape, trg_indices.shape)\n",
        "\n",
        "\n",
        "# plt.pcolormesh(mask)\n",
        "b=64\n",
        "mask = torch.zeros(batch ,length)\n",
        "mask[torch.arange(batch).unsqueeze(-1), trg_indices] = 1\n",
        "mask[torch.arange(batch).unsqueeze(-1), context_indices] = .5\n",
        "mask = mask[None,...]\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "# imshow(mask)\n",
        "import torchvision\n",
        "imshow(torchvision.utils.make_grid(mask, nrow=8))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (4,4)\n",
        "# plt.plot(ttc,ttt)\n",
        "plt.scatter(ttc,ttt)\n",
        "plt.xlabel('context masks')\n",
        "plt.ylabel('target masks')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "37BYpacmIcw1",
        "outputId": "e7caba76-09a8-43b0-8e89-75fa0255123d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ttc=[]\n",
        "ttt=[]\n",
        "def mean(x): return sum(x)/len(x)\n",
        "\n",
        "for i in range(1000):\n",
        "    # collated_masks_enc, collated_masks_pred = mask_collator(1)\n",
        "    # context_indices, trg_indices = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "    # context_indices, trg_indices = simplexmask1d(seq=200, ctx_scale=(.8,1), trg_scale=(.2,.8), B=1, chaos=[3,.5])\n",
        "    context_indices, trg_indices = simplexmask1d(seq=200, ctx_scale=(.85,1), trg_scale=(.7,.8), B=1, chaos=[1,.5])\n",
        "    ttc.append(context_indices.shape[-1])\n",
        "    ttt.append(trg_indices.shape[-1])\n",
        "\n",
        "print(mean(ttc), mean(ttt))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"figure.figsize\"] = (4,4)\n",
        "plt.hist(ttc, bins=20, alpha=.5, label='context mask')\n",
        "plt.hist(ttt, bins=20, alpha=.5, label='target mask')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "58nNtUss4YFX",
        "outputId": "3a40f9ca-d158-4053-b2e8-a7ccb5c4d7a4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35.525 149.368\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-zdjdJixtOu",
        "outputId": "a17b18d2-f109-4aff-98b9-4fe239f919e7",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "38128\n",
            "torch.Size([4, 110, 16])\n"
          ]
        }
      ],
      "source": [
        "# @title TransformerModel/Predictor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, n_heads=4, d_hid=None, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, 200, d_model)*.02)\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=200, base=10000), requires_grad=False)\n",
        "\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, n_heads=n_heads) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.randn(1,1,d_model)*0.02) # randn zeros\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim)# if out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = x.shape\n",
        "        # x = x * self.pos_enc(context_indices)\n",
        "        # print(\"Trans pred\",x.shape, self.pos_emb[0,context_indices].shape)\n",
        "        x = x + self.pos_emb[0,context_indices]\n",
        "        # print('pred fwd', self.pos_emb[:,context_indices].shape)\n",
        "\n",
        "        # pred_tokens = self.cls * self.pos_enc(trg_indices) # [M, num_trg_toks, d_model]\n",
        "        pred_tokens = self.cls + self.pos_emb[0,trg_indices]\n",
        "        # print(\"pred fwd\", x.shape, pred_tokens.shape)\n",
        "        x = torch.cat([x, pred_tokens], dim=1) # [batch, seq_len+num_trg_toks, d_model]\n",
        "        out = self.transformer(x)\n",
        "\n",
        "        out = self.norm(out)\n",
        "        out = out[:,seq:] # [batch, num_trg_toks, d_model]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, drop=0):\n",
        "    def __init__(self, patch_size, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0):\n",
        "        super().__init__()\n",
        "        # patch_size=32\n",
        "        self.embed = nn.Sequential(\n",
        "            # # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.MaxPool1d(2,2), #nn.MaxPool1d(3, 2, 3//2),\n",
        "            # # nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.Conv1d(d_model, d_model,3,2,3//2)\n",
        "            nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2),\n",
        "            # nn.Conv1d(in_dim, d_model, patch_size, patch_size), # like patch\n",
        "            )\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, 200, d_model)*.02)\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=200, base=10000), requires_grad=False)#.unsqueeze(0)\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, n_heads=n_heads) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "        # try: print(\"Trans fwd\",x.shape, context_indices.shape)\n",
        "        # except: print(\"Trans fwd noind\",x.shape)\n",
        "        # x = self.pos_enc(x)\n",
        "        x = x + self.pos_emb[:,:x.shape[1]]\n",
        "        if context_indices != None: x = x[torch.arange(x.shape[0]).unsqueeze(-1), context_indices] # [batch, num_context_toks, d_model]\n",
        "\n",
        "        # print(\"TransformerModel\",x.shape)\n",
        "        x = self.transformer(x)\n",
        "        out = self.norm(x)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "batch, seq_len, d_model = 4,3500,16\n",
        "in_dim = 3\n",
        "patch_size=32\n",
        "model = TransformerModel(patch_size, in_dim, d_model, n_heads=4, nlayers=10, drop=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "# # # print(out)\n",
        "# model = TransformerPredictor(in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1).to(device)\n",
        "# out = model(out)\n",
        "# print(out.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ardu1zJwdHM3",
        "outputId": "0e1d2d7a-bed5-467d-9d1b-2c8bd82d17b1",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "109920\n",
            "torch.Size([])\n"
          ]
        }
      ],
      "source": [
        "# @title SeqJEPA\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class SeqJEPA(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, nlayers=2, n_heads=4):\n",
        "        super().__init__()\n",
        "        if out_dim is None: out_dim = d_model\n",
        "        self.patch_size = 32\n",
        "        self.student = TransformerModel(self.patch_size, in_dim, d_model, out_dim=out_dim, n_heads=n_heads, nlayers=nlayers, drop=0.)\n",
        "        self.predicter = TransformerPredictor(out_dim, d_model//2, out_dim, n_heads=4, nlayers=1, drop=0.)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "        # self.transform = RandomResizedCrop1d(3500, scale=(.8,1.))\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]\n",
        "        batch, seq, dim = x.shape\n",
        "        # target_mask = multiblock(seq//self.patch_size, min_s=.2, max_s=.3, M=4, B=1).any(1).squeeze(1) # best.2.3M4 og.15.2M4# mask out targets to be predicted # [M, seq]\n",
        "        # # target_mask = randpatch(seq//self.patch_size, mask_size=8, gamma=.9).unsqueeze(0) # 8.9 [seq]\n",
        "\n",
        "        # print(target_mask.shape, x.shape)\n",
        "        # context_mask = ~multiblock(seq//self.patch_size, min_s=.85, max_s=1, M=1, B=1).squeeze(1)|target_mask # og .85,1.M1 # [1, seq], True->Mask\n",
        "        # context_mask = torch.zeros((1,seq//self.patch_size), dtype=bool)|target_mask # [1,h,w], True->Mask\n",
        "\n",
        "        context_indices, trg_indices = simplexmask1d(seq//self.patch_size, ctx_scale=(.85,1), trg_scale=(.7,.8), B=batch, chaos=[3,.5])\n",
        "        # context_indices, trg_indices = simplexmask1d(seq//self.patch_size, ctx_scale=(.8,.9), trg_scale=(.7,.8), B=batch, chaos=[1,.5])\n",
        "        # context_indices, trg_indices = simplexmask1d(seq//self.patch_size, ctx_scale=(.8,1), trg_scale=(.2,.8), B=batch, chaos=[1,.5])\n",
        "        # print(context_indices.shape, trg_indices.shape)\n",
        "\n",
        "        # context_indices = context_indices.repeat(batch,1)\n",
        "        # trg_indices = trg_indices.repeat(batch,1)\n",
        "        # context_mask = ~context_mask|target_mask # [1,]\n",
        "        # context_indices = (~context_mask).nonzero()[:,1].unsqueeze(0).repeat(batch,1)\n",
        "        # # print(trg_indices.shape, context_indices.shape)\n",
        "        # # print(context_mask.shape,target_mask.shape, x.shape)\n",
        "        # target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "        # # # target_mask, context_mask = target_mask.repeat(batch,1), context_mask.repeat(batch,1)\n",
        "        # # x_ = x * F.adaptive_avg_pool1d((~context_mask).float(), x.shape[1]).unsqueeze(-1) # zero masked locations\n",
        "\n",
        "        # context_indices = (~context_mask).nonzero()[:,1].unflatten(0, (batch,-1)) # int idx [num_context_toks] , idx of context not masked\n",
        "        # trg_indices = target_mask.nonzero()[:,1].unflatten(0, (batch,-1)) # int idx [num_trg_toks] , idx of targets that are masked\n",
        "\n",
        "\n",
        "        # mask_collator = MaskCollator(length=seq//self.patch_size, enc_mask_scale=(.85,1), pred_mask_scale=(.15,.2), nenc=1, npred=4, min_keep=4, allow_overlap=False)\n",
        "        # collated_masks_enc, collated_masks_pred = mask_collator(batch) # idx of ctx, idx of masked trg\n",
        "        # # # collated_masks_enc, collated_masks_pred = mask_collator(1) # idx of ctx, idx of masked trg\n",
        "        # context_indices, trg_indices = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "        # # # # zero_mask = torch.zeros(batch ,seq//self.patch_size, device=device)\n",
        "        # # # # zero_mask[torch.arange(batch).unsqueeze(-1), context_indices] = 1\n",
        "        # # # zero_mask = torch.zeros(1 ,seq//self.patch_size, device=device)\n",
        "        # # # zero_mask[:, context_indices] = 1\n",
        "        # # # x_ = x * F.adaptive_avg_pool1d(zero_mask, x.shape[1]).unsqueeze(-1) # zero masked locations\n",
        "        # # # context_indices, trg_indices = context_indices.repeat(batch,1), trg_indices.repeat(batch,1)\n",
        "\n",
        "\n",
        "        # print('x_',x_.shape, context_indices.shape, trg_indices.shape)\n",
        "\n",
        "        sx = self.student(x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "        # print('seq_jepa loss sx',sx.shape)\n",
        "        sy_ = self.predicter(sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "        sy_ = F.layer_norm(sy_, (sy_.size(-1),))\n",
        "        with torch.no_grad():\n",
        "            sy = self.teacher(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "            sy = sy[torch.arange(sy.shape[0]).unsqueeze(-1), trg_indices] # [batch, num_context_toks, d_model] # nan bec len(trg_ind)==0 # print('loss sy',torch.isnan(sy).any())\n",
        "            sy = F.layer_norm(sy, (sy.size(-1),))\n",
        "        loss = F.mse_loss(sy, sy_)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        sx = self.student(x)\n",
        "        out = sx.mean(dim=1)\n",
        "        return out\n",
        "\n",
        "# min_s=0.15, max_s, M\n",
        "# trg.15.2M4 C.85 1\n",
        "\n",
        "# 1e-2,1e-3 < 3e-3,1e-3\n",
        "# patch16 < patch32\n",
        "# NoPE good but sus\n",
        "\n",
        "\n",
        "# seq_jepa = SeqJEPA(in_dim=3, d_model=32, out_dim=16, nlayers=4, n_heads=4).to(device)#.to(torch.float)\n",
        "seq_jepa = SeqJEPA(in_dim=3, d_model=64, out_dim=16, nlayers=1, n_heads=8).to(device)#.to(torch.float)\n",
        "optim = torch.optim.AdamW(seq_jepa.parameters(), lr=1e-3) # 1e-3?\n",
        "# optim = torch.optim.AdamW([{'params': seq_jepa.student.parameters()},\n",
        "#     {'params': seq_jepa.predicter.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # default 1e-2, 5e-2\n",
        "    # {'params': seq_jepa.predicter.parameters(), 'lr': 1e-2}], lr=1e-3, weight_decay=1e-2)\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/configs/in1k_vith14_ep300.yaml\n",
        "# d_model 1024,384\n",
        "# depth 12,6/12\n",
        "# wd 5e-2 - 4e-1\n",
        "# adamw 1e-4 - 1e-3 - 1e-6\n",
        "# ema 0.996-1\n",
        "\n",
        "print(sum(p.numel() for p in seq_jepa.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.parameters())) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.predicter.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.student.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in seq_jepa.teacher.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# d_model^2 * nlayers\n",
        "\n",
        "x = torch.rand((24, 3500, in_dim), device=device)\n",
        "out = seq_jepa.loss(x)\n",
        "print(out.shape)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(16, 18).to(device)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n",
        "# optim = torch.optim.AdamW([{'params': seq_jepa.parameters()}, {'params': classifier.parameters(), 'lr': 1e-3}], lr=1e-3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4rj4LfPuN1H",
        "outputId": "24fa1196-5fe9-4519-c50d-6b788393c491"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 16])\n"
          ]
        }
      ],
      "source": [
        "# @title TransformerVICReg\n",
        "import torch\n",
        "from torch import nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class TransformerVICReg(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, n_heads=4, d_hid=None, nlayers=1, drop=0):\n",
        "        super().__init__()\n",
        "        act = nn.GELU()\n",
        "        # patch_size=4\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Linear(in_dim, d_model), act\n",
        "            # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(3,2, 3//2),\n",
        "            nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            nn.Conv1d(d_model, d_model,3,2,3//2),\n",
        "            # nn.Conv1d(in_dim, d_model, patch_size, patch_size), # patch\n",
        "            )\n",
        "        self.pos_enc = RoPE(d_model, seq_len=200, base=10000) # 10000\n",
        "        # self.pos_emb = nn.Parameter(torch.randn(1, 200, d_model))\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=200, base=10000).unsqueeze(0), requires_grad=False)\n",
        "\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, n_heads=n_heads) for _ in range(nlayers)])\n",
        "\n",
        "        # out_dim = out_dim or d_model\n",
        "        self.lin = nn.Linear(d_model, out_dim)\n",
        "        self.attn_pool = nn.Linear(d_model, 1, bias=False)\n",
        "\n",
        "        dim_v = d_model * 4\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(out_dim, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v, bias=False),\n",
        "            )\n",
        "\n",
        "    def forward(self, x): # [b,t,d] / [b,c,h,w]\n",
        "        # x = self.embed(x).flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c]\n",
        "        x = self.embed(x.transpose(-2,-1)).transpose(-2,-1) # [b,t,d]\n",
        "        # x = self.embed(x)\n",
        "        x = self.pos_enc(x)\n",
        "        # x = x + self.pos_emb[:,:x.shape[1]]\n",
        "\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        attn = self.attn_pool(x).squeeze(-1) # [batch, seq] # seq_pool\n",
        "        out = (torch.softmax(attn, dim=-1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, seq] @ [batch, seq, dim] -> [batch, dim]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch, ntoken]\n",
        "\n",
        "    def expand(self, x):\n",
        "        sx = self.forward(x)\n",
        "        vx = self.exp(sx)\n",
        "        return vx\n",
        "\n",
        "batch, seq_len = 4,3500\n",
        "in_dim, d_model, out_dim=16,64,16\n",
        "model = TransformerVICReg(in_dim, d_model, out_dim, n_heads=8, nlayers=1, drop=0.).to(device)\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "# x =  torch.rand((batch, in_dim, 32,32), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObiHp-LSuRBA",
        "outputId": "c6ce8734-e0a0-47b2-b11d-e0f8e53af582"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "in vicreg  1.9939420276162247e-16 24.746832251548767 1.6621681808715039e-09\n",
            "(tensor(7.9758e-18, device='cuda:0', grad_fn=<MseLossBackward0>), tensor(0.9899, device='cuda:0', grad_fn=<AddBackward0>), tensor(1.6622e-09, device='cuda:0', grad_fn=<AddBackward0>))\n"
          ]
        }
      ],
      "source": [
        "# @title Violet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class Violet(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, nlayers=2, n_heads=4):\n",
        "        super().__init__()\n",
        "        out_dim = out_dim or d_model\n",
        "        self.student = TransformerVICReg(in_dim, d_model, out_dim=out_dim, n_heads=n_heads, nlayers=nlayers, drop=0.)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "\n",
        "        # vicreg\n",
        "        self.sim_coeff=25. # 10.0 # 25.0 # λ\n",
        "        self.std_coeff=25. # 1.0 # 25.0 # µ\n",
        "        self.cov_coeff=1. # 25.0 # 1.0 # ν\n",
        "\n",
        "        # self.transform = RandomResizedCrop1d(3500, scale=(.8,1.))\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]c/ [b,c,h,w]\n",
        "        # print(x.shape)\n",
        "        # self.transform(x)\n",
        "        vx = self.student.expand(x) # [batch, num_context_toks, out_dim]\n",
        "        with torch.no_grad(): vy = self.teacher.expand(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "        loss = self.vicreg(vx, vy)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        return self.student(x)\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size, num_features = x.shape\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = self.sim_coeff * repr_loss + self.std_coeff * std_loss + self.cov_coeff * cov_loss\n",
        "        print(\"in vicreg \",self.sim_coeff * repr_loss.item() , self.std_coeff * std_loss.item() , self.cov_coeff * cov_loss.item())\n",
        "        # return loss\n",
        "        return repr_loss, std_loss, cov_loss\n",
        "\n",
        "# lr1e-3\n",
        "# n_heads=8 < 4\n",
        "\n",
        "in_dim=3\n",
        "violet = Violet(in_dim, d_model=64, out_dim=16, nlayers=1, n_heads=8).to(device)\n",
        "voptim = torch.optim.AdamW(violet.parameters(), lr=1e-3) # 1e-3?\n",
        "# voptim = torch.optim.AdamW([{'params': violet.student.transformer.parameters()},\n",
        "#     {'params': violet.student.exp.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # default 1e-2\n",
        "# voptim = torch.optim.AdamW([{'params': violet.student.exp.parameters(), 'lr': 3e-3},\n",
        "#     {'params': [p for n, p in violet.named_parameters() if 'student.exp' not in n]}], lr=1e-3, weight_decay=1e-2) # default 1e-2\n",
        "\n",
        "# print(sum(p.numel() for p in violet.parameters() if p.requires_grad)) # 27584\n",
        "\n",
        "\n",
        "x = torch.rand((2,1000,in_dim), device=device)\n",
        "loss = violet.loss(x)\n",
        "# print(out.shape)\n",
        "print(loss)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(16, 18).to(device) # torch/autograd/graph.py RuntimeError: CUDA error: device-side assert triggered CUDA kernel errors\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IACKDymhit7"
      },
      "outputs": [],
      "source": [
        "# optim.param_groups[0]['lr'] = 1e-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "form",
        "id": "7zghfu_jeOQk"
      },
      "outputs": [],
      "source": [
        "# @title RankMe\n",
        "# RankMe: Assessing the Downstream Performance of Pretrained Self-Supervised Representations by Their Rank jun 2023 https://arxiv.org/pdf/2210.02885\n",
        "import torch\n",
        "\n",
        "# https://github.com/Spidartist/IJEPA_endoscopy/blob/main/src/helper.py#L22\n",
        "def RankMe(Z):\n",
        "    \"\"\"\n",
        "    Calculate the RankMe score (the higher, the better).\n",
        "    RankMe(Z) = exp(-sum_{k=1}^{min(N, K)} p_k * log(p_k)),\n",
        "    where p_k = sigma_k (Z) / ||sigma_k (Z)||_1 + epsilon\n",
        "    where sigma_k is the kth singular value of Z.\n",
        "    where Z is the matrix of embeddings (N × K)\n",
        "    \"\"\"\n",
        "    # compute the singular values of the embeddings\n",
        "    # _u, s, _vh = torch.linalg.svd(Z, full_matrices=False)  # s.shape = (min(N, K),)\n",
        "    # s = torch.linalg.svd(Z, full_matrices=False).S\n",
        "    s = torch.linalg.svdvals(Z)\n",
        "    p = s / torch.sum(s, axis=0) + 1e-7\n",
        "    return torch.exp(-torch.sum(p * torch.log(p)))\n",
        "\n",
        "# Z = torch.randn(5, 3)\n",
        "# o = RankMe(Z)\n",
        "# print(o)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "form",
        "id": "Pj25OrEk14pR"
      },
      "outputs": [],
      "source": [
        "# @title LiDAR stable_ssl next\n",
        "# https://github.com/rbalestr-lab/stable-ssl/blob/main/stable_ssl/monitors.py#L106\n",
        "\n",
        "def LiDAR(embeddings, eps=1e-7, delta=1e-3):\n",
        "    embeddings = embeddings.unflatten(0, (-1,2))\n",
        "    (local_n, q, d), device = embeddings.shape, embeddings.device\n",
        "\n",
        "    class_means = embeddings.mean(dim=1) # mu_x\n",
        "    grand_mean_local = class_means.mean(dim=0) # mu\n",
        "    # print(embeddings.shape, class_means.shape, grand_mean_local.shape) # [50, 2, 64], [50, 64], [64]\n",
        "\n",
        "    # local_Sb = torch.zeros(d, d, device=device)\n",
        "    # local_Sw = torch.zeros(d, d, device=device)\n",
        "\n",
        "\n",
        "    # print(diff_b.shape, diff_w.shape) # [64,1], [64,1]\n",
        "    # print(local_Sb.shape, local_Sw.shape) # [64,64], [64,64]\n",
        "\n",
        "    diff_b = (class_means - grand_mean_local).unsqueeze(-1)\n",
        "    # print(diff_b.shape)\n",
        "    # # local_Sb = (diff_b @ diff_b.T).sum()\n",
        "    local_Sb = (diff_b @ diff_b.transpose(-2,-1)).sum(0)\n",
        "    # # print(embeddings.shape, class_means.shape)\n",
        "    diff_w = (embeddings - class_means.unsqueeze(1)).reshape(-1,d,1)\n",
        "    # # print(diff_w.shape)\n",
        "    # # local_Sw = (diff_w @ diff_w.T).sum()\n",
        "    local_Sw = (diff_w @ diff_w.transpose(-2,-1)).sum(0)\n",
        "\n",
        "    # print(local_Sb.shape, local_Sw.shape)\n",
        "    S_b = local_Sb / (local_n - 1)\n",
        "    S_w = local_Sw / (local_n * (q - 1))\n",
        "    S_w += delta * torch.eye(d, device=device)\n",
        "    # print(S_w.shape, d)\n",
        "\n",
        "    eigvals_w, eigvecs_w = torch.linalg.eigh(S_w)\n",
        "    eigvals_w = torch.clamp(eigvals_w, min=eps)\n",
        "\n",
        "    invsqrt_w = (eigvecs_w * (1.0 / torch.sqrt(eigvals_w))) @ eigvecs_w.transpose(-1, -2)\n",
        "    Sigma_lidar = invsqrt_w @ S_b @ invsqrt_w\n",
        "\n",
        "    # lam, _ = torch.linalg.eigh(Sigma_lidar)\n",
        "    lam = torch.linalg.eigh(Sigma_lidar)[0]\n",
        "    # print(lam)\n",
        "    # lam = torch.clamp(lam, min=0.0)\n",
        "\n",
        "    p = lam / lam.sum() + eps\n",
        "    # print(p)\n",
        "    # p = s / torch.sum(s, axis=0) + 1e-7\n",
        "    return torch.exp(-torch.sum(p * torch.log(p)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "id": "2Nd-sGe6Ku4S",
        "outputId": "5487cace-556a-47ee-9e8e-20b316e48b4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>█▇▇▅▅▄▅▅▄▅▄▅▃▄▃▄▄▄▄▂▃▃▂▂▂▁▃▂▂▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>correct</td><td>▁▁▂▃▃▅▃▃▄▅▄▄▄▅▅▇▅▅▆▅▇▅▅▆▅▅█▇▅▅██▇▇▇█▇▇▇▆</td></tr><tr><td>lidar</td><td>▇▅▅▅▆▆▅▅▄▇▅▅▆▆▅█▇▄▆▆▆▅▄▄▄▄▃▅▃▆▃▆▅▄▅▆▁▄▆▂</td></tr><tr><td>rankme</td><td>▁▂▂▃▃▄▄▄▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇▇▇█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>1.57481</td></tr><tr><td>correct</td><td>0.54688</td></tr><tr><td>lidar</td><td>8.84105</td></tr><tr><td>rankme</td><td>12.86068</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">helpful-wave-174</strong> at: <a href='https://wandb.ai/bobdole/SeqJEPA/runs/ibnlhsel' target=\"_blank\">https://wandb.ai/bobdole/SeqJEPA/runs/ibnlhsel</a><br> View project at: <a href='https://wandb.ai/bobdole/SeqJEPA' target=\"_blank\">https://wandb.ai/bobdole/SeqJEPA</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250417_060124-ibnlhsel/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250417_072437-x65n7xbj</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/SeqJEPA/runs/x65n7xbj' target=\"_blank\">blooming-dawn-175</a></strong> to <a href='https://wandb.ai/bobdole/SeqJEPA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/SeqJEPA' target=\"_blank\">https://wandb.ai/bobdole/SeqJEPA</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/SeqJEPA/runs/x65n7xbj' target=\"_blank\">https://wandb.ai/bobdole/SeqJEPA/runs/x65n7xbj</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "!pip install -q wandb\n",
        "import wandb # https://docs.wandb.ai/quickstart\n",
        "wandb.login(key='487a2109e55dce4e13fc70681781de9f50f27be7')\n",
        "try: run.finish()\n",
        "except NameError: pass\n",
        "run = wandb.init(project=\"SeqJEPA\", config={\"model\": \"res18\",}) # violet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-eTjgAhmp_t",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title strain ctrain test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "def strain(model, dataloader, optim, scheduler=None):\n",
        "    model.train()\n",
        "    for i, (x, _) in enumerate(dataloader):\n",
        "        x = x[...,1:].to(device).to(torch.bfloat16)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            loss = model.loss(x)\n",
        "\n",
        "            # repr_loss, std_loss, cov_loss = model.loss(x)\n",
        "            # loss = model.sim_coeff * repr_loss + model.std_coeff * std_loss + model.cov_coeff * cov_loss\n",
        "\n",
        "        optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # total_norm = 0\n",
        "        # for p in model.parameters(): total_norm += p.grad.data.norm(2).item() ** 2\n",
        "        # total_norm = total_norm**.5\n",
        "        # print('total_norm', total_norm)\n",
        "        # for p in list(filter(lambda p: p.grad is not None, model.parameters())):\n",
        "        #     print(p.grad.data.norm(2).item())\n",
        "        # print(\"max grad norm\", max([p.grad.data.norm(2).item() for p in list(filter(lambda p: p.grad is not None, model.parameters()))]))\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 5) # 0.5\n",
        "\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            m=0.999 # 0.99 m = next(momentum_scheduler)\n",
        "            for param_q, param_k in zip(model.student.parameters(), model.teacher.parameters()):\n",
        "                param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        print(\"strain\",loss.item())\n",
        "        # for param in seq_jepa.context_encoder.cls: print(param.data)\n",
        "        # for param in seq_jepa.predicter.cls: print(param.data)\n",
        "        try: wandb.log({\"loss\": loss.item(), \"repr/I\": repr_loss.item(), \"std/V\": std_loss.item(), \"cov/C\": cov_loss.item()})\n",
        "        # try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=50: break\n",
        "\n",
        "\n",
        "def ctrain(model, classifier, dataloader, coptim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.eval()\n",
        "    classifier.train()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        # x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        x, y = x[...,1:].to(device).to(torch.bfloat16), y[1].to(device) # [batch, ] # (id, activity)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            with torch.no_grad():\n",
        "                sx = model(x).detach()\n",
        "            # print(sx[0][0])\n",
        "            y_ = classifier(sx)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "        # print(classifier.classifier.weight[0])\n",
        "        # print(y_.shape, y.shape)\n",
        "        # print(loss)\n",
        "        coptim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 5) # .5\n",
        "        scaler.step(coptim)\n",
        "        scaler.update()\n",
        "        print(\"classify\",loss.item())\n",
        "        try: wandb.log({\"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        # x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        x, y = x[...,1:].to(device).to(torch.float), y[1].to(device) # [batch, ] # (id, activity)\n",
        "        with torch.no_grad():\n",
        "            sx = model(x)\n",
        "            rankme = RankMe(sx).item()\n",
        "            lidar = LiDAR(sx).item()\n",
        "            y_ = classifier(sx)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        print(correct/len(y))\n",
        "        # try: wandb.log({\"correct\": correct/len(y)})\n",
        "        try: wandb.log({\"correct\": correct/len(y), \"rankme\": rankme, \"lidar\": lidar})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "for i in range(1600): #\n",
        "    # train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    # test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    np.random.shuffle(train_indices); np.random.shuffle(val_indices)\n",
        "    train_sampler, valid_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(val_indices)\n",
        "    # batch_size = 64 #512\n",
        "    train_loader = DataLoader(train_data, sampler=train_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "    test_loader = DataLoader(train_data, sampler=valid_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True)\n",
        "\n",
        "    strain(seq_jepa, train_loader, optim)\n",
        "    ctrain(seq_jepa, classifier, train_loader, coptim)\n",
        "    test(seq_jepa, classifier, test_loader)\n",
        "\n",
        "    # strain(violet, train_loader, voptim)\n",
        "    # ctrain(violet, classifier, train_loader, coptim)\n",
        "    # test(violet, classifier, test_loader)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4J2ahp3wmqcg"
      },
      "outputs": [],
      "source": [
        "# @title supervised train test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "def strain(model, classifier, dataloader, optim, coptim, scheduler=None):\n",
        "    model.train()\n",
        "    classifier.train()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        # x = x.to(device).flatten(2).transpose(-2,-1)#.to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "        # y = y.to(device)\n",
        "        x, y = x[...,1:].to(device).to(torch.bfloat16), y[1].to(device) # [batch, ] # (id, activity)\n",
        "\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        # print(loss)\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # for p in list(filter(lambda p: p.grad is not None, model.parameters())):\n",
        "        #     print(p.grad.data.norm(2).item())\n",
        "        # print(\"max grad norm\", max([p.grad.data.norm(2).item() for p in list(filter(lambda p: p.grad is not None, model.parameters()))]))\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 10) # 0.5\n",
        "\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        print(\"strain\",loss.item())\n",
        "        # for param in seq_jepa.context_encoder.cls: print(param.data)\n",
        "        # for param in seq_jepa.predicter.cls: print(param.data)\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=50: break\n",
        "\n",
        "\n",
        "# def test(model, dataloader):\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        # x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        x, y = x[...,1:].to(device).to(torch.float), y[1].to(device) # [batch, ] # (id, activity)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "\n",
        "        loss = F.cross_entropy(y_, y)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        print(correct/len(y))\n",
        "        try: wandb.log({\"correct\": correct/len(y), \"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "for i in range(1000):\n",
        "    # train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    # test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    np.random.shuffle(train_indices); np.random.shuffle(val_indices)\n",
        "    train_sampler, valid_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(val_indices)\n",
        "    # batch_size = 64 #512\n",
        "    train_loader = DataLoader(train_data, sampler=train_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "    test_loader = DataLoader(train_data, sampler=valid_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True)\n",
        "\n",
        "    # strain(seq_jepa, train_loader, optim)\n",
        "    # test(seq_jepa, test_loader)\n",
        "    strain(violet, train_loader, voptim)\n",
        "    test(violet, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JT8AgOx0E_KM",
        "outputId": "a15c1ed1-8e68-4da5-c7fa-f5e48fa2b185"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# @title save/load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder='/content/drive/MyDrive/jepa/'\n",
        "\n",
        "# # modelsd, optimsd = torch.load(folder+'SeqJEPA.pkl', map_location=device).values()\n",
        "# modelsd, optimsd = torch.load('SeqJEPA.pkl', map_location=device).values()\n",
        "# seq_jepa.load_state_dict(modelsd, strict=False)\n",
        "# optim.load_state_dict(optimsd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpbLPvjiE-pD"
      },
      "outputs": [],
      "source": [
        "checkpoint = {'model': seq_jepa.state_dict(), 'optimizer': optim.state_dict()}\n",
        "torch.save(checkpoint, folder+'SeqJEPA.pkl')\n",
        "torch.save(checkpoint, 'SeqJEPA.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vScvFGGSeN6"
      },
      "source": [
        "## drawer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ge36SCxOl2Oq"
      },
      "outputs": [],
      "source": [
        "# @title RoPE\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def RoPE(dim, seq_len=512, base=10000):\n",
        "    theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "    pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "    angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "    rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [1, seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "    return rot_emb\n",
        "\n",
        "# class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "#     def __init__(self, dim, seq_len=512, base=10000):\n",
        "#         super().__init__()\n",
        "#         self.dim, self.base = dim, base\n",
        "#         theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "#         pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "#         angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "#         self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         seq_len = x.size(1)\n",
        "#         if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.base)\n",
        "#         return x * self.rot_emb[:seq_len]\n",
        "\n",
        "class RotEmb(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, top=torch.pi, base=10000):\n",
        "        super().__init__()\n",
        "        self.theta = top / (base ** (torch.arange(0, dim, step=2, device=device) / dim))\n",
        "        # self.theta = top / (base ** torch.linspace(0, 1, dim//2, device=device))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (pos.unsqueeze(-1) * self.theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [seq_len, dim]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pQfM2fYvcTh6"
      },
      "outputs": [],
      "source": [
        "# @title masks\n",
        "import torch\n",
        "\n",
        "def multiblock(seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "    return target_mask\n",
        "\n",
        "def multiblock(seq, min_s, max_s, B=64, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    # mask_len = torch.rand(B, 1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_len = torch.rand(1, M) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(B, M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    # indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    indices = torch.arange(seq)[None,None,...] # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [B, M, seq]\n",
        "    return target_mask\n",
        "\n",
        "def multiblock2d(hw=(8,8), scale=(.15,.2), aspect_ratio=(.75,1.5), M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_aspect = torch.rand(1) * (aspect_ratio[1] - aspect_ratio[0]) + aspect_ratio[0] # in (min_s, max_s) # all blocks same size\n",
        "    mask_scale = torch.rand(1) * (scale[1] - scale[0]) + scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    h = (mask_scale/mask_aspect)**.5# h*(h*aspect) = scale\n",
        "    w = h * mask_aspect\n",
        "    h_pos, w_pos = torch.rand(M)*(1-w), torch.rand(M)*(1-h) # in (0, 1 - mask_len)\n",
        "    h_len, h_pos = (h*hw[0]).int(), h_pos*hw[0]\n",
        "    w_len, w_pos = (w*hw[1]).int(), w_pos*hw[1]\n",
        "    h_ind, w_ind = torch.arange(hw[0]).unsqueeze(0), torch.arange(hw[1]).unsqueeze(0) # [1, seq]\n",
        "    h_mask = (h_ind>=h_pos.unsqueeze(-1)) & (h_ind<(h_pos+h_len).unsqueeze(-1)) # [M, seq]\n",
        "    w_mask = (w_ind>=w_pos.unsqueeze(-1)) & (w_ind<(w_pos+w_len).unsqueeze(-1)) # [M, seq]\n",
        "    target_mask = h_mask.unsqueeze(-1) & w_mask.unsqueeze(-2) # [M, seq, seq]\n",
        "    return target_mask\n",
        "\n",
        "def multiblock2d(hw=(8,8), scale=(.15,.2), aspect_ratio=(.75,1.5), B=64, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_aspect = torch.rand(B, 1) * (aspect_ratio[1] - aspect_ratio[0]) + aspect_ratio[0] # in (min_s, max_s) # all blocks same size\n",
        "    mask_scale = torch.rand(B, 1) * (scale[1] - scale[0]) + scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    h = (mask_scale/mask_aspect)**.5# h*(h*aspect) = scale\n",
        "    w = h * mask_aspect\n",
        "    h_pos, w_pos = torch.rand(M)*(1-w), torch.rand(M)*(1-h) # in (0, 1 - mask_len)\n",
        "    h_len, h_pos = (h*hw[0]).int(), h_pos*hw[0]\n",
        "    w_len, w_pos = (w*hw[1]).int(), w_pos*hw[1]\n",
        "    h_ind, w_ind = torch.arange(hw[0]).unsqueeze(0), torch.arange(hw[1]).unsqueeze(0) # [1, seq]\n",
        "    h_mask = (h_ind>=h_pos.unsqueeze(-1)) & (h_ind<(h_pos+h_len).unsqueeze(-1)) # [M, seq]\n",
        "    w_mask = (w_ind>=w_pos.unsqueeze(-1)) & (w_ind<(w_pos+w_len).unsqueeze(-1)) # [M, seq]\n",
        "    target_mask = h_mask.unsqueeze(-1) & w_mask.unsqueeze(-2) # [M, seq, seq]\n",
        "    return target_mask\n",
        "\n",
        "# https://arxiv.org/pdf/2210.07224\n",
        "def randpatch(seq, mask_size=8, gamma=0.9): # num patches of seq, mask patch size, masking ratio\n",
        "    # mask = torch.rand(seq//mask_size)<gamma\n",
        "    length = seq//mask_size\n",
        "    g = torch.normal(gamma, std=.1, size=(1,)).clamp(.5,.9)\n",
        "    # g = gamma\n",
        "    idx = torch.randperm(length)[:int(length*g)]\n",
        "    mask = torch.zeros(length, dtype=bool)\n",
        "    mask[idx] = True\n",
        "    mask = mask.repeat_interleave(mask_size, dim=-1)\n",
        "    return mask # [seq] , True -> mask\n",
        "\n",
        "\n",
        "import torch\n",
        "def apply_masks(x, mask): # [b,t,d], [mask_size] # https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "    mask_keep = mask.unsqueeze(-1).repeat(x.size(0), 1, x.size(-1)) # [batch,T,dim]\n",
        "    return torch.gather(x, dim=1, index=mask_keep) # [batch,mask_size,dim]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title simplex\n",
        "# !pip install -q opensimplex\n",
        "import opensimplex\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def simplexmask(hw=(8,8), scale=(.15,.2)):\n",
        "    ix = iy = np.linspace(0, 1, num=8)\n",
        "    ix, iy = ix+np.random.randint(1e10), iy+np.random.randint(1e10)\n",
        "    y=opensimplex.noise2array(ix, iy)\n",
        "    y = torch.from_numpy(y)\n",
        "    mask_scale = torch.rand(1) * (scale[1] - scale[0]) + scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    yy = y.flatten().sort()[0][int(hw[0]*hw[1]*mask_scale)]\n",
        "    mask = (y<=yy.item())\n",
        "    return mask # T/F [h,w]\n",
        "\n",
        "def simplexmask1d(seq=512, scale=(.15,.2), chaos=2):\n",
        "    i = np.linspace(0, chaos, num=seq) # 2\n",
        "    j = np.random.randint(1e10, size=1)\n",
        "    y=opensimplex.noise2array(i, j) # [1, seq]\n",
        "    # plt.pcolormesh(y)\n",
        "    # plt.show()\n",
        "    y = torch.from_numpy(y)\n",
        "    # print(y.shape)\n",
        "    mask_scale = torch.rand(1) * (scale[1] - scale[0]) + scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    # print(a.shape, int(seq*mask_scale))\n",
        "    val, ind = y.sort()\n",
        "    yy = val[:,int(seq*mask_scale)]\n",
        "    mask = (y<=yy.item())\n",
        "    index = ind[:,:int(seq*mask_scale)]\n",
        "    return index, mask # T/F [h,w]\n",
        "\n",
        "def simplexmask1d(seq=512, ctx_scale=(.85,1), trg_scale=(.6,.8), B=64, chaos=2):\n",
        "    i = np.linspace(0, chaos, num=seq) # 2-5\n",
        "    j = np.random.randint(1e10, size=B)\n",
        "    noise = opensimplex.noise2array(i, j) # [B, seq]\n",
        "    # plt.pcolormesh(noise[:1])\n",
        "    # plt.rcParams[\"figure.figsize\"] = (20,3)\n",
        "    # plt.show()\n",
        "    noise = torch.from_numpy(noise)\n",
        "    ctx_mask_scale = torch.rand(1) * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    trg_mask_scale = torch.rand(1) * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "    val, ind = noise.sort()\n",
        "    trg_index = ind[:,-int(seq*trg_mask_scale):]\n",
        "    ctx_index = ind[:,-int(seq*ctx_mask_scale):-int(seq*trg_mask_scale)] # ctx wraps trg ; most similar to multiblock\n",
        "    # ctx_index = ind[:,:int(seq*ctx_mask_scale)-int(seq*trg_mask_scale)] # ctx hug bottom\n",
        "    # ctx_index = ind[:,-int(seq*ctx_mask_scale)-int(seq*trg_mask_scale):-int(seq*trg_mask_scale)] # ctx wraps trg ; most similar to multiblock\n",
        "    # ctx_index = ind[:,:int(seq*ctx_mask_scale)] # ctx hug bottom\n",
        "    return ctx_index, trg_index\n",
        "\n",
        "\n",
        "# mask = torch.zeros(1 ,200)\n",
        "# mask[:, trg_index[:1]] = 1\n",
        "# mask[:, ctx_index[:1]] = .5\n",
        "# plt.rcParams[\"figure.figsize\"] = (20,1)\n",
        "# plt.pcolormesh(mask)\n",
        "# plt.show()\n",
        "\n",
        "def simplexmask1d(seq=512, ctx_scale=(.85,1), trg_scale=(.6,.8), B=64, chaos=[1,.5]):\n",
        "    i = np.linspace(0, chaos[0], num=seq) # 2-5\n",
        "    noise = opensimplex.noise2array(i, np.random.randint(1e10, size=B)) # [B, seq]\n",
        "    noise = torch.from_numpy(noise)\n",
        "    ctx_mask_scale = torch.rand(1) * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    trg_mask_scale = torch.rand(1) * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "\n",
        "    ctx_len, trg_len = int(seq*ctx_mask_scale), int(seq*trg_mask_scale)\n",
        "    val, trg_index = torch.topk(noise, trg_len, dim=1, sorted=False)\n",
        "    ctx_len = ctx_len - trg_len\n",
        "\n",
        "    remove_mask = torch.ones((B,seq), dtype=bool) # [B, S]\n",
        "    remove_mask.scatter_(1, trg_index, False).flatten()\n",
        "    ind = torch.arange(seq).unsqueeze(0).repeat(B,1)[remove_mask].reshape(B, -1)\n",
        "    print(ind.shape, ind)\n",
        "\n",
        "    i = np.linspace(0, chaos[1], num=seq) # 2-5\n",
        "    noise = opensimplex.noise2array(i, np.random.randint(1e10, size=B)) # [B, seq]\n",
        "    noise = torch.from_numpy(noise)[remove_mask].reshape(B, -1)\n",
        "    val, ctx_ind = torch.topk(noise, ctx_len, dim=1, sorted=False)\n",
        "    print(ctx_ind.shape, ctx_ind)\n",
        "\n",
        "\n",
        "    ctx_index = ind[torch.arange(B).unsqueeze(-1), ctx_ind]\n",
        "    return ctx_index, trg_index\n",
        "\n",
        "\n",
        "# mask = simplexmask(hw=(8,8), scale=(.6,.8))\n",
        "# index, mask = simplexmask1d(seq=100, scale=(.7,.8))\n",
        "# ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.85,1), trg_scale=(.6,.8), B=64, chaos=5)\n",
        "# ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.85,1), trg_scale=(.7,.8), B=64, chaos=2)\n",
        "# ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.7,.9), trg_scale=(.6,.7), B=64, chaos=5)\n",
        "# ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.2,.3), trg_scale=(.4,.5), B=64, chaos=3)\n",
        "# ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.1,.3), trg_scale=(.4,.6), B=64, chaos=3)\n",
        "# # print(trg_index[0], ctx_index[0])\n",
        "\n",
        "ctx_index, trg_index = simplexmask1d(seq=200, ctx_scale=(.85,1), trg_scale=(.7,.8), B=64, chaos=[3,.5])\n",
        "mask = torch.zeros(b ,200)\n",
        "mask[torch.arange(b).unsqueeze(-1), trg_index] = 1\n",
        "mask[torch.arange(b).unsqueeze(-1), ctx_index] = .5\n",
        "mask = mask[None,...]\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "imshow(mask)\n",
        "import torchvision\n",
        "imshow(torchvision.utils.make_grid(mask, nrow=8))\n",
        "\n",
        "# print(index)\n",
        "# print(index.shape)\n",
        "# print(mask)\n",
        "# print(mask.shape)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "9rPxvrrsYI_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwpnHW4wn9S1",
        "outputId": "40413ecd-7f4b-49a2-984b-fe812ec57e26"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:08<00:00, 19.5MB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title data\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "transform = transforms.Compose([transforms.ToTensor(),])\n",
        "\n",
        "# MNIST CIFAR10\n",
        "train_data = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
        "test_data = torchvision.datasets.CIFAR10(root='data', train=False, download=True, transform=transform)\n",
        "batch_size = 128 # 128 1024\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    # print(npimg.shape) # (3, 64, 64)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# dataiter = iter(train_data)\n",
        "# x,y = next(dataiter)\n",
        "# print(x.shape) # [3, 32, 32]\n",
        "# imshow(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AtPUcVu2kSLI"
      },
      "outputs": [],
      "source": [
        "# @title TransformerClassifier\n",
        "import math\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class TransformerClassifier(nn.Module):\n",
        "    # def __init__(self, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, drop = 0.):\n",
        "    def __init__(self, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, drop=0):\n",
        "        super().__init__()\n",
        "        # self.embedding = nn.Embedding(ntoken, d_model)\n",
        "        self.pos_encoder = RoPE(d_model, seq_len=200, base=10000)\n",
        "        # d_hid = d_hid or d_model#*2\n",
        "        # encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, d_hid, drop, batch_first=True)\n",
        "        # self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "\n",
        "        out_dim = out_dim or d_model\n",
        "        self.lin = nn.Linear(d_model, out_dim)\n",
        "        # self.lin = nn.Linear(d_model*2, out_dim)\n",
        "        # self.cls = nn.Parameter(torch.randn(1,1,d_model))\n",
        "        self.attention_pool = nn.Linear(d_model, 1, bias=False)\n",
        "        # self.out = nn.Sequential(\n",
        "        #     nn.Dropout(drop), nn.Linear(d_model, 3), nn.SiLU(),\n",
        "        #     nn.Dropout(drop), nn.Linear(3, out_dim), nn.Sigmoid()\n",
        "        # )\n",
        "\n",
        "    def forward(self, x, src_key_padding_mask = None): # [batch, seq, d_model], [batch, seq] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # batch, seq_len, d_model = x.shape\n",
        "        # x = torch.cat([self.cls.repeat(batch,1,1), x], dim=1)\n",
        "        # src_key_padding_mask = torch.cat([torch.zeros((batch, 1), dtype=torch.bool), src_key_padding_mask], dim=1)\n",
        "        x = self.pos_encoder(x)\n",
        "\n",
        "        x = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
        "        # print(\"fwd\",out.shape) # float [batch, seq_len, d_model]\n",
        "        # out = x.mean(dim=1) # average pool\n",
        "        # out = x.max(dim=1)#[0]\n",
        "\n",
        "        attn = self.attention_pool(x).squeeze(-1) # [batch, seq] # seq_pool\n",
        "        out = (torch.softmax(attn, dim=-1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, seq] @ [batch, seq, dim] -> [batch, dim]\n",
        "        # out = self.out(out) # optional (from GlobalContext) like squeeze excitation\n",
        "\n",
        "        # out = out[:, 0] # first token\n",
        "        # out = torch.cat([out, mean_pool], dim=-1)\n",
        "\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "batch, seq_len, d_model = 4,7,512\n",
        "model = TransformerModel(in_dim, d_model, d_head=4, nlayers=2, dropout=0.).to(device)\n",
        "# model = TransformerClassifier(d_model, nhead=8, nlayers=2, dropout=0.).to(device)\n",
        "x =  torch.rand(batch, seq_len, d_model)\n",
        "src_key_padding_mask = torch.stack([(torch.arange(seq_len) < seq_len - v) for v in torch.randint(seq_len, (batch,))]) # True will be ignored # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "print(src_key_padding_mask)\n",
        "out = model(x, src_key_padding_mask)\n",
        "print(out.shape)\n",
        "\n",
        "# GlobalAveragePooling1D layer, followed by a Dense layer. The final output of the transformer is produced by a softmax layer,\n",
        "# x = GlobalAveragePooling1D()(x)\n",
        "# x = Dropout(0.1)(x)\n",
        "# x = Dense(20, activation=\"relu\")(x)\n",
        "# x = Dropout(0.1)(x)\n",
        "# outputs = Dense(2, activation=\"softmax\")(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-8i1WLsvH_vn"
      },
      "outputs": [],
      "source": [
        "# @title Transformer Model\n",
        "import torch\n",
        "from torch import nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        # self.embed = nn.Sequential(\n",
        "        #     nn.Linear(in_dim, d_model), #act,\n",
        "        #     # nn.Linear(d_model, d_model), act,\n",
        "        # )\n",
        "        self.embed = nn.Linear(in_dim, d_model) if in_dim != d_model else None\n",
        "        # self.embed = nn.Sequential(nn.Conv1d(in_dim,d_model,7,2,7//2), nn.MaxPool1d(3, 2, 3//2))\n",
        "        self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, d_hid or d_model, dropout, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.d_model = d_model\n",
        "        self.cls = nn.Parameter(torch.zeros(1,1,d_model)) # randn\n",
        "        out_dim = out_dim or d_model\n",
        "        # self.lin = nn.Linear(d_model, out_dim)\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim != d_model else None\n",
        "        self.norm = nn.LayerNorm(out_dim)\n",
        "\n",
        "\n",
        "    def forward(self, src, src_key_padding_mask=None, cls_mask=None, context_indices=None, trg_indices=None): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # if cls_mask != None: src[cls_mask] = self.cls.to(src.dtype)\n",
        "\n",
        "        if self.embed != None: src = self.embed(src) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = src.shape\n",
        "        # src = self.pos_encoder(src)\n",
        "        if context_indices != None:\n",
        "            # print(src.shape, context_indices.shape, self.pos_encoder(context_indices).shape) # [2, 88, 32]) torch.Size([88]) torch.Size([88, 32]\n",
        "            # print(src[0], self.pos_encoder(context_indices)[0])\n",
        "            src = src * self.pos_encoder(context_indices) # context/predictor # src = src + self.positional_emb[:,context_indices]\n",
        "            # print(src[0])\n",
        "        else: src = src * self.pos_encoder(torch.arange(seq, device=device)) # target # src = src + self.positional_emb[:,:seq]\n",
        "            # print(\"trans fwd\", src.shape, self.pos_encoder(src).shape)\n",
        "\n",
        "        if trg_indices != None: # [M, num_trg_toks]\n",
        "            pred_tokens = self.cls * self.pos_encoder(trg_indices) # [M, num_trg_toks, d_model] # pred_tokens = self.cls + self.positional_emb[0,trg_indices]\n",
        "            pred_tokens = pred_tokens.repeat(batch, 1, 1) # [batch*M, num_trg_toks, d_model]\n",
        "            # print(pred_tokens.requires_grad)\n",
        "            src = src.repeat_interleave(trg_indices.shape[0], dim=0) # [batch, seq_len, d_model] -> [batch*M, seq_len, d_model]\n",
        "            src = torch.cat([src, pred_tokens], dim=1) # [batch*M, seq_len+num_trg_toks, d_model]\n",
        "\n",
        "        out = self.transformer_encoder(src, src_key_padding_mask=src_key_padding_mask) # float [seq_len, batch_size, d_model]\n",
        "        if trg_indices != None:\n",
        "            # print(out.shape)\n",
        "            out = out[:,seq:] # [batch*M, num_trg_toks, d_model]\n",
        "        if self.lin != None: out = self.lin(out)\n",
        "        out = self.norm(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "batch, seq_len, d_model = 4,7,64\n",
        "in_dim = 3\n",
        "model = TransformerModel(in_dim, d_model, nhead=8, nlayers=2, dropout=0.).to(device)\n",
        "x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "# print(out.shape)\n",
        "# # print(out)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "0vScvFGGSeN6",
        "8DxjYI9RMQoP"
      ],
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}