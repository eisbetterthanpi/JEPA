{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/I_JEPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxACli7GdyGq",
        "outputId": "b7b853be-3700-490f-9e91-552d7d550bfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 43.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title data\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "transform = transforms.Compose([transforms.ToTensor(),])\n",
        "# transform = transforms.Compose([transforms.RandomResizedCrop((32,32), scale=(.3,1.)), transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "# CIFAR10: 60000 32x32 color images in 10 classes, with 6000 images per class\n",
        "train_data = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
        "test_data = torchvision.datasets.CIFAR10(root='data', train=False, download=True, transform=transform)\n",
        "batch_size = 64 # 4\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# dataiter = iter(train_loader) # get some random training images\n",
        "# images, labels = next(dataiter)\n",
        "# print(images.shape) # [batch, 3, 32, 32]\n",
        "# imshow(torchvision.utils.make_grid(images))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3EO3PgMPH1x"
      },
      "source": [
        "## hiera"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "j3-vvMS1-gVn"
      },
      "outputs": [],
      "source": [
        "# @title ResBlock\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters(): p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, kernel=3):\n",
        "        super().__init__()\n",
        "        out_ch = out_ch or in_ch\n",
        "        act = nn.SiLU() #\n",
        "        self.res_conv = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
        "        # self.block = nn.Sequential( # best?\n",
        "        #     nn.Conv2d(in_ch, out_ch, 3, padding=1), nn.BatchNorm2d(out_ch), act,\n",
        "        #     zero_module(nn.Conv2d(out_ch, out_ch, 3, padding=1)), nn.BatchNorm2d(out_ch), act,\n",
        "        #     )\n",
        "        self.block = nn.Sequential(\n",
        "            nn.BatchNorm2d(in_ch), act, nn.Conv2d(in_ch, out_ch, kernel, padding=kernel//2),\n",
        "            nn.BatchNorm2d(out_ch), act, zero_module(nn.Conv2d(out_ch, out_ch, kernel, padding=kernel//2)),\n",
        "            )\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w]\n",
        "        return self.block(x) + self.res_conv(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0lclc9myo2c",
        "outputId": "35ea2b51-c2dc-4d96-f2ae-c27bd760dcd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([12, 2, 16, 32, 32])\n"
          ]
        }
      ],
      "source": [
        "# @title UpDownBlock_me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class PixelShuffleConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, kernel=1, r=1):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        r = max(r, int(1/r))\n",
        "        out_ch = out_ch or in_ch\n",
        "        if self.r>1: self.net = nn.Sequential(ResBlock(in_ch, out_ch*r**2, kernel), nn.PixelShuffle(r))\n",
        "        # if self.r>1: self.net = nn.Sequential(Attention(in_ch, out_ch*r**2), nn.PixelShuffle(r))\n",
        "\n",
        "        elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), ResBlock(in_ch*r**2, out_ch, kernel))\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), Attention(in_ch*r**2, out_ch))\n",
        "        elif in_ch != out_ch: self.net = ResBlock(in_ch*r**2, out_ch, kernel)\n",
        "        else: self.net = lambda x: torch.zeros_like(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def AdaptiveAvgPool_nd(n, *args, **kwargs): return [nn.Identity, nn.AdaptiveAvgPool1d, nn.AdaptiveAvgPool2d, nn.AdaptiveAvgPool3d][n](*args, **kwargs)\n",
        "def AdaptiveMaxPool_nd(n, *args, **kwargs): return [nn.Identity, nn.AdaptiveMaxPool1d, nn.AdaptiveMaxPool2d, nn.AdaptiveMaxPool3d][n](*args, **kwargs)\n",
        "\n",
        "def adaptive_avg_pool_nd(n, x, output_size): return [nn.Identity, F.adaptive_avg_pool1d, F.adaptive_avg_pool2d, F.adaptive_avg_pool3d][n](x, output_size)\n",
        "def adaptive_max_pool_nd(n, x, output_size): return [nn.Identity, F.adaptive_max_pool1d, F.adaptive_max_pool2d, F.adaptive_max_pool3d][n](x, output_size)\n",
        "\n",
        "class AdaptivePool_at(nn.AdaptiveAvgPool1d): # AdaptiveAvgPool1d AdaptiveMaxPool1d\n",
        "    def __init__(self, dim=1, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.dim=dim\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(self.dim,-1)\n",
        "        shape = x.shape\n",
        "        return super().forward(x.flatten(0,-2)).unflatten(0, shape[:-1]).transpose(self.dim,-1)\n",
        "\n",
        "\n",
        "def adaptive_pool_at(x, dim, output_size, pool='avg'): # [b,c,h,w]\n",
        "    x = x.transpose(dim,-1)\n",
        "    shape = x.shape\n",
        "    parent={'avg':F.adaptive_avg_pool1d, 'max':F.adaptive_max_pool1d}[pool]\n",
        "    return parent(x.flatten(0,-2), output_size).unflatten(0, shape[:-1]).transpose(dim,-1)\n",
        "\n",
        "\n",
        "class ZeroExtend():\n",
        "    def __init__(self, dim=1, output_size=16):\n",
        "        self.dim, self.out = dim, output_size\n",
        "    def __call__(self, x): # [b,c,h,w]\n",
        "        return torch.cat((x, torch.zeros(*x.shape[:self.dim], self.out - x.shape[self.dim], *x.shape[self.dim+1:])), dim=self.dim)\n",
        "\n",
        "def make_pool_at(pool='avg', dim=1, output_size=5):\n",
        "    parent={'avg':nn.AdaptiveAvgPool1d, 'max':nn.AdaptiveMaxPool1d}[pool]\n",
        "    class AdaptivePool_at(parent): # AdaptiveAvgPool1d AdaptiveMaxPool1d\n",
        "        def __init__(self, dim=1, *args, **kwargs):\n",
        "            super().__init__(*args, **kwargs)\n",
        "            self.dim=dim\n",
        "        def forward(self, x):\n",
        "            x = x.transpose(self.dim,-1)\n",
        "            shape = x.shape\n",
        "            return super().forward(x.flatten(0,-2)).unflatten(0, shape[:-1]).transpose(self.dim,-1)\n",
        "    return AdaptivePool_at(dim, output_size=output_size)\n",
        "\n",
        "class Shortcut():\n",
        "    def __init__(self, dim=1, c=3, sp=(3,3), nd=2):\n",
        "        self.dim = dim\n",
        "        # self.ch_pool = make_pool_at(pool='avg', dim=dim, output_size=c)\n",
        "        self.ch_pool = make_pool_at(pool='max', dim=dim, output_size=c)\n",
        "        # self.ch_pool = ZeroExtend(dim, output_size=c) # only for out_dim>=in_dim\n",
        "        # self.sp_pool = AdaptiveAvgPool_nd(nd, sp)\n",
        "        self.sp_pool = AdaptiveMaxPool_nd(nd, sp)\n",
        "\n",
        "    def __call__(self, x): # [b,c,h,w]\n",
        "        x = self.sp_pool(x) # spatial first preserves spatial more?\n",
        "        x = self.ch_pool(x)\n",
        "        return x\n",
        "\n",
        "class UpDownBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, kernel=7, r=1):\n",
        "        super().__init__()\n",
        "        act = nn.SiLU()\n",
        "        self.r = r\n",
        "        self.block = PixelShuffleConv(in_ch, out_ch, kernel=kernel, r=r)\n",
        "        # self.block = nn.Sequential(\n",
        "        #     nn.BatchNorm2d(in_ch), act, PixelShuffleConv(in_ch, out_ch, kernel=kernel, r=r)\n",
        "        # )\n",
        "        # if self.r>1: self.res_conv = nn.Sequential(nn.ConvTranspose2d(in_ch, out_ch, kernel, 2, kernel//2, output_padding=1))\n",
        "        # if self.r>1: self.res_conv = nn.Sequential(nn.Upsample(scale_factor=2, mode='nearest'), nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity())\n",
        "        # if self.r>1: self.res_conv = nn.Sequential(nn.Upsample(scale_factor=2, mode='bilinear'), nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity())\n",
        "        # if self.r>1: self.res_conv = nn.Sequential(nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True), nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity())\n",
        "\n",
        "\n",
        "        # elif self.r<1: self.res_conv = nn.Sequential(nn.Conv2d(in_ch, out_ch, kernel, 2, kernel//2))\n",
        "        # elif self.r<1: self.res_conv = nn.Sequential(nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity(), nn.MaxPool2d(2,2))\n",
        "        # elif self.r<1: self.res_conv = nn.Sequential(nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity(), nn.AvgPool2d(2,2))\n",
        "        # elif self.r<1: self.res_conv = AttentionBlock(in_ch, out_ch, n_heads=4, q_stride=(2,2))\n",
        "\n",
        "        # else: self.res_conv = nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity()\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w]\n",
        "        b, num_tok, c, *win = x.shape\n",
        "        x = x.flatten(0,1)\n",
        "        out = self.block(x)\n",
        "        # # shortcut = F.interpolate(x.unsqueeze(1), size=out.shape[1:], mode='nearest-exact').squeeze(1) # pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html\n",
        "        # shortcut = F.adaptive_avg_pool3d(x, out.shape[1:]) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "        # # shortcut = F.adaptive_max_pool3d(x, out.shape[1:]) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "        # # shortcut = F.adaptive_avg_pool3d(x, out.shape[1:]) if out.shape[1]>=x.shape[1] else F.adaptive_max_pool3d(x, out.shape[1:])\n",
        "        # shortcut(x)\n",
        "        shortcut = Shortcut(dim=1, c=out.shape[1], sp=out.shape[-2:], nd=2)(x)\n",
        "        out = out + shortcut\n",
        "        out = out.unflatten(0, (b, num_tok))\n",
        "        return out\n",
        "\n",
        "        # return out + shortcut + self.res_conv(x)\n",
        "        # return out + self.res_conv(x)\n",
        "        # return self.res_conv(x)\n",
        "\n",
        "# if out>in, inter=max=ave=near.\n",
        "# if out<in, inter=ave. max=max\n",
        "\n",
        "# stride2\n",
        "# interconv/convpool\n",
        "# pixelconv\n",
        "# pixeluib\n",
        "# pixelres\n",
        "# shortcut\n",
        "\n",
        "# in_ch, out_ch = 16,3\n",
        "in_ch, out_ch = 3,16\n",
        "model = UpDownBlock(in_ch, out_ch, r=1/2).to(device)\n",
        "# model = UpDownBlock(in_ch, out_ch, r=2).to(device)\n",
        "\n",
        "x = torch.rand(12, in_ch, 64,64, device=device)\n",
        "x = torch.rand(12, 2, in_ch, 64,64, device=device)\n",
        "out = model(x)\n",
        "\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAyKHKivc0j7",
        "outputId": "e66e47ef-f7e7-44c1-f15f-168e0adf1807"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MUattn 256\n",
            "torch.Size([2, 3, 16, 16, 16])\n"
          ]
        }
      ],
      "source": [
        "# @title maxpool path bchw\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def conv_nd(n, *args, **kwargs): return [nn.Identity, nn.Conv1d, nn.Conv2d, nn.Conv3d][n](*args, **kwargs)\n",
        "def maxpool_nd(n, *args, **kwargs): return [nn.Identity, nn.MaxPool1d, nn.MaxPool2d, nn.MaxPool3d][n](*args, **kwargs)\n",
        "def avgpool_nd(n, *args, **kwargs): return [nn.Identity, nn.AvgPool1d, nn.AvgPool2d, nn.AvgPool3d][n](*args, **kwargs)\n",
        "\n",
        "import math\n",
        "class MaskUnitAttention(nn.Module):\n",
        "    # def __init__(self, d_model=16, n_heads=4, q_stride=None, nd=2):\n",
        "    def __init__(self, in_dim, d_model=16, n_heads=4, q_stride=None, nd=2):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads, self.d_head = n_heads, d_model // n_heads\n",
        "        self.scale = self.d_head**-.5\n",
        "        # self.qkv = conv_nd(nd, d_model, 3*d_model, 1, bias=False)\n",
        "        self.qkv = conv_nd(nd, in_dim, 3*d_model, 1, bias=False)\n",
        "        # self.out = conv_nd(nd, d_model, d_model, 1)\n",
        "        self.out = nn.Linear(d_model, d_model, 1)\n",
        "        self.q_stride = q_stride # If greater than 1, pool q with this stride. The stride should be flattened (e.g., 2x2 = 4).\n",
        "        if q_stride:\n",
        "            self.q_stride = (q_stride,)*nd if type(q_stride)==int else q_stride\n",
        "            self.q_pool = maxpool_nd(nd, self.q_stride, stride=self.q_stride)\n",
        "\n",
        "    def forward(self, x): # [b,num_tok,c,win1,win2]\n",
        "        b, num_tok, c, *win = x.shape\n",
        "        x = x.flatten(0,1) # [b*num_tok,c,win1,win2]\n",
        "        # print(x.shape)\n",
        "        # q, k, v = self.qkv(x).reshape(b, 3, self.n_heads, self.d_head, num_tok, win*win).permute(1,0,2,4,5,3) # [b,3*d_model,num_tok*win,win] -> 3* [b, n_heads, num_tok, win*win, d_head]\n",
        "        # self.qkv(x).flatten(1,-2).unflatten(-1, (self.heads,-1)).chunk(3, dim=-1) # [b,sp,n_heads,d_head]\n",
        "        q,k,v = self.qkv(x).chunk(3, dim=1) # [b*num_tok,d,win,win]\n",
        "        if self.q_stride:\n",
        "            q = self.q_pool(q)\n",
        "            win=[w//s for w,s in zip(win, self.q_stride)] # win = win/q_stride\n",
        "        if math.prod(win) >= 200:\n",
        "            print('MUattn', math.prod(win))\n",
        "            q, k, v = map(lambda t: t.reshape(b*num_tok, self.n_heads, self.d_head, -1).transpose(-2,-1), (q,k,v)) # [b*num_tok, n_heads, win*win, d_head]\n",
        "        else:\n",
        "            # q, k, v = map(lambda t: t.reshape(b, num_tok, self.n_heads, self.d_head, -1).permute(0,2,4,1,3), (q,k,v)) # downsampling attention # [b, n_heads, win*win, num_tok, d_head]\n",
        "            q, k, v = map(lambda t: t.reshape(b, num_tok, self.n_heads, self.d_head, -1).permute(0,2,1,4,3).flatten(2,3), (q,k,v)) # [b, n_heads, num_tok*win*win, d_head]\n",
        "\n",
        "        # x = F.scaled_dot_product_attention(q,k,v, attn_mask=None) # https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html\n",
        "        q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2) # [b, n_heads, t(/pool), d_head]\n",
        "        context = k.transpose(-2,-1) @ v # [b, n_heads, d_head, d_head]\n",
        "        x = q @ context # [b, n_heads, t(/pool), d_head]\n",
        "\n",
        "        # print('attn fwd 2',x.shape)\n",
        "        # x = x.transpose(1, 3).reshape(B, -1, self.d_model)\n",
        "        x = x.transpose(1,2).reshape(b, -1, self.d_model) # [b,t,d]\n",
        "        # x = x.transpose(-2,-1).reshape(x.shape[0], self.d_model, ) # [b,t,d]\n",
        "        # [b, n_heads, num_tok, win*win, d_head] -> [b, n_heads, d_head, num_tok, win*win] -> [b,c,num_tok*win,win]\n",
        "        # x = x.permute(0,1,4,2,3).reshape(b, self.d_model, num_tok*win,win) # [b, n_heads, num_tok, win*win, d_head]\n",
        "        # x = x.transpose(-2,-1).reshape(b, self.d_model, num_tok, *win) # [b*num_tok, n_heads, win*win, d_head]\n",
        "        x = self.out(x)\n",
        "        L=len(win)\n",
        "        x = x.reshape(b, num_tok, *win, self.d_model).permute(0,1,L+2,*range(2,L+2)) # [b, num_tok, out_dim, *win]\n",
        "        # x = x.unflatten(0, (b, num_tok))\n",
        "        return x # [b,num_tok,c,win1,win2]\n",
        "\n",
        "d_model=16\n",
        "model = MaskUnitAttention(d_model, n_heads=4, q_stride=2)\n",
        "# MaskUnitAttention(d_model=16, n_heads=4, q_stride=None, nd=2)\n",
        "# x=torch.randn(2,4,4,3)\n",
        "# x=torch.randn(2,3,d_model,8,8)\n",
        "x=torch.randn(2,3,d_model,32,32)\n",
        "# [b*num_tok,c,win,win]\n",
        "\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lec1Nq7nkbgt",
        "outputId": "bd142f82-f226-4884-ab9a-33d38e8b9c67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "124928\n",
            "torch.Size([4, 64, 64])\n",
            "124928\n"
          ]
        }
      ],
      "source": [
        "# @title hiera vit me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class LayerNorm_at(nn.RMSNorm): # LayerNorm RMSNorm\n",
        "    def __init__(self, dim=1, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.dim=dim\n",
        "    def forward(self, x):\n",
        "        return super().forward(x.transpose(self.dim,-1)).transpose(self.dim,-1)\n",
        "\n",
        "# # [b, h/win1* w/win2, c, win1,win2] -> [b,c,h,w]\n",
        "# def shuffle(x, window_shape): # [b,win*win, c, h/win, w/win] -> [b,1,c,h,w]\n",
        "#     # out_shape = [x.shape[0], -1, x.shape[2]] + [x*w for x,w in zip(x.shape[3:], window_shape)] # [b,1,c,h/win*wim, w/win*wim]\n",
        "#     out_shape = [x.shape[0], -1, x.shape[2]] + [x*w for x,w in zip(x.shape[3:], window_shape)] # [b,1,c,h/win*wim, w/win*wim]\n",
        "#     x = x.unflatten(1, (-1, *window_shape)) # [b,num_tok,win,win, c, h/win, w/win]\n",
        "#     D=x.dim()+1\n",
        "#     permute = [0,1,D//2] + [val for tup in zip(range(1+D//2, D), range(2, D//2)) for val in tup]\n",
        "#     x = x.permute(permute).reshape(out_shape)\n",
        "#     return x\n",
        "\n",
        "def unshuffle(x, window_shape): # [b,c,h,w] -> [b, h/win1* w/win2, c, win1,win2]\n",
        "    new_shape = list(x.shape[:2]) + [val for xx, win in zip(list(x.shape[2:]), window_shape) for val in [xx//win, win]] # [h,w]->[h/win1, win1, w/win2, win2]\n",
        "    x = x.reshape(new_shape) # [b, c, h/win1, win1, w/win2, win2]\n",
        "    # print('unsh',x.shape, window_shape, new_shape)\n",
        "    L = len(new_shape)\n",
        "    permute = ([0] + list(range(2, L - 1, 2)) + [1] + list(range(3, L, 2))) # [0,2,4,1,3,5] / [0,2,4,6,1,3,5,6]\n",
        "    return x.permute(permute).flatten(1, L//2-1) # [b, h/win1* w/win2, c, win1,win2]\n",
        "\n",
        "class HieraBlock(nn.Module):\n",
        "    # def __init__(self, d_model, n_heads, q_stride=None, mult=4, drop=0, nd=2):\n",
        "    def __init__(self, in_dim, d_model, n_heads, q_stride=None, mult=4, drop=0, nd=2):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        # self.norm = LayerNorm_at(2, d_model) # LayerNorm RMSNorm\n",
        "        self.norm = LayerNorm_at(2, in_dim) # LayerNorm RMSNorm\n",
        "        self.drop = nn.Dropout(drop)\n",
        "        # self.attn = MaskUnitAttention(d_model, n_heads, q_stride)\n",
        "        self.attn = MaskUnitAttention(in_dim, d_model, n_heads, q_stride)\n",
        "        ff_dim=d_model*mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), nn.Linear(d_model, ff_dim), nn.ReLU(), # ReLU GELU\n",
        "            nn.RMSNorm(ff_dim), nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "            # nn.RMSNorm(d_model), act, nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Linear(ff_dim, d_model)\n",
        "            # nn.RMSNorm(d_model), nn.Linear(d_model, ff_dim), nn.ReLU(), nn.Dropout(dropout), # ReLU GELU\n",
        "            # nn.Linear(ff_dim, d_model), nn.Dropout(dropout),\n",
        "        )\n",
        "        # self.res = conv_nd(nd, d_model, d_model, q_stride, q_stride) if q_stride else nn.Identity()\n",
        "\n",
        "        # self.res = nn.Sequential(\n",
        "        #     conv_nd(nd, in_dim, d_model, 1, 1) if in_dim!=d_model else nn.Identity(),\n",
        "        #     # maxpool_nd(nd, q_stride, q_stride) if q_stride else nn.Identity(),\n",
        "        #     avgpool_nd(nd, q_stride, q_stride) if q_stride else nn.Identity(),\n",
        "        # )\n",
        "        self.res = UpDownBlock(in_dim, d_model, kernel=1, r=1/2 if q_stride else 1)\n",
        "\n",
        "        # x = self.proj(x_norm).unflatten(1, (self.attn.q_stride, -1)).max(dim=1).values # pooling res # [b, (Sy, Sx, h/Sy, w/Sx), c] -> [b, (Sy, Sx), (h/Sy, w/Sx), c] -> [b, (h/Sy, w/Sx), c]\n",
        "\n",
        "\n",
        "    def forward(self, x): # [b, num_tok, c, *win]\n",
        "        b, num_tok, c, *win = x.shape\n",
        "        # print('attnblk fwd',x.shape)\n",
        "        # x = x + self.drop(self.self(self.norm(x)))\n",
        "        # print('attnblk fwd',self.res(x.flatten(0,1)).shape, self.drop(self.attn(self.norm(x))).flatten(0,1).shape)\n",
        "        # x = self.res(x.flatten(0,1)) + self.drop(self.attn(self.norm(x))).flatten(0,1) # [b*num_tok,c,win1,win2,win3]\n",
        "        x = self.res(x) + self.drop(self.attn(self.norm(x))) # [b*num_tok,c,win1,win2,win3]\n",
        "        # x = x + self.ff(x)\n",
        "        # x = x + self.ff(x.transpose(1,-1)).transpose(1,-1)\n",
        "        x = x + self.ff(x.transpose(2,-1)).transpose(2,-1)\n",
        "        # x = self.ff(x)\n",
        "        # x = x.unflatten(0, (b, num_tok))\n",
        "        return x\n",
        "\n",
        "    # def forward(self, x): # [b,t,c] # [b, (Sy, Sx, h/Sy, w/Sx), c]\n",
        "    #     # Attention + Q Pooling\n",
        "    #     x_norm = self.norm1(x)\n",
        "    #     if self.dim != self.dim_out:\n",
        "    #         x = self.proj(x_norm).unflatten(1, (self.attn.q_stride, -1)).max(dim=1).values # pooling res # [b, (Sy, Sx, h/Sy, w/Sx), c] -> [b, (Sy, Sx), (h/Sy, w/Sx), c] -> [b, (h/Sy, w/Sx), c]\n",
        "    #     x = x + self.drop_path(self.attn(x_norm))\n",
        "    #     x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "    #     return x\n",
        "\n",
        "\n",
        "class levelBlock(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, n_heads=None, depth=1, r=1):\n",
        "        super().__init__()\n",
        "        self.seq = nn.Sequential(\n",
        "            UpDownBlock(in_ch, out_ch, r=min(1,r)) if in_ch != out_ch or r<1 else nn.Identity(),\n",
        "            # AttentionBlock(d_model, d_model, n_heads, q_stride) if in_ch != out_ch or r<1 else nn.Identity(),\n",
        "            # AttentionBlock(d_model, d_model, n_heads, q_stride=(2,2)),\n",
        "            *[HieraBlock(d_model, d_model, n_heads) for i in range(1)],\n",
        "            # UpDownBlock(out_ch, out_ch, r=r) if r>1 else nn.Identity(),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.seq(x)\n",
        "\n",
        "\n",
        "class Hiera(nn.Module):\n",
        "    def __init__(self, patch_size, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "    # def __init__(self, in_dim, out_dim, d_model, n_heads, depth):\n",
        "        # patch_size=4\n",
        "        super().__init__()\n",
        "        self.embed = nn.Sequential( # in, out, kernel, stride, pad\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.MaxPool2d(3,2,1), # nn.MaxPool2d(2,2)\n",
        "            nn.Conv2d(in_dim, d_model, 7, 1, 7//2, bias=False),\n",
        "            # nn.Conv2d(in_dim, d_model, patch_size, patch_size), # like patch\n",
        "            # nn.Conv2d(in_dim, d_model, 3, 2, 3//2, bias=False), nn.BatchNorm2d(d_model), nn.ReLU(),\n",
        "            # UpDownBlock(in_dim, dim, r=1/2, kernel=3), UpDownBlock(dim, dim, r=1/2, kernel=3)\n",
        "            # nn.PixelUnshuffle(2), nn.Conv2d(in_dim*2**2, dim, 1, bias=False),\n",
        "            # ResBlock(in_dim, d_model, kernel),\n",
        "            )\n",
        "        # # self.pos_emb = LearnedRoPE2D(dim) # LearnedRoPE2D, RoPE2D\n",
        "\n",
        "        emb_shape = (32,32)\n",
        "        # emb_shape = (8,32,32)\n",
        "        if len(emb_shape) == 3: # for video\n",
        "            pos_spatial = nn.Parameter(torch.randn(1, emb_shape[1]*emb_shape[2], d_model)*.02)\n",
        "            pos_temporal = nn.Parameter(torch.randn(1, emb_shape[0], d_model)*.02)\n",
        "            self.pos_emb = pos_spatial.repeat(1, emb_shape[0], 1) + torch.repeat_interleave(pos_temporal, emb_shape[1] * emb_shape[2], dim=1)\n",
        "        elif len(emb_shape) == 2: # for img\n",
        "            self.pos_emb = nn.Parameter(torch.randn(1, d_model, *emb_shape)*.02) # 56*56=3136\n",
        "        # self.pos_emb = self.pos_emb.flatten(2).transpose(-2,-1)\n",
        "\n",
        "        # self.blocks = nn.Sequential(*[AttentionBlock(d_model, n_heads, q_stride=(2,2) if i in [1,3] else None) for i in range(depth)])\n",
        "        mult = [1,1,1,1]\n",
        "        # mult = [1,2,4,8] # [1,2,3,4] [1,2,2,2]\n",
        "        ch_list = [d_model * m for m in mult] # [128, 256, 384, 512]\n",
        "\n",
        "        self.blocks = nn.Sequential(\n",
        "            # HieraBlock(ch_list[0], ch_list[1], n_heads, q_stride=(2,2)),\n",
        "            # UpDownBlock(ch_list[0], ch_list[1], kernel=1, r=1/2),\n",
        "            HieraBlock(ch_list[1], ch_list[1], n_heads),\n",
        "            # HieraBlock(ch_list[1], ch_list[2], n_heads, q_stride=(2,2)),\n",
        "            # UpDownBlock(ch_list[1], ch_list[2], kernel=1, r=1/2),\n",
        "            # HieraBlock(ch_list[2], ch_list[2], n_heads),\n",
        "            )\n",
        "        self.norm = nn.RMSNorm(ch_list[2]) # LayerNorm RMSNorm\n",
        "        self.out = nn.Linear(ch_list[2], out_dim, bias=False) if out_dim and out_dim != ch_list[2] else None\n",
        "\n",
        "\n",
        "    def forward(self, x, context_indices=None): # [b,c,h,w], [b,num_tok]\n",
        "        x = self.embed(x)\n",
        "        # print('vit fwd', x.shape, self.pos_emb.shape)\n",
        "        x = x + self.pos_emb\n",
        "        x = unshuffle(x, (4,4)) # [b,num_tok,c,win1,win2,win3] or [b,1,c,f,h,w]\n",
        "        if context_indices != None: x = x[torch.arange(x.shape[0]).unsqueeze(-1), context_indices] # [batch, num_context_toks, d_model]\n",
        "\n",
        "        # print('vit fwd1', x.shape)\n",
        "        x = self.blocks(x) # [b,num_tok,c,1,1,1]\n",
        "        # print('vit fwd2', x.shape)\n",
        "        # x = x.mean(-1).mean(-1)\n",
        "        x = x.flatten(-2).max(-1)[0]\n",
        "        # print('vit fwd3', x.shape)\n",
        "        x = x.squeeze() # [b,num_tok,d]\n",
        "        out = self.norm(x)\n",
        "        if self.out: out = self.out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "# pos_emb rope < learn < learned\n",
        "# conv > pixel?\n",
        "# droppath not required\n",
        "\n",
        "# norm,act,conv < conv,norm,act\n",
        "# 2*s1 < uib < resblock\n",
        "# gatedadaln 3 < 2 = 1 < ffmult4 = 2*gatedadaln\n",
        "# MaxPool2d(2,2) < MaxPool2d(3,2,3//2)\n",
        "\n",
        "# patchattn only for\n",
        "\n",
        "# multiendfusion negligible diff\n",
        "\n",
        "\n",
        "d_model = 64\n",
        "in_dim = 3\n",
        "patch_size = 2\n",
        "# model = Hiera(patch_size, in_dim, d_model, n_heads=4, nlayers=4, drop=0.).to(device)\n",
        "model = Hiera(patch_size, in_dim, d_model, n_heads=4, nlayers=1, drop=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x = torch.rand((4, in_dim, 32, 32), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 59850\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3XSZZyUPY1i"
      },
      "source": [
        "## vit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "eDqkaM2v0_zS"
      },
      "outputs": [],
      "source": [
        "# @title FSQ me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def ste_round(x): return x.round().detach() + x - x.detach()\n",
        "\n",
        "class FSQ(nn.Module):\n",
        "    def __init__(self, levels):\n",
        "        super().__init__()\n",
        "        self.levels = torch.tensor(levels, device=device)\n",
        "        self.basis = torch.cumprod(torch.tensor([*levels[1:], 1], device=device).flip(-1), dim=0).flip(-1)\n",
        "        self.half_width = (self.levels-1)/2\n",
        "        self.codebook_size = torch.prod(self.levels).item()\n",
        "        self.codebook = self.indexes_to_codes(torch.arange(self.codebook_size, device=device))\n",
        "\n",
        "    def forward(self, z, beta=1): # beta in (0,1). beta->0 => values more spread out\n",
        "        offset = (self.levels+1) % 2 /2 # .5 if even, 0 if odd\n",
        "        bound = (F.sigmoid(z)-1/2) * (self.levels-beta) + offset\n",
        "        # print('fwd', bound) #\n",
        "        quantized = ste_round(bound)\n",
        "        # print('fwd', quantized) # 4: -1012\n",
        "        return (quantized-offset) / self.half_width # split [-1,1]\n",
        "\n",
        "    def codes_to_indexes(self, zhat):\n",
        "        zhat = (zhat + 1) * self.half_width\n",
        "        return (zhat * self.basis).sum(axis=-1)#.int()\n",
        "\n",
        "    def indexes_to_codes(self, indices):\n",
        "        indices = indices.unsqueeze(-1)\n",
        "        codes = torch.remainder(indices//self.basis, self.levels)\n",
        "        # print(\"codes\",codes)\n",
        "        return codes / self.half_width - 1\n",
        "\n",
        "# fsq = FSQ(levels = [5,4,3,2])\n",
        "# # print(fsq.codebook)\n",
        "# batch_size, seq_len = 2, 4\n",
        "# # x = torch.rand((batch_size, seq_len,3),device=device)\n",
        "# x = torch.linspace(-2,2,7).repeat(4,1).T\n",
        "# la = fsq(x)\n",
        "# print(la)\n",
        "# lact = fsq.codes_to_indexes(la)\n",
        "# print(lact)\n",
        "# # la = fsq.indexes_to_codes(lact)\n",
        "# # print(la)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6T4F651kmGh",
        "outputId": "056d3c4a-f4d7-4260-9110-f763575ed74c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "162112\n",
            "torch.Size([5, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# @title AttentionBlock\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class SelfAttn(nn.Module):\n",
        "    def __init__(self, dim, n_heads):\n",
        "        super().__init__()\n",
        "        self.dim, self.n_heads = dim, n_heads\n",
        "        d_head = dim//n_heads\n",
        "        self.qkv = nn.Linear(dim, dim*3, bias=False)\n",
        "        # self.lin = nn.Linear(dim, dim)\n",
        "        self.lin = zero_module(nn.Linear(dim, dim))\n",
        "        # self.rope = RoPE(d_head, seq_len=512, base=10000)\n",
        "        # self.rope = RoPE2D(d_head, h=64, w=64, base=100)\n",
        "        self.scale = d_head**-.5\n",
        "\n",
        "    def forward(self, x): # [b,t,d]\n",
        "        q,k,v = self.qkv(x).unflatten(-1, (self.n_heads,-1)).transpose(1,2).chunk(3, dim=-1) # [b, t, n_heads, d_head] -> [b, n_heads, t, d_head]\n",
        "        # q, k = self.rope(q), self.rope(k)\n",
        "        # x = F.scaled_dot_product_attention(q,k,v, attn_mask=None) # https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html\n",
        "        q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2)\n",
        "        context = k.transpose(-2,-1) @ v # [batch, n_heads, d_head, d_head]\n",
        "        x = q @ context # [b, n_heads, t, d_head]\n",
        "        # # print('SelfAttn', x.shape)\n",
        "        x = x.transpose(1,2).flatten(2)\n",
        "        return self.lin(x)\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, mult=4, drop=0.):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        # self.norm = nn.RMSNorm(d_model, elementwise_affine=False) # LayerNorm RMSNorm\n",
        "        self.norm1, self.norm2 = nn.LayerNorm(d_model), nn.LayerNorm(d_model)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "        self.attn = SelfAttn(d_model, n_heads)\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        ff_dim=d_model*mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), act, nn.Dropout(drop), nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "            nn.RMSNorm(ff_dim), act, nn.Dropout(drop), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(drop), nn.Linear(d_model, ff_dim), act,\n",
        "            # nn.RMSNorm(ff_dim), nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x): # [batch, seq_len, d_model]\n",
        "        # print('attnblk fwd',x.shape)\n",
        "        x = x + self.drop(self.attn(self.norm1(x)))\n",
        "        x = x + self.ff(x)\n",
        "        # x = x + self.drop(self.ff(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, patch_size, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        patch_size=2\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Conv2d(in_dim, d_model, patch_size, patch_size), # like patch\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 1, 7//2, bias=False)\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.MaxPool2d(3,2,1)\n",
        "            nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.BatchNorm2d(d_model), nn.ReLU(),\n",
        "            nn.Conv2d(d_model, d_model, 3, 2, 3//2, bias=False)\n",
        "            )\n",
        "        # self.embed.requires_grad=False\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, 32*32, d_model)*.02)\n",
        "        # self.pos_emb = nn.Parameter(RoPE2D(dim=d_model, h=8, w=8, base=1000), requires_grad=False)\n",
        "\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=(32//patch_size)**2, base=10000), requires_grad=False)\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, d_model, n_heads) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x).flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c] # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        # x = self.pos_enc(x)\n",
        "        # print(\"TransformerModel\",x.shape, self.pos_emb.shape)\n",
        "        x = x + self.pos_emb[:,:x.shape[1]]\n",
        "        if context_indices != None: x = x[torch.arange(x.shape[0]).unsqueeze(-1), context_indices] # [batch, num_context_toks, d_model]\n",
        "\n",
        "        x = self.transformer(x)\n",
        "        out = self.norm(x)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "d_model = 64\n",
        "in_dim = 3\n",
        "patch_size = 2\n",
        "# model = ViT(patch_size, in_dim, d_model, n_heads=4, nlayers=4, drop=0.).to(device)\n",
        "model = ViT(patch_size, in_dim, d_model, n_heads=4, nlayers=1, drop=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x = torch.rand((5, in_dim, 32, 32), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "Fh9o__m2-j7K",
        "outputId": "377b0226-1e1c-4157-a7a8-db58bada8228"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title simplex\n",
        "# !pip install -q opensimplex\n",
        "import opensimplex\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def simplexmask2d(hw=(32,32), ctx_scale=(.85,1.), trg_scale=(.6,.8), B=64, chaos=[1,.5]):\n",
        "    ix, iy = np.split(np.linspace(0, chaos[0], num=sum(hw)), [hw[0]])\n",
        "    noise = opensimplex.noise3array(ix, iy, np.random.randint(1e10, size=B)) # [b,h,w]\n",
        "    noise = torch.from_numpy(noise).flatten(1)\n",
        "    trunc_normal = torch.fmod(torch.randn(2)*.3,1)/2 + .5\n",
        "    ctx_mask_scale = torch.rand(1) * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    trg_mask_scale = torch.rand(1) * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "    # ctx_mask_scale = trunc_normal[0] * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    # trg_mask_scale = trunc_normal[1] * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "    seq = hw[0]*hw[1]\n",
        "    ctx_len, trg_len = int(seq*ctx_mask_scale), int(seq*trg_mask_scale)\n",
        "    val, trg_index = torch.topk(noise, trg_len, dim=1, sorted=False)\n",
        "    # ctx_len = ctx_len - trg_len\n",
        "    ctx_len = min(ctx_len, seq-trg_len)\n",
        "\n",
        "    remove_mask = torch.ones((B,seq), dtype=bool) # [B, S]\n",
        "    remove_mask.scatter_(1, trg_index, False).flatten()\n",
        "    ind = torch.arange(seq).unsqueeze(0).repeat(B,1)[remove_mask].reshape(B, -1)\n",
        "\n",
        "    ix, iy = np.split(np.linspace(0, chaos[1], num=sum(hw)), [hw[0]])\n",
        "    noise = opensimplex.noise3array(ix, iy, np.random.randint(1e10, size=B)) # [b,h,w]\n",
        "    noise = torch.from_numpy(noise).flatten(1)[remove_mask].reshape(B, -1)\n",
        "    val, ctx_ind = torch.topk(noise, ctx_len, dim=1, sorted=False)\n",
        "    ctx_index = ind[torch.arange(B).unsqueeze(-1), ctx_ind]\n",
        "    return ctx_index, trg_index\n",
        "\n",
        "b=16\n",
        "# ctx_index, trg_index = simplexmask2d(hw=(32,32), ctx_scale=(.85,1), trg_scale=(.7,.8), B=b, chaos=[3,.5])\n",
        "# ctx_index, trg_index = simplexmask2d(hw=(32,32), ctx_scale=(.85,1), trg_scale=(.7,.8), B=b, chaos=[3,1])\n",
        "# ctx_index, trg_index = simplexmask2d(hw=(32,32), ctx_scale=(.85,1), trg_scale=(.2,.8), B=b, chaos=[3,1])\n",
        "ctx_index, trg_index = simplexmask2d(hw=(32,32), ctx_scale=(.05,.2), trg_scale=(.7,.8), B=b, chaos=[3,1])\n",
        "# ctx_index, trg_index = simplexmask2d(hw=(32,32), ctx_scale=(.85,1), trg_scale=(.5,.5), B=b, chaos=[.01,.5])\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "# imshow(mask)\n",
        "\n",
        "mask = torch.zeros(b ,32*32)\n",
        "# mask = torch.ones(b ,32*32)*.3\n",
        "mask[torch.arange(b).unsqueeze(-1), trg_index] = 1\n",
        "mask[torch.arange(b).unsqueeze(-1), ctx_index] = .5\n",
        "# print((mask==1).sum(1))\n",
        "# print((mask==.5).sum(1))\n",
        "mask = mask.reshape(b,1,32,32)\n",
        "import torchvision\n",
        "imshow(torchvision.utils.make_grid(mask, nrow=8))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "pQfM2fYvcTh6",
        "outputId": "26570245-03af-400d-8d40-0a6d6d34cd9d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title masks\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def multiblock(seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "    return target_mask\n",
        "\n",
        "\n",
        "def multiblock2d(hw=(8,8), scale=(.15,.2), aspect_ratio=(.75,1.5), M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_aspect = torch.rand(1) * (aspect_ratio[1] - aspect_ratio[0]) + aspect_ratio[0] # in (min_s, max_s) # all blocks same size\n",
        "    mask_scale = torch.rand(1) * (scale[1] - scale[0]) + scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    h = (mask_scale/mask_aspect)**.5# h*(h*aspect) = scale\n",
        "    w = h * mask_aspect\n",
        "    h_pos, w_pos = torch.rand(M)*(1-w), torch.rand(M)*(1-h) # in (0, 1 - mask_len)\n",
        "    h_len, h_pos = (h*hw[0]).int(), h_pos*hw[0]\n",
        "    w_len, w_pos = (w*hw[1]).int(), w_pos*hw[1]\n",
        "    h_ind, w_ind = torch.arange(hw[0]).unsqueeze(0), torch.arange(hw[1]).unsqueeze(0) # [1, seq]\n",
        "    h_mask = (h_ind>=h_pos.unsqueeze(-1)) & (h_ind<(h_pos+h_len).unsqueeze(-1)) # [M, seq]\n",
        "    w_mask = (w_ind>=w_pos.unsqueeze(-1)) & (w_ind<(w_pos+w_len).unsqueeze(-1)) # [M, seq]\n",
        "    target_mask = h_mask.unsqueeze(-1) & w_mask.unsqueeze(-2) # [M, seq, seq]\n",
        "    return target_mask\n",
        "\n",
        "# https://arxiv.org/pdf/2210.07224\n",
        "def randpatch(seq, mask_size=8, gamma=0.9): # num patches of seq, mask patch size, masking ratio\n",
        "    # mask = torch.rand(seq//mask_size)<gamma\n",
        "    length = seq//mask_size\n",
        "    g = torch.normal(gamma, std=.1, size=(1,)).clamp(.5,.9)\n",
        "    # g = gamma\n",
        "    idx = torch.randperm(length)[:int(length*g)]\n",
        "    mask = torch.zeros(length, dtype=bool)\n",
        "    mask[idx] = True\n",
        "    mask = mask.repeat_interleave(mask_size, dim=-1)\n",
        "    return mask # [seq] , True -> mask\n",
        "\n",
        "\n",
        "import torch\n",
        "def apply_masks(x, mask): # [b,t,d], [mask_size] # https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "    mask_keep = mask.unsqueeze(-1).repeat(x.size(0), 1, x.size(-1)) # [batch,T,dim]\n",
        "    return torch.gather(x, dim=1, index=mask_keep) # [batch,mask_size,dim]\n",
        "\n",
        "\n",
        "# @title ijepa multiblock next\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "import math\n",
        "from multiprocessing import Value\n",
        "import torch\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(self, hw=(224, 224), enc_mask_scale=(.85,1), pred_mask_scale=(.15,.2), aspect_ratio=(.75,1.25),\n",
        "        nenc=1, npred=2, min_keep=4, allow_overlap=False):\n",
        "        super().__init__()\n",
        "        self.height, self.width = hw\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.aspect_ratio = aspect_ratio\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "\n",
        "    def _sample_block_size(self, scale, aspect_ratio_scale):\n",
        "        _rand = torch.rand(1).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.height * self.width * mask_scale) # num patches to keep\n",
        "        # -- Sample block aspect-ratio\n",
        "        min_ar, max_ar = aspect_ratio_scale\n",
        "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
        "        while h >= self.height: h -= 1 # crop mask to be smaller than img\n",
        "        while w >= self.width: w -= 1\n",
        "        return (h, w)\n",
        "\n",
        "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
        "        h, w = b_size\n",
        "\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # --\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            top = torch.randint(0, self.height - h, (1,))\n",
        "            left = torch.randint(0, self.width - w, (1,))\n",
        "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
        "            mask[top:top+h, left:left+w] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
        "        mask_complement[top:top+h, left:left+w] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, B):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        p_size = self._sample_block_size(scale=self.pred_mask_scale, aspect_ratio_scale=self.aspect_ratio)\n",
        "        e_size = self._sample_block_size(scale=self.enc_mask_scale, aspect_ratio_scale=(1., 1.))\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.height * self.width\n",
        "        min_keep_enc = self.height * self.width\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                print(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "        return collated_masks_enc, collated_masks_pred\n",
        "\n",
        "mask_collator = MaskCollator(hw=(32,32), enc_mask_scale=(.85, 1.), pred_mask_scale=(.15, .2), aspect_ratio=(.75, 1.5),\n",
        "        nenc=1, npred=4, min_keep=4,\n",
        "        # allow_overlap=True)\n",
        "        allow_overlap=False)\n",
        "\n",
        "b=16\n",
        "collated_masks_enc, collated_masks_pred = mask_collator(b)\n",
        "ctx_index, trg_index = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "\n",
        "# mask = torch.zeros(1 ,32*32)\n",
        "# mask[:, trg_index[:1]] = 1\n",
        "# mask[:, ctx_index[:1]] = .5\n",
        "# mask = mask.reshape(1,32,32)\n",
        "\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "# imshow(mask)\n",
        "\n",
        "mask = torch.zeros(b ,32*32)\n",
        "mask[torch.arange(b).unsqueeze(-1), trg_index] = 1\n",
        "mask[torch.arange(b).unsqueeze(-1), ctx_index] = .5\n",
        "mask = mask.reshape(b,1,32,32)\n",
        "import torchvision\n",
        "imshow(torchvision.utils.make_grid(mask, nrow=8))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask_collator = MaskCollator(hw=(1024,1024), enc_mask_scale=(.85, 1.), pred_mask_scale=(.15, .2), aspect_ratio=(.75, 1.5), nenc=1, npred=4, min_keep=4, allow_overlap=False)\n",
        "# %timeit collated_masks_enc, collated_masks_pred = mask_collator(64) # 225 ms 1024:4.79 s\n",
        "# %timeit ctx_index, trg_index = simplexmask2d(hw=(1024,1024), ctx_scale=(.85,1), trg_scale=(.5,.6), B=b, chaos=.5) # 265 ms ;topk 203 ms 1024:4.27 s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nj5bI5hSEdJ1",
        "outputId": "6a7d20de-cdae-47f0-82ac-64546f16f829"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slowest run took 5.27 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "4.79 s ± 3.03 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ttc=[]\n",
        "ttt=[]\n",
        "def mean(x): return sum(x)/len(x)\n",
        "\n",
        "for i in range(1000):\n",
        "    # collated_masks_enc, collated_masks_pred = mask_collator(1)\n",
        "    # context_indices, trg_indices = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "    # context_indices, trg_indices = simplexmask2d(hw=(32,32), ctx_scale=(.85,1.), trg_scale=(.7,.8), B=1, chaos=[3,1])\n",
        "    context_indices, trg_indices = simplexmask2d(hw=(32,32), ctx_scale=(.05,.5), trg_scale=(.2,.8), B=1, chaos=[3,1])\n",
        "    ttc.append(context_indices.shape[-1])\n",
        "    ttt.append(trg_indices.shape[-1])\n",
        "\n",
        "print(mean(ttc), mean(ttt))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"figure.figsize\"] = (4,4)\n",
        "plt.hist(ttc, bins=20, alpha=.5, label='context mask')\n",
        "plt.hist(ttt, bins=20, alpha=.5, label='target mask')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "bC6elHNF5xzL",
        "outputId": "03d9bedf-8c95-4bca-f1b3-f3c851dfc6c1"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "258.673 499.481\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "M-zdjdJixtOu"
      },
      "outputs": [],
      "source": [
        "# @title Transformer Predictor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, 32*32, d_model)*0.02)\n",
        "        # self.pos_emb = nn.Parameter(RoPE2D(dim=d_model, h=8, w=8, base=1000), requires_grad=False)\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=32*32, base=10000), requires_grad=False)\n",
        "\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, d_model, n_heads) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.randn(1,1,d_model)*0.02) # randn zeros\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim)# if out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = x.shape\n",
        "        # x = x * self.pos_enc(context_indices)\n",
        "        x = x + self.pos_emb[0,context_indices]\n",
        "\n",
        "        # pred_tokens = self.cls * self.pos_enc(trg_indices) # [M, num_trg_toks, d_model]\n",
        "        pred_tokens = self.cls + self.pos_emb[0,trg_indices]\n",
        "        # print(\"pred fwd\", x.shape, pred_tokens.shape)\n",
        "        x = torch.cat([x, pred_tokens], dim=1) # [batch, seq_len+num_trg_toks, d_model]\n",
        "        out = self.transformer(x)\n",
        "\n",
        "        out = self.norm(out)\n",
        "\n",
        "        out = out[:,seq:] # [batch, num_trg_toks, d_model]\n",
        "        # ind = torch.cat([context_indices, trg_indices], dim=-1).unsqueeze(-1).repeat(1,1,x.shape[-1]) # [b,t]->[b,t,d]\n",
        "        # out=torch.gather(out, 1, ind)\n",
        "\n",
        "        # print(\"pred fwd\", context_indices.shape, trg_indices.shape, out.shape)\n",
        "\n",
        "        out = self.lin(out)\n",
        "        # print(\"pred fwd\", out.shape)\n",
        "        return out # [seq_len, batch_size, ntoken]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ardu1zJwdHM3",
        "outputId": "b2aa31b1-af01-4045-80ce-5dffb83cf286"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "245184\n",
            "torch.Size([])\n"
          ]
        }
      ],
      "source": [
        "# @title IJEPA\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class IJEPA(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=64, out_dim=None, nlayers=1, n_heads=4):\n",
        "        super().__init__()\n",
        "        self.out_dim = out_dim = out_dim or d_model\n",
        "        self.patch_size = 4 # 4\n",
        "        # self.student = Hiera(self.patch_size, in_dim, d_model, out_dim=out_dim, n_heads=n_heads, nlayers=nlayers, drop=0.)\n",
        "        self.student = ViT(self.patch_size, in_dim, d_model, out_dim=out_dim, n_heads=n_heads, nlayers=nlayers, drop=0.)\n",
        "        self.predicter = TransformerPredictor(out_dim, d_model//2, out_dim, n_heads=4, nlayers=1, drop=0.)\n",
        "        # self.predicter = TransformerPredictor(out_dim, 3*d_model//8, out_dim, n_heads=4, nlayers=1, drop=0.)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "\n",
        "    def loss(self, x): #\n",
        "        b,c,h,w = x.shape\n",
        "        # print('ijepa loss x',x.shape)\n",
        "        hw=(8,8)\n",
        "        # hw=(32,32)\n",
        "        mask_collator = MaskCollator(hw, enc_mask_scale=(.85, 1.), pred_mask_scale=(.15, .2), aspect_ratio=(.75, 1.5), nenc=1, npred=4, min_keep=4, allow_overlap=False)\n",
        "        collated_masks_enc, collated_masks_pred = mask_collator(b)\n",
        "        context_indices, trg_indices = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "        # print(context_indices.shape, trg_indices.shape)\n",
        "\n",
        "        # context_indices, trg_indices = simplexmask2d(hw, ctx_scale=(.85,1), trg_scale=(.7,.8), B=b, chaos=[3,.5])\n",
        "        # context_indices, trg_indices = simplexmask2d(hw, ctx_scale=(.85,1), trg_scale=(.7,.8), B=b, chaos=[5,.5])\n",
        "        # context_indices, trg_indices = simplexmask2d(hw, ctx_scale=(.05,.2), trg_scale=(.7,.8), B=b, chaos=[3,1])\n",
        "        # context_indices, trg_indices = context_indices.repeat(b,1), trg_indices.repeat(b,1)\n",
        "\n",
        "        # zero_mask = torch.zeros(b ,*hw, device=device).flatten(1)\n",
        "        # zero_mask[torch.arange(b).unsqueeze(-1), context_indices] = 1\n",
        "        # x_ = x * F.interpolate(zero_mask.reshape(b,1,*hw), size=x.shape[2:], mode='nearest-exact') # zero masked locations\n",
        "\n",
        "        sx = self.student(x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "        # print('ijepa loss sx',sx.shape)\n",
        "        sy_ = self.predicter(sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "        sy_ = F.layer_norm(sy_, (sy_.size(-1),))\n",
        "        with torch.no_grad():\n",
        "            sy = self.teacher(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "            sy = sy[torch.arange(sy.shape[0]).unsqueeze(-1), trg_indices] # [batch, num_context_toks, d_model] # nan bec len(trg_ind)==0 # print('loss sy',torch.isnan(sy).any())\n",
        "            sy = F.layer_norm(sy, (sy.size(-1),))\n",
        "        loss = F.mse_loss(sy, sy_)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        sx = self.student(x)\n",
        "        out = sx.mean(dim=1)\n",
        "        return out\n",
        "\n",
        "# min_s=0.15, max_s, M\n",
        "# trg.15.2M4 C.85 1\n",
        "# dont normalise data\n",
        "# randmsk < simplex 1msk < multiblk\n",
        "# teacher=trans ~? teacher=copy\n",
        "# learn convemb\n",
        "# lr pred,student: 3e-3/1e-2, 1e-3\n",
        "\n",
        "# lr 1e-3 < 1e-2,1e-3? slower but dun plateau\n",
        "\n",
        "# vit 1lyr 15.4sec 15.6\n",
        "# hiera downsampling attn 20.5\n",
        "\n",
        "# ijepa = IJEPA(in_dim=3, d_model=64, out_dim=64, nlayers=4, n_heads=4).to(device)\n",
        "ijepa = IJEPA(in_dim=3, d_model=64, out_dim=64, nlayers=1, n_heads=8).to(device)\n",
        "# ijepa = IJEPA(in_dim=3, d_model=32, out_dim=32, nlayers=1, n_heads=4).to(device)\n",
        "# optim = torch.optim.AdamW(ijepa.parameters(), lr=1e-3) # 1e-3?\n",
        "optim = torch.optim.AdamW([{'params': ijepa.student.parameters()},\n",
        "#     {'params': ijepa.predicter.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # good 3e-3,1e-3 ; default 1e-2, 5e-2\n",
        "    {'params': ijepa.predicter.parameters(), 'lr': 1e-2}], lr=1e-3, weight_decay=1e-2)\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/configs/in1k_vith14_ep300.yaml\n",
        "# d_model 1024,384\n",
        "# depth 12,6/12\n",
        "# wd 5e-2 - 4e-1\n",
        "# adamw 1e-4 - 1e-3 - 1e-6\n",
        "# ema 0.996-1\n",
        "\n",
        "print(sum(p.numel() for p in ijepa.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in ijepa.parameters())) # 27584\n",
        "# print(sum(p.numel() for p in ijepa.predicter.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# d_model^2 * nlayers\n",
        "\n",
        "x = torch.rand((64,3,32,32), device=device)\n",
        "out = ijepa.loss(x)\n",
        "print(out.shape)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(ijepa.out_dim, 10).to(device)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n",
        "# optim = torch.optim.AdamW([{'params': ijepa.parameters()}, {'params': classifier.parameters(), 'lr': 1e-3}], lr=1e-3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODpKypTCsfIt",
        "outputId": "b2f4380b-03a8-49be-95b5-eed2b564dc05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 16])\n"
          ]
        }
      ],
      "source": [
        "# @title TransformerVICReg\n",
        "import torch\n",
        "from torch import nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class TransformerVICReg(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, drop=0):\n",
        "        super().__init__()\n",
        "        act = nn.GELU()\n",
        "        patch_size=4\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Linear(in_dim, d_model), act\n",
        "            # nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            # nn.Conv1d(d_model, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            # nn.Conv1d(d_model, d_model,3,2,3//2),\n",
        "            nn.Conv2d(in_dim, d_model, patch_size, patch_size), # patch\n",
        "            )\n",
        "        self.pos_enc = RoPE(d_model, seq_len=200, base=10000)\n",
        "        # self.pos_emb = nn.Parameter(torch.randn(1, 8*8, d_model))\n",
        "        # self.pos_emb = nn.Parameter(RoPE2D(dim=d_model, h=8, w=8, base=10000).unsqueeze(0), requires_grad=False)\n",
        "\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "\n",
        "        # out_dim = out_dim or d_model\n",
        "        self.lin = nn.Linear(d_model, out_dim)\n",
        "        self.attn_pool = nn.Linear(d_model, 1, bias=False)\n",
        "\n",
        "        dim_v = d_model * 4\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(out_dim, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v, bias=False),\n",
        "            )\n",
        "\n",
        "    def forward(self, x): # [b,t,d] / [b,c,h,w]\n",
        "        x = self.embed(x).flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c]\n",
        "        # x = self.embed(x.transpose(-2,-1)).transpose(-2,-1) # [b,t,d]\n",
        "        x = self.pos_enc(x)\n",
        "        # x = x + self.pos_emb\n",
        "\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        attn = self.attn_pool(x).squeeze(-1) # [batch, seq] # seq_pool\n",
        "        out = (torch.softmax(attn, dim=-1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, seq] @ [batch, seq, dim] -> [batch, dim]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch, ntoken]\n",
        "\n",
        "    def expand(self, x):\n",
        "        sx = self.forward(x)\n",
        "        vx = self.exp(sx)\n",
        "        return vx\n",
        "\n",
        "batch, seq_len, d_model = 4,3500,512\n",
        "in_dim, out_dim=3,16\n",
        "model = TransformerVICReg(in_dim, d_model, out_dim, d_head=4, nlayers=2, drop=0.).to(device)\n",
        "# x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "x =  torch.rand((batch, in_dim, 32,32), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qwg9dG4sQ3h",
        "outputId": "2f0092f5-5bb7-4725-db2b-dcf7b0ba1a96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "62850\n",
            "in vicreg  0.00019920778413506923 24.74469393491745 4.793645480560826e-09\n",
            "(tensor(7.9683e-06, grad_fn=<MseLossBackward0>), tensor(0.9898, grad_fn=<AddBackward0>), tensor(4.7936e-09, grad_fn=<AddBackward0>))\n"
          ]
        }
      ],
      "source": [
        "# @title Violet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class Violet(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, nlayers=2, d_head=4):\n",
        "        super().__init__()\n",
        "        out_dim = out_dim or d_model\n",
        "        self.student = TransformerVICReg(in_dim, d_model, out_dim=out_dim, d_head=d_head, nlayers=nlayers, drop=0.)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "\n",
        "        # vicreg\n",
        "        self.sim_coeff=25. # 10.0 # 25.0 # λ\n",
        "        self.std_coeff=25. # 1.0 # 25.0 # µ\n",
        "        self.cov_coeff=1. # 25.0 # 1.0 # ν\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]c/ [b,c,h,w]\n",
        "        # print(x.shape)\n",
        "        # mask = simplexmask(hw=(8,8), scale=(.7,.8)).unsqueeze(0) # .6.8\n",
        "        # vx = self.student.expand(x, mask) # [batch, num_context_toks, out_dim]\n",
        "\n",
        "        vx = self.student.expand(x) # [batch, num_context_toks, out_dim]\n",
        "\n",
        "        with torch.no_grad(): vy = self.teacher.expand(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "        loss = self.vicreg(vx, vy)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        return self.student(x)\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size, num_features = x.shape\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = self.sim_coeff * repr_loss + self.std_coeff * std_loss + self.cov_coeff * cov_loss\n",
        "        print(\"in vicreg \",self.sim_coeff * repr_loss.item() , self.std_coeff * std_loss.item() , self.cov_coeff * cov_loss.item())\n",
        "        # return loss\n",
        "        return repr_loss, std_loss, cov_loss\n",
        "\n",
        "\n",
        "violet = Violet(in_dim=3, d_model=32, out_dim=16, nlayers=2, d_head=4).to(device)\n",
        "voptim = torch.optim.AdamW(violet.parameters(), lr=1e-3) # 1e-3?\n",
        "# voptim = torch.optim.AdamW([{'params': violet.student.transformer.parameters()},\n",
        "#     {'params': violet.student.exp.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # default 1e-2\n",
        "print(sum(p.numel() for p in violet.parameters() if p.requires_grad)) # 27584\n",
        "\n",
        "# x = torch.rand((2,1000,3), device=device)\n",
        "x = torch.rand((2,3,32,32), device=device)\n",
        "# x = torch.rand((2,1,16), device=device)\n",
        "loss = violet.loss(x)\n",
        "# print(out.shape)\n",
        "print(loss)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(16).to(device)\n",
        "# classifier = Classifier(16, 18).to(device)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_BFm8Kh-j3u"
      },
      "outputs": [],
      "source": [
        "for name, param in ijepa.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name, param)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "form",
        "id": "eb1n4BhimG_N"
      },
      "outputs": [],
      "source": [
        "# @title RankMe\n",
        "# RankMe: Assessing the Downstream Performance of Pretrained Self-Supervised Representations by Their Rank jun 2023 https://arxiv.org/pdf/2210.02885\n",
        "import torch\n",
        "\n",
        "# https://github.com/Spidartist/IJEPA_endoscopy/blob/main/src/helper.py#L22\n",
        "def RankMe(Z):\n",
        "    \"\"\"\n",
        "    Calculate the RankMe score (the higher, the better).\n",
        "    RankMe(Z) = exp(-sum_{k=1}^{min(N, K)} p_k * log(p_k)),\n",
        "    where p_k = sigma_k (Z) / ||sigma_k (Z)||_1 + epsilon\n",
        "    where sigma_k is the kth singular value of Z.\n",
        "    where Z is the matrix of embeddings (N × K)\n",
        "    \"\"\"\n",
        "    # compute the singular values of the embeddings\n",
        "    # _u, s, _vh = torch.linalg.svd(Z, full_matrices=False)  # s.shape = (min(N, K),)\n",
        "    # s = torch.linalg.svd(Z, full_matrices=False).S\n",
        "    s = torch.linalg.svdvals(Z)\n",
        "    p = s / torch.sum(s, axis=0) + 1e-7\n",
        "    return torch.exp(-torch.sum(p * torch.log(p)))\n",
        "\n",
        "# Z = torch.randn(5, 3)\n",
        "# rankme = RankMe(Z)\n",
        "# print(rankme)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "form",
        "id": "y24ZNFqW7woQ"
      },
      "outputs": [],
      "source": [
        "# @title LiDAR\n",
        "# LiDAR: Sensing Linear Probing Performance In Joint Embedding SSL Architectures https://arxiv.org/pdf/2312.04000\n",
        "# https://github.com/rbalestr-lab/stable-ssl/blob/main/stable_ssl/monitors.py#L106\n",
        "\n",
        "def LiDAR(sx, eps=1e-7, delta=1e-3):\n",
        "    sx = sx.unflatten(0, (-1, 2))\n",
        "    n, q, d = sx.shape\n",
        "    mu_x = sx.mean(dim=1) # mu_x # [n,d]\n",
        "    mu = mu_x.mean(dim=0) # mu # [d]\n",
        "\n",
        "    diff_b = (mu_x - mu).unsqueeze(-1) # [n,d,1]\n",
        "    S_b = (diff_b @ diff_b.transpose(-2,-1)).sum(0) / (n - 1) # [n,d,d] -> [d,d]\n",
        "    diff_w = (sx - mu_x.unsqueeze(1)).reshape(-1,d,1) # [n,q,d] -> [nq,d,1]\n",
        "    S_w = (diff_w @ diff_w.transpose(-2,-1)).sum(0) / (n * (q - 1)) + delta * torch.eye(d, device=sx.device) # [nq,d,d] -> [d,d]\n",
        "\n",
        "    eigvals_w, eigvecs_w = torch.linalg.eigh(S_w)\n",
        "    eigvals_w = torch.clamp(eigvals_w, min=eps)\n",
        "\n",
        "    invsqrt_w = (eigvecs_w * (1. / torch.sqrt(eigvals_w))) @ eigvecs_w.transpose(-1, -2)\n",
        "    S_lidar = invsqrt_w @ S_b @ invsqrt_w\n",
        "    lam = torch.linalg.eigh(S_lidar)[0].clamp(min=0)\n",
        "    p = lam / lam.sum() + eps\n",
        "    return torch.exp(-torch.sum(p * torch.log(p)))\n",
        "\n",
        "# sx = torch.randn(32, 128)\n",
        "# print(sx.shape)\n",
        "# lidar = LiDAR(sx)\n",
        "# print(lidar.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "2Nd-sGe6Ku4S",
        "outputId": "ab4e875c-5052-4b35-ef27-97f848d6a4a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>█▅▅▆▄▆▅▅▅▅▅▃▃▆▄▅▄▅▆▅▄▄▄▂▄▂▂▃▃▂▄▂▁▂▄▂▃▂▂▃</td></tr><tr><td>correct</td><td>▂▄▁▃▁▃▃▂▄▄▂▃▃▅▃▄▄▆▄▅▃▄▅▅▄▅▆▇▆▃▆▇▄▅▄▄▅▇█▅</td></tr><tr><td>lidar</td><td>▃▅▃▅▄▃▇▄▄▄▅▅▆▆▃▄▅▅▅▅▅▄▅▄▅▄▅▄▄▄█▆▁▇▇▃▅▄▅▂</td></tr><tr><td>loss</td><td>▁▂▃▃▃▂▄▂▃▃▅▃▃▃▄▄▅▄▃▄▇▄▃▅▃▇▅▄▆▅█▄▅▇▅▄▆▅▃▅</td></tr><tr><td>rankme</td><td>▁▃▃▄▄▄▄▄▄▅▅▆▆▆▆▆▆▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>1.68835</td></tr><tr><td>correct</td><td>0.39062</td></tr><tr><td>lidar</td><td>11.46467</td></tr><tr><td>loss</td><td>0.2822</td></tr><tr><td>rankme</td><td>22.55086</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">eager-tree-90</strong> at: <a href='https://wandb.ai/bobdole/ijepa/runs/aymdmtg3' target=\"_blank\">https://wandb.ai/bobdole/ijepa/runs/aymdmtg3</a><br> View project at: <a href='https://wandb.ai/bobdole/ijepa' target=\"_blank\">https://wandb.ai/bobdole/ijepa</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250425_042620-aymdmtg3/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250425_052235-hgy4t99p</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/ijepa/runs/hgy4t99p' target=\"_blank\">different-music-91</a></strong> to <a href='https://wandb.ai/bobdole/ijepa' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/ijepa' target=\"_blank\">https://wandb.ai/bobdole/ijepa</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/ijepa/runs/hgy4t99p' target=\"_blank\">https://wandb.ai/bobdole/ijepa/runs/hgy4t99p</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "!pip install -q wandb\n",
        "import wandb # https://docs.wandb.ai/quickstart\n",
        "wandb.login(key='487a2109e55dce4e13fc70681781de9f50f27be7')\n",
        "try: run.finish()\n",
        "except NameError: pass\n",
        "run = wandb.init(project=\"ijepa\", config={\"model\": \"res18\",})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "34de8800-fa4b-4460-803c-3cd8dbfb6c70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5429c48-e190-4091-fdd9-fcb9eb4949f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "strain 1.9913508892059326\n",
            "strain 0.8377746939659119\n",
            "strain 0.7852575182914734\n",
            "strain 0.8044323921203613\n",
            "strain 0.8042545318603516\n",
            "strain 0.7597675919532776\n",
            "classify 2.4066162109375\n",
            "classify 2.4217529296875\n",
            "classify 2.4010009765625\n",
            "classify 2.489990234375\n",
            "classify 2.296142578125\n",
            "classify 2.400390625\n",
            "classify 2.48046875\n",
            "classify 2.3642578125\n",
            "classify 2.37060546875\n",
            "classify 2.37548828125\n",
            "classify 2.3211669921875\n",
            "0.109375\n",
            "0.109375\n",
            "0.0625\n",
            "0.109375\n",
            "0.15625\n",
            "0.125\n",
            "0.125\n",
            "0.140625\n",
            "0.125\n",
            "0.125\n",
            "0.109375\n",
            "time: 4.655221939086914 4.65522313117981\n",
            "1\n",
            "strain 0.7883387804031372\n",
            "strain 0.7660420536994934\n",
            "strain 0.7952257990837097\n",
            "strain 0.7429350018501282\n",
            "strain 0.7149000763893127\n",
            "strain 0.7253251075744629\n",
            "classify 2.3016357421875\n",
            "classify 2.3302001953125\n",
            "classify 2.3592529296875\n",
            "classify 2.393310546875\n",
            "classify 2.394775390625\n",
            "classify 2.4605712890625\n",
            "classify 2.3785400390625\n",
            "classify 2.4632568359375\n",
            "classify 2.37939453125\n",
            "classify 2.2816162109375\n",
            "classify 2.32763671875\n",
            "0.078125\n",
            "0.078125\n",
            "0.09375\n",
            "0.0625\n",
            "0.15625\n",
            "0.0625\n",
            "0.15625\n",
            "0.125\n",
            "0.078125\n",
            "0.078125\n",
            "0.109375\n",
            "time: 3.8278968334198 4.241783022880554\n",
            "2\n",
            "strain 0.6349595785140991\n",
            "strain 0.7173463702201843\n",
            "strain 0.6673377156257629\n",
            "strain 0.6611346006393433\n",
            "strain 0.6046431064605713\n",
            "strain 0.5755916833877563\n",
            "classify 2.4569091796875\n",
            "classify 2.32958984375\n",
            "classify 2.2833251953125\n",
            "classify 2.413330078125\n",
            "classify 2.3350830078125\n",
            "classify 2.3385009765625\n",
            "classify 2.3302001953125\n",
            "classify 2.319091796875\n",
            "classify 2.3568115234375\n",
            "classify 2.3494873046875\n",
            "classify 2.2777099609375\n",
            "0.03125\n",
            "0.109375\n",
            "0.1875\n",
            "0.09375\n",
            "0.078125\n",
            "0.203125\n",
            "0.203125\n",
            "0.125\n",
            "0.078125\n",
            "0.171875\n",
            "0.09375\n",
            "time: 3.857832670211792 4.113953510920207\n",
            "3\n",
            "strain 0.5423411130905151\n",
            "strain 0.5391519665718079\n",
            "strain 0.5404828786849976\n",
            "strain 0.5395541191101074\n",
            "strain 0.4877735674381256\n",
            "strain 0.4114471971988678\n",
            "classify 2.312255859375\n",
            "classify 2.3372802734375\n",
            "classify 2.3798828125\n",
            "classify 2.297607421875\n",
            "classify 2.3262939453125\n",
            "classify 2.3167724609375\n",
            "classify 2.330078125\n",
            "classify 2.414794921875\n",
            "classify 2.4140625\n",
            "classify 2.3394775390625\n",
            "classify 2.2235107421875\n",
            "0.078125\n",
            "0.1875\n",
            "0.09375\n",
            "0.140625\n",
            "0.15625\n",
            "0.234375\n",
            "0.078125\n",
            "0.15625\n",
            "0.0625\n",
            "0.109375\n",
            "0.09375\n",
            "time: 4.458709955215454 4.200459539890289\n",
            "4\n",
            "strain 0.4563484191894531\n",
            "strain 0.49393805861473083\n",
            "strain 0.5078994631767273\n",
            "strain 0.4355246126651764\n",
            "strain 0.4620479643344879\n",
            "strain 0.41200846433639526\n",
            "classify 2.34814453125\n",
            "classify 2.41796875\n",
            "classify 2.3941650390625\n",
            "classify 2.343505859375\n",
            "classify 2.39453125\n",
            "classify 2.340087890625\n",
            "classify 2.4068603515625\n",
            "classify 2.359619140625\n",
            "classify 2.32177734375\n",
            "classify 2.3441162109375\n",
            "classify 2.373779296875\n",
            "0.125\n",
            "0.171875\n",
            "0.109375\n",
            "0.140625\n",
            "0.09375\n",
            "0.15625\n",
            "0.125\n",
            "0.078125\n",
            "0.125\n",
            "0.140625\n",
            "0.109375\n",
            "time: 3.7622241973876953 4.112900018692017\n",
            "5\n",
            "strain 0.4126397669315338\n",
            "strain 0.3739764094352722\n",
            "strain 0.42056533694267273\n",
            "strain 0.3646315634250641\n",
            "strain 0.3885849416255951\n",
            "strain 0.38584813475608826\n",
            "classify 2.3470458984375\n",
            "classify 2.283203125\n",
            "classify 2.269775390625\n",
            "classify 2.3868408203125\n",
            "classify 2.365478515625\n",
            "classify 2.3729248046875\n",
            "classify 2.4013671875\n",
            "classify 2.3250732421875\n",
            "classify 2.36767578125\n",
            "classify 2.3470458984375\n",
            "classify 2.36279296875\n",
            "0.109375\n",
            "0.1875\n",
            "0.1875\n",
            "0.109375\n",
            "0.09375\n",
            "0.1875\n",
            "0.125\n",
            "0.09375\n",
            "0.078125\n",
            "0.109375\n",
            "0.109375\n",
            "time: 3.773009777069092 4.056314865748088\n",
            "6\n",
            "strain 0.37685850262641907\n",
            "strain 0.37548741698265076\n",
            "strain 0.3725740313529968\n",
            "strain 0.3566715717315674\n",
            "strain 0.4118068218231201\n",
            "strain 0.3810564875602722\n",
            "classify 2.4063720703125\n",
            "classify 2.356201171875\n",
            "classify 2.3701171875\n",
            "classify 2.3941650390625\n",
            "classify 2.333251953125\n",
            "classify 2.31640625\n",
            "classify 2.309814453125\n",
            "classify 2.3692626953125\n",
            "classify 2.2841796875\n",
            "classify 2.388427734375\n",
            "classify 2.3212890625\n",
            "0.109375\n",
            "0.078125\n",
            "0.140625\n",
            "0.0625\n",
            "0.0625\n",
            "0.09375\n",
            "0.078125\n",
            "0.109375\n",
            "0.078125\n",
            "0.125\n",
            "0.140625\n",
            "time: 4.544835090637207 4.126166786466326\n",
            "7\n",
            "strain 0.3518739640712738\n",
            "strain 0.3575195074081421\n",
            "strain 0.344402015209198\n",
            "strain 0.3158065676689148\n",
            "strain 0.3419170379638672\n",
            "strain 0.36458492279052734\n",
            "classify 2.236328125\n",
            "classify 2.3319091796875\n",
            "classify 2.3568115234375\n",
            "classify 2.2996826171875\n",
            "classify 2.482666015625\n",
            "classify 2.3564453125\n",
            "classify 2.320556640625\n",
            "classify 2.33447265625\n",
            "classify 2.28759765625\n",
            "classify 2.3658447265625\n",
            "classify 2.339111328125\n",
            "0.09375\n",
            "0.078125\n",
            "0.09375\n",
            "0.109375\n",
            "0.0625\n",
            "0.171875\n",
            "0.078125\n",
            "0.078125\n",
            "0.15625\n",
            "0.15625\n",
            "0.125\n",
            "time: 3.8258211612701416 4.088681161403656\n",
            "8\n",
            "strain 0.3368191719055176\n",
            "strain 0.30828166007995605\n",
            "strain 0.3761845529079437\n",
            "strain 0.3086754381656647\n",
            "strain 0.3487560749053955\n",
            "strain 0.3507424592971802\n",
            "classify 2.3104248046875\n",
            "classify 2.310546875\n",
            "classify 2.4530029296875\n",
            "classify 2.3223876953125\n",
            "classify 2.383544921875\n",
            "classify 2.369873046875\n",
            "classify 2.3720703125\n",
            "classify 2.32568359375\n",
            "classify 2.351806640625\n",
            "classify 2.3702392578125\n",
            "classify 2.30859375\n",
            "0.09375\n",
            "0.125\n",
            "0.109375\n",
            "0.09375\n",
            "0.09375\n",
            "0.109375\n",
            "0.078125\n",
            "0.21875\n",
            "0.140625\n",
            "0.046875\n",
            "0.0625\n",
            "time: 3.8488869667053223 4.062081495920817\n",
            "9\n",
            "strain 0.3090946674346924\n",
            "strain 0.27603060007095337\n",
            "strain 0.33283525705337524\n",
            "strain 0.30587658286094666\n",
            "strain 0.3543107211589813\n",
            "strain 0.3375624716281891\n",
            "classify 2.34814453125\n",
            "classify 2.3448486328125\n",
            "classify 2.4044189453125\n",
            "classify 2.3865966796875\n",
            "classify 2.3333740234375\n",
            "classify 2.3924560546875\n",
            "classify 2.3602294921875\n",
            "classify 2.31201171875\n",
            "classify 2.2691650390625\n",
            "classify 2.3524169921875\n",
            "classify 2.3316650390625\n",
            "0.09375\n",
            "0.125\n",
            "0.15625\n",
            "0.078125\n",
            "0.09375\n",
            "0.140625\n",
            "0.09375\n",
            "0.140625\n",
            "0.125\n",
            "0.140625\n",
            "0.078125\n",
            "time: 4.550578594207764 4.110970592498779\n",
            "10\n",
            "strain 0.35538774728775024\n",
            "strain 0.328288197517395\n",
            "strain 0.3252568244934082\n",
            "strain 0.27554306387901306\n",
            "strain 0.3058236241340637\n",
            "strain 0.30060112476348877\n",
            "classify 2.3070068359375\n",
            "classify 2.3228759765625\n",
            "classify 2.383544921875\n",
            "classify 2.3662109375\n",
            "classify 2.2645263671875\n",
            "classify 2.357666015625\n",
            "classify 2.36572265625\n",
            "classify 2.340576171875\n",
            "classify 2.3028564453125\n",
            "classify 2.35595703125\n",
            "classify 2.307861328125\n",
            "0.15625\n",
            "0.109375\n",
            "0.0625\n",
            "0.078125\n",
            "0.09375\n",
            "0.0625\n",
            "0.15625\n",
            "0.109375\n",
            "0.078125\n",
            "0.15625\n",
            "0.15625\n",
            "time: 3.8905563354492188 4.090970559553667\n",
            "11\n",
            "strain 0.29467591643333435\n",
            "strain 0.2830151617527008\n",
            "strain 0.31222349405288696\n",
            "strain 0.3070089519023895\n",
            "strain 0.27833664417266846\n",
            "strain 0.2681891620159149\n",
            "classify 2.2958984375\n",
            "classify 2.313720703125\n",
            "classify 2.2772216796875\n",
            "classify 2.3414306640625\n",
            "classify 2.25341796875\n",
            "classify 2.284423828125\n",
            "classify 2.3199462890625\n",
            "classify 2.34423828125\n",
            "classify 2.2767333984375\n",
            "classify 2.3583984375\n",
            "classify 2.3609619140625\n",
            "0.140625\n",
            "0.171875\n",
            "0.046875\n",
            "0.125\n",
            "0.140625\n",
            "0.15625\n",
            "0.171875\n",
            "0.1875\n",
            "0.0625\n",
            "0.140625\n",
            "0.109375\n",
            "time: 3.7928719520568848 4.066164950529735\n",
            "12\n",
            "strain 0.2737172842025757\n",
            "strain 0.3012405037879944\n",
            "strain 0.26656243205070496\n",
            "strain 0.25331807136535645\n",
            "strain 0.2933475077152252\n",
            "strain 0.2745635211467743\n",
            "classify 2.36328125\n",
            "classify 2.2757568359375\n",
            "classify 2.3392333984375\n",
            "classify 2.30419921875\n",
            "classify 2.3309326171875\n",
            "classify 2.3096923828125\n",
            "classify 2.3594970703125\n",
            "classify 2.304931640625\n",
            "classify 2.350830078125\n",
            "classify 2.3577880859375\n",
            "classify 2.2607421875\n",
            "0.1875\n",
            "0.125\n",
            "0.203125\n",
            "0.140625\n",
            "0.109375\n",
            "0.125\n",
            "0.125\n",
            "0.1875\n",
            "0.09375\n",
            "0.1875\n",
            "0.171875\n",
            "time: 4.27700662612915 4.0824154523702765\n",
            "13\n",
            "strain 0.24776138365268707\n",
            "strain 0.25463297963142395\n",
            "strain 0.24674595892429352\n",
            "strain 0.2504701614379883\n",
            "strain 0.2579283118247986\n",
            "strain 0.2799617350101471\n",
            "classify 2.384765625\n",
            "classify 2.310302734375\n",
            "classify 2.2586669921875\n",
            "classify 2.3092041015625\n",
            "classify 2.36181640625\n",
            "classify 2.2750244140625\n",
            "classify 2.3363037109375\n",
            "classify 2.3505859375\n",
            "classify 2.3349609375\n",
            "classify 2.2742919921875\n",
            "classify 2.2984619140625\n",
            "0.125\n",
            "0.21875\n",
            "0.15625\n",
            "0.203125\n",
            "0.125\n",
            "0.1875\n",
            "0.21875\n",
            "0.109375\n",
            "0.203125\n",
            "0.125\n",
            "0.125\n",
            "time: 3.9720654487609863 4.074611800057547\n",
            "14\n",
            "strain 0.2868974208831787\n",
            "strain 0.24225012958049774\n",
            "strain 0.28807440400123596\n",
            "strain 0.2580496072769165\n",
            "strain 0.2686520516872406\n",
            "strain 0.2547808289527893\n",
            "classify 2.279296875\n",
            "classify 2.346435546875\n",
            "classify 2.2855224609375\n",
            "classify 2.29443359375\n",
            "classify 2.33203125\n",
            "classify 2.3355712890625\n",
            "classify 2.2777099609375\n",
            "classify 2.30078125\n",
            "classify 2.372802734375\n",
            "classify 2.33203125\n",
            "classify 2.2901611328125\n",
            "0.15625\n",
            "0.171875\n",
            "0.125\n",
            "0.09375\n",
            "0.28125\n",
            "0.09375\n",
            "0.15625\n",
            "0.171875\n",
            "0.140625\n",
            "0.15625\n",
            "0.1875\n",
            "time: 3.8068554401397705 4.056790208816528\n",
            "15\n",
            "strain 0.2730838656425476\n",
            "strain 0.320667564868927\n",
            "strain 0.25945326685905457\n",
            "strain 0.2656153738498688\n",
            "strain 0.2929438650608063\n",
            "strain 0.2842289209365845\n",
            "classify 2.317626953125\n",
            "classify 2.330078125\n",
            "classify 2.34912109375\n",
            "classify 2.3416748046875\n",
            "classify 2.358642578125\n",
            "classify 2.2149658203125\n",
            "classify 2.3221435546875\n",
            "classify 2.261474609375\n",
            "classify 2.23095703125\n",
            "classify 2.3612060546875\n",
            "classify 2.290771484375\n",
            "0.140625\n",
            "0.203125\n",
            "0.171875\n",
            "0.1875\n",
            "0.140625\n",
            "0.046875\n",
            "0.15625\n",
            "0.125\n",
            "0.140625\n",
            "0.09375\n",
            "0.1875\n",
            "time: 4.145597696304321 4.0623666644096375\n",
            "16\n",
            "strain 0.2617771029472351\n",
            "strain 0.2553898096084595\n",
            "strain 0.2942555546760559\n",
            "strain 0.3040178418159485\n",
            "strain 0.23844024538993835\n",
            "strain 0.27650076150894165\n",
            "classify 2.3424072265625\n",
            "classify 2.2554931640625\n",
            "classify 2.341064453125\n",
            "classify 2.2978515625\n",
            "classify 2.2940673828125\n",
            "classify 2.3399658203125\n",
            "classify 2.23291015625\n",
            "classify 2.2252197265625\n",
            "classify 2.323486328125\n",
            "classify 2.31494140625\n",
            "classify 2.293212890625\n",
            "0.140625\n",
            "0.15625\n",
            "0.21875\n",
            "0.171875\n",
            "0.203125\n",
            "0.234375\n",
            "0.09375\n",
            "0.1875\n",
            "0.171875\n",
            "0.203125\n",
            "0.109375\n",
            "time: 4.123153448104858 4.065968836055083\n",
            "17\n",
            "strain 0.26695573329925537\n",
            "strain 0.2618858218193054\n",
            "strain 0.2724370062351227\n",
            "strain 0.29783591628074646\n",
            "strain 0.27881598472595215\n",
            "strain 0.29899105429649353\n",
            "classify 2.292236328125\n",
            "classify 2.3026123046875\n",
            "classify 2.3533935546875\n",
            "classify 2.26171875\n",
            "classify 2.2686767578125\n",
            "classify 2.278564453125\n",
            "classify 2.271484375\n",
            "classify 2.3037109375\n",
            "classify 2.2852783203125\n",
            "classify 2.265625\n",
            "classify 2.2950439453125\n",
            "0.140625\n",
            "0.125\n",
            "0.140625\n",
            "0.09375\n",
            "0.078125\n",
            "0.140625\n",
            "0.046875\n",
            "0.125\n",
            "0.15625\n",
            "0.09375\n",
            "0.171875\n",
            "time: 3.8265540599823 4.052691645092434\n",
            "18\n",
            "strain 0.24870194494724274\n",
            "strain 0.28131943941116333\n",
            "strain 0.2515700161457062\n",
            "strain 0.28453493118286133\n",
            "strain 0.2590748071670532\n",
            "strain 0.30534517765045166\n",
            "classify 2.30419921875\n",
            "classify 2.30859375\n",
            "classify 2.3197021484375\n",
            "classify 2.293701171875\n",
            "classify 2.271484375\n",
            "classify 2.3233642578125\n",
            "classify 2.2861328125\n",
            "classify 2.318115234375\n",
            "classify 2.3919677734375\n",
            "classify 2.349853515625\n",
            "classify 2.2896728515625\n",
            "0.265625\n",
            "0.1875\n",
            "0.125\n",
            "0.109375\n",
            "0.109375\n",
            "0.125\n",
            "0.125\n",
            "0.078125\n",
            "0.09375\n",
            "0.15625\n",
            "0.15625\n",
            "time: 4.028429746627808 4.0514382814106185\n",
            "19\n",
            "strain 0.3038766384124756\n",
            "strain 0.32548534870147705\n",
            "strain 0.28848183155059814\n",
            "strain 0.25832200050354004\n",
            "strain 0.27354422211647034\n",
            "strain 0.2556293308734894\n",
            "classify 2.25390625\n",
            "classify 2.291015625\n",
            "classify 2.27294921875\n",
            "classify 2.28564453125\n",
            "classify 2.3118896484375\n",
            "classify 2.320556640625\n",
            "classify 2.254638671875\n",
            "classify 2.30078125\n",
            "classify 2.34423828125\n",
            "classify 2.3421630859375\n",
            "classify 2.261962890625\n",
            "0.15625\n",
            "0.125\n",
            "0.109375\n",
            "0.15625\n",
            "0.15625\n",
            "0.125\n",
            "0.125\n",
            "0.109375\n",
            "0.15625\n",
            "0.203125\n",
            "0.171875\n",
            "time: 4.270534992218018 4.062441205978393\n",
            "20\n",
            "strain 0.3079080283641815\n",
            "strain 0.250135213136673\n",
            "strain 0.28791913390159607\n",
            "strain 0.2717490792274475\n",
            "strain 0.23700444400310516\n",
            "strain 0.24135714769363403\n",
            "classify 2.2548828125\n",
            "classify 2.3587646484375\n",
            "classify 2.2169189453125\n",
            "classify 2.260986328125\n",
            "classify 2.3040771484375\n",
            "classify 2.300537109375\n",
            "classify 2.2762451171875\n",
            "classify 2.3382568359375\n",
            "classify 2.2689208984375\n",
            "classify 2.2623291015625\n",
            "classify 2.2747802734375\n",
            "0.046875\n",
            "0.09375\n",
            "0.109375\n",
            "0.078125\n",
            "0.21875\n",
            "0.09375\n",
            "0.25\n",
            "0.140625\n",
            "0.140625\n",
            "0.140625\n",
            "0.046875\n",
            "time: 3.7943437099456787 4.049694390524001\n",
            "21\n",
            "strain 0.2566329836845398\n",
            "strain 0.264883428812027\n",
            "strain 0.29241132736206055\n",
            "strain 0.2587665617465973\n",
            "strain 0.2564868927001953\n",
            "strain 0.2545626163482666\n",
            "classify 2.34521484375\n",
            "classify 2.263427734375\n",
            "classify 2.267333984375\n",
            "classify 2.3109130859375\n",
            "classify 2.2099609375\n",
            "classify 2.274658203125\n",
            "classify 2.267578125\n",
            "classify 2.34326171875\n",
            "classify 2.275634765625\n",
            "classify 2.3421630859375\n",
            "classify 2.2816162109375\n",
            "0.15625\n",
            "0.171875\n",
            "0.109375\n",
            "0.171875\n",
            "0.171875\n",
            "0.140625\n",
            "0.078125\n",
            "0.15625\n",
            "0.0625\n",
            "0.0625\n",
            "0.09375\n",
            "time: 3.899219512939453 4.042874227870595\n",
            "22\n",
            "strain 0.23835356533527374\n",
            "strain 0.2553893029689789\n",
            "strain 0.2146259993314743\n",
            "strain 0.2336122989654541\n",
            "strain 0.2881123721599579\n",
            "strain 0.28561481833457947\n",
            "classify 2.2783203125\n",
            "classify 2.2972412109375\n",
            "classify 2.3021240234375\n",
            "classify 2.27978515625\n",
            "classify 2.3416748046875\n",
            "classify 2.2550048828125\n",
            "classify 2.2916259765625\n",
            "classify 2.3013916015625\n",
            "classify 2.2481689453125\n",
            "classify 2.2958984375\n",
            "classify 2.300537109375\n",
            "0.140625\n",
            "0.171875\n",
            "0.078125\n",
            "0.078125\n",
            "0.0625\n",
            "0.125\n",
            "0.09375\n",
            "0.1875\n",
            "0.125\n",
            "0.171875\n",
            "0.109375\n",
            "time: 4.36663031578064 4.056970886562182\n",
            "23\n",
            "strain 0.27078595757484436\n",
            "strain 0.2900833785533905\n",
            "strain 0.2993784546852112\n",
            "strain 0.25618797540664673\n",
            "strain 0.26031050086021423\n",
            "strain 0.23448297381401062\n",
            "classify 2.3270263671875\n",
            "classify 2.30908203125\n",
            "classify 2.2728271484375\n",
            "classify 2.2733154296875\n",
            "classify 2.3226318359375\n",
            "classify 2.31591796875\n",
            "classify 2.2847900390625\n",
            "classify 2.3116455078125\n",
            "classify 2.250244140625\n",
            "classify 2.277099609375\n",
            "classify 2.302978515625\n",
            "0.140625\n",
            "0.09375\n",
            "0.203125\n",
            "0.171875\n",
            "0.109375\n",
            "0.078125\n",
            "0.140625\n",
            "0.171875\n",
            "0.078125\n",
            "0.15625\n",
            "0.15625\n",
            "time: 3.7968480587005615 4.046155045429866\n",
            "24\n",
            "strain 0.3046022057533264\n",
            "strain 0.237710103392601\n",
            "strain 0.3254150152206421\n",
            "strain 0.23148207366466522\n",
            "strain 0.2826162278652191\n",
            "strain 0.25286972522735596\n",
            "classify 2.2978515625\n",
            "classify 2.33740234375\n",
            "classify 2.2860107421875\n",
            "classify 2.309814453125\n",
            "classify 2.3333740234375\n",
            "classify 2.3165283203125\n",
            "classify 2.2291259765625\n",
            "classify 2.2667236328125\n",
            "classify 2.3448486328125\n",
            "classify 2.205322265625\n",
            "classify 2.27783203125\n",
            "0.109375\n",
            "0.0625\n",
            "0.09375\n",
            "0.125\n",
            "0.109375\n",
            "0.109375\n",
            "0.0625\n",
            "0.09375\n",
            "0.109375\n",
            "0.109375\n",
            "0.15625\n",
            "time: 3.7812671661376953 4.0355764007568355\n",
            "25\n",
            "strain 0.3007441759109497\n",
            "strain 0.2873850464820862\n",
            "strain 0.2107059210538864\n",
            "strain 0.2899145781993866\n",
            "strain 0.2550472319126129\n",
            "strain 0.2655291259288788\n",
            "classify 2.2596435546875\n",
            "classify 2.2825927734375\n",
            "classify 2.3037109375\n",
            "classify 2.2479248046875\n",
            "classify 2.2879638671875\n",
            "classify 2.3021240234375\n",
            "classify 2.3009033203125\n",
            "classify 2.2816162109375\n",
            "classify 2.2674560546875\n",
            "classify 2.2855224609375\n",
            "classify 2.2689208984375\n",
            "0.140625\n",
            "0.09375\n",
            "0.171875\n",
            "0.125\n",
            "0.109375\n",
            "0.125\n",
            "0.125\n",
            "0.125\n",
            "0.140625\n",
            "0.109375\n",
            "0.140625\n",
            "time: 4.47315526008606 4.052421799072852\n",
            "26\n",
            "strain 0.24641259014606476\n",
            "strain 0.25631776452064514\n",
            "strain 0.28427034616470337\n",
            "strain 0.2952897548675537\n",
            "strain 0.27101364731788635\n",
            "strain 0.2874298691749573\n",
            "classify 2.2628173828125\n",
            "classify 2.3182373046875\n",
            "classify 2.238037109375\n",
            "classify 2.2725830078125\n",
            "classify 2.1802978515625\n",
            "classify 2.2772216796875\n",
            "classify 2.26904296875\n",
            "classify 2.249755859375\n",
            "classify 2.275634765625\n",
            "classify 2.23583984375\n",
            "classify 2.372314453125\n",
            "0.0625\n",
            "0.109375\n",
            "0.125\n",
            "0.140625\n",
            "0.109375\n",
            "0.078125\n",
            "0.09375\n",
            "0.109375\n",
            "0.078125\n",
            "0.140625\n",
            "0.125\n",
            "time: 3.797396659851074 4.042993969387478\n",
            "27\n",
            "strain 0.31110262870788574\n",
            "strain 0.253018856048584\n",
            "strain 0.26130667328834534\n",
            "strain 0.28138911724090576\n",
            "strain 0.31047946214675903\n",
            "strain 0.2381342351436615\n",
            "classify 2.2857666015625\n",
            "classify 2.3258056640625\n",
            "classify 2.282958984375\n",
            "classify 2.2105712890625\n",
            "classify 2.262939453125\n",
            "classify 2.28564453125\n",
            "classify 2.281982421875\n",
            "classify 2.3065185546875\n",
            "classify 2.2799072265625\n",
            "classify 2.3155517578125\n",
            "classify 2.3084716796875\n",
            "0.078125\n",
            "0.171875\n",
            "0.109375\n",
            "0.109375\n",
            "0.109375\n",
            "0.171875\n",
            "0.0625\n",
            "0.078125\n",
            "0.171875\n",
            "0.140625\n",
            "0.09375\n",
            "time: 3.773446559906006 4.03338234765189\n",
            "28\n",
            "strain 0.26369449496269226\n",
            "strain 0.24434581398963928\n",
            "strain 0.25684285163879395\n",
            "strain 0.22422178089618683\n",
            "strain 0.2660793960094452\n",
            "strain 0.24399206042289734\n",
            "classify 2.299072265625\n",
            "classify 2.3519287109375\n",
            "classify 2.26220703125\n",
            "classify 2.3653564453125\n",
            "classify 2.2552490234375\n",
            "classify 2.271240234375\n",
            "classify 2.2918701171875\n",
            "classify 2.261474609375\n",
            "classify 2.3343505859375\n",
            "classify 2.2864990234375\n",
            "classify 2.2894287109375\n",
            "0.109375\n",
            "0.125\n",
            "0.15625\n",
            "0.140625\n",
            "0.171875\n",
            "0.203125\n",
            "0.09375\n",
            "0.171875\n",
            "0.109375\n",
            "0.09375\n",
            "0.03125\n",
            "time: 4.459439754486084 4.0480896851112105\n",
            "29\n",
            "strain 0.2609962224960327\n",
            "strain 0.2303740680217743\n",
            "strain 0.3019154667854309\n",
            "strain 0.2515980005264282\n",
            "strain 0.25885090231895447\n",
            "strain 0.2538864016532898\n",
            "classify 2.2642822265625\n",
            "classify 2.2630615234375\n",
            "classify 2.3060302734375\n",
            "classify 2.300048828125\n",
            "classify 2.258056640625\n",
            "classify 2.2869873046875\n",
            "classify 2.344482421875\n",
            "classify 2.2803955078125\n",
            "classify 2.2723388671875\n",
            "classify 2.2196044921875\n",
            "classify 2.28662109375\n",
            "0.109375\n",
            "0.125\n",
            "0.109375\n",
            "0.125\n",
            "0.125\n",
            "0.078125\n",
            "0.140625\n",
            "0.140625\n",
            "0.1875\n",
            "0.15625\n",
            "0.140625\n",
            "time: 3.7771663665771484 4.039073586463928\n",
            "30\n",
            "strain 0.27656397223472595\n",
            "strain 0.23557186126708984\n",
            "strain 0.19784241914749146\n",
            "strain 0.23643428087234497\n",
            "strain 0.25687846541404724\n",
            "strain 0.2395082265138626\n",
            "classify 2.2845458984375\n",
            "classify 2.2247314453125\n",
            "classify 2.219482421875\n",
            "classify 2.2525634765625\n",
            "classify 2.2958984375\n",
            "classify 2.322265625\n",
            "classify 2.2984619140625\n",
            "classify 2.2628173828125\n",
            "classify 2.2904052734375\n",
            "classify 2.2574462890625\n",
            "classify 2.254150390625\n",
            "0.125\n",
            "0.09375\n",
            "0.09375\n",
            "0.078125\n",
            "0.078125\n",
            "0.0625\n",
            "0.0625\n",
            "0.015625\n",
            "0.078125\n",
            "0.046875\n",
            "0.09375\n",
            "time: 3.7931697368621826 4.031154847914173\n",
            "31\n",
            "strain 0.2334119826555252\n",
            "strain 0.2628736197948456\n",
            "strain 0.21881219744682312\n",
            "strain 0.24912211298942566\n",
            "strain 0.2443251609802246\n",
            "strain 0.23633864521980286\n",
            "classify 2.280517578125\n",
            "classify 2.2716064453125\n",
            "classify 2.2994384765625\n",
            "classify 2.323486328125\n",
            "classify 2.3116455078125\n",
            "classify 2.3095703125\n",
            "classify 2.258544921875\n",
            "classify 2.245361328125\n",
            "classify 2.236572265625\n",
            "classify 2.237548828125\n",
            "classify 2.31005859375\n",
            "0.125\n",
            "0.109375\n",
            "0.078125\n",
            "0.09375\n",
            "0.0625\n",
            "0.0625\n",
            "0.140625\n",
            "0.140625\n",
            "0.171875\n",
            "0.140625\n",
            "0.109375\n",
            "time: 4.5006935596466064 4.045847594738007\n",
            "32\n",
            "strain 0.26091688871383667\n",
            "strain 0.23322753608226776\n",
            "strain 0.2633047103881836\n",
            "strain 0.21210618317127228\n",
            "strain 0.21456098556518555\n",
            "strain 0.29147887229919434\n",
            "classify 2.2696533203125\n",
            "classify 2.2535400390625\n",
            "classify 2.29736328125\n",
            "classify 2.2708740234375\n",
            "classify 2.2921142578125\n",
            "classify 2.2264404296875\n",
            "classify 2.2191162109375\n",
            "classify 2.271240234375\n",
            "classify 2.2744140625\n",
            "classify 2.26513671875\n",
            "classify 2.24365234375\n",
            "0.09375\n",
            "0.125\n",
            "0.09375\n",
            "0.140625\n",
            "0.109375\n",
            "0.109375\n",
            "0.140625\n",
            "0.171875\n",
            "0.1875\n",
            "0.125\n",
            "0.125\n",
            "time: 3.821241855621338 4.0390557303573145\n",
            "33\n",
            "strain 0.21665826439857483\n",
            "strain 0.22983768582344055\n",
            "strain 0.24154043197631836\n",
            "strain 0.2736212909221649\n",
            "strain 0.2860238254070282\n",
            "strain 0.23262453079223633\n",
            "classify 2.2730712890625\n",
            "classify 2.2344970703125\n",
            "classify 2.2843017578125\n",
            "classify 2.238037109375\n",
            "classify 2.3050537109375\n",
            "classify 2.24267578125\n",
            "classify 2.30615234375\n",
            "classify 2.247314453125\n",
            "classify 2.26806640625\n",
            "classify 2.2523193359375\n",
            "classify 2.233154296875\n",
            "0.125\n",
            "0.0625\n",
            "0.078125\n",
            "0.0625\n",
            "0.15625\n",
            "0.046875\n",
            "0.15625\n",
            "0.09375\n",
            "0.109375\n",
            "0.046875\n",
            "0.0625\n",
            "time: 3.800283670425415 4.032045869266286\n",
            "34\n",
            "strain 0.20365269482135773\n",
            "strain 0.24588271975517273\n",
            "strain 0.2553715109825134\n",
            "strain 0.23232407867908478\n",
            "strain 0.21771758794784546\n",
            "strain 0.28428205847740173\n",
            "classify 2.270263671875\n",
            "classify 2.25341796875\n",
            "classify 2.265625\n",
            "classify 2.3248291015625\n",
            "classify 2.2493896484375\n",
            "classify 2.236572265625\n",
            "classify 2.2713623046875\n",
            "classify 2.2650146484375\n",
            "classify 2.2525634765625\n",
            "classify 2.2181396484375\n",
            "classify 2.3565673828125\n",
            "0.109375\n",
            "0.078125\n",
            "0.171875\n",
            "0.0625\n",
            "0.140625\n",
            "0.15625\n",
            "0.125\n",
            "0.140625\n",
            "0.09375\n",
            "0.140625\n",
            "0.15625\n",
            "time: 4.451557636260986 4.044043513706752\n",
            "35\n",
            "strain 0.26421114802360535\n",
            "strain 0.22944360971450806\n",
            "strain 0.30881452560424805\n",
            "strain 0.24925629794597626\n",
            "strain 0.23943302035331726\n",
            "strain 0.24860644340515137\n",
            "classify 2.300048828125\n",
            "classify 2.20947265625\n",
            "classify 2.300537109375\n",
            "classify 2.30517578125\n",
            "classify 2.2904052734375\n",
            "classify 2.2474365234375\n",
            "classify 2.32275390625\n",
            "classify 2.236083984375\n",
            "classify 2.2353515625\n",
            "classify 2.259521484375\n",
            "classify 2.267578125\n",
            "0.140625\n",
            "0.09375\n",
            "0.125\n",
            "0.09375\n",
            "0.140625\n",
            "0.109375\n",
            "0.09375\n",
            "0.15625\n",
            "0.140625\n",
            "0.203125\n",
            "0.09375\n",
            "time: 4.047600984573364 4.044154153929816\n",
            "36\n",
            "strain 0.2488706111907959\n",
            "strain 0.24885854125022888\n",
            "strain 0.2453227937221527\n",
            "strain 0.24158413708209991\n",
            "strain 0.24427397549152374\n",
            "strain 0.2282094955444336\n",
            "classify 2.297119140625\n",
            "classify 2.2972412109375\n",
            "classify 2.29638671875\n",
            "classify 2.2696533203125\n",
            "classify 2.2073974609375\n",
            "classify 2.275146484375\n",
            "classify 2.26611328125\n",
            "classify 2.2059326171875\n",
            "classify 2.2603759765625\n",
            "classify 2.2314453125\n",
            "classify 2.2464599609375\n",
            "0.0625\n",
            "0.125\n",
            "0.09375\n",
            "0.125\n",
            "0.09375\n",
            "0.078125\n",
            "0.078125\n",
            "0.125\n",
            "0.140625\n",
            "0.140625\n",
            "0.125\n",
            "time: 3.8042423725128174 4.037682030652021\n",
            "37\n",
            "strain 0.29241976141929626\n",
            "strain 0.26208215951919556\n",
            "strain 0.2382487952709198\n",
            "strain 0.21459634602069855\n",
            "strain 0.25068485736846924\n",
            "strain 0.2621966004371643\n",
            "classify 2.2415771484375\n",
            "classify 2.2486572265625\n",
            "classify 2.25048828125\n",
            "classify 2.2962646484375\n",
            "classify 2.283447265625\n",
            "classify 2.2841796875\n",
            "classify 2.216552734375\n",
            "classify 2.27587890625\n",
            "classify 2.2864990234375\n",
            "classify 2.2581787109375\n",
            "classify 2.281005859375\n",
            "0.140625\n",
            "0.078125\n",
            "0.0625\n",
            "0.140625\n",
            "0.09375\n",
            "0.125\n",
            "0.125\n",
            "0.09375\n",
            "0.09375\n",
            "0.109375\n",
            "0.09375\n",
            "time: 4.479795217514038 4.049328000921952\n",
            "38\n",
            "strain 0.30858588218688965\n",
            "strain 0.37111133337020874\n",
            "strain 0.23085013031959534\n",
            "strain 0.2035336047410965\n",
            "strain 0.26531538367271423\n",
            "strain 0.21891672909259796\n",
            "classify 2.2652587890625\n",
            "classify 2.2745361328125\n",
            "classify 2.2833251953125\n",
            "classify 2.2677001953125\n",
            "classify 2.2601318359375\n",
            "classify 2.2574462890625\n",
            "classify 2.251708984375\n",
            "classify 2.2401123046875\n",
            "classify 2.2562255859375\n",
            "classify 2.2445068359375\n",
            "classify 2.242431640625\n",
            "0.109375\n",
            "0.125\n",
            "0.109375\n",
            "0.140625\n",
            "0.046875\n",
            "0.046875\n",
            "0.125\n",
            "0.109375\n",
            "0.140625\n",
            "0.078125\n",
            "0.09375\n",
            "time: 3.8035435676574707 4.043037218925281\n",
            "39\n",
            "strain 0.2305900752544403\n",
            "strain 0.2158099263906479\n",
            "strain 0.2192518264055252\n",
            "strain 0.2812145948410034\n",
            "strain 0.22092589735984802\n",
            "strain 0.24476906657218933\n",
            "classify 2.252685546875\n",
            "classify 2.243896484375\n",
            "classify 2.1834716796875\n",
            "classify 2.2718505859375\n",
            "classify 2.2064208984375\n",
            "classify 2.35009765625\n",
            "classify 2.24609375\n",
            "classify 2.295166015625\n",
            "classify 2.2264404296875\n",
            "classify 2.2391357421875\n",
            "classify 2.274169921875\n",
            "0.09375\n",
            "0.125\n",
            "0.109375\n",
            "0.140625\n",
            "0.140625\n",
            "0.078125\n",
            "0.203125\n",
            "0.125\n",
            "0.140625\n",
            "0.140625\n",
            "0.0625\n",
            "time: 3.7464427947998047 4.035633373260498\n",
            "40\n",
            "strain 0.27297696471214294\n",
            "strain 0.24990832805633545\n",
            "strain 0.2403971552848816\n",
            "strain 0.2785554528236389\n",
            "strain 0.2737868130207062\n",
            "strain 0.199888214468956\n",
            "classify 2.254150390625\n",
            "classify 2.231689453125\n",
            "classify 2.21240234375\n",
            "classify 2.291748046875\n",
            "classify 2.2913818359375\n",
            "classify 2.3065185546875\n",
            "classify 2.3021240234375\n",
            "classify 2.281494140625\n",
            "classify 2.2734375\n",
            "classify 2.289794921875\n",
            "classify 2.278076171875\n",
            "0.171875\n",
            "0.171875\n",
            "0.125\n",
            "0.140625\n",
            "0.125\n",
            "0.109375\n",
            "0.234375\n",
            "0.171875\n",
            "0.125\n",
            "0.046875\n",
            "0.046875\n",
            "time: 4.28265905380249 4.041671915752132\n",
            "41\n",
            "strain 0.25281816720962524\n",
            "strain 0.22501055896282196\n",
            "strain 0.2871439456939697\n",
            "strain 0.25140658020973206\n",
            "strain 0.23585335910320282\n",
            "strain 0.2822120487689972\n",
            "classify 2.2740478515625\n",
            "classify 2.299072265625\n",
            "classify 2.1767578125\n",
            "classify 2.201904296875\n",
            "classify 2.2989501953125\n",
            "classify 2.276611328125\n",
            "classify 2.23583984375\n",
            "classify 2.228515625\n",
            "classify 2.2213134765625\n",
            "classify 2.26171875\n",
            "classify 2.2615966796875\n",
            "0.140625\n",
            "0.046875\n",
            "0.125\n",
            "0.1875\n",
            "0.15625\n",
            "0.109375\n",
            "0.15625\n",
            "0.0625\n",
            "0.1875\n",
            "0.125\n",
            "0.125\n",
            "time: 4.016270637512207 4.041085850624811\n",
            "42\n",
            "strain 0.23761214315891266\n",
            "strain 0.21797595918178558\n",
            "strain 0.2238236367702484\n",
            "strain 0.21111445128917694\n",
            "strain 0.21802978217601776\n",
            "strain 0.25115227699279785\n",
            "classify 2.27783203125\n",
            "classify 2.316650390625\n",
            "classify 2.3011474609375\n",
            "classify 2.2479248046875\n",
            "classify 2.2569580078125\n",
            "classify 2.314697265625\n",
            "classify 2.239013671875\n",
            "classify 2.224609375\n",
            "classify 2.3070068359375\n",
            "classify 2.2337646484375\n",
            "classify 2.236083984375\n",
            "0.078125\n",
            "0.109375\n",
            "0.125\n",
            "0.140625\n",
            "0.0625\n",
            "0.1875\n",
            "0.125\n",
            "0.125\n",
            "0.234375\n",
            "0.140625\n",
            "0.109375\n",
            "time: 3.774695873260498 4.03490353185077\n",
            "43\n",
            "strain 0.27044594287872314\n",
            "strain 0.243721142411232\n",
            "strain 0.23425926268100739\n",
            "strain 0.24576255679130554\n",
            "strain 0.24458064138889313\n",
            "strain 0.23669423162937164\n",
            "classify 2.2940673828125\n",
            "classify 2.301513671875\n",
            "classify 2.2762451171875\n",
            "classify 2.311767578125\n",
            "classify 2.2960205078125\n",
            "classify 2.27197265625\n",
            "classify 2.2349853515625\n",
            "classify 2.2674560546875\n",
            "classify 2.21630859375\n",
            "classify 2.2318115234375\n",
            "classify 2.267822265625\n",
            "0.125\n",
            "0.15625\n",
            "0.109375\n",
            "0.140625\n",
            "0.109375\n",
            "0.046875\n",
            "0.171875\n",
            "0.109375\n",
            "0.125\n",
            "0.09375\n",
            "0.125\n",
            "time: 4.063175916671753 4.035555828701366\n",
            "44\n",
            "strain 0.22653600573539734\n",
            "strain 0.213967427611351\n",
            "strain 0.22776059806346893\n",
            "strain 0.22750239074230194\n",
            "strain 0.25636327266693115\n"
          ]
        }
      ],
      "source": [
        "# @title strain ctrain test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "def strain(model, dataloader, optim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.train()\n",
        "    for i, (x, _) in enumerate(dataloader):\n",
        "        x = x.to(device)#.to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            loss = model.loss(x)\n",
        "\n",
        "            # repr_loss, std_loss, cov_loss = model.loss(x)\n",
        "            # j_loss, (repr_loss, std_loss, cov_loss) = model.loss(x)\n",
        "            # loss = model.sim_coeff * repr_loss + model.std_coeff * std_loss + model.cov_coeff * cov_loss\n",
        "            # loss = j_loss + loss\n",
        "\n",
        "        optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 5) # 0.5\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            m=0.999 # 0.99 m = next(momentum_scheduler)\n",
        "            norms=[]\n",
        "            for param_q, param_k in zip(model.student.parameters(), model.teacher.parameters()):\n",
        "                param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        if i%10==0: print(\"strain\",loss.item())\n",
        "        # try: wandb.log({\"loss\": loss.item(), \"repr/I\": repr_loss.item(), \"std/V\": std_loss.item(), \"cov/C\": cov_loss.item()})\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=50: break\n",
        "\n",
        "\n",
        "def ctrain(model, classifier, dataloader, coptim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.eval()\n",
        "    classifier.train()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            with torch.no_grad():\n",
        "                sx = model(x).detach()\n",
        "            y_ = classifier(sx)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "        coptim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(coptim)\n",
        "        scaler.update()\n",
        "        print(\"classify\",loss.item())\n",
        "        try: wandb.log({\"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        with torch.no_grad():\n",
        "            sx = model(x)\n",
        "            rankme = RankMe(sx).item()\n",
        "            lidar = LiDAR(sx).item()\n",
        "            y_ = classifier(sx)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        print(correct/len(y))\n",
        "        # try: wandb.log({\"correct\": correct/len(y)})\n",
        "        try: wandb.log({\"correct\": correct/len(y), \"rankme\": rankme, \"lidar\": lidar})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "import time\n",
        "start = begin = time.time()\n",
        "# for i in range(1):\n",
        "for i in range(1000): # 1000\n",
        "    print(i)\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    strain(ijepa, train_loader, optim)\n",
        "    ctrain(ijepa, classifier, train_loader, coptim)\n",
        "    test(ijepa, classifier, test_loader)\n",
        "\n",
        "    # strain(violet, train_loader, voptim)\n",
        "    # ctrain(violet, classifier, train_loader, coptim)\n",
        "    # test(violet, classifier, test_loader)\n",
        "\n",
        "    print('time:',time.time() - start, (time.time()-begin)/(i+1))\n",
        "    start = time.time()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4J2ahp3wmqcg"
      },
      "outputs": [],
      "source": [
        "# @title supervised train test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "# def strain(model, dataloader, optim, scheduler=None):\n",
        "def strain(model, classifier, dataloader, optim, coptim, scheduler=None):\n",
        "    model.train()\n",
        "    classifier.train()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x = x.to(device)#.to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "        y = y.to(device)\n",
        "\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        # print(loss)\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # for p in list(filter(lambda p: p.grad is not None, model.parameters())):\n",
        "        #     print(p.grad.data.norm(2).item())\n",
        "        # print(\"max grad norm\", max([p.grad.data.norm(2).item() for p in list(filter(lambda p: p.grad is not None, model.parameters()))]))\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 10) # 0.5\n",
        "\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        print(\"strain\",loss.item())\n",
        "        # for param in ijepa.student.cls: print(param.data)\n",
        "        # for param in ijepa.predicter.cls: print(param.data)\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=50: break\n",
        "\n",
        "\n",
        "# def test(model, dataloader):\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        # .to(torch.bfloat16)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "        loss = F.cross_entropy(y_, y)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        print(correct/len(y))\n",
        "        try: wandb.log({\"correct\": correct/len(y), \"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "for i in range(1000):\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # np.random.shuffle(train_indices); np.random.shuffle(val_indices)\n",
        "    # train_sampler, valid_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(val_indices)\n",
        "    # # batch_size = 64 #512\n",
        "    # train_loader = DataLoader(train_data, sampler=train_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "    # test_loader = DataLoader(train_data, sampler=valid_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True)\n",
        "\n",
        "    # strain(ijepa, train_loader, optim)\n",
        "    strain(ijepa, classifier, train_loader, optim, coptim)\n",
        "    test(ijepa, classifier, test_loader)\n",
        "\n",
        "    # strain(violet, train_loader, voptim)\n",
        "    # test(violet, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzo9DMDPcOxu",
        "outputId": "2aa950ff-7a1c-46e0-924c-c6922b2ca522"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# @title save/load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder='/content/drive/MyDrive/jepa/'\n",
        "\n",
        "# # modelsd, optimsd = torch.load(folder+'SeqJEPA.pkl', map_location=device).values()\n",
        "# modelsd, optimsd = torch.load('SeqJEPA.pkl', map_location=device).values()\n",
        "# seq_jepa.load_state_dict(modelsd, strict=False)\n",
        "# optim.load_state_dict(optimsd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "DNNPOuUmcSNf",
        "outputId": "9c0fdeca-f315-457a-a102-ff87f9290f75"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'seq_jepa' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-06e8be6b0f78>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mseq_jepa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'IJEPA.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# torch.save(checkpoint, 'IJEPA.pkl')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'seq_jepa' is not defined"
          ]
        }
      ],
      "source": [
        "checkpoint = {'model': seq_jepa.state_dict(), 'optimizer': optim.state_dict()}\n",
        "torch.save(checkpoint, folder+'IJEPA.pkl')\n",
        "# torch.save(checkpoint, 'IJEPA.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLT74ihtMnh3"
      },
      "source": [
        "## drawer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMTvHTtTRInh",
        "outputId": "91ca330e-fd81-42c6-af3c-d4e5316df589"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "345200\n",
            "(tensor(1.9985, device='cuda:0', grad_fn=<MseLossBackward0>), (tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>), tensor(0.9865, device='cuda:0', grad_fn=<AddBackward0>), tensor(7.4222e-06, device='cuda:0', grad_fn=<AddBackward0>)))\n"
          ]
        }
      ],
      "source": [
        "# @title IJEPA + VICReg\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "    def __init__(self, patch_size, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        patch_size=4\n",
        "        act = nn.GELU()\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Conv2d(in_dim, d_model, patch_size, patch_size), # like patch\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 1, 7//2, bias=False)\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.MaxPool2d(3,2,1)\n",
        "            nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.BatchNorm2d(d_model), nn.ReLU(),\n",
        "            nn.Conv2d(d_model, d_model, 3, 2, 3//2, bias=False)\n",
        "            )\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, 32*32, d_model)*.02)\n",
        "        # self.pos_emb = nn.Parameter(RoPE2D(dim=d_model, h=8, w=8, base=1000), requires_grad=False)\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=(32//patch_size)**2, base=10000), requires_grad=False)\n",
        "\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, d_model, n_heads) for _ in range(nlayers)])\n",
        "\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "        # self.lin = nn.Linear(d_model, out_dim)\n",
        "        self.attn_pool = nn.Linear(d_model, 1, bias=False)\n",
        "\n",
        "        dim_v = d_model * 4\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(out_dim, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v, bias=False),\n",
        "            )\n",
        "\n",
        "    # def forward(self, x, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "    def jepa_fwd(self, x, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x).flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c] # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        # x = self.pos_enc(x)\n",
        "        # print(\"TransformerModel\",x.shape, self.pos_emb.shape)\n",
        "        x = x + self.pos_emb[:,:x.shape[1]]\n",
        "        if context_indices != None: x = x[torch.arange(x.shape[0]).unsqueeze(-1), context_indices] # [batch, num_context_toks, d_model]\n",
        "\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        out = self.norm(x)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out\n",
        "\n",
        "    def pool(self, x):\n",
        "        # sx = self.forward(x)\n",
        "        # attn = self.attn_pool(x).squeeze(-1) # [batch, seq] # seq_pool\n",
        "        # out = (torch.softmax(attn, dim=-1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, seq] @ [batch, seq, dim] -> [batch, dim]\n",
        "        # if self.lin: out = self.lin(out)\n",
        "        out = x.mean(dim=1)\n",
        "        return out\n",
        "\n",
        "    def forward(self, x, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.jepa_fwd(x, context_indices=None)\n",
        "        out = self.pool(x)\n",
        "        return out\n",
        "\n",
        "    def expand(self, x):\n",
        "        # sx = self.forward(x)\n",
        "        sx = self.pool(x)\n",
        "        vx = self.exp(sx)\n",
        "        return vx\n",
        "\n",
        "\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class IJEPAVICReg(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=64, out_dim=None, nlayers=1, n_heads=4):\n",
        "        super().__init__()\n",
        "        self.out_dim = out_dim = out_dim or d_model\n",
        "        self.patch_size = 4 # 4\n",
        "        self.student = ViT(self.patch_size, in_dim, d_model, out_dim=out_dim, n_heads=n_heads, nlayers=nlayers, drop=0.)\n",
        "        self.predicter = TransformerPredictor(out_dim, 3*d_model//8, out_dim, n_heads=4, nlayers=1, drop=0.)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "\n",
        "        # vicreg\n",
        "        self.sim_coeff=25. # 10.0 # 25.0 # λ\n",
        "        self.std_coeff=25. # 1.0 # 25.0 # µ\n",
        "        self.cov_coeff=1. # 25.0 # 1.0 # ν\n",
        "\n",
        "\n",
        "    def loss(self, x): #\n",
        "        b,c,h,w = x.shape\n",
        "        # print('ijepa loss x',x.shape)\n",
        "        hw=(8,8)\n",
        "        # hw=(32,32)\n",
        "        mask_collator = MaskCollator(hw, enc_mask_scale=(.85, 1.), pred_mask_scale=(.15, .2), aspect_ratio=(.75, 1.5), nenc=1, npred=4, min_keep=4, allow_overlap=False)\n",
        "        collated_masks_enc, collated_masks_pred = mask_collator(b)\n",
        "        context_indices, trg_indices = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "        # print(context_indices.shape, trg_indices.shape)\n",
        "\n",
        "        # context_indices, trg_indices = simplexmask2d(hw, ctx_scale=(.85,1), trg_scale=(.6,.8), B=b, chaos=3)\n",
        "\n",
        "        # zero_mask = torch.zeros(b ,*hw, device=device).flatten(1)\n",
        "        # zero_mask[torch.arange(b).unsqueeze(-1), context_indices] = 1\n",
        "        # x_ = x * F.interpolate(zero_mask.reshape(b,1,*hw), size=x.shape[2:], mode='nearest-exact') # zero masked locations\n",
        "\n",
        "        sx = self.student.jepa_fwd(x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "\n",
        "        # print('ijepa loss sx',sx.shape)\n",
        "        sy_ = self.predicter(sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "        sy_ = F.layer_norm(sy_, (sy_.size(-1),))\n",
        "        vx = self.student.expand(sx) # [batch, num_context_toks, out_dim]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            sy = self.teacher.jepa_fwd(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "            sy = sy[torch.arange(sy.shape[0]).unsqueeze(-1), trg_indices] # [batch, num_context_toks, d_model] # nan bec len(trg_ind)==0 # print('loss sy',torch.isnan(sy).any())\n",
        "            sy = F.layer_norm(sy, (sy.size(-1),))\n",
        "\n",
        "            vy = self.teacher.expand(sy.detach()) # [batch, num_trg_toks, out_dim]\n",
        "\n",
        "        vic_loss = self.vicreg(vx, vy)\n",
        "        j_loss = F.mse_loss(sy, sy_)\n",
        "        # return loss\n",
        "        return j_loss, vic_loss\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        out = self.student(x)\n",
        "        return out\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size, num_features = x.shape\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = self.sim_coeff * repr_loss + self.std_coeff * std_loss + self.cov_coeff * cov_loss\n",
        "        # print(\"in vicreg \",self.sim_coeff * repr_loss.item() , self.std_coeff * std_loss.item() , self.cov_coeff * cov_loss.item())\n",
        "        # return loss\n",
        "        return repr_loss, std_loss, cov_loss\n",
        "\n",
        "# for jepa+vic, attnpool < meanpool\n",
        "# jepa+vic < jepa\n",
        "\n",
        "\n",
        "ijepa = IJEPAVICReg(in_dim=3, d_model=64, out_dim=64, nlayers=1, n_heads=4).to(device)\n",
        "print(sum(p.numel() for p in ijepa.parameters() if p.requires_grad)) # 27584\n",
        "optim = torch.optim.AdamW(ijepa.parameters(), lr=1e-3) # 1e-3?\n",
        "# optim = torch.optim.AdamW([{'params': ijepa.student.parameters()},\n",
        "#     {'params': ijepa.predicter.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # good 3e-3,1e-3 ; default 1e-2, 5e-2\n",
        "#     # {'params': ijepa.predicter.parameters(), 'lr': 1e-2}], lr=1e-3, weight_decay=1e-2)\n",
        "\n",
        "x = torch.rand((2,3,32,32), device=device)\n",
        "loss = ijepa.loss(x)\n",
        "print(loss)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(ijepa.out_dim, 10).to(device)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ge36SCxOl2Oq"
      },
      "outputs": [],
      "source": [
        "# @title RoPE2D & RotEmb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "def RoPE(dim, seq_len=512, base=10000):\n",
        "    theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "    pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "    angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "    rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [1, seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "    return rot_emb\n",
        "\n",
        "# class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "#     def __init__(self, dim, seq_len=512, base=10000):\n",
        "#         super().__init__()\n",
        "#         self.dim, self.base = dim, base\n",
        "#         theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "#         pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "#         angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "#         self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [1, seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "#     def forward(self, x): # [batch, T, dim] / [batch, T, n_heads, d_head]\n",
        "#         seq_len = x.size(1)\n",
        "#         if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.base)\n",
        "#         # print('rope fwd', x.shape, self.rot_emb.shape)\n",
        "#         if x.dim()==4: return x * self.rot_emb[:,:seq_len].unsqueeze(2)\n",
        "#         return x * self.rot_emb[:,:seq_len]\n",
        "\n",
        "\n",
        "def RoPE2D(dim=16, h=8, w=8, base=10000):\n",
        "    # theta = 1. / (base ** (torch.arange(0, dim, step=4) / dim))\n",
        "    # theta = 1. / (base**torch.linspace(0,1,dim//4)).unsqueeze(0)\n",
        "    theta = 1. / (base**torch.linspace(0,1,dim//2)).unsqueeze(0)\n",
        "    y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\") # print(y,x) # [h,w], y:row_num, x:col_num\n",
        "    y, x = (y.reshape(-1,1) * theta).unsqueeze(-1), (x.reshape(-1,1) * theta).unsqueeze(-1) # [h*w,1]*[1,dim//4] = [h*w, dim//4, 1]\n",
        "    # rot_emb = torch.cat([x.sin(), x.cos(), y.sin(), y.cos()], dim=-1).flatten(-2) # [h*w, dim//4 ,4] -> [h*w, dim]\n",
        "    # rot_emb = torch.cat([x.sin(), x.cos(), y.sin(), y.cos()], dim=-1)#.reshape(dim, h, w).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "    rot_emb = torch.cat([x.sin(), y.sin()], dim=-1).flatten(-2).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "    # rot_emb = torch.cat([x.cos(), y.cos()], dim=-1).reshape(h, w, dim).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "    return rot_emb\n",
        "\n",
        "\n",
        "\n",
        "# class RoPE2D(nn.Module): # Rotary Positional Embeddings\n",
        "#     def __init__(self, dim, h=224, w=224, base=10000):\n",
        "#         super().__init__()\n",
        "#         self.dim, self.h, self.w = dim, h, w\n",
        "#         # # theta = 1. / (base ** (torch.arange(0, dim, step=4) / dim))\n",
        "#         # theta = 1. / (base**torch.linspace(0,1,dim//4)).unsqueeze(0)\n",
        "#         theta = 1. / (base**torch.linspace(0,1,dim//2)).unsqueeze(0)\n",
        "#         y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\") # print(y,x) # [h,w], y:row_num, x:col_num\n",
        "#         y, x = (y.reshape(-1,1) * theta).unsqueeze(-1), (x.reshape(-1,1) * theta).unsqueeze(-1) # [h*w,1]*[1,dim//4] = [h*w, dim//4, 1]\n",
        "#         # # self.rot_emb = torch.cat([x.sin(), x.cos(), y.sin(), y.cos()], dim=-1).flatten(-2) # [h*w, dim//4 ,4] -> [h*w, dim]\n",
        "#         # self.rot_emb = torch.cat([x.sin(), x.cos(), y.sin(), y.cos()], dim=-1).reshape(h, w, dim).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "#         self.rot_emb = torch.cat([x.sin(), y.sin()], dim=-1).reshape(h, w, dim).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "#         # self.rot_emb = torch.cat([x.cos(), y.cos()], dim=-1).reshape(h, w, dim).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "\n",
        "#     def forward(self, img): #\n",
        "#         # batch, dim, h, w = img.shape\n",
        "#         # print(img.shape)\n",
        "#         hw = img.shape[1] # [b, hw, dim] / [b, hw, n_heads, d_head]\n",
        "#         h=w=int(hw**.5)\n",
        "#         if self.h < h or self.w < w: self.__init__(self.dim, h, w)\n",
        "#         # print(self.rot_emb.shape)\n",
        "#         # rot_emb = self.rot_emb[:, :h, :w].unsqueeze(0) # [1, h, w, dim]\n",
        "#         rot_emb = self.rot_emb[:h, :w] # [h, w, dim]\n",
        "#         # return img * rot_emb.flatten(end_dim=1).unsqueeze(0) # [b, hw, dim] * [1, hw, dim]\n",
        "#         return img * rot_emb.flatten(end_dim=1)[None,:,None,:] # [b, hw, n_heads, d_head] * [1, hw, 1, dim]\n",
        "#         # return img * self.rot_emb\n",
        "\n",
        "class RotEmb(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, top=torch.pi, base=10000):\n",
        "        super().__init__()\n",
        "        self.theta = top / (base ** (torch.arange(0, dim, step=2, device=device) / dim))\n",
        "        # self.theta = top / (base ** torch.linspace(0, 1, dim//2, device=device))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (pos.unsqueeze(-1) * self.theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [seq_len, dim]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3bpiybH7M0O1"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# @title test multiblock params\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "dataiter = iter(train_data)\n",
        "img,y = next(dataiter)\n",
        "img = img.unsqueeze(0)\n",
        "img = F.interpolate(img, (8,8))\n",
        "b,c,h,w = img.shape\n",
        "# img = img.flatten(2).transpose(-2,-1).to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "print(img.shape)\n",
        "# batch, seq,_ = img.shape\n",
        "\n",
        "\n",
        "# # target_mask = randpatch(seq, mask_size=8, gamma=.9) # 8.9 [seq]\n",
        "# target_mask = multiblock(seq, min_s=0.2, max_s=0.3, M=4).any(0) # best.2.3M4 og.15.2M4# mask out targets to be predicted # [M, seq]\n",
        "# target_mask = multiblock(seq, min_s=0.2, max_s=0.3, M=4).any(0)\n",
        "\n",
        "target_mask = multiblock2d((8,8), M=4).any(0).unsqueeze(0)\n",
        "\n",
        "\n",
        "# context_mask = ~multiblock(seq, min_s=0.85, max_s=1., M=1)|target_mask # [1, seq], True->Mask\n",
        "context_mask = ~multiblock2d((8,8), scale=(.85,.85), aspect_ratio=(.9,1.1), M=1)|target_mask # [1, seq], True->Mask\n",
        "\n",
        "# print(target_mask, context_mask)\n",
        "\n",
        "print(target_mask.shape, context_mask.shape)\n",
        "# target_img, context_img = img*target_mask.unsqueeze(-1), img*context_mask.unsqueeze(-1)\n",
        "# target_img, context_img = img*~target_mask.unsqueeze(-1), img*~context_mask.unsqueeze(-1)\n",
        "target_img, context_img = img*~target_mask.unsqueeze(0), img*~context_mask.unsqueeze(0)\n",
        "# print(target_img.shape, context_img.shape)\n",
        "target_img, context_img = target_img.transpose(-2,-1).reshape(b,c,h,w), context_img.transpose(-2,-1).reshape(b,c,h,w)\n",
        "target_img, context_img = target_img.float(), context_img.float()\n",
        "\n",
        "# imshow(out.detach().cpu())\n",
        "imshow(target_img[0])\n",
        "imshow(context_img[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1iZQ6UNwoty",
        "outputId": "5554e8f3-71ec-4b70-c7d7-219b59e9a1d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9920\n",
            "torch.Size([4, 64, 16])\n"
          ]
        }
      ],
      "source": [
        "# @title ViT me more\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "# class LayerNorm2d(nn.LayerNorm):\n",
        "class LayerNorm2d(nn.RMSNorm):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        x = super().forward(x)\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads=None, d_head=8, cond_dim=None, dropout=0.): # .1\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_head = d_head\n",
        "        self.n_heads = d_model // d_head\n",
        "        # self.d_head = d_model // n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or d_model, 2*d_model, bias=False)\n",
        "        # self.k = nn.Sequential(nn.Dropout(dropout), nn.Linear(cond_dim, d_model, bias=False))\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        # self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        # self.lin = nn.Sequential(nn.Dropout(dropout), zero_module(nn.Linear(d_model, d_model)))\n",
        "        self.drop = nn.Dropout(dropout) # indp before q,k,v; after linout\n",
        "        self.scale = self.d_head ** -.5\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [batch, T, d_model]=[batch, h*w, c], [batch, num_tok, cond_dim], [batch,T]\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        Q = self.q(x).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        # K = self.k(x).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2)\n",
        "        K, V = self.kv(cond).unflatten(-1, (self.n_heads, 2*self.d_head)).transpose(1, 2).chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # linear attention # Softmax(Q) @ (Softmax(K).T @ V)\n",
        "        if mask != None:\n",
        "            mask = mask[:, None, :, None] # [batch,T] -> [batch,1,T,1]\n",
        "            K, V = K.masked_fill(mask, -torch.finfo(x.dtype).max), V.masked_fill(mask, -torch.finfo(x.dtype).max)\n",
        "        Q, K = Q.softmax(dim=-1)*self.scale, K.softmax(dim=-2)\n",
        "        context = K.transpose(-2,-1) @ V # [batch, n_heads, d_head, d_head]\n",
        "        out = Q @ context # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # (quadratic) attention # Softmax(Q @ K.T) @ V\n",
        "        # attn = Q @ K.transpose(-2,-1) * self.scale # [batch, n_heads, T] # [batch, n_heads, T, T/num_tok]\n",
        "        # if mask != None: attn = attn.masked_fill(mask[:, None, :, None], -torch.finfo(attn.dtype).max) # [batch,T]->[batch,1,T,1]\n",
        "        # attention = torch.softmax(attn, dim=-1)\n",
        "        # out = self.drop(attention) @ V # [batch, n_heads, T, d_head]\n",
        "\n",
        "        out = out.transpose(1, 2).flatten(2)\n",
        "        return self.drop(self.lin(out)) # [batch, T, d_model]\n",
        "\n",
        "# if self, dont pass cond_dim in init, dont pass cond in fwd\n",
        "# Softmax(Q @ K.T) @ V ~ Softmax(Q) @ Softmax(K).T @ V\n",
        "\n",
        "# https://github.com/lucidrains/x-transformers/blob/main/x_transformers/x_transformers.py#L1855\n",
        "\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_head, cond_dim=None, mult=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.cond_dim = cond_dim\n",
        "        self.self = MultiHeadAttention(d_model, d_head=d_head, dropout=0) # 16448\n",
        "        # self.self = Pooling()\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        # self.ff = nn.Sequential(\n",
        "        #     *[nn.BatchNorm2d(d_model), act, SeparableConv2d(d_model, d_model),]*3\n",
        "        #     )\n",
        "        # self.ff = ResBlock(d_model) # 74112\n",
        "        # self.ff = UIB(d_model, mult=4) # uib m4 36992, m2 18944\n",
        "        ff_dim=d_model*mult\n",
        "        self.ff = nn.Sequential(\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(dropout), nn.Linear(d_model, ff_dim), act, # ReLU GELU\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(dropout), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            nn.RMSNorm(d_model), act, nn.Dropout(dropout), nn.Linear(d_model, ff_dim),\n",
        "            nn.RMSNorm(ff_dim), act, nn.Dropout(dropout), zero_module(nn.Linear(ff_dim, d_model))\n",
        "        )\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,c,h,w], [batch, num_tok, cond_dim], [batch,T]\n",
        "        bchw = x.shape\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "        # print('attnblk fwd',x.shape)\n",
        "\n",
        "        # if self.cond_dim==None: cond=None # is self attn\n",
        "        x = x + self.drop(self.self(self.norm(x)))\n",
        "        # x = x.transpose(1,2).reshape(*bchw)\n",
        "        x = x + self.ff(x)\n",
        "        # x = self.ff(x)\n",
        "        # x = x + self.drop(self.norm2(self.ff(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "import torch\n",
        "# def apply_masks(x, masks): # [b,t,d], [M,mask_size] # https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "#     all_x = []\n",
        "#     for m in masks: # [1,T]\n",
        "#         mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [1,T,dim]\n",
        "#         all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "#     return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "def apply_masks(x, mask): # [b,t,d], [mask_size] # https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "    mask_keep = mask.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [1,T,dim]\n",
        "    # return torch.cat(torch.gather(x, dim=1, index=mask_keep), dim=0)  # [M*batch,mask_size,dim]\n",
        "    return torch.gather(x, dim=1, index=mask_keep) # [M*batch,mask_size,dim]\n",
        "\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        # self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        self.positional_emb = nn.Parameter(torch.zeros(1, 8*8, d_model)) # positional_embedding == 'learnable'\n",
        "        # self.positional_emb = nn.Parameter(torch.zeros(1, dim, 8,8))\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.zeros(1,1,d_model)) # randn zeros\n",
        "        nn.init.trunc_normal_(self.cls, std=.02)\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model)\n",
        "        self.lin = nn.Linear(d_model, out_dim)# if out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = x.shape\n",
        "        # x = x * self.pos_encoder(context_indices)\n",
        "        x = x + self.positional_emb[:,context_indices]\n",
        "        # x = apply_masks(x, [context_indices])\n",
        "\n",
        "        # pred_tokens = self.cls * self.pos_encoder(trg_indices) # [M, num_trg_toks, d_model]\n",
        "        pred_tokens = self.cls + self.positional_emb[0,trg_indices]\n",
        "        print(self.cls.shape, self.positional_emb[0,trg_indices].shape, trg_indices.shape)\n",
        "        pred_tokens = pred_tokens.repeat(batch, 1, 1) # [batch*M, num_trg_toks, d_model]\n",
        "        # print(pred_tokens.requires_grad)\n",
        "        print(\"pred fwd\", x.shape, pred_tokens.shape)\n",
        "        # x = x.repeat_interleave(trg_indices.shape[0], dim=0) # [batch, seq_len, d_model] -> [batch*M, seq_len, d_model]\n",
        "        x = torch.cat([x, pred_tokens], dim=1) # [batch*M, seq_len+num_trg_toks, d_model]\n",
        "\n",
        "        # x = x.transpose(1,2).unflatten(-1, (8,8))#.reshape(*bchw)\n",
        "        out = self.transformer_encoder(x) # float [seq_len, batch_size, d_model]\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "\n",
        "        out = self.norm(out)\n",
        "        out = out[:,seq:] # [batch*M, num_trg_toks, d_model]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Sequential(\n",
        "            # # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.MaxPool1d(2,2), #nn.MaxPool1d(3, 2, 3//2),\n",
        "            # # nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.Conv1d(d_model, d_model,3,2,3//2)\n",
        "            # nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            # nn.Conv1d(d_model, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            # nn.Conv1d(d_model, d_model,3,2,3//2),\n",
        "            # # nn.Conv1d(in_dim, d_model,2,2,0), # like patch\n",
        "            nn.Conv2d(in_dim, d_model, kernel_size=7, stride=2, padding=7//2, bias=False), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "            )\n",
        "        # self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        self.positional_emb = nn.Parameter(torch.zeros(1, 8*8, d_model)) # positional_embedding == 'learnable'\n",
        "        # self.positional_emb = nn.Parameter(torch.zeros(1, dim, 8,8)) # positional_embedding == 'learnable'\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model)\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # x = self.embed(x.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "        x = self.embed(x)\n",
        "        # bchw = x.shape\n",
        "        x = x.flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c] # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "\n",
        "        # x = self.pos_encoder(x)\n",
        "        # x = x * self.pos_encoder(torch.arange(seq, device=device)).unsqueeze(0)\n",
        "        # print(x.shape, self.positional_emb.shape)\n",
        "        x = x + self.positional_emb\n",
        "        if context_indices != None:\n",
        "            # x = apply_masks(x, [context_indices])\n",
        "            x = apply_masks(x, context_indices)\n",
        "\n",
        "\n",
        "        # x = x.transpose(1,2).reshape(*bchw)\n",
        "        x = self.transformer_encoder(x) # float [seq_len, batch_size, d_model]\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "        out = self.norm(x)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "\n",
        "# pos_emb rope < learn < learned\n",
        "# conv > pixel?\n",
        "# droppath not required\n",
        "\n",
        "# norm,act,conv < conv,norm,act\n",
        "# 2*s1 < uib < resblock\n",
        "# gatedadaln 3 < 2 = 1 < ffmult4 = 2*gatedadaln\n",
        "# MaxPool2d(2,2) < MaxPool2d(3,2,3//2)\n",
        "\n",
        "\n",
        "# dim = 64\n",
        "# dim_head = 8\n",
        "# heads = dim // dim_head\n",
        "# num_classes = 10\n",
        "# # model = SimpleViT(image_size=32, patch_size=4, num_classes=num_classes, dim=dim, depth=1, heads=heads, mlp_dim=dim*4, channels = 3, dim_head = dim_head)\n",
        "# model = SimpleViT(in_dim=3, out_dim=num_classes, dim=dim, depth=1, heads=heads, mlp_dim=dim*4, channels = 3, dim_head = dim_head).to(device)\n",
        "# print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 59850\n",
        "# optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "# # print(images.shape) # [batch, 3, 32, 32]\n",
        "# logits = model(x)\n",
        "# print(logits.shape)\n",
        "# # print(logits[0])\n",
        "# # print(logits[0].argmax(1))\n",
        "# pred_probab = nn.Softmax(dim=1)(logits)\n",
        "# y_pred = pred_probab.argmax(1)\n",
        "# # print(f\"Predicted class: {y_pred}\")\n",
        "\n",
        "\n",
        "batch, seq_len, d_model = 4,1024,16\n",
        "in_dim = 3\n",
        "model = TransformerModel(in_dim, d_model, d_head=4, nlayers=2, dropout=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x = torch.rand((batch, in_dim, 32, 32), device=device)\n",
        "# x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "# # # print(out)\n",
        "# model = TransformerPredictor(in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1).to(device)\n",
        "# out = model(out)\n",
        "# print(out.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "e3dAhWh45F4M"
      },
      "outputs": [],
      "source": [
        "# @title simplex\n",
        "# !pip install -q opensimplex\n",
        "import opensimplex\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def simplexmask2d(hw=(32,32), ctx_scale=(.85,1), trg_scale=(.6,.8), B=64, chaos=2):\n",
        "    seed = np.random.randint(1e10, size=B)\n",
        "    ix = iy = np.linspace(0, chaos, num=max(hw))\n",
        "    noise = opensimplex.noise3array(ix, iy, seed) # [b,h,w]\n",
        "    # plt.pcolormesh(noise[:1])\n",
        "    # plt.rcParams[\"figure.figsize\"] = (20,3)\n",
        "    # plt.show()\n",
        "    noise = torch.from_numpy(noise)\n",
        "    ctx_mask_scale = torch.rand(1) * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    trg_mask_scale = torch.rand(1) * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "    val, ind = noise.flatten(1).sort() # [b,h*w]\n",
        "    seq = hw[0]*hw[1]\n",
        "    # trg_index = ind[:,-int(seq*trg_mask_scale):]\n",
        "    # ctx_index = ind[:,-int(seq*ctx_mask_scale):-int(seq*trg_mask_scale)] # ctx wraps trg ; most similar to multiblock\n",
        "    # ctx_index = ind[:,:int(seq*ctx_mask_scale)-int(seq*trg_mask_scale)] # ctx hug bottom\n",
        "    # ctx_index = ind[:,-int(seq*ctx_mask_scale)-int(seq*trg_mask_scale):-int(seq*trg_mask_scale)] # ctx wraps trg ; most similar to multiblock\n",
        "    # ctx_index = ind[:,:int(seq*ctx_mask_scale)] # ctx hug bottom\n",
        "    return ctx_index, trg_index\n",
        "\n",
        "\n",
        "def simplexmask2d(hw=(32,32), ctx_scale=(.85,1), trg_scale=(.6,.8), B=64, chaos=2):\n",
        "    seed = np.random.randint(1e10, size=B)\n",
        "    ix = np.linspace(0, chaos, num=hw[1])\n",
        "    iy = np.linspace(0, chaos, num=hw[0])\n",
        "    noise = opensimplex.noise3array(ix, iy, seed) # [b,h,w]\n",
        "    noise = torch.from_numpy(noise)\n",
        "    ctx_mask_scale = torch.rand(1) * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    trg_mask_scale = torch.rand(1) * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "    val, ind = noise.flatten(1).sort() # [b,h*w]\n",
        "\n",
        "    seq = hw[0]*hw[1]\n",
        "\n",
        "    ctx_len = int(seq*ctx_mask_scale)\n",
        "    trg_len = int(seq*trg_mask_scale)\n",
        "    trg_pos = (torch.rand(B) * (seq-trg_len)).int()\n",
        "    # print(trg_pos)\n",
        "\n",
        "    trg_ind = trg_pos.unsqueeze(-1) + torch.arange(trg_len).unsqueeze(0)\n",
        "    trg_index = ind[torch.arange(B).unsqueeze(-1), trg_ind]\n",
        "    ctx_len = ctx_len - trg_len\n",
        "\n",
        "    remove_mask = torch.ones((B,seq), dtype=bool) # [B, S]\n",
        "    remove_mask.scatter_(1, trg_ind, False).flatten()\n",
        "    ctx_ind = torch.arange(seq).unsqueeze(0).repeat(B,1)[remove_mask].reshape(B, -1)\n",
        "    ctx_index = ind[torch.arange(B).unsqueeze(-1), ctx_ind[:,-ctx_len:]]\n",
        "    return ctx_index, trg_index\n",
        "\n",
        "\n",
        "# @title simplex\n",
        "!pip install opensimplex\n",
        "from opensimplex import OpenSimplex\n",
        "\n",
        "seed=0\n",
        "noise = OpenSimplex(seed)  # Replace 'seed' with your starting value\n",
        "noise_scale = 10  # Adjust for desired noise range\n",
        "\n",
        "def get_noise(x, y):\n",
        "    global seed  # Assuming seed is a global variable\n",
        "    # noise_val = noise.noise2(x / noise_scale, y / noise_scale)\n",
        "    noise_val = noise.noise2(x, y)\n",
        "    # seed += 1  # Increment seed after each call\n",
        "    return noise_val\n",
        "\n",
        "x,y=0,0\n",
        "a=[[],[],[],[],[]]\n",
        "\n",
        "fps=2\n",
        "f=[]\n",
        "for i in range(10*fps):\n",
        "    f.append(i/fps)\n",
        "    x = x+0.3\n",
        "    # y = y+1\n",
        "    # noise_value = get_noise(x, y)\n",
        "    for k,j in enumerate([0,1,2,3,4]):\n",
        "        a[k].append(get_noise(x, y+j))\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "for aa in a:\n",
        "    plt.plot(f,aa)\n",
        "plt.show()\n",
        "\n",
        "# b,y,g,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vtPSM8mbnJZy"
      },
      "outputs": [],
      "source": [
        "# @title pos_embed.py\n",
        "# https://github.com/facebookresearch/mae/blob/main/util/pos_embed.py#L20\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim=16, grid_size=8, cls_token=False): #\n",
        "    grid_h = np.arange(grid_size, dtype=np.float32)\n",
        "    grid_w = np.arange(grid_size, dtype=np.float32)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token: pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed # [(1+)grid_size*grid_size, embed_dim]\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    # assert embed_dim % 2 == 0\n",
        "\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1) # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos): #\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=float)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega  # (D/2,)\n",
        "\n",
        "    pos = pos.reshape(-1)  # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
        "\n",
        "    emb_sin = np.sin(out) # (M, D/2)\n",
        "    emb_cos = np.cos(out) # (M, D/2)\n",
        "\n",
        "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def interpolate_pos_embed(model, checkpoint_model):\n",
        "    if 'pos_embed' in checkpoint_model:\n",
        "        pos_embed_checkpoint = checkpoint_model['pos_embed']\n",
        "        embedding_size = pos_embed_checkpoint.shape[-1]\n",
        "        num_patches = model.patch_embed.num_patches\n",
        "        num_extra_tokens = model.pos_embed.shape[-2] - num_patches\n",
        "        # height (== width) for the checkpoint position embedding\n",
        "        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n",
        "        # height (== width) for the new position embedding\n",
        "        new_size = int(num_patches ** 0.5)\n",
        "        # class_token and dist_token are kept unchanged\n",
        "        if orig_size != new_size:\n",
        "            print(\"Position interpolate from %dx%d to %dx%d\" % (orig_size, orig_size, new_size, new_size))\n",
        "            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n",
        "            # only the position tokens are interpolated\n",
        "            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n",
        "            pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n",
        "            pos_tokens = torch.nn.functional.interpolate(\n",
        "                pos_tokens, size=(new_size, new_size), mode='bicubic', align_corners=False)\n",
        "            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n",
        "            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n",
        "            checkpoint_model['pos_embed'] = new_pos_embed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "dLbXQ-3XXRMq"
      },
      "outputs": [],
      "source": [
        "# @title ijepa src.utils.tensors\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/utils/tensors.py\n",
        "\n",
        "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
        "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n",
        "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
        "    def norm_cdf(x):\n",
        "        # Computes standard normal cumulative distribution function\n",
        "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Values are generated by using a truncated uniform distribution and\n",
        "        # then using the inverse CDF for the normal distribution.\n",
        "        # Get upper and lower cdf values\n",
        "        l = norm_cdf((a - mean) / std)\n",
        "        u = norm_cdf((b - mean) / std)\n",
        "\n",
        "        # Uniformly fill tensor with values from [l, u], then translate to\n",
        "        # [2l-1, 2u-1].\n",
        "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
        "\n",
        "        # Use inverse cdf transform for normal distribution to get truncated\n",
        "        # standard normal\n",
        "        tensor.erfinv_()\n",
        "\n",
        "        # Transform to proper mean, std\n",
        "        tensor.mul_(std * math.sqrt(2.))\n",
        "        tensor.add_(mean)\n",
        "\n",
        "        # Clamp to ensure it's in the proper range\n",
        "        tensor.clamp_(min=a, max=b)\n",
        "        return tensor\n",
        "\n",
        "\n",
        "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
        "    # type: (Tensor, float, float, float, float) -> Tensor\n",
        "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n",
        "\n",
        "def repeat_interleave_batch(x, B, repeat): # [batch*M,...]? , M?\n",
        "    N = len(x) // B\n",
        "    x = torch.cat([\n",
        "        torch.cat([x[i*B:(i+1)*B] for _ in range(repeat)], dim=0)\n",
        "        for i in range(N)\n",
        "    ], dim=0)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "COvEnBXPYth3"
      },
      "outputs": [],
      "source": [
        "# @title ijepa multiblock down\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "import math\n",
        "from multiprocessing import Value\n",
        "import torch\n",
        "_GLOBAL_SEED = 0\n",
        "\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=(224, 224),\n",
        "        patch_size=16,\n",
        "        enc_mask_scale=(0.2, 0.8),\n",
        "        pred_mask_scale=(0.2, 0.8),\n",
        "        aspect_ratio=(0.3, 3.0),\n",
        "        nenc=1,\n",
        "        npred=2,\n",
        "        min_keep=4,\n",
        "        allow_overlap=False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if not isinstance(input_size, tuple):\n",
        "            input_size = (input_size, ) * 2\n",
        "        self.patch_size = patch_size\n",
        "        self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.aspect_ratio = aspect_ratio\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "        self._itr_counter = Value('i', -1)  # collator is shared across worker processes\n",
        "\n",
        "    def step(self):\n",
        "        i = self._itr_counter\n",
        "        with i.get_lock():\n",
        "            i.value += 1\n",
        "            v = i.value\n",
        "        return v\n",
        "\n",
        "    def _sample_block_size(self, generator, scale, aspect_ratio_scale):\n",
        "        _rand = torch.rand(1, generator=generator).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.height * self.width * mask_scale)\n",
        "        # -- Sample block aspect-ratio\n",
        "        min_ar, max_ar = aspect_ratio_scale\n",
        "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
        "        while h >= self.height:\n",
        "            h -= 1\n",
        "        while w >= self.width:\n",
        "            w -= 1\n",
        "        return (h, w)\n",
        "\n",
        "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
        "        h, w = b_size\n",
        "\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # --\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            top = torch.randint(0, self.height - h, (1,))\n",
        "            left = torch.randint(0, self.width - w, (1,))\n",
        "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
        "            mask[top:top+h, left:left+w] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
        "        mask_complement[top:top+h, left:left+w] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        B = len(batch)\n",
        "\n",
        "        collated_batch = torch.utils.data.default_collate(batch)\n",
        "\n",
        "        seed = self.step()\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(seed)\n",
        "        p_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.pred_mask_scale,\n",
        "            aspect_ratio_scale=self.aspect_ratio)\n",
        "        e_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.enc_mask_scale,\n",
        "            aspect_ratio_scale=(1., 1.))\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.height * self.width\n",
        "        min_keep_enc = self.height * self.width\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                print(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "        return collated_batch, collated_masks_enc, collated_masks_pred\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aH1UOD4WW8I2",
        "outputId": "e8671cd3-eba7-4649-84ca-94c6467ce62d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Mask generator says: \"Valid mask not found, decreasing acceptable-regions [1]\"\n",
            "WARNING:root:Mask generator says: \"Valid mask not found, decreasing acceptable-regions [2]\"\n",
            "WARNING:root:Mask generator says: \"Valid mask not found, decreasing acceptable-regions [3]\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vit fwd torch.Size([4, 3, 224, 224])\n",
            "vit fwd torch.Size([4, 196, 768])\n",
            "vit fwd torch.Size([4, 11, 768])\n",
            "predictor fwd1 torch.Size([4, 11, 384]) torch.Size([4, 196, 384])\n"
          ]
        }
      ],
      "source": [
        "# @title ijepa vision_transformer.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/models/vision_transformer.py\n",
        "import math\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# from src.utils.tensors import (trunc_normal_, repeat_interleave_batch)\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid height and width\n",
        "    return:\n",
        "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid_h = np.arange(grid_size, dtype=float)\n",
        "    grid_w = np.arange(grid_size, dtype=float)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0) # [1+h*w, dim]\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    assert embed_dim % 2 == 0\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid length\n",
        "    return:\n",
        "    pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid = np.arange(grid_size, dtype=float)\n",
        "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=float)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega   # (D/2,)\n",
        "    pos = pos.reshape(-1)   # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
        "    emb = np.concatenate([np.sin(out), np.cos(out)], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\"\"\"\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x, attn\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x, return_attention=False):\n",
        "        y, attn = self.attn(self.norm1(x))\n",
        "        if return_attention:\n",
        "            return attn\n",
        "        x = x + self.drop_path(y)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2) # ->[b,h*w,c]\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvEmbed(nn.Module):\n",
        "    \"\"\"3x3 Convolution stems for ViT following ViTC models\"\"\"\n",
        "    def __init__(self, channels, strides, img_size=224, in_chans=3, batch_norm=True):\n",
        "        super().__init__()\n",
        "        # Build the stems\n",
        "        stem = []\n",
        "        channels = [in_chans] + channels\n",
        "        for i in range(len(channels) - 2):\n",
        "            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3, stride=strides[i], padding=1, bias=(not batch_norm))]\n",
        "            if batch_norm:\n",
        "                stem += [nn.BatchNorm2d(channels[i+1])]\n",
        "            stem += [nn.ReLU(inplace=True)]\n",
        "        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n",
        "        self.stem = nn.Sequential(*stem)\n",
        "\n",
        "        # Comptute the number of patches\n",
        "        stride_prod = int(np.prod(strides))\n",
        "        self.num_patches = (img_size[0] // stride_prod)**2\n",
        "\n",
        "    def forward(self, x):\n",
        "        p = self.stem(x)\n",
        "        return p.flatten(2).transpose(1, 2)\n",
        "\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "def apply_masks(x, masks):\n",
        "    \"\"\":param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
        "    :param masks: list of tensors containing indices of patches in [N] to keep\"\"\"\n",
        "    all_x = []\n",
        "    for m in masks:\n",
        "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [batch,T,dim]\n",
        "        all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "    return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "\n",
        "class VisionTransformerPredictor(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, num_patches, embed_dim=768, predictor_embed_dim=384,\n",
        "        depth=6, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.init_std = init_std\n",
        "        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        # --\n",
        "        self.predictor_pos_embed = nn.Parameter(torch.zeros(1, num_patches, predictor_embed_dim), requires_grad=False)\n",
        "        predictor_pos_embed = get_2d_sincos_pos_embed(self.predictor_pos_embed.shape[-1], int(num_patches**.5), cls_token=False)\n",
        "        self.predictor_pos_embed.data.copy_(torch.from_numpy(predictor_pos_embed).float().unsqueeze(0)) # [1, num_patches, predictor_embed_dim]\n",
        "        # --\n",
        "        self.predictor_blocks = nn.ModuleList([Block(dim=predictor_embed_dim, num_heads=num_heads, drop=drop_rate, drop_path=dpr[i]) for i in range(depth)])\n",
        "        self.predictor_norm = nn.LayerNorm(predictor_embed_dim)\n",
        "        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n",
        "        # ------\n",
        "        # trunc_normal_(self.mask_token, std=self.init_std)\n",
        "\n",
        "    def forward(self, x, masks_x, masks): # [batch, num_context_patches, embed_dim], nenc*[batch, num_context_patches], npred*[batch, num_target_patches]\n",
        "        assert (masks is not None) and (masks_x is not None), 'Cannot run predictor without mask indices'\n",
        "        if not isinstance(masks_x, list): masks_x = [masks_x] # context mask\n",
        "        if not isinstance(masks, list): masks = [masks] # pred mask\n",
        "\n",
        "        # -- Batch Size\n",
        "        B = len(x) // len(masks_x)\n",
        "        # print(\"predictor fwd0\", x.shape) # [3, 121, 768])\n",
        "        # -- map from encoder-dim to pedictor-dim\n",
        "        x = self.predictor_embed(x) # [batch, num_patches, predictor_embed_dim]\n",
        "\n",
        "        # -- add positional embedding to x tokens\n",
        "        x_pos_embed = self.predictor_pos_embed.repeat(B, 1, 1) # [batch, num_patches, predictor_embed_dim]\n",
        "        print(\"predictor fwd1\", x.shape, x_pos_embed.shape) # [3, 121 or 11, 384], [3, 196, 384]\n",
        "        # print(\"predictor fwd masks_x\", masks_x)\n",
        "        x += apply_masks(x_pos_embed, masks_x) # get pos emb of context patches\n",
        "\n",
        "        # print(\"predictor fwd x\", x.shape) # [4, 11, 384])\n",
        "        _, N_ctxt, D = x.shape\n",
        "\n",
        "        # -- concat mask tokens to x\n",
        "        pos_embs = self.predictor_pos_embed.repeat(B, 1, 1) # [batch, num_patches, predictor_embed_dim]\n",
        "        pos_embs = apply_masks(pos_embs, masks) # [16, 104, predictor_embed_dim]\n",
        "        pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x)) # [16, 104, predictor_embed_dim]\n",
        "        # --\n",
        "        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1) # [16, 104, predictor_embed_dim]\n",
        "        # --\n",
        "        # print(\"predictor fwd pred_tokens\", pred_tokens.shape)\n",
        "\n",
        "        pred_tokens += pos_embs\n",
        "        x = x.repeat(len(masks), 1, 1)\n",
        "        x = torch.cat([x, pred_tokens], dim=1)\n",
        "\n",
        "        # -- fwd prop\n",
        "        for blk in self.predictor_blocks:\n",
        "            x = blk(x)\n",
        "        x = self.predictor_norm(x)\n",
        "\n",
        "        # -- return preds for mask tokens\n",
        "        x = x[:, N_ctxt:]\n",
        "        x = self.predictor_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, img_size=[224], patch_size=16, in_chans=3, embed_dim=768, predictor_embed_dim=384, depth=12, predictor_depth=12,\n",
        "        num_heads=12, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0, norm_layer=nn.LayerNorm, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_features = self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.init_std = init_std\n",
        "        # --\n",
        "        self.patch_embed = PatchEmbed(img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        # --\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=False)\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "        # --\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "    def forward(self, x, masks=None): # [batch,3,224,224]\n",
        "        if masks is not None:\n",
        "            if not isinstance(masks, list): masks = [masks]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        x = self.patch_embed(x)\n",
        "        B, N, D = x.shape # [batch, num_patches, embed_dim]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        # -- add positional embedding to x\n",
        "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
        "        x = x + pos_embed\n",
        "\n",
        "        if masks is not None: x = apply_masks(x, masks) # [batch, num_context_patches, embed_dim]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            x = blk(x)\n",
        "\n",
        "        if self.norm is not None: x = self.norm(x)\n",
        "        return x # [batch, num_context_patches, embed_dim]\n",
        "\n",
        "    def interpolate_pos_encoding(self, x, pos_embed):\n",
        "        npatch = x.shape[1] - 1\n",
        "        N = pos_embed.shape[1] - 1\n",
        "        if npatch == N: return pos_embed\n",
        "        class_emb, pos_embed = pos_embed[:, 0], pos_embed[:, 1:]\n",
        "        dim = x.shape[-1]\n",
        "        pos_embed = nn.functional.interpolate(pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2), scale_factor=math.sqrt(npatch / N), mode='bicubic',)\n",
        "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
        "\n",
        "# from functools import partial\n",
        "# def vit_predictor(**kwargs):\n",
        "#     model = VisionTransformerPredictor(\n",
        "#         mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "#     return model\n",
        "\n",
        "def vit(patch_size=16, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, # tiny\n",
        "        # patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, # small\n",
        "        # patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, # base\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "# encoder = vit()\n",
        "encoder = VisionTransformer(\n",
        "        patch_size=16, embed_dim=768, depth=1, num_heads=3, mlp_ratio=1,\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
        "\n",
        "# num_patches = (224/patch_size)^2\n",
        "# model = VisionTransformerPredictor(num_patches, embed_dim=768, predictor_embed_dim=384, depth=6, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02, **kwargs)\n",
        "model = VisionTransformerPredictor(num_patches=196, embed_dim=768, predictor_embed_dim=384, depth=1, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02)\n",
        "\n",
        "batch = 4\n",
        "img = torch.rand(batch, 3, 224, 224)\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/train.py\n",
        "mask_collator = MaskCollator(\n",
        "        input_size=(224, 224),\n",
        "        patch_size=16,\n",
        "        enc_mask_scale=(0.2, 0.8),\n",
        "        pred_mask_scale=(0.2, 0.8),\n",
        "        aspect_ratio=(0.3, 3.0),\n",
        "        nenc=1,\n",
        "        npred=4,\n",
        "        min_keep=4,\n",
        "        # allow_overlap=True)\n",
        "        allow_overlap=False)\n",
        "# self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
        "\n",
        "\n",
        "\n",
        "# v = mask_collator.step()\n",
        "# should pass the collater a list batch of idx?\n",
        "# collated_batch, collated_masks_enc, collated_masks_pred = mask_collator([batch,3,8])\n",
        "collated_batch, collated_masks_enc, collated_masks_pred = mask_collator([0,1,2,3])\n",
        "# print(v)\n",
        "\n",
        "\n",
        "def forward_target():\n",
        "    with torch.no_grad():\n",
        "        h = target_encoder(imgs)\n",
        "        h = F.layer_norm(h, (h.size(-1),))  # normalize over feature-dim\n",
        "        B = len(h)\n",
        "        # -- create targets (masked regions of h)\n",
        "        h = apply_masks(h, masks_pred)\n",
        "        h = repeat_interleave_batch(h, B, repeat=len(masks_enc))\n",
        "        return h\n",
        "\n",
        "def forward_context():\n",
        "    z = encoder(imgs, masks_enc)\n",
        "    z = predictor(z, masks_enc, masks_pred)\n",
        "    return z\n",
        "\n",
        "# # num_context_patches = 121 if allow_overlap=True else 11\n",
        "z = encoder(img, collated_masks_enc) # [batch, num_context_patches, embed_dim]\n",
        "# print(len(collated_masks_enc), len(collated_masks_pred)) # nenc, npred\n",
        "# print(collated_masks_enc[0].shape, collated_masks_pred[0].shape) # , [batch, num_context_patches = 121 or 11], [batch, num_target_patches]\n",
        "out = model(z, collated_masks_enc, collated_masks_pred)\n",
        "# # print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Or3eQvUVg7l_"
      },
      "outputs": [],
      "source": [
        "\n",
        "# print(224/16) # 14\n",
        "# print(len(collated_masks_enc), len(collated_masks_pred))\n",
        "print(collated_masks_enc, collated_masks_pred)\n",
        "# multiples of 14\n",
        "\n",
        "# print(collated_masks_pred[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2mLjXAmXcwJ"
      },
      "outputs": [],
      "source": [
        "print(collated_masks_enc, collated_masks_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XQ7PxBMlsYCI"
      },
      "outputs": [],
      "source": [
        "# @title ijepa multiblock.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "import math\n",
        "from multiprocessing import Value\n",
        "from logging import getLogger\n",
        "import torch\n",
        "_GLOBAL_SEED = 0\n",
        "logger = getLogger()\n",
        "\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=(224, 224),\n",
        "        patch_size=16,\n",
        "        enc_mask_scale=(0.2, 0.8),\n",
        "        pred_mask_scale=(0.2, 0.8),\n",
        "        aspect_ratio=(0.3, 3.0),\n",
        "        nenc=1,\n",
        "        npred=2,\n",
        "        min_keep=4,\n",
        "        allow_overlap=False\n",
        "    ):\n",
        "        super(MaskCollator, self).__init__()\n",
        "        if not isinstance(input_size, tuple):\n",
        "            input_size = (input_size, ) * 2\n",
        "        self.patch_size = patch_size\n",
        "        self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.aspect_ratio = aspect_ratio\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "        self._itr_counter = Value('i', -1)  # collator is shared across worker processes\n",
        "\n",
        "    def step(self):\n",
        "        i = self._itr_counter\n",
        "        with i.get_lock():\n",
        "            i.value += 1\n",
        "            v = i.value\n",
        "        return v\n",
        "\n",
        "    def _sample_block_size(self, generator, scale, aspect_ratio_scale):\n",
        "        _rand = torch.rand(1, generator=generator).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.height * self.width * mask_scale)\n",
        "        # -- Sample block aspect-ratio\n",
        "        min_ar, max_ar = aspect_ratio_scale\n",
        "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
        "        while h >= self.height:\n",
        "            h -= 1\n",
        "        while w >= self.width:\n",
        "            w -= 1\n",
        "\n",
        "        return (h, w)\n",
        "\n",
        "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
        "        h, w = b_size\n",
        "\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # --\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            top = torch.randint(0, self.height - h, (1,))\n",
        "            left = torch.randint(0, self.width - w, (1,))\n",
        "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
        "            mask[top:top+h, left:left+w] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "                    logger.warning(f'Mask generator says: \"Valid mask not found, decreasing acceptable-regions [{tries}]\"')\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
        "        mask_complement[top:top+h, left:left+w] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        B = len(batch)\n",
        "\n",
        "        collated_batch = torch.utils.data.default_collate(batch)\n",
        "\n",
        "        seed = self.step()\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(seed)\n",
        "        p_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.pred_mask_scale,\n",
        "            aspect_ratio_scale=self.aspect_ratio)\n",
        "        e_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.enc_mask_scale,\n",
        "            aspect_ratio_scale=(1., 1.))\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.height * self.width\n",
        "        min_keep_enc = self.height * self.width\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                logger.warning(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "\n",
        "        return collated_batch, collated_masks_enc, collated_masks_pred\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "iVUPGP93dpAN"
      },
      "outputs": [],
      "source": [
        "# @title ijepa train.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/train.py\n",
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "#\n",
        "\n",
        "import os\n",
        "\n",
        "# -- FOR DISTRIBUTED TRAINING ENSURE ONLY 1 DEVICE VISIBLE PER PROCESS\n",
        "try:\n",
        "    # -- WARNING: IF DOING DISTRIBUTED TRAINING ON A NON-SLURM CLUSTER, MAKE\n",
        "    # --          SURE TO UPDATE THIS TO GET LOCAL-RANK ON NODE, OR ENSURE\n",
        "    # --          THAT YOUR JOBS ARE LAUNCHED WITH ONLY 1 DEVICE VISIBLE\n",
        "    # --          TO EACH PROCESS\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = os.environ['SLURM_LOCALID']\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "import copy\n",
        "import logging\n",
        "import sys\n",
        "import yaml\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.multiprocessing as mp\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "\n",
        "from src.masks.multiblock import MaskCollator as MBMaskCollator\n",
        "from src.masks.utils import apply_masks\n",
        "from src.utils.distributed import (\n",
        "    init_distributed,\n",
        "    AllReduce\n",
        ")\n",
        "from src.utils.logging import (\n",
        "    CSVLogger,\n",
        "    gpu_timer,\n",
        "    grad_logger,\n",
        "    AverageMeter)\n",
        "from src.utils.tensors import repeat_interleave_batch\n",
        "from src.datasets.imagenet1k import make_imagenet1k\n",
        "\n",
        "from src.helper import (\n",
        "    load_checkpoint,\n",
        "    init_model,\n",
        "    init_opt)\n",
        "from src.transforms import make_transforms\n",
        "\n",
        "# --\n",
        "log_timings = True\n",
        "log_freq = 10\n",
        "checkpoint_freq = 50\n",
        "# --\n",
        "\n",
        "_GLOBAL_SEED = 0\n",
        "np.random.seed(_GLOBAL_SEED)\n",
        "torch.manual_seed(_GLOBAL_SEED)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logger = logging.getLogger()\n",
        "\n",
        "\n",
        "def main(args, resume_preempt=False):\n",
        "\n",
        "    # ----------------------------------------------------------------------- #\n",
        "    #  PASSED IN PARAMS FROM CONFIG FILE\n",
        "    # ----------------------------------------------------------------------- #\n",
        "\n",
        "    # -- META\n",
        "    use_bfloat16 = args['meta']['use_bfloat16']\n",
        "    model_name = args['meta']['model_name']\n",
        "    load_model = args['meta']['load_checkpoint'] or resume_preempt\n",
        "    r_file = args['meta']['read_checkpoint']\n",
        "    copy_data = args['meta']['copy_data']\n",
        "    pred_depth = args['meta']['pred_depth']\n",
        "    pred_emb_dim = args['meta']['pred_emb_dim']\n",
        "    if not torch.cuda.is_available():\n",
        "        device = torch.device('cpu')\n",
        "    else:\n",
        "        device = torch.device('cuda:0')\n",
        "        torch.cuda.set_device(device)\n",
        "\n",
        "    # -- DATA\n",
        "    use_gaussian_blur = args['data']['use_gaussian_blur']\n",
        "    use_horizontal_flip = args['data']['use_horizontal_flip']\n",
        "    use_color_distortion = args['data']['use_color_distortion']\n",
        "    color_jitter = args['data']['color_jitter_strength']\n",
        "    # --\n",
        "    batch_size = args['data']['batch_size']\n",
        "    pin_mem = args['data']['pin_mem']\n",
        "    num_workers = args['data']['num_workers']\n",
        "    root_path = args['data']['root_path']\n",
        "    image_folder = args['data']['image_folder']\n",
        "    crop_size = args['data']['crop_size']\n",
        "    crop_scale = args['data']['crop_scale']\n",
        "    # --\n",
        "\n",
        "    # -- MASK\n",
        "    allow_overlap = args['mask']['allow_overlap']  # whether to allow overlap b/w context and target blocks\n",
        "    patch_size = args['mask']['patch_size']  # patch-size for model training\n",
        "    num_enc_masks = args['mask']['num_enc_masks']  # number of context blocks\n",
        "    min_keep = args['mask']['min_keep']  # min number of patches in context block\n",
        "    enc_mask_scale = args['mask']['enc_mask_scale']  # scale of context blocks\n",
        "    num_pred_masks = args['mask']['num_pred_masks']  # number of target blocks\n",
        "    pred_mask_scale = args['mask']['pred_mask_scale']  # scale of target blocks\n",
        "    aspect_ratio = args['mask']['aspect_ratio']  # aspect ratio of target blocks\n",
        "    # --\n",
        "\n",
        "    # -- OPTIMIZATION\n",
        "    ema = args['optimization']['ema']\n",
        "    ipe_scale = args['optimization']['ipe_scale']  # scheduler scale factor (def: 1.0)\n",
        "    wd = float(args['optimization']['weight_decay'])\n",
        "    final_wd = float(args['optimization']['final_weight_decay'])\n",
        "    num_epochs = args['optimization']['epochs']\n",
        "    warmup = args['optimization']['warmup']\n",
        "    start_lr = args['optimization']['start_lr']\n",
        "    lr = args['optimization']['lr']\n",
        "    final_lr = args['optimization']['final_lr']\n",
        "\n",
        "    # -- LOGGING\n",
        "    folder = args['logging']['folder']\n",
        "    tag = args['logging']['write_tag']\n",
        "\n",
        "    dump = os.path.join(folder, 'params-ijepa.yaml')\n",
        "    with open(dump, 'w') as f:\n",
        "        yaml.dump(args, f)\n",
        "    # ----------------------------------------------------------------------- #\n",
        "\n",
        "    try:\n",
        "        mp.set_start_method('spawn')\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # -- init torch distributed backend\n",
        "    world_size, rank = init_distributed()\n",
        "    logger.info(f'Initialized (rank/world-size) {rank}/{world_size}')\n",
        "    if rank > 0:\n",
        "        logger.setLevel(logging.ERROR)\n",
        "\n",
        "    # -- log/checkpointing paths\n",
        "    log_file = os.path.join(folder, f'{tag}_r{rank}.csv')\n",
        "    save_path = os.path.join(folder, f'{tag}' + '-ep{epoch}.pth.tar')\n",
        "    latest_path = os.path.join(folder, f'{tag}-latest.pth.tar')\n",
        "    load_path = None\n",
        "    if load_model:\n",
        "        load_path = os.path.join(folder, r_file) if r_file is not None else latest_path\n",
        "\n",
        "    # -- make csv_logger\n",
        "    csv_logger = CSVLogger(log_file,\n",
        "                           ('%d', 'epoch'),\n",
        "                           ('%d', 'itr'),\n",
        "                           ('%.5f', 'loss'),\n",
        "                           ('%.5f', 'mask-A'),\n",
        "                           ('%.5f', 'mask-B'),\n",
        "                           ('%d', 'time (ms)'))\n",
        "\n",
        "    # -- init model\n",
        "    encoder, predictor = init_model(\n",
        "        device=device,\n",
        "        patch_size=patch_size,\n",
        "        crop_size=crop_size,\n",
        "        pred_depth=pred_depth,\n",
        "        pred_emb_dim=pred_emb_dim,\n",
        "        model_name=model_name)\n",
        "    target_encoder = copy.deepcopy(encoder)\n",
        "\n",
        "    # -- make data transforms\n",
        "    mask_collator = MBMaskCollator(\n",
        "        input_size=crop_size,\n",
        "        patch_size=patch_size,\n",
        "        pred_mask_scale=pred_mask_scale,\n",
        "        enc_mask_scale=enc_mask_scale,\n",
        "        aspect_ratio=aspect_ratio,\n",
        "        nenc=num_enc_masks,\n",
        "        npred=num_pred_masks,\n",
        "        allow_overlap=allow_overlap,\n",
        "        min_keep=min_keep)\n",
        "\n",
        "    transform = make_transforms(\n",
        "        crop_size=crop_size,\n",
        "        crop_scale=crop_scale,\n",
        "        gaussian_blur=use_gaussian_blur,\n",
        "        horizontal_flip=use_horizontal_flip,\n",
        "        color_distortion=use_color_distortion,\n",
        "        color_jitter=color_jitter)\n",
        "\n",
        "    # -- init data-loaders/samplers\n",
        "    _, unsupervised_loader, unsupervised_sampler = make_imagenet1k(\n",
        "            transform=transform,\n",
        "            batch_size=batch_size,\n",
        "            collator=mask_collator,\n",
        "            pin_mem=pin_mem,\n",
        "            training=True,\n",
        "            num_workers=num_workers,\n",
        "            world_size=world_size,\n",
        "            rank=rank,\n",
        "            root_path=root_path,\n",
        "            image_folder=image_folder,\n",
        "            copy_data=copy_data,\n",
        "            drop_last=True)\n",
        "    ipe = len(unsupervised_loader)\n",
        "\n",
        "    # -- init optimizer and scheduler\n",
        "    optimizer, scaler, scheduler, wd_scheduler = init_opt(\n",
        "        encoder=encoder,\n",
        "        predictor=predictor,\n",
        "        wd=wd,\n",
        "        final_wd=final_wd,\n",
        "        start_lr=start_lr,\n",
        "        ref_lr=lr,\n",
        "        final_lr=final_lr,\n",
        "        iterations_per_epoch=ipe,\n",
        "        warmup=warmup,\n",
        "        num_epochs=num_epochs,\n",
        "        ipe_scale=ipe_scale,\n",
        "        use_bfloat16=use_bfloat16)\n",
        "    encoder = DistributedDataParallel(encoder, static_graph=True)\n",
        "    predictor = DistributedDataParallel(predictor, static_graph=True)\n",
        "    target_encoder = DistributedDataParallel(target_encoder)\n",
        "    for p in target_encoder.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    # -- momentum schedule\n",
        "    momentum_scheduler = (ema[0] + i*(ema[1]-ema[0])/(ipe*num_epochs*ipe_scale)\n",
        "                          for i in range(int(ipe*num_epochs*ipe_scale)+1))\n",
        "\n",
        "    start_epoch = 0\n",
        "    # -- load training checkpoint\n",
        "    if load_model:\n",
        "        encoder, predictor, target_encoder, optimizer, scaler, start_epoch = load_checkpoint(\n",
        "            device=device,\n",
        "            r_path=load_path,\n",
        "            encoder=encoder,\n",
        "            predictor=predictor,\n",
        "            target_encoder=target_encoder,\n",
        "            opt=optimizer,\n",
        "            scaler=scaler)\n",
        "        for _ in range(start_epoch*ipe):\n",
        "            scheduler.step()\n",
        "            wd_scheduler.step()\n",
        "            next(momentum_scheduler)\n",
        "            mask_collator.step()\n",
        "\n",
        "    def save_checkpoint(epoch):\n",
        "        save_dict = {\n",
        "            'encoder': encoder.state_dict(),\n",
        "            'predictor': predictor.state_dict(),\n",
        "            'target_encoder': target_encoder.state_dict(),\n",
        "            'opt': optimizer.state_dict(),\n",
        "            'scaler': None if scaler is None else scaler.state_dict(),\n",
        "            'epoch': epoch,\n",
        "            'loss': loss_meter.avg,\n",
        "            'batch_size': batch_size,\n",
        "            'world_size': world_size,\n",
        "            'lr': lr\n",
        "        }\n",
        "        if rank == 0:\n",
        "            torch.save(save_dict, latest_path)\n",
        "            if (epoch + 1) % checkpoint_freq == 0:\n",
        "                torch.save(save_dict, save_path.format(epoch=f'{epoch + 1}'))\n",
        "\n",
        "    # -- TRAINING LOOP\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        logger.info('Epoch %d' % (epoch + 1))\n",
        "\n",
        "        # -- update distributed-data-loader epoch\n",
        "        unsupervised_sampler.set_epoch(epoch)\n",
        "\n",
        "        loss_meter = AverageMeter()\n",
        "        maskA_meter = AverageMeter()\n",
        "        maskB_meter = AverageMeter()\n",
        "        time_meter = AverageMeter()\n",
        "\n",
        "        for itr, (udata, masks_enc, masks_pred) in enumerate(unsupervised_loader):\n",
        "\n",
        "            def load_imgs():\n",
        "                # -- unsupervised imgs\n",
        "                imgs = udata[0].to(device, non_blocking=True)\n",
        "                masks_1 = [u.to(device, non_blocking=True) for u in masks_enc]\n",
        "                masks_2 = [u.to(device, non_blocking=True) for u in masks_pred]\n",
        "                return (imgs, masks_1, masks_2)\n",
        "            imgs, masks_enc, masks_pred = load_imgs()\n",
        "            maskA_meter.update(len(masks_enc[0][0]))\n",
        "            maskB_meter.update(len(masks_pred[0][0]))\n",
        "\n",
        "            def train_step():\n",
        "                _new_lr = scheduler.step()\n",
        "                _new_wd = wd_scheduler.step()\n",
        "                # --\n",
        "\n",
        "                def forward_target():\n",
        "                    with torch.no_grad():\n",
        "                        h = target_encoder(imgs)\n",
        "                        h = F.layer_norm(h, (h.size(-1),))  # normalize over feature-dim\n",
        "                        B = len(h)\n",
        "                        # -- create targets (masked regions of h)\n",
        "                        h = apply_masks(h, masks_pred)\n",
        "                        h = repeat_interleave_batch(h, B, repeat=len(masks_enc))\n",
        "                        return h\n",
        "\n",
        "                def forward_context():\n",
        "                    z = encoder(imgs, masks_enc)\n",
        "                    z = predictor(z, masks_enc, masks_pred)\n",
        "                    return z\n",
        "\n",
        "                def loss_fn(z, h):\n",
        "                    loss = F.smooth_l1_loss(z, h)\n",
        "                    loss = AllReduce.apply(loss)\n",
        "                    return loss\n",
        "\n",
        "                # Step 1. Forward\n",
        "                with torch.cuda.amp.autocast(dtype=torch.bfloat16, enabled=use_bfloat16):\n",
        "                    h = forward_target()\n",
        "                    z = forward_context()\n",
        "                    loss = loss_fn(z, h)\n",
        "\n",
        "                #  Step 2. Backward & step\n",
        "                if use_bfloat16:\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                else:\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                grad_stats = grad_logger(encoder.named_parameters())\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Step 3. momentum update of target encoder\n",
        "                with torch.no_grad():\n",
        "                    m = next(momentum_scheduler)\n",
        "                    for param_q, param_k in zip(encoder.parameters(), target_encoder.parameters()):\n",
        "                        param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "\n",
        "                return (float(loss), _new_lr, _new_wd, grad_stats)\n",
        "            (loss, _new_lr, _new_wd, grad_stats), etime = gpu_timer(train_step)\n",
        "            loss_meter.update(loss)\n",
        "            time_meter.update(etime)\n",
        "\n",
        "            # -- Logging\n",
        "            def log_stats():\n",
        "                csv_logger.log(epoch + 1, itr, loss, maskA_meter.val, maskB_meter.val, etime)\n",
        "                if (itr % log_freq == 0) or np.isnan(loss) or np.isinf(loss):\n",
        "                    logger.info('[%d, %5d] loss: %.3f '\n",
        "                                'masks: %.1f %.1f '\n",
        "                                '[wd: %.2e] [lr: %.2e] '\n",
        "                                '[mem: %.2e] '\n",
        "                                '(%.1f ms)'\n",
        "                                % (epoch + 1, itr,\n",
        "                                   loss_meter.avg,\n",
        "                                   maskA_meter.avg,\n",
        "                                   maskB_meter.avg,\n",
        "                                   _new_wd,\n",
        "                                   _new_lr,\n",
        "                                   torch.cuda.max_memory_allocated() / 1024.**2,\n",
        "                                   time_meter.avg))\n",
        "\n",
        "                    if grad_stats is not None:\n",
        "                        logger.info('[%d, %5d] grad_stats: [%.2e %.2e] (%.2e, %.2e)'\n",
        "                                    % (epoch + 1, itr,\n",
        "                                       grad_stats.first_layer,\n",
        "                                       grad_stats.last_layer,\n",
        "                                       grad_stats.min,\n",
        "                                       grad_stats.max))\n",
        "\n",
        "            log_stats()\n",
        "\n",
        "            assert not np.isnan(loss), 'loss is nan'\n",
        "\n",
        "        # -- Save Checkpoint after every epoch\n",
        "        logger.info('avg. loss %.3f' % loss_meter.avg)\n",
        "        save_checkpoint(epoch+1)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "s3EO3PgMPH1x"
      ],
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}