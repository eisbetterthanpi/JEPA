{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/I_JEPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxACli7GdyGq",
        "outputId": "8ded98bc-ce09-4c5a-a986-819f4d503b24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:06<00:00, 27.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title data\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "transform = transforms.Compose([transforms.ToTensor(),])\n",
        "# transform = transforms.Compose([transforms.RandomResizedCrop((32,32), scale=(.3,1.)), transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "# CIFAR10: 60000 32x32 color images in 10 classes, with 6000 images per class\n",
        "train_data = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
        "test_data = torchvision.datasets.CIFAR10(root='data', train=False, download=True, transform=transform)\n",
        "batch_size = 64 # 4\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# dataiter = iter(train_loader) # get some random training images\n",
        "# images, labels = next(dataiter)\n",
        "# print(images.shape) # [batch, 3, 32, 32]\n",
        "# imshow(torchvision.utils.make_grid(images))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3EO3PgMPH1x"
      },
      "source": [
        "## hiera"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "j3-vvMS1-gVn"
      },
      "outputs": [],
      "source": [
        "# @title ResBlock\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters(): p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, kernel=3):\n",
        "        super().__init__()\n",
        "        out_ch = out_ch or in_ch\n",
        "        act = nn.SiLU() #\n",
        "        self.res_conv = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
        "        # self.block = nn.Sequential( # best?\n",
        "        #     nn.Conv2d(in_ch, out_ch, 3, padding=1), nn.BatchNorm2d(out_ch), act,\n",
        "        #     zero_module(nn.Conv2d(out_ch, out_ch, 3, padding=1)), nn.BatchNorm2d(out_ch), act,\n",
        "        #     )\n",
        "        self.block = nn.Sequential(\n",
        "            nn.BatchNorm2d(in_ch), act, nn.Conv2d(in_ch, out_ch, kernel, padding=kernel//2),\n",
        "            nn.BatchNorm2d(out_ch), act, zero_module(nn.Conv2d(out_ch, out_ch, kernel, padding=kernel//2)),\n",
        "            )\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w]\n",
        "        return self.block(x) + self.res_conv(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0lclc9myo2c",
        "outputId": "35ea2b51-c2dc-4d96-f2ae-c27bd760dcd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([12, 2, 16, 32, 32])\n"
          ]
        }
      ],
      "source": [
        "# @title UpDownBlock_me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class PixelShuffleConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, kernel=1, r=1):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        r = max(r, int(1/r))\n",
        "        out_ch = out_ch or in_ch\n",
        "        if self.r>1: self.net = nn.Sequential(ResBlock(in_ch, out_ch*r**2, kernel), nn.PixelShuffle(r))\n",
        "        # if self.r>1: self.net = nn.Sequential(Attention(in_ch, out_ch*r**2), nn.PixelShuffle(r))\n",
        "\n",
        "        elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), ResBlock(in_ch*r**2, out_ch, kernel))\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), Attention(in_ch*r**2, out_ch))\n",
        "        elif in_ch != out_ch: self.net = ResBlock(in_ch*r**2, out_ch, kernel)\n",
        "        else: self.net = lambda x: torch.zeros_like(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def AdaptiveAvgPool_nd(n, *args, **kwargs): return [nn.Identity, nn.AdaptiveAvgPool1d, nn.AdaptiveAvgPool2d, nn.AdaptiveAvgPool3d][n](*args, **kwargs)\n",
        "def AdaptiveMaxPool_nd(n, *args, **kwargs): return [nn.Identity, nn.AdaptiveMaxPool1d, nn.AdaptiveMaxPool2d, nn.AdaptiveMaxPool3d][n](*args, **kwargs)\n",
        "\n",
        "def adaptive_avg_pool_nd(n, x, output_size): return [nn.Identity, F.adaptive_avg_pool1d, F.adaptive_avg_pool2d, F.adaptive_avg_pool3d][n](x, output_size)\n",
        "def adaptive_max_pool_nd(n, x, output_size): return [nn.Identity, F.adaptive_max_pool1d, F.adaptive_max_pool2d, F.adaptive_max_pool3d][n](x, output_size)\n",
        "\n",
        "class AdaptivePool_at(nn.AdaptiveAvgPool1d): # AdaptiveAvgPool1d AdaptiveMaxPool1d\n",
        "    def __init__(self, dim=1, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.dim=dim\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(self.dim,-1)\n",
        "        shape = x.shape\n",
        "        return super().forward(x.flatten(0,-2)).unflatten(0, shape[:-1]).transpose(self.dim,-1)\n",
        "\n",
        "\n",
        "def adaptive_pool_at(x, dim, output_size, pool='avg'): # [b,c,h,w]\n",
        "    x = x.transpose(dim,-1)\n",
        "    shape = x.shape\n",
        "    parent={'avg':F.adaptive_avg_pool1d, 'max':F.adaptive_max_pool1d}[pool]\n",
        "    return parent(x.flatten(0,-2), output_size).unflatten(0, shape[:-1]).transpose(dim,-1)\n",
        "\n",
        "\n",
        "class ZeroExtend():\n",
        "    def __init__(self, dim=1, output_size=16):\n",
        "        self.dim, self.out = dim, output_size\n",
        "    def __call__(self, x): # [b,c,h,w]\n",
        "        return torch.cat((x, torch.zeros(*x.shape[:self.dim], self.out - x.shape[self.dim], *x.shape[self.dim+1:])), dim=self.dim)\n",
        "\n",
        "def make_pool_at(pool='avg', dim=1, output_size=5):\n",
        "    parent={'avg':nn.AdaptiveAvgPool1d, 'max':nn.AdaptiveMaxPool1d}[pool]\n",
        "    class AdaptivePool_at(parent): # AdaptiveAvgPool1d AdaptiveMaxPool1d\n",
        "        def __init__(self, dim=1, *args, **kwargs):\n",
        "            super().__init__(*args, **kwargs)\n",
        "            self.dim=dim\n",
        "        def forward(self, x):\n",
        "            x = x.transpose(self.dim,-1)\n",
        "            shape = x.shape\n",
        "            return super().forward(x.flatten(0,-2)).unflatten(0, shape[:-1]).transpose(self.dim,-1)\n",
        "    return AdaptivePool_at(dim, output_size=output_size)\n",
        "\n",
        "class Shortcut():\n",
        "    def __init__(self, dim=1, c=3, sp=(3,3), nd=2):\n",
        "        self.dim = dim\n",
        "        # self.ch_pool = make_pool_at(pool='avg', dim=dim, output_size=c)\n",
        "        self.ch_pool = make_pool_at(pool='max', dim=dim, output_size=c)\n",
        "        # self.ch_pool = ZeroExtend(dim, output_size=c) # only for out_dim>=in_dim\n",
        "        # self.sp_pool = AdaptiveAvgPool_nd(nd, sp)\n",
        "        self.sp_pool = AdaptiveMaxPool_nd(nd, sp)\n",
        "\n",
        "    def __call__(self, x): # [b,c,h,w]\n",
        "        x = self.sp_pool(x) # spatial first preserves spatial more?\n",
        "        x = self.ch_pool(x)\n",
        "        return x\n",
        "\n",
        "class UpDownBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, kernel=7, r=1):\n",
        "        super().__init__()\n",
        "        act = nn.SiLU()\n",
        "        self.r = r\n",
        "        self.block = PixelShuffleConv(in_ch, out_ch, kernel=kernel, r=r)\n",
        "        # self.block = nn.Sequential(\n",
        "        #     nn.BatchNorm2d(in_ch), act, PixelShuffleConv(in_ch, out_ch, kernel=kernel, r=r)\n",
        "        # )\n",
        "        # if self.r>1: self.res_conv = nn.Sequential(nn.ConvTranspose2d(in_ch, out_ch, kernel, 2, kernel//2, output_padding=1))\n",
        "        # if self.r>1: self.res_conv = nn.Sequential(nn.Upsample(scale_factor=2, mode='nearest'), nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity())\n",
        "        # if self.r>1: self.res_conv = nn.Sequential(nn.Upsample(scale_factor=2, mode='bilinear'), nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity())\n",
        "        # if self.r>1: self.res_conv = nn.Sequential(nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True), nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity())\n",
        "\n",
        "\n",
        "        # elif self.r<1: self.res_conv = nn.Sequential(nn.Conv2d(in_ch, out_ch, kernel, 2, kernel//2))\n",
        "        # elif self.r<1: self.res_conv = nn.Sequential(nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity(), nn.MaxPool2d(2,2))\n",
        "        # elif self.r<1: self.res_conv = nn.Sequential(nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity(), nn.AvgPool2d(2,2))\n",
        "        # elif self.r<1: self.res_conv = AttentionBlock(in_ch, out_ch, n_heads=4, q_stride=(2,2))\n",
        "\n",
        "        # else: self.res_conv = nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity()\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w]\n",
        "        b, num_tok, c, *win = x.shape\n",
        "        x = x.flatten(0,1)\n",
        "        out = self.block(x)\n",
        "        # # shortcut = F.interpolate(x.unsqueeze(1), size=out.shape[1:], mode='nearest-exact').squeeze(1) # pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html\n",
        "        # shortcut = F.adaptive_avg_pool3d(x, out.shape[1:]) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "        # # shortcut = F.adaptive_max_pool3d(x, out.shape[1:]) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "        # # shortcut = F.adaptive_avg_pool3d(x, out.shape[1:]) if out.shape[1]>=x.shape[1] else F.adaptive_max_pool3d(x, out.shape[1:])\n",
        "        # shortcut(x)\n",
        "        shortcut = Shortcut(dim=1, c=out.shape[1], sp=out.shape[-2:], nd=2)(x)\n",
        "        out = out + shortcut\n",
        "        out = out.unflatten(0, (b, num_tok))\n",
        "        return out\n",
        "\n",
        "        # return out + shortcut + self.res_conv(x)\n",
        "        # return out + self.res_conv(x)\n",
        "        # return self.res_conv(x)\n",
        "\n",
        "# if out>in, inter=max=ave=near.\n",
        "# if out<in, inter=ave. max=max\n",
        "\n",
        "# stride2\n",
        "# interconv/convpool\n",
        "# pixelconv\n",
        "# pixeluib\n",
        "# pixelres\n",
        "# shortcut\n",
        "\n",
        "# in_ch, out_ch = 16,3\n",
        "in_ch, out_ch = 3,16\n",
        "model = UpDownBlock(in_ch, out_ch, r=1/2).to(device)\n",
        "# model = UpDownBlock(in_ch, out_ch, r=2).to(device)\n",
        "\n",
        "x = torch.rand(12, in_ch, 64,64, device=device)\n",
        "x = torch.rand(12, 2, in_ch, 64,64, device=device)\n",
        "out = model(x)\n",
        "\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAyKHKivc0j7",
        "outputId": "e66e47ef-f7e7-44c1-f15f-168e0adf1807"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MUattn 256\n",
            "torch.Size([2, 3, 16, 16, 16])\n"
          ]
        }
      ],
      "source": [
        "# @title maxpool path bchw\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def conv_nd(n, *args, **kwargs): return [nn.Identity, nn.Conv1d, nn.Conv2d, nn.Conv3d][n](*args, **kwargs)\n",
        "def maxpool_nd(n, *args, **kwargs): return [nn.Identity, nn.MaxPool1d, nn.MaxPool2d, nn.MaxPool3d][n](*args, **kwargs)\n",
        "def avgpool_nd(n, *args, **kwargs): return [nn.Identity, nn.AvgPool1d, nn.AvgPool2d, nn.AvgPool3d][n](*args, **kwargs)\n",
        "\n",
        "import math\n",
        "class MaskUnitAttention(nn.Module):\n",
        "    # def __init__(self, d_model=16, n_heads=4, q_stride=None, nd=2):\n",
        "    def __init__(self, in_dim, d_model=16, n_heads=4, q_stride=None, nd=2):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads, self.d_head = n_heads, d_model // n_heads\n",
        "        self.scale = self.d_head**-.5\n",
        "        # self.qkv = conv_nd(nd, d_model, 3*d_model, 1, bias=False)\n",
        "        self.qkv = conv_nd(nd, in_dim, 3*d_model, 1, bias=False)\n",
        "        # self.out = conv_nd(nd, d_model, d_model, 1)\n",
        "        self.out = nn.Linear(d_model, d_model, 1)\n",
        "        self.q_stride = q_stride # If greater than 1, pool q with this stride. The stride should be flattened (e.g., 2x2 = 4).\n",
        "        if q_stride:\n",
        "            self.q_stride = (q_stride,)*nd if type(q_stride)==int else q_stride\n",
        "            self.q_pool = maxpool_nd(nd, self.q_stride, stride=self.q_stride)\n",
        "\n",
        "    def forward(self, x): # [b,num_tok,c,win1,win2]\n",
        "        b, num_tok, c, *win = x.shape\n",
        "        x = x.flatten(0,1) # [b*num_tok,c,win1,win2]\n",
        "        # print(x.shape)\n",
        "        # q, k, v = self.qkv(x).reshape(b, 3, self.n_heads, self.d_head, num_tok, win*win).permute(1,0,2,4,5,3) # [b,3*d_model,num_tok*win,win] -> 3* [b, n_heads, num_tok, win*win, d_head]\n",
        "        # self.qkv(x).flatten(1,-2).unflatten(-1, (self.heads,-1)).chunk(3, dim=-1) # [b,sp,n_heads,d_head]\n",
        "        q,k,v = self.qkv(x).chunk(3, dim=1) # [b*num_tok,d,win,win]\n",
        "        if self.q_stride:\n",
        "            q = self.q_pool(q)\n",
        "            win=[w//s for w,s in zip(win, self.q_stride)] # win = win/q_stride\n",
        "        if math.prod(win) >= 200:\n",
        "            print('MUattn', math.prod(win))\n",
        "            q, k, v = map(lambda t: t.reshape(b*num_tok, self.n_heads, self.d_head, -1).transpose(-2,-1), (q,k,v)) # [b*num_tok, n_heads, win*win, d_head]\n",
        "        else:\n",
        "            # q, k, v = map(lambda t: t.reshape(b, num_tok, self.n_heads, self.d_head, -1).permute(0,2,4,1,3), (q,k,v)) # downsampling attention # [b, n_heads, win*win, num_tok, d_head]\n",
        "            q, k, v = map(lambda t: t.reshape(b, num_tok, self.n_heads, self.d_head, -1).permute(0,2,1,4,3).flatten(2,3), (q,k,v)) # [b, n_heads, num_tok*win*win, d_head]\n",
        "\n",
        "        # x = F.scaled_dot_product_attention(q,k,v, attn_mask=None) # https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html\n",
        "        q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2) # [b, n_heads, t(/pool), d_head]\n",
        "        context = k.transpose(-2,-1) @ v # [b, n_heads, d_head, d_head]\n",
        "        x = q @ context # [b, n_heads, t(/pool), d_head]\n",
        "\n",
        "        # print('attn fwd 2',x.shape)\n",
        "        # x = x.transpose(1, 3).reshape(B, -1, self.d_model)\n",
        "        x = x.transpose(1,2).reshape(b, -1, self.d_model) # [b,t,d]\n",
        "        # x = x.transpose(-2,-1).reshape(x.shape[0], self.d_model, ) # [b,t,d]\n",
        "        # [b, n_heads, num_tok, win*win, d_head] -> [b, n_heads, d_head, num_tok, win*win] -> [b,c,num_tok*win,win]\n",
        "        # x = x.permute(0,1,4,2,3).reshape(b, self.d_model, num_tok*win,win) # [b, n_heads, num_tok, win*win, d_head]\n",
        "        # x = x.transpose(-2,-1).reshape(b, self.d_model, num_tok, *win) # [b*num_tok, n_heads, win*win, d_head]\n",
        "        x = self.out(x)\n",
        "        L=len(win)\n",
        "        x = x.reshape(b, num_tok, *win, self.d_model).permute(0,1,L+2,*range(2,L+2)) # [b, num_tok, out_dim, *win]\n",
        "        # x = x.unflatten(0, (b, num_tok))\n",
        "        return x # [b,num_tok,c,win1,win2]\n",
        "\n",
        "d_model=16\n",
        "model = MaskUnitAttention(d_model, n_heads=4, q_stride=2)\n",
        "# MaskUnitAttention(d_model=16, n_heads=4, q_stride=None, nd=2)\n",
        "# x=torch.randn(2,4,4,3)\n",
        "# x=torch.randn(2,3,d_model,8,8)\n",
        "x=torch.randn(2,3,d_model,32,32)\n",
        "# [b*num_tok,c,win,win]\n",
        "\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lec1Nq7nkbgt",
        "outputId": "bd142f82-f226-4884-ab9a-33d38e8b9c67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "124928\n",
            "torch.Size([4, 64, 64])\n",
            "124928\n"
          ]
        }
      ],
      "source": [
        "# @title hiera vit me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class LayerNorm_at(nn.RMSNorm): # LayerNorm RMSNorm\n",
        "    def __init__(self, dim=1, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.dim=dim\n",
        "    def forward(self, x):\n",
        "        return super().forward(x.transpose(self.dim,-1)).transpose(self.dim,-1)\n",
        "\n",
        "# # [b, h/win1* w/win2, c, win1,win2] -> [b,c,h,w]\n",
        "# def shuffle(x, window_shape): # [b,win*win, c, h/win, w/win] -> [b,1,c,h,w]\n",
        "#     # out_shape = [x.shape[0], -1, x.shape[2]] + [x*w for x,w in zip(x.shape[3:], window_shape)] # [b,1,c,h/win*wim, w/win*wim]\n",
        "#     out_shape = [x.shape[0], -1, x.shape[2]] + [x*w for x,w in zip(x.shape[3:], window_shape)] # [b,1,c,h/win*wim, w/win*wim]\n",
        "#     x = x.unflatten(1, (-1, *window_shape)) # [b,num_tok,win,win, c, h/win, w/win]\n",
        "#     D=x.dim()+1\n",
        "#     permute = [0,1,D//2] + [val for tup in zip(range(1+D//2, D), range(2, D//2)) for val in tup]\n",
        "#     x = x.permute(permute).reshape(out_shape)\n",
        "#     return x\n",
        "\n",
        "def unshuffle(x, window_shape): # [b,c,h,w] -> [b, h/win1* w/win2, c, win1,win2]\n",
        "    new_shape = list(x.shape[:2]) + [val for xx, win in zip(list(x.shape[2:]), window_shape) for val in [xx//win, win]] # [h,w]->[h/win1, win1, w/win2, win2]\n",
        "    x = x.reshape(new_shape) # [b, c, h/win1, win1, w/win2, win2]\n",
        "    # print('unsh',x.shape, window_shape, new_shape)\n",
        "    L = len(new_shape)\n",
        "    permute = ([0] + list(range(2, L - 1, 2)) + [1] + list(range(3, L, 2))) # [0,2,4,1,3,5] / [0,2,4,6,1,3,5,6]\n",
        "    return x.permute(permute).flatten(1, L//2-1) # [b, h/win1* w/win2, c, win1,win2]\n",
        "\n",
        "class HieraBlock(nn.Module):\n",
        "    # def __init__(self, d_model, n_heads, q_stride=None, mult=4, drop=0, nd=2):\n",
        "    def __init__(self, in_dim, d_model, n_heads, q_stride=None, mult=4, drop=0, nd=2):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        # self.norm = LayerNorm_at(2, d_model) # LayerNorm RMSNorm\n",
        "        self.norm = LayerNorm_at(2, in_dim) # LayerNorm RMSNorm\n",
        "        self.drop = nn.Dropout(drop)\n",
        "        # self.attn = MaskUnitAttention(d_model, n_heads, q_stride)\n",
        "        self.attn = MaskUnitAttention(in_dim, d_model, n_heads, q_stride)\n",
        "        ff_dim=d_model*mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), nn.Linear(d_model, ff_dim), nn.ReLU(), # ReLU GELU\n",
        "            nn.RMSNorm(ff_dim), nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "            # nn.RMSNorm(d_model), act, nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Linear(ff_dim, d_model)\n",
        "            # nn.RMSNorm(d_model), nn.Linear(d_model, ff_dim), nn.ReLU(), nn.Dropout(dropout), # ReLU GELU\n",
        "            # nn.Linear(ff_dim, d_model), nn.Dropout(dropout),\n",
        "        )\n",
        "        # self.res = conv_nd(nd, d_model, d_model, q_stride, q_stride) if q_stride else nn.Identity()\n",
        "\n",
        "        # self.res = nn.Sequential(\n",
        "        #     conv_nd(nd, in_dim, d_model, 1, 1) if in_dim!=d_model else nn.Identity(),\n",
        "        #     # maxpool_nd(nd, q_stride, q_stride) if q_stride else nn.Identity(),\n",
        "        #     avgpool_nd(nd, q_stride, q_stride) if q_stride else nn.Identity(),\n",
        "        # )\n",
        "        self.res = UpDownBlock(in_dim, d_model, kernel=1, r=1/2 if q_stride else 1)\n",
        "\n",
        "        # x = self.proj(x_norm).unflatten(1, (self.attn.q_stride, -1)).max(dim=1).values # pooling res # [b, (Sy, Sx, h/Sy, w/Sx), c] -> [b, (Sy, Sx), (h/Sy, w/Sx), c] -> [b, (h/Sy, w/Sx), c]\n",
        "\n",
        "\n",
        "    def forward(self, x): # [b, num_tok, c, *win]\n",
        "        b, num_tok, c, *win = x.shape\n",
        "        # print('attnblk fwd',x.shape)\n",
        "        # x = x + self.drop(self.self(self.norm(x)))\n",
        "        # print('attnblk fwd',self.res(x.flatten(0,1)).shape, self.drop(self.attn(self.norm(x))).flatten(0,1).shape)\n",
        "        # x = self.res(x.flatten(0,1)) + self.drop(self.attn(self.norm(x))).flatten(0,1) # [b*num_tok,c,win1,win2,win3]\n",
        "        x = self.res(x) + self.drop(self.attn(self.norm(x))) # [b*num_tok,c,win1,win2,win3]\n",
        "        # x = x + self.ff(x)\n",
        "        # x = x + self.ff(x.transpose(1,-1)).transpose(1,-1)\n",
        "        x = x + self.ff(x.transpose(2,-1)).transpose(2,-1)\n",
        "        # x = self.ff(x)\n",
        "        # x = x.unflatten(0, (b, num_tok))\n",
        "        return x\n",
        "\n",
        "    # def forward(self, x): # [b,t,c] # [b, (Sy, Sx, h/Sy, w/Sx), c]\n",
        "    #     # Attention + Q Pooling\n",
        "    #     x_norm = self.norm1(x)\n",
        "    #     if self.dim != self.dim_out:\n",
        "    #         x = self.proj(x_norm).unflatten(1, (self.attn.q_stride, -1)).max(dim=1).values # pooling res # [b, (Sy, Sx, h/Sy, w/Sx), c] -> [b, (Sy, Sx), (h/Sy, w/Sx), c] -> [b, (h/Sy, w/Sx), c]\n",
        "    #     x = x + self.drop_path(self.attn(x_norm))\n",
        "    #     x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "    #     return x\n",
        "\n",
        "\n",
        "class levelBlock(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, n_heads=None, depth=1, r=1):\n",
        "        super().__init__()\n",
        "        self.seq = nn.Sequential(\n",
        "            UpDownBlock(in_ch, out_ch, r=min(1,r)) if in_ch != out_ch or r<1 else nn.Identity(),\n",
        "            # AttentionBlock(d_model, d_model, n_heads, q_stride) if in_ch != out_ch or r<1 else nn.Identity(),\n",
        "            # AttentionBlock(d_model, d_model, n_heads, q_stride=(2,2)),\n",
        "            *[HieraBlock(d_model, d_model, n_heads) for i in range(1)],\n",
        "            # UpDownBlock(out_ch, out_ch, r=r) if r>1 else nn.Identity(),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.seq(x)\n",
        "\n",
        "\n",
        "class Hiera(nn.Module):\n",
        "    def __init__(self, patch_size, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "    # def __init__(self, in_dim, out_dim, d_model, n_heads, depth):\n",
        "        # patch_size=4\n",
        "        super().__init__()\n",
        "        self.embed = nn.Sequential( # in, out, kernel, stride, pad\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.MaxPool2d(3,2,1), # nn.MaxPool2d(2,2)\n",
        "            nn.Conv2d(in_dim, d_model, 7, 1, 7//2, bias=False),\n",
        "            # nn.Conv2d(in_dim, d_model, patch_size, patch_size), # like patch\n",
        "            # nn.Conv2d(in_dim, d_model, 3, 2, 3//2, bias=False), nn.BatchNorm2d(d_model), nn.ReLU(),\n",
        "            # UpDownBlock(in_dim, dim, r=1/2, kernel=3), UpDownBlock(dim, dim, r=1/2, kernel=3)\n",
        "            # nn.PixelUnshuffle(2), nn.Conv2d(in_dim*2**2, dim, 1, bias=False),\n",
        "            # ResBlock(in_dim, d_model, kernel),\n",
        "            )\n",
        "        # # self.pos_emb = LearnedRoPE2D(dim) # LearnedRoPE2D, RoPE2D\n",
        "\n",
        "        emb_shape = (32,32)\n",
        "        # emb_shape = (8,32,32)\n",
        "        if len(emb_shape) == 3: # for video\n",
        "            pos_spatial = nn.Parameter(torch.randn(1, emb_shape[1]*emb_shape[2], d_model)*.02)\n",
        "            pos_temporal = nn.Parameter(torch.randn(1, emb_shape[0], d_model)*.02)\n",
        "            self.pos_emb = pos_spatial.repeat(1, emb_shape[0], 1) + torch.repeat_interleave(pos_temporal, emb_shape[1] * emb_shape[2], dim=1)\n",
        "        elif len(emb_shape) == 2: # for img\n",
        "            self.pos_emb = nn.Parameter(torch.randn(1, d_model, *emb_shape)*.02) # 56*56=3136\n",
        "        # self.pos_emb = self.pos_emb.flatten(2).transpose(-2,-1)\n",
        "\n",
        "        # self.blocks = nn.Sequential(*[AttentionBlock(d_model, n_heads, q_stride=(2,2) if i in [1,3] else None) for i in range(depth)])\n",
        "        mult = [1,1,1,1]\n",
        "        # mult = [1,2,4,8] # [1,2,3,4] [1,2,2,2]\n",
        "        ch_list = [d_model * m for m in mult] # [128, 256, 384, 512]\n",
        "\n",
        "        self.blocks = nn.Sequential(\n",
        "            # HieraBlock(ch_list[0], ch_list[1], n_heads, q_stride=(2,2)),\n",
        "            # UpDownBlock(ch_list[0], ch_list[1], kernel=1, r=1/2),\n",
        "            HieraBlock(ch_list[1], ch_list[1], n_heads),\n",
        "            # HieraBlock(ch_list[1], ch_list[2], n_heads, q_stride=(2,2)),\n",
        "            # UpDownBlock(ch_list[1], ch_list[2], kernel=1, r=1/2),\n",
        "            # HieraBlock(ch_list[2], ch_list[2], n_heads),\n",
        "            )\n",
        "        self.norm = nn.RMSNorm(ch_list[2]) # LayerNorm RMSNorm\n",
        "        self.out = nn.Linear(ch_list[2], out_dim, bias=False) if out_dim and out_dim != ch_list[2] else None\n",
        "\n",
        "\n",
        "    def forward(self, x, context_indices=None): # [b,c,h,w], [b,num_tok]\n",
        "        x = self.embed(x)\n",
        "        # print('vit fwd', x.shape, self.pos_emb.shape)\n",
        "        x = x + self.pos_emb\n",
        "        x = unshuffle(x, (4,4)) # [b,num_tok,c,win1,win2,win3] or [b,1,c,f,h,w]\n",
        "        if context_indices != None: x = x[torch.arange(x.shape[0]).unsqueeze(-1), context_indices] # [batch, num_context_toks, d_model]\n",
        "\n",
        "        # print('vit fwd1', x.shape)\n",
        "        x = self.blocks(x) # [b,num_tok,c,1,1,1]\n",
        "        # print('vit fwd2', x.shape)\n",
        "        # x = x.mean(-1).mean(-1)\n",
        "        x = x.flatten(-2).max(-1)[0]\n",
        "        # print('vit fwd3', x.shape)\n",
        "        x = x.squeeze() # [b,num_tok,d]\n",
        "        out = self.norm(x)\n",
        "        if self.out: out = self.out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "# pos_emb rope < learn < learned\n",
        "# conv > pixel?\n",
        "# droppath not required\n",
        "\n",
        "# norm,act,conv < conv,norm,act\n",
        "# 2*s1 < uib < resblock\n",
        "# gatedadaln 3 < 2 = 1 < ffmult4 = 2*gatedadaln\n",
        "# MaxPool2d(2,2) < MaxPool2d(3,2,3//2)\n",
        "\n",
        "# patchattn only for\n",
        "\n",
        "# multiendfusion negligible diff\n",
        "\n",
        "\n",
        "d_model = 64\n",
        "in_dim = 3\n",
        "patch_size = 2\n",
        "# model = Hiera(patch_size, in_dim, d_model, n_heads=4, nlayers=4, drop=0.).to(device)\n",
        "model = Hiera(patch_size, in_dim, d_model, n_heads=4, nlayers=1, drop=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x = torch.rand((4, in_dim, 32, 32), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 59850\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3XSZZyUPY1i"
      },
      "source": [
        "## vit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "eDqkaM2v0_zS"
      },
      "outputs": [],
      "source": [
        "# @title FSQ me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def ste_round(x): return x.round().detach() + x - x.detach()\n",
        "\n",
        "class FSQ(nn.Module):\n",
        "    def __init__(self, levels):\n",
        "        super().__init__()\n",
        "        self.levels = torch.tensor(levels, device=device)\n",
        "        self.basis = torch.cumprod(torch.tensor([*levels[1:], 1], device=device).flip(-1), dim=0).flip(-1)\n",
        "        self.half_width = (self.levels-1)/2\n",
        "        self.codebook_size = torch.prod(self.levels).item()\n",
        "        self.codebook = self.indexes_to_codes(torch.arange(self.codebook_size, device=device))\n",
        "\n",
        "    def forward(self, z, beta=1): # beta in (0,1). beta->0 => values more spread out\n",
        "        offset = (self.levels+1) % 2 /2 # .5 if even, 0 if odd\n",
        "        bound = (F.sigmoid(z)-1/2) * (self.levels-beta) + offset\n",
        "        # print('fwd', bound) #\n",
        "        quantized = ste_round(bound)\n",
        "        # print('fwd', quantized) # 4: -1012\n",
        "        return (quantized-offset) / self.half_width # split [-1,1]\n",
        "\n",
        "    def codes_to_indexes(self, zhat):\n",
        "        zhat = (zhat + 1) * self.half_width\n",
        "        return (zhat * self.basis).sum(axis=-1)#.int()\n",
        "\n",
        "    def indexes_to_codes(self, indices):\n",
        "        indices = indices.unsqueeze(-1)\n",
        "        codes = torch.remainder(indices//self.basis, self.levels)\n",
        "        # print(\"codes\",codes)\n",
        "        return codes / self.half_width - 1\n",
        "\n",
        "# fsq = FSQ(levels = [5,4,3,2])\n",
        "# # print(fsq.codebook)\n",
        "# batch_size, seq_len = 2, 4\n",
        "# # x = torch.rand((batch_size, seq_len,3),device=device)\n",
        "# x = torch.linspace(-2,2,7).repeat(4,1).T\n",
        "# la = fsq(x)\n",
        "# print(la)\n",
        "# lact = fsq.codes_to_indexes(la)\n",
        "# print(lact)\n",
        "# # la = fsq.indexes_to_codes(lact)\n",
        "# # print(la)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6T4F651kmGh",
        "outputId": "e0ef3153-c51b-44a1-f906-2e4908b2f1be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "162112\n",
            "torch.Size([5, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# @title AttentionBlock\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class SelfAttn(nn.Module):\n",
        "    def __init__(self, dim, n_heads):\n",
        "        super().__init__()\n",
        "        self.dim, self.n_heads = dim, n_heads\n",
        "        d_head = dim//n_heads\n",
        "        self.qkv = nn.Linear(dim, dim*3, bias=False)\n",
        "        # self.lin = nn.Linear(dim, dim)\n",
        "        self.lin = zero_module(nn.Linear(dim, dim))\n",
        "        # self.rope = RoPE(d_head, seq_len=512, base=10000)\n",
        "        # self.rope = RoPE2D(d_head, h=64, w=64, base=100)\n",
        "        self.scale = d_head**-.5\n",
        "\n",
        "    def forward(self, x): # [b,t,d]\n",
        "        q,k,v = self.qkv(x).unflatten(-1, (self.n_heads,-1)).transpose(1,2).chunk(3, dim=-1) # [b, t, n_heads, d_head] -> [b, n_heads, t, d_head]\n",
        "        # q, k = self.rope(q), self.rope(k)\n",
        "        # x = F.scaled_dot_product_attention(q,k,v, attn_mask=None) # https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html\n",
        "        q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2)\n",
        "        context = k.transpose(-2,-1) @ v # [batch, n_heads, d_head, d_head]\n",
        "        x = q @ context # [b, n_heads, t, d_head]\n",
        "        # # print('SelfAttn', x.shape)\n",
        "        x = x.transpose(1,2).flatten(2)\n",
        "        return self.lin(x)\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, mult=4, drop=0.):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        # self.norm = nn.RMSNorm(d_model, elementwise_affine=False) # LayerNorm RMSNorm\n",
        "        self.norm1, self.norm2 = nn.LayerNorm(d_model), nn.LayerNorm(d_model)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "        self.attn = SelfAttn(d_model, n_heads)\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        ff_dim=d_model*mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), act, nn.Dropout(drop), nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "            nn.RMSNorm(ff_dim), act, nn.Dropout(drop), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(drop), nn.Linear(d_model, ff_dim), act,\n",
        "            # nn.RMSNorm(ff_dim), nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x): # [batch, seq_len, d_model]\n",
        "        # print('attnblk fwd',x.shape)\n",
        "        x = x + self.drop(self.attn(self.norm1(x)))\n",
        "        x = x + self.ff(x)\n",
        "        # x = x + self.drop(self.ff(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, patch_size, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        patch_size=2\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Conv2d(in_dim, d_model, patch_size, patch_size), # like patch\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 1, 7//2, bias=False)\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.MaxPool2d(3,2,1)\n",
        "            nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.BatchNorm2d(d_model), nn.ReLU(),\n",
        "            nn.Conv2d(d_model, d_model, 3, 2, 3//2, bias=False)\n",
        "            )\n",
        "        # self.embed.requires_grad=False\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, 32*32, d_model)*.02)\n",
        "        # self.pos_emb = nn.Parameter(RoPE2D(dim=d_model, h=8, w=8, base=1000), requires_grad=False)\n",
        "\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=(32//patch_size)**2, base=10000), requires_grad=False)\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, d_model, n_heads) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x).flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c] # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        # x = self.pos_enc(x)\n",
        "        # print(\"TransformerModel\",x.shape, self.pos_emb.shape)\n",
        "        x = x + self.pos_emb[:,:x.shape[1]]\n",
        "        if context_indices != None: x = x[torch.arange(x.shape[0]).unsqueeze(-1), context_indices] # [batch, num_context_toks, d_model]\n",
        "\n",
        "        x = self.transformer(x)\n",
        "        out = self.norm(x)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "d_model = 64\n",
        "in_dim = 3\n",
        "patch_size = 2\n",
        "# model = ViT(patch_size, in_dim, d_model, n_heads=4, nlayers=4, drop=0.).to(device)\n",
        "model = ViT(patch_size, in_dim, d_model, n_heads=4, nlayers=1, drop=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x = torch.rand((5, in_dim, 32, 32), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "Fh9o__m2-j7K",
        "outputId": "a718eb68-28da-441e-fafb-8d787f6be1c5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title simplex\n",
        "# !pip install -q opensimplex\n",
        "import opensimplex\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def simplexmask2d(hw=(32,32), ctx_scale=(.85,1.), trg_scale=(.6,.8), B=64, chaos=[1,.5]):\n",
        "    ix, iy = np.split(np.linspace(0, chaos[0], num=sum(hw)), [hw[0]])\n",
        "    noise = opensimplex.noise3array(ix, iy, np.random.randint(1e10, size=B)) # [b,h,w]\n",
        "    noise = torch.from_numpy(noise).flatten(1)\n",
        "    trunc_normal = torch.fmod(torch.randn(2)*.3,1)/2 + .5\n",
        "    ctx_mask_scale = torch.rand(1) * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    trg_mask_scale = torch.rand(1) * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "    # ctx_mask_scale = trunc_normal[0] * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    # trg_mask_scale = trunc_normal[1] * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "    seq = hw[0]*hw[1]\n",
        "    ctx_len, trg_len = int(seq*ctx_mask_scale), int(seq*trg_mask_scale)\n",
        "    val, trg_index = torch.topk(noise, trg_len, dim=1, sorted=False)\n",
        "    ctx_len = ctx_len - trg_len\n",
        "\n",
        "    remove_mask = torch.ones((B,seq), dtype=bool) # [B, S]\n",
        "    remove_mask.scatter_(1, trg_index, False).flatten()\n",
        "    ind = torch.arange(seq).unsqueeze(0).repeat(B,1)[remove_mask].reshape(B, -1)\n",
        "\n",
        "    ix, iy = np.split(np.linspace(0, chaos[1], num=sum(hw)), [hw[0]])\n",
        "    noise = opensimplex.noise3array(ix, iy, np.random.randint(1e10, size=B)) # [b,h,w]\n",
        "    noise = torch.from_numpy(noise).flatten(1)[remove_mask].reshape(B, -1)\n",
        "    val, ctx_ind = torch.topk(noise, ctx_len, dim=1, sorted=False)\n",
        "    ctx_index = ind[torch.arange(B).unsqueeze(-1), ctx_ind]\n",
        "    return ctx_index, trg_index\n",
        "\n",
        "b=16\n",
        "# ctx_index, trg_index = simplexmask2d(hw=(32,32), ctx_scale=(.85,1), trg_scale=(.7,.8), B=b, chaos=[3,.5])\n",
        "ctx_index, trg_index = simplexmask2d(hw=(32,32), ctx_scale=(.85,1), trg_scale=(.7,.8), B=b, chaos=[3,1])\n",
        "# ctx_index, trg_index = simplexmask2d(hw=(32,32), ctx_scale=(.85,1), trg_scale=(.2,.8), B=b, chaos=[3,1])\n",
        "# ctx_index, trg_index = simplexmask2d(hw=(32,32), ctx_scale=(.85,1), trg_scale=(.5,.5), B=b, chaos=[.01,.5])\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "# imshow(mask)\n",
        "\n",
        "mask = torch.zeros(b ,32*32)\n",
        "# mask = torch.ones(b ,32*32)*.3\n",
        "mask[torch.arange(b).unsqueeze(-1), trg_index] = 1\n",
        "mask[torch.arange(b).unsqueeze(-1), ctx_index] = .5\n",
        "# print((mask==1).sum(1))\n",
        "# print((mask==.5).sum(1))\n",
        "mask = mask.reshape(b,1,32,32)\n",
        "import torchvision\n",
        "imshow(torchvision.utils.make_grid(mask, nrow=8))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "pQfM2fYvcTh6",
        "outputId": "6541bc4b-a556-46d1-f308-917b30374459"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAApsAAADOCAYAAABxTskJAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAMWpJREFUeJzt3XlwHNWdB/Bv91y6j9EtIdvyhU982wgHMFiLTSAFC7VLEqqWsFSosDYLmE1YpxK8UKmYsFUJlcQhG5IybNUCWWqXsGFjg5Gv2Ei2keVDSJYlWbJkS6PRNTPSjObst3+4NBtZ50jT03N8P1Wqsrt7+v1mXnfPb16/fk8SQggQEREREalA1joAIiIiIopfTDaJiIiISDVMNomIiIhINUw2iYiIiEg1TDaJiIiISDVMNomIiIhINUw2iYiIiEg1TDaJiIiISDVMNomIiIhINUw2iYiIiEg1qiWb+/btw7x585CUlIRNmzbh9OnTahVFRERERFFKlWTz97//PXbt2oU9e/bg7NmzWLVqFbZt2war1apGcUREREQUpSQhhAj3Tjdt2oQNGzbgl7/8JQBAURSUlpbi2WefxT//8z9P+lpFUdDZ2Yn09HRIkhTu0IiIiIholoQQGBwcRHFxMWR58rZLfbgL93q9qKmpwe7du4PLZFlGRUUFqqqqxmzv8Xjg8XiC/79+/TqWLVsW7rCIiIiIKMw6Ojpwyy23TLpN2G+j9/b2IhAIoKCgYNTygoICWCyWMdvv3bsXmZmZwT8mmkRERESxIT09fcptNH8afffu3bDb7cG/jo4OrUMiIpVIkqTZH9FM8bglmth0jtOw30bPzc2FTqdDd3f3qOXd3d0oLCwcs73JZILJZJp0n6WlpZgzZw5PvJtYLBZcuXIFiqKMWTdv3rwpm7UTUWdnJ1pbW3FzV2VJkjB//nwUFRVpFFn06ujowNWrV8csl2UZCxYsGHMXYyJFRUWYP3/+lH171BAIBNDS0jLmuqQGIQSuXr2Ka9eujVmn1+uxcOFC5Obmqh5HLBFCoLW1FZ2dnWPWGQwGLFq0CGazWYPIgKysLCxevHjK7ym1jJx/N1+zFEWZ8Jg2mUxYvHgxMjMzIxVmTPD7/WhubkZvb++YdcnJyVi8ePG0WukSic/nQ1NTE/r7+2e1n7Anm0ajEevWrUNlZSUefvhhADdOisrKSuzcuXNG+5w3bx7uvvtuJps3qa2txdWrV8ckm5IkYcGCBbjjjjs0iix6nT59GlevXkUgEBi1XJZlLFq0CBs2bNAosuh14sQJtLe3j/my0+l0WLp0KVatWjWt/axfvx733Xcf9PqwX3am5PV6ceDAAZw/f171soQQOHLkyITJ5ooVK7B06VLV44glgUAAPp9v3GTTaDRi1apVWLhwoQaRAWVlZXjooYeQkZGhSfknT57EkSNHxlznfT4fXC7XuMlmUlIS1qxZg7lz50YqzJjgdrsxODg4YbK5fv16FBcXaxBZ9HI6nejv74++ZBMAdu3ahSeeeALr16/Hxo0b8cYbb8DpdOLJJ5+c0f4kSYIsy5Mmm9nZ2cjNzdUkIQ0EAuju7sbQ0FBEy53svY58ZiNMJhOKiopgNBojEdoYdrsdVqt1TMISaaF8ZnQjcQrXZybLMnQ6nSbJZiAQgE6ni0j9KorC4yxE4TzOwk3L41YIMeFxO9V3Io+zsSb7PEa6LfAzGy1cn4cqZ89jjz2Gnp4evPzyy7BYLFi9ejUOHjw47dttM7FgwQLcddddmhwobrcbn376KS5fvhzxsqcrMzMTW7Zs0ez23cWLF1FZWQm/369J+URERKQN1X6q7dy5c8a3zWfCaDQiNTUVOp0uYmWOkGVZk1+9odDpdEhJSUFaWpom5ZtMJnaDIM1IkoTU1NSw9vvzeDxwuVyat9YTEUW76M6QiIjCQK/XY926dViyZEnY9tnU1ISqqir4fL6w7ZOIKB4x2QwDSZKg0+lgMBjCvm8hBAKBAFtPYkCk+gSOh8fJ5GRZhtlsDmvLZm9vL1vriYimgclmGBgMBqxevRrz5s0L+757e3tx7tw5DA8Ph33fFD4jT8yON7xXJPT396O2thYul0uT8omIiCbCZDMM9Ho9FixYoMq+W1tbUV9fz2Qzyun1esyfP1+zIW3a29tRX1/PZJOIiKIOk02KiLy8PKxbt27cAehnYnh4GC0tLUyuiGZBkiSUlJSgqKhIky4BXq8XV65cwcDAQMTLJqLIYbJJETHyhRYuPT096OnpYbJJNAuSJGHx4sUoLy/XJNm02+2w2WxMNoniHJNNighZlsP68ExSUhKKi4vHnUJueHgYfX19Y2YJIopmBoMBubm5EZ14QZIkZGVlQa/Xa5JsalUuEUUWk02KSRkZGbjnnnvGTShbW1tx6NAhtnpSTElPT8eWLVtUnfxiPElJSREtj4gSD5NNikk6nQ7p6enjrktNTeWUYxRzZFlGeno6srKytA6FiCis+I1MRERERKphsklEREREqmGySURERESqYbJJRERERKrhA0JEREREE0hLS0NpaSkMBoMm5VutVnR3d0MIoUn54cBkk4iIiGgCeXl52Lp164QjoKhJCIGqqip0d3dHvOxwYrJJRERENAFJkmA0GsedRERtQgjodDpIkhTTLZvss0lEREREqmGySURERESqYbJJRERERKphn02iOCJJ0oxfG4n+QEKICcuZTeyxZrz3mkjvXysz+YzDVS8zOb8mO1+IYgmTTaI4kJGRgY0bN2J4eHhGr+/p6UFTUxP8fn+YI/t/XV1dOHny5Jh562VZRllZGUpKSuI+4dLr9Vi6dCnuvPPOMetSU1ORlpamQVSJQafTYdGiRcjPzw/pddnZ2WF5MGRoaAiNjY1wOp0hva69vZ0JJ8U8JptEcSAzMxObNm2a8eu//PJLtLa2qppsdnZ2orOzc0xCKcsy9Ho9SkpKVCs7WhgMBixfvhxbtmwZd328J9taGkn0V65cGfJrw1EvQ0NDOH36NHp7e0N6HVs3KR7ETbLZ39+Py5cvj2k1mS2dTofCwkK2OFBUkyRpVl+IkUhyRr4wb/7iTLQES5KksF+naHq0/OxHkkZFUTQpn0hLcZNstrS0oKOjI+z7TUlJwfbt25lsEhEREc1A3CSbPp8PPp9PlX0HAgFV9ktEN1p8nE4n+vr6tA4lJENDQ7y9SQkrEAhgcHAwpr4fPR4PvF6v1mEkpLhJNokoNgkhcPHiRbS1tWkdSkicTqeqfVyJopnD4cDRo0dj6keioiiw2Wxah5GQmGwSkaaEELDb7bDb7VqHQhQXhBDw+/2q3e0DAJfLBYvFAqvVqloZFD+YbBIREcWR4eFhnDp1CvX19aqWMTg4qNr+Kb4w2SQiIvY/jSM+nw/Nzc1ah0EUxGSTiIjQ39+P1tbWiD7w4Xa74XA4IlYeaUOv1+Puu+/G0qVLNSm/r68Pn376aUz1L403TDaJiAhdXV04cuQI3G53RMvluJPxz2Qy4bHHHsPf/d3faVL+xYsXUVdXx2RTQ0w2KSIGBwfR19cXkVt13d3dMTUcB82M0WhEXl4eDAaDJuUPDQ1F7JgOt0AggN7eXrhcruAyq9UKn8/Hc0clJpMJJSUlSE1NDS6z2WxR/XS0wWDArbfeitzc3FntJzk5GbfccktYpv2cCaPROO3JIyRJQk5OzqixtQsLC6HT6dQKLyEw2aSIuHr1Ko4eParq05Ej/H5/xFtnKPIyMjJwzz33zPqLcKbq6+tx5MiRiBzT4ebxeFBVVYXW1tbgMjXHKqYbU8pu3bo1mMwLIVBdXY3Tp09H7Q+WtLQ0PPvss9i2bdus9iNJEsxmc5iiUpder8fatWuxbNmyUcuSk5M1jCr2MdmkiPB6vbDb7fwyo7DR6XRIT09HVlaWJuUnJyfH7FSbiqLA6XRyuKkI0uv1SE9PD/5fURQkJSVpGNHUZFlGXl4e5s6dq3UoESNJEpKTkzW7rsSrkCaJ3bt3LzZs2ID09HTk5+fj4YcfRmNj46ht3G43duzYEWyGfvTRR9Hd3R3WoImIiIgoNoSUbB47dgw7duxAdXU1Dh06BJ/Ph/vuuw9OpzO4zQsvvIA//vGP+OCDD3Ds2DF0dnbikUceCXvgREREFBpJkqDX62EwGKb1J8shpQkxSwgBRVEQCASirluDJEmQZRk6nW7af7IsQwiBQCAw679wCOk2+sGDB0f9/+2330Z+fj5qampw1113wW6343e/+x3effdd3HvvvQCA/fv3Y+nSpaiursbtt98elqCJiIgodHPnzsU3v/lN5OTkTLltcnIyli9fHoGotOdyuXD69Gm0trZizZo1WLt2rdYhBZWVlUGvD63XY19fH9577z1cuXJlVmX7fD709PTMah/ALPtsjvT3Gen4W1NTA5/Ph4qKiuA2S5YswZw5c1BVVTVusunxeODxeIL/55hrRERE6igpKcHf//3fY/78+VqHElXcbjfq6uqQmpoaluQqXCRJQklJCUpKSkJ6XWtrK9rb21FdXa1SZKGZcfu4oih4/vnnsXnzZqxYsQIAYLFYYDQax3SsLSgogMViGXc/e/fuRWZmZvCvtLR0piERERHRNEiSNO0/0lYodRWtdTbjZHPHjh2oq6vD+++/P6sAdu/eDbvdHvzr6OiY1f6IiIiIKHrM6Db6zp078fHHH+P48eO45ZZbgssLCwvh9Xphs9lGtW52d3ejsLBw3H2ZTCbNBnolIqL4ptfrkZ2djZSUFI6VSKSRkFo2hRDYuXMnPvzwQxw+fBhlZWWj1q9btw4GgwGVlZXBZY2NjWhvb0d5eXl4IiYiIpqmjIwM3Hvvvfja177GblpEGgmpZXPHjh1499138dFHHyE9PT3YDzMzMxPJycnIzMzEU089hV27dsFsNiMjIwPPPvssysvL+SQ6xTUhBLxe76jp/6ZDp9OFNJUaEYVGp9MhKytLs5mmiCjEZPPNN98EAGzZsmXU8v379+Nb3/oWAOBnP/sZZFnGo48+Co/Hg23btuFXv/pVWIIlilZerxdnzpzBpUuXQnpdcXExNmzYEPUziRAREc1USMnmdAY6TUpKwr59+7Bv374ZB0XhJ4QIDlqrVfnxLBAI4Nq1azN63Zo1a1SIaGojdRLvdRNttDwP/zIGIqJI4dzoCWJoaAg1NTVITU3VpPzOzs6wzURA4dPW1ob29nZYrVb4/X6tw4l7fr8fDQ0N+POf/6xpHF6vF/39/ZrGQESJg8lmghgaGsKZM2c0jYGtKdFFCIG2tjb8+c9/DrZ8k7p8Ph/q6+u1DgMAz0ciihwmm1MYuT2qVauc1WqFz+cLy7745UI3G0kyeWxEDj9vIko0TDan4PF4cOrUKeh0Ok3KDwQC8Hq9mpRNRERENFtMNqcghBg1dzsRERERTd+Mp6skIiIiIpoKWzaJiIiinKIoo0aM8Pl8HOGDYgaTTSIioijX09OD2traYLcuRVFmNLYvkRaYbBIREUU5h8OBuro6OJ1OrUOJChzRIbbEbbLp8XjQ3NyMwcFBrUNRjZZDMsUzq9WKtra2iMzyIoQI29BWM3Ht2jVUV1dPeuHu6OjghT2GeL1etLS0wG63ax3KtCmKgq6urnHX+Xw+XLp0CX19fTPa95w5c7Bt27bZhKea6Zx/I3p6ejS9VkSbhoYGHDlyZFqfSWdnJ3p7eyMQFU0kbpNNl8uFL774Ah0dHVqHohpFUZgEqOD69es4fPhwRGbUMZlMmg1tJYRAS0sLPvvss0mPI62nVqTQuN1unD17Fq2trVqHEpKJjjOv14tz585BkqQZ7Xfp0qVR2egwcv5VVlZO6xyLhmlOo8nZs2exZ8+eadWtEIIzpGksJpJNm82G1tbWkC42g4ODcDqdbPmLAIfDgf7+/phIfPv6+saNUwiBvr4+tLa2oqenB36/PyLHjtbHp6IoCAQCEa07t9sd/Ixnw2az4cSJE8jKygpPYCFqaWlBS0vLmPchhIDNZhv3NYqiwGq1Ijk5WbW4XC4XhoaGND+2wmk2SVakj++bWSwWNDY2jnucXLlyBX6/PyaundFGURR4PJ6w/1gPBAJoaGhAZWVlcJnZbMayZctgMpnCWlYiiYlks7GxEW1tbSG9RlEUDA8PqxMQjXLlyhUcP348Jr7cvF7vuF9ciqLgwoULuHTpErxeb0y8l1jV19eHQ4cOzfo2r06nwwcffABZ1mYEN5/PB7fbPW6i4Ha7x32N1+vF6dOnUVtbq1pcvPZFl88//xw/+MEPxm2BczgcTDSjjMfjwW9/+1u89957wWV33nknfvrTn6KwsFDDyGJbTCSbHo+HA6uHyO12R6zPlt1uh8PhiPkEze12T5gk0PhcLlfIx5ndbg8eM7M1MDAw631E2vDwMJPBBOJyuXDt2rWovJVPYwkhMDAwMOra0tPTE/Pfb1qLiWSTQiOEwJdffolr167NuJ9TKGXFQ6JJofP7/aitrUVzc3NIr3O73XC5XCpFRURE0YbJZpy6+ZcZRSdFUeD1ejVpuQ8EArP6kSCEQG9vL5/yJIohiqLA5/PF/N3CRHngRwgxo2u1z+eLqi4aTDaJNNTQ0IBXX30VKSkpES9bCDHtYVeIKD60tbXhtddeQ2ZmptahzEpDQ0PMJ8zTdfjwYRw4cCCkB+UcDkdUjUjBZJNIQ21tbfjtb3+rdRhElCC6urrwzjvvaB0GTZMQAmfOnMEvfvGLmO6ups1jnERERESUEJhsEhEREZFqeBudiIjiltvtRn19vWa3IK9cuRLTtz/pRv/Hc+fOobOzM+JlCyFw7dq1mO9bL4koewcOhyPmOy4TEVF0MBqNKCoqQlJSkiblOxwOdHd3c6rJGJaSkoKioiLo9dq0z/X19UX1qB92ux0ZGRmTbsNkk4iIiIhmZDrJJvtsEhEREZFqmGwSERERkWqYbBIRERGRaphsEhEREZFqYmLoo1tvvRVLly6FJEkRLzsQCKCzsxNDQ0MRL3sqNpsN3d3d4w6JkJeXh5ycHA2iim4DAwOwWq1jPjNJkpCfn4/s7GyNIhvLbDYjPz9fk+NeURR0dXXB4XCgr68PPT09Y7aRJAmFhYV8oG8cPT096OvrG7NclmUUFRUhPT1dg6iilxACVqsVAwMDY9bpdDoUFRUhLS1Ng8iilxACFosFdrt9zDq9Xo/i4mJNpsEFgLS0NBQXF0On02lS/sj5d/N1fuS6Njg4OOY1RqMRxcXFqoxakJWVhYKCAshy5Nv3hBDo7u4e99yajkAggK6urlnnQFGfbEqShAcffBDf+973NDlwnU4nDhw4gCtXrkS87KmcP38elZWV8Pv9o5ZLkoRly5Zh48aNmiQq0UoIgZqaGvT09Iy5CMmyjJUrV2Lt2rUaRTfWunXrsGXLFk2Oe7fbjU8//RQNDQ2orq5Gb2/vmM9Mr9dj9erVWLlyZcTji2aKouDEiRPjJptGoxHr1q3DkiVLNIgsegUCARw9ehQ1NTVj1plMJmzatAkLFizQILLo5fV6UVlZiYsXL45Zl5KSgjvuuANz5szRIDJgwYIF2L59O1JTUyNetqIoOHnyJKqqqsasG7muXbp0acy61NRUfOUrX0FxcXHYY1q+fDn+6q/+CkajMez7norf70dlZSXOnz8/o9c7nU4cPHgw/pNNAEhOTkZOTo4mX7omkwlpaWlITk6OeNlTMRqNEyaTBoNBs1+10Wyqzyya6jkjI0Oz497tdiM9PR3JyckwGAwTbmc0GqPqM4sGiqJMOh6fyWTiZ3aTQCAw4WcmSRI/s3Ho9fqo/czS0tJgNps1aY1WFAUZGRlISUkZ9w7WRNdTWZaRlJSkymeWkZEBs9kMk8kU9n1PxefzBa/lM6EoSlhaZNlnk4iIiIhUExMtm1qSJAkGgyFizd+BQIBTm1Fck2VZs5k4gBu/1G/uekKJTZIk6PV6zbod8ZikeMdkcwpGoxHr16/HrbfeqnpZgUAAFy5cQFtbm+plEWklOzsb69at06Q/FwA0NTWhvr6e0wdSUEpKCtavXw+z2axJ+devX8e5c+fg9Xo1KZ9IbUw2p2AwGDB//vyIlOXz+XD9+nUmmxTXUlNTsXTpUk2e/BdCwOl0oqGhIeJlU/QymUxYuHAhSktLNSnfYDCM+6APUbxgspkgUlNTsXDhwoR8aCgQCKCtrQ1Wq1XrUIiIiBLOrJLN1157Dbt378Zzzz2HN954A8CNp1hffPFFvP/++/B4PNi2bRt+9atfoaCgIBzx0gylp6ejvLwceXl5WocScV6vF5988gmTTSIiIg3MONk8c+YM/u3f/g233XbbqOUvvPAC/vd//xcffPABMjMzsXPnTjzyyCM4efLkrIOlmZMkCbIsazbIrpZ0Oh3HGyWiqJWamop58+bB4/FM+zUej4eTA1DMmFGyOTQ0hMcffxxvvfUWfvSjHwWX2+12/O53v8O7776Le++9FwCwf/9+LF26FNXV1bj99tvDEzUREVGcKC4uxle/+tVxZ4ObiNPpxPHjx1WMiih8ZpRs7tixAw888AAqKipGJZs1NTXw+XyoqKgILluyZAnmzJmDqqqqcZNNj8cz6tecw+GYSUhEREQxyWAwTDp5wnh0Op0mM9JQbJEkCSkpKVNOK+x2u0NqWQ9VyMnm+++/j7Nnz+LMmTNj1lksFhiNRmRlZY1aXlBQAIvFMu7+9u7di1deeSXUMIiIiIhoErIsY/Xq1ZNO96ooCr744gvU19erFkdIyWZHRweee+45HDp0KGyT1e/evRu7du0K/t/hcGg2/AQRUaKabCq/SJVPROElyzJycnKQk5Mz4TaBQACXL19WNY6Qks2amhpYrVasXbs2uCwQCOD48eP45S9/iU8++QRerxc2m21U62Z3dzcKCwvH3afJZNJkvlAiIvp/RUVFWL58uSazO/l8PjQ0NKC6ujriZROR+kK6qmzdunXMwLNPPvkklixZgpdeegmlpaUwGAyorKzEo48+CgBobGxEe3s7ysvLwxc1ERGFldlsxrp16zT58e/1eidskCCi2BdSspmeno4VK1aMWpaamoqcnJzg8qeeegq7du2C2WxGRkYGnn32WZSXl/NJdCKiGMDb2UQUbmG/X/Kzn/0Msizj0UcfHTWoOxERERElnlknm0ePHh31/6SkJOzbtw/79u2b7a6JKMaYTCaYzeZJHzTJy8vTpF8gERFpg1d8Igqb/Px8VFRUIC0tbcJt9Ho9UlJSIhgVERFpickmEYWNwWBAdnY2MjIytA4l5siyDJPJBFmWNSnfZDKxvyYRqYLJJhFRFEhLS0N5eTnMZrMm5aenp7N7AxGpglcWIqIoYDQaMXfuXBQXF2sdChElGEmSIMsyhBAQQoR9/0w2iYiIiBKUJElYuHAhkpOT0dXVhcuXLyMQCIS1DG06BxERERGR5iRJwvz583HnnXdi8eLFqkxbGxMtmz09Paivrw+p47zRaERJSQmfeqWQZWdno7CwUJOHJRRFgcVigc1mQ19fX8jHfbj4fD7Y7faIl0tEsW9wcBCXL1/WZDYqIQR6e3sjXu5kbDYbLl26NGmf6IyMDBQXF6uS6E0lEt91UZ9sCiHQ1NSEjz/+OKQv3aysLDzwwANMNilkc+bMwdatW2EwGCJettfrxWeffQabzYbW1lZ0dHRokvQKIeDz+SJeLhHFPqvVik8//VSz0Q18Pp8q/Q5nqqOjAxaLZdLPY8mSJcjLy9Mk2YyEqE82AcDv98Ptdod04CYlJUFRFBWjonil1+uRnJysSbKp0+mCFxu/3w+/3x/xGIgo/smyjPT0dE1aH4EbP6wdDkdCfE8HAoEp+0B6vd4IRaONmEg2iYiIKHySkpKwefNmlJaWalL+9evXceTIETidTk3Kp8hisqmSQCAAv98fUlO+3+8P+xNgRIlCCBE87yYT6nlJ8W+k24jb7Q7pdTqdDnq9PiYHw9fpdDCbzSgqKtKk/OHh4bi9ZUxjMdlUSXd3N86ePQuPxzPt1wgh0NnZqWJURPGtubkZ9fX1kyaTvb29TDZpFKfTiZMnT4bcx3/evHlYtWoVB8MnmgLPEJU4HA7U19fD5XJpHUpU4Jc7RYLVakVdXV1C9AOj8PF6vWhpaQn5dQaDAbfddpsKERHFFyabKjGbzdi4caOqnX4dDodm8yiHymq14sqVK5okAYFAAFarNeLlEpH2UlJSsGjRIqSmpoZ93yUlJTFzDSbSEpNNleTl5eHOO+9UtYyrV6/GTJ+Xzs5OHD16VLPhdNiySpSYUlNTsXHjRhQWFoZ935IkxWR/TaJIY7KpEkmSVE8EY+kiJ4SAoii8vUmqys7Oxvz588P+48Lv98NqtWJ4eDis+6XIkGU5Zn6YT8Xn86G7uxsDAwNwOBxah0MTkCQJZrMZWVlZ09q+oKAgrlvJmWwSUdxYvHgx5s2bF/Zkc2hoCJ988gmuXr0a1v0ShWpoaAjHjx/H1atX0d7ernU4NAFJkrBy5UqsXbt2WtsbDAZNxnaOFCabRFEmNTUV2dnZYdufx+PB8PBw3HclkCQJJpNJtUGq46VljGKboihwuVwYGhqK6Vm+9Ho9MjMzVT2vvF4vXC6XJte+ketRenp6TN2FVAuTTaIootfrsWbNGixevDhs+2xubkZ1dXVMfzERUXzJy8vDtm3bVB1burW1FZ9//nncz84TC5hsEkURWZaRm5uL3NzcsO3TZrPxlzVFlZv7b/v9fvbnTjDJycm45ZZbVC3D6XTCaDQGjy29Xh/X/SKjGZNNIiKKqIGBAZw/fz74wJXP52P/Qwq7wsJC3HvvvcHW08HBQZw+fRoXL17UOLLEw2STiIgiyuFw4Pz587Db7QBujIXb3d2tcVQUb8xm86j+73a7HWazWcOIEheTzQThcrlQX1+Pjo6OCbdJSkpCWVlZyFO2TUdOTg5Wr1496a2yvr4+dHR0cH54IiIKC3Yhig5MNhOEw+HAiRMnJj3xcnNzkZOTo0qyWVpaiuLi4km3uXjxIrq6uphsEhERxREmmwlCCAG/3z/pNn6/X7UhImRZnrJjtl7PwzHWeTwedHZ2Bm+PhlNmZiaHESEiikH8dieisLFarTh48GDYn/iUJAm333471q9fH9b9EhGR+phsElHY+Hw+2Gy2sO9XluW4nypSCBEcrHsmdDodTCYTh3YhoqjDZJOIKAoMDg7i+PHjM54BqaCgAOXl5UhNTQ1zZEREsxPXyebNAwerQZIk9iEjolnzer2zGmvS5/NxligiAnDjTslMnsFQ67mNuE02h4eHUVtbi5aWFtXKSEtLw/Lly5GWlqZaGUREREShcLvdqK+vD7lbU3d3tyojwsRtsul2u3H+/HlVWx0LCgowb948JptEREQUNdxuNy5cuBDy3RK2bM6QWh8cxa/+/n7U19dDp9ONWi5JEgoKCpCTk8OuExR1nE4nLl++PK1xcvV6PUpKSpCenh6ByIhICzO9la6GuE82iULV3t6Orq6uMctlWcbdd9+NnJwcDaIimlxfXx8qKyun9UMoNTUVX/3qV5lsElFEMNkkukkgEBi3z4osy1MOjE+kFUVR4PF4prWtTqfjTF0JLhAIYGBgQNU56XU6HTIzM2EwGFQrg2JDyMnm9evX8dJLL+HAgQNwuVxYuHAh9u/fHxxsWQiBPXv24K233oLNZsPmzZvx5ptvYtGiRWEPnoiIiELndrvx+eef44svvlCtjKysLNx7770oKChQrQyKDSElmwMDA9i8eTPuueceHDhwAHl5eWhqakJ2dnZwm9dffx0///nP8c4776CsrAw//OEPsW3bNtTX1yMpKSnsb4CIiIhCoygKBgYGVC3D7/dzOC4CEGKy+ZOf/ASlpaXYv39/cFlZWVnw30IIvPHGG/jBD36Ahx56CADw7//+7ygoKMAf/vAHfP3rXw9T2EREREQUC0Ka1+x//ud/sH79evzN3/wN8vPzsWbNGrz11lvB9a2trbBYLKioqAguy8zMxKZNm1BVVTXuPj0eDxwOx6i/WDPyxFek/4iIaHq0uk7zuk8UYsvmlStX8Oabb2LXrl34/ve/jzNnzuAf//EfYTQa8cQTT8BisQDAmP4ZBQUFwXU327t3L1555ZUZhq8tp9OJ2tpazcbZbGpqYid/IqJJuFwunDt3Lm6evHe5XBgcHNQ6DKKQhJRsKoqC9evX48c//jEAYM2aNairq8Ovf/1rPPHEEzMKYPfu3di1a1fw/w6HA6WlpTPaV6QNDg7i9OnTmpXf2Nio+nScRESxzOl04syZM1qHEVa87lOsCSnZLCoqwrJly0YtW7p0Kf7rv/4LAFBYWAjgxnRHRUVFwW26u7uxevXqcfdpMplgMplCCSOqaHnS84JD0U4IgZ6enll3j5EkCcnJyRgaGtJkQH2Xy4ULFy5MeIfmL0XiwQsKDa+VFGlCCPT29qK5uVmT8oeGhuB2uzUpezwhJZubN29GY2PjqGWXL1/G3LlzAdx4WKiwsBCVlZXB5NLhcODUqVN45plnwhMxEcWMQCCACxcu4Pz587Pe15/+9CcYjcYwRBW6kTEsp9NtRQgBr9cbgaiIKFopioK6uroxOVOkCCFiN9l84YUXcMcdd+DHP/4x/vZv/xanT5/Gb37zG/zmN78BcKP14fnnn8ePfvQjLFq0KDj0UXFxMR5++GE14ieKqOHh4ZhrtXI6narue7LPw+/3w+FwYGhoSLUYiCg6BQIBOBwO9Pf3B5clJSUhOTk5bqb89Xg8GB4ejtuHt1wuV1iGrwop2dywYQM+/PBD7N69G6+++irKysrwxhtv4PHHHw9u873vfQ9OpxNPP/00bDYbvvKVr+DgwYMcY5NinhACdXV1aG9v1zqUkDidTlVmPvL7/Th79uykv9yFELDZbGEvm4ii39DQEI4dOzaqq9zy5cuxfv166HQ6DSMLn9bWVpw6dSpuH9ZVFAV9fX2z3k/IMwg9+OCDePDBBydcL0kSXn31Vbz66quzCowo2owkTlMlT4qixNWFZ6L3IoRAf3//qFYLii6BQGDc/oo+nw8ejwcul0uDqG7MXuPz+YItJhPFGU+EEAgEAmFrAfP5fFH/mfn9/jHTYRYWFk4Zu6IoUBQl7K2Fw8PDk17PZjIIvd1ux7Vr1ziV8RQ4NzpRGAkh0NzcjKamJq1DCQshBCwWS9zeIopniqLg0qVLaGtrG7NOr9fj4sWLyMrKinhcwI2+/G1tbcG+rYqi4Nq1a5rEEikOhwPnzp0LW5eSQCCA69evh2VfkXT16lUcOnRo0tvo7e3t+PLLL8OeTHs8HtTW1o67zul04tSpU0hNTQ1pn1arNa4aF9TCZJMozK5fv67qfMNE0yGEQEdHx4THYnV1dYQjSmwulwt1dXXo7e3VOhRNdXd3j2ntvFlNTQ0OHjwY0dZCj8eDhoaGiJWXaJhsUtDw8DDq6+s1a2G4fv26Zr8QhRBob2+HLIc0qda4++nq6gpTVBRvAoEAWlpaInL7WlGUKb/UY4HP50NTUxPsdrvWocyK3W6P2NPBXq8XjY2NMZvYtre3R30XAQoNk00Kcjqd+PzzzzV7SnCkT5MWFEVBQ0NDWIap4EWSJuLz+XDu3DlcuHAhIuXFw+09r9eLs2fPxvzTy5G8vg0PD+PMmTMx+5mN9Nmk+MFkM0F4vV709fWFZQiDWDYwMDBh/0Ne4CgSeJyFLh6S5kjjZ0bRhMlmgrDb7aisrEz4J4fdbje/6ImIiCIoJpJNr9eLwcHBmL0loBaPxzNhK53H48Hg4GDw/w6HAzabLeYGJCciIqLYFhPJZix3dFaTw+EY91aJEAL19fWjHlS5OfkkIiIiigRJRNkAeg6HA5mZmVqHQURERERTsNvtyMjImHSb2Y3zQkREREQ0CSabRERERKQaJptEREREpBomm0RERESkmqhLNqPseSUiIiIimsB08raoSzY5PA8RERFRbJhO3hZ1Qx8pioLGxkYsW7YMHR0dUz5OT9HL4XCgtLSU9RjDWIexj3UY+1iHsS8e61AIgcHBQRQXF0OWJ2+7jLpB3WVZRklJCQAgIyMjbiolkbEeYx/rMPaxDmMf6zD2xVsdTndc9Ki7jU5ERERE8YPJJhERERGpJiqTTZPJhD179sBkMmkdCs0C6zH2sQ5jH+sw9rEOY1+i12HUPSBERERERPEjKls2iYiIiCg+MNkkIiIiItUw2SQiIiIi1TDZJCIiIiLVRGWyuW/fPsybNw9JSUnYtGkTTp8+rXVINIF/+Zd/gSRJo/6WLFkSXO92u7Fjxw7k5OQgLS0Njz76KLq7uzWMmI4fP46vfe1rKC4uhiRJ+MMf/jBqvRACL7/8MoqKipCcnIyKigo0NTWN2qa/vx+PP/44MjIykJWVhaeeegpDQ0MRfBeJbao6/Na3vjXmvNy+ffuobViH2tq7dy82bNiA9PR05Ofn4+GHH0ZjY+OobaZz/Wxvb8cDDzyAlJQU5Ofn47vf/S78fn8k30rCmk4dbtmyZcy5+J3vfGfUNolQh1GXbP7+97/Hrl27sGfPHpw9exarVq3Ctm3bYLVatQ6NJrB8+XJ0dXUF/06cOBFc98ILL+CPf/wjPvjgAxw7dgydnZ145JFHNIyWnE4nVq1ahX379o27/vXXX8fPf/5z/PrXv8apU6eQmpqKbdu2we12B7d5/PHH8eWXX+LQoUP4+OOPcfz4cTz99NORegsJb6o6BIDt27ePOi/fe++9UetZh9o6duwYduzYgerqahw6dAg+nw/33XcfnE5ncJuprp+BQAAPPPAAvF4vPv/8c7zzzjt4++238fLLL2vxlhLOdOoQAL797W+POhdff/314LqEqUMRZTZu3Ch27NgR/H8gEBDFxcVi7969GkZFE9mzZ49YtWrVuOtsNpswGAzigw8+CC5raGgQAERVVVWEIqTJABAffvhh8P+KoojCwkLxr//6r8FlNptNmEwm8d577wkhhKivrxcAxJkzZ4LbHDhwQEiSJK5fvx6x2OmGm+tQCCGeeOIJ8dBDD034GtZh9LFarQKAOHbsmBBietfPP/3pT0KWZWGxWILbvPnmmyIjI0N4PJ7IvgEaU4dCCHH33XeL5557bsLXJEodRlXLptfrRU1NDSoqKoLLZFlGRUUFqqqqNIyMJtPU1ITi4mLMnz8fjz/+ONrb2wEANTU18Pl8o+pzyZIlmDNnDuszSrW2tsJisYyqs8zMTGzatClYZ1VVVcjKysL69euD21RUVECWZZw6dSriMdP4jh49ivz8fNx666145pln0NfXF1zHOow+drsdAGA2mwFM7/pZVVWFlStXoqCgILjNtm3b4HA48OWXX0YwegLG1uGI//iP/0Bubi5WrFiB3bt3w+VyBdclSh3qtQ7gL/X29iIQCIz60AGgoKAAly5d0igqmsymTZvw9ttv49Zbb0VXVxdeeeUV3Hnnnairq4PFYoHRaERWVtao1xQUFMBisWgTME1qpF7GOwdH1lksFuTn549ar9frYTabWa9RYvv27XjkkUdQVlaGlpYWfP/738f999+Pqqoq6HQ61mGUURQFzz//PDZv3owVK1YAwLSunxaLZdxzdWQdRc54dQgA3/zmNzF37lwUFxfjwoULeOmll9DY2Ij//u//BpA4dRhVySbFnvvvvz/479tuuw2bNm3C3Llz8Z//+Z9ITk7WMDKixPX1r389+O+VK1fitttuw4IFC3D06FFs3bpVw8hoPDt27EBdXd2o/u4UWyaqw7/sB71y5UoUFRVh69ataGlpwYIFCyIdpmai6jZ6bm4udDrdmKfturu7UVhYqFFUFIqsrCwsXrwYzc3NKCwshNfrhc1mG7UN6zN6jdTLZOdgYWHhmAf2/H4/+vv7Wa9Rav78+cjNzUVzczMA1mE02blzJz7++GMcOXIEt9xyS3D5dK6fhYWF456rI+soMiaqw/Fs2rQJAEadi4lQh1GVbBqNRqxbtw6VlZXBZYqioLKyEuXl5RpGRtM1NDSElpYWFBUVYd26dTAYDKPqs7GxEe3t7azPKFVWVobCwsJRdeZwOHDq1KlgnZWXl8Nms6Gmpia4zeHDh6EoSvBCStHl2rVr6OvrQ1FREQDWYTQQQmDnzp348MMPcfjwYZSVlY1aP53rZ3l5OS5evDjqh8OhQ4eQkZGBZcuWReaNJLCp6nA8586dA4BR52JC1KHWTyjd7P333xcmk0m8/fbbor6+Xjz99NMiKytr1JNaFD1efPFFcfToUdHa2ipOnjwpKioqRG5urrBarUIIIb7zne+IOXPmiMOHD4svvvhClJeXi/Lyco2jTmyDg4OitrZW1NbWCgDipz/9qaitrRVXr14VQgjx2muviaysLPHRRx+JCxcuiIceekiUlZWJ4eHh4D62b98u1qxZI06dOiVOnDghFi1aJL7xjW9o9ZYSzmR1ODg4KP7pn/5JVFVVidbWVvHZZ5+JtWvXikWLFgm32x3cB+tQW88884zIzMwUR48eFV1dXcE/l8sV3Gaq66ff7xcrVqwQ9913nzh37pw4ePCgyMvLE7t379biLSWcqeqwublZvPrqq+KLL74Qra2t4qOPPhLz588Xd911V3AfiVKHUZdsCiHEL37xCzFnzhxhNBrFxo0bRXV1tdYh0QQee+wxUVRUJIxGoygpKRGPPfaYaG5uDq4fHh4W//AP/yCys7NFSkqK+Ou//mvR1dWlYcR05MgRAWDM3xNPPCGEuDH80Q9/+ENRUFAgTCaT2Lp1q2hsbBy1j76+PvGNb3xDpKWliYyMDPHkk0+KwcFBDd5NYpqsDl0ul7jvvvtEXl6eMBgMYu7cueLb3/72mB/srENtjVd/AMT+/fuD20zn+tnW1ibuv/9+kZycLHJzc8WLL74ofD5fhN9NYpqqDtvb28Vdd90lzGazMJlMYuHCheK73/2usNvto/aTCHUoCSFE5NpRiYiIiCiRRFWfTSIiIiKKL0w2iYiIiEg1TDaJiIiISDVMNomIiIhINUw2iYiIiEg1TDaJiIiISDVMNomIiIhINUw2iYiIiEg1TDaJiIiISDVMNomIiIhINUw2iYiIiEg1TDaJiIiISDX/B+Mp8pPKQQyPAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title masks\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def multiblock(seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "    return target_mask\n",
        "\n",
        "\n",
        "def multiblock2d(hw=(8,8), scale=(.15,.2), aspect_ratio=(.75,1.5), M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_aspect = torch.rand(1) * (aspect_ratio[1] - aspect_ratio[0]) + aspect_ratio[0] # in (min_s, max_s) # all blocks same size\n",
        "    mask_scale = torch.rand(1) * (scale[1] - scale[0]) + scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    h = (mask_scale/mask_aspect)**.5# h*(h*aspect) = scale\n",
        "    w = h * mask_aspect\n",
        "    h_pos, w_pos = torch.rand(M)*(1-w), torch.rand(M)*(1-h) # in (0, 1 - mask_len)\n",
        "    h_len, h_pos = (h*hw[0]).int(), h_pos*hw[0]\n",
        "    w_len, w_pos = (w*hw[1]).int(), w_pos*hw[1]\n",
        "    h_ind, w_ind = torch.arange(hw[0]).unsqueeze(0), torch.arange(hw[1]).unsqueeze(0) # [1, seq]\n",
        "    h_mask = (h_ind>=h_pos.unsqueeze(-1)) & (h_ind<(h_pos+h_len).unsqueeze(-1)) # [M, seq]\n",
        "    w_mask = (w_ind>=w_pos.unsqueeze(-1)) & (w_ind<(w_pos+w_len).unsqueeze(-1)) # [M, seq]\n",
        "    target_mask = h_mask.unsqueeze(-1) & w_mask.unsqueeze(-2) # [M, seq, seq]\n",
        "    return target_mask\n",
        "\n",
        "# https://arxiv.org/pdf/2210.07224\n",
        "def randpatch(seq, mask_size=8, gamma=0.9): # num patches of seq, mask patch size, masking ratio\n",
        "    # mask = torch.rand(seq//mask_size)<gamma\n",
        "    length = seq//mask_size\n",
        "    g = torch.normal(gamma, std=.1, size=(1,)).clamp(.5,.9)\n",
        "    # g = gamma\n",
        "    idx = torch.randperm(length)[:int(length*g)]\n",
        "    mask = torch.zeros(length, dtype=bool)\n",
        "    mask[idx] = True\n",
        "    mask = mask.repeat_interleave(mask_size, dim=-1)\n",
        "    return mask # [seq] , True -> mask\n",
        "\n",
        "\n",
        "import torch\n",
        "def apply_masks(x, mask): # [b,t,d], [mask_size] # https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "    mask_keep = mask.unsqueeze(-1).repeat(x.size(0), 1, x.size(-1)) # [batch,T,dim]\n",
        "    return torch.gather(x, dim=1, index=mask_keep) # [batch,mask_size,dim]\n",
        "\n",
        "\n",
        "# @title ijepa multiblock next\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "import math\n",
        "from multiprocessing import Value\n",
        "import torch\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(self, hw=(224, 224), enc_mask_scale=(.85,1), pred_mask_scale=(.15,.2), aspect_ratio=(.75,1.25),\n",
        "        nenc=1, npred=2, min_keep=4, allow_overlap=False):\n",
        "        super().__init__()\n",
        "        self.height, self.width = hw\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.aspect_ratio = aspect_ratio\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "\n",
        "    def _sample_block_size(self, scale, aspect_ratio_scale):\n",
        "        _rand = torch.rand(1).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.height * self.width * mask_scale) # num patches to keep\n",
        "        # -- Sample block aspect-ratio\n",
        "        min_ar, max_ar = aspect_ratio_scale\n",
        "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
        "        while h >= self.height: h -= 1 # crop mask to be smaller than img\n",
        "        while w >= self.width: w -= 1\n",
        "        return (h, w)\n",
        "\n",
        "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
        "        h, w = b_size\n",
        "\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # --\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            top = torch.randint(0, self.height - h, (1,))\n",
        "            left = torch.randint(0, self.width - w, (1,))\n",
        "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
        "            mask[top:top+h, left:left+w] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
        "        mask_complement[top:top+h, left:left+w] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, B):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        p_size = self._sample_block_size(scale=self.pred_mask_scale, aspect_ratio_scale=self.aspect_ratio)\n",
        "        e_size = self._sample_block_size(scale=self.enc_mask_scale, aspect_ratio_scale=(1., 1.))\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.height * self.width\n",
        "        min_keep_enc = self.height * self.width\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                print(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "        return collated_masks_enc, collated_masks_pred\n",
        "\n",
        "mask_collator = MaskCollator(hw=(32,32), enc_mask_scale=(.85, 1.), pred_mask_scale=(.15, .2), aspect_ratio=(.75, 1.5),\n",
        "        nenc=1, npred=4, min_keep=4,\n",
        "        # allow_overlap=True)\n",
        "        allow_overlap=False)\n",
        "\n",
        "b=16\n",
        "collated_masks_enc, collated_masks_pred = mask_collator(b)\n",
        "ctx_index, trg_index = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "\n",
        "# mask = torch.zeros(1 ,32*32)\n",
        "# mask[:, trg_index[:1]] = 1\n",
        "# mask[:, ctx_index[:1]] = .5\n",
        "# mask = mask.reshape(1,32,32)\n",
        "\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "# imshow(mask)\n",
        "\n",
        "mask = torch.zeros(b ,32*32)\n",
        "mask[torch.arange(b).unsqueeze(-1), trg_index] = 1\n",
        "mask[torch.arange(b).unsqueeze(-1), ctx_index] = .5\n",
        "mask = mask.reshape(b,1,32,32)\n",
        "import torchvision\n",
        "imshow(torchvision.utils.make_grid(mask, nrow=8))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask_collator = MaskCollator(hw=(1024,1024), enc_mask_scale=(.85, 1.), pred_mask_scale=(.15, .2), aspect_ratio=(.75, 1.5), nenc=1, npred=4, min_keep=4, allow_overlap=False)\n",
        "# %timeit collated_masks_enc, collated_masks_pred = mask_collator(64) # 225 ms 1024:4.79 s\n",
        "# %timeit ctx_index, trg_index = simplexmask2d(hw=(1024,1024), ctx_scale=(.85,1), trg_scale=(.5,.6), B=b, chaos=.5) # 265 ms ;topk 203 ms 1024:4.27 s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nj5bI5hSEdJ1",
        "outputId": "6a7d20de-cdae-47f0-82ac-64546f16f829"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slowest run took 5.27 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "4.79 s ± 3.03 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ttc=[]\n",
        "ttt=[]\n",
        "def mean(x): return sum(x)/len(x)\n",
        "\n",
        "for i in range(1000):\n",
        "    # collated_masks_enc, collated_masks_pred = mask_collator(1)\n",
        "    # context_indices, trg_indices = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "    context_indices, trg_indices = simplexmask2d(hw=(32,32), ctx_scale=(.85,1.), trg_scale=(.7,.8), B=b, chaos=[3,1])\n",
        "    ttc.append(context_indices.shape[-1])\n",
        "    ttt.append(trg_indices.shape[-1])\n",
        "\n",
        "print(mean(ttc), mean(ttt))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"figure.figsize\"] = (4,4)\n",
        "plt.hist(ttc, bins=20, alpha=.5, label='context mask')\n",
        "plt.hist(ttt, bins=20, alpha=.5, label='target mask')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "bC6elHNF5xzL",
        "outputId": "614a7664-0ace-4067-f96b-4075f447f96c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "180.775 767.987\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "M-zdjdJixtOu"
      },
      "outputs": [],
      "source": [
        "# @title Transformer Predictor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, 32*32, d_model)*0.02)\n",
        "        # self.pos_emb = nn.Parameter(RoPE2D(dim=d_model, h=8, w=8, base=1000), requires_grad=False)\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=32*32, base=10000), requires_grad=False)\n",
        "\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, d_model, n_heads) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.randn(1,1,d_model)*0.02) # randn zeros\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim)# if out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = x.shape\n",
        "        # x = x * self.pos_enc(context_indices)\n",
        "        x = x + self.pos_emb[0,context_indices]\n",
        "\n",
        "        # pred_tokens = self.cls * self.pos_enc(trg_indices) # [M, num_trg_toks, d_model]\n",
        "        pred_tokens = self.cls + self.pos_emb[0,trg_indices]\n",
        "        # print(\"pred fwd\", x.shape, pred_tokens.shape)\n",
        "        x = torch.cat([x, pred_tokens], dim=1) # [batch, seq_len+num_trg_toks, d_model]\n",
        "        out = self.transformer(x)\n",
        "\n",
        "        out = self.norm(out)\n",
        "\n",
        "        out = out[:,seq:] # [batch, num_trg_toks, d_model]\n",
        "        # ind = torch.cat([context_indices, trg_indices], dim=-1).unsqueeze(-1).repeat(1,1,x.shape[-1]) # [b,t]->[b,t,d]\n",
        "        # out=torch.gather(out, 1, ind)\n",
        "\n",
        "        # print(\"pred fwd\", context_indices.shape, trg_indices.shape, out.shape)\n",
        "\n",
        "        out = self.lin(out)\n",
        "        # print(\"pred fwd\", out.shape)\n",
        "        return out # [seq_len, batch_size, ntoken]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ardu1zJwdHM3",
        "outputId": "7c848318-65bc-40f8-b815-96c27b732128"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "230448\n",
            "torch.Size([])\n"
          ]
        }
      ],
      "source": [
        "# @title IJEPA\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class IJEPA(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=64, out_dim=None, nlayers=1, n_heads=4):\n",
        "        super().__init__()\n",
        "        self.out_dim = out_dim = out_dim or d_model\n",
        "        self.patch_size = 4 # 4\n",
        "        # self.student = Hiera(self.patch_size, in_dim, d_model, out_dim=out_dim, n_heads=n_heads, nlayers=nlayers, drop=0.)\n",
        "        self.student = ViT(self.patch_size, in_dim, d_model, out_dim=out_dim, n_heads=n_heads, nlayers=nlayers, drop=0.)\n",
        "        # self.predicter = TransformerPredictor(out_dim, d_model//2, out_dim, n_heads=n_heads, nlayers=nlayers//2, drop=0.)\n",
        "        self.predicter = TransformerPredictor(out_dim, 3*d_model//8, out_dim, n_heads=4, nlayers=1, drop=0.)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "\n",
        "    def loss(self, x): #\n",
        "        b,c,h,w = x.shape\n",
        "        # print('ijepa loss x',x.shape)\n",
        "        hw=(8,8)\n",
        "        # hw=(32,32)\n",
        "        # mask_collator = MaskCollator(hw, enc_mask_scale=(.85, 1.), pred_mask_scale=(.15, .2), aspect_ratio=(.75, 1.5), nenc=1, npred=4, min_keep=4, allow_overlap=False)\n",
        "        # collated_masks_enc, collated_masks_pred = mask_collator(b)\n",
        "        # context_indices, trg_indices = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "        # print(context_indices.shape, trg_indices.shape)\n",
        "\n",
        "        # context_indices, trg_indices = simplexmask2d(hw, ctx_scale=(.85,1), trg_scale=(.7,.8), B=b, chaos=[3,.5])\n",
        "        context_indices, trg_indices = simplexmask2d(hw, ctx_scale=(.85,1), trg_scale=(.6,.7), B=b, chaos=[3,.5])\n",
        "\n",
        "        # zero_mask = torch.zeros(b ,*hw, device=device).flatten(1)\n",
        "        # zero_mask[torch.arange(b).unsqueeze(-1), context_indices] = 1\n",
        "        # x_ = x * F.interpolate(zero_mask.reshape(b,1,*hw), size=x.shape[2:], mode='nearest-exact') # zero masked locations\n",
        "\n",
        "        sx = self.student(x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "        # print('ijepa loss sx',sx.shape)\n",
        "        sy_ = self.predicter(sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "        sy_ = F.layer_norm(sy_, (sy_.size(-1),))\n",
        "        with torch.no_grad():\n",
        "            sy = self.teacher(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "            sy = sy[torch.arange(sy.shape[0]).unsqueeze(-1), trg_indices] # [batch, num_context_toks, d_model] # nan bec len(trg_ind)==0 # print('loss sy',torch.isnan(sy).any())\n",
        "            sy = F.layer_norm(sy, (sy.size(-1),))\n",
        "        loss = F.mse_loss(sy, sy_)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        sx = self.student(x)\n",
        "        out = sx.mean(dim=1)\n",
        "        return out\n",
        "\n",
        "# min_s=0.15, max_s, M\n",
        "# trg.15.2M4 C.85 1\n",
        "# dont normalise data\n",
        "# randmsk < simplex 1msk < multiblk\n",
        "# teacher=trans ~? teacher=copy\n",
        "# learn convemb\n",
        "# lr pred,student: 3e-3/1e-2, 1e-3\n",
        "\n",
        "# lr 1e-3 < 1e-2,1e-3? slower but dun plateau\n",
        "\n",
        "# vit 1lyr 15.4sec 15.6\n",
        "# hiera downsampling attn 20.5\n",
        "\n",
        "# ijepa = IJEPA(in_dim=3, d_model=64, out_dim=64, nlayers=4, n_heads=4).to(device)\n",
        "ijepa = IJEPA(in_dim=3, d_model=64, out_dim=64, nlayers=1, n_heads=8).to(device)\n",
        "# ijepa = IJEPA(in_dim=3, d_model=32, out_dim=32, nlayers=1, n_heads=4).to(device)\n",
        "# optim = torch.optim.AdamW(ijepa.parameters(), lr=1e-3) # 1e-3?\n",
        "optim = torch.optim.AdamW([{'params': ijepa.student.parameters()},\n",
        "#     {'params': ijepa.predicter.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # good 3e-3,1e-3 ; default 1e-2, 5e-2\n",
        "    {'params': ijepa.predicter.parameters(), 'lr': 1e-2}], lr=1e-3, weight_decay=1e-2)\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/configs/in1k_vith14_ep300.yaml\n",
        "# d_model 1024,384\n",
        "# depth 12,6/12\n",
        "# wd 5e-2 - 4e-1\n",
        "# adamw 1e-4 - 1e-3 - 1e-6\n",
        "# ema 0.996-1\n",
        "\n",
        "print(sum(p.numel() for p in ijepa.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in ijepa.parameters())) # 27584\n",
        "# print(sum(p.numel() for p in ijepa.predicter.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# d_model^2 * nlayers\n",
        "\n",
        "x = torch.rand((64,3,32,32), device=device)\n",
        "out = ijepa.loss(x)\n",
        "print(out.shape)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(ijepa.out_dim, 10).to(device)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n",
        "# optim = torch.optim.AdamW([{'params': ijepa.parameters()}, {'params': classifier.parameters(), 'lr': 1e-3}], lr=1e-3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODpKypTCsfIt",
        "outputId": "b2f4380b-03a8-49be-95b5-eed2b564dc05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 16])\n"
          ]
        }
      ],
      "source": [
        "# @title TransformerVICReg\n",
        "import torch\n",
        "from torch import nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class TransformerVICReg(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, drop=0):\n",
        "        super().__init__()\n",
        "        act = nn.GELU()\n",
        "        patch_size=4\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Linear(in_dim, d_model), act\n",
        "            # nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            # nn.Conv1d(d_model, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            # nn.Conv1d(d_model, d_model,3,2,3//2),\n",
        "            nn.Conv2d(in_dim, d_model, patch_size, patch_size), # patch\n",
        "            )\n",
        "        self.pos_enc = RoPE(d_model, seq_len=200, base=10000)\n",
        "        # self.pos_emb = nn.Parameter(torch.randn(1, 8*8, d_model))\n",
        "        # self.pos_emb = nn.Parameter(RoPE2D(dim=d_model, h=8, w=8, base=10000).unsqueeze(0), requires_grad=False)\n",
        "\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "\n",
        "        # out_dim = out_dim or d_model\n",
        "        self.lin = nn.Linear(d_model, out_dim)\n",
        "        self.attn_pool = nn.Linear(d_model, 1, bias=False)\n",
        "\n",
        "        dim_v = d_model * 4\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(out_dim, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v, bias=False),\n",
        "            )\n",
        "\n",
        "    def forward(self, x): # [b,t,d] / [b,c,h,w]\n",
        "        x = self.embed(x).flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c]\n",
        "        # x = self.embed(x.transpose(-2,-1)).transpose(-2,-1) # [b,t,d]\n",
        "        x = self.pos_enc(x)\n",
        "        # x = x + self.pos_emb\n",
        "\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        attn = self.attn_pool(x).squeeze(-1) # [batch, seq] # seq_pool\n",
        "        out = (torch.softmax(attn, dim=-1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, seq] @ [batch, seq, dim] -> [batch, dim]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch, ntoken]\n",
        "\n",
        "    def expand(self, x):\n",
        "        sx = self.forward(x)\n",
        "        vx = self.exp(sx)\n",
        "        return vx\n",
        "\n",
        "batch, seq_len, d_model = 4,3500,512\n",
        "in_dim, out_dim=3,16\n",
        "model = TransformerVICReg(in_dim, d_model, out_dim, d_head=4, nlayers=2, drop=0.).to(device)\n",
        "# x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "x =  torch.rand((batch, in_dim, 32,32), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qwg9dG4sQ3h",
        "outputId": "2f0092f5-5bb7-4725-db2b-dcf7b0ba1a96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "62850\n",
            "in vicreg  0.00019920778413506923 24.74469393491745 4.793645480560826e-09\n",
            "(tensor(7.9683e-06, grad_fn=<MseLossBackward0>), tensor(0.9898, grad_fn=<AddBackward0>), tensor(4.7936e-09, grad_fn=<AddBackward0>))\n"
          ]
        }
      ],
      "source": [
        "# @title Violet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class Violet(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, nlayers=2, d_head=4):\n",
        "        super().__init__()\n",
        "        out_dim = out_dim or d_model\n",
        "        self.student = TransformerVICReg(in_dim, d_model, out_dim=out_dim, d_head=d_head, nlayers=nlayers, drop=0.)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "\n",
        "        # vicreg\n",
        "        self.sim_coeff=25. # 10.0 # 25.0 # λ\n",
        "        self.std_coeff=25. # 1.0 # 25.0 # µ\n",
        "        self.cov_coeff=1. # 25.0 # 1.0 # ν\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]c/ [b,c,h,w]\n",
        "        # print(x.shape)\n",
        "        # mask = simplexmask(hw=(8,8), scale=(.7,.8)).unsqueeze(0) # .6.8\n",
        "        # vx = self.student.expand(x, mask) # [batch, num_context_toks, out_dim]\n",
        "\n",
        "        vx = self.student.expand(x) # [batch, num_context_toks, out_dim]\n",
        "\n",
        "        with torch.no_grad(): vy = self.teacher.expand(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "        loss = self.vicreg(vx, vy)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        return self.student(x)\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size, num_features = x.shape\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = self.sim_coeff * repr_loss + self.std_coeff * std_loss + self.cov_coeff * cov_loss\n",
        "        print(\"in vicreg \",self.sim_coeff * repr_loss.item() , self.std_coeff * std_loss.item() , self.cov_coeff * cov_loss.item())\n",
        "        # return loss\n",
        "        return repr_loss, std_loss, cov_loss\n",
        "\n",
        "\n",
        "violet = Violet(in_dim=3, d_model=32, out_dim=16, nlayers=2, d_head=4).to(device)\n",
        "voptim = torch.optim.AdamW(violet.parameters(), lr=1e-3) # 1e-3?\n",
        "# voptim = torch.optim.AdamW([{'params': violet.student.transformer.parameters()},\n",
        "#     {'params': violet.student.exp.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # default 1e-2\n",
        "print(sum(p.numel() for p in violet.parameters() if p.requires_grad)) # 27584\n",
        "\n",
        "# x = torch.rand((2,1000,3), device=device)\n",
        "x = torch.rand((2,3,32,32), device=device)\n",
        "# x = torch.rand((2,1,16), device=device)\n",
        "loss = violet.loss(x)\n",
        "# print(out.shape)\n",
        "print(loss)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(16).to(device)\n",
        "# classifier = Classifier(16, 18).to(device)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_BFm8Kh-j3u"
      },
      "outputs": [],
      "source": [
        "for name, param in ijepa.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name, param)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "eb1n4BhimG_N"
      },
      "outputs": [],
      "source": [
        "# @title RankMe\n",
        "# RankMe: Assessing the Downstream Performance of Pretrained Self-Supervised Representations by Their Rank jun 2023 https://arxiv.org/pdf/2210.02885\n",
        "import torch\n",
        "\n",
        "# https://github.com/Spidartist/IJEPA_endoscopy/blob/main/src/helper.py#L22\n",
        "def RankMe(Z):\n",
        "    \"\"\"\n",
        "    Calculate the RankMe score (the higher, the better).\n",
        "    RankMe(Z) = exp(-sum_{k=1}^{min(N, K)} p_k * log(p_k)),\n",
        "    where p_k = sigma_k (Z) / ||sigma_k (Z)||_1 + epsilon\n",
        "    where sigma_k is the kth singular value of Z.\n",
        "    where Z is the matrix of embeddings (N × K)\n",
        "    \"\"\"\n",
        "    # compute the singular values of the embeddings\n",
        "    # _u, s, _vh = torch.linalg.svd(Z, full_matrices=False)  # s.shape = (min(N, K),)\n",
        "    # s = torch.linalg.svd(Z, full_matrices=False).S\n",
        "    s = torch.linalg.svdvals(Z)\n",
        "    p = s / torch.sum(s, axis=0) + 1e-7\n",
        "    return torch.exp(-torch.sum(p * torch.log(p)))\n",
        "\n",
        "# Z = torch.randn(5, 3)\n",
        "# rankme = RankMe(Z)\n",
        "# print(rankme)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "y24ZNFqW7woQ"
      },
      "outputs": [],
      "source": [
        "# @title LiDAR\n",
        "# LiDAR: Sensing Linear Probing Performance In Joint Embedding SSL Architectures https://arxiv.org/pdf/2312.04000\n",
        "# https://github.com/rbalestr-lab/stable-ssl/blob/main/stable_ssl/monitors.py#L106\n",
        "\n",
        "def LiDAR(sx, eps=1e-7, delta=1e-3):\n",
        "    sx = sx.unflatten(0, (-1, 2))\n",
        "    n, q, d = sx.shape\n",
        "    mu_x = sx.mean(dim=1) # mu_x # [n,d]\n",
        "    mu = mu_x.mean(dim=0) # mu # [d]\n",
        "\n",
        "    diff_b = (mu_x - mu).unsqueeze(-1) # [n,d,1]\n",
        "    S_b = (diff_b @ diff_b.transpose(-2,-1)).sum(0) / (n - 1) # [n,d,d] -> [d,d]\n",
        "    diff_w = (sx - mu_x.unsqueeze(1)).reshape(-1,d,1) # [n,q,d] -> [nq,d,1]\n",
        "    S_w = (diff_w @ diff_w.transpose(-2,-1)).sum(0) / (n * (q - 1)) + delta * torch.eye(d, device=sx.device) # [nq,d,d] -> [d,d]\n",
        "\n",
        "    eigvals_w, eigvecs_w = torch.linalg.eigh(S_w)\n",
        "    eigvals_w = torch.clamp(eigvals_w, min=eps)\n",
        "\n",
        "    invsqrt_w = (eigvecs_w * (1. / torch.sqrt(eigvals_w))) @ eigvecs_w.transpose(-1, -2)\n",
        "    S_lidar = invsqrt_w @ S_b @ invsqrt_w\n",
        "    lam = torch.linalg.eigh(S_lidar)[0].clamp(min=0)\n",
        "    p = lam / lam.sum() + eps\n",
        "    return torch.exp(-torch.sum(p * torch.log(p)))\n",
        "\n",
        "# sx = torch.randn(32, 128)\n",
        "# print(sx.shape)\n",
        "# lidar = LiDAR(sx)\n",
        "# print(lidar.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "2Nd-sGe6Ku4S",
        "outputId": "ea118fcd-3cf0-4ef3-dd24-343faf9b0552"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>█▇▇▇▇▆▆▅▅▅▅▅▄▄▄▃▃▄▄▃▅▃▃▂▃▂▄▂▂▃▁▂▃▁▂▂▂▂▁▂</td></tr><tr><td>correct</td><td>▁▂▂▂▃▃▃▃▃▃▄▄▄▄▅▄▅▇▅▄▆▅▆▅▆▅▆▄▆▅▅▅▆▆█▅▅▅█▇</td></tr><tr><td>lidar</td><td>▅▇▇▂▆▇▆▃▁▃█▇▅▂▆▅▄▇▄▅▃▃▃▂▄▃▁▅▁▅▂▇▄▆▇▁▆▆▃▃</td></tr><tr><td>loss</td><td>▆▁▁▄▄▄▂▂▂▂▅▄▅▅█▄▅▅▄▅▄▃▃▅▄▃▃▅▇▅█▅▃▅▇▆▅▆▃▄</td></tr><tr><td>rankme</td><td>▁▁▂▂▃▄▅▅▅▅▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇█████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>1.72052</td></tr><tr><td>correct</td><td>0.4375</td></tr><tr><td>lidar</td><td>12.71652</td></tr><tr><td>loss</td><td>0.34729</td></tr><tr><td>rankme</td><td>23.9165</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">likely-pine-85</strong> at: <a href='https://wandb.ai/bobdole/ijepa/runs/upht3uyg' target=\"_blank\">https://wandb.ai/bobdole/ijepa/runs/upht3uyg</a><br> View project at: <a href='https://wandb.ai/bobdole/ijepa' target=\"_blank\">https://wandb.ai/bobdole/ijepa</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250424_090710-upht3uyg/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250424_095928-yhc7la23</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/ijepa/runs/yhc7la23' target=\"_blank\">valiant-waterfall-86</a></strong> to <a href='https://wandb.ai/bobdole/ijepa' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/ijepa' target=\"_blank\">https://wandb.ai/bobdole/ijepa</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/ijepa/runs/yhc7la23' target=\"_blank\">https://wandb.ai/bobdole/ijepa/runs/yhc7la23</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "!pip install -q wandb\n",
        "import wandb # https://docs.wandb.ai/quickstart\n",
        "wandb.login(key='487a2109e55dce4e13fc70681781de9f50f27be7')\n",
        "try: run.finish()\n",
        "except NameError: pass\n",
        "run = wandb.init(project=\"ijepa\", config={\"model\": \"res18\",})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "34de8800-fa4b-4460-803c-3cd8dbfb6c70"
      },
      "outputs": [],
      "source": [
        "# @title strain ctrain test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "def strain(model, dataloader, optim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.train()\n",
        "    for i, (x, _) in enumerate(dataloader):\n",
        "        x = x.to(device)#.to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            loss = model.loss(x)\n",
        "\n",
        "            # repr_loss, std_loss, cov_loss = model.loss(x)\n",
        "            # j_loss, (repr_loss, std_loss, cov_loss) = model.loss(x)\n",
        "            # loss = model.sim_coeff * repr_loss + model.std_coeff * std_loss + model.cov_coeff * cov_loss\n",
        "            # loss = j_loss + loss\n",
        "\n",
        "        optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 5) # 0.5\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            m=0.999 # 0.99 m = next(momentum_scheduler)\n",
        "            norms=[]\n",
        "            for param_q, param_k in zip(model.student.parameters(), model.teacher.parameters()):\n",
        "                param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        if i%10==0: print(\"strain\",loss.item())\n",
        "        # try: wandb.log({\"loss\": loss.item(), \"repr/I\": repr_loss.item(), \"std/V\": std_loss.item(), \"cov/C\": cov_loss.item()})\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=50: break\n",
        "\n",
        "\n",
        "def ctrain(model, classifier, dataloader, coptim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.eval()\n",
        "    classifier.train()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            with torch.no_grad():\n",
        "                sx = model(x).detach()\n",
        "            y_ = classifier(sx)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "        coptim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(coptim)\n",
        "        scaler.update()\n",
        "        print(\"classify\",loss.item())\n",
        "        try: wandb.log({\"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        with torch.no_grad():\n",
        "            sx = model(x)\n",
        "            rankme = RankMe(sx).item()\n",
        "            lidar = LiDAR(sx).item()\n",
        "            y_ = classifier(sx)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        print(correct/len(y))\n",
        "        # try: wandb.log({\"correct\": correct/len(y)})\n",
        "        try: wandb.log({\"correct\": correct/len(y), \"rankme\": rankme, \"lidar\": lidar})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "import time\n",
        "start = begin = time.time()\n",
        "# for i in range(1):\n",
        "for i in range(1000):\n",
        "    print(i)\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    strain(ijepa, train_loader, optim)\n",
        "    ctrain(ijepa, classifier, train_loader, coptim)\n",
        "    test(ijepa, classifier, test_loader)\n",
        "\n",
        "    # strain(violet, train_loader, voptim)\n",
        "    # ctrain(violet, classifier, train_loader, coptim)\n",
        "    # test(violet, classifier, test_loader)\n",
        "\n",
        "    print('time:',time.time() - start, (time.time()-begin)/(i+1))\n",
        "    start = time.time()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4J2ahp3wmqcg"
      },
      "outputs": [],
      "source": [
        "# @title supervised train test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "# def strain(model, dataloader, optim, scheduler=None):\n",
        "def strain(model, classifier, dataloader, optim, coptim, scheduler=None):\n",
        "    model.train()\n",
        "    classifier.train()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x = x.to(device)#.to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "        y = y.to(device)\n",
        "\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        # print(loss)\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # for p in list(filter(lambda p: p.grad is not None, model.parameters())):\n",
        "        #     print(p.grad.data.norm(2).item())\n",
        "        # print(\"max grad norm\", max([p.grad.data.norm(2).item() for p in list(filter(lambda p: p.grad is not None, model.parameters()))]))\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 10) # 0.5\n",
        "\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        print(\"strain\",loss.item())\n",
        "        # for param in ijepa.student.cls: print(param.data)\n",
        "        # for param in ijepa.predicter.cls: print(param.data)\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=50: break\n",
        "\n",
        "\n",
        "# def test(model, dataloader):\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        # .to(torch.bfloat16)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "        loss = F.cross_entropy(y_, y)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        print(correct/len(y))\n",
        "        try: wandb.log({\"correct\": correct/len(y), \"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "for i in range(1000):\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # np.random.shuffle(train_indices); np.random.shuffle(val_indices)\n",
        "    # train_sampler, valid_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(val_indices)\n",
        "    # # batch_size = 64 #512\n",
        "    # train_loader = DataLoader(train_data, sampler=train_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "    # test_loader = DataLoader(train_data, sampler=valid_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True)\n",
        "\n",
        "    # strain(ijepa, train_loader, optim)\n",
        "    strain(ijepa, classifier, train_loader, optim, coptim)\n",
        "    test(ijepa, classifier, test_loader)\n",
        "\n",
        "    # strain(violet, train_loader, voptim)\n",
        "    # test(violet, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzo9DMDPcOxu",
        "outputId": "2aa950ff-7a1c-46e0-924c-c6922b2ca522"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# @title save/load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder='/content/drive/MyDrive/jepa/'\n",
        "\n",
        "# # modelsd, optimsd = torch.load(folder+'SeqJEPA.pkl', map_location=device).values()\n",
        "# modelsd, optimsd = torch.load('SeqJEPA.pkl', map_location=device).values()\n",
        "# seq_jepa.load_state_dict(modelsd, strict=False)\n",
        "# optim.load_state_dict(optimsd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "DNNPOuUmcSNf",
        "outputId": "9c0fdeca-f315-457a-a102-ff87f9290f75"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'seq_jepa' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-06e8be6b0f78>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mseq_jepa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'IJEPA.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# torch.save(checkpoint, 'IJEPA.pkl')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'seq_jepa' is not defined"
          ]
        }
      ],
      "source": [
        "checkpoint = {'model': seq_jepa.state_dict(), 'optimizer': optim.state_dict()}\n",
        "torch.save(checkpoint, folder+'IJEPA.pkl')\n",
        "# torch.save(checkpoint, 'IJEPA.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLT74ihtMnh3"
      },
      "source": [
        "## drawer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMTvHTtTRInh",
        "outputId": "91ca330e-fd81-42c6-af3c-d4e5316df589"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "345200\n",
            "(tensor(1.9985, device='cuda:0', grad_fn=<MseLossBackward0>), (tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>), tensor(0.9865, device='cuda:0', grad_fn=<AddBackward0>), tensor(7.4222e-06, device='cuda:0', grad_fn=<AddBackward0>)))\n"
          ]
        }
      ],
      "source": [
        "# @title IJEPA + VICReg\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "    def __init__(self, patch_size, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        patch_size=4\n",
        "        act = nn.GELU()\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Conv2d(in_dim, d_model, patch_size, patch_size), # like patch\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 1, 7//2, bias=False)\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.MaxPool2d(3,2,1)\n",
        "            nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.BatchNorm2d(d_model), nn.ReLU(),\n",
        "            nn.Conv2d(d_model, d_model, 3, 2, 3//2, bias=False)\n",
        "            )\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, 32*32, d_model)*.02)\n",
        "        # self.pos_emb = nn.Parameter(RoPE2D(dim=d_model, h=8, w=8, base=1000), requires_grad=False)\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=(32//patch_size)**2, base=10000), requires_grad=False)\n",
        "\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, d_model, n_heads) for _ in range(nlayers)])\n",
        "\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "        # self.lin = nn.Linear(d_model, out_dim)\n",
        "        self.attn_pool = nn.Linear(d_model, 1, bias=False)\n",
        "\n",
        "        dim_v = d_model * 4\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(out_dim, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v, bias=False),\n",
        "            )\n",
        "\n",
        "    # def forward(self, x, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "    def jepa_fwd(self, x, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x).flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c] # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        # x = self.pos_enc(x)\n",
        "        # print(\"TransformerModel\",x.shape, self.pos_emb.shape)\n",
        "        x = x + self.pos_emb[:,:x.shape[1]]\n",
        "        if context_indices != None: x = x[torch.arange(x.shape[0]).unsqueeze(-1), context_indices] # [batch, num_context_toks, d_model]\n",
        "\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        out = self.norm(x)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out\n",
        "\n",
        "    def pool(self, x):\n",
        "        # sx = self.forward(x)\n",
        "        # attn = self.attn_pool(x).squeeze(-1) # [batch, seq] # seq_pool\n",
        "        # out = (torch.softmax(attn, dim=-1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, seq] @ [batch, seq, dim] -> [batch, dim]\n",
        "        # if self.lin: out = self.lin(out)\n",
        "        out = x.mean(dim=1)\n",
        "        return out\n",
        "\n",
        "    def forward(self, x, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.jepa_fwd(x, context_indices=None)\n",
        "        out = self.pool(x)\n",
        "        return out\n",
        "\n",
        "    def expand(self, x):\n",
        "        # sx = self.forward(x)\n",
        "        sx = self.pool(x)\n",
        "        vx = self.exp(sx)\n",
        "        return vx\n",
        "\n",
        "\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class IJEPAVICReg(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=64, out_dim=None, nlayers=1, n_heads=4):\n",
        "        super().__init__()\n",
        "        self.out_dim = out_dim = out_dim or d_model\n",
        "        self.patch_size = 4 # 4\n",
        "        self.student = ViT(self.patch_size, in_dim, d_model, out_dim=out_dim, n_heads=n_heads, nlayers=nlayers, drop=0.)\n",
        "        self.predicter = TransformerPredictor(out_dim, 3*d_model//8, out_dim, n_heads=4, nlayers=1, drop=0.)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "\n",
        "        # vicreg\n",
        "        self.sim_coeff=25. # 10.0 # 25.0 # λ\n",
        "        self.std_coeff=25. # 1.0 # 25.0 # µ\n",
        "        self.cov_coeff=1. # 25.0 # 1.0 # ν\n",
        "\n",
        "\n",
        "    def loss(self, x): #\n",
        "        b,c,h,w = x.shape\n",
        "        # print('ijepa loss x',x.shape)\n",
        "        hw=(8,8)\n",
        "        # hw=(32,32)\n",
        "        mask_collator = MaskCollator(hw, enc_mask_scale=(.85, 1.), pred_mask_scale=(.15, .2), aspect_ratio=(.75, 1.5), nenc=1, npred=4, min_keep=4, allow_overlap=False)\n",
        "        collated_masks_enc, collated_masks_pred = mask_collator(b)\n",
        "        context_indices, trg_indices = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "        # print(context_indices.shape, trg_indices.shape)\n",
        "\n",
        "        # context_indices, trg_indices = simplexmask2d(hw, ctx_scale=(.85,1), trg_scale=(.6,.8), B=b, chaos=3)\n",
        "\n",
        "        # zero_mask = torch.zeros(b ,*hw, device=device).flatten(1)\n",
        "        # zero_mask[torch.arange(b).unsqueeze(-1), context_indices] = 1\n",
        "        # x_ = x * F.interpolate(zero_mask.reshape(b,1,*hw), size=x.shape[2:], mode='nearest-exact') # zero masked locations\n",
        "\n",
        "        sx = self.student.jepa_fwd(x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "\n",
        "        # print('ijepa loss sx',sx.shape)\n",
        "        sy_ = self.predicter(sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "        sy_ = F.layer_norm(sy_, (sy_.size(-1),))\n",
        "        vx = self.student.expand(sx) # [batch, num_context_toks, out_dim]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            sy = self.teacher.jepa_fwd(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "            sy = sy[torch.arange(sy.shape[0]).unsqueeze(-1), trg_indices] # [batch, num_context_toks, d_model] # nan bec len(trg_ind)==0 # print('loss sy',torch.isnan(sy).any())\n",
        "            sy = F.layer_norm(sy, (sy.size(-1),))\n",
        "\n",
        "            vy = self.teacher.expand(sy.detach()) # [batch, num_trg_toks, out_dim]\n",
        "\n",
        "        vic_loss = self.vicreg(vx, vy)\n",
        "        j_loss = F.mse_loss(sy, sy_)\n",
        "        # return loss\n",
        "        return j_loss, vic_loss\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        out = self.student(x)\n",
        "        return out\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size, num_features = x.shape\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = self.sim_coeff * repr_loss + self.std_coeff * std_loss + self.cov_coeff * cov_loss\n",
        "        # print(\"in vicreg \",self.sim_coeff * repr_loss.item() , self.std_coeff * std_loss.item() , self.cov_coeff * cov_loss.item())\n",
        "        # return loss\n",
        "        return repr_loss, std_loss, cov_loss\n",
        "\n",
        "# for jepa+vic, attnpool < meanpool\n",
        "# jepa+vic < jepa\n",
        "\n",
        "\n",
        "ijepa = IJEPAVICReg(in_dim=3, d_model=64, out_dim=64, nlayers=1, n_heads=4).to(device)\n",
        "print(sum(p.numel() for p in ijepa.parameters() if p.requires_grad)) # 27584\n",
        "optim = torch.optim.AdamW(ijepa.parameters(), lr=1e-3) # 1e-3?\n",
        "# optim = torch.optim.AdamW([{'params': ijepa.student.parameters()},\n",
        "#     {'params': ijepa.predicter.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # good 3e-3,1e-3 ; default 1e-2, 5e-2\n",
        "#     # {'params': ijepa.predicter.parameters(), 'lr': 1e-2}], lr=1e-3, weight_decay=1e-2)\n",
        "\n",
        "x = torch.rand((2,3,32,32), device=device)\n",
        "loss = ijepa.loss(x)\n",
        "print(loss)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(ijepa.out_dim, 10).to(device)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ge36SCxOl2Oq"
      },
      "outputs": [],
      "source": [
        "# @title RoPE2D & RotEmb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "def RoPE(dim, seq_len=512, base=10000):\n",
        "    theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "    pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "    angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "    rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [1, seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "    return rot_emb\n",
        "\n",
        "# class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "#     def __init__(self, dim, seq_len=512, base=10000):\n",
        "#         super().__init__()\n",
        "#         self.dim, self.base = dim, base\n",
        "#         theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "#         pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "#         angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "#         self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [1, seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "#     def forward(self, x): # [batch, T, dim] / [batch, T, n_heads, d_head]\n",
        "#         seq_len = x.size(1)\n",
        "#         if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.base)\n",
        "#         # print('rope fwd', x.shape, self.rot_emb.shape)\n",
        "#         if x.dim()==4: return x * self.rot_emb[:,:seq_len].unsqueeze(2)\n",
        "#         return x * self.rot_emb[:,:seq_len]\n",
        "\n",
        "\n",
        "def RoPE2D(dim=16, h=8, w=8, base=10000):\n",
        "    # theta = 1. / (base ** (torch.arange(0, dim, step=4) / dim))\n",
        "    # theta = 1. / (base**torch.linspace(0,1,dim//4)).unsqueeze(0)\n",
        "    theta = 1. / (base**torch.linspace(0,1,dim//2)).unsqueeze(0)\n",
        "    y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\") # print(y,x) # [h,w], y:row_num, x:col_num\n",
        "    y, x = (y.reshape(-1,1) * theta).unsqueeze(-1), (x.reshape(-1,1) * theta).unsqueeze(-1) # [h*w,1]*[1,dim//4] = [h*w, dim//4, 1]\n",
        "    # rot_emb = torch.cat([x.sin(), x.cos(), y.sin(), y.cos()], dim=-1).flatten(-2) # [h*w, dim//4 ,4] -> [h*w, dim]\n",
        "    # rot_emb = torch.cat([x.sin(), x.cos(), y.sin(), y.cos()], dim=-1)#.reshape(dim, h, w).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "    rot_emb = torch.cat([x.sin(), y.sin()], dim=-1).flatten(-2).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "    # rot_emb = torch.cat([x.cos(), y.cos()], dim=-1).reshape(h, w, dim).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "    return rot_emb\n",
        "\n",
        "\n",
        "\n",
        "# class RoPE2D(nn.Module): # Rotary Positional Embeddings\n",
        "#     def __init__(self, dim, h=224, w=224, base=10000):\n",
        "#         super().__init__()\n",
        "#         self.dim, self.h, self.w = dim, h, w\n",
        "#         # # theta = 1. / (base ** (torch.arange(0, dim, step=4) / dim))\n",
        "#         # theta = 1. / (base**torch.linspace(0,1,dim//4)).unsqueeze(0)\n",
        "#         theta = 1. / (base**torch.linspace(0,1,dim//2)).unsqueeze(0)\n",
        "#         y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\") # print(y,x) # [h,w], y:row_num, x:col_num\n",
        "#         y, x = (y.reshape(-1,1) * theta).unsqueeze(-1), (x.reshape(-1,1) * theta).unsqueeze(-1) # [h*w,1]*[1,dim//4] = [h*w, dim//4, 1]\n",
        "#         # # self.rot_emb = torch.cat([x.sin(), x.cos(), y.sin(), y.cos()], dim=-1).flatten(-2) # [h*w, dim//4 ,4] -> [h*w, dim]\n",
        "#         # self.rot_emb = torch.cat([x.sin(), x.cos(), y.sin(), y.cos()], dim=-1).reshape(h, w, dim).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "#         self.rot_emb = torch.cat([x.sin(), y.sin()], dim=-1).reshape(h, w, dim).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "#         # self.rot_emb = torch.cat([x.cos(), y.cos()], dim=-1).reshape(h, w, dim).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "\n",
        "#     def forward(self, img): #\n",
        "#         # batch, dim, h, w = img.shape\n",
        "#         # print(img.shape)\n",
        "#         hw = img.shape[1] # [b, hw, dim] / [b, hw, n_heads, d_head]\n",
        "#         h=w=int(hw**.5)\n",
        "#         if self.h < h or self.w < w: self.__init__(self.dim, h, w)\n",
        "#         # print(self.rot_emb.shape)\n",
        "#         # rot_emb = self.rot_emb[:, :h, :w].unsqueeze(0) # [1, h, w, dim]\n",
        "#         rot_emb = self.rot_emb[:h, :w] # [h, w, dim]\n",
        "#         # return img * rot_emb.flatten(end_dim=1).unsqueeze(0) # [b, hw, dim] * [1, hw, dim]\n",
        "#         return img * rot_emb.flatten(end_dim=1)[None,:,None,:] # [b, hw, n_heads, d_head] * [1, hw, 1, dim]\n",
        "#         # return img * self.rot_emb\n",
        "\n",
        "class RotEmb(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, top=torch.pi, base=10000):\n",
        "        super().__init__()\n",
        "        self.theta = top / (base ** (torch.arange(0, dim, step=2, device=device) / dim))\n",
        "        # self.theta = top / (base ** torch.linspace(0, 1, dim//2, device=device))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (pos.unsqueeze(-1) * self.theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [seq_len, dim]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3bpiybH7M0O1"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# @title test multiblock params\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "dataiter = iter(train_data)\n",
        "img,y = next(dataiter)\n",
        "img = img.unsqueeze(0)\n",
        "img = F.interpolate(img, (8,8))\n",
        "b,c,h,w = img.shape\n",
        "# img = img.flatten(2).transpose(-2,-1).to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "print(img.shape)\n",
        "# batch, seq,_ = img.shape\n",
        "\n",
        "\n",
        "# # target_mask = randpatch(seq, mask_size=8, gamma=.9) # 8.9 [seq]\n",
        "# target_mask = multiblock(seq, min_s=0.2, max_s=0.3, M=4).any(0) # best.2.3M4 og.15.2M4# mask out targets to be predicted # [M, seq]\n",
        "# target_mask = multiblock(seq, min_s=0.2, max_s=0.3, M=4).any(0)\n",
        "\n",
        "target_mask = multiblock2d((8,8), M=4).any(0).unsqueeze(0)\n",
        "\n",
        "\n",
        "# context_mask = ~multiblock(seq, min_s=0.85, max_s=1., M=1)|target_mask # [1, seq], True->Mask\n",
        "context_mask = ~multiblock2d((8,8), scale=(.85,.85), aspect_ratio=(.9,1.1), M=1)|target_mask # [1, seq], True->Mask\n",
        "\n",
        "# print(target_mask, context_mask)\n",
        "\n",
        "print(target_mask.shape, context_mask.shape)\n",
        "# target_img, context_img = img*target_mask.unsqueeze(-1), img*context_mask.unsqueeze(-1)\n",
        "# target_img, context_img = img*~target_mask.unsqueeze(-1), img*~context_mask.unsqueeze(-1)\n",
        "target_img, context_img = img*~target_mask.unsqueeze(0), img*~context_mask.unsqueeze(0)\n",
        "# print(target_img.shape, context_img.shape)\n",
        "target_img, context_img = target_img.transpose(-2,-1).reshape(b,c,h,w), context_img.transpose(-2,-1).reshape(b,c,h,w)\n",
        "target_img, context_img = target_img.float(), context_img.float()\n",
        "\n",
        "# imshow(out.detach().cpu())\n",
        "imshow(target_img[0])\n",
        "imshow(context_img[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1iZQ6UNwoty",
        "outputId": "5554e8f3-71ec-4b70-c7d7-219b59e9a1d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9920\n",
            "torch.Size([4, 64, 16])\n"
          ]
        }
      ],
      "source": [
        "# @title ViT me more\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "# class LayerNorm2d(nn.LayerNorm):\n",
        "class LayerNorm2d(nn.RMSNorm):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        x = super().forward(x)\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads=None, d_head=8, cond_dim=None, dropout=0.): # .1\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_head = d_head\n",
        "        self.n_heads = d_model // d_head\n",
        "        # self.d_head = d_model // n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or d_model, 2*d_model, bias=False)\n",
        "        # self.k = nn.Sequential(nn.Dropout(dropout), nn.Linear(cond_dim, d_model, bias=False))\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        # self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        # self.lin = nn.Sequential(nn.Dropout(dropout), zero_module(nn.Linear(d_model, d_model)))\n",
        "        self.drop = nn.Dropout(dropout) # indp before q,k,v; after linout\n",
        "        self.scale = self.d_head ** -.5\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [batch, T, d_model]=[batch, h*w, c], [batch, num_tok, cond_dim], [batch,T]\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        Q = self.q(x).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        # K = self.k(x).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2)\n",
        "        K, V = self.kv(cond).unflatten(-1, (self.n_heads, 2*self.d_head)).transpose(1, 2).chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # linear attention # Softmax(Q) @ (Softmax(K).T @ V)\n",
        "        if mask != None:\n",
        "            mask = mask[:, None, :, None] # [batch,T] -> [batch,1,T,1]\n",
        "            K, V = K.masked_fill(mask, -torch.finfo(x.dtype).max), V.masked_fill(mask, -torch.finfo(x.dtype).max)\n",
        "        Q, K = Q.softmax(dim=-1)*self.scale, K.softmax(dim=-2)\n",
        "        context = K.transpose(-2,-1) @ V # [batch, n_heads, d_head, d_head]\n",
        "        out = Q @ context # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # (quadratic) attention # Softmax(Q @ K.T) @ V\n",
        "        # attn = Q @ K.transpose(-2,-1) * self.scale # [batch, n_heads, T] # [batch, n_heads, T, T/num_tok]\n",
        "        # if mask != None: attn = attn.masked_fill(mask[:, None, :, None], -torch.finfo(attn.dtype).max) # [batch,T]->[batch,1,T,1]\n",
        "        # attention = torch.softmax(attn, dim=-1)\n",
        "        # out = self.drop(attention) @ V # [batch, n_heads, T, d_head]\n",
        "\n",
        "        out = out.transpose(1, 2).flatten(2)\n",
        "        return self.drop(self.lin(out)) # [batch, T, d_model]\n",
        "\n",
        "# if self, dont pass cond_dim in init, dont pass cond in fwd\n",
        "# Softmax(Q @ K.T) @ V ~ Softmax(Q) @ Softmax(K).T @ V\n",
        "\n",
        "# https://github.com/lucidrains/x-transformers/blob/main/x_transformers/x_transformers.py#L1855\n",
        "\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_head, cond_dim=None, mult=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.cond_dim = cond_dim\n",
        "        self.self = MultiHeadAttention(d_model, d_head=d_head, dropout=0) # 16448\n",
        "        # self.self = Pooling()\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        # self.ff = nn.Sequential(\n",
        "        #     *[nn.BatchNorm2d(d_model), act, SeparableConv2d(d_model, d_model),]*3\n",
        "        #     )\n",
        "        # self.ff = ResBlock(d_model) # 74112\n",
        "        # self.ff = UIB(d_model, mult=4) # uib m4 36992, m2 18944\n",
        "        ff_dim=d_model*mult\n",
        "        self.ff = nn.Sequential(\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(dropout), nn.Linear(d_model, ff_dim), act, # ReLU GELU\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(dropout), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            nn.RMSNorm(d_model), act, nn.Dropout(dropout), nn.Linear(d_model, ff_dim),\n",
        "            nn.RMSNorm(ff_dim), act, nn.Dropout(dropout), zero_module(nn.Linear(ff_dim, d_model))\n",
        "        )\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,c,h,w], [batch, num_tok, cond_dim], [batch,T]\n",
        "        bchw = x.shape\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "        # print('attnblk fwd',x.shape)\n",
        "\n",
        "        # if self.cond_dim==None: cond=None # is self attn\n",
        "        x = x + self.drop(self.self(self.norm(x)))\n",
        "        # x = x.transpose(1,2).reshape(*bchw)\n",
        "        x = x + self.ff(x)\n",
        "        # x = self.ff(x)\n",
        "        # x = x + self.drop(self.norm2(self.ff(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "import torch\n",
        "# def apply_masks(x, masks): # [b,t,d], [M,mask_size] # https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "#     all_x = []\n",
        "#     for m in masks: # [1,T]\n",
        "#         mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [1,T,dim]\n",
        "#         all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "#     return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "def apply_masks(x, mask): # [b,t,d], [mask_size] # https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "    mask_keep = mask.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [1,T,dim]\n",
        "    # return torch.cat(torch.gather(x, dim=1, index=mask_keep), dim=0)  # [M*batch,mask_size,dim]\n",
        "    return torch.gather(x, dim=1, index=mask_keep) # [M*batch,mask_size,dim]\n",
        "\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        # self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        self.positional_emb = nn.Parameter(torch.zeros(1, 8*8, d_model)) # positional_embedding == 'learnable'\n",
        "        # self.positional_emb = nn.Parameter(torch.zeros(1, dim, 8,8))\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.zeros(1,1,d_model)) # randn zeros\n",
        "        nn.init.trunc_normal_(self.cls, std=.02)\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model)\n",
        "        self.lin = nn.Linear(d_model, out_dim)# if out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = x.shape\n",
        "        # x = x * self.pos_encoder(context_indices)\n",
        "        x = x + self.positional_emb[:,context_indices]\n",
        "        # x = apply_masks(x, [context_indices])\n",
        "\n",
        "        # pred_tokens = self.cls * self.pos_encoder(trg_indices) # [M, num_trg_toks, d_model]\n",
        "        pred_tokens = self.cls + self.positional_emb[0,trg_indices]\n",
        "        print(self.cls.shape, self.positional_emb[0,trg_indices].shape, trg_indices.shape)\n",
        "        pred_tokens = pred_tokens.repeat(batch, 1, 1) # [batch*M, num_trg_toks, d_model]\n",
        "        # print(pred_tokens.requires_grad)\n",
        "        print(\"pred fwd\", x.shape, pred_tokens.shape)\n",
        "        # x = x.repeat_interleave(trg_indices.shape[0], dim=0) # [batch, seq_len, d_model] -> [batch*M, seq_len, d_model]\n",
        "        x = torch.cat([x, pred_tokens], dim=1) # [batch*M, seq_len+num_trg_toks, d_model]\n",
        "\n",
        "        # x = x.transpose(1,2).unflatten(-1, (8,8))#.reshape(*bchw)\n",
        "        out = self.transformer_encoder(x) # float [seq_len, batch_size, d_model]\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "\n",
        "        out = self.norm(out)\n",
        "        out = out[:,seq:] # [batch*M, num_trg_toks, d_model]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Sequential(\n",
        "            # # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.MaxPool1d(2,2), #nn.MaxPool1d(3, 2, 3//2),\n",
        "            # # nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.Conv1d(d_model, d_model,3,2,3//2)\n",
        "            # nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            # nn.Conv1d(d_model, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            # nn.Conv1d(d_model, d_model,3,2,3//2),\n",
        "            # # nn.Conv1d(in_dim, d_model,2,2,0), # like patch\n",
        "            nn.Conv2d(in_dim, d_model, kernel_size=7, stride=2, padding=7//2, bias=False), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "            )\n",
        "        # self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        self.positional_emb = nn.Parameter(torch.zeros(1, 8*8, d_model)) # positional_embedding == 'learnable'\n",
        "        # self.positional_emb = nn.Parameter(torch.zeros(1, dim, 8,8)) # positional_embedding == 'learnable'\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model)\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # x = self.embed(x.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "        x = self.embed(x)\n",
        "        # bchw = x.shape\n",
        "        x = x.flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c] # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "\n",
        "        # x = self.pos_encoder(x)\n",
        "        # x = x * self.pos_encoder(torch.arange(seq, device=device)).unsqueeze(0)\n",
        "        # print(x.shape, self.positional_emb.shape)\n",
        "        x = x + self.positional_emb\n",
        "        if context_indices != None:\n",
        "            # x = apply_masks(x, [context_indices])\n",
        "            x = apply_masks(x, context_indices)\n",
        "\n",
        "\n",
        "        # x = x.transpose(1,2).reshape(*bchw)\n",
        "        x = self.transformer_encoder(x) # float [seq_len, batch_size, d_model]\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "        out = self.norm(x)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "\n",
        "# pos_emb rope < learn < learned\n",
        "# conv > pixel?\n",
        "# droppath not required\n",
        "\n",
        "# norm,act,conv < conv,norm,act\n",
        "# 2*s1 < uib < resblock\n",
        "# gatedadaln 3 < 2 = 1 < ffmult4 = 2*gatedadaln\n",
        "# MaxPool2d(2,2) < MaxPool2d(3,2,3//2)\n",
        "\n",
        "\n",
        "# dim = 64\n",
        "# dim_head = 8\n",
        "# heads = dim // dim_head\n",
        "# num_classes = 10\n",
        "# # model = SimpleViT(image_size=32, patch_size=4, num_classes=num_classes, dim=dim, depth=1, heads=heads, mlp_dim=dim*4, channels = 3, dim_head = dim_head)\n",
        "# model = SimpleViT(in_dim=3, out_dim=num_classes, dim=dim, depth=1, heads=heads, mlp_dim=dim*4, channels = 3, dim_head = dim_head).to(device)\n",
        "# print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 59850\n",
        "# optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "# # print(images.shape) # [batch, 3, 32, 32]\n",
        "# logits = model(x)\n",
        "# print(logits.shape)\n",
        "# # print(logits[0])\n",
        "# # print(logits[0].argmax(1))\n",
        "# pred_probab = nn.Softmax(dim=1)(logits)\n",
        "# y_pred = pred_probab.argmax(1)\n",
        "# # print(f\"Predicted class: {y_pred}\")\n",
        "\n",
        "\n",
        "batch, seq_len, d_model = 4,1024,16\n",
        "in_dim = 3\n",
        "model = TransformerModel(in_dim, d_model, d_head=4, nlayers=2, dropout=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x = torch.rand((batch, in_dim, 32, 32), device=device)\n",
        "# x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "# # # print(out)\n",
        "# model = TransformerPredictor(in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1).to(device)\n",
        "# out = model(out)\n",
        "# print(out.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "e3dAhWh45F4M"
      },
      "outputs": [],
      "source": [
        "# @title simplex\n",
        "# !pip install -q opensimplex\n",
        "import opensimplex\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def simplexmask2d(hw=(32,32), ctx_scale=(.85,1), trg_scale=(.6,.8), B=64, chaos=2):\n",
        "    seed = np.random.randint(1e10, size=B)\n",
        "    ix = iy = np.linspace(0, chaos, num=max(hw))\n",
        "    noise = opensimplex.noise3array(ix, iy, seed) # [b,h,w]\n",
        "    # plt.pcolormesh(noise[:1])\n",
        "    # plt.rcParams[\"figure.figsize\"] = (20,3)\n",
        "    # plt.show()\n",
        "    noise = torch.from_numpy(noise)\n",
        "    ctx_mask_scale = torch.rand(1) * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    trg_mask_scale = torch.rand(1) * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "    val, ind = noise.flatten(1).sort() # [b,h*w]\n",
        "    seq = hw[0]*hw[1]\n",
        "    # trg_index = ind[:,-int(seq*trg_mask_scale):]\n",
        "    # ctx_index = ind[:,-int(seq*ctx_mask_scale):-int(seq*trg_mask_scale)] # ctx wraps trg ; most similar to multiblock\n",
        "    # ctx_index = ind[:,:int(seq*ctx_mask_scale)-int(seq*trg_mask_scale)] # ctx hug bottom\n",
        "    # ctx_index = ind[:,-int(seq*ctx_mask_scale)-int(seq*trg_mask_scale):-int(seq*trg_mask_scale)] # ctx wraps trg ; most similar to multiblock\n",
        "    # ctx_index = ind[:,:int(seq*ctx_mask_scale)] # ctx hug bottom\n",
        "    return ctx_index, trg_index\n",
        "\n",
        "\n",
        "def simplexmask2d(hw=(32,32), ctx_scale=(.85,1), trg_scale=(.6,.8), B=64, chaos=2):\n",
        "    seed = np.random.randint(1e10, size=B)\n",
        "    ix = np.linspace(0, chaos, num=hw[1])\n",
        "    iy = np.linspace(0, chaos, num=hw[0])\n",
        "    noise = opensimplex.noise3array(ix, iy, seed) # [b,h,w]\n",
        "    noise = torch.from_numpy(noise)\n",
        "    ctx_mask_scale = torch.rand(1) * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    trg_mask_scale = torch.rand(1) * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "    val, ind = noise.flatten(1).sort() # [b,h*w]\n",
        "\n",
        "    seq = hw[0]*hw[1]\n",
        "\n",
        "    ctx_len = int(seq*ctx_mask_scale)\n",
        "    trg_len = int(seq*trg_mask_scale)\n",
        "    trg_pos = (torch.rand(B) * (seq-trg_len)).int()\n",
        "    # print(trg_pos)\n",
        "\n",
        "    trg_ind = trg_pos.unsqueeze(-1) + torch.arange(trg_len).unsqueeze(0)\n",
        "    trg_index = ind[torch.arange(B).unsqueeze(-1), trg_ind]\n",
        "    ctx_len = ctx_len - trg_len\n",
        "\n",
        "    remove_mask = torch.ones((B,seq), dtype=bool) # [B, S]\n",
        "    remove_mask.scatter_(1, trg_ind, False).flatten()\n",
        "    ctx_ind = torch.arange(seq).unsqueeze(0).repeat(B,1)[remove_mask].reshape(B, -1)\n",
        "    ctx_index = ind[torch.arange(B).unsqueeze(-1), ctx_ind[:,-ctx_len:]]\n",
        "    return ctx_index, trg_index\n",
        "\n",
        "\n",
        "# @title simplex\n",
        "!pip install opensimplex\n",
        "from opensimplex import OpenSimplex\n",
        "\n",
        "seed=0\n",
        "noise = OpenSimplex(seed)  # Replace 'seed' with your starting value\n",
        "noise_scale = 10  # Adjust for desired noise range\n",
        "\n",
        "def get_noise(x, y):\n",
        "    global seed  # Assuming seed is a global variable\n",
        "    # noise_val = noise.noise2(x / noise_scale, y / noise_scale)\n",
        "    noise_val = noise.noise2(x, y)\n",
        "    # seed += 1  # Increment seed after each call\n",
        "    return noise_val\n",
        "\n",
        "x,y=0,0\n",
        "a=[[],[],[],[],[]]\n",
        "\n",
        "fps=2\n",
        "f=[]\n",
        "for i in range(10*fps):\n",
        "    f.append(i/fps)\n",
        "    x = x+0.3\n",
        "    # y = y+1\n",
        "    # noise_value = get_noise(x, y)\n",
        "    for k,j in enumerate([0,1,2,3,4]):\n",
        "        a[k].append(get_noise(x, y+j))\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "for aa in a:\n",
        "    plt.plot(f,aa)\n",
        "plt.show()\n",
        "\n",
        "# b,y,g,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vtPSM8mbnJZy"
      },
      "outputs": [],
      "source": [
        "# @title pos_embed.py\n",
        "# https://github.com/facebookresearch/mae/blob/main/util/pos_embed.py#L20\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim=16, grid_size=8, cls_token=False): #\n",
        "    grid_h = np.arange(grid_size, dtype=np.float32)\n",
        "    grid_w = np.arange(grid_size, dtype=np.float32)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token: pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed # [(1+)grid_size*grid_size, embed_dim]\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    # assert embed_dim % 2 == 0\n",
        "\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1) # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos): #\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=float)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega  # (D/2,)\n",
        "\n",
        "    pos = pos.reshape(-1)  # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
        "\n",
        "    emb_sin = np.sin(out) # (M, D/2)\n",
        "    emb_cos = np.cos(out) # (M, D/2)\n",
        "\n",
        "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def interpolate_pos_embed(model, checkpoint_model):\n",
        "    if 'pos_embed' in checkpoint_model:\n",
        "        pos_embed_checkpoint = checkpoint_model['pos_embed']\n",
        "        embedding_size = pos_embed_checkpoint.shape[-1]\n",
        "        num_patches = model.patch_embed.num_patches\n",
        "        num_extra_tokens = model.pos_embed.shape[-2] - num_patches\n",
        "        # height (== width) for the checkpoint position embedding\n",
        "        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n",
        "        # height (== width) for the new position embedding\n",
        "        new_size = int(num_patches ** 0.5)\n",
        "        # class_token and dist_token are kept unchanged\n",
        "        if orig_size != new_size:\n",
        "            print(\"Position interpolate from %dx%d to %dx%d\" % (orig_size, orig_size, new_size, new_size))\n",
        "            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n",
        "            # only the position tokens are interpolated\n",
        "            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n",
        "            pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n",
        "            pos_tokens = torch.nn.functional.interpolate(\n",
        "                pos_tokens, size=(new_size, new_size), mode='bicubic', align_corners=False)\n",
        "            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n",
        "            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n",
        "            checkpoint_model['pos_embed'] = new_pos_embed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "dLbXQ-3XXRMq"
      },
      "outputs": [],
      "source": [
        "# @title ijepa src.utils.tensors\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/utils/tensors.py\n",
        "\n",
        "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
        "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n",
        "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
        "    def norm_cdf(x):\n",
        "        # Computes standard normal cumulative distribution function\n",
        "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Values are generated by using a truncated uniform distribution and\n",
        "        # then using the inverse CDF for the normal distribution.\n",
        "        # Get upper and lower cdf values\n",
        "        l = norm_cdf((a - mean) / std)\n",
        "        u = norm_cdf((b - mean) / std)\n",
        "\n",
        "        # Uniformly fill tensor with values from [l, u], then translate to\n",
        "        # [2l-1, 2u-1].\n",
        "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
        "\n",
        "        # Use inverse cdf transform for normal distribution to get truncated\n",
        "        # standard normal\n",
        "        tensor.erfinv_()\n",
        "\n",
        "        # Transform to proper mean, std\n",
        "        tensor.mul_(std * math.sqrt(2.))\n",
        "        tensor.add_(mean)\n",
        "\n",
        "        # Clamp to ensure it's in the proper range\n",
        "        tensor.clamp_(min=a, max=b)\n",
        "        return tensor\n",
        "\n",
        "\n",
        "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
        "    # type: (Tensor, float, float, float, float) -> Tensor\n",
        "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n",
        "\n",
        "def repeat_interleave_batch(x, B, repeat): # [batch*M,...]? , M?\n",
        "    N = len(x) // B\n",
        "    x = torch.cat([\n",
        "        torch.cat([x[i*B:(i+1)*B] for _ in range(repeat)], dim=0)\n",
        "        for i in range(N)\n",
        "    ], dim=0)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "COvEnBXPYth3"
      },
      "outputs": [],
      "source": [
        "# @title ijepa multiblock down\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "import math\n",
        "from multiprocessing import Value\n",
        "import torch\n",
        "_GLOBAL_SEED = 0\n",
        "\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=(224, 224),\n",
        "        patch_size=16,\n",
        "        enc_mask_scale=(0.2, 0.8),\n",
        "        pred_mask_scale=(0.2, 0.8),\n",
        "        aspect_ratio=(0.3, 3.0),\n",
        "        nenc=1,\n",
        "        npred=2,\n",
        "        min_keep=4,\n",
        "        allow_overlap=False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if not isinstance(input_size, tuple):\n",
        "            input_size = (input_size, ) * 2\n",
        "        self.patch_size = patch_size\n",
        "        self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.aspect_ratio = aspect_ratio\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "        self._itr_counter = Value('i', -1)  # collator is shared across worker processes\n",
        "\n",
        "    def step(self):\n",
        "        i = self._itr_counter\n",
        "        with i.get_lock():\n",
        "            i.value += 1\n",
        "            v = i.value\n",
        "        return v\n",
        "\n",
        "    def _sample_block_size(self, generator, scale, aspect_ratio_scale):\n",
        "        _rand = torch.rand(1, generator=generator).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.height * self.width * mask_scale)\n",
        "        # -- Sample block aspect-ratio\n",
        "        min_ar, max_ar = aspect_ratio_scale\n",
        "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
        "        while h >= self.height:\n",
        "            h -= 1\n",
        "        while w >= self.width:\n",
        "            w -= 1\n",
        "        return (h, w)\n",
        "\n",
        "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
        "        h, w = b_size\n",
        "\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # --\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            top = torch.randint(0, self.height - h, (1,))\n",
        "            left = torch.randint(0, self.width - w, (1,))\n",
        "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
        "            mask[top:top+h, left:left+w] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
        "        mask_complement[top:top+h, left:left+w] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        B = len(batch)\n",
        "\n",
        "        collated_batch = torch.utils.data.default_collate(batch)\n",
        "\n",
        "        seed = self.step()\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(seed)\n",
        "        p_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.pred_mask_scale,\n",
        "            aspect_ratio_scale=self.aspect_ratio)\n",
        "        e_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.enc_mask_scale,\n",
        "            aspect_ratio_scale=(1., 1.))\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.height * self.width\n",
        "        min_keep_enc = self.height * self.width\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                print(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "        return collated_batch, collated_masks_enc, collated_masks_pred\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aH1UOD4WW8I2",
        "outputId": "e8671cd3-eba7-4649-84ca-94c6467ce62d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Mask generator says: \"Valid mask not found, decreasing acceptable-regions [1]\"\n",
            "WARNING:root:Mask generator says: \"Valid mask not found, decreasing acceptable-regions [2]\"\n",
            "WARNING:root:Mask generator says: \"Valid mask not found, decreasing acceptable-regions [3]\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vit fwd torch.Size([4, 3, 224, 224])\n",
            "vit fwd torch.Size([4, 196, 768])\n",
            "vit fwd torch.Size([4, 11, 768])\n",
            "predictor fwd1 torch.Size([4, 11, 384]) torch.Size([4, 196, 384])\n"
          ]
        }
      ],
      "source": [
        "# @title ijepa vision_transformer.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/models/vision_transformer.py\n",
        "import math\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# from src.utils.tensors import (trunc_normal_, repeat_interleave_batch)\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid height and width\n",
        "    return:\n",
        "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid_h = np.arange(grid_size, dtype=float)\n",
        "    grid_w = np.arange(grid_size, dtype=float)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0) # [1+h*w, dim]\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    assert embed_dim % 2 == 0\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid length\n",
        "    return:\n",
        "    pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid = np.arange(grid_size, dtype=float)\n",
        "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=float)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega   # (D/2,)\n",
        "    pos = pos.reshape(-1)   # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
        "    emb = np.concatenate([np.sin(out), np.cos(out)], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\"\"\"\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x, attn\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x, return_attention=False):\n",
        "        y, attn = self.attn(self.norm1(x))\n",
        "        if return_attention:\n",
        "            return attn\n",
        "        x = x + self.drop_path(y)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2) # ->[b,h*w,c]\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvEmbed(nn.Module):\n",
        "    \"\"\"3x3 Convolution stems for ViT following ViTC models\"\"\"\n",
        "    def __init__(self, channels, strides, img_size=224, in_chans=3, batch_norm=True):\n",
        "        super().__init__()\n",
        "        # Build the stems\n",
        "        stem = []\n",
        "        channels = [in_chans] + channels\n",
        "        for i in range(len(channels) - 2):\n",
        "            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3, stride=strides[i], padding=1, bias=(not batch_norm))]\n",
        "            if batch_norm:\n",
        "                stem += [nn.BatchNorm2d(channels[i+1])]\n",
        "            stem += [nn.ReLU(inplace=True)]\n",
        "        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n",
        "        self.stem = nn.Sequential(*stem)\n",
        "\n",
        "        # Comptute the number of patches\n",
        "        stride_prod = int(np.prod(strides))\n",
        "        self.num_patches = (img_size[0] // stride_prod)**2\n",
        "\n",
        "    def forward(self, x):\n",
        "        p = self.stem(x)\n",
        "        return p.flatten(2).transpose(1, 2)\n",
        "\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "def apply_masks(x, masks):\n",
        "    \"\"\":param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
        "    :param masks: list of tensors containing indices of patches in [N] to keep\"\"\"\n",
        "    all_x = []\n",
        "    for m in masks:\n",
        "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [batch,T,dim]\n",
        "        all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "    return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "\n",
        "class VisionTransformerPredictor(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, num_patches, embed_dim=768, predictor_embed_dim=384,\n",
        "        depth=6, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.init_std = init_std\n",
        "        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        # --\n",
        "        self.predictor_pos_embed = nn.Parameter(torch.zeros(1, num_patches, predictor_embed_dim), requires_grad=False)\n",
        "        predictor_pos_embed = get_2d_sincos_pos_embed(self.predictor_pos_embed.shape[-1], int(num_patches**.5), cls_token=False)\n",
        "        self.predictor_pos_embed.data.copy_(torch.from_numpy(predictor_pos_embed).float().unsqueeze(0)) # [1, num_patches, predictor_embed_dim]\n",
        "        # --\n",
        "        self.predictor_blocks = nn.ModuleList([Block(dim=predictor_embed_dim, num_heads=num_heads, drop=drop_rate, drop_path=dpr[i]) for i in range(depth)])\n",
        "        self.predictor_norm = nn.LayerNorm(predictor_embed_dim)\n",
        "        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n",
        "        # ------\n",
        "        # trunc_normal_(self.mask_token, std=self.init_std)\n",
        "\n",
        "    def forward(self, x, masks_x, masks): # [batch, num_context_patches, embed_dim], nenc*[batch, num_context_patches], npred*[batch, num_target_patches]\n",
        "        assert (masks is not None) and (masks_x is not None), 'Cannot run predictor without mask indices'\n",
        "        if not isinstance(masks_x, list): masks_x = [masks_x] # context mask\n",
        "        if not isinstance(masks, list): masks = [masks] # pred mask\n",
        "\n",
        "        # -- Batch Size\n",
        "        B = len(x) // len(masks_x)\n",
        "        # print(\"predictor fwd0\", x.shape) # [3, 121, 768])\n",
        "        # -- map from encoder-dim to pedictor-dim\n",
        "        x = self.predictor_embed(x) # [batch, num_patches, predictor_embed_dim]\n",
        "\n",
        "        # -- add positional embedding to x tokens\n",
        "        x_pos_embed = self.predictor_pos_embed.repeat(B, 1, 1) # [batch, num_patches, predictor_embed_dim]\n",
        "        print(\"predictor fwd1\", x.shape, x_pos_embed.shape) # [3, 121 or 11, 384], [3, 196, 384]\n",
        "        # print(\"predictor fwd masks_x\", masks_x)\n",
        "        x += apply_masks(x_pos_embed, masks_x) # get pos emb of context patches\n",
        "\n",
        "        # print(\"predictor fwd x\", x.shape) # [4, 11, 384])\n",
        "        _, N_ctxt, D = x.shape\n",
        "\n",
        "        # -- concat mask tokens to x\n",
        "        pos_embs = self.predictor_pos_embed.repeat(B, 1, 1) # [batch, num_patches, predictor_embed_dim]\n",
        "        pos_embs = apply_masks(pos_embs, masks) # [16, 104, predictor_embed_dim]\n",
        "        pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x)) # [16, 104, predictor_embed_dim]\n",
        "        # --\n",
        "        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1) # [16, 104, predictor_embed_dim]\n",
        "        # --\n",
        "        # print(\"predictor fwd pred_tokens\", pred_tokens.shape)\n",
        "\n",
        "        pred_tokens += pos_embs\n",
        "        x = x.repeat(len(masks), 1, 1)\n",
        "        x = torch.cat([x, pred_tokens], dim=1)\n",
        "\n",
        "        # -- fwd prop\n",
        "        for blk in self.predictor_blocks:\n",
        "            x = blk(x)\n",
        "        x = self.predictor_norm(x)\n",
        "\n",
        "        # -- return preds for mask tokens\n",
        "        x = x[:, N_ctxt:]\n",
        "        x = self.predictor_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, img_size=[224], patch_size=16, in_chans=3, embed_dim=768, predictor_embed_dim=384, depth=12, predictor_depth=12,\n",
        "        num_heads=12, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0, norm_layer=nn.LayerNorm, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_features = self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.init_std = init_std\n",
        "        # --\n",
        "        self.patch_embed = PatchEmbed(img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        # --\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=False)\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "        # --\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "    def forward(self, x, masks=None): # [batch,3,224,224]\n",
        "        if masks is not None:\n",
        "            if not isinstance(masks, list): masks = [masks]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        x = self.patch_embed(x)\n",
        "        B, N, D = x.shape # [batch, num_patches, embed_dim]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        # -- add positional embedding to x\n",
        "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
        "        x = x + pos_embed\n",
        "\n",
        "        if masks is not None: x = apply_masks(x, masks) # [batch, num_context_patches, embed_dim]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            x = blk(x)\n",
        "\n",
        "        if self.norm is not None: x = self.norm(x)\n",
        "        return x # [batch, num_context_patches, embed_dim]\n",
        "\n",
        "    def interpolate_pos_encoding(self, x, pos_embed):\n",
        "        npatch = x.shape[1] - 1\n",
        "        N = pos_embed.shape[1] - 1\n",
        "        if npatch == N: return pos_embed\n",
        "        class_emb, pos_embed = pos_embed[:, 0], pos_embed[:, 1:]\n",
        "        dim = x.shape[-1]\n",
        "        pos_embed = nn.functional.interpolate(pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2), scale_factor=math.sqrt(npatch / N), mode='bicubic',)\n",
        "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
        "\n",
        "# from functools import partial\n",
        "# def vit_predictor(**kwargs):\n",
        "#     model = VisionTransformerPredictor(\n",
        "#         mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "#     return model\n",
        "\n",
        "def vit(patch_size=16, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, # tiny\n",
        "        # patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, # small\n",
        "        # patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, # base\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "# encoder = vit()\n",
        "encoder = VisionTransformer(\n",
        "        patch_size=16, embed_dim=768, depth=1, num_heads=3, mlp_ratio=1,\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
        "\n",
        "# num_patches = (224/patch_size)^2\n",
        "# model = VisionTransformerPredictor(num_patches, embed_dim=768, predictor_embed_dim=384, depth=6, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02, **kwargs)\n",
        "model = VisionTransformerPredictor(num_patches=196, embed_dim=768, predictor_embed_dim=384, depth=1, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02)\n",
        "\n",
        "batch = 4\n",
        "img = torch.rand(batch, 3, 224, 224)\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/train.py\n",
        "mask_collator = MaskCollator(\n",
        "        input_size=(224, 224),\n",
        "        patch_size=16,\n",
        "        enc_mask_scale=(0.2, 0.8),\n",
        "        pred_mask_scale=(0.2, 0.8),\n",
        "        aspect_ratio=(0.3, 3.0),\n",
        "        nenc=1,\n",
        "        npred=4,\n",
        "        min_keep=4,\n",
        "        # allow_overlap=True)\n",
        "        allow_overlap=False)\n",
        "# self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
        "\n",
        "\n",
        "\n",
        "# v = mask_collator.step()\n",
        "# should pass the collater a list batch of idx?\n",
        "# collated_batch, collated_masks_enc, collated_masks_pred = mask_collator([batch,3,8])\n",
        "collated_batch, collated_masks_enc, collated_masks_pred = mask_collator([0,1,2,3])\n",
        "# print(v)\n",
        "\n",
        "\n",
        "def forward_target():\n",
        "    with torch.no_grad():\n",
        "        h = target_encoder(imgs)\n",
        "        h = F.layer_norm(h, (h.size(-1),))  # normalize over feature-dim\n",
        "        B = len(h)\n",
        "        # -- create targets (masked regions of h)\n",
        "        h = apply_masks(h, masks_pred)\n",
        "        h = repeat_interleave_batch(h, B, repeat=len(masks_enc))\n",
        "        return h\n",
        "\n",
        "def forward_context():\n",
        "    z = encoder(imgs, masks_enc)\n",
        "    z = predictor(z, masks_enc, masks_pred)\n",
        "    return z\n",
        "\n",
        "# # num_context_patches = 121 if allow_overlap=True else 11\n",
        "z = encoder(img, collated_masks_enc) # [batch, num_context_patches, embed_dim]\n",
        "# print(len(collated_masks_enc), len(collated_masks_pred)) # nenc, npred\n",
        "# print(collated_masks_enc[0].shape, collated_masks_pred[0].shape) # , [batch, num_context_patches = 121 or 11], [batch, num_target_patches]\n",
        "out = model(z, collated_masks_enc, collated_masks_pred)\n",
        "# # print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Or3eQvUVg7l_"
      },
      "outputs": [],
      "source": [
        "\n",
        "# print(224/16) # 14\n",
        "# print(len(collated_masks_enc), len(collated_masks_pred))\n",
        "print(collated_masks_enc, collated_masks_pred)\n",
        "# multiples of 14\n",
        "\n",
        "# print(collated_masks_pred[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2mLjXAmXcwJ"
      },
      "outputs": [],
      "source": [
        "print(collated_masks_enc, collated_masks_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XQ7PxBMlsYCI"
      },
      "outputs": [],
      "source": [
        "# @title ijepa multiblock.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "import math\n",
        "from multiprocessing import Value\n",
        "from logging import getLogger\n",
        "import torch\n",
        "_GLOBAL_SEED = 0\n",
        "logger = getLogger()\n",
        "\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=(224, 224),\n",
        "        patch_size=16,\n",
        "        enc_mask_scale=(0.2, 0.8),\n",
        "        pred_mask_scale=(0.2, 0.8),\n",
        "        aspect_ratio=(0.3, 3.0),\n",
        "        nenc=1,\n",
        "        npred=2,\n",
        "        min_keep=4,\n",
        "        allow_overlap=False\n",
        "    ):\n",
        "        super(MaskCollator, self).__init__()\n",
        "        if not isinstance(input_size, tuple):\n",
        "            input_size = (input_size, ) * 2\n",
        "        self.patch_size = patch_size\n",
        "        self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.aspect_ratio = aspect_ratio\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "        self._itr_counter = Value('i', -1)  # collator is shared across worker processes\n",
        "\n",
        "    def step(self):\n",
        "        i = self._itr_counter\n",
        "        with i.get_lock():\n",
        "            i.value += 1\n",
        "            v = i.value\n",
        "        return v\n",
        "\n",
        "    def _sample_block_size(self, generator, scale, aspect_ratio_scale):\n",
        "        _rand = torch.rand(1, generator=generator).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.height * self.width * mask_scale)\n",
        "        # -- Sample block aspect-ratio\n",
        "        min_ar, max_ar = aspect_ratio_scale\n",
        "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
        "        while h >= self.height:\n",
        "            h -= 1\n",
        "        while w >= self.width:\n",
        "            w -= 1\n",
        "\n",
        "        return (h, w)\n",
        "\n",
        "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
        "        h, w = b_size\n",
        "\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # --\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            top = torch.randint(0, self.height - h, (1,))\n",
        "            left = torch.randint(0, self.width - w, (1,))\n",
        "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
        "            mask[top:top+h, left:left+w] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "                    logger.warning(f'Mask generator says: \"Valid mask not found, decreasing acceptable-regions [{tries}]\"')\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
        "        mask_complement[top:top+h, left:left+w] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        B = len(batch)\n",
        "\n",
        "        collated_batch = torch.utils.data.default_collate(batch)\n",
        "\n",
        "        seed = self.step()\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(seed)\n",
        "        p_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.pred_mask_scale,\n",
        "            aspect_ratio_scale=self.aspect_ratio)\n",
        "        e_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.enc_mask_scale,\n",
        "            aspect_ratio_scale=(1., 1.))\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.height * self.width\n",
        "        min_keep_enc = self.height * self.width\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                logger.warning(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "\n",
        "        return collated_batch, collated_masks_enc, collated_masks_pred\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "iVUPGP93dpAN"
      },
      "outputs": [],
      "source": [
        "# @title ijepa train.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/train.py\n",
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "#\n",
        "\n",
        "import os\n",
        "\n",
        "# -- FOR DISTRIBUTED TRAINING ENSURE ONLY 1 DEVICE VISIBLE PER PROCESS\n",
        "try:\n",
        "    # -- WARNING: IF DOING DISTRIBUTED TRAINING ON A NON-SLURM CLUSTER, MAKE\n",
        "    # --          SURE TO UPDATE THIS TO GET LOCAL-RANK ON NODE, OR ENSURE\n",
        "    # --          THAT YOUR JOBS ARE LAUNCHED WITH ONLY 1 DEVICE VISIBLE\n",
        "    # --          TO EACH PROCESS\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = os.environ['SLURM_LOCALID']\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "import copy\n",
        "import logging\n",
        "import sys\n",
        "import yaml\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.multiprocessing as mp\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "\n",
        "from src.masks.multiblock import MaskCollator as MBMaskCollator\n",
        "from src.masks.utils import apply_masks\n",
        "from src.utils.distributed import (\n",
        "    init_distributed,\n",
        "    AllReduce\n",
        ")\n",
        "from src.utils.logging import (\n",
        "    CSVLogger,\n",
        "    gpu_timer,\n",
        "    grad_logger,\n",
        "    AverageMeter)\n",
        "from src.utils.tensors import repeat_interleave_batch\n",
        "from src.datasets.imagenet1k import make_imagenet1k\n",
        "\n",
        "from src.helper import (\n",
        "    load_checkpoint,\n",
        "    init_model,\n",
        "    init_opt)\n",
        "from src.transforms import make_transforms\n",
        "\n",
        "# --\n",
        "log_timings = True\n",
        "log_freq = 10\n",
        "checkpoint_freq = 50\n",
        "# --\n",
        "\n",
        "_GLOBAL_SEED = 0\n",
        "np.random.seed(_GLOBAL_SEED)\n",
        "torch.manual_seed(_GLOBAL_SEED)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logger = logging.getLogger()\n",
        "\n",
        "\n",
        "def main(args, resume_preempt=False):\n",
        "\n",
        "    # ----------------------------------------------------------------------- #\n",
        "    #  PASSED IN PARAMS FROM CONFIG FILE\n",
        "    # ----------------------------------------------------------------------- #\n",
        "\n",
        "    # -- META\n",
        "    use_bfloat16 = args['meta']['use_bfloat16']\n",
        "    model_name = args['meta']['model_name']\n",
        "    load_model = args['meta']['load_checkpoint'] or resume_preempt\n",
        "    r_file = args['meta']['read_checkpoint']\n",
        "    copy_data = args['meta']['copy_data']\n",
        "    pred_depth = args['meta']['pred_depth']\n",
        "    pred_emb_dim = args['meta']['pred_emb_dim']\n",
        "    if not torch.cuda.is_available():\n",
        "        device = torch.device('cpu')\n",
        "    else:\n",
        "        device = torch.device('cuda:0')\n",
        "        torch.cuda.set_device(device)\n",
        "\n",
        "    # -- DATA\n",
        "    use_gaussian_blur = args['data']['use_gaussian_blur']\n",
        "    use_horizontal_flip = args['data']['use_horizontal_flip']\n",
        "    use_color_distortion = args['data']['use_color_distortion']\n",
        "    color_jitter = args['data']['color_jitter_strength']\n",
        "    # --\n",
        "    batch_size = args['data']['batch_size']\n",
        "    pin_mem = args['data']['pin_mem']\n",
        "    num_workers = args['data']['num_workers']\n",
        "    root_path = args['data']['root_path']\n",
        "    image_folder = args['data']['image_folder']\n",
        "    crop_size = args['data']['crop_size']\n",
        "    crop_scale = args['data']['crop_scale']\n",
        "    # --\n",
        "\n",
        "    # -- MASK\n",
        "    allow_overlap = args['mask']['allow_overlap']  # whether to allow overlap b/w context and target blocks\n",
        "    patch_size = args['mask']['patch_size']  # patch-size for model training\n",
        "    num_enc_masks = args['mask']['num_enc_masks']  # number of context blocks\n",
        "    min_keep = args['mask']['min_keep']  # min number of patches in context block\n",
        "    enc_mask_scale = args['mask']['enc_mask_scale']  # scale of context blocks\n",
        "    num_pred_masks = args['mask']['num_pred_masks']  # number of target blocks\n",
        "    pred_mask_scale = args['mask']['pred_mask_scale']  # scale of target blocks\n",
        "    aspect_ratio = args['mask']['aspect_ratio']  # aspect ratio of target blocks\n",
        "    # --\n",
        "\n",
        "    # -- OPTIMIZATION\n",
        "    ema = args['optimization']['ema']\n",
        "    ipe_scale = args['optimization']['ipe_scale']  # scheduler scale factor (def: 1.0)\n",
        "    wd = float(args['optimization']['weight_decay'])\n",
        "    final_wd = float(args['optimization']['final_weight_decay'])\n",
        "    num_epochs = args['optimization']['epochs']\n",
        "    warmup = args['optimization']['warmup']\n",
        "    start_lr = args['optimization']['start_lr']\n",
        "    lr = args['optimization']['lr']\n",
        "    final_lr = args['optimization']['final_lr']\n",
        "\n",
        "    # -- LOGGING\n",
        "    folder = args['logging']['folder']\n",
        "    tag = args['logging']['write_tag']\n",
        "\n",
        "    dump = os.path.join(folder, 'params-ijepa.yaml')\n",
        "    with open(dump, 'w') as f:\n",
        "        yaml.dump(args, f)\n",
        "    # ----------------------------------------------------------------------- #\n",
        "\n",
        "    try:\n",
        "        mp.set_start_method('spawn')\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # -- init torch distributed backend\n",
        "    world_size, rank = init_distributed()\n",
        "    logger.info(f'Initialized (rank/world-size) {rank}/{world_size}')\n",
        "    if rank > 0:\n",
        "        logger.setLevel(logging.ERROR)\n",
        "\n",
        "    # -- log/checkpointing paths\n",
        "    log_file = os.path.join(folder, f'{tag}_r{rank}.csv')\n",
        "    save_path = os.path.join(folder, f'{tag}' + '-ep{epoch}.pth.tar')\n",
        "    latest_path = os.path.join(folder, f'{tag}-latest.pth.tar')\n",
        "    load_path = None\n",
        "    if load_model:\n",
        "        load_path = os.path.join(folder, r_file) if r_file is not None else latest_path\n",
        "\n",
        "    # -- make csv_logger\n",
        "    csv_logger = CSVLogger(log_file,\n",
        "                           ('%d', 'epoch'),\n",
        "                           ('%d', 'itr'),\n",
        "                           ('%.5f', 'loss'),\n",
        "                           ('%.5f', 'mask-A'),\n",
        "                           ('%.5f', 'mask-B'),\n",
        "                           ('%d', 'time (ms)'))\n",
        "\n",
        "    # -- init model\n",
        "    encoder, predictor = init_model(\n",
        "        device=device,\n",
        "        patch_size=patch_size,\n",
        "        crop_size=crop_size,\n",
        "        pred_depth=pred_depth,\n",
        "        pred_emb_dim=pred_emb_dim,\n",
        "        model_name=model_name)\n",
        "    target_encoder = copy.deepcopy(encoder)\n",
        "\n",
        "    # -- make data transforms\n",
        "    mask_collator = MBMaskCollator(\n",
        "        input_size=crop_size,\n",
        "        patch_size=patch_size,\n",
        "        pred_mask_scale=pred_mask_scale,\n",
        "        enc_mask_scale=enc_mask_scale,\n",
        "        aspect_ratio=aspect_ratio,\n",
        "        nenc=num_enc_masks,\n",
        "        npred=num_pred_masks,\n",
        "        allow_overlap=allow_overlap,\n",
        "        min_keep=min_keep)\n",
        "\n",
        "    transform = make_transforms(\n",
        "        crop_size=crop_size,\n",
        "        crop_scale=crop_scale,\n",
        "        gaussian_blur=use_gaussian_blur,\n",
        "        horizontal_flip=use_horizontal_flip,\n",
        "        color_distortion=use_color_distortion,\n",
        "        color_jitter=color_jitter)\n",
        "\n",
        "    # -- init data-loaders/samplers\n",
        "    _, unsupervised_loader, unsupervised_sampler = make_imagenet1k(\n",
        "            transform=transform,\n",
        "            batch_size=batch_size,\n",
        "            collator=mask_collator,\n",
        "            pin_mem=pin_mem,\n",
        "            training=True,\n",
        "            num_workers=num_workers,\n",
        "            world_size=world_size,\n",
        "            rank=rank,\n",
        "            root_path=root_path,\n",
        "            image_folder=image_folder,\n",
        "            copy_data=copy_data,\n",
        "            drop_last=True)\n",
        "    ipe = len(unsupervised_loader)\n",
        "\n",
        "    # -- init optimizer and scheduler\n",
        "    optimizer, scaler, scheduler, wd_scheduler = init_opt(\n",
        "        encoder=encoder,\n",
        "        predictor=predictor,\n",
        "        wd=wd,\n",
        "        final_wd=final_wd,\n",
        "        start_lr=start_lr,\n",
        "        ref_lr=lr,\n",
        "        final_lr=final_lr,\n",
        "        iterations_per_epoch=ipe,\n",
        "        warmup=warmup,\n",
        "        num_epochs=num_epochs,\n",
        "        ipe_scale=ipe_scale,\n",
        "        use_bfloat16=use_bfloat16)\n",
        "    encoder = DistributedDataParallel(encoder, static_graph=True)\n",
        "    predictor = DistributedDataParallel(predictor, static_graph=True)\n",
        "    target_encoder = DistributedDataParallel(target_encoder)\n",
        "    for p in target_encoder.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    # -- momentum schedule\n",
        "    momentum_scheduler = (ema[0] + i*(ema[1]-ema[0])/(ipe*num_epochs*ipe_scale)\n",
        "                          for i in range(int(ipe*num_epochs*ipe_scale)+1))\n",
        "\n",
        "    start_epoch = 0\n",
        "    # -- load training checkpoint\n",
        "    if load_model:\n",
        "        encoder, predictor, target_encoder, optimizer, scaler, start_epoch = load_checkpoint(\n",
        "            device=device,\n",
        "            r_path=load_path,\n",
        "            encoder=encoder,\n",
        "            predictor=predictor,\n",
        "            target_encoder=target_encoder,\n",
        "            opt=optimizer,\n",
        "            scaler=scaler)\n",
        "        for _ in range(start_epoch*ipe):\n",
        "            scheduler.step()\n",
        "            wd_scheduler.step()\n",
        "            next(momentum_scheduler)\n",
        "            mask_collator.step()\n",
        "\n",
        "    def save_checkpoint(epoch):\n",
        "        save_dict = {\n",
        "            'encoder': encoder.state_dict(),\n",
        "            'predictor': predictor.state_dict(),\n",
        "            'target_encoder': target_encoder.state_dict(),\n",
        "            'opt': optimizer.state_dict(),\n",
        "            'scaler': None if scaler is None else scaler.state_dict(),\n",
        "            'epoch': epoch,\n",
        "            'loss': loss_meter.avg,\n",
        "            'batch_size': batch_size,\n",
        "            'world_size': world_size,\n",
        "            'lr': lr\n",
        "        }\n",
        "        if rank == 0:\n",
        "            torch.save(save_dict, latest_path)\n",
        "            if (epoch + 1) % checkpoint_freq == 0:\n",
        "                torch.save(save_dict, save_path.format(epoch=f'{epoch + 1}'))\n",
        "\n",
        "    # -- TRAINING LOOP\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        logger.info('Epoch %d' % (epoch + 1))\n",
        "\n",
        "        # -- update distributed-data-loader epoch\n",
        "        unsupervised_sampler.set_epoch(epoch)\n",
        "\n",
        "        loss_meter = AverageMeter()\n",
        "        maskA_meter = AverageMeter()\n",
        "        maskB_meter = AverageMeter()\n",
        "        time_meter = AverageMeter()\n",
        "\n",
        "        for itr, (udata, masks_enc, masks_pred) in enumerate(unsupervised_loader):\n",
        "\n",
        "            def load_imgs():\n",
        "                # -- unsupervised imgs\n",
        "                imgs = udata[0].to(device, non_blocking=True)\n",
        "                masks_1 = [u.to(device, non_blocking=True) for u in masks_enc]\n",
        "                masks_2 = [u.to(device, non_blocking=True) for u in masks_pred]\n",
        "                return (imgs, masks_1, masks_2)\n",
        "            imgs, masks_enc, masks_pred = load_imgs()\n",
        "            maskA_meter.update(len(masks_enc[0][0]))\n",
        "            maskB_meter.update(len(masks_pred[0][0]))\n",
        "\n",
        "            def train_step():\n",
        "                _new_lr = scheduler.step()\n",
        "                _new_wd = wd_scheduler.step()\n",
        "                # --\n",
        "\n",
        "                def forward_target():\n",
        "                    with torch.no_grad():\n",
        "                        h = target_encoder(imgs)\n",
        "                        h = F.layer_norm(h, (h.size(-1),))  # normalize over feature-dim\n",
        "                        B = len(h)\n",
        "                        # -- create targets (masked regions of h)\n",
        "                        h = apply_masks(h, masks_pred)\n",
        "                        h = repeat_interleave_batch(h, B, repeat=len(masks_enc))\n",
        "                        return h\n",
        "\n",
        "                def forward_context():\n",
        "                    z = encoder(imgs, masks_enc)\n",
        "                    z = predictor(z, masks_enc, masks_pred)\n",
        "                    return z\n",
        "\n",
        "                def loss_fn(z, h):\n",
        "                    loss = F.smooth_l1_loss(z, h)\n",
        "                    loss = AllReduce.apply(loss)\n",
        "                    return loss\n",
        "\n",
        "                # Step 1. Forward\n",
        "                with torch.cuda.amp.autocast(dtype=torch.bfloat16, enabled=use_bfloat16):\n",
        "                    h = forward_target()\n",
        "                    z = forward_context()\n",
        "                    loss = loss_fn(z, h)\n",
        "\n",
        "                #  Step 2. Backward & step\n",
        "                if use_bfloat16:\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                else:\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                grad_stats = grad_logger(encoder.named_parameters())\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Step 3. momentum update of target encoder\n",
        "                with torch.no_grad():\n",
        "                    m = next(momentum_scheduler)\n",
        "                    for param_q, param_k in zip(encoder.parameters(), target_encoder.parameters()):\n",
        "                        param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "\n",
        "                return (float(loss), _new_lr, _new_wd, grad_stats)\n",
        "            (loss, _new_lr, _new_wd, grad_stats), etime = gpu_timer(train_step)\n",
        "            loss_meter.update(loss)\n",
        "            time_meter.update(etime)\n",
        "\n",
        "            # -- Logging\n",
        "            def log_stats():\n",
        "                csv_logger.log(epoch + 1, itr, loss, maskA_meter.val, maskB_meter.val, etime)\n",
        "                if (itr % log_freq == 0) or np.isnan(loss) or np.isinf(loss):\n",
        "                    logger.info('[%d, %5d] loss: %.3f '\n",
        "                                'masks: %.1f %.1f '\n",
        "                                '[wd: %.2e] [lr: %.2e] '\n",
        "                                '[mem: %.2e] '\n",
        "                                '(%.1f ms)'\n",
        "                                % (epoch + 1, itr,\n",
        "                                   loss_meter.avg,\n",
        "                                   maskA_meter.avg,\n",
        "                                   maskB_meter.avg,\n",
        "                                   _new_wd,\n",
        "                                   _new_lr,\n",
        "                                   torch.cuda.max_memory_allocated() / 1024.**2,\n",
        "                                   time_meter.avg))\n",
        "\n",
        "                    if grad_stats is not None:\n",
        "                        logger.info('[%d, %5d] grad_stats: [%.2e %.2e] (%.2e, %.2e)'\n",
        "                                    % (epoch + 1, itr,\n",
        "                                       grad_stats.first_layer,\n",
        "                                       grad_stats.last_layer,\n",
        "                                       grad_stats.min,\n",
        "                                       grad_stats.max))\n",
        "\n",
        "            log_stats()\n",
        "\n",
        "            assert not np.isnan(loss), 'loss is nan'\n",
        "\n",
        "        # -- Save Checkpoint after every epoch\n",
        "        logger.info('avg. loss %.3f' % loss_meter.avg)\n",
        "        save_checkpoint(epoch+1)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "s3EO3PgMPH1x"
      ],
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}