{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/I_JEPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxACli7GdyGq",
        "outputId": "464470aa-5c1c-4200-ce90-692c4f998223"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 61.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title data\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "transform = transforms.Compose([transforms.ToTensor(),])\n",
        "# transform = transforms.Compose([transforms.RandomResizedCrop((32,32), scale=(.3,1.)), transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "# CIFAR10: 60000 32x32 color images in 10 classes, with 6000 images per class\n",
        "train_data = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
        "test_data = torchvision.datasets.CIFAR10(root='data', train=False, download=True, transform=transform)\n",
        "batch_size = 64 # 4\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# dataiter = iter(train_loader) # get some random training images\n",
        "# images, labels = next(dataiter)\n",
        "# print(images.shape) # [batch, 3, 32, 32]\n",
        "# imshow(torchvision.utils.make_grid(images))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3EO3PgMPH1x"
      },
      "source": [
        "## hiera"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "j3-vvMS1-gVn"
      },
      "outputs": [],
      "source": [
        "# @title ResBlock\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters(): p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, kernel=3):\n",
        "        super().__init__()\n",
        "        out_ch = out_ch or in_ch\n",
        "        act = nn.SiLU() #\n",
        "        self.res_conv = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
        "        # self.block = nn.Sequential( # best?\n",
        "        #     nn.Conv2d(in_ch, out_ch, 3, padding=1), nn.BatchNorm2d(out_ch), act,\n",
        "        #     zero_module(nn.Conv2d(out_ch, out_ch, 3, padding=1)), nn.BatchNorm2d(out_ch), act,\n",
        "        #     )\n",
        "        self.block = nn.Sequential(\n",
        "            nn.BatchNorm2d(in_ch), act, nn.Conv2d(in_ch, out_ch, kernel, padding=kernel//2),\n",
        "            nn.BatchNorm2d(out_ch), act, zero_module(nn.Conv2d(out_ch, out_ch, kernel, padding=kernel//2)),\n",
        "            )\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w]\n",
        "        return self.block(x) + self.res_conv(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0lclc9myo2c",
        "outputId": "35ea2b51-c2dc-4d96-f2ae-c27bd760dcd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([12, 2, 16, 32, 32])\n"
          ]
        }
      ],
      "source": [
        "# @title UpDownBlock_me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class PixelShuffleConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, kernel=1, r=1):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        r = max(r, int(1/r))\n",
        "        out_ch = out_ch or in_ch\n",
        "        if self.r>1: self.net = nn.Sequential(ResBlock(in_ch, out_ch*r**2, kernel), nn.PixelShuffle(r))\n",
        "        # if self.r>1: self.net = nn.Sequential(Attention(in_ch, out_ch*r**2), nn.PixelShuffle(r))\n",
        "\n",
        "        elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), ResBlock(in_ch*r**2, out_ch, kernel))\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), Attention(in_ch*r**2, out_ch))\n",
        "        elif in_ch != out_ch: self.net = ResBlock(in_ch*r**2, out_ch, kernel)\n",
        "        else: self.net = lambda x: torch.zeros_like(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def AdaptiveAvgPool_nd(n, *args, **kwargs): return [nn.Identity, nn.AdaptiveAvgPool1d, nn.AdaptiveAvgPool2d, nn.AdaptiveAvgPool3d][n](*args, **kwargs)\n",
        "def AdaptiveMaxPool_nd(n, *args, **kwargs): return [nn.Identity, nn.AdaptiveMaxPool1d, nn.AdaptiveMaxPool2d, nn.AdaptiveMaxPool3d][n](*args, **kwargs)\n",
        "\n",
        "def adaptive_avg_pool_nd(n, x, output_size): return [nn.Identity, F.adaptive_avg_pool1d, F.adaptive_avg_pool2d, F.adaptive_avg_pool3d][n](x, output_size)\n",
        "def adaptive_max_pool_nd(n, x, output_size): return [nn.Identity, F.adaptive_max_pool1d, F.adaptive_max_pool2d, F.adaptive_max_pool3d][n](x, output_size)\n",
        "\n",
        "class AdaptivePool_at(nn.AdaptiveAvgPool1d): # AdaptiveAvgPool1d AdaptiveMaxPool1d\n",
        "    def __init__(self, dim=1, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.dim=dim\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(self.dim,-1)\n",
        "        shape = x.shape\n",
        "        return super().forward(x.flatten(0,-2)).unflatten(0, shape[:-1]).transpose(self.dim,-1)\n",
        "\n",
        "\n",
        "def adaptive_pool_at(x, dim, output_size, pool='avg'): # [b,c,h,w]\n",
        "    x = x.transpose(dim,-1)\n",
        "    shape = x.shape\n",
        "    parent={'avg':F.adaptive_avg_pool1d, 'max':F.adaptive_max_pool1d}[pool]\n",
        "    return parent(x.flatten(0,-2), output_size).unflatten(0, shape[:-1]).transpose(dim,-1)\n",
        "\n",
        "\n",
        "class ZeroExtend():\n",
        "    def __init__(self, dim=1, output_size=16):\n",
        "        self.dim, self.out = dim, output_size\n",
        "    def __call__(self, x): # [b,c,h,w]\n",
        "        return torch.cat((x, torch.zeros(*x.shape[:self.dim], self.out - x.shape[self.dim], *x.shape[self.dim+1:])), dim=self.dim)\n",
        "\n",
        "def make_pool_at(pool='avg', dim=1, output_size=5):\n",
        "    parent={'avg':nn.AdaptiveAvgPool1d, 'max':nn.AdaptiveMaxPool1d}[pool]\n",
        "    class AdaptivePool_at(parent): # AdaptiveAvgPool1d AdaptiveMaxPool1d\n",
        "        def __init__(self, dim=1, *args, **kwargs):\n",
        "            super().__init__(*args, **kwargs)\n",
        "            self.dim=dim\n",
        "        def forward(self, x):\n",
        "            x = x.transpose(self.dim,-1)\n",
        "            shape = x.shape\n",
        "            return super().forward(x.flatten(0,-2)).unflatten(0, shape[:-1]).transpose(self.dim,-1)\n",
        "    return AdaptivePool_at(dim, output_size=output_size)\n",
        "\n",
        "class Shortcut():\n",
        "    def __init__(self, dim=1, c=3, sp=(3,3), nd=2):\n",
        "        self.dim = dim\n",
        "        # self.ch_pool = make_pool_at(pool='avg', dim=dim, output_size=c)\n",
        "        self.ch_pool = make_pool_at(pool='max', dim=dim, output_size=c)\n",
        "        # self.ch_pool = ZeroExtend(dim, output_size=c) # only for out_dim>=in_dim\n",
        "        # self.sp_pool = AdaptiveAvgPool_nd(nd, sp)\n",
        "        self.sp_pool = AdaptiveMaxPool_nd(nd, sp)\n",
        "\n",
        "    def __call__(self, x): # [b,c,h,w]\n",
        "        x = self.sp_pool(x) # spatial first preserves spatial more?\n",
        "        x = self.ch_pool(x)\n",
        "        return x\n",
        "\n",
        "class UpDownBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, kernel=7, r=1):\n",
        "        super().__init__()\n",
        "        act = nn.SiLU()\n",
        "        self.r = r\n",
        "        self.block = PixelShuffleConv(in_ch, out_ch, kernel=kernel, r=r)\n",
        "        # self.block = nn.Sequential(\n",
        "        #     nn.BatchNorm2d(in_ch), act, PixelShuffleConv(in_ch, out_ch, kernel=kernel, r=r)\n",
        "        # )\n",
        "        # if self.r>1: self.res_conv = nn.Sequential(nn.ConvTranspose2d(in_ch, out_ch, kernel, 2, kernel//2, output_padding=1))\n",
        "        # if self.r>1: self.res_conv = nn.Sequential(nn.Upsample(scale_factor=2, mode='nearest'), nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity())\n",
        "        # if self.r>1: self.res_conv = nn.Sequential(nn.Upsample(scale_factor=2, mode='bilinear'), nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity())\n",
        "        # if self.r>1: self.res_conv = nn.Sequential(nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True), nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity())\n",
        "\n",
        "\n",
        "        # elif self.r<1: self.res_conv = nn.Sequential(nn.Conv2d(in_ch, out_ch, kernel, 2, kernel//2))\n",
        "        # elif self.r<1: self.res_conv = nn.Sequential(nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity(), nn.MaxPool2d(2,2))\n",
        "        # elif self.r<1: self.res_conv = nn.Sequential(nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity(), nn.AvgPool2d(2,2))\n",
        "        # elif self.r<1: self.res_conv = AttentionBlock(in_ch, out_ch, n_heads=4, q_stride=(2,2))\n",
        "\n",
        "        # else: self.res_conv = nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity()\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w]\n",
        "        b, num_tok, c, *win = x.shape\n",
        "        x = x.flatten(0,1)\n",
        "        out = self.block(x)\n",
        "        # # shortcut = F.interpolate(x.unsqueeze(1), size=out.shape[1:], mode='nearest-exact').squeeze(1) # pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html\n",
        "        # shortcut = F.adaptive_avg_pool3d(x, out.shape[1:]) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "        # # shortcut = F.adaptive_max_pool3d(x, out.shape[1:]) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "        # # shortcut = F.adaptive_avg_pool3d(x, out.shape[1:]) if out.shape[1]>=x.shape[1] else F.adaptive_max_pool3d(x, out.shape[1:])\n",
        "        # shortcut(x)\n",
        "        shortcut = Shortcut(dim=1, c=out.shape[1], sp=out.shape[-2:], nd=2)(x)\n",
        "        out = out + shortcut\n",
        "        out = out.unflatten(0, (b, num_tok))\n",
        "        return out\n",
        "\n",
        "        # return out + shortcut + self.res_conv(x)\n",
        "        # return out + self.res_conv(x)\n",
        "        # return self.res_conv(x)\n",
        "\n",
        "# if out>in, inter=max=ave=near.\n",
        "# if out<in, inter=ave. max=max\n",
        "\n",
        "# stride2\n",
        "# interconv/convpool\n",
        "# pixelconv\n",
        "# pixeluib\n",
        "# pixelres\n",
        "# shortcut\n",
        "\n",
        "# in_ch, out_ch = 16,3\n",
        "in_ch, out_ch = 3,16\n",
        "model = UpDownBlock(in_ch, out_ch, r=1/2).to(device)\n",
        "# model = UpDownBlock(in_ch, out_ch, r=2).to(device)\n",
        "\n",
        "x = torch.rand(12, in_ch, 64,64, device=device)\n",
        "x = torch.rand(12, 2, in_ch, 64,64, device=device)\n",
        "out = model(x)\n",
        "\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAyKHKivc0j7",
        "outputId": "e66e47ef-f7e7-44c1-f15f-168e0adf1807"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MUattn 256\n",
            "torch.Size([2, 3, 16, 16, 16])\n"
          ]
        }
      ],
      "source": [
        "# @title maxpool path bchw\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def conv_nd(n, *args, **kwargs): return [nn.Identity, nn.Conv1d, nn.Conv2d, nn.Conv3d][n](*args, **kwargs)\n",
        "def maxpool_nd(n, *args, **kwargs): return [nn.Identity, nn.MaxPool1d, nn.MaxPool2d, nn.MaxPool3d][n](*args, **kwargs)\n",
        "def avgpool_nd(n, *args, **kwargs): return [nn.Identity, nn.AvgPool1d, nn.AvgPool2d, nn.AvgPool3d][n](*args, **kwargs)\n",
        "\n",
        "import math\n",
        "class MaskUnitAttention(nn.Module):\n",
        "    # def __init__(self, d_model=16, n_heads=4, q_stride=None, nd=2):\n",
        "    def __init__(self, in_dim, d_model=16, n_heads=4, q_stride=None, nd=2):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads, self.d_head = n_heads, d_model // n_heads\n",
        "        self.scale = self.d_head**-.5\n",
        "        # self.qkv = conv_nd(nd, d_model, 3*d_model, 1, bias=False)\n",
        "        self.qkv = conv_nd(nd, in_dim, 3*d_model, 1, bias=False)\n",
        "        # self.out = conv_nd(nd, d_model, d_model, 1)\n",
        "        self.out = nn.Linear(d_model, d_model, 1)\n",
        "        self.q_stride = q_stride # If greater than 1, pool q with this stride. The stride should be flattened (e.g., 2x2 = 4).\n",
        "        if q_stride:\n",
        "            self.q_stride = (q_stride,)*nd if type(q_stride)==int else q_stride\n",
        "            self.q_pool = maxpool_nd(nd, self.q_stride, stride=self.q_stride)\n",
        "\n",
        "    def forward(self, x): # [b,num_tok,c,win1,win2]\n",
        "        b, num_tok, c, *win = x.shape\n",
        "        x = x.flatten(0,1) # [b*num_tok,c,win1,win2]\n",
        "        # print(x.shape)\n",
        "        # q, k, v = self.qkv(x).reshape(b, 3, self.n_heads, self.d_head, num_tok, win*win).permute(1,0,2,4,5,3) # [b,3*d_model,num_tok*win,win] -> 3* [b, n_heads, num_tok, win*win, d_head]\n",
        "        # self.qkv(x).flatten(1,-2).unflatten(-1, (self.heads,-1)).chunk(3, dim=-1) # [b,sp,n_heads,d_head]\n",
        "        q,k,v = self.qkv(x).chunk(3, dim=1) # [b*num_tok,d,win,win]\n",
        "        if self.q_stride:\n",
        "            q = self.q_pool(q)\n",
        "            win=[w//s for w,s in zip(win, self.q_stride)] # win = win/q_stride\n",
        "        if math.prod(win) >= 200:\n",
        "            print('MUattn', math.prod(win))\n",
        "            q, k, v = map(lambda t: t.reshape(b*num_tok, self.n_heads, self.d_head, -1).transpose(-2,-1), (q,k,v)) # [b*num_tok, n_heads, win*win, d_head]\n",
        "        else:\n",
        "            # q, k, v = map(lambda t: t.reshape(b, num_tok, self.n_heads, self.d_head, -1).permute(0,2,4,1,3), (q,k,v)) # downsampling attention # [b, n_heads, win*win, num_tok, d_head]\n",
        "            q, k, v = map(lambda t: t.reshape(b, num_tok, self.n_heads, self.d_head, -1).permute(0,2,1,4,3).flatten(2,3), (q,k,v)) # [b, n_heads, num_tok*win*win, d_head]\n",
        "\n",
        "        # x = F.scaled_dot_product_attention(q,k,v, attn_mask=None) # https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html\n",
        "        q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2) # [b, n_heads, t(/pool), d_head]\n",
        "        context = k.transpose(-2,-1) @ v # [b, n_heads, d_head, d_head]\n",
        "        x = q @ context # [b, n_heads, t(/pool), d_head]\n",
        "\n",
        "        # print('attn fwd 2',x.shape)\n",
        "        # x = x.transpose(1, 3).reshape(B, -1, self.d_model)\n",
        "        x = x.transpose(1,2).reshape(b, -1, self.d_model) # [b,t,d]\n",
        "        # x = x.transpose(-2,-1).reshape(x.shape[0], self.d_model, ) # [b,t,d]\n",
        "        # [b, n_heads, num_tok, win*win, d_head] -> [b, n_heads, d_head, num_tok, win*win] -> [b,c,num_tok*win,win]\n",
        "        # x = x.permute(0,1,4,2,3).reshape(b, self.d_model, num_tok*win,win) # [b, n_heads, num_tok, win*win, d_head]\n",
        "        # x = x.transpose(-2,-1).reshape(b, self.d_model, num_tok, *win) # [b*num_tok, n_heads, win*win, d_head]\n",
        "        x = self.out(x)\n",
        "        L=len(win)\n",
        "        x = x.reshape(b, num_tok, *win, self.d_model).permute(0,1,L+2,*range(2,L+2)) # [b, num_tok, out_dim, *win]\n",
        "        # x = x.unflatten(0, (b, num_tok))\n",
        "        return x # [b,num_tok,c,win1,win2]\n",
        "\n",
        "d_model=16\n",
        "model = MaskUnitAttention(d_model, n_heads=4, q_stride=2)\n",
        "# MaskUnitAttention(d_model=16, n_heads=4, q_stride=None, nd=2)\n",
        "# x=torch.randn(2,4,4,3)\n",
        "# x=torch.randn(2,3,d_model,8,8)\n",
        "x=torch.randn(2,3,d_model,32,32)\n",
        "# [b*num_tok,c,win,win]\n",
        "\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lec1Nq7nkbgt",
        "outputId": "bd142f82-f226-4884-ab9a-33d38e8b9c67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "124928\n",
            "torch.Size([4, 64, 64])\n",
            "124928\n"
          ]
        }
      ],
      "source": [
        "# @title hiera vit me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class LayerNorm_at(nn.RMSNorm): # LayerNorm RMSNorm\n",
        "    def __init__(self, dim=1, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.dim=dim\n",
        "    def forward(self, x):\n",
        "        return super().forward(x.transpose(self.dim,-1)).transpose(self.dim,-1)\n",
        "\n",
        "# # [b, h/win1* w/win2, c, win1,win2] -> [b,c,h,w]\n",
        "# def shuffle(x, window_shape): # [b,win*win, c, h/win, w/win] -> [b,1,c,h,w]\n",
        "#     # out_shape = [x.shape[0], -1, x.shape[2]] + [x*w for x,w in zip(x.shape[3:], window_shape)] # [b,1,c,h/win*wim, w/win*wim]\n",
        "#     out_shape = [x.shape[0], -1, x.shape[2]] + [x*w for x,w in zip(x.shape[3:], window_shape)] # [b,1,c,h/win*wim, w/win*wim]\n",
        "#     x = x.unflatten(1, (-1, *window_shape)) # [b,num_tok,win,win, c, h/win, w/win]\n",
        "#     D=x.dim()+1\n",
        "#     permute = [0,1,D//2] + [val for tup in zip(range(1+D//2, D), range(2, D//2)) for val in tup]\n",
        "#     x = x.permute(permute).reshape(out_shape)\n",
        "#     return x\n",
        "\n",
        "def unshuffle(x, window_shape): # [b,c,h,w] -> [b, h/win1* w/win2, c, win1,win2]\n",
        "    new_shape = list(x.shape[:2]) + [val for xx, win in zip(list(x.shape[2:]), window_shape) for val in [xx//win, win]] # [h,w]->[h/win1, win1, w/win2, win2]\n",
        "    x = x.reshape(new_shape) # [b, c, h/win1, win1, w/win2, win2]\n",
        "    # print('unsh',x.shape, window_shape, new_shape)\n",
        "    L = len(new_shape)\n",
        "    permute = ([0] + list(range(2, L - 1, 2)) + [1] + list(range(3, L, 2))) # [0,2,4,1,3,5] / [0,2,4,6,1,3,5,6]\n",
        "    return x.permute(permute).flatten(1, L//2-1) # [b, h/win1* w/win2, c, win1,win2]\n",
        "\n",
        "class HieraBlock(nn.Module):\n",
        "    # def __init__(self, d_model, n_heads, q_stride=None, mult=4, drop=0, nd=2):\n",
        "    def __init__(self, in_dim, d_model, n_heads, q_stride=None, mult=4, drop=0, nd=2):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        # self.norm = LayerNorm_at(2, d_model) # LayerNorm RMSNorm\n",
        "        self.norm = LayerNorm_at(2, in_dim) # LayerNorm RMSNorm\n",
        "        self.drop = nn.Dropout(drop)\n",
        "        # self.attn = MaskUnitAttention(d_model, n_heads, q_stride)\n",
        "        self.attn = MaskUnitAttention(in_dim, d_model, n_heads, q_stride)\n",
        "        ff_dim=d_model*mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), nn.Linear(d_model, ff_dim), nn.ReLU(), # ReLU GELU\n",
        "            nn.RMSNorm(ff_dim), nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "            # nn.RMSNorm(d_model), act, nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Linear(ff_dim, d_model)\n",
        "            # nn.RMSNorm(d_model), nn.Linear(d_model, ff_dim), nn.ReLU(), nn.Dropout(dropout), # ReLU GELU\n",
        "            # nn.Linear(ff_dim, d_model), nn.Dropout(dropout),\n",
        "        )\n",
        "        # self.res = conv_nd(nd, d_model, d_model, q_stride, q_stride) if q_stride else nn.Identity()\n",
        "\n",
        "        # self.res = nn.Sequential(\n",
        "        #     conv_nd(nd, in_dim, d_model, 1, 1) if in_dim!=d_model else nn.Identity(),\n",
        "        #     # maxpool_nd(nd, q_stride, q_stride) if q_stride else nn.Identity(),\n",
        "        #     avgpool_nd(nd, q_stride, q_stride) if q_stride else nn.Identity(),\n",
        "        # )\n",
        "        self.res = UpDownBlock(in_dim, d_model, kernel=1, r=1/2 if q_stride else 1)\n",
        "\n",
        "        # x = self.proj(x_norm).unflatten(1, (self.attn.q_stride, -1)).max(dim=1).values # pooling res # [b, (Sy, Sx, h/Sy, w/Sx), c] -> [b, (Sy, Sx), (h/Sy, w/Sx), c] -> [b, (h/Sy, w/Sx), c]\n",
        "\n",
        "\n",
        "    def forward(self, x): # [b, num_tok, c, *win]\n",
        "        b, num_tok, c, *win = x.shape\n",
        "        # print('attnblk fwd',x.shape)\n",
        "        # x = x + self.drop(self.self(self.norm(x)))\n",
        "        # print('attnblk fwd',self.res(x.flatten(0,1)).shape, self.drop(self.attn(self.norm(x))).flatten(0,1).shape)\n",
        "        # x = self.res(x.flatten(0,1)) + self.drop(self.attn(self.norm(x))).flatten(0,1) # [b*num_tok,c,win1,win2,win3]\n",
        "        x = self.res(x) + self.drop(self.attn(self.norm(x))) # [b*num_tok,c,win1,win2,win3]\n",
        "        # x = x + self.ff(x)\n",
        "        # x = x + self.ff(x.transpose(1,-1)).transpose(1,-1)\n",
        "        x = x + self.ff(x.transpose(2,-1)).transpose(2,-1)\n",
        "        # x = self.ff(x)\n",
        "        # x = x.unflatten(0, (b, num_tok))\n",
        "        return x\n",
        "\n",
        "    # def forward(self, x): # [b,t,c] # [b, (Sy, Sx, h/Sy, w/Sx), c]\n",
        "    #     # Attention + Q Pooling\n",
        "    #     x_norm = self.norm1(x)\n",
        "    #     if self.dim != self.dim_out:\n",
        "    #         x = self.proj(x_norm).unflatten(1, (self.attn.q_stride, -1)).max(dim=1).values # pooling res # [b, (Sy, Sx, h/Sy, w/Sx), c] -> [b, (Sy, Sx), (h/Sy, w/Sx), c] -> [b, (h/Sy, w/Sx), c]\n",
        "    #     x = x + self.drop_path(self.attn(x_norm))\n",
        "    #     x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "    #     return x\n",
        "\n",
        "\n",
        "class levelBlock(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, n_heads=None, depth=1, r=1):\n",
        "        super().__init__()\n",
        "        self.seq = nn.Sequential(\n",
        "            UpDownBlock(in_ch, out_ch, r=min(1,r)) if in_ch != out_ch or r<1 else nn.Identity(),\n",
        "            # AttentionBlock(d_model, d_model, n_heads, q_stride) if in_ch != out_ch or r<1 else nn.Identity(),\n",
        "            # AttentionBlock(d_model, d_model, n_heads, q_stride=(2,2)),\n",
        "            *[HieraBlock(d_model, d_model, n_heads) for i in range(1)],\n",
        "            # UpDownBlock(out_ch, out_ch, r=r) if r>1 else nn.Identity(),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.seq(x)\n",
        "\n",
        "\n",
        "class Hiera(nn.Module):\n",
        "    def __init__(self, patch_size, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "    # def __init__(self, in_dim, out_dim, d_model, n_heads, depth):\n",
        "        # patch_size=4\n",
        "        super().__init__()\n",
        "        self.embed = nn.Sequential( # in, out, kernel, stride, pad\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.MaxPool2d(3,2,1), # nn.MaxPool2d(2,2)\n",
        "            nn.Conv2d(in_dim, d_model, 7, 1, 7//2, bias=False),\n",
        "            # nn.Conv2d(in_dim, d_model, patch_size, patch_size), # like patch\n",
        "            # nn.Conv2d(in_dim, d_model, 3, 2, 3//2, bias=False), nn.BatchNorm2d(d_model), nn.ReLU(),\n",
        "            # UpDownBlock(in_dim, dim, r=1/2, kernel=3), UpDownBlock(dim, dim, r=1/2, kernel=3)\n",
        "            # nn.PixelUnshuffle(2), nn.Conv2d(in_dim*2**2, dim, 1, bias=False),\n",
        "            # ResBlock(in_dim, d_model, kernel),\n",
        "            )\n",
        "        # # self.pos_emb = LearnedRoPE2D(dim) # LearnedRoPE2D, RoPE2D\n",
        "\n",
        "        emb_shape = (32,32)\n",
        "        # emb_shape = (8,32,32)\n",
        "        if len(emb_shape) == 3: # for video\n",
        "            pos_spatial = nn.Parameter(torch.randn(1, emb_shape[1]*emb_shape[2], d_model)*.02)\n",
        "            pos_temporal = nn.Parameter(torch.randn(1, emb_shape[0], d_model)*.02)\n",
        "            self.pos_emb = pos_spatial.repeat(1, emb_shape[0], 1) + torch.repeat_interleave(pos_temporal, emb_shape[1] * emb_shape[2], dim=1)\n",
        "        elif len(emb_shape) == 2: # for img\n",
        "            self.pos_emb = nn.Parameter(torch.randn(1, d_model, *emb_shape)*.02) # 56*56=3136\n",
        "        # self.pos_emb = self.pos_emb.flatten(2).transpose(-2,-1)\n",
        "\n",
        "        # self.blocks = nn.Sequential(*[AttentionBlock(d_model, n_heads, q_stride=(2,2) if i in [1,3] else None) for i in range(depth)])\n",
        "        mult = [1,1,1,1]\n",
        "        # mult = [1,2,4,8] # [1,2,3,4] [1,2,2,2]\n",
        "        ch_list = [d_model * m for m in mult] # [128, 256, 384, 512]\n",
        "\n",
        "        self.blocks = nn.Sequential(\n",
        "            # HieraBlock(ch_list[0], ch_list[1], n_heads, q_stride=(2,2)),\n",
        "            # UpDownBlock(ch_list[0], ch_list[1], kernel=1, r=1/2),\n",
        "            HieraBlock(ch_list[1], ch_list[1], n_heads),\n",
        "            # HieraBlock(ch_list[1], ch_list[2], n_heads, q_stride=(2,2)),\n",
        "            # UpDownBlock(ch_list[1], ch_list[2], kernel=1, r=1/2),\n",
        "            # HieraBlock(ch_list[2], ch_list[2], n_heads),\n",
        "            )\n",
        "        self.norm = nn.RMSNorm(ch_list[2]) # LayerNorm RMSNorm\n",
        "        self.out = nn.Linear(ch_list[2], out_dim, bias=False) if out_dim and out_dim != ch_list[2] else None\n",
        "\n",
        "\n",
        "    def forward(self, x, context_indices=None): # [b,c,h,w], [b,num_tok]\n",
        "        x = self.embed(x)\n",
        "        # print('vit fwd', x.shape, self.pos_emb.shape)\n",
        "        x = x + self.pos_emb\n",
        "        x = unshuffle(x, (4,4)) # [b,num_tok,c,win1,win2,win3] or [b,1,c,f,h,w]\n",
        "        if context_indices != None: x = x[torch.arange(x.shape[0]).unsqueeze(-1), context_indices] # [batch, num_context_toks, d_model]\n",
        "\n",
        "        # print('vit fwd1', x.shape)\n",
        "        x = self.blocks(x) # [b,num_tok,c,1,1,1]\n",
        "        # print('vit fwd2', x.shape)\n",
        "        # x = x.mean(-1).mean(-1)\n",
        "        x = x.flatten(-2).max(-1)[0]\n",
        "        # print('vit fwd3', x.shape)\n",
        "        x = x.squeeze() # [b,num_tok,d]\n",
        "        out = self.norm(x)\n",
        "        if self.out: out = self.out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "# pos_emb rope < learn < learned\n",
        "# conv > pixel?\n",
        "# droppath not required\n",
        "\n",
        "# norm,act,conv < conv,norm,act\n",
        "# 2*s1 < uib < resblock\n",
        "# gatedadaln 3 < 2 = 1 < ffmult4 = 2*gatedadaln\n",
        "# MaxPool2d(2,2) < MaxPool2d(3,2,3//2)\n",
        "\n",
        "# patchattn only for\n",
        "\n",
        "# multiendfusion negligible diff\n",
        "\n",
        "\n",
        "d_model = 64\n",
        "in_dim = 3\n",
        "patch_size = 2\n",
        "# model = Hiera(patch_size, in_dim, d_model, n_heads=4, nlayers=4, drop=0.).to(device)\n",
        "model = Hiera(patch_size, in_dim, d_model, n_heads=4, nlayers=1, drop=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x = torch.rand((4, in_dim, 32, 32), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 59850\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3XSZZyUPY1i"
      },
      "source": [
        "## vit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "eDqkaM2v0_zS"
      },
      "outputs": [],
      "source": [
        "# @title FSQ me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def ste_round(x): return x.round().detach() + x - x.detach()\n",
        "\n",
        "class FSQ(nn.Module):\n",
        "    def __init__(self, levels):\n",
        "        super().__init__()\n",
        "        self.levels = torch.tensor(levels, device=device)\n",
        "        self.basis = torch.cumprod(torch.tensor([*levels[1:], 1], device=device).flip(-1), dim=0).flip(-1)\n",
        "        self.half_width = (self.levels-1)/2\n",
        "        self.codebook_size = torch.prod(self.levels).item()\n",
        "        self.codebook = self.indexes_to_codes(torch.arange(self.codebook_size, device=device))\n",
        "\n",
        "    def forward(self, z, beta=1): # beta in (0,1). beta->0 => values more spread out\n",
        "        offset = (self.levels+1) % 2 /2 # .5 if even, 0 if odd\n",
        "        bound = (F.sigmoid(z)-1/2) * (self.levels-beta) + offset\n",
        "        # print('fwd', bound) #\n",
        "        quantized = ste_round(bound)\n",
        "        # print('fwd', quantized) # 4: -1012\n",
        "        return (quantized-offset) / self.half_width # split [-1,1]\n",
        "\n",
        "    def codes_to_indexes(self, zhat):\n",
        "        zhat = (zhat + 1) * self.half_width\n",
        "        return (zhat * self.basis).sum(axis=-1)#.int()\n",
        "\n",
        "    def indexes_to_codes(self, indices):\n",
        "        indices = indices.unsqueeze(-1)\n",
        "        codes = torch.remainder(indices//self.basis, self.levels)\n",
        "        # print(\"codes\",codes)\n",
        "        return codes / self.half_width - 1\n",
        "\n",
        "# fsq = FSQ(levels = [5,4,3,2])\n",
        "# # print(fsq.codebook)\n",
        "# batch_size, seq_len = 2, 4\n",
        "# # x = torch.rand((batch_size, seq_len,3),device=device)\n",
        "# x = torch.linspace(-2,2,7).repeat(4,1).T\n",
        "# la = fsq(x)\n",
        "# print(la)\n",
        "# lact = fsq.codes_to_indexes(la)\n",
        "# print(lact)\n",
        "# # la = fsq.indexes_to_codes(lact)\n",
        "# # print(la)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6T4F651kmGh",
        "outputId": "29788161-59f6-40aa-fd5c-219c732a73c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "162112\n",
            "torch.Size([5, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# @title AttentionBlock\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class SelfAttn(nn.Module):\n",
        "    def __init__(self, dim, n_heads):\n",
        "        super().__init__()\n",
        "        self.dim, self.n_heads = dim, n_heads\n",
        "        d_head = dim//n_heads\n",
        "        self.qkv = nn.Linear(dim, dim*3, bias=False)\n",
        "        # self.lin = nn.Linear(dim, dim)\n",
        "        self.lin = zero_module(nn.Linear(dim, dim))\n",
        "        # self.rope = RoPE(d_head, seq_len=512, base=10000)\n",
        "        # self.rope = RoPE2D(d_head, h=64, w=64, base=100)\n",
        "        self.scale = d_head**-.5\n",
        "\n",
        "    def forward(self, x): # [b,t,d]\n",
        "        q,k,v = self.qkv(x).unflatten(-1, (self.n_heads,-1)).transpose(1,2).chunk(3, dim=-1) # [b, t, n_heads, d_head] -> [b, n_heads, t, d_head]\n",
        "        # q, k = self.rope(q), self.rope(k)\n",
        "        # x = F.scaled_dot_product_attention(q,k,v, attn_mask=None) # https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html\n",
        "        q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2)\n",
        "        context = k.transpose(-2,-1) @ v # [batch, n_heads, d_head, d_head]\n",
        "        x = q @ context # [b, n_heads, t, d_head]\n",
        "        # # print('SelfAttn', x.shape)\n",
        "        x = x.transpose(1,2).flatten(2)\n",
        "        return self.lin(x)\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, mult=4, drop=0.):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        # self.norm = nn.RMSNorm(d_model, elementwise_affine=False) # LayerNorm RMSNorm\n",
        "        self.norm1, self.norm2 = nn.LayerNorm(d_model), nn.LayerNorm(d_model)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "        self.attn = SelfAttn(d_model, n_heads)\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        ff_dim=d_model*mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), act, nn.Dropout(drop), nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "            nn.RMSNorm(ff_dim), act, nn.Dropout(drop), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(drop), nn.Linear(d_model, ff_dim), act,\n",
        "            # nn.RMSNorm(ff_dim), nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x): # [batch, seq_len, d_model]\n",
        "        # print('attnblk fwd',x.shape)\n",
        "        x = x + self.drop(self.attn(self.norm1(x)))\n",
        "        x = x + self.ff(x)\n",
        "        # x = x + self.drop(self.ff(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, patch_size, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        patch_size=2\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Conv2d(in_dim, d_model, patch_size, patch_size), # like patch\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 1, 7//2, bias=False)\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.MaxPool2d(3,2,1)\n",
        "            nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.BatchNorm2d(d_model), nn.ReLU(),\n",
        "            nn.Conv2d(d_model, d_model, 3, 2, 3//2, bias=False)\n",
        "            )\n",
        "        # self.embed.requires_grad=False\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, 32*32, d_model)*.02)\n",
        "        # self.pos_emb = nn.Parameter(RoPE2D(dim=d_model, h=8, w=8, base=1000), requires_grad=False)\n",
        "\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=(32//patch_size)**2, base=10000), requires_grad=False)\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, d_model, n_heads) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x).flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c] # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        # x = self.pos_enc(x)\n",
        "        # print(\"TransformerModel\",x.shape, self.pos_emb.shape)\n",
        "        x = x + self.pos_emb[:,:x.shape[1]]\n",
        "        if context_indices != None: x = x[torch.arange(x.shape[0]).unsqueeze(-1), context_indices] # [batch, num_context_toks, d_model]\n",
        "\n",
        "        x = self.transformer(x)\n",
        "        out = self.norm(x)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "d_model = 64\n",
        "in_dim = 3\n",
        "patch_size = 2\n",
        "# model = ViT(patch_size, in_dim, d_model, n_heads=4, nlayers=4, drop=0.).to(device)\n",
        "model = ViT(patch_size, in_dim, d_model, n_heads=4, nlayers=1, drop=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x = torch.rand((5, in_dim, 32, 32), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title random_masking\n",
        "import torch\n",
        "\n",
        "def random_masking(length, mask_ratio, b=64):\n",
        "    noise = torch.rand(b, length)\n",
        "    len_mask = int(length * mask_ratio)\n",
        "    _, msk_ind = torch.topk(noise, k=len_mask, dim=-1, sorted=False) # val, ind -> [b,len_mask]\n",
        "    _, keep_ind = torch.topk(noise, k=length-len_mask, largest=False, dim=-1, sorted=False) # val, ind -> [b,len_keep]\n",
        "    return msk_ind, keep_ind\n",
        "\n",
        "# msk_ind, keep_ind = random_masking(10, .3, b=2)\n",
        "\n",
        "# x_ = torch.rand(4, 3, 2)\n",
        "# print(x_)\n",
        "# # ids = torch.tensor([0, 2, 1])[None,:,None]\n",
        "# # ids = torch.tensor([0, 2, 1])[None,:,None].repeat(4,1,2)\n",
        "# ids = torch.tensor([1, 2, 0])[None,:,None].repeat(4,1,2)\n",
        "# # o = torch.gather(x_, dim=1, index=ids)\n",
        "# o = torch.zeros_like(x_).scatter_(dim=1, index=ids, src=x_)\n",
        "# print(o)\n"
      ],
      "metadata": {
        "id": "k3nfeLM4wtkJ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "Fh9o__m2-j7K",
        "outputId": "a217bb8c-8bfb-4683-8059-27e0f3ee24dc",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/268.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.0/268.0 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title simplex\n",
        "# !pip install -q opensimplex\n",
        "import opensimplex\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def simplexmask2d(hw=(32,32), ctx_scale=(.85,1.), trg_scale=(.6,.8), B=64, chaos=[1,.5]):\n",
        "    ix, iy = np.split(np.linspace(0, chaos[0], num=sum(hw)), [hw[0]])\n",
        "    noise = opensimplex.noise3array(ix, iy, np.random.randint(1e10, size=B)) # [b,h,w]\n",
        "    noise = torch.from_numpy(noise).flatten(1)\n",
        "    trunc_normal = torch.fmod(torch.randn(2)*.3,1)/2 + .5\n",
        "    ctx_mask_scale = torch.rand(1) * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    trg_mask_scale = torch.rand(1) * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "    # ctx_mask_scale = trunc_normal[0] * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    # trg_mask_scale = trunc_normal[1] * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "    seq = hw[0]*hw[1]\n",
        "    ctx_len, trg_len = int(seq*ctx_mask_scale), int(seq*trg_mask_scale)\n",
        "    val, trg_index = torch.topk(noise, trg_len, dim=1, sorted=False)\n",
        "    # ctx_len = ctx_len - trg_len\n",
        "    ctx_len = min(ctx_len, seq-trg_len)\n",
        "\n",
        "    remove_mask = torch.ones((B,seq), dtype=bool) # [B, S]\n",
        "    remove_mask.scatter_(1, trg_index, False).flatten()\n",
        "    ind = torch.arange(seq).unsqueeze(0).repeat(B,1)[remove_mask].reshape(B, -1)\n",
        "\n",
        "    ix, iy = np.split(np.linspace(0, chaos[1], num=sum(hw)), [hw[0]])\n",
        "    noise = opensimplex.noise3array(ix, iy, np.random.randint(1e10, size=B)) # [b,h,w]\n",
        "    noise = torch.from_numpy(noise).flatten(1)[remove_mask].reshape(B, -1)\n",
        "    val, ctx_ind = torch.topk(noise, ctx_len, dim=1, sorted=False)\n",
        "    ctx_index = ind[torch.arange(B).unsqueeze(-1), ctx_ind]\n",
        "    return ctx_index, trg_index\n",
        "\n",
        "b=16\n",
        "# ctx_index, trg_index = simplexmask2d(hw=(32,32), ctx_scale=(.85,1), trg_scale=(.7,.8), B=b, chaos=[3,.5])\n",
        "# ctx_index, trg_index = simplexmask2d(hw=(32,32), ctx_scale=(.85,1), trg_scale=(.7,.8), B=b, chaos=[3,1])\n",
        "# ctx_index, trg_index = simplexmask2d(hw=(32,32), ctx_scale=(.85,1), trg_scale=(.2,.8), B=b, chaos=[3,1])\n",
        "ctx_index, trg_index = simplexmask2d(hw=(32,32), ctx_scale=(.05,.2), trg_scale=(.7,.8), B=b, chaos=[3,1])\n",
        "# ctx_index, trg_index = simplexmask2d(hw=(32,32), ctx_scale=(.85,1), trg_scale=(.5,.5), B=b, chaos=[.01,.5])\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "# imshow(mask)\n",
        "\n",
        "mask = torch.zeros(b ,32*32)\n",
        "# mask = torch.ones(b ,32*32)*.3\n",
        "mask[torch.arange(b).unsqueeze(-1), trg_index] = 1\n",
        "mask[torch.arange(b).unsqueeze(-1), ctx_index] = .5\n",
        "# print((mask==1).sum(1))\n",
        "# print((mask==.5).sum(1))\n",
        "mask = mask.reshape(b,1,32,32)\n",
        "import torchvision\n",
        "imshow(torchvision.utils.make_grid(mask, nrow=8))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "pQfM2fYvcTh6",
        "outputId": "5067de41-f150-40ca-c9bf-003494493fbc",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title masks\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# def multiblock(seq, min_s, max_s, M=1):\n",
        "#     mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "#     mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "#     mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "#     indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "#     target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "#     return target_mask\n",
        "\n",
        "\n",
        "# def multiblock2d(hw=(8,8), scale=(.15,.2), aspect_ratio=(.75,1.5), M=1):\n",
        "#     mask_aspect = torch.rand(1) * (aspect_ratio[1] - aspect_ratio[0]) + aspect_ratio[0] # in (min_s, max_s) # all blocks same size\n",
        "#     mask_scale = torch.rand(1) * (scale[1] - scale[0]) + scale[0] # in (min_s, max_s) # all blocks same size\n",
        "#     h = (mask_scale/mask_aspect)**.5# h*(h*aspect) = scale\n",
        "#     w = h * mask_aspect\n",
        "#     h_pos, w_pos = torch.rand(M)*(1-w), torch.rand(M)*(1-h) # in (0, 1 - mask_len)\n",
        "#     h_len, h_pos = (h*hw[0]).int(), h_pos*hw[0]\n",
        "#     w_len, w_pos = (w*hw[1]).int(), w_pos*hw[1]\n",
        "#     h_ind, w_ind = torch.arange(hw[0]).unsqueeze(0), torch.arange(hw[1]).unsqueeze(0) # [1, seq]\n",
        "#     h_mask = (h_ind>=h_pos.unsqueeze(-1)) & (h_ind<(h_pos+h_len).unsqueeze(-1)) # [M, seq]\n",
        "#     w_mask = (w_ind>=w_pos.unsqueeze(-1)) & (w_ind<(w_pos+w_len).unsqueeze(-1)) # [M, seq]\n",
        "#     target_mask = h_mask.unsqueeze(-1) & w_mask.unsqueeze(-2) # [M, seq, seq]\n",
        "#     return target_mask\n",
        "\n",
        "# # https://arxiv.org/pdf/2210.07224\n",
        "# def randpatch(seq, mask_size=8, gamma=0.9): # num patches of seq, mask patch size, masking ratio\n",
        "#     # mask = torch.rand(seq//mask_size)<gamma\n",
        "#     length = seq//mask_size\n",
        "#     g = torch.normal(gamma, std=.1, size=(1,)).clamp(.5,.9)\n",
        "#     # g = gamma\n",
        "#     idx = torch.randperm(length)[:int(length*g)]\n",
        "#     mask = torch.zeros(length, dtype=bool)\n",
        "#     mask[idx] = True\n",
        "#     mask = mask.repeat_interleave(mask_size, dim=-1)\n",
        "#     return mask # [seq] , True -> mask\n",
        "\n",
        "\n",
        "# import torch\n",
        "# def apply_masks(x, mask): # [b,t,d], [mask_size] # https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "#     mask_keep = mask.unsqueeze(-1).repeat(x.size(0), 1, x.size(-1)) # [batch,T,dim]\n",
        "#     return torch.gather(x, dim=1, index=mask_keep) # [batch,mask_size,dim]\n",
        "\n",
        "\n",
        "# @title ijepa multiblock next\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "import math\n",
        "from multiprocessing import Value\n",
        "import torch\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(self, hw=(224, 224), enc_mask_scale=(.85,1), pred_mask_scale=(.15,.2), aspect_ratio=(.75,1.25),\n",
        "        nenc=1, npred=2, min_keep=4, allow_overlap=False):\n",
        "        super().__init__()\n",
        "        self.height, self.width = hw\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.aspect_ratio = aspect_ratio\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "\n",
        "    def _sample_block_size(self, scale, aspect_ratio_scale):\n",
        "        _rand = torch.rand(1).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.height * self.width * mask_scale) # num patches to keep\n",
        "        # -- Sample block aspect-ratio\n",
        "        min_ar, max_ar = aspect_ratio_scale\n",
        "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
        "        while h >= self.height: h -= 1 # crop mask to be smaller than img\n",
        "        while w >= self.width: w -= 1\n",
        "        return (h, w)\n",
        "\n",
        "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
        "        h, w = b_size\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            top = torch.randint(0, self.height - h, (1,))\n",
        "            left = torch.randint(0, self.width - w, (1,))\n",
        "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
        "            mask[top:top+h, left:left+w] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
        "        mask_complement[top:top+h, left:left+w] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, B):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        p_size = self._sample_block_size(scale=self.pred_mask_scale, aspect_ratio_scale=self.aspect_ratio)\n",
        "        e_size = self._sample_block_size(scale=self.enc_mask_scale, aspect_ratio_scale=(1., 1.))\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.height * self.width\n",
        "        min_keep_enc = self.height * self.width\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                print(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "        return collated_masks_enc, collated_masks_pred\n",
        "\n",
        "mask_collator = MaskCollator(hw=(32,32), enc_mask_scale=(.85, 1.), pred_mask_scale=(.15, .2), aspect_ratio=(.75, 1.5),\n",
        "        nenc=1, npred=4, min_keep=4,\n",
        "        # allow_overlap=True)\n",
        "        allow_overlap=False)\n",
        "\n",
        "b=16\n",
        "collated_masks_enc, collated_masks_pred = mask_collator(b)\n",
        "ctx_index, trg_index = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "\n",
        "# mask = torch.zeros(1 ,32*32)\n",
        "# mask[:, trg_index[:1]] = 1\n",
        "# mask[:, ctx_index[:1]] = .5\n",
        "# mask = mask.reshape(1,32,32)\n",
        "\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "# imshow(mask)\n",
        "\n",
        "mask = torch.zeros(b ,32*32)\n",
        "mask[torch.arange(b).unsqueeze(-1), trg_index] = 1\n",
        "mask[torch.arange(b).unsqueeze(-1), ctx_index] = .5\n",
        "mask = mask.reshape(b,1,32,32)\n",
        "import torchvision\n",
        "imshow(torchvision.utils.make_grid(mask, nrow=8))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GD7ezZzmhTHU"
      },
      "outputs": [],
      "source": [
        "# @title RoPE\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def RoPE(dim, seq_len=512, base=10000):\n",
        "    theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "    pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "    angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "    rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [1, seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "    return rot_emb\n",
        "\n",
        "# class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "#     def __init__(self, dim, seq_len=512, base=10000):\n",
        "#         super().__init__()\n",
        "#         self.dim, self.base = dim, base\n",
        "#         theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "#         pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "#         angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "#         self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         seq_len = x.size(1)\n",
        "#         if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.base)\n",
        "#         return x * self.rot_emb[:seq_len]\n",
        "\n",
        "class RotEmb(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, top=torch.pi, base=10000):\n",
        "        super().__init__()\n",
        "        self.theta = top / (base ** (torch.arange(0, dim, step=2, device=device) / dim))\n",
        "        # self.theta = top / (base ** torch.linspace(0, 1, dim//2, device=device))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (pos.unsqueeze(-1) * self.theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [seq_len, dim]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask_collator = MaskCollator(hw=(1024,1024), enc_mask_scale=(.85, 1.), pred_mask_scale=(.15, .2), aspect_ratio=(.75, 1.5), nenc=1, npred=4, min_keep=4, allow_overlap=False)\n",
        "# %timeit collated_masks_enc, collated_masks_pred = mask_collator(64) # 225 ms 1024:4.79 s\n",
        "# %timeit ctx_index, trg_index = simplexmask2d(hw=(1024,1024), ctx_scale=(.85,1), trg_scale=(.5,.6), B=b, chaos=.5) # 265 ms ;topk 203 ms 1024:4.27 s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nj5bI5hSEdJ1",
        "outputId": "6a7d20de-cdae-47f0-82ac-64546f16f829"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slowest run took 5.27 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "4.79 s ± 3.03 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ttc=[]\n",
        "ttt=[]\n",
        "def mean(x): return sum(x)/len(x)\n",
        "\n",
        "for i in range(1000):\n",
        "    # collated_masks_enc, collated_masks_pred = mask_collator(1)\n",
        "    # context_indices, trg_indices = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "    # context_indices, trg_indices = simplexmask2d(hw=(32,32), ctx_scale=(.85,1.), trg_scale=(.7,.8), B=1, chaos=[3,1])\n",
        "    context_indices, trg_indices = simplexmask2d(hw=(32,32), ctx_scale=(.05,.5), trg_scale=(.2,.8), B=1, chaos=[3,1])\n",
        "    ttc.append(context_indices.shape[-1])\n",
        "    ttt.append(trg_indices.shape[-1])\n",
        "\n",
        "print(mean(ttc), mean(ttt))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"figure.figsize\"] = (4,4)\n",
        "plt.hist(ttc, bins=20, alpha=.5, label='context mask')\n",
        "plt.hist(ttt, bins=20, alpha=.5, label='target mask')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "bC6elHNF5xzL",
        "outputId": "03d9bedf-8c95-4bca-f1b3-f3c851dfc6c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "258.673 499.481\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "M-zdjdJixtOu"
      },
      "outputs": [],
      "source": [
        "# @title Transformer Predictor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, 32*32, d_model)*0.02)\n",
        "        # self.pos_emb = nn.Parameter(RoPE2D(dim=d_model, h=8, w=8, base=1000), requires_grad=False)\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=32*32, base=10000), requires_grad=False)\n",
        "\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, d_model, n_heads) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.randn(1,1,d_model)*0.02) # randn zeros\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim)# if out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = x.shape\n",
        "        # x = x * self.pos_enc(context_indices)\n",
        "        x = x + self.pos_emb[0,context_indices]\n",
        "\n",
        "        # pred_tokens = self.cls * self.pos_enc(trg_indices) # [M, num_trg_toks, d_model]\n",
        "        pred_tokens = self.cls + self.pos_emb[0,trg_indices]\n",
        "        # print(\"pred fwd\", x.shape, pred_tokens.shape)\n",
        "        x = torch.cat([x, pred_tokens], dim=1) # [batch, seq_len+num_trg_toks, d_model]\n",
        "        out = self.transformer(x)\n",
        "\n",
        "        out = self.norm(out)\n",
        "\n",
        "        out = out[:,seq:] # [batch, num_trg_toks, d_model]\n",
        "        # ind = torch.cat([context_indices, trg_indices], dim=-1).unsqueeze(-1).repeat(1,1,x.shape[-1]) # [b,t]->[b,t,d]\n",
        "        # out=torch.gather(out, 1, ind)\n",
        "\n",
        "        # print(\"pred fwd\", context_indices.shape, trg_indices.shape, out.shape)\n",
        "\n",
        "        out = self.lin(out)\n",
        "        # print(\"pred fwd\", out.shape)\n",
        "        return out # [seq_len, batch_size, ntoken]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ardu1zJwdHM3",
        "outputId": "7cdcf6fd-bb9b-47ef-e2ac-7976d1182ab7",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "245184\n",
            "torch.Size([])\n"
          ]
        }
      ],
      "source": [
        "# @title IJEPA\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class IJEPA(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=64, out_dim=None, nlayers=1, n_heads=4):\n",
        "        super().__init__()\n",
        "        self.out_dim = out_dim = out_dim or d_model\n",
        "        self.patch_size = 4 # 4\n",
        "        # self.student = Hiera(self.patch_size, in_dim, d_model, out_dim=out_dim, n_heads=n_heads, nlayers=nlayers, drop=0.)\n",
        "        self.student = ViT(self.patch_size, in_dim, d_model, out_dim=out_dim, n_heads=n_heads, nlayers=nlayers, drop=0.)\n",
        "        self.predicter = TransformerPredictor(out_dim, d_model//2, out_dim, n_heads=4, nlayers=1, drop=0.)\n",
        "        # self.predicter = TransformerPredictor(out_dim, 3*d_model//8, out_dim, n_heads=4, nlayers=1, drop=0.)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "\n",
        "    def loss(self, x): #\n",
        "        b,c,h,w = x.shape\n",
        "        # print('ijepa loss x',x.shape)\n",
        "        hw=(8,8)\n",
        "        # hw=(32,32)\n",
        "        mask_collator = MaskCollator(hw, enc_mask_scale=(.85, 1.), pred_mask_scale=(.15, .2), aspect_ratio=(.75, 1.5), nenc=1, npred=4, min_keep=4, allow_overlap=False)\n",
        "        collated_masks_enc, collated_masks_pred = mask_collator(b)\n",
        "        context_indices, trg_indices = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "        # print(context_indices.shape, trg_indices.shape)\n",
        "\n",
        "        # context_indices, trg_indices = simplexmask2d(hw, ctx_scale=(.85,1), trg_scale=(.7,.8), B=b, chaos=[3,.5])\n",
        "        # context_indices, trg_indices = simplexmask2d(hw, ctx_scale=(.85,1), trg_scale=(.7,.8), B=b, chaos=[5,.5])\n",
        "        # context_indices, trg_indices = simplexmask2d(hw, ctx_scale=(.05,.2), trg_scale=(.7,.8), B=b, chaos=[3,1])\n",
        "        # context_indices, trg_indices = context_indices.repeat(b,1), trg_indices.repeat(b,1)\n",
        "\n",
        "        # zero_mask = torch.zeros(b ,*hw, device=device).flatten(1)\n",
        "        # zero_mask[torch.arange(b).unsqueeze(-1), context_indices] = 1\n",
        "        # x_ = x * F.interpolate(zero_mask.reshape(b,1,*hw), size=x.shape[2:], mode='nearest-exact') # zero masked locations\n",
        "\n",
        "        sx = self.student(x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "        # print('ijepa loss sx',sx.shape)\n",
        "        sy_ = self.predicter(sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "        sy_ = F.layer_norm(sy_, (sy_.size(-1),))\n",
        "        with torch.no_grad():\n",
        "            sy = self.teacher(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "            sy = sy[torch.arange(sy.shape[0]).unsqueeze(-1), trg_indices] # [batch, num_context_toks, d_model] # nan bec len(trg_ind)==0 # print('loss sy',torch.isnan(sy).any())\n",
        "            sy = F.layer_norm(sy, (sy.size(-1),))\n",
        "        loss = F.mse_loss(sy, sy_)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        sx = self.student(x)\n",
        "        out = sx.mean(dim=1)\n",
        "        return out\n",
        "\n",
        "# min_s=0.15, max_s, M\n",
        "# trg.15.2M4 C.85 1\n",
        "# dont normalise data\n",
        "# randmsk < simplex 1msk < multiblk\n",
        "# teacher=trans ~? teacher=copy\n",
        "# learn convemb\n",
        "# lr pred,student: 3e-3/1e-2, 1e-3\n",
        "\n",
        "# lr 1e-3 < 1e-2,1e-3? slower but dun plateau\n",
        "\n",
        "# vit 1lyr 15.4sec 15.6\n",
        "# hiera downsampling attn 20.5\n",
        "\n",
        "# ijepa = IJEPA(in_dim=3, d_model=64, out_dim=64, nlayers=4, n_heads=4).to(device)\n",
        "ijepa = IJEPA(in_dim=3, d_model=64, out_dim=64, nlayers=1, n_heads=8).to(device)\n",
        "# ijepa = IJEPA(in_dim=3, d_model=32, out_dim=32, nlayers=1, n_heads=4).to(device)\n",
        "# optim = torch.optim.AdamW(ijepa.parameters(), lr=1e-3) # 1e-3?\n",
        "optim = torch.optim.AdamW([{'params': ijepa.student.parameters()},\n",
        "#     {'params': ijepa.predicter.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # good 3e-3,1e-3 ; default 1e-2, 5e-2\n",
        "    {'params': ijepa.predicter.parameters(), 'lr': 1e-2}], lr=1e-3, weight_decay=1e-2)\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/configs/in1k_vith14_ep300.yaml\n",
        "# d_model 1024,384\n",
        "# depth 12,6/12\n",
        "# wd 5e-2 - 4e-1\n",
        "# adamw 1e-4 - 1e-3 - 1e-6\n",
        "# ema 0.996-1\n",
        "\n",
        "print(sum(p.numel() for p in ijepa.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in ijepa.parameters())) # 27584\n",
        "# print(sum(p.numel() for p in ijepa.predicter.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# d_model^2 * nlayers\n",
        "\n",
        "x = torch.rand((64,3,32,32), device=device)\n",
        "out = ijepa.loss(x)\n",
        "print(out.shape)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(ijepa.out_dim, 10).to(device)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n",
        "# optim = torch.optim.AdamW([{'params': ijepa.parameters()}, {'params': classifier.parameters(), 'lr': 1e-3}], lr=1e-3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title mae me enc,dec nocls\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    def __init__(self, patch_size, in_chans, embed_dim):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.proj = nn.Conv1d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "    def forward(self, x):\n",
        "        b,t,d = x.shape\n",
        "        pad = (self.patch_size - t % self.patch_size) % self.patch_size\n",
        "        x = F.pad(x, (0,pad))\n",
        "        return self.proj(x)\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    def __init__(self, patch_size, in_chans, embed_dim):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "    def forward(self, x):\n",
        "        b,c,h,w = x.shape\n",
        "        pad_h = (self.patch_size[0] - h % self.patch_size[0]) % self.patch_size[0]\n",
        "        pad_w = (self.patch_size[1] - w % self.patch_size[1]) % self.patch_size[1]\n",
        "        x = F.pad(x, (0, pad_w, 0, pad_h))\n",
        "        x = self.proj(x)\n",
        "        x = x.flatten(2).transpose(1, 2)  # NCHW -> NLC\n",
        "        return x\n",
        "# self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, drop=0):\n",
        "    def __init__(self, patch_size, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0):\n",
        "        super().__init__()\n",
        "        act = nn.ReLU() # ReLU SiLU GELU\n",
        "        # self.embed = nn.Sequential(\n",
        "        #     # # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.MaxPool1d(2,2), #nn.MaxPool1d(3, 2, 3//2),\n",
        "        #     nn.Conv1d(in_dim, d_model,7,2,7//2), nn.BatchNorm1d(d_model), act,\n",
        "        #     nn.Conv1d(d_model, d_model,5,2,5//2), nn.BatchNorm1d(d_model), act,\n",
        "        #     # nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), act, nn.MaxPool1d(2,2),\n",
        "        #     # nn.Conv1d(d_model, d_model,3,2,3//2), nn.BatchNorm1d(d_model), act, nn.MaxPool1d(2,2),\n",
        "        #     nn.Conv1d(d_model, d_model,3,2,3//2),\n",
        "\n",
        "        #     # nn.Conv1d(in_dim, d_model, patch_size, patch_size), # like patch\n",
        "        #     # nn.Conv1d(in_dim, d_model, 1, 1), # like patch\n",
        "\n",
        "        #     # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.Dropout(drop), nn.BatchNorm1d(d_model), snake,\n",
        "        #     )\n",
        "\n",
        "        # self.embed = PatchEmbed(patch_size, in_dim, d_model)\n",
        "        self.embed = PatchEmbed((patch_size, patch_size), in_dim, d_model)\n",
        "\n",
        "        # self.pos_emb = nn.Parameter(torch.randn(1, 200, d_model)*.02)\n",
        "        self.pos_emb = RoPE(d_model, seq_len=500, base=10000)\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, n_heads=n_heads) for _ in range(nlayers)])\n",
        "        self.norm = nn.LayerNorm(d_model) # LayerNorm RMSNorm\n",
        "        # self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, mask_indices=None): # [b,t,in], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # x = self.embed(x.transpose(-2,-1)).transpose(-2,-1) # [b,t,d]\n",
        "        x = self.embed(x) # [b,t,d]\n",
        "        # print(\"mae enc fwd\",x.shape)\n",
        "        b,t = x.shape[:2]\n",
        "        x = x + self.pos_emb[:,:t]\n",
        "        # x = x * self.pos_emb[:,:t]\n",
        "        if mask_indices != None: x = x[torch.arange(b).unsqueeze(-1), mask_indices] # [batch, num_context_toks, d_model]\n",
        "        x = self.transformer(x)\n",
        "        out = self.norm(x)\n",
        "        # if self.lin: out = self.lin(out)\n",
        "        return out\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        # self.pos_emb = nn.Parameter(torch.randn(1, 200, d_model)*.02)\n",
        "        self.pos_emb = RoPE(d_model, seq_len=200, base=10000)\n",
        "\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, n_heads=n_heads) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.randn(1,1,d_model)*0.02) # randn zeros\n",
        "        self.norm = nn.LayerNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim or d_model)\n",
        "\n",
        "    def forward(self, x, mask_indices, trg_indices): # [b,m,d], [b,m], [b,t-m]\n",
        "        x = self.embed(x) # [b,m,d]\n",
        "        ids_restore = torch.cat([mask_indices, trg_indices], dim=-1).unsqueeze(-1).repeat(1,1,x.shape[-1]) # [b,t]\n",
        "\n",
        "        x = torch.cat([x, self.cls.repeat(x.shape[0],trg_indices.shape[1],1)], dim=1) # [b,m+(t-m),d]\n",
        "        # print(\"Trans pred\",x.shape, ids_restore.shape)\n",
        "        x = torch.zeros_like(x, device=device).scatter_(dim=1, index=ids_restore.to(device), src=x) # unshuffle # The backward pass is implemented only for src.shape == index.shape\n",
        "        # x = torch.scatter(torch.zeros_like(x), dim=1, index=ids, src=x) # unshuffle\n",
        "        x = x + self.pos_emb[0,:x.shape[1]]\n",
        "\n",
        "        out = self.transformer(x)\n",
        "        out = self.norm(out)\n",
        "        out = self.lin(out)\n",
        "        return out # [b,t,d]\n",
        "\n",
        "\n",
        "# b,c,h,w = 2,3,64,64\n",
        "# patch_size=8\n",
        "# model = Encoder(patch_size, c, d_model, n_heads=4, nlayers=1, drop=0.).to(device)\n",
        "# print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "# x =  torch.rand((b,c,h,w), device=device)\n",
        "# out = model(x)\n",
        "# print(out.shape)\n",
        "# # # print(out)\n",
        "# model = Decoder(in_dim, d_model, out_dim=None, n_heads=4, nlayers=1).to(device)\n",
        "# out = model(out)\n",
        "# print(out.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "Ve_r-NBWZBA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title MAE\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# def patchify(seq, p=16): # [b,c,h*p,w*p] [b,t*p,d]\n",
        "#     return seq.unflatten(1, (seq.shape[1]//p, p)).flatten(2) # [b,t,p*d]\n",
        "\n",
        "def patchify(imgs, p=16): # [b,c,h,w]\n",
        "    assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0\n",
        "    h = w = imgs.shape[2] // p\n",
        "    x = imgs.reshape(imgs.shape[0],3,h,p,w,p)\n",
        "    x = torch.einsum('nchpwq->nhwpqc', x)\n",
        "    x = x.reshape(imgs.shape[0], h*w, p**2 *3)\n",
        "    return x # [b, h*w, p**2 *3]\n",
        "\n",
        "class MAE(nn.Module):\n",
        "    def __init__(self, in_dim=16, d_model=64, out_dim=None, nlayers=1, n_heads=8, drop=0):\n",
        "        super().__init__()\n",
        "        if out_dim is None: out_dim = d_model\n",
        "        self.patch_size = 2 # 8 32\n",
        "        enc_dim = dec_dim = d_model\n",
        "        self.encoder = Encoder(self.patch_size, in_dim, enc_dim, out_dim=out_dim, n_heads=n_heads, nlayers=nlayers, drop=drop)\n",
        "        # self.decoder = Decoder(enc_dim, dec_dim, out_dim=self.patch_size*in_dim, n_heads=n_heads, nlayers=nlayers, drop=drop)\n",
        "        self.decoder = Decoder(enc_dim, dec_dim, out_dim=self.patch_size**2*in_dim, n_heads=n_heads, nlayers=nlayers, drop=drop)\n",
        "\n",
        "        # self.transform = RandomResizedCrop1d(3500, scale=(.8,1.))\n",
        "\n",
        "    #     self.apply(self.init_weights)\n",
        "    #     self.apply(self.zero_last_layers)\n",
        "    # def init_weights(self, m):\n",
        "    #     if isinstance(m, (nn.Linear, nn.Conv1d)):\n",
        "    #         torch.nn.init.normal_(m.weight, std=.02)\n",
        "    #         if m.bias is not None: nn.init.zeros_(m.bias)\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]\n",
        "        y = patchify(x, self.patch_size)\n",
        "        # batch, seq, dim = y.shape\n",
        "        b,c,h,w = x.shape\n",
        "        # print(x.shape, y.shape)\n",
        "        print((h//self.patch_size)*(w//self.patch_size))\n",
        "        # context_indices, trg_indices = random_masking(seq//self.patch_size, .3, b=batch)\n",
        "        context_indices, trg_indices = random_masking((h//self.patch_size)*(w//self.patch_size), .3, b=b)\n",
        "        print(len(context_indices), len(trg_indices))\n",
        "\n",
        "        # target_mask = multiblock(seq//self.patch_size, min_s=.2, max_s=.3, M=4, B=1).any(1).squeeze(1) # best.2.3M4 og.15.2M4# mask out targets to be predicted # [M, seq]\n",
        "\n",
        "        # print(target_mask.shape, x.shape)\n",
        "        # context_mask = ~multiblock(seq//self.patch_size, min_s=.85, max_s=1, M=1, B=1).squeeze(1)|target_mask # og .85,1.M1 # [1, seq], True->Mask\n",
        "        # context_mask = torch.zeros((1,seq//self.patch_size), dtype=bool)|target_mask # [1,h,w], True->Mask\n",
        "\n",
        "\n",
        "        # print('x',x.shape, context_indices.shape, trg_indices.shape)\n",
        "\n",
        "        sx = self.encoder(x, mask_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "        # print('seq_jepa loss sx',sx.shape)\n",
        "        y_ = self.decoder(sx, mask_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "        print(y.shape, y_.shape)\n",
        "        y = y[torch.arange(b).unsqueeze(-1), trg_indices] # [batch, num_context_toks, d_model] # nan bec len(trg_ind)==0 # print('loss sy',torch.isnan(sy).any())\n",
        "        y_ = y_[torch.arange(b).unsqueeze(-1), trg_indices] # [batch, num_context_toks, d_model] # nan bec len(trg_ind)==0 # print('loss sy',torch.isnan(sy).any())\n",
        "\n",
        "        loss = F.mse_loss(y, y_)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x): # [b,T,3]\n",
        "        sx = self.encoder(x)\n",
        "        out = sx.mean(dim=1)\n",
        "        return out\n",
        "\n",
        "# try:\n",
        "#     in_dim = X[0].shape[-1] # 3\n",
        "#     out_dim = train_data.vocab_size # 16\n",
        "# except NameError:\n",
        "#     in_dim, out_dim = 3,16\n",
        "# print(in_dim, out_dim)\n",
        "d_model=64\n",
        "in_dim=3\n",
        "mae = MAE(in_dim=in_dim, d_model=d_model, out_dim=None, nlayers=1, n_heads=8, drop=.0).to(device)#.to(torch.float)\n",
        "optim = torch.optim.AdamW(mae.parameters(), lr=1e-3) # 1e-3? default 1e-2\n",
        "# optim = torch.optim.AdamW(mae.parameters(), lr=3e-4) # 1e-3? default 1e-2\n",
        "# optim = torch.optim.AdamW([{'params': mae.encoder.parameters()},\n",
        "#     {'params': mae.decoder.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # default 1e-2, 5e-2\n",
        "    # {'params': mae.decoder.parameters(), 'lr': 1e-2}], lr=1e-3, weight_decay=1e-2)\n",
        "\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/configs/in1k_vith14_ep300.yaml\n",
        "# d_model 1024,384\n",
        "# depth 12,6/12\n",
        "# wd 5e-2 - 4e-1\n",
        "# adamw 1e-4 - 1e-3 - 1e-6\n",
        "# ema 0.996-1\n",
        "\n",
        "print(sum(p.numel() for p in mae.parameters() if p.requires_grad)) # 27584\n",
        "\n",
        "# x = torch.rand((24, 1600, in_dim), device=device)\n",
        "x = torch.rand((2, in_dim,32,32), device=device)\n",
        "out = mae.loss(x)\n",
        "print(out.shape)\n",
        "\n",
        "out_dim=d_model\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(d_model, out_dim).to(device)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3) # 1e-3\n",
        "# optim = torch.optim.AdamW([{'params': mae.parameters()}, {'params': classifier.parameters(), 'lr': 1e-3}], lr=1e-3)\n",
        "# coptim = torch.optim.AdamW(classifier.parameters(), lr=1e-3)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "0_lQ9en7ZDbf",
        "outputId": "f4fd4043-0986-46a5-f4ce-9cae777b506b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "106316\n",
            "256\n",
            "2 2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (256) must match the size of tensor b (200) at non-singleton dimension 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-22-3170484621.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;31m# x = torch.rand((24, 1600, in_dim), device=device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-22-3170484621.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0msx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext_indices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [batch, num_context_toks, out_dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# print('seq_jepa loss sx',sx.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0my_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrg_indices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [batch*M, num_trg_toks, out_dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_indices\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# [batch, num_context_toks, d_model] # nan bec len(trg_ind)==0 # print('loss sy',torch.isnan(sy).any())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-14-1469908503.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask_indices, trg_indices)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mids_restore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# unshuffle # The backward pass is implemented only for src.shape == index.shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;31m# x = torch.scatter(torch.zeros_like(x), dim=1, index=ids, src=x) # unshuffle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_emb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (256) must match the size of tensor b (200) at non-singleton dimension 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODpKypTCsfIt",
        "outputId": "b2f4380b-03a8-49be-95b5-eed2b564dc05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 16])\n"
          ]
        }
      ],
      "source": [
        "# @title TransformerVICReg\n",
        "import torch\n",
        "from torch import nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class TransformerVICReg(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, drop=0):\n",
        "        super().__init__()\n",
        "        act = nn.GELU()\n",
        "        patch_size=4\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Linear(in_dim, d_model), act\n",
        "            # nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            # nn.Conv1d(d_model, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            # nn.Conv1d(d_model, d_model,3,2,3//2),\n",
        "            nn.Conv2d(in_dim, d_model, patch_size, patch_size), # patch\n",
        "            )\n",
        "        self.pos_enc = RoPE(d_model, seq_len=200, base=10000)\n",
        "        # self.pos_emb = nn.Parameter(torch.randn(1, 8*8, d_model))\n",
        "        # self.pos_emb = nn.Parameter(RoPE2D(dim=d_model, h=8, w=8, base=10000).unsqueeze(0), requires_grad=False)\n",
        "\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "\n",
        "        # out_dim = out_dim or d_model\n",
        "        self.lin = nn.Linear(d_model, out_dim)\n",
        "        self.attn_pool = nn.Linear(d_model, 1, bias=False)\n",
        "\n",
        "        dim_v = d_model * 4\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(out_dim, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v, bias=False),\n",
        "            )\n",
        "\n",
        "    def forward(self, x): # [b,t,d] / [b,c,h,w]\n",
        "        x = self.embed(x).flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c]\n",
        "        # x = self.embed(x.transpose(-2,-1)).transpose(-2,-1) # [b,t,d]\n",
        "        x = self.pos_enc(x)\n",
        "        # x = x + self.pos_emb\n",
        "\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        attn = self.attn_pool(x).squeeze(-1) # [batch, seq] # seq_pool\n",
        "        out = (torch.softmax(attn, dim=-1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, seq] @ [batch, seq, dim] -> [batch, dim]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch, ntoken]\n",
        "\n",
        "    def expand(self, x):\n",
        "        sx = self.forward(x)\n",
        "        vx = self.exp(sx)\n",
        "        return vx\n",
        "\n",
        "batch, seq_len, d_model = 4,3500,512\n",
        "in_dim, out_dim=3,16\n",
        "model = TransformerVICReg(in_dim, d_model, out_dim, d_head=4, nlayers=2, drop=0.).to(device)\n",
        "# x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "x =  torch.rand((batch, in_dim, 32,32), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qwg9dG4sQ3h",
        "outputId": "2f0092f5-5bb7-4725-db2b-dcf7b0ba1a96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "62850\n",
            "in vicreg  0.00019920778413506923 24.74469393491745 4.793645480560826e-09\n",
            "(tensor(7.9683e-06, grad_fn=<MseLossBackward0>), tensor(0.9898, grad_fn=<AddBackward0>), tensor(4.7936e-09, grad_fn=<AddBackward0>))\n"
          ]
        }
      ],
      "source": [
        "# @title Violet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class Violet(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, nlayers=2, d_head=4):\n",
        "        super().__init__()\n",
        "        out_dim = out_dim or d_model\n",
        "        self.student = TransformerVICReg(in_dim, d_model, out_dim=out_dim, d_head=d_head, nlayers=nlayers, drop=0.)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "\n",
        "        # vicreg\n",
        "        self.sim_coeff=25. # 10.0 # 25.0 # λ\n",
        "        self.std_coeff=25. # 1.0 # 25.0 # µ\n",
        "        self.cov_coeff=1. # 25.0 # 1.0 # ν\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]c/ [b,c,h,w]\n",
        "        # print(x.shape)\n",
        "        # mask = simplexmask(hw=(8,8), scale=(.7,.8)).unsqueeze(0) # .6.8\n",
        "        # vx = self.student.expand(x, mask) # [batch, num_context_toks, out_dim]\n",
        "\n",
        "        vx = self.student.expand(x) # [batch, num_context_toks, out_dim]\n",
        "\n",
        "        with torch.no_grad(): vy = self.teacher.expand(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "        loss = self.vicreg(vx, vy)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        return self.student(x)\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size, num_features = x.shape\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = self.sim_coeff * repr_loss + self.std_coeff * std_loss + self.cov_coeff * cov_loss\n",
        "        print(\"in vicreg \",self.sim_coeff * repr_loss.item() , self.std_coeff * std_loss.item() , self.cov_coeff * cov_loss.item())\n",
        "        # return loss\n",
        "        return repr_loss, std_loss, cov_loss\n",
        "\n",
        "\n",
        "violet = Violet(in_dim=3, d_model=32, out_dim=16, nlayers=2, d_head=4).to(device)\n",
        "voptim = torch.optim.AdamW(violet.parameters(), lr=1e-3) # 1e-3?\n",
        "# voptim = torch.optim.AdamW([{'params': violet.student.transformer.parameters()},\n",
        "#     {'params': violet.student.exp.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # default 1e-2\n",
        "print(sum(p.numel() for p in violet.parameters() if p.requires_grad)) # 27584\n",
        "\n",
        "# x = torch.rand((2,1000,3), device=device)\n",
        "x = torch.rand((2,3,32,32), device=device)\n",
        "# x = torch.rand((2,1,16), device=device)\n",
        "loss = violet.loss(x)\n",
        "# print(out.shape)\n",
        "print(loss)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(16).to(device)\n",
        "# classifier = Classifier(16, 18).to(device)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_BFm8Kh-j3u"
      },
      "outputs": [],
      "source": [
        "for name, param in ijepa.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name, param)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "eb1n4BhimG_N"
      },
      "outputs": [],
      "source": [
        "# @title RankMe\n",
        "# RankMe: Assessing the Downstream Performance of Pretrained Self-Supervised Representations by Their Rank jun 2023 https://arxiv.org/pdf/2210.02885\n",
        "import torch\n",
        "\n",
        "# https://github.com/Spidartist/IJEPA_endoscopy/blob/main/src/helper.py#L22\n",
        "def RankMe(Z):\n",
        "    \"\"\"\n",
        "    Calculate the RankMe score (the higher, the better).\n",
        "    RankMe(Z) = exp(-sum_{k=1}^{min(N, K)} p_k * log(p_k)),\n",
        "    where p_k = sigma_k (Z) / ||sigma_k (Z)||_1 + epsilon\n",
        "    where sigma_k is the kth singular value of Z.\n",
        "    where Z is the matrix of embeddings (N × K)\n",
        "    \"\"\"\n",
        "    # compute the singular values of the embeddings\n",
        "    # _u, s, _vh = torch.linalg.svd(Z, full_matrices=False)  # s.shape = (min(N, K),)\n",
        "    # s = torch.linalg.svd(Z, full_matrices=False).S\n",
        "    s = torch.linalg.svdvals(Z)\n",
        "    p = s / torch.sum(s, axis=0) + 1e-7\n",
        "    return torch.exp(-torch.sum(p * torch.log(p)))\n",
        "\n",
        "# Z = torch.randn(5, 3)\n",
        "# rankme = RankMe(Z)\n",
        "# print(rankme)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "y24ZNFqW7woQ"
      },
      "outputs": [],
      "source": [
        "# @title LiDAR\n",
        "# LiDAR: Sensing Linear Probing Performance In Joint Embedding SSL Architectures https://arxiv.org/pdf/2312.04000\n",
        "# https://github.com/rbalestr-lab/stable-ssl/blob/main/stable_ssl/monitors.py#L106\n",
        "\n",
        "def LiDAR(sx, eps=1e-7, delta=1e-3):\n",
        "    sx = sx.unflatten(0, (-1, 2))\n",
        "    n, q, d = sx.shape\n",
        "    mu_x = sx.mean(dim=1) # mu_x # [n,d]\n",
        "    mu = mu_x.mean(dim=0) # mu # [d]\n",
        "\n",
        "    diff_b = (mu_x - mu).unsqueeze(-1) # [n,d,1]\n",
        "    S_b = (diff_b @ diff_b.transpose(-2,-1)).sum(0) / (n - 1) # [n,d,d] -> [d,d]\n",
        "    diff_w = (sx - mu_x.unsqueeze(1)).reshape(-1,d,1) # [n,q,d] -> [nq,d,1]\n",
        "    S_w = (diff_w @ diff_w.transpose(-2,-1)).sum(0) / (n * (q - 1)) + delta * torch.eye(d, device=sx.device) # [nq,d,d] -> [d,d]\n",
        "\n",
        "    eigvals_w, eigvecs_w = torch.linalg.eigh(S_w)\n",
        "    eigvals_w = torch.clamp(eigvals_w, min=eps)\n",
        "\n",
        "    invsqrt_w = (eigvecs_w * (1. / torch.sqrt(eigvals_w))) @ eigvecs_w.transpose(-1, -2)\n",
        "    S_lidar = invsqrt_w @ S_b @ invsqrt_w\n",
        "    lam = torch.linalg.eigh(S_lidar)[0].clamp(min=0)\n",
        "    p = lam / lam.sum() + eps\n",
        "    return torch.exp(-torch.sum(p * torch.log(p)))\n",
        "\n",
        "# sx = torch.randn(32, 128)\n",
        "# print(sx.shape)\n",
        "# lidar = LiDAR(sx)\n",
        "# print(lidar.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "2Nd-sGe6Ku4S",
        "outputId": "de1c0a6b-c9d6-4150-d91c-6576752ece06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbobdole\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250623_081427-x6eb4oee</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/ijepa/runs/x6eb4oee' target=\"_blank\">polished-deluge-93</a></strong> to <a href='https://wandb.ai/bobdole/ijepa' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/ijepa' target=\"_blank\">https://wandb.ai/bobdole/ijepa</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/ijepa/runs/x6eb4oee' target=\"_blank\">https://wandb.ai/bobdole/ijepa/runs/x6eb4oee</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "!pip install -q wandb\n",
        "import wandb # https://docs.wandb.ai/quickstart\n",
        "wandb.login(key='487a2109e55dce4e13fc70681781de9f50f27be7')\n",
        "try: run.finish()\n",
        "except NameError: pass\n",
        "run = wandb.init(project=\"ijepa\", config={\"model\": \"res18\",})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "34de8800-fa4b-4460-803c-3cd8dbfb6c70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3555cc6f-a454-4e16-f5ec-cd7c80f6a851"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "classify 2.922553300857544\n",
            "classify 2.969341993331909\n",
            "0.125\n",
            "0.09375\n",
            "0.203125\n",
            "0.109375\n",
            "0.09375\n",
            "0.078125\n",
            "0.09375\n",
            "0.109375\n",
            "0.140625\n",
            "0.140625\n",
            "0.1875\n",
            "time: 16.488832235336304 16.849884463595107\n",
            "77\n",
            "strain 0.03027847595512867\n",
            "strain 0.025553392246365547\n",
            "strain 0.03062990866601467\n",
            "strain 0.028563112020492554\n",
            "strain 0.029227452352643013\n",
            "strain 0.02440505288541317\n",
            "classify 2.8592865467071533\n",
            "classify 2.9229307174682617\n",
            "classify 2.9369935989379883\n",
            "classify 2.9903581142425537\n",
            "classify 2.916908025741577\n",
            "classify 2.893395185470581\n",
            "classify 2.974020481109619\n",
            "classify 2.888070583343506\n",
            "classify 2.90563702583313\n",
            "classify 2.926546096801758\n",
            "classify 2.8677120208740234\n",
            "0.09375\n",
            "0.1875\n",
            "0.109375\n",
            "0.09375\n",
            "0.09375\n",
            "0.078125\n",
            "0.1875\n",
            "0.125\n",
            "0.15625\n",
            "0.203125\n",
            "0.03125\n",
            "time: 17.752708673477173 16.861466520871872\n",
            "78\n",
            "strain 0.03126025199890137\n",
            "strain 0.029160305857658386\n",
            "strain 0.03282711282372475\n",
            "strain 0.0291092861443758\n",
            "strain 0.03112727403640747\n",
            "strain 0.029649533331394196\n",
            "classify 2.891587972640991\n",
            "classify 2.914668083190918\n",
            "classify 2.894951581954956\n",
            "classify 2.906510829925537\n",
            "classify 2.9298155307769775\n",
            "classify 2.934736967086792\n",
            "classify 2.8877992630004883\n",
            "classify 2.9212028980255127\n",
            "classify 2.9607958793640137\n",
            "classify 2.8999087810516357\n",
            "classify 2.8951668739318848\n",
            "0.125\n",
            "0.125\n",
            "0.140625\n",
            "0.125\n",
            "0.15625\n",
            "0.15625\n",
            "0.078125\n",
            "0.15625\n",
            "0.046875\n",
            "0.09375\n",
            "0.140625\n",
            "time: 16.573262453079224 16.857825897916964\n",
            "79\n",
            "strain 0.027971863746643066\n",
            "strain 0.02678084932267666\n",
            "strain 0.02799077518284321\n",
            "strain 0.02968926727771759\n",
            "strain 0.026928706094622612\n",
            "strain 0.025684794411063194\n",
            "classify 2.8830060958862305\n",
            "classify 2.961493968963623\n",
            "classify 2.943603038787842\n",
            "classify 2.884119749069214\n",
            "classify 2.8828189373016357\n",
            "classify 2.878918170928955\n",
            "classify 2.8494884967803955\n",
            "classify 2.888597011566162\n",
            "classify 2.8792693614959717\n",
            "classify 2.940941095352173\n",
            "classify 2.88720440864563\n",
            "0.109375\n",
            "0.109375\n",
            "0.125\n",
            "0.1875\n",
            "0.078125\n",
            "0.15625\n",
            "0.09375\n",
            "0.09375\n",
            "0.140625\n",
            "0.140625\n",
            "0.15625\n",
            "time: 16.451395988464355 16.852753537893296\n",
            "80\n",
            "strain 0.027752185240387917\n",
            "strain 0.02679276466369629\n",
            "strain 0.03182785585522652\n",
            "strain 0.02812599577009678\n",
            "strain 0.028950562700629234\n",
            "strain 0.029160859063267708\n",
            "classify 2.850963830947876\n",
            "classify 2.9111406803131104\n",
            "classify 2.9359657764434814\n",
            "classify 2.872786283493042\n",
            "classify 2.8429298400878906\n",
            "classify 2.9323511123657227\n",
            "classify 2.9033706188201904\n",
            "classify 2.927424907684326\n",
            "classify 2.9360947608947754\n",
            "classify 2.845505952835083\n",
            "classify 2.9279658794403076\n",
            "0.171875\n",
            "0.171875\n",
            "0.09375\n",
            "0.140625\n",
            "0.03125\n",
            "0.171875\n",
            "0.109375\n",
            "0.140625\n",
            "0.0625\n",
            "0.109375\n",
            "0.125\n",
            "time: 16.520928621292114 16.848663789254648\n",
            "81\n",
            "strain 0.028526445850729942\n",
            "strain 0.028740793466567993\n",
            "strain 0.028269795700907707\n",
            "strain 0.02575278840959072\n",
            "strain 0.028025316074490547\n",
            "strain 0.028828954324126244\n",
            "classify 2.843233108520508\n",
            "classify 2.896371841430664\n",
            "classify 2.8432705402374268\n",
            "classify 2.8763768672943115\n",
            "classify 2.912579298019409\n",
            "classify 2.8776180744171143\n",
            "classify 2.8175711631774902\n",
            "classify 2.8475029468536377\n",
            "classify 2.8805360794067383\n",
            "classify 2.806497573852539\n",
            "classify 2.85036563873291\n",
            "0.109375\n",
            "0.078125\n",
            "0.109375\n",
            "0.171875\n",
            "0.140625\n",
            "0.078125\n",
            "0.140625\n",
            "0.140625\n",
            "0.15625\n",
            "0.125\n",
            "0.140625\n",
            "time: 17.706251621246338 16.859128591490954\n",
            "82\n",
            "strain 0.028426870703697205\n",
            "strain 0.027396807447075844\n",
            "strain 0.028211938217282295\n",
            "strain 0.029827600345015526\n",
            "strain 0.026303626596927643\n",
            "strain 0.029963424429297447\n",
            "classify 2.872493267059326\n",
            "classify 2.849607467651367\n",
            "classify 2.8612608909606934\n",
            "classify 2.8640706539154053\n",
            "classify 2.906083583831787\n",
            "classify 2.9055330753326416\n",
            "classify 2.8781163692474365\n",
            "classify 2.918166160583496\n",
            "classify 2.892766237258911\n",
            "classify 2.859227418899536\n",
            "classify 2.888890027999878\n",
            "0.171875\n",
            "0.125\n",
            "0.109375\n",
            "0.140625\n",
            "0.109375\n",
            "0.15625\n",
            "0.109375\n",
            "0.109375\n",
            "0.078125\n",
            "0.046875\n",
            "0.21875\n",
            "time: 16.517675161361694 16.855022309774377\n",
            "83\n",
            "strain 0.026632634922862053\n",
            "strain 0.030746005475521088\n",
            "strain 0.026961689814925194\n",
            "strain 0.02847941778600216\n",
            "strain 0.02653711847960949\n",
            "strain 0.02917446196079254\n",
            "classify 2.896458625793457\n",
            "classify 2.8639912605285645\n",
            "classify 2.8232789039611816\n",
            "classify 2.8301784992218018\n",
            "classify 2.876584529876709\n",
            "classify 2.8429458141326904\n",
            "classify 2.844116687774658\n",
            "classify 2.889849901199341\n",
            "classify 2.8850796222686768\n",
            "classify 2.86631441116333\n",
            "classify 2.874358892440796\n",
            "0.09375\n",
            "0.125\n",
            "0.0625\n",
            "0.109375\n",
            "0.09375\n",
            "0.078125\n",
            "0.125\n",
            "0.140625\n",
            "0.140625\n",
            "0.09375\n",
            "0.125\n",
            "time: 16.482384204864502 16.850592837447213\n",
            "84\n",
            "strain 0.028127960860729218\n",
            "strain 0.030422789976000786\n",
            "strain 0.028755271807312965\n",
            "strain 0.02605748362839222\n",
            "strain 0.03164689615368843\n",
            "strain 0.03005053661763668\n",
            "classify 2.887275218963623\n",
            "classify 2.914001703262329\n",
            "classify 2.8691744804382324\n",
            "classify 2.835897445678711\n",
            "classify 2.789750814437866\n",
            "classify 2.8238115310668945\n",
            "classify 2.8233580589294434\n",
            "classify 2.792919635772705\n",
            "classify 2.8810911178588867\n",
            "classify 2.8376598358154297\n",
            "classify 2.8613061904907227\n",
            "0.1875\n",
            "0.125\n",
            "0.09375\n",
            "0.109375\n",
            "0.109375\n",
            "0.125\n",
            "0.140625\n",
            "0.203125\n",
            "0.078125\n",
            "0.125\n",
            "0.09375\n",
            "time: 16.74968934059143 16.849411984050974\n",
            "85\n",
            "strain 0.0248695258051157\n",
            "strain 0.02543056197464466\n",
            "strain 0.03005075454711914\n",
            "strain 0.028777921572327614\n",
            "strain 0.024984115734696388\n",
            "strain 0.02637369930744171\n",
            "classify 2.8591649532318115\n",
            "classify 2.827458620071411\n",
            "classify 2.839036703109741\n",
            "classify 2.8411779403686523\n",
            "classify 2.8160486221313477\n",
            "classify 2.837812662124634\n",
            "classify 2.8638553619384766\n",
            "classify 2.784883499145508\n",
            "classify 2.830655336380005\n",
            "classify 2.800640344619751\n",
            "classify 2.8272204399108887\n",
            "0.09375\n",
            "0.125\n",
            "0.09375\n",
            "0.125\n",
            "0.09375\n",
            "0.125\n",
            "0.03125\n",
            "0.171875\n",
            "0.109375\n",
            "0.21875\n",
            "0.171875\n",
            "time: 17.401273250579834 16.855836358181264\n",
            "86\n",
            "strain 0.028218448162078857\n",
            "strain 0.030447224155068398\n",
            "strain 0.02298658899962902\n",
            "strain 0.03175525739789009\n",
            "strain 0.024708211421966553\n",
            "strain 0.028944410383701324\n",
            "classify 2.8267288208007812\n",
            "classify 2.8819024562835693\n",
            "classify 2.8638062477111816\n",
            "classify 2.823491096496582\n",
            "classify 2.86155104637146\n",
            "classify 2.8498029708862305\n",
            "classify 2.821742296218872\n",
            "classify 2.815690040588379\n",
            "classify 2.8374180793762207\n",
            "classify 2.795469045639038\n",
            "classify 2.830551862716675\n",
            "0.109375\n",
            "0.140625\n",
            "0.09375\n",
            "0.125\n",
            "0.109375\n",
            "0.140625\n",
            "0.125\n",
            "0.15625\n",
            "0.078125\n",
            "0.140625\n",
            "0.140625\n",
            "time: 16.46751594543457 16.851380849706715\n",
            "87\n",
            "strain 0.028519706800580025\n",
            "strain 0.02657126821577549\n",
            "strain 0.02670697122812271\n",
            "strain 0.02833106927573681\n",
            "strain 0.028354084119200706\n",
            "strain 0.03025180846452713\n",
            "classify 2.867856502532959\n",
            "classify 2.909836530685425\n",
            "classify 2.8435943126678467\n",
            "classify 2.863586187362671\n",
            "classify 2.8144547939300537\n",
            "classify 2.8410799503326416\n",
            "classify 2.838230848312378\n",
            "classify 2.8266212940216064\n",
            "classify 2.8484842777252197\n",
            "classify 2.818173885345459\n",
            "classify 2.7898378372192383\n",
            "0.140625\n",
            "0.109375\n",
            "0.046875\n",
            "0.109375\n",
            "0.15625\n",
            "0.078125\n",
            "0.046875\n",
            "0.109375\n",
            "0.140625\n",
            "0.15625\n",
            "0.125\n",
            "time: 16.43509864807129 16.84665773402561\n",
            "88\n",
            "strain 0.02726658247411251\n",
            "strain 0.02965257503092289\n",
            "strain 0.028892135247588158\n",
            "strain 0.030033325776457787\n",
            "strain 0.0248013436794281\n",
            "strain 0.02348332293331623\n",
            "classify 2.89062237739563\n",
            "classify 2.831251621246338\n",
            "classify 2.809966564178467\n",
            "classify 2.8345892429351807\n",
            "classify 2.881958246231079\n",
            "classify 2.8351545333862305\n",
            "classify 2.8462700843811035\n",
            "classify 2.8757662773132324\n",
            "classify 2.8834426403045654\n",
            "classify 2.8463077545166016\n",
            "classify 2.867863655090332\n",
            "0.078125\n",
            "0.171875\n",
            "0.09375\n",
            "0.125\n",
            "0.109375\n",
            "0.09375\n",
            "0.09375\n",
            "0.109375\n",
            "0.09375\n",
            "0.125\n",
            "0.15625\n",
            "time: 17.02535915374756 16.848671749736486\n",
            "89\n",
            "strain 0.028132574632763863\n",
            "strain 0.025661712512373924\n",
            "strain 0.027576414868235588\n",
            "strain 0.030404673889279366\n",
            "strain 0.030180655419826508\n",
            "strain 0.030494099482893944\n",
            "classify 2.9370217323303223\n",
            "classify 2.883202075958252\n",
            "classify 2.8640177249908447\n",
            "classify 2.8866379261016846\n",
            "classify 2.8248443603515625\n",
            "classify 2.879523515701294\n",
            "classify 2.91416597366333\n",
            "classify 2.8852663040161133\n",
            "classify 2.906620740890503\n",
            "classify 2.9467742443084717\n",
            "classify 2.921449661254883\n",
            "0.078125\n",
            "0.09375\n",
            "0.0625\n",
            "0.078125\n",
            "0.125\n",
            "0.078125\n",
            "0.03125\n",
            "0.125\n",
            "0.171875\n",
            "0.125\n",
            "0.078125\n",
            "time: 17.138980865478516 16.851910265286765\n",
            "90\n",
            "strain 0.026014231145381927\n",
            "strain 0.026304341852664948\n",
            "strain 0.028065836057066917\n",
            "strain 0.02669195644557476\n",
            "strain 0.029727362096309662\n",
            "strain 0.028220361098647118\n",
            "classify 2.8433735370635986\n",
            "classify 2.8367815017700195\n",
            "classify 2.8794026374816895\n",
            "classify 2.8386943340301514\n",
            "classify 2.858140707015991\n",
            "classify 2.9029574394226074\n",
            "classify 2.837963104248047\n",
            "classify 2.8951492309570312\n",
            "classify 2.8528904914855957\n",
            "classify 2.8883590698242188\n",
            "classify 2.865781307220459\n",
            "0.1875\n",
            "0.125\n",
            "0.0625\n",
            "0.125\n",
            "0.140625\n",
            "0.0625\n",
            "0.125\n",
            "0.078125\n",
            "0.078125\n",
            "0.171875\n",
            "0.078125\n",
            "time: 16.47537088394165 16.84777819979322\n",
            "91\n",
            "strain 0.02532847784459591\n",
            "strain 0.0283855851739645\n",
            "strain 0.03021291457116604\n",
            "strain 0.028452781960368156\n",
            "strain 0.02817864529788494\n",
            "strain 0.030165746808052063\n",
            "classify 2.7725844383239746\n",
            "classify 2.7940309047698975\n",
            "classify 2.8134472370147705\n",
            "classify 2.811002254486084\n",
            "classify 2.817020893096924\n",
            "classify 2.8374664783477783\n",
            "classify 2.862382411956787\n",
            "classify 2.848572015762329\n",
            "classify 2.770707607269287\n",
            "classify 2.8781239986419678\n",
            "classify 2.836416721343994\n",
            "0.203125\n",
            "0.015625\n",
            "0.109375\n",
            "0.140625\n",
            "0.1875\n",
            "0.171875\n",
            "0.078125\n",
            "0.109375\n",
            "0.125\n",
            "0.109375\n",
            "0.109375\n",
            "time: 16.580246210098267 16.84487655888433\n",
            "92\n",
            "strain 0.027348630130290985\n",
            "strain 0.022654594853520393\n",
            "strain 0.02875545620918274\n",
            "strain 0.024408601224422455\n",
            "strain 0.026535747572779655\n",
            "strain 0.02740483172237873\n",
            "classify 2.8249549865722656\n",
            "classify 2.8020620346069336\n",
            "classify 2.7929530143737793\n",
            "classify 2.854942798614502\n",
            "classify 2.7905168533325195\n",
            "classify 2.781264305114746\n",
            "classify 2.8350372314453125\n",
            "classify 2.8410680294036865\n",
            "classify 2.7654495239257812\n",
            "classify 2.8132622241973877\n",
            "classify 2.846304416656494\n",
            "0.09375\n",
            "0.140625\n",
            "0.09375\n",
            "0.09375\n",
            "0.078125\n",
            "0.125\n",
            "0.203125\n",
            "0.21875\n",
            "0.125\n",
            "0.15625\n",
            "0.1875\n",
            "time: 17.425293445587158 16.851122745903588\n",
            "93\n",
            "strain 0.027520840987563133\n",
            "strain 0.03307590261101723\n",
            "strain 0.02878330834209919\n",
            "strain 0.027016254141926765\n",
            "strain 0.026506314054131508\n",
            "strain 0.028712458908557892\n",
            "classify 2.819257974624634\n",
            "classify 2.821593999862671\n",
            "classify 2.8738248348236084\n",
            "classify 2.812274932861328\n",
            "classify 2.8469431400299072\n",
            "classify 2.813310384750366\n",
            "classify 2.824972152709961\n",
            "classify 2.8827855587005615\n",
            "classify 2.8291001319885254\n",
            "classify 2.810950517654419\n",
            "classify 2.8267972469329834\n",
            "0.125\n",
            "0.171875\n",
            "0.0625\n",
            "0.15625\n",
            "0.140625\n",
            "0.171875\n",
            "0.109375\n",
            "0.203125\n",
            "0.140625\n",
            "0.078125\n",
            "0.15625\n",
            "time: 16.707279205322266 16.84960104556794\n",
            "94\n",
            "strain 0.02746814303100109\n",
            "strain 0.02937869168817997\n",
            "strain 0.030068211257457733\n",
            "strain 0.02962053380906582\n",
            "strain 0.03046833723783493\n",
            "strain 0.024622047320008278\n",
            "classify 2.791429042816162\n",
            "classify 2.8308403491973877\n",
            "classify 2.8121626377105713\n",
            "classify 2.8542819023132324\n",
            "classify 2.8413963317871094\n",
            "classify 2.857560873031616\n",
            "classify 2.8238024711608887\n",
            "classify 2.855889320373535\n",
            "classify 2.8106274604797363\n",
            "classify 2.777872323989868\n",
            "classify 2.825791120529175\n",
            "0.140625\n",
            "0.15625\n",
            "0.234375\n",
            "0.125\n",
            "0.125\n",
            "0.171875\n",
            "0.109375\n",
            "0.171875\n",
            "0.109375\n",
            "0.078125\n",
            "0.15625\n",
            "time: 16.549216508865356 16.84644418766624\n",
            "95\n",
            "strain 0.026347635313868523\n",
            "strain 0.02512390911579132\n",
            "strain 0.02696954645216465\n",
            "strain 0.029873505234718323\n",
            "strain 0.027801191434264183\n",
            "strain 0.030465297400951385\n",
            "classify 2.8847708702087402\n",
            "classify 2.791435480117798\n",
            "classify 2.8158390522003174\n",
            "classify 2.814415454864502\n",
            "classify 2.8618204593658447\n",
            "classify 2.829385757446289\n",
            "classify 2.822835683822632\n",
            "classify 2.8291351795196533\n",
            "classify 2.831308126449585\n",
            "classify 2.8342769145965576\n",
            "classify 2.7709474563598633\n",
            "0.125\n",
            "0.125\n",
            "0.1875\n",
            "0.171875\n",
            "0.125\n",
            "0.140625\n",
            "0.1875\n",
            "0.078125\n",
            "0.09375\n",
            "0.21875\n",
            "0.09375\n",
            "time: 16.494861841201782 16.84278779725234\n",
            "96\n",
            "strain 0.02852208912372589\n",
            "strain 0.02660992741584778\n",
            "strain 0.02701311744749546\n",
            "strain 0.02682514674961567\n",
            "strain 0.028246669098734856\n",
            "strain 0.029902659356594086\n",
            "classify 2.808061361312866\n",
            "classify 2.8092458248138428\n",
            "classify 2.7813303470611572\n",
            "classify 2.834545135498047\n",
            "classify 2.7610361576080322\n",
            "classify 2.821218967437744\n",
            "classify 2.8127684593200684\n",
            "classify 2.8512520790100098\n",
            "classify 2.818801164627075\n",
            "classify 2.753384828567505\n",
            "classify 2.792210340499878\n",
            "0.109375\n",
            "0.0625\n",
            "0.109375\n",
            "0.109375\n",
            "0.078125\n",
            "0.171875\n",
            "0.125\n",
            "0.15625\n",
            "0.109375\n",
            "0.078125\n",
            "0.203125\n",
            "time: 17.82025122642517 16.852871211533692\n",
            "97\n",
            "strain 0.027689600363373756\n",
            "strain 0.0272859875112772\n",
            "strain 0.026592865586280823\n",
            "strain 0.028124356642365456\n",
            "strain 0.02633860893547535\n",
            "strain 0.028186479583382607\n",
            "classify 2.8579201698303223\n",
            "classify 2.77882981300354\n",
            "classify 2.8305656909942627\n",
            "classify 2.8286292552948\n",
            "classify 2.826150417327881\n",
            "classify 2.841681718826294\n",
            "classify 2.805410623550415\n",
            "classify 2.781116008758545\n",
            "classify 2.8293566703796387\n",
            "classify 2.7601442337036133\n",
            "classify 2.8086323738098145\n",
            "0.15625\n",
            "0.109375\n",
            "0.078125\n",
            "0.15625\n",
            "0.140625\n",
            "0.15625\n",
            "0.125\n",
            "0.125\n",
            "0.09375\n",
            "0.078125\n",
            "0.15625\n",
            "time: 16.41889977455139 16.84844982624054\n",
            "98\n",
            "strain 0.029929153621196747\n",
            "strain 0.028759831562638283\n",
            "strain 0.0282654520124197\n",
            "strain 0.027057932689785957\n",
            "strain 0.027353202924132347\n",
            "strain 0.028799565508961678\n",
            "classify 2.854530096054077\n",
            "classify 2.7553067207336426\n",
            "classify 2.814177989959717\n",
            "classify 2.718029499053955\n",
            "classify 2.825847864151001\n",
            "classify 2.7995147705078125\n",
            "classify 2.761413812637329\n",
            "classify 2.807145833969116\n",
            "classify 2.7120490074157715\n",
            "classify 2.781742572784424\n",
            "classify 2.7676031589508057\n",
            "0.21875\n",
            "0.140625\n",
            "0.109375\n",
            "0.140625\n",
            "0.234375\n",
            "0.203125\n",
            "0.1875\n",
            "0.203125\n",
            "0.171875\n",
            "0.171875\n",
            "0.125\n",
            "time: 16.499982833862305 16.844938148151744\n",
            "99\n",
            "strain 0.027128899469971657\n",
            "strain 0.028279105201363564\n",
            "strain 0.02534945122897625\n",
            "strain 0.02636433206498623\n",
            "strain 0.029225707054138184\n",
            "strain 0.026134418323636055\n",
            "classify 2.767073631286621\n",
            "classify 2.785402536392212\n",
            "classify 2.7195076942443848\n",
            "classify 2.7917563915252686\n",
            "classify 2.7837231159210205\n",
            "classify 2.7273244857788086\n",
            "classify 2.7273943424224854\n",
            "classify 2.7641046047210693\n",
            "classify 2.73675799369812\n",
            "classify 2.7832751274108887\n",
            "classify 2.762725591659546\n",
            "0.140625\n",
            "0.1875\n",
            "0.046875\n",
            "0.15625\n",
            "0.1875\n",
            "0.140625\n",
            "0.1875\n",
            "0.171875\n",
            "0.171875\n",
            "0.15625\n",
            "0.140625\n",
            "time: 16.448400735855103 16.84097880601883\n",
            "100\n",
            "strain 0.02947859652340412\n",
            "strain 0.02790425904095173\n",
            "strain 0.028705695644021034\n",
            "strain 0.029152242466807365\n",
            "strain 0.028696322813630104\n",
            "strain 0.02419646643102169\n",
            "classify 2.8541338443756104\n",
            "classify 2.820427417755127\n",
            "classify 2.7747104167938232\n",
            "classify 2.8595690727233887\n",
            "classify 2.8336081504821777\n",
            "classify 2.8199050426483154\n",
            "classify 2.8115100860595703\n",
            "classify 2.7174177169799805\n",
            "classify 2.732017755508423\n",
            "classify 2.796114206314087\n",
            "classify 2.800576686859131\n",
            "0.15625\n",
            "0.125\n",
            "0.203125\n",
            "0.203125\n",
            "0.171875\n",
            "0.171875\n",
            "0.203125\n",
            "0.125\n",
            "0.125\n",
            "0.125\n",
            "0.15625\n",
            "time: 17.814093112945557 16.850618938408275\n",
            "101\n",
            "strain 0.028383443132042885\n",
            "strain 0.03276405856013298\n",
            "strain 0.028335625305771828\n",
            "strain 0.031182795763015747\n",
            "strain 0.025673210620880127\n",
            "strain 0.02621247060596943\n",
            "classify 2.777310609817505\n",
            "classify 2.8433663845062256\n",
            "classify 2.770400047302246\n",
            "classify 2.7962546348571777\n",
            "classify 2.8448450565338135\n",
            "classify 2.7412326335906982\n",
            "classify 2.7975683212280273\n",
            "classify 2.808450698852539\n",
            "classify 2.778698205947876\n",
            "classify 2.8188514709472656\n",
            "classify 2.745915174484253\n",
            "0.171875\n",
            "0.171875\n",
            "0.15625\n",
            "0.078125\n",
            "0.140625\n",
            "0.109375\n",
            "0.046875\n",
            "0.171875\n",
            "0.109375\n",
            "0.078125\n",
            "0.125\n",
            "time: 16.48489761352539 16.847039585020028\n",
            "102\n",
            "strain 0.025675103068351746\n",
            "strain 0.030064912512898445\n",
            "strain 0.02797105722129345\n",
            "strain 0.026572467759251595\n",
            "strain 0.02717888355255127\n",
            "strain 0.029443785548210144\n",
            "classify 2.8317697048187256\n",
            "classify 2.7293529510498047\n",
            "classify 2.830911159515381\n",
            "classify 2.7598702907562256\n",
            "classify 2.7699756622314453\n",
            "classify 2.7747490406036377\n",
            "classify 2.7906875610351562\n",
            "classify 2.799001693725586\n",
            "classify 2.7916717529296875\n",
            "classify 2.7667694091796875\n",
            "classify 2.766333818435669\n",
            "0.109375\n",
            "0.140625\n",
            "0.0625\n",
            "0.171875\n",
            "0.09375\n",
            "0.09375\n",
            "0.140625\n",
            "0.0625\n",
            "0.109375\n",
            "0.140625\n",
            "0.15625\n",
            "time: 16.46866250038147 16.84337172693419\n",
            "103\n",
            "strain 0.030133813619613647\n",
            "strain 0.028224503621459007\n",
            "strain 0.028639836236834526\n",
            "strain 0.026266807690262794\n",
            "strain 0.026869887486100197\n",
            "strain 0.028743771836161613\n",
            "classify 2.7762625217437744\n",
            "classify 2.805873394012451\n",
            "classify 2.790416955947876\n",
            "classify 2.852041721343994\n",
            "classify 2.779242992401123\n",
            "classify 2.8112802505493164\n",
            "classify 2.7510488033294678\n",
            "classify 2.765377998352051\n",
            "classify 2.7805404663085938\n",
            "classify 2.737689733505249\n",
            "classify 2.7825775146484375\n",
            "0.078125\n",
            "0.078125\n",
            "0.171875\n",
            "0.0625\n",
            "0.109375\n",
            "0.078125\n",
            "0.125\n",
            "0.046875\n",
            "0.046875\n",
            "0.15625\n",
            "0.1875\n",
            "time: 16.676376819610596 16.84177173100985\n",
            "104\n",
            "strain 0.030500998720526695\n",
            "strain 0.023091599345207214\n",
            "strain 0.02687782794237137\n",
            "strain 0.024481872096657753\n",
            "strain 0.027669668197631836\n",
            "strain 0.02830672264099121\n",
            "classify 2.7194409370422363\n",
            "classify 2.78483247756958\n",
            "classify 2.785879373550415\n",
            "classify 2.69128155708313\n",
            "classify 2.746352434158325\n",
            "classify 2.7492775917053223\n",
            "classify 2.7670416831970215\n",
            "classify 2.724409818649292\n",
            "classify 2.7094995975494385\n",
            "classify 2.7344067096710205\n",
            "classify 2.720846652984619\n",
            "0.140625\n",
            "0.125\n",
            "0.140625\n",
            "0.109375\n",
            "0.125\n",
            "0.15625\n",
            "0.203125\n",
            "0.078125\n",
            "0.03125\n",
            "0.1875\n",
            "0.21875\n",
            "time: 17.479994773864746 16.84786175546192\n",
            "105\n",
            "strain 0.02625994384288788\n",
            "strain 0.030711019411683083\n",
            "strain 0.03152673318982124\n",
            "strain 0.02571120299398899\n",
            "strain 0.029967917129397392\n",
            "strain 0.025967322289943695\n",
            "classify 2.749976396560669\n",
            "classify 2.8339333534240723\n",
            "classify 2.7635650634765625\n",
            "classify 2.8195130825042725\n",
            "classify 2.782090187072754\n",
            "classify 2.8147201538085938\n",
            "classify 2.813239812850952\n",
            "classify 2.8060355186462402\n",
            "classify 2.807420253753662\n",
            "classify 2.7714993953704834\n",
            "classify 2.808795213699341\n",
            "0.203125\n",
            "0.09375\n",
            "0.15625\n",
            "0.15625\n",
            "0.046875\n",
            "0.125\n",
            "0.171875\n",
            "0.15625\n",
            "0.15625\n",
            "0.125\n",
            "0.078125\n",
            "time: 16.465503215789795 16.844259662448234\n",
            "106\n",
            "strain 0.025339903309941292\n",
            "strain 0.03275961056351662\n",
            "strain 0.02750292420387268\n",
            "strain 0.028490202501416206\n",
            "strain 0.025083370506763458\n",
            "strain 0.027795335277915\n",
            "classify 2.708019256591797\n",
            "classify 2.7910923957824707\n",
            "classify 2.7883121967315674\n",
            "classify 2.7429049015045166\n",
            "classify 2.784494400024414\n",
            "classify 2.826519250869751\n",
            "classify 2.777500867843628\n",
            "classify 2.7648983001708984\n",
            "classify 2.7430450916290283\n",
            "classify 2.6968939304351807\n",
            "classify 2.825822353363037\n",
            "0.109375\n",
            "0.0625\n",
            "0.09375\n",
            "0.125\n",
            "0.234375\n",
            "0.171875\n",
            "0.125\n",
            "0.1875\n",
            "0.09375\n",
            "0.1875\n",
            "0.09375\n",
            "time: 16.451195001602173 16.840591085291354\n",
            "107\n",
            "strain 0.029466187581419945\n",
            "strain 0.030133366584777832\n",
            "strain 0.026573142036795616\n",
            "strain 0.02762630023062229\n",
            "strain 0.02726925164461136\n",
            "strain 0.027673915028572083\n",
            "classify 2.751932144165039\n",
            "classify 2.779161214828491\n",
            "classify 2.793750286102295\n",
            "classify 2.7622580528259277\n",
            "classify 2.8115131855010986\n",
            "classify 2.7646007537841797\n",
            "classify 2.7088422775268555\n",
            "classify 2.793550491333008\n",
            "classify 2.7574126720428467\n",
            "classify 2.759382486343384\n",
            "classify 2.7799506187438965\n",
            "0.140625\n",
            "0.0625\n",
            "0.078125\n",
            "0.1875\n",
            "0.125\n",
            "0.171875\n",
            "0.171875\n",
            "0.09375\n",
            "0.125\n",
            "0.125\n",
            "0.109375\n",
            "time: 16.94001317024231 16.841517048853415\n",
            "108\n",
            "strain 0.027082443237304688\n",
            "strain 0.02489892579615116\n",
            "strain 0.026259317994117737\n",
            "strain 0.024394838139414787\n",
            "strain 0.029149489477276802\n",
            "strain 0.02512805350124836\n",
            "classify 2.7871644496917725\n",
            "classify 2.82285737991333\n",
            "classify 2.751769781112671\n",
            "classify 2.7902822494506836\n",
            "classify 2.783764362335205\n",
            "classify 2.7710628509521484\n",
            "classify 2.749091625213623\n",
            "classify 2.7617948055267334\n",
            "classify 2.7721009254455566\n",
            "classify 2.7798924446105957\n",
            "classify 2.7568705081939697\n",
            "0.140625\n",
            "0.15625\n",
            "0.15625\n",
            "0.109375\n",
            "0.203125\n",
            "0.15625\n",
            "0.109375\n",
            "0.171875\n",
            "0.15625\n",
            "0.21875\n",
            "0.171875\n",
            "time: 17.318520307540894 16.845898527617848\n",
            "109\n",
            "strain 0.029162203893065453\n",
            "strain 0.02746541239321232\n",
            "strain 0.025661079213023186\n",
            "strain 0.02617393434047699\n",
            "strain 0.026423582807183266\n",
            "strain 0.026004714891314507\n",
            "classify 2.796128511428833\n",
            "classify 2.783512592315674\n",
            "classify 2.763972043991089\n",
            "classify 2.800442695617676\n",
            "classify 2.790907621383667\n",
            "classify 2.771286725997925\n",
            "classify 2.806633710861206\n",
            "classify 2.759816884994507\n",
            "classify 2.8033463954925537\n",
            "classify 2.745732307434082\n",
            "classify 2.8108325004577637\n",
            "0.140625\n",
            "0.125\n",
            "0.203125\n",
            "0.171875\n",
            "0.078125\n",
            "0.171875\n",
            "0.15625\n",
            "0.140625\n",
            "0.125\n",
            "0.09375\n",
            "0.03125\n",
            "time: 16.528800010681152 16.843023131110453\n",
            "110\n",
            "strain 0.025597894564270973\n",
            "strain 0.02825637347996235\n",
            "strain 0.031047223135828972\n",
            "strain 0.026025816798210144\n",
            "strain 0.024622956290841103\n",
            "strain 0.027055181562900543\n",
            "classify 2.7089555263519287\n",
            "classify 2.7620036602020264\n",
            "classify 2.7386763095855713\n",
            "classify 2.7942686080932617\n",
            "classify 2.787876844406128\n",
            "classify 2.765241861343384\n",
            "classify 2.7623589038848877\n",
            "classify 2.7307565212249756\n",
            "classify 2.705521583557129\n",
            "classify 2.7788031101226807\n",
            "classify 2.716257095336914\n",
            "0.078125\n",
            "0.046875\n",
            "0.109375\n",
            "0.125\n",
            "0.140625\n",
            "0.125\n",
            "0.15625\n",
            "0.0625\n",
            "0.125\n",
            "0.09375\n",
            "0.15625\n",
            "time: 16.563048601150513 16.84050941467285\n",
            "111\n",
            "strain 0.029537102207541466\n",
            "strain 0.028532614931464195\n",
            "strain 0.029185093939304352\n",
            "strain 0.02840360440313816\n",
            "strain 0.027662480250000954\n",
            "strain 0.026836654171347618\n",
            "classify 2.758880138397217\n",
            "classify 2.8139708042144775\n",
            "classify 2.7909984588623047\n",
            "classify 2.7778208255767822\n",
            "classify 2.7689123153686523\n",
            "classify 2.741487503051758\n",
            "classify 2.7903220653533936\n",
            "classify 2.779491424560547\n",
            "classify 2.7893569469451904\n",
            "classify 2.7514870166778564\n",
            "classify 2.785879373550415\n",
            "0.0625\n",
            "0.09375\n",
            "0.1875\n",
            "0.203125\n",
            "0.109375\n",
            "0.109375\n",
            "0.140625\n",
            "0.125\n",
            "0.1875\n",
            "0.09375\n",
            "0.109375\n",
            "time: 17.33585810661316 16.844937458634377\n",
            "112\n",
            "strain 0.025801323354244232\n",
            "strain 0.02522365003824234\n",
            "strain 0.030085453763604164\n",
            "strain 0.0262808445841074\n",
            "strain 0.02917875163257122\n",
            "strain 0.02923339605331421\n",
            "classify 2.8372042179107666\n",
            "classify 2.7929649353027344\n",
            "classify 2.7810699939727783\n",
            "classify 2.756439208984375\n",
            "classify 2.778759479522705\n",
            "classify 2.770712375640869\n",
            "classify 2.7692954540252686\n",
            "classify 2.7641830444335938\n",
            "classify 2.752941846847534\n",
            "classify 2.7570888996124268\n",
            "classify 2.7040815353393555\n",
            "0.109375\n",
            "0.078125\n",
            "0.09375\n",
            "0.109375\n",
            "0.109375\n",
            "0.125\n",
            "0.125\n",
            "0.1875\n",
            "0.234375\n",
            "0.140625\n",
            "0.125\n",
            "time: 16.753867387771606 16.844138027292438\n",
            "113\n",
            "strain 0.028464386239647865\n",
            "strain 0.025614917278289795\n",
            "strain 0.026971867308020592\n",
            "strain 0.026149092242121696\n",
            "strain 0.026769595220685005\n",
            "strain 0.025052888318896294\n",
            "classify 2.6904842853546143\n",
            "classify 2.6877853870391846\n",
            "classify 2.711946964263916\n",
            "classify 2.7302029132843018\n",
            "classify 2.6673290729522705\n",
            "classify 2.7254700660705566\n",
            "classify 2.736152410507202\n",
            "classify 2.7220449447631836\n",
            "classify 2.7139875888824463\n",
            "classify 2.7310428619384766\n",
            "classify 2.720635175704956\n",
            "0.21875\n",
            "0.15625\n",
            "0.15625\n",
            "0.03125\n",
            "0.1875\n",
            "0.109375\n",
            "0.078125\n",
            "0.171875\n",
            "0.140625\n",
            "0.140625\n",
            "0.125\n",
            "time: 16.50425362586975 16.841161732088054\n",
            "114\n",
            "strain 0.027939172461628914\n",
            "strain 0.028914092108607292\n",
            "strain 0.026259398087859154\n",
            "strain 0.028732866048812866\n",
            "strain 0.02795863337814808\n",
            "strain 0.02170637995004654\n",
            "classify 2.7113845348358154\n",
            "classify 2.7381882667541504\n",
            "classify 2.7730095386505127\n",
            "classify 2.735819101333618\n",
            "classify 2.841641426086426\n",
            "classify 2.710258960723877\n",
            "classify 2.7827701568603516\n",
            "classify 2.746373176574707\n",
            "classify 2.764403820037842\n",
            "classify 2.760077714920044\n",
            "classify 2.74139142036438\n",
            "0.171875\n",
            "0.203125\n",
            "0.28125\n",
            "0.09375\n",
            "0.140625\n",
            "0.140625\n",
            "0.15625\n",
            "0.140625\n",
            "0.125\n",
            "0.140625\n",
            "0.140625\n",
            "time: 16.492139101028442 16.838131585328476\n",
            "115\n",
            "strain 0.024682477116584778\n",
            "strain 0.029997577890753746\n",
            "strain 0.02710559032857418\n",
            "strain 0.02650807797908783\n",
            "strain 0.027310078963637352\n",
            "strain 0.03000449948012829\n",
            "classify 2.681865930557251\n",
            "classify 2.7724835872650146\n",
            "classify 2.7710254192352295\n",
            "classify 2.6951723098754883\n",
            "classify 2.7465853691101074\n",
            "classify 2.7190704345703125\n",
            "classify 2.756488800048828\n",
            "classify 2.7274832725524902\n",
            "classify 2.7400267124176025\n",
            "classify 2.71614670753479\n",
            "classify 2.7107648849487305\n",
            "0.234375\n",
            "0.203125\n",
            "0.171875\n",
            "0.140625\n",
            "0.203125\n",
            "0.15625\n",
            "0.125\n",
            "0.125\n",
            "0.296875\n",
            "0.1875\n",
            "0.1875\n",
            "time: 17.67063021659851 16.845312821454016\n",
            "116\n",
            "strain 0.028323819860816002\n",
            "strain 0.02653302252292633\n",
            "strain 0.027490094304084778\n",
            "strain 0.027507541701197624\n",
            "strain 0.022749057039618492\n",
            "strain 0.025617508217692375\n",
            "classify 2.7329261302948\n",
            "classify 2.742237091064453\n",
            "classify 2.675762891769409\n",
            "classify 2.750541925430298\n",
            "classify 2.787290096282959\n",
            "classify 2.7789721488952637\n",
            "classify 2.6857500076293945\n",
            "classify 2.76809024810791\n",
            "classify 2.718761682510376\n",
            "classify 2.741339921951294\n",
            "classify 2.713440418243408\n",
            "0.203125\n",
            "0.140625\n",
            "0.171875\n",
            "0.296875\n",
            "0.140625\n",
            "0.140625\n",
            "0.203125\n",
            "0.234375\n",
            "0.15625\n",
            "0.203125\n",
            "0.15625\n",
            "time: 16.561792373657227 16.842894539873825\n",
            "117\n",
            "strain 0.02731570415198803\n",
            "strain 0.025135433301329613\n",
            "strain 0.02479170449078083\n",
            "strain 0.026690706610679626\n",
            "strain 0.026241401210427284\n",
            "strain 0.025723285973072052\n",
            "classify 2.7167091369628906\n",
            "classify 2.7455952167510986\n",
            "classify 2.709980010986328\n",
            "classify 2.7568516731262207\n",
            "classify 2.665787696838379\n",
            "classify 2.7096097469329834\n",
            "classify 2.8060824871063232\n",
            "classify 2.6998748779296875\n",
            "classify 2.691148519515991\n",
            "classify 2.6917197704315186\n",
            "classify 2.752939462661743\n",
            "0.203125\n",
            "0.15625\n",
            "0.125\n",
            "0.234375\n",
            "0.15625\n",
            "0.109375\n",
            "0.125\n",
            "0.171875\n",
            "0.21875\n",
            "0.125\n",
            "0.203125\n",
            "time: 16.49198818206787 16.839925610412987\n",
            "118\n",
            "strain 0.028501952067017555\n",
            "strain 0.02971452660858631\n",
            "strain 0.026614084839820862\n",
            "strain 0.028308680281043053\n",
            "strain 0.02925456129014492\n",
            "strain 0.030882567167282104\n",
            "classify 2.6810343265533447\n",
            "classify 2.7394843101501465\n",
            "classify 2.7230277061462402\n",
            "classify 2.738497495651245\n",
            "classify 2.6953210830688477\n",
            "classify 2.7619385719299316\n",
            "classify 2.714975118637085\n",
            "classify 2.748931407928467\n",
            "classify 2.671100616455078\n",
            "classify 2.6969573497772217\n",
            "classify 2.785691261291504\n",
            "0.296875\n",
            "0.125\n",
            "0.15625\n",
            "0.09375\n",
            "0.15625\n",
            "0.21875\n",
            "0.15625\n",
            "0.1875\n",
            "0.1875\n",
            "0.140625\n",
            "0.203125\n",
            "time: 16.52766728401184 16.83730667378722\n",
            "119\n",
            "strain 0.025853080675005913\n",
            "strain 0.029225008562207222\n",
            "strain 0.024051791056990623\n",
            "strain 0.025058738887310028\n",
            "strain 0.027117682620882988\n",
            "strain 0.02574319578707218\n",
            "classify 2.7063839435577393\n",
            "classify 2.7078475952148438\n",
            "classify 2.7488231658935547\n",
            "classify 2.7091877460479736\n",
            "classify 2.7501726150512695\n",
            "classify 2.709878921508789\n",
            "classify 2.769031047821045\n",
            "classify 2.7521181106567383\n",
            "classify 2.669442892074585\n",
            "classify 2.7280218601226807\n",
            "classify 2.7481157779693604\n",
            "0.171875\n",
            "0.125\n",
            "0.15625\n",
            "0.109375\n",
            "0.09375\n",
            "0.21875\n",
            "0.125\n",
            "0.15625\n",
            "0.234375\n",
            "0.1875\n",
            "0.140625\n",
            "time: 17.7872896194458 16.845227720340095\n",
            "120\n",
            "strain 0.02624300867319107\n",
            "strain 0.024859847500920296\n",
            "strain 0.025824883952736855\n",
            "strain 0.026004552841186523\n",
            "strain 0.02842298150062561\n",
            "strain 0.02407546155154705\n",
            "classify 2.6814661026000977\n",
            "classify 2.7278800010681152\n",
            "classify 2.6905252933502197\n",
            "classify 2.6490743160247803\n",
            "classify 2.7192201614379883\n",
            "classify 2.6710503101348877\n",
            "classify 2.67779278755188\n",
            "classify 2.7319960594177246\n",
            "classify 2.7124075889587402\n",
            "classify 2.667452335357666\n",
            "classify 2.7178914546966553\n",
            "0.15625\n",
            "0.125\n",
            "0.046875\n",
            "0.109375\n",
            "0.078125\n",
            "0.21875\n",
            "0.140625\n",
            "0.15625\n",
            "0.203125\n",
            "0.171875\n",
            "0.109375\n",
            "time: 16.513579607009888 16.84249231638002\n",
            "121\n",
            "strain 0.030389809980988503\n",
            "strain 0.027600253000855446\n",
            "strain 0.031828347593545914\n",
            "strain 0.029323140159249306\n",
            "strain 0.026178041473031044\n",
            "strain 0.028022801503539085\n",
            "classify 2.722020149230957\n",
            "classify 2.645923614501953\n",
            "classify 2.723999500274658\n",
            "classify 2.719196081161499\n",
            "classify 2.706157922744751\n",
            "classify 2.741476058959961\n",
            "classify 2.721966028213501\n",
            "classify 2.7179765701293945\n",
            "classify 2.689098358154297\n",
            "classify 2.667372703552246\n",
            "classify 2.7306857109069824\n",
            "0.078125\n",
            "0.1875\n",
            "0.1875\n",
            "0.203125\n",
            "0.171875\n",
            "0.15625\n",
            "0.125\n",
            "0.15625\n",
            "0.09375\n",
            "0.125\n",
            "0.1875\n",
            "time: 16.513246297836304 16.83979690270346\n",
            "122\n",
            "strain 0.027485400438308716\n",
            "strain 0.027724623680114746\n",
            "strain 0.028559381142258644\n",
            "strain 0.025044851005077362\n",
            "strain 0.02646753378212452\n",
            "strain 0.025615906342864037\n",
            "classify 2.7139127254486084\n",
            "classify 2.7377305030822754\n",
            "classify 2.708266258239746\n",
            "classify 2.7796504497528076\n",
            "classify 2.7324228286743164\n",
            "classify 2.7031261920928955\n",
            "classify 2.695340156555176\n",
            "classify 2.711178779602051\n",
            "classify 2.7183780670166016\n",
            "classify 2.7037243843078613\n",
            "classify 2.697540283203125\n",
            "0.171875\n",
            "0.21875\n",
            "0.109375\n",
            "0.171875\n",
            "0.140625\n",
            "0.15625\n",
            "0.140625\n",
            "0.140625\n",
            "0.140625\n",
            "0.265625\n",
            "0.140625\n",
            "time: 16.568105697631836 16.837592683187346\n",
            "123\n",
            "strain 0.028912253677845\n",
            "strain 0.0261967983096838\n",
            "strain 0.028595028445124626\n",
            "strain 0.02494668774306774\n",
            "strain 0.028292352333664894\n",
            "strain 0.026697007939219475\n",
            "classify 2.681821823120117\n",
            "classify 2.716527223587036\n",
            "classify 2.7718281745910645\n",
            "classify 2.6929922103881836\n",
            "classify 2.7069575786590576\n",
            "classify 2.733260154724121\n",
            "classify 2.7538504600524902\n",
            "classify 2.717902421951294\n",
            "classify 2.733794689178467\n",
            "classify 2.70725679397583\n",
            "classify 2.7150158882141113\n",
            "0.171875\n",
            "0.3125\n",
            "0.171875\n",
            "0.09375\n",
            "0.125\n",
            "0.25\n",
            "0.15625\n",
            "0.1875\n",
            "0.1875\n",
            "0.125\n",
            "0.0625\n",
            "time: 17.628387928009033 16.843976743759647\n",
            "124\n",
            "strain 0.030020803213119507\n",
            "strain 0.025355353951454163\n",
            "strain 0.026342848315835\n",
            "strain 0.025104129686951637\n",
            "strain 0.026595642790198326\n",
            "strain 0.029147818684577942\n",
            "classify 2.689401149749756\n",
            "classify 2.702725410461426\n",
            "classify 2.718956708908081\n",
            "classify 2.7149035930633545\n",
            "classify 2.687690019607544\n",
            "classify 2.690798759460449\n",
            "classify 2.673672676086426\n",
            "classify 2.7317490577697754\n",
            "classify 2.705047369003296\n",
            "classify 2.7110540866851807\n",
            "classify 2.730241537094116\n",
            "0.171875\n",
            "0.15625\n",
            "0.1875\n",
            "0.109375\n",
            "0.265625\n",
            "0.125\n",
            "0.234375\n",
            "0.140625\n",
            "0.203125\n",
            "0.171875\n",
            "0.171875\n",
            "time: 16.47417449951172 16.841022975921632\n",
            "125\n",
            "strain 0.025754334405064583\n",
            "strain 0.027722718194127083\n",
            "strain 0.02791806124150753\n",
            "strain 0.02687179110944271\n",
            "strain 0.030228788033127785\n",
            "strain 0.030006257817149162\n",
            "classify 2.752035140991211\n",
            "classify 2.7051169872283936\n",
            "classify 2.6769957542419434\n",
            "classify 2.720547914505005\n",
            "classify 2.7304916381835938\n",
            "classify 2.702838659286499\n",
            "classify 2.654618263244629\n",
            "classify 2.6907901763916016\n",
            "classify 2.63242769241333\n",
            "classify 2.6963820457458496\n",
            "classify 2.7369258403778076\n",
            "0.09375\n",
            "0.15625\n",
            "0.203125\n",
            "0.203125\n",
            "0.125\n",
            "0.1875\n",
            "0.25\n",
            "0.203125\n",
            "0.171875\n",
            "0.265625\n",
            "0.140625\n",
            "time: 16.471195459365845 16.838093254301285\n",
            "126\n",
            "strain 0.02945547364652157\n",
            "strain 0.025631578639149666\n",
            "strain 0.024812670424580574\n",
            "strain 0.027949905022978783\n",
            "strain 0.026395024731755257\n",
            "strain 0.02848905883729458\n",
            "classify 2.6803650856018066\n",
            "classify 2.7468440532684326\n",
            "classify 2.690319061279297\n",
            "classify 2.698317289352417\n",
            "classify 2.676375150680542\n",
            "classify 2.665879487991333\n",
            "classify 2.7032573223114014\n",
            "classify 2.6974077224731445\n",
            "classify 2.6872916221618652\n",
            "classify 2.688999652862549\n",
            "classify 2.65228271484375\n",
            "0.125\n",
            "0.09375\n",
            "0.21875\n",
            "0.21875\n",
            "0.21875\n",
            "0.265625\n",
            "0.140625\n",
            "0.171875\n",
            "0.0625\n",
            "0.1875\n",
            "0.21875\n",
            "time: 17.063472509384155 16.839872694390966\n",
            "127\n",
            "strain 0.024865329265594482\n",
            "strain 0.028411122038960457\n",
            "strain 0.026144757866859436\n",
            "strain 0.026948772370815277\n",
            "strain 0.027480343356728554\n",
            "strain 0.026288742199540138\n",
            "classify 2.6613152027130127\n",
            "classify 2.798200845718384\n",
            "classify 2.6866652965545654\n",
            "classify 2.7220945358276367\n",
            "classify 2.7144665718078613\n",
            "classify 2.7077367305755615\n",
            "classify 2.7280941009521484\n",
            "classify 2.760112762451172\n",
            "classify 2.7470040321350098\n",
            "classify 2.714804172515869\n",
            "classify 2.742732286453247\n",
            "0.203125\n",
            "0.0625\n",
            "0.109375\n",
            "0.15625\n",
            "0.171875\n",
            "0.125\n",
            "0.171875\n",
            "0.1875\n",
            "0.0625\n",
            "0.234375\n",
            "0.125\n",
            "time: 17.121757984161377 16.842084346339107\n",
            "128\n",
            "strain 0.028669089078903198\n",
            "strain 0.02791384607553482\n",
            "strain 0.026942439377307892\n",
            "strain 0.028579482808709145\n",
            "strain 0.026675039902329445\n",
            "strain 0.024533627554774284\n",
            "classify 2.667705774307251\n",
            "classify 2.7259814739227295\n",
            "classify 2.677600383758545\n",
            "classify 2.733785390853882\n",
            "classify 2.7513747215270996\n",
            "classify 2.7231554985046387\n",
            "classify 2.709271192550659\n",
            "classify 2.7090580463409424\n",
            "classify 2.7189977169036865\n",
            "classify 2.704592704772949\n",
            "classify 2.7007226943969727\n",
            "0.171875\n",
            "0.125\n",
            "0.234375\n",
            "0.1875\n",
            "0.125\n",
            "0.078125\n",
            "0.078125\n",
            "0.203125\n",
            "0.15625\n",
            "0.171875\n",
            "0.15625\n",
            "time: 16.48006772994995 16.839281540508416\n",
            "129\n",
            "strain 0.025385694578289986\n",
            "strain 0.026684435084462166\n",
            "strain 0.023572692647576332\n",
            "strain 0.027168795466423035\n",
            "strain 0.027386950328946114\n",
            "strain 0.027365686371922493\n",
            "classify 2.708326578140259\n",
            "classify 2.6864495277404785\n",
            "classify 2.705371618270874\n",
            "classify 2.7381696701049805\n",
            "classify 2.7431795597076416\n",
            "classify 2.7295053005218506\n",
            "classify 2.7095718383789062\n",
            "classify 2.6678671836853027\n",
            "classify 2.686526298522949\n",
            "classify 2.6741185188293457\n",
            "classify 2.7464938163757324\n",
            "0.171875\n",
            "0.171875\n",
            "0.109375\n",
            "0.15625\n",
            "0.1875\n",
            "0.1875\n",
            "0.21875\n",
            "0.140625\n",
            "0.171875\n",
            "0.15625\n",
            "0.078125\n",
            "time: 16.559216737747192 16.837131014237038\n",
            "130\n",
            "strain 0.03134290128946304\n",
            "strain 0.024551237002015114\n",
            "strain 0.02736820839345455\n",
            "strain 0.027060994878411293\n",
            "strain 0.026062941178679466\n",
            "strain 0.028106369078159332\n",
            "classify 2.6855571269989014\n",
            "classify 2.698072910308838\n",
            "classify 2.6548831462860107\n",
            "classify 2.6532204151153564\n",
            "classify 2.708531618118286\n",
            "classify 2.714989423751831\n",
            "classify 2.6633503437042236\n",
            "classify 2.6425325870513916\n",
            "classify 2.6828854084014893\n",
            "classify 2.7060861587524414\n",
            "classify 2.6558289527893066\n",
            "0.171875\n",
            "0.171875\n",
            "0.109375\n",
            "0.125\n",
            "0.09375\n",
            "0.15625\n",
            "0.203125\n",
            "0.171875\n",
            "0.15625\n",
            "0.171875\n",
            "0.125\n",
            "time: 17.423314809799194 16.841608868300458\n",
            "131\n",
            "strain 0.025430353358387947\n",
            "strain 0.026905203238129616\n",
            "strain 0.02513820491731167\n",
            "strain 0.02505706250667572\n",
            "strain 0.02564013935625553\n",
            "strain 0.028773313388228416\n",
            "classify 2.678262710571289\n",
            "classify 2.668673515319824\n",
            "classify 2.666701316833496\n",
            "classify 2.649099826812744\n",
            "classify 2.6651806831359863\n",
            "classify 2.6121723651885986\n",
            "classify 2.6766889095306396\n",
            "classify 2.6862661838531494\n",
            "classify 2.6903398036956787\n",
            "classify 2.6614840030670166\n",
            "classify 2.70041823387146\n",
            "0.15625\n",
            "0.078125\n",
            "0.125\n",
            "0.140625\n",
            "0.125\n",
            "0.125\n",
            "0.1875\n",
            "0.125\n",
            "0.140625\n",
            "0.09375\n",
            "0.078125\n",
            "time: 16.85378384590149 16.84170611699422\n",
            "132\n",
            "strain 0.02416001260280609\n",
            "strain 0.026409706100821495\n",
            "strain 0.026647930964827538\n",
            "strain 0.025739146396517754\n",
            "strain 0.029030585661530495\n",
            "strain 0.026525968685746193\n",
            "classify 2.7125189304351807\n",
            "classify 2.721841812133789\n",
            "classify 2.631204605102539\n",
            "classify 2.7248241901397705\n",
            "classify 2.6425209045410156\n",
            "classify 2.663663864135742\n",
            "classify 2.7208874225616455\n",
            "classify 2.654688835144043\n",
            "classify 2.709200143814087\n",
            "classify 2.7281553745269775\n",
            "classify 2.642934799194336\n",
            "0.203125\n",
            "0.109375\n",
            "0.109375\n",
            "0.109375\n",
            "0.15625\n",
            "0.125\n",
            "0.203125\n",
            "0.171875\n",
            "0.171875\n",
            "0.171875\n",
            "0.21875\n",
            "time: 16.533307790756226 16.839391645632293\n",
            "133\n",
            "strain 0.0261538028717041\n",
            "strain 0.024608075618743896\n",
            "strain 0.028615474700927734\n",
            "strain 0.025603435933589935\n",
            "strain 0.022737285122275352\n",
            "strain 0.02411932311952114\n",
            "classify 2.7065606117248535\n",
            "classify 2.756268262863159\n",
            "classify 2.7045578956604004\n",
            "classify 2.7224113941192627\n",
            "classify 2.6968636512756348\n",
            "classify 2.698157548904419\n",
            "classify 2.7589828968048096\n",
            "classify 2.7086896896362305\n",
            "classify 2.6551594734191895\n",
            "classify 2.6889569759368896\n",
            "classify 2.69639253616333\n",
            "0.125\n",
            "0.15625\n",
            "0.15625\n",
            "0.1875\n",
            "0.1875\n",
            "0.15625\n",
            "0.171875\n",
            "0.140625\n",
            "0.09375\n",
            "0.1875\n",
            "0.125\n",
            "time: 16.564406394958496 16.837342721312794\n",
            "134\n",
            "strain 0.028934486210346222\n",
            "strain 0.02879132330417633\n",
            "strain 0.02473793365061283\n",
            "strain 0.024929702281951904\n",
            "strain 0.02559606172144413\n",
            "strain 0.02755487710237503\n",
            "classify 2.7226805686950684\n",
            "classify 2.6903674602508545\n",
            "classify 2.636512041091919\n",
            "classify 2.708413600921631\n",
            "classify 2.6361210346221924\n",
            "classify 2.6518309116363525\n",
            "classify 2.6499733924865723\n",
            "classify 2.640846014022827\n",
            "classify 2.621555805206299\n",
            "classify 2.611126184463501\n",
            "classify 2.7211101055145264\n",
            "0.109375\n",
            "0.140625\n",
            "0.15625\n",
            "0.1875\n",
            "0.125\n",
            "0.125\n",
            "0.1875\n",
            "0.125\n",
            "0.234375\n",
            "0.15625\n",
            "0.1875\n",
            "time: 17.796795129776 16.84445464875963\n",
            "135\n",
            "strain 0.02763104997575283\n",
            "strain 0.026181025430560112\n",
            "strain 0.02620486356317997\n",
            "strain 0.026071393862366676\n",
            "strain 0.02507622539997101\n",
            "strain 0.02699059434235096\n",
            "classify 2.692410469055176\n",
            "classify 2.681234359741211\n",
            "classify 2.7295656204223633\n",
            "classify 2.6909143924713135\n",
            "classify 2.6314427852630615\n",
            "classify 2.7130074501037598\n",
            "classify 2.7236557006835938\n",
            "classify 2.670078754425049\n",
            "classify 2.696410894393921\n",
            "classify 2.748906135559082\n",
            "classify 2.6837997436523438\n",
            "0.09375\n",
            "0.125\n",
            "0.0625\n",
            "0.109375\n",
            "0.078125\n",
            "0.125\n",
            "0.109375\n",
            "0.140625\n",
            "0.09375\n",
            "0.078125\n",
            "0.15625\n",
            "time: 16.558387756347656 16.842354835832822\n",
            "136\n",
            "strain 0.027120256796479225\n",
            "strain 0.023353317752480507\n",
            "strain 0.02584228478372097\n",
            "strain 0.029042335227131844\n",
            "strain 0.02604038268327713\n",
            "strain 0.028365911915898323\n",
            "classify 2.7301766872406006\n",
            "classify 2.684502124786377\n",
            "classify 2.6736574172973633\n",
            "classify 2.656653881072998\n",
            "classify 2.6998186111450195\n",
            "classify 2.636493444442749\n",
            "classify 2.636882781982422\n",
            "classify 2.661224603652954\n",
            "classify 2.6819422245025635\n",
            "classify 2.7113611698150635\n",
            "classify 2.68969464302063\n",
            "0.203125\n",
            "0.171875\n",
            "0.125\n",
            "0.09375\n",
            "0.171875\n",
            "0.15625\n",
            "0.140625\n",
            "0.125\n",
            "0.203125\n",
            "0.203125\n",
            "0.234375\n",
            "time: 16.48919129371643 16.839781113784678\n",
            "137\n",
            "strain 0.02572694420814514\n",
            "strain 0.025139220058918\n",
            "strain 0.021501347422599792\n",
            "strain 0.027429385110735893\n",
            "strain 0.026259588077664375\n",
            "strain 0.027165062725543976\n",
            "classify 2.6831247806549072\n",
            "classify 2.6744580268859863\n",
            "classify 2.628835916519165\n",
            "classify 2.6555871963500977\n",
            "classify 2.659233570098877\n",
            "classify 2.6633193492889404\n",
            "classify 2.6768057346343994\n",
            "classify 2.609529733657837\n",
            "classify 2.707768678665161\n",
            "classify 2.6540274620056152\n",
            "classify 2.6587727069854736\n",
            "0.21875\n",
            "0.15625\n",
            "0.171875\n",
            "0.15625\n",
            "0.265625\n",
            "0.125\n",
            "0.09375\n",
            "0.15625\n",
            "0.21875\n",
            "0.15625\n",
            "0.21875\n",
            "time: 16.579935312271118 16.83790229714435\n",
            "138\n",
            "strain 0.025610430166125298\n",
            "strain 0.027867451310157776\n",
            "strain 0.02471168525516987\n",
            "strain 0.028115177527070045\n",
            "strain 0.03011695295572281\n",
            "strain 0.028215808793902397\n",
            "classify 2.6713876724243164\n",
            "classify 2.7028965950012207\n",
            "classify 2.6771483421325684\n",
            "classify 2.7067983150482178\n",
            "classify 2.638697385787964\n",
            "classify 2.6521389484405518\n",
            "classify 2.6893813610076904\n",
            "classify 2.619429588317871\n",
            "classify 2.6586599349975586\n",
            "classify 2.6897175312042236\n",
            "classify 2.7035083770751953\n",
            "0.203125\n",
            "0.15625\n",
            "0.140625\n",
            "0.140625\n",
            "0.1875\n",
            "0.171875\n",
            "0.15625\n",
            "0.15625\n",
            "0.140625\n",
            "0.203125\n",
            "0.140625\n",
            "time: 17.70953392982483 16.844175952801603\n",
            "139\n",
            "strain 0.02890142798423767\n",
            "strain 0.025049254298210144\n",
            "strain 0.02671067602932453\n",
            "strain 0.024511372670531273\n",
            "strain 0.023655548691749573\n",
            "strain 0.024679280817508698\n",
            "classify 2.7085654735565186\n",
            "classify 2.7190816402435303\n",
            "classify 2.7019731998443604\n",
            "classify 2.6685869693756104\n",
            "classify 2.691279649734497\n",
            "classify 2.7051024436950684\n",
            "classify 2.735276699066162\n",
            "classify 2.6742875576019287\n",
            "classify 2.7286124229431152\n",
            "classify 2.6801977157592773\n",
            "classify 2.7317848205566406\n",
            "0.234375\n",
            "0.28125\n",
            "0.15625\n",
            "0.0625\n",
            "0.078125\n",
            "0.125\n",
            "0.1875\n",
            "0.15625\n",
            "0.1875\n",
            "0.234375\n",
            "0.234375\n",
            "time: 16.531957864761353 16.841950052125114\n",
            "140\n",
            "strain 0.02377513237297535\n",
            "strain 0.030790716409683228\n",
            "strain 0.027584992349147797\n",
            "strain 0.025223994627594948\n",
            "strain 0.024784892797470093\n",
            "strain 0.027045434340834618\n",
            "classify 2.7317748069763184\n",
            "classify 2.652726173400879\n",
            "classify 2.6687958240509033\n",
            "classify 2.686906337738037\n",
            "classify 2.7198588848114014\n",
            "classify 2.6794803142547607\n",
            "classify 2.7035253047943115\n",
            "classify 2.659079074859619\n",
            "classify 2.673804521560669\n",
            "classify 2.665477991104126\n",
            "classify 2.6692655086517334\n",
            "0.171875\n",
            "0.171875\n",
            "0.1875\n",
            "0.109375\n",
            "0.109375\n",
            "0.140625\n",
            "0.203125\n",
            "0.1875\n",
            "0.140625\n",
            "0.171875\n",
            "0.15625\n",
            "time: 16.443986177444458 16.83913087506666\n",
            "141\n",
            "strain 0.02797006629407406\n",
            "strain 0.025615179911255836\n",
            "strain 0.028105109930038452\n",
            "strain 0.027783600613474846\n",
            "strain 0.02820587158203125\n",
            "strain 0.02642364241182804\n",
            "classify 2.6676156520843506\n",
            "classify 2.6974375247955322\n",
            "classify 2.6409664154052734\n",
            "classify 2.6780712604522705\n",
            "classify 2.6822445392608643\n",
            "classify 2.6935195922851562\n",
            "classify 2.677093029022217\n",
            "classify 2.6584115028381348\n",
            "classify 2.657702684402466\n",
            "classify 2.6444873809814453\n",
            "classify 2.702317237854004\n",
            "0.125\n",
            "0.125\n",
            "0.125\n",
            "0.1875\n",
            "0.15625\n",
            "0.125\n",
            "0.171875\n",
            "0.125\n",
            "0.3125\n",
            "0.140625\n",
            "0.203125\n",
            "time: 16.904041051864624 16.83959359014538\n",
            "142\n",
            "strain 0.02445988915860653\n",
            "strain 0.02926737628877163\n",
            "strain 0.026867486536502838\n",
            "strain 0.02624308317899704\n",
            "strain 0.026852428913116455\n",
            "strain 0.024846604093909264\n",
            "classify 2.5648598670959473\n",
            "classify 2.642150640487671\n",
            "classify 2.6815855503082275\n",
            "classify 2.6692957878112793\n",
            "classify 2.648820400238037\n",
            "classify 2.6067800521850586\n",
            "classify 2.657545804977417\n",
            "classify 2.6044492721557617\n",
            "classify 2.641948938369751\n",
            "classify 2.600252389907837\n",
            "classify 2.6298422813415527\n",
            "0.171875\n",
            "0.1875\n",
            "0.203125\n",
            "0.140625\n",
            "0.1875\n",
            "0.140625\n",
            "0.125\n",
            "0.1875\n",
            "0.203125\n",
            "0.203125\n",
            "0.171875\n",
            "time: 17.40176224708557 16.843533280846124\n",
            "143\n",
            "strain 0.027208169922232628\n",
            "strain 0.025474518537521362\n",
            "strain 0.02757636457681656\n",
            "strain 0.02797551453113556\n",
            "strain 0.027587128803133965\n",
            "strain 0.02846977859735489\n",
            "classify 2.6767044067382812\n",
            "classify 2.675748348236084\n",
            "classify 2.652116537094116\n",
            "classify 2.6985983848571777\n",
            "classify 2.6825318336486816\n",
            "classify 2.6346652507781982\n",
            "classify 2.6570065021514893\n",
            "classify 2.622344493865967\n",
            "classify 2.6534852981567383\n",
            "classify 2.663198709487915\n",
            "classify 2.6374027729034424\n",
            "0.125\n",
            "0.140625\n",
            "0.109375\n",
            "0.15625\n",
            "0.21875\n",
            "0.0625\n",
            "0.1875\n",
            "0.125\n",
            "0.171875\n",
            "0.1875\n",
            "0.125\n",
            "time: 16.527613639831543 16.84134332007832\n",
            "144\n",
            "strain 0.023985542356967926\n",
            "strain 0.02315814606845379\n",
            "strain 0.02724630944430828\n",
            "strain 0.02911212481558323\n",
            "strain 0.02863144315779209\n",
            "strain 0.025647005066275597\n",
            "classify 2.6673836708068848\n",
            "classify 2.7108702659606934\n",
            "classify 2.6922125816345215\n",
            "classify 2.6130149364471436\n",
            "classify 2.6254594326019287\n",
            "classify 2.7194628715515137\n",
            "classify 2.658705472946167\n",
            "classify 2.646062135696411\n",
            "classify 2.657858371734619\n",
            "classify 2.673464059829712\n",
            "classify 2.6431033611297607\n",
            "0.109375\n",
            "0.1875\n",
            "0.15625\n",
            "0.1875\n",
            "0.125\n",
            "0.25\n",
            "0.1875\n",
            "0.1875\n",
            "0.109375\n",
            "0.109375\n",
            "0.3125\n",
            "time: 16.53264808654785 16.839218797354864\n",
            "145\n",
            "strain 0.026836596429347992\n",
            "strain 0.023770945146679878\n",
            "strain 0.022390663623809814\n",
            "strain 0.025849737226963043\n",
            "strain 0.025433814153075218\n",
            "strain 0.028082208707928658\n",
            "classify 2.654142141342163\n",
            "classify 2.6981096267700195\n",
            "classify 2.6578562259674072\n",
            "classify 2.647906541824341\n",
            "classify 2.690701723098755\n",
            "classify 2.7170073986053467\n",
            "classify 2.631824016571045\n",
            "classify 2.6456894874572754\n",
            "classify 2.648101806640625\n",
            "classify 2.6614880561828613\n",
            "classify 2.66640567779541\n",
            "0.203125\n",
            "0.1875\n",
            "0.140625\n",
            "0.15625\n",
            "0.203125\n",
            "0.1875\n",
            "0.15625\n",
            "0.140625\n",
            "0.09375\n",
            "0.21875\n",
            "0.15625\n",
            "time: 17.321635246276855 16.842526703664703\n",
            "146\n",
            "strain 0.02705998905003071\n",
            "strain 0.02373643033206463\n",
            "strain 0.02552209235727787\n",
            "strain 0.02603834681212902\n",
            "strain 0.024784987792372704\n",
            "strain 0.024174705147743225\n",
            "classify 2.7010016441345215\n",
            "classify 2.6904568672180176\n",
            "classify 2.631136417388916\n",
            "classify 2.6329498291015625\n",
            "classify 2.650771141052246\n",
            "classify 2.6952826976776123\n",
            "classify 2.5918405055999756\n",
            "classify 2.6332931518554688\n",
            "classify 2.6851401329040527\n",
            "classify 2.7109556198120117\n",
            "classify 2.781816244125366\n",
            "0.1875\n",
            "0.1875\n",
            "0.15625\n",
            "0.140625\n",
            "0.109375\n",
            "0.09375\n",
            "0.234375\n",
            "0.203125\n",
            "0.203125\n",
            "0.09375\n",
            "0.171875\n",
            "time: 16.96965456008911 16.843399300867198\n",
            "147\n",
            "strain 0.02429196983575821\n",
            "strain 0.02695528231561184\n",
            "strain 0.026596784591674805\n",
            "strain 0.02519320882856846\n",
            "strain 0.026379486545920372\n",
            "strain 0.02604254148900509\n",
            "classify 2.707357168197632\n",
            "classify 2.6840035915374756\n",
            "classify 2.6760003566741943\n",
            "classify 2.6287472248077393\n",
            "classify 2.6640477180480957\n",
            "classify 2.6525731086730957\n",
            "classify 2.678767204284668\n",
            "classify 2.6782870292663574\n",
            "classify 2.745150089263916\n",
            "classify 2.6735785007476807\n",
            "classify 2.6593759059906006\n",
            "0.125\n",
            "0.15625\n",
            "0.15625\n",
            "0.203125\n",
            "0.296875\n",
            "0.203125\n",
            "0.125\n",
            "0.1875\n",
            "0.140625\n",
            "0.21875\n",
            "0.28125\n",
            "time: 16.536038875579834 16.84132545380979\n",
            "148\n",
            "strain 0.028001369908452034\n",
            "strain 0.026778096333146095\n",
            "strain 0.024445032700896263\n",
            "strain 0.026360495015978813\n",
            "strain 0.031252194195985794\n",
            "strain 0.02418028749525547\n",
            "classify 2.646209716796875\n",
            "classify 2.6274659633636475\n",
            "classify 2.6759064197540283\n",
            "classify 2.6884899139404297\n",
            "classify 2.633092164993286\n",
            "classify 2.6158714294433594\n",
            "classify 2.621086359024048\n",
            "classify 2.630387544631958\n",
            "classify 2.6245288848876953\n",
            "classify 2.604520559310913\n",
            "classify 2.6481690406799316\n",
            "0.140625\n",
            "0.09375\n",
            "0.1875\n",
            "0.15625\n",
            "0.109375\n",
            "0.140625\n",
            "0.203125\n",
            "0.109375\n",
            "0.171875\n",
            "0.140625\n",
            "0.125\n",
            "time: 16.544850826263428 16.839339601913554\n",
            "149\n",
            "strain 0.026179522275924683\n",
            "strain 0.02239282988011837\n",
            "strain 0.0267020370811224\n",
            "strain 0.02584703266620636\n",
            "strain 0.03216017782688141\n",
            "strain 0.030901363119482994\n",
            "classify 2.6931419372558594\n",
            "classify 2.6673758029937744\n",
            "classify 2.638171672821045\n",
            "classify 2.6427488327026367\n",
            "classify 2.631392240524292\n",
            "classify 2.654215097427368\n",
            "classify 2.6379897594451904\n",
            "classify 2.658914804458618\n",
            "classify 2.6401453018188477\n",
            "classify 2.6466457843780518\n",
            "classify 2.661594867706299\n",
            "0.109375\n",
            "0.171875\n",
            "0.1875\n",
            "0.203125\n",
            "0.15625\n",
            "0.21875\n",
            "0.171875\n",
            "0.1875\n",
            "0.140625\n",
            "0.1875\n",
            "0.09375\n",
            "time: 17.721864938735962 16.84522624492645\n",
            "150\n",
            "strain 0.024510733783245087\n",
            "strain 0.0245321337133646\n",
            "strain 0.026433801278471947\n",
            "strain 0.026066655293107033\n",
            "strain 0.024108020588755608\n",
            "strain 0.02512517385184765\n",
            "classify 2.699023485183716\n",
            "classify 2.6431922912597656\n",
            "classify 2.6197001934051514\n",
            "classify 2.6477115154266357\n",
            "classify 2.6165201663970947\n",
            "classify 2.636427879333496\n",
            "classify 2.697457790374756\n",
            "classify 2.6363959312438965\n",
            "classify 2.7017757892608643\n",
            "classify 2.6582019329071045\n",
            "classify 2.6863512992858887\n",
            "0.125\n",
            "0.125\n",
            "0.15625\n",
            "0.125\n",
            "0.203125\n",
            "0.125\n",
            "0.203125\n",
            "0.234375\n",
            "0.203125\n",
            "0.15625\n",
            "0.21875\n",
            "time: 16.551816701889038 16.843292807901143\n",
            "151\n",
            "strain 0.025918764993548393\n",
            "strain 0.027178434655070305\n",
            "strain 0.024898352101445198\n",
            "strain 0.02958076260983944\n",
            "strain 0.02730805240571499\n",
            "strain 0.02626454271376133\n",
            "classify 2.6371726989746094\n",
            "classify 2.650757312774658\n",
            "classify 2.67130184173584\n",
            "classify 2.6976137161254883\n",
            "classify 2.666243076324463\n",
            "classify 2.593822956085205\n",
            "classify 2.6672017574310303\n",
            "classify 2.7065672874450684\n",
            "classify 2.62100887298584\n",
            "classify 2.6812381744384766\n",
            "classify 2.6019561290740967\n",
            "0.1875\n",
            "0.09375\n",
            "0.140625\n",
            "0.09375\n",
            "0.125\n",
            "0.1875\n",
            "0.171875\n",
            "0.140625\n",
            "0.171875\n",
            "0.125\n",
            "0.1875\n",
            "time: 16.496402978897095 16.8410146393274\n",
            "152\n",
            "strain 0.028361819684505463\n",
            "strain 0.02445857971906662\n",
            "strain 0.023631595075130463\n",
            "strain 0.02492254041135311\n",
            "strain 0.026559501886367798\n",
            "strain 0.028885887935757637\n",
            "classify 2.6545567512512207\n",
            "classify 2.6614084243774414\n",
            "classify 2.7242109775543213\n",
            "classify 2.6382288932800293\n",
            "classify 2.660104751586914\n",
            "classify 2.6855275630950928\n",
            "classify 2.6822197437286377\n",
            "classify 2.650952100753784\n",
            "classify 2.63600754737854\n",
            "classify 2.661658525466919\n",
            "classify 2.606924057006836\n",
            "0.140625\n",
            "0.203125\n",
            "0.234375\n",
            "0.1875\n",
            "0.203125\n",
            "0.09375\n",
            "0.21875\n",
            "0.171875\n",
            "0.1875\n",
            "0.203125\n",
            "0.203125\n",
            "time: 16.51935577392578 16.83891546336654\n",
            "153\n",
            "strain 0.02541263960301876\n",
            "strain 0.029538145288825035\n",
            "strain 0.028552673757076263\n",
            "strain 0.027642659842967987\n",
            "strain 0.023729244247078896\n",
            "strain 0.028816184028983116\n",
            "classify 2.647601366043091\n",
            "classify 2.6368393898010254\n",
            "classify 2.5313072204589844\n",
            "classify 2.6598265171051025\n",
            "classify 2.6211724281311035\n",
            "classify 2.660313367843628\n",
            "classify 2.5807487964630127\n",
            "classify 2.678230047225952\n",
            "classify 2.5880908966064453\n",
            "classify 2.5953385829925537\n",
            "classify 2.6204774379730225\n",
            "0.109375\n",
            "0.21875\n",
            "0.1875\n",
            "0.109375\n",
            "0.15625\n",
            "0.171875\n",
            "0.1875\n",
            "0.109375\n",
            "0.140625\n",
            "0.15625\n",
            "0.15625\n",
            "time: 17.798936128616333 16.84515319396923\n",
            "154\n",
            "strain 0.024779682978987694\n",
            "strain 0.02873922698199749\n",
            "strain 0.02859645150601864\n",
            "strain 0.024718409404158592\n",
            "strain 0.027257582172751427\n",
            "strain 0.022491993382573128\n",
            "classify 2.5889697074890137\n",
            "classify 2.6875011920928955\n",
            "classify 2.683882236480713\n",
            "classify 2.6286842823028564\n",
            "classify 2.6753597259521484\n",
            "classify 2.6529650688171387\n",
            "classify 2.6508898735046387\n",
            "classify 2.6433582305908203\n",
            "classify 2.638685941696167\n",
            "classify 2.6759731769561768\n",
            "classify 2.687588691711426\n",
            "0.109375\n",
            "0.125\n",
            "0.140625\n",
            "0.15625\n",
            "0.125\n",
            "0.109375\n",
            "0.296875\n",
            "0.125\n",
            "0.09375\n",
            "0.234375\n",
            "0.1875\n",
            "time: 16.492181539535522 16.84287985678642\n",
            "155\n",
            "strain 0.02546682208776474\n",
            "strain 0.026883812621235847\n",
            "strain 0.027472713962197304\n",
            "strain 0.026564879342913628\n",
            "strain 0.02702142298221588\n",
            "strain 0.023157117888331413\n",
            "classify 2.6078202724456787\n",
            "classify 2.5998685359954834\n",
            "classify 2.6660473346710205\n",
            "classify 2.6572821140289307\n",
            "classify 2.587745428085327\n",
            "classify 2.605647087097168\n",
            "classify 2.588721990585327\n",
            "classify 2.613654613494873\n",
            "classify 2.574064254760742\n",
            "classify 2.603238105773926\n",
            "classify 2.600339651107788\n",
            "0.140625\n",
            "0.25\n",
            "0.15625\n",
            "0.078125\n",
            "0.125\n",
            "0.234375\n",
            "0.171875\n",
            "0.15625\n",
            "0.15625\n",
            "0.125\n",
            "0.21875\n",
            "time: 16.5489559173584 16.84100023446939\n",
            "156\n",
            "strain 0.029576532542705536\n",
            "strain 0.029549002647399902\n",
            "strain 0.024914225563406944\n",
            "strain 0.02957632578909397\n",
            "strain 0.02822616882622242\n",
            "strain 0.02762657403945923\n",
            "classify 2.640881061553955\n",
            "classify 2.568986177444458\n",
            "classify 2.6546895503997803\n",
            "classify 2.58705472946167\n",
            "classify 2.5866777896881104\n",
            "classify 2.5995001792907715\n",
            "classify 2.5784144401550293\n",
            "classify 2.670973062515259\n",
            "classify 2.6544508934020996\n",
            "classify 2.593304395675659\n",
            "classify 2.594390392303467\n",
            "0.21875\n",
            "0.203125\n",
            "0.203125\n",
            "0.171875\n",
            "0.234375\n",
            "0.109375\n",
            "0.140625\n",
            "0.171875\n",
            "0.140625\n",
            "0.21875\n",
            "0.203125\n",
            "time: 16.606740951538086 16.839511954860324\n",
            "157\n",
            "strain 0.025938577950000763\n",
            "strain 0.02652451954782009\n",
            "strain 0.029187968000769615\n",
            "strain 0.025685230270028114\n",
            "strain 0.029804514721035957\n",
            "strain 0.02600497007369995\n",
            "classify 2.6190950870513916\n",
            "classify 2.5862340927124023\n",
            "classify 2.6584677696228027\n",
            "classify 2.632025718688965\n",
            "classify 2.5988705158233643\n",
            "classify 2.621946334838867\n",
            "classify 2.654534339904785\n",
            "classify 2.6550323963165283\n",
            "classify 2.576765537261963\n",
            "classify 2.640115737915039\n",
            "classify 2.628938913345337\n",
            "0.15625\n",
            "0.140625\n",
            "0.21875\n",
            "0.28125\n",
            "0.15625\n",
            "0.125\n",
            "0.1875\n",
            "0.125\n",
            "0.265625\n",
            "0.171875\n",
            "0.1875\n",
            "time: 17.660726308822632 16.844712856449657\n",
            "158\n",
            "strain 0.028227893635630608\n",
            "strain 0.025367818772792816\n",
            "strain 0.02807907946407795\n",
            "strain 0.027887171134352684\n",
            "strain 0.02827439270913601\n",
            "strain 0.02554219961166382\n",
            "classify 2.6150691509246826\n",
            "classify 2.6003611087799072\n",
            "classify 2.6165900230407715\n",
            "classify 2.577450752258301\n",
            "classify 2.5606071949005127\n",
            "classify 2.535862445831299\n",
            "classify 2.551952362060547\n",
            "classify 2.604795217514038\n",
            "classify 2.6031875610351562\n",
            "classify 2.6133363246917725\n",
            "classify 2.614691972732544\n",
            "0.25\n",
            "0.15625\n",
            "0.125\n",
            "0.203125\n",
            "0.15625\n",
            "0.140625\n",
            "0.234375\n",
            "0.15625\n",
            "0.21875\n",
            "0.21875\n",
            "0.21875\n",
            "time: 16.45530104637146 16.8422673828197\n",
            "159\n",
            "strain 0.02732238359749317\n",
            "strain 0.02541695535182953\n",
            "strain 0.026200806722044945\n",
            "strain 0.029353013262152672\n",
            "strain 0.028461039066314697\n",
            "strain 0.02459178864955902\n",
            "classify 2.6525630950927734\n",
            "classify 2.6334035396575928\n",
            "classify 2.6574738025665283\n",
            "classify 2.7032387256622314\n",
            "classify 2.648603677749634\n",
            "classify 2.665215253829956\n",
            "classify 2.684640884399414\n",
            "classify 2.675779342651367\n",
            "classify 2.632382869720459\n",
            "classify 2.6279168128967285\n",
            "classify 2.653348207473755\n",
            "0.140625\n",
            "0.21875\n",
            "0.203125\n",
            "0.171875\n",
            "0.171875\n",
            "0.171875\n",
            "0.171875\n",
            "0.171875\n",
            "0.21875\n",
            "0.203125\n",
            "0.21875\n",
            "time: 16.48601722717285 16.840044532716274\n",
            "160\n",
            "strain 0.025783494114875793\n",
            "strain 0.022947972640395164\n",
            "strain 0.023069947957992554\n",
            "strain 0.024791179224848747\n",
            "strain 0.028151147067546844\n",
            "strain 0.02483457885682583\n",
            "classify 2.6084303855895996\n",
            "classify 2.563282012939453\n",
            "classify 2.6149985790252686\n",
            "classify 2.6600747108459473\n",
            "classify 2.6474449634552\n",
            "classify 2.6503968238830566\n",
            "classify 2.652276039123535\n",
            "classify 2.638154983520508\n",
            "classify 2.6991007328033447\n",
            "classify 2.64551043510437\n",
            "classify 2.662200689315796\n",
            "0.1875\n",
            "0.25\n",
            "0.234375\n",
            "0.21875\n",
            "0.203125\n",
            "0.1875\n",
            "0.203125\n",
            "0.171875\n",
            "0.078125\n",
            "0.15625\n",
            "0.09375\n",
            "time: 17.05972647666931 16.841412720472917\n",
            "161\n",
            "strain 0.02800915576517582\n",
            "strain 0.026768505573272705\n",
            "strain 0.025660911574959755\n",
            "strain 0.023104028776288033\n",
            "strain 0.025127558037638664\n",
            "strain 0.02498990297317505\n",
            "classify 2.6123523712158203\n",
            "classify 2.6523425579071045\n",
            "classify 2.635820150375366\n",
            "classify 2.661794424057007\n",
            "classify 2.6210711002349854\n",
            "classify 2.6605591773986816\n",
            "classify 2.6272406578063965\n",
            "classify 2.671421527862549\n",
            "classify 2.640390396118164\n",
            "classify 2.660991907119751\n",
            "classify 2.6660995483398438\n",
            "0.15625\n",
            "0.109375\n",
            "0.1875\n",
            "0.15625\n",
            "0.171875\n",
            "0.1875\n",
            "0.15625\n",
            "0.140625\n",
            "0.140625\n",
            "0.203125\n",
            "0.125\n",
            "time: 17.120901346206665 16.843142007604058\n",
            "162\n",
            "strain 0.023648055270314217\n",
            "strain 0.029225004836916924\n",
            "strain 0.02600722759962082\n",
            "strain 0.023126674816012383\n",
            "strain 0.025125963613390923\n",
            "strain 0.027792342007160187\n",
            "classify 2.635326862335205\n",
            "classify 2.5909550189971924\n",
            "classify 2.6099421977996826\n",
            "classify 2.619699001312256\n",
            "classify 2.607769012451172\n",
            "classify 2.595341205596924\n",
            "classify 2.6372530460357666\n",
            "classify 2.6399521827697754\n",
            "classify 2.5305750370025635\n",
            "classify 2.632648229598999\n",
            "classify 2.568171501159668\n",
            "0.203125\n",
            "0.21875\n",
            "0.125\n",
            "0.25\n",
            "0.171875\n",
            "0.21875\n",
            "0.171875\n",
            "0.171875\n",
            "0.265625\n",
            "0.140625\n",
            "0.125\n",
            "time: 16.511998414993286 16.841113453262423\n",
            "163\n",
            "strain 0.028059080243110657\n",
            "strain 0.024619147181510925\n",
            "strain 0.0254663098603487\n",
            "strain 0.025472713634371758\n",
            "strain 0.02618619240820408\n",
            "strain 0.028752470389008522\n",
            "classify 2.639539957046509\n",
            "classify 2.5922107696533203\n",
            "classify 2.610429286956787\n",
            "classify 2.6112804412841797\n",
            "classify 2.639286518096924\n",
            "classify 2.627408266067505\n",
            "classify 2.610095977783203\n",
            "classify 2.6433143615722656\n",
            "classify 2.603332281112671\n",
            "classify 2.6186771392822266\n",
            "classify 2.564919948577881\n",
            "0.265625\n",
            "0.0625\n",
            "0.1875\n",
            "0.203125\n",
            "0.109375\n",
            "0.078125\n",
            "0.1875\n",
            "0.15625\n",
            "0.171875\n",
            "0.125\n",
            "0.09375\n",
            "time: 16.47176241874695 16.838863907790767\n",
            "164\n",
            "strain 0.025557866320014\n",
            "strain 0.02947787009179592\n",
            "strain 0.026624247431755066\n",
            "strain 0.03013957478106022\n",
            "strain 0.02454456128180027\n",
            "strain 0.02615758776664734\n",
            "classify 2.637725591659546\n",
            "classify 2.6194534301757812\n",
            "classify 2.623762369155884\n",
            "classify 2.5822901725769043\n",
            "classify 2.656611204147339\n",
            "classify 2.5974814891815186\n",
            "classify 2.646876335144043\n",
            "classify 2.56630277633667\n",
            "classify 2.6360273361206055\n",
            "classify 2.63012957572937\n",
            "classify 2.6744906902313232\n",
            "0.265625\n",
            "0.21875\n",
            "0.1875\n",
            "0.234375\n",
            "0.171875\n",
            "0.21875\n",
            "0.25\n",
            "0.140625\n",
            "0.203125\n",
            "0.21875\n",
            "0.171875\n",
            "time: 17.323214054107666 16.84180165493127\n",
            "165\n",
            "strain 0.028919139876961708\n",
            "strain 0.026094375178217888\n",
            "strain 0.026703542098402977\n",
            "strain 0.026090966537594795\n",
            "strain 0.02525971829891205\n",
            "strain 0.02207101136445999\n",
            "classify 2.624441623687744\n",
            "classify 2.6315040588378906\n",
            "classify 2.5978548526763916\n",
            "classify 2.5447869300842285\n",
            "classify 2.5867531299591064\n",
            "classify 2.628026008605957\n",
            "classify 2.5896780490875244\n",
            "classify 2.6181740760803223\n",
            "classify 2.628636360168457\n",
            "classify 2.613224506378174\n",
            "classify 2.610870361328125\n",
            "0.125\n",
            "0.171875\n",
            "0.109375\n",
            "0.171875\n",
            "0.140625\n",
            "0.125\n",
            "0.234375\n",
            "0.171875\n",
            "0.15625\n",
            "0.140625\n",
            "0.171875\n",
            "time: 16.832059144973755 16.84174501608653\n",
            "166\n",
            "strain 0.02457602322101593\n",
            "strain 0.026493653655052185\n",
            "strain 0.02398536168038845\n",
            "strain 0.025685718283057213\n",
            "strain 0.025712579488754272\n",
            "strain 0.029339930042624474\n",
            "classify 2.599456310272217\n",
            "classify 2.6616227626800537\n",
            "classify 2.621408224105835\n",
            "classify 2.6247878074645996\n",
            "classify 2.674302339553833\n",
            "classify 2.642637252807617\n",
            "classify 2.5807414054870605\n",
            "classify 2.649038791656494\n",
            "classify 2.6063671112060547\n",
            "classify 2.552743673324585\n",
            "classify 2.5885984897613525\n",
            "0.125\n",
            "0.234375\n",
            "0.1875\n",
            "0.203125\n",
            "0.1875\n",
            "0.1875\n",
            "0.109375\n",
            "0.265625\n",
            "0.09375\n",
            "0.09375\n",
            "0.25\n",
            "time: 16.651286840438843 16.84060808998382\n",
            "167\n",
            "strain 0.02602405846118927\n",
            "strain 0.02598920278251171\n",
            "strain 0.02647642232477665\n",
            "strain 0.027024440467357635\n",
            "strain 0.023366248235106468\n",
            "strain 0.02312530018389225\n",
            "classify 2.5766963958740234\n",
            "classify 2.6437690258026123\n",
            "classify 2.586094617843628\n",
            "classify 2.611612558364868\n",
            "classify 2.6012940406799316\n",
            "classify 2.5783276557922363\n",
            "classify 2.6075334548950195\n",
            "classify 2.6343085765838623\n",
            "classify 2.532564163208008\n",
            "classify 2.5783655643463135\n",
            "classify 2.5874969959259033\n",
            "0.203125\n",
            "0.1875\n",
            "0.140625\n",
            "0.171875\n",
            "0.1875\n",
            "0.15625\n",
            "0.203125\n",
            "0.15625\n",
            "0.296875\n",
            "0.21875\n",
            "0.171875\n",
            "time: 16.55970072746277 16.838938765582583\n",
            "168\n",
            "strain 0.025869468227028847\n",
            "strain 0.025101618841290474\n",
            "strain 0.025899233296513557\n",
            "strain 0.024845683947205544\n",
            "strain 0.028004063293337822\n",
            "strain 0.025119835510849953\n",
            "classify 2.622337579727173\n",
            "classify 2.5891709327697754\n",
            "classify 2.5552475452423096\n",
            "classify 2.5792229175567627\n",
            "classify 2.5889642238616943\n",
            "classify 2.5598747730255127\n",
            "classify 2.6197643280029297\n",
            "classify 2.599388599395752\n",
            "classify 2.5111958980560303\n",
            "classify 2.6199841499328613\n",
            "classify 2.628840208053589\n",
            "0.21875\n",
            "0.203125\n",
            "0.171875\n",
            "0.15625\n",
            "0.25\n",
            "0.171875\n",
            "0.1875\n",
            "0.125\n",
            "0.1875\n",
            "0.171875\n",
            "0.3125\n",
            "time: 17.721959829330444 16.844171374507205\n",
            "169\n",
            "strain 0.029615534469485283\n",
            "strain 0.02794560231268406\n",
            "strain 0.027874574065208435\n",
            "strain 0.027973031625151634\n",
            "strain 0.0236979853361845\n",
            "strain 0.026015937328338623\n",
            "classify 2.5940756797790527\n",
            "classify 2.5855917930603027\n",
            "classify 2.6245880126953125\n",
            "classify 2.6154086589813232\n",
            "classify 2.5842461585998535\n",
            "classify 2.590606212615967\n",
            "classify 2.6564583778381348\n",
            "classify 2.6030282974243164\n",
            "classify 2.5998051166534424\n",
            "classify 2.551046848297119\n",
            "classify 2.595344066619873\n",
            "0.171875\n",
            "0.125\n",
            "0.296875\n",
            "0.203125\n",
            "0.125\n",
            "0.140625\n",
            "0.15625\n",
            "0.21875\n",
            "0.21875\n",
            "0.1875\n",
            "0.125\n",
            "time: 16.44391441345215 16.84182061027078\n",
            "170\n",
            "strain 0.027998946607112885\n",
            "strain 0.029276303946971893\n",
            "strain 0.025924870744347572\n",
            "strain 0.02947554737329483\n",
            "strain 0.026988625526428223\n",
            "strain 0.02939293347299099\n",
            "classify 2.5344412326812744\n",
            "classify 2.6253509521484375\n",
            "classify 2.6139557361602783\n",
            "classify 2.6570026874542236\n",
            "classify 2.681361436843872\n",
            "classify 2.5704572200775146\n",
            "classify 2.6270992755889893\n",
            "classify 2.6414811611175537\n",
            "classify 2.6049625873565674\n",
            "classify 2.628662586212158\n",
            "classify 2.656362295150757\n",
            "0.203125\n",
            "0.109375\n",
            "0.171875\n",
            "0.171875\n",
            "0.109375\n",
            "0.21875\n",
            "0.171875\n",
            "0.15625\n",
            "0.1875\n",
            "0.1875\n",
            "0.203125\n",
            "time: 16.455357551574707 16.839563930243777\n",
            "171\n",
            "strain 0.025615835562348366\n",
            "strain 0.027492066845297813\n",
            "strain 0.026420200243592262\n",
            "strain 0.026186339557170868\n",
            "strain 0.02632584609091282\n",
            "strain 0.025734303519129753\n",
            "classify 2.5927631855010986\n",
            "classify 2.584634780883789\n",
            "classify 2.6565089225769043\n",
            "classify 2.5993733406066895\n",
            "classify 2.5746209621429443\n",
            "classify 2.5669972896575928\n",
            "classify 2.619192361831665\n",
            "classify 2.6207048892974854\n",
            "classify 2.605727195739746\n",
            "classify 2.579404830932617\n",
            "classify 2.6011526584625244\n",
            "0.21875\n",
            "0.125\n",
            "0.140625\n",
            "0.140625\n",
            "0.1875\n",
            "0.234375\n",
            "0.1875\n",
            "0.171875\n",
            "0.171875\n",
            "0.140625\n",
            "0.15625\n",
            "time: 16.58232831954956 16.838071096775142\n",
            "172\n",
            "strain 0.02717181295156479\n",
            "strain 0.025366561487317085\n",
            "strain 0.02584645338356495\n",
            "strain 0.025041043758392334\n",
            "strain 0.02505563758313656\n",
            "strain 0.030466563999652863\n",
            "classify 2.618464469909668\n",
            "classify 2.613300085067749\n",
            "classify 2.5845084190368652\n",
            "classify 2.542112112045288\n",
            "classify 2.621493339538574\n",
            "classify 2.571394920349121\n",
            "classify 2.644786834716797\n",
            "classify 2.603947639465332\n",
            "classify 2.579227924346924\n",
            "classify 2.576936721801758\n",
            "classify 2.6370363235473633\n",
            "0.125\n",
            "0.109375\n",
            "0.203125\n",
            "0.15625\n",
            "0.09375\n",
            "0.1875\n",
            "0.15625\n",
            "0.09375\n",
            "0.1875\n",
            "0.09375\n",
            "0.1875\n",
            "time: 17.741324424743652 16.84329557832266\n",
            "173\n",
            "strain 0.025733662769198418\n",
            "strain 0.02412320487201214\n",
            "strain 0.02609012834727764\n",
            "strain 0.03045836091041565\n",
            "strain 0.024741366505622864\n",
            "strain 0.026146329939365387\n",
            "classify 2.563756227493286\n",
            "classify 2.5602638721466064\n",
            "classify 2.593724489212036\n",
            "classify 2.632225513458252\n",
            "classify 2.62634539604187\n",
            "classify 2.6085708141326904\n",
            "classify 2.5818071365356445\n",
            "classify 2.6552085876464844\n",
            "classify 2.603641986846924\n",
            "classify 2.635786533355713\n",
            "classify 2.6114375591278076\n",
            "0.234375\n",
            "0.109375\n",
            "0.1875\n",
            "0.234375\n",
            "0.09375\n",
            "0.1875\n",
            "0.15625\n",
            "0.15625\n",
            "0.171875\n",
            "0.125\n",
            "0.15625\n",
            "time: 16.506178379058838 16.841361537746998\n",
            "174\n",
            "strain 0.027572190389037132\n",
            "strain 0.029006093740463257\n",
            "strain 0.02671392820775509\n",
            "strain 0.026744605973362923\n",
            "strain 0.028045713901519775\n",
            "strain 0.02709808014333248\n",
            "classify 2.5801808834075928\n",
            "classify 2.631070137023926\n",
            "classify 2.5637128353118896\n",
            "classify 2.5868616104125977\n",
            "classify 2.6404576301574707\n",
            "classify 2.663473606109619\n",
            "classify 2.623692750930786\n",
            "classify 2.594176769256592\n",
            "classify 2.596012592315674\n",
            "classify 2.5902676582336426\n",
            "classify 2.5693371295928955\n",
            "0.265625\n",
            "0.171875\n",
            "0.140625\n",
            "0.140625\n",
            "0.09375\n",
            "0.1875\n",
            "0.09375\n",
            "0.234375\n",
            "0.203125\n",
            "0.125\n",
            "0.234375\n",
            "time: 16.49969720840454 16.839413452148438\n",
            "175\n",
            "strain 0.02612665854394436\n",
            "strain 0.02353072725236416\n",
            "strain 0.02825799770653248\n",
            "strain 0.02628529630601406\n",
            "strain 0.030057815834879875\n",
            "strain 0.024597501382231712\n",
            "classify 2.548783302307129\n",
            "classify 2.6292572021484375\n",
            "classify 2.547614336013794\n",
            "classify 2.572021484375\n",
            "classify 2.6026437282562256\n",
            "classify 2.556602716445923\n",
            "classify 2.49440860748291\n",
            "classify 2.5832860469818115\n",
            "classify 2.5727250576019287\n",
            "classify 2.6283421516418457\n",
            "classify 2.6057989597320557\n",
            "0.203125\n",
            "0.171875\n",
            "0.140625\n",
            "0.203125\n",
            "0.171875\n",
            "0.171875\n",
            "0.140625\n",
            "0.171875\n",
            "0.171875\n",
            "0.234375\n",
            "0.09375\n",
            "time: 16.804752826690674 16.839220611886546\n",
            "176\n",
            "strain 0.023813901469111443\n",
            "strain 0.025775397196412086\n",
            "strain 0.02698616497218609\n",
            "strain 0.024139652028679848\n",
            "strain 0.026127435266971588\n",
            "strain 0.02758629061281681\n",
            "classify 2.572836399078369\n",
            "classify 2.5648467540740967\n",
            "classify 2.595931053161621\n",
            "classify 2.6440482139587402\n",
            "classify 2.567657470703125\n",
            "classify 2.514333963394165\n",
            "classify 2.529970645904541\n",
            "classify 2.58597469329834\n",
            "classify 2.663999557495117\n",
            "classify 2.6238536834716797\n",
            "classify 2.5941483974456787\n",
            "0.234375\n",
            "0.171875\n",
            "0.21875\n",
            "0.140625\n",
            "0.21875\n",
            "0.140625\n",
            "0.171875\n",
            "0.09375\n",
            "0.21875\n",
            "0.234375\n",
            "0.1875\n",
            "time: 17.362790822982788 16.842185044692734\n",
            "177\n",
            "strain 0.026968369260430336\n",
            "strain 0.025048023089766502\n",
            "strain 0.023458868265151978\n",
            "strain 0.022566579282283783\n",
            "strain 0.02465779148042202\n",
            "strain 0.024847010150551796\n",
            "classify 2.5840799808502197\n",
            "classify 2.5260531902313232\n",
            "classify 2.621192216873169\n",
            "classify 2.5779738426208496\n",
            "classify 2.65860915184021\n",
            "classify 2.569483518600464\n",
            "classify 2.5676939487457275\n",
            "classify 2.5811498165130615\n",
            "classify 2.5505948066711426\n",
            "classify 2.580237865447998\n",
            "classify 2.555295944213867\n",
            "0.140625\n",
            "0.203125\n",
            "0.125\n",
            "0.15625\n",
            "0.15625\n",
            "0.15625\n",
            "0.203125\n",
            "0.234375\n",
            "0.1875\n",
            "0.125\n",
            "0.1875\n",
            "time: 16.482921838760376 16.84016994679912\n",
            "178\n",
            "strain 0.0255601704120636\n",
            "strain 0.028164813295006752\n",
            "strain 0.032796699553728104\n",
            "strain 0.025745108723640442\n",
            "strain 0.027811596170067787\n",
            "strain 0.027517152950167656\n",
            "classify 2.5936496257781982\n",
            "classify 2.5592055320739746\n",
            "classify 2.524801731109619\n",
            "classify 2.5280165672302246\n",
            "classify 2.5129966735839844\n",
            "classify 2.5805373191833496\n",
            "classify 2.4880967140197754\n",
            "classify 2.5696704387664795\n",
            "classify 2.58872652053833\n",
            "classify 2.556027412414551\n",
            "classify 2.5597984790802\n",
            "0.234375\n",
            "0.140625\n",
            "0.125\n",
            "0.1875\n",
            "0.15625\n",
            "0.265625\n",
            "0.125\n",
            "0.203125\n",
            "0.234375\n",
            "0.09375\n",
            "0.125\n",
            "time: 16.503878593444824 16.838293583033472\n",
            "179\n",
            "strain 0.025541529059410095\n",
            "strain 0.025824597105383873\n",
            "strain 0.026240115985274315\n",
            "strain 0.02420300431549549\n",
            "strain 0.02623206563293934\n",
            "strain 0.02629055082798004\n",
            "classify 2.588925838470459\n",
            "classify 2.620720386505127\n",
            "classify 2.573521375656128\n",
            "classify 2.6111323833465576\n",
            "classify 2.6067302227020264\n",
            "classify 2.631225109100342\n",
            "classify 2.575127601623535\n",
            "classify 2.560482978820801\n",
            "classify 2.513632297515869\n",
            "classify 2.636923313140869\n",
            "classify 2.5736398696899414\n",
            "0.1875\n",
            "0.125\n",
            "0.203125\n",
            "0.15625\n",
            "0.15625\n",
            "0.125\n",
            "0.125\n",
            "0.21875\n",
            "0.203125\n",
            "0.140625\n",
            "0.1875\n",
            "time: 17.29173254966736 16.840816095140244\n",
            "180\n",
            "strain 0.028877265751361847\n",
            "strain 0.026961028575897217\n",
            "strain 0.025401843711733818\n",
            "strain 0.029114050790667534\n",
            "strain 0.024904625490307808\n",
            "strain 0.025097375735640526\n",
            "classify 2.6006643772125244\n",
            "classify 2.527444362640381\n",
            "classify 2.6209776401519775\n",
            "classify 2.6222174167633057\n",
            "classify 2.5896553993225098\n",
            "classify 2.574669361114502\n",
            "classify 2.5712127685546875\n",
            "classify 2.543874502182007\n",
            "classify 2.61373233795166\n",
            "classify 2.569195032119751\n",
            "classify 2.62495493888855\n",
            "0.203125\n",
            "0.140625\n",
            "0.171875\n",
            "0.15625\n",
            "0.171875\n",
            "0.09375\n",
            "0.1875\n",
            "0.21875\n",
            "0.15625\n",
            "0.140625\n",
            "0.203125\n",
            "time: 17.005489110946655 16.841732062028917\n",
            "181\n",
            "strain 0.03096860647201538\n",
            "strain 0.028206855058670044\n",
            "strain 0.0279254037886858\n",
            "strain 0.02526117116212845\n",
            "strain 0.029009969905018806\n",
            "strain 0.029059089720249176\n",
            "classify 2.5902812480926514\n",
            "classify 2.6413114070892334\n",
            "classify 2.587902545928955\n",
            "classify 2.508425712585449\n",
            "classify 2.587650775909424\n",
            "classify 2.551882028579712\n",
            "classify 2.60544753074646\n",
            "classify 2.5679402351379395\n",
            "classify 2.5539448261260986\n",
            "classify 2.6402597427368164\n",
            "classify 2.585167169570923\n",
            "0.234375\n",
            "0.1875\n",
            "0.171875\n",
            "0.125\n",
            "0.203125\n",
            "0.234375\n",
            "0.234375\n",
            "0.140625\n",
            "0.265625\n",
            "0.125\n",
            "0.140625\n",
            "time: 16.531381368637085 16.84003026668842\n",
            "182\n",
            "strain 0.025226138532161713\n",
            "strain 0.024574095383286476\n",
            "strain 0.022906705737113953\n",
            "strain 0.029181530699133873\n",
            "strain 0.025031426921486855\n",
            "strain 0.027600916102528572\n",
            "classify 2.5239145755767822\n",
            "classify 2.5315022468566895\n",
            "classify 2.5619311332702637\n",
            "classify 2.5855350494384766\n",
            "classify 2.5733821392059326\n",
            "classify 2.563767194747925\n",
            "classify 2.560837984085083\n",
            "classify 2.625377893447876\n",
            "classify 2.5985782146453857\n",
            "classify 2.5508134365081787\n",
            "classify 2.5517117977142334\n",
            "0.1875\n",
            "0.3125\n",
            "0.171875\n",
            "0.25\n",
            "0.1875\n",
            "0.21875\n",
            "0.109375\n",
            "0.203125\n",
            "0.1875\n",
            "0.171875\n",
            "0.15625\n",
            "time: 16.5365629196167 16.838374995143035\n",
            "183\n",
            "strain 0.02502625621855259\n",
            "strain 0.027669375762343407\n",
            "strain 0.025759072974324226\n",
            "strain 0.02611621655523777\n",
            "strain 0.027318840846419334\n",
            "strain 0.025573978200554848\n",
            "classify 2.516925096511841\n",
            "classify 2.559678792953491\n",
            "classify 2.5470244884490967\n",
            "classify 2.563276767730713\n",
            "classify 2.594123363494873\n",
            "classify 2.5454294681549072\n",
            "classify 2.536668539047241\n",
            "classify 2.5323288440704346\n",
            "classify 2.576514720916748\n",
            "classify 2.5688745975494385\n",
            "classify 2.5681726932525635\n",
            "0.1875\n",
            "0.140625\n",
            "0.125\n",
            "0.140625\n",
            "0.140625\n",
            "0.171875\n",
            "0.15625\n",
            "0.203125\n",
            "0.25\n",
            "0.296875\n",
            "0.203125\n",
            "time: 17.80266571044922 16.8436186456162\n",
            "184\n",
            "strain 0.025408754125237465\n",
            "strain 0.02395731210708618\n",
            "strain 0.024519184604287148\n",
            "strain 0.02653137408196926\n",
            "strain 0.02311384119093418\n",
            "strain 0.02262199856340885\n",
            "classify 2.598151206970215\n",
            "classify 2.588549852371216\n",
            "classify 2.522855520248413\n",
            "classify 2.6024341583251953\n",
            "classify 2.5412192344665527\n",
            "classify 2.5491926670074463\n",
            "classify 2.5741569995880127\n",
            "classify 2.5316691398620605\n",
            "classify 2.5461063385009766\n",
            "classify 2.536914587020874\n",
            "classify 2.541271924972534\n",
            "0.203125\n",
            "0.234375\n",
            "0.203125\n",
            "0.25\n",
            "0.203125\n",
            "0.1875\n",
            "0.203125\n",
            "0.21875\n",
            "0.171875\n",
            "0.125\n",
            "0.171875\n",
            "time: 16.463998556137085 16.841570113156294\n",
            "185\n",
            "strain 0.02757498063147068\n",
            "strain 0.0288547296077013\n",
            "strain 0.026951782405376434\n",
            "strain 0.025221562013030052\n",
            "strain 0.02573489397764206\n",
            "strain 0.030619757249951363\n",
            "classify 2.657183885574341\n",
            "classify 2.63616943359375\n",
            "classify 2.5831501483917236\n",
            "classify 2.594642162322998\n",
            "classify 2.5484373569488525\n",
            "classify 2.6069540977478027\n",
            "classify 2.5958142280578613\n",
            "classify 2.6473114490509033\n",
            "classify 2.6113672256469727\n",
            "classify 2.607957124710083\n",
            "classify 2.5795068740844727\n",
            "0.203125\n",
            "0.171875\n",
            "0.203125\n",
            "0.15625\n",
            "0.25\n",
            "0.140625\n",
            "0.1875\n",
            "0.15625\n",
            "0.125\n",
            "0.21875\n",
            "0.171875\n",
            "time: 16.59325909614563 16.840238872394767\n",
            "186\n",
            "strain 0.025274910032749176\n",
            "strain 0.026463352143764496\n",
            "strain 0.02429567277431488\n",
            "strain 0.028500467538833618\n",
            "strain 0.024820884689688683\n",
            "strain 0.028517207130789757\n",
            "classify 2.602771043777466\n",
            "classify 2.6248044967651367\n",
            "classify 2.5610764026641846\n",
            "classify 2.6055383682250977\n",
            "classify 2.530714273452759\n",
            "classify 2.548661470413208\n",
            "classify 2.5413730144500732\n",
            "classify 2.5563762187957764\n",
            "classify 2.5617823600769043\n",
            "classify 2.5811684131622314\n",
            "classify 2.5415713787078857\n",
            "0.15625\n",
            "0.15625\n",
            "0.28125\n",
            "0.234375\n",
            "0.203125\n",
            "0.21875\n",
            "0.15625\n",
            "0.234375\n",
            "0.28125\n",
            "0.234375\n",
            "0.21875\n",
            "time: 16.59236454963684 16.838916500621938\n",
            "187\n",
            "strain 0.025500953197479248\n",
            "strain 0.02709883451461792\n",
            "strain 0.02480204962193966\n",
            "strain 0.027832457795739174\n",
            "strain 0.027842434123158455\n",
            "strain 0.026892082765698433\n",
            "classify 2.549812078475952\n",
            "classify 2.6406853199005127\n",
            "classify 2.58868670463562\n",
            "classify 2.586103916168213\n",
            "classify 2.5625908374786377\n",
            "classify 2.572190523147583\n",
            "classify 2.538628101348877\n",
            "classify 2.5838189125061035\n",
            "classify 2.5704421997070312\n",
            "classify 2.546082019805908\n",
            "classify 2.5104222297668457\n",
            "0.265625\n",
            "0.171875\n",
            "0.1875\n",
            "0.15625\n",
            "0.21875\n",
            "0.171875\n",
            "0.140625\n",
            "0.265625\n",
            "0.171875\n",
            "0.171875\n",
            "0.109375\n",
            "time: 17.72240662574768 16.84361829783054\n",
            "188\n",
            "strain 0.023583978414535522\n",
            "strain 0.02606891095638275\n",
            "strain 0.02723509818315506\n",
            "strain 0.025197239592671394\n",
            "strain 0.025565743446350098\n",
            "strain 0.023416871204972267\n",
            "classify 2.567845344543457\n",
            "classify 2.483879327774048\n",
            "classify 2.6049413681030273\n",
            "classify 2.5615720748901367\n",
            "classify 2.5862298011779785\n",
            "classify 2.533881425857544\n",
            "classify 2.5749659538269043\n",
            "classify 2.5881381034851074\n",
            "classify 2.6209256649017334\n",
            "classify 2.463397979736328\n",
            "classify 2.5659773349761963\n",
            "0.140625\n",
            "0.171875\n",
            "0.140625\n",
            "0.234375\n",
            "0.109375\n",
            "0.265625\n",
            "0.15625\n",
            "0.171875\n",
            "0.15625\n",
            "0.109375\n",
            "0.171875\n",
            "time: 16.52174210548401 16.841918529026092\n",
            "189\n",
            "strain 0.024571135640144348\n",
            "strain 0.025357550010085106\n",
            "strain 0.025504402816295624\n",
            "strain 0.025329874828457832\n",
            "strain 0.02501317672431469\n",
            "strain 0.02733653225004673\n",
            "classify 2.5300512313842773\n",
            "classify 2.540205478668213\n",
            "classify 2.5371766090393066\n",
            "classify 2.581657886505127\n",
            "classify 2.5783438682556152\n",
            "classify 2.6215786933898926\n",
            "classify 2.59987735748291\n",
            "classify 2.629897356033325\n",
            "classify 2.58298397064209\n",
            "classify 2.6586318016052246\n",
            "classify 2.544127941131592\n",
            "0.203125\n",
            "0.15625\n",
            "0.203125\n",
            "0.234375\n",
            "0.25\n",
            "0.25\n",
            "0.28125\n",
            "0.28125\n",
            "0.1875\n",
            "0.078125\n",
            "0.15625\n",
            "time: 16.545692443847656 16.84036237691578\n",
            "190\n",
            "strain 0.026138460263609886\n",
            "strain 0.022269466891884804\n",
            "strain 0.026841148734092712\n",
            "strain 0.026112904772162437\n",
            "strain 0.026765599846839905\n",
            "strain 0.027051860466599464\n",
            "classify 2.588409900665283\n",
            "classify 2.4967780113220215\n",
            "classify 2.549927234649658\n",
            "classify 2.5070080757141113\n",
            "classify 2.548687696456909\n",
            "classify 2.5037217140197754\n",
            "classify 2.532421350479126\n",
            "classify 2.478182315826416\n",
            "classify 2.565772771835327\n",
            "classify 2.4532647132873535\n",
            "classify 2.5198655128479004\n",
            "0.28125\n",
            "0.171875\n",
            "0.265625\n",
            "0.15625\n",
            "0.203125\n",
            "0.203125\n",
            "0.171875\n",
            "0.21875\n",
            "0.21875\n",
            "0.203125\n",
            "0.375\n",
            "time: 16.715136766433716 16.839709678869596\n",
            "191\n",
            "strain 0.026619506999850273\n",
            "strain 0.024725018069148064\n",
            "strain 0.023856282234191895\n",
            "strain 0.028561612591147423\n",
            "strain 0.025858454406261444\n",
            "strain 0.024994144216179848\n",
            "classify 2.5785999298095703\n",
            "classify 2.5838751792907715\n",
            "classify 2.6110284328460693\n",
            "classify 2.5792236328125\n",
            "classify 2.578378438949585\n",
            "classify 2.5996479988098145\n",
            "classify 2.593655586242676\n",
            "classify 2.5597667694091797\n",
            "classify 2.577101230621338\n",
            "classify 2.5630271434783936\n",
            "classify 2.5748636722564697\n",
            "0.109375\n",
            "0.171875\n",
            "0.09375\n",
            "0.234375\n",
            "0.21875\n",
            "0.078125\n",
            "0.109375\n",
            "0.140625\n",
            "0.140625\n",
            "0.21875\n",
            "0.21875\n",
            "time: 17.48911762237549 16.843098082890112\n",
            "192\n",
            "strain 0.023808693513274193\n",
            "strain 0.024016426876187325\n",
            "strain 0.026706552132964134\n",
            "strain 0.025038735941052437\n",
            "strain 0.025367960333824158\n",
            "strain 0.024873634800314903\n",
            "classify 2.5450234413146973\n",
            "classify 2.5594661235809326\n",
            "classify 2.5706238746643066\n",
            "classify 2.616950273513794\n",
            "classify 2.5611000061035156\n",
            "classify 2.51898455619812\n",
            "classify 2.5771825313568115\n",
            "classify 2.5847647190093994\n",
            "classify 2.5113065242767334\n",
            "classify 2.501774311065674\n",
            "classify 2.550529956817627\n",
            "0.21875\n",
            "0.109375\n",
            "0.109375\n",
            "0.171875\n",
            "0.140625\n",
            "0.125\n",
            "0.15625\n",
            "0.203125\n",
            "0.15625\n",
            "0.1875\n",
            "0.171875\n",
            "time: 16.537071228027344 16.841515448426954\n",
            "193\n",
            "strain 0.02565923146903515\n",
            "strain 0.02825530432164669\n",
            "strain 0.022266924381256104\n",
            "strain 0.024436920881271362\n",
            "strain 0.024149110540747643\n",
            "strain 0.025525502860546112\n",
            "classify 2.55472993850708\n",
            "classify 2.5689682960510254\n",
            "classify 2.5701706409454346\n",
            "classify 2.650416135787964\n",
            "classify 2.5499558448791504\n",
            "classify 2.563570976257324\n",
            "classify 2.623922109603882\n",
            "classify 2.557590961456299\n",
            "classify 2.495347738265991\n",
            "classify 2.5352344512939453\n",
            "classify 2.4839882850646973\n",
            "0.1875\n",
            "0.171875\n",
            "0.265625\n",
            "0.15625\n",
            "0.21875\n",
            "0.171875\n",
            "0.203125\n",
            "0.15625\n",
            "0.3125\n",
            "0.140625\n",
            "0.1875\n",
            "time: 16.47934579849243 16.83965215240557\n",
            "194\n",
            "strain 0.02438465692102909\n",
            "strain 0.022366901859641075\n",
            "strain 0.02617906592786312\n",
            "strain 0.025017045438289642\n",
            "strain 0.024414891377091408\n",
            "strain 0.02572859264910221\n",
            "classify 2.578465223312378\n",
            "classify 2.516117811203003\n",
            "classify 2.590529680252075\n",
            "classify 2.5649266242980957\n",
            "classify 2.5279486179351807\n",
            "classify 2.520770788192749\n",
            "classify 2.548840284347534\n",
            "classify 2.6146483421325684\n",
            "classify 2.524580478668213\n",
            "classify 2.561441421508789\n",
            "classify 2.5204036235809326\n",
            "0.078125\n",
            "0.171875\n",
            "0.265625\n",
            "0.203125\n",
            "0.25\n",
            "0.25\n",
            "0.171875\n",
            "0.1875\n",
            "0.21875\n",
            "0.1875\n",
            "0.21875\n",
            "time: 17.114354610443115 16.84106353246249\n",
            "195\n",
            "strain 0.027497833594679832\n",
            "strain 0.024328505620360374\n",
            "strain 0.025155439972877502\n",
            "strain 0.025121571496129036\n",
            "strain 0.026729246601462364\n",
            "strain 0.0238263551145792\n",
            "classify 2.583343267440796\n",
            "classify 2.5474910736083984\n",
            "classify 2.55020809173584\n",
            "classify 2.5458433628082275\n",
            "classify 2.5800042152404785\n",
            "classify 2.515070676803589\n",
            "classify 2.5471765995025635\n",
            "classify 2.584787368774414\n",
            "classify 2.5439863204956055\n",
            "classify 2.500732660293579\n",
            "classify 2.5411245822906494\n",
            "0.203125\n",
            "0.265625\n",
            "0.125\n",
            "0.1875\n",
            "0.171875\n",
            "0.078125\n",
            "0.140625\n",
            "0.140625\n",
            "0.15625\n",
            "0.21875\n",
            "0.25\n",
            "time: 17.059462308883667 16.842181071943166\n",
            "196\n",
            "strain 0.026117905974388123\n",
            "strain 0.025476768612861633\n",
            "strain 0.026581695303320885\n",
            "strain 0.025963088497519493\n",
            "strain 0.023843584582209587\n",
            "strain 0.027135437354445457\n",
            "classify 2.5329742431640625\n",
            "classify 2.5202128887176514\n",
            "classify 2.5723581314086914\n",
            "classify 2.58892822265625\n",
            "classify 2.4959754943847656\n",
            "classify 2.50114369392395\n",
            "classify 2.484147548675537\n",
            "classify 2.5387680530548096\n",
            "classify 2.5331459045410156\n",
            "classify 2.4989705085754395\n",
            "classify 2.5316338539123535\n",
            "0.171875\n",
            "0.1875\n",
            "0.171875\n",
            "0.234375\n",
            "0.171875\n",
            "0.203125\n",
            "0.234375\n",
            "0.234375\n",
            "0.140625\n",
            "0.1875\n",
            "0.140625\n",
            "time: 16.54392385482788 16.840670123318127\n",
            "197\n",
            "strain 0.03035074658691883\n",
            "strain 0.024539783596992493\n",
            "strain 0.026608869433403015\n",
            "strain 0.02533525787293911\n",
            "strain 0.022776776924729347\n",
            "strain 0.024494102224707603\n",
            "classify 2.4865562915802\n",
            "classify 2.6303324699401855\n",
            "classify 2.6053013801574707\n",
            "classify 2.5745081901550293\n",
            "classify 2.587812662124634\n",
            "classify 2.5891671180725098\n",
            "classify 2.5721664428710938\n",
            "classify 2.5787081718444824\n",
            "classify 2.5458714962005615\n",
            "classify 2.5324337482452393\n",
            "classify 2.570053815841675\n",
            "0.15625\n",
            "0.21875\n",
            "0.15625\n",
            "0.234375\n",
            "0.25\n",
            "0.203125\n",
            "0.203125\n",
            "0.171875\n",
            "0.15625\n",
            "0.15625\n",
            "0.234375\n",
            "time: 16.43016219139099 16.838599561440823\n",
            "198\n",
            "strain 0.030051447451114655\n",
            "strain 0.02584145963191986\n",
            "strain 0.025309739634394646\n",
            "strain 0.028811508789658546\n",
            "strain 0.026040298864245415\n",
            "strain 0.025021107867360115\n",
            "classify 2.5200624465942383\n",
            "classify 2.559736490249634\n",
            "classify 2.5139596462249756\n",
            "classify 2.5408008098602295\n",
            "classify 2.5364203453063965\n",
            "classify 2.5624008178710938\n",
            "classify 2.5101661682128906\n",
            "classify 2.531344413757324\n",
            "classify 2.5322484970092773\n",
            "classify 2.508098602294922\n",
            "classify 2.521425724029541\n",
            "0.1875\n",
            "0.265625\n",
            "0.265625\n",
            "0.25\n",
            "0.203125\n",
            "0.171875\n",
            "0.21875\n",
            "0.34375\n",
            "0.21875\n",
            "0.203125\n",
            "0.234375\n",
            "time: 17.440552234649658 16.841627268335927\n",
            "199\n",
            "strain 0.0264756977558136\n",
            "strain 0.024683095514774323\n",
            "strain 0.023633001372218132\n",
            "strain 0.027157075703144073\n",
            "strain 0.02668708562850952\n",
            "strain 0.02778557501733303\n",
            "classify 2.541750192642212\n",
            "classify 2.5708117485046387\n",
            "classify 2.508572816848755\n",
            "classify 2.5775177478790283\n",
            "classify 2.5317721366882324\n",
            "classify 2.5496318340301514\n",
            "classify 2.5387532711029053\n",
            "classify 2.5118613243103027\n",
            "classify 2.495959758758545\n",
            "classify 2.513978958129883\n",
            "classify 2.517188787460327\n",
            "0.1875\n",
            "0.109375\n",
            "0.3125\n",
            "0.171875\n",
            "0.15625\n",
            "0.171875\n",
            "0.125\n",
            "0.234375\n",
            "0.203125\n",
            "0.109375\n",
            "0.1875\n",
            "time: 16.77881360054016 16.841318802833555\n",
            "200\n",
            "strain 0.023844337090849876\n",
            "strain 0.023738065734505653\n",
            "strain 0.028134355321526527\n",
            "strain 0.02425546385347843\n",
            "strain 0.02519298903644085\n",
            "strain 0.023800591006875038\n",
            "classify 2.47011661529541\n",
            "classify 2.55387806892395\n",
            "classify 2.5602753162384033\n",
            "classify 2.525902271270752\n",
            "classify 2.542539119720459\n",
            "classify 2.5700843334198\n",
            "classify 2.4786505699157715\n",
            "classify 2.500260829925537\n",
            "classify 2.5137710571289062\n",
            "classify 2.5454957485198975\n",
            "classify 2.5166237354278564\n",
            "0.296875\n",
            "0.3125\n",
            "0.234375\n",
            "0.28125\n",
            "0.21875\n",
            "0.203125\n",
            "0.140625\n",
            "0.1875\n",
            "0.140625\n",
            "0.171875\n",
            "0.28125\n",
            "time: 16.541931867599487 16.83983129173962\n",
            "201\n",
            "strain 0.030759377405047417\n",
            "strain 0.02596585638821125\n",
            "strain 0.023860974237322807\n",
            "strain 0.025476908311247826\n",
            "strain 0.025679543614387512\n",
            "strain 0.024387583136558533\n",
            "classify 2.463613271713257\n",
            "classify 2.6131768226623535\n",
            "classify 2.5373191833496094\n",
            "classify 2.547473669052124\n",
            "classify 2.5818963050842285\n",
            "classify 2.5424110889434814\n",
            "classify 2.5364136695861816\n",
            "classify 2.554344654083252\n",
            "classify 2.598054885864258\n",
            "classify 2.5836684703826904\n",
            "classify 2.4760446548461914\n",
            "0.234375\n",
            "0.21875\n",
            "0.21875\n",
            "0.25\n",
            "0.3125\n",
            "0.21875\n",
            "0.234375\n",
            "0.171875\n",
            "0.171875\n",
            "0.1875\n",
            "0.234375\n",
            "time: 16.474651098251343 16.8380263099576\n",
            "202\n",
            "strain 0.02428501285612583\n",
            "strain 0.023221254348754883\n",
            "strain 0.023466333746910095\n",
            "strain 0.027702637016773224\n",
            "strain 0.025043614208698273\n",
            "strain 0.025529935956001282\n",
            "classify 2.570984125137329\n",
            "classify 2.5523204803466797\n",
            "classify 2.5145668983459473\n",
            "classify 2.5719292163848877\n",
            "classify 2.558985471725464\n",
            "classify 2.539132833480835\n",
            "classify 2.569020986557007\n",
            "classify 2.584056854248047\n",
            "classify 2.6244335174560547\n",
            "classify 2.6263787746429443\n",
            "classify 2.6077685356140137\n",
            "0.203125\n",
            "0.1875\n",
            "0.265625\n",
            "0.1875\n",
            "0.265625\n",
            "0.140625\n",
            "0.265625\n",
            "0.234375\n",
            "0.109375\n",
            "0.15625\n",
            "0.1875\n",
            "time: 17.75067138671875 16.842524933697554\n",
            "203\n",
            "strain 0.028510794043540955\n",
            "strain 0.024864578619599342\n",
            "strain 0.025505810976028442\n",
            "strain 0.028472403064370155\n",
            "strain 0.026840396225452423\n",
            "strain 0.029670322313904762\n",
            "classify 2.461951494216919\n",
            "classify 2.5475473403930664\n",
            "classify 2.536165237426758\n",
            "classify 2.551847219467163\n",
            "classify 2.6170153617858887\n",
            "classify 2.5410032272338867\n",
            "classify 2.6018741130828857\n",
            "classify 2.468477249145508\n",
            "classify 2.5647144317626953\n",
            "classify 2.4811036586761475\n",
            "classify 2.495272636413574\n",
            "0.28125\n",
            "0.15625\n",
            "0.25\n",
            "0.171875\n",
            "0.140625\n",
            "0.203125\n",
            "0.171875\n",
            "0.21875\n",
            "0.15625\n",
            "0.140625\n",
            "0.234375\n",
            "time: 16.436583280563354 16.840537865956623\n",
            "204\n",
            "strain 0.02538633905351162\n",
            "strain 0.02620609849691391\n",
            "strain 0.026164792478084564\n",
            "strain 0.027245327830314636\n",
            "strain 0.024402551352977753\n",
            "strain 0.02977929264307022\n",
            "classify 2.546114921569824\n",
            "classify 2.5475292205810547\n",
            "classify 2.5418035984039307\n",
            "classify 2.5431015491485596\n",
            "classify 2.5338943004608154\n",
            "classify 2.5648157596588135\n",
            "classify 2.4995827674865723\n",
            "classify 2.5531907081604004\n",
            "classify 2.4907147884368896\n",
            "classify 2.5784380435943604\n",
            "classify 2.4955966472625732\n",
            "0.234375\n",
            "0.1875\n",
            "0.25\n",
            "0.21875\n",
            "0.21875\n",
            "0.125\n",
            "0.203125\n",
            "0.140625\n",
            "0.140625\n",
            "0.1875\n",
            "0.21875\n",
            "time: 16.4605929851532 16.838687212874248\n",
            "205\n",
            "strain 0.02521485649049282\n",
            "strain 0.026830995455384254\n",
            "strain 0.026750460267066956\n",
            "strain 0.02523798495531082\n",
            "strain 0.02604774571955204\n",
            "strain 0.02551441825926304\n",
            "classify 2.497278928756714\n",
            "classify 2.5754449367523193\n",
            "classify 2.506988525390625\n",
            "classify 2.485381841659546\n",
            "classify 2.49786376953125\n",
            "classify 2.4563956260681152\n",
            "classify 2.476252794265747\n",
            "classify 2.4937851428985596\n",
            "classify 2.608480215072632\n",
            "classify 2.4856979846954346\n",
            "classify 2.479858875274658\n",
            "0.15625\n",
            "0.171875\n",
            "0.171875\n",
            "0.109375\n",
            "0.234375\n",
            "0.28125\n",
            "0.234375\n",
            "0.15625\n",
            "0.171875\n",
            "0.109375\n",
            "0.203125\n",
            "time: 16.5953950881958 16.837508849727296\n",
            "206\n",
            "strain 0.029084928333759308\n",
            "strain 0.025437450036406517\n",
            "strain 0.02964913100004196\n",
            "strain 0.030129356309771538\n",
            "strain 0.025113224983215332\n",
            "strain 0.021472597494721413\n",
            "classify 2.568962574005127\n",
            "classify 2.473562479019165\n",
            "classify 2.5686604976654053\n",
            "classify 2.5252761840820312\n",
            "classify 2.482912540435791\n",
            "classify 2.5146639347076416\n",
            "classify 2.490320920944214\n",
            "classify 2.5359504222869873\n",
            "classify 2.5974771976470947\n",
            "classify 2.467729091644287\n",
            "classify 2.533738374710083\n",
            "0.109375\n",
            "0.140625\n",
            "0.125\n",
            "0.21875\n",
            "0.203125\n",
            "0.25\n",
            "0.109375\n",
            "0.21875\n",
            "0.234375\n",
            "0.203125\n",
            "0.21875\n",
            "time: 17.731505155563354 16.84183014418192\n",
            "207\n",
            "strain 0.026368459686636925\n",
            "strain 0.02333103120326996\n",
            "strain 0.02876293659210205\n",
            "strain 0.02500484138727188\n",
            "strain 0.027803510427474976\n",
            "strain 0.028678998351097107\n",
            "classify 2.5729963779449463\n",
            "classify 2.6117043495178223\n",
            "classify 2.505558967590332\n",
            "classify 2.512767791748047\n",
            "classify 2.507298469543457\n",
            "classify 2.530465841293335\n",
            "classify 2.4921300411224365\n",
            "classify 2.445765256881714\n",
            "classify 2.475907564163208\n",
            "classify 2.565274715423584\n",
            "classify 2.557666778564453\n",
            "0.1875\n",
            "0.109375\n",
            "0.109375\n",
            "0.1875\n",
            "0.25\n",
            "0.140625\n",
            "0.171875\n",
            "0.109375\n",
            "0.203125\n",
            "0.21875\n",
            "0.109375\n",
            "time: 16.53936219215393 16.84037857560011\n",
            "208\n",
            "strain 0.028229733929038048\n",
            "strain 0.028671404346823692\n",
            "strain 0.02626478672027588\n",
            "strain 0.027268344536423683\n",
            "strain 0.024128630757331848\n",
            "strain 0.026698678731918335\n",
            "classify 2.508165121078491\n",
            "classify 2.5286052227020264\n",
            "classify 2.5090491771698\n",
            "classify 2.5624539852142334\n",
            "classify 2.584563970565796\n",
            "classify 2.502408266067505\n",
            "classify 2.581278085708618\n",
            "classify 2.5556607246398926\n",
            "classify 2.594067096710205\n",
            "classify 2.4786393642425537\n",
            "classify 2.6198534965515137\n",
            "0.203125\n",
            "0.1875\n",
            "0.21875\n",
            "0.21875\n",
            "0.28125\n",
            "0.125\n",
            "0.171875\n",
            "0.140625\n",
            "0.265625\n",
            "0.265625\n",
            "0.1875\n",
            "time: 16.445155382156372 16.838490223770506\n",
            "209\n",
            "strain 0.026029830798506737\n",
            "strain 0.024707207456231117\n",
            "strain 0.025114139541983604\n",
            "strain 0.022753044962882996\n",
            "strain 0.024908145889639854\n",
            "strain 0.02354143001139164\n",
            "classify 2.5044522285461426\n",
            "classify 2.4991817474365234\n",
            "classify 2.543637275695801\n",
            "classify 2.5871729850769043\n",
            "classify 2.4927873611450195\n",
            "classify 2.5100302696228027\n",
            "classify 2.489022970199585\n",
            "classify 2.5494613647460938\n",
            "classify 2.5002827644348145\n",
            "classify 2.539433717727661\n",
            "classify 2.5320420265197754\n",
            "0.171875\n",
            "0.265625\n",
            "0.09375\n",
            "0.28125\n",
            "0.15625\n",
            "0.21875\n",
            "0.09375\n",
            "0.140625\n",
            "0.171875\n",
            "0.234375\n",
            "0.140625\n",
            "time: 16.733827352523804 16.837995338439942\n",
            "210\n",
            "strain 0.025812575593590736\n",
            "strain 0.026313265785574913\n",
            "strain 0.02647493965923786\n",
            "strain 0.028041623532772064\n",
            "strain 0.025077344849705696\n",
            "strain 0.025232434272766113\n",
            "classify 2.4522743225097656\n",
            "classify 2.500110626220703\n",
            "classify 2.5268309116363525\n",
            "classify 2.4755911827087402\n",
            "classify 2.526597261428833\n",
            "classify 2.483093738555908\n",
            "classify 2.593953847885132\n",
            "classify 2.5642588138580322\n",
            "classify 2.473937749862671\n",
            "classify 2.5012896060943604\n",
            "classify 2.55291485786438\n",
            "0.203125\n",
            "0.1875\n",
            "0.1875\n",
            "0.15625\n",
            "0.28125\n",
            "0.09375\n",
            "0.21875\n",
            "0.1875\n",
            "0.25\n",
            "0.21875\n",
            "0.21875\n",
            "time: 17.49425172805786 16.841111276952013\n",
            "211\n",
            "strain 0.03026549518108368\n",
            "strain 0.028280319646000862\n",
            "strain 0.026946259662508965\n",
            "strain 0.026287322863936424\n",
            "strain 0.022093849256634712\n",
            "strain 0.027736350893974304\n",
            "classify 2.5078349113464355\n",
            "classify 2.484306573867798\n",
            "classify 2.5744247436523438\n",
            "classify 2.549828052520752\n",
            "classify 2.5653114318847656\n",
            "classify 2.5844359397888184\n",
            "classify 2.5307343006134033\n",
            "classify 2.5846199989318848\n",
            "classify 2.5720715522766113\n",
            "classify 2.602442741394043\n",
            "classify 2.579413652420044\n",
            "0.3125\n",
            "0.3125\n",
            "0.234375\n",
            "0.21875\n",
            "0.25\n",
            "0.1875\n",
            "0.328125\n",
            "0.171875\n",
            "0.21875\n",
            "0.21875\n",
            "0.25\n",
            "time: 16.495306730270386 16.839482226461733\n",
            "212\n",
            "strain 0.026471788063645363\n",
            "strain 0.028426380828022957\n",
            "strain 0.024320922791957855\n",
            "strain 0.03144000843167305\n",
            "strain 0.021829798817634583\n",
            "strain 0.02893473021686077\n",
            "classify 2.591076612472534\n",
            "classify 2.4842658042907715\n",
            "classify 2.4941964149475098\n",
            "classify 2.4891560077667236\n",
            "classify 2.513707399368286\n",
            "classify 2.4805212020874023\n",
            "classify 2.516869306564331\n",
            "classify 2.5639734268188477\n",
            "classify 2.497250556945801\n",
            "classify 2.5413007736206055\n",
            "classify 2.568449020385742\n",
            "0.125\n",
            "0.1875\n",
            "0.203125\n",
            "0.203125\n",
            "0.25\n",
            "0.171875\n",
            "0.28125\n",
            "0.25\n",
            "0.234375\n",
            "0.1875\n",
            "0.1875\n",
            "time: 16.5383563041687 16.838072268615868\n",
            "213\n",
            "strain 0.02550557255744934\n",
            "strain 0.0281917005777359\n",
            "strain 0.027495862916111946\n",
            "strain 0.02860535867512226\n",
            "strain 0.02472270093858242\n",
            "strain 0.025997450575232506\n",
            "classify 2.472066640853882\n",
            "classify 2.4865124225616455\n",
            "classify 2.485630512237549\n",
            "classify 2.516051769256592\n",
            "classify 2.6042912006378174\n",
            "classify 2.49629282951355\n",
            "classify 2.48526930809021\n",
            "classify 2.5168087482452393\n",
            "classify 2.4468259811401367\n",
            "classify 2.562234401702881\n",
            "classify 2.4670345783233643\n",
            "0.21875\n",
            "0.15625\n",
            "0.21875\n",
            "0.203125\n",
            "0.125\n",
            "0.203125\n",
            "0.234375\n",
            "0.296875\n",
            "0.1875\n",
            "0.125\n",
            "0.3125\n",
            "time: 17.04233431816101 16.839035419660195\n",
            "214\n",
            "strain 0.027096228674054146\n",
            "strain 0.0238688662648201\n",
            "strain 0.025395574048161507\n",
            "strain 0.02691981941461563\n",
            "strain 0.027767419815063477\n",
            "strain 0.028586125001311302\n",
            "classify 2.5080153942108154\n",
            "classify 2.502563238143921\n",
            "classify 2.5197701454162598\n",
            "classify 2.519744396209717\n",
            "classify 2.5480005741119385\n",
            "classify 2.5190112590789795\n",
            "classify 2.506098985671997\n",
            "classify 2.476296901702881\n",
            "classify 2.5141947269439697\n",
            "classify 2.5155553817749023\n",
            "classify 2.522935628890991\n",
            "0.234375\n",
            "0.140625\n",
            "0.1875\n",
            "0.1875\n",
            "0.28125\n",
            "0.234375\n",
            "0.234375\n",
            "0.21875\n",
            "0.203125\n",
            "0.203125\n",
            "0.21875\n",
            "time: 17.102206468582153 16.840265979323277\n",
            "215\n",
            "strain 0.024458037689328194\n",
            "strain 0.02869628556072712\n",
            "strain 0.0248870849609375\n",
            "strain 0.022289970889687538\n",
            "strain 0.025856995955109596\n",
            "strain 0.023763418197631836\n",
            "classify 2.5233030319213867\n",
            "classify 2.4722399711608887\n",
            "classify 2.540712833404541\n",
            "classify 2.4848086833953857\n",
            "classify 2.5493521690368652\n",
            "classify 2.4975931644439697\n",
            "classify 2.582738161087036\n",
            "classify 2.5332632064819336\n",
            "classify 2.512364387512207\n",
            "classify 2.547112464904785\n",
            "classify 2.510566473007202\n",
            "0.140625\n",
            "0.25\n",
            "0.25\n",
            "0.296875\n",
            "0.125\n",
            "0.1875\n",
            "0.203125\n",
            "0.1875\n",
            "0.21875\n",
            "0.28125\n",
            "0.203125\n",
            "time: 16.47871494293213 16.83859489582203\n",
            "216\n",
            "strain 0.02717846818268299\n",
            "strain 0.02503337524831295\n",
            "strain 0.023471945896744728\n",
            "strain 0.023326365277171135\n",
            "strain 0.026744797825813293\n",
            "strain 0.024624407291412354\n",
            "classify 2.551694393157959\n",
            "classify 2.6103129386901855\n",
            "classify 2.5980424880981445\n",
            "classify 2.4833972454071045\n",
            "classify 2.513394355773926\n",
            "classify 2.5294313430786133\n",
            "classify 2.4827959537506104\n",
            "classify 2.461949586868286\n",
            "classify 2.5096027851104736\n",
            "classify 2.426147222518921\n",
            "classify 2.575021266937256\n",
            "0.140625\n",
            "0.15625\n",
            "0.21875\n",
            "0.1875\n",
            "0.234375\n",
            "0.125\n",
            "0.25\n",
            "0.15625\n",
            "0.171875\n",
            "0.171875\n",
            "0.171875\n",
            "time: 16.499691486358643 16.837035868025044\n",
            "217\n",
            "strain 0.028322501108050346\n",
            "strain 0.027860127389431\n",
            "strain 0.027924008667469025\n",
            "strain 0.023968935012817383\n",
            "strain 0.02659269981086254\n",
            "strain 0.025307172909379005\n",
            "classify 2.551522970199585\n",
            "classify 2.4484477043151855\n",
            "classify 2.535637140274048\n",
            "classify 2.5495357513427734\n",
            "classify 2.4464974403381348\n",
            "classify 2.536505699157715\n",
            "classify 2.462721347808838\n",
            "classify 2.522446393966675\n",
            "classify 2.4888575077056885\n",
            "classify 2.53974986076355\n",
            "classify 2.515610456466675\n",
            "0.25\n",
            "0.140625\n",
            "0.1875\n",
            "0.1875\n",
            "0.171875\n",
            "0.203125\n",
            "0.21875\n",
            "0.125\n",
            "0.125\n",
            "0.25\n",
            "0.203125\n",
            "time: 17.47422218322754 16.839961425973733\n",
            "218\n",
            "strain 0.026753311976790428\n",
            "strain 0.0244307741522789\n",
            "strain 0.026243505999445915\n",
            "strain 0.027193685993552208\n",
            "strain 0.02472146786749363\n",
            "strain 0.025084665045142174\n",
            "classify 2.558161735534668\n",
            "classify 2.500663995742798\n",
            "classify 2.5341289043426514\n",
            "classify 2.5465478897094727\n",
            "classify 2.482581377029419\n",
            "classify 2.5340511798858643\n",
            "classify 2.431727647781372\n",
            "classify 2.5213122367858887\n",
            "classify 2.6057140827178955\n",
            "classify 2.548959255218506\n",
            "classify 2.519822835922241\n",
            "0.171875\n",
            "0.171875\n",
            "0.15625\n",
            "0.171875\n",
            "0.265625\n",
            "0.234375\n",
            "0.203125\n",
            "0.171875\n",
            "0.203125\n",
            "0.203125\n",
            "0.234375\n",
            "time: 16.74776530265808 16.83955911523131\n",
            "219\n",
            "strain 0.02434605360031128\n",
            "strain 0.027196364477276802\n",
            "strain 0.026197580620646477\n",
            "strain 0.02694597654044628\n",
            "strain 0.027716025710105896\n",
            "strain 0.023173971101641655\n",
            "classify 2.451824903488159\n",
            "classify 2.5089633464813232\n",
            "classify 2.4706153869628906\n",
            "classify 2.512855052947998\n",
            "classify 2.490856170654297\n",
            "classify 2.483677387237549\n",
            "classify 2.4591479301452637\n",
            "classify 2.4887001514434814\n",
            "classify 2.559178590774536\n",
            "classify 2.582777738571167\n",
            "classify 2.5498082637786865\n",
            "0.1875\n",
            "0.1875\n",
            "0.1875\n",
            "0.203125\n",
            "0.25\n",
            "0.1875\n",
            "0.234375\n",
            "0.234375\n",
            "0.21875\n",
            "0.203125\n",
            "0.171875\n",
            "time: 16.479569673538208 16.837926205721768\n",
            "220\n",
            "strain 0.02153082750737667\n",
            "strain 0.025680074468255043\n",
            "strain 0.02212551049888134\n",
            "strain 0.023915544152259827\n",
            "strain 0.02695131115615368\n",
            "strain 0.02556532621383667\n",
            "classify 2.463479995727539\n",
            "classify 2.5632612705230713\n",
            "classify 2.49661922454834\n",
            "classify 2.5372276306152344\n",
            "classify 2.5912210941314697\n",
            "classify 2.514706611633301\n",
            "classify 2.5801472663879395\n",
            "classify 2.5076706409454346\n",
            "classify 2.480935573577881\n",
            "classify 2.5457451343536377\n",
            "classify 2.513279676437378\n",
            "0.21875\n",
            "0.25\n",
            "0.171875\n",
            "0.15625\n",
            "0.171875\n",
            "0.21875\n",
            "0.1875\n",
            "0.203125\n",
            "0.15625\n",
            "0.203125\n",
            "0.25\n",
            "time: 16.519147396087646 16.836486483051765\n",
            "221\n",
            "strain 0.025855055078864098\n",
            "strain 0.020344708114862442\n",
            "strain 0.024577340111136436\n",
            "strain 0.025053223595023155\n",
            "strain 0.025878330692648888\n",
            "strain 0.02651033364236355\n",
            "classify 2.5416738986968994\n",
            "classify 2.5562026500701904\n",
            "classify 2.527714490890503\n",
            "classify 2.525951862335205\n",
            "classify 2.5045957565307617\n",
            "classify 2.561184883117676\n",
            "classify 2.4939308166503906\n",
            "classify 2.5015461444854736\n",
            "classify 2.5524983406066895\n",
            "classify 2.5730087757110596\n",
            "classify 2.4252448081970215\n",
            "0.140625\n",
            "0.171875\n",
            "0.15625\n",
            "0.15625\n",
            "0.171875\n",
            "0.1875\n",
            "0.25\n",
            "0.203125\n",
            "0.15625\n",
            "0.171875\n",
            "0.21875\n",
            "time: 17.767507076263428 16.84068329914196\n",
            "222\n",
            "strain 0.022683078423142433\n",
            "strain 0.02906768210232258\n",
            "strain 0.028379766270518303\n",
            "strain 0.024526230990886688\n",
            "strain 0.02421017549932003\n",
            "strain 0.027883470058441162\n",
            "classify 2.5502562522888184\n",
            "classify 2.565107583999634\n",
            "classify 2.5011279582977295\n",
            "classify 2.4895222187042236\n",
            "classify 2.5003933906555176\n",
            "classify 2.498790740966797\n",
            "classify 2.4747629165649414\n",
            "classify 2.543802261352539\n",
            "classify 2.553163528442383\n",
            "classify 2.5585618019104004\n",
            "classify 2.4623327255249023\n",
            "0.21875\n",
            "0.203125\n",
            "0.140625\n",
            "0.15625\n",
            "0.203125\n",
            "0.21875\n",
            "0.140625\n",
            "0.234375\n",
            "0.15625\n",
            "0.21875\n",
            "0.140625\n",
            "time: 16.445061206817627 16.838911641339017\n",
            "223\n",
            "strain 0.024780169129371643\n",
            "strain 0.02719786949455738\n",
            "strain 0.028531061485409737\n",
            "strain 0.02629527449607849\n",
            "strain 0.021261557936668396\n",
            "strain 0.02798663079738617\n",
            "classify 2.5167477130889893\n",
            "classify 2.4673001766204834\n",
            "classify 2.4413163661956787\n",
            "classify 2.5969629287719727\n",
            "classify 2.5349276065826416\n",
            "classify 2.5057201385498047\n",
            "classify 2.5099546909332275\n",
            "classify 2.526865005493164\n",
            "classify 2.425098419189453\n",
            "classify 2.466521739959717\n",
            "classify 2.493820905685425\n",
            "0.109375\n",
            "0.1875\n",
            "0.21875\n",
            "0.25\n",
            "0.21875\n",
            "0.203125\n",
            "0.140625\n",
            "0.25\n",
            "0.1875\n",
            "0.1875\n",
            "0.234375\n",
            "time: 16.58146333694458 16.837764897516795\n",
            "224\n",
            "strain 0.025875575840473175\n",
            "strain 0.023590296506881714\n",
            "strain 0.025656046345829964\n",
            "strain 0.026154736056923866\n",
            "strain 0.02238026261329651\n",
            "strain 0.027247874066233635\n",
            "classify 2.5609476566314697\n",
            "classify 2.4997634887695312\n",
            "classify 2.4793193340301514\n",
            "classify 2.580868721008301\n",
            "classify 2.4744083881378174\n",
            "classify 2.4699432849884033\n",
            "classify 2.511287212371826\n",
            "classify 2.510861873626709\n",
            "classify 2.5287368297576904\n",
            "classify 2.4217448234558105\n",
            "classify 2.4640002250671387\n",
            "0.25\n",
            "0.1875\n",
            "0.140625\n",
            "0.1875\n",
            "0.1875\n",
            "0.125\n",
            "0.0625\n",
            "0.171875\n",
            "0.171875\n",
            "0.203125\n",
            "0.21875\n",
            "time: 16.41546893119812 16.835890675650703\n",
            "225\n",
            "strain 0.02280321717262268\n",
            "strain 0.023478709161281586\n",
            "strain 0.026155829429626465\n",
            "strain 0.022823430597782135\n",
            "strain 0.027273990213871002\n",
            "strain 0.02783050946891308\n",
            "classify 2.496793746948242\n",
            "classify 2.501328229904175\n",
            "classify 2.494943857192993\n",
            "classify 2.434018850326538\n",
            "classify 2.5450971126556396\n",
            "classify 2.399765729904175\n",
            "classify 2.527101993560791\n",
            "classify 2.534459352493286\n",
            "classify 2.527153968811035\n",
            "classify 2.551961898803711\n",
            "classify 2.49552845954895\n",
            "0.234375\n",
            "0.171875\n",
            "0.15625\n",
            "0.1875\n",
            "0.1875\n",
            "0.1875\n",
            "0.234375\n",
            "0.234375\n",
            "0.1875\n",
            "0.125\n",
            "0.125\n",
            "time: 17.71766948699951 16.83979501555451\n",
            "226\n",
            "strain 0.025713181123137474\n",
            "strain 0.026245737448334694\n",
            "strain 0.022784745320677757\n",
            "strain 0.023404128849506378\n",
            "strain 0.025463314726948738\n",
            "strain 0.027742907404899597\n",
            "classify 2.5585803985595703\n",
            "classify 2.5227439403533936\n",
            "classify 2.504929542541504\n",
            "classify 2.5196127891540527\n",
            "classify 2.4732043743133545\n",
            "classify 2.4786148071289062\n",
            "classify 2.448561906814575\n",
            "classify 2.5692291259765625\n",
            "classify 2.55961537361145\n",
            "classify 2.450990915298462\n",
            "classify 2.5526626110076904\n",
            "0.203125\n",
            "0.1875\n",
            "0.140625\n",
            "0.234375\n",
            "0.25\n",
            "0.15625\n",
            "0.15625\n",
            "0.171875\n",
            "0.234375\n",
            "0.15625\n",
            "0.09375\n",
            "time: 16.518892765045166 16.838383958203153\n",
            "227\n",
            "strain 0.02762778103351593\n",
            "strain 0.024351703003048897\n",
            "strain 0.024769065901637077\n",
            "strain 0.02785986103117466\n",
            "strain 0.02282143384218216\n",
            "strain 0.025704629719257355\n",
            "classify 2.4919793605804443\n",
            "classify 2.4747164249420166\n",
            "classify 2.5104877948760986\n",
            "classify 2.570077419281006\n",
            "classify 2.5142862796783447\n",
            "classify 2.5609207153320312\n",
            "classify 2.4300572872161865\n",
            "classify 2.5015740394592285\n",
            "classify 2.5177001953125\n",
            "classify 2.493762493133545\n",
            "classify 2.4820163249969482\n",
            "0.28125\n",
            "0.25\n",
            "0.328125\n",
            "0.140625\n",
            "0.25\n",
            "0.21875\n",
            "0.1875\n",
            "0.1875\n",
            "0.125\n",
            "0.203125\n",
            "0.1875\n",
            "time: 16.453877687454224 16.83670027214184\n",
            "228\n",
            "strain 0.023985883221030235\n",
            "strain 0.024935288354754448\n",
            "strain 0.026017645373940468\n",
            "strain 0.025322124361991882\n",
            "strain 0.026392867788672447\n",
            "strain 0.027591494843363762\n",
            "classify 2.5536863803863525\n",
            "classify 2.4588611125946045\n",
            "classify 2.4473626613616943\n",
            "classify 2.5372488498687744\n",
            "classify 2.4651761054992676\n",
            "classify 2.4710350036621094\n",
            "classify 2.4612739086151123\n",
            "classify 2.5816116333007812\n",
            "classify 2.5212740898132324\n",
            "classify 2.4884893894195557\n",
            "classify 2.522636890411377\n",
            "0.203125\n",
            "0.296875\n",
            "0.25\n",
            "0.21875\n",
            "0.234375\n",
            "0.21875\n",
            "0.171875\n",
            "0.1875\n",
            "0.234375\n",
            "0.1875\n",
            "0.1875\n",
            "time: 16.893059968948364 16.83696611895832\n",
            "229\n",
            "strain 0.027214020490646362\n",
            "strain 0.027968930080533028\n",
            "strain 0.023414133116602898\n",
            "strain 0.024589059874415398\n",
            "strain 0.026804232969880104\n",
            "strain 0.028579501435160637\n",
            "classify 2.479055166244507\n",
            "classify 2.4965059757232666\n",
            "classify 2.545063018798828\n",
            "classify 2.4397692680358887\n",
            "classify 2.53670334815979\n",
            "classify 2.49139666557312\n",
            "classify 2.465552806854248\n",
            "classify 2.5413568019866943\n",
            "classify 2.492680549621582\n",
            "classify 2.4994375705718994\n",
            "classify 2.5758728981018066\n",
            "0.140625\n",
            "0.234375\n",
            "0.328125\n",
            "0.25\n",
            "0.1875\n",
            "0.25\n",
            "0.109375\n",
            "0.140625\n",
            "0.1875\n",
            "0.15625\n",
            "0.203125\n",
            "time: 17.399738788604736 16.839419202182604\n",
            "230\n",
            "strain 0.027216782793402672\n",
            "strain 0.025148818269371986\n",
            "strain 0.026575272902846336\n",
            "strain 0.027679607272148132\n",
            "strain 0.026594726368784904\n",
            "strain 0.023505812510848045\n",
            "classify 2.5140247344970703\n",
            "classify 2.456728935241699\n",
            "classify 2.4751315116882324\n",
            "classify 2.4800243377685547\n",
            "classify 2.4534807205200195\n",
            "classify 2.5166072845458984\n",
            "classify 2.5193448066711426\n",
            "classify 2.5410633087158203\n",
            "classify 2.4430344104766846\n",
            "classify 2.489410638809204\n",
            "classify 2.502073287963867\n",
            "0.234375\n",
            "0.25\n",
            "0.125\n",
            "0.171875\n",
            "0.234375\n",
            "0.234375\n",
            "0.234375\n",
            "0.171875\n",
            "0.234375\n",
            "0.234375\n",
            "0.1875\n",
            "time: 16.552101135253906 16.838178011246058\n",
            "231\n",
            "strain 0.026957949623465538\n",
            "strain 0.024751944467425346\n",
            "strain 0.024340340867638588\n",
            "strain 0.025837207213044167\n",
            "strain 0.026991749182343483\n",
            "strain 0.026623135432600975\n",
            "classify 2.5459861755371094\n",
            "classify 2.531644105911255\n",
            "classify 2.437446355819702\n",
            "classify 2.517819881439209\n",
            "classify 2.4763550758361816\n",
            "classify 2.5278773307800293\n",
            "classify 2.4747633934020996\n",
            "classify 2.480970859527588\n",
            "classify 2.4300143718719482\n",
            "classify 2.5034680366516113\n",
            "classify 2.4216041564941406\n",
            "0.203125\n",
            "0.234375\n",
            "0.203125\n",
            "0.15625\n",
            "0.203125\n",
            "0.234375\n",
            "0.171875\n",
            "0.1875\n",
            "0.1875\n",
            "0.15625\n",
            "0.171875\n",
            "time: 16.43756914138794 16.836453933140326\n",
            "232\n",
            "strain 0.024222562089562416\n",
            "strain 0.023897267878055573\n",
            "strain 0.022921020165085793\n",
            "strain 0.028093518689274788\n",
            "strain 0.02673380635678768\n",
            "strain 0.0279520396143198\n",
            "classify 2.523224353790283\n",
            "classify 2.5263969898223877\n",
            "classify 2.460886240005493\n",
            "classify 2.5159213542938232\n",
            "classify 2.431758165359497\n",
            "classify 2.456226110458374\n",
            "classify 2.5094211101531982\n",
            "classify 2.5022828578948975\n",
            "classify 2.5010480880737305\n",
            "classify 2.553884983062744\n",
            "classify 2.5088725090026855\n",
            "0.21875\n",
            "0.203125\n",
            "0.234375\n",
            "0.1875\n",
            "0.203125\n",
            "0.171875\n",
            "0.21875\n",
            "0.203125\n",
            "0.34375\n",
            "0.109375\n",
            "0.125\n",
            "time: 17.085835933685303 16.837526771643642\n",
            "233\n",
            "strain 0.02432924509048462\n",
            "strain 0.028956128284335136\n",
            "strain 0.025145089253783226\n",
            "strain 0.023565083742141724\n",
            "strain 0.022821426391601562\n",
            "strain 0.02622147649526596\n",
            "classify 2.5133488178253174\n",
            "classify 2.481172800064087\n",
            "classify 2.471576690673828\n",
            "classify 2.47196364402771\n",
            "classify 2.5262341499328613\n",
            "classify 2.5397720336914062\n",
            "classify 2.4795846939086914\n",
            "classify 2.487445116043091\n",
            "classify 2.522376775741577\n",
            "classify 2.493945837020874\n",
            "classify 2.5009303092956543\n",
            "0.171875\n",
            "0.15625\n",
            "0.21875\n",
            "0.1875\n",
            "0.203125\n",
            "0.171875\n",
            "0.140625\n",
            "0.203125\n",
            "0.296875\n",
            "0.125\n",
            "0.1875\n",
            "time: 17.069217920303345 16.838519379623936\n",
            "234\n",
            "strain 0.024299338459968567\n",
            "strain 0.0275026336312294\n",
            "strain 0.02712615393102169\n",
            "strain 0.02342662215232849\n",
            "strain 0.024760616943240166\n",
            "strain 0.024386994540691376\n",
            "classify 2.5074973106384277\n",
            "classify 2.465712308883667\n",
            "classify 2.566676139831543\n",
            "classify 2.4509687423706055\n",
            "classify 2.4422547817230225\n",
            "classify 2.481482744216919\n",
            "classify 2.433607339859009\n",
            "classify 2.522705316543579\n",
            "classify 2.481815814971924\n",
            "classify 2.4235482215881348\n",
            "classify 2.4597184658050537\n",
            "0.3125\n",
            "0.265625\n",
            "0.21875\n",
            "0.171875\n",
            "0.140625\n",
            "0.21875\n",
            "0.15625\n",
            "0.1875\n",
            "0.171875\n",
            "0.1875\n",
            "0.171875\n",
            "time: 16.485848903656006 16.837021264624088\n",
            "235\n",
            "strain 0.02755875699222088\n",
            "strain 0.02727191150188446\n",
            "strain 0.025714045390486717\n",
            "strain 0.024513743817806244\n",
            "strain 0.025634223595261574\n",
            "strain 0.027327870950102806\n",
            "classify 2.526583194732666\n",
            "classify 2.446944236755371\n",
            "classify 2.4508049488067627\n",
            "classify 2.555628776550293\n",
            "classify 2.5425007343292236\n",
            "classify 2.5148515701293945\n",
            "classify 2.5413079261779785\n",
            "classify 2.5158941745758057\n",
            "classify 2.4992308616638184\n",
            "classify 2.536221742630005\n",
            "classify 2.547213077545166\n",
            "0.234375\n",
            "0.125\n",
            "0.140625\n",
            "0.203125\n",
            "0.140625\n",
            "0.234375\n",
            "0.234375\n",
            "0.09375\n",
            "0.265625\n",
            "0.296875\n",
            "0.234375\n",
            "time: 16.546261072158813 16.835791545399164\n",
            "236\n",
            "strain 0.028202936053276062\n",
            "strain 0.026007220149040222\n",
            "strain 0.026673922315239906\n",
            "strain 0.02506905235350132\n",
            "strain 0.025105200707912445\n",
            "strain 0.02213645912706852\n",
            "classify 2.4546101093292236\n",
            "classify 2.6022260189056396\n",
            "classify 2.50712251663208\n",
            "classify 2.505729913711548\n",
            "classify 2.4838337898254395\n",
            "classify 2.470681667327881\n",
            "classify 2.4967687129974365\n",
            "classify 2.3997044563293457\n",
            "classify 2.465881586074829\n",
            "classify 2.5139083862304688\n",
            "classify 2.436067581176758\n",
            "0.171875\n",
            "0.1875\n",
            "0.1875\n",
            "0.140625\n",
            "0.1875\n",
            "0.3125\n",
            "0.1875\n",
            "0.203125\n",
            "0.265625\n",
            "0.234375\n",
            "0.1875\n",
            "time: 17.350443601608276 16.837964933129804\n",
            "237\n",
            "strain 0.022843273356556892\n",
            "strain 0.0238575991243124\n",
            "strain 0.02736213617026806\n",
            "strain 0.02736676298081875\n",
            "strain 0.024790240451693535\n",
            "strain 0.027765892446041107\n",
            "classify 2.515867233276367\n",
            "classify 2.4953672885894775\n",
            "classify 2.472935676574707\n",
            "classify 2.44810152053833\n",
            "classify 2.4264211654663086\n",
            "classify 2.531064748764038\n",
            "classify 2.4790101051330566\n",
            "classify 2.4936962127685547\n",
            "classify 2.5098726749420166\n",
            "classify 2.4703216552734375\n",
            "classify 2.443039655685425\n",
            "0.203125\n",
            "0.109375\n",
            "0.1875\n",
            "0.203125\n",
            "0.203125\n",
            "0.15625\n",
            "0.140625\n",
            "0.203125\n",
            "0.171875\n",
            "0.125\n",
            "0.171875\n",
            "time: 16.69702434539795 16.83737537339956\n",
            "238\n",
            "strain 0.02657371200621128\n",
            "strain 0.023217586800456047\n",
            "strain 0.027375711128115654\n",
            "strain 0.0267490241676569\n",
            "strain 0.028349557891488075\n",
            "strain 0.026699313893914223\n",
            "classify 2.535919427871704\n",
            "classify 2.4921185970306396\n",
            "classify 2.5515635013580322\n",
            "classify 2.5373072624206543\n",
            "classify 2.399661064147949\n",
            "classify 2.4412498474121094\n",
            "classify 2.462306022644043\n",
            "classify 2.4384827613830566\n",
            "classify 2.4574506282806396\n",
            "classify 2.501331329345703\n",
            "classify 2.418707847595215\n",
            "0.15625\n",
            "0.1875\n",
            "0.265625\n",
            "0.1875\n",
            "0.25\n",
            "0.15625\n",
            "0.234375\n",
            "0.234375\n",
            "0.265625\n",
            "0.15625\n",
            "0.15625\n",
            "time: 16.493673086166382 16.835939455231863\n",
            "239\n",
            "strain 0.028041861951351166\n",
            "strain 0.019102804362773895\n",
            "strain 0.025095926597714424\n",
            "strain 0.026985349133610725\n",
            "strain 0.027538498863577843\n",
            "strain 0.023118240758776665\n",
            "classify 2.48869252204895\n",
            "classify 2.437154531478882\n",
            "classify 2.5172481536865234\n",
            "classify 2.4729530811309814\n",
            "classify 2.425227165222168\n",
            "classify 2.474198579788208\n",
            "classify 2.5028178691864014\n",
            "classify 2.4662747383117676\n",
            "classify 2.4025731086730957\n",
            "classify 2.446279287338257\n",
            "classify 2.4855268001556396\n",
            "0.234375\n",
            "0.109375\n",
            "0.3125\n",
            "0.1875\n",
            "0.203125\n",
            "0.203125\n",
            "0.15625\n",
            "0.25\n",
            "0.3125\n",
            "0.234375\n",
            "0.25\n",
            "time: 16.49264621734619 16.834511500597\n",
            "240\n",
            "strain 0.025525368750095367\n",
            "strain 0.02622942440211773\n",
            "strain 0.024609090760350227\n",
            "strain 0.026375191286206245\n",
            "strain 0.023700088262557983\n",
            "strain 0.02721385471522808\n",
            "classify 2.466729164123535\n",
            "classify 2.491107225418091\n",
            "classify 2.501230001449585\n",
            "classify 2.5043747425079346\n",
            "classify 2.4892611503601074\n",
            "classify 2.5063154697418213\n",
            "classify 2.5230014324188232\n",
            "classify 2.4531033039093018\n",
            "classify 2.4387173652648926\n",
            "classify 2.430227279663086\n",
            "classify 2.522376298904419\n",
            "0.296875\n",
            "0.1875\n",
            "0.28125\n",
            "0.1875\n",
            "0.1875\n",
            "0.25\n",
            "0.125\n",
            "0.1875\n",
            "0.203125\n",
            "0.234375\n",
            "0.109375\n",
            "time: 17.774544954299927 16.83841579385813\n",
            "241\n",
            "strain 0.027536742389202118\n",
            "strain 0.023387597873806953\n",
            "strain 0.025530626997351646\n",
            "strain 0.027479132637381554\n",
            "strain 0.0275826845318079\n",
            "strain 0.025506136938929558\n",
            "classify 2.5477583408355713\n",
            "classify 2.488903760910034\n",
            "classify 2.546847343444824\n",
            "classify 2.5050597190856934\n",
            "classify 2.4468486309051514\n",
            "classify 2.4609217643737793\n",
            "classify 2.5602829456329346\n",
            "classify 2.5724775791168213\n",
            "classify 2.5247600078582764\n",
            "classify 2.478008985519409\n",
            "classify 2.463456392288208\n",
            "0.09375\n",
            "0.109375\n",
            "0.234375\n",
            "0.15625\n",
            "0.203125\n",
            "0.25\n",
            "0.21875\n",
            "0.21875\n",
            "0.21875\n",
            "0.234375\n",
            "0.15625\n",
            "time: 16.418678045272827 16.83668366838093\n",
            "242\n",
            "strain 0.024980461224913597\n",
            "strain 0.0223806444555521\n",
            "strain 0.024028219282627106\n",
            "strain 0.024354586377739906\n",
            "strain 0.024945082142949104\n",
            "strain 0.02501341700553894\n",
            "classify 2.5030975341796875\n",
            "classify 2.520399570465088\n",
            "classify 2.497873067855835\n",
            "classify 2.4876434803009033\n",
            "classify 2.532728672027588\n",
            "classify 2.4447760581970215\n",
            "classify 2.484799385070801\n",
            "classify 2.44517183303833\n",
            "classify 2.4540748596191406\n",
            "classify 2.4828763008117676\n",
            "classify 2.4294626712799072\n",
            "0.171875\n",
            "0.21875\n",
            "0.125\n",
            "0.203125\n",
            "0.28125\n",
            "0.25\n",
            "0.1875\n",
            "0.15625\n",
            "0.21875\n",
            "0.21875\n",
            "0.203125\n",
            "time: 16.519071102142334 16.83537873024803\n",
            "243\n",
            "strain 0.023899316787719727\n",
            "strain 0.026828916743397713\n",
            "strain 0.02742481231689453\n",
            "strain 0.029606470838189125\n",
            "strain 0.02251274883747101\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-17-3464925582.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;31m# test(ijepa, classifier, test_loader)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0mstrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0mctrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-17-3464925582.py\u001b[0m in \u001b[0;36mstrain\u001b[0;34m(model, dataloader, optim, scheduler)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# bfloat16 float16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;31m# repr_loss, std_loss, cov_loss = model.loss(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-15-2053788863.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0msx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext_indices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [batch, num_context_toks, out_dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m# print('seq_jepa loss sx',sx.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0my_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrg_indices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [batch*M, num_trg_toks, out_dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;31m# print(y.shape, y_.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-14-1469908503.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask_indices, trg_indices)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m \u001b[0;31m# [b,t,d]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# @title strain ctrain test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "def strain(model, dataloader, optim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.train()\n",
        "    for i, (x, _) in enumerate(dataloader):\n",
        "        x = x.to(device)#.to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            loss = model.loss(x)\n",
        "\n",
        "            # repr_loss, std_loss, cov_loss = model.loss(x)\n",
        "            # j_loss, (repr_loss, std_loss, cov_loss) = model.loss(x)\n",
        "            # loss = model.sim_coeff * repr_loss + model.std_coeff * std_loss + model.cov_coeff * cov_loss\n",
        "            # loss = j_loss + loss\n",
        "\n",
        "        optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 5) # 0.5\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        # with torch.no_grad():\n",
        "        #     m=0.999 # 0.99 m = next(momentum_scheduler)\n",
        "        #     norms=[]\n",
        "        #     for param_q, param_k in zip(model.student.parameters(), model.teacher.parameters()):\n",
        "        #         param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        if i%10==0: print(\"strain\",loss.item())\n",
        "        # try: wandb.log({\"loss\": loss.item(), \"repr/I\": repr_loss.item(), \"std/V\": std_loss.item(), \"cov/C\": cov_loss.item()})\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=50: break\n",
        "\n",
        "\n",
        "def ctrain(model, classifier, dataloader, coptim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.eval()\n",
        "    classifier.train()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            with torch.no_grad():\n",
        "                sx = model(x).detach()\n",
        "            y_ = classifier(sx)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "        coptim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(coptim)\n",
        "        scaler.update()\n",
        "        print(\"classify\",loss.item())\n",
        "        try: wandb.log({\"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        with torch.no_grad():\n",
        "            sx = model(x)\n",
        "            # rankme = RankMe(sx).item()\n",
        "            # lidar = LiDAR(sx).item()\n",
        "            y_ = classifier(sx)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        print(correct/len(y))\n",
        "        try: wandb.log({\"correct\": correct/len(y)})\n",
        "        # try: wandb.log({\"correct\": correct/len(y), \"rankme\": rankme, \"lidar\": lidar})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "import time\n",
        "start = begin = time.time()\n",
        "# for i in range(1):\n",
        "for i in range(1000): # 1000\n",
        "    print(i)\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # strain(ijepa, train_loader, optim)\n",
        "    # ctrain(ijepa, classifier, train_loader, coptim)\n",
        "    # test(ijepa, classifier, test_loader)\n",
        "\n",
        "    strain(mae, train_loader, optim)\n",
        "    ctrain(mae, classifier, train_loader, coptim)\n",
        "    test(mae, classifier, test_loader)\n",
        "\n",
        "    # strain(violet, train_loader, voptim)\n",
        "    # ctrain(violet, classifier, train_loader, coptim)\n",
        "    # test(violet, classifier, test_loader)\n",
        "\n",
        "    print('time:',time.time() - start, (time.time()-begin)/(i+1))\n",
        "    start = time.time()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4J2ahp3wmqcg"
      },
      "outputs": [],
      "source": [
        "# @title supervised train test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "# def strain(model, dataloader, optim, scheduler=None):\n",
        "def strain(model, classifier, dataloader, optim, coptim, scheduler=None):\n",
        "    model.train()\n",
        "    classifier.train()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x = x.to(device)#.to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "        y = y.to(device)\n",
        "\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        # print(loss)\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # for p in list(filter(lambda p: p.grad is not None, model.parameters())):\n",
        "        #     print(p.grad.data.norm(2).item())\n",
        "        # print(\"max grad norm\", max([p.grad.data.norm(2).item() for p in list(filter(lambda p: p.grad is not None, model.parameters()))]))\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 10) # 0.5\n",
        "\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        print(\"strain\",loss.item())\n",
        "        # for param in ijepa.student.cls: print(param.data)\n",
        "        # for param in ijepa.predicter.cls: print(param.data)\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=50: break\n",
        "\n",
        "\n",
        "# def test(model, dataloader):\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        # .to(torch.bfloat16)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "        loss = F.cross_entropy(y_, y)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        print(correct/len(y))\n",
        "        try: wandb.log({\"correct\": correct/len(y), \"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "for i in range(1000):\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # np.random.shuffle(train_indices); np.random.shuffle(val_indices)\n",
        "    # train_sampler, valid_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(val_indices)\n",
        "    # # batch_size = 64 #512\n",
        "    # train_loader = DataLoader(train_data, sampler=train_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "    # test_loader = DataLoader(train_data, sampler=valid_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True)\n",
        "\n",
        "    # strain(ijepa, train_loader, optim)\n",
        "    strain(ijepa, classifier, train_loader, optim, coptim)\n",
        "    test(ijepa, classifier, test_loader)\n",
        "\n",
        "    # strain(violet, train_loader, voptim)\n",
        "    # test(violet, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzo9DMDPcOxu",
        "outputId": "2aa950ff-7a1c-46e0-924c-c6922b2ca522"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# @title save/load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder='/content/drive/MyDrive/jepa/'\n",
        "\n",
        "# # modelsd, optimsd = torch.load(folder+'SeqJEPA.pkl', map_location=device).values()\n",
        "# modelsd, optimsd = torch.load('SeqJEPA.pkl', map_location=device).values()\n",
        "# seq_jepa.load_state_dict(modelsd, strict=False)\n",
        "# optim.load_state_dict(optimsd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "DNNPOuUmcSNf",
        "outputId": "9c0fdeca-f315-457a-a102-ff87f9290f75"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'seq_jepa' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-06e8be6b0f78>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mseq_jepa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'IJEPA.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# torch.save(checkpoint, 'IJEPA.pkl')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'seq_jepa' is not defined"
          ]
        }
      ],
      "source": [
        "checkpoint = {'model': seq_jepa.state_dict(), 'optimizer': optim.state_dict()}\n",
        "torch.save(checkpoint, folder+'IJEPA.pkl')\n",
        "# torch.save(checkpoint, 'IJEPA.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLT74ihtMnh3"
      },
      "source": [
        "## drawer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMTvHTtTRInh",
        "outputId": "91ca330e-fd81-42c6-af3c-d4e5316df589"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "345200\n",
            "(tensor(1.9985, device='cuda:0', grad_fn=<MseLossBackward0>), (tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>), tensor(0.9865, device='cuda:0', grad_fn=<AddBackward0>), tensor(7.4222e-06, device='cuda:0', grad_fn=<AddBackward0>)))\n"
          ]
        }
      ],
      "source": [
        "# @title IJEPA + VICReg\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "    def __init__(self, patch_size, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        patch_size=4\n",
        "        act = nn.GELU()\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Conv2d(in_dim, d_model, patch_size, patch_size), # like patch\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 1, 7//2, bias=False)\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.MaxPool2d(3,2,1)\n",
        "            nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.BatchNorm2d(d_model), nn.ReLU(),\n",
        "            nn.Conv2d(d_model, d_model, 3, 2, 3//2, bias=False)\n",
        "            )\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, 32*32, d_model)*.02)\n",
        "        # self.pos_emb = nn.Parameter(RoPE2D(dim=d_model, h=8, w=8, base=1000), requires_grad=False)\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=(32//patch_size)**2, base=10000), requires_grad=False)\n",
        "\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, d_model, n_heads) for _ in range(nlayers)])\n",
        "\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "        # self.lin = nn.Linear(d_model, out_dim)\n",
        "        self.attn_pool = nn.Linear(d_model, 1, bias=False)\n",
        "\n",
        "        dim_v = d_model * 4\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(out_dim, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v, bias=False),\n",
        "            )\n",
        "\n",
        "    # def forward(self, x, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "    def jepa_fwd(self, x, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x).flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c] # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        # x = self.pos_enc(x)\n",
        "        # print(\"TransformerModel\",x.shape, self.pos_emb.shape)\n",
        "        x = x + self.pos_emb[:,:x.shape[1]]\n",
        "        if context_indices != None: x = x[torch.arange(x.shape[0]).unsqueeze(-1), context_indices] # [batch, num_context_toks, d_model]\n",
        "\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        out = self.norm(x)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out\n",
        "\n",
        "    def pool(self, x):\n",
        "        # sx = self.forward(x)\n",
        "        # attn = self.attn_pool(x).squeeze(-1) # [batch, seq] # seq_pool\n",
        "        # out = (torch.softmax(attn, dim=-1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, seq] @ [batch, seq, dim] -> [batch, dim]\n",
        "        # if self.lin: out = self.lin(out)\n",
        "        out = x.mean(dim=1)\n",
        "        return out\n",
        "\n",
        "    def forward(self, x, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.jepa_fwd(x, context_indices=None)\n",
        "        out = self.pool(x)\n",
        "        return out\n",
        "\n",
        "    def expand(self, x):\n",
        "        # sx = self.forward(x)\n",
        "        sx = self.pool(x)\n",
        "        vx = self.exp(sx)\n",
        "        return vx\n",
        "\n",
        "\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class IJEPAVICReg(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=64, out_dim=None, nlayers=1, n_heads=4):\n",
        "        super().__init__()\n",
        "        self.out_dim = out_dim = out_dim or d_model\n",
        "        self.patch_size = 4 # 4\n",
        "        self.student = ViT(self.patch_size, in_dim, d_model, out_dim=out_dim, n_heads=n_heads, nlayers=nlayers, drop=0.)\n",
        "        self.predicter = TransformerPredictor(out_dim, 3*d_model//8, out_dim, n_heads=4, nlayers=1, drop=0.)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "\n",
        "        # vicreg\n",
        "        self.sim_coeff=25. # 10.0 # 25.0 # λ\n",
        "        self.std_coeff=25. # 1.0 # 25.0 # µ\n",
        "        self.cov_coeff=1. # 25.0 # 1.0 # ν\n",
        "\n",
        "\n",
        "    def loss(self, x): #\n",
        "        b,c,h,w = x.shape\n",
        "        # print('ijepa loss x',x.shape)\n",
        "        hw=(8,8)\n",
        "        # hw=(32,32)\n",
        "        mask_collator = MaskCollator(hw, enc_mask_scale=(.85, 1.), pred_mask_scale=(.15, .2), aspect_ratio=(.75, 1.5), nenc=1, npred=4, min_keep=4, allow_overlap=False)\n",
        "        collated_masks_enc, collated_masks_pred = mask_collator(b)\n",
        "        context_indices, trg_indices = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "        # print(context_indices.shape, trg_indices.shape)\n",
        "\n",
        "        # context_indices, trg_indices = simplexmask2d(hw, ctx_scale=(.85,1), trg_scale=(.6,.8), B=b, chaos=3)\n",
        "\n",
        "        # zero_mask = torch.zeros(b ,*hw, device=device).flatten(1)\n",
        "        # zero_mask[torch.arange(b).unsqueeze(-1), context_indices] = 1\n",
        "        # x_ = x * F.interpolate(zero_mask.reshape(b,1,*hw), size=x.shape[2:], mode='nearest-exact') # zero masked locations\n",
        "\n",
        "        sx = self.student.jepa_fwd(x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "\n",
        "        # print('ijepa loss sx',sx.shape)\n",
        "        sy_ = self.predicter(sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "        sy_ = F.layer_norm(sy_, (sy_.size(-1),))\n",
        "        vx = self.student.expand(sx) # [batch, num_context_toks, out_dim]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            sy = self.teacher.jepa_fwd(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "            sy = sy[torch.arange(sy.shape[0]).unsqueeze(-1), trg_indices] # [batch, num_context_toks, d_model] # nan bec len(trg_ind)==0 # print('loss sy',torch.isnan(sy).any())\n",
        "            sy = F.layer_norm(sy, (sy.size(-1),))\n",
        "\n",
        "            vy = self.teacher.expand(sy.detach()) # [batch, num_trg_toks, out_dim]\n",
        "\n",
        "        vic_loss = self.vicreg(vx, vy)\n",
        "        j_loss = F.mse_loss(sy, sy_)\n",
        "        # return loss\n",
        "        return j_loss, vic_loss\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        out = self.student(x)\n",
        "        return out\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size, num_features = x.shape\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = self.sim_coeff * repr_loss + self.std_coeff * std_loss + self.cov_coeff * cov_loss\n",
        "        # print(\"in vicreg \",self.sim_coeff * repr_loss.item() , self.std_coeff * std_loss.item() , self.cov_coeff * cov_loss.item())\n",
        "        # return loss\n",
        "        return repr_loss, std_loss, cov_loss\n",
        "\n",
        "# for jepa+vic, attnpool < meanpool\n",
        "# jepa+vic < jepa\n",
        "\n",
        "\n",
        "ijepa = IJEPAVICReg(in_dim=3, d_model=64, out_dim=64, nlayers=1, n_heads=4).to(device)\n",
        "print(sum(p.numel() for p in ijepa.parameters() if p.requires_grad)) # 27584\n",
        "optim = torch.optim.AdamW(ijepa.parameters(), lr=1e-3) # 1e-3?\n",
        "# optim = torch.optim.AdamW([{'params': ijepa.student.parameters()},\n",
        "#     {'params': ijepa.predicter.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # good 3e-3,1e-3 ; default 1e-2, 5e-2\n",
        "#     # {'params': ijepa.predicter.parameters(), 'lr': 1e-2}], lr=1e-3, weight_decay=1e-2)\n",
        "\n",
        "x = torch.rand((2,3,32,32), device=device)\n",
        "loss = ijepa.loss(x)\n",
        "print(loss)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(ijepa.out_dim, 10).to(device)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ge36SCxOl2Oq"
      },
      "outputs": [],
      "source": [
        "# @title RoPE2D & RotEmb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "def RoPE(dim, seq_len=512, base=10000):\n",
        "    theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "    pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "    angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "    rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [1, seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "    return rot_emb\n",
        "\n",
        "# class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "#     def __init__(self, dim, seq_len=512, base=10000):\n",
        "#         super().__init__()\n",
        "#         self.dim, self.base = dim, base\n",
        "#         theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "#         pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "#         angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "#         self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [1, seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "#     def forward(self, x): # [batch, T, dim] / [batch, T, n_heads, d_head]\n",
        "#         seq_len = x.size(1)\n",
        "#         if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.base)\n",
        "#         # print('rope fwd', x.shape, self.rot_emb.shape)\n",
        "#         if x.dim()==4: return x * self.rot_emb[:,:seq_len].unsqueeze(2)\n",
        "#         return x * self.rot_emb[:,:seq_len]\n",
        "\n",
        "\n",
        "def RoPE2D(dim=16, h=8, w=8, base=10000):\n",
        "    # theta = 1. / (base ** (torch.arange(0, dim, step=4) / dim))\n",
        "    # theta = 1. / (base**torch.linspace(0,1,dim//4)).unsqueeze(0)\n",
        "    theta = 1. / (base**torch.linspace(0,1,dim//2)).unsqueeze(0)\n",
        "    y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\") # print(y,x) # [h,w], y:row_num, x:col_num\n",
        "    y, x = (y.reshape(-1,1) * theta).unsqueeze(-1), (x.reshape(-1,1) * theta).unsqueeze(-1) # [h*w,1]*[1,dim//4] = [h*w, dim//4, 1]\n",
        "    # rot_emb = torch.cat([x.sin(), x.cos(), y.sin(), y.cos()], dim=-1).flatten(-2) # [h*w, dim//4 ,4] -> [h*w, dim]\n",
        "    # rot_emb = torch.cat([x.sin(), x.cos(), y.sin(), y.cos()], dim=-1)#.reshape(dim, h, w).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "    rot_emb = torch.cat([x.sin(), y.sin()], dim=-1).flatten(-2).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "    # rot_emb = torch.cat([x.cos(), y.cos()], dim=-1).reshape(h, w, dim).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "    return rot_emb\n",
        "\n",
        "\n",
        "\n",
        "# class RoPE2D(nn.Module): # Rotary Positional Embeddings\n",
        "#     def __init__(self, dim, h=224, w=224, base=10000):\n",
        "#         super().__init__()\n",
        "#         self.dim, self.h, self.w = dim, h, w\n",
        "#         # # theta = 1. / (base ** (torch.arange(0, dim, step=4) / dim))\n",
        "#         # theta = 1. / (base**torch.linspace(0,1,dim//4)).unsqueeze(0)\n",
        "#         theta = 1. / (base**torch.linspace(0,1,dim//2)).unsqueeze(0)\n",
        "#         y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\") # print(y,x) # [h,w], y:row_num, x:col_num\n",
        "#         y, x = (y.reshape(-1,1) * theta).unsqueeze(-1), (x.reshape(-1,1) * theta).unsqueeze(-1) # [h*w,1]*[1,dim//4] = [h*w, dim//4, 1]\n",
        "#         # # self.rot_emb = torch.cat([x.sin(), x.cos(), y.sin(), y.cos()], dim=-1).flatten(-2) # [h*w, dim//4 ,4] -> [h*w, dim]\n",
        "#         # self.rot_emb = torch.cat([x.sin(), x.cos(), y.sin(), y.cos()], dim=-1).reshape(h, w, dim).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "#         self.rot_emb = torch.cat([x.sin(), y.sin()], dim=-1).reshape(h, w, dim).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "#         # self.rot_emb = torch.cat([x.cos(), y.cos()], dim=-1).reshape(h, w, dim).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "\n",
        "#     def forward(self, img): #\n",
        "#         # batch, dim, h, w = img.shape\n",
        "#         # print(img.shape)\n",
        "#         hw = img.shape[1] # [b, hw, dim] / [b, hw, n_heads, d_head]\n",
        "#         h=w=int(hw**.5)\n",
        "#         if self.h < h or self.w < w: self.__init__(self.dim, h, w)\n",
        "#         # print(self.rot_emb.shape)\n",
        "#         # rot_emb = self.rot_emb[:, :h, :w].unsqueeze(0) # [1, h, w, dim]\n",
        "#         rot_emb = self.rot_emb[:h, :w] # [h, w, dim]\n",
        "#         # return img * rot_emb.flatten(end_dim=1).unsqueeze(0) # [b, hw, dim] * [1, hw, dim]\n",
        "#         return img * rot_emb.flatten(end_dim=1)[None,:,None,:] # [b, hw, n_heads, d_head] * [1, hw, 1, dim]\n",
        "#         # return img * self.rot_emb\n",
        "\n",
        "class RotEmb(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, top=torch.pi, base=10000):\n",
        "        super().__init__()\n",
        "        self.theta = top / (base ** (torch.arange(0, dim, step=2, device=device) / dim))\n",
        "        # self.theta = top / (base ** torch.linspace(0, 1, dim//2, device=device))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (pos.unsqueeze(-1) * self.theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [seq_len, dim]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3bpiybH7M0O1"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# @title test multiblock params\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "dataiter = iter(train_data)\n",
        "img,y = next(dataiter)\n",
        "img = img.unsqueeze(0)\n",
        "img = F.interpolate(img, (8,8))\n",
        "b,c,h,w = img.shape\n",
        "# img = img.flatten(2).transpose(-2,-1).to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "print(img.shape)\n",
        "# batch, seq,_ = img.shape\n",
        "\n",
        "\n",
        "# # target_mask = randpatch(seq, mask_size=8, gamma=.9) # 8.9 [seq]\n",
        "# target_mask = multiblock(seq, min_s=0.2, max_s=0.3, M=4).any(0) # best.2.3M4 og.15.2M4# mask out targets to be predicted # [M, seq]\n",
        "# target_mask = multiblock(seq, min_s=0.2, max_s=0.3, M=4).any(0)\n",
        "\n",
        "target_mask = multiblock2d((8,8), M=4).any(0).unsqueeze(0)\n",
        "\n",
        "\n",
        "# context_mask = ~multiblock(seq, min_s=0.85, max_s=1., M=1)|target_mask # [1, seq], True->Mask\n",
        "context_mask = ~multiblock2d((8,8), scale=(.85,.85), aspect_ratio=(.9,1.1), M=1)|target_mask # [1, seq], True->Mask\n",
        "\n",
        "# print(target_mask, context_mask)\n",
        "\n",
        "print(target_mask.shape, context_mask.shape)\n",
        "# target_img, context_img = img*target_mask.unsqueeze(-1), img*context_mask.unsqueeze(-1)\n",
        "# target_img, context_img = img*~target_mask.unsqueeze(-1), img*~context_mask.unsqueeze(-1)\n",
        "target_img, context_img = img*~target_mask.unsqueeze(0), img*~context_mask.unsqueeze(0)\n",
        "# print(target_img.shape, context_img.shape)\n",
        "target_img, context_img = target_img.transpose(-2,-1).reshape(b,c,h,w), context_img.transpose(-2,-1).reshape(b,c,h,w)\n",
        "target_img, context_img = target_img.float(), context_img.float()\n",
        "\n",
        "# imshow(out.detach().cpu())\n",
        "imshow(target_img[0])\n",
        "imshow(context_img[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1iZQ6UNwoty",
        "outputId": "5554e8f3-71ec-4b70-c7d7-219b59e9a1d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9920\n",
            "torch.Size([4, 64, 16])\n"
          ]
        }
      ],
      "source": [
        "# @title ViT me more\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "# class LayerNorm2d(nn.LayerNorm):\n",
        "class LayerNorm2d(nn.RMSNorm):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        x = super().forward(x)\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads=None, d_head=8, cond_dim=None, dropout=0.): # .1\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_head = d_head\n",
        "        self.n_heads = d_model // d_head\n",
        "        # self.d_head = d_model // n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or d_model, 2*d_model, bias=False)\n",
        "        # self.k = nn.Sequential(nn.Dropout(dropout), nn.Linear(cond_dim, d_model, bias=False))\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        # self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        # self.lin = nn.Sequential(nn.Dropout(dropout), zero_module(nn.Linear(d_model, d_model)))\n",
        "        self.drop = nn.Dropout(dropout) # indp before q,k,v; after linout\n",
        "        self.scale = self.d_head ** -.5\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [batch, T, d_model]=[batch, h*w, c], [batch, num_tok, cond_dim], [batch,T]\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        Q = self.q(x).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        # K = self.k(x).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2)\n",
        "        K, V = self.kv(cond).unflatten(-1, (self.n_heads, 2*self.d_head)).transpose(1, 2).chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # linear attention # Softmax(Q) @ (Softmax(K).T @ V)\n",
        "        if mask != None:\n",
        "            mask = mask[:, None, :, None] # [batch,T] -> [batch,1,T,1]\n",
        "            K, V = K.masked_fill(mask, -torch.finfo(x.dtype).max), V.masked_fill(mask, -torch.finfo(x.dtype).max)\n",
        "        Q, K = Q.softmax(dim=-1)*self.scale, K.softmax(dim=-2)\n",
        "        context = K.transpose(-2,-1) @ V # [batch, n_heads, d_head, d_head]\n",
        "        out = Q @ context # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # (quadratic) attention # Softmax(Q @ K.T) @ V\n",
        "        # attn = Q @ K.transpose(-2,-1) * self.scale # [batch, n_heads, T] # [batch, n_heads, T, T/num_tok]\n",
        "        # if mask != None: attn = attn.masked_fill(mask[:, None, :, None], -torch.finfo(attn.dtype).max) # [batch,T]->[batch,1,T,1]\n",
        "        # attention = torch.softmax(attn, dim=-1)\n",
        "        # out = self.drop(attention) @ V # [batch, n_heads, T, d_head]\n",
        "\n",
        "        out = out.transpose(1, 2).flatten(2)\n",
        "        return self.drop(self.lin(out)) # [batch, T, d_model]\n",
        "\n",
        "# if self, dont pass cond_dim in init, dont pass cond in fwd\n",
        "# Softmax(Q @ K.T) @ V ~ Softmax(Q) @ Softmax(K).T @ V\n",
        "\n",
        "# https://github.com/lucidrains/x-transformers/blob/main/x_transformers/x_transformers.py#L1855\n",
        "\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_head, cond_dim=None, mult=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.cond_dim = cond_dim\n",
        "        self.self = MultiHeadAttention(d_model, d_head=d_head, dropout=0) # 16448\n",
        "        # self.self = Pooling()\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        # self.ff = nn.Sequential(\n",
        "        #     *[nn.BatchNorm2d(d_model), act, SeparableConv2d(d_model, d_model),]*3\n",
        "        #     )\n",
        "        # self.ff = ResBlock(d_model) # 74112\n",
        "        # self.ff = UIB(d_model, mult=4) # uib m4 36992, m2 18944\n",
        "        ff_dim=d_model*mult\n",
        "        self.ff = nn.Sequential(\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(dropout), nn.Linear(d_model, ff_dim), act, # ReLU GELU\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(dropout), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            nn.RMSNorm(d_model), act, nn.Dropout(dropout), nn.Linear(d_model, ff_dim),\n",
        "            nn.RMSNorm(ff_dim), act, nn.Dropout(dropout), zero_module(nn.Linear(ff_dim, d_model))\n",
        "        )\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,c,h,w], [batch, num_tok, cond_dim], [batch,T]\n",
        "        bchw = x.shape\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "        # print('attnblk fwd',x.shape)\n",
        "\n",
        "        # if self.cond_dim==None: cond=None # is self attn\n",
        "        x = x + self.drop(self.self(self.norm(x)))\n",
        "        # x = x.transpose(1,2).reshape(*bchw)\n",
        "        x = x + self.ff(x)\n",
        "        # x = self.ff(x)\n",
        "        # x = x + self.drop(self.norm2(self.ff(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "import torch\n",
        "# def apply_masks(x, masks): # [b,t,d], [M,mask_size] # https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "#     all_x = []\n",
        "#     for m in masks: # [1,T]\n",
        "#         mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [1,T,dim]\n",
        "#         all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "#     return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "def apply_masks(x, mask): # [b,t,d], [mask_size] # https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "    mask_keep = mask.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [1,T,dim]\n",
        "    # return torch.cat(torch.gather(x, dim=1, index=mask_keep), dim=0)  # [M*batch,mask_size,dim]\n",
        "    return torch.gather(x, dim=1, index=mask_keep) # [M*batch,mask_size,dim]\n",
        "\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        # self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        self.positional_emb = nn.Parameter(torch.zeros(1, 8*8, d_model)) # positional_embedding == 'learnable'\n",
        "        # self.positional_emb = nn.Parameter(torch.zeros(1, dim, 8,8))\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.zeros(1,1,d_model)) # randn zeros\n",
        "        nn.init.trunc_normal_(self.cls, std=.02)\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model)\n",
        "        self.lin = nn.Linear(d_model, out_dim)# if out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = x.shape\n",
        "        # x = x * self.pos_encoder(context_indices)\n",
        "        x = x + self.positional_emb[:,context_indices]\n",
        "        # x = apply_masks(x, [context_indices])\n",
        "\n",
        "        # pred_tokens = self.cls * self.pos_encoder(trg_indices) # [M, num_trg_toks, d_model]\n",
        "        pred_tokens = self.cls + self.positional_emb[0,trg_indices]\n",
        "        print(self.cls.shape, self.positional_emb[0,trg_indices].shape, trg_indices.shape)\n",
        "        pred_tokens = pred_tokens.repeat(batch, 1, 1) # [batch*M, num_trg_toks, d_model]\n",
        "        # print(pred_tokens.requires_grad)\n",
        "        print(\"pred fwd\", x.shape, pred_tokens.shape)\n",
        "        # x = x.repeat_interleave(trg_indices.shape[0], dim=0) # [batch, seq_len, d_model] -> [batch*M, seq_len, d_model]\n",
        "        x = torch.cat([x, pred_tokens], dim=1) # [batch*M, seq_len+num_trg_toks, d_model]\n",
        "\n",
        "        # x = x.transpose(1,2).unflatten(-1, (8,8))#.reshape(*bchw)\n",
        "        out = self.transformer_encoder(x) # float [seq_len, batch_size, d_model]\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "\n",
        "        out = self.norm(out)\n",
        "        out = out[:,seq:] # [batch*M, num_trg_toks, d_model]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Sequential(\n",
        "            # # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.MaxPool1d(2,2), #nn.MaxPool1d(3, 2, 3//2),\n",
        "            # # nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.Conv1d(d_model, d_model,3,2,3//2)\n",
        "            # nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            # nn.Conv1d(d_model, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            # nn.Conv1d(d_model, d_model,3,2,3//2),\n",
        "            # # nn.Conv1d(in_dim, d_model,2,2,0), # like patch\n",
        "            nn.Conv2d(in_dim, d_model, kernel_size=7, stride=2, padding=7//2, bias=False), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "            )\n",
        "        # self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        self.positional_emb = nn.Parameter(torch.zeros(1, 8*8, d_model)) # positional_embedding == 'learnable'\n",
        "        # self.positional_emb = nn.Parameter(torch.zeros(1, dim, 8,8)) # positional_embedding == 'learnable'\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model)\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # x = self.embed(x.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "        x = self.embed(x)\n",
        "        # bchw = x.shape\n",
        "        x = x.flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c] # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "\n",
        "        # x = self.pos_encoder(x)\n",
        "        # x = x * self.pos_encoder(torch.arange(seq, device=device)).unsqueeze(0)\n",
        "        # print(x.shape, self.positional_emb.shape)\n",
        "        x = x + self.positional_emb\n",
        "        if context_indices != None:\n",
        "            # x = apply_masks(x, [context_indices])\n",
        "            x = apply_masks(x, context_indices)\n",
        "\n",
        "\n",
        "        # x = x.transpose(1,2).reshape(*bchw)\n",
        "        x = self.transformer_encoder(x) # float [seq_len, batch_size, d_model]\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "        out = self.norm(x)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "\n",
        "# pos_emb rope < learn < learned\n",
        "# conv > pixel?\n",
        "# droppath not required\n",
        "\n",
        "# norm,act,conv < conv,norm,act\n",
        "# 2*s1 < uib < resblock\n",
        "# gatedadaln 3 < 2 = 1 < ffmult4 = 2*gatedadaln\n",
        "# MaxPool2d(2,2) < MaxPool2d(3,2,3//2)\n",
        "\n",
        "\n",
        "# dim = 64\n",
        "# dim_head = 8\n",
        "# heads = dim // dim_head\n",
        "# num_classes = 10\n",
        "# # model = SimpleViT(image_size=32, patch_size=4, num_classes=num_classes, dim=dim, depth=1, heads=heads, mlp_dim=dim*4, channels = 3, dim_head = dim_head)\n",
        "# model = SimpleViT(in_dim=3, out_dim=num_classes, dim=dim, depth=1, heads=heads, mlp_dim=dim*4, channels = 3, dim_head = dim_head).to(device)\n",
        "# print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 59850\n",
        "# optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "# # print(images.shape) # [batch, 3, 32, 32]\n",
        "# logits = model(x)\n",
        "# print(logits.shape)\n",
        "# # print(logits[0])\n",
        "# # print(logits[0].argmax(1))\n",
        "# pred_probab = nn.Softmax(dim=1)(logits)\n",
        "# y_pred = pred_probab.argmax(1)\n",
        "# # print(f\"Predicted class: {y_pred}\")\n",
        "\n",
        "\n",
        "batch, seq_len, d_model = 4,1024,16\n",
        "in_dim = 3\n",
        "model = TransformerModel(in_dim, d_model, d_head=4, nlayers=2, dropout=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x = torch.rand((batch, in_dim, 32, 32), device=device)\n",
        "# x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "# # # print(out)\n",
        "# model = TransformerPredictor(in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1).to(device)\n",
        "# out = model(out)\n",
        "# print(out.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "e3dAhWh45F4M"
      },
      "outputs": [],
      "source": [
        "# @title simplex\n",
        "# !pip install -q opensimplex\n",
        "import opensimplex\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def simplexmask2d(hw=(32,32), ctx_scale=(.85,1), trg_scale=(.6,.8), B=64, chaos=2):\n",
        "    seed = np.random.randint(1e10, size=B)\n",
        "    ix = iy = np.linspace(0, chaos, num=max(hw))\n",
        "    noise = opensimplex.noise3array(ix, iy, seed) # [b,h,w]\n",
        "    # plt.pcolormesh(noise[:1])\n",
        "    # plt.rcParams[\"figure.figsize\"] = (20,3)\n",
        "    # plt.show()\n",
        "    noise = torch.from_numpy(noise)\n",
        "    ctx_mask_scale = torch.rand(1) * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    trg_mask_scale = torch.rand(1) * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "    val, ind = noise.flatten(1).sort() # [b,h*w]\n",
        "    seq = hw[0]*hw[1]\n",
        "    # trg_index = ind[:,-int(seq*trg_mask_scale):]\n",
        "    # ctx_index = ind[:,-int(seq*ctx_mask_scale):-int(seq*trg_mask_scale)] # ctx wraps trg ; most similar to multiblock\n",
        "    # ctx_index = ind[:,:int(seq*ctx_mask_scale)-int(seq*trg_mask_scale)] # ctx hug bottom\n",
        "    # ctx_index = ind[:,-int(seq*ctx_mask_scale)-int(seq*trg_mask_scale):-int(seq*trg_mask_scale)] # ctx wraps trg ; most similar to multiblock\n",
        "    # ctx_index = ind[:,:int(seq*ctx_mask_scale)] # ctx hug bottom\n",
        "    return ctx_index, trg_index\n",
        "\n",
        "\n",
        "def simplexmask2d(hw=(32,32), ctx_scale=(.85,1), trg_scale=(.6,.8), B=64, chaos=2):\n",
        "    seed = np.random.randint(1e10, size=B)\n",
        "    ix = np.linspace(0, chaos, num=hw[1])\n",
        "    iy = np.linspace(0, chaos, num=hw[0])\n",
        "    noise = opensimplex.noise3array(ix, iy, seed) # [b,h,w]\n",
        "    noise = torch.from_numpy(noise)\n",
        "    ctx_mask_scale = torch.rand(1) * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    trg_mask_scale = torch.rand(1) * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "    val, ind = noise.flatten(1).sort() # [b,h*w]\n",
        "\n",
        "    seq = hw[0]*hw[1]\n",
        "\n",
        "    ctx_len = int(seq*ctx_mask_scale)\n",
        "    trg_len = int(seq*trg_mask_scale)\n",
        "    trg_pos = (torch.rand(B) * (seq-trg_len)).int()\n",
        "    # print(trg_pos)\n",
        "\n",
        "    trg_ind = trg_pos.unsqueeze(-1) + torch.arange(trg_len).unsqueeze(0)\n",
        "    trg_index = ind[torch.arange(B).unsqueeze(-1), trg_ind]\n",
        "    ctx_len = ctx_len - trg_len\n",
        "\n",
        "    remove_mask = torch.ones((B,seq), dtype=bool) # [B, S]\n",
        "    remove_mask.scatter_(1, trg_ind, False).flatten()\n",
        "    ctx_ind = torch.arange(seq).unsqueeze(0).repeat(B,1)[remove_mask].reshape(B, -1)\n",
        "    ctx_index = ind[torch.arange(B).unsqueeze(-1), ctx_ind[:,-ctx_len:]]\n",
        "    return ctx_index, trg_index\n",
        "\n",
        "\n",
        "# @title simplex\n",
        "!pip install opensimplex\n",
        "from opensimplex import OpenSimplex\n",
        "\n",
        "seed=0\n",
        "noise = OpenSimplex(seed)  # Replace 'seed' with your starting value\n",
        "noise_scale = 10  # Adjust for desired noise range\n",
        "\n",
        "def get_noise(x, y):\n",
        "    global seed  # Assuming seed is a global variable\n",
        "    # noise_val = noise.noise2(x / noise_scale, y / noise_scale)\n",
        "    noise_val = noise.noise2(x, y)\n",
        "    # seed += 1  # Increment seed after each call\n",
        "    return noise_val\n",
        "\n",
        "x,y=0,0\n",
        "a=[[],[],[],[],[]]\n",
        "\n",
        "fps=2\n",
        "f=[]\n",
        "for i in range(10*fps):\n",
        "    f.append(i/fps)\n",
        "    x = x+0.3\n",
        "    # y = y+1\n",
        "    # noise_value = get_noise(x, y)\n",
        "    for k,j in enumerate([0,1,2,3,4]):\n",
        "        a[k].append(get_noise(x, y+j))\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "for aa in a:\n",
        "    plt.plot(f,aa)\n",
        "plt.show()\n",
        "\n",
        "# b,y,g,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vtPSM8mbnJZy"
      },
      "outputs": [],
      "source": [
        "# @title pos_embed.py\n",
        "# https://github.com/facebookresearch/mae/blob/main/util/pos_embed.py#L20\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim=16, grid_size=8, cls_token=False): #\n",
        "    grid_h = np.arange(grid_size, dtype=np.float32)\n",
        "    grid_w = np.arange(grid_size, dtype=np.float32)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token: pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed # [(1+)grid_size*grid_size, embed_dim]\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    # assert embed_dim % 2 == 0\n",
        "\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1) # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos): #\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=float)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega  # (D/2,)\n",
        "\n",
        "    pos = pos.reshape(-1)  # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
        "\n",
        "    emb_sin = np.sin(out) # (M, D/2)\n",
        "    emb_cos = np.cos(out) # (M, D/2)\n",
        "\n",
        "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def interpolate_pos_embed(model, checkpoint_model):\n",
        "    if 'pos_embed' in checkpoint_model:\n",
        "        pos_embed_checkpoint = checkpoint_model['pos_embed']\n",
        "        embedding_size = pos_embed_checkpoint.shape[-1]\n",
        "        num_patches = model.patch_embed.num_patches\n",
        "        num_extra_tokens = model.pos_embed.shape[-2] - num_patches\n",
        "        # height (== width) for the checkpoint position embedding\n",
        "        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n",
        "        # height (== width) for the new position embedding\n",
        "        new_size = int(num_patches ** 0.5)\n",
        "        # class_token and dist_token are kept unchanged\n",
        "        if orig_size != new_size:\n",
        "            print(\"Position interpolate from %dx%d to %dx%d\" % (orig_size, orig_size, new_size, new_size))\n",
        "            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n",
        "            # only the position tokens are interpolated\n",
        "            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n",
        "            pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n",
        "            pos_tokens = torch.nn.functional.interpolate(\n",
        "                pos_tokens, size=(new_size, new_size), mode='bicubic', align_corners=False)\n",
        "            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n",
        "            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n",
        "            checkpoint_model['pos_embed'] = new_pos_embed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "dLbXQ-3XXRMq"
      },
      "outputs": [],
      "source": [
        "# @title ijepa src.utils.tensors\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/utils/tensors.py\n",
        "\n",
        "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
        "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n",
        "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
        "    def norm_cdf(x):\n",
        "        # Computes standard normal cumulative distribution function\n",
        "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Values are generated by using a truncated uniform distribution and\n",
        "        # then using the inverse CDF for the normal distribution.\n",
        "        # Get upper and lower cdf values\n",
        "        l = norm_cdf((a - mean) / std)\n",
        "        u = norm_cdf((b - mean) / std)\n",
        "\n",
        "        # Uniformly fill tensor with values from [l, u], then translate to\n",
        "        # [2l-1, 2u-1].\n",
        "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
        "\n",
        "        # Use inverse cdf transform for normal distribution to get truncated\n",
        "        # standard normal\n",
        "        tensor.erfinv_()\n",
        "\n",
        "        # Transform to proper mean, std\n",
        "        tensor.mul_(std * math.sqrt(2.))\n",
        "        tensor.add_(mean)\n",
        "\n",
        "        # Clamp to ensure it's in the proper range\n",
        "        tensor.clamp_(min=a, max=b)\n",
        "        return tensor\n",
        "\n",
        "\n",
        "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
        "    # type: (Tensor, float, float, float, float) -> Tensor\n",
        "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n",
        "\n",
        "def repeat_interleave_batch(x, B, repeat): # [batch*M,...]? , M?\n",
        "    N = len(x) // B\n",
        "    x = torch.cat([\n",
        "        torch.cat([x[i*B:(i+1)*B] for _ in range(repeat)], dim=0)\n",
        "        for i in range(N)\n",
        "    ], dim=0)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "COvEnBXPYth3"
      },
      "outputs": [],
      "source": [
        "# @title ijepa multiblock down\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "import math\n",
        "from multiprocessing import Value\n",
        "import torch\n",
        "_GLOBAL_SEED = 0\n",
        "\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=(224, 224),\n",
        "        patch_size=16,\n",
        "        enc_mask_scale=(0.2, 0.8),\n",
        "        pred_mask_scale=(0.2, 0.8),\n",
        "        aspect_ratio=(0.3, 3.0),\n",
        "        nenc=1,\n",
        "        npred=2,\n",
        "        min_keep=4,\n",
        "        allow_overlap=False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if not isinstance(input_size, tuple):\n",
        "            input_size = (input_size, ) * 2\n",
        "        self.patch_size = patch_size\n",
        "        self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.aspect_ratio = aspect_ratio\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "        self._itr_counter = Value('i', -1)  # collator is shared across worker processes\n",
        "\n",
        "    def step(self):\n",
        "        i = self._itr_counter\n",
        "        with i.get_lock():\n",
        "            i.value += 1\n",
        "            v = i.value\n",
        "        return v\n",
        "\n",
        "    def _sample_block_size(self, generator, scale, aspect_ratio_scale):\n",
        "        _rand = torch.rand(1, generator=generator).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.height * self.width * mask_scale)\n",
        "        # -- Sample block aspect-ratio\n",
        "        min_ar, max_ar = aspect_ratio_scale\n",
        "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
        "        while h >= self.height:\n",
        "            h -= 1\n",
        "        while w >= self.width:\n",
        "            w -= 1\n",
        "        return (h, w)\n",
        "\n",
        "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
        "        h, w = b_size\n",
        "\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # --\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            top = torch.randint(0, self.height - h, (1,))\n",
        "            left = torch.randint(0, self.width - w, (1,))\n",
        "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
        "            mask[top:top+h, left:left+w] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
        "        mask_complement[top:top+h, left:left+w] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        B = len(batch)\n",
        "\n",
        "        collated_batch = torch.utils.data.default_collate(batch)\n",
        "\n",
        "        seed = self.step()\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(seed)\n",
        "        p_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.pred_mask_scale,\n",
        "            aspect_ratio_scale=self.aspect_ratio)\n",
        "        e_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.enc_mask_scale,\n",
        "            aspect_ratio_scale=(1., 1.))\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.height * self.width\n",
        "        min_keep_enc = self.height * self.width\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                print(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "        return collated_batch, collated_masks_enc, collated_masks_pred\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aH1UOD4WW8I2",
        "outputId": "e8671cd3-eba7-4649-84ca-94c6467ce62d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Mask generator says: \"Valid mask not found, decreasing acceptable-regions [1]\"\n",
            "WARNING:root:Mask generator says: \"Valid mask not found, decreasing acceptable-regions [2]\"\n",
            "WARNING:root:Mask generator says: \"Valid mask not found, decreasing acceptable-regions [3]\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vit fwd torch.Size([4, 3, 224, 224])\n",
            "vit fwd torch.Size([4, 196, 768])\n",
            "vit fwd torch.Size([4, 11, 768])\n",
            "predictor fwd1 torch.Size([4, 11, 384]) torch.Size([4, 196, 384])\n"
          ]
        }
      ],
      "source": [
        "# @title ijepa vision_transformer.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/models/vision_transformer.py\n",
        "import math\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# from src.utils.tensors import (trunc_normal_, repeat_interleave_batch)\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid height and width\n",
        "    return:\n",
        "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid_h = np.arange(grid_size, dtype=float)\n",
        "    grid_w = np.arange(grid_size, dtype=float)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0) # [1+h*w, dim]\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    assert embed_dim % 2 == 0\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid length\n",
        "    return:\n",
        "    pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid = np.arange(grid_size, dtype=float)\n",
        "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=float)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega   # (D/2,)\n",
        "    pos = pos.reshape(-1)   # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
        "    emb = np.concatenate([np.sin(out), np.cos(out)], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\"\"\"\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x, attn\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x, return_attention=False):\n",
        "        y, attn = self.attn(self.norm1(x))\n",
        "        if return_attention:\n",
        "            return attn\n",
        "        x = x + self.drop_path(y)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2) # ->[b,h*w,c]\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvEmbed(nn.Module):\n",
        "    \"\"\"3x3 Convolution stems for ViT following ViTC models\"\"\"\n",
        "    def __init__(self, channels, strides, img_size=224, in_chans=3, batch_norm=True):\n",
        "        super().__init__()\n",
        "        # Build the stems\n",
        "        stem = []\n",
        "        channels = [in_chans] + channels\n",
        "        for i in range(len(channels) - 2):\n",
        "            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3, stride=strides[i], padding=1, bias=(not batch_norm))]\n",
        "            if batch_norm:\n",
        "                stem += [nn.BatchNorm2d(channels[i+1])]\n",
        "            stem += [nn.ReLU(inplace=True)]\n",
        "        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n",
        "        self.stem = nn.Sequential(*stem)\n",
        "\n",
        "        # Comptute the number of patches\n",
        "        stride_prod = int(np.prod(strides))\n",
        "        self.num_patches = (img_size[0] // stride_prod)**2\n",
        "\n",
        "    def forward(self, x):\n",
        "        p = self.stem(x)\n",
        "        return p.flatten(2).transpose(1, 2)\n",
        "\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "def apply_masks(x, masks):\n",
        "    \"\"\":param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
        "    :param masks: list of tensors containing indices of patches in [N] to keep\"\"\"\n",
        "    all_x = []\n",
        "    for m in masks:\n",
        "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [batch,T,dim]\n",
        "        all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "    return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "\n",
        "class VisionTransformerPredictor(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, num_patches, embed_dim=768, predictor_embed_dim=384,\n",
        "        depth=6, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.init_std = init_std\n",
        "        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        # --\n",
        "        self.predictor_pos_embed = nn.Parameter(torch.zeros(1, num_patches, predictor_embed_dim), requires_grad=False)\n",
        "        predictor_pos_embed = get_2d_sincos_pos_embed(self.predictor_pos_embed.shape[-1], int(num_patches**.5), cls_token=False)\n",
        "        self.predictor_pos_embed.data.copy_(torch.from_numpy(predictor_pos_embed).float().unsqueeze(0)) # [1, num_patches, predictor_embed_dim]\n",
        "        # --\n",
        "        self.predictor_blocks = nn.ModuleList([Block(dim=predictor_embed_dim, num_heads=num_heads, drop=drop_rate, drop_path=dpr[i]) for i in range(depth)])\n",
        "        self.predictor_norm = nn.LayerNorm(predictor_embed_dim)\n",
        "        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n",
        "        # ------\n",
        "        # trunc_normal_(self.mask_token, std=self.init_std)\n",
        "\n",
        "    def forward(self, x, masks_x, masks): # [batch, num_context_patches, embed_dim], nenc*[batch, num_context_patches], npred*[batch, num_target_patches]\n",
        "        assert (masks is not None) and (masks_x is not None), 'Cannot run predictor without mask indices'\n",
        "        if not isinstance(masks_x, list): masks_x = [masks_x] # context mask\n",
        "        if not isinstance(masks, list): masks = [masks] # pred mask\n",
        "\n",
        "        # -- Batch Size\n",
        "        B = len(x) // len(masks_x)\n",
        "        # print(\"predictor fwd0\", x.shape) # [3, 121, 768])\n",
        "        # -- map from encoder-dim to pedictor-dim\n",
        "        x = self.predictor_embed(x) # [batch, num_patches, predictor_embed_dim]\n",
        "\n",
        "        # -- add positional embedding to x tokens\n",
        "        x_pos_embed = self.predictor_pos_embed.repeat(B, 1, 1) # [batch, num_patches, predictor_embed_dim]\n",
        "        print(\"predictor fwd1\", x.shape, x_pos_embed.shape) # [3, 121 or 11, 384], [3, 196, 384]\n",
        "        # print(\"predictor fwd masks_x\", masks_x)\n",
        "        x += apply_masks(x_pos_embed, masks_x) # get pos emb of context patches\n",
        "\n",
        "        # print(\"predictor fwd x\", x.shape) # [4, 11, 384])\n",
        "        _, N_ctxt, D = x.shape\n",
        "\n",
        "        # -- concat mask tokens to x\n",
        "        pos_embs = self.predictor_pos_embed.repeat(B, 1, 1) # [batch, num_patches, predictor_embed_dim]\n",
        "        pos_embs = apply_masks(pos_embs, masks) # [16, 104, predictor_embed_dim]\n",
        "        pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x)) # [16, 104, predictor_embed_dim]\n",
        "        # --\n",
        "        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1) # [16, 104, predictor_embed_dim]\n",
        "        # --\n",
        "        # print(\"predictor fwd pred_tokens\", pred_tokens.shape)\n",
        "\n",
        "        pred_tokens += pos_embs\n",
        "        x = x.repeat(len(masks), 1, 1)\n",
        "        x = torch.cat([x, pred_tokens], dim=1)\n",
        "\n",
        "        # -- fwd prop\n",
        "        for blk in self.predictor_blocks:\n",
        "            x = blk(x)\n",
        "        x = self.predictor_norm(x)\n",
        "\n",
        "        # -- return preds for mask tokens\n",
        "        x = x[:, N_ctxt:]\n",
        "        x = self.predictor_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, img_size=[224], patch_size=16, in_chans=3, embed_dim=768, predictor_embed_dim=384, depth=12, predictor_depth=12,\n",
        "        num_heads=12, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0, norm_layer=nn.LayerNorm, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_features = self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.init_std = init_std\n",
        "        # --\n",
        "        self.patch_embed = PatchEmbed(img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        # --\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=False)\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "        # --\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "    def forward(self, x, masks=None): # [batch,3,224,224]\n",
        "        if masks is not None:\n",
        "            if not isinstance(masks, list): masks = [masks]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        x = self.patch_embed(x)\n",
        "        B, N, D = x.shape # [batch, num_patches, embed_dim]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        # -- add positional embedding to x\n",
        "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
        "        x = x + pos_embed\n",
        "\n",
        "        if masks is not None: x = apply_masks(x, masks) # [batch, num_context_patches, embed_dim]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            x = blk(x)\n",
        "\n",
        "        if self.norm is not None: x = self.norm(x)\n",
        "        return x # [batch, num_context_patches, embed_dim]\n",
        "\n",
        "    def interpolate_pos_encoding(self, x, pos_embed):\n",
        "        npatch = x.shape[1] - 1\n",
        "        N = pos_embed.shape[1] - 1\n",
        "        if npatch == N: return pos_embed\n",
        "        class_emb, pos_embed = pos_embed[:, 0], pos_embed[:, 1:]\n",
        "        dim = x.shape[-1]\n",
        "        pos_embed = nn.functional.interpolate(pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2), scale_factor=math.sqrt(npatch / N), mode='bicubic',)\n",
        "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
        "\n",
        "# from functools import partial\n",
        "# def vit_predictor(**kwargs):\n",
        "#     model = VisionTransformerPredictor(\n",
        "#         mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "#     return model\n",
        "\n",
        "def vit(patch_size=16, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, # tiny\n",
        "        # patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, # small\n",
        "        # patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, # base\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "# encoder = vit()\n",
        "encoder = VisionTransformer(\n",
        "        patch_size=16, embed_dim=768, depth=1, num_heads=3, mlp_ratio=1,\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
        "\n",
        "# num_patches = (224/patch_size)^2\n",
        "# model = VisionTransformerPredictor(num_patches, embed_dim=768, predictor_embed_dim=384, depth=6, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02, **kwargs)\n",
        "model = VisionTransformerPredictor(num_patches=196, embed_dim=768, predictor_embed_dim=384, depth=1, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02)\n",
        "\n",
        "batch = 4\n",
        "img = torch.rand(batch, 3, 224, 224)\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/train.py\n",
        "mask_collator = MaskCollator(\n",
        "        input_size=(224, 224),\n",
        "        patch_size=16,\n",
        "        enc_mask_scale=(0.2, 0.8),\n",
        "        pred_mask_scale=(0.2, 0.8),\n",
        "        aspect_ratio=(0.3, 3.0),\n",
        "        nenc=1,\n",
        "        npred=4,\n",
        "        min_keep=4,\n",
        "        # allow_overlap=True)\n",
        "        allow_overlap=False)\n",
        "# self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
        "\n",
        "\n",
        "\n",
        "# v = mask_collator.step()\n",
        "# should pass the collater a list batch of idx?\n",
        "# collated_batch, collated_masks_enc, collated_masks_pred = mask_collator([batch,3,8])\n",
        "collated_batch, collated_masks_enc, collated_masks_pred = mask_collator([0,1,2,3])\n",
        "# print(v)\n",
        "\n",
        "\n",
        "def forward_target():\n",
        "    with torch.no_grad():\n",
        "        h = target_encoder(imgs)\n",
        "        h = F.layer_norm(h, (h.size(-1),))  # normalize over feature-dim\n",
        "        B = len(h)\n",
        "        # -- create targets (masked regions of h)\n",
        "        h = apply_masks(h, masks_pred)\n",
        "        h = repeat_interleave_batch(h, B, repeat=len(masks_enc))\n",
        "        return h\n",
        "\n",
        "def forward_context():\n",
        "    z = encoder(imgs, masks_enc)\n",
        "    z = predictor(z, masks_enc, masks_pred)\n",
        "    return z\n",
        "\n",
        "# # num_context_patches = 121 if allow_overlap=True else 11\n",
        "z = encoder(img, collated_masks_enc) # [batch, num_context_patches, embed_dim]\n",
        "# print(len(collated_masks_enc), len(collated_masks_pred)) # nenc, npred\n",
        "# print(collated_masks_enc[0].shape, collated_masks_pred[0].shape) # , [batch, num_context_patches = 121 or 11], [batch, num_target_patches]\n",
        "out = model(z, collated_masks_enc, collated_masks_pred)\n",
        "# # print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Or3eQvUVg7l_"
      },
      "outputs": [],
      "source": [
        "\n",
        "# print(224/16) # 14\n",
        "# print(len(collated_masks_enc), len(collated_masks_pred))\n",
        "print(collated_masks_enc, collated_masks_pred)\n",
        "# multiples of 14\n",
        "\n",
        "# print(collated_masks_pred[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2mLjXAmXcwJ"
      },
      "outputs": [],
      "source": [
        "print(collated_masks_enc, collated_masks_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XQ7PxBMlsYCI"
      },
      "outputs": [],
      "source": [
        "# @title ijepa multiblock.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "import math\n",
        "from multiprocessing import Value\n",
        "from logging import getLogger\n",
        "import torch\n",
        "_GLOBAL_SEED = 0\n",
        "logger = getLogger()\n",
        "\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=(224, 224),\n",
        "        patch_size=16,\n",
        "        enc_mask_scale=(0.2, 0.8),\n",
        "        pred_mask_scale=(0.2, 0.8),\n",
        "        aspect_ratio=(0.3, 3.0),\n",
        "        nenc=1,\n",
        "        npred=2,\n",
        "        min_keep=4,\n",
        "        allow_overlap=False\n",
        "    ):\n",
        "        super(MaskCollator, self).__init__()\n",
        "        if not isinstance(input_size, tuple):\n",
        "            input_size = (input_size, ) * 2\n",
        "        self.patch_size = patch_size\n",
        "        self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.aspect_ratio = aspect_ratio\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "        self._itr_counter = Value('i', -1)  # collator is shared across worker processes\n",
        "\n",
        "    def step(self):\n",
        "        i = self._itr_counter\n",
        "        with i.get_lock():\n",
        "            i.value += 1\n",
        "            v = i.value\n",
        "        return v\n",
        "\n",
        "    def _sample_block_size(self, generator, scale, aspect_ratio_scale):\n",
        "        _rand = torch.rand(1, generator=generator).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.height * self.width * mask_scale)\n",
        "        # -- Sample block aspect-ratio\n",
        "        min_ar, max_ar = aspect_ratio_scale\n",
        "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
        "        while h >= self.height:\n",
        "            h -= 1\n",
        "        while w >= self.width:\n",
        "            w -= 1\n",
        "\n",
        "        return (h, w)\n",
        "\n",
        "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
        "        h, w = b_size\n",
        "\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # --\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            top = torch.randint(0, self.height - h, (1,))\n",
        "            left = torch.randint(0, self.width - w, (1,))\n",
        "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
        "            mask[top:top+h, left:left+w] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "                    logger.warning(f'Mask generator says: \"Valid mask not found, decreasing acceptable-regions [{tries}]\"')\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
        "        mask_complement[top:top+h, left:left+w] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        B = len(batch)\n",
        "\n",
        "        collated_batch = torch.utils.data.default_collate(batch)\n",
        "\n",
        "        seed = self.step()\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(seed)\n",
        "        p_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.pred_mask_scale,\n",
        "            aspect_ratio_scale=self.aspect_ratio)\n",
        "        e_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.enc_mask_scale,\n",
        "            aspect_ratio_scale=(1., 1.))\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.height * self.width\n",
        "        min_keep_enc = self.height * self.width\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                logger.warning(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "\n",
        "        return collated_batch, collated_masks_enc, collated_masks_pred\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "iVUPGP93dpAN"
      },
      "outputs": [],
      "source": [
        "# @title ijepa train.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/train.py\n",
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "#\n",
        "\n",
        "import os\n",
        "\n",
        "# -- FOR DISTRIBUTED TRAINING ENSURE ONLY 1 DEVICE VISIBLE PER PROCESS\n",
        "try:\n",
        "    # -- WARNING: IF DOING DISTRIBUTED TRAINING ON A NON-SLURM CLUSTER, MAKE\n",
        "    # --          SURE TO UPDATE THIS TO GET LOCAL-RANK ON NODE, OR ENSURE\n",
        "    # --          THAT YOUR JOBS ARE LAUNCHED WITH ONLY 1 DEVICE VISIBLE\n",
        "    # --          TO EACH PROCESS\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = os.environ['SLURM_LOCALID']\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "import copy\n",
        "import logging\n",
        "import sys\n",
        "import yaml\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.multiprocessing as mp\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "\n",
        "from src.masks.multiblock import MaskCollator as MBMaskCollator\n",
        "from src.masks.utils import apply_masks\n",
        "from src.utils.distributed import (\n",
        "    init_distributed,\n",
        "    AllReduce\n",
        ")\n",
        "from src.utils.logging import (\n",
        "    CSVLogger,\n",
        "    gpu_timer,\n",
        "    grad_logger,\n",
        "    AverageMeter)\n",
        "from src.utils.tensors import repeat_interleave_batch\n",
        "from src.datasets.imagenet1k import make_imagenet1k\n",
        "\n",
        "from src.helper import (\n",
        "    load_checkpoint,\n",
        "    init_model,\n",
        "    init_opt)\n",
        "from src.transforms import make_transforms\n",
        "\n",
        "# --\n",
        "log_timings = True\n",
        "log_freq = 10\n",
        "checkpoint_freq = 50\n",
        "# --\n",
        "\n",
        "_GLOBAL_SEED = 0\n",
        "np.random.seed(_GLOBAL_SEED)\n",
        "torch.manual_seed(_GLOBAL_SEED)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logger = logging.getLogger()\n",
        "\n",
        "\n",
        "def main(args, resume_preempt=False):\n",
        "\n",
        "    # ----------------------------------------------------------------------- #\n",
        "    #  PASSED IN PARAMS FROM CONFIG FILE\n",
        "    # ----------------------------------------------------------------------- #\n",
        "\n",
        "    # -- META\n",
        "    use_bfloat16 = args['meta']['use_bfloat16']\n",
        "    model_name = args['meta']['model_name']\n",
        "    load_model = args['meta']['load_checkpoint'] or resume_preempt\n",
        "    r_file = args['meta']['read_checkpoint']\n",
        "    copy_data = args['meta']['copy_data']\n",
        "    pred_depth = args['meta']['pred_depth']\n",
        "    pred_emb_dim = args['meta']['pred_emb_dim']\n",
        "    if not torch.cuda.is_available():\n",
        "        device = torch.device('cpu')\n",
        "    else:\n",
        "        device = torch.device('cuda:0')\n",
        "        torch.cuda.set_device(device)\n",
        "\n",
        "    # -- DATA\n",
        "    use_gaussian_blur = args['data']['use_gaussian_blur']\n",
        "    use_horizontal_flip = args['data']['use_horizontal_flip']\n",
        "    use_color_distortion = args['data']['use_color_distortion']\n",
        "    color_jitter = args['data']['color_jitter_strength']\n",
        "    # --\n",
        "    batch_size = args['data']['batch_size']\n",
        "    pin_mem = args['data']['pin_mem']\n",
        "    num_workers = args['data']['num_workers']\n",
        "    root_path = args['data']['root_path']\n",
        "    image_folder = args['data']['image_folder']\n",
        "    crop_size = args['data']['crop_size']\n",
        "    crop_scale = args['data']['crop_scale']\n",
        "    # --\n",
        "\n",
        "    # -- MASK\n",
        "    allow_overlap = args['mask']['allow_overlap']  # whether to allow overlap b/w context and target blocks\n",
        "    patch_size = args['mask']['patch_size']  # patch-size for model training\n",
        "    num_enc_masks = args['mask']['num_enc_masks']  # number of context blocks\n",
        "    min_keep = args['mask']['min_keep']  # min number of patches in context block\n",
        "    enc_mask_scale = args['mask']['enc_mask_scale']  # scale of context blocks\n",
        "    num_pred_masks = args['mask']['num_pred_masks']  # number of target blocks\n",
        "    pred_mask_scale = args['mask']['pred_mask_scale']  # scale of target blocks\n",
        "    aspect_ratio = args['mask']['aspect_ratio']  # aspect ratio of target blocks\n",
        "    # --\n",
        "\n",
        "    # -- OPTIMIZATION\n",
        "    ema = args['optimization']['ema']\n",
        "    ipe_scale = args['optimization']['ipe_scale']  # scheduler scale factor (def: 1.0)\n",
        "    wd = float(args['optimization']['weight_decay'])\n",
        "    final_wd = float(args['optimization']['final_weight_decay'])\n",
        "    num_epochs = args['optimization']['epochs']\n",
        "    warmup = args['optimization']['warmup']\n",
        "    start_lr = args['optimization']['start_lr']\n",
        "    lr = args['optimization']['lr']\n",
        "    final_lr = args['optimization']['final_lr']\n",
        "\n",
        "    # -- LOGGING\n",
        "    folder = args['logging']['folder']\n",
        "    tag = args['logging']['write_tag']\n",
        "\n",
        "    dump = os.path.join(folder, 'params-ijepa.yaml')\n",
        "    with open(dump, 'w') as f:\n",
        "        yaml.dump(args, f)\n",
        "    # ----------------------------------------------------------------------- #\n",
        "\n",
        "    try:\n",
        "        mp.set_start_method('spawn')\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # -- init torch distributed backend\n",
        "    world_size, rank = init_distributed()\n",
        "    logger.info(f'Initialized (rank/world-size) {rank}/{world_size}')\n",
        "    if rank > 0:\n",
        "        logger.setLevel(logging.ERROR)\n",
        "\n",
        "    # -- log/checkpointing paths\n",
        "    log_file = os.path.join(folder, f'{tag}_r{rank}.csv')\n",
        "    save_path = os.path.join(folder, f'{tag}' + '-ep{epoch}.pth.tar')\n",
        "    latest_path = os.path.join(folder, f'{tag}-latest.pth.tar')\n",
        "    load_path = None\n",
        "    if load_model:\n",
        "        load_path = os.path.join(folder, r_file) if r_file is not None else latest_path\n",
        "\n",
        "    # -- make csv_logger\n",
        "    csv_logger = CSVLogger(log_file,\n",
        "                           ('%d', 'epoch'),\n",
        "                           ('%d', 'itr'),\n",
        "                           ('%.5f', 'loss'),\n",
        "                           ('%.5f', 'mask-A'),\n",
        "                           ('%.5f', 'mask-B'),\n",
        "                           ('%d', 'time (ms)'))\n",
        "\n",
        "    # -- init model\n",
        "    encoder, predictor = init_model(\n",
        "        device=device,\n",
        "        patch_size=patch_size,\n",
        "        crop_size=crop_size,\n",
        "        pred_depth=pred_depth,\n",
        "        pred_emb_dim=pred_emb_dim,\n",
        "        model_name=model_name)\n",
        "    target_encoder = copy.deepcopy(encoder)\n",
        "\n",
        "    # -- make data transforms\n",
        "    mask_collator = MBMaskCollator(\n",
        "        input_size=crop_size,\n",
        "        patch_size=patch_size,\n",
        "        pred_mask_scale=pred_mask_scale,\n",
        "        enc_mask_scale=enc_mask_scale,\n",
        "        aspect_ratio=aspect_ratio,\n",
        "        nenc=num_enc_masks,\n",
        "        npred=num_pred_masks,\n",
        "        allow_overlap=allow_overlap,\n",
        "        min_keep=min_keep)\n",
        "\n",
        "    transform = make_transforms(\n",
        "        crop_size=crop_size,\n",
        "        crop_scale=crop_scale,\n",
        "        gaussian_blur=use_gaussian_blur,\n",
        "        horizontal_flip=use_horizontal_flip,\n",
        "        color_distortion=use_color_distortion,\n",
        "        color_jitter=color_jitter)\n",
        "\n",
        "    # -- init data-loaders/samplers\n",
        "    _, unsupervised_loader, unsupervised_sampler = make_imagenet1k(\n",
        "            transform=transform,\n",
        "            batch_size=batch_size,\n",
        "            collator=mask_collator,\n",
        "            pin_mem=pin_mem,\n",
        "            training=True,\n",
        "            num_workers=num_workers,\n",
        "            world_size=world_size,\n",
        "            rank=rank,\n",
        "            root_path=root_path,\n",
        "            image_folder=image_folder,\n",
        "            copy_data=copy_data,\n",
        "            drop_last=True)\n",
        "    ipe = len(unsupervised_loader)\n",
        "\n",
        "    # -- init optimizer and scheduler\n",
        "    optimizer, scaler, scheduler, wd_scheduler = init_opt(\n",
        "        encoder=encoder,\n",
        "        predictor=predictor,\n",
        "        wd=wd,\n",
        "        final_wd=final_wd,\n",
        "        start_lr=start_lr,\n",
        "        ref_lr=lr,\n",
        "        final_lr=final_lr,\n",
        "        iterations_per_epoch=ipe,\n",
        "        warmup=warmup,\n",
        "        num_epochs=num_epochs,\n",
        "        ipe_scale=ipe_scale,\n",
        "        use_bfloat16=use_bfloat16)\n",
        "    encoder = DistributedDataParallel(encoder, static_graph=True)\n",
        "    predictor = DistributedDataParallel(predictor, static_graph=True)\n",
        "    target_encoder = DistributedDataParallel(target_encoder)\n",
        "    for p in target_encoder.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    # -- momentum schedule\n",
        "    momentum_scheduler = (ema[0] + i*(ema[1]-ema[0])/(ipe*num_epochs*ipe_scale)\n",
        "                          for i in range(int(ipe*num_epochs*ipe_scale)+1))\n",
        "\n",
        "    start_epoch = 0\n",
        "    # -- load training checkpoint\n",
        "    if load_model:\n",
        "        encoder, predictor, target_encoder, optimizer, scaler, start_epoch = load_checkpoint(\n",
        "            device=device,\n",
        "            r_path=load_path,\n",
        "            encoder=encoder,\n",
        "            predictor=predictor,\n",
        "            target_encoder=target_encoder,\n",
        "            opt=optimizer,\n",
        "            scaler=scaler)\n",
        "        for _ in range(start_epoch*ipe):\n",
        "            scheduler.step()\n",
        "            wd_scheduler.step()\n",
        "            next(momentum_scheduler)\n",
        "            mask_collator.step()\n",
        "\n",
        "    def save_checkpoint(epoch):\n",
        "        save_dict = {\n",
        "            'encoder': encoder.state_dict(),\n",
        "            'predictor': predictor.state_dict(),\n",
        "            'target_encoder': target_encoder.state_dict(),\n",
        "            'opt': optimizer.state_dict(),\n",
        "            'scaler': None if scaler is None else scaler.state_dict(),\n",
        "            'epoch': epoch,\n",
        "            'loss': loss_meter.avg,\n",
        "            'batch_size': batch_size,\n",
        "            'world_size': world_size,\n",
        "            'lr': lr\n",
        "        }\n",
        "        if rank == 0:\n",
        "            torch.save(save_dict, latest_path)\n",
        "            if (epoch + 1) % checkpoint_freq == 0:\n",
        "                torch.save(save_dict, save_path.format(epoch=f'{epoch + 1}'))\n",
        "\n",
        "    # -- TRAINING LOOP\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        logger.info('Epoch %d' % (epoch + 1))\n",
        "\n",
        "        # -- update distributed-data-loader epoch\n",
        "        unsupervised_sampler.set_epoch(epoch)\n",
        "\n",
        "        loss_meter = AverageMeter()\n",
        "        maskA_meter = AverageMeter()\n",
        "        maskB_meter = AverageMeter()\n",
        "        time_meter = AverageMeter()\n",
        "\n",
        "        for itr, (udata, masks_enc, masks_pred) in enumerate(unsupervised_loader):\n",
        "\n",
        "            def load_imgs():\n",
        "                # -- unsupervised imgs\n",
        "                imgs = udata[0].to(device, non_blocking=True)\n",
        "                masks_1 = [u.to(device, non_blocking=True) for u in masks_enc]\n",
        "                masks_2 = [u.to(device, non_blocking=True) for u in masks_pred]\n",
        "                return (imgs, masks_1, masks_2)\n",
        "            imgs, masks_enc, masks_pred = load_imgs()\n",
        "            maskA_meter.update(len(masks_enc[0][0]))\n",
        "            maskB_meter.update(len(masks_pred[0][0]))\n",
        "\n",
        "            def train_step():\n",
        "                _new_lr = scheduler.step()\n",
        "                _new_wd = wd_scheduler.step()\n",
        "                # --\n",
        "\n",
        "                def forward_target():\n",
        "                    with torch.no_grad():\n",
        "                        h = target_encoder(imgs)\n",
        "                        h = F.layer_norm(h, (h.size(-1),))  # normalize over feature-dim\n",
        "                        B = len(h)\n",
        "                        # -- create targets (masked regions of h)\n",
        "                        h = apply_masks(h, masks_pred)\n",
        "                        h = repeat_interleave_batch(h, B, repeat=len(masks_enc))\n",
        "                        return h\n",
        "\n",
        "                def forward_context():\n",
        "                    z = encoder(imgs, masks_enc)\n",
        "                    z = predictor(z, masks_enc, masks_pred)\n",
        "                    return z\n",
        "\n",
        "                def loss_fn(z, h):\n",
        "                    loss = F.smooth_l1_loss(z, h)\n",
        "                    loss = AllReduce.apply(loss)\n",
        "                    return loss\n",
        "\n",
        "                # Step 1. Forward\n",
        "                with torch.cuda.amp.autocast(dtype=torch.bfloat16, enabled=use_bfloat16):\n",
        "                    h = forward_target()\n",
        "                    z = forward_context()\n",
        "                    loss = loss_fn(z, h)\n",
        "\n",
        "                #  Step 2. Backward & step\n",
        "                if use_bfloat16:\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                else:\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                grad_stats = grad_logger(encoder.named_parameters())\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Step 3. momentum update of target encoder\n",
        "                with torch.no_grad():\n",
        "                    m = next(momentum_scheduler)\n",
        "                    for param_q, param_k in zip(encoder.parameters(), target_encoder.parameters()):\n",
        "                        param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "\n",
        "                return (float(loss), _new_lr, _new_wd, grad_stats)\n",
        "            (loss, _new_lr, _new_wd, grad_stats), etime = gpu_timer(train_step)\n",
        "            loss_meter.update(loss)\n",
        "            time_meter.update(etime)\n",
        "\n",
        "            # -- Logging\n",
        "            def log_stats():\n",
        "                csv_logger.log(epoch + 1, itr, loss, maskA_meter.val, maskB_meter.val, etime)\n",
        "                if (itr % log_freq == 0) or np.isnan(loss) or np.isinf(loss):\n",
        "                    logger.info('[%d, %5d] loss: %.3f '\n",
        "                                'masks: %.1f %.1f '\n",
        "                                '[wd: %.2e] [lr: %.2e] '\n",
        "                                '[mem: %.2e] '\n",
        "                                '(%.1f ms)'\n",
        "                                % (epoch + 1, itr,\n",
        "                                   loss_meter.avg,\n",
        "                                   maskA_meter.avg,\n",
        "                                   maskB_meter.avg,\n",
        "                                   _new_wd,\n",
        "                                   _new_lr,\n",
        "                                   torch.cuda.max_memory_allocated() / 1024.**2,\n",
        "                                   time_meter.avg))\n",
        "\n",
        "                    if grad_stats is not None:\n",
        "                        logger.info('[%d, %5d] grad_stats: [%.2e %.2e] (%.2e, %.2e)'\n",
        "                                    % (epoch + 1, itr,\n",
        "                                       grad_stats.first_layer,\n",
        "                                       grad_stats.last_layer,\n",
        "                                       grad_stats.min,\n",
        "                                       grad_stats.max))\n",
        "\n",
        "            log_stats()\n",
        "\n",
        "            assert not np.isnan(loss), 'loss is nan'\n",
        "\n",
        "        # -- Save Checkpoint after every epoch\n",
        "        logger.info('avg. loss %.3f' % loss_meter.avg)\n",
        "        save_checkpoint(epoch+1)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "s3EO3PgMPH1x"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}