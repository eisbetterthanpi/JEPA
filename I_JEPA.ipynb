{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/I_JEPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "LxACli7GdyGq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a6b9e8c-d71f-440c-d372-6450415f14cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:18<00:00, 9.12MB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title data\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "transform = transforms.Compose([transforms.ToTensor(),])\n",
        "# transform = transforms.Compose([transforms.RandomResizedCrop((32,32), scale=(.3,1.)), transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "# CIFAR10: 60000 32x32 color images in 10 classes, with 6000 images per class\n",
        "train_data = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
        "test_data = torchvision.datasets.CIFAR10(root='data', train=False, download=True, transform=transform)\n",
        "batch_size = 64 # 4\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# dataiter = iter(train_loader) # get some random training images\n",
        "# images, labels = next(dataiter)\n",
        "# print(images.shape) # [batch, 3, 32, 32]\n",
        "# imshow(torchvision.utils.make_grid(images))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title RoPE2D & RotEmb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "def RoPE(dim, seq_len=512, base=10000):\n",
        "    theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "    pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "    angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "    rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [1, seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "    return rot_emb\n",
        "\n",
        "# class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "#     def __init__(self, dim, seq_len=512, base=10000):\n",
        "#         super().__init__()\n",
        "#         self.dim, self.base = dim, base\n",
        "#         theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "#         pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "#         angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "#         self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [1, seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "#     def forward(self, x): # [batch, T, dim] / [batch, T, n_heads, d_head]\n",
        "#         seq_len = x.size(1)\n",
        "#         if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.base)\n",
        "#         # print('rope fwd', x.shape, self.rot_emb.shape)\n",
        "#         if x.dim()==4: return x * self.rot_emb[:,:seq_len].unsqueeze(2)\n",
        "#         return x * self.rot_emb[:,:seq_len]\n",
        "\n",
        "\n",
        "def RoPE2D(dim=16, h=8, w=8, base=10000):\n",
        "    # theta = 1. / (base ** (torch.arange(0, dim, step=4) / dim))\n",
        "    # theta = 1. / (base**torch.linspace(0,1,dim//4)).unsqueeze(0)\n",
        "    theta = 1. / (base**torch.linspace(0,1,dim//2)).unsqueeze(0)\n",
        "    y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\") # print(y,x) # [h,w], y:row_num, x:col_num\n",
        "    y, x = (y.reshape(-1,1) * theta).unsqueeze(-1), (x.reshape(-1,1) * theta).unsqueeze(-1) # [h*w,1]*[1,dim//4] = [h*w, dim//4, 1]\n",
        "    # rot_emb = torch.cat([x.sin(), x.cos(), y.sin(), y.cos()], dim=-1).flatten(-2) # [h*w, dim//4 ,4] -> [h*w, dim]\n",
        "    # rot_emb = torch.cat([x.sin(), x.cos(), y.sin(), y.cos()], dim=-1)#.reshape(dim, h, w).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "    rot_emb = torch.cat([x.sin(), y.sin()], dim=-1).flatten(-2).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "    # rot_emb = torch.cat([x.cos(), y.cos()], dim=-1).reshape(h, w, dim).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "    return rot_emb\n",
        "\n",
        "\n",
        "\n",
        "# class RoPE2D(nn.Module): # Rotary Positional Embeddings\n",
        "#     def __init__(self, dim, h=224, w=224, base=10000):\n",
        "#         super().__init__()\n",
        "#         self.dim, self.h, self.w = dim, h, w\n",
        "#         # # theta = 1. / (base ** (torch.arange(0, dim, step=4) / dim))\n",
        "#         # theta = 1. / (base**torch.linspace(0,1,dim//4)).unsqueeze(0)\n",
        "#         theta = 1. / (base**torch.linspace(0,1,dim//2)).unsqueeze(0)\n",
        "#         y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\") # print(y,x) # [h,w], y:row_num, x:col_num\n",
        "#         y, x = (y.reshape(-1,1) * theta).unsqueeze(-1), (x.reshape(-1,1) * theta).unsqueeze(-1) # [h*w,1]*[1,dim//4] = [h*w, dim//4, 1]\n",
        "#         # # self.rot_emb = torch.cat([x.sin(), x.cos(), y.sin(), y.cos()], dim=-1).flatten(-2) # [h*w, dim//4 ,4] -> [h*w, dim]\n",
        "#         # self.rot_emb = torch.cat([x.sin(), x.cos(), y.sin(), y.cos()], dim=-1).reshape(h, w, dim).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "#         self.rot_emb = torch.cat([x.sin(), y.sin()], dim=-1).reshape(h, w, dim).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "#         # self.rot_emb = torch.cat([x.cos(), y.cos()], dim=-1).reshape(h, w, dim).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "\n",
        "#     def forward(self, img): #\n",
        "#         # batch, dim, h, w = img.shape\n",
        "#         # print(img.shape)\n",
        "#         hw = img.shape[1] # [b, hw, dim] / [b, hw, n_heads, d_head]\n",
        "#         h=w=int(hw**.5)\n",
        "#         if self.h < h or self.w < w: self.__init__(self.dim, h, w)\n",
        "#         # print(self.rot_emb.shape)\n",
        "#         # rot_emb = self.rot_emb[:, :h, :w].unsqueeze(0) # [1, h, w, dim]\n",
        "#         rot_emb = self.rot_emb[:h, :w] # [h, w, dim]\n",
        "#         # return img * rot_emb.flatten(end_dim=1).unsqueeze(0) # [b, hw, dim] * [1, hw, dim]\n",
        "#         return img * rot_emb.flatten(end_dim=1)[None,:,None,:] # [b, hw, n_heads, d_head] * [1, hw, 1, dim]\n",
        "#         # return img * self.rot_emb\n",
        "\n",
        "class RotEmb(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, top=torch.pi, base=10000):\n",
        "        super().__init__()\n",
        "        self.theta = top / (base ** (torch.arange(0, dim, step=2, device=device) / dim))\n",
        "        # self.theta = top / (base ** torch.linspace(0, 1, dim//2, device=device))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (pos.unsqueeze(-1) * self.theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [seq_len, dim]\n"
      ],
      "metadata": {
        "id": "ge36SCxOl2Oq",
        "cellView": "form"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title AttentionBlock\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class SelfAttn(nn.Module):\n",
        "    def __init__(self, dim, n_heads):\n",
        "        super().__init__()\n",
        "        self.dim, self.n_heads = dim, n_heads\n",
        "        d_head = dim//n_heads\n",
        "        self.qkv = nn.Linear(dim, dim*3, bias=False)\n",
        "        # self.lin = nn.Linear(dim, dim)\n",
        "        self.lin = zero_module(nn.Linear(dim, dim))\n",
        "        # self.rope = RoPE(d_head, seq_len=512, base=10000)\n",
        "        # self.rope = RoPE2D(d_head, h=64, w=64, base=100)\n",
        "        self.scale = d_head**-.5\n",
        "\n",
        "    def forward(self, x): # [b,t,d]\n",
        "        q,k,v = self.qkv(x).unflatten(-1, (self.n_heads,-1)).transpose(1,2).chunk(3, dim=-1) # [b, t, n_heads, d_head] -> [b, n_heads, t, d_head]\n",
        "        # q, k = self.rope(q), self.rope(k)\n",
        "        q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2)\n",
        "        context = k.transpose(-2,-1) @ v # [batch, n_heads, d_head, d_head]\n",
        "        x = q @ context # [b, n_heads, t, d_head]\n",
        "        # print('SelfAttn', x.shape)\n",
        "        x = x.transpose(1,2).flatten(2)\n",
        "        return self.lin(x)\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, mult=4, drop=0.):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        # self.norm = nn.RMSNorm(d_model, elementwise_affine=False) # LayerNorm RMSNorm\n",
        "        self.norm1, self.norm2 = nn.LayerNorm(d_model), nn.LayerNorm(d_model)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "        self.attn = SelfAttn(d_model, n_heads)\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        ff_dim=d_model*mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), nn.Dropout(drop), act, nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "            nn.RMSNorm(ff_dim), nn.Dropout(drop), act, zero_module(nn.Linear(ff_dim, d_model))\n",
        "            # nn.RMSNorm(d_model), act, nn.Dropout(drop), nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "            # nn.Linear(d_model, ff_dim), nn.GELU(), nn.Dropout(drop),\n",
        "            # nn.Linear(ff_dim, d_model), nn.Dropout(drop),\n",
        "        )\n",
        "\n",
        "    def forward(self, x): # [batch, seq_len, d_model]\n",
        "        # print('attnblk fwd',x.shape)\n",
        "        x = x + self.drop(self.attn(self.norm1(x)))\n",
        "        x = x + self.ff(x)\n",
        "        # x = x + self.drop(self.ff(self.norm2(x)))\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "f6T4F651kmGh",
        "cellView": "form"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title masks\n",
        "import torch\n",
        "\n",
        "def multiblock(seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "    return target_mask\n",
        "\n",
        "\n",
        "def multiblock2d(hw=(8,8), scale=(.15,.2), aspect_ratio=(.75,1.5), M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_aspect = torch.rand(1) * (aspect_ratio[1] - aspect_ratio[0]) + aspect_ratio[0] # in (min_s, max_s) # all blocks same size\n",
        "    mask_scale = torch.rand(1) * (scale[1] - scale[0]) + scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    h = (mask_scale/mask_aspect)**.5# h*(h*aspect) = scale\n",
        "    w = h * mask_aspect\n",
        "    h_pos, w_pos = torch.rand(M)*(1-w), torch.rand(M)*(1-h) # in (0, 1 - mask_len)\n",
        "    h_len, h_pos = (h*hw[0]).int(), h_pos*hw[0]\n",
        "    w_len, w_pos = (w*hw[1]).int(), w_pos*hw[1]\n",
        "    h_ind, w_ind = torch.arange(hw[0]).unsqueeze(0), torch.arange(hw[1]).unsqueeze(0) # [1, seq]\n",
        "    h_mask = (h_ind>=h_pos.unsqueeze(-1)) & (h_ind<(h_pos+h_len).unsqueeze(-1)) # [M, seq]\n",
        "    w_mask = (w_ind>=w_pos.unsqueeze(-1)) & (w_ind<(w_pos+w_len).unsqueeze(-1)) # [M, seq]\n",
        "    target_mask = h_mask.unsqueeze(-1) & w_mask.unsqueeze(-2) # [M, seq, seq]\n",
        "    return target_mask\n",
        "\n",
        "# https://arxiv.org/pdf/2210.07224\n",
        "def randpatch(seq, mask_size=8, gamma=0.9): # num patches of seq, mask patch size, masking ratio\n",
        "    # mask = torch.rand(seq//mask_size)<gamma\n",
        "    length = seq//mask_size\n",
        "    g = torch.normal(gamma, std=.1, size=(1,)).clamp(.5,.9)\n",
        "    # g = gamma\n",
        "    idx = torch.randperm(length)[:int(length*g)]\n",
        "    mask = torch.zeros(length, dtype=bool)\n",
        "    mask[idx] = True\n",
        "    mask = mask.repeat_interleave(mask_size, dim=-1)\n",
        "    return mask # [seq] , True -> mask\n",
        "\n",
        "\n",
        "import torch\n",
        "def apply_masks(x, mask): # [b,t,d], [mask_size] # https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "    mask_keep = mask.unsqueeze(-1).repeat(x.size(0), 1, x.size(-1)) # [batch,T,dim]\n",
        "    return torch.gather(x, dim=1, index=mask_keep) # [batch,mask_size,dim]\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "pQfM2fYvcTh6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title simplex\n",
        "!pip install -q opensimplex\n",
        "import opensimplex\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def simplexmask(hw=(8,8), scale=(.15,.2)):\n",
        "    ix = iy = np.linspace(0, 1, num=8)\n",
        "    ix, iy = ix+np.random.randint(1e10), iy+np.random.randint(1e10)\n",
        "    y=opensimplex.noise2array(ix, iy)\n",
        "    y = torch.from_numpy(y)\n",
        "    mask_scale = torch.rand(1) * (scale[1] - scale[0]) + scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    yy = y.flatten().sort()[0][int(hw[0]*hw[1]*mask_scale)]\n",
        "    mask = (y<=yy.item())\n",
        "    return mask # T/F [h,w]\n",
        "\n",
        "# mask = simplexmask(hw=(8,8), scale=(.6,.8))\n",
        "# mask = ~simplexmask(hw=(8,8), scale=(.85,1))\n",
        "\n",
        "# from matplotlib import pyplot as plt\n",
        "# # plt.pcolormesh(y)\n",
        "# plt.pcolormesh(mask)\n",
        "# plt.show()\n",
        "\n",
        "# print(mask)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Fh9o__m2-j7K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc2fee4c-deb1-442e-9460-77ad56bf98c2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/268.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.0/268.0 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title TransformerModel/Predictor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        # self.pos_emb = nn.Parameter(torch.randn(1, 8*8, d_model)*0.02)\n",
        "        # self.pos_emb = nn.Parameter(RoPE2D(dim=d_model, h=8, w=8, base=1000), requires_grad=False)\n",
        "        self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=8*8, base=10000), requires_grad=False)\n",
        "\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, n_heads) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.randn(1,1,d_model)*0.02) # randn zeros\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim)# if out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = x.shape\n",
        "        # x = x * self.pos_enc(context_indices)\n",
        "        # print(\"Trans pred\",x.shape, self.pos_emb.shape)\n",
        "        # print(\"Trans pred\",x.shape, self.pos_emb[:,context_indices].shape)\n",
        "        x = x + self.pos_emb[:,context_indices]\n",
        "\n",
        "        # pred_tokens = self.cls * self.pos_enc(trg_indices) # [M, num_trg_toks, d_model]\n",
        "        # pred_tokens = self.cls + self.positional_emb[0,trg_indices]\n",
        "        pred_tokens = self.cls + self.pos_emb[:,trg_indices]\n",
        "        # print(\"Trans pred\",pred_tokens.shape, self.cls.shape, self.positional_emb[:,trg_indices].shape)\n",
        "        pred_tokens = pred_tokens.repeat(batch, 1, 1) # [batch, num_trg_toks, d_model]\n",
        "        # print(pred_tokens.requires_grad)\n",
        "        # print(\"pred fwd\", x.shape, pred_tokens.shape)\n",
        "        x = torch.cat([x, pred_tokens], dim=1) # [batch, seq_len+num_trg_toks, d_model]\n",
        "        out = self.transformer(x)\n",
        "\n",
        "        out = self.norm(out)\n",
        "        out = out[:,seq:] # [batch, num_trg_toks, d_model]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, patch_size, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        # patch_size=4\n",
        "        self.embed = nn.Sequential(\n",
        "            nn.Conv2d(in_dim, d_model, patch_size, patch_size), # like patch\n",
        "            # nn.Conv2d(in_dim, d_model, kernel_size=7, stride=2, padding=7//2, bias=False), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "            # nn.Conv2d(in_dim, d_model, 3, 2, 3//2, bias=False), nn.BatchNorm2d(d_model), nn.ReLU(),\n",
        "            )\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        # self.pos_emb = nn.Parameter(torch.randn(1, 8*8, d_model)*0.02)\n",
        "        # self.pos_emb = nn.Parameter(RoPE2D(dim=d_model, h=8, w=8, base=1000), requires_grad=False)\n",
        "        self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=8*8, base=10000), requires_grad=False)\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, n_heads) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # x = self.embed(x.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "        x = self.embed(x).flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c] # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        # x = self.pos_enc(x)\n",
        "        # print(\"TransformerModel\",x.shape, self.pos_emb.shape)\n",
        "        x = x + self.pos_emb\n",
        "        if context_indices != None: x = x[:,context_indices]\n",
        "\n",
        "        x = self.transformer(x)\n",
        "        out = self.norm(x)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "batch, seq_len, d_model = 4,1024,32\n",
        "in_dim = 3\n",
        "patch_size = 4\n",
        "model = TransformerModel(patch_size, in_dim, d_model, n_heads=4, nlayers=4, drop=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x = torch.rand((batch, in_dim, 32, 32), device=device)\n",
        "# x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "# # # print(out)\n",
        "# model = TransformerPredictor(in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1).to(device)\n",
        "# out = model(out)\n",
        "# print(out.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a241b09c-51fe-4d0e-8360-9480de8ad4a9",
        "id": "M-zdjdJixtOu",
        "cellView": "form"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "52672\n",
            "torch.Size([4, 64, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title IJEPA\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class IJEPA(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, nlayers=2, n_heads=4):\n",
        "        super().__init__()\n",
        "        if out_dim is None: out_dim = d_model\n",
        "        self.patch_size = 4\n",
        "        self.student = TransformerModel(self.patch_size, in_dim, d_model, out_dim=out_dim, n_heads=n_heads, nlayers=nlayers, drop=0.)\n",
        "        # self.predicter = TransformerPredictor(out_dim, d_model//2, out_dim, n_heads=n_heads, nlayers=nlayers//2, drop=0.)\n",
        "        self.predicter = TransformerPredictor(out_dim, 3*d_model//8, out_dim, n_heads=4, nlayers=1, drop=0.)\n",
        "        import copy\n",
        "        # self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher = TransformerModel(self.patch_size, in_dim, d_model, out_dim=out_dim, n_heads=n_heads, nlayers=nlayers, drop=0.)\n",
        "        self.teacher.requires_grad_(False)\n",
        "        # self.classifier = nn.Linear(out_dim, 18) # 10 18\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]\n",
        "        # batch, seq, dim = x.shape\n",
        "        b,c,h,w = x.shape\n",
        "        # # print(x.shape)\n",
        "        # target_mask = multiblock(seq//self.patch_size, min_s=0.15, max_s=0.2, M=4).any(0) # best.2.3M4 og.15.2M4# mask out targets to be predicted # [M, seq]\n",
        "        # # target_mask = multiblock(seq//self.patch_size, min_s=0.2, max_s=0.3, M=4).any(0) # best.2.3M4 og.15.2M4# mask out targets to be predicted # [M, seq]\n",
        "        # # target_mask = randpatch(seq//self.patch_size, mask_size=8, gamma=.9) # 8.9 [seq]\n",
        "        # context_mask = ~multiblock(seq//self.patch_size, min_s=0.85, max_s=1., M=1)|target_mask # og .85,1.M1 # [1, seq], True->Mask\n",
        "\n",
        "        # target_mask = multiblock2d((8,8), scale=(.2,.3), aspect_ratio=(.75,1.5), M=4).any(0).unsqueeze(0) # [1,h,w], True->Mask\n",
        "        target_mask = simplexmask(hw=(8,8), scale=(.7,.8)).unsqueeze(0) # .6.8\n",
        "        context_mask = ~multiblock2d((8,8), scale=(.85,1), aspect_ratio=(1,1), M=1)|target_mask # [1,h,w], True->Mask\n",
        "        # context_mask = torch.zeros((1,8,8), dtype=bool)|target_mask # [1,h,w], True->Mask\n",
        "        # # context_mask = ~simplexmask(hw=(8,8), scale=(.85,1)).unsqueeze(0)|target_mask # [1,h,w]\n",
        "\n",
        "        # imshow(target_mask)\n",
        "        # imshow(context_mask)\n",
        "\n",
        "        target_mask, context_mask = target_mask.flatten(), context_mask.flatten() # [8*8]\n",
        "        target_mask, context_mask = target_mask.to(device), context_mask.to(device)\n",
        "        # print(target_mask.shape, context_mask.shape)\n",
        "        # print(target_mask)\n",
        "        # print(context_mask)\n",
        "\n",
        "        context_indices = (~context_mask).nonzero().squeeze(-1) # int idx [num_context_toks] , idx of context not masked\n",
        "        # print('ijepa loss context_indices',context_indices)\n",
        "        # print('ijepa loss x',x.shape)\n",
        "        sx = self.student(x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "        # print('ijepa loss sx',sx.shape)\n",
        "\n",
        "        trg_indices = target_mask.nonzero().squeeze(-1) # int idx [num_trg_toks] , idx of targets that are masked\n",
        "        # print(trg_indices.shape)\n",
        "        sy_ = self.predicter(sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "        sy_ = F.layer_norm(sy_, (sy_.size(-1),))\n",
        "        with torch.no_grad():\n",
        "            sy = self.teacher(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "            # print(sy.shape, sy[0,:,:6])\n",
        "            sy = sy[:,trg_indices] # nan bec len(trg_ind)==0 # print('loss sy',torch.isnan(sy).any())\n",
        "            # print(sy.shape, sy[0,:,:6])\n",
        "            # print(trg_indices)\n",
        "            sy = F.layer_norm(sy, (sy.size(-1),))\n",
        "        loss = F.mse_loss(sy, sy_)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        sx = self.student(x)\n",
        "        out = sx.mean(dim=1)\n",
        "        return out\n",
        "\n",
        "    # def classify(self, x): # [batch, T, 3]\n",
        "    #     sx = self.forward(x)\n",
        "    #     out = self.classifier(sx)\n",
        "    #     return out\n",
        "\n",
        "# min_s=0.15, max_s, M\n",
        "# trg.15.2M4 C.85 1\n",
        "\n",
        "ijepa = IJEPA(in_dim=3, d_model=32, out_dim=16, nlayers=4, n_heads=4).to(device)#.to(torch.float)\n",
        "# ijepa = IJEPA(in_dim=3, d_model=1024, out_dim=16, nlayers=12, d_head=16).to(device)#.to(torch.float)\n",
        "# optim = torch.optim.AdamW(ijepa.parameters(), lr=1e-3) # 1e-3?\n",
        "optim = torch.optim.AdamW([{'params': ijepa.student.parameters()},\n",
        "    # {'params': ijepa.predicter.parameters(), 'lr': 1e-2}], lr=1e-3)#, weight_decay=0) default 1e-2\n",
        "    {'params': ijepa.predicter.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # default 1e-2, 5e-2\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/configs/in1k_vith14_ep300.yaml\n",
        "# d_model 1024,384\n",
        "# depth 12,6/12\n",
        "# wd 5e-2 - 4e-1\n",
        "# adamw 1e-4 - 1e-3 - 1e-6\n",
        "# ema 0.996-1\n",
        "\n",
        "print(sum(p.numel() for p in ijepa.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in ijepa.parameters())) # 27584\n",
        "# print(sum(p.numel() for p in ijepa.predicter.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in ijepa.student.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in ijepa.teacher.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# d_model^2 * nlayers\n",
        "\n",
        "x = torch.rand((2,3,32,32), device=device)\n",
        "out = ijepa.loss(x)\n",
        "print(out.shape)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(16).to(device)\n",
        "# classifier = Classifier(16, 18).to(device)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n",
        "# optim = torch.optim.AdamW([{'params': ijepa.parameters()}, {'params': classifier.parameters(), 'lr': 1e-3}], lr=1e-3)\n",
        "\n"
      ],
      "metadata": {
        "id": "ardu1zJwdHM3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8661043-76ce-4a7b-bd1d-49b87bda9132"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "55544\n",
            "torch.Size([])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title TransformerVICReg\n",
        "import torch\n",
        "from torch import nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class TransformerVICReg(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, drop=0):\n",
        "        super().__init__()\n",
        "        act = nn.GELU()\n",
        "        patch_size=4\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Linear(in_dim, d_model), act\n",
        "            # nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            # nn.Conv1d(d_model, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            # nn.Conv1d(d_model, d_model,3,2,3//2),\n",
        "            nn.Conv2d(in_dim, d_model, patch_size, patch_size), # patch\n",
        "            )\n",
        "        self.pos_enc = RoPE(d_model, seq_len=200, base=10000)\n",
        "        # self.pos_emb = nn.Parameter(torch.randn(1, 8*8, d_model))\n",
        "        # self.pos_emb = nn.Parameter(RoPE2D(dim=d_model, h=8, w=8, base=10000).unsqueeze(0), requires_grad=False)\n",
        "\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "\n",
        "        # out_dim = out_dim or d_model\n",
        "        self.lin = nn.Linear(d_model, out_dim)\n",
        "        self.attn_pool = nn.Linear(d_model, 1, bias=False)\n",
        "\n",
        "        dim_v = d_model * 4\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(out_dim, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v, bias=False),\n",
        "            )\n",
        "\n",
        "    def forward(self, x): # [b,t,d] / [b,c,h,w]\n",
        "        x = self.embed(x).flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c]\n",
        "        # x = self.embed(x.transpose(-2,-1)).transpose(-2,-1) # [b,t,d]\n",
        "        x = self.pos_enc(x)\n",
        "        # x = x + self.pos_emb\n",
        "\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        attn = self.attn_pool(x).squeeze(-1) # [batch, seq] # seq_pool\n",
        "        out = (torch.softmax(attn, dim=-1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, seq] @ [batch, seq, dim] -> [batch, dim]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch, ntoken]\n",
        "\n",
        "    def expand(self, x):\n",
        "        sx = self.forward(x)\n",
        "        vx = self.exp(sx)\n",
        "        return vx\n",
        "\n",
        "batch, seq_len, d_model = 4,3500,512\n",
        "in_dim, out_dim=3,16\n",
        "model = TransformerVICReg(in_dim, d_model, out_dim, d_head=4, nlayers=2, drop=0.).to(device)\n",
        "# x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "x =  torch.rand((batch, in_dim, 32,32), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ],
      "metadata": {
        "id": "ODpKypTCsfIt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2f4380b-03a8-49be-95b5-eed2b564dc05",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Violet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class Violet(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, nlayers=2, d_head=4):\n",
        "        super().__init__()\n",
        "        out_dim = out_dim or d_model\n",
        "        self.student = TransformerVICReg(in_dim, d_model, out_dim=out_dim, d_head=d_head, nlayers=nlayers, drop=0.)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "\n",
        "        # vicreg\n",
        "        self.sim_coeff=25. # 10.0 # 25.0 # λ\n",
        "        self.std_coeff=25. # 1.0 # 25.0 # µ\n",
        "        self.cov_coeff=1. # 25.0 # 1.0 # ν\n",
        "\n",
        "        self.classifier = nn.Linear(out_dim, 18) # 10 18\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]c/ [b,c,h,w]\n",
        "        # print(x.shape)\n",
        "        vx = self.student.expand(x) # [batch, num_context_toks, out_dim]\n",
        "        with torch.no_grad(): vy = self.teacher.expand(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "        loss = self.vicreg(vx, vy)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        return self.student(x)\n",
        "\n",
        "    def classify(self, x): # [batch, T, 3]\n",
        "        sx = self.forward(x)\n",
        "        out = self.classifier(sx)\n",
        "        return out\n",
        "\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size, num_features = x.shape\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = self.sim_coeff * repr_loss + self.std_coeff * std_loss + self.cov_coeff * cov_loss\n",
        "        print(\"in vicreg \",self.sim_coeff * repr_loss.item() , self.std_coeff * std_loss.item() , self.cov_coeff * cov_loss.item())\n",
        "        # return loss\n",
        "        return repr_loss, std_loss, cov_loss\n",
        "\n",
        "\n",
        "violet = Violet(in_dim=3, d_model=32, out_dim=16, nlayers=2, d_head=4).to(device)\n",
        "voptim = torch.optim.AdamW(violet.parameters(), lr=1e-3) # 1e-3?\n",
        "# voptim = torch.optim.AdamW([{'params': violet.student.transformer.parameters()},\n",
        "#     {'params': violet.student.exp.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # default 1e-2\n",
        "print(sum(p.numel() for p in violet.parameters() if p.requires_grad)) # 27584\n",
        "\n",
        "# x = torch.rand((2,1000,3), device=device)\n",
        "x = torch.rand((2,3,32,32), device=device)\n",
        "# x = torch.rand((2,1,16), device=device)\n",
        "loss = violet.loss(x)\n",
        "# print(out.shape)\n",
        "print(loss)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(16).to(device)\n",
        "# classifier = Classifier(16, 18).to(device)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n",
        "\n"
      ],
      "metadata": {
        "id": "5qwg9dG4sQ3h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f0092f5-5bb7-4725-db2b-dcf7b0ba1a96",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "62850\n",
            "in vicreg  0.00019920778413506923 24.74469393491745 4.793645480560826e-09\n",
            "(tensor(7.9683e-06, grad_fn=<MseLossBackward0>), tensor(0.9898, grad_fn=<AddBackward0>), tensor(4.7936e-09, grad_fn=<AddBackward0>))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in ijepa.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name, param)"
      ],
      "metadata": {
        "id": "m_BFm8Kh-j3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34de8800-fa4b-4460-803c-3cd8dbfb6c70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec8e36f2-fe9c-4449-9b73-9387fde21000",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "strain 0.36385244131088257\n",
            "strain 0.337560772895813\n",
            "strain 0.31995251774787903\n",
            "strain 0.4488065242767334\n",
            "strain 0.4592253267765045\n",
            "strain 0.37776777148246765\n",
            "strain 0.413584440946579\n",
            "classify 1.97503662109375\n",
            "classify 2.0474853515625\n",
            "classify 1.91357421875\n",
            "classify 1.9669189453125\n",
            "classify 2.072021484375\n",
            "classify 2.032958984375\n",
            "classify 1.95025634765625\n",
            "classify 1.98876953125\n",
            "classify 1.7940673828125\n",
            "classify 1.8330078125\n",
            "classify 1.8641357421875\n",
            "0.3125\n",
            "0.203125\n",
            "0.25\n",
            "0.3125\n",
            "0.171875\n",
            "0.234375\n",
            "0.25\n",
            "0.265625\n",
            "0.28125\n",
            "0.390625\n",
            "0.25\n",
            "335\n",
            "strain 0.3256820738315582\n",
            "strain 0.43757757544517517\n",
            "strain 0.251610666513443\n",
            "strain 0.27669206261634827\n",
            "strain 0.2922835350036621\n",
            "strain 0.4215180277824402\n",
            "strain 0.4378739893436432\n",
            "strain 0.28676918148994446\n",
            "strain 0.3654462397098541\n",
            "strain 0.3969525098800659\n",
            "strain 0.3485243618488312\n",
            "strain 0.29085883498191833\n",
            "strain 0.38598281145095825\n",
            "strain 0.37995612621307373\n",
            "strain 0.28794610500335693\n",
            "strain 0.41518551111221313\n",
            "strain 0.29241418838500977\n",
            "strain 0.26432526111602783\n",
            "strain 0.36872151494026184\n",
            "strain 0.29780709743499756\n",
            "strain 0.27712059020996094\n",
            "strain 0.40349966287612915\n",
            "strain 0.35415518283843994\n",
            "strain 0.35068103671073914\n",
            "strain 0.4584495425224304\n",
            "strain 0.27059751749038696\n",
            "strain 0.2367139309644699\n",
            "strain 0.29660388827323914\n",
            "strain 0.4112820625305176\n",
            "strain 0.30399730801582336\n",
            "strain 0.36124488711357117\n",
            "strain 0.28518322110176086\n",
            "strain 0.4096747636795044\n",
            "strain 0.4419982135295868\n",
            "strain 0.32545241713523865\n",
            "strain 0.4355846643447876\n",
            "strain 0.37655219435691833\n",
            "strain 0.3650335669517517\n",
            "strain 0.298007994890213\n",
            "strain 0.3586161136627197\n",
            "strain 0.2715436816215515\n",
            "strain 0.4453605115413666\n",
            "strain 0.40341126918792725\n",
            "strain 0.4388602375984192\n",
            "strain 0.3126479387283325\n",
            "strain 0.39603203535079956\n",
            "strain 0.29080167412757874\n",
            "strain 0.29244962334632874\n",
            "strain 0.32300353050231934\n",
            "strain 0.3097604215145111\n",
            "strain 0.3064194321632385\n",
            "classify 1.93701171875\n",
            "classify 1.943603515625\n",
            "classify 1.92236328125\n",
            "classify 1.930419921875\n",
            "classify 2.01556396484375\n",
            "classify 1.9066162109375\n",
            "classify 1.905029296875\n",
            "classify 2.0386962890625\n",
            "classify 1.9937744140625\n",
            "classify 1.941162109375\n",
            "classify 2.0106201171875\n",
            "0.296875\n",
            "0.203125\n",
            "0.34375\n",
            "0.3125\n",
            "0.3125\n",
            "0.234375\n",
            "0.265625\n",
            "0.21875\n",
            "0.375\n",
            "0.234375\n",
            "0.21875\n",
            "336\n",
            "strain 0.4635790288448334\n",
            "strain 0.41780418157577515\n",
            "strain 0.35011640191078186\n",
            "strain 0.4140769839286804\n",
            "strain 0.3594716787338257\n",
            "strain 0.4775390326976776\n",
            "strain 0.2966115474700928\n",
            "strain 0.2858886122703552\n",
            "strain 0.40622854232788086\n",
            "strain 0.289640873670578\n",
            "strain 0.2676074504852295\n",
            "strain 0.25768792629241943\n",
            "strain 0.4014917016029358\n",
            "strain 0.440778523683548\n",
            "strain 0.2563919126987457\n",
            "strain 0.3742671310901642\n",
            "strain 0.2987838089466095\n",
            "strain 0.4489469528198242\n",
            "strain 0.3144388794898987\n",
            "strain 0.31360867619514465\n",
            "strain 0.37959471344947815\n",
            "strain 0.3914828896522522\n",
            "strain 0.4369093179702759\n",
            "strain 0.41394171118736267\n",
            "strain 0.30436617136001587\n",
            "strain 0.3234001398086548\n",
            "strain 0.30523255467414856\n",
            "strain 0.28381913900375366\n",
            "strain 0.4857723116874695\n",
            "strain 0.3099679946899414\n",
            "strain 0.33196792006492615\n",
            "strain 0.31029751896858215\n",
            "strain 0.2967481315135956\n",
            "strain 0.4413832426071167\n",
            "strain 0.48363345861434937\n",
            "strain 0.36846962571144104\n",
            "strain 0.27272215485572815\n",
            "strain 0.41811811923980713\n",
            "strain 0.31941723823547363\n",
            "strain 0.3129757344722748\n",
            "strain 0.38032492995262146\n",
            "strain 0.363216757774353\n",
            "strain 0.3354036211967468\n",
            "strain 0.4834563434123993\n",
            "strain 0.3929027020931244\n",
            "strain 0.27086925506591797\n",
            "strain 0.37636488676071167\n",
            "strain 0.40139442682266235\n",
            "strain 0.4584288001060486\n",
            "strain 0.454638808965683\n",
            "strain 0.28981396555900574\n",
            "classify 1.97979736328125\n",
            "classify 1.95263671875\n",
            "classify 1.83111572265625\n",
            "classify 1.9560546875\n",
            "classify 1.9600830078125\n",
            "classify 1.998779296875\n",
            "classify 1.8975830078125\n",
            "classify 2.086669921875\n",
            "classify 1.9693603515625\n",
            "classify 1.95556640625\n",
            "classify 2.025390625\n",
            "0.21875\n",
            "0.3125\n",
            "0.21875\n",
            "0.296875\n",
            "0.34375\n",
            "0.25\n",
            "0.328125\n",
            "0.265625\n",
            "0.265625\n",
            "0.328125\n",
            "0.234375\n",
            "337\n",
            "strain 0.44042572379112244\n",
            "strain 0.3617270290851593\n",
            "strain 0.29216888546943665\n",
            "strain 0.33418458700180054\n",
            "strain 0.420878142118454\n",
            "strain 0.45726117491722107\n",
            "strain 0.4207059442996979\n",
            "strain 0.33319568634033203\n",
            "strain 0.33531343936920166\n",
            "strain 0.2871110737323761\n",
            "strain 0.5222094655036926\n",
            "strain 0.3564414978027344\n",
            "strain 0.3187215030193329\n",
            "strain 0.3916952610015869\n",
            "strain 0.2958044707775116\n",
            "strain 0.34854358434677124\n",
            "strain 0.40816745162010193\n",
            "strain 0.2904925048351288\n",
            "strain 0.2960329055786133\n",
            "strain 0.3437156677246094\n",
            "strain 0.38396063446998596\n",
            "strain 0.3284011781215668\n",
            "strain 0.3471148908138275\n",
            "strain 0.4446302652359009\n",
            "strain 0.3908226191997528\n",
            "strain 0.4381604492664337\n",
            "strain 0.44999268651008606\n",
            "strain 0.4768042266368866\n",
            "strain 0.25627413392066956\n",
            "strain 0.23640909790992737\n",
            "strain 0.4156864583492279\n",
            "strain 0.43278735876083374\n",
            "strain 0.2877543270587921\n",
            "strain 0.2725648581981659\n",
            "strain 0.3082033395767212\n",
            "strain 0.41153448820114136\n",
            "strain 0.4629147946834564\n",
            "strain 0.5140197277069092\n",
            "strain 0.40231963992118835\n",
            "strain 0.350809246301651\n",
            "strain 0.44978827238082886\n",
            "strain 0.2556318938732147\n",
            "strain 0.3198225498199463\n",
            "strain 0.2891775071620941\n",
            "strain 0.26722049713134766\n",
            "strain 0.27491429448127747\n",
            "strain 0.2938721776008606\n",
            "strain 0.3348461985588074\n",
            "strain 0.33627960085868835\n",
            "strain 0.4254216253757477\n",
            "strain 0.33806735277175903\n",
            "classify 1.927978515625\n",
            "classify 1.88427734375\n",
            "classify 1.93505859375\n",
            "classify 1.8935546875\n",
            "classify 1.986328125\n",
            "classify 2.031005859375\n",
            "classify 1.995361328125\n",
            "classify 1.951904296875\n",
            "classify 2.04345703125\n",
            "classify 2.097412109375\n",
            "classify 1.8184814453125\n",
            "0.28125\n",
            "0.28125\n",
            "0.28125\n",
            "0.296875\n",
            "0.25\n",
            "0.3125\n",
            "0.25\n",
            "0.25\n",
            "0.1875\n",
            "0.296875\n",
            "0.1875\n",
            "338\n",
            "strain 0.4121994972229004\n",
            "strain 0.3251400589942932\n",
            "strain 0.4010028541088104\n",
            "strain 0.32151293754577637\n",
            "strain 0.2813751697540283\n",
            "strain 0.477378249168396\n",
            "strain 0.5309523940086365\n",
            "strain 0.48139211535453796\n",
            "strain 0.3106136918067932\n",
            "strain 0.4106678068637848\n",
            "strain 0.41205134987831116\n",
            "strain 0.29586634039878845\n",
            "strain 0.43201974034309387\n",
            "strain 0.3637549877166748\n",
            "strain 0.30463796854019165\n",
            "strain 0.43180179595947266\n",
            "strain 0.3429225981235504\n",
            "strain 0.2961525321006775\n",
            "strain 0.3953244686126709\n",
            "strain 0.31965094804763794\n",
            "strain 0.3939780592918396\n",
            "strain 0.36046966910362244\n",
            "strain 0.39615708589553833\n",
            "strain 0.39066728949546814\n",
            "strain 0.3846037685871124\n",
            "strain 0.4478714168071747\n",
            "strain 0.301673948764801\n",
            "strain 0.3082576394081116\n",
            "strain 0.3541654944419861\n",
            "strain 0.3088519871234894\n",
            "strain 0.3092944920063019\n",
            "strain 0.3800976276397705\n",
            "strain 0.2709748148918152\n",
            "strain 0.2720196843147278\n",
            "strain 0.44232550263404846\n",
            "strain 0.3919808864593506\n",
            "strain 0.2894099950790405\n",
            "strain 0.3980269432067871\n",
            "strain 0.3467103838920593\n",
            "strain 0.2916555106639862\n",
            "strain 0.4133231043815613\n",
            "strain 0.3235537111759186\n",
            "strain 0.4460630714893341\n",
            "strain 0.3417867422103882\n",
            "strain 0.2789332866668701\n",
            "strain 0.28543418645858765\n",
            "strain 0.3076140284538269\n",
            "strain 0.36714187264442444\n",
            "strain 0.42958444356918335\n",
            "strain 0.3887011706829071\n",
            "strain 0.5396859049797058\n",
            "classify 1.96673583984375\n",
            "classify 1.980224609375\n",
            "classify 1.8475341796875\n",
            "classify 1.95855712890625\n",
            "classify 1.90576171875\n",
            "classify 2.043701171875\n",
            "classify 1.916748046875\n",
            "classify 1.88739013671875\n",
            "classify 1.9156494140625\n",
            "classify 2.00262451171875\n",
            "classify 1.85150146484375\n",
            "0.21875\n",
            "0.34375\n",
            "0.296875\n",
            "0.3125\n",
            "0.25\n",
            "0.3125\n",
            "0.265625\n",
            "0.46875\n",
            "0.25\n",
            "0.3125\n",
            "0.28125\n",
            "339\n",
            "strain 0.430558443069458\n",
            "strain 0.4259724020957947\n",
            "strain 0.3262149691581726\n",
            "strain 0.24731269478797913\n",
            "strain 0.29247239232063293\n",
            "strain 0.3407219350337982\n",
            "strain 0.31524670124053955\n",
            "strain 0.368855744600296\n",
            "strain 0.4522208571434021\n",
            "strain 0.25936436653137207\n",
            "strain 0.3382738530635834\n",
            "strain 0.37951064109802246\n",
            "strain 0.4359849691390991\n",
            "strain 0.301282674074173\n",
            "strain 0.34544336795806885\n",
            "strain 0.2733387053012848\n",
            "strain 0.3233565390110016\n",
            "strain 0.36465978622436523\n",
            "strain 0.4066838324069977\n",
            "strain 0.30591142177581787\n",
            "strain 0.3444328010082245\n",
            "strain 0.3507208824157715\n",
            "strain 0.36013713479042053\n",
            "strain 0.3732822835445404\n",
            "strain 0.35395175218582153\n",
            "strain 0.36541375517845154\n",
            "strain 0.4631361961364746\n",
            "strain 0.39910417795181274\n",
            "strain 0.3412508964538574\n",
            "strain 0.32971906661987305\n",
            "strain 0.38696521520614624\n",
            "strain 0.326375275850296\n",
            "strain 0.3774411678314209\n",
            "strain 0.4309483766555786\n",
            "strain 0.3540290892124176\n",
            "strain 0.3756442964076996\n",
            "strain 0.29635217785835266\n",
            "strain 0.310077965259552\n",
            "strain 0.4463447034358978\n",
            "strain 0.26860278844833374\n",
            "strain 0.2414848506450653\n",
            "strain 0.370855450630188\n",
            "strain 0.38443711400032043\n",
            "strain 0.4378257691860199\n",
            "strain 0.33212459087371826\n",
            "strain 0.35647305846214294\n",
            "strain 0.33466652035713196\n",
            "strain 0.32054799795150757\n",
            "strain 0.3545309901237488\n",
            "strain 0.34091895818710327\n",
            "strain 0.3694295883178711\n",
            "classify 1.99676513671875\n",
            "classify 1.9091796875\n",
            "classify 1.88519287109375\n",
            "classify 1.9775390625\n",
            "classify 1.98455810546875\n",
            "classify 1.9898681640625\n",
            "classify 2.10693359375\n",
            "classify 1.8917236328125\n",
            "classify 2.0394287109375\n",
            "classify 1.8984375\n",
            "classify 1.87493896484375\n",
            "0.296875\n",
            "0.296875\n",
            "0.28125\n",
            "0.34375\n",
            "0.28125\n",
            "0.28125\n",
            "0.328125\n",
            "0.25\n",
            "0.265625\n",
            "0.234375\n",
            "0.34375\n",
            "340\n",
            "strain 0.3402903378009796\n",
            "strain 0.49730780720710754\n",
            "strain 0.3272101879119873\n",
            "strain 0.3771776258945465\n",
            "strain 0.3248306214809418\n",
            "strain 0.416766881942749\n",
            "strain 0.25916218757629395\n",
            "strain 0.34165138006210327\n",
            "strain 0.42130404710769653\n",
            "strain 0.3582667112350464\n",
            "strain 0.4390042722225189\n",
            "strain 0.29280489683151245\n",
            "strain 0.43816694617271423\n",
            "strain 0.36441272497177124\n",
            "strain 0.3708399832248688\n",
            "strain 0.3367440402507782\n",
            "strain 0.4496009647846222\n",
            "strain 0.4734061062335968\n",
            "strain 0.3965631127357483\n",
            "strain 0.541027307510376\n",
            "strain 0.3356396555900574\n",
            "strain 0.2517075836658478\n",
            "strain 0.36032551527023315\n",
            "strain 0.26613757014274597\n",
            "strain 0.41307857632637024\n",
            "strain 0.40699997544288635\n",
            "strain 0.38219544291496277\n",
            "strain 0.38232243061065674\n",
            "strain 0.3392452895641327\n",
            "strain 0.3724478781223297\n",
            "strain 0.45106062293052673\n",
            "strain 0.29619958996772766\n",
            "strain 0.43137022852897644\n",
            "strain 0.40739622712135315\n",
            "strain 0.43018829822540283\n",
            "strain 0.3952721357345581\n",
            "strain 0.3476852774620056\n",
            "strain 0.31360092759132385\n",
            "strain 0.34499815106391907\n",
            "strain 0.28078243136405945\n",
            "strain 0.3403565287590027\n",
            "strain 0.25823378562927246\n",
            "strain 0.3194812536239624\n",
            "strain 0.42336103320121765\n",
            "strain 0.40632468461990356\n",
            "strain 0.3886217474937439\n",
            "strain 0.32439079880714417\n",
            "strain 0.27603092789649963\n",
            "strain 0.32184740900993347\n",
            "strain 0.37209150195121765\n",
            "strain 0.42106032371520996\n",
            "classify 1.9241943359375\n",
            "classify 1.9808349609375\n",
            "classify 1.9912109375\n",
            "classify 1.9632568359375\n",
            "classify 1.85394287109375\n",
            "classify 2.01202392578125\n",
            "classify 1.74139404296875\n",
            "classify 2.0911865234375\n",
            "classify 1.86773681640625\n",
            "classify 1.94573974609375\n",
            "classify 1.9559326171875\n",
            "0.25\n",
            "0.34375\n",
            "0.28125\n",
            "0.28125\n",
            "0.375\n",
            "0.171875\n",
            "0.359375\n",
            "0.28125\n",
            "0.359375\n",
            "0.265625\n",
            "0.1875\n",
            "341\n",
            "strain 0.35575446486473083\n",
            "strain 0.24109044671058655\n",
            "strain 0.3620193302631378\n",
            "strain 0.3106424808502197\n",
            "strain 0.30570197105407715\n",
            "strain 0.32649335265159607\n",
            "strain 0.35217007994651794\n",
            "strain 0.3538305163383484\n",
            "strain 0.3321569263935089\n",
            "strain 0.35719403624534607\n",
            "strain 0.3097068965435028\n",
            "strain 0.30826282501220703\n",
            "strain 0.3204103410243988\n",
            "strain 0.31239113211631775\n",
            "strain 0.3571503162384033\n",
            "strain 0.29863134026527405\n",
            "strain 0.2743833065032959\n",
            "strain 0.3228382468223572\n",
            "strain 0.4060091972351074\n",
            "strain 0.5140838027000427\n",
            "strain 0.21084387600421906\n",
            "strain 0.32624438405036926\n",
            "strain 0.3363865315914154\n",
            "strain 0.30946215987205505\n",
            "strain 0.3792692720890045\n",
            "strain 0.43549588322639465\n",
            "strain 0.3237547278404236\n",
            "strain 0.2880181670188904\n",
            "strain 0.2982648015022278\n",
            "strain 0.5029119849205017\n",
            "strain 0.36043229699134827\n",
            "strain 0.34062647819519043\n",
            "strain 0.34159937500953674\n",
            "strain 0.4891674518585205\n",
            "strain 0.4054146409034729\n",
            "strain 0.32545432448387146\n",
            "strain 0.3111148774623871\n",
            "strain 0.29794758558273315\n",
            "strain 0.3487149477005005\n",
            "strain 0.3458491265773773\n",
            "strain 0.3839288651943207\n",
            "strain 0.28673848509788513\n",
            "strain 0.3053515553474426\n",
            "strain 0.4053046703338623\n",
            "strain 0.39122840762138367\n",
            "strain 0.3879440724849701\n",
            "strain 0.4107080399990082\n",
            "strain 0.4424544870853424\n",
            "strain 0.45797213912010193\n",
            "strain 0.2765376567840576\n",
            "strain 0.32966193556785583\n",
            "classify 1.8592529296875\n",
            "classify 2.02325439453125\n",
            "classify 1.9544677734375\n",
            "classify 1.9996337890625\n",
            "classify 1.9993896484375\n",
            "classify 2.0484619140625\n",
            "classify 1.796875\n",
            "classify 1.857177734375\n",
            "classify 1.79351806640625\n",
            "classify 2.035888671875\n",
            "classify 1.94482421875\n",
            "0.28125\n",
            "0.328125\n",
            "0.234375\n",
            "0.296875\n",
            "0.3125\n",
            "0.328125\n",
            "0.3125\n",
            "0.265625\n",
            "0.203125\n",
            "0.265625\n",
            "0.171875\n",
            "342\n",
            "strain 0.3615875244140625\n",
            "strain 0.33178725838661194\n",
            "strain 0.3759710490703583\n",
            "strain 0.33282777667045593\n",
            "strain 0.3457019031047821\n",
            "strain 0.37298399209976196\n",
            "strain 0.383396714925766\n",
            "strain 0.30842581391334534\n",
            "strain 0.45963916182518005\n",
            "strain 0.273232638835907\n",
            "strain 0.3743407726287842\n",
            "strain 0.3051140010356903\n",
            "strain 0.41771990060806274\n",
            "strain 0.3925548791885376\n",
            "strain 0.4014553129673004\n",
            "strain 0.4669783413410187\n",
            "strain 0.365189790725708\n",
            "strain 0.4074670672416687\n",
            "strain 0.3109356760978699\n",
            "strain 0.3907221257686615\n",
            "strain 0.4410223960876465\n",
            "strain 0.29520338773727417\n",
            "strain 0.34965217113494873\n",
            "strain 0.3467450737953186\n",
            "strain 0.3597049117088318\n",
            "strain 0.3241940438747406\n",
            "strain 0.3394912779331207\n",
            "strain 0.4639948010444641\n",
            "strain 0.31268176436424255\n",
            "strain 0.3079933226108551\n",
            "strain 0.46599483489990234\n",
            "strain 0.42772015929222107\n",
            "strain 0.3549339473247528\n",
            "strain 0.40469545125961304\n",
            "strain 0.28233954310417175\n",
            "strain 0.48580434918403625\n",
            "strain 0.35351866483688354\n",
            "strain 0.29076552391052246\n",
            "strain 0.4255472421646118\n",
            "strain 0.3318674564361572\n",
            "strain 0.41296303272247314\n",
            "strain 0.35351860523223877\n",
            "strain 0.3217419385910034\n",
            "strain 0.4500734210014343\n",
            "strain 0.36602112650871277\n",
            "strain 0.3928714394569397\n",
            "strain 0.32395729422569275\n",
            "strain 0.3474905788898468\n",
            "strain 0.2734542787075043\n",
            "strain 0.46501424908638\n",
            "strain 0.3205223083496094\n",
            "classify 2.00067138671875\n",
            "classify 1.9534912109375\n",
            "classify 1.85626220703125\n",
            "classify 2.0950927734375\n",
            "classify 1.97235107421875\n",
            "classify 1.9407958984375\n",
            "classify 2.00299072265625\n",
            "classify 1.835693359375\n",
            "classify 1.9945068359375\n",
            "classify 1.9649658203125\n",
            "classify 1.88800048828125\n",
            "0.28125\n",
            "0.40625\n",
            "0.3125\n",
            "0.390625\n",
            "0.375\n",
            "0.234375\n",
            "0.46875\n",
            "0.25\n",
            "0.484375\n",
            "0.25\n",
            "0.25\n",
            "343\n",
            "strain 0.3785344958305359\n",
            "strain 0.3420046865940094\n",
            "strain 0.2733091711997986\n",
            "strain 0.40712597966194153\n",
            "strain 0.36544737219810486\n",
            "strain 0.29021579027175903\n",
            "strain 0.36150577664375305\n",
            "strain 0.3332592844963074\n",
            "strain 0.2773747444152832\n",
            "strain 0.49866539239883423\n",
            "strain 0.3416822850704193\n",
            "strain 0.3607558012008667\n",
            "strain 0.4101887345314026\n",
            "strain 0.3387831449508667\n",
            "strain 0.44903072714805603\n",
            "strain 0.43905726075172424\n",
            "strain 0.44723302125930786\n",
            "strain 0.26776430010795593\n",
            "strain 0.354818195104599\n",
            "strain 0.3917960226535797\n",
            "strain 0.29139459133148193\n",
            "strain 0.32809171080589294\n",
            "strain 0.3733748197555542\n",
            "strain 0.44391897320747375\n",
            "strain 0.3082342743873596\n",
            "strain 0.40049847960472107\n",
            "strain 0.3081532418727875\n",
            "strain 0.26395946741104126\n",
            "strain 0.31384533643722534\n",
            "strain 0.4108017683029175\n",
            "strain 0.42593076825141907\n",
            "strain 0.28103652596473694\n",
            "strain 0.29742303490638733\n",
            "strain 0.38451167941093445\n",
            "strain 0.30885636806488037\n",
            "strain 0.36107075214385986\n",
            "strain 0.38371121883392334\n",
            "strain 0.3377285897731781\n",
            "strain 0.34946203231811523\n",
            "strain 0.5779019594192505\n",
            "strain 0.48058292269706726\n",
            "strain 0.3519389033317566\n",
            "strain 0.4681296944618225\n",
            "strain 0.36777952313423157\n",
            "strain 0.3413189947605133\n",
            "strain 0.360220342874527\n",
            "strain 0.26998093724250793\n",
            "strain 0.3469752073287964\n",
            "strain 0.38877421617507935\n",
            "strain 0.3235119581222534\n",
            "strain 0.47962766885757446\n",
            "classify 1.994384765625\n",
            "classify 1.9349365234375\n",
            "classify 1.9317626953125\n",
            "classify 2.007568359375\n",
            "classify 2.10418701171875\n",
            "classify 1.9176025390625\n",
            "classify 1.982421875\n",
            "classify 1.8636474609375\n",
            "classify 1.9088134765625\n",
            "classify 1.9815673828125\n",
            "classify 1.84735107421875\n",
            "0.3125\n",
            "0.25\n",
            "0.234375\n",
            "0.265625\n",
            "0.328125\n",
            "0.28125\n",
            "0.265625\n",
            "0.34375\n",
            "0.171875\n",
            "0.328125\n",
            "0.25\n",
            "344\n",
            "strain 0.4151996076107025\n",
            "strain 0.3950653076171875\n",
            "strain 0.2823169231414795\n",
            "strain 0.23804636299610138\n",
            "strain 0.4016290605068207\n",
            "strain 0.30351021885871887\n",
            "strain 0.4375584125518799\n",
            "strain 0.4282068610191345\n",
            "strain 0.3289833664894104\n",
            "strain 0.3774179518222809\n",
            "strain 0.4154837429523468\n",
            "strain 0.38503527641296387\n",
            "strain 0.43013137578964233\n",
            "strain 0.38796645402908325\n",
            "strain 0.31017786264419556\n",
            "strain 0.3641829788684845\n",
            "strain 0.3760492205619812\n",
            "strain 0.4294075667858124\n",
            "strain 0.3611977994441986\n",
            "strain 0.3340579569339752\n",
            "strain 0.3242427706718445\n",
            "strain 0.33971595764160156\n",
            "strain 0.37286853790283203\n",
            "strain 0.42080414295196533\n",
            "strain 0.3304237425327301\n",
            "strain 0.2971629500389099\n",
            "strain 0.28671926259994507\n",
            "strain 0.33470797538757324\n",
            "strain 0.3450367748737335\n",
            "strain 0.3637295365333557\n",
            "strain 0.412671834230423\n",
            "strain 0.34015366435050964\n",
            "strain 0.2910866439342499\n",
            "strain 0.2923476994037628\n",
            "strain 0.39981383085250854\n",
            "strain 0.36958739161491394\n",
            "strain 0.2883926331996918\n",
            "strain 0.33857661485671997\n",
            "strain 0.40420201420783997\n",
            "strain 0.44065073132514954\n",
            "strain 0.3483576476573944\n",
            "strain 0.3259400725364685\n",
            "strain 0.4433756172657013\n",
            "strain 0.38899341225624084\n",
            "strain 0.342723548412323\n",
            "strain 0.37253257632255554\n",
            "strain 0.30456340312957764\n",
            "strain 0.36698415875434875\n",
            "strain 0.2792596220970154\n",
            "strain 0.42401307821273804\n",
            "strain 0.38495683670043945\n",
            "classify 2.0006103515625\n",
            "classify 2.1480712890625\n",
            "classify 1.9996337890625\n",
            "classify 1.88824462890625\n",
            "classify 1.9317626953125\n",
            "classify 1.89434814453125\n",
            "classify 2.039306640625\n",
            "classify 1.90325927734375\n",
            "classify 1.95623779296875\n",
            "classify 2.0450439453125\n",
            "classify 1.987060546875\n",
            "0.28125\n",
            "0.265625\n",
            "0.28125\n",
            "0.40625\n",
            "0.34375\n",
            "0.28125\n",
            "0.234375\n",
            "0.296875\n",
            "0.296875\n",
            "0.3125\n",
            "0.234375\n",
            "345\n",
            "strain 0.25963565707206726\n",
            "strain 0.315401166677475\n",
            "strain 0.36316367983818054\n",
            "strain 0.2975930869579315\n",
            "strain 0.41104233264923096\n",
            "strain 0.34785759449005127\n",
            "strain 0.2824168801307678\n",
            "strain 0.26668789982795715\n",
            "strain 0.4070567190647125\n",
            "strain 0.34552067518234253\n",
            "strain 0.4293012320995331\n",
            "strain 0.25356733798980713\n",
            "strain 0.2948647439479828\n",
            "strain 0.24709945917129517\n",
            "strain 0.2524043619632721\n",
            "strain 0.4320856034755707\n",
            "strain 0.44200849533081055\n",
            "strain 0.40276914834976196\n",
            "strain 0.43529292941093445\n",
            "strain 0.3396283984184265\n",
            "strain 0.4315180778503418\n",
            "strain 0.3274840712547302\n",
            "strain 0.29514795541763306\n",
            "strain 0.3880275785923004\n",
            "strain 0.322641521692276\n",
            "strain 0.3097535967826843\n",
            "strain 0.36686277389526367\n",
            "strain 0.34202900528907776\n",
            "strain 0.4539971947669983\n",
            "strain 0.2531437575817108\n",
            "strain 0.345337837934494\n",
            "strain 0.3400613069534302\n",
            "strain 0.4939267337322235\n",
            "strain 0.2992885112762451\n",
            "strain 0.2934473752975464\n",
            "strain 0.24897807836532593\n",
            "strain 0.442430317401886\n",
            "strain 0.35807207226753235\n",
            "strain 0.29669129848480225\n",
            "strain 0.4073709547519684\n",
            "strain 0.4193708896636963\n",
            "strain 0.32687732577323914\n",
            "strain 0.22251905500888824\n",
            "strain 0.4303947687149048\n",
            "strain 0.44876793026924133\n",
            "strain 0.39403676986694336\n",
            "strain 0.29276639223098755\n",
            "strain 0.3340890109539032\n",
            "strain 0.38373443484306335\n",
            "strain 0.454243004322052\n",
            "strain 0.2849108576774597\n",
            "classify 1.8895263671875\n",
            "classify 2.062744140625\n",
            "classify 1.98193359375\n",
            "classify 1.92877197265625\n",
            "classify 1.9686279296875\n",
            "classify 2.007568359375\n",
            "classify 1.9930419921875\n",
            "classify 1.82904052734375\n",
            "classify 1.904541015625\n",
            "classify 2.0489501953125\n",
            "classify 1.9945068359375\n",
            "0.328125\n",
            "0.359375\n",
            "0.25\n",
            "0.40625\n",
            "0.3125\n",
            "0.34375\n",
            "0.265625\n",
            "0.3125\n",
            "0.296875\n",
            "0.28125\n",
            "0.25\n",
            "346\n",
            "strain 0.3768812417984009\n",
            "strain 0.37213224172592163\n",
            "strain 0.46453940868377686\n",
            "strain 0.3423622250556946\n",
            "strain 0.4442178010940552\n",
            "strain 0.3881855010986328\n",
            "strain 0.3715088963508606\n",
            "strain 0.3721715807914734\n",
            "strain 0.34482237696647644\n",
            "strain 0.4991214871406555\n",
            "strain 0.358774334192276\n",
            "strain 0.3937899172306061\n",
            "strain 0.34094536304473877\n",
            "strain 0.3237013816833496\n",
            "strain 0.39291995763778687\n",
            "strain 0.34639596939086914\n",
            "strain 0.28371912240982056\n",
            "strain 0.30251544713974\n",
            "strain 0.2863743305206299\n",
            "strain 0.3140046000480652\n",
            "strain 0.3996632695198059\n",
            "strain 0.3334839642047882\n",
            "strain 0.374908447265625\n",
            "strain 0.4765731394290924\n",
            "strain 0.4205893874168396\n",
            "strain 0.32767289876937866\n",
            "strain 0.5473515391349792\n",
            "strain 0.2621495723724365\n",
            "strain 0.3250248432159424\n",
            "strain 0.28728702664375305\n",
            "strain 0.34434905648231506\n",
            "strain 0.38974517583847046\n",
            "strain 0.2614519000053406\n",
            "strain 0.36699795722961426\n",
            "strain 0.38035473227500916\n",
            "strain 0.42075371742248535\n",
            "strain 0.42226311564445496\n",
            "strain 0.29746484756469727\n",
            "strain 0.30335068702697754\n",
            "strain 0.45544183254241943\n",
            "strain 0.3896450102329254\n",
            "strain 0.3047649562358856\n",
            "strain 0.26106351613998413\n",
            "strain 0.30676621198654175\n",
            "strain 0.30956754088401794\n",
            "strain 0.33245715498924255\n",
            "strain 0.46799707412719727\n",
            "strain 0.37518221139907837\n",
            "strain 0.26728737354278564\n",
            "strain 0.28098630905151367\n",
            "strain 0.32117408514022827\n",
            "classify 1.97772216796875\n",
            "classify 1.9918212890625\n",
            "classify 1.93017578125\n",
            "classify 1.9849853515625\n",
            "classify 2.0845947265625\n",
            "classify 2.0758056640625\n",
            "classify 1.8663330078125\n",
            "classify 1.8961181640625\n",
            "classify 1.95123291015625\n",
            "classify 1.9866943359375\n",
            "classify 1.76202392578125\n",
            "0.265625\n",
            "0.21875\n",
            "0.3125\n",
            "0.328125\n",
            "0.203125\n",
            "0.296875\n",
            "0.265625\n",
            "0.28125\n",
            "0.234375\n",
            "0.328125\n",
            "0.1875\n",
            "347\n",
            "strain 0.38181421160697937\n",
            "strain 0.46198758482933044\n",
            "strain 0.3347143232822418\n",
            "strain 0.3539096415042877\n",
            "strain 0.32071393728256226\n",
            "strain 0.3029578626155853\n",
            "strain 0.2815232574939728\n",
            "strain 0.4070450961589813\n",
            "strain 0.3874616324901581\n",
            "strain 0.3491573929786682\n",
            "strain 0.3426784574985504\n",
            "strain 0.2892768383026123\n",
            "strain 0.2688225507736206\n",
            "strain 0.35341793298721313\n",
            "strain 0.29675862193107605\n",
            "strain 0.36791735887527466\n",
            "strain 0.427339106798172\n",
            "strain 0.2332555055618286\n",
            "strain 0.33958157896995544\n",
            "strain 0.43624019622802734\n",
            "strain 0.2979985475540161\n",
            "strain 0.2959078848361969\n",
            "strain 0.4133063554763794\n",
            "strain 0.35001516342163086\n",
            "strain 0.4510543942451477\n",
            "strain 0.28574275970458984\n",
            "strain 0.33758339285850525\n",
            "strain 0.4704059660434723\n",
            "strain 0.29628318548202515\n",
            "strain 0.34711331129074097\n",
            "strain 0.2576647400856018\n",
            "strain 0.4970053434371948\n",
            "strain 0.29601383209228516\n",
            "strain 0.3679984509944916\n",
            "strain 0.27607831358909607\n",
            "strain 0.4498193562030792\n",
            "strain 0.32910582423210144\n",
            "strain 0.39430731534957886\n",
            "strain 0.2583237886428833\n",
            "strain 0.35725119709968567\n",
            "strain 0.323036789894104\n",
            "strain 0.2700379490852356\n",
            "strain 0.30492106080055237\n",
            "strain 0.4559195637702942\n",
            "strain 0.26317471265792847\n",
            "strain 0.2495582401752472\n",
            "strain 0.34924888610839844\n",
            "strain 0.3711091876029968\n",
            "strain 0.4950087368488312\n",
            "strain 0.30474498867988586\n",
            "strain 0.5206676721572876\n",
            "classify 1.97265625\n",
            "classify 1.8497314453125\n",
            "classify 1.96221923828125\n",
            "classify 1.9278564453125\n",
            "classify 2.05242919921875\n",
            "classify 1.9501953125\n",
            "classify 2.026123046875\n",
            "classify 1.92041015625\n",
            "classify 1.9962158203125\n",
            "classify 1.8682861328125\n",
            "classify 1.948974609375\n",
            "0.234375\n",
            "0.359375\n",
            "0.25\n",
            "0.203125\n",
            "0.3125\n",
            "0.421875\n",
            "0.25\n",
            "0.265625\n",
            "0.234375\n",
            "0.21875\n",
            "0.28125\n",
            "348\n",
            "strain 0.4218277037143707\n",
            "strain 0.41835808753967285\n",
            "strain 0.4214005172252655\n",
            "strain 0.4498000741004944\n",
            "strain 0.40912941098213196\n",
            "strain 0.326914519071579\n",
            "strain 0.2843449115753174\n",
            "strain 0.29522833228111267\n",
            "strain 0.33579927682876587\n",
            "strain 0.32242724299430847\n",
            "strain 0.39151090383529663\n",
            "strain 0.2588776648044586\n",
            "strain 0.27730080485343933\n",
            "strain 0.3048403561115265\n",
            "strain 0.4144724905490875\n",
            "strain 0.2852151095867157\n",
            "strain 0.27455949783325195\n",
            "strain 0.3013571798801422\n",
            "strain 0.28544434905052185\n",
            "strain 0.2977992594242096\n",
            "strain 0.2916925251483917\n",
            "strain 0.46252116560935974\n",
            "strain 0.2958761155605316\n",
            "strain 0.3270665407180786\n",
            "strain 0.39018553495407104\n",
            "strain 0.43378543853759766\n",
            "strain 0.4014764428138733\n",
            "strain 0.35525041818618774\n",
            "strain 0.34117579460144043\n",
            "strain 0.4702547490596771\n",
            "strain 0.42460936307907104\n",
            "strain 0.4605065584182739\n",
            "strain 0.3417915105819702\n",
            "strain 0.42856743931770325\n",
            "strain 0.25601181387901306\n",
            "strain 0.3744499087333679\n",
            "strain 0.33803343772888184\n",
            "strain 0.4196637272834778\n",
            "strain 0.30972179770469666\n",
            "strain 0.31460830569267273\n",
            "strain 0.3548927307128906\n",
            "strain 0.27675068378448486\n",
            "strain 0.2988811731338501\n",
            "strain 0.29364824295043945\n",
            "strain 0.40125828981399536\n",
            "strain 0.4091179072856903\n",
            "strain 0.2878427803516388\n",
            "strain 0.28667035698890686\n",
            "strain 0.3407597243785858\n",
            "strain 0.23881173133850098\n",
            "strain 0.3471362590789795\n",
            "classify 1.91265869140625\n",
            "classify 2.0526123046875\n",
            "classify 2.02667236328125\n",
            "classify 1.8533935546875\n",
            "classify 1.9847412109375\n",
            "classify 1.923095703125\n",
            "classify 2.100341796875\n",
            "classify 1.8380126953125\n",
            "classify 1.964599609375\n",
            "classify 2.027099609375\n",
            "classify 1.900146484375\n",
            "0.265625\n",
            "0.3125\n",
            "0.359375\n",
            "0.453125\n",
            "0.328125\n",
            "0.3125\n",
            "0.390625\n",
            "0.25\n",
            "0.28125\n",
            "0.328125\n",
            "0.203125\n",
            "349\n",
            "strain 0.35568153858184814\n",
            "strain 0.3691614270210266\n",
            "strain 0.2869105935096741\n",
            "strain 0.4602597951889038\n",
            "strain 0.29004180431365967\n",
            "strain 0.44924473762512207\n",
            "strain 0.4520525336265564\n",
            "strain 0.28519323468208313\n",
            "strain 0.4252106547355652\n",
            "strain 0.41402357816696167\n",
            "strain 0.28491681814193726\n",
            "strain 0.39742177724838257\n",
            "strain 0.35533013939857483\n",
            "strain 0.36528846621513367\n",
            "strain 0.36624544858932495\n",
            "strain 0.32991012930870056\n",
            "strain 0.32584425806999207\n",
            "strain 0.3611271381378174\n",
            "strain 0.29052188992500305\n",
            "strain 0.40731728076934814\n",
            "strain 0.2844415605068207\n",
            "strain 0.3442692756652832\n",
            "strain 0.481659471988678\n",
            "strain 0.4157540202140808\n",
            "strain 0.4043465554714203\n",
            "strain 0.3710196614265442\n",
            "strain 0.4057270288467407\n",
            "strain 0.32612594962120056\n",
            "strain 0.3591512143611908\n",
            "strain 0.32489439845085144\n",
            "strain 0.2518915832042694\n",
            "strain 0.3014635741710663\n",
            "strain 0.2750895917415619\n",
            "strain 0.2653259038925171\n",
            "strain 0.3256983757019043\n",
            "strain 0.3240428864955902\n",
            "strain 0.37960731983184814\n",
            "strain 0.33184757828712463\n",
            "strain 0.2842687666416168\n",
            "strain 0.31399959325790405\n",
            "strain 0.3790454864501953\n",
            "strain 0.3370758295059204\n",
            "strain 0.42760950326919556\n",
            "strain 0.2965584099292755\n",
            "strain 0.3991941809654236\n",
            "strain 0.4111100733280182\n",
            "strain 0.3586971163749695\n",
            "strain 0.2854791283607483\n",
            "strain 0.39540621638298035\n",
            "strain 0.27474477887153625\n",
            "strain 0.4039233326911926\n",
            "classify 2.146728515625\n",
            "classify 1.9984130859375\n",
            "classify 1.9935302734375\n",
            "classify 1.899658203125\n",
            "classify 1.9525146484375\n",
            "classify 1.92852783203125\n",
            "classify 1.8504638671875\n",
            "classify 1.9439697265625\n",
            "classify 1.8538818359375\n",
            "classify 1.959228515625\n",
            "classify 1.98828125\n",
            "0.296875\n",
            "0.28125\n",
            "0.28125\n",
            "0.203125\n",
            "0.3125\n",
            "0.234375\n",
            "0.359375\n",
            "0.234375\n",
            "0.3125\n",
            "0.15625\n",
            "0.234375\n",
            "350\n",
            "strain 0.46525827050209045\n",
            "strain 0.3270363509654999\n",
            "strain 0.47046658396720886\n",
            "strain 0.3287610411643982\n",
            "strain 0.40498459339141846\n",
            "strain 0.3358429968357086\n",
            "strain 0.4449741840362549\n",
            "strain 0.342366099357605\n",
            "strain 0.4095006585121155\n",
            "strain 0.3762645721435547\n",
            "strain 0.26625189185142517\n",
            "strain 0.2954760789871216\n",
            "strain 0.4037039577960968\n",
            "strain 0.3486350178718567\n",
            "strain 0.3909114897251129\n",
            "strain 0.4317929446697235\n",
            "strain 0.3460279405117035\n",
            "strain 0.2826310396194458\n",
            "strain 0.24520431458950043\n",
            "strain 0.4247484803199768\n",
            "strain 0.5057092905044556\n",
            "strain 0.42381975054740906\n",
            "strain 0.3862665891647339\n",
            "strain 0.27667370438575745\n",
            "strain 0.30157768726348877\n",
            "strain 0.34397682547569275\n",
            "strain 0.4098280966281891\n",
            "strain 0.3652327358722687\n",
            "strain 0.43060770630836487\n",
            "strain 0.440963476896286\n",
            "strain 0.2691919505596161\n",
            "strain 0.3438200056552887\n",
            "strain 0.4199656844139099\n",
            "strain 0.35262835025787354\n",
            "strain 0.2910202443599701\n",
            "strain 0.3168931305408478\n",
            "strain 0.39050355553627014\n",
            "strain 0.3514949083328247\n",
            "strain 0.39033493399620056\n",
            "strain 0.3164377808570862\n",
            "strain 0.3195226490497589\n",
            "strain 0.4397676885128021\n",
            "strain 0.25800827145576477\n",
            "strain 0.3362152576446533\n",
            "strain 0.38786327838897705\n",
            "strain 0.37895482778549194\n",
            "strain 0.31314632296562195\n",
            "strain 0.45861661434173584\n",
            "strain 0.25431087613105774\n",
            "strain 0.4185556173324585\n",
            "strain 0.3580051064491272\n",
            "classify 1.9356689453125\n",
            "classify 1.9002685546875\n",
            "classify 1.9542236328125\n",
            "classify 1.79913330078125\n",
            "classify 1.97509765625\n",
            "classify 2.0931396484375\n",
            "classify 2.002685546875\n",
            "classify 1.943359375\n",
            "classify 1.949951171875\n",
            "classify 1.8365478515625\n",
            "classify 1.839111328125\n",
            "0.265625\n",
            "0.1875\n",
            "0.3125\n",
            "0.265625\n",
            "0.265625\n",
            "0.296875\n",
            "0.359375\n",
            "0.21875\n",
            "0.3125\n",
            "0.265625\n",
            "0.296875\n",
            "351\n",
            "strain 0.430950790643692\n",
            "strain 0.4164937436580658\n",
            "strain 0.32642802596092224\n",
            "strain 0.3468497693538666\n",
            "strain 0.3331925868988037\n",
            "strain 0.4642733037471771\n",
            "strain 0.35873091220855713\n",
            "strain 0.39866265654563904\n",
            "strain 0.46660152077674866\n",
            "strain 0.30240681767463684\n",
            "strain 0.3160700798034668\n",
            "strain 0.37627312541007996\n",
            "strain 0.240533247590065\n",
            "strain 0.3976214528083801\n",
            "strain 0.3242609202861786\n",
            "strain 0.3737928569316864\n",
            "strain 0.25183215737342834\n",
            "strain 0.2538665235042572\n",
            "strain 0.24847692251205444\n",
            "strain 0.4213428795337677\n",
            "strain 0.3427291810512543\n",
            "strain 0.42886435985565186\n",
            "strain 0.46352776885032654\n",
            "strain 0.35249823331832886\n",
            "strain 0.27514415979385376\n",
            "strain 0.39563125371932983\n",
            "strain 0.35643792152404785\n",
            "strain 0.4134993255138397\n",
            "strain 0.30346688628196716\n",
            "strain 0.41542279720306396\n",
            "strain 0.26358142495155334\n",
            "strain 0.3206355571746826\n",
            "strain 0.41582056879997253\n",
            "strain 0.28937822580337524\n",
            "strain 0.49437886476516724\n",
            "strain 0.37931597232818604\n",
            "strain 0.41114768385887146\n",
            "strain 0.26913273334503174\n",
            "strain 0.5285917520523071\n",
            "strain 0.4225703179836273\n",
            "strain 0.3630088269710541\n",
            "strain 0.4411810338497162\n",
            "strain 0.36018845438957214\n",
            "strain 0.27177610993385315\n",
            "strain 0.3454030454158783\n",
            "strain 0.40258678793907166\n",
            "strain 0.4394764304161072\n",
            "strain 0.4572993218898773\n",
            "strain 0.34651049971580505\n",
            "strain 0.4571802616119385\n",
            "strain 0.40753114223480225\n",
            "classify 2.005615234375\n",
            "classify 2.021728515625\n",
            "classify 1.8558349609375\n",
            "classify 1.9267578125\n",
            "classify 1.97430419921875\n",
            "classify 2.0035400390625\n",
            "classify 1.871826171875\n",
            "classify 1.9876708984375\n",
            "classify 1.9432373046875\n",
            "classify 1.92388916015625\n",
            "classify 1.9617919921875\n",
            "0.40625\n",
            "0.234375\n",
            "0.203125\n",
            "0.25\n",
            "0.21875\n",
            "0.28125\n",
            "0.296875\n",
            "0.3125\n",
            "0.328125\n",
            "0.265625\n",
            "0.25\n",
            "352\n",
            "strain 0.2899867296218872\n",
            "strain 0.4205840229988098\n",
            "strain 0.39069297909736633\n",
            "strain 0.39647457003593445\n",
            "strain 0.42341193556785583\n",
            "strain 0.3052333891391754\n",
            "strain 0.30699995160102844\n",
            "strain 0.28934186697006226\n",
            "strain 0.4289063513278961\n",
            "strain 0.33435383439064026\n",
            "strain 0.35946202278137207\n",
            "strain 0.4534386992454529\n",
            "strain 0.3699151277542114\n",
            "strain 0.29760047793388367\n",
            "strain 0.2504153251647949\n",
            "strain 0.3440600037574768\n",
            "strain 0.28528693318367004\n",
            "strain 0.4183441400527954\n",
            "strain 0.46876823902130127\n",
            "strain 0.42410826683044434\n",
            "strain 0.3280230760574341\n",
            "strain 0.3053480386734009\n",
            "strain 0.4022373855113983\n",
            "strain 0.42912203073501587\n",
            "strain 0.33305615186691284\n",
            "strain 0.3327714502811432\n",
            "strain 0.3684443533420563\n",
            "strain 0.42754626274108887\n",
            "strain 0.29553350806236267\n",
            "strain 0.3683503270149231\n",
            "strain 0.366505891084671\n",
            "strain 0.2784801423549652\n",
            "strain 0.3745144307613373\n",
            "strain 0.2894674837589264\n",
            "strain 0.40647467970848083\n",
            "strain 0.3267532289028168\n",
            "strain 0.29397836327552795\n",
            "strain 0.37921127676963806\n",
            "strain 0.34002548456192017\n",
            "strain 0.40343984961509705\n",
            "strain 0.2876320481300354\n",
            "strain 0.2801882326602936\n",
            "strain 0.30055955052375793\n",
            "strain 0.2654995322227478\n",
            "strain 0.5009831190109253\n",
            "strain 0.3536197245121002\n",
            "strain 0.3009265065193176\n",
            "strain 0.3256103992462158\n",
            "strain 0.42327067255973816\n",
            "strain 0.26860475540161133\n",
            "strain 0.38559797406196594\n",
            "classify 1.9676513671875\n",
            "classify 1.885986328125\n",
            "classify 1.8260498046875\n",
            "classify 1.92022705078125\n",
            "classify 1.97796630859375\n",
            "classify 1.88970947265625\n",
            "classify 2.073974609375\n",
            "classify 1.85626220703125\n",
            "classify 1.954833984375\n",
            "classify 1.9984130859375\n",
            "classify 2.0321044921875\n",
            "0.328125\n",
            "0.296875\n",
            "0.328125\n",
            "0.359375\n",
            "0.203125\n",
            "0.296875\n",
            "0.234375\n",
            "0.1875\n",
            "0.359375\n",
            "0.328125\n",
            "0.34375\n",
            "353\n",
            "strain 0.383818119764328\n",
            "strain 0.4122079908847809\n",
            "strain 0.3485530912876129\n",
            "strain 0.2578784227371216\n",
            "strain 0.33523455262184143\n",
            "strain 0.34366941452026367\n",
            "strain 0.3941187262535095\n",
            "strain 0.31878775358200073\n",
            "strain 0.3384946286678314\n",
            "strain 0.4033403694629669\n",
            "strain 0.2628144323825836\n",
            "strain 0.3220347464084625\n",
            "strain 0.3939376771450043\n",
            "strain 0.4060406982898712\n",
            "strain 0.46876367926597595\n",
            "strain 0.3252808153629303\n",
            "strain 0.3815755546092987\n",
            "strain 0.3105432391166687\n",
            "strain 0.3563360571861267\n",
            "strain 0.3307424485683441\n",
            "strain 0.5496928691864014\n",
            "strain 0.2894304394721985\n",
            "strain 0.3500019907951355\n",
            "strain 0.4259800314903259\n",
            "strain 0.27187684178352356\n",
            "strain 0.392255038022995\n",
            "strain 0.32871875166893005\n",
            "strain 0.48160234093666077\n",
            "strain 0.38251733779907227\n",
            "strain 0.285518616437912\n",
            "strain 0.4343099296092987\n",
            "strain 0.3625841736793518\n",
            "strain 0.31653696298599243\n",
            "strain 0.3492690324783325\n",
            "strain 0.3738623857498169\n",
            "strain 0.3212880790233612\n",
            "strain 0.30646827816963196\n",
            "strain 0.2732233703136444\n",
            "strain 0.3694840371608734\n",
            "strain 0.39621323347091675\n",
            "strain 0.32388120889663696\n",
            "strain 0.2970638871192932\n",
            "strain 0.3371429443359375\n",
            "strain 0.2976957857608795\n",
            "strain 0.29691600799560547\n",
            "strain 0.2874170243740082\n",
            "strain 0.37494993209838867\n",
            "strain 0.38581496477127075\n",
            "strain 0.2963048815727234\n",
            "strain 0.24825996160507202\n",
            "strain 0.31693634390830994\n",
            "classify 2.03985595703125\n",
            "classify 1.8961181640625\n",
            "classify 2.053466796875\n",
            "classify 1.925048828125\n",
            "classify 1.84765625\n",
            "classify 1.96734619140625\n",
            "classify 1.97991943359375\n",
            "classify 1.86981201171875\n",
            "classify 1.944091796875\n",
            "classify 1.879150390625\n",
            "classify 1.807861328125\n",
            "0.296875\n",
            "0.34375\n",
            "0.265625\n",
            "0.28125\n",
            "0.359375\n",
            "0.359375\n",
            "0.3125\n",
            "0.265625\n",
            "0.359375\n",
            "0.234375\n",
            "0.25\n",
            "354\n",
            "strain 0.4815468490123749\n",
            "strain 0.2978167235851288\n",
            "strain 0.27813848853111267\n",
            "strain 0.3062629699707031\n",
            "strain 0.4906499683856964\n",
            "strain 0.36507460474967957\n",
            "strain 0.3322489857673645\n",
            "strain 0.36083006858825684\n",
            "strain 0.5585572123527527\n",
            "strain 0.2991172969341278\n",
            "strain 0.34513628482818604\n",
            "strain 0.2868112325668335\n",
            "strain 0.25551649928092957\n",
            "strain 0.28609326481819153\n",
            "strain 0.29992198944091797\n",
            "strain 0.38263803720474243\n",
            "strain 0.34089595079421997\n",
            "strain 0.2917926609516144\n",
            "strain 0.3055892884731293\n",
            "strain 0.402332603931427\n",
            "strain 0.321973592042923\n",
            "strain 0.46552154421806335\n",
            "strain 0.3647492229938507\n",
            "strain 0.31093108654022217\n",
            "strain 0.2488294094800949\n",
            "strain 0.5038822293281555\n",
            "strain 0.4120327830314636\n",
            "strain 0.3580586910247803\n",
            "strain 0.2710643410682678\n",
            "strain 0.42859214544296265\n",
            "strain 0.383207231760025\n",
            "strain 0.34397226572036743\n",
            "strain 0.2647179961204529\n",
            "strain 0.4582985043525696\n",
            "strain 0.32326409220695496\n",
            "strain 0.3960738480091095\n",
            "strain 0.42508041858673096\n",
            "strain 0.2867944538593292\n",
            "strain 0.382081001996994\n",
            "strain 0.3068244755268097\n",
            "strain 0.35156694054603577\n",
            "strain 0.40618711709976196\n",
            "strain 0.39741069078445435\n",
            "strain 0.3314753770828247\n",
            "strain 0.3720598518848419\n",
            "strain 0.5075364112854004\n",
            "strain 0.3234608471393585\n",
            "strain 0.44988125562667847\n",
            "strain 0.3364557921886444\n",
            "strain 0.4642745852470398\n",
            "strain 0.33835068345069885\n",
            "classify 1.99566650390625\n",
            "classify 1.830810546875\n",
            "classify 1.8511962890625\n",
            "classify 2.0628662109375\n",
            "classify 2.04833984375\n",
            "classify 1.8360595703125\n",
            "classify 2.0343017578125\n",
            "classify 1.91943359375\n",
            "classify 1.9403076171875\n",
            "classify 1.922119140625\n",
            "classify 1.92236328125\n",
            "0.25\n",
            "0.34375\n",
            "0.375\n",
            "0.28125\n",
            "0.296875\n",
            "0.234375\n",
            "0.3125\n",
            "0.296875\n",
            "0.296875\n",
            "0.328125\n",
            "0.25\n",
            "355\n",
            "strain 0.29322534799575806\n",
            "strain 0.3075729012489319\n",
            "strain 0.3467864394187927\n",
            "strain 0.29673144221305847\n",
            "strain 0.3050908148288727\n",
            "strain 0.31071963906288147\n",
            "strain 0.293235719203949\n",
            "strain 0.3578580319881439\n",
            "strain 0.37446296215057373\n",
            "strain 0.34218502044677734\n",
            "strain 0.47459226846694946\n",
            "strain 0.3372250497341156\n",
            "strain 0.3741099238395691\n",
            "strain 0.3716813623905182\n",
            "strain 0.3041870892047882\n",
            "strain 0.313506543636322\n",
            "strain 0.3670552968978882\n",
            "strain 0.3988530933856964\n",
            "strain 0.2932402491569519\n",
            "strain 0.28304553031921387\n",
            "strain 0.352595716714859\n",
            "strain 0.40702712535858154\n",
            "strain 0.29384154081344604\n",
            "strain 0.2795338034629822\n",
            "strain 0.3860260844230652\n",
            "strain 0.26728472113609314\n",
            "strain 0.3483154773712158\n",
            "strain 0.29193270206451416\n",
            "strain 0.41038525104522705\n",
            "strain 0.3594217598438263\n",
            "strain 0.3825649917125702\n",
            "strain 0.5292965769767761\n",
            "strain 0.3551849126815796\n",
            "strain 0.4173440635204315\n",
            "strain 0.2789565920829773\n",
            "strain 0.37847191095352173\n",
            "strain 0.32531148195266724\n",
            "strain 0.42444267868995667\n",
            "strain 0.33395710587501526\n",
            "strain 0.323119580745697\n",
            "strain 0.31812790036201477\n",
            "strain 0.5293216109275818\n",
            "strain 0.3091318607330322\n",
            "strain 0.28814369440078735\n",
            "strain 0.4594411253929138\n",
            "strain 0.41522693634033203\n",
            "strain 0.3984491229057312\n",
            "strain 0.3075537383556366\n",
            "strain 0.4376462399959564\n",
            "strain 0.2421579509973526\n",
            "strain 0.40913739800453186\n",
            "classify 1.967529296875\n",
            "classify 1.955078125\n",
            "classify 1.89715576171875\n",
            "classify 1.8465576171875\n",
            "classify 1.922607421875\n",
            "classify 1.95556640625\n",
            "classify 1.9034423828125\n",
            "classify 1.8529052734375\n",
            "classify 1.918701171875\n",
            "classify 1.92529296875\n",
            "classify 2.04150390625\n",
            "0.3125\n",
            "0.28125\n",
            "0.265625\n",
            "0.375\n",
            "0.203125\n",
            "0.265625\n",
            "0.328125\n",
            "0.3125\n",
            "0.25\n",
            "0.296875\n",
            "0.328125\n",
            "356\n",
            "strain 0.25558507442474365\n",
            "strain 0.2991255819797516\n",
            "strain 0.24231790006160736\n",
            "strain 0.30402234196662903\n",
            "strain 0.36187943816185\n",
            "strain 0.39718106389045715\n",
            "strain 0.3775792717933655\n",
            "strain 0.367737740278244\n",
            "strain 0.40411055088043213\n",
            "strain 0.3661547601222992\n",
            "strain 0.3318687975406647\n",
            "strain 0.39794251322746277\n",
            "strain 0.31262195110321045\n",
            "strain 0.25368988513946533\n",
            "strain 0.32853060960769653\n",
            "strain 0.43106377124786377\n",
            "strain 0.24713367223739624\n",
            "strain 0.31793326139450073\n",
            "strain 0.3937046229839325\n",
            "strain 0.2852219045162201\n",
            "strain 0.44688844680786133\n",
            "strain 0.28507840633392334\n",
            "strain 0.3438820242881775\n",
            "strain 0.44436198472976685\n",
            "strain 0.3522329330444336\n",
            "strain 0.3109380006790161\n",
            "strain 0.2941073477268219\n",
            "strain 0.33017855882644653\n",
            "strain 0.3429451584815979\n",
            "strain 0.2591641843318939\n",
            "strain 0.3156384527683258\n",
            "strain 0.5179768800735474\n",
            "strain 0.4832095503807068\n",
            "strain 0.29592466354370117\n",
            "strain 0.2807622253894806\n",
            "strain 0.3095548152923584\n",
            "strain 0.29297444224357605\n",
            "strain 0.4322465658187866\n",
            "strain 0.422596275806427\n",
            "strain 0.29705512523651123\n",
            "strain 0.29666632413864136\n",
            "strain 0.37489983439445496\n",
            "strain 0.3134424388408661\n",
            "strain 0.4806145429611206\n",
            "strain 0.3983917236328125\n",
            "strain 0.23379741609096527\n",
            "strain 0.3798334300518036\n",
            "strain 0.42211711406707764\n",
            "strain 0.4247351884841919\n",
            "strain 0.3566263020038605\n",
            "strain 0.245380699634552\n",
            "classify 1.9537353515625\n",
            "classify 1.91998291015625\n",
            "classify 1.8466796875\n",
            "classify 1.958251953125\n",
            "classify 2.007568359375\n",
            "classify 1.9482421875\n",
            "classify 1.9425048828125\n",
            "classify 2.17816162109375\n",
            "classify 1.9482421875\n",
            "classify 2.0565185546875\n",
            "classify 1.98651123046875\n",
            "0.34375\n",
            "0.21875\n",
            "0.25\n",
            "0.328125\n",
            "0.265625\n",
            "0.328125\n",
            "0.234375\n",
            "0.25\n",
            "0.21875\n",
            "0.296875\n",
            "0.1875\n",
            "357\n",
            "strain 0.46909672021865845\n",
            "strain 0.4796081483364105\n",
            "strain 0.41686034202575684\n",
            "strain 0.35466712713241577\n",
            "strain 0.36698877811431885\n",
            "strain 0.3277089595794678\n",
            "strain 0.4703328013420105\n",
            "strain 0.2774357497692108\n",
            "strain 0.2957570552825928\n",
            "strain 0.425045907497406\n",
            "strain 0.38540956377983093\n",
            "strain 0.4336942136287689\n",
            "strain 0.37010517716407776\n",
            "strain 0.3409510850906372\n",
            "strain 0.24456791579723358\n",
            "strain 0.3628120720386505\n",
            "strain 0.2813490927219391\n",
            "strain 0.28910014033317566\n",
            "strain 0.28610581159591675\n",
            "strain 0.30732861161231995\n",
            "strain 0.38155558705329895\n",
            "strain 0.3260402977466583\n",
            "strain 0.38678407669067383\n",
            "strain 0.3602348268032074\n",
            "strain 0.42610475420951843\n",
            "strain 0.3880595266819\n",
            "strain 0.34977221488952637\n",
            "strain 0.3719232976436615\n",
            "strain 0.39735710620880127\n",
            "strain 0.3739570677280426\n",
            "strain 0.2737565040588379\n",
            "strain 0.4004838466644287\n",
            "strain 0.5378273129463196\n",
            "strain 0.34548240900039673\n",
            "strain 0.3123241364955902\n",
            "strain 0.4777892529964447\n",
            "strain 0.4128050208091736\n",
            "strain 0.33650174736976624\n",
            "strain 0.3872320055961609\n",
            "strain 0.3612266480922699\n",
            "strain 0.4486539661884308\n",
            "strain 0.3546862006187439\n",
            "strain 0.33663690090179443\n",
            "strain 0.30831366777420044\n",
            "strain 0.3980822265148163\n",
            "strain 0.43646207451820374\n",
            "strain 0.3414302468299866\n",
            "strain 0.34333083033561707\n",
            "strain 0.2942526936531067\n",
            "strain 0.27790603041648865\n",
            "strain 0.30838263034820557\n",
            "classify 2.01904296875\n",
            "classify 1.92401123046875\n",
            "classify 1.929931640625\n",
            "classify 1.972412109375\n",
            "classify 2.0089111328125\n",
            "classify 1.87982177734375\n",
            "classify 1.8719482421875\n",
            "classify 1.91864013671875\n",
            "classify 1.99822998046875\n",
            "classify 1.86920166015625\n",
            "classify 1.9244384765625\n",
            "0.296875\n",
            "0.484375\n",
            "0.296875\n",
            "0.265625\n",
            "0.3125\n",
            "0.328125\n",
            "0.203125\n",
            "0.34375\n",
            "0.296875\n",
            "0.296875\n",
            "0.234375\n",
            "358\n",
            "strain 0.3178083300590515\n",
            "strain 0.31942155957221985\n",
            "strain 0.31147605180740356\n",
            "strain 0.384813517332077\n",
            "strain 0.48892006278038025\n",
            "strain 0.46339622139930725\n",
            "strain 0.39959827065467834\n",
            "strain 0.4791514575481415\n",
            "strain 0.4473203122615814\n",
            "strain 0.41901975870132446\n",
            "strain 0.42798417806625366\n",
            "strain 0.4776245057582855\n",
            "strain 0.3978835344314575\n",
            "strain 0.4250275492668152\n",
            "strain 0.3554539382457733\n",
            "strain 0.2575991451740265\n",
            "strain 0.28425872325897217\n",
            "strain 0.4174179434776306\n",
            "strain 0.4199111759662628\n",
            "strain 0.3125126361846924\n",
            "strain 0.2993730306625366\n",
            "strain 0.3154171109199524\n",
            "strain 0.2849116027355194\n",
            "strain 0.394399493932724\n",
            "strain 0.29460710287094116\n",
            "strain 0.31096240878105164\n",
            "strain 0.4113227128982544\n",
            "strain 0.23527467250823975\n",
            "strain 0.30486804246902466\n",
            "strain 0.2792157232761383\n",
            "strain 0.32123491168022156\n",
            "strain 0.30810272693634033\n",
            "strain 0.3116883337497711\n",
            "strain 0.5069364905357361\n",
            "strain 0.3739568889141083\n",
            "strain 0.3215595483779907\n",
            "strain 0.4220225214958191\n",
            "strain 0.3744827210903168\n",
            "strain 0.323199599981308\n",
            "strain 0.287221223115921\n",
            "strain 0.30661436915397644\n",
            "strain 0.4115534722805023\n",
            "strain 0.34085631370544434\n",
            "strain 0.46855732798576355\n",
            "strain 0.4620407819747925\n",
            "strain 0.2578451335430145\n",
            "strain 0.3388810455799103\n",
            "strain 0.35851073265075684\n",
            "strain 0.46164843440055847\n",
            "strain 0.29453402757644653\n",
            "strain 0.3846728801727295\n",
            "classify 2.00897216796875\n",
            "classify 2.01470947265625\n",
            "classify 1.9501953125\n",
            "classify 1.99090576171875\n",
            "classify 1.98687744140625\n",
            "classify 1.86920166015625\n",
            "classify 2.1092529296875\n",
            "classify 2.009521484375\n",
            "classify 1.97412109375\n",
            "classify 1.834716796875\n",
            "classify 1.924560546875\n",
            "0.21875\n",
            "0.34375\n",
            "0.25\n",
            "0.3125\n",
            "0.359375\n",
            "0.328125\n",
            "0.390625\n",
            "0.34375\n",
            "0.28125\n",
            "0.28125\n",
            "0.28125\n",
            "359\n",
            "strain 0.3331361711025238\n",
            "strain 0.29901522397994995\n",
            "strain 0.4024675786495209\n",
            "strain 0.29329341650009155\n",
            "strain 0.37694051861763\n",
            "strain 0.39985328912734985\n",
            "strain 0.36650213599205017\n",
            "strain 0.36478254199028015\n",
            "strain 0.5081382393836975\n",
            "strain 0.26695141196250916\n",
            "strain 0.2857104241847992\n",
            "strain 0.3677022159099579\n",
            "strain 0.4660092890262604\n",
            "strain 0.2778353691101074\n",
            "strain 0.3406221866607666\n",
            "strain 0.45131340622901917\n",
            "strain 0.3222109079360962\n",
            "strain 0.3240754008293152\n",
            "strain 0.29410046339035034\n",
            "strain 0.3017159402370453\n",
            "strain 0.3214063346385956\n",
            "strain 0.3481013774871826\n",
            "strain 0.28386878967285156\n",
            "strain 0.48362722992897034\n",
            "strain 0.3879029154777527\n",
            "strain 0.26490527391433716\n",
            "strain 0.34152650833129883\n",
            "strain 0.24557630717754364\n",
            "strain 0.3579282760620117\n",
            "strain 0.38622480630874634\n",
            "strain 0.2678675651550293\n",
            "strain 0.2882959246635437\n",
            "strain 0.2996810972690582\n",
            "strain 0.46219533681869507\n",
            "strain 0.3946271538734436\n",
            "strain 0.43351811170578003\n",
            "strain 0.41587820649147034\n",
            "strain 0.3992254137992859\n",
            "strain 0.39941084384918213\n",
            "strain 0.35008203983306885\n",
            "strain 0.2553655505180359\n",
            "strain 0.34560754895210266\n",
            "strain 0.48868608474731445\n",
            "strain 0.3207922577857971\n",
            "strain 0.39915624260902405\n",
            "strain 0.38801243901252747\n",
            "strain 0.28056615591049194\n",
            "strain 0.44297903776168823\n",
            "strain 0.35335856676101685\n",
            "strain 0.3193221092224121\n",
            "strain 0.3644200563430786\n",
            "classify 2.0504150390625\n",
            "classify 1.96185302734375\n",
            "classify 2.0958251953125\n",
            "classify 1.955322265625\n",
            "classify 1.953857421875\n",
            "classify 1.92999267578125\n",
            "classify 2.0181884765625\n",
            "classify 1.9083251953125\n",
            "classify 1.82611083984375\n",
            "classify 1.908935546875\n",
            "classify 2.0296630859375\n",
            "0.34375\n",
            "0.390625\n",
            "0.21875\n",
            "0.234375\n",
            "0.203125\n",
            "0.28125\n",
            "0.203125\n",
            "0.328125\n",
            "0.3125\n",
            "0.28125\n",
            "0.328125\n",
            "360\n",
            "strain 0.2800017297267914\n",
            "strain 0.27579084038734436\n",
            "strain 0.4370425045490265\n",
            "strain 0.3868715763092041\n",
            "strain 0.3149506151676178\n",
            "strain 0.26265981793403625\n",
            "strain 0.4446898102760315\n",
            "strain 0.3753774166107178\n",
            "strain 0.2896003723144531\n",
            "strain 0.2443041205406189\n",
            "strain 0.44121214747428894\n",
            "strain 0.4010419249534607\n",
            "strain 0.3675864636898041\n",
            "strain 0.3094065189361572\n",
            "strain 0.39111489057540894\n",
            "strain 0.4054308533668518\n",
            "strain 0.3085245192050934\n",
            "strain 0.3554973304271698\n",
            "strain 0.32854515314102173\n",
            "strain 0.35326239466667175\n",
            "strain 0.4247019588947296\n",
            "strain 0.2844725549221039\n",
            "strain 0.39772582054138184\n",
            "strain 0.2313401997089386\n",
            "strain 0.3925645351409912\n",
            "strain 0.34341201186180115\n",
            "strain 0.4499329626560211\n",
            "strain 0.3424830734729767\n",
            "strain 0.5327209234237671\n",
            "strain 0.39305320382118225\n",
            "strain 0.404884397983551\n",
            "strain 0.4377059042453766\n",
            "strain 0.2949923276901245\n",
            "strain 0.4274666905403137\n",
            "strain 0.2698642909526825\n",
            "strain 0.3630630075931549\n",
            "strain 0.2979675233364105\n",
            "strain 0.37611836194992065\n",
            "strain 0.36805370450019836\n",
            "strain 0.39327722787857056\n",
            "strain 0.40687131881713867\n",
            "strain 0.3051154315471649\n",
            "strain 0.31766894459724426\n",
            "strain 0.32140275835990906\n",
            "strain 0.3818119764328003\n",
            "strain 0.33712610602378845\n",
            "strain 0.40119999647140503\n",
            "strain 0.4756116271018982\n",
            "strain 0.2613934278488159\n",
            "strain 0.4616716206073761\n",
            "strain 0.41337496042251587\n",
            "classify 1.732421875\n",
            "classify 1.859375\n",
            "classify 1.950927734375\n",
            "classify 2.01123046875\n",
            "classify 1.890625\n",
            "classify 1.98858642578125\n",
            "classify 1.9150390625\n",
            "classify 2.134033203125\n",
            "classify 1.7877197265625\n",
            "classify 1.98004150390625\n",
            "classify 1.93548583984375\n",
            "0.25\n",
            "0.265625\n",
            "0.34375\n",
            "0.296875\n",
            "0.40625\n",
            "0.390625\n",
            "0.296875\n",
            "0.328125\n",
            "0.234375\n",
            "0.28125\n",
            "0.375\n",
            "361\n",
            "strain 0.32397252321243286\n",
            "strain 0.34598594903945923\n",
            "strain 0.3040785491466522\n",
            "strain 0.2742384672164917\n",
            "strain 0.4349469542503357\n",
            "strain 0.387215256690979\n",
            "strain 0.2700834274291992\n",
            "strain 0.2791045010089874\n",
            "strain 0.491984099149704\n",
            "strain 0.5142671465873718\n",
            "strain 0.3536662459373474\n",
            "strain 0.3244100511074066\n",
            "strain 0.41762614250183105\n",
            "strain 0.3728156089782715\n",
            "strain 0.3810105621814728\n",
            "strain 0.286396861076355\n",
            "strain 0.4158013164997101\n",
            "strain 0.28920578956604004\n",
            "strain 0.3124227225780487\n",
            "strain 0.3940350115299225\n",
            "strain 0.2748531699180603\n",
            "strain 0.3367723524570465\n",
            "strain 0.39942386746406555\n",
            "strain 0.3874880373477936\n",
            "strain 0.30627351999282837\n",
            "strain 0.35921764373779297\n",
            "strain 0.537295401096344\n",
            "strain 0.34497109055519104\n",
            "strain 0.2796831727027893\n",
            "strain 0.40403521060943604\n",
            "strain 0.4057302474975586\n",
            "strain 0.3247149586677551\n",
            "strain 0.5100053548812866\n",
            "strain 0.4267277419567108\n",
            "strain 0.41804495453834534\n",
            "strain 0.24997572600841522\n",
            "strain 0.2976894974708557\n",
            "strain 0.28911104798316956\n",
            "strain 0.32511240243911743\n",
            "strain 0.28559577465057373\n",
            "strain 0.3697151839733124\n",
            "strain 0.30061379075050354\n",
            "strain 0.40173304080963135\n",
            "strain 0.476317822933197\n",
            "strain 0.4463261067867279\n",
            "strain 0.2847006320953369\n",
            "strain 0.44007501006126404\n",
            "strain 0.27901849150657654\n",
            "strain 0.34669968485832214\n",
            "strain 0.3186039924621582\n",
            "strain 0.4577305316925049\n",
            "classify 1.9561767578125\n",
            "classify 1.960205078125\n",
            "classify 1.9947509765625\n",
            "classify 1.85498046875\n",
            "classify 1.833984375\n",
            "classify 1.9324951171875\n",
            "classify 1.86083984375\n",
            "classify 2.0533447265625\n",
            "classify 1.957275390625\n",
            "classify 1.83251953125\n",
            "classify 2.005126953125\n",
            "0.265625\n",
            "0.25\n",
            "0.28125\n",
            "0.265625\n",
            "0.34375\n",
            "0.296875\n",
            "0.265625\n",
            "0.3125\n",
            "0.265625\n",
            "0.359375\n",
            "0.375\n",
            "362\n",
            "strain 0.28271764516830444\n",
            "strain 0.2988193929195404\n",
            "strain 0.43327897787094116\n",
            "strain 0.3033371567726135\n",
            "strain 0.3605564832687378\n",
            "strain 0.2869057357311249\n",
            "strain 0.42149075865745544\n",
            "strain 0.3392602801322937\n",
            "strain 0.26697808504104614\n",
            "strain 0.32933011651039124\n",
            "strain 0.5042709708213806\n",
            "strain 0.3623444139957428\n",
            "strain 0.36298978328704834\n",
            "strain 0.30229994654655457\n",
            "strain 0.3304286599159241\n",
            "strain 0.3179832696914673\n",
            "strain 0.29375359416007996\n",
            "strain 0.36738696694374084\n",
            "strain 0.2691766023635864\n",
            "strain 0.4065205752849579\n",
            "strain 0.5281182527542114\n",
            "strain 0.4268353283405304\n",
            "strain 0.363900363445282\n",
            "strain 0.2570524513721466\n",
            "strain 0.4021967649459839\n",
            "strain 0.27757880091667175\n",
            "strain 0.33092644810676575\n",
            "strain 0.27476367354393005\n",
            "strain 0.4091240167617798\n",
            "strain 0.2742380201816559\n",
            "strain 0.3412214517593384\n",
            "strain 0.4445860981941223\n",
            "strain 0.41680270433425903\n",
            "strain 0.30336636304855347\n",
            "strain 0.4374374747276306\n",
            "strain 0.2772709131240845\n",
            "strain 0.3520386517047882\n",
            "strain 0.36294618248939514\n",
            "strain 0.38613080978393555\n",
            "strain 0.3187863528728485\n",
            "strain 0.3607192039489746\n",
            "strain 0.40037062764167786\n",
            "strain 0.30807167291641235\n",
            "strain 0.3549100458621979\n",
            "strain 0.31710827350616455\n",
            "strain 0.3965059220790863\n",
            "strain 0.42926323413848877\n",
            "strain 0.4803985059261322\n",
            "strain 0.2730310559272766\n",
            "strain 0.45301398634910583\n",
            "strain 0.31718021631240845\n",
            "classify 1.92041015625\n",
            "classify 1.8846435546875\n",
            "classify 1.9735107421875\n",
            "classify 2.006103515625\n",
            "classify 1.9234619140625\n",
            "classify 1.9681396484375\n",
            "classify 2.07275390625\n",
            "classify 1.97412109375\n",
            "classify 1.997314453125\n",
            "classify 1.97784423828125\n",
            "classify 2.01318359375\n",
            "0.265625\n",
            "0.25\n",
            "0.28125\n",
            "0.21875\n",
            "0.296875\n",
            "0.28125\n",
            "0.1875\n",
            "0.34375\n",
            "0.234375\n",
            "0.25\n",
            "0.3125\n",
            "363\n",
            "strain 0.443394273519516\n",
            "strain 0.39490246772766113\n",
            "strain 0.405187726020813\n",
            "strain 0.27264150977134705\n",
            "strain 0.3657495081424713\n",
            "strain 0.27045953273773193\n",
            "strain 0.3870573341846466\n",
            "strain 0.2522937059402466\n",
            "strain 0.2967301607131958\n",
            "strain 0.4012300968170166\n",
            "strain 0.27446210384368896\n",
            "strain 0.2424616515636444\n",
            "strain 0.41557058691978455\n",
            "strain 0.38199254870414734\n",
            "strain 0.4148656129837036\n",
            "strain 0.34191450476646423\n",
            "strain 0.4482743442058563\n",
            "strain 0.3491729199886322\n",
            "strain 0.42767056822776794\n",
            "strain 0.3017655313014984\n",
            "strain 0.39057889580726624\n",
            "strain 0.4606586992740631\n",
            "strain 0.4239896237850189\n",
            "strain 0.32350820302963257\n",
            "strain 0.43152204155921936\n",
            "strain 0.30458658933639526\n",
            "strain 0.32110077142715454\n",
            "strain 0.4057111442089081\n",
            "strain 0.4052896201610565\n",
            "strain 0.3984951674938202\n",
            "strain 0.4497857391834259\n",
            "strain 0.27672427892684937\n",
            "strain 0.3884989321231842\n",
            "strain 0.32853779196739197\n",
            "strain 0.4134621024131775\n",
            "strain 0.3249283730983734\n",
            "strain 0.32635200023651123\n",
            "strain 0.3994045853614807\n",
            "strain 0.3056076467037201\n",
            "strain 0.2904090881347656\n",
            "strain 0.3836732506752014\n",
            "strain 0.3019971549510956\n",
            "strain 0.3530576229095459\n",
            "strain 0.33814460039138794\n",
            "strain 0.3160198926925659\n",
            "strain 0.31538617610931396\n",
            "strain 0.23802529275417328\n",
            "strain 0.27536898851394653\n",
            "strain 0.38590121269226074\n",
            "strain 0.29421770572662354\n",
            "strain 0.4223948121070862\n",
            "classify 2.16650390625\n",
            "classify 1.993896484375\n",
            "classify 2.0906982421875\n",
            "classify 1.9337158203125\n",
            "classify 1.9290771484375\n",
            "classify 1.948486328125\n",
            "classify 2.035400390625\n",
            "classify 1.94940185546875\n",
            "classify 2.0201416015625\n",
            "classify 1.95703125\n",
            "classify 1.92437744140625\n",
            "0.25\n",
            "0.296875\n",
            "0.265625\n",
            "0.296875\n",
            "0.234375\n",
            "0.265625\n",
            "0.1875\n",
            "0.3125\n",
            "0.265625\n",
            "0.1875\n",
            "0.359375\n",
            "364\n",
            "strain 0.4258069097995758\n",
            "strain 0.32909223437309265\n",
            "strain 0.3365832567214966\n",
            "strain 0.2596931457519531\n",
            "strain 0.3455648124217987\n",
            "strain 0.421362042427063\n",
            "strain 0.46325448155403137\n",
            "strain 0.2671908140182495\n",
            "strain 0.40599924325942993\n",
            "strain 0.32221031188964844\n",
            "strain 0.4113258719444275\n",
            "strain 0.3626496493816376\n",
            "strain 0.2944919466972351\n",
            "strain 0.36438143253326416\n",
            "strain 0.30908387899398804\n",
            "strain 0.49944809079170227\n",
            "strain 0.4062082767486572\n",
            "strain 0.3476620316505432\n",
            "strain 0.3655782639980316\n",
            "strain 0.29956820607185364\n",
            "strain 0.4570419490337372\n",
            "strain 0.345530241727829\n",
            "strain 0.2975517809391022\n",
            "strain 0.382108598947525\n",
            "strain 0.3178386390209198\n",
            "strain 0.4023655652999878\n",
            "strain 0.27601826190948486\n",
            "strain 0.2931975722312927\n",
            "strain 0.27297917008399963\n",
            "strain 0.3742419481277466\n",
            "strain 0.30160149931907654\n",
            "strain 0.31705886125564575\n",
            "strain 0.37655752897262573\n",
            "strain 0.3974296748638153\n",
            "strain 0.27757784724235535\n",
            "strain 0.3457243740558624\n",
            "strain 0.31980594992637634\n",
            "strain 0.46562644839286804\n",
            "strain 0.4083872139453888\n",
            "strain 0.49707648158073425\n",
            "strain 0.32753986120224\n",
            "strain 0.46970707178115845\n",
            "strain 0.39480480551719666\n",
            "strain 0.35672834515571594\n",
            "strain 0.39866992831230164\n",
            "strain 0.4195291996002197\n",
            "strain 0.2836049497127533\n",
            "strain 0.4518200159072876\n",
            "strain 0.3411135971546173\n",
            "strain 0.32277801632881165\n",
            "strain 0.385551780462265\n",
            "classify 2.091796875\n",
            "classify 1.70599365234375\n",
            "classify 1.9681396484375\n",
            "classify 1.91412353515625\n",
            "classify 2.029541015625\n",
            "classify 1.9901123046875\n",
            "classify 1.9873046875\n",
            "classify 1.81866455078125\n",
            "classify 1.832275390625\n",
            "classify 1.91217041015625\n",
            "classify 1.88604736328125\n",
            "0.234375\n",
            "0.34375\n",
            "0.34375\n",
            "0.328125\n",
            "0.328125\n",
            "0.390625\n",
            "0.25\n",
            "0.296875\n",
            "0.296875\n",
            "0.328125\n",
            "0.359375\n",
            "365\n",
            "strain 0.5625098943710327\n",
            "strain 0.3413968086242676\n",
            "strain 0.32833346724510193\n",
            "strain 0.4248872697353363\n",
            "strain 0.30053964257240295\n",
            "strain 0.33558541536331177\n",
            "strain 0.393593966960907\n",
            "strain 0.3561108708381653\n",
            "strain 0.4129258990287781\n",
            "strain 0.29895541071891785\n",
            "strain 0.304099977016449\n",
            "strain 0.322272926568985\n",
            "strain 0.2743838131427765\n",
            "strain 0.2781556248664856\n",
            "strain 0.40484532713890076\n",
            "strain 0.253665030002594\n",
            "strain 0.3348247706890106\n",
            "strain 0.28369709849357605\n",
            "strain 0.3765507936477661\n",
            "strain 0.3482745289802551\n",
            "strain 0.28504955768585205\n",
            "strain 0.23253634572029114\n",
            "strain 0.3590616285800934\n",
            "strain 0.2806472182273865\n",
            "strain 0.2752707898616791\n",
            "strain 0.2966073155403137\n",
            "strain 0.36835578083992004\n",
            "strain 0.33725881576538086\n",
            "strain 0.3846374452114105\n",
            "strain 0.4365357756614685\n",
            "strain 0.3689095377922058\n",
            "strain 0.34011736512184143\n",
            "strain 0.42680102586746216\n",
            "strain 0.36759430170059204\n",
            "strain 0.45215126872062683\n",
            "strain 0.38624370098114014\n",
            "strain 0.34228047728538513\n",
            "strain 0.4089601933956146\n",
            "strain 0.41484856605529785\n",
            "strain 0.4818287193775177\n",
            "strain 0.4456598162651062\n",
            "strain 0.2493828982114792\n",
            "strain 0.489533394575119\n",
            "strain 0.4321451187133789\n",
            "strain 0.4524137079715729\n",
            "strain 0.37967929244041443\n",
            "strain 0.2962094247341156\n",
            "strain 0.3873720169067383\n",
            "strain 0.2988297939300537\n",
            "strain 0.4956800639629364\n",
            "strain 0.32752054929733276\n",
            "classify 1.96630859375\n",
            "classify 1.94635009765625\n",
            "classify 1.89697265625\n",
            "classify 1.9600830078125\n",
            "classify 1.8277587890625\n",
            "classify 2.0335693359375\n",
            "classify 2.0206298828125\n",
            "classify 2.103271484375\n",
            "classify 2.040283203125\n",
            "classify 1.94384765625\n",
            "classify 1.8966064453125\n",
            "0.359375\n",
            "0.390625\n",
            "0.296875\n",
            "0.296875\n",
            "0.21875\n",
            "0.25\n",
            "0.296875\n",
            "0.296875\n",
            "0.15625\n",
            "0.265625\n",
            "0.328125\n",
            "366\n",
            "strain 0.3144388198852539\n",
            "strain 0.3123505711555481\n",
            "strain 0.2735086977481842\n",
            "strain 0.3161214590072632\n",
            "strain 0.3107340335845947\n",
            "strain 0.3419012427330017\n",
            "strain 0.34449058771133423\n",
            "strain 0.4072919189929962\n",
            "strain 0.3814874291419983\n",
            "strain 0.3576183319091797\n",
            "strain 0.3761241137981415\n",
            "strain 0.3406845033168793\n",
            "strain 0.4962444603443146\n",
            "strain 0.28674352169036865\n",
            "strain 0.4970822334289551\n",
            "strain 0.26396653056144714\n",
            "strain 0.3272998332977295\n",
            "strain 0.48752567172050476\n",
            "strain 0.30338799953460693\n",
            "strain 0.37855762243270874\n",
            "strain 0.36616548895835876\n",
            "strain 0.36481159925460815\n",
            "strain 0.3813228905200958\n",
            "strain 0.3858308494091034\n",
            "strain 0.30375936627388\n",
            "strain 0.3393420875072479\n",
            "strain 0.32128632068634033\n",
            "strain 0.3604012429714203\n",
            "strain 0.30813953280448914\n",
            "strain 0.3105770945549011\n",
            "strain 0.33457449078559875\n",
            "strain 0.36664852499961853\n",
            "strain 0.34117424488067627\n",
            "strain 0.38468503952026367\n",
            "strain 0.388141393661499\n",
            "strain 0.4473704993724823\n",
            "strain 0.37149491906166077\n",
            "strain 0.4112621247768402\n",
            "strain 0.38311606645584106\n",
            "strain 0.3935762047767639\n",
            "strain 0.33339017629623413\n",
            "strain 0.34579986333847046\n",
            "strain 0.38852494955062866\n",
            "strain 0.4308689832687378\n",
            "strain 0.3095931112766266\n",
            "strain 0.3754103481769562\n",
            "strain 0.274325966835022\n",
            "strain 0.4190504550933838\n",
            "strain 0.28202784061431885\n",
            "strain 0.41291821002960205\n",
            "strain 0.3558526933193207\n",
            "classify 1.92669677734375\n",
            "classify 1.8851318359375\n",
            "classify 1.864013671875\n",
            "classify 1.8702392578125\n",
            "classify 2.06158447265625\n",
            "classify 1.85888671875\n",
            "classify 1.973876953125\n",
            "classify 1.8966064453125\n",
            "classify 1.9794921875\n",
            "classify 1.967529296875\n",
            "classify 1.92779541015625\n",
            "0.359375\n",
            "0.28125\n",
            "0.15625\n",
            "0.234375\n",
            "0.296875\n",
            "0.25\n",
            "0.28125\n",
            "0.328125\n",
            "0.328125\n",
            "0.234375\n",
            "0.265625\n",
            "367\n",
            "strain 0.4125942885875702\n",
            "strain 0.4725140631198883\n",
            "strain 0.280162513256073\n",
            "strain 0.3992881178855896\n",
            "strain 0.3412698209285736\n",
            "strain 0.37427082657814026\n",
            "strain 0.46929579973220825\n",
            "strain 0.3826785683631897\n",
            "strain 0.34642255306243896\n",
            "strain 0.46248501539230347\n",
            "strain 0.32028913497924805\n",
            "strain 0.31800273060798645\n",
            "strain 0.26464322209358215\n",
            "strain 0.3344852924346924\n",
            "strain 0.3304462432861328\n",
            "strain 0.3423153758049011\n",
            "strain 0.4364429712295532\n",
            "strain 0.24665069580078125\n",
            "strain 0.40392470359802246\n",
            "strain 0.35571208596229553\n",
            "strain 0.3153340816497803\n",
            "strain 0.28878769278526306\n",
            "strain 0.34970104694366455\n",
            "strain 0.3581359386444092\n",
            "strain 0.3514212369918823\n",
            "strain 0.25418853759765625\n",
            "strain 0.3059154152870178\n",
            "strain 0.3624056279659271\n",
            "strain 0.4425983726978302\n",
            "strain 0.3024928569793701\n",
            "strain 0.47469761967658997\n",
            "strain 0.31297799944877625\n",
            "strain 0.2744113504886627\n",
            "strain 0.45593711733818054\n",
            "strain 0.4042815566062927\n",
            "strain 0.3939330279827118\n",
            "strain 0.36272093653678894\n",
            "strain 0.24944362044334412\n",
            "strain 0.38347044587135315\n",
            "strain 0.3408063054084778\n",
            "strain 0.2797972857952118\n",
            "strain 0.4061700105667114\n",
            "strain 0.2919897735118866\n",
            "strain 0.2816300690174103\n",
            "strain 0.38675522804260254\n",
            "strain 0.2426682710647583\n",
            "strain 0.24501857161521912\n",
            "strain 0.4675562083721161\n",
            "strain 0.4151475727558136\n",
            "strain 0.4033246338367462\n",
            "strain 0.2960098683834076\n",
            "classify 1.81317138671875\n",
            "classify 1.99920654296875\n",
            "classify 1.973876953125\n",
            "classify 2.007080078125\n",
            "classify 1.906005859375\n",
            "classify 2.1043701171875\n",
            "classify 1.8597412109375\n",
            "classify 1.916015625\n",
            "classify 2.004150390625\n",
            "classify 1.90325927734375\n",
            "classify 1.927490234375\n",
            "0.234375\n",
            "0.203125\n",
            "0.34375\n",
            "0.40625\n",
            "0.25\n",
            "0.3125\n",
            "0.25\n",
            "0.265625\n",
            "0.25\n",
            "0.25\n",
            "0.375\n",
            "368\n",
            "strain 0.392406165599823\n",
            "strain 0.3837921917438507\n",
            "strain 0.4825431704521179\n",
            "strain 0.32238781452178955\n",
            "strain 0.3648488223552704\n",
            "strain 0.43066808581352234\n",
            "strain 0.39297327399253845\n",
            "strain 0.384449303150177\n",
            "strain 0.42874953150749207\n",
            "strain 0.2988113462924957\n",
            "strain 0.2998628318309784\n",
            "strain 0.4441720247268677\n",
            "strain 0.44698065519332886\n",
            "strain 0.46059924364089966\n",
            "strain 0.42550110816955566\n",
            "strain 0.5177268385887146\n",
            "strain 0.4879079759120941\n",
            "strain 0.361478716135025\n",
            "strain 0.24327141046524048\n",
            "strain 0.30057334899902344\n",
            "strain 0.477669358253479\n",
            "strain 0.3117048442363739\n",
            "strain 0.3501875698566437\n",
            "strain 0.3373003304004669\n",
            "strain 0.3368309438228607\n",
            "strain 0.3384438753128052\n",
            "strain 0.2962913513183594\n",
            "strain 0.3302980363368988\n",
            "strain 0.3666279911994934\n",
            "strain 0.42466500401496887\n",
            "strain 0.3519122302532196\n",
            "strain 0.3597553074359894\n",
            "strain 0.48060640692710876\n",
            "strain 0.3112007677555084\n",
            "strain 0.4199519753456116\n",
            "strain 0.4766962230205536\n",
            "strain 0.3697229325771332\n",
            "strain 0.28532281517982483\n",
            "strain 0.42060673236846924\n",
            "strain 0.34591397643089294\n",
            "strain 0.3034306466579437\n",
            "strain 0.29207852482795715\n",
            "strain 0.28488555550575256\n",
            "strain 0.2915608882904053\n",
            "strain 0.2723356783390045\n",
            "strain 0.4454384446144104\n",
            "strain 0.3765963017940521\n",
            "strain 0.27220380306243896\n",
            "strain 0.2945680618286133\n",
            "strain 0.45052751898765564\n",
            "strain 0.42532628774642944\n",
            "classify 1.8856201171875\n",
            "classify 1.94677734375\n",
            "classify 1.92578125\n",
            "classify 1.9979248046875\n",
            "classify 1.957275390625\n",
            "classify 1.90447998046875\n",
            "classify 1.95025634765625\n",
            "classify 1.9136962890625\n",
            "classify 1.85699462890625\n",
            "classify 1.87286376953125\n",
            "classify 2.0032958984375\n",
            "0.328125\n",
            "0.171875\n",
            "0.359375\n",
            "0.25\n",
            "0.3125\n",
            "0.328125\n",
            "0.3125\n",
            "0.359375\n",
            "0.234375\n",
            "0.25\n",
            "0.25\n",
            "369\n",
            "strain 0.3488471508026123\n",
            "strain 0.3423350155353546\n",
            "strain 0.32772403955459595\n",
            "strain 0.32687610387802124\n",
            "strain 0.4324142336845398\n",
            "strain 0.3086695075035095\n",
            "strain 0.4804885983467102\n",
            "strain 0.38561445474624634\n",
            "strain 0.43425220251083374\n",
            "strain 0.42160138487815857\n",
            "strain 0.4783138930797577\n",
            "strain 0.3998289406299591\n",
            "strain 0.4388416111469269\n",
            "strain 0.32701027393341064\n",
            "strain 0.3889407217502594\n",
            "strain 0.3811909258365631\n",
            "strain 0.3738110363483429\n",
            "strain 0.2739463150501251\n",
            "strain 0.2843429148197174\n",
            "strain 0.547636866569519\n",
            "strain 0.32346686720848083\n",
            "strain 0.33703622221946716\n",
            "strain 0.3389923572540283\n",
            "strain 0.3634759783744812\n",
            "strain 0.3521875739097595\n",
            "strain 0.41811317205429077\n",
            "strain 0.38275349140167236\n",
            "strain 0.4177517890930176\n",
            "strain 0.46759045124053955\n",
            "strain 0.2583141624927521\n",
            "strain 0.4200669825077057\n",
            "strain 0.33653882145881653\n",
            "strain 0.30928969383239746\n",
            "strain 0.35434624552726746\n",
            "strain 0.23250053822994232\n",
            "strain 0.2901034653186798\n",
            "strain 0.3827739953994751\n",
            "strain 0.4586864709854126\n",
            "strain 0.39023557305336\n",
            "strain 0.37140825390815735\n",
            "strain 0.3662418723106384\n",
            "strain 0.23568665981292725\n",
            "strain 0.3404357433319092\n",
            "strain 0.32760560512542725\n",
            "strain 0.31062042713165283\n",
            "strain 0.41400107741355896\n",
            "strain 0.36642512679100037\n",
            "strain 0.3979954421520233\n",
            "strain 0.4545912742614746\n",
            "strain 0.2854529917240143\n",
            "strain 0.3847399055957794\n",
            "classify 2.03961181640625\n",
            "classify 1.9888916015625\n",
            "classify 1.81866455078125\n",
            "classify 2.0401611328125\n",
            "classify 1.96612548828125\n",
            "classify 1.9012451171875\n",
            "classify 1.9884033203125\n",
            "classify 1.86749267578125\n",
            "classify 1.947265625\n",
            "classify 1.92254638671875\n",
            "classify 1.9449462890625\n",
            "0.234375\n",
            "0.265625\n",
            "0.25\n",
            "0.3125\n",
            "0.265625\n",
            "0.28125\n",
            "0.265625\n",
            "0.296875\n",
            "0.34375\n",
            "0.34375\n",
            "0.21875\n",
            "370\n",
            "strain 0.26328012347221375\n",
            "strain 0.33612188696861267\n",
            "strain 0.4939277768135071\n",
            "strain 0.3220127820968628\n",
            "strain 0.35071682929992676\n",
            "strain 0.3150165379047394\n",
            "strain 0.40594303607940674\n",
            "strain 0.3131752610206604\n",
            "strain 0.28091368079185486\n",
            "strain 0.4367571473121643\n",
            "strain 0.38877755403518677\n",
            "strain 0.3045681118965149\n",
            "strain 0.34506040811538696\n",
            "strain 0.4021107852458954\n",
            "strain 0.33799436688423157\n",
            "strain 0.3657063841819763\n",
            "strain 0.36588019132614136\n",
            "strain 0.42777860164642334\n",
            "strain 0.2809130847454071\n",
            "strain 0.2772374749183655\n",
            "strain 0.4402766525745392\n",
            "strain 0.321003794670105\n",
            "strain 0.2878347635269165\n",
            "strain 0.34645625948905945\n",
            "strain 0.3712683916091919\n",
            "strain 0.44752344489097595\n",
            "strain 0.42698782682418823\n",
            "strain 0.3734363615512848\n",
            "strain 0.39682263135910034\n",
            "strain 0.33505457639694214\n",
            "strain 0.2676650285720825\n",
            "strain 0.2735525071620941\n",
            "strain 0.3298117518424988\n",
            "strain 0.28125\n",
            "strain 0.2872890830039978\n",
            "strain 0.2842581272125244\n",
            "strain 0.29782620072364807\n",
            "strain 0.36019167304039\n",
            "strain 0.3879072666168213\n",
            "strain 0.37840378284454346\n",
            "strain 0.36438626050949097\n",
            "strain 0.28171584010124207\n",
            "strain 0.3556307256221771\n",
            "strain 0.30595460534095764\n",
            "strain 0.28074854612350464\n",
            "strain 0.43110164999961853\n",
            "strain 0.6761254072189331\n",
            "strain 0.4197007417678833\n",
            "strain 0.4151691794395447\n",
            "strain 0.41566693782806396\n",
            "strain 0.37741947174072266\n",
            "classify 1.9752197265625\n",
            "classify 1.98114013671875\n",
            "classify 1.9102783203125\n",
            "classify 1.987548828125\n",
            "classify 1.876953125\n",
            "classify 2.00946044921875\n",
            "classify 2.09564208984375\n",
            "classify 1.9984130859375\n",
            "classify 1.90087890625\n",
            "classify 1.9844970703125\n",
            "classify 1.9193115234375\n",
            "0.296875\n",
            "0.1875\n",
            "0.34375\n",
            "0.21875\n",
            "0.203125\n",
            "0.375\n",
            "0.3125\n",
            "0.234375\n",
            "0.265625\n",
            "0.25\n",
            "0.265625\n",
            "371\n",
            "strain 0.40529799461364746\n",
            "strain 0.28513091802597046\n",
            "strain 0.35501205921173096\n",
            "strain 0.3670942485332489\n",
            "strain 0.3845551908016205\n",
            "strain 0.38559481501579285\n",
            "strain 0.27287599444389343\n",
            "strain 0.4022884666919708\n",
            "strain 0.330651193857193\n",
            "strain 0.30717888474464417\n",
            "strain 0.4314654469490051\n",
            "strain 0.4201751947402954\n",
            "strain 0.2800496518611908\n",
            "strain 0.30308908224105835\n",
            "strain 0.35728713870048523\n",
            "strain 0.49380630254745483\n",
            "strain 0.2986419200897217\n",
            "strain 0.3022225499153137\n",
            "strain 0.35826554894447327\n",
            "strain 0.4960986375808716\n",
            "strain 0.394793301820755\n",
            "strain 0.32048237323760986\n",
            "strain 0.33947932720184326\n",
            "strain 0.32709547877311707\n",
            "strain 0.29712992906570435\n",
            "strain 0.29123079776763916\n",
            "strain 0.27539610862731934\n",
            "strain 0.3502201735973358\n",
            "strain 0.4796017110347748\n",
            "strain 0.37470120191574097\n",
            "strain 0.549187421798706\n",
            "strain 0.2962426245212555\n",
            "strain 0.3701360523700714\n",
            "strain 0.3917137384414673\n",
            "strain 0.4388650059700012\n",
            "strain 0.2957848906517029\n",
            "strain 0.4461427927017212\n",
            "strain 0.40453577041625977\n",
            "strain 0.3036941587924957\n",
            "strain 0.42173266410827637\n",
            "strain 0.3749208450317383\n",
            "strain 0.3775062561035156\n",
            "strain 0.2852320969104767\n",
            "strain 0.3093975782394409\n",
            "strain 0.3823411762714386\n",
            "strain 0.35090744495391846\n",
            "strain 0.2959967851638794\n",
            "strain 0.5043615102767944\n",
            "strain 0.4357971251010895\n",
            "strain 0.3112046718597412\n",
            "strain 0.30819129943847656\n",
            "classify 1.9560546875\n",
            "classify 1.89617919921875\n",
            "classify 2.06494140625\n",
            "classify 2.0294189453125\n",
            "classify 1.9205322265625\n",
            "classify 2.020263671875\n",
            "classify 1.8375244140625\n",
            "classify 1.88287353515625\n",
            "classify 2.0479736328125\n",
            "classify 1.829345703125\n",
            "classify 1.915283203125\n",
            "0.25\n",
            "0.265625\n",
            "0.28125\n",
            "0.265625\n",
            "0.328125\n",
            "0.3125\n",
            "0.28125\n",
            "0.296875\n",
            "0.28125\n",
            "0.359375\n",
            "0.3125\n",
            "372\n",
            "strain 0.5129757523536682\n",
            "strain 0.3787795901298523\n",
            "strain 0.3214206099510193\n",
            "strain 0.27228400111198425\n",
            "strain 0.3093884289264679\n",
            "strain 0.34720540046691895\n",
            "strain 0.32008665800094604\n",
            "strain 0.34382253885269165\n",
            "strain 0.4170604944229126\n",
            "strain 0.44368866086006165\n",
            "strain 0.39925864338874817\n",
            "strain 0.2773412764072418\n",
            "strain 0.38180771470069885\n",
            "strain 0.4056415557861328\n",
            "strain 0.45636895298957825\n",
            "strain 0.35100021958351135\n",
            "strain 0.39628148078918457\n",
            "strain 0.3912018835544586\n",
            "strain 0.35159215331077576\n",
            "strain 0.2572835683822632\n",
            "strain 0.42991769313812256\n",
            "strain 0.43051978945732117\n",
            "strain 0.43519026041030884\n",
            "strain 0.30425551533699036\n",
            "strain 0.3038763105869293\n",
            "strain 0.39129576086997986\n",
            "strain 0.43751028180122375\n",
            "strain 0.383709579706192\n",
            "strain 0.2720238268375397\n",
            "strain 0.36771371960639954\n",
            "strain 0.2683296501636505\n",
            "strain 0.3389318585395813\n",
            "strain 0.47954675555229187\n",
            "strain 0.3604913651943207\n",
            "strain 0.44128715991973877\n",
            "strain 0.2613518536090851\n",
            "strain 0.39426738023757935\n",
            "strain 0.43441230058670044\n",
            "strain 0.30948710441589355\n",
            "strain 0.4012202322483063\n",
            "strain 0.27843475341796875\n",
            "strain 0.28390026092529297\n",
            "strain 0.32767239212989807\n",
            "strain 0.31809207797050476\n",
            "strain 0.3350702226161957\n",
            "strain 0.3418824076652527\n",
            "strain 0.3418070077896118\n",
            "strain 0.4077424705028534\n",
            "strain 0.3198293149471283\n",
            "strain 0.29655763506889343\n",
            "strain 0.33101293444633484\n",
            "classify 2.186279296875\n",
            "classify 1.9581298828125\n",
            "classify 1.85821533203125\n",
            "classify 1.9307861328125\n",
            "classify 1.9493408203125\n",
            "classify 1.92645263671875\n",
            "classify 1.9091796875\n",
            "classify 1.9595947265625\n",
            "classify 1.9039306640625\n",
            "classify 1.951171875\n",
            "classify 1.910400390625\n",
            "0.234375\n",
            "0.3125\n",
            "0.390625\n",
            "0.359375\n",
            "0.3125\n",
            "0.234375\n",
            "0.296875\n",
            "0.28125\n",
            "0.21875\n",
            "0.375\n",
            "0.296875\n",
            "373\n",
            "strain 0.4010404646396637\n",
            "strain 0.3084929585456848\n",
            "strain 0.46199607849121094\n",
            "strain 0.2780892252922058\n",
            "strain 0.2514328062534332\n",
            "strain 0.3111274242401123\n",
            "strain 0.4150126576423645\n",
            "strain 0.36579716205596924\n",
            "strain 0.41596370935440063\n",
            "strain 0.4397692084312439\n",
            "strain 0.4422857165336609\n",
            "strain 0.41264772415161133\n",
            "strain 0.4908328056335449\n",
            "strain 0.41958266496658325\n",
            "strain 0.289025217294693\n",
            "strain 0.3810769319534302\n",
            "strain 0.2732795774936676\n",
            "strain 0.2521960735321045\n",
            "strain 0.38891157507896423\n",
            "strain 0.30487361550331116\n",
            "strain 0.4790771007537842\n",
            "strain 0.382996141910553\n",
            "strain 0.4708127975463867\n",
            "strain 0.39658185839653015\n",
            "strain 0.3451545238494873\n",
            "strain 0.25460541248321533\n",
            "strain 0.3218088150024414\n",
            "strain 0.3492729365825653\n",
            "strain 0.31192201375961304\n",
            "strain 0.28153419494628906\n",
            "strain 0.48082736134529114\n",
            "strain 0.485879123210907\n",
            "strain 0.2935490012168884\n",
            "strain 0.32670217752456665\n",
            "strain 0.3938136100769043\n",
            "strain 0.36778131127357483\n",
            "strain 0.4196809232234955\n",
            "strain 0.4929473102092743\n",
            "strain 0.42801201343536377\n",
            "strain 0.27180156111717224\n",
            "strain 0.446614146232605\n",
            "strain 0.4295065104961395\n",
            "strain 0.4657476842403412\n",
            "strain 0.376799076795578\n",
            "strain 0.4740496277809143\n",
            "strain 0.3466261625289917\n",
            "strain 0.34252220392227173\n",
            "strain 0.3805166184902191\n",
            "strain 0.3743380308151245\n",
            "strain 0.2444572150707245\n",
            "strain 0.31141892075538635\n",
            "classify 1.949462890625\n",
            "classify 2.045166015625\n",
            "classify 1.93414306640625\n",
            "classify 1.97119140625\n",
            "classify 1.98236083984375\n",
            "classify 1.870361328125\n",
            "classify 1.892333984375\n",
            "classify 1.86761474609375\n",
            "classify 1.9224853515625\n",
            "classify 1.99169921875\n",
            "classify 1.9434814453125\n",
            "0.21875\n",
            "0.296875\n",
            "0.21875\n",
            "0.25\n",
            "0.265625\n",
            "0.296875\n",
            "0.34375\n",
            "0.25\n",
            "0.234375\n",
            "0.28125\n",
            "0.328125\n",
            "374\n",
            "strain 0.23839157819747925\n",
            "strain 0.3258591592311859\n",
            "strain 0.40139028429985046\n",
            "strain 0.26609253883361816\n",
            "strain 0.27830013632774353\n",
            "strain 0.5033783316612244\n",
            "strain 0.40497925877571106\n",
            "strain 0.3636012077331543\n",
            "strain 0.44907188415527344\n",
            "strain 0.48131030797958374\n",
            "strain 0.2514829635620117\n",
            "strain 0.41927570104599\n",
            "strain 0.45571038126945496\n",
            "strain 0.3354867100715637\n",
            "strain 0.36068084836006165\n",
            "strain 0.43846395611763\n",
            "strain 0.38737863302230835\n",
            "strain 0.3673975467681885\n",
            "strain 0.4517545700073242\n",
            "strain 0.30995112657546997\n",
            "strain 0.3843815326690674\n",
            "strain 0.37240785360336304\n",
            "strain 0.49505284428596497\n",
            "strain 0.3603714406490326\n",
            "strain 0.3217846751213074\n",
            "strain 0.2624223530292511\n",
            "strain 0.33785849809646606\n",
            "strain 0.33053284883499146\n",
            "strain 0.3516075313091278\n",
            "strain 0.27697649598121643\n",
            "strain 0.42603692412376404\n",
            "strain 0.28270840644836426\n",
            "strain 0.37219831347465515\n",
            "strain 0.43156862258911133\n",
            "strain 0.43562424182891846\n",
            "strain 0.3879035711288452\n",
            "strain 0.3850019872188568\n",
            "strain 0.27555790543556213\n",
            "strain 0.4546368420124054\n",
            "strain 0.31567490100860596\n",
            "strain 0.4095578193664551\n",
            "strain 0.4025455713272095\n",
            "strain 0.32923826575279236\n",
            "strain 0.28599119186401367\n",
            "strain 0.3795683979988098\n",
            "strain 0.25924450159072876\n",
            "strain 0.41367781162261963\n",
            "strain 0.34747937321662903\n",
            "strain 0.29719725251197815\n",
            "strain 0.4336419105529785\n",
            "strain 0.3431416451931\n",
            "classify 1.89752197265625\n",
            "classify 1.8699951171875\n",
            "classify 1.91278076171875\n",
            "classify 1.8951416015625\n",
            "classify 1.892333984375\n",
            "classify 2.00634765625\n",
            "classify 1.9150390625\n",
            "classify 1.9700927734375\n",
            "classify 1.88983154296875\n",
            "classify 1.98992919921875\n",
            "classify 2.02105712890625\n",
            "0.265625\n",
            "0.296875\n",
            "0.109375\n",
            "0.28125\n",
            "0.3125\n",
            "0.3125\n",
            "0.328125\n",
            "0.359375\n",
            "0.25\n",
            "0.25\n",
            "0.3125\n",
            "375\n",
            "strain 0.3056488633155823\n",
            "strain 0.24436905980110168\n",
            "strain 0.29221218824386597\n",
            "strain 0.33769407868385315\n",
            "strain 0.310651957988739\n",
            "strain 0.2713315188884735\n",
            "strain 0.3034636974334717\n",
            "strain 0.4804527163505554\n",
            "strain 0.38276898860931396\n",
            "strain 0.38509833812713623\n",
            "strain 0.321107417345047\n",
            "strain 0.4078829288482666\n",
            "strain 0.3038036823272705\n",
            "strain 0.319904625415802\n",
            "strain 0.2697901129722595\n",
            "strain 0.3329501152038574\n",
            "strain 0.42271944880485535\n",
            "strain 0.2649438679218292\n",
            "strain 0.3447072207927704\n",
            "strain 0.34901395440101624\n",
            "strain 0.3052118122577667\n",
            "strain 0.44036048650741577\n",
            "strain 0.3495038151741028\n",
            "strain 0.3495638072490692\n",
            "strain 0.3190939724445343\n",
            "strain 0.41759300231933594\n",
            "strain 0.24864761531352997\n",
            "strain 0.3206080496311188\n",
            "strain 0.4879184663295746\n",
            "strain 0.42229658365249634\n",
            "strain 0.37855780124664307\n",
            "strain 0.3684036433696747\n",
            "strain 0.27416521310806274\n",
            "strain 0.3522981107234955\n",
            "strain 0.45288026332855225\n",
            "strain 0.34413671493530273\n",
            "strain 0.3320784866809845\n",
            "strain 0.441725492477417\n",
            "strain 0.3879005014896393\n",
            "strain 0.30841147899627686\n",
            "strain 0.47607356309890747\n",
            "strain 0.2844516336917877\n",
            "strain 0.3907487392425537\n",
            "strain 0.4085558354854584\n",
            "strain 0.49923035502433777\n",
            "strain 0.4602264165878296\n",
            "strain 0.3754481077194214\n",
            "strain 0.45465725660324097\n",
            "strain 0.40262895822525024\n",
            "strain 0.4259319305419922\n",
            "strain 0.4310545027256012\n",
            "classify 1.9769287109375\n",
            "classify 1.8525390625\n",
            "classify 1.83380126953125\n",
            "classify 1.76348876953125\n",
            "classify 1.9368896484375\n",
            "classify 1.921142578125\n",
            "classify 1.921630859375\n",
            "classify 1.85699462890625\n",
            "classify 1.96600341796875\n",
            "classify 2.1307373046875\n",
            "classify 1.8438720703125\n",
            "0.296875\n",
            "0.390625\n",
            "0.3125\n",
            "0.1875\n",
            "0.25\n",
            "0.3125\n",
            "0.265625\n",
            "0.3125\n",
            "0.328125\n",
            "0.25\n",
            "0.203125\n",
            "376\n",
            "strain 0.42953789234161377\n",
            "strain 0.38871482014656067\n",
            "strain 0.42853519320487976\n",
            "strain 0.2916538119316101\n",
            "strain 0.4132041931152344\n",
            "strain 0.3257998824119568\n",
            "strain 0.3272828757762909\n",
            "strain 0.36611223220825195\n",
            "strain 0.5387646555900574\n",
            "strain 0.38442152738571167\n",
            "strain 0.35633283853530884\n",
            "strain 0.3645590841770172\n",
            "strain 0.27926045656204224\n",
            "strain 0.40562015771865845\n",
            "strain 0.3666168749332428\n",
            "strain 0.41955050826072693\n",
            "strain 0.3119596838951111\n",
            "strain 0.35520225763320923\n",
            "strain 0.2797689437866211\n",
            "strain 0.29325351119041443\n",
            "strain 0.4343121647834778\n",
            "strain 0.35295575857162476\n",
            "strain 0.32239779829978943\n",
            "strain 0.28582021594047546\n",
            "strain 0.342659056186676\n",
            "strain 0.39270296692848206\n",
            "strain 0.37079691886901855\n",
            "strain 0.26661717891693115\n",
            "strain 0.3986280560493469\n",
            "strain 0.3218373954296112\n",
            "strain 0.330902636051178\n",
            "strain 0.4237668514251709\n",
            "strain 0.37303146719932556\n",
            "strain 0.25703954696655273\n",
            "strain 0.3779680132865906\n",
            "strain 0.4030555188655853\n",
            "strain 0.4145538806915283\n",
            "strain 0.4314858615398407\n",
            "strain 0.45261383056640625\n",
            "strain 0.39215660095214844\n",
            "strain 0.39720618724823\n",
            "strain 0.26582077145576477\n",
            "strain 0.3219491243362427\n",
            "strain 0.2861412465572357\n",
            "strain 0.31527841091156006\n",
            "strain 0.3345715403556824\n",
            "strain 0.3559093177318573\n",
            "strain 0.4874650239944458\n",
            "strain 0.3728257417678833\n",
            "strain 0.3153400421142578\n",
            "strain 0.3330773711204529\n",
            "classify 1.81622314453125\n",
            "classify 2.02655029296875\n",
            "classify 2.05029296875\n",
            "classify 2.0166015625\n",
            "classify 1.87664794921875\n",
            "classify 1.9296875\n",
            "classify 1.9107666015625\n",
            "classify 1.89154052734375\n",
            "classify 1.87548828125\n",
            "classify 2.0081787109375\n",
            "classify 1.9376220703125\n",
            "0.28125\n",
            "0.25\n",
            "0.359375\n",
            "0.234375\n",
            "0.265625\n",
            "0.28125\n",
            "0.3125\n",
            "0.34375\n",
            "0.21875\n",
            "0.328125\n",
            "0.25\n",
            "377\n",
            "strain 0.41959020495414734\n",
            "strain 0.4497389793395996\n",
            "strain 0.3068884015083313\n",
            "strain 0.3732486069202423\n",
            "strain 0.411722332239151\n",
            "strain 0.3058030307292938\n",
            "strain 0.33881548047065735\n",
            "strain 0.3478311002254486\n",
            "strain 0.3558882474899292\n",
            "strain 0.257408082485199\n",
            "strain 0.3202948570251465\n",
            "strain 0.4332537353038788\n",
            "strain 0.47195306420326233\n",
            "strain 0.375131219625473\n",
            "strain 0.47266870737075806\n",
            "strain 0.3809655010700226\n",
            "strain 0.3531532883644104\n",
            "strain 0.34086307883262634\n",
            "strain 0.27106210589408875\n",
            "strain 0.4184158742427826\n",
            "strain 0.3387300670146942\n",
            "strain 0.28944966197013855\n",
            "strain 0.4475584626197815\n",
            "strain 0.2508021593093872\n",
            "strain 0.30312904715538025\n",
            "strain 0.38981860876083374\n",
            "strain 0.41910800337791443\n",
            "strain 0.3342626094818115\n",
            "strain 0.4896218776702881\n",
            "strain 0.38231536746025085\n",
            "strain 0.290758341550827\n",
            "strain 0.41671907901763916\n",
            "strain 0.3493698537349701\n",
            "strain 0.3575078845024109\n",
            "strain 0.2945821285247803\n",
            "strain 0.337380975484848\n",
            "strain 0.26658111810684204\n",
            "strain 0.28142106533050537\n",
            "strain 0.334966778755188\n",
            "strain 0.40765249729156494\n",
            "strain 0.3156082034111023\n",
            "strain 0.4657175540924072\n",
            "strain 0.33277463912963867\n",
            "strain 0.2887205481529236\n",
            "strain 0.41880351305007935\n",
            "strain 0.3764066696166992\n",
            "strain 0.3139759302139282\n",
            "strain 0.3573199212551117\n",
            "strain 0.3437754809856415\n",
            "strain 0.37834855914115906\n",
            "strain 0.2878072261810303\n",
            "classify 1.88372802734375\n",
            "classify 2.0205078125\n",
            "classify 1.96978759765625\n",
            "classify 2.0694580078125\n",
            "classify 1.96112060546875\n",
            "classify 2.0391845703125\n",
            "classify 1.862060546875\n",
            "classify 2.04931640625\n",
            "classify 1.7276611328125\n",
            "classify 1.863525390625\n",
            "classify 1.87860107421875\n",
            "0.28125\n",
            "0.359375\n",
            "0.359375\n",
            "0.265625\n",
            "0.3125\n",
            "0.296875\n",
            "0.296875\n",
            "0.296875\n",
            "0.234375\n",
            "0.171875\n",
            "0.265625\n",
            "378\n",
            "strain 0.3282114863395691\n",
            "strain 0.4257117509841919\n",
            "strain 0.5608752369880676\n",
            "strain 0.3051886558532715\n",
            "strain 0.3784237205982208\n",
            "strain 0.36075130105018616\n",
            "strain 0.2983929514884949\n",
            "strain 0.3770039677619934\n",
            "strain 0.24463413655757904\n",
            "strain 0.40405285358428955\n",
            "strain 0.4218086898326874\n",
            "strain 0.2945203483104706\n",
            "strain 0.3787475526332855\n",
            "strain 0.28655490279197693\n",
            "strain 0.47036197781562805\n",
            "strain 0.33402135968208313\n",
            "strain 0.42536288499832153\n",
            "strain 0.3728950619697571\n",
            "strain 0.297980397939682\n",
            "strain 0.42575711011886597\n",
            "strain 0.34532222151756287\n",
            "strain 0.40290722250938416\n",
            "strain 0.3712279498577118\n",
            "strain 0.442646861076355\n",
            "strain 0.4274332821369171\n",
            "strain 0.28947722911834717\n",
            "strain 0.4574407637119293\n",
            "strain 0.3653285801410675\n",
            "strain 0.35610607266426086\n",
            "strain 0.2986215054988861\n",
            "strain 0.3935573995113373\n",
            "strain 0.27435627579689026\n",
            "strain 0.4044225811958313\n",
            "strain 0.33988863229751587\n",
            "strain 0.3254147171974182\n",
            "strain 0.3585965633392334\n",
            "strain 0.3745974600315094\n",
            "strain 0.3833061754703522\n",
            "strain 0.2956556975841522\n",
            "strain 0.37786731123924255\n",
            "strain 0.3403223752975464\n",
            "strain 0.43865296244621277\n",
            "strain 0.2909620404243469\n",
            "strain 0.5163013935089111\n",
            "strain 0.2835148274898529\n",
            "strain 0.4140937030315399\n",
            "strain 0.4071928560733795\n",
            "strain 0.4731208086013794\n",
            "strain 0.4885043501853943\n",
            "strain 0.41077426075935364\n",
            "strain 0.3666556179523468\n",
            "classify 1.9244384765625\n",
            "classify 1.77532958984375\n",
            "classify 1.9549560546875\n",
            "classify 1.9013671875\n",
            "classify 1.9300537109375\n",
            "classify 1.9527587890625\n",
            "classify 1.84222412109375\n",
            "classify 1.977294921875\n",
            "classify 2.00103759765625\n",
            "classify 1.927978515625\n",
            "classify 1.89923095703125\n",
            "0.234375\n",
            "0.296875\n",
            "0.3125\n",
            "0.328125\n",
            "0.328125\n",
            "0.265625\n",
            "0.1875\n",
            "0.28125\n",
            "0.265625\n",
            "0.234375\n",
            "0.296875\n",
            "379\n",
            "strain 0.31731322407722473\n",
            "strain 0.4207542836666107\n",
            "strain 0.2855932414531708\n",
            "strain 0.42720848321914673\n",
            "strain 0.3306480944156647\n",
            "strain 0.38160520792007446\n",
            "strain 0.3836078345775604\n",
            "strain 0.4015004634857178\n",
            "strain 0.37033095955848694\n",
            "strain 0.43250730633735657\n",
            "strain 0.32799530029296875\n",
            "strain 0.31691986322402954\n",
            "strain 0.42034685611724854\n",
            "strain 0.45147326588630676\n",
            "strain 0.40565308928489685\n",
            "strain 0.3251264989376068\n",
            "strain 0.40569016337394714\n",
            "strain 0.2955331802368164\n",
            "strain 0.39448079466819763\n",
            "strain 0.4090285003185272\n",
            "strain 0.3079271912574768\n",
            "strain 0.3885972201824188\n",
            "strain 0.3625684976577759\n",
            "strain 0.3147490322589874\n",
            "strain 0.40174540877342224\n",
            "strain 0.32286137342453003\n",
            "strain 0.3721575438976288\n",
            "strain 0.3756851553916931\n",
            "strain 0.44014009833335876\n",
            "strain 0.39960935711860657\n",
            "strain 0.3015604019165039\n",
            "strain 0.3211124837398529\n",
            "strain 0.3201015889644623\n",
            "strain 0.2830027639865875\n",
            "strain 0.36517274379730225\n",
            "strain 0.40740373730659485\n",
            "strain 0.42281681299209595\n",
            "strain 0.3820287585258484\n",
            "strain 0.30775630474090576\n",
            "strain 0.5013386607170105\n",
            "strain 0.2833317220211029\n",
            "strain 0.37370648980140686\n",
            "strain 0.2916642427444458\n",
            "strain 0.4089583456516266\n",
            "strain 0.4528287649154663\n",
            "strain 0.3613699674606323\n",
            "strain 0.3550177216529846\n",
            "strain 0.3425329029560089\n",
            "strain 0.35364246368408203\n",
            "strain 0.24600888788700104\n",
            "strain 0.3395840525627136\n",
            "classify 1.88494873046875\n",
            "classify 1.9791259765625\n",
            "classify 1.9373779296875\n",
            "classify 1.8328857421875\n",
            "classify 1.9249267578125\n",
            "classify 1.94256591796875\n",
            "classify 1.92620849609375\n",
            "classify 1.8935546875\n",
            "classify 1.97216796875\n",
            "classify 1.9598388671875\n",
            "classify 1.92462158203125\n",
            "0.390625\n",
            "0.34375\n",
            "0.390625\n",
            "0.3125\n",
            "0.359375\n",
            "0.359375\n",
            "0.296875\n",
            "0.421875\n",
            "0.203125\n",
            "0.265625\n",
            "0.390625\n",
            "380\n",
            "strain 0.26497331261634827\n",
            "strain 0.33818256855010986\n",
            "strain 0.32388174533843994\n",
            "strain 0.3649483323097229\n",
            "strain 0.2882848083972931\n",
            "strain 0.466266006231308\n",
            "strain 0.2805890738964081\n",
            "strain 0.32163313031196594\n",
            "strain 0.4175764322280884\n",
            "strain 0.4813283383846283\n",
            "strain 0.2835979759693146\n",
            "strain 0.2906094491481781\n",
            "strain 0.4335515797138214\n",
            "strain 0.34605664014816284\n",
            "strain 0.3559209704399109\n",
            "strain 0.2818869352340698\n",
            "strain 0.28321564197540283\n",
            "strain 0.3800638020038605\n",
            "strain 0.2555314898490906\n",
            "strain 0.33255714178085327\n",
            "strain 0.32070392370224\n",
            "strain 0.28599444031715393\n",
            "strain 0.3638223707675934\n",
            "strain 0.3080267608165741\n",
            "strain 0.3131774961948395\n",
            "strain 0.3792242407798767\n",
            "strain 0.42454493045806885\n",
            "strain 0.34985655546188354\n",
            "strain 0.332761287689209\n",
            "strain 0.47114554047584534\n",
            "strain 0.3696240782737732\n",
            "strain 0.39530497789382935\n",
            "strain 0.3217911720275879\n",
            "strain 0.39160048961639404\n",
            "strain 0.35982975363731384\n",
            "strain 0.43401795625686646\n",
            "strain 0.34214305877685547\n",
            "strain 0.4010351300239563\n",
            "strain 0.3352663815021515\n",
            "strain 0.38452157378196716\n",
            "strain 0.3338485062122345\n",
            "strain 0.3792276978492737\n",
            "strain 0.3412463665008545\n",
            "strain 0.3543737828731537\n",
            "strain 0.3064519166946411\n",
            "strain 0.4456746578216553\n",
            "strain 0.2514696717262268\n",
            "strain 0.33946773409843445\n",
            "strain 0.3495199680328369\n",
            "strain 0.2905072867870331\n",
            "strain 0.3703591227531433\n",
            "classify 1.927490234375\n",
            "classify 1.9046630859375\n",
            "classify 1.98828125\n",
            "classify 1.9439697265625\n",
            "classify 1.83148193359375\n",
            "classify 2.03857421875\n",
            "classify 1.875\n",
            "classify 1.91748046875\n",
            "classify 1.97607421875\n",
            "classify 1.94873046875\n",
            "classify 1.78997802734375\n",
            "0.375\n",
            "0.1875\n",
            "0.265625\n",
            "0.25\n",
            "0.25\n",
            "0.328125\n",
            "0.28125\n",
            "0.375\n",
            "0.3125\n",
            "0.296875\n",
            "0.3125\n",
            "381\n",
            "strain 0.5007706880569458\n",
            "strain 0.28060171008110046\n",
            "strain 0.3524845540523529\n",
            "strain 0.446257084608078\n",
            "strain 0.33525118231773376\n",
            "strain 0.30184584856033325\n",
            "strain 0.31529608368873596\n",
            "strain 0.40990105271339417\n",
            "strain 0.43432289361953735\n",
            "strain 0.4838082790374756\n",
            "strain 0.3975800573825836\n",
            "strain 0.363375186920166\n",
            "strain 0.330172598361969\n",
            "strain 0.2991197109222412\n",
            "strain 0.3541561961174011\n",
            "strain 0.4048396348953247\n",
            "strain 0.27562010288238525\n",
            "strain 0.41530370712280273\n",
            "strain 0.375329852104187\n",
            "strain 0.31666725873947144\n",
            "strain 0.4010365605354309\n",
            "strain 0.34142738580703735\n",
            "strain 0.5098495483398438\n",
            "strain 0.40217143297195435\n",
            "strain 0.47015270590782166\n",
            "strain 0.36839592456817627\n",
            "strain 0.28819119930267334\n",
            "strain 0.3984057307243347\n",
            "strain 0.2970881760120392\n",
            "strain 0.3886837959289551\n",
            "strain 0.41994452476501465\n",
            "strain 0.504460871219635\n",
            "strain 0.27635958790779114\n",
            "strain 0.39511838555336\n",
            "strain 0.309491902589798\n",
            "strain 0.3576170802116394\n",
            "strain 0.32937246561050415\n",
            "strain 0.2504454553127289\n",
            "strain 0.33125653862953186\n",
            "strain 0.38664597272872925\n",
            "strain 0.2860240936279297\n",
            "strain 0.28022632002830505\n",
            "strain 0.33599090576171875\n",
            "strain 0.3053467571735382\n",
            "strain 0.3103472590446472\n",
            "strain 0.3359355330467224\n",
            "strain 0.45658180117607117\n",
            "strain 0.2898280918598175\n",
            "strain 0.3568241596221924\n",
            "strain 0.4423912763595581\n",
            "strain 0.2995990514755249\n",
            "classify 1.728515625\n",
            "classify 1.86865234375\n",
            "classify 2.020263671875\n",
            "classify 1.80487060546875\n",
            "classify 1.9964599609375\n",
            "classify 1.9632568359375\n",
            "classify 1.94110107421875\n",
            "classify 1.928955078125\n",
            "classify 1.91131591796875\n",
            "classify 1.95489501953125\n",
            "classify 2.0673828125\n",
            "0.21875\n",
            "0.28125\n",
            "0.28125\n",
            "0.34375\n",
            "0.3125\n",
            "0.296875\n",
            "0.296875\n",
            "0.3125\n",
            "0.34375\n",
            "0.203125\n",
            "0.3125\n",
            "382\n",
            "strain 0.3720361888408661\n",
            "strain 0.38967829942703247\n",
            "strain 0.4588756561279297\n",
            "strain 0.36755943298339844\n",
            "strain 0.3791939616203308\n",
            "strain 0.315281480550766\n",
            "strain 0.28790727257728577\n",
            "strain 0.438029021024704\n",
            "strain 0.2830768823623657\n",
            "strain 0.2655128240585327\n",
            "strain 0.3286901116371155\n",
            "strain 0.44593381881713867\n",
            "strain 0.26336926221847534\n",
            "strain 0.2592594623565674\n",
            "strain 0.2688068151473999\n",
            "strain 0.46546176075935364\n",
            "strain 0.3236982226371765\n",
            "strain 0.35355502367019653\n",
            "strain 0.34720686078071594\n",
            "strain 0.3755834102630615\n",
            "strain 0.46648725867271423\n",
            "strain 0.4039255380630493\n",
            "strain 0.4049498736858368\n",
            "strain 0.45781397819519043\n",
            "strain 0.3525156080722809\n",
            "strain 0.3314518630504608\n",
            "strain 0.471676230430603\n",
            "strain 0.4004783034324646\n",
            "strain 0.4252614676952362\n",
            "strain 0.42843908071517944\n",
            "strain 0.37951406836509705\n",
            "strain 0.49955934286117554\n",
            "strain 0.4192841947078705\n",
            "strain 0.37134483456611633\n",
            "strain 0.44183045625686646\n",
            "strain 0.3381926417350769\n",
            "strain 0.4117870330810547\n",
            "strain 0.587309718132019\n",
            "strain 0.3695433437824249\n",
            "strain 0.3944300413131714\n",
            "strain 0.2914835810661316\n",
            "strain 0.36346012353897095\n",
            "strain 0.41729652881622314\n",
            "strain 0.42759010195732117\n",
            "strain 0.3365626037120819\n",
            "strain 0.3021917939186096\n",
            "strain 0.30182120203971863\n",
            "strain 0.28214573860168457\n",
            "strain 0.4349365830421448\n",
            "strain 0.44984525442123413\n",
            "strain 0.3633735775947571\n",
            "classify 1.94781494140625\n",
            "classify 1.80035400390625\n",
            "classify 1.9747314453125\n",
            "classify 1.9752197265625\n",
            "classify 2.0001220703125\n",
            "classify 1.91412353515625\n",
            "classify 1.9921875\n",
            "classify 1.880126953125\n",
            "classify 1.74993896484375\n",
            "classify 1.91558837890625\n",
            "classify 1.80670166015625\n",
            "0.3125\n",
            "0.375\n",
            "0.28125\n",
            "0.25\n",
            "0.375\n",
            "0.328125\n",
            "0.34375\n",
            "0.25\n",
            "0.3125\n",
            "0.234375\n",
            "0.3125\n",
            "383\n",
            "strain 0.3142976760864258\n",
            "strain 0.40318915247917175\n",
            "strain 0.31897592544555664\n",
            "strain 0.42413330078125\n",
            "strain 0.4322676360607147\n",
            "strain 0.2877720594406128\n",
            "strain 0.29504624009132385\n",
            "strain 0.36695417761802673\n",
            "strain 0.3603888154029846\n",
            "strain 0.31479090452194214\n",
            "strain 0.4687996506690979\n",
            "strain 0.28175094723701477\n",
            "strain 0.25630873441696167\n",
            "strain 0.3563728630542755\n",
            "strain 0.26600125432014465\n",
            "strain 0.3169808089733124\n",
            "strain 0.3645341396331787\n",
            "strain 0.3701317310333252\n",
            "strain 0.375079482793808\n",
            "strain 0.32318517565727234\n",
            "strain 0.307786762714386\n",
            "strain 0.27465951442718506\n",
            "strain 0.44313034415245056\n",
            "strain 0.3737223744392395\n",
            "strain 0.30412429571151733\n",
            "strain 0.4971613883972168\n",
            "strain 0.49257519841194153\n",
            "strain 0.28307226300239563\n",
            "strain 0.33859023451805115\n",
            "strain 0.3015102744102478\n",
            "strain 0.31071344017982483\n",
            "strain 0.3930017054080963\n",
            "strain 0.3439459502696991\n",
            "strain 0.3088339865207672\n",
            "strain 0.496845006942749\n",
            "strain 0.3710381090641022\n",
            "strain 0.4110831320285797\n",
            "strain 0.39598894119262695\n",
            "strain 0.4734607934951782\n",
            "strain 0.38885626196861267\n",
            "strain 0.48400771617889404\n",
            "strain 0.3600125014781952\n",
            "strain 0.32811975479125977\n",
            "strain 0.42327556014060974\n",
            "strain 0.5196256637573242\n",
            "strain 0.48139768838882446\n",
            "strain 0.32431724667549133\n",
            "strain 0.32966652512550354\n",
            "strain 0.32949572801589966\n",
            "strain 0.3190174698829651\n",
            "strain 0.37451109290122986\n",
            "classify 1.9505615234375\n",
            "classify 1.88140869140625\n",
            "classify 1.946044921875\n",
            "classify 1.921630859375\n",
            "classify 2.1036376953125\n",
            "classify 2.02655029296875\n",
            "classify 1.95361328125\n",
            "classify 1.9825439453125\n",
            "classify 1.84539794921875\n",
            "classify 1.87158203125\n",
            "classify 2.02392578125\n",
            "0.296875\n",
            "0.328125\n",
            "0.34375\n",
            "0.328125\n",
            "0.3125\n",
            "0.328125\n",
            "0.21875\n",
            "0.296875\n",
            "0.296875\n",
            "0.359375\n",
            "0.203125\n",
            "384\n",
            "strain 0.32458487153053284\n",
            "strain 0.3864835202693939\n",
            "strain 0.3344446122646332\n",
            "strain 0.3168361783027649\n",
            "strain 0.3124408721923828\n",
            "strain 0.2786336839199066\n",
            "strain 0.3775268495082855\n",
            "strain 0.30447158217430115\n",
            "strain 0.34706294536590576\n",
            "strain 0.47343534231185913\n",
            "strain 0.39188429713249207\n",
            "strain 0.3189016878604889\n",
            "strain 0.4042890667915344\n",
            "strain 0.4193436801433563\n",
            "strain 0.3260665833950043\n",
            "strain 0.3104074001312256\n",
            "strain 0.42107895016670227\n",
            "strain 0.33733320236206055\n",
            "strain 0.3917456269264221\n",
            "strain 0.3035903573036194\n",
            "strain 0.26970377564430237\n",
            "strain 0.3207502067089081\n",
            "strain 0.30244436860084534\n",
            "strain 0.46472468972206116\n",
            "strain 0.3709455132484436\n",
            "strain 0.3824741244316101\n",
            "strain 0.2820700705051422\n",
            "strain 0.40711626410484314\n",
            "strain 0.40007922053337097\n",
            "strain 0.3299802541732788\n",
            "strain 0.40470755100250244\n",
            "strain 0.3817601501941681\n",
            "strain 0.40038827061653137\n",
            "strain 0.369320273399353\n",
            "strain 0.3994995057582855\n",
            "strain 0.4435252845287323\n",
            "strain 0.2675999402999878\n",
            "strain 0.29347342252731323\n",
            "strain 0.40928080677986145\n",
            "strain 0.33225053548812866\n",
            "strain 0.32298406958580017\n",
            "strain 0.3097829818725586\n",
            "strain 0.39526447653770447\n",
            "strain 0.35256361961364746\n",
            "strain 0.3701847195625305\n",
            "strain 0.28993192315101624\n",
            "strain 0.41000521183013916\n",
            "strain 0.2455250322818756\n",
            "strain 0.3608640432357788\n",
            "strain 0.4516112804412842\n",
            "strain 0.46649953722953796\n",
            "classify 1.91326904296875\n",
            "classify 2.01434326171875\n",
            "classify 1.9290771484375\n",
            "classify 1.9329833984375\n",
            "classify 1.92620849609375\n",
            "classify 2.03350830078125\n",
            "classify 1.8956298828125\n",
            "classify 1.8426513671875\n",
            "classify 1.924560546875\n",
            "classify 1.98248291015625\n",
            "classify 1.9180908203125\n",
            "0.21875\n",
            "0.3125\n",
            "0.234375\n",
            "0.234375\n",
            "0.25\n",
            "0.21875\n",
            "0.375\n",
            "0.234375\n",
            "0.3125\n",
            "0.40625\n",
            "0.234375\n",
            "385\n",
            "strain 0.3836766481399536\n",
            "strain 0.28068283200263977\n",
            "strain 0.436455100774765\n",
            "strain 0.28806954622268677\n",
            "strain 0.27484506368637085\n",
            "strain 0.33685776591300964\n",
            "strain 0.45812126994132996\n",
            "strain 0.3572995066642761\n",
            "strain 0.33038434386253357\n",
            "strain 0.3746603727340698\n",
            "strain 0.2856013774871826\n",
            "strain 0.4108545184135437\n",
            "strain 0.3009167015552521\n",
            "strain 0.3075064718723297\n",
            "strain 0.4342980980873108\n",
            "strain 0.4437792897224426\n",
            "strain 0.34395065903663635\n",
            "strain 0.3524326682090759\n",
            "strain 0.5597144365310669\n",
            "strain 0.3294937312602997\n",
            "strain 0.3314674198627472\n",
            "strain 0.383402943611145\n",
            "strain 0.3833848536014557\n",
            "strain 0.436360239982605\n",
            "strain 0.3525034189224243\n",
            "strain 0.49817773699760437\n",
            "strain 0.2827798128128052\n",
            "strain 0.3166293203830719\n",
            "strain 0.38627883791923523\n",
            "strain 0.3373759090900421\n",
            "strain 0.32670384645462036\n",
            "strain 0.4845157861709595\n",
            "strain 0.39593663811683655\n",
            "strain 0.34546440839767456\n",
            "strain 0.3729471266269684\n",
            "strain 0.4161233603954315\n",
            "strain 0.4091929495334625\n",
            "strain 0.2560438811779022\n",
            "strain 0.3622298240661621\n",
            "strain 0.49149322509765625\n",
            "strain 0.40876349806785583\n",
            "strain 0.28741905093193054\n",
            "strain 0.36284613609313965\n",
            "strain 0.3869514465332031\n",
            "strain 0.41652995347976685\n",
            "strain 0.36056050658226013\n",
            "strain 0.3517056703567505\n",
            "strain 0.2930564284324646\n",
            "strain 0.35343652963638306\n",
            "strain 0.39152073860168457\n",
            "strain 0.3711981177330017\n",
            "classify 2.0052490234375\n",
            "classify 1.843505859375\n",
            "classify 1.880615234375\n",
            "classify 1.8814697265625\n",
            "classify 1.84014892578125\n",
            "classify 1.90185546875\n",
            "classify 1.89642333984375\n",
            "classify 1.904052734375\n",
            "classify 1.81201171875\n",
            "classify 1.99267578125\n",
            "classify 1.8690185546875\n",
            "0.5\n",
            "0.421875\n",
            "0.296875\n",
            "0.34375\n",
            "0.296875\n",
            "0.328125\n",
            "0.34375\n",
            "0.40625\n",
            "0.328125\n",
            "0.265625\n",
            "0.3125\n",
            "386\n",
            "strain 0.32375195622444153\n",
            "strain 0.32462549209594727\n",
            "strain 0.523773193359375\n",
            "strain 0.4270431697368622\n",
            "strain 0.29805025458335876\n",
            "strain 0.3775302469730377\n",
            "strain 0.4713245928287506\n",
            "strain 0.4198077321052551\n",
            "strain 0.36467310786247253\n",
            "strain 0.2616240680217743\n",
            "strain 0.45721685886383057\n",
            "strain 0.4092772305011749\n",
            "strain 0.30352261662483215\n",
            "strain 0.2704721987247467\n",
            "strain 0.3126269578933716\n",
            "strain 0.329225093126297\n",
            "strain 0.34957656264305115\n",
            "strain 0.42610153555870056\n",
            "strain 0.3060506284236908\n",
            "strain 0.31782472133636475\n",
            "strain 0.4625306725502014\n",
            "strain 0.3161436915397644\n",
            "strain 0.4151931405067444\n",
            "strain 0.37219154834747314\n",
            "strain 0.2603415250778198\n",
            "strain 0.2586047649383545\n",
            "strain 0.36224600672721863\n",
            "strain 0.3143775761127472\n",
            "strain 0.3187486529350281\n",
            "strain 0.31877824664115906\n",
            "strain 0.39905285835266113\n",
            "strain 0.47181737422943115\n",
            "strain 0.4144759178161621\n",
            "strain 0.3592044711112976\n",
            "strain 0.4434991180896759\n",
            "strain 0.2829822599887848\n",
            "strain 0.3718337416648865\n",
            "strain 0.3447776734828949\n",
            "strain 0.39378952980041504\n",
            "strain 0.4347021281719208\n",
            "strain 0.491299033164978\n",
            "strain 0.5717739462852478\n",
            "strain 0.3176250159740448\n",
            "strain 0.45666393637657166\n",
            "strain 0.43233954906463623\n",
            "strain 0.3639642298221588\n",
            "strain 0.39363545179367065\n",
            "strain 0.3348257839679718\n",
            "strain 0.36079540848731995\n",
            "strain 0.37347516417503357\n",
            "strain 0.4019523561000824\n",
            "classify 1.97265625\n",
            "classify 2.04156494140625\n",
            "classify 2.061767578125\n",
            "classify 1.87066650390625\n",
            "classify 1.9434814453125\n",
            "classify 2.0052490234375\n",
            "classify 1.96246337890625\n",
            "classify 1.98358154296875\n",
            "classify 1.931884765625\n",
            "classify 1.9442138671875\n",
            "classify 2.0191650390625\n",
            "0.34375\n",
            "0.25\n",
            "0.34375\n",
            "0.359375\n",
            "0.25\n",
            "0.25\n",
            "0.21875\n",
            "0.265625\n",
            "0.171875\n",
            "0.234375\n",
            "0.359375\n",
            "387\n",
            "strain 0.3088059425354004\n",
            "strain 0.3830231726169586\n",
            "strain 0.40276655554771423\n",
            "strain 0.4729408621788025\n",
            "strain 0.46493661403656006\n",
            "strain 0.3785436451435089\n",
            "strain 0.5055853724479675\n",
            "strain 0.41612038016319275\n",
            "strain 0.4411606192588806\n",
            "strain 0.30346202850341797\n",
            "strain 0.28198665380477905\n",
            "strain 0.4195471405982971\n",
            "strain 0.42053642868995667\n",
            "strain 0.39291009306907654\n",
            "strain 0.3081866502761841\n",
            "strain 0.4121973216533661\n",
            "strain 0.3239370584487915\n",
            "strain 0.2684670686721802\n",
            "strain 0.3636549115180969\n",
            "strain 0.3704945147037506\n",
            "strain 0.36452609300613403\n",
            "strain 0.26077279448509216\n",
            "strain 0.31566986441612244\n",
            "strain 0.38943102955818176\n",
            "strain 0.4506479799747467\n",
            "strain 0.4262348711490631\n",
            "strain 0.3645269572734833\n",
            "strain 0.3355763852596283\n",
            "strain 0.31883060932159424\n",
            "strain 0.2626160979270935\n",
            "strain 0.2999463379383087\n",
            "strain 0.35303038358688354\n",
            "strain 0.34859684109687805\n",
            "strain 0.3808597922325134\n",
            "strain 0.4098344147205353\n",
            "strain 0.4786580502986908\n",
            "strain 0.4138815701007843\n",
            "strain 0.46517473459243774\n",
            "strain 0.25596946477890015\n",
            "strain 0.3445925712585449\n",
            "strain 0.33490434288978577\n",
            "strain 0.4003105163574219\n",
            "strain 0.2803182601928711\n",
            "strain 0.27176082134246826\n",
            "strain 0.38590988516807556\n",
            "strain 0.3704203963279724\n",
            "strain 0.3404163420200348\n",
            "strain 0.41981539130210876\n",
            "strain 0.3110126852989197\n",
            "strain 0.5244677066802979\n",
            "strain 0.31997761130332947\n",
            "classify 1.93389892578125\n",
            "classify 1.9205322265625\n",
            "classify 1.87890625\n",
            "classify 1.9674072265625\n",
            "classify 1.93115234375\n",
            "classify 1.8984375\n",
            "classify 1.864501953125\n",
            "classify 1.990966796875\n",
            "classify 1.93133544921875\n",
            "classify 1.8768310546875\n",
            "classify 1.91534423828125\n",
            "0.1875\n",
            "0.296875\n",
            "0.28125\n",
            "0.28125\n",
            "0.234375\n",
            "0.234375\n",
            "0.359375\n",
            "0.375\n",
            "0.265625\n",
            "0.328125\n",
            "0.34375\n",
            "388\n",
            "strain 0.3412640690803528\n",
            "strain 0.38325613737106323\n",
            "strain 0.43758246302604675\n",
            "strain 0.3574633002281189\n",
            "strain 0.30730271339416504\n",
            "strain 0.3458750247955322\n",
            "strain 0.33183807134628296\n",
            "strain 0.415038138628006\n",
            "strain 0.27399393916130066\n",
            "strain 0.35952499508857727\n",
            "strain 0.35523632168769836\n",
            "strain 0.3612153232097626\n",
            "strain 0.43316999077796936\n",
            "strain 0.46149805188179016\n",
            "strain 0.3423188626766205\n",
            "strain 0.41442742943763733\n",
            "strain 0.3436847925186157\n",
            "strain 0.4317346513271332\n",
            "strain 0.3080800473690033\n",
            "strain 0.3186005651950836\n",
            "strain 0.31525707244873047\n",
            "strain 0.3608831465244293\n",
            "strain 0.3738924562931061\n",
            "strain 0.35331809520721436\n",
            "strain 0.3209395408630371\n",
            "strain 0.37080326676368713\n",
            "strain 0.31535792350769043\n",
            "strain 0.29123246669769287\n",
            "strain 0.3807230591773987\n",
            "strain 0.3055102527141571\n",
            "strain 0.3807390630245209\n",
            "strain 0.32560110092163086\n",
            "strain 0.45366179943084717\n",
            "strain 0.2774282693862915\n",
            "strain 0.46105167269706726\n",
            "strain 0.36909157037734985\n",
            "strain 0.3045865297317505\n",
            "strain 0.42977359890937805\n",
            "strain 0.4111180901527405\n",
            "strain 0.31182146072387695\n",
            "strain 0.45758047699928284\n",
            "strain 0.2900650203227997\n",
            "strain 0.2753928005695343\n",
            "strain 0.39094191789627075\n",
            "strain 0.3071412444114685\n",
            "strain 0.4591900706291199\n",
            "strain 0.3767901360988617\n",
            "strain 0.2687929570674896\n",
            "strain 0.3274781405925751\n",
            "strain 0.26871538162231445\n",
            "strain 0.3548998236656189\n",
            "classify 1.9754638671875\n",
            "classify 1.9522705078125\n",
            "classify 1.8546142578125\n",
            "classify 1.84832763671875\n",
            "classify 1.8458251953125\n",
            "classify 1.90362548828125\n",
            "classify 1.94659423828125\n",
            "classify 2.076416015625\n",
            "classify 1.98980712890625\n",
            "classify 1.89434814453125\n",
            "classify 1.86529541015625\n",
            "0.3125\n",
            "0.28125\n",
            "0.1875\n",
            "0.3125\n",
            "0.21875\n",
            "0.328125\n",
            "0.40625\n",
            "0.25\n",
            "0.328125\n",
            "0.296875\n",
            "0.34375\n",
            "389\n",
            "strain 0.5265306830406189\n",
            "strain 0.274020791053772\n",
            "strain 0.3779938220977783\n",
            "strain 0.31171056628227234\n",
            "strain 0.38064926862716675\n",
            "strain 0.34259527921676636\n",
            "strain 0.3141959011554718\n",
            "strain 0.3187735676765442\n",
            "strain 0.2908947467803955\n",
            "strain 0.3339741826057434\n",
            "strain 0.26608991622924805\n",
            "strain 0.3191620409488678\n",
            "strain 0.2683696746826172\n",
            "strain 0.2848374843597412\n",
            "strain 0.42365002632141113\n",
            "strain 0.45664721727371216\n",
            "strain 0.27657243609428406\n",
            "strain 0.29501885175704956\n",
            "strain 0.43952056765556335\n",
            "strain 0.4580126404762268\n",
            "strain 0.4640018343925476\n",
            "strain 0.30601274967193604\n",
            "strain 0.4398789405822754\n",
            "strain 0.30542081594467163\n",
            "strain 0.33845755457878113\n",
            "strain 0.45330098271369934\n",
            "strain 0.2842634618282318\n",
            "strain 0.382577508687973\n",
            "strain 0.2971709668636322\n",
            "strain 0.37466496229171753\n",
            "strain 0.2759847939014435\n",
            "strain 0.327062726020813\n",
            "strain 0.39977404475212097\n",
            "strain 0.3493949770927429\n",
            "strain 0.3380357325077057\n",
            "strain 0.41136839985847473\n",
            "strain 0.3483477532863617\n",
            "strain 0.44662153720855713\n",
            "strain 0.4713594913482666\n",
            "strain 0.4036860764026642\n",
            "strain 0.363390177488327\n",
            "strain 0.2987481951713562\n",
            "strain 0.32102036476135254\n",
            "strain 0.4437488913536072\n",
            "strain 0.3922211527824402\n",
            "strain 0.40046951174736023\n",
            "strain 0.32042092084884644\n",
            "strain 0.3648470342159271\n",
            "strain 0.3123547434806824\n",
            "strain 0.3954453766345978\n",
            "strain 0.4510605037212372\n",
            "classify 1.8369140625\n",
            "classify 2.0284423828125\n",
            "classify 1.9237060546875\n",
            "classify 1.9688720703125\n",
            "classify 1.97454833984375\n",
            "classify 1.93115234375\n",
            "classify 1.8765869140625\n",
            "classify 1.9246826171875\n",
            "classify 1.90118408203125\n",
            "classify 2.0362548828125\n",
            "classify 1.9002685546875\n",
            "0.28125\n",
            "0.296875\n",
            "0.328125\n",
            "0.234375\n",
            "0.3125\n",
            "0.203125\n",
            "0.234375\n",
            "0.3125\n",
            "0.1875\n",
            "0.25\n",
            "0.3125\n",
            "390\n",
            "strain 0.4113472104072571\n",
            "strain 0.3463584780693054\n",
            "strain 0.3416547477245331\n",
            "strain 0.4474470317363739\n",
            "strain 0.26899462938308716\n",
            "strain 0.42917609214782715\n",
            "strain 0.4610684812068939\n",
            "strain 0.4228714406490326\n",
            "strain 0.3595103621482849\n",
            "strain 0.3306507170200348\n",
            "strain 0.32087501883506775\n",
            "strain 0.3692670166492462\n",
            "strain 0.4717746376991272\n",
            "strain 0.36135050654411316\n",
            "strain 0.2757253348827362\n",
            "strain 0.3951433002948761\n",
            "strain 0.2788201868534088\n",
            "strain 0.4526946246623993\n",
            "strain 0.36659494042396545\n",
            "strain 0.3093259036540985\n",
            "strain 0.3316948413848877\n",
            "strain 0.2941265106201172\n",
            "strain 0.3622114062309265\n",
            "strain 0.4482564926147461\n",
            "strain 0.34587380290031433\n",
            "strain 0.31894564628601074\n",
            "strain 0.458306223154068\n",
            "strain 0.29569074511528015\n",
            "strain 0.4170770049095154\n",
            "strain 0.33149978518486023\n",
            "strain 0.41921553015708923\n",
            "strain 0.44139131903648376\n",
            "strain 0.34689900279045105\n",
            "strain 0.35925689339637756\n",
            "strain 0.4650369882583618\n",
            "strain 0.3885623812675476\n",
            "strain 0.44269925355911255\n",
            "strain 0.4350697100162506\n",
            "strain 0.3347596526145935\n",
            "strain 0.38765808939933777\n",
            "strain 0.2702900469303131\n",
            "strain 0.3533683717250824\n",
            "strain 0.423326313495636\n",
            "strain 0.3672594130039215\n",
            "strain 0.2722248136997223\n",
            "strain 0.2995874285697937\n",
            "strain 0.3292226195335388\n",
            "strain 0.4074118733406067\n",
            "strain 0.3289209008216858\n",
            "strain 0.3421575427055359\n",
            "strain 0.399556428194046\n",
            "classify 1.87261962890625\n",
            "classify 1.8277587890625\n",
            "classify 1.91558837890625\n",
            "classify 1.93426513671875\n",
            "classify 1.9461669921875\n",
            "classify 1.8511962890625\n",
            "classify 1.9512939453125\n",
            "classify 1.8966064453125\n",
            "classify 2.0032958984375\n",
            "classify 2.002685546875\n",
            "classify 1.89312744140625\n",
            "0.25\n",
            "0.3125\n",
            "0.21875\n",
            "0.328125\n",
            "0.3125\n",
            "0.25\n",
            "0.28125\n",
            "0.28125\n",
            "0.359375\n",
            "0.28125\n",
            "0.3125\n",
            "391\n",
            "strain 0.43780046701431274\n",
            "strain 0.3471149206161499\n",
            "strain 0.333649605512619\n",
            "strain 0.4014507234096527\n",
            "strain 0.3579843044281006\n",
            "strain 0.4354809820652008\n",
            "strain 0.3436141908168793\n",
            "strain 0.4532342553138733\n",
            "strain 0.4367871582508087\n",
            "strain 0.3026842176914215\n",
            "strain 0.45514923334121704\n",
            "strain 0.2872771918773651\n",
            "strain 0.25764280557632446\n",
            "strain 0.43069347739219666\n",
            "strain 0.3370000422000885\n",
            "strain 0.37174537777900696\n",
            "strain 0.4452196955680847\n",
            "strain 0.3212931752204895\n",
            "strain 0.3528739809989929\n",
            "strain 0.36427778005599976\n",
            "strain 0.30743008852005005\n",
            "strain 0.29029566049575806\n",
            "strain 0.4495088756084442\n",
            "strain 0.29084667563438416\n",
            "strain 0.43843933939933777\n",
            "strain 0.4165528118610382\n",
            "strain 0.3206581473350525\n",
            "strain 0.3092450499534607\n",
            "strain 0.29067420959472656\n",
            "strain 0.3811817765235901\n",
            "strain 0.3782678544521332\n",
            "strain 0.3334469795227051\n",
            "strain 0.41391387581825256\n",
            "strain 0.2850817143917084\n",
            "strain 0.4104188084602356\n",
            "strain 0.37894782423973083\n",
            "strain 0.4356139302253723\n",
            "strain 0.3584863841533661\n",
            "strain 0.28025010228157043\n",
            "strain 0.4882654845714569\n",
            "strain 0.38878709077835083\n",
            "strain 0.45788028836250305\n",
            "strain 0.27294957637786865\n",
            "strain 0.28037795424461365\n",
            "strain 0.40413036942481995\n",
            "strain 0.4111176133155823\n",
            "strain 0.4204477071762085\n",
            "strain 0.3630678355693817\n",
            "strain 0.39374393224716187\n",
            "strain 0.39188745617866516\n",
            "strain 0.30784401297569275\n",
            "classify 1.9190673828125\n",
            "classify 1.94354248046875\n",
            "classify 1.9552001953125\n",
            "classify 1.898193359375\n",
            "classify 1.8583984375\n",
            "classify 1.93994140625\n",
            "classify 1.9041748046875\n",
            "classify 1.890869140625\n",
            "classify 1.84857177734375\n",
            "classify 1.8828125\n",
            "classify 1.897705078125\n",
            "0.34375\n",
            "0.3125\n",
            "0.28125\n",
            "0.375\n",
            "0.3125\n",
            "0.3125\n",
            "0.328125\n",
            "0.25\n",
            "0.34375\n",
            "0.265625\n",
            "0.28125\n",
            "392\n",
            "strain 0.4971138834953308\n",
            "strain 0.42289549112319946\n",
            "strain 0.27296262979507446\n",
            "strain 0.49361878633499146\n",
            "strain 0.31719791889190674\n",
            "strain 0.32581403851509094\n",
            "strain 0.2934325635433197\n",
            "strain 0.30292296409606934\n",
            "strain 0.4310443103313446\n",
            "strain 0.48191356658935547\n",
            "strain 0.3769557476043701\n",
            "strain 0.3682553768157959\n",
            "strain 0.282007098197937\n",
            "strain 0.3534584939479828\n",
            "strain 0.40740880370140076\n",
            "strain 0.39133790135383606\n",
            "strain 0.41955503821372986\n",
            "strain 0.33759069442749023\n",
            "strain 0.32991546392440796\n",
            "strain 0.25324884057044983\n",
            "strain 0.3466523587703705\n",
            "strain 0.3310697078704834\n",
            "strain 0.35681381821632385\n",
            "strain 0.35117071866989136\n",
            "strain 0.36498069763183594\n",
            "strain 0.2881567180156708\n",
            "strain 0.49126195907592773\n",
            "strain 0.27742576599121094\n",
            "strain 0.4630964696407318\n",
            "strain 0.29990535974502563\n",
            "strain 0.34069544076919556\n",
            "strain 0.3877834677696228\n",
            "strain 0.36375245451927185\n",
            "strain 0.450947642326355\n",
            "strain 0.32828015089035034\n",
            "strain 0.3188079595565796\n",
            "strain 0.4373772442340851\n",
            "strain 0.5083079934120178\n",
            "strain 0.4277692139148712\n",
            "strain 0.43255695700645447\n",
            "strain 0.3375217318534851\n",
            "strain 0.4296988248825073\n",
            "strain 0.2877774238586426\n",
            "strain 0.43198758363723755\n",
            "strain 0.30256325006484985\n",
            "strain 0.2941577434539795\n",
            "strain 0.4418209493160248\n",
            "strain 0.42556753754615784\n",
            "strain 0.40120092034339905\n",
            "strain 0.3988126218318939\n",
            "strain 0.3506484031677246\n",
            "classify 1.893798828125\n",
            "classify 1.950439453125\n",
            "classify 1.972412109375\n",
            "classify 1.89031982421875\n",
            "classify 1.9761962890625\n",
            "classify 1.9241943359375\n",
            "classify 1.9656982421875\n",
            "classify 2.05047607421875\n",
            "classify 1.8153076171875\n",
            "classify 1.994873046875\n",
            "classify 2.025634765625\n",
            "0.234375\n",
            "0.328125\n",
            "0.171875\n",
            "0.25\n",
            "0.390625\n",
            "0.25\n",
            "0.203125\n",
            "0.28125\n",
            "0.296875\n",
            "0.359375\n",
            "0.4375\n",
            "393\n",
            "strain 0.3955687880516052\n",
            "strain 0.32593342661857605\n",
            "strain 0.49471035599708557\n",
            "strain 0.26409751176834106\n",
            "strain 0.37505584955215454\n",
            "strain 0.33929115533828735\n",
            "strain 0.28891798853874207\n",
            "strain 0.3319145441055298\n",
            "strain 0.2872442305088043\n",
            "strain 0.31792688369750977\n",
            "strain 0.41594576835632324\n",
            "strain 0.396772563457489\n",
            "strain 0.48395130038261414\n",
            "strain 0.2927115857601166\n",
            "strain 0.2887992858886719\n",
            "strain 0.3285955488681793\n",
            "strain 0.36808910965919495\n",
            "strain 0.4211065173149109\n",
            "strain 0.3509543836116791\n",
            "strain 0.3963642418384552\n",
            "strain 0.4187658131122589\n",
            "strain 0.30401289463043213\n",
            "strain 0.3699300289154053\n",
            "strain 0.461622416973114\n",
            "strain 0.38931646943092346\n",
            "strain 0.44756826758384705\n",
            "strain 0.35433927178382874\n",
            "strain 0.4403932988643646\n",
            "strain 0.3001317083835602\n",
            "strain 0.43823811411857605\n",
            "strain 0.34558334946632385\n",
            "strain 0.3275621831417084\n",
            "strain 0.43096378445625305\n",
            "strain 0.29373908042907715\n",
            "strain 0.42983442544937134\n",
            "strain 0.33452916145324707\n",
            "strain 0.3489719331264496\n",
            "strain 0.2634795904159546\n",
            "strain 0.3459516763687134\n",
            "strain 0.441239595413208\n",
            "strain 0.40873876214027405\n",
            "strain 0.41055911779403687\n",
            "strain 0.32795971632003784\n",
            "strain 0.3193591237068176\n",
            "strain 0.29056984186172485\n",
            "strain 0.31177499890327454\n",
            "strain 0.33718591928482056\n",
            "strain 0.4486217796802521\n",
            "strain 0.4663257598876953\n",
            "strain 0.40817365050315857\n",
            "strain 0.4217693507671356\n",
            "classify 1.9921875\n",
            "classify 1.99932861328125\n",
            "classify 1.8350830078125\n",
            "classify 1.9193115234375\n",
            "classify 2.042724609375\n",
            "classify 1.93853759765625\n",
            "classify 1.95263671875\n",
            "classify 1.966796875\n",
            "classify 1.9747314453125\n",
            "classify 1.94921875\n",
            "classify 1.892822265625\n",
            "0.296875\n",
            "0.265625\n",
            "0.375\n",
            "0.296875\n",
            "0.359375\n",
            "0.21875\n",
            "0.296875\n",
            "0.296875\n",
            "0.28125\n",
            "0.359375\n",
            "0.265625\n",
            "394\n",
            "strain 0.2743377387523651\n",
            "strain 0.4291722774505615\n",
            "strain 0.37641263008117676\n",
            "strain 0.35978469252586365\n",
            "strain 0.2827569842338562\n",
            "strain 0.3261839747428894\n",
            "strain 0.3374136686325073\n",
            "strain 0.49012473225593567\n",
            "strain 0.29986312985420227\n",
            "strain 0.37669143080711365\n",
            "strain 0.33887895941734314\n",
            "strain 0.3389020562171936\n",
            "strain 0.4271404445171356\n",
            "strain 0.48940131068229675\n",
            "strain 0.3860728144645691\n",
            "strain 0.4466959834098816\n",
            "strain 0.4152525067329407\n",
            "strain 0.37824684381484985\n",
            "strain 0.4407769739627838\n",
            "strain 0.22799402475357056\n",
            "strain 0.47499755024909973\n",
            "strain 0.32743775844573975\n",
            "strain 0.3147937059402466\n",
            "strain 0.492374986410141\n",
            "strain 0.45620617270469666\n",
            "strain 0.29253512620925903\n",
            "strain 0.29930153489112854\n",
            "strain 0.34160444140434265\n",
            "strain 0.41030412912368774\n",
            "strain 0.3517458736896515\n",
            "strain 0.3980236351490021\n",
            "strain 0.3138543367385864\n",
            "strain 0.3751344680786133\n",
            "strain 0.36971816420555115\n",
            "strain 0.32362908124923706\n",
            "strain 0.41956448554992676\n",
            "strain 0.4258342981338501\n",
            "strain 0.37857887148857117\n",
            "strain 0.35365745425224304\n",
            "strain 0.3348902761936188\n",
            "strain 0.3165058493614197\n",
            "strain 0.5047554969787598\n",
            "strain 0.32530340552330017\n",
            "strain 0.3451831042766571\n",
            "strain 0.4302917718887329\n",
            "strain 0.4057586193084717\n",
            "strain 0.3033425807952881\n",
            "strain 0.43077486753463745\n",
            "strain 0.3623952269554138\n",
            "strain 0.43063780665397644\n",
            "strain 0.2972607910633087\n",
            "classify 1.99871826171875\n",
            "classify 2.0467529296875\n",
            "classify 2.0233154296875\n",
            "classify 1.947265625\n",
            "classify 2.1363525390625\n",
            "classify 1.88037109375\n",
            "classify 1.89501953125\n",
            "classify 1.8626708984375\n",
            "classify 1.7637939453125\n",
            "classify 2.01885986328125\n",
            "classify 1.924072265625\n",
            "0.34375\n",
            "0.21875\n",
            "0.3125\n",
            "0.34375\n",
            "0.3125\n",
            "0.21875\n",
            "0.390625\n",
            "0.21875\n",
            "0.3125\n",
            "0.28125\n",
            "0.25\n",
            "395\n",
            "strain 0.3746657371520996\n",
            "strain 0.3861925005912781\n",
            "strain 0.322176992893219\n",
            "strain 0.3705337345600128\n",
            "strain 0.34779393672943115\n",
            "strain 0.26541978120803833\n",
            "strain 0.4099747836589813\n",
            "strain 0.37161189317703247\n",
            "strain 0.3137378692626953\n",
            "strain 0.3431853652000427\n",
            "strain 0.3239554464817047\n",
            "strain 0.4492116868495941\n",
            "strain 0.4158645570278168\n",
            "strain 0.32980233430862427\n",
            "strain 0.3826713562011719\n",
            "strain 0.290303498506546\n",
            "strain 0.2920588552951813\n",
            "strain 0.451003760099411\n",
            "strain 0.371467262506485\n",
            "strain 0.41905421018600464\n",
            "strain 0.30704739689826965\n",
            "strain 0.4350363314151764\n",
            "strain 0.5129423141479492\n",
            "strain 0.3149987459182739\n",
            "strain 0.2993660867214203\n",
            "strain 0.2908218801021576\n",
            "strain 0.313894122838974\n",
            "strain 0.3247383236885071\n",
            "strain 0.35209372639656067\n",
            "strain 0.3974769115447998\n",
            "strain 0.5100009441375732\n",
            "strain 0.3378641903400421\n",
            "strain 0.3528408110141754\n",
            "strain 0.4190653860569\n",
            "strain 0.3888742923736572\n",
            "strain 0.2952471971511841\n",
            "strain 0.3649815320968628\n",
            "strain 0.368976891040802\n",
            "strain 0.48110419511795044\n",
            "strain 0.4367946684360504\n",
            "strain 0.36559396982192993\n",
            "strain 0.47611498832702637\n",
            "strain 0.27827534079551697\n",
            "strain 0.2585012912750244\n",
            "strain 0.355567067861557\n",
            "strain 0.3479668200016022\n",
            "strain 0.4146097004413605\n",
            "strain 0.2906479835510254\n",
            "strain 0.42170462012290955\n",
            "strain 0.35336729884147644\n",
            "strain 0.26787781715393066\n",
            "classify 2.02783203125\n",
            "classify 1.94244384765625\n",
            "classify 1.976318359375\n",
            "classify 2.04052734375\n",
            "classify 1.85675048828125\n",
            "classify 1.92279052734375\n",
            "classify 1.831298828125\n",
            "classify 1.898193359375\n",
            "classify 1.9202880859375\n",
            "classify 1.89544677734375\n",
            "classify 1.9158935546875\n",
            "0.328125\n",
            "0.328125\n",
            "0.28125\n",
            "0.34375\n",
            "0.328125\n",
            "0.265625\n",
            "0.234375\n",
            "0.28125\n",
            "0.265625\n",
            "0.234375\n",
            "0.25\n",
            "396\n",
            "strain 0.4799800515174866\n",
            "strain 0.3700462281703949\n",
            "strain 0.37256869673728943\n",
            "strain 0.3368350863456726\n",
            "strain 0.5002796053886414\n",
            "strain 0.3380640149116516\n",
            "strain 0.3392780125141144\n",
            "strain 0.26721876859664917\n",
            "strain 0.4407733380794525\n",
            "strain 0.32073625922203064\n",
            "strain 0.400836318731308\n",
            "strain 0.29001909494400024\n",
            "strain 0.4113607406616211\n",
            "strain 0.2632471024990082\n",
            "strain 0.40212416648864746\n",
            "strain 0.4202771782875061\n",
            "strain 0.5185062885284424\n",
            "strain 0.38967499136924744\n",
            "strain 0.2831195890903473\n",
            "strain 0.3751581907272339\n",
            "strain 0.3913447856903076\n",
            "strain 0.2876366376876831\n",
            "strain 0.43249619007110596\n",
            "strain 0.36063194274902344\n",
            "strain 0.31967827677726746\n",
            "strain 0.4030373692512512\n",
            "strain 0.43962833285331726\n",
            "strain 0.2784181237220764\n",
            "strain 0.41146424412727356\n",
            "strain 0.30417054891586304\n",
            "strain 0.3161272406578064\n",
            "strain 0.318747878074646\n",
            "strain 0.3223712742328644\n",
            "strain 0.296537846326828\n",
            "strain 0.3380727767944336\n",
            "strain 0.3915235698223114\n",
            "strain 0.27997344732284546\n",
            "strain 0.406280517578125\n",
            "strain 0.3136095702648163\n",
            "strain 0.2781407833099365\n",
            "strain 0.4437163174152374\n",
            "strain 0.3171382546424866\n",
            "strain 0.3172006607055664\n",
            "strain 0.36115938425064087\n",
            "strain 0.4088006317615509\n",
            "strain 0.27369555830955505\n",
            "strain 0.34174591302871704\n",
            "strain 0.37447723746299744\n",
            "strain 0.36270302534103394\n",
            "strain 0.3801509737968445\n",
            "strain 0.2502807080745697\n",
            "classify 1.8848876953125\n",
            "classify 1.922607421875\n",
            "classify 1.9105224609375\n",
            "classify 1.94488525390625\n",
            "classify 1.8974609375\n",
            "classify 1.9476318359375\n",
            "classify 1.92205810546875\n",
            "classify 2.0008544921875\n",
            "classify 2.0369873046875\n",
            "classify 2.01513671875\n",
            "classify 1.963134765625\n",
            "0.359375\n",
            "0.359375\n",
            "0.34375\n",
            "0.296875\n",
            "0.3125\n",
            "0.234375\n",
            "0.171875\n",
            "0.359375\n",
            "0.265625\n",
            "0.28125\n",
            "0.25\n",
            "397\n",
            "strain 0.28015196323394775\n",
            "strain 0.44009172916412354\n",
            "strain 0.28924182057380676\n",
            "strain 0.26978158950805664\n",
            "strain 0.33552753925323486\n",
            "strain 0.42877280712127686\n",
            "strain 0.3623596727848053\n",
            "strain 0.28054162859916687\n",
            "strain 0.32980015873908997\n",
            "strain 0.37217462062835693\n",
            "strain 0.38794466853141785\n",
            "strain 0.3832866847515106\n",
            "strain 0.2625535726547241\n",
            "strain 0.3328445851802826\n",
            "strain 0.3495628237724304\n",
            "strain 0.3075021207332611\n",
            "strain 0.3492037355899811\n",
            "strain 0.29866042733192444\n",
            "strain 0.3079727590084076\n",
            "strain 0.31906867027282715\n",
            "strain 0.35856106877326965\n",
            "strain 0.2773889899253845\n",
            "strain 0.30024272203445435\n",
            "strain 0.3511684834957123\n",
            "strain 0.30761221051216125\n",
            "strain 0.4037322402000427\n",
            "strain 0.334060400724411\n",
            "strain 0.3209686279296875\n",
            "strain 0.29670849442481995\n",
            "strain 0.43684542179107666\n",
            "strain 0.2890433669090271\n",
            "strain 0.39056992530822754\n",
            "strain 0.344409704208374\n",
            "strain 0.33870193362236023\n",
            "strain 0.38505375385284424\n",
            "strain 0.3995130658149719\n",
            "strain 0.2763429284095764\n",
            "strain 0.3567991256713867\n",
            "strain 0.33142995834350586\n",
            "strain 0.3005005717277527\n",
            "strain 0.3045983910560608\n",
            "strain 0.44992750883102417\n",
            "strain 0.3365550637245178\n",
            "strain 0.30275797843933105\n",
            "strain 0.37508106231689453\n",
            "strain 0.4908360242843628\n",
            "strain 0.32091090083122253\n",
            "strain 0.4224078357219696\n",
            "strain 0.5131447911262512\n",
            "strain 0.40038150548934937\n",
            "strain 0.25924918055534363\n",
            "classify 1.9991455078125\n",
            "classify 1.7838134765625\n",
            "classify 1.912353515625\n",
            "classify 1.9912109375\n",
            "classify 1.9898681640625\n",
            "classify 1.964111328125\n",
            "classify 1.8131103515625\n",
            "classify 1.9940185546875\n",
            "classify 1.85394287109375\n",
            "classify 1.9329833984375\n",
            "classify 2.0435791015625\n",
            "0.28125\n",
            "0.34375\n",
            "0.25\n",
            "0.265625\n",
            "0.265625\n",
            "0.34375\n",
            "0.25\n",
            "0.15625\n",
            "0.375\n",
            "0.3125\n",
            "0.296875\n",
            "398\n",
            "strain 0.42562025785446167\n",
            "strain 0.36911043524742126\n",
            "strain 0.3552491366863251\n",
            "strain 0.4108390212059021\n",
            "strain 0.2501814663410187\n",
            "strain 0.3872902989387512\n",
            "strain 0.2692086398601532\n",
            "strain 0.2904881238937378\n",
            "strain 0.314215749502182\n",
            "strain 0.28920215368270874\n",
            "strain 0.3102065622806549\n",
            "strain 0.3666154146194458\n",
            "strain 0.42301613092422485\n",
            "strain 0.30621328949928284\n",
            "strain 0.3566071689128876\n",
            "strain 0.32364553213119507\n",
            "strain 0.2694133520126343\n",
            "strain 0.3568975031375885\n",
            "strain 0.38624927401542664\n",
            "strain 0.3477292060852051\n",
            "strain 0.3130238950252533\n",
            "strain 0.4115949869155884\n",
            "strain 0.3932255506515503\n",
            "strain 0.4145909249782562\n",
            "strain 0.3201029300689697\n",
            "strain 0.4368724226951599\n",
            "strain 0.34423738718032837\n",
            "strain 0.3138226568698883\n",
            "strain 0.29112935066223145\n",
            "strain 0.5227004289627075\n",
            "strain 0.39928531646728516\n",
            "strain 0.3134225308895111\n",
            "strain 0.4364415407180786\n",
            "strain 0.3856843411922455\n",
            "strain 0.3127405047416687\n",
            "strain 0.29555895924568176\n",
            "strain 0.3931275010108948\n",
            "strain 0.3032938241958618\n",
            "strain 0.3715458810329437\n",
            "strain 0.3909405767917633\n",
            "strain 0.3777037262916565\n",
            "strain 0.28038784861564636\n",
            "strain 0.33293843269348145\n",
            "strain 0.3182696998119354\n",
            "strain 0.34884071350097656\n",
            "strain 0.3228698670864105\n",
            "strain 0.47211381793022156\n",
            "strain 0.42827728390693665\n",
            "strain 0.3289860188961029\n",
            "strain 0.39128726720809937\n",
            "strain 0.38938894867897034\n",
            "classify 1.953857421875\n",
            "classify 1.83477783203125\n",
            "classify 1.9666748046875\n",
            "classify 1.9364013671875\n",
            "classify 1.93646240234375\n",
            "classify 1.8526611328125\n",
            "classify 1.92340087890625\n",
            "classify 1.869873046875\n",
            "classify 2.0609130859375\n",
            "classify 1.90380859375\n",
            "classify 1.919921875\n",
            "0.265625\n",
            "0.359375\n",
            "0.1875\n",
            "0.265625\n",
            "0.40625\n",
            "0.28125\n",
            "0.453125\n",
            "0.265625\n",
            "0.234375\n",
            "0.296875\n",
            "0.265625\n",
            "399\n",
            "strain 0.4259815812110901\n",
            "strain 0.2758217751979828\n",
            "strain 0.36706113815307617\n",
            "strain 0.42997023463249207\n",
            "strain 0.29295507073402405\n",
            "strain 0.4027216136455536\n",
            "strain 0.47403907775878906\n",
            "strain 0.38338443636894226\n",
            "strain 0.26355504989624023\n",
            "strain 0.3793647885322571\n",
            "strain 0.440987765789032\n",
            "strain 0.4108841121196747\n",
            "strain 0.3248618245124817\n",
            "strain 0.3697628676891327\n",
            "strain 0.3067547380924225\n",
            "strain 0.32965534925460815\n",
            "strain 0.3026393949985504\n",
            "strain 0.39646387100219727\n",
            "strain 0.4840810298919678\n",
            "strain 0.3691752254962921\n",
            "strain 0.4486454725265503\n",
            "strain 0.4231707751750946\n",
            "strain 0.38450032472610474\n",
            "strain 0.28566351532936096\n",
            "strain 0.3001129627227783\n",
            "strain 0.44872015714645386\n",
            "strain 0.4368050694465637\n",
            "strain 0.28795501589775085\n",
            "strain 0.3286154270172119\n",
            "strain 0.3032747507095337\n",
            "strain 0.33328860998153687\n",
            "strain 0.31148648262023926\n",
            "strain 0.40483832359313965\n",
            "strain 0.3304969072341919\n",
            "strain 0.27354827523231506\n",
            "strain 0.38650187849998474\n",
            "strain 0.48282894492149353\n",
            "strain 0.2951965630054474\n",
            "strain 0.34121257066726685\n",
            "strain 0.30583545565605164\n",
            "strain 0.2736753821372986\n",
            "strain 0.3677625060081482\n",
            "strain 0.2916175127029419\n",
            "strain 0.3028179705142975\n",
            "strain 0.5602725148200989\n",
            "strain 0.3829347491264343\n",
            "strain 0.4731680154800415\n",
            "strain 0.42844003438949585\n",
            "strain 0.4787420928478241\n",
            "strain 0.31832849979400635\n",
            "strain 0.41976508498191833\n",
            "classify 1.9783935546875\n",
            "classify 1.9635009765625\n",
            "classify 1.87286376953125\n",
            "classify 1.94989013671875\n",
            "classify 1.89080810546875\n",
            "classify 1.806884765625\n",
            "classify 1.9105224609375\n",
            "classify 2.07244873046875\n",
            "classify 1.8687744140625\n",
            "classify 1.9189453125\n",
            "classify 2.06060791015625\n",
            "0.234375\n",
            "0.3125\n",
            "0.265625\n",
            "0.28125\n",
            "0.265625\n",
            "0.328125\n",
            "0.265625\n",
            "0.296875\n",
            "0.328125\n",
            "0.28125\n",
            "0.25\n",
            "400\n",
            "strain 0.41436004638671875\n",
            "strain 0.4092245399951935\n",
            "strain 0.3309295177459717\n",
            "strain 0.25516220927238464\n",
            "strain 0.3683508634567261\n",
            "strain 0.46653851866722107\n",
            "strain 0.28504678606987\n",
            "strain 0.3010808825492859\n",
            "strain 0.2856341004371643\n",
            "strain 0.3243442177772522\n",
            "strain 0.40463393926620483\n",
            "strain 0.33764341473579407\n",
            "strain 0.3385346829891205\n",
            "strain 0.27883121371269226\n",
            "strain 0.25626590847969055\n",
            "strain 0.46079277992248535\n",
            "strain 0.3609391748905182\n",
            "strain 0.42071712017059326\n",
            "strain 0.30078181624412537\n",
            "strain 0.4439258277416229\n",
            "strain 0.3540703058242798\n",
            "strain 0.29329484701156616\n",
            "strain 0.4162253737449646\n",
            "strain 0.48879504203796387\n",
            "strain 0.30829569697380066\n",
            "strain 0.43055039644241333\n",
            "strain 0.37767156958580017\n",
            "strain 0.26714950799942017\n",
            "strain 0.3481749892234802\n",
            "strain 0.3796723783016205\n",
            "strain 0.32015353441238403\n",
            "strain 0.4103010594844818\n",
            "strain 0.3238392174243927\n",
            "strain 0.4105411469936371\n",
            "strain 0.3698720932006836\n",
            "strain 0.3408605456352234\n",
            "strain 0.3518816828727722\n",
            "strain 0.36615705490112305\n",
            "strain 0.37996912002563477\n",
            "strain 0.3042769134044647\n",
            "strain 0.353169709444046\n",
            "strain 0.3228282034397125\n",
            "strain 0.4048286974430084\n",
            "strain 0.41080886125564575\n",
            "strain 0.25212422013282776\n",
            "strain 0.38694852590560913\n",
            "strain 0.5108830332756042\n",
            "strain 0.5273456573486328\n",
            "strain 0.4501885175704956\n",
            "strain 0.4242934286594391\n",
            "strain 0.3215499222278595\n",
            "classify 2.0157470703125\n",
            "classify 1.94573974609375\n",
            "classify 1.984130859375\n",
            "classify 1.9656982421875\n",
            "classify 1.98638916015625\n",
            "classify 1.9609375\n",
            "classify 1.8360595703125\n",
            "classify 1.8804931640625\n",
            "classify 2.04248046875\n",
            "classify 2.0701904296875\n",
            "classify 1.838623046875\n",
            "0.296875\n",
            "0.28125\n",
            "0.375\n",
            "0.21875\n",
            "0.296875\n",
            "0.234375\n",
            "0.1875\n",
            "0.296875\n",
            "0.265625\n",
            "0.296875\n",
            "0.25\n",
            "401\n",
            "strain 0.35008737444877625\n",
            "strain 0.2886106073856354\n",
            "strain 0.3982122540473938\n",
            "strain 0.3223581612110138\n",
            "strain 0.3650857210159302\n",
            "strain 0.2846943736076355\n",
            "strain 0.46381282806396484\n",
            "strain 0.49455687403678894\n",
            "strain 0.53099125623703\n",
            "strain 0.4066198766231537\n",
            "strain 0.29546648263931274\n",
            "strain 0.3261403739452362\n",
            "strain 0.31062591075897217\n",
            "strain 0.2986873984336853\n",
            "strain 0.3981010615825653\n",
            "strain 0.3463973104953766\n",
            "strain 0.28329789638519287\n",
            "strain 0.3536040782928467\n",
            "strain 0.344907283782959\n",
            "strain 0.29711851477622986\n",
            "strain 0.3449467122554779\n",
            "strain 0.480720579624176\n",
            "strain 0.3733198046684265\n",
            "strain 0.3168598413467407\n",
            "strain 0.38306599855422974\n",
            "strain 0.32047179341316223\n",
            "strain 0.2723630368709564\n",
            "strain 0.46393927931785583\n",
            "strain 0.27130594849586487\n",
            "strain 0.30877774953842163\n",
            "strain 0.30757734179496765\n",
            "strain 0.41672635078430176\n",
            "strain 0.2864765524864197\n",
            "strain 0.2873924970626831\n",
            "strain 0.28066474199295044\n",
            "strain 0.305109441280365\n",
            "strain 0.39419788122177124\n",
            "strain 0.37179669737815857\n",
            "strain 0.35585689544677734\n",
            "strain 0.3052078187465668\n",
            "strain 0.4003525972366333\n",
            "strain 0.3440476655960083\n",
            "strain 0.2526796758174896\n",
            "strain 0.42252957820892334\n",
            "strain 0.36516308784484863\n",
            "strain 0.374103307723999\n",
            "strain 0.2757713496685028\n",
            "strain 0.3777979612350464\n",
            "strain 0.3747784197330475\n",
            "strain 0.3453808128833771\n",
            "strain 0.33962467312812805\n",
            "classify 1.8892822265625\n",
            "classify 2.0185546875\n",
            "classify 2.0516357421875\n",
            "classify 1.8885498046875\n",
            "classify 1.9423828125\n",
            "classify 2.0517578125\n",
            "classify 1.910888671875\n",
            "classify 1.90460205078125\n",
            "classify 1.951171875\n",
            "classify 1.8258056640625\n",
            "classify 1.949951171875\n",
            "0.3125\n",
            "0.296875\n",
            "0.265625\n",
            "0.28125\n",
            "0.21875\n",
            "0.28125\n",
            "0.28125\n",
            "0.25\n",
            "0.28125\n",
            "0.234375\n",
            "0.328125\n",
            "402\n",
            "strain 0.3453429639339447\n",
            "strain 0.40680816769599915\n",
            "strain 0.3018578886985779\n",
            "strain 0.2680540978908539\n",
            "strain 0.34425589442253113\n",
            "strain 0.4350029528141022\n",
            "strain 0.40915411710739136\n",
            "strain 0.2802619934082031\n",
            "strain 0.3084263801574707\n",
            "strain 0.408998042345047\n",
            "strain 0.3682446777820587\n",
            "strain 0.4026908278465271\n"
          ]
        }
      ],
      "source": [
        "# @title strain ctrain test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "def strain(model, dataloader, optim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.train()\n",
        "    for i, (x, _) in enumerate(dataloader):\n",
        "        x = x.to(device)#.to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            loss = model.loss(x)\n",
        "\n",
        "            # repr_loss, std_loss, cov_loss = model.loss(x)\n",
        "            # loss = model.sim_coeff * repr_loss + model.std_coeff * std_loss + model.cov_coeff * cov_loss\n",
        "\n",
        "        optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 5) # 0.5\n",
        "\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            m=0.999 # 0.99 m = next(momentum_scheduler)\n",
        "            norms=[]\n",
        "            for param_q, param_k in zip(model.student.parameters(), model.teacher.parameters()):\n",
        "                param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        print(\"strain\",loss.item())\n",
        "        # try: wandb.log({\"loss\": loss.item(), \"repr/I\": repr_loss.item(), \"std/V\": std_loss.item(), \"cov/C\": cov_loss.item()})\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=50: break\n",
        "\n",
        "\n",
        "def ctrain(model, classifier, dataloader, coptim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.eval()\n",
        "    classifier.train()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        # x = x.flatten(2).transpose(-2,-1).to(torch.bfloat16)\n",
        "        # x, y = x[...,1:].to(device).to(torch.bfloat16), y[1].to(device) # [batch, ] # (id, activity)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            with torch.no_grad():\n",
        "                sx = model(x).detach()\n",
        "            # print(sx[0][0])\n",
        "            y_ = classifier(sx)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "\n",
        "        coptim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(coptim)\n",
        "        scaler.update()\n",
        "        print(\"classify\",loss.item())\n",
        "        try: wandb.log({\"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        # x = x.flatten(2).transpose(-2,-1)#.to(torch.bfloat16)\n",
        "        # x, y = x[...,1:].to(device).to(torch.float), y[1].to(device) # [batch, ] # (id, activity)\n",
        "        with torch.no_grad():\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        print(correct/len(y))\n",
        "        try: wandb.log({\"correct\": correct/len(y)})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "# for i in range(1):\n",
        "for i in range(500):\n",
        "    print(i)\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # np.random.shuffle(train_indices); np.random.shuffle(val_indices)\n",
        "    # train_sampler, valid_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(val_indices)\n",
        "    # # batch_size = 64 #512\n",
        "    # train_loader = DataLoader(train_data, sampler=train_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "    # test_loader = DataLoader(train_data, sampler=valid_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True)\n",
        "\n",
        "    strain(ijepa, train_loader, optim)\n",
        "    ctrain(ijepa, classifier, train_loader, coptim)\n",
        "    test(ijepa, classifier, test_loader)\n",
        "\n",
        "    # strain(violet, train_loader, voptim)\n",
        "    # ctrain(violet, classifier, train_loader, coptim)\n",
        "    # test(violet, classifier, test_loader)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2Nd-sGe6Ku4S",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "outputId": "51490f8e-36d1-42b8-89d9-92696b053381"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>▇█▆▆▆▆▄▄▅▃▅▄▄▄▃▄▃▃▃▃▃▄▃▂▃▂▃▂▃▁▄▁▂▂▂▃▁▃▃▃</td></tr><tr><td>correct</td><td>▂▁▆▅▅▄▇▂▄▆▅▄▅▄▄█▄▅▅▄▅▅▅▅▇▅▅▅▆▇▇▄▇▇▅▅▇▆█▄</td></tr><tr><td>loss</td><td>▁▁▁▁▁▂▁▁▃▁▂▂▁▁▄▃▂▄▂▂▄▃▃▂▄▄▄▄▄▅▄▅▅▇▄█▆▄▄█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>1.99219</td></tr><tr><td>correct</td><td>0.15625</td></tr><tr><td>loss</td><td>0.26535</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">woven-terrain-49</strong> at: <a href='https://wandb.ai/bobdole/ijepa/runs/e68ogtc4' target=\"_blank\">https://wandb.ai/bobdole/ijepa/runs/e68ogtc4</a><br> View project at: <a href='https://wandb.ai/bobdole/ijepa' target=\"_blank\">https://wandb.ai/bobdole/ijepa</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250329_114235-e68ogtc4/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250329_120902-xdwihefm</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/ijepa/runs/xdwihefm' target=\"_blank\">solar-donkey-50</a></strong> to <a href='https://wandb.ai/bobdole/ijepa' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/ijepa' target=\"_blank\">https://wandb.ai/bobdole/ijepa</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/ijepa/runs/xdwihefm' target=\"_blank\">https://wandb.ai/bobdole/ijepa/runs/xdwihefm</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "!pip install -q wandb\n",
        "import wandb # https://docs.wandb.ai/quickstart\n",
        "wandb.login(key='487a2109e55dce4e13fc70681781de9f50f27be7')\n",
        "try: run.finish()\n",
        "except NameError: pass\n",
        "run = wandb.init(project=\"ijepa\", config={\"model\": \"res18\",})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# @title supervised train test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "# def strain(model, dataloader, optim, scheduler=None):\n",
        "def strain(model, classifier, dataloader, optim, coptim, scheduler=None):\n",
        "    model.train()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x = x.to(device)#.to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "        y = y.to(device)\n",
        "\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            # y_ = model.classify(x)\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        # print(loss)\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # for p in list(filter(lambda p: p.grad is not None, model.parameters())):\n",
        "        #     print(p.grad.data.norm(2).item())\n",
        "        # print(\"max grad norm\", max([p.grad.data.norm(2).item() for p in list(filter(lambda p: p.grad is not None, model.parameters()))]))\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 10) # 0.5\n",
        "\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        print(\"strain\",loss.item())\n",
        "        # for param in ijepa.student.cls: print(param.data)\n",
        "        # for param in ijepa.predicter.cls: print(param.data)\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=50: break\n",
        "\n",
        "\n",
        "# def test(model, dataloader):\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        # .to(torch.bfloat16)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # y_ = model.classify(x)\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "        loss = F.cross_entropy(y_, y)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        print(correct/len(y))\n",
        "        try: wandb.log({\"correct\": correct/len(y), \"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "for i in range(1000):\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # np.random.shuffle(train_indices); np.random.shuffle(val_indices)\n",
        "    # train_sampler, valid_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(val_indices)\n",
        "    # # batch_size = 64 #512\n",
        "    # train_loader = DataLoader(train_data, sampler=train_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "    # test_loader = DataLoader(train_data, sampler=valid_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True)\n",
        "\n",
        "    # strain(ijepa, train_loader, optim)\n",
        "    strain(ijepa, classifier, train_loader, optim, coptim)\n",
        "    test(ijepa, classifier, test_loader)\n",
        "\n",
        "    # strain(violet, train_loader, voptim)\n",
        "    # test(violet, test_loader)\n"
      ],
      "metadata": {
        "id": "4J2ahp3wmqcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## drawer"
      ],
      "metadata": {
        "id": "aLT74ihtMnh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# @title test multiblock params\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "dataiter = iter(train_data)\n",
        "img,y = next(dataiter)\n",
        "img = img.unsqueeze(0)\n",
        "img = F.interpolate(img, (8,8))\n",
        "b,c,h,w = img.shape\n",
        "# img = img.flatten(2).transpose(-2,-1).to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "print(img.shape)\n",
        "# batch, seq,_ = img.shape\n",
        "\n",
        "\n",
        "# # target_mask = randpatch(seq, mask_size=8, gamma=.9) # 8.9 [seq]\n",
        "# target_mask = multiblock(seq, min_s=0.2, max_s=0.3, M=4).any(0) # best.2.3M4 og.15.2M4# mask out targets to be predicted # [M, seq]\n",
        "# target_mask = multiblock(seq, min_s=0.2, max_s=0.3, M=4).any(0)\n",
        "\n",
        "target_mask = multiblock2d((8,8), M=4).any(0).unsqueeze(0)\n",
        "\n",
        "\n",
        "# context_mask = ~multiblock(seq, min_s=0.85, max_s=1., M=1)|target_mask # [1, seq], True->Mask\n",
        "context_mask = ~multiblock2d((8,8), scale=(.85,.85), aspect_ratio=(.9,1.1), M=1)|target_mask # [1, seq], True->Mask\n",
        "\n",
        "# print(target_mask, context_mask)\n",
        "\n",
        "print(target_mask.shape, context_mask.shape)\n",
        "# target_img, context_img = img*target_mask.unsqueeze(-1), img*context_mask.unsqueeze(-1)\n",
        "# target_img, context_img = img*~target_mask.unsqueeze(-1), img*~context_mask.unsqueeze(-1)\n",
        "target_img, context_img = img*~target_mask.unsqueeze(0), img*~context_mask.unsqueeze(0)\n",
        "# print(target_img.shape, context_img.shape)\n",
        "target_img, context_img = target_img.transpose(-2,-1).reshape(b,c,h,w), context_img.transpose(-2,-1).reshape(b,c,h,w)\n",
        "target_img, context_img = target_img.float(), context_img.float()\n",
        "\n",
        "# imshow(out.detach().cpu())\n",
        "imshow(target_img[0])\n",
        "imshow(context_img[0])\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "3bpiybH7M0O1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 879
        },
        "outputId": "3d5e3a1a-7e09-4033-a6ad-5e68f9844da1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 8, 8])\n",
            "torch.Size([1, 8, 8]) torch.Size([1, 8, 8])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ViT me more\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "# class LayerNorm2d(nn.LayerNorm):\n",
        "class LayerNorm2d(nn.RMSNorm):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        x = super().forward(x)\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads=None, d_head=8, cond_dim=None, dropout=0.): # .1\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_head = d_head\n",
        "        self.n_heads = d_model // d_head\n",
        "        # self.d_head = d_model // n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or d_model, 2*d_model, bias=False)\n",
        "        # self.k = nn.Sequential(nn.Dropout(dropout), nn.Linear(cond_dim, d_model, bias=False))\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        # self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        # self.lin = nn.Sequential(nn.Dropout(dropout), zero_module(nn.Linear(d_model, d_model)))\n",
        "        self.drop = nn.Dropout(dropout) # indp before q,k,v; after linout\n",
        "        self.scale = self.d_head ** -.5\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [batch, T, d_model]=[batch, h*w, c], [batch, num_tok, cond_dim], [batch,T]\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        Q = self.q(x).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        # K = self.k(x).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2)\n",
        "        K, V = self.kv(cond).unflatten(-1, (self.n_heads, 2*self.d_head)).transpose(1, 2).chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # linear attention # Softmax(Q) @ (Softmax(K).T @ V)\n",
        "        if mask != None:\n",
        "            mask = mask[:, None, :, None] # [batch,T] -> [batch,1,T,1]\n",
        "            K, V = K.masked_fill(mask, -torch.finfo(x.dtype).max), V.masked_fill(mask, -torch.finfo(x.dtype).max)\n",
        "        Q, K = Q.softmax(dim=-1)*self.scale, K.softmax(dim=-2)\n",
        "        context = K.transpose(-2,-1) @ V # [batch, n_heads, d_head, d_head]\n",
        "        out = Q @ context # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # (quadratic) attention # Softmax(Q @ K.T) @ V\n",
        "        # attn = Q @ K.transpose(-2,-1) * self.scale # [batch, n_heads, T] # [batch, n_heads, T, T/num_tok]\n",
        "        # if mask != None: attn = attn.masked_fill(mask[:, None, :, None], -torch.finfo(attn.dtype).max) # [batch,T]->[batch,1,T,1]\n",
        "        # attention = torch.softmax(attn, dim=-1)\n",
        "        # out = self.drop(attention) @ V # [batch, n_heads, T, d_head]\n",
        "\n",
        "        out = out.transpose(1, 2).flatten(2)\n",
        "        return self.drop(self.lin(out)) # [batch, T, d_model]\n",
        "\n",
        "# if self, dont pass cond_dim in init, dont pass cond in fwd\n",
        "# Softmax(Q @ K.T) @ V ~ Softmax(Q) @ Softmax(K).T @ V\n",
        "\n",
        "# https://github.com/lucidrains/x-transformers/blob/main/x_transformers/x_transformers.py#L1855\n",
        "\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_head, cond_dim=None, mult=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.cond_dim = cond_dim\n",
        "        self.self = MultiHeadAttention(d_model, d_head=d_head, dropout=0) # 16448\n",
        "        # self.self = Pooling()\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        # self.ff = nn.Sequential(\n",
        "        #     *[nn.BatchNorm2d(d_model), act, SeparableConv2d(d_model, d_model),]*3\n",
        "        #     )\n",
        "        # self.ff = ResBlock(d_model) # 74112\n",
        "        # self.ff = UIB(d_model, mult=4) # uib m4 36992, m2 18944\n",
        "        ff_dim=d_model*mult\n",
        "        self.ff = nn.Sequential(\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(dropout), nn.Linear(d_model, ff_dim), act, # ReLU GELU\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(dropout), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            nn.RMSNorm(d_model), act, nn.Dropout(dropout), nn.Linear(d_model, ff_dim),\n",
        "            nn.RMSNorm(ff_dim), act, nn.Dropout(dropout), zero_module(nn.Linear(ff_dim, d_model))\n",
        "        )\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,c,h,w], [batch, num_tok, cond_dim], [batch,T]\n",
        "        bchw = x.shape\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "        # print('attnblk fwd',x.shape)\n",
        "\n",
        "        # if self.cond_dim==None: cond=None # is self attn\n",
        "        x = x + self.drop(self.self(self.norm(x)))\n",
        "        # x = x.transpose(1,2).reshape(*bchw)\n",
        "        x = x + self.ff(x)\n",
        "        # x = self.ff(x)\n",
        "        # x = x + self.drop(self.norm2(self.ff(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "import torch\n",
        "# def apply_masks(x, masks): # [b,t,d], [M,mask_size] # https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "#     all_x = []\n",
        "#     for m in masks: # [1,T]\n",
        "#         mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [1,T,dim]\n",
        "#         all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "#     return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "def apply_masks(x, mask): # [b,t,d], [mask_size] # https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "    mask_keep = mask.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [1,T,dim]\n",
        "    # return torch.cat(torch.gather(x, dim=1, index=mask_keep), dim=0)  # [M*batch,mask_size,dim]\n",
        "    return torch.gather(x, dim=1, index=mask_keep) # [M*batch,mask_size,dim]\n",
        "\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        # self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        self.positional_emb = nn.Parameter(torch.zeros(1, 8*8, d_model)) # positional_embedding == 'learnable'\n",
        "        # self.positional_emb = nn.Parameter(torch.zeros(1, dim, 8,8))\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.zeros(1,1,d_model)) # randn zeros\n",
        "        nn.init.trunc_normal_(self.cls, std=.02)\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model)\n",
        "        self.lin = nn.Linear(d_model, out_dim)# if out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = x.shape\n",
        "        # x = x * self.pos_encoder(context_indices)\n",
        "        x = x + self.positional_emb[:,context_indices]\n",
        "        # x = apply_masks(x, [context_indices])\n",
        "\n",
        "        # pred_tokens = self.cls * self.pos_encoder(trg_indices) # [M, num_trg_toks, d_model]\n",
        "        pred_tokens = self.cls + self.positional_emb[0,trg_indices]\n",
        "        print(self.cls.shape, self.positional_emb[0,trg_indices].shape, trg_indices.shape)\n",
        "        pred_tokens = pred_tokens.repeat(batch, 1, 1) # [batch*M, num_trg_toks, d_model]\n",
        "        # print(pred_tokens.requires_grad)\n",
        "        print(\"pred fwd\", x.shape, pred_tokens.shape)\n",
        "        # x = x.repeat_interleave(trg_indices.shape[0], dim=0) # [batch, seq_len, d_model] -> [batch*M, seq_len, d_model]\n",
        "        x = torch.cat([x, pred_tokens], dim=1) # [batch*M, seq_len+num_trg_toks, d_model]\n",
        "\n",
        "        # x = x.transpose(1,2).unflatten(-1, (8,8))#.reshape(*bchw)\n",
        "        out = self.transformer_encoder(x) # float [seq_len, batch_size, d_model]\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "\n",
        "        out = self.norm(out)\n",
        "        out = out[:,seq:] # [batch*M, num_trg_toks, d_model]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Sequential(\n",
        "            # # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.MaxPool1d(2,2), #nn.MaxPool1d(3, 2, 3//2),\n",
        "            # # nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.Conv1d(d_model, d_model,3,2,3//2)\n",
        "            # nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            # nn.Conv1d(d_model, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            # nn.Conv1d(d_model, d_model,3,2,3//2),\n",
        "            # # nn.Conv1d(in_dim, d_model,2,2,0), # like patch\n",
        "            nn.Conv2d(in_dim, d_model, kernel_size=7, stride=2, padding=7//2, bias=False), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "            )\n",
        "        # self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        self.positional_emb = nn.Parameter(torch.zeros(1, 8*8, d_model)) # positional_embedding == 'learnable'\n",
        "        # self.positional_emb = nn.Parameter(torch.zeros(1, dim, 8,8)) # positional_embedding == 'learnable'\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model)\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # x = self.embed(x.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "        x = self.embed(x)\n",
        "        # bchw = x.shape\n",
        "        x = x.flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c] # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "\n",
        "        # x = self.pos_encoder(x)\n",
        "        # x = x * self.pos_encoder(torch.arange(seq, device=device)).unsqueeze(0)\n",
        "        # print(x.shape, self.positional_emb.shape)\n",
        "        x = x + self.positional_emb\n",
        "        if context_indices != None:\n",
        "            # x = apply_masks(x, [context_indices])\n",
        "            x = apply_masks(x, context_indices)\n",
        "\n",
        "\n",
        "        # x = x.transpose(1,2).reshape(*bchw)\n",
        "        x = self.transformer_encoder(x) # float [seq_len, batch_size, d_model]\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "        out = self.norm(x)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "\n",
        "# pos_emb rope < learn < learned\n",
        "# conv > pixel?\n",
        "# droppath not required\n",
        "\n",
        "# norm,act,conv < conv,norm,act\n",
        "# 2*s1 < uib < resblock\n",
        "# gatedadaln 3 < 2 = 1 < ffmult4 = 2*gatedadaln\n",
        "# MaxPool2d(2,2) < MaxPool2d(3,2,3//2)\n",
        "\n",
        "\n",
        "# dim = 64\n",
        "# dim_head = 8\n",
        "# heads = dim // dim_head\n",
        "# num_classes = 10\n",
        "# # model = SimpleViT(image_size=32, patch_size=4, num_classes=num_classes, dim=dim, depth=1, heads=heads, mlp_dim=dim*4, channels = 3, dim_head = dim_head)\n",
        "# model = SimpleViT(in_dim=3, out_dim=num_classes, dim=dim, depth=1, heads=heads, mlp_dim=dim*4, channels = 3, dim_head = dim_head).to(device)\n",
        "# print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 59850\n",
        "# optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "# # print(images.shape) # [batch, 3, 32, 32]\n",
        "# logits = model(x)\n",
        "# print(logits.shape)\n",
        "# # print(logits[0])\n",
        "# # print(logits[0].argmax(1))\n",
        "# pred_probab = nn.Softmax(dim=1)(logits)\n",
        "# y_pred = pred_probab.argmax(1)\n",
        "# # print(f\"Predicted class: {y_pred}\")\n",
        "\n",
        "\n",
        "batch, seq_len, d_model = 4,1024,16\n",
        "in_dim = 3\n",
        "model = TransformerModel(in_dim, d_model, d_head=4, nlayers=2, dropout=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x = torch.rand((batch, in_dim, 32, 32), device=device)\n",
        "# x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "# # # print(out)\n",
        "# model = TransformerPredictor(in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1).to(device)\n",
        "# out = model(out)\n",
        "# print(out.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5554e8f3-71ec-4b70-c7d7-219b59e9a1d3",
        "cellView": "form",
        "id": "C1iZQ6UNwoty"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9920\n",
            "torch.Size([4, 64, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title simplex\n",
        "!pip install opensimplex\n",
        "from opensimplex import OpenSimplex\n",
        "\n",
        "seed=0\n",
        "noise = OpenSimplex(seed)  # Replace 'seed' with your starting value\n",
        "noise_scale = 10  # Adjust for desired noise range\n",
        "\n",
        "def get_noise(x, y):\n",
        "    global seed  # Assuming seed is a global variable\n",
        "    # noise_val = noise.noise2(x / noise_scale, y / noise_scale)\n",
        "    noise_val = noise.noise2(x, y)\n",
        "    # seed += 1  # Increment seed after each call\n",
        "    return noise_val\n",
        "\n",
        "x,y=0,0\n",
        "a=[[],[],[],[],[]]\n",
        "\n",
        "fps=2\n",
        "f=[]\n",
        "for i in range(10*fps):\n",
        "    f.append(i/fps)\n",
        "    x = x+0.3\n",
        "    # y = y+1\n",
        "    # noise_value = get_noise(x, y)\n",
        "    for k,j in enumerate([0,1,2,3,4]):\n",
        "        a[k].append(get_noise(x, y+j))\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "for aa in a:\n",
        "    plt.plot(f,aa)\n",
        "plt.show()\n",
        "\n",
        "# b,y,g,"
      ],
      "metadata": {
        "id": "e3dAhWh45F4M",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/facebookresearch/ijepa.git\n",
        "# !cd ijepa\n",
        "# # !pip install -e .\n",
        "\n",
        "\n",
        "!python main.py --fname configs/in1k_vith14_ep300.yaml --devices cuda:0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iK62R7q7MEa7",
        "outputId": "503e140e-e5b1-4348-d19d-1f84352afd34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ijepa'...\n",
            "remote: Enumerating objects: 32, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 32 (delta 6), reused 3 (delta 3), pack-reused 4 (from 1)\u001b[K\n",
            "Receiving objects: 100% (32/32), 33.02 KiB | 2.06 MiB/s, done.\n",
            "Resolving deltas: 100% (6/6), done.\n",
            "Obtaining file:///content\n",
            "\u001b[31mERROR: file:///content does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title pos_embed.py\n",
        "# https://github.com/facebookresearch/mae/blob/main/util/pos_embed.py#L20\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim=16, grid_size=8, cls_token=False): #\n",
        "    grid_h = np.arange(grid_size, dtype=np.float32)\n",
        "    grid_w = np.arange(grid_size, dtype=np.float32)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token: pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed # [(1+)grid_size*grid_size, embed_dim]\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    # assert embed_dim % 2 == 0\n",
        "\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1) # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos): #\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=float)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega  # (D/2,)\n",
        "\n",
        "    pos = pos.reshape(-1)  # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
        "\n",
        "    emb_sin = np.sin(out) # (M, D/2)\n",
        "    emb_cos = np.cos(out) # (M, D/2)\n",
        "\n",
        "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def interpolate_pos_embed(model, checkpoint_model):\n",
        "    if 'pos_embed' in checkpoint_model:\n",
        "        pos_embed_checkpoint = checkpoint_model['pos_embed']\n",
        "        embedding_size = pos_embed_checkpoint.shape[-1]\n",
        "        num_patches = model.patch_embed.num_patches\n",
        "        num_extra_tokens = model.pos_embed.shape[-2] - num_patches\n",
        "        # height (== width) for the checkpoint position embedding\n",
        "        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n",
        "        # height (== width) for the new position embedding\n",
        "        new_size = int(num_patches ** 0.5)\n",
        "        # class_token and dist_token are kept unchanged\n",
        "        if orig_size != new_size:\n",
        "            print(\"Position interpolate from %dx%d to %dx%d\" % (orig_size, orig_size, new_size, new_size))\n",
        "            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n",
        "            # only the position tokens are interpolated\n",
        "            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n",
        "            pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n",
        "            pos_tokens = torch.nn.functional.interpolate(\n",
        "                pos_tokens, size=(new_size, new_size), mode='bicubic', align_corners=False)\n",
        "            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n",
        "            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n",
        "            checkpoint_model['pos_embed'] = new_pos_embed\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "vtPSM8mbnJZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ijepa src.utils.tensors\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/utils/tensors.py\n",
        "\n",
        "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
        "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n",
        "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
        "    def norm_cdf(x):\n",
        "        # Computes standard normal cumulative distribution function\n",
        "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Values are generated by using a truncated uniform distribution and\n",
        "        # then using the inverse CDF for the normal distribution.\n",
        "        # Get upper and lower cdf values\n",
        "        l = norm_cdf((a - mean) / std)\n",
        "        u = norm_cdf((b - mean) / std)\n",
        "\n",
        "        # Uniformly fill tensor with values from [l, u], then translate to\n",
        "        # [2l-1, 2u-1].\n",
        "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
        "\n",
        "        # Use inverse cdf transform for normal distribution to get truncated\n",
        "        # standard normal\n",
        "        tensor.erfinv_()\n",
        "\n",
        "        # Transform to proper mean, std\n",
        "        tensor.mul_(std * math.sqrt(2.))\n",
        "        tensor.add_(mean)\n",
        "\n",
        "        # Clamp to ensure it's in the proper range\n",
        "        tensor.clamp_(min=a, max=b)\n",
        "        return tensor\n",
        "\n",
        "\n",
        "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
        "    # type: (Tensor, float, float, float, float) -> Tensor\n",
        "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n",
        "\n",
        "def repeat_interleave_batch(x, B, repeat): # [batch*M,...]? , M?\n",
        "    N = len(x) // B\n",
        "    x = torch.cat([\n",
        "        torch.cat([x[i*B:(i+1)*B] for _ in range(repeat)], dim=0)\n",
        "        for i in range(N)\n",
        "    ], dim=0)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "dLbXQ-3XXRMq",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ijepa multiblock down\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "import math\n",
        "from multiprocessing import Value\n",
        "import torch\n",
        "_GLOBAL_SEED = 0\n",
        "\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=(224, 224),\n",
        "        patch_size=16,\n",
        "        enc_mask_scale=(0.2, 0.8),\n",
        "        pred_mask_scale=(0.2, 0.8),\n",
        "        aspect_ratio=(0.3, 3.0),\n",
        "        nenc=1,\n",
        "        npred=2,\n",
        "        min_keep=4,\n",
        "        allow_overlap=False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if not isinstance(input_size, tuple):\n",
        "            input_size = (input_size, ) * 2\n",
        "        self.patch_size = patch_size\n",
        "        self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.aspect_ratio = aspect_ratio\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "        self._itr_counter = Value('i', -1)  # collator is shared across worker processes\n",
        "\n",
        "    def step(self):\n",
        "        i = self._itr_counter\n",
        "        with i.get_lock():\n",
        "            i.value += 1\n",
        "            v = i.value\n",
        "        return v\n",
        "\n",
        "    def _sample_block_size(self, generator, scale, aspect_ratio_scale):\n",
        "        _rand = torch.rand(1, generator=generator).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.height * self.width * mask_scale)\n",
        "        # -- Sample block aspect-ratio\n",
        "        min_ar, max_ar = aspect_ratio_scale\n",
        "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
        "        while h >= self.height:\n",
        "            h -= 1\n",
        "        while w >= self.width:\n",
        "            w -= 1\n",
        "        return (h, w)\n",
        "\n",
        "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
        "        h, w = b_size\n",
        "\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # --\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            top = torch.randint(0, self.height - h, (1,))\n",
        "            left = torch.randint(0, self.width - w, (1,))\n",
        "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
        "            mask[top:top+h, left:left+w] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
        "        mask_complement[top:top+h, left:left+w] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        B = len(batch)\n",
        "\n",
        "        collated_batch = torch.utils.data.default_collate(batch)\n",
        "\n",
        "        seed = self.step()\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(seed)\n",
        "        p_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.pred_mask_scale,\n",
        "            aspect_ratio_scale=self.aspect_ratio)\n",
        "        e_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.enc_mask_scale,\n",
        "            aspect_ratio_scale=(1., 1.))\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.height * self.width\n",
        "        min_keep_enc = self.height * self.width\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                print(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "        return collated_batch, collated_masks_enc, collated_masks_pred\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "COvEnBXPYth3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aH1UOD4WW8I2",
        "outputId": "e8671cd3-eba7-4649-84ca-94c6467ce62d",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Mask generator says: \"Valid mask not found, decreasing acceptable-regions [1]\"\n",
            "WARNING:root:Mask generator says: \"Valid mask not found, decreasing acceptable-regions [2]\"\n",
            "WARNING:root:Mask generator says: \"Valid mask not found, decreasing acceptable-regions [3]\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vit fwd torch.Size([4, 3, 224, 224])\n",
            "vit fwd torch.Size([4, 196, 768])\n",
            "vit fwd torch.Size([4, 11, 768])\n",
            "predictor fwd1 torch.Size([4, 11, 384]) torch.Size([4, 196, 384])\n"
          ]
        }
      ],
      "source": [
        "# @title ijepa vision_transformer.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/models/vision_transformer.py\n",
        "import math\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# from src.utils.tensors import (trunc_normal_, repeat_interleave_batch)\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid height and width\n",
        "    return:\n",
        "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid_h = np.arange(grid_size, dtype=float)\n",
        "    grid_w = np.arange(grid_size, dtype=float)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0) # [1+h*w, dim]\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    assert embed_dim % 2 == 0\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid length\n",
        "    return:\n",
        "    pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid = np.arange(grid_size, dtype=float)\n",
        "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=float)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega   # (D/2,)\n",
        "    pos = pos.reshape(-1)   # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
        "    emb = np.concatenate([np.sin(out), np.cos(out)], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\"\"\"\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x, attn\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x, return_attention=False):\n",
        "        y, attn = self.attn(self.norm1(x))\n",
        "        if return_attention:\n",
        "            return attn\n",
        "        x = x + self.drop_path(y)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2) # ->[b,h*w,c]\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvEmbed(nn.Module):\n",
        "    \"\"\"3x3 Convolution stems for ViT following ViTC models\"\"\"\n",
        "    def __init__(self, channels, strides, img_size=224, in_chans=3, batch_norm=True):\n",
        "        super().__init__()\n",
        "        # Build the stems\n",
        "        stem = []\n",
        "        channels = [in_chans] + channels\n",
        "        for i in range(len(channels) - 2):\n",
        "            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3, stride=strides[i], padding=1, bias=(not batch_norm))]\n",
        "            if batch_norm:\n",
        "                stem += [nn.BatchNorm2d(channels[i+1])]\n",
        "            stem += [nn.ReLU(inplace=True)]\n",
        "        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n",
        "        self.stem = nn.Sequential(*stem)\n",
        "\n",
        "        # Comptute the number of patches\n",
        "        stride_prod = int(np.prod(strides))\n",
        "        self.num_patches = (img_size[0] // stride_prod)**2\n",
        "\n",
        "    def forward(self, x):\n",
        "        p = self.stem(x)\n",
        "        return p.flatten(2).transpose(1, 2)\n",
        "\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "def apply_masks(x, masks):\n",
        "    \"\"\":param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
        "    :param masks: list of tensors containing indices of patches in [N] to keep\"\"\"\n",
        "    all_x = []\n",
        "    for m in masks:\n",
        "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [batch,T,dim]\n",
        "        all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "    return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "\n",
        "class VisionTransformerPredictor(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, num_patches, embed_dim=768, predictor_embed_dim=384,\n",
        "        depth=6, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.init_std = init_std\n",
        "        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        # --\n",
        "        self.predictor_pos_embed = nn.Parameter(torch.zeros(1, num_patches, predictor_embed_dim), requires_grad=False)\n",
        "        predictor_pos_embed = get_2d_sincos_pos_embed(self.predictor_pos_embed.shape[-1], int(num_patches**.5), cls_token=False)\n",
        "        self.predictor_pos_embed.data.copy_(torch.from_numpy(predictor_pos_embed).float().unsqueeze(0)) # [1, num_patches, predictor_embed_dim]\n",
        "        # --\n",
        "        self.predictor_blocks = nn.ModuleList([Block(dim=predictor_embed_dim, num_heads=num_heads, drop=drop_rate, drop_path=dpr[i]) for i in range(depth)])\n",
        "        self.predictor_norm = nn.LayerNorm(predictor_embed_dim)\n",
        "        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n",
        "        # ------\n",
        "        # trunc_normal_(self.mask_token, std=self.init_std)\n",
        "\n",
        "    def forward(self, x, masks_x, masks): # [batch, num_context_patches, embed_dim], nenc*[batch, num_context_patches], npred*[batch, num_target_patches]\n",
        "        assert (masks is not None) and (masks_x is not None), 'Cannot run predictor without mask indices'\n",
        "        if not isinstance(masks_x, list): masks_x = [masks_x] # context mask\n",
        "        if not isinstance(masks, list): masks = [masks] # pred mask\n",
        "\n",
        "        # -- Batch Size\n",
        "        B = len(x) // len(masks_x)\n",
        "        # print(\"predictor fwd0\", x.shape) # [3, 121, 768])\n",
        "        # -- map from encoder-dim to pedictor-dim\n",
        "        x = self.predictor_embed(x) # [batch, num_patches, predictor_embed_dim]\n",
        "\n",
        "        # -- add positional embedding to x tokens\n",
        "        x_pos_embed = self.predictor_pos_embed.repeat(B, 1, 1) # [batch, num_patches, predictor_embed_dim]\n",
        "        print(\"predictor fwd1\", x.shape, x_pos_embed.shape) # [3, 121 or 11, 384], [3, 196, 384]\n",
        "        # print(\"predictor fwd masks_x\", masks_x)\n",
        "        x += apply_masks(x_pos_embed, masks_x) # get pos emb of context patches\n",
        "\n",
        "        # print(\"predictor fwd x\", x.shape) # [4, 11, 384])\n",
        "        _, N_ctxt, D = x.shape\n",
        "\n",
        "        # -- concat mask tokens to x\n",
        "        pos_embs = self.predictor_pos_embed.repeat(B, 1, 1) # [batch, num_patches, predictor_embed_dim]\n",
        "        pos_embs = apply_masks(pos_embs, masks) # [16, 104, predictor_embed_dim]\n",
        "        pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x)) # [16, 104, predictor_embed_dim]\n",
        "        # --\n",
        "        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1) # [16, 104, predictor_embed_dim]\n",
        "        # --\n",
        "        # print(\"predictor fwd pred_tokens\", pred_tokens.shape)\n",
        "\n",
        "        pred_tokens += pos_embs\n",
        "        x = x.repeat(len(masks), 1, 1)\n",
        "        x = torch.cat([x, pred_tokens], dim=1)\n",
        "\n",
        "        # -- fwd prop\n",
        "        for blk in self.predictor_blocks:\n",
        "            x = blk(x)\n",
        "        x = self.predictor_norm(x)\n",
        "\n",
        "        # -- return preds for mask tokens\n",
        "        x = x[:, N_ctxt:]\n",
        "        x = self.predictor_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, img_size=[224], patch_size=16, in_chans=3, embed_dim=768, predictor_embed_dim=384, depth=12, predictor_depth=12,\n",
        "        num_heads=12, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0, norm_layer=nn.LayerNorm, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_features = self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.init_std = init_std\n",
        "        # --\n",
        "        self.patch_embed = PatchEmbed(img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        # --\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=False)\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "        # --\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "    def forward(self, x, masks=None): # [batch,3,224,224]\n",
        "        if masks is not None:\n",
        "            if not isinstance(masks, list): masks = [masks]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        x = self.patch_embed(x)\n",
        "        B, N, D = x.shape # [batch, num_patches, embed_dim]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        # -- add positional embedding to x\n",
        "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
        "        x = x + pos_embed\n",
        "\n",
        "        if masks is not None: x = apply_masks(x, masks) # [batch, num_context_patches, embed_dim]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            x = blk(x)\n",
        "\n",
        "        if self.norm is not None: x = self.norm(x)\n",
        "        return x # [batch, num_context_patches, embed_dim]\n",
        "\n",
        "    def interpolate_pos_encoding(self, x, pos_embed):\n",
        "        npatch = x.shape[1] - 1\n",
        "        N = pos_embed.shape[1] - 1\n",
        "        if npatch == N: return pos_embed\n",
        "        class_emb, pos_embed = pos_embed[:, 0], pos_embed[:, 1:]\n",
        "        dim = x.shape[-1]\n",
        "        pos_embed = nn.functional.interpolate(pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2), scale_factor=math.sqrt(npatch / N), mode='bicubic',)\n",
        "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
        "\n",
        "# from functools import partial\n",
        "# def vit_predictor(**kwargs):\n",
        "#     model = VisionTransformerPredictor(\n",
        "#         mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "#     return model\n",
        "\n",
        "def vit(patch_size=16, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, # tiny\n",
        "        # patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, # small\n",
        "        # patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, # base\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "# encoder = vit()\n",
        "encoder = VisionTransformer(\n",
        "        patch_size=16, embed_dim=768, depth=1, num_heads=3, mlp_ratio=1,\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
        "\n",
        "# num_patches = (224/patch_size)^2\n",
        "# model = VisionTransformerPredictor(num_patches, embed_dim=768, predictor_embed_dim=384, depth=6, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02, **kwargs)\n",
        "model = VisionTransformerPredictor(num_patches=196, embed_dim=768, predictor_embed_dim=384, depth=1, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02)\n",
        "\n",
        "batch = 4\n",
        "img = torch.rand(batch, 3, 224, 224)\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/train.py\n",
        "mask_collator = MaskCollator(\n",
        "        input_size=(224, 224),\n",
        "        patch_size=16,\n",
        "        enc_mask_scale=(0.2, 0.8),\n",
        "        pred_mask_scale=(0.2, 0.8),\n",
        "        aspect_ratio=(0.3, 3.0),\n",
        "        nenc=1,\n",
        "        npred=4,\n",
        "        min_keep=4,\n",
        "        # allow_overlap=True)\n",
        "        allow_overlap=False)\n",
        "# self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
        "\n",
        "\n",
        "\n",
        "# v = mask_collator.step()\n",
        "# should pass the collater a list batch of idx?\n",
        "# collated_batch, collated_masks_enc, collated_masks_pred = mask_collator([batch,3,8])\n",
        "collated_batch, collated_masks_enc, collated_masks_pred = mask_collator([0,1,2,3])\n",
        "# print(v)\n",
        "\n",
        "\n",
        "def forward_target():\n",
        "    with torch.no_grad():\n",
        "        h = target_encoder(imgs)\n",
        "        h = F.layer_norm(h, (h.size(-1),))  # normalize over feature-dim\n",
        "        B = len(h)\n",
        "        # -- create targets (masked regions of h)\n",
        "        h = apply_masks(h, masks_pred)\n",
        "        h = repeat_interleave_batch(h, B, repeat=len(masks_enc))\n",
        "        return h\n",
        "\n",
        "def forward_context():\n",
        "    z = encoder(imgs, masks_enc)\n",
        "    z = predictor(z, masks_enc, masks_pred)\n",
        "    return z\n",
        "\n",
        "# # num_context_patches = 121 if allow_overlap=True else 11\n",
        "z = encoder(img, collated_masks_enc) # [batch, num_context_patches, embed_dim]\n",
        "# print(len(collated_masks_enc), len(collated_masks_pred)) # nenc, npred\n",
        "# print(collated_masks_enc[0].shape, collated_masks_pred[0].shape) # , [batch, num_context_patches = 121 or 11], [batch, num_target_patches]\n",
        "out = model(z, collated_masks_enc, collated_masks_pred)\n",
        "# # print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# print(224/16) # 14\n",
        "# print(len(collated_masks_enc), len(collated_masks_pred))\n",
        "print(collated_masks_enc, collated_masks_pred)\n",
        "# multiples of 14\n",
        "\n",
        "# print(collated_masks_pred[1])\n"
      ],
      "metadata": {
        "id": "Or3eQvUVg7l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(collated_masks_enc, collated_masks_pred)"
      ],
      "metadata": {
        "id": "u2mLjXAmXcwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ijepa multiblock.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "import math\n",
        "from multiprocessing import Value\n",
        "from logging import getLogger\n",
        "import torch\n",
        "_GLOBAL_SEED = 0\n",
        "logger = getLogger()\n",
        "\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=(224, 224),\n",
        "        patch_size=16,\n",
        "        enc_mask_scale=(0.2, 0.8),\n",
        "        pred_mask_scale=(0.2, 0.8),\n",
        "        aspect_ratio=(0.3, 3.0),\n",
        "        nenc=1,\n",
        "        npred=2,\n",
        "        min_keep=4,\n",
        "        allow_overlap=False\n",
        "    ):\n",
        "        super(MaskCollator, self).__init__()\n",
        "        if not isinstance(input_size, tuple):\n",
        "            input_size = (input_size, ) * 2\n",
        "        self.patch_size = patch_size\n",
        "        self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.aspect_ratio = aspect_ratio\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "        self._itr_counter = Value('i', -1)  # collator is shared across worker processes\n",
        "\n",
        "    def step(self):\n",
        "        i = self._itr_counter\n",
        "        with i.get_lock():\n",
        "            i.value += 1\n",
        "            v = i.value\n",
        "        return v\n",
        "\n",
        "    def _sample_block_size(self, generator, scale, aspect_ratio_scale):\n",
        "        _rand = torch.rand(1, generator=generator).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.height * self.width * mask_scale)\n",
        "        # -- Sample block aspect-ratio\n",
        "        min_ar, max_ar = aspect_ratio_scale\n",
        "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
        "        while h >= self.height:\n",
        "            h -= 1\n",
        "        while w >= self.width:\n",
        "            w -= 1\n",
        "\n",
        "        return (h, w)\n",
        "\n",
        "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
        "        h, w = b_size\n",
        "\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # --\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            top = torch.randint(0, self.height - h, (1,))\n",
        "            left = torch.randint(0, self.width - w, (1,))\n",
        "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
        "            mask[top:top+h, left:left+w] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "                    logger.warning(f'Mask generator says: \"Valid mask not found, decreasing acceptable-regions [{tries}]\"')\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
        "        mask_complement[top:top+h, left:left+w] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        B = len(batch)\n",
        "\n",
        "        collated_batch = torch.utils.data.default_collate(batch)\n",
        "\n",
        "        seed = self.step()\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(seed)\n",
        "        p_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.pred_mask_scale,\n",
        "            aspect_ratio_scale=self.aspect_ratio)\n",
        "        e_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.enc_mask_scale,\n",
        "            aspect_ratio_scale=(1., 1.))\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.height * self.width\n",
        "        min_keep_enc = self.height * self.width\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                logger.warning(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "\n",
        "        return collated_batch, collated_masks_enc, collated_masks_pred\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XQ7PxBMlsYCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ijepa train.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/train.py\n",
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "#\n",
        "\n",
        "import os\n",
        "\n",
        "# -- FOR DISTRIBUTED TRAINING ENSURE ONLY 1 DEVICE VISIBLE PER PROCESS\n",
        "try:\n",
        "    # -- WARNING: IF DOING DISTRIBUTED TRAINING ON A NON-SLURM CLUSTER, MAKE\n",
        "    # --          SURE TO UPDATE THIS TO GET LOCAL-RANK ON NODE, OR ENSURE\n",
        "    # --          THAT YOUR JOBS ARE LAUNCHED WITH ONLY 1 DEVICE VISIBLE\n",
        "    # --          TO EACH PROCESS\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = os.environ['SLURM_LOCALID']\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "import copy\n",
        "import logging\n",
        "import sys\n",
        "import yaml\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.multiprocessing as mp\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "\n",
        "from src.masks.multiblock import MaskCollator as MBMaskCollator\n",
        "from src.masks.utils import apply_masks\n",
        "from src.utils.distributed import (\n",
        "    init_distributed,\n",
        "    AllReduce\n",
        ")\n",
        "from src.utils.logging import (\n",
        "    CSVLogger,\n",
        "    gpu_timer,\n",
        "    grad_logger,\n",
        "    AverageMeter)\n",
        "from src.utils.tensors import repeat_interleave_batch\n",
        "from src.datasets.imagenet1k import make_imagenet1k\n",
        "\n",
        "from src.helper import (\n",
        "    load_checkpoint,\n",
        "    init_model,\n",
        "    init_opt)\n",
        "from src.transforms import make_transforms\n",
        "\n",
        "# --\n",
        "log_timings = True\n",
        "log_freq = 10\n",
        "checkpoint_freq = 50\n",
        "# --\n",
        "\n",
        "_GLOBAL_SEED = 0\n",
        "np.random.seed(_GLOBAL_SEED)\n",
        "torch.manual_seed(_GLOBAL_SEED)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logger = logging.getLogger()\n",
        "\n",
        "\n",
        "def main(args, resume_preempt=False):\n",
        "\n",
        "    # ----------------------------------------------------------------------- #\n",
        "    #  PASSED IN PARAMS FROM CONFIG FILE\n",
        "    # ----------------------------------------------------------------------- #\n",
        "\n",
        "    # -- META\n",
        "    use_bfloat16 = args['meta']['use_bfloat16']\n",
        "    model_name = args['meta']['model_name']\n",
        "    load_model = args['meta']['load_checkpoint'] or resume_preempt\n",
        "    r_file = args['meta']['read_checkpoint']\n",
        "    copy_data = args['meta']['copy_data']\n",
        "    pred_depth = args['meta']['pred_depth']\n",
        "    pred_emb_dim = args['meta']['pred_emb_dim']\n",
        "    if not torch.cuda.is_available():\n",
        "        device = torch.device('cpu')\n",
        "    else:\n",
        "        device = torch.device('cuda:0')\n",
        "        torch.cuda.set_device(device)\n",
        "\n",
        "    # -- DATA\n",
        "    use_gaussian_blur = args['data']['use_gaussian_blur']\n",
        "    use_horizontal_flip = args['data']['use_horizontal_flip']\n",
        "    use_color_distortion = args['data']['use_color_distortion']\n",
        "    color_jitter = args['data']['color_jitter_strength']\n",
        "    # --\n",
        "    batch_size = args['data']['batch_size']\n",
        "    pin_mem = args['data']['pin_mem']\n",
        "    num_workers = args['data']['num_workers']\n",
        "    root_path = args['data']['root_path']\n",
        "    image_folder = args['data']['image_folder']\n",
        "    crop_size = args['data']['crop_size']\n",
        "    crop_scale = args['data']['crop_scale']\n",
        "    # --\n",
        "\n",
        "    # -- MASK\n",
        "    allow_overlap = args['mask']['allow_overlap']  # whether to allow overlap b/w context and target blocks\n",
        "    patch_size = args['mask']['patch_size']  # patch-size for model training\n",
        "    num_enc_masks = args['mask']['num_enc_masks']  # number of context blocks\n",
        "    min_keep = args['mask']['min_keep']  # min number of patches in context block\n",
        "    enc_mask_scale = args['mask']['enc_mask_scale']  # scale of context blocks\n",
        "    num_pred_masks = args['mask']['num_pred_masks']  # number of target blocks\n",
        "    pred_mask_scale = args['mask']['pred_mask_scale']  # scale of target blocks\n",
        "    aspect_ratio = args['mask']['aspect_ratio']  # aspect ratio of target blocks\n",
        "    # --\n",
        "\n",
        "    # -- OPTIMIZATION\n",
        "    ema = args['optimization']['ema']\n",
        "    ipe_scale = args['optimization']['ipe_scale']  # scheduler scale factor (def: 1.0)\n",
        "    wd = float(args['optimization']['weight_decay'])\n",
        "    final_wd = float(args['optimization']['final_weight_decay'])\n",
        "    num_epochs = args['optimization']['epochs']\n",
        "    warmup = args['optimization']['warmup']\n",
        "    start_lr = args['optimization']['start_lr']\n",
        "    lr = args['optimization']['lr']\n",
        "    final_lr = args['optimization']['final_lr']\n",
        "\n",
        "    # -- LOGGING\n",
        "    folder = args['logging']['folder']\n",
        "    tag = args['logging']['write_tag']\n",
        "\n",
        "    dump = os.path.join(folder, 'params-ijepa.yaml')\n",
        "    with open(dump, 'w') as f:\n",
        "        yaml.dump(args, f)\n",
        "    # ----------------------------------------------------------------------- #\n",
        "\n",
        "    try:\n",
        "        mp.set_start_method('spawn')\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # -- init torch distributed backend\n",
        "    world_size, rank = init_distributed()\n",
        "    logger.info(f'Initialized (rank/world-size) {rank}/{world_size}')\n",
        "    if rank > 0:\n",
        "        logger.setLevel(logging.ERROR)\n",
        "\n",
        "    # -- log/checkpointing paths\n",
        "    log_file = os.path.join(folder, f'{tag}_r{rank}.csv')\n",
        "    save_path = os.path.join(folder, f'{tag}' + '-ep{epoch}.pth.tar')\n",
        "    latest_path = os.path.join(folder, f'{tag}-latest.pth.tar')\n",
        "    load_path = None\n",
        "    if load_model:\n",
        "        load_path = os.path.join(folder, r_file) if r_file is not None else latest_path\n",
        "\n",
        "    # -- make csv_logger\n",
        "    csv_logger = CSVLogger(log_file,\n",
        "                           ('%d', 'epoch'),\n",
        "                           ('%d', 'itr'),\n",
        "                           ('%.5f', 'loss'),\n",
        "                           ('%.5f', 'mask-A'),\n",
        "                           ('%.5f', 'mask-B'),\n",
        "                           ('%d', 'time (ms)'))\n",
        "\n",
        "    # -- init model\n",
        "    encoder, predictor = init_model(\n",
        "        device=device,\n",
        "        patch_size=patch_size,\n",
        "        crop_size=crop_size,\n",
        "        pred_depth=pred_depth,\n",
        "        pred_emb_dim=pred_emb_dim,\n",
        "        model_name=model_name)\n",
        "    target_encoder = copy.deepcopy(encoder)\n",
        "\n",
        "    # -- make data transforms\n",
        "    mask_collator = MBMaskCollator(\n",
        "        input_size=crop_size,\n",
        "        patch_size=patch_size,\n",
        "        pred_mask_scale=pred_mask_scale,\n",
        "        enc_mask_scale=enc_mask_scale,\n",
        "        aspect_ratio=aspect_ratio,\n",
        "        nenc=num_enc_masks,\n",
        "        npred=num_pred_masks,\n",
        "        allow_overlap=allow_overlap,\n",
        "        min_keep=min_keep)\n",
        "\n",
        "    transform = make_transforms(\n",
        "        crop_size=crop_size,\n",
        "        crop_scale=crop_scale,\n",
        "        gaussian_blur=use_gaussian_blur,\n",
        "        horizontal_flip=use_horizontal_flip,\n",
        "        color_distortion=use_color_distortion,\n",
        "        color_jitter=color_jitter)\n",
        "\n",
        "    # -- init data-loaders/samplers\n",
        "    _, unsupervised_loader, unsupervised_sampler = make_imagenet1k(\n",
        "            transform=transform,\n",
        "            batch_size=batch_size,\n",
        "            collator=mask_collator,\n",
        "            pin_mem=pin_mem,\n",
        "            training=True,\n",
        "            num_workers=num_workers,\n",
        "            world_size=world_size,\n",
        "            rank=rank,\n",
        "            root_path=root_path,\n",
        "            image_folder=image_folder,\n",
        "            copy_data=copy_data,\n",
        "            drop_last=True)\n",
        "    ipe = len(unsupervised_loader)\n",
        "\n",
        "    # -- init optimizer and scheduler\n",
        "    optimizer, scaler, scheduler, wd_scheduler = init_opt(\n",
        "        encoder=encoder,\n",
        "        predictor=predictor,\n",
        "        wd=wd,\n",
        "        final_wd=final_wd,\n",
        "        start_lr=start_lr,\n",
        "        ref_lr=lr,\n",
        "        final_lr=final_lr,\n",
        "        iterations_per_epoch=ipe,\n",
        "        warmup=warmup,\n",
        "        num_epochs=num_epochs,\n",
        "        ipe_scale=ipe_scale,\n",
        "        use_bfloat16=use_bfloat16)\n",
        "    encoder = DistributedDataParallel(encoder, static_graph=True)\n",
        "    predictor = DistributedDataParallel(predictor, static_graph=True)\n",
        "    target_encoder = DistributedDataParallel(target_encoder)\n",
        "    for p in target_encoder.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    # -- momentum schedule\n",
        "    momentum_scheduler = (ema[0] + i*(ema[1]-ema[0])/(ipe*num_epochs*ipe_scale)\n",
        "                          for i in range(int(ipe*num_epochs*ipe_scale)+1))\n",
        "\n",
        "    start_epoch = 0\n",
        "    # -- load training checkpoint\n",
        "    if load_model:\n",
        "        encoder, predictor, target_encoder, optimizer, scaler, start_epoch = load_checkpoint(\n",
        "            device=device,\n",
        "            r_path=load_path,\n",
        "            encoder=encoder,\n",
        "            predictor=predictor,\n",
        "            target_encoder=target_encoder,\n",
        "            opt=optimizer,\n",
        "            scaler=scaler)\n",
        "        for _ in range(start_epoch*ipe):\n",
        "            scheduler.step()\n",
        "            wd_scheduler.step()\n",
        "            next(momentum_scheduler)\n",
        "            mask_collator.step()\n",
        "\n",
        "    def save_checkpoint(epoch):\n",
        "        save_dict = {\n",
        "            'encoder': encoder.state_dict(),\n",
        "            'predictor': predictor.state_dict(),\n",
        "            'target_encoder': target_encoder.state_dict(),\n",
        "            'opt': optimizer.state_dict(),\n",
        "            'scaler': None if scaler is None else scaler.state_dict(),\n",
        "            'epoch': epoch,\n",
        "            'loss': loss_meter.avg,\n",
        "            'batch_size': batch_size,\n",
        "            'world_size': world_size,\n",
        "            'lr': lr\n",
        "        }\n",
        "        if rank == 0:\n",
        "            torch.save(save_dict, latest_path)\n",
        "            if (epoch + 1) % checkpoint_freq == 0:\n",
        "                torch.save(save_dict, save_path.format(epoch=f'{epoch + 1}'))\n",
        "\n",
        "    # -- TRAINING LOOP\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        logger.info('Epoch %d' % (epoch + 1))\n",
        "\n",
        "        # -- update distributed-data-loader epoch\n",
        "        unsupervised_sampler.set_epoch(epoch)\n",
        "\n",
        "        loss_meter = AverageMeter()\n",
        "        maskA_meter = AverageMeter()\n",
        "        maskB_meter = AverageMeter()\n",
        "        time_meter = AverageMeter()\n",
        "\n",
        "        for itr, (udata, masks_enc, masks_pred) in enumerate(unsupervised_loader):\n",
        "\n",
        "            def load_imgs():\n",
        "                # -- unsupervised imgs\n",
        "                imgs = udata[0].to(device, non_blocking=True)\n",
        "                masks_1 = [u.to(device, non_blocking=True) for u in masks_enc]\n",
        "                masks_2 = [u.to(device, non_blocking=True) for u in masks_pred]\n",
        "                return (imgs, masks_1, masks_2)\n",
        "            imgs, masks_enc, masks_pred = load_imgs()\n",
        "            maskA_meter.update(len(masks_enc[0][0]))\n",
        "            maskB_meter.update(len(masks_pred[0][0]))\n",
        "\n",
        "            def train_step():\n",
        "                _new_lr = scheduler.step()\n",
        "                _new_wd = wd_scheduler.step()\n",
        "                # --\n",
        "\n",
        "                def forward_target():\n",
        "                    with torch.no_grad():\n",
        "                        h = target_encoder(imgs)\n",
        "                        h = F.layer_norm(h, (h.size(-1),))  # normalize over feature-dim\n",
        "                        B = len(h)\n",
        "                        # -- create targets (masked regions of h)\n",
        "                        h = apply_masks(h, masks_pred)\n",
        "                        h = repeat_interleave_batch(h, B, repeat=len(masks_enc))\n",
        "                        return h\n",
        "\n",
        "                def forward_context():\n",
        "                    z = encoder(imgs, masks_enc)\n",
        "                    z = predictor(z, masks_enc, masks_pred)\n",
        "                    return z\n",
        "\n",
        "                def loss_fn(z, h):\n",
        "                    loss = F.smooth_l1_loss(z, h)\n",
        "                    loss = AllReduce.apply(loss)\n",
        "                    return loss\n",
        "\n",
        "                # Step 1. Forward\n",
        "                with torch.cuda.amp.autocast(dtype=torch.bfloat16, enabled=use_bfloat16):\n",
        "                    h = forward_target()\n",
        "                    z = forward_context()\n",
        "                    loss = loss_fn(z, h)\n",
        "\n",
        "                #  Step 2. Backward & step\n",
        "                if use_bfloat16:\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                else:\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                grad_stats = grad_logger(encoder.named_parameters())\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Step 3. momentum update of target encoder\n",
        "                with torch.no_grad():\n",
        "                    m = next(momentum_scheduler)\n",
        "                    for param_q, param_k in zip(encoder.parameters(), target_encoder.parameters()):\n",
        "                        param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "\n",
        "                return (float(loss), _new_lr, _new_wd, grad_stats)\n",
        "            (loss, _new_lr, _new_wd, grad_stats), etime = gpu_timer(train_step)\n",
        "            loss_meter.update(loss)\n",
        "            time_meter.update(etime)\n",
        "\n",
        "            # -- Logging\n",
        "            def log_stats():\n",
        "                csv_logger.log(epoch + 1, itr, loss, maskA_meter.val, maskB_meter.val, etime)\n",
        "                if (itr % log_freq == 0) or np.isnan(loss) or np.isinf(loss):\n",
        "                    logger.info('[%d, %5d] loss: %.3f '\n",
        "                                'masks: %.1f %.1f '\n",
        "                                '[wd: %.2e] [lr: %.2e] '\n",
        "                                '[mem: %.2e] '\n",
        "                                '(%.1f ms)'\n",
        "                                % (epoch + 1, itr,\n",
        "                                   loss_meter.avg,\n",
        "                                   maskA_meter.avg,\n",
        "                                   maskB_meter.avg,\n",
        "                                   _new_wd,\n",
        "                                   _new_lr,\n",
        "                                   torch.cuda.max_memory_allocated() / 1024.**2,\n",
        "                                   time_meter.avg))\n",
        "\n",
        "                    if grad_stats is not None:\n",
        "                        logger.info('[%d, %5d] grad_stats: [%.2e %.2e] (%.2e, %.2e)'\n",
        "                                    % (epoch + 1, itr,\n",
        "                                       grad_stats.first_layer,\n",
        "                                       grad_stats.last_layer,\n",
        "                                       grad_stats.min,\n",
        "                                       grad_stats.max))\n",
        "\n",
        "            log_stats()\n",
        "\n",
        "            assert not np.isnan(loss), 'loss is nan'\n",
        "\n",
        "        # -- Save Checkpoint after every epoch\n",
        "        logger.info('avg. loss %.3f' % loss_meter.avg)\n",
        "        save_checkpoint(epoch+1)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "iVUPGP93dpAN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}