{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "s3EO3PgMPH1x"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/I_JEPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LxACli7GdyGq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "574ec42e-c969-416f-b206-0aa8630f77af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 48.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title data\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "transform = transforms.Compose([transforms.ToTensor(),])\n",
        "# transform = transforms.Compose([transforms.RandomResizedCrop((32,32), scale=(.3,1.)), transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "# CIFAR10: 60000 32x32 color images in 10 classes, with 6000 images per class\n",
        "train_data = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
        "test_data = torchvision.datasets.CIFAR10(root='data', train=False, download=True, transform=transform)\n",
        "batch_size = 64 # 4\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# dataiter = iter(train_loader) # get some random training images\n",
        "# images, labels = next(dataiter)\n",
        "# print(images.shape) # [batch, 3, 32, 32]\n",
        "# imshow(torchvision.utils.make_grid(images))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hiera"
      ],
      "metadata": {
        "id": "s3EO3PgMPH1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ResBlock\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters(): p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, kernel=3):\n",
        "        super().__init__()\n",
        "        out_ch = out_ch or in_ch\n",
        "        act = nn.SiLU() #\n",
        "        self.res_conv = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
        "        # self.block = nn.Sequential( # best?\n",
        "        #     nn.Conv2d(in_ch, out_ch, 3, padding=1), nn.BatchNorm2d(out_ch), act,\n",
        "        #     zero_module(nn.Conv2d(out_ch, out_ch, 3, padding=1)), nn.BatchNorm2d(out_ch), act,\n",
        "        #     )\n",
        "        self.block = nn.Sequential(\n",
        "            nn.BatchNorm2d(in_ch), act, nn.Conv2d(in_ch, out_ch, kernel, padding=kernel//2),\n",
        "            nn.BatchNorm2d(out_ch), act, zero_module(nn.Conv2d(out_ch, out_ch, kernel, padding=kernel//2)),\n",
        "            )\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w]\n",
        "        return self.block(x) + self.res_conv(x)\n"
      ],
      "metadata": {
        "id": "j3-vvMS1-gVn",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title UpDownBlock_me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class PixelShuffleConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, kernel=1, r=1):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        r = max(r, int(1/r))\n",
        "        out_ch = out_ch or in_ch\n",
        "        if self.r>1: self.net = nn.Sequential(ResBlock(in_ch, out_ch*r**2, kernel), nn.PixelShuffle(r))\n",
        "        # if self.r>1: self.net = nn.Sequential(Attention(in_ch, out_ch*r**2), nn.PixelShuffle(r))\n",
        "\n",
        "        elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), ResBlock(in_ch*r**2, out_ch, kernel))\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), Attention(in_ch*r**2, out_ch))\n",
        "        elif in_ch != out_ch: self.net = ResBlock(in_ch*r**2, out_ch, kernel)\n",
        "        else: self.net = lambda x: torch.zeros_like(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def AdaptiveAvgPool_nd(n, *args, **kwargs): return [nn.Identity, nn.AdaptiveAvgPool1d, nn.AdaptiveAvgPool2d, nn.AdaptiveAvgPool3d][n](*args, **kwargs)\n",
        "def AdaptiveMaxPool_nd(n, *args, **kwargs): return [nn.Identity, nn.AdaptiveMaxPool1d, nn.AdaptiveMaxPool2d, nn.AdaptiveMaxPool3d][n](*args, **kwargs)\n",
        "\n",
        "def adaptive_avg_pool_nd(n, x, output_size): return [nn.Identity, F.adaptive_avg_pool1d, F.adaptive_avg_pool2d, F.adaptive_avg_pool3d][n](x, output_size)\n",
        "def adaptive_max_pool_nd(n, x, output_size): return [nn.Identity, F.adaptive_max_pool1d, F.adaptive_max_pool2d, F.adaptive_max_pool3d][n](x, output_size)\n",
        "\n",
        "class AdaptivePool_at(nn.AdaptiveAvgPool1d): # AdaptiveAvgPool1d AdaptiveMaxPool1d\n",
        "    def __init__(self, dim=1, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.dim=dim\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(self.dim,-1)\n",
        "        shape = x.shape\n",
        "        return super().forward(x.flatten(0,-2)).unflatten(0, shape[:-1]).transpose(self.dim,-1)\n",
        "\n",
        "\n",
        "def adaptive_pool_at(x, dim, output_size, pool='avg'): # [b,c,h,w]\n",
        "    x = x.transpose(dim,-1)\n",
        "    shape = x.shape\n",
        "    parent={'avg':F.adaptive_avg_pool1d, 'max':F.adaptive_max_pool1d}[pool]\n",
        "    return parent(x.flatten(0,-2), output_size).unflatten(0, shape[:-1]).transpose(dim,-1)\n",
        "\n",
        "\n",
        "class ZeroExtend():\n",
        "    def __init__(self, dim=1, output_size=16):\n",
        "        self.dim, self.out = dim, output_size\n",
        "    def __call__(self, x): # [b,c,h,w]\n",
        "        return torch.cat((x, torch.zeros(*x.shape[:self.dim], self.out - x.shape[self.dim], *x.shape[self.dim+1:])), dim=self.dim)\n",
        "\n",
        "def make_pool_at(pool='avg', dim=1, output_size=5):\n",
        "    parent={'avg':nn.AdaptiveAvgPool1d, 'max':nn.AdaptiveMaxPool1d}[pool]\n",
        "    class AdaptivePool_at(parent): # AdaptiveAvgPool1d AdaptiveMaxPool1d\n",
        "        def __init__(self, dim=1, *args, **kwargs):\n",
        "            super().__init__(*args, **kwargs)\n",
        "            self.dim=dim\n",
        "        def forward(self, x):\n",
        "            x = x.transpose(self.dim,-1)\n",
        "            shape = x.shape\n",
        "            return super().forward(x.flatten(0,-2)).unflatten(0, shape[:-1]).transpose(self.dim,-1)\n",
        "    return AdaptivePool_at(dim, output_size=output_size)\n",
        "\n",
        "class Shortcut():\n",
        "    def __init__(self, dim=1, c=3, sp=(3,3), nd=2):\n",
        "        self.dim = dim\n",
        "        # self.ch_pool = make_pool_at(pool='avg', dim=dim, output_size=c)\n",
        "        self.ch_pool = make_pool_at(pool='max', dim=dim, output_size=c)\n",
        "        # self.ch_pool = ZeroExtend(dim, output_size=c) # only for out_dim>=in_dim\n",
        "        # self.sp_pool = AdaptiveAvgPool_nd(nd, sp)\n",
        "        self.sp_pool = AdaptiveMaxPool_nd(nd, sp)\n",
        "\n",
        "    def __call__(self, x): # [b,c,h,w]\n",
        "        x = self.sp_pool(x) # spatial first preserves spatial more?\n",
        "        x = self.ch_pool(x)\n",
        "        return x\n",
        "\n",
        "class UpDownBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, kernel=7, r=1):\n",
        "        super().__init__()\n",
        "        act = nn.SiLU()\n",
        "        self.r = r\n",
        "        self.block = PixelShuffleConv(in_ch, out_ch, kernel=kernel, r=r)\n",
        "        # self.block = nn.Sequential(\n",
        "        #     nn.BatchNorm2d(in_ch), act, PixelShuffleConv(in_ch, out_ch, kernel=kernel, r=r)\n",
        "        # )\n",
        "        # if self.r>1: self.res_conv = nn.Sequential(nn.ConvTranspose2d(in_ch, out_ch, kernel, 2, kernel//2, output_padding=1))\n",
        "        # if self.r>1: self.res_conv = nn.Sequential(nn.Upsample(scale_factor=2, mode='nearest'), nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity())\n",
        "        # if self.r>1: self.res_conv = nn.Sequential(nn.Upsample(scale_factor=2, mode='bilinear'), nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity())\n",
        "        # if self.r>1: self.res_conv = nn.Sequential(nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True), nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity())\n",
        "\n",
        "\n",
        "        # elif self.r<1: self.res_conv = nn.Sequential(nn.Conv2d(in_ch, out_ch, kernel, 2, kernel//2))\n",
        "        # elif self.r<1: self.res_conv = nn.Sequential(nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity(), nn.MaxPool2d(2,2))\n",
        "        # elif self.r<1: self.res_conv = nn.Sequential(nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity(), nn.AvgPool2d(2,2))\n",
        "        # elif self.r<1: self.res_conv = AttentionBlock(in_ch, out_ch, n_heads=4, q_stride=(2,2))\n",
        "\n",
        "        # else: self.res_conv = nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity()\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w]\n",
        "        b, num_tok, c, *win = x.shape\n",
        "        x = x.flatten(0,1)\n",
        "        out = self.block(x)\n",
        "        # # shortcut = F.interpolate(x.unsqueeze(1), size=out.shape[1:], mode='nearest-exact').squeeze(1) # pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html\n",
        "        # shortcut = F.adaptive_avg_pool3d(x, out.shape[1:]) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "        # # shortcut = F.adaptive_max_pool3d(x, out.shape[1:]) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "        # # shortcut = F.adaptive_avg_pool3d(x, out.shape[1:]) if out.shape[1]>=x.shape[1] else F.adaptive_max_pool3d(x, out.shape[1:])\n",
        "        # shortcut(x)\n",
        "        shortcut = Shortcut(dim=1, c=out.shape[1], sp=out.shape[-2:], nd=2)(x)\n",
        "        out = out + shortcut\n",
        "        out = out.unflatten(0, (b, num_tok))\n",
        "        return out\n",
        "\n",
        "        # return out + shortcut + self.res_conv(x)\n",
        "        # return out + self.res_conv(x)\n",
        "        # return self.res_conv(x)\n",
        "\n",
        "# if out>in, inter=max=ave=near.\n",
        "# if out<in, inter=ave. max=max\n",
        "\n",
        "# stride2\n",
        "# interconv/convpool\n",
        "# pixelconv\n",
        "# pixeluib\n",
        "# pixelres\n",
        "# shortcut\n",
        "\n",
        "# in_ch, out_ch = 16,3\n",
        "in_ch, out_ch = 3,16\n",
        "model = UpDownBlock(in_ch, out_ch, r=1/2).to(device)\n",
        "# model = UpDownBlock(in_ch, out_ch, r=2).to(device)\n",
        "\n",
        "x = torch.rand(12, in_ch, 64,64, device=device)\n",
        "x = torch.rand(12, 2, in_ch, 64,64, device=device)\n",
        "out = model(x)\n",
        "\n",
        "print(out.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0lclc9myo2c",
        "outputId": "6a87c120-7a2f-4100-961d-41f45f7fb4ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([12, 2, 16, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title maxpool path bchw\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def conv_nd(n, *args, **kwargs): return [nn.Identity, nn.Conv1d, nn.Conv2d, nn.Conv3d][n](*args, **kwargs)\n",
        "def maxpool_nd(n, *args, **kwargs): return [nn.Identity, nn.MaxPool1d, nn.MaxPool2d, nn.MaxPool3d][n](*args, **kwargs)\n",
        "def avgpool_nd(n, *args, **kwargs): return [nn.Identity, nn.AvgPool1d, nn.AvgPool2d, nn.AvgPool3d][n](*args, **kwargs)\n",
        "import math\n",
        "\n",
        "class MaskUnitAttention(nn.Module):\n",
        "    # def __init__(self, d_model=16, n_heads=4, q_stride=None, nd=2):\n",
        "    def __init__(self, in_dim, d_model=16, n_heads=4, q_stride=None, nd=2):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads, self.d_head = n_heads, d_model // n_heads\n",
        "        self.scale = self.d_head**-.5\n",
        "        # self.qkv = conv_nd(nd, d_model, 3*d_model, 1, bias=False)\n",
        "        self.qkv = conv_nd(nd, in_dim, 3*d_model, 1, bias=False)\n",
        "        # self.out = conv_nd(nd, d_model, d_model, 1)\n",
        "        self.out = nn.Linear(d_model, d_model, 1)\n",
        "        self.q_stride = q_stride # If greater than 1, pool q with this stride. The stride should be flattened (e.g., 2x2 = 4).\n",
        "        if q_stride:\n",
        "            self.q_stride = (q_stride,)*nd if type(q_stride)==int else q_stride\n",
        "            self.q_pool = maxpool_nd(nd, self.q_stride, stride=self.q_stride)\n",
        "\n",
        "    def forward(self, x): # [b,num_tok,c,win1,win2]\n",
        "        b, num_tok, c, *win = x.shape\n",
        "        # x = x.transpose(1,2).flatten(0,1) # [b*num_tok,c,win1,win2]\n",
        "        x = x.flatten(0,1) # [b*num_tok,c,win1,win2]\n",
        "        # print(x.shape)\n",
        "        # q, k, v = self.qkv(x).reshape(b, 3, self.n_heads, self.d_head, num_tok, win*win).permute(1,0,2,4,5,3) # [b,3*d_model,num_tok*win,win] -> 3* [b, n_heads, num_tok, win*win, d_head]\n",
        "        # self.qkv(x).flatten(1,-2).unflatten(-1, (self.heads,-1)).chunk(3, dim=-1) # [b,sp,n_heads,d_head]\n",
        "        q,k,v = self.qkv(x).chunk(3, dim=1) # [b*num_tok,d,win,win]\n",
        "        if self.q_stride:\n",
        "            q = self.q_pool(q)\n",
        "            win=[w//s for w,s in zip(win, self.q_stride)] # win = win/q_stride\n",
        "        if math.prod(win) >= 200:\n",
        "            print('MUattn', math.prod(win))\n",
        "            q, k, v = map(lambda t: t.reshape(b*num_tok, self.n_heads, self.d_head, -1).transpose(-2,-1), (q,k,v)) # [b*num_tok, n_heads, win*win, d_head]\n",
        "        else: q, k, v = map(lambda t: t.reshape(b, num_tok, self.n_heads, self.d_head, -1).permute(0,2,1,4,3).flatten(2,3), (q,k,v)) # [b, n_heads, num_tok*win*win, d_head]\n",
        "\n",
        "        q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2) # [b, n_heads, t(/pool), d_head]\n",
        "        context = k.transpose(-2,-1) @ v # [b, n_heads, d_head, d_head]\n",
        "        x = q @ context # [b, n_heads, t(/pool), d_head]\n",
        "\n",
        "        # print('attn fwd 2',x.shape)\n",
        "        # x = x.transpose(1, 3).reshape(B, -1, self.d_model)\n",
        "        x = x.transpose(1,2).reshape(b, -1, self.d_model) # [b,t,d]\n",
        "        # x = x.transpose(-2,-1).reshape(x.shape[0], self.d_model, ) # [b,t,d]\n",
        "        # [b, n_heads, num_tok, win*win, d_head] -> [b, n_heads, d_head, num_tok, win*win] -> [b,c,num_tok*win,win]\n",
        "        # x = x.permute(0,1,4,2,3).reshape(b, self.d_model, num_tok*win,win) # [b, n_heads, num_tok, win*win, d_head]\n",
        "        # x = x.transpose(-2,-1).reshape(b, self.d_model, num_tok, *win) # [b*num_tok, n_heads, win*win, d_head]\n",
        "        x = self.out(x)\n",
        "        L=len(win)\n",
        "        x = x.reshape(b, num_tok, *win, self.d_model).permute(0,1,L+2,*range(2,L+2)) # [b, num_tok, out_dim, *win]\n",
        "        # x = x.unflatten(0, (b, num_tok))\n",
        "        return x # [b,num_tok,c,win1,win2]\n",
        "\n",
        "d_model=16\n",
        "model = MaskUnitAttention(d_model, n_heads=4, q_stride=2)\n",
        "# MaskUnitAttention(d_model=16, n_heads=4, q_stride=None, nd=2)\n",
        "# x=torch.randn(2,4,4,3)\n",
        "# x=torch.randn(2,3,d_model,8,8)\n",
        "x=torch.randn(2,3,d_model,32,32)\n",
        "# [b*num_tok,c,win,win]\n",
        "\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48f3f074-0637-4f27-8a71-4915a866a0c2",
        "id": "ZAyKHKivc0j7",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MUattn 256\n",
            "torch.Size([2, 3, 16, 16, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title hiera vit me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class LayerNorm_at(nn.RMSNorm): # LayerNorm RMSNorm\n",
        "    def __init__(self, dim=1, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.dim=dim\n",
        "    def forward(self, x):\n",
        "        return super().forward(x.transpose(self.dim,-1)).transpose(self.dim,-1)\n",
        "\n",
        "def unshuffle(x, window_shape): # [b,c,h,w] -> [b, h/win1* w/win2, c, win1,win2]\n",
        "    new_shape = list(x.shape[:2]) + [val for xx, win in zip(list(x.shape[2:]), window_shape) for val in [xx//win, win]] # [h,w]->[h/win1, win1, w/win2, win2]\n",
        "    x = x.reshape(new_shape) # [b, c, h/win1, win1, w/win2, win2]\n",
        "    # print('unsh',x.shape, window_shape, new_shape)\n",
        "    L = len(new_shape)\n",
        "    permute = ([0] + list(range(2, L - 1, 2)) + [1] + list(range(3, L, 2))) # [0,2,4,1,3,5] / [0,2,4,6,1,3,5,6]\n",
        "    return x.permute(permute).flatten(1, L//2-1) # [b, h/win1* w/win2, c, win1,win2]\n",
        "\n",
        "class HieraBlock(nn.Module):\n",
        "    # def __init__(self, d_model, n_heads, q_stride=None, mult=4, drop=0, nd=2):\n",
        "    def __init__(self, in_dim, d_model, n_heads, q_stride=None, mult=4, drop=0, nd=2):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        # self.norm = LayerNorm_at(2, d_model) # LayerNorm RMSNorm\n",
        "        self.norm = LayerNorm_at(2, in_dim) # LayerNorm RMSNorm\n",
        "        self.drop = nn.Dropout(drop)\n",
        "        # self.attn = MaskUnitAttention(d_model, n_heads, q_stride)\n",
        "        self.attn = MaskUnitAttention(in_dim, d_model, n_heads, q_stride)\n",
        "        ff_dim=d_model*mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), nn.Linear(d_model, ff_dim), nn.ReLU(), # ReLU GELU\n",
        "            nn.RMSNorm(ff_dim), nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "            # nn.RMSNorm(d_model), act, nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Linear(ff_dim, d_model)\n",
        "            # nn.RMSNorm(d_model), nn.Linear(d_model, ff_dim), nn.ReLU(), nn.Dropout(dropout), # ReLU GELU\n",
        "            # nn.Linear(ff_dim, d_model), nn.Dropout(dropout),\n",
        "        )\n",
        "        # self.res = conv_nd(nd, d_model, d_model, q_stride, q_stride) if q_stride else nn.Identity()\n",
        "\n",
        "        # self.res = nn.Sequential(\n",
        "        #     conv_nd(nd, in_dim, d_model, 1, 1) if in_dim!=d_model else nn.Identity(),\n",
        "        #     # maxpool_nd(nd, q_stride, q_stride) if q_stride else nn.Identity(),\n",
        "        #     avgpool_nd(nd, q_stride, q_stride) if q_stride else nn.Identity(),\n",
        "        # )\n",
        "        self.res = UpDownBlock(in_dim, d_model, kernel=1, r=1/2 if q_stride else 1)\n",
        "\n",
        "        # x = self.proj(x_norm).unflatten(1, (self.attn.q_stride, -1)).max(dim=1).values # pooling res # [b, (Sy, Sx, h/Sy, w/Sx), c] -> [b, (Sy, Sx), (h/Sy, w/Sx), c] -> [b, (h/Sy, w/Sx), c]\n",
        "\n",
        "\n",
        "    def forward(self, x): # [b, num_tok, c, *win]\n",
        "        b, num_tok, c, *win = x.shape\n",
        "        # print('attnblk fwd',x.shape)\n",
        "        # x = x + self.drop(self.self(self.norm(x)))\n",
        "        # print('attnblk fwd',self.res(x.flatten(0,1)).shape, self.drop(self.attn(self.norm(x))).flatten(0,1).shape)\n",
        "        # x = self.res(x.flatten(0,1)) + self.drop(self.attn(self.norm(x))).flatten(0,1) # [b*num_tok,c,win1,win2,win3]\n",
        "        x = self.res(x) + self.drop(self.attn(self.norm(x))) # [b*num_tok,c,win1,win2,win3]\n",
        "        # x = x + self.ff(x)\n",
        "        # x = x + self.ff(x.transpose(1,-1)).transpose(1,-1)\n",
        "        x = x + self.ff(x.transpose(2,-1)).transpose(2,-1)\n",
        "        # x = self.ff(x)\n",
        "        # x = x.unflatten(0, (b, num_tok))\n",
        "        return x\n",
        "\n",
        "    # def forward(self, x): # [b,t,c] # [b, (Sy, Sx, h/Sy, w/Sx), c]\n",
        "    #     # Attention + Q Pooling\n",
        "    #     x_norm = self.norm1(x)\n",
        "    #     if self.dim != self.dim_out:\n",
        "    #         x = self.proj(x_norm).unflatten(1, (self.attn.q_stride, -1)).max(dim=1).values # pooling res # [b, (Sy, Sx, h/Sy, w/Sx), c] -> [b, (Sy, Sx), (h/Sy, w/Sx), c] -> [b, (h/Sy, w/Sx), c]\n",
        "    #     x = x + self.drop_path(self.attn(x_norm))\n",
        "    #     x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "    #     return x\n",
        "\n",
        "\n",
        "class levelBlock(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, n_heads=None, depth=1, r=1):\n",
        "        super().__init__()\n",
        "        self.seq = nn.Sequential(\n",
        "            UpDownBlock(in_ch, out_ch, r=min(1,r)) if in_ch != out_ch or r<1 else nn.Identity(),\n",
        "            # AttentionBlock(d_model, d_model, n_heads, q_stride) if in_ch != out_ch or r<1 else nn.Identity(),\n",
        "            # AttentionBlock(d_model, d_model, n_heads, q_stride=(2,2)),\n",
        "            *[HieraBlock(d_model, d_model, n_heads) for i in range(1)],\n",
        "            # UpDownBlock(out_ch, out_ch, r=r) if r>1 else nn.Identity(),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.seq(x)\n",
        "\n",
        "\n",
        "class Hiera(nn.Module):\n",
        "    def __init__(self, patch_size, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "    # def __init__(self, in_dim, out_dim, d_model, n_heads, depth):\n",
        "        # patch_size=4\n",
        "        super().__init__()\n",
        "        self.embed = nn.Sequential( # in, out, kernel, stride, pad\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.MaxPool2d(3,2,1), # nn.MaxPool2d(2,2)\n",
        "            nn.Conv2d(in_dim, d_model, 7, 1, 7//2, bias=False),\n",
        "            # nn.Conv2d(in_dim, d_model, patch_size, patch_size), # like patch\n",
        "            # nn.Conv2d(in_dim, d_model, 3, 2, 3//2, bias=False), nn.BatchNorm2d(d_model), nn.ReLU(),\n",
        "            # UpDownBlock(in_dim, dim, r=1/2, kernel=3), UpDownBlock(dim, dim, r=1/2, kernel=3)\n",
        "            # nn.PixelUnshuffle(2), nn.Conv2d(in_dim*2**2, dim, 1, bias=False),\n",
        "            # ResBlock(in_dim, d_model, kernel),\n",
        "            )\n",
        "        # # self.pos_emb = LearnedRoPE2D(dim) # LearnedRoPE2D, RoPE2D\n",
        "\n",
        "        emb_shape = (32,32)\n",
        "        # emb_shape = (8,32,32)\n",
        "        if len(emb_shape) == 3: # for video\n",
        "            pos_spatial = nn.Parameter(torch.randn(1, emb_shape[1]*emb_shape[2], d_model)*.02)\n",
        "            pos_temporal = nn.Parameter(torch.randn(1, emb_shape[0], d_model)*.02)\n",
        "            self.pos_emb = pos_spatial.repeat(1, emb_shape[0], 1) + torch.repeat_interleave(pos_temporal, emb_shape[1] * emb_shape[2], dim=1)\n",
        "        elif len(emb_shape) == 2: # for img\n",
        "            self.pos_emb = nn.Parameter(torch.randn(1, d_model, *emb_shape)*.02) # 56*56=3136\n",
        "        # self.pos_emb = self.pos_emb.flatten(2).transpose(-2,-1)\n",
        "\n",
        "        # self.blocks = nn.Sequential(*[AttentionBlock(d_model, n_heads, q_stride=(2,2) if i in [1,3] else None) for i in range(depth)])\n",
        "        # mult = [1,1,1,1]\n",
        "        mult = [1,2,4,8] # [1,2,3,4] [1,2,2,2]\n",
        "        ch_list = [d_model * m for m in mult] # [128, 256, 384, 512]\n",
        "\n",
        "        self.blocks = nn.Sequential(\n",
        "            # HieraBlock(ch_list[0], ch_list[1], n_heads, q_stride=(2,2)),\n",
        "            UpDownBlock(ch_list[0], ch_list[1], kernel=1, r=1/2),\n",
        "            HieraBlock(ch_list[1], ch_list[1], n_heads),\n",
        "            # HieraBlock(ch_list[1], ch_list[2], n_heads, q_stride=(2,2)),\n",
        "            UpDownBlock(ch_list[1], ch_list[2], kernel=1, r=1/2),\n",
        "            HieraBlock(ch_list[2], ch_list[2], n_heads),\n",
        "            )\n",
        "        self.norm = nn.RMSNorm(ch_list[2]) # LayerNorm RMSNorm\n",
        "        self.out = nn.Linear(ch_list[2], out_dim, bias=False) if out_dim and out_dim != ch_list[2] else None\n",
        "\n",
        "\n",
        "    def forward(self, x, context_indices=None): # [b,c,h,w], [b,num_tok]\n",
        "        x = self.embed(x)\n",
        "        # print('vit fwd', x.shape, self.pos_emb.shape)\n",
        "        x = x + self.pos_emb\n",
        "        x = unshuffle(x, (4,4)) # [b,num_tok,c,win1,win2,win3] or [b,1,c,f,h,w]\n",
        "        if context_indices != None: x = x[torch.arange(x.shape[0]).unsqueeze(-1), context_indices] # [batch, num_context_toks, d_model]\n",
        "\n",
        "        # print('vit fwd1', x.shape)\n",
        "        x = self.blocks(x) # [b,num_tok,c,1,1,1]\n",
        "        # print('vit fwd2', x.shape)\n",
        "        x = x.squeeze() # [b,num_tok,d]\n",
        "        out = self.norm(x)\n",
        "        if self.out: out = self.out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "# pos_emb rope < learn < learned\n",
        "# conv > pixel?\n",
        "# droppath not required\n",
        "\n",
        "# norm,act,conv < conv,norm,act\n",
        "# 2*s1 < uib < resblock\n",
        "# gatedadaln 3 < 2 = 1 < ffmult4 = 2*gatedadaln\n",
        "# MaxPool2d(2,2) < MaxPool2d(3,2,3//2)\n",
        "\n",
        "# patchattn only for\n",
        "\n",
        "# multiendfusion negligible diff\n",
        "\n",
        "\n",
        "d_model = 64\n",
        "in_dim = 3\n",
        "patch_size = 2\n",
        "model = Hiera(patch_size, in_dim, d_model, n_heads=4, nlayers=4, drop=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x = torch.rand((4, in_dim, 32, 32), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 59850\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lec1Nq7nkbgt",
        "outputId": "34dd2b5b-977d-486b-efc2-464249a29b32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1475904\n",
            "torch.Size([4, 64, 256])\n",
            "1475904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## vit"
      ],
      "metadata": {
        "id": "g3XSZZyUPY1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title FSQ me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def ste_round(x): return x.round().detach() + x - x.detach()\n",
        "\n",
        "class FSQ(nn.Module):\n",
        "    def __init__(self, levels):\n",
        "        super().__init__()\n",
        "        self.levels = torch.tensor(levels, device=device)\n",
        "        self.basis = torch.cumprod(torch.tensor([*levels[1:], 1], device=device).flip(-1), dim=0).flip(-1)\n",
        "        self.half_width = (self.levels-1)/2\n",
        "        self.codebook_size = torch.prod(self.levels).item()\n",
        "        self.codebook = self.indexes_to_codes(torch.arange(self.codebook_size, device=device))\n",
        "\n",
        "    def forward(self, z, beta=1): # beta in (0,1). beta->0 => values more spread out\n",
        "        offset = (self.levels+1) % 2 /2 # .5 if even, 0 if odd\n",
        "        bound = (F.sigmoid(z)-1/2) * (self.levels-beta) + offset\n",
        "        # print('fwd', bound) #\n",
        "        quantized = ste_round(bound)\n",
        "        # print('fwd', quantized) # 4: -1012\n",
        "        return (quantized-offset) / self.half_width # split [-1,1]\n",
        "\n",
        "    def codes_to_indexes(self, zhat):\n",
        "        zhat = (zhat + 1) * self.half_width\n",
        "        return (zhat * self.basis).sum(axis=-1)#.int()\n",
        "\n",
        "    def indexes_to_codes(self, indices):\n",
        "        indices = indices.unsqueeze(-1)\n",
        "        codes = torch.remainder(indices//self.basis, self.levels)\n",
        "        # print(\"codes\",codes)\n",
        "        return codes / self.half_width - 1\n",
        "\n",
        "# fsq = FSQ(levels = [5,4,3,2])\n",
        "# # print(fsq.codebook)\n",
        "# batch_size, seq_len = 2, 4\n",
        "# # x = torch.rand((batch_size, seq_len,3),device=device)\n",
        "# x = torch.linspace(-2,2,7).repeat(4,1).T\n",
        "# la = fsq(x)\n",
        "# print(la)\n",
        "# lact = fsq.codes_to_indexes(la)\n",
        "# print(lact)\n",
        "# # la = fsq.indexes_to_codes(lact)\n",
        "# # print(la)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "eDqkaM2v0_zS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title RoPE2D & RotEmb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "def RoPE(dim, seq_len=512, base=10000):\n",
        "    theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "    pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "    angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "    rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [1, seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "    return rot_emb\n",
        "\n",
        "# class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "#     def __init__(self, dim, seq_len=512, base=10000):\n",
        "#         super().__init__()\n",
        "#         self.dim, self.base = dim, base\n",
        "#         theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "#         pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "#         angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "#         self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [1, seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "#     def forward(self, x): # [batch, T, dim] / [batch, T, n_heads, d_head]\n",
        "#         seq_len = x.size(1)\n",
        "#         if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.base)\n",
        "#         # print('rope fwd', x.shape, self.rot_emb.shape)\n",
        "#         if x.dim()==4: return x * self.rot_emb[:,:seq_len].unsqueeze(2)\n",
        "#         return x * self.rot_emb[:,:seq_len]\n",
        "\n",
        "\n",
        "def RoPE2D(dim=16, h=8, w=8, base=10000):\n",
        "    # theta = 1. / (base ** (torch.arange(0, dim, step=4) / dim))\n",
        "    # theta = 1. / (base**torch.linspace(0,1,dim//4)).unsqueeze(0)\n",
        "    theta = 1. / (base**torch.linspace(0,1,dim//2)).unsqueeze(0)\n",
        "    y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\") # print(y,x) # [h,w], y:row_num, x:col_num\n",
        "    y, x = (y.reshape(-1,1) * theta).unsqueeze(-1), (x.reshape(-1,1) * theta).unsqueeze(-1) # [h*w,1]*[1,dim//4] = [h*w, dim//4, 1]\n",
        "    # rot_emb = torch.cat([x.sin(), x.cos(), y.sin(), y.cos()], dim=-1).flatten(-2) # [h*w, dim//4 ,4] -> [h*w, dim]\n",
        "    # rot_emb = torch.cat([x.sin(), x.cos(), y.sin(), y.cos()], dim=-1)#.reshape(dim, h, w).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "    rot_emb = torch.cat([x.sin(), y.sin()], dim=-1).flatten(-2).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "    # rot_emb = torch.cat([x.cos(), y.cos()], dim=-1).reshape(h, w, dim).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "    return rot_emb\n",
        "\n",
        "\n",
        "\n",
        "# class RoPE2D(nn.Module): # Rotary Positional Embeddings\n",
        "#     def __init__(self, dim, h=224, w=224, base=10000):\n",
        "#         super().__init__()\n",
        "#         self.dim, self.h, self.w = dim, h, w\n",
        "#         # # theta = 1. / (base ** (torch.arange(0, dim, step=4) / dim))\n",
        "#         # theta = 1. / (base**torch.linspace(0,1,dim//4)).unsqueeze(0)\n",
        "#         theta = 1. / (base**torch.linspace(0,1,dim//2)).unsqueeze(0)\n",
        "#         y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\") # print(y,x) # [h,w], y:row_num, x:col_num\n",
        "#         y, x = (y.reshape(-1,1) * theta).unsqueeze(-1), (x.reshape(-1,1) * theta).unsqueeze(-1) # [h*w,1]*[1,dim//4] = [h*w, dim//4, 1]\n",
        "#         # # self.rot_emb = torch.cat([x.sin(), x.cos(), y.sin(), y.cos()], dim=-1).flatten(-2) # [h*w, dim//4 ,4] -> [h*w, dim]\n",
        "#         # self.rot_emb = torch.cat([x.sin(), x.cos(), y.sin(), y.cos()], dim=-1).reshape(h, w, dim).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "#         self.rot_emb = torch.cat([x.sin(), y.sin()], dim=-1).reshape(h, w, dim).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "#         # self.rot_emb = torch.cat([x.cos(), y.cos()], dim=-1).reshape(h, w, dim).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "\n",
        "#     def forward(self, img): #\n",
        "#         # batch, dim, h, w = img.shape\n",
        "#         # print(img.shape)\n",
        "#         hw = img.shape[1] # [b, hw, dim] / [b, hw, n_heads, d_head]\n",
        "#         h=w=int(hw**.5)\n",
        "#         if self.h < h or self.w < w: self.__init__(self.dim, h, w)\n",
        "#         # print(self.rot_emb.shape)\n",
        "#         # rot_emb = self.rot_emb[:, :h, :w].unsqueeze(0) # [1, h, w, dim]\n",
        "#         rot_emb = self.rot_emb[:h, :w] # [h, w, dim]\n",
        "#         # return img * rot_emb.flatten(end_dim=1).unsqueeze(0) # [b, hw, dim] * [1, hw, dim]\n",
        "#         return img * rot_emb.flatten(end_dim=1)[None,:,None,:] # [b, hw, n_heads, d_head] * [1, hw, 1, dim]\n",
        "#         # return img * self.rot_emb\n",
        "\n",
        "class RotEmb(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, top=torch.pi, base=10000):\n",
        "        super().__init__()\n",
        "        self.theta = top / (base ** (torch.arange(0, dim, step=2, device=device) / dim))\n",
        "        # self.theta = top / (base ** torch.linspace(0, 1, dim//2, device=device))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (pos.unsqueeze(-1) * self.theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [seq_len, dim]\n"
      ],
      "metadata": {
        "id": "ge36SCxOl2Oq",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title AttentionBlock\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class SelfAttn(nn.Module):\n",
        "    def __init__(self, dim, n_heads):\n",
        "        super().__init__()\n",
        "        self.dim, self.n_heads = dim, n_heads\n",
        "        d_head = dim//n_heads\n",
        "        self.qkv = nn.Linear(dim, dim*3, bias=False)\n",
        "        # self.lin = nn.Linear(dim, dim)\n",
        "        self.lin = zero_module(nn.Linear(dim, dim))\n",
        "        # self.rope = RoPE(d_head, seq_len=512, base=10000)\n",
        "        # self.rope = RoPE2D(d_head, h=64, w=64, base=100)\n",
        "        self.scale = d_head**-.5\n",
        "\n",
        "    def forward(self, x): # [b,t,d]\n",
        "        q,k,v = self.qkv(x).unflatten(-1, (self.n_heads,-1)).transpose(1,2).chunk(3, dim=-1) # [b, t, n_heads, d_head] -> [b, n_heads, t, d_head]\n",
        "        # q, k = self.rope(q), self.rope(k)\n",
        "        q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2)\n",
        "        context = k.transpose(-2,-1) @ v # [batch, n_heads, d_head, d_head]\n",
        "        x = q @ context # [b, n_heads, t, d_head]\n",
        "        # print('SelfAttn', x.shape)\n",
        "        x = x.transpose(1,2).flatten(2)\n",
        "        return self.lin(x)\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, mult=4, drop=0.):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        # self.norm = nn.RMSNorm(d_model, elementwise_affine=False) # LayerNorm RMSNorm\n",
        "        self.norm1, self.norm2 = nn.LayerNorm(d_model), nn.LayerNorm(d_model)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "        self.attn = SelfAttn(d_model, n_heads)\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        ff_dim=d_model*mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), act, nn.Dropout(drop), nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "            nn.RMSNorm(ff_dim), act, nn.Dropout(drop), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(drop), nn.Linear(d_model, ff_dim), act,\n",
        "            # nn.RMSNorm(ff_dim), nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x): # [batch, seq_len, d_model]\n",
        "        # print('attnblk fwd',x.shape)\n",
        "        x = x + self.drop(self.attn(self.norm1(x)))\n",
        "        x = x + self.ff(x)\n",
        "        # x = x + self.drop(self.ff(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, patch_size, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        patch_size=2\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Conv2d(in_dim, d_model, patch_size, patch_size), # like patch\n",
        "            nn.Conv2d(in_dim, d_model, 7, 1, 7//2, bias=False)\n",
        "            # nn.Conv2d(in_dim, d_model, kernel_size=7, stride=2, padding=7//2, bias=False), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "            # nn.Conv2d(in_dim, d_model, 3, 2, 3//2, bias=False), nn.BatchNorm2d(d_model), nn.ReLU(),\n",
        "            )\n",
        "        # self.embed.requires_grad=False\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, 32*32, d_model)*.02)\n",
        "        # self.pos_emb = nn.Parameter(RoPE2D(dim=d_model, h=8, w=8, base=1000), requires_grad=False)\n",
        "\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=(32//patch_size)**2, base=10000), requires_grad=False)\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, d_model, n_heads) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x).flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c] # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        # x = self.pos_enc(x)\n",
        "        # print(\"TransformerModel\",x.shape, self.pos_emb.shape)\n",
        "        x = x + self.pos_emb[:,:x.shape[1]]\n",
        "        if context_indices != None: x = x[torch.arange(x.shape[0]).unsqueeze(-1), context_indices] # [batch, num_context_toks, d_model]\n",
        "\n",
        "        x = self.transformer(x)\n",
        "        out = self.norm(x)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "d_model = 64\n",
        "in_dim = 3\n",
        "patch_size = 2\n",
        "model = ViT(patch_size, in_dim, d_model, n_heads=4, nlayers=4, drop=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x = torch.rand((4, in_dim, 32, 32), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "f6T4F651kmGh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25fb9c30-62f2-4433-a1a2-ac88b938424f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "275456\n",
            "torch.Size([4, 1024, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title masks\n",
        "import torch\n",
        "\n",
        "def multiblock(seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "    return target_mask\n",
        "\n",
        "\n",
        "def multiblock2d(hw=(8,8), scale=(.15,.2), aspect_ratio=(.75,1.5), M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_aspect = torch.rand(1) * (aspect_ratio[1] - aspect_ratio[0]) + aspect_ratio[0] # in (min_s, max_s) # all blocks same size\n",
        "    mask_scale = torch.rand(1) * (scale[1] - scale[0]) + scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    h = (mask_scale/mask_aspect)**.5# h*(h*aspect) = scale\n",
        "    w = h * mask_aspect\n",
        "    h_pos, w_pos = torch.rand(M)*(1-w), torch.rand(M)*(1-h) # in (0, 1 - mask_len)\n",
        "    h_len, h_pos = (h*hw[0]).int(), h_pos*hw[0]\n",
        "    w_len, w_pos = (w*hw[1]).int(), w_pos*hw[1]\n",
        "    h_ind, w_ind = torch.arange(hw[0]).unsqueeze(0), torch.arange(hw[1]).unsqueeze(0) # [1, seq]\n",
        "    h_mask = (h_ind>=h_pos.unsqueeze(-1)) & (h_ind<(h_pos+h_len).unsqueeze(-1)) # [M, seq]\n",
        "    w_mask = (w_ind>=w_pos.unsqueeze(-1)) & (w_ind<(w_pos+w_len).unsqueeze(-1)) # [M, seq]\n",
        "    target_mask = h_mask.unsqueeze(-1) & w_mask.unsqueeze(-2) # [M, seq, seq]\n",
        "    return target_mask\n",
        "\n",
        "# https://arxiv.org/pdf/2210.07224\n",
        "def randpatch(seq, mask_size=8, gamma=0.9): # num patches of seq, mask patch size, masking ratio\n",
        "    # mask = torch.rand(seq//mask_size)<gamma\n",
        "    length = seq//mask_size\n",
        "    g = torch.normal(gamma, std=.1, size=(1,)).clamp(.5,.9)\n",
        "    # g = gamma\n",
        "    idx = torch.randperm(length)[:int(length*g)]\n",
        "    mask = torch.zeros(length, dtype=bool)\n",
        "    mask[idx] = True\n",
        "    mask = mask.repeat_interleave(mask_size, dim=-1)\n",
        "    return mask # [seq] , True -> mask\n",
        "\n",
        "\n",
        "import torch\n",
        "def apply_masks(x, mask): # [b,t,d], [mask_size] # https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "    mask_keep = mask.unsqueeze(-1).repeat(x.size(0), 1, x.size(-1)) # [batch,T,dim]\n",
        "    return torch.gather(x, dim=1, index=mask_keep) # [batch,mask_size,dim]\n",
        "\n",
        "\n",
        "# @title ijepa multiblock next\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "import math\n",
        "from multiprocessing import Value\n",
        "import torch\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(self, hw=(224, 224), enc_mask_scale=(.85,1), pred_mask_scale=(.15,.2), aspect_ratio=(.75,1.25),\n",
        "        nenc=1, npred=2, min_keep=4, allow_overlap=False):\n",
        "        super().__init__()\n",
        "        self.height, self.width = hw\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.aspect_ratio = aspect_ratio\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "\n",
        "    def _sample_block_size(self, scale, aspect_ratio_scale):\n",
        "        _rand = torch.rand(1).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.height * self.width * mask_scale) # num patches to keep\n",
        "        # -- Sample block aspect-ratio\n",
        "        min_ar, max_ar = aspect_ratio_scale\n",
        "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
        "        while h >= self.height: h -= 1 # crop mask to be smaller than img\n",
        "        while w >= self.width: w -= 1\n",
        "        return (h, w)\n",
        "\n",
        "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
        "        h, w = b_size\n",
        "\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # --\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            top = torch.randint(0, self.height - h, (1,))\n",
        "            left = torch.randint(0, self.width - w, (1,))\n",
        "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
        "            mask[top:top+h, left:left+w] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
        "        mask_complement[top:top+h, left:left+w] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, B):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        p_size = self._sample_block_size(scale=self.pred_mask_scale, aspect_ratio_scale=self.aspect_ratio)\n",
        "        e_size = self._sample_block_size(scale=self.enc_mask_scale, aspect_ratio_scale=(1., 1.))\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.height * self.width\n",
        "        min_keep_enc = self.height * self.width\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                print(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "        return collated_masks_enc, collated_masks_pred\n",
        "\n",
        "mask_collator = MaskCollator(hw=(32,32), enc_mask_scale=(.85, 1.), pred_mask_scale=(.15, .2), aspect_ratio=(.75, 1.5),\n",
        "        nenc=1, npred=4, min_keep=4,\n",
        "        # allow_overlap=True)\n",
        "        allow_overlap=False)\n",
        "\n",
        "collated_masks_enc, collated_masks_pred = mask_collator(4)\n",
        "ctx_index, trg_index = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "\n",
        "mask = torch.zeros(1 ,32*32)\n",
        "mask[:, trg_index[:1]] = 1\n",
        "mask[:, ctx_index[:1]] = .5\n",
        "mask = mask.reshape(1,32,32)\n",
        "\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "imshow(mask)\n",
        "\n"
      ],
      "metadata": {
        "id": "pQfM2fYvcTh6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "cellView": "form",
        "outputId": "7d9fdfc3-b32d-4960-ce99-364b383c5308"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title simplex\n",
        "!pip install -q opensimplex\n",
        "import opensimplex\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def simplexmask2d(hw=(32,32), ctx_scale=(.85,1), trg_scale=(.6,.8), B=64, chaos=2):\n",
        "    seed = np.random.randint(1e10, size=B)\n",
        "    ix = iy = np.linspace(0, chaos, num=max(hw))\n",
        "    noise = opensimplex.noise3array(ix, iy, seed) # [b,h,w]\n",
        "    # plt.pcolormesh(noise[:1])\n",
        "    # plt.rcParams[\"figure.figsize\"] = (20,3)\n",
        "    # plt.show()\n",
        "    noise = torch.from_numpy(noise)\n",
        "    ctx_mask_scale = torch.rand(1) * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    trg_mask_scale = torch.rand(1) * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "    val, ind = noise.flatten(1).sort() # [b,h*w]\n",
        "    seq = hw[0]*hw[1]\n",
        "    trg_index = ind[:,-int(seq*trg_mask_scale):]\n",
        "    ctx_index = ind[:,-int(seq*ctx_mask_scale):-int(seq*trg_mask_scale)] # ctx wraps trg ; most similar to multiblock\n",
        "    # ctx_index = ind[:,:-(int(seq*ctx_mask_scale)-int(seq*trg_mask_scale))] # ctx hug bottom\n",
        "    return ctx_index, trg_index\n",
        "\n",
        "# def simplexmask3d(seq=512, ctx_scale=(.85,1), trg_scale=(.6,.8), B=64, chaos=2):\n",
        "def simplexmask3d(thw=(8,32,32), ctx_scale=(.85,1), trg_scale=(.6,.8), B=64, chaos=(.2,2)):\n",
        "    it = np.linspace(0, chaos[0], num=thw[0])\n",
        "    ix = iy = np.linspace(0, chaos[1], num=max(thw[1],thw[2]))\n",
        "    seed = np.random.randint(1e10, size=B)\n",
        "    noise = opensimplex.noise4array(ix, iy, it, seed) # [b,t,h,w]\n",
        "    # plt.pcolormesh(noise[:1])\n",
        "    # plt.rcParams[\"figure.figsize\"] = (20,3)\n",
        "    # plt.show()\n",
        "    noise = torch.from_numpy(noise)\n",
        "    ctx_mask_scale = torch.rand(1) * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    trg_mask_scale = torch.rand(1) * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "    # yy = y.flatten(2).sort()[0][int(hw[0]*hw[1]*mask_scale)]\n",
        "    val, ind = noise.flatten(2).sort() # [b,t,h*w]\n",
        "    # val, ind = noise.flatten(1).sort() # [b,t*h*w]\n",
        "    print(val.shape, ind.shape)\n",
        "    seq = thw[0]*thw[1]*thw[2]\n",
        "    trg_index = ind[:,:,-int(seq*trg_mask_scale):]\n",
        "    ctx_index = ind[:,:,-int(seq*ctx_mask_scale):-int(seq*trg_mask_scale)] # ctx wraps trg ; most similar to multiblock\n",
        "    # # ctx_index = ind[:,:-(int(seq*ctx_mask_scale)-int(seq*trg_mask_scale))] # ctx hug bottom\n",
        "    return ctx_index, trg_index\n",
        "\n",
        "\n",
        "# trg_index, ctx_index = simplexmask1d(seq=500, ctx_scale=(.85,1), trg_scale=(.6,.8), B=64, chaos=3)\n",
        "# simplexmask3d(thw=(8,32,32), ctx_scale=(.85,1), trg_scale=(.6,.8), B=64, chaos=(.2,2))\n",
        "\n",
        "ctx_index, trg_index = simplexmask2d(hw=(32,32), ctx_scale=(.85,1), trg_scale=(.6,.8), B=64, chaos=3)\n",
        "mask = torch.zeros(1 ,32*32)\n",
        "mask[:, trg_index[:1]] = 1\n",
        "mask[:, ctx_index[:1]] = .5\n",
        "mask = mask.reshape(1,32,32)\n",
        "# mask = mask.reshape(32,32)\n",
        "# print(mask)\n",
        "# plt.plot(mask)\n",
        "# from matplotlib import pyplot as plt\n",
        "# # # plt.pcolormesh(y)\n",
        "# plt.pcolormesh(mask)\n",
        "# plt.show()\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "imshow(mask)\n",
        "\n",
        "# from matplotlib import pyplot as plt\n",
        "# # plt.pcolormesh(y)\n",
        "# plt.pcolormesh(mask)\n",
        "# plt.show()\n",
        "\n",
        "# print(mask)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Fh9o__m2-j7K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "outputId": "c93d58f1-928c-4d83-d206-87786252267f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/268.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m266.2/268.0 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.0/268.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Transformer Predictor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, 32*32, d_model)*0.02)\n",
        "        # self.pos_emb = nn.Parameter(RoPE2D(dim=d_model, h=8, w=8, base=1000), requires_grad=False)\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=32*32, base=10000), requires_grad=False)\n",
        "\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, d_model, n_heads) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.randn(1,1,d_model)*0.02) # randn zeros\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim)# if out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = x.shape\n",
        "        # x = x * self.pos_enc(context_indices)\n",
        "        x = x + self.pos_emb[0,context_indices]\n",
        "\n",
        "        # pred_tokens = self.cls * self.pos_enc(trg_indices) # [M, num_trg_toks, d_model]\n",
        "        pred_tokens = self.cls + self.pos_emb[0,trg_indices]\n",
        "        # print(\"pred fwd\", x.shape, pred_tokens.shape)\n",
        "        x = torch.cat([x, pred_tokens], dim=1) # [batch, seq_len+num_trg_toks, d_model]\n",
        "        out = self.transformer(x)\n",
        "\n",
        "        out = self.norm(out)\n",
        "        out = out[:,seq:] # [batch, num_trg_toks, d_model]\n",
        "        out = self.lin(out)\n",
        "        # print(\"pred fwd\", out.shape)\n",
        "        return out # [seq_len, batch_size, ntoken]\n"
      ],
      "metadata": {
        "id": "M-zdjdJixtOu",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title IJEPA\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class IJEPA(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, nlayers=2, n_heads=4):\n",
        "        super().__init__()\n",
        "        out_dim = out_dim or d_model\n",
        "        self.patch_size = 4 # 4\n",
        "        self.student = Hiera(self.patch_size, in_dim, d_model, out_dim=out_dim, n_heads=n_heads, nlayers=nlayers, drop=0.)\n",
        "        self.student = ViT(self.patch_size, in_dim, d_model, out_dim=out_dim, n_heads=n_heads, nlayers=nlayers, drop=0.)\n",
        "        # self.predicter = TransformerPredictor(out_dim, d_model//2, out_dim, n_heads=n_heads, nlayers=nlayers//2, drop=0.)\n",
        "        self.predicter = TransformerPredictor(out_dim, 3*d_model//8, out_dim, n_heads=4, nlayers=1, drop=0.)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "\n",
        "    def loss(self, x): #\n",
        "        b,c,h,w = x.shape\n",
        "        # print('ijepa loss x',x.shape)\n",
        "        # mask_collator = MaskCollator(hw=(8,8), enc_mask_scale=(.85, 1.), pred_mask_scale=(.15, .2), aspect_ratio=(.75, 1.5), nenc=1, npred=4, min_keep=4, allow_overlap=False)\n",
        "        mask_collator = MaskCollator(hw=(32,32), enc_mask_scale=(.85, 1.), pred_mask_scale=(.15, .2), aspect_ratio=(.75, 1.5), nenc=1, npred=4, min_keep=4, allow_overlap=False)\n",
        "        collated_masks_enc, collated_masks_pred = mask_collator(b)\n",
        "        context_indices, trg_indices = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "        # print(context_indices.shape, trg_indices.shape)\n",
        "\n",
        "        # context_indices, trg_indices = simplexmask2d(hw=(8,8), ctx_scale=(.85,1), trg_scale=(.6,.8), B=b, chaos=3)\n",
        "        # context_indices, trg_indices = simplexmask2d(hw=(32,32), ctx_scale=(.85,1), trg_scale=(.6,.8), B=b, chaos=3)\n",
        "\n",
        "        # zero_mask = torch.zeros(b ,8*8, device=device)\n",
        "        # zero_mask[torch.arange(b).unsqueeze(-1), context_indices] = 1\n",
        "        # zero_mask = zero_mask.reshape(b,1,8,8)\n",
        "        # x = x * F.interpolate(mask, size=x.shape[2:], mode='nearest-exact') # zero masked locations\n",
        "\n",
        "        sx = self.student(x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "        # print('ijepa loss sx',sx.shape)\n",
        "        sy_ = self.predicter(sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "        sy_ = F.layer_norm(sy_, (sy_.size(-1),))\n",
        "        with torch.no_grad():\n",
        "            sy = self.teacher(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "            sy = sy[torch.arange(sy.shape[0]).unsqueeze(-1), trg_indices] # [batch, num_context_toks, d_model] # nan bec len(trg_ind)==0 # print('loss sy',torch.isnan(sy).any())\n",
        "            sy = F.layer_norm(sy, (sy.size(-1),))\n",
        "        loss = F.mse_loss(sy, sy_)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        sx = self.student(x)\n",
        "        out = sx.mean(dim=1)\n",
        "        return out\n",
        "\n",
        "# min_s=0.15, max_s, M\n",
        "# trg.15.2M4 C.85 1\n",
        "# dont normalise data\n",
        "# randmsk < multiblk < simplex\n",
        "# teacher=trans ~? teacher=copy\n",
        "# learn convemb\n",
        "# lr pred,student: 3e-3/1e-2, 1e-3\n",
        "\n",
        "ijepa = IJEPA(in_dim=3, d_model=64, out_dim=64, nlayers=4, n_heads=4).to(device)#.to(torch.float)\n",
        "# ijepa = IJEPA(in_dim=3, d_model=1024, out_dim=16, nlayers=12, d_head=16).to(device)#.to(torch.float)\n",
        "# optim = torch.optim.AdamW(ijepa.parameters(), lr=1e-3) # 1e-3?\n",
        "optim = torch.optim.AdamW([{'params': ijepa.student.parameters()},\n",
        "    {'params': ijepa.predicter.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # good 3e-3,1e-3 ; default 1e-2, 5e-2\n",
        "    # {'params': ijepa.predicter.parameters(), 'lr': 1e-2}], lr=1e-3, weight_decay=1e-2)\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/configs/in1k_vith14_ep300.yaml\n",
        "# d_model 1024,384\n",
        "# depth 12,6/12\n",
        "# wd 5e-2 - 4e-1\n",
        "# adamw 1e-4 - 1e-3 - 1e-6\n",
        "# ema 0.996-1\n",
        "\n",
        "print(sum(p.numel() for p in ijepa.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in ijepa.parameters())) # 27584\n",
        "# print(sum(p.numel() for p in ijepa.predicter.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in ijepa.student.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in ijepa.teacher.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# d_model^2 * nlayers\n",
        "\n",
        "x = torch.rand((64,3,32,32), device=device)\n",
        "out = ijepa.loss(x)\n",
        "print(out.shape)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(32, 10).to(device)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n",
        "# optim = torch.optim.AdamW([{'params': ijepa.parameters()}, {'params': classifier.parameters(), 'lr': 1e-3}], lr=1e-3)\n",
        "\n"
      ],
      "metadata": {
        "id": "ardu1zJwdHM3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9e062e4-5bef-494f-d1d2-0745703e95ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "103608\n",
            "torch.Size([])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title TransformerVICReg\n",
        "import torch\n",
        "from torch import nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class TransformerVICReg(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, drop=0):\n",
        "        super().__init__()\n",
        "        act = nn.GELU()\n",
        "        patch_size=4\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Linear(in_dim, d_model), act\n",
        "            # nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            # nn.Conv1d(d_model, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            # nn.Conv1d(d_model, d_model,3,2,3//2),\n",
        "            nn.Conv2d(in_dim, d_model, patch_size, patch_size), # patch\n",
        "            )\n",
        "        self.pos_enc = RoPE(d_model, seq_len=200, base=10000)\n",
        "        # self.pos_emb = nn.Parameter(torch.randn(1, 8*8, d_model))\n",
        "        # self.pos_emb = nn.Parameter(RoPE2D(dim=d_model, h=8, w=8, base=10000).unsqueeze(0), requires_grad=False)\n",
        "\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "\n",
        "        # out_dim = out_dim or d_model\n",
        "        self.lin = nn.Linear(d_model, out_dim)\n",
        "        self.attn_pool = nn.Linear(d_model, 1, bias=False)\n",
        "\n",
        "        dim_v = d_model * 4\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(out_dim, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v, bias=False),\n",
        "            )\n",
        "\n",
        "    def forward(self, x): # [b,t,d] / [b,c,h,w]\n",
        "        x = self.embed(x).flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c]\n",
        "        # x = self.embed(x.transpose(-2,-1)).transpose(-2,-1) # [b,t,d]\n",
        "        x = self.pos_enc(x)\n",
        "        # x = x + self.pos_emb\n",
        "\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        attn = self.attn_pool(x).squeeze(-1) # [batch, seq] # seq_pool\n",
        "        out = (torch.softmax(attn, dim=-1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, seq] @ [batch, seq, dim] -> [batch, dim]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch, ntoken]\n",
        "\n",
        "    def expand(self, x):\n",
        "        sx = self.forward(x)\n",
        "        vx = self.exp(sx)\n",
        "        return vx\n",
        "\n",
        "batch, seq_len, d_model = 4,3500,512\n",
        "in_dim, out_dim=3,16\n",
        "model = TransformerVICReg(in_dim, d_model, out_dim, d_head=4, nlayers=2, drop=0.).to(device)\n",
        "# x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "x =  torch.rand((batch, in_dim, 32,32), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ],
      "metadata": {
        "id": "ODpKypTCsfIt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2f4380b-03a8-49be-95b5-eed2b564dc05",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Violet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class Violet(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, nlayers=2, d_head=4):\n",
        "        super().__init__()\n",
        "        out_dim = out_dim or d_model\n",
        "        self.student = TransformerVICReg(in_dim, d_model, out_dim=out_dim, d_head=d_head, nlayers=nlayers, drop=0.)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "\n",
        "        # vicreg\n",
        "        self.sim_coeff=25. # 10.0 # 25.0 # λ\n",
        "        self.std_coeff=25. # 1.0 # 25.0 # µ\n",
        "        self.cov_coeff=1. # 25.0 # 1.0 # ν\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]c/ [b,c,h,w]\n",
        "        # print(x.shape)\n",
        "        # mask = simplexmask(hw=(8,8), scale=(.7,.8)).unsqueeze(0) # .6.8\n",
        "        # vx = self.student.expand(x, mask) # [batch, num_context_toks, out_dim]\n",
        "\n",
        "        vx = self.student.expand(x) # [batch, num_context_toks, out_dim]\n",
        "\n",
        "        with torch.no_grad(): vy = self.teacher.expand(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "        loss = self.vicreg(vx, vy)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        return self.student(x)\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size, num_features = x.shape\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = self.sim_coeff * repr_loss + self.std_coeff * std_loss + self.cov_coeff * cov_loss\n",
        "        print(\"in vicreg \",self.sim_coeff * repr_loss.item() , self.std_coeff * std_loss.item() , self.cov_coeff * cov_loss.item())\n",
        "        # return loss\n",
        "        return repr_loss, std_loss, cov_loss\n",
        "\n",
        "\n",
        "violet = Violet(in_dim=3, d_model=32, out_dim=16, nlayers=2, d_head=4).to(device)\n",
        "voptim = torch.optim.AdamW(violet.parameters(), lr=1e-3) # 1e-3?\n",
        "# voptim = torch.optim.AdamW([{'params': violet.student.transformer.parameters()},\n",
        "#     {'params': violet.student.exp.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # default 1e-2\n",
        "print(sum(p.numel() for p in violet.parameters() if p.requires_grad)) # 27584\n",
        "\n",
        "# x = torch.rand((2,1000,3), device=device)\n",
        "x = torch.rand((2,3,32,32), device=device)\n",
        "# x = torch.rand((2,1,16), device=device)\n",
        "loss = violet.loss(x)\n",
        "# print(out.shape)\n",
        "print(loss)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(16).to(device)\n",
        "# classifier = Classifier(16, 18).to(device)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n",
        "\n"
      ],
      "metadata": {
        "id": "5qwg9dG4sQ3h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f0092f5-5bb7-4725-db2b-dcf7b0ba1a96",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "62850\n",
            "in vicreg  0.00019920778413506923 24.74469393491745 4.793645480560826e-09\n",
            "(tensor(7.9683e-06, grad_fn=<MseLossBackward0>), tensor(0.9898, grad_fn=<AddBackward0>), tensor(4.7936e-09, grad_fn=<AddBackward0>))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in ijepa.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name, param)"
      ],
      "metadata": {
        "id": "m_BFm8Kh-j3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title RankMe\n",
        "# RankMe: Assessing the Downstream Performance of Pretrained Self-Supervised Representations by Their Rank jun 2023 https://arxiv.org/pdf/2210.02885\n",
        "import torch\n",
        "\n",
        "# https://github.com/Spidartist/IJEPA_endoscopy/blob/main/src/helper.py#L22\n",
        "def RankMe(Z):\n",
        "    \"\"\"\n",
        "    Calculate the RankMe score (the higher, the better).\n",
        "    RankMe(Z) = exp(-sum_{k=1}^{min(N, K)} p_k * log(p_k)),\n",
        "    where p_k = sigma_k (Z) / ||sigma_k (Z)||_1 + epsilon\n",
        "    where sigma_k is the kth singular value of Z.\n",
        "    where Z is the matrix of embeddings (N × K)\n",
        "    \"\"\"\n",
        "    # compute the singular values of the embeddings\n",
        "    # _u, s, _vh = torch.linalg.svd(Z, full_matrices=False)  # s.shape = (min(N, K),)\n",
        "    # s = torch.linalg.svd(Z, full_matrices=False).S\n",
        "    s = torch.linalg.svdvals(Z)\n",
        "    p = s / torch.sum(s, axis=0) + 1e-7\n",
        "    return torch.exp(-torch.sum(p * torch.log(p)))\n",
        "\n",
        "# Z = torch.randn(5, 3)\n",
        "# rankme = RankMe(Z)\n",
        "# print(rankme)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "eb1n4BhimG_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title LiDAR\n",
        "# LiDAR: Sensing Linear Probing Performance In Joint Embedding SSL Architectures https://arxiv.org/pdf/2312.04000\n",
        "# https://github.com/rbalestr-lab/stable-ssl/blob/main/stable_ssl/monitors.py#L106\n",
        "\n",
        "def LiDAR(sx, eps=1e-7, delta=1e-3):\n",
        "    sx = sx.unflatten(0, (-1, 2))\n",
        "    n, q, d = sx.shape\n",
        "    mu_x = sx.mean(dim=1) # mu_x # [n,d]\n",
        "    mu = mu_x.mean(dim=0) # mu # [d]\n",
        "\n",
        "    diff_b = (mu_x - mu).unsqueeze(-1) # [n,d,1]\n",
        "    S_b = (diff_b @ diff_b.transpose(-2,-1)).sum(0) / (n - 1) # [n,d,d] -> [d,d]\n",
        "    diff_w = (sx - mu_x.unsqueeze(1)).reshape(-1,d,1) # [n,q,d] -> [nq,d,1]\n",
        "    S_w = (diff_w @ diff_w.transpose(-2,-1)).sum(0) / (n * (q - 1)) + delta * torch.eye(d, device=sx.device) # [nq,d,d] -> [d,d]\n",
        "\n",
        "    eigvals_w, eigvecs_w = torch.linalg.eigh(S_w)\n",
        "    eigvals_w = torch.clamp(eigvals_w, min=eps)\n",
        "\n",
        "    invsqrt_w = (eigvecs_w * (1. / torch.sqrt(eigvals_w))) @ eigvecs_w.transpose(-1, -2)\n",
        "    S_lidar = invsqrt_w @ S_b @ invsqrt_w\n",
        "    lam = torch.linalg.eigh(S_lidar)[0].clamp(min=0)\n",
        "    p = lam / lam.sum() + eps\n",
        "    return torch.exp(-torch.sum(p * torch.log(p)))\n",
        "\n",
        "# sx = torch.randn(32, 128)\n",
        "# print(sx.shape)\n",
        "# lidar = LiDAR(sx)\n",
        "# print(lidar.item())\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "y24ZNFqW7woQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Nd-sGe6Ku4S",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "outputId": "a1a7e711-e2ff-4cfa-e579-74b6e89bcf24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>▅██▅▄▆▆▆▆▅▆▆▅▄▆▄▄▂▃▃▃▃▄▂▃▂▄▃▂▂▄▂▂▂▃▃▁▃▁▂</td></tr><tr><td>correct</td><td>▆▄▂▁▅█▃▅▆▃▆▃▆▁▆█▇▃▃▃▃▃▆▇▅▆▇▆▃▆▆▆▆▆█▄▆▆▆▃</td></tr><tr><td>lidar</td><td>▂▃▂▁▃▂▄▃▁▄▄▅▂▃▄▃▅▆▆▇▅▅▅▃▅▅▄▆▄▆▅▆▅█▇▇█▆██</td></tr><tr><td>loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▄▂▂</td></tr><tr><td>rankme</td><td>▁▁▂▄▄▃▄▃▄▄▅▄▅▅▅▅▆▆▆▆▆▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇█▇█▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>2.25342</td></tr><tr><td>correct</td><td>0.17188</td></tr><tr><td>lidar</td><td>8.55179</td></tr><tr><td>loss</td><td>0.11076</td></tr><tr><td>rankme</td><td>4.1951</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fresh-sun-62</strong> at: <a href='https://wandb.ai/bobdole/ijepa/runs/7uzdotg4' target=\"_blank\">https://wandb.ai/bobdole/ijepa/runs/7uzdotg4</a><br> View project at: <a href='https://wandb.ai/bobdole/ijepa' target=\"_blank\">https://wandb.ai/bobdole/ijepa</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250409_232221-7uzdotg4/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250409_234603-t620ohli</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/ijepa/runs/t620ohli' target=\"_blank\">noble-serenity-63</a></strong> to <a href='https://wandb.ai/bobdole/ijepa' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/ijepa' target=\"_blank\">https://wandb.ai/bobdole/ijepa</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/ijepa/runs/t620ohli' target=\"_blank\">https://wandb.ai/bobdole/ijepa/runs/t620ohli</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "!pip install -q wandb\n",
        "import wandb # https://docs.wandb.ai/quickstart\n",
        "wandb.login(key='487a2109e55dce4e13fc70681781de9f50f27be7')\n",
        "try: run.finish()\n",
        "except NameError: pass\n",
        "run = wandb.init(project=\"ijepa\", config={\"model\": \"res18\",})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34de8800-fa4b-4460-803c-3cd8dbfb6c70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57ea4e23-1129-4f19-d0bc-05d2a30b1cc6",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "0.125\n",
            "0.140625\n",
            "0.140625\n",
            "0.09375\n",
            "0.109375\n",
            "0.109375\n",
            "0.25\n",
            "0.125\n",
            "0.109375\n",
            "0.0625\n",
            "0.140625\n",
            "58\n",
            "strain 0.03941548243165016\n",
            "strain 0.045556314289569855\n",
            "strain 0.0339389368891716\n",
            "strain 0.045380137860774994\n",
            "strain 0.0416809543967247\n",
            "strain 0.04124179854989052\n",
            "strain 0.03512683883309364\n",
            "strain 0.04056186228990555\n",
            "strain 0.03845356032252312\n",
            "strain 0.05156373977661133\n",
            "strain 0.03789595142006874\n",
            "strain 0.03659912571310997\n",
            "strain 0.04028726741671562\n",
            "strain 0.034354038536548615\n",
            "strain 0.042168907821178436\n",
            "strain 0.04173533618450165\n",
            "strain 0.04145574942231178\n",
            "strain 0.038266368210315704\n",
            "strain 0.04969961941242218\n",
            "strain 0.03621554747223854\n",
            "strain 0.041688963770866394\n",
            "strain 0.04192989319562912\n",
            "strain 0.047454141080379486\n",
            "strain 0.04710529372096062\n",
            "strain 0.04254285991191864\n",
            "strain 0.03584969416260719\n",
            "strain 0.03250974044203758\n",
            "strain 0.044434912502765656\n",
            "strain 0.049259573221206665\n",
            "strain 0.04126684367656708\n",
            "strain 0.051552724093198776\n",
            "strain 0.037025049328804016\n",
            "strain 0.041044048964977264\n",
            "strain 0.045091357082128525\n",
            "strain 0.04238959401845932\n",
            "strain 0.038776710629463196\n",
            "strain 0.03177262470126152\n",
            "strain 0.04364483803510666\n",
            "strain 0.045029155910015106\n",
            "strain 0.044237375259399414\n",
            "strain 0.041635725647211075\n",
            "strain 0.036140214651823044\n",
            "strain 0.03239351511001587\n",
            "strain 0.038033246994018555\n",
            "strain 0.03515797480940819\n",
            "strain 0.05806238576769829\n",
            "strain 0.041467078030109406\n",
            "strain 0.05943962186574936\n",
            "strain 0.04093803092837334\n",
            "strain 0.0563662052154541\n",
            "strain 0.04144846275448799\n",
            "classify 2.2943115234375\n",
            "classify 2.337158203125\n",
            "classify 2.3326416015625\n",
            "classify 2.2733154296875\n",
            "classify 2.2574462890625\n",
            "classify 2.30322265625\n",
            "classify 2.281494140625\n",
            "classify 2.306396484375\n",
            "classify 2.3218994140625\n",
            "classify 2.251220703125\n",
            "classify 2.266845703125\n",
            "0.125\n",
            "0.125\n",
            "0.109375\n",
            "0.171875\n",
            "0.140625\n",
            "0.140625\n",
            "0.125\n",
            "0.25\n",
            "0.109375\n",
            "0.125\n",
            "0.125\n",
            "59\n",
            "strain 0.03842291608452797\n",
            "strain 0.03826109692454338\n",
            "strain 0.044002607464790344\n",
            "strain 0.03846343234181404\n",
            "strain 0.060627829283475876\n",
            "strain 0.03742489963769913\n",
            "strain 0.046779341995716095\n",
            "strain 0.03410224989056587\n",
            "strain 0.041069429367780685\n",
            "strain 0.042939670383930206\n",
            "strain 0.0308610200881958\n",
            "strain 0.04680735990405083\n",
            "strain 0.05013035237789154\n",
            "strain 0.03989236429333687\n",
            "strain 0.060008704662323\n",
            "strain 0.050435490906238556\n",
            "strain 0.041545119136571884\n",
            "strain 0.033223509788513184\n",
            "strain 0.04815114289522171\n",
            "strain 0.054238319396972656\n",
            "strain 0.03297749534249306\n",
            "strain 0.03508409112691879\n",
            "strain 0.04063158482313156\n",
            "strain 0.058006953448057175\n",
            "strain 0.04517628252506256\n",
            "strain 0.05432802438735962\n",
            "strain 0.04482220858335495\n",
            "strain 0.05666092038154602\n",
            "strain 0.04023923724889755\n",
            "strain 0.037174422293901443\n",
            "strain 0.039183810353279114\n",
            "strain 0.04995717853307724\n",
            "strain 0.03914360702037811\n",
            "strain 0.05519063398241997\n",
            "strain 0.030086636543273926\n",
            "strain 0.04211875796318054\n",
            "strain 0.04482463002204895\n",
            "strain 0.0447082594037056\n",
            "strain 0.040342021733522415\n",
            "strain 0.0391569659113884\n",
            "strain 0.033165086060762405\n",
            "strain 0.041780222207307816\n",
            "strain 0.04661807045340538\n",
            "strain 0.03599453344941139\n",
            "strain 0.0450567789375782\n",
            "strain 0.04095449671149254\n",
            "strain 0.039698489010334015\n",
            "strain 0.030887233093380928\n",
            "strain 0.044268470257520676\n",
            "strain 0.049567367881536484\n",
            "strain 0.033924758434295654\n",
            "classify 2.2864990234375\n",
            "classify 2.2486572265625\n",
            "classify 2.289306640625\n",
            "classify 2.2291259765625\n",
            "classify 2.2603759765625\n",
            "classify 2.2457275390625\n",
            "classify 2.318115234375\n",
            "classify 2.2325439453125\n",
            "classify 2.288330078125\n",
            "classify 2.275390625\n",
            "classify 2.2330322265625\n",
            "0.140625\n",
            "0.15625\n",
            "0.1875\n",
            "0.125\n",
            "0.140625\n",
            "0.15625\n",
            "0.109375\n",
            "0.15625\n",
            "0.140625\n",
            "0.109375\n",
            "0.109375\n",
            "60\n",
            "strain 0.04248771071434021\n",
            "strain 0.035425979644060135\n",
            "strain 0.04277779534459114\n",
            "strain 0.03575962036848068\n",
            "strain 0.044678714126348495\n",
            "strain 0.036036454141139984\n",
            "strain 0.031245000660419464\n",
            "strain 0.046715617179870605\n",
            "strain 0.032626133412122726\n",
            "strain 0.04369238018989563\n",
            "strain 0.04390093311667442\n",
            "strain 0.04687487334012985\n",
            "strain 0.040775492787361145\n",
            "strain 0.0375802256166935\n",
            "strain 0.04132026061415672\n",
            "strain 0.04480354115366936\n",
            "strain 0.027668936178088188\n",
            "strain 0.03577796742320061\n",
            "strain 0.04062841832637787\n",
            "strain 0.047726891934871674\n",
            "strain 0.050074733793735504\n",
            "strain 0.04556634649634361\n",
            "strain 0.03445448353886604\n",
            "strain 0.03312389925122261\n",
            "strain 0.03825049847364426\n",
            "strain 0.0372430682182312\n",
            "strain 0.044832855463027954\n",
            "strain 0.04221412539482117\n",
            "strain 0.04031195491552353\n",
            "strain 0.047633156180381775\n",
            "strain 0.04403035715222359\n",
            "strain 0.03641187772154808\n",
            "strain 0.03907307609915733\n",
            "strain 0.04129531979560852\n",
            "strain 0.036091793328523636\n",
            "strain 0.03656537085771561\n",
            "strain 0.052185237407684326\n",
            "strain 0.031205888837575912\n",
            "strain 0.04949554428458214\n",
            "strain 0.042581167072057724\n",
            "strain 0.044395845383405685\n",
            "strain 0.03794727474451065\n",
            "strain 0.0411095917224884\n",
            "strain 0.033054668456315994\n",
            "strain 0.03405200317502022\n",
            "strain 0.0441843681037426\n",
            "strain 0.034777846187353134\n",
            "strain 0.04191187769174576\n",
            "strain 0.04022851958870888\n",
            "strain 0.042065370827913284\n",
            "strain 0.03250647336244583\n",
            "classify 2.2462158203125\n",
            "classify 2.24609375\n",
            "classify 2.2373046875\n",
            "classify 2.267333984375\n",
            "classify 2.310302734375\n",
            "classify 2.3214111328125\n",
            "classify 2.3109130859375\n",
            "classify 2.26708984375\n",
            "classify 2.3052978515625\n",
            "classify 2.3074951171875\n",
            "classify 2.3087158203125\n",
            "0.140625\n",
            "0.171875\n",
            "0.078125\n",
            "0.15625\n",
            "0.140625\n",
            "0.109375\n",
            "0.15625\n",
            "0.09375\n",
            "0.125\n",
            "0.078125\n",
            "0.125\n",
            "61\n",
            "strain 0.04793431609869003\n",
            "strain 0.03684651106595993\n",
            "strain 0.036791156977415085\n",
            "strain 0.04093562439084053\n",
            "strain 0.0357680507004261\n",
            "strain 0.04057569056749344\n",
            "strain 0.036718860268592834\n",
            "strain 0.051254015415906906\n",
            "strain 0.03438054397702217\n",
            "strain 0.03404594957828522\n",
            "strain 0.03635019063949585\n",
            "strain 0.04591565206646919\n",
            "strain 0.03985259309411049\n",
            "strain 0.03424147143959999\n",
            "strain 0.048327431082725525\n",
            "strain 0.0404733382165432\n",
            "strain 0.03327304869890213\n",
            "strain 0.043909184634685516\n",
            "strain 0.043479956686496735\n",
            "strain 0.0445978157222271\n",
            "strain 0.0646677166223526\n",
            "strain 0.04249970242381096\n",
            "strain 0.055469971150159836\n",
            "strain 0.046934012323617935\n",
            "strain 0.04234177619218826\n",
            "strain 0.04558774456381798\n",
            "strain 0.03744148463010788\n",
            "strain 0.03668663650751114\n",
            "strain 0.049869291484355927\n",
            "strain 0.04004184901714325\n",
            "strain 0.04223328456282616\n",
            "strain 0.04070402309298515\n",
            "strain 0.04910573363304138\n",
            "strain 0.041600823402404785\n",
            "strain 0.036977462470531464\n",
            "strain 0.04345070570707321\n",
            "strain 0.04280747100710869\n",
            "strain 0.04503416642546654\n",
            "strain 0.04705829545855522\n",
            "strain 0.04076831415295601\n",
            "strain 0.039594609290361404\n",
            "strain 0.056736793369054794\n",
            "strain 0.040872473269701004\n",
            "strain 0.04676413536071777\n",
            "strain 0.04742126166820526\n",
            "strain 0.037674468010663986\n",
            "strain 0.04117298126220703\n",
            "strain 0.04197835177183151\n",
            "strain 0.040152717381715775\n",
            "strain 0.04299492761492729\n",
            "strain 0.041468311101198196\n",
            "classify 2.25537109375\n",
            "classify 2.3043212890625\n",
            "classify 2.32177734375\n",
            "classify 2.3233642578125\n",
            "classify 2.3238525390625\n",
            "classify 2.306640625\n",
            "classify 2.276611328125\n",
            "classify 2.2740478515625\n",
            "classify 2.2630615234375\n",
            "classify 2.2618408203125\n",
            "classify 2.3572998046875\n",
            "0.125\n",
            "0.09375\n",
            "0.1875\n",
            "0.125\n",
            "0.15625\n",
            "0.171875\n",
            "0.1875\n",
            "0.125\n",
            "0.109375\n",
            "0.140625\n",
            "0.109375\n",
            "62\n",
            "strain 0.03335793316364288\n",
            "strain 0.032174818217754364\n",
            "strain 0.05339910462498665\n",
            "strain 0.04154987260699272\n",
            "strain 0.04005300998687744\n",
            "strain 0.0415046364068985\n",
            "strain 0.04454289749264717\n",
            "strain 0.0413576140999794\n",
            "strain 0.0401364229619503\n",
            "strain 0.03927230462431908\n",
            "strain 0.03912901133298874\n",
            "strain 0.03795523941516876\n",
            "strain 0.05286533758044243\n",
            "strain 0.035260628908872604\n",
            "strain 0.04160913825035095\n",
            "strain 0.04024485498666763\n",
            "strain 0.04761876165866852\n",
            "strain 0.04842105135321617\n",
            "strain 0.04655217379331589\n",
            "strain 0.037296246737241745\n",
            "strain 0.04498429223895073\n",
            "strain 0.04948987066745758\n",
            "strain 0.04084101319313049\n",
            "strain 0.03942321985960007\n",
            "strain 0.03790232166647911\n",
            "strain 0.042505551129579544\n",
            "strain 0.0493716262280941\n",
            "strain 0.04808230325579643\n",
            "strain 0.03370336443185806\n",
            "strain 0.03949744254350662\n",
            "strain 0.03845130652189255\n",
            "strain 0.03400744870305061\n",
            "strain 0.03999077528715134\n",
            "strain 0.03632020950317383\n",
            "strain 0.04151109978556633\n",
            "strain 0.047725506126880646\n",
            "strain 0.05912186950445175\n",
            "strain 0.04118110612034798\n",
            "strain 0.03572043403983116\n",
            "strain 0.04443838447332382\n",
            "strain 0.037523239850997925\n",
            "strain 0.03957729414105415\n",
            "strain 0.04025329649448395\n",
            "strain 0.04238303378224373\n",
            "strain 0.040418319404125214\n",
            "strain 0.04312162846326828\n",
            "strain 0.03155871108174324\n",
            "strain 0.0422624871134758\n",
            "strain 0.041388146579265594\n",
            "strain 0.04457143694162369\n",
            "strain 0.050684407353401184\n",
            "classify 2.3502197265625\n",
            "classify 2.3018798828125\n",
            "classify 2.263671875\n",
            "classify 2.2833251953125\n",
            "classify 2.27880859375\n",
            "classify 2.3121337890625\n",
            "classify 2.26513671875\n",
            "classify 2.2266845703125\n",
            "classify 2.32177734375\n",
            "classify 2.2960205078125\n",
            "classify 2.3087158203125\n",
            "0.171875\n",
            "0.078125\n",
            "0.125\n",
            "0.15625\n",
            "0.265625\n",
            "0.09375\n",
            "0.15625\n",
            "0.109375\n",
            "0.109375\n",
            "0.09375\n",
            "0.1875\n",
            "63\n",
            "strain 0.04904842749238014\n",
            "strain 0.0426986962556839\n",
            "strain 0.0393800288438797\n",
            "strain 0.046168357133865356\n",
            "strain 0.03734785318374634\n",
            "strain 0.052390918135643005\n",
            "strain 0.03933095932006836\n",
            "strain 0.037918318063020706\n",
            "strain 0.0515812411904335\n",
            "strain 0.04058435186743736\n",
            "strain 0.05504412576556206\n",
            "strain 0.05509096384048462\n",
            "strain 0.039266686886548996\n",
            "strain 0.04200579226016998\n",
            "strain 0.050591807812452316\n",
            "strain 0.04299892112612724\n",
            "strain 0.03526066988706589\n",
            "strain 0.056369803845882416\n",
            "strain 0.05502880737185478\n",
            "strain 0.04780540242791176\n",
            "strain 0.03634660691022873\n",
            "strain 0.04823734238743782\n",
            "strain 0.04235608130693436\n",
            "strain 0.039125509560108185\n",
            "strain 0.04432612657546997\n",
            "strain 0.05156130716204643\n",
            "strain 0.038739245384931564\n",
            "strain 0.04339207336306572\n",
            "strain 0.0469551607966423\n",
            "strain 0.05446655675768852\n",
            "strain 0.03920963406562805\n",
            "strain 0.03608769178390503\n",
            "strain 0.03909403085708618\n",
            "strain 0.03677653148770332\n",
            "strain 0.03502059727907181\n",
            "strain 0.04028384014964104\n",
            "strain 0.03972794488072395\n",
            "strain 0.03468671441078186\n",
            "strain 0.05043473094701767\n",
            "strain 0.046243131160736084\n",
            "strain 0.04388713836669922\n",
            "strain 0.04258059337735176\n",
            "strain 0.04085727035999298\n",
            "strain 0.033515073359012604\n",
            "strain 0.046583741903305054\n",
            "strain 0.04620974138379097\n",
            "strain 0.04587313532829285\n",
            "strain 0.05026891082525253\n",
            "strain 0.04249286651611328\n",
            "strain 0.04378024861216545\n",
            "strain 0.040473662316799164\n",
            "classify 2.2720947265625\n",
            "classify 2.2935791015625\n",
            "classify 2.3001708984375\n",
            "classify 2.2869873046875\n",
            "classify 2.2469482421875\n",
            "classify 2.273681640625\n",
            "classify 2.29931640625\n",
            "classify 2.3079833984375\n",
            "classify 2.3621826171875\n",
            "classify 2.283203125\n",
            "classify 2.2784423828125\n",
            "0.09375\n",
            "0.078125\n",
            "0.25\n",
            "0.09375\n",
            "0.15625\n",
            "0.125\n",
            "0.109375\n",
            "0.09375\n",
            "0.109375\n",
            "0.171875\n",
            "0.078125\n",
            "64\n",
            "strain 0.043729785829782486\n",
            "strain 0.03557179123163223\n",
            "strain 0.0384785458445549\n",
            "strain 0.03772418946027756\n",
            "strain 0.03762882947921753\n",
            "strain 0.04259317368268967\n",
            "strain 0.051591090857982635\n",
            "strain 0.035619694739580154\n",
            "strain 0.055984996259212494\n",
            "strain 0.04775320738554001\n",
            "strain 0.03685739263892174\n",
            "strain 0.04068242758512497\n",
            "strain 0.038469310849905014\n",
            "strain 0.03732968866825104\n",
            "strain 0.04195966571569443\n",
            "strain 0.04586527869105339\n",
            "strain 0.04349548742175102\n",
            "strain 0.04482777416706085\n",
            "strain 0.04257207736372948\n",
            "strain 0.036319077014923096\n",
            "strain 0.05143287777900696\n",
            "strain 0.04614052176475525\n",
            "strain 0.04389839619398117\n",
            "strain 0.03694971278309822\n",
            "strain 0.044676631689071655\n",
            "strain 0.03750375285744667\n",
            "strain 0.05505934730172157\n",
            "strain 0.03542580083012581\n",
            "strain 0.048910874873399734\n",
            "strain 0.04397903010249138\n",
            "strain 0.04471750184893608\n",
            "strain 0.04787072539329529\n",
            "strain 0.03519953787326813\n",
            "strain 0.04311727359890938\n",
            "strain 0.04342033341526985\n",
            "strain 0.04406265169382095\n",
            "strain 0.037460971623659134\n",
            "strain 0.03812607005238533\n",
            "strain 0.05368155986070633\n",
            "strain 0.03948283940553665\n",
            "strain 0.05149361863732338\n",
            "strain 0.03571327030658722\n",
            "strain 0.04322252795100212\n",
            "strain 0.04756005108356476\n",
            "strain 0.05411570519208908\n",
            "strain 0.05352852866053581\n",
            "strain 0.05622735247015953\n",
            "strain 0.04626435041427612\n",
            "strain 0.056384067982435226\n",
            "strain 0.05344856530427933\n",
            "strain 0.04518752545118332\n",
            "classify 2.2642822265625\n",
            "classify 2.3206787109375\n",
            "classify 2.2078857421875\n",
            "classify 2.275146484375\n",
            "classify 2.3194580078125\n",
            "classify 2.3043212890625\n",
            "classify 2.2161865234375\n",
            "classify 2.271484375\n",
            "classify 2.28466796875\n",
            "classify 2.2830810546875\n",
            "classify 2.2928466796875\n",
            "0.171875\n",
            "0.21875\n",
            "0.078125\n",
            "0.078125\n",
            "0.125\n",
            "0.109375\n",
            "0.140625\n",
            "0.15625\n",
            "0.0625\n",
            "0.15625\n",
            "0.109375\n",
            "65\n",
            "strain 0.04124477133154869\n",
            "strain 0.058166198432445526\n",
            "strain 0.04887636750936508\n",
            "strain 0.04720040410757065\n",
            "strain 0.03916407376527786\n",
            "strain 0.046730004251003265\n",
            "strain 0.06314737349748611\n",
            "strain 0.03994452580809593\n",
            "strain 0.052074626088142395\n",
            "strain 0.03677083179354668\n",
            "strain 0.0505288764834404\n",
            "strain 0.04357997700572014\n",
            "strain 0.04377719759941101\n",
            "strain 0.04855019599199295\n",
            "strain 0.04900842905044556\n",
            "strain 0.03615207597613335\n",
            "strain 0.037496168166399\n",
            "strain 0.044511325657367706\n",
            "strain 0.03696295991539955\n",
            "strain 0.04839840158820152\n",
            "strain 0.04532880336046219\n",
            "strain 0.04806743562221527\n",
            "strain 0.05838015303015709\n",
            "strain 0.03849539905786514\n",
            "strain 0.051719360053539276\n",
            "strain 0.04669420048594475\n",
            "strain 0.036897219717502594\n",
            "strain 0.04865085706114769\n",
            "strain 0.03937671706080437\n",
            "strain 0.05317315086722374\n",
            "strain 0.040156833827495575\n",
            "strain 0.04535053297877312\n",
            "strain 0.04813799634575844\n",
            "strain 0.050462592393159866\n",
            "strain 0.0423625111579895\n",
            "strain 0.05246717855334282\n",
            "strain 0.045118480920791626\n",
            "strain 0.05621332675218582\n",
            "strain 0.0457356721162796\n",
            "strain 0.041627753525972366\n",
            "strain 0.0492638535797596\n",
            "strain 0.0704139769077301\n",
            "strain 0.05162578821182251\n",
            "strain 0.045288391411304474\n",
            "strain 0.04248318821191788\n",
            "strain 0.04615427181124687\n",
            "strain 0.04454651474952698\n",
            "strain 0.04446873068809509\n",
            "strain 0.040170229971408844\n",
            "strain 0.03684600070118904\n",
            "strain 0.03466922044754028\n",
            "classify 2.3416748046875\n",
            "classify 2.2286376953125\n",
            "classify 2.2880859375\n",
            "classify 2.2613525390625\n",
            "classify 2.288330078125\n",
            "classify 2.308349609375\n",
            "classify 2.3272705078125\n",
            "classify 2.25537109375\n",
            "classify 2.2655029296875\n",
            "classify 2.3023681640625\n",
            "classify 2.300537109375\n",
            "0.109375\n",
            "0.109375\n",
            "0.078125\n",
            "0.15625\n",
            "0.109375\n",
            "0.09375\n",
            "0.15625\n",
            "0.171875\n",
            "0.078125\n",
            "0.171875\n",
            "0.171875\n",
            "66\n",
            "strain 0.03881104663014412\n",
            "strain 0.04398591071367264\n",
            "strain 0.06217241287231445\n",
            "strain 0.036708611994981766\n",
            "strain 0.04017075151205063\n",
            "strain 0.052052006125450134\n",
            "strain 0.03901423513889313\n",
            "strain 0.048763927072286606\n",
            "strain 0.04076910763978958\n",
            "strain 0.046817243099212646\n",
            "strain 0.04190288484096527\n",
            "strain 0.047906529158353806\n",
            "strain 0.03318782150745392\n",
            "strain 0.06280364841222763\n",
            "strain 0.04104933142662048\n",
            "strain 0.04574596509337425\n",
            "strain 0.042447905987501144\n",
            "strain 0.04797721654176712\n",
            "strain 0.045804109424352646\n",
            "strain 0.041845135390758514\n",
            "strain 0.03564075008034706\n",
            "strain 0.04245183989405632\n",
            "strain 0.05131297558546066\n",
            "strain 0.03476770594716072\n",
            "strain 0.053982824087142944\n",
            "strain 0.034802332520484924\n",
            "strain 0.05701851472258568\n",
            "strain 0.03508847951889038\n",
            "strain 0.04680362343788147\n",
            "strain 0.05508800595998764\n",
            "strain 0.06346292793750763\n",
            "strain 0.04177132248878479\n",
            "strain 0.039087142795324326\n",
            "strain 0.04503285139799118\n",
            "strain 0.0484544113278389\n",
            "strain 0.04757849499583244\n",
            "strain 0.04506813362240791\n",
            "strain 0.040477458387613297\n",
            "strain 0.0595584362745285\n",
            "strain 0.042186200618743896\n",
            "strain 0.042742908000946045\n",
            "strain 0.05196048319339752\n",
            "strain 0.05897371098399162\n",
            "strain 0.04084508866071701\n",
            "strain 0.053430162370204926\n",
            "strain 0.05323414504528046\n",
            "strain 0.049390409141778946\n",
            "strain 0.039715416729450226\n",
            "strain 0.044839419424533844\n",
            "strain 0.03640829026699066\n",
            "strain 0.05878780409693718\n",
            "classify 2.240966796875\n",
            "classify 2.2620849609375\n",
            "classify 2.247314453125\n",
            "classify 2.2796630859375\n",
            "classify 2.2705078125\n",
            "classify 2.290771484375\n",
            "classify 2.3167724609375\n",
            "classify 2.26318359375\n",
            "classify 2.2919921875\n",
            "classify 2.324951171875\n",
            "classify 2.227783203125\n",
            "0.15625\n",
            "0.046875\n",
            "0.109375\n",
            "0.140625\n",
            "0.046875\n",
            "0.125\n",
            "0.109375\n",
            "0.234375\n",
            "0.140625\n",
            "0.171875\n",
            "0.203125\n",
            "67\n",
            "strain 0.040588703006505966\n",
            "strain 0.04019349440932274\n",
            "strain 0.03790666535496712\n",
            "strain 0.05294736474752426\n",
            "strain 0.0488114058971405\n",
            "strain 0.04147280007600784\n",
            "strain 0.041476164013147354\n",
            "strain 0.04162529855966568\n",
            "strain 0.04511793702840805\n",
            "strain 0.04594896361231804\n",
            "strain 0.034413933753967285\n",
            "strain 0.04066045209765434\n",
            "strain 0.040760088711977005\n",
            "strain 0.04700016230344772\n",
            "strain 0.0482630580663681\n",
            "strain 0.04964243248105049\n",
            "strain 0.0371258407831192\n",
            "strain 0.05397447198629379\n",
            "strain 0.050342950969934464\n",
            "strain 0.044964637607336044\n",
            "strain 0.039751119911670685\n",
            "strain 0.04187561571598053\n",
            "strain 0.04319397732615471\n",
            "strain 0.04791441559791565\n",
            "strain 0.05935542285442352\n",
            "strain 0.03224929794669151\n",
            "strain 0.04072700813412666\n",
            "strain 0.03713015839457512\n",
            "strain 0.0369606576859951\n",
            "strain 0.0416395477950573\n",
            "strain 0.04053562134504318\n",
            "strain 0.037833817303180695\n",
            "strain 0.06190135329961777\n",
            "strain 0.04413006454706192\n",
            "strain 0.04477021098136902\n",
            "strain 0.04996111989021301\n",
            "strain 0.04532422497868538\n",
            "strain 0.04203871265053749\n",
            "strain 0.04041618853807449\n",
            "strain 0.0421890951693058\n",
            "strain 0.04252666234970093\n",
            "strain 0.04936138913035393\n",
            "strain 0.04450834542512894\n",
            "strain 0.044744305312633514\n",
            "strain 0.03996849060058594\n",
            "strain 0.039166100323200226\n",
            "strain 0.04088107869029045\n",
            "strain 0.04892563447356224\n",
            "strain 0.0445251539349556\n",
            "strain 0.04463080316781998\n",
            "strain 0.046554576605558395\n",
            "classify 2.2689208984375\n",
            "classify 2.3095703125\n",
            "classify 2.2730712890625\n",
            "classify 2.3045654296875\n",
            "classify 2.2381591796875\n",
            "classify 2.25830078125\n",
            "classify 2.29296875\n",
            "classify 2.249267578125\n",
            "classify 2.273681640625\n",
            "classify 2.265625\n",
            "classify 2.330078125\n",
            "0.15625\n",
            "0.109375\n",
            "0.046875\n",
            "0.125\n",
            "0.15625\n",
            "0.09375\n",
            "0.1875\n",
            "0.109375\n",
            "0.09375\n",
            "0.140625\n",
            "0.1875\n",
            "68\n",
            "strain 0.04439159110188484\n",
            "strain 0.06310873478651047\n",
            "strain 0.04655352607369423\n",
            "strain 0.046147387474775314\n",
            "strain 0.042892150580883026\n",
            "strain 0.045543372631073\n",
            "strain 0.05450592562556267\n",
            "strain 0.03864830732345581\n",
            "strain 0.0470520555973053\n",
            "strain 0.057739030569791794\n",
            "strain 0.05463036522269249\n",
            "strain 0.0357176847755909\n",
            "strain 0.05007496103644371\n",
            "strain 0.040798164904117584\n",
            "strain 0.059612274169921875\n",
            "strain 0.050753798335790634\n",
            "strain 0.0453297421336174\n",
            "strain 0.044116146862506866\n",
            "strain 0.04959486797451973\n",
            "strain 0.0409507229924202\n",
            "strain 0.04766755551099777\n",
            "strain 0.04717915505170822\n",
            "strain 0.04832952469587326\n",
            "strain 0.041063036769628525\n",
            "strain 0.04085950925946236\n",
            "strain 0.038053590804338455\n",
            "strain 0.04397275671362877\n",
            "strain 0.059209078550338745\n",
            "strain 0.050196707248687744\n",
            "strain 0.05176477134227753\n",
            "strain 0.046637970954179764\n",
            "strain 0.04591900482773781\n",
            "strain 0.04025620222091675\n",
            "strain 0.038573578000068665\n",
            "strain 0.06092952936887741\n",
            "strain 0.04268365725874901\n",
            "strain 0.0381050668656826\n",
            "strain 0.04578682780265808\n",
            "strain 0.05284988507628441\n",
            "strain 0.044106580317020416\n",
            "strain 0.035968389362096786\n",
            "strain 0.04846992716193199\n",
            "strain 0.05048937723040581\n",
            "strain 0.05155567452311516\n",
            "strain 0.03894698619842529\n",
            "strain 0.0351797491312027\n",
            "strain 0.04591349512338638\n",
            "strain 0.04852167144417763\n",
            "strain 0.05541784316301346\n",
            "strain 0.05150379240512848\n",
            "strain 0.053388893604278564\n",
            "classify 2.294921875\n",
            "classify 2.288330078125\n",
            "classify 2.290771484375\n",
            "classify 2.31591796875\n",
            "classify 2.32861328125\n",
            "classify 2.3304443359375\n",
            "classify 2.277099609375\n",
            "classify 2.2564697265625\n",
            "classify 2.2567138671875\n",
            "classify 2.2891845703125\n",
            "classify 2.32568359375\n",
            "0.109375\n",
            "0.109375\n",
            "0.109375\n",
            "0.203125\n",
            "0.15625\n",
            "0.09375\n",
            "0.09375\n",
            "0.125\n",
            "0.140625\n",
            "0.15625\n",
            "0.125\n",
            "69\n",
            "strain 0.04426056519150734\n",
            "strain 0.044010039418935776\n",
            "strain 0.05899658426642418\n",
            "strain 0.07404877245426178\n",
            "strain 0.043516214936971664\n",
            "strain 0.04712247848510742\n",
            "strain 0.044306427240371704\n",
            "strain 0.049659423530101776\n",
            "strain 0.046538203954696655\n",
            "strain 0.03970286250114441\n",
            "strain 0.041518643498420715\n",
            "strain 0.042763471603393555\n",
            "strain 0.04595697671175003\n",
            "strain 0.035872019827365875\n",
            "strain 0.06816425919532776\n",
            "strain 0.06951310485601425\n",
            "strain 0.05287318304181099\n",
            "strain 0.03953685984015465\n",
            "strain 0.05437399446964264\n",
            "strain 0.05710886791348457\n",
            "strain 0.043943390250205994\n",
            "strain 0.04436039179563522\n",
            "strain 0.045731544494628906\n",
            "strain 0.04745045676827431\n",
            "strain 0.047650907188653946\n",
            "strain 0.045938216149806976\n",
            "strain 0.03873482346534729\n",
            "strain 0.04431590810418129\n",
            "strain 0.06309935450553894\n",
            "strain 0.04660206288099289\n",
            "strain 0.058469753712415695\n",
            "strain 0.045617248862981796\n",
            "strain 0.05113875865936279\n",
            "strain 0.048292726278305054\n",
            "strain 0.0370788499712944\n",
            "strain 0.04244372621178627\n",
            "strain 0.04785439372062683\n",
            "strain 0.07235704362392426\n",
            "strain 0.03969024494290352\n",
            "strain 0.051257096230983734\n",
            "strain 0.057486169040203094\n",
            "strain 0.048783522099256516\n",
            "strain 0.06130564212799072\n",
            "strain 0.05608886107802391\n",
            "strain 0.0626128762960434\n",
            "strain 0.05425529181957245\n",
            "strain 0.04781026765704155\n",
            "strain 0.04534997045993805\n",
            "strain 0.05316585302352905\n",
            "strain 0.047419317066669464\n",
            "strain 0.0486210435628891\n",
            "classify 2.2562255859375\n",
            "classify 2.262939453125\n",
            "classify 2.1854248046875\n",
            "classify 2.2825927734375\n",
            "classify 2.3048095703125\n",
            "classify 2.3599853515625\n",
            "classify 2.2508544921875\n",
            "classify 2.2489013671875\n",
            "classify 2.255615234375\n",
            "classify 2.2669677734375\n",
            "classify 2.259033203125\n",
            "0.1875\n",
            "0.15625\n",
            "0.171875\n",
            "0.203125\n",
            "0.1875\n",
            "0.21875\n",
            "0.109375\n",
            "0.125\n",
            "0.171875\n",
            "0.109375\n",
            "0.125\n",
            "70\n",
            "strain 0.053207408636808395\n",
            "strain 0.04791022464632988\n",
            "strain 0.03929780796170235\n",
            "strain 0.046622615307569504\n",
            "strain 0.046825896948575974\n",
            "strain 0.05322090908885002\n",
            "strain 0.047026485204696655\n",
            "strain 0.042519062757492065\n",
            "strain 0.05454292893409729\n",
            "strain 0.04257830232381821\n",
            "strain 0.05203115940093994\n",
            "strain 0.044356152415275574\n",
            "strain 0.04402506351470947\n",
            "strain 0.04804680868983269\n",
            "strain 0.04704459011554718\n",
            "strain 0.05399073660373688\n",
            "strain 0.04541092365980148\n",
            "strain 0.0481991171836853\n",
            "strain 0.03957565501332283\n",
            "strain 0.04240492731332779\n",
            "strain 0.043413419276475906\n",
            "strain 0.05376873537898064\n",
            "strain 0.0455937460064888\n",
            "strain 0.051735665649175644\n",
            "strain 0.055039357393980026\n",
            "strain 0.04459519684314728\n",
            "strain 0.038473356515169144\n",
            "strain 0.06316779553890228\n",
            "strain 0.047930069267749786\n",
            "strain 0.06015224754810333\n",
            "strain 0.0500723198056221\n",
            "strain 0.04712183400988579\n",
            "strain 0.06018003821372986\n",
            "strain 0.0572470985352993\n",
            "strain 0.04920078068971634\n",
            "strain 0.04379000514745712\n",
            "strain 0.04919859394431114\n",
            "strain 0.0387120358645916\n",
            "strain 0.04630023613572121\n",
            "strain 0.04238038510084152\n",
            "strain 0.04770249128341675\n",
            "strain 0.04681387171149254\n",
            "strain 0.07053517550230026\n",
            "strain 0.0536351203918457\n",
            "strain 0.04568653926253319\n",
            "strain 0.055818796157836914\n",
            "strain 0.0419236421585083\n",
            "strain 0.041221193969249725\n",
            "strain 0.046064313501119614\n",
            "strain 0.05125720426440239\n",
            "strain 0.07060670852661133\n",
            "classify 2.2713623046875\n",
            "classify 2.21630859375\n",
            "classify 2.3280029296875\n",
            "classify 2.3134765625\n",
            "classify 2.360107421875\n",
            "classify 2.280517578125\n",
            "classify 2.2646484375\n",
            "classify 2.2569580078125\n",
            "classify 2.322509765625\n",
            "classify 2.2587890625\n",
            "classify 2.3304443359375\n",
            "0.09375\n",
            "0.09375\n",
            "0.109375\n",
            "0.15625\n",
            "0.15625\n",
            "0.171875\n",
            "0.1875\n",
            "0.09375\n",
            "0.171875\n",
            "0.09375\n",
            "0.09375\n",
            "71\n",
            "strain 0.049272339791059494\n",
            "strain 0.05076912045478821\n",
            "strain 0.05075622722506523\n",
            "strain 0.039703257381916046\n",
            "strain 0.078654944896698\n",
            "strain 0.05321284383535385\n",
            "strain 0.04726770520210266\n",
            "strain 0.039735231548547745\n",
            "strain 0.052841298282146454\n",
            "strain 0.04265225678682327\n",
            "strain 0.055416449904441833\n",
            "strain 0.05335795879364014\n",
            "strain 0.048752814531326294\n",
            "strain 0.04091862589120865\n",
            "strain 0.037975847721099854\n",
            "strain 0.04114134609699249\n",
            "strain 0.04210026562213898\n",
            "strain 0.04492322355508804\n",
            "strain 0.04964856058359146\n",
            "strain 0.041281621903181076\n",
            "strain 0.04815061390399933\n",
            "strain 0.04657891392707825\n",
            "strain 0.059150174260139465\n",
            "strain 0.0493069663643837\n",
            "strain 0.059283338487148285\n",
            "strain 0.0475710853934288\n",
            "strain 0.05206987261772156\n",
            "strain 0.04426612704992294\n",
            "strain 0.05091622471809387\n",
            "strain 0.04580368101596832\n",
            "strain 0.0439552366733551\n",
            "strain 0.04147353023290634\n",
            "strain 0.04228257015347481\n",
            "strain 0.059899184852838516\n",
            "strain 0.04581733047962189\n",
            "strain 0.05053996667265892\n",
            "strain 0.054616570472717285\n",
            "strain 0.042857058346271515\n",
            "strain 0.056337952613830566\n",
            "strain 0.046145837754011154\n",
            "strain 0.0577106773853302\n",
            "strain 0.04756378382444382\n",
            "strain 0.05100036785006523\n",
            "strain 0.04984181374311447\n",
            "strain 0.05443914234638214\n",
            "strain 0.040002841502428055\n",
            "strain 0.03642294928431511\n",
            "strain 0.04684847220778465\n",
            "strain 0.07371717691421509\n",
            "strain 0.044470131397247314\n",
            "strain 0.04937058687210083\n",
            "classify 2.27734375\n",
            "classify 2.201171875\n",
            "classify 2.3106689453125\n",
            "classify 2.274658203125\n",
            "classify 2.2628173828125\n",
            "classify 2.2935791015625\n",
            "classify 2.1851806640625\n",
            "classify 2.228271484375\n",
            "classify 2.3223876953125\n",
            "classify 2.279541015625\n",
            "classify 2.2498779296875\n",
            "0.109375\n",
            "0.15625\n",
            "0.1875\n",
            "0.09375\n",
            "0.140625\n",
            "0.1875\n",
            "0.1875\n",
            "0.078125\n",
            "0.078125\n",
            "0.109375\n",
            "0.171875\n",
            "72\n",
            "strain 0.044437043368816376\n",
            "strain 0.05077771097421646\n",
            "strain 0.0669718086719513\n",
            "strain 0.05799739062786102\n",
            "strain 0.05282968282699585\n",
            "strain 0.05497266352176666\n",
            "strain 0.07062339782714844\n",
            "strain 0.049468960613012314\n",
            "strain 0.06504593789577484\n",
            "strain 0.03491222485899925\n",
            "strain 0.05158424377441406\n",
            "strain 0.05224399268627167\n",
            "strain 0.04044761136174202\n",
            "strain 0.04895510897040367\n",
            "strain 0.04332524538040161\n",
            "strain 0.046787139028310776\n",
            "strain 0.043837498873472214\n",
            "strain 0.05665075406432152\n",
            "strain 0.04977628216147423\n",
            "strain 0.04831309616565704\n",
            "strain 0.05125631392002106\n",
            "strain 0.05106760933995247\n",
            "strain 0.04865460842847824\n",
            "strain 0.05163843557238579\n",
            "strain 0.047333765774965286\n",
            "strain 0.0435568243265152\n",
            "strain 0.032753992825746536\n",
            "strain 0.05271010473370552\n",
            "strain 0.04715442657470703\n",
            "strain 0.05513268709182739\n",
            "strain 0.05712353438138962\n",
            "strain 0.04448641464114189\n",
            "strain 0.05106113851070404\n",
            "strain 0.04986097291111946\n",
            "strain 0.04945685714483261\n",
            "strain 0.04119085893034935\n",
            "strain 0.055128227919340134\n",
            "strain 0.04325331747531891\n",
            "strain 0.05111493170261383\n",
            "strain 0.041963446885347366\n",
            "strain 0.04500787332653999\n",
            "strain 0.047661345452070236\n",
            "strain 0.0498976856470108\n",
            "strain 0.04404541477560997\n",
            "strain 0.0402405671775341\n",
            "strain 0.05478727072477341\n",
            "strain 0.05301203951239586\n",
            "strain 0.04978295788168907\n",
            "strain 0.047985274344682693\n",
            "strain 0.05057111009955406\n",
            "strain 0.04605817049741745\n",
            "classify 2.2862548828125\n",
            "classify 2.2713623046875\n",
            "classify 2.2852783203125\n",
            "classify 2.246337890625\n",
            "classify 2.2178955078125\n",
            "classify 2.2918701171875\n",
            "classify 2.246337890625\n",
            "classify 2.2264404296875\n",
            "classify 2.2650146484375\n",
            "classify 2.3050537109375\n",
            "classify 2.2501220703125\n",
            "0.15625\n",
            "0.125\n",
            "0.203125\n",
            "0.09375\n",
            "0.21875\n",
            "0.078125\n",
            "0.109375\n",
            "0.078125\n",
            "0.140625\n",
            "0.109375\n",
            "0.1875\n",
            "73\n",
            "strain 0.045029353350400925\n",
            "strain 0.040105246007442474\n",
            "strain 0.053353678435087204\n",
            "strain 0.06498127430677414\n",
            "strain 0.04717487841844559\n",
            "strain 0.052489347755908966\n",
            "strain 0.0509176068007946\n",
            "strain 0.04050001874566078\n",
            "strain 0.045026496052742004\n",
            "strain 0.057407159358263016\n",
            "strain 0.041933152824640274\n",
            "strain 0.05757894739508629\n",
            "strain 0.06721623241901398\n",
            "strain 0.04928247630596161\n",
            "strain 0.045660149306058884\n",
            "strain 0.047287486493587494\n",
            "strain 0.04281746223568916\n",
            "strain 0.049672774970531464\n",
            "strain 0.04380672052502632\n",
            "strain 0.0580102875828743\n",
            "strain 0.06604538857936859\n",
            "strain 0.047162096947431564\n",
            "strain 0.051151636987924576\n",
            "strain 0.05016258358955383\n",
            "strain 0.053645871579647064\n",
            "strain 0.04917926713824272\n",
            "strain 0.053261976689100266\n",
            "strain 0.06241890415549278\n",
            "strain 0.043726205825805664\n",
            "strain 0.05469071865081787\n",
            "strain 0.06306742131710052\n",
            "strain 0.04362368583679199\n",
            "strain 0.03504640981554985\n",
            "strain 0.038383424282073975\n",
            "strain 0.04622366279363632\n",
            "strain 0.05030655860900879\n",
            "strain 0.051309991627931595\n",
            "strain 0.04466487839818001\n",
            "strain 0.04564914107322693\n",
            "strain 0.07317668944597244\n",
            "strain 0.03786642849445343\n",
            "strain 0.04711737111210823\n",
            "strain 0.050911203026771545\n",
            "strain 0.0465947650372982\n",
            "strain 0.04925588145852089\n",
            "strain 0.05516071245074272\n",
            "strain 0.037594396620988846\n",
            "strain 0.04416441172361374\n",
            "strain 0.059340450912714005\n",
            "strain 0.04855747148394585\n",
            "strain 0.05363599956035614\n",
            "classify 2.2774658203125\n",
            "classify 2.2269287109375\n",
            "classify 2.2406005859375\n",
            "classify 2.278076171875\n",
            "classify 2.2724609375\n",
            "classify 2.2418212890625\n",
            "classify 2.3182373046875\n",
            "classify 2.24365234375\n",
            "classify 2.2913818359375\n",
            "classify 2.26806640625\n",
            "classify 2.2314453125\n",
            "0.109375\n",
            "0.125\n",
            "0.109375\n",
            "0.109375\n",
            "0.15625\n",
            "0.171875\n",
            "0.15625\n",
            "0.15625\n",
            "0.140625\n",
            "0.109375\n",
            "0.15625\n",
            "74\n",
            "strain 0.06076943129301071\n",
            "strain 0.060035061091184616\n",
            "strain 0.04065895825624466\n",
            "strain 0.05452379584312439\n",
            "strain 0.04912764951586723\n",
            "strain 0.049495942890644073\n",
            "strain 0.04811777547001839\n",
            "strain 0.04622538015246391\n",
            "strain 0.044750042259693146\n",
            "strain 0.04998744651675224\n",
            "strain 0.04965849220752716\n",
            "strain 0.050050828605890274\n",
            "strain 0.05754499137401581\n",
            "strain 0.04421874135732651\n",
            "strain 0.0633225291967392\n",
            "strain 0.050493210554122925\n",
            "strain 0.05894065648317337\n",
            "strain 0.0417308434844017\n",
            "strain 0.04431132599711418\n",
            "strain 0.048132628202438354\n",
            "strain 0.04630228877067566\n",
            "strain 0.0559225007891655\n",
            "strain 0.04528183862566948\n",
            "strain 0.0461554192006588\n",
            "strain 0.0656341165304184\n",
            "strain 0.06433465331792831\n",
            "strain 0.04979591816663742\n",
            "strain 0.054171353578567505\n",
            "strain 0.057771649211645126\n",
            "strain 0.04843902587890625\n",
            "strain 0.053295254707336426\n",
            "strain 0.049144163727760315\n",
            "strain 0.058549754321575165\n",
            "strain 0.05737387761473656\n",
            "strain 0.05541665107011795\n",
            "strain 0.04555150866508484\n",
            "strain 0.050319474190473557\n",
            "strain 0.06200292706489563\n",
            "strain 0.06993858516216278\n",
            "strain 0.056124117225408554\n",
            "strain 0.05642396956682205\n",
            "strain 0.046687643975019455\n",
            "strain 0.044510915875434875\n",
            "strain 0.06046866998076439\n",
            "strain 0.04450111836194992\n",
            "strain 0.05484543368220329\n",
            "strain 0.06665369123220444\n",
            "strain 0.05078160762786865\n",
            "strain 0.04518433287739754\n",
            "strain 0.07594671100378036\n",
            "strain 0.0650240108370781\n",
            "classify 2.2230224609375\n",
            "classify 2.251708984375\n",
            "classify 2.2603759765625\n",
            "classify 2.2960205078125\n",
            "classify 2.2734375\n",
            "classify 2.2613525390625\n",
            "classify 2.269775390625\n",
            "classify 2.2864990234375\n",
            "classify 2.264892578125\n",
            "classify 2.3392333984375\n",
            "classify 2.2421875\n",
            "0.0625\n",
            "0.109375\n",
            "0.1875\n",
            "0.078125\n",
            "0.203125\n",
            "0.109375\n",
            "0.203125\n",
            "0.03125\n",
            "0.078125\n",
            "0.09375\n",
            "0.171875\n",
            "75\n",
            "strain 0.04693175107240677\n",
            "strain 0.05399463698267937\n",
            "strain 0.054165761917829514\n",
            "strain 0.05014892667531967\n",
            "strain 0.05476713553071022\n",
            "strain 0.05043744295835495\n",
            "strain 0.056350916624069214\n",
            "strain 0.0523020438849926\n",
            "strain 0.05807819217443466\n",
            "strain 0.055062826722860336\n",
            "strain 0.062410514801740646\n",
            "strain 0.04593833535909653\n",
            "strain 0.046525657176971436\n",
            "strain 0.03957148268818855\n",
            "strain 0.03419564664363861\n",
            "strain 0.042869362980127335\n",
            "strain 0.0412151962518692\n",
            "strain 0.06097304821014404\n",
            "strain 0.04289868846535683\n",
            "strain 0.04678795114159584\n",
            "strain 0.04704207181930542\n",
            "strain 0.043019846081733704\n",
            "strain 0.05583067983388901\n",
            "strain 0.05851312354207039\n",
            "strain 0.05291303992271423\n",
            "strain 0.05510573834180832\n",
            "strain 0.04476109892129898\n",
            "strain 0.04272421449422836\n",
            "strain 0.044432297348976135\n",
            "strain 0.0403873585164547\n",
            "strain 0.045268408954143524\n",
            "strain 0.04045367240905762\n",
            "strain 0.04608989506959915\n",
            "strain 0.0432511530816555\n",
            "strain 0.057759206742048264\n",
            "strain 0.046844400465488434\n",
            "strain 0.04650260880589485\n",
            "strain 0.05475694686174393\n",
            "strain 0.07241356372833252\n",
            "strain 0.05655964836478233\n",
            "strain 0.051530081778764725\n",
            "strain 0.05859079211950302\n",
            "strain 0.04586246982216835\n",
            "strain 0.048592984676361084\n",
            "strain 0.041792694479227066\n",
            "strain 0.04042429476976395\n",
            "strain 0.056287627667188644\n",
            "strain 0.05022500082850456\n",
            "strain 0.0609864741563797\n",
            "strain 0.05546850711107254\n",
            "strain 0.04935362935066223\n",
            "classify 2.2935791015625\n",
            "classify 2.3240966796875\n",
            "classify 2.2774658203125\n",
            "classify 2.3287353515625\n",
            "classify 2.308349609375\n",
            "classify 2.21826171875\n",
            "classify 2.2337646484375\n",
            "classify 2.279296875\n",
            "classify 2.26708984375\n",
            "classify 2.2769775390625\n",
            "classify 2.3336181640625\n",
            "0.125\n",
            "0.21875\n",
            "0.109375\n",
            "0.125\n",
            "0.109375\n",
            "0.078125\n",
            "0.125\n",
            "0.125\n",
            "0.109375\n",
            "0.25\n",
            "0.125\n",
            "76\n",
            "strain 0.050228510051965714\n",
            "strain 0.04508934170007706\n",
            "strain 0.03865813463926315\n",
            "strain 0.054528482258319855\n",
            "strain 0.04643281176686287\n",
            "strain 0.052917055785655975\n",
            "strain 0.05233336240053177\n",
            "strain 0.044846758246421814\n",
            "strain 0.05047701299190521\n",
            "strain 0.050761397927999496\n",
            "strain 0.04544403403997421\n",
            "strain 0.051100991666316986\n",
            "strain 0.04508050158619881\n",
            "strain 0.07754848897457123\n",
            "strain 0.05219399183988571\n",
            "strain 0.048804886639118195\n",
            "strain 0.044462453573942184\n",
            "strain 0.056389451026916504\n",
            "strain 0.04194768890738487\n",
            "strain 0.05332082509994507\n",
            "strain 0.04449250549077988\n",
            "strain 0.05533076077699661\n",
            "strain 0.05760575458407402\n",
            "strain 0.04482812434434891\n",
            "strain 0.04197891056537628\n",
            "strain 0.04915943369269371\n",
            "strain 0.05435042455792427\n",
            "strain 0.06232749670743942\n",
            "strain 0.04763972759246826\n",
            "strain 0.04604453966021538\n",
            "strain 0.08383553475141525\n",
            "strain 0.04770338907837868\n",
            "strain 0.04714072868227959\n",
            "strain 0.05170557647943497\n",
            "strain 0.0476129874587059\n",
            "strain 0.05585765838623047\n",
            "strain 0.0694827139377594\n",
            "strain 0.047919921576976776\n",
            "strain 0.039691030979156494\n",
            "strain 0.04853450134396553\n",
            "strain 0.0592641606926918\n",
            "strain 0.038336239755153656\n",
            "strain 0.05758988857269287\n",
            "strain 0.06058583781123161\n",
            "strain 0.050705213099718094\n",
            "strain 0.05257496237754822\n",
            "strain 0.04906649515032768\n",
            "strain 0.04835439473390579\n",
            "strain 0.04088137671351433\n",
            "strain 0.05177503079175949\n",
            "strain 0.05527356266975403\n",
            "classify 2.247314453125\n",
            "classify 2.294189453125\n",
            "classify 2.290283203125\n",
            "classify 2.2935791015625\n",
            "classify 2.343017578125\n",
            "classify 2.256591796875\n",
            "classify 2.2784423828125\n",
            "classify 2.277587890625\n",
            "classify 2.2415771484375\n",
            "classify 2.278076171875\n",
            "classify 2.269775390625\n",
            "0.140625\n",
            "0.078125\n",
            "0.140625\n",
            "0.046875\n",
            "0.171875\n",
            "0.140625\n",
            "0.078125\n",
            "0.0625\n",
            "0.125\n",
            "0.125\n",
            "0.109375\n",
            "77\n",
            "strain 0.05515187233686447\n",
            "strain 0.04742639511823654\n",
            "strain 0.0461406372487545\n",
            "strain 0.04235997796058655\n",
            "strain 0.06002691388130188\n",
            "strain 0.04853454977273941\n",
            "strain 0.03994889557361603\n",
            "strain 0.04822409152984619\n",
            "strain 0.04720866680145264\n",
            "strain 0.04606248438358307\n",
            "strain 0.05159857124090195\n",
            "strain 0.050152216106653214\n",
            "strain 0.05075214058160782\n",
            "strain 0.06112963706254959\n",
            "strain 0.0567905567586422\n",
            "strain 0.04252108186483383\n",
            "strain 0.05167335271835327\n",
            "strain 0.05329955741763115\n",
            "strain 0.04837936908006668\n",
            "strain 0.04423199221491814\n",
            "strain 0.04934016987681389\n",
            "strain 0.04899917542934418\n",
            "strain 0.05255645513534546\n",
            "strain 0.0637732446193695\n",
            "strain 0.049064427614212036\n",
            "strain 0.04942616820335388\n",
            "strain 0.04812629148364067\n",
            "strain 0.05721092224121094\n",
            "strain 0.07273354381322861\n",
            "strain 0.058048002421855927\n",
            "strain 0.08503413200378418\n",
            "strain 0.04928051307797432\n",
            "strain 0.0637688934803009\n",
            "strain 0.06439006328582764\n",
            "strain 0.05502907186746597\n",
            "strain 0.06442777812480927\n",
            "strain 0.04715312272310257\n",
            "strain 0.05989031493663788\n",
            "strain 0.05554249882698059\n",
            "strain 0.043417997658252716\n",
            "strain 0.05218319594860077\n",
            "strain 0.06201866641640663\n",
            "strain 0.05653069540858269\n",
            "strain 0.044806450605392456\n",
            "strain 0.05482695624232292\n",
            "strain 0.06605802476406097\n",
            "strain 0.05289146304130554\n",
            "strain 0.03754551708698273\n",
            "strain 0.05126294121146202\n",
            "strain 0.05555523931980133\n",
            "strain 0.043369147926568985\n",
            "classify 2.2962646484375\n",
            "classify 2.2706298828125\n",
            "classify 2.2623291015625\n",
            "classify 2.2796630859375\n",
            "classify 2.2196044921875\n",
            "classify 2.262451171875\n",
            "classify 2.278076171875\n",
            "classify 2.25537109375\n",
            "classify 2.25341796875\n",
            "classify 2.2479248046875\n",
            "classify 2.1849365234375\n",
            "0.15625\n",
            "0.09375\n",
            "0.109375\n",
            "0.1875\n",
            "0.171875\n",
            "0.109375\n",
            "0.0625\n",
            "0.15625\n",
            "0.09375\n",
            "0.09375\n",
            "0.125\n",
            "78\n",
            "strain 0.04280494898557663\n",
            "strain 0.042893778532743454\n",
            "strain 0.04946450889110565\n",
            "strain 0.04418375715613365\n",
            "strain 0.087081179022789\n",
            "strain 0.04699338227510452\n",
            "strain 0.047258030623197556\n",
            "strain 0.04942278191447258\n",
            "strain 0.0498349666595459\n",
            "strain 0.0674850195646286\n",
            "strain 0.04954596236348152\n",
            "strain 0.06152419373393059\n",
            "strain 0.04157455638051033\n",
            "strain 0.0633641704916954\n",
            "strain 0.051732175052165985\n",
            "strain 0.03937460109591484\n",
            "strain 0.05620589852333069\n",
            "strain 0.057779550552368164\n",
            "strain 0.04904923588037491\n",
            "strain 0.04834485426545143\n",
            "strain 0.05738670751452446\n",
            "strain 0.04832712933421135\n",
            "strain 0.04389622062444687\n",
            "strain 0.05182328075170517\n",
            "strain 0.05644145607948303\n",
            "strain 0.053588688373565674\n",
            "strain 0.04180838167667389\n",
            "strain 0.056251995265483856\n",
            "strain 0.05995526909828186\n",
            "strain 0.05406972020864487\n",
            "strain 0.05183864384889603\n",
            "strain 0.050803832709789276\n",
            "strain 0.04988879710435867\n",
            "strain 0.05071750283241272\n",
            "strain 0.05352894589304924\n",
            "strain 0.05064043775200844\n",
            "strain 0.04828199744224548\n",
            "strain 0.0417146235704422\n",
            "strain 0.05615507811307907\n",
            "strain 0.05212454870343208\n",
            "strain 0.06677427142858505\n",
            "strain 0.05273909494280815\n",
            "strain 0.054890621453523636\n",
            "strain 0.05084729194641113\n",
            "strain 0.06558718532323837\n",
            "strain 0.05278080329298973\n",
            "strain 0.06405186653137207\n",
            "strain 0.05974727123975754\n",
            "strain 0.056804198771715164\n",
            "strain 0.049974486231803894\n",
            "strain 0.04714639112353325\n",
            "classify 2.3382568359375\n",
            "classify 2.2376708984375\n",
            "classify 2.2352294921875\n",
            "classify 2.2767333984375\n",
            "classify 2.296142578125\n",
            "classify 2.2716064453125\n",
            "classify 2.2772216796875\n",
            "classify 2.2127685546875\n",
            "classify 2.2733154296875\n",
            "classify 2.2867431640625\n",
            "classify 2.27978515625\n",
            "0.171875\n",
            "0.1875\n",
            "0.140625\n",
            "0.203125\n",
            "0.171875\n",
            "0.25\n",
            "0.125\n",
            "0.15625\n",
            "0.171875\n",
            "0.109375\n",
            "0.171875\n",
            "79\n",
            "strain 0.06004653871059418\n",
            "strain 0.04900798201560974\n",
            "strain 0.049034614115953445\n",
            "strain 0.05007075145840645\n",
            "strain 0.05492796748876572\n",
            "strain 0.045872412621974945\n",
            "strain 0.04829373583197594\n",
            "strain 0.06471690535545349\n",
            "strain 0.054320573806762695\n",
            "strain 0.052472326904535294\n",
            "strain 0.05338592082262039\n",
            "strain 0.045653101056814194\n",
            "strain 0.04922899603843689\n",
            "strain 0.043407876044511795\n",
            "strain 0.04363487288355827\n",
            "strain 0.0429929755628109\n",
            "strain 0.0464257076382637\n",
            "strain 0.06503832340240479\n",
            "strain 0.0513201542198658\n",
            "strain 0.0394737683236599\n",
            "strain 0.07039858400821686\n",
            "strain 0.0608707033097744\n",
            "strain 0.05202806368470192\n",
            "strain 0.04943664371967316\n",
            "strain 0.05708848685026169\n",
            "strain 0.06420285999774933\n",
            "strain 0.04560350626707077\n",
            "strain 0.04707027226686478\n",
            "strain 0.04981489107012749\n",
            "strain 0.041799210011959076\n",
            "strain 0.04246508330106735\n",
            "strain 0.05714333802461624\n",
            "strain 0.04758840799331665\n",
            "strain 0.04225725680589676\n",
            "strain 0.057557981461286545\n",
            "strain 0.04532498121261597\n",
            "strain 0.05345779284834862\n",
            "strain 0.04818939417600632\n",
            "strain 0.0520167276263237\n",
            "strain 0.04982703924179077\n",
            "strain 0.042987532913684845\n",
            "strain 0.07449217140674591\n",
            "strain 0.04488153010606766\n",
            "strain 0.0568985641002655\n",
            "strain 0.06307520717382431\n",
            "strain 0.05126155912876129\n",
            "strain 0.05058032274246216\n",
            "strain 0.06655817478895187\n",
            "strain 0.05243247002363205\n",
            "strain 0.05970189720392227\n",
            "strain 0.05336187034845352\n",
            "classify 2.244140625\n",
            "classify 2.2667236328125\n",
            "classify 2.29833984375\n",
            "classify 2.2457275390625\n",
            "classify 2.22705078125\n",
            "classify 2.253173828125\n",
            "classify 2.2249755859375\n",
            "classify 2.2333984375\n",
            "classify 2.2275390625\n",
            "classify 2.281494140625\n",
            "classify 2.3070068359375\n",
            "0.109375\n",
            "0.125\n",
            "0.078125\n",
            "0.078125\n",
            "0.125\n",
            "0.109375\n",
            "0.171875\n",
            "0.171875\n",
            "0.09375\n",
            "0.046875\n",
            "0.234375\n",
            "80\n",
            "strain 0.04798054322600365\n",
            "strain 0.04954808950424194\n",
            "strain 0.05389850586652756\n",
            "strain 0.0535445399582386\n",
            "strain 0.04381386935710907\n",
            "strain 0.052009474486112595\n",
            "strain 0.04485287517309189\n",
            "strain 0.06040544807910919\n",
            "strain 0.050209179520606995\n",
            "strain 0.06419724225997925\n",
            "strain 0.06866587698459625\n",
            "strain 0.06285655498504639\n",
            "strain 0.05253499373793602\n",
            "strain 0.061886079609394073\n",
            "strain 0.0571223646402359\n",
            "strain 0.05104722082614899\n",
            "strain 0.04375290498137474\n",
            "strain 0.058311428874731064\n",
            "strain 0.05512620508670807\n",
            "strain 0.05334587022662163\n",
            "strain 0.08290636539459229\n",
            "strain 0.05795446038246155\n",
            "strain 0.05880212038755417\n",
            "strain 0.06142381951212883\n",
            "strain 0.06592880934476852\n",
            "strain 0.05434131622314453\n",
            "strain 0.04120736941695213\n",
            "strain 0.04830004647374153\n",
            "strain 0.046717651188373566\n",
            "strain 0.05071680247783661\n",
            "strain 0.053655531257390976\n",
            "strain 0.05590352043509483\n",
            "strain 0.06517624855041504\n",
            "strain 0.05498645454645157\n",
            "strain 0.04940062016248703\n",
            "strain 0.052606724202632904\n",
            "strain 0.04097906872630119\n",
            "strain 0.05733555555343628\n",
            "strain 0.05611925944685936\n",
            "strain 0.05416058748960495\n",
            "strain 0.053303830325603485\n",
            "strain 0.04596655070781708\n",
            "strain 0.0697018951177597\n",
            "strain 0.054408006370067596\n",
            "strain 0.05022628605365753\n",
            "strain 0.06275351345539093\n",
            "strain 0.06717188656330109\n",
            "strain 0.057986192405223846\n",
            "strain 0.051943887025117874\n",
            "strain 0.04699797183275223\n",
            "strain 0.04717479273676872\n",
            "classify 2.3065185546875\n",
            "classify 2.2667236328125\n",
            "classify 2.30517578125\n",
            "classify 2.3006591796875\n",
            "classify 2.2896728515625\n",
            "classify 2.29638671875\n",
            "classify 2.3026123046875\n",
            "classify 2.2578125\n",
            "classify 2.2216796875\n",
            "classify 2.284423828125\n",
            "classify 2.3155517578125\n",
            "0.09375\n",
            "0.109375\n",
            "0.1875\n",
            "0.109375\n",
            "0.21875\n",
            "0.15625\n",
            "0.140625\n",
            "0.109375\n",
            "0.1875\n",
            "0.109375\n",
            "0.203125\n",
            "81\n",
            "strain 0.0537588931620121\n",
            "strain 0.05488332733511925\n",
            "strain 0.05053357407450676\n",
            "strain 0.06920943409204483\n",
            "strain 0.04707366228103638\n",
            "strain 0.07186545431613922\n",
            "strain 0.05121776461601257\n",
            "strain 0.05217042192816734\n",
            "strain 0.059160344302654266\n",
            "strain 0.05749664828181267\n",
            "strain 0.05041421204805374\n",
            "strain 0.05852212756872177\n",
            "strain 0.07051996141672134\n",
            "strain 0.061242908239364624\n",
            "strain 0.05673094838857651\n",
            "strain 0.05652257427573204\n",
            "strain 0.04625527933239937\n",
            "strain 0.054307978600263596\n",
            "strain 0.04283352568745613\n",
            "strain 0.05798087269067764\n",
            "strain 0.0733867660164833\n",
            "strain 0.05174049362540245\n",
            "strain 0.05221226438879967\n",
            "strain 0.041189804673194885\n",
            "strain 0.053901415318250656\n",
            "strain 0.05769752338528633\n",
            "strain 0.047138817608356476\n",
            "strain 0.054778747260570526\n",
            "strain 0.06677499413490295\n",
            "strain 0.04967138543725014\n",
            "strain 0.058967750519514084\n",
            "strain 0.052347734570503235\n",
            "strain 0.06507397443056107\n",
            "strain 0.04432274028658867\n",
            "strain 0.05059205740690231\n",
            "strain 0.05167695879936218\n",
            "strain 0.05137656256556511\n",
            "strain 0.04912281781435013\n",
            "strain 0.045456573367118835\n",
            "strain 0.04786602035164833\n",
            "strain 0.044526055455207825\n",
            "strain 0.04383079335093498\n",
            "strain 0.05907171219587326\n",
            "strain 0.054444681853055954\n",
            "strain 0.07643848657608032\n",
            "strain 0.04521409049630165\n",
            "strain 0.0696408823132515\n",
            "strain 0.04431074485182762\n",
            "strain 0.05706516653299332\n",
            "strain 0.06835226714611053\n",
            "strain 0.04895774647593498\n",
            "classify 2.2449951171875\n",
            "classify 2.2474365234375\n",
            "classify 2.2650146484375\n",
            "classify 2.252685546875\n",
            "classify 2.2958984375\n",
            "classify 2.2412109375\n",
            "classify 2.270751953125\n",
            "classify 2.30810546875\n",
            "classify 2.248046875\n",
            "classify 2.2615966796875\n",
            "classify 2.27978515625\n",
            "0.125\n",
            "0.0625\n",
            "0.15625\n",
            "0.140625\n",
            "0.125\n",
            "0.171875\n",
            "0.171875\n",
            "0.203125\n",
            "0.171875\n",
            "0.09375\n",
            "0.15625\n",
            "82\n",
            "strain 0.05326408892869949\n",
            "strain 0.050293661653995514\n",
            "strain 0.06019022315740585\n",
            "strain 0.056908637285232544\n",
            "strain 0.06120084226131439\n",
            "strain 0.07143473625183105\n",
            "strain 0.06288819015026093\n",
            "strain 0.05533720552921295\n",
            "strain 0.04777292534708977\n",
            "strain 0.04946637153625488\n",
            "strain 0.055977120995521545\n",
            "strain 0.06678681075572968\n",
            "strain 0.05657834932208061\n",
            "strain 0.05388616770505905\n",
            "strain 0.06304987519979477\n",
            "strain 0.056908249855041504\n",
            "strain 0.05728617310523987\n",
            "strain 0.059369318187236786\n",
            "strain 0.05109449848532677\n",
            "strain 0.04859723150730133\n",
            "strain 0.07901766896247864\n",
            "strain 0.05251917615532875\n",
            "strain 0.0583568811416626\n",
            "strain 0.04465067386627197\n",
            "strain 0.04206863418221474\n",
            "strain 0.05937053635716438\n",
            "strain 0.050488367676734924\n",
            "strain 0.04511969909071922\n",
            "strain 0.04519239440560341\n",
            "strain 0.07840753346681595\n",
            "strain 0.06551486253738403\n",
            "strain 0.05044000223278999\n",
            "strain 0.04806279391050339\n",
            "strain 0.05687961354851723\n",
            "strain 0.05220354348421097\n",
            "strain 0.06171140819787979\n",
            "strain 0.06254300475120544\n",
            "strain 0.06959179043769836\n",
            "strain 0.05159149318933487\n",
            "strain 0.057110514491796494\n",
            "strain 0.05764247104525566\n",
            "strain 0.06812592595815659\n",
            "strain 0.06226076930761337\n",
            "strain 0.0645115077495575\n",
            "strain 0.0654602199792862\n",
            "strain 0.04890214651823044\n",
            "strain 0.05893254280090332\n",
            "strain 0.06746269762516022\n",
            "strain 0.06184445694088936\n",
            "strain 0.05392978712916374\n",
            "strain 0.046275947242975235\n",
            "classify 2.224365234375\n",
            "classify 2.2926025390625\n",
            "classify 2.2496337890625\n",
            "classify 2.2421875\n",
            "classify 2.2381591796875\n",
            "classify 2.228271484375\n",
            "classify 2.2535400390625\n",
            "classify 2.271240234375\n",
            "classify 2.2864990234375\n",
            "classify 2.278564453125\n",
            "classify 2.2603759765625\n",
            "0.1875\n",
            "0.171875\n",
            "0.125\n",
            "0.15625\n",
            "0.1875\n",
            "0.1875\n",
            "0.171875\n",
            "0.15625\n",
            "0.140625\n",
            "0.203125\n",
            "0.15625\n",
            "83\n",
            "strain 0.055728863924741745\n",
            "strain 0.05727989226579666\n",
            "strain 0.05594774708151817\n",
            "strain 0.05395575240254402\n",
            "strain 0.051542870700359344\n",
            "strain 0.05904999375343323\n",
            "strain 0.0537581630051136\n",
            "strain 0.05235756188631058\n",
            "strain 0.06018945202231407\n",
            "strain 0.04535706341266632\n",
            "strain 0.05108829215168953\n",
            "strain 0.059191688895225525\n",
            "strain 0.046636175364255905\n",
            "strain 0.05388404801487923\n",
            "strain 0.049439214169979095\n",
            "strain 0.036747317761182785\n",
            "strain 0.06271832436323166\n",
            "strain 0.058593183755874634\n",
            "strain 0.05371040105819702\n",
            "strain 0.0500943548977375\n",
            "strain 0.05610163137316704\n",
            "strain 0.06677383184432983\n",
            "strain 0.0646902546286583\n",
            "strain 0.04535633325576782\n",
            "strain 0.06207584589719772\n",
            "strain 0.042745817452669144\n",
            "strain 0.05048735812306404\n",
            "strain 0.0605061836540699\n",
            "strain 0.05384047329425812\n",
            "strain 0.0545802116394043\n",
            "strain 0.06617167592048645\n",
            "strain 0.050638068467378616\n",
            "strain 0.05868754908442497\n",
            "strain 0.04937087744474411\n",
            "strain 0.056123487651348114\n",
            "strain 0.04975445941090584\n",
            "strain 0.05541034787893295\n",
            "strain 0.0633573904633522\n",
            "strain 0.05587531253695488\n",
            "strain 0.05429798364639282\n",
            "strain 0.057013679295778275\n",
            "strain 0.0458095446228981\n",
            "strain 0.06245613843202591\n",
            "strain 0.04834271967411041\n",
            "strain 0.057054366916418076\n",
            "strain 0.04965594410896301\n",
            "strain 0.050151526927948\n",
            "strain 0.05052589997649193\n",
            "strain 0.055207543075084686\n",
            "strain 0.04711947962641716\n",
            "strain 0.05700189992785454\n",
            "classify 2.2972412109375\n",
            "classify 2.2750244140625\n",
            "classify 2.273681640625\n",
            "classify 2.25390625\n",
            "classify 2.302978515625\n",
            "classify 2.26123046875\n",
            "classify 2.26953125\n",
            "classify 2.2493896484375\n",
            "classify 2.30029296875\n",
            "classify 2.2890625\n",
            "classify 2.2525634765625\n",
            "0.078125\n",
            "0.15625\n",
            "0.171875\n",
            "0.109375\n",
            "0.140625\n",
            "0.234375\n",
            "0.171875\n",
            "0.203125\n",
            "0.125\n",
            "0.140625\n",
            "0.21875\n",
            "84\n",
            "strain 0.05022893846035004\n",
            "strain 0.045048754662275314\n",
            "strain 0.07346581667661667\n",
            "strain 0.06093629449605942\n",
            "strain 0.0500141866505146\n",
            "strain 0.05279509350657463\n",
            "strain 0.07664266228675842\n",
            "strain 0.055219873785972595\n",
            "strain 0.04881822690367699\n",
            "strain 0.047082144767045975\n",
            "strain 0.053824275732040405\n",
            "strain 0.05994953215122223\n",
            "strain 0.04893739894032478\n",
            "strain 0.06107917055487633\n",
            "strain 0.05060245096683502\n",
            "strain 0.06015763804316521\n",
            "strain 0.04474189504981041\n",
            "strain 0.05631985515356064\n",
            "strain 0.059960417449474335\n",
            "strain 0.057481203228235245\n",
            "strain 0.064356230199337\n",
            "strain 0.062082767486572266\n",
            "strain 0.0747680813074112\n",
            "strain 0.05918671935796738\n",
            "strain 0.06990090012550354\n",
            "strain 0.06770719587802887\n",
            "strain 0.05899859219789505\n",
            "strain 0.05579878389835358\n",
            "strain 0.04742702469229698\n",
            "strain 0.050557367503643036\n",
            "strain 0.04782279580831528\n",
            "strain 0.04973144829273224\n",
            "strain 0.05858715623617172\n",
            "strain 0.07418062537908554\n",
            "strain 0.04515225812792778\n",
            "strain 0.06255224347114563\n",
            "strain 0.05531434342265129\n",
            "strain 0.0578153133392334\n",
            "strain 0.050409942865371704\n",
            "strain 0.04952651262283325\n",
            "strain 0.0601486898958683\n",
            "strain 0.05244699493050575\n",
            "strain 0.056444089859724045\n",
            "strain 0.0506034754216671\n",
            "strain 0.06893925368785858\n",
            "strain 0.06202585622668266\n",
            "strain 0.06671509146690369\n",
            "strain 0.04299157112836838\n",
            "strain 0.06138993799686432\n",
            "strain 0.05406560003757477\n",
            "strain 0.05116550251841545\n",
            "classify 2.2802734375\n",
            "classify 2.2578125\n",
            "classify 2.2470703125\n",
            "classify 2.3143310546875\n",
            "classify 2.2626953125\n",
            "classify 2.323974609375\n",
            "classify 2.2476806640625\n",
            "classify 2.24169921875\n",
            "classify 2.2139892578125\n",
            "classify 2.3275146484375\n",
            "classify 2.25634765625\n",
            "0.125\n",
            "0.15625\n",
            "0.109375\n",
            "0.171875\n",
            "0.125\n",
            "0.25\n",
            "0.125\n",
            "0.0625\n",
            "0.09375\n",
            "0.140625\n",
            "0.109375\n",
            "85\n",
            "strain 0.059775419533252716\n",
            "strain 0.05898594111204147\n",
            "strain 0.05789455771446228\n",
            "strain 0.06274792551994324\n",
            "strain 0.055616896599531174\n",
            "strain 0.05534183606505394\n",
            "strain 0.04831286147236824\n",
            "strain 0.05510575696825981\n",
            "strain 0.0533742792904377\n",
            "strain 0.04867493361234665\n",
            "strain 0.057825300842523575\n",
            "strain 0.05605557933449745\n",
            "strain 0.052713945508003235\n",
            "strain 0.06629060953855515\n",
            "strain 0.057432785630226135\n",
            "strain 0.058451950550079346\n",
            "strain 0.05877220630645752\n",
            "strain 0.06582237780094147\n",
            "strain 0.06131924316287041\n",
            "strain 0.06419157981872559\n",
            "strain 0.061786144971847534\n",
            "strain 0.052912116050720215\n",
            "strain 0.05277667194604874\n",
            "strain 0.05995466932654381\n",
            "strain 0.05424407124519348\n",
            "strain 0.04990698769688606\n",
            "strain 0.06054596230387688\n",
            "strain 0.06163731589913368\n",
            "strain 0.04084443300962448\n",
            "strain 0.0668274462223053\n",
            "strain 0.047197818756103516\n",
            "strain 0.054756369441747665\n",
            "strain 0.048939626663923264\n",
            "strain 0.053244978189468384\n",
            "strain 0.051490455865859985\n",
            "strain 0.057340219616889954\n",
            "strain 0.05259272828698158\n",
            "strain 0.05322952941060066\n",
            "strain 0.049302972853183746\n",
            "strain 0.062049172818660736\n",
            "strain 0.05768793076276779\n",
            "strain 0.06105193495750427\n",
            "strain 0.05978674814105034\n",
            "strain 0.05535171553492546\n",
            "strain 0.05839207395911217\n",
            "strain 0.05275917798280716\n",
            "strain 0.05170389637351036\n",
            "strain 0.06501549482345581\n",
            "strain 0.05902458727359772\n",
            "strain 0.05493268370628357\n",
            "strain 0.06644657254219055\n",
            "classify 2.2572021484375\n",
            "classify 2.291259765625\n",
            "classify 2.36474609375\n",
            "classify 2.285888671875\n",
            "classify 2.280517578125\n",
            "classify 2.22705078125\n",
            "classify 2.242431640625\n",
            "classify 2.2508544921875\n",
            "classify 2.212646484375\n",
            "classify 2.221923828125\n",
            "classify 2.192138671875\n",
            "0.09375\n",
            "0.140625\n",
            "0.203125\n",
            "0.125\n",
            "0.109375\n",
            "0.125\n",
            "0.140625\n",
            "0.265625\n",
            "0.171875\n",
            "0.15625\n",
            "0.109375\n",
            "86\n",
            "strain 0.0746806263923645\n",
            "strain 0.05052946135401726\n",
            "strain 0.0625002533197403\n",
            "strain 0.05247128754854202\n",
            "strain 0.05899753049015999\n",
            "strain 0.05791783332824707\n",
            "strain 0.06846543401479721\n",
            "strain 0.05548125505447388\n",
            "strain 0.05625367909669876\n",
            "strain 0.05149908363819122\n",
            "strain 0.05548828840255737\n",
            "strain 0.06367040425539017\n",
            "strain 0.06737718731164932\n",
            "strain 0.07440000772476196\n",
            "strain 0.05173945426940918\n",
            "strain 0.0538283996284008\n",
            "strain 0.051427990198135376\n",
            "strain 0.05107715353369713\n",
            "strain 0.049608662724494934\n",
            "strain 0.07071639597415924\n",
            "strain 0.06134985014796257\n",
            "strain 0.05484693497419357\n",
            "strain 0.051905494183301926\n",
            "strain 0.05262384191155434\n",
            "strain 0.059029918164014816\n",
            "strain 0.05048619210720062\n",
            "strain 0.06785590946674347\n",
            "strain 0.07851317524909973\n",
            "strain 0.04962119460105896\n",
            "strain 0.06905063986778259\n",
            "strain 0.06451021879911423\n",
            "strain 0.05384653061628342\n",
            "strain 0.053581222891807556\n",
            "strain 0.05872906744480133\n",
            "strain 0.05893260985612869\n",
            "strain 0.057514987885951996\n",
            "strain 0.04979013651609421\n",
            "strain 0.04242190718650818\n",
            "strain 0.05964042991399765\n",
            "strain 0.049601178616285324\n",
            "strain 0.056837402284145355\n",
            "strain 0.0513513907790184\n",
            "strain 0.05658368766307831\n",
            "strain 0.07973836362361908\n",
            "strain 0.06477198004722595\n",
            "strain 0.05387667939066887\n",
            "strain 0.05461689829826355\n",
            "strain 0.05686897784471512\n",
            "strain 0.05004946142435074\n",
            "strain 0.044115837663412094\n",
            "strain 0.0557095967233181\n",
            "classify 2.2530517578125\n",
            "classify 2.27685546875\n",
            "classify 2.259521484375\n",
            "classify 2.2596435546875\n",
            "classify 2.3104248046875\n",
            "classify 2.2286376953125\n",
            "classify 2.238037109375\n",
            "classify 2.306884765625\n",
            "classify 2.2921142578125\n",
            "classify 2.2574462890625\n",
            "classify 2.23828125\n",
            "0.203125\n",
            "0.234375\n",
            "0.125\n",
            "0.171875\n",
            "0.21875\n",
            "0.171875\n",
            "0.171875\n",
            "0.203125\n",
            "0.171875\n",
            "0.21875\n",
            "0.125\n",
            "87\n",
            "strain 0.05663221329450607\n",
            "strain 0.055307697504758835\n",
            "strain 0.06183333322405815\n",
            "strain 0.05397563800215721\n",
            "strain 0.04835393652319908\n",
            "strain 0.0634998008608818\n",
            "strain 0.0647992491722107\n",
            "strain 0.05804779753088951\n",
            "strain 0.054850030690431595\n",
            "strain 0.05784080550074577\n",
            "strain 0.05021005868911743\n",
            "strain 0.05089699104428291\n",
            "strain 0.04904231056571007\n",
            "strain 0.04503212496638298\n",
            "strain 0.049573641270399094\n",
            "strain 0.0458122082054615\n",
            "strain 0.05407477915287018\n",
            "strain 0.06156262755393982\n",
            "strain 0.05905022844672203\n",
            "strain 0.056490976363420486\n",
            "strain 0.07259476184844971\n",
            "strain 0.06057068333029747\n",
            "strain 0.0608254075050354\n",
            "strain 0.06885366886854172\n",
            "strain 0.06383039802312851\n",
            "strain 0.06864210963249207\n",
            "strain 0.060203686356544495\n",
            "strain 0.09011735767126083\n",
            "strain 0.06478946655988693\n",
            "strain 0.06993158161640167\n",
            "strain 0.058954086154699326\n",
            "strain 0.06233534216880798\n",
            "strain 0.07245670258998871\n",
            "strain 0.053087394684553146\n",
            "strain 0.047865599393844604\n",
            "strain 0.05893480032682419\n",
            "strain 0.05786140635609627\n",
            "strain 0.05467640608549118\n",
            "strain 0.0701887235045433\n",
            "strain 0.060415882617235184\n",
            "strain 0.05468542501330376\n",
            "strain 0.0547330379486084\n",
            "strain 0.06151460483670235\n",
            "strain 0.06268252432346344\n",
            "strain 0.054753344506025314\n",
            "strain 0.05680741369724274\n",
            "strain 0.05368836969137192\n",
            "strain 0.058630846440792084\n",
            "strain 0.056596748530864716\n",
            "strain 0.057993292808532715\n",
            "strain 0.06790517270565033\n",
            "classify 2.3076171875\n",
            "classify 2.4058837890625\n",
            "classify 2.2470703125\n",
            "classify 2.27783203125\n",
            "classify 2.2855224609375\n",
            "classify 2.309814453125\n",
            "classify 2.257568359375\n",
            "classify 2.2498779296875\n",
            "classify 2.291259765625\n",
            "classify 2.32177734375\n",
            "classify 2.258056640625\n",
            "0.09375\n",
            "0.109375\n",
            "0.125\n",
            "0.109375\n",
            "0.21875\n",
            "0.171875\n",
            "0.0625\n",
            "0.140625\n",
            "0.0625\n",
            "0.140625\n",
            "0.09375\n",
            "88\n",
            "strain 0.05595236271619797\n",
            "strain 0.06462185829877853\n",
            "strain 0.062491171061992645\n",
            "strain 0.05106797441840172\n",
            "strain 0.06597237288951874\n",
            "strain 0.049227237701416016\n",
            "strain 0.05186455696821213\n",
            "strain 0.060801345854997635\n",
            "strain 0.058644022792577744\n",
            "strain 0.05429082736372948\n",
            "strain 0.054112207144498825\n",
            "strain 0.06540587544441223\n",
            "strain 0.05787800997495651\n",
            "strain 0.05982491001486778\n",
            "strain 0.05270098149776459\n",
            "strain 0.06767183542251587\n",
            "strain 0.0776161327958107\n",
            "strain 0.05751411244273186\n",
            "strain 0.04778110235929489\n",
            "strain 0.0613018199801445\n",
            "strain 0.04380393773317337\n",
            "strain 0.07686760276556015\n",
            "strain 0.05925259366631508\n",
            "strain 0.05645841360092163\n",
            "strain 0.0668964833021164\n",
            "strain 0.06681384891271591\n",
            "strain 0.05328645557165146\n",
            "strain 0.05924881994724274\n",
            "strain 0.053088750690221786\n",
            "strain 0.06447155773639679\n",
            "strain 0.060171760618686676\n",
            "strain 0.05471986532211304\n",
            "strain 0.05071244388818741\n",
            "strain 0.05252756178379059\n",
            "strain 0.0521591491997242\n",
            "strain 0.04581649973988533\n",
            "strain 0.054524146020412445\n",
            "strain 0.06595274806022644\n",
            "strain 0.06453786045312881\n",
            "strain 0.06404872983694077\n",
            "strain 0.05957881733775139\n",
            "strain 0.06034478172659874\n",
            "strain 0.050841983407735825\n",
            "strain 0.07549233734607697\n",
            "strain 0.08161047101020813\n",
            "strain 0.05988898500800133\n",
            "strain 0.04749475046992302\n",
            "strain 0.06679454445838928\n",
            "strain 0.049805689603090286\n",
            "strain 0.04522709175944328\n",
            "strain 0.08968421071767807\n",
            "classify 2.3206787109375\n",
            "classify 2.2677001953125\n",
            "classify 2.2501220703125\n",
            "classify 2.255126953125\n",
            "classify 2.2603759765625\n",
            "classify 2.2723388671875\n",
            "classify 2.2501220703125\n",
            "classify 2.2376708984375\n",
            "classify 2.307861328125\n",
            "classify 2.2015380859375\n",
            "classify 2.3048095703125\n",
            "0.09375\n",
            "0.109375\n",
            "0.125\n",
            "0.109375\n",
            "0.140625\n",
            "0.203125\n",
            "0.125\n",
            "0.203125\n",
            "0.1875\n",
            "0.078125\n",
            "0.125\n",
            "89\n",
            "strain 0.057078175246715546\n",
            "strain 0.05630570277571678\n",
            "strain 0.061925869435071945\n",
            "strain 0.06283599883317947\n",
            "strain 0.06125931069254875\n",
            "strain 0.0601169653236866\n",
            "strain 0.04899923875927925\n",
            "strain 0.049356549978256226\n",
            "strain 0.07238723337650299\n",
            "strain 0.049806538969278336\n",
            "strain 0.042601704597473145\n",
            "strain 0.05222775414586067\n",
            "strain 0.05022427439689636\n",
            "strain 0.052181653678417206\n",
            "strain 0.07501360774040222\n",
            "strain 0.06096784025430679\n",
            "strain 0.054470036178827286\n",
            "strain 0.05053379386663437\n",
            "strain 0.049684129655361176\n",
            "strain 0.05727226287126541\n",
            "strain 0.04997006058692932\n",
            "strain 0.0598263144493103\n",
            "strain 0.05345954746007919\n",
            "strain 0.05227129906415939\n",
            "strain 0.05477715656161308\n",
            "strain 0.05299504101276398\n",
            "strain 0.04690878838300705\n",
            "strain 0.05459640547633171\n",
            "strain 0.07231894135475159\n",
            "strain 0.060332708060741425\n",
            "strain 0.06634583324193954\n",
            "strain 0.061191823333501816\n",
            "strain 0.07320535182952881\n",
            "strain 0.04259713366627693\n",
            "strain 0.062190309166908264\n",
            "strain 0.0643472820520401\n",
            "strain 0.06147526949644089\n",
            "strain 0.050179291516542435\n",
            "strain 0.06400620192289352\n",
            "strain 0.05104322358965874\n",
            "strain 0.0547381155192852\n",
            "strain 0.05574416369199753\n",
            "strain 0.08353915810585022\n",
            "strain 0.05415918678045273\n",
            "strain 0.06109486147761345\n",
            "strain 0.06228543817996979\n",
            "strain 0.054625868797302246\n",
            "strain 0.09928475320339203\n",
            "strain 0.06259474158287048\n",
            "strain 0.053604137152433395\n",
            "strain 0.06394770741462708\n",
            "classify 2.31640625\n",
            "classify 2.2752685546875\n",
            "classify 2.2501220703125\n",
            "classify 2.2672119140625\n",
            "classify 2.2733154296875\n",
            "classify 2.19775390625\n",
            "classify 2.2320556640625\n",
            "classify 2.2266845703125\n",
            "classify 2.2325439453125\n",
            "classify 2.3052978515625\n",
            "classify 2.2813720703125\n",
            "0.140625\n",
            "0.09375\n",
            "0.171875\n",
            "0.109375\n",
            "0.109375\n",
            "0.21875\n",
            "0.140625\n",
            "0.15625\n",
            "0.15625\n",
            "0.171875\n",
            "0.140625\n",
            "90\n",
            "strain 0.0607098788022995\n",
            "strain 0.06122521683573723\n",
            "strain 0.05997506156563759\n",
            "strain 0.06832841038703918\n",
            "strain 0.04953363165259361\n",
            "strain 0.057129595428705215\n",
            "strain 0.05002972111105919\n",
            "strain 0.06433378159999847\n",
            "strain 0.0602496862411499\n",
            "strain 0.06609342992305756\n",
            "strain 0.06591574102640152\n",
            "strain 0.05623387172818184\n",
            "strain 0.056244149804115295\n",
            "strain 0.04970043525099754\n",
            "strain 0.04983179271221161\n",
            "strain 0.05531217157840729\n",
            "strain 0.056632936000823975\n",
            "strain 0.055945105850696564\n",
            "strain 0.06071817874908447\n",
            "strain 0.06681635230779648\n",
            "strain 0.058092158287763596\n",
            "strain 0.051541540771722794\n",
            "strain 0.05689943581819534\n",
            "strain 0.062157876789569855\n",
            "strain 0.07001765817403793\n",
            "strain 0.06547912210226059\n",
            "strain 0.04405932500958443\n",
            "strain 0.06706654280424118\n",
            "strain 0.05754689499735832\n",
            "strain 0.0508245974779129\n",
            "strain 0.04217728599905968\n",
            "strain 0.05799134820699692\n",
            "strain 0.057703979313373566\n",
            "strain 0.06141069531440735\n",
            "strain 0.05997401475906372\n",
            "strain 0.05293440818786621\n",
            "strain 0.05480838567018509\n",
            "strain 0.05374712869524956\n",
            "strain 0.06143913418054581\n",
            "strain 0.07447272539138794\n",
            "strain 0.07876664400100708\n",
            "strain 0.07208342105150223\n",
            "strain 0.047526128590106964\n",
            "strain 0.061203669756650925\n",
            "strain 0.0593615286052227\n",
            "strain 0.055937185883522034\n",
            "strain 0.06959964334964752\n",
            "strain 0.06385578215122223\n",
            "strain 0.06550884246826172\n",
            "strain 0.05638732761144638\n",
            "strain 0.05927858129143715\n",
            "classify 2.2811279296875\n",
            "classify 2.276611328125\n",
            "classify 2.2503662109375\n",
            "classify 2.2811279296875\n",
            "classify 2.23046875\n",
            "classify 2.3018798828125\n",
            "classify 2.260498046875\n",
            "classify 2.3106689453125\n",
            "classify 2.2945556640625\n",
            "classify 2.28125\n",
            "classify 2.306396484375\n",
            "0.15625\n",
            "0.09375\n",
            "0.203125\n",
            "0.078125\n",
            "0.171875\n",
            "0.078125\n",
            "0.125\n",
            "0.109375\n",
            "0.125\n",
            "0.140625\n",
            "0.1875\n",
            "91\n",
            "strain 0.05423302575945854\n",
            "strain 0.056306492537260056\n",
            "strain 0.05554702505469322\n",
            "strain 0.06819814443588257\n",
            "strain 0.0647488608956337\n",
            "strain 0.050652313977479935\n",
            "strain 0.06521052122116089\n",
            "strain 0.06949066370725632\n",
            "strain 0.05893263965845108\n",
            "strain 0.051368098706007004\n",
            "strain 0.05929585546255112\n",
            "strain 0.06695540249347687\n",
            "strain 0.06561845541000366\n",
            "strain 0.047858826816082\n",
            "strain 0.05905383825302124\n",
            "strain 0.05612122267484665\n",
            "strain 0.0600811243057251\n",
            "strain 0.04659499228000641\n",
            "strain 0.06129610538482666\n",
            "strain 0.056278713047504425\n",
            "strain 0.05633648484945297\n",
            "strain 0.0616178959608078\n",
            "strain 0.055745091289281845\n",
            "strain 0.053588300943374634\n",
            "strain 0.06355666369199753\n",
            "strain 0.05564931035041809\n",
            "strain 0.05483713373541832\n",
            "strain 0.07709018886089325\n",
            "strain 0.05447211489081383\n",
            "strain 0.05622835457324982\n",
            "strain 0.058529749512672424\n",
            "strain 0.0746256411075592\n",
            "strain 0.05988185852766037\n",
            "strain 0.058770835399627686\n",
            "strain 0.08133822679519653\n",
            "strain 0.06364237517118454\n",
            "strain 0.04923573136329651\n",
            "strain 0.05009278282523155\n",
            "strain 0.05707138031721115\n",
            "strain 0.08151163160800934\n",
            "strain 0.0561557337641716\n",
            "strain 0.06675992161035538\n",
            "strain 0.06562010198831558\n",
            "strain 0.055377598851919174\n",
            "strain 0.0678914338350296\n",
            "strain 0.06658279150724411\n",
            "strain 0.056781478226184845\n",
            "strain 0.049759794026613235\n",
            "strain 0.04514499008655548\n",
            "strain 0.053025491535663605\n",
            "strain 0.0720139592885971\n",
            "classify 2.2701416015625\n",
            "classify 2.251708984375\n",
            "classify 2.2354736328125\n",
            "classify 2.2855224609375\n",
            "classify 2.2020263671875\n",
            "classify 2.2415771484375\n",
            "classify 2.2431640625\n",
            "classify 2.2303466796875\n",
            "classify 2.2958984375\n",
            "classify 2.2069091796875\n",
            "classify 2.2642822265625\n",
            "0.171875\n",
            "0.203125\n",
            "0.171875\n",
            "0.140625\n",
            "0.109375\n",
            "0.171875\n",
            "0.125\n",
            "0.125\n",
            "0.140625\n",
            "0.1875\n",
            "0.15625\n",
            "92\n",
            "strain 0.057016100734472275\n",
            "strain 0.05786385014653206\n",
            "strain 0.06891661882400513\n",
            "strain 0.06683263927698135\n",
            "strain 0.0513557605445385\n",
            "strain 0.06589612364768982\n",
            "strain 0.058207917958498\n",
            "strain 0.05630302429199219\n",
            "strain 0.06484394520521164\n",
            "strain 0.04991275072097778\n",
            "strain 0.06773128360509872\n",
            "strain 0.06040836498141289\n",
            "strain 0.05435481294989586\n",
            "strain 0.06225796416401863\n",
            "strain 0.06503483653068542\n",
            "strain 0.08575193583965302\n",
            "strain 0.07668095082044601\n",
            "strain 0.05920591577887535\n",
            "strain 0.059750426560640335\n",
            "strain 0.05902037397027016\n",
            "strain 0.0575101263821125\n",
            "strain 0.05195467919111252\n",
            "strain 0.06236749887466431\n",
            "strain 0.055138781666755676\n",
            "strain 0.05678539723157883\n",
            "strain 0.06586192548274994\n",
            "strain 0.07016684859991074\n",
            "strain 0.055991947650909424\n",
            "strain 0.053029969334602356\n",
            "strain 0.059554025530815125\n",
            "strain 0.05281303450465202\n",
            "strain 0.05089763179421425\n",
            "strain 0.061983995139598846\n",
            "strain 0.0756119042634964\n",
            "strain 0.07235997915267944\n",
            "strain 0.05633419752120972\n",
            "strain 0.06379508972167969\n",
            "strain 0.05935529246926308\n",
            "strain 0.05894158408045769\n",
            "strain 0.07218924164772034\n",
            "strain 0.05555909126996994\n",
            "strain 0.06337672472000122\n",
            "strain 0.0591701939702034\n",
            "strain 0.05696117877960205\n",
            "strain 0.06879732012748718\n",
            "strain 0.06263486295938492\n",
            "strain 0.05182464420795441\n",
            "strain 0.05378594622015953\n",
            "strain 0.06155868247151375\n",
            "strain 0.08783803880214691\n",
            "strain 0.06155883148312569\n",
            "classify 2.2109375\n",
            "classify 2.299560546875\n",
            "classify 2.2330322265625\n",
            "classify 2.26220703125\n",
            "classify 2.234375\n",
            "classify 2.265380859375\n",
            "classify 2.2603759765625\n",
            "classify 2.244140625\n",
            "classify 2.235107421875\n",
            "classify 2.2476806640625\n",
            "classify 2.278564453125\n",
            "0.234375\n",
            "0.109375\n",
            "0.15625\n",
            "0.125\n",
            "0.109375\n",
            "0.1875\n",
            "0.140625\n",
            "0.125\n",
            "0.09375\n",
            "0.21875\n",
            "0.234375\n",
            "93\n",
            "strain 0.06263727694749832\n",
            "strain 0.06106821075081825\n",
            "strain 0.05395554378628731\n",
            "strain 0.06417910754680634\n",
            "strain 0.07055604457855225\n",
            "strain 0.06267804652452469\n",
            "strain 0.06658775359392166\n",
            "strain 0.059540845453739166\n",
            "strain 0.04944330453872681\n",
            "strain 0.056800492107868195\n",
            "strain 0.07837210595607758\n",
            "strain 0.08029703795909882\n",
            "strain 0.08330382406711578\n",
            "strain 0.06056962534785271\n",
            "strain 0.05797958001494408\n",
            "strain 0.0744231641292572\n",
            "strain 0.04916548728942871\n",
            "strain 0.066130131483078\n",
            "strain 0.06615599989891052\n",
            "strain 0.06901784241199493\n",
            "strain 0.06398437172174454\n",
            "strain 0.06027323752641678\n",
            "strain 0.07726435363292694\n",
            "strain 0.06044209003448486\n",
            "strain 0.07280503958463669\n",
            "strain 0.07168322801589966\n",
            "strain 0.06945376098155975\n",
            "strain 0.06119285896420479\n",
            "strain 0.05606061965227127\n",
            "strain 0.08167123049497604\n",
            "strain 0.056753650307655334\n",
            "strain 0.06946749985218048\n",
            "strain 0.058435093611478806\n",
            "strain 0.05491242930293083\n",
            "strain 0.07622962445020676\n",
            "strain 0.05633252114057541\n",
            "strain 0.05984403192996979\n",
            "strain 0.06049583479762077\n",
            "strain 0.06370649486780167\n",
            "strain 0.09611974656581879\n",
            "strain 0.060651686042547226\n",
            "strain 0.06094038486480713\n",
            "strain 0.05403563007712364\n",
            "strain 0.05323825404047966\n",
            "strain 0.051535818725824356\n",
            "strain 0.05349533632397652\n",
            "strain 0.08380250632762909\n",
            "strain 0.07738097012042999\n",
            "strain 0.07216432690620422\n",
            "strain 0.060247208923101425\n",
            "strain 0.05665804073214531\n",
            "classify 2.1829833984375\n",
            "classify 2.3167724609375\n",
            "classify 2.254150390625\n",
            "classify 2.1983642578125\n",
            "classify 2.312744140625\n",
            "classify 2.2906494140625\n",
            "classify 2.248046875\n",
            "classify 2.25439453125\n",
            "classify 2.2877197265625\n",
            "classify 2.284423828125\n",
            "classify 2.2305908203125\n",
            "0.171875\n",
            "0.109375\n",
            "0.171875\n",
            "0.125\n",
            "0.171875\n",
            "0.21875\n",
            "0.078125\n",
            "0.09375\n",
            "0.109375\n",
            "0.109375\n",
            "0.125\n",
            "94\n",
            "strain 0.06871946901082993\n",
            "strain 0.06145571917295456\n",
            "strain 0.05925833433866501\n",
            "strain 0.05582445114850998\n",
            "strain 0.060011498630046844\n",
            "strain 0.0629403218626976\n",
            "strain 0.05952588841319084\n",
            "strain 0.053808774799108505\n",
            "strain 0.058333899825811386\n",
            "strain 0.0534290075302124\n",
            "strain 0.060928504914045334\n",
            "strain 0.06417505443096161\n",
            "strain 0.05350623279809952\n",
            "strain 0.07916222512722015\n",
            "strain 0.06442002952098846\n",
            "strain 0.05296541750431061\n",
            "strain 0.052419207990169525\n",
            "strain 0.05747337266802788\n",
            "strain 0.06485487520694733\n",
            "strain 0.05056484043598175\n",
            "strain 0.060312457382678986\n",
            "strain 0.0689513310790062\n",
            "strain 0.06552119553089142\n",
            "strain 0.051636841148138046\n",
            "strain 0.05480081960558891\n",
            "strain 0.056072771549224854\n",
            "strain 0.0548856146633625\n",
            "strain 0.07170107215642929\n",
            "strain 0.05555146560072899\n",
            "strain 0.05715355649590492\n",
            "strain 0.08261726051568985\n",
            "strain 0.056826360523700714\n",
            "strain 0.05135686695575714\n",
            "strain 0.06501010060310364\n",
            "strain 0.05630476027727127\n",
            "strain 0.0564810149371624\n",
            "strain 0.05772842839360237\n",
            "strain 0.07335349172353745\n",
            "strain 0.08406592905521393\n",
            "strain 0.06617112457752228\n",
            "strain 0.06204982101917267\n",
            "strain 0.04975300282239914\n",
            "strain 0.05199730396270752\n",
            "strain 0.051439087837934494\n",
            "strain 0.04590681940317154\n",
            "strain 0.06609237194061279\n",
            "strain 0.06500332802534103\n",
            "strain 0.05774632468819618\n",
            "strain 0.08344879001379013\n",
            "strain 0.06260037422180176\n",
            "strain 0.05456148460507393\n",
            "classify 2.260498046875\n",
            "classify 2.2891845703125\n",
            "classify 2.1990966796875\n",
            "classify 2.288818359375\n",
            "classify 2.21484375\n",
            "classify 2.2725830078125\n",
            "classify 2.20654296875\n",
            "classify 2.261474609375\n",
            "classify 2.2420654296875\n",
            "classify 2.275146484375\n",
            "classify 2.253662109375\n",
            "0.078125\n",
            "0.171875\n",
            "0.234375\n",
            "0.125\n",
            "0.125\n",
            "0.171875\n",
            "0.109375\n",
            "0.15625\n",
            "0.203125\n",
            "0.171875\n",
            "0.078125\n",
            "95\n",
            "strain 0.05107756331562996\n",
            "strain 0.05766139551997185\n",
            "strain 0.07833567261695862\n",
            "strain 0.052541326731443405\n",
            "strain 0.07294874638319016\n",
            "strain 0.06506720930337906\n",
            "strain 0.05487953871488571\n",
            "strain 0.05582645535469055\n",
            "strain 0.07252653688192368\n",
            "strain 0.05281274765729904\n",
            "strain 0.06759240478277206\n",
            "strain 0.07161224633455276\n",
            "strain 0.061306800693273544\n",
            "strain 0.06705587357282639\n",
            "strain 0.0612272284924984\n",
            "strain 0.06659765541553497\n",
            "strain 0.07004208862781525\n",
            "strain 0.0618923120200634\n",
            "strain 0.060449257493019104\n",
            "strain 0.06503837555646896\n",
            "strain 0.0624065026640892\n",
            "strain 0.066781185567379\n",
            "strain 0.06608948111534119\n",
            "strain 0.06186547130346298\n",
            "strain 0.06600090861320496\n",
            "strain 0.06419461220502853\n",
            "strain 0.07421199232339859\n",
            "strain 0.05462126061320305\n",
            "strain 0.058086469769477844\n",
            "strain 0.0704287439584732\n",
            "strain 0.06832253932952881\n",
            "strain 0.04877781122922897\n",
            "strain 0.06747675687074661\n",
            "strain 0.05864633619785309\n",
            "strain 0.058951642364263535\n",
            "strain 0.06055738031864166\n",
            "strain 0.06374546885490417\n",
            "strain 0.05366672948002815\n",
            "strain 0.09332506358623505\n",
            "strain 0.0681392103433609\n",
            "strain 0.05442269146442413\n",
            "strain 0.05262943357229233\n",
            "strain 0.0607047863304615\n",
            "strain 0.07726506143808365\n",
            "strain 0.05350314453244209\n",
            "strain 0.07342161983251572\n",
            "strain 0.06394537538290024\n",
            "strain 0.06087719649076462\n",
            "strain 0.08634462207555771\n",
            "strain 0.052705924957990646\n",
            "strain 0.05809938162565231\n",
            "classify 2.2420654296875\n",
            "classify 2.2532958984375\n",
            "classify 2.2666015625\n",
            "classify 2.267822265625\n",
            "classify 2.2373046875\n",
            "classify 2.2431640625\n",
            "classify 2.22412109375\n",
            "classify 2.2886962890625\n",
            "classify 2.2601318359375\n",
            "classify 2.264404296875\n",
            "classify 2.2725830078125\n",
            "0.140625\n",
            "0.140625\n",
            "0.125\n",
            "0.1875\n",
            "0.140625\n",
            "0.25\n",
            "0.203125\n",
            "0.15625\n",
            "0.171875\n",
            "0.15625\n",
            "0.1875\n",
            "96\n",
            "strain 0.0526559017598629\n",
            "strain 0.05449219420552254\n",
            "strain 0.06163399666547775\n",
            "strain 0.04697520658373833\n",
            "strain 0.05276312306523323\n",
            "strain 0.05518891289830208\n",
            "strain 0.06471367180347443\n",
            "strain 0.06357183307409286\n",
            "strain 0.05162188038229942\n",
            "strain 0.07947909086942673\n",
            "strain 0.05124243348836899\n",
            "strain 0.05288993567228317\n",
            "strain 0.05276830494403839\n",
            "strain 0.062140464782714844\n",
            "strain 0.06280004978179932\n",
            "strain 0.05972945690155029\n",
            "strain 0.07511693239212036\n",
            "strain 0.0579289086163044\n",
            "strain 0.05929914489388466\n",
            "strain 0.05610291287302971\n",
            "strain 0.05841071531176567\n",
            "strain 0.052151698619127274\n",
            "strain 0.0644308477640152\n",
            "strain 0.07872512936592102\n",
            "strain 0.06992980092763901\n",
            "strain 0.06312408298254013\n",
            "strain 0.054517410695552826\n",
            "strain 0.05546845868229866\n",
            "strain 0.05161415413022041\n",
            "strain 0.05690261349081993\n",
            "strain 0.055321235209703445\n",
            "strain 0.05589068681001663\n",
            "strain 0.07565271854400635\n",
            "strain 0.0781925618648529\n",
            "strain 0.0588625930249691\n",
            "strain 0.06011772155761719\n",
            "strain 0.058680228888988495\n",
            "strain 0.07463862746953964\n",
            "strain 0.08213665336370468\n",
            "strain 0.08224573731422424\n",
            "strain 0.06174308806657791\n",
            "strain 0.06501758098602295\n",
            "strain 0.06526435911655426\n",
            "strain 0.07306572794914246\n",
            "strain 0.05898614600300789\n",
            "strain 0.058587245643138885\n",
            "strain 0.0642375573515892\n",
            "strain 0.06136483699083328\n",
            "strain 0.08476152271032333\n",
            "strain 0.05931368097662926\n",
            "strain 0.0745895653963089\n",
            "classify 2.275634765625\n",
            "classify 2.30322265625\n",
            "classify 2.276123046875\n",
            "classify 2.2825927734375\n",
            "classify 2.24658203125\n",
            "classify 2.2642822265625\n",
            "classify 2.26220703125\n",
            "classify 2.21484375\n",
            "classify 2.297607421875\n",
            "classify 2.2696533203125\n",
            "classify 2.14013671875\n",
            "0.15625\n",
            "0.171875\n",
            "0.140625\n",
            "0.15625\n",
            "0.109375\n",
            "0.171875\n",
            "0.140625\n",
            "0.125\n",
            "0.078125\n",
            "0.109375\n",
            "0.171875\n",
            "97\n",
            "strain 0.05110051855444908\n",
            "strain 0.05949296057224274\n",
            "strain 0.06922522187232971\n",
            "strain 0.08942972868680954\n",
            "strain 0.06492768228054047\n",
            "strain 0.06552504003047943\n",
            "strain 0.05665866658091545\n",
            "strain 0.05727815628051758\n",
            "strain 0.07212886959314346\n",
            "strain 0.05628170073032379\n",
            "strain 0.059262752532958984\n",
            "strain 0.06881707906723022\n",
            "strain 0.04974689707159996\n",
            "strain 0.06302911043167114\n",
            "strain 0.06374172121286392\n",
            "strain 0.06459574401378632\n",
            "strain 0.059083037078380585\n",
            "strain 0.07038785517215729\n",
            "strain 0.07127779722213745\n",
            "strain 0.06233295798301697\n",
            "strain 0.058032043278217316\n",
            "strain 0.05525243282318115\n",
            "strain 0.062139082700014114\n",
            "strain 0.05157915875315666\n",
            "strain 0.0643136203289032\n",
            "strain 0.05191710218787193\n",
            "strain 0.05208394676446915\n",
            "strain 0.07293595373630524\n",
            "strain 0.058526962995529175\n",
            "strain 0.051533158868551254\n",
            "strain 0.06649460643529892\n",
            "strain 0.05437115952372551\n",
            "strain 0.06737668067216873\n",
            "strain 0.06566985696554184\n",
            "strain 0.05235719680786133\n",
            "strain 0.05918539687991142\n",
            "strain 0.06444454938173294\n",
            "strain 0.07215853780508041\n",
            "strain 0.06475348770618439\n",
            "strain 0.07362693548202515\n",
            "strain 0.09205304086208344\n",
            "strain 0.05981462076306343\n",
            "strain 0.06254462152719498\n",
            "strain 0.07073519378900528\n",
            "strain 0.06405559927225113\n",
            "strain 0.05772458389401436\n",
            "strain 0.05226292461156845\n",
            "strain 0.07220889627933502\n",
            "strain 0.05423804745078087\n",
            "strain 0.08080083131790161\n",
            "strain 0.0634809285402298\n",
            "classify 2.2685546875\n",
            "classify 2.282958984375\n",
            "classify 2.25732421875\n",
            "classify 2.229736328125\n",
            "classify 2.2537841796875\n",
            "classify 2.2718505859375\n",
            "classify 2.250244140625\n",
            "classify 2.2149658203125\n",
            "classify 2.22509765625\n",
            "classify 2.267578125\n",
            "classify 2.2449951171875\n",
            "0.125\n",
            "0.125\n",
            "0.09375\n",
            "0.15625\n",
            "0.140625\n",
            "0.109375\n",
            "0.171875\n",
            "0.203125\n",
            "0.046875\n",
            "0.171875\n",
            "0.109375\n",
            "98\n",
            "strain 0.05861971154808998\n",
            "strain 0.06147196516394615\n",
            "strain 0.05171140655875206\n",
            "strain 0.06015710160136223\n",
            "strain 0.04505502060055733\n",
            "strain 0.07838626950979233\n",
            "strain 0.06409599632024765\n",
            "strain 0.07463330030441284\n",
            "strain 0.07159977406263351\n",
            "strain 0.059532735496759415\n",
            "strain 0.06048864126205444\n",
            "strain 0.0631997361779213\n",
            "strain 0.05849871784448624\n",
            "strain 0.05932231247425079\n",
            "strain 0.059779293835163116\n",
            "strain 0.07124069333076477\n",
            "strain 0.053653668612241745\n",
            "strain 0.06859549880027771\n",
            "strain 0.059803806245326996\n",
            "strain 0.05922944098711014\n",
            "strain 0.0506209172308445\n",
            "strain 0.0700032114982605\n",
            "strain 0.05606420710682869\n",
            "strain 0.06951513886451721\n",
            "strain 0.05337386205792427\n",
            "strain 0.06562785059213638\n",
            "strain 0.058524224907159805\n",
            "strain 0.05871919170022011\n",
            "strain 0.05679849907755852\n",
            "strain 0.0898725837469101\n",
            "strain 0.0869598537683487\n",
            "strain 0.05770496651530266\n",
            "strain 0.06556765735149384\n",
            "strain 0.055276285856962204\n",
            "strain 0.05477654188871384\n",
            "strain 0.06402361392974854\n",
            "strain 0.07192429155111313\n",
            "strain 0.0903087854385376\n",
            "strain 0.06669019162654877\n",
            "strain 0.0611145943403244\n",
            "strain 0.056437235325574875\n",
            "strain 0.07149133831262589\n",
            "strain 0.07333213835954666\n",
            "strain 0.054585643112659454\n",
            "strain 0.09165114164352417\n",
            "strain 0.05862058326601982\n",
            "strain 0.054175686091184616\n",
            "strain 0.049189548939466476\n",
            "strain 0.05864274501800537\n",
            "strain 0.0605984590947628\n",
            "strain 0.05627067759633064\n",
            "classify 2.2142333984375\n",
            "classify 2.3087158203125\n",
            "classify 2.2509765625\n",
            "classify 2.23583984375\n",
            "classify 2.2398681640625\n",
            "classify 2.2772216796875\n",
            "classify 2.283203125\n",
            "classify 2.2144775390625\n",
            "classify 2.2569580078125\n",
            "classify 2.235107421875\n",
            "classify 2.2637939453125\n",
            "0.140625\n",
            "0.140625\n",
            "0.109375\n",
            "0.171875\n",
            "0.09375\n",
            "0.171875\n",
            "0.15625\n",
            "0.21875\n",
            "0.125\n",
            "0.28125\n",
            "0.171875\n",
            "99\n",
            "strain 0.0752091184258461\n",
            "strain 0.08868958801031113\n",
            "strain 0.07471447438001633\n",
            "strain 0.07554785907268524\n",
            "strain 0.06846226751804352\n",
            "strain 0.059241704642772675\n",
            "strain 0.06076507270336151\n",
            "strain 0.057693615555763245\n",
            "strain 0.06232413277029991\n",
            "strain 0.07246753573417664\n",
            "strain 0.06755381077528\n",
            "strain 0.05925146862864494\n",
            "strain 0.08073855936527252\n",
            "strain 0.06261154264211655\n",
            "strain 0.07060378044843674\n",
            "strain 0.06637084484100342\n",
            "strain 0.054923225194215775\n",
            "strain 0.0691128671169281\n",
            "strain 0.08088260889053345\n",
            "strain 0.07121174037456512\n",
            "strain 0.07298934459686279\n",
            "strain 0.062118273228406906\n",
            "strain 0.059159066528081894\n",
            "strain 0.06325843185186386\n",
            "strain 0.07783646881580353\n",
            "strain 0.06799165904521942\n",
            "strain 0.05479758232831955\n",
            "strain 0.06253506988286972\n",
            "strain 0.07980324327945709\n",
            "strain 0.0668703019618988\n",
            "strain 0.06643111258745193\n",
            "strain 0.06324788182973862\n",
            "strain 0.05878977105021477\n",
            "strain 0.05821561440825462\n",
            "strain 0.06596262753009796\n",
            "strain 0.0698944479227066\n",
            "strain 0.05363095924258232\n",
            "strain 0.0565006360411644\n",
            "strain 0.0793183222413063\n",
            "strain 0.060481514781713486\n",
            "strain 0.06553377211093903\n",
            "strain 0.06787818670272827\n",
            "strain 0.0800883024930954\n",
            "strain 0.07566914707422256\n",
            "strain 0.060158684849739075\n",
            "strain 0.06537263095378876\n",
            "strain 0.05105571076273918\n",
            "strain 0.055865660309791565\n",
            "strain 0.07008959352970123\n",
            "strain 0.05497191846370697\n",
            "strain 0.0841897577047348\n",
            "classify 2.22607421875\n",
            "classify 2.2373046875\n",
            "classify 2.2806396484375\n",
            "classify 2.2828369140625\n",
            "classify 2.26611328125\n",
            "classify 2.2069091796875\n",
            "classify 2.2781982421875\n",
            "classify 2.29931640625\n",
            "classify 2.2720947265625\n",
            "classify 2.2606201171875\n",
            "classify 2.241455078125\n",
            "0.1875\n",
            "0.171875\n",
            "0.09375\n",
            "0.21875\n",
            "0.234375\n",
            "0.15625\n",
            "0.15625\n",
            "0.125\n",
            "0.15625\n",
            "0.140625\n",
            "0.109375\n",
            "100\n",
            "strain 0.07484342157840729\n",
            "strain 0.07499587535858154\n",
            "strain 0.07406949251890182\n",
            "strain 0.06583186239004135\n",
            "strain 0.07489791512489319\n",
            "strain 0.06429097056388855\n",
            "strain 0.07116438448429108\n",
            "strain 0.06931290030479431\n",
            "strain 0.06904782354831696\n",
            "strain 0.06283731013536453\n",
            "strain 0.06369577348232269\n",
            "strain 0.06893911957740784\n",
            "strain 0.06763320416212082\n",
            "strain 0.05803335830569267\n",
            "strain 0.05758291482925415\n",
            "strain 0.06341856718063354\n",
            "strain 0.0564914271235466\n",
            "strain 0.05522887781262398\n",
            "strain 0.06613031029701233\n",
            "strain 0.057301901280879974\n",
            "strain 0.0705249086022377\n",
            "strain 0.06977798789739609\n",
            "strain 0.05975983291864395\n",
            "strain 0.053591709583997726\n",
            "strain 0.06367843598127365\n",
            "strain 0.06571939587593079\n",
            "strain 0.06778759509325027\n",
            "strain 0.05824754759669304\n",
            "strain 0.06414120644330978\n",
            "strain 0.06073283404111862\n",
            "strain 0.055155277252197266\n",
            "strain 0.06990604847669601\n",
            "strain 0.07154810428619385\n",
            "strain 0.05897285044193268\n",
            "strain 0.06636069715023041\n",
            "strain 0.07339886575937271\n",
            "strain 0.05352997034788132\n",
            "strain 0.07011064887046814\n",
            "strain 0.06385815143585205\n",
            "strain 0.0643017366528511\n",
            "strain 0.06468365341424942\n",
            "strain 0.06335648894309998\n",
            "strain 0.056773193180561066\n",
            "strain 0.07042332738637924\n",
            "strain 0.058507245033979416\n",
            "strain 0.08216223120689392\n",
            "strain 0.05901411548256874\n",
            "strain 0.07482363283634186\n",
            "strain 0.07224488258361816\n",
            "strain 0.05286260321736336\n",
            "strain 0.06448306143283844\n",
            "classify 2.2474365234375\n",
            "classify 2.2490234375\n",
            "classify 2.2716064453125\n",
            "classify 2.24560546875\n",
            "classify 2.27392578125\n",
            "classify 2.336669921875\n",
            "classify 2.2266845703125\n",
            "classify 2.2476806640625\n",
            "classify 2.2325439453125\n",
            "classify 2.3282470703125\n",
            "classify 2.2572021484375\n",
            "0.125\n",
            "0.171875\n",
            "0.15625\n",
            "0.171875\n",
            "0.203125\n",
            "0.140625\n",
            "0.09375\n",
            "0.1875\n",
            "0.140625\n",
            "0.15625\n",
            "0.078125\n",
            "101\n",
            "strain 0.06189128756523132\n",
            "strain 0.09331782907247543\n",
            "strain 0.06305330991744995\n",
            "strain 0.07065075635910034\n",
            "strain 0.06952759623527527\n",
            "strain 0.06801585853099823\n",
            "strain 0.06027556210756302\n",
            "strain 0.05662640184164047\n",
            "strain 0.07166705280542374\n",
            "strain 0.06218373775482178\n",
            "strain 0.07348383218050003\n",
            "strain 0.06165827810764313\n",
            "strain 0.07916362583637238\n",
            "strain 0.08578961342573166\n",
            "strain 0.06188037618994713\n",
            "strain 0.06606702506542206\n",
            "strain 0.06864234060049057\n",
            "strain 0.06672963500022888\n",
            "strain 0.05889955163002014\n",
            "strain 0.05970694124698639\n",
            "strain 0.09097543358802795\n",
            "strain 0.06352661550045013\n",
            "strain 0.0664263591170311\n",
            "strain 0.07472935318946838\n",
            "strain 0.10325263440608978\n",
            "strain 0.06334152817726135\n",
            "strain 0.06099725514650345\n",
            "strain 0.05951107665896416\n",
            "strain 0.058348219841718674\n",
            "strain 0.06513816118240356\n",
            "strain 0.06428790092468262\n",
            "strain 0.0749981701374054\n",
            "strain 0.05593399330973625\n",
            "strain 0.062122952193021774\n",
            "strain 0.06852477788925171\n",
            "strain 0.07022649049758911\n",
            "strain 0.08582551777362823\n",
            "strain 0.0872139036655426\n",
            "strain 0.07400999218225479\n",
            "strain 0.08217369019985199\n",
            "strain 0.06261050701141357\n",
            "strain 0.07628866285085678\n",
            "strain 0.06327877193689346\n",
            "strain 0.05198538303375244\n",
            "strain 0.07075229287147522\n",
            "strain 0.06420931220054626\n",
            "strain 0.07925081253051758\n",
            "strain 0.0733267217874527\n",
            "strain 0.06729428470134735\n",
            "strain 0.05618724599480629\n",
            "strain 0.06724700331687927\n",
            "classify 2.30322265625\n",
            "classify 2.2904052734375\n",
            "classify 2.3560791015625\n",
            "classify 2.2557373046875\n",
            "classify 2.226318359375\n",
            "classify 2.282470703125\n",
            "classify 2.291748046875\n",
            "classify 2.2305908203125\n",
            "classify 2.3580322265625\n",
            "classify 2.2801513671875\n",
            "classify 2.283447265625\n",
            "0.140625\n",
            "0.125\n",
            "0.09375\n",
            "0.109375\n",
            "0.109375\n",
            "0.140625\n",
            "0.078125\n",
            "0.15625\n",
            "0.234375\n",
            "0.125\n",
            "0.203125\n",
            "102\n",
            "strain 0.06212345138192177\n",
            "strain 0.06533476710319519\n",
            "strain 0.05904969573020935\n",
            "strain 0.06378090381622314\n",
            "strain 0.059983041137456894\n",
            "strain 0.058895282447338104\n",
            "strain 0.056453198194503784\n",
            "strain 0.0628935769200325\n",
            "strain 0.06696903705596924\n",
            "strain 0.08301910758018494\n",
            "strain 0.08537422120571136\n",
            "strain 0.06201649829745293\n",
            "strain 0.0818067118525505\n",
            "strain 0.07244627922773361\n",
            "strain 0.05959951877593994\n",
            "strain 0.05605171248316765\n",
            "strain 0.05517767369747162\n",
            "strain 0.06544274091720581\n",
            "strain 0.07322883605957031\n",
            "strain 0.09735159575939178\n",
            "strain 0.07742573320865631\n",
            "strain 0.06654150038957596\n",
            "strain 0.07409749925136566\n",
            "strain 0.06621114909648895\n",
            "strain 0.05887743458151817\n",
            "strain 0.06769072264432907\n",
            "strain 0.06279057264328003\n",
            "strain 0.08011634647846222\n",
            "strain 0.06024289131164551\n",
            "strain 0.06365662813186646\n",
            "strain 0.08297494798898697\n",
            "strain 0.0810348242521286\n",
            "strain 0.0617862269282341\n",
            "strain 0.06396935135126114\n",
            "strain 0.06278874725103378\n",
            "strain 0.06586062163114548\n",
            "strain 0.07205523550510406\n",
            "strain 0.06838390976190567\n",
            "strain 0.08810576051473618\n",
            "strain 0.061091262847185135\n",
            "strain 0.06943696737289429\n",
            "strain 0.05715782195329666\n",
            "strain 0.07518342137336731\n",
            "strain 0.0762811005115509\n",
            "strain 0.07759331166744232\n",
            "strain 0.06142377853393555\n",
            "strain 0.07423529773950577\n",
            "strain 0.06850002706050873\n",
            "strain 0.05649455264210701\n",
            "strain 0.08618760108947754\n",
            "strain 0.05733794718980789\n",
            "classify 2.269287109375\n",
            "classify 2.2830810546875\n",
            "classify 2.2440185546875\n",
            "classify 2.3057861328125\n",
            "classify 2.2164306640625\n",
            "classify 2.261474609375\n",
            "classify 2.2786865234375\n",
            "classify 2.248291015625\n",
            "classify 2.248291015625\n",
            "classify 2.264404296875\n",
            "classify 2.2879638671875\n",
            "0.125\n",
            "0.078125\n",
            "0.046875\n",
            "0.109375\n",
            "0.1875\n",
            "0.15625\n",
            "0.125\n",
            "0.078125\n",
            "0.15625\n",
            "0.203125\n",
            "0.1875\n",
            "103\n",
            "strain 0.0747276023030281\n",
            "strain 0.06979943811893463\n",
            "strain 0.07167323678731918\n",
            "strain 0.07844223082065582\n",
            "strain 0.06421191245317459\n",
            "strain 0.07237997651100159\n",
            "strain 0.0572434663772583\n",
            "strain 0.05280508100986481\n",
            "strain 0.06424739211797714\n",
            "strain 0.07603359967470169\n",
            "strain 0.05807873234152794\n",
            "strain 0.06985674053430557\n",
            "strain 0.06041054055094719\n",
            "strain 0.06721675395965576\n",
            "strain 0.06733439862728119\n",
            "strain 0.06805822253227234\n",
            "strain 0.058184899389743805\n",
            "strain 0.0632915049791336\n",
            "strain 0.05771363526582718\n",
            "strain 0.05945555493235588\n",
            "strain 0.0648464784026146\n",
            "strain 0.056235868483781815\n",
            "strain 0.06461499631404877\n",
            "strain 0.069948211312294\n",
            "strain 0.07042825222015381\n",
            "strain 0.06577269732952118\n",
            "strain 0.0724143385887146\n",
            "strain 0.07065828889608383\n",
            "strain 0.055489737540483475\n",
            "strain 0.0639968067407608\n",
            "strain 0.07311400771141052\n",
            "strain 0.06201668828725815\n",
            "strain 0.07167467474937439\n",
            "strain 0.07367261499166489\n",
            "strain 0.05486568808555603\n",
            "strain 0.07956676930189133\n",
            "strain 0.06495749205350876\n",
            "strain 0.05573548749089241\n",
            "strain 0.07250845432281494\n",
            "strain 0.06620287895202637\n",
            "strain 0.09180627763271332\n",
            "strain 0.061338264495134354\n",
            "strain 0.06384389102458954\n",
            "strain 0.062059421092271805\n",
            "strain 0.06946447491645813\n",
            "strain 0.05332152917981148\n",
            "strain 0.05852433666586876\n",
            "strain 0.05621461197733879\n",
            "strain 0.07866629213094711\n",
            "strain 0.08913759887218475\n",
            "strain 0.06124918907880783\n",
            "classify 2.265380859375\n",
            "classify 2.2459716796875\n",
            "classify 2.293701171875\n",
            "classify 2.2279052734375\n",
            "classify 2.2852783203125\n",
            "classify 2.27099609375\n",
            "classify 2.2572021484375\n",
            "classify 2.2623291015625\n",
            "classify 2.2843017578125\n",
            "classify 2.2960205078125\n",
            "classify 2.287841796875\n",
            "0.21875\n",
            "0.25\n",
            "0.140625\n",
            "0.09375\n",
            "0.140625\n",
            "0.171875\n",
            "0.140625\n",
            "0.15625\n",
            "0.203125\n",
            "0.203125\n",
            "0.125\n",
            "104\n",
            "strain 0.06089501455426216\n",
            "strain 0.07922377437353134\n",
            "strain 0.06590118259191513\n",
            "strain 0.08035260438919067\n",
            "strain 0.06330923736095428\n",
            "strain 0.05806171894073486\n",
            "strain 0.05871419981122017\n",
            "strain 0.07532796263694763\n",
            "strain 0.0637192577123642\n",
            "strain 0.0663781687617302\n",
            "strain 0.062154173851013184\n",
            "strain 0.09287014603614807\n",
            "strain 0.07514675706624985\n",
            "strain 0.062443822622299194\n",
            "strain 0.07370849698781967\n",
            "strain 0.1105671152472496\n",
            "strain 0.05695097893476486\n",
            "strain 0.05420969799160957\n",
            "strain 0.053813185542821884\n",
            "strain 0.08274749666452408\n",
            "strain 0.06459374725818634\n",
            "strain 0.07333505153656006\n",
            "strain 0.06731057167053223\n",
            "strain 0.06571394205093384\n",
            "strain 0.07031460851430893\n",
            "strain 0.06593944877386093\n",
            "strain 0.05627826973795891\n",
            "strain 0.05923023447394371\n",
            "strain 0.05587451905012131\n",
            "strain 0.07549303025007248\n",
            "strain 0.0720277950167656\n",
            "strain 0.060479603707790375\n",
            "strain 0.05157501623034477\n",
            "strain 0.06294163316488266\n",
            "strain 0.06412405520677567\n",
            "strain 0.09588639438152313\n",
            "strain 0.06926491856575012\n",
            "strain 0.05201437696814537\n",
            "strain 0.0625850260257721\n",
            "strain 0.08005419373512268\n",
            "strain 0.04899568855762482\n",
            "strain 0.06200847402215004\n",
            "strain 0.05948042869567871\n",
            "strain 0.07317598909139633\n",
            "strain 0.07019272446632385\n",
            "strain 0.05974235758185387\n",
            "strain 0.05735564976930618\n",
            "strain 0.0642608180642128\n",
            "strain 0.05911407992243767\n",
            "strain 0.05149615928530693\n",
            "strain 0.06620247662067413\n",
            "classify 2.2139892578125\n",
            "classify 2.2044677734375\n",
            "classify 2.26953125\n",
            "classify 2.28173828125\n",
            "classify 2.23974609375\n",
            "classify 2.2889404296875\n",
            "classify 2.262451171875\n",
            "classify 2.2550048828125\n",
            "classify 2.2158203125\n",
            "classify 2.209716796875\n",
            "classify 2.264404296875\n",
            "0.140625\n",
            "0.140625\n",
            "0.125\n",
            "0.1875\n",
            "0.21875\n",
            "0.125\n",
            "0.078125\n",
            "0.203125\n",
            "0.15625\n",
            "0.21875\n",
            "0.15625\n",
            "105\n",
            "strain 0.05788201838731766\n",
            "strain 0.06210869550704956\n",
            "strain 0.07549810409545898\n",
            "strain 0.06730907410383224\n",
            "strain 0.06070621684193611\n",
            "strain 0.07086128741502762\n",
            "strain 0.0713845044374466\n",
            "strain 0.07238899916410446\n",
            "strain 0.05518123134970665\n",
            "strain 0.0661049634218216\n",
            "strain 0.08563882112503052\n",
            "strain 0.05122402682900429\n",
            "strain 0.07384881377220154\n",
            "strain 0.06592482328414917\n",
            "strain 0.0762699618935585\n",
            "strain 0.07222945243120193\n",
            "strain 0.0786886066198349\n",
            "strain 0.06939136981964111\n",
            "strain 0.0734032392501831\n",
            "strain 0.06919272989034653\n",
            "strain 0.06642169505357742\n",
            "strain 0.10989078134298325\n",
            "strain 0.07253875583410263\n",
            "strain 0.07205908000469208\n",
            "strain 0.06930668652057648\n",
            "strain 0.09616689383983612\n",
            "strain 0.06565629690885544\n",
            "strain 0.07165371626615524\n",
            "strain 0.06053702160716057\n",
            "strain 0.07510579377412796\n",
            "strain 0.0865277498960495\n",
            "strain 0.07523360848426819\n",
            "strain 0.06003853306174278\n",
            "strain 0.06660250574350357\n",
            "strain 0.06624461710453033\n",
            "strain 0.06357766687870026\n",
            "strain 0.06502585858106613\n",
            "strain 0.0682101845741272\n",
            "strain 0.0606667585670948\n",
            "strain 0.06815005838871002\n",
            "strain 0.08144479990005493\n",
            "strain 0.05798521265387535\n",
            "strain 0.07311917841434479\n",
            "strain 0.07597653567790985\n",
            "strain 0.07006853073835373\n",
            "strain 0.08623769134283066\n",
            "strain 0.07541291415691376\n",
            "strain 0.05577608570456505\n",
            "strain 0.07470767199993134\n",
            "strain 0.06796911358833313\n",
            "strain 0.07493627071380615\n",
            "classify 2.2562255859375\n",
            "classify 2.2537841796875\n",
            "classify 2.217041015625\n",
            "classify 2.2108154296875\n",
            "classify 2.2635498046875\n",
            "classify 2.27734375\n",
            "classify 2.2535400390625\n",
            "classify 2.2542724609375\n",
            "classify 2.277587890625\n",
            "classify 2.318115234375\n",
            "classify 2.2386474609375\n",
            "0.140625\n",
            "0.25\n",
            "0.234375\n",
            "0.171875\n",
            "0.25\n",
            "0.109375\n",
            "0.125\n",
            "0.125\n",
            "0.203125\n",
            "0.15625\n",
            "0.21875\n",
            "106\n",
            "strain 0.07790766656398773\n",
            "strain 0.09316151589155197\n",
            "strain 0.08260588347911835\n",
            "strain 0.1118209958076477\n",
            "strain 0.07305509597063065\n",
            "strain 0.07128589600324631\n",
            "strain 0.07326959818601608\n",
            "strain 0.06663671880960464\n",
            "strain 0.06736002117395401\n",
            "strain 0.059567611664533615\n",
            "strain 0.07918094098567963\n",
            "strain 0.0713166669011116\n",
            "strain 0.05659447982907295\n",
            "strain 0.08942890167236328\n",
            "strain 0.06752413511276245\n",
            "strain 0.05940897390246391\n",
            "strain 0.06616106629371643\n",
            "strain 0.06273388862609863\n",
            "strain 0.05927121639251709\n",
            "strain 0.05666595697402954\n",
            "strain 0.0738639384508133\n",
            "strain 0.07020701467990875\n",
            "strain 0.054354146122932434\n",
            "strain 0.07306567579507828\n",
            "strain 0.0749763697385788\n",
            "strain 0.0808200016617775\n",
            "strain 0.060241419821977615\n",
            "strain 0.05561373010277748\n",
            "strain 0.06746070086956024\n",
            "strain 0.07910904288291931\n",
            "strain 0.06458082795143127\n",
            "strain 0.054581377655267715\n",
            "strain 0.06521354615688324\n",
            "strain 0.07564318925142288\n",
            "strain 0.06306777149438858\n",
            "strain 0.07471567392349243\n",
            "strain 0.10065580904483795\n",
            "strain 0.07096575200557709\n",
            "strain 0.06354671716690063\n",
            "strain 0.08620618283748627\n",
            "strain 0.07569395750761032\n",
            "strain 0.05281933769583702\n",
            "strain 0.09879162162542343\n",
            "strain 0.08658342808485031\n",
            "strain 0.06902787834405899\n",
            "strain 0.07198970764875412\n",
            "strain 0.06142761930823326\n",
            "strain 0.0651533231139183\n",
            "strain 0.06088476628065109\n",
            "strain 0.07503217458724976\n",
            "strain 0.06569115072488785\n",
            "classify 2.2125244140625\n",
            "classify 2.201171875\n",
            "classify 2.2799072265625\n",
            "classify 2.208251953125\n",
            "classify 2.2374267578125\n",
            "classify 2.236328125\n",
            "classify 2.3065185546875\n",
            "classify 2.2186279296875\n",
            "classify 2.2391357421875\n",
            "classify 2.196533203125\n",
            "classify 2.28369140625\n",
            "0.140625\n",
            "0.125\n",
            "0.125\n",
            "0.1875\n",
            "0.21875\n",
            "0.203125\n",
            "0.125\n",
            "0.15625\n",
            "0.203125\n",
            "0.25\n",
            "0.078125\n",
            "107\n",
            "strain 0.07209941744804382\n",
            "strain 0.05927800014615059\n",
            "strain 0.059103913605213165\n",
            "strain 0.07838475704193115\n",
            "strain 0.07538213580846786\n",
            "strain 0.07003463059663773\n",
            "strain 0.056952815502882004\n",
            "strain 0.0695885419845581\n",
            "strain 0.07193170487880707\n",
            "strain 0.07002235949039459\n",
            "strain 0.0687597393989563\n",
            "strain 0.07286102324724197\n",
            "strain 0.07524202018976212\n",
            "strain 0.07412100583314896\n",
            "strain 0.0734490305185318\n",
            "strain 0.06226806715130806\n",
            "strain 0.07875722646713257\n",
            "strain 0.06441887468099594\n",
            "strain 0.0739535465836525\n",
            "strain 0.08118801563978195\n",
            "strain 0.07691635191440582\n",
            "strain 0.07626330852508545\n",
            "strain 0.1073417216539383\n",
            "strain 0.07684953510761261\n",
            "strain 0.06931507587432861\n",
            "strain 0.07922825217247009\n",
            "strain 0.0699375569820404\n",
            "strain 0.07379700243473053\n",
            "strain 0.06455174088478088\n",
            "strain 0.07002338767051697\n",
            "strain 0.06268094480037689\n",
            "strain 0.07531508058309555\n",
            "strain 0.08240146189928055\n",
            "strain 0.06216220185160637\n",
            "strain 0.08473515510559082\n",
            "strain 0.0811883732676506\n",
            "strain 0.07596210390329361\n",
            "strain 0.0751975029706955\n",
            "strain 0.07669038325548172\n",
            "strain 0.07364074140787125\n",
            "strain 0.06638342887163162\n",
            "strain 0.07849279046058655\n",
            "strain 0.07439274340867996\n",
            "strain 0.07150910049676895\n",
            "strain 0.07553459703922272\n",
            "strain 0.08346235752105713\n",
            "strain 0.07199914753437042\n",
            "strain 0.05181106552481651\n",
            "strain 0.06265868991613388\n",
            "strain 0.07069313526153564\n",
            "strain 0.061339426785707474\n",
            "classify 2.2607421875\n",
            "classify 2.242919921875\n",
            "classify 2.3028564453125\n",
            "classify 2.1890869140625\n",
            "classify 2.2445068359375\n",
            "classify 2.2515869140625\n",
            "classify 2.2735595703125\n",
            "classify 2.2479248046875\n",
            "classify 2.296142578125\n",
            "classify 2.26513671875\n",
            "classify 2.267333984375\n",
            "0.1875\n",
            "0.171875\n",
            "0.265625\n",
            "0.078125\n",
            "0.15625\n",
            "0.09375\n",
            "0.203125\n",
            "0.125\n",
            "0.21875\n",
            "0.21875\n",
            "0.140625\n",
            "108\n",
            "strain 0.07900168001651764\n",
            "strain 0.06980429589748383\n",
            "strain 0.08640661090612411\n",
            "strain 0.0653468668460846\n",
            "strain 0.06833531707525253\n",
            "strain 0.08251255750656128\n",
            "strain 0.06171397492289543\n",
            "strain 0.07007087767124176\n",
            "strain 0.07033593207597733\n",
            "strain 0.08106397837400436\n",
            "strain 0.057779062539339066\n",
            "strain 0.08946877717971802\n",
            "strain 0.07088286429643631\n",
            "strain 0.052890632301568985\n",
            "strain 0.07397971302270889\n",
            "strain 0.061688132584095\n",
            "strain 0.06437365710735321\n",
            "strain 0.06986565887928009\n",
            "strain 0.07622896134853363\n",
            "strain 0.07680504024028778\n",
            "strain 0.07099243253469467\n",
            "strain 0.07089197635650635\n",
            "strain 0.06835062801837921\n",
            "strain 0.07285306602716446\n",
            "strain 0.05676620453596115\n",
            "strain 0.07085084915161133\n",
            "strain 0.07001747190952301\n",
            "strain 0.07743518799543381\n",
            "strain 0.07049998641014099\n",
            "strain 0.06561378389596939\n",
            "strain 0.06332426518201828\n",
            "strain 0.06267141550779343\n",
            "strain 0.0691903829574585\n",
            "strain 0.06720153242349625\n",
            "strain 0.07136938720941544\n",
            "strain 0.08958534896373749\n",
            "strain 0.08578357100486755\n",
            "strain 0.05948705971240997\n",
            "strain 0.0768052190542221\n",
            "strain 0.07227369397878647\n",
            "strain 0.05459078401327133\n",
            "strain 0.06661171466112137\n",
            "strain 0.07139047980308533\n",
            "strain 0.06828738749027252\n",
            "strain 0.09922783076763153\n",
            "strain 0.06343118846416473\n",
            "strain 0.07999292016029358\n",
            "strain 0.07358281314373016\n",
            "strain 0.07030197978019714\n",
            "strain 0.08623506873846054\n",
            "strain 0.07438724488019943\n",
            "classify 2.2664794921875\n",
            "classify 2.2347412109375\n",
            "classify 2.2569580078125\n",
            "classify 2.2645263671875\n",
            "classify 2.2186279296875\n",
            "classify 2.1953125\n",
            "classify 2.2489013671875\n",
            "classify 2.2352294921875\n",
            "classify 2.2457275390625\n",
            "classify 2.2752685546875\n",
            "classify 2.2115478515625\n",
            "0.140625\n",
            "0.15625\n",
            "0.234375\n",
            "0.171875\n",
            "0.125\n",
            "0.109375\n",
            "0.15625\n",
            "0.09375\n",
            "0.140625\n",
            "0.15625\n",
            "0.125\n",
            "109\n",
            "strain 0.08044485747814178\n",
            "strain 0.05219804495573044\n",
            "strain 0.06814605742692947\n",
            "strain 0.07711014896631241\n",
            "strain 0.06440304219722748\n",
            "strain 0.08245979249477386\n",
            "strain 0.06268370151519775\n",
            "strain 0.05840935930609703\n",
            "strain 0.06189494580030441\n",
            "strain 0.07514367997646332\n",
            "strain 0.06909419596195221\n",
            "strain 0.07975256443023682\n",
            "strain 0.06767306476831436\n",
            "strain 0.0715789794921875\n",
            "strain 0.05792727693915367\n",
            "strain 0.07906419783830643\n",
            "strain 0.0850241407752037\n",
            "strain 0.0786365494132042\n",
            "strain 0.07830467820167542\n",
            "strain 0.07471184432506561\n",
            "strain 0.08778926730155945\n",
            "strain 0.06790302693843842\n",
            "strain 0.05174439772963524\n",
            "strain 0.0653216689825058\n",
            "strain 0.09306231141090393\n",
            "strain 0.09137027710676193\n",
            "strain 0.05929317697882652\n",
            "strain 0.07627762854099274\n",
            "strain 0.06927645206451416\n",
            "strain 0.08247622847557068\n",
            "strain 0.09947711229324341\n",
            "strain 0.07364293187856674\n",
            "strain 0.05506707727909088\n",
            "strain 0.06839501857757568\n",
            "strain 0.07052987068891525\n",
            "strain 0.06212949752807617\n",
            "strain 0.07330494374036789\n",
            "strain 0.07610447704792023\n",
            "strain 0.06782381236553192\n",
            "strain 0.06844799220561981\n",
            "strain 0.07186559587717056\n",
            "strain 0.06714188307523727\n",
            "strain 0.05574578046798706\n",
            "strain 0.06392809003591537\n",
            "strain 0.087679922580719\n",
            "strain 0.071050725877285\n",
            "strain 0.07158844918012619\n",
            "strain 0.06437399238348007\n",
            "strain 0.0664922371506691\n",
            "strain 0.07084645330905914\n",
            "strain 0.08736767619848251\n",
            "classify 2.2791748046875\n",
            "classify 2.240478515625\n",
            "classify 2.2518310546875\n",
            "classify 2.2147216796875\n",
            "classify 2.249755859375\n",
            "classify 2.28271484375\n",
            "classify 2.2359619140625\n",
            "classify 2.2127685546875\n",
            "classify 2.2008056640625\n",
            "classify 2.2518310546875\n",
            "classify 2.22412109375\n",
            "0.171875\n",
            "0.125\n",
            "0.21875\n",
            "0.171875\n",
            "0.09375\n",
            "0.140625\n",
            "0.125\n",
            "0.171875\n",
            "0.234375\n",
            "0.21875\n",
            "0.1875\n",
            "110\n",
            "strain 0.08818166702985764\n",
            "strain 0.08089067041873932\n",
            "strain 0.0720207691192627\n",
            "strain 0.07111885398626328\n",
            "strain 0.06745144724845886\n",
            "strain 0.06187311187386513\n",
            "strain 0.07196316123008728\n",
            "strain 0.06539136916399002\n",
            "strain 0.06551668792963028\n",
            "strain 0.07562144845724106\n",
            "strain 0.07894335687160492\n",
            "strain 0.06831223517656326\n",
            "strain 0.08846809715032578\n",
            "strain 0.06074513494968414\n",
            "strain 0.06912574172019958\n",
            "strain 0.061662621796131134\n",
            "strain 0.08070139586925507\n",
            "strain 0.06944567710161209\n",
            "strain 0.0652831420302391\n",
            "strain 0.07330160588026047\n",
            "strain 0.056646525859832764\n",
            "strain 0.07567624002695084\n",
            "strain 0.0785578116774559\n",
            "strain 0.07978624105453491\n",
            "strain 0.09567319601774216\n",
            "strain 0.08331309258937836\n",
            "strain 0.09266480058431625\n",
            "strain 0.07487525045871735\n",
            "strain 0.07126174867153168\n",
            "strain 0.07664757966995239\n",
            "strain 0.07295910269021988\n",
            "strain 0.0766180157661438\n",
            "strain 0.06782536953687668\n",
            "strain 0.0944904312491417\n",
            "strain 0.07152317464351654\n",
            "strain 0.06033184751868248\n",
            "strain 0.10058655589818954\n",
            "strain 0.09413614124059677\n",
            "strain 0.09018608182668686\n",
            "strain 0.06388837844133377\n",
            "strain 0.08387453854084015\n",
            "strain 0.09496377408504486\n",
            "strain 0.06968361884355545\n",
            "strain 0.10074341297149658\n",
            "strain 0.06732994318008423\n",
            "strain 0.08683421462774277\n",
            "strain 0.0736844390630722\n",
            "strain 0.09060174226760864\n",
            "strain 0.10022302716970444\n",
            "strain 0.06913840770721436\n",
            "strain 0.07457621395587921\n",
            "classify 2.2486572265625\n",
            "classify 2.265869140625\n",
            "classify 2.247802734375\n",
            "classify 2.28466796875\n",
            "classify 2.2603759765625\n",
            "classify 2.258544921875\n",
            "classify 2.2330322265625\n",
            "classify 2.28662109375\n",
            "classify 2.3231201171875\n",
            "classify 2.1943359375\n",
            "classify 2.216064453125\n",
            "0.171875\n",
            "0.171875\n",
            "0.21875\n",
            "0.09375\n",
            "0.1875\n",
            "0.140625\n",
            "0.140625\n",
            "0.15625\n",
            "0.1875\n",
            "0.171875\n",
            "0.125\n",
            "111\n",
            "strain 0.06868140399456024\n",
            "strain 0.07105157524347305\n",
            "strain 0.08370070159435272\n",
            "strain 0.054196763783693314\n",
            "strain 0.06503742933273315\n",
            "strain 0.06119786575436592\n",
            "strain 0.07595125585794449\n",
            "strain 0.07666152715682983\n",
            "strain 0.06537988781929016\n",
            "strain 0.0672302097082138\n",
            "strain 0.09390415996313095\n",
            "strain 0.06399081647396088\n",
            "strain 0.08284298330545425\n",
            "strain 0.07619360834360123\n",
            "strain 0.06967619061470032\n",
            "strain 0.06311988830566406\n",
            "strain 0.06302761286497116\n",
            "strain 0.0812583640217781\n",
            "strain 0.05828952044248581\n",
            "strain 0.09687509387731552\n",
            "strain 0.1081327348947525\n",
            "strain 0.06373557448387146\n",
            "strain 0.07719442248344421\n",
            "strain 0.07575970143079758\n",
            "strain 0.07574714720249176\n",
            "strain 0.07170171290636063\n",
            "strain 0.06786487251520157\n",
            "strain 0.06518466025590897\n",
            "strain 0.06809400767087936\n",
            "strain 0.07189103215932846\n",
            "strain 0.07264281809329987\n",
            "strain 0.0708022341132164\n",
            "strain 0.07201136648654938\n",
            "strain 0.061678942292928696\n",
            "strain 0.06291849166154861\n",
            "strain 0.07205280661582947\n",
            "strain 0.06687655299901962\n",
            "strain 0.06715339422225952\n",
            "strain 0.057958025485277176\n",
            "strain 0.06271969527006149\n",
            "strain 0.07412886619567871\n",
            "strain 0.08813079446554184\n",
            "strain 0.07761824131011963\n",
            "strain 0.0735974907875061\n",
            "strain 0.07010818272829056\n",
            "strain 0.06862924993038177\n",
            "strain 0.07307334989309311\n",
            "strain 0.0731496512889862\n",
            "strain 0.0779370665550232\n",
            "strain 0.05330030247569084\n",
            "strain 0.07928436249494553\n",
            "classify 2.243896484375\n",
            "classify 2.248779296875\n",
            "classify 2.298583984375\n",
            "classify 2.283935546875\n",
            "classify 2.2623291015625\n",
            "classify 2.2777099609375\n",
            "classify 2.255126953125\n",
            "classify 2.2686767578125\n",
            "classify 2.2381591796875\n",
            "classify 2.226806640625\n",
            "classify 2.29443359375\n",
            "0.078125\n",
            "0.140625\n",
            "0.1875\n",
            "0.125\n",
            "0.15625\n",
            "0.15625\n",
            "0.09375\n",
            "0.15625\n",
            "0.125\n",
            "0.171875\n",
            "0.09375\n",
            "112\n",
            "strain 0.07877655327320099\n",
            "strain 0.06515970081090927\n",
            "strain 0.0959867462515831\n",
            "strain 0.058529727160930634\n",
            "strain 0.06759225577116013\n",
            "strain 0.06855189055204391\n",
            "strain 0.07131610065698624\n",
            "strain 0.07757967710494995\n",
            "strain 0.06931071728467941\n",
            "strain 0.07113929092884064\n",
            "strain 0.08264468610286713\n",
            "strain 0.08006464689970016\n",
            "strain 0.060058847069740295\n",
            "strain 0.07336989045143127\n",
            "strain 0.0692288726568222\n",
            "strain 0.05434797704219818\n",
            "strain 0.06830888986587524\n",
            "strain 0.06024445593357086\n",
            "strain 0.06466798484325409\n",
            "strain 0.07853807508945465\n",
            "strain 0.09045672416687012\n",
            "strain 0.0800291895866394\n",
            "strain 0.08495151251554489\n",
            "strain 0.0811803787946701\n",
            "strain 0.07251647859811783\n",
            "strain 0.0721704363822937\n",
            "strain 0.08428668975830078\n",
            "strain 0.06289926171302795\n",
            "strain 0.0757477656006813\n",
            "strain 0.08842095732688904\n",
            "strain 0.08027558773756027\n",
            "strain 0.07192007452249527\n",
            "strain 0.08007500320672989\n",
            "strain 0.07978996634483337\n",
            "strain 0.08963035047054291\n",
            "strain 0.06395523250102997\n",
            "strain 0.07125680148601532\n",
            "strain 0.08981192857027054\n",
            "strain 0.07374651730060577\n",
            "strain 0.08744918555021286\n",
            "strain 0.07620172947645187\n",
            "strain 0.07221619039773941\n",
            "strain 0.06105418875813484\n",
            "strain 0.07835134863853455\n",
            "strain 0.062432512640953064\n",
            "strain 0.06513786315917969\n",
            "strain 0.07541888952255249\n",
            "strain 0.06784704327583313\n",
            "strain 0.07944485545158386\n",
            "strain 0.08726684749126434\n",
            "strain 0.07453157007694244\n",
            "classify 2.3282470703125\n",
            "classify 2.2142333984375\n",
            "classify 2.263427734375\n",
            "classify 2.203857421875\n",
            "classify 2.26025390625\n",
            "classify 2.2481689453125\n",
            "classify 2.2188720703125\n",
            "classify 2.2718505859375\n",
            "classify 2.2242431640625\n",
            "classify 2.2532958984375\n",
            "classify 2.1787109375\n",
            "0.140625\n",
            "0.171875\n",
            "0.140625\n",
            "0.078125\n",
            "0.078125\n",
            "0.125\n",
            "0.0625\n",
            "0.171875\n",
            "0.203125\n",
            "0.140625\n",
            "0.046875\n",
            "113\n",
            "strain 0.08035777509212494\n",
            "strain 0.07184771448373795\n",
            "strain 0.06256785988807678\n",
            "strain 0.07220550626516342\n",
            "strain 0.09933378547430038\n",
            "strain 0.07932920753955841\n",
            "strain 0.07543090730905533\n",
            "strain 0.06774909049272537\n",
            "strain 0.08658239245414734\n",
            "strain 0.07057283073663712\n",
            "strain 0.07808974385261536\n",
            "strain 0.07848962396383286\n",
            "strain 0.07648459076881409\n",
            "strain 0.06979060173034668\n",
            "strain 0.08449941873550415\n",
            "strain 0.0695878192782402\n",
            "strain 0.07122039794921875\n",
            "strain 0.06767312437295914\n",
            "strain 0.06045087054371834\n",
            "strain 0.0759267807006836\n",
            "strain 0.08759988844394684\n",
            "strain 0.07382234930992126\n",
            "strain 0.0668371394276619\n",
            "strain 0.07569307833909988\n",
            "strain 0.0790790393948555\n",
            "strain 0.07152349501848221\n",
            "strain 0.07627546787261963\n",
            "strain 0.07494202256202698\n",
            "strain 0.07540000975131989\n",
            "strain 0.07385788857936859\n",
            "strain 0.067515529692173\n",
            "strain 0.08203048259019852\n",
            "strain 0.0751747190952301\n",
            "strain 0.057533398270606995\n",
            "strain 0.07620322704315186\n",
            "strain 0.0764085054397583\n",
            "strain 0.07523199915885925\n",
            "strain 0.07632403820753098\n",
            "strain 0.06583453714847565\n",
            "strain 0.06734748929738998\n",
            "strain 0.06331533938646317\n",
            "strain 0.06465531885623932\n",
            "strain 0.0680442601442337\n",
            "strain 0.07472796738147736\n",
            "strain 0.058853257447481155\n",
            "strain 0.08413398265838623\n",
            "strain 0.06157704070210457\n",
            "strain 0.07230732589960098\n",
            "strain 0.06393150985240936\n",
            "strain 0.07184737175703049\n",
            "strain 0.07763040065765381\n",
            "classify 2.19091796875\n",
            "classify 2.2923583984375\n",
            "classify 2.2493896484375\n",
            "classify 2.24560546875\n",
            "classify 2.2354736328125\n",
            "classify 2.2723388671875\n",
            "classify 2.2857666015625\n",
            "classify 2.2713623046875\n",
            "classify 2.233642578125\n",
            "classify 2.31201171875\n",
            "classify 2.30908203125\n",
            "0.15625\n",
            "0.15625\n",
            "0.125\n",
            "0.171875\n",
            "0.1875\n",
            "0.078125\n",
            "0.171875\n",
            "0.109375\n",
            "0.15625\n",
            "0.125\n",
            "0.203125\n",
            "114\n",
            "strain 0.06913882493972778\n",
            "strain 0.060838211327791214\n",
            "strain 0.07563167065382004\n",
            "strain 0.06748403608798981\n",
            "strain 0.07013116776943207\n",
            "strain 0.07265819609165192\n",
            "strain 0.09124328941106796\n",
            "strain 0.06645660847425461\n",
            "strain 0.08611072599887848\n",
            "strain 0.0716361328959465\n",
            "strain 0.06782887130975723\n",
            "strain 0.06658530980348587\n",
            "strain 0.06358359009027481\n",
            "strain 0.06735958158969879\n",
            "strain 0.0727933794260025\n",
            "strain 0.06781833618879318\n",
            "strain 0.063909612596035\n",
            "strain 0.07009973376989365\n",
            "strain 0.0661405622959137\n",
            "strain 0.06541436165571213\n",
            "strain 0.06314997375011444\n",
            "strain 0.061242420226335526\n",
            "strain 0.1013912782073021\n",
            "strain 0.06141353398561478\n",
            "strain 0.07539212703704834\n",
            "strain 0.08557973057031631\n",
            "strain 0.07956977933645248\n",
            "strain 0.06925496459007263\n",
            "strain 0.07125431299209595\n",
            "strain 0.08190754801034927\n",
            "strain 0.058140549808740616\n",
            "strain 0.08121815323829651\n",
            "strain 0.08364890515804291\n",
            "strain 0.08822880685329437\n",
            "strain 0.0795738622546196\n",
            "strain 0.07993842661380768\n",
            "strain 0.08062518388032913\n",
            "strain 0.06312836706638336\n",
            "strain 0.08403478562831879\n",
            "strain 0.08384817838668823\n",
            "strain 0.05890999361872673\n",
            "strain 0.06180354580283165\n",
            "strain 0.07748575508594513\n",
            "strain 0.08611124753952026\n",
            "strain 0.07722097635269165\n",
            "strain 0.06974858045578003\n",
            "strain 0.06658605486154556\n",
            "strain 0.06455085426568985\n",
            "strain 0.0587124228477478\n",
            "strain 0.07232657074928284\n",
            "strain 0.06804747134447098\n",
            "classify 2.2412109375\n",
            "classify 2.1964111328125\n",
            "classify 2.27734375\n",
            "classify 2.2979736328125\n",
            "classify 2.298828125\n",
            "classify 2.2686767578125\n",
            "classify 2.3057861328125\n",
            "classify 2.248291015625\n",
            "classify 2.2100830078125\n",
            "classify 2.2459716796875\n",
            "classify 2.2667236328125\n",
            "0.140625\n",
            "0.109375\n",
            "0.171875\n",
            "0.1875\n",
            "0.234375\n",
            "0.171875\n",
            "0.171875\n",
            "0.140625\n",
            "0.21875\n",
            "0.15625\n",
            "0.1875\n",
            "115\n",
            "strain 0.06430468708276749\n",
            "strain 0.08212705701589584\n",
            "strain 0.06621832400560379\n",
            "strain 0.08567579090595245\n",
            "strain 0.08766059577465057\n",
            "strain 0.07083296775817871\n",
            "strain 0.05792813375592232\n",
            "strain 0.08035926520824432\n",
            "strain 0.07739715278148651\n",
            "strain 0.07486413419246674\n",
            "strain 0.06887103617191315\n",
            "strain 0.08822992444038391\n",
            "strain 0.0979267954826355\n",
            "strain 0.06847657263278961\n",
            "strain 0.05962742120027542\n",
            "strain 0.08567877858877182\n",
            "strain 0.07909447699785233\n",
            "strain 0.11766473948955536\n",
            "strain 0.07925322651863098\n",
            "strain 0.08570772409439087\n",
            "strain 0.07671593874692917\n",
            "strain 0.06854596734046936\n",
            "strain 0.06590402871370316\n",
            "strain 0.07423549890518188\n",
            "strain 0.07176267355680466\n",
            "strain 0.07513271272182465\n",
            "strain 0.06700614839792252\n",
            "strain 0.06522324681282043\n",
            "strain 0.08682787418365479\n",
            "strain 0.09478583931922913\n",
            "strain 0.09574981033802032\n",
            "strain 0.07534287869930267\n",
            "strain 0.08337509632110596\n",
            "strain 0.08373244851827621\n",
            "strain 0.06992361694574356\n",
            "strain 0.08149338513612747\n",
            "strain 0.08431471884250641\n",
            "strain 0.06786283850669861\n",
            "strain 0.06699636578559875\n",
            "strain 0.07674963027238846\n",
            "strain 0.06877563893795013\n",
            "strain 0.06483448296785355\n",
            "strain 0.06851249188184738\n",
            "strain 0.06509578227996826\n",
            "strain 0.07465066015720367\n",
            "strain 0.061674073338508606\n",
            "strain 0.09102021902799606\n",
            "strain 0.06519308686256409\n",
            "strain 0.07498996704816818\n",
            "strain 0.08375319093465805\n",
            "strain 0.0723523274064064\n",
            "classify 2.282958984375\n",
            "classify 2.2286376953125\n",
            "classify 2.3096923828125\n",
            "classify 2.2972412109375\n",
            "classify 2.283447265625\n",
            "classify 2.23876953125\n",
            "classify 2.2830810546875\n",
            "classify 2.2313232421875\n",
            "classify 2.20703125\n",
            "classify 2.2789306640625\n",
            "classify 2.197998046875\n",
            "0.234375\n",
            "0.109375\n",
            "0.109375\n",
            "0.203125\n",
            "0.125\n",
            "0.203125\n",
            "0.125\n",
            "0.125\n",
            "0.234375\n",
            "0.125\n",
            "0.1875\n",
            "116\n",
            "strain 0.11475543677806854\n",
            "strain 0.0666179358959198\n",
            "strain 0.07557865977287292\n",
            "strain 0.059232380241155624\n",
            "strain 0.06433387100696564\n",
            "strain 0.06350575387477875\n",
            "strain 0.067329540848732\n",
            "strain 0.06737785041332245\n",
            "strain 0.06402008980512619\n",
            "strain 0.071711465716362\n",
            "strain 0.06345551460981369\n",
            "strain 0.08848248422145844\n",
            "strain 0.074000783264637\n",
            "strain 0.10085201263427734\n",
            "strain 0.06617353856563568\n",
            "strain 0.08108411729335785\n",
            "strain 0.09635189175605774\n",
            "strain 0.07987536489963531\n",
            "strain 0.06069876253604889\n",
            "strain 0.08041086792945862\n",
            "strain 0.09357921779155731\n",
            "strain 0.07607021927833557\n",
            "strain 0.06756053119897842\n",
            "strain 0.07084278017282486\n",
            "strain 0.07813040167093277\n",
            "strain 0.06443651765584946\n",
            "strain 0.07235442101955414\n",
            "strain 0.0840374007821083\n",
            "strain 0.10168975591659546\n",
            "strain 0.05719592794775963\n",
            "strain 0.07739919424057007\n",
            "strain 0.07501505315303802\n",
            "strain 0.07715547829866409\n",
            "strain 0.06515798717737198\n",
            "strain 0.07469725608825684\n",
            "strain 0.06876447796821594\n",
            "strain 0.07624135911464691\n",
            "strain 0.07581928372383118\n",
            "strain 0.07647490501403809\n",
            "strain 0.0744335949420929\n",
            "strain 0.06883641332387924\n",
            "strain 0.09046425670385361\n",
            "strain 0.0771644115447998\n",
            "strain 0.07814623415470123\n",
            "strain 0.06651972979307175\n",
            "strain 0.09106038510799408\n",
            "strain 0.08526547998189926\n",
            "strain 0.07151748985052109\n",
            "strain 0.07526206970214844\n",
            "strain 0.07319561392068863\n",
            "strain 0.069085992872715\n",
            "classify 2.2235107421875\n",
            "classify 2.258544921875\n",
            "classify 2.193115234375\n",
            "classify 2.2891845703125\n",
            "classify 2.28173828125\n",
            "classify 2.2557373046875\n",
            "classify 2.2734375\n",
            "classify 2.2862548828125\n",
            "classify 2.28369140625\n",
            "classify 2.2305908203125\n",
            "classify 2.263671875\n",
            "0.140625\n",
            "0.078125\n",
            "0.140625\n",
            "0.21875\n",
            "0.171875\n",
            "0.078125\n",
            "0.078125\n",
            "0.09375\n",
            "0.09375\n",
            "0.109375\n",
            "0.15625\n",
            "117\n",
            "strain 0.06528041511774063\n",
            "strain 0.07193446159362793\n",
            "strain 0.06039493530988693\n",
            "strain 0.08509879559278488\n",
            "strain 0.07777035981416702\n",
            "strain 0.08154788613319397\n",
            "strain 0.08080574125051498\n",
            "strain 0.06616371870040894\n",
            "strain 0.07631272822618484\n",
            "strain 0.06435418128967285\n",
            "strain 0.11288195103406906\n",
            "strain 0.07391159981489182\n",
            "strain 0.07223866879940033\n",
            "strain 0.0734698474407196\n",
            "strain 0.09592445194721222\n",
            "strain 0.07624313980340958\n",
            "strain 0.07810279726982117\n",
            "strain 0.08714862912893295\n",
            "strain 0.07289576530456543\n",
            "strain 0.07910866290330887\n",
            "strain 0.08206477761268616\n",
            "strain 0.07732363790273666\n",
            "strain 0.08270826935768127\n",
            "strain 0.10239176452159882\n",
            "strain 0.07529494166374207\n",
            "strain 0.06477277725934982\n",
            "strain 0.08956883102655411\n",
            "strain 0.08736681193113327\n",
            "strain 0.07410329580307007\n",
            "strain 0.09402187168598175\n",
            "strain 0.08845027536153793\n",
            "strain 0.0623629093170166\n",
            "strain 0.058756571263074875\n",
            "strain 0.07362918555736542\n",
            "strain 0.08055844902992249\n",
            "strain 0.07599364221096039\n",
            "strain 0.06700131297111511\n",
            "strain 0.09771792590618134\n",
            "strain 0.09402790665626526\n",
            "strain 0.07543938606977463\n",
            "strain 0.10145492851734161\n",
            "strain 0.0812283530831337\n",
            "strain 0.08654733747243881\n",
            "strain 0.0648701936006546\n",
            "strain 0.06990104913711548\n",
            "strain 0.072265625\n",
            "strain 0.08847641199827194\n",
            "strain 0.06696262955665588\n",
            "strain 0.08583074808120728\n",
            "strain 0.07252244651317596\n",
            "strain 0.06710805743932724\n",
            "classify 2.29736328125\n",
            "classify 2.3153076171875\n",
            "classify 2.2459716796875\n",
            "classify 2.2340087890625\n",
            "classify 2.2418212890625\n",
            "classify 2.2291259765625\n",
            "classify 2.2734375\n",
            "classify 2.2784423828125\n",
            "classify 2.260986328125\n",
            "classify 2.304443359375\n",
            "classify 2.311279296875\n",
            "0.078125\n",
            "0.171875\n",
            "0.171875\n",
            "0.09375\n",
            "0.125\n",
            "0.078125\n",
            "0.140625\n",
            "0.15625\n",
            "0.078125\n",
            "0.109375\n",
            "0.109375\n",
            "118\n",
            "strain 0.0743003711104393\n",
            "strain 0.08110691606998444\n",
            "strain 0.07920585572719574\n",
            "strain 0.07163229584693909\n",
            "strain 0.06569436192512512\n",
            "strain 0.06331399828195572\n",
            "strain 0.0814177542924881\n",
            "strain 0.08174454420804977\n",
            "strain 0.06204325705766678\n",
            "strain 0.0651775449514389\n",
            "strain 0.07010487467050552\n",
            "strain 0.07634660601615906\n",
            "strain 0.06471407413482666\n",
            "strain 0.07633863389492035\n",
            "strain 0.06624661386013031\n",
            "strain 0.07033440470695496\n",
            "strain 0.07921624183654785\n",
            "strain 0.0678044855594635\n",
            "strain 0.07467800378799438\n",
            "strain 0.08278375118970871\n",
            "strain 0.07198682427406311\n",
            "strain 0.07010810822248459\n",
            "strain 0.06528546661138535\n",
            "strain 0.0782751515507698\n",
            "strain 0.11206379532814026\n",
            "strain 0.06981395184993744\n",
            "strain 0.07802359759807587\n",
            "strain 0.0718550756573677\n",
            "strain 0.061666831374168396\n",
            "strain 0.07233321666717529\n",
            "strain 0.10054539889097214\n",
            "strain 0.07316214591264725\n",
            "strain 0.07206324487924576\n",
            "strain 0.07189492136240005\n",
            "strain 0.0878203958272934\n",
            "strain 0.08501122891902924\n",
            "strain 0.07389305531978607\n",
            "strain 0.07404477149248123\n",
            "strain 0.06229128688573837\n",
            "strain 0.07232635468244553\n",
            "strain 0.07393068820238113\n",
            "strain 0.07348883152008057\n",
            "strain 0.0746135339140892\n",
            "strain 0.0717158392071724\n",
            "strain 0.08577457070350647\n",
            "strain 0.0583379790186882\n",
            "strain 0.07780740410089493\n",
            "strain 0.061232395470142365\n",
            "strain 0.08725636452436447\n",
            "strain 0.08064796030521393\n",
            "strain 0.075030617415905\n",
            "classify 2.275390625\n",
            "classify 2.295654296875\n",
            "classify 2.22998046875\n",
            "classify 2.2684326171875\n",
            "classify 2.3125\n",
            "classify 2.23095703125\n",
            "classify 2.230224609375\n",
            "classify 2.251708984375\n",
            "classify 2.2391357421875\n",
            "classify 2.2733154296875\n",
            "classify 2.27587890625\n",
            "0.125\n",
            "0.078125\n",
            "0.140625\n",
            "0.1875\n",
            "0.125\n",
            "0.28125\n",
            "0.078125\n",
            "0.15625\n",
            "0.171875\n",
            "0.140625\n",
            "0.171875\n",
            "119\n",
            "strain 0.0715591087937355\n",
            "strain 0.09915709495544434\n",
            "strain 0.08985823392868042\n",
            "strain 0.06824401766061783\n",
            "strain 0.07136493921279907\n",
            "strain 0.07419560104608536\n",
            "strain 0.058475758880376816\n",
            "strain 0.062390003353357315\n",
            "strain 0.07890520989894867\n",
            "strain 0.07374356687068939\n",
            "strain 0.07658997923135757\n",
            "strain 0.08251601457595825\n",
            "strain 0.06578672677278519\n",
            "strain 0.07718104869127274\n",
            "strain 0.08054652810096741\n",
            "strain 0.09543049335479736\n",
            "strain 0.06961104273796082\n",
            "strain 0.07581332325935364\n",
            "strain 0.07626397907733917\n",
            "strain 0.08813732862472534\n",
            "strain 0.07739896327257156\n",
            "strain 0.06908828765153885\n",
            "strain 0.07986333966255188\n",
            "strain 0.07663338631391525\n",
            "strain 0.07418114691972733\n",
            "strain 0.08482114225625992\n",
            "strain 0.07109661400318146\n",
            "strain 0.07676153630018234\n",
            "strain 0.08794094622135162\n",
            "strain 0.07752567529678345\n",
            "strain 0.08745962381362915\n",
            "strain 0.08025600016117096\n",
            "strain 0.12267901748418808\n",
            "strain 0.06002264469861984\n",
            "strain 0.06518612802028656\n",
            "strain 0.07835280895233154\n",
            "strain 0.0876268595457077\n",
            "strain 0.07593949139118195\n",
            "strain 0.06798077374696732\n",
            "strain 0.06337461620569229\n",
            "strain 0.06698332726955414\n",
            "strain 0.07336755841970444\n",
            "strain 0.0836368203163147\n",
            "strain 0.07098829746246338\n",
            "strain 0.07667971402406693\n",
            "strain 0.08804068714380264\n",
            "strain 0.07752866297960281\n",
            "strain 0.08211007714271545\n",
            "strain 0.07252799719572067\n",
            "strain 0.07076112926006317\n",
            "strain 0.06159473955631256\n",
            "classify 2.2366943359375\n",
            "classify 2.188720703125\n",
            "classify 2.277099609375\n",
            "classify 2.317626953125\n",
            "classify 2.220947265625\n",
            "classify 2.2713623046875\n",
            "classify 2.198486328125\n",
            "classify 2.2264404296875\n",
            "classify 2.251708984375\n",
            "classify 2.28857421875\n",
            "classify 2.2691650390625\n",
            "0.1875\n",
            "0.15625\n",
            "0.125\n",
            "0.09375\n",
            "0.125\n",
            "0.203125\n",
            "0.1875\n",
            "0.1875\n",
            "0.1875\n",
            "0.125\n",
            "0.1875\n",
            "120\n",
            "strain 0.07320402562618256\n",
            "strain 0.06762298196554184\n",
            "strain 0.08103536069393158\n",
            "strain 0.07232552021741867\n",
            "strain 0.07324777543544769\n",
            "strain 0.06191381439566612\n",
            "strain 0.08927619457244873\n",
            "strain 0.08577682822942734\n",
            "strain 0.06874096393585205\n",
            "strain 0.0825268030166626\n",
            "strain 0.076462522149086\n",
            "strain 0.0884663388133049\n",
            "strain 0.10665960609912872\n",
            "strain 0.057625822722911835\n",
            "strain 0.0757574513554573\n",
            "strain 0.07765185832977295\n",
            "strain 0.07792285084724426\n",
            "strain 0.06373324990272522\n",
            "strain 0.07097404450178146\n",
            "strain 0.06935546547174454\n",
            "strain 0.06722328066825867\n",
            "strain 0.09097735583782196\n",
            "strain 0.07543008029460907\n",
            "strain 0.08379264920949936\n",
            "strain 0.07997133582830429\n",
            "strain 0.0757211297750473\n",
            "strain 0.10044529289007187\n",
            "strain 0.07838120311498642\n",
            "strain 0.071426622569561\n",
            "strain 0.06917844712734222\n",
            "strain 0.0868460163474083\n",
            "strain 0.080450139939785\n",
            "strain 0.06485185772180557\n",
            "strain 0.06528007984161377\n",
            "strain 0.06449049711227417\n",
            "strain 0.10408728569746017\n",
            "strain 0.12075891345739365\n",
            "strain 0.0921124666929245\n",
            "strain 0.09373757988214493\n",
            "strain 0.08678708225488663\n",
            "strain 0.07362491637468338\n",
            "strain 0.08337168395519257\n",
            "strain 0.06551185995340347\n",
            "strain 0.07156925648450851\n",
            "strain 0.08695951104164124\n",
            "strain 0.06385493278503418\n",
            "strain 0.07878142595291138\n",
            "strain 0.09968271106481552\n",
            "strain 0.05717772990465164\n",
            "strain 0.0738329142332077\n",
            "strain 0.07368002831935883\n",
            "classify 2.2113037109375\n",
            "classify 2.251953125\n",
            "classify 2.282958984375\n",
            "classify 2.2801513671875\n",
            "classify 2.277587890625\n",
            "classify 2.227294921875\n",
            "classify 2.3253173828125\n",
            "classify 2.3304443359375\n",
            "classify 2.3238525390625\n",
            "classify 2.2664794921875\n",
            "classify 2.22900390625\n",
            "0.203125\n",
            "0.15625\n",
            "0.171875\n",
            "0.1875\n",
            "0.0625\n",
            "0.15625\n",
            "0.125\n",
            "0.125\n",
            "0.109375\n",
            "0.1875\n",
            "0.234375\n",
            "121\n",
            "strain 0.06639891862869263\n",
            "strain 0.06965481489896774\n",
            "strain 0.07928586006164551\n",
            "strain 0.07839695364236832\n",
            "strain 0.06370989233255386\n",
            "strain 0.057156603783369064\n",
            "strain 0.08007611334323883\n",
            "strain 0.07698328793048859\n",
            "strain 0.0887090191245079\n",
            "strain 0.06511028856039047\n",
            "strain 0.06849397718906403\n",
            "strain 0.08504758775234222\n",
            "strain 0.11226832866668701\n",
            "strain 0.07829708606004715\n",
            "strain 0.08906346559524536\n",
            "strain 0.10687030851840973\n",
            "strain 0.07505625486373901\n",
            "strain 0.0862162858247757\n",
            "strain 0.08659671247005463\n",
            "strain 0.08267715573310852\n",
            "strain 0.07968688011169434\n",
            "strain 0.08050226420164108\n",
            "strain 0.06760480999946594\n",
            "strain 0.07653496414422989\n",
            "strain 0.08058642596006393\n",
            "strain 0.07805989682674408\n",
            "strain 0.07198823988437653\n",
            "strain 0.0706091821193695\n",
            "strain 0.08439352363348007\n",
            "strain 0.09138163179159164\n",
            "strain 0.08241428434848785\n",
            "strain 0.08158758282661438\n",
            "strain 0.08935055136680603\n",
            "strain 0.07082775235176086\n",
            "strain 0.07498228549957275\n",
            "strain 0.06572405993938446\n",
            "strain 0.06681358814239502\n",
            "strain 0.06893710047006607\n",
            "strain 0.0631934255361557\n",
            "strain 0.0767834335565567\n",
            "strain 0.09351616352796555\n",
            "strain 0.07844249159097672\n",
            "strain 0.05993727967143059\n",
            "strain 0.08350937068462372\n",
            "strain 0.07811517268419266\n",
            "strain 0.0742310956120491\n",
            "strain 0.08346547931432724\n",
            "strain 0.08515439927577972\n",
            "strain 0.0673292800784111\n",
            "strain 0.09015814960002899\n",
            "strain 0.06580030918121338\n",
            "classify 2.2540283203125\n",
            "classify 2.2811279296875\n",
            "classify 2.2705078125\n",
            "classify 2.1851806640625\n",
            "classify 2.2288818359375\n",
            "classify 2.271484375\n",
            "classify 2.2218017578125\n",
            "classify 2.2000732421875\n",
            "classify 2.226806640625\n",
            "classify 2.3076171875\n",
            "classify 2.267333984375\n",
            "0.140625\n",
            "0.109375\n",
            "0.171875\n",
            "0.15625\n",
            "0.140625\n",
            "0.0625\n",
            "0.234375\n",
            "0.3125\n",
            "0.140625\n",
            "0.125\n",
            "0.140625\n",
            "122\n",
            "strain 0.06494317948818207\n",
            "strain 0.0692080408334732\n",
            "strain 0.07402254641056061\n",
            "strain 0.06982126832008362\n",
            "strain 0.07947346568107605\n",
            "strain 0.07809331268072128\n",
            "strain 0.07369476556777954\n",
            "strain 0.08230308443307877\n",
            "strain 0.08431820571422577\n",
            "strain 0.07190727442502975\n",
            "strain 0.093560591340065\n",
            "strain 0.07238080352544785\n",
            "strain 0.08026274293661118\n",
            "strain 0.08275621384382248\n",
            "strain 0.08557415008544922\n",
            "strain 0.07466983795166016\n",
            "strain 0.07175082713365555\n",
            "strain 0.08986824005842209\n",
            "strain 0.0766950473189354\n",
            "strain 0.08924535661935806\n",
            "strain 0.08306609094142914\n",
            "strain 0.0650152638554573\n",
            "strain 0.0673067569732666\n",
            "strain 0.08332811295986176\n",
            "strain 0.07830151915550232\n",
            "strain 0.05725117400288582\n",
            "strain 0.08655595779418945\n",
            "strain 0.09158297628164291\n",
            "strain 0.07580789923667908\n",
            "strain 0.06383021920919418\n",
            "strain 0.07950383424758911\n",
            "strain 0.089535653591156\n",
            "strain 0.07169272750616074\n",
            "strain 0.0886002704501152\n",
            "strain 0.06522516906261444\n",
            "strain 0.06265868246555328\n",
            "strain 0.08298733085393906\n",
            "strain 0.08186307549476624\n",
            "strain 0.07548144459724426\n",
            "strain 0.09324009716510773\n",
            "strain 0.07216776907444\n",
            "strain 0.06735137104988098\n",
            "strain 0.07478638738393784\n",
            "strain 0.08617483824491501\n",
            "strain 0.07086475193500519\n",
            "strain 0.09383117407560349\n",
            "strain 0.07481071352958679\n",
            "strain 0.06646033376455307\n",
            "strain 0.0855538472533226\n",
            "strain 0.07946616411209106\n",
            "strain 0.06566935777664185\n",
            "classify 2.2696533203125\n",
            "classify 2.27490234375\n",
            "classify 2.349853515625\n",
            "classify 2.229736328125\n",
            "classify 2.3651123046875\n",
            "classify 2.2860107421875\n",
            "classify 2.2772216796875\n",
            "classify 2.3160400390625\n",
            "classify 2.2376708984375\n",
            "classify 2.2540283203125\n",
            "classify 2.2322998046875\n",
            "0.171875\n",
            "0.1875\n",
            "0.140625\n",
            "0.15625\n",
            "0.15625\n",
            "0.171875\n",
            "0.1875\n",
            "0.09375\n",
            "0.140625\n",
            "0.203125\n",
            "0.171875\n",
            "123\n",
            "strain 0.06844907253980637\n",
            "strain 0.07748305797576904\n",
            "strain 0.08682183921337128\n",
            "strain 0.0909409373998642\n",
            "strain 0.08349625766277313\n",
            "strain 0.1062394231557846\n",
            "strain 0.07386289536952972\n",
            "strain 0.06585570424795151\n",
            "strain 0.07907138019800186\n",
            "strain 0.08619064092636108\n",
            "strain 0.08820031583309174\n",
            "strain 0.07723142206668854\n",
            "strain 0.08626719564199448\n",
            "strain 0.07967960089445114\n",
            "strain 0.07046329230070114\n",
            "strain 0.06373482197523117\n",
            "strain 0.07771214097738266\n",
            "strain 0.0897158607840538\n",
            "strain 0.08358832448720932\n",
            "strain 0.07762181758880615\n",
            "strain 0.06544984877109528\n",
            "strain 0.06976953148841858\n",
            "strain 0.08905723690986633\n",
            "strain 0.06159389019012451\n",
            "strain 0.07376344501972198\n",
            "strain 0.0839860588312149\n",
            "strain 0.08884453773498535\n",
            "strain 0.07791972160339355\n",
            "strain 0.08115746825933456\n",
            "strain 0.05740979686379433\n",
            "strain 0.08042419701814651\n",
            "strain 0.061325524002313614\n",
            "strain 0.08964541554450989\n",
            "strain 0.07663116604089737\n",
            "strain 0.09128395467996597\n",
            "strain 0.056520186364650726\n",
            "strain 0.08865257352590561\n",
            "strain 0.07352650910615921\n",
            "strain 0.0918782502412796\n",
            "strain 0.07896921783685684\n",
            "strain 0.07780352979898453\n",
            "strain 0.08321306854486465\n",
            "strain 0.07079754769802094\n",
            "strain 0.0815369188785553\n",
            "strain 0.07037997245788574\n",
            "strain 0.06768970936536789\n",
            "strain 0.07670550793409348\n",
            "strain 0.07703288644552231\n",
            "strain 0.07449588179588318\n",
            "strain 0.07626951485872269\n",
            "strain 0.06400680541992188\n",
            "classify 2.2647705078125\n",
            "classify 2.2535400390625\n",
            "classify 2.2279052734375\n",
            "classify 2.236328125\n",
            "classify 2.2720947265625\n",
            "classify 2.2657470703125\n",
            "classify 2.2677001953125\n",
            "classify 2.23681640625\n",
            "classify 2.333251953125\n",
            "classify 2.2628173828125\n",
            "classify 2.21533203125\n",
            "0.09375\n",
            "0.125\n",
            "0.15625\n",
            "0.125\n",
            "0.140625\n",
            "0.046875\n",
            "0.21875\n",
            "0.109375\n",
            "0.171875\n",
            "0.1875\n",
            "0.203125\n",
            "124\n",
            "strain 0.06728599220514297\n",
            "strain 0.09488644450902939\n",
            "strain 0.0711597204208374\n",
            "strain 0.06911017745733261\n",
            "strain 0.08707873523235321\n",
            "strain 0.06803786754608154\n",
            "strain 0.07207126915454865\n",
            "strain 0.06906984746456146\n",
            "strain 0.0686214491724968\n",
            "strain 0.0856613963842392\n",
            "strain 0.08192698657512665\n",
            "strain 0.06263061612844467\n",
            "strain 0.0698259249329567\n",
            "strain 0.05345797538757324\n",
            "strain 0.07487121224403381\n",
            "strain 0.0763731375336647\n",
            "strain 0.08190100640058517\n",
            "strain 0.07521525025367737\n",
            "strain 0.07236215472221375\n",
            "strain 0.06682722270488739\n",
            "strain 0.06274914741516113\n",
            "strain 0.07462657988071442\n",
            "strain 0.08605172485113144\n",
            "strain 0.06522347033023834\n",
            "strain 0.07515881955623627\n",
            "strain 0.08182870596647263\n",
            "strain 0.1053626760840416\n",
            "strain 0.0675979033112526\n",
            "strain 0.07635942846536636\n",
            "strain 0.10906460136175156\n",
            "strain 0.07238944619894028\n",
            "strain 0.08121244609355927\n",
            "strain 0.06922760605812073\n",
            "strain 0.07258936762809753\n",
            "strain 0.07869213074445724\n",
            "strain 0.09764997661113739\n",
            "strain 0.07426831871271133\n",
            "strain 0.07462557405233383\n",
            "strain 0.07611438632011414\n",
            "strain 0.08121730387210846\n",
            "strain 0.08042250573635101\n",
            "strain 0.07736633718013763\n",
            "strain 0.07709193229675293\n",
            "strain 0.08262129873037338\n",
            "strain 0.06566112488508224\n",
            "strain 0.07223924249410629\n",
            "strain 0.08437659591436386\n",
            "strain 0.07508962601423264\n",
            "strain 0.07240863889455795\n",
            "strain 0.06929776817560196\n",
            "strain 0.07143155485391617\n",
            "classify 2.2947998046875\n",
            "classify 2.18310546875\n",
            "classify 2.2874755859375\n",
            "classify 2.2193603515625\n",
            "classify 2.2552490234375\n",
            "classify 2.2099609375\n",
            "classify 2.3267822265625\n",
            "classify 2.2427978515625\n",
            "classify 2.31396484375\n",
            "classify 2.2215576171875\n",
            "classify 2.2689208984375\n",
            "0.171875\n",
            "0.15625\n",
            "0.15625\n",
            "0.203125\n",
            "0.109375\n",
            "0.15625\n",
            "0.125\n",
            "0.15625\n",
            "0.09375\n",
            "0.140625\n",
            "0.125\n",
            "125\n",
            "strain 0.10416558384895325\n",
            "strain 0.07154294103384018\n",
            "strain 0.05564494431018829\n",
            "strain 0.05913376808166504\n",
            "strain 0.08880496025085449\n",
            "strain 0.0721108540892601\n",
            "strain 0.06973526626825333\n",
            "strain 0.07254162430763245\n",
            "strain 0.06853079050779343\n",
            "strain 0.08843901008367538\n",
            "strain 0.08836527168750763\n",
            "strain 0.07296843081712723\n",
            "strain 0.06978234648704529\n",
            "strain 0.05578312277793884\n",
            "strain 0.09215952455997467\n",
            "strain 0.07342423498630524\n",
            "strain 0.0770375058054924\n",
            "strain 0.09320157021284103\n",
            "strain 0.077259860932827\n",
            "strain 0.08724844455718994\n",
            "strain 0.0990624651312828\n",
            "strain 0.089665487408638\n",
            "strain 0.06829439103603363\n",
            "strain 0.07396645843982697\n",
            "strain 0.07309257984161377\n",
            "strain 0.07380181550979614\n",
            "strain 0.07112189382314682\n",
            "strain 0.07804284989833832\n",
            "strain 0.0908164381980896\n",
            "strain 0.0909283384680748\n"
          ]
        }
      ],
      "source": [
        "# @title strain ctrain test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "def strain(model, dataloader, optim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.train()\n",
        "    for i, (x, _) in enumerate(dataloader):\n",
        "        x = x.to(device)#.to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            loss = model.loss(x)\n",
        "\n",
        "            # repr_loss, std_loss, cov_loss = model.loss(x)\n",
        "            # loss = model.sim_coeff * repr_loss + model.std_coeff * std_loss + model.cov_coeff * cov_loss\n",
        "\n",
        "        optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 5) # 0.5\n",
        "\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            m=0.999 # 0.99 m = next(momentum_scheduler)\n",
        "            norms=[]\n",
        "            for param_q, param_k in zip(model.student.parameters(), model.teacher.parameters()):\n",
        "                param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        print(\"strain\",loss.item())\n",
        "        # try: wandb.log({\"loss\": loss.item(), \"repr/I\": repr_loss.item(), \"std/V\": std_loss.item(), \"cov/C\": cov_loss.item()})\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=50: break\n",
        "\n",
        "\n",
        "def ctrain(model, classifier, dataloader, coptim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.eval()\n",
        "    classifier.train()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            with torch.no_grad():\n",
        "                sx = model(x).detach()\n",
        "            # print(sx[0][0])\n",
        "            y_ = classifier(sx)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "\n",
        "        coptim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(coptim)\n",
        "        scaler.update()\n",
        "        print(\"classify\",loss.item())\n",
        "        try: wandb.log({\"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        with torch.no_grad():\n",
        "            sx = model(x)\n",
        "            rankme = RankMe(sx).item()\n",
        "            lidar = LiDAR(sx).item()\n",
        "            y_ = classifier(sx)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        print(correct/len(y))\n",
        "        # try: wandb.log({\"correct\": correct/len(y)})\n",
        "        try: wandb.log({\"correct\": correct/len(y), \"rankme\": rankme, \"lidar\": lidar})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "# for i in range(1):\n",
        "for i in range(500):\n",
        "    print(i)\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # np.random.shuffle(train_indices); np.random.shuffle(val_indices)\n",
        "    # train_sampler, valid_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(val_indices)\n",
        "    # # batch_size = 64 #512\n",
        "    # train_loader = DataLoader(train_data, sampler=train_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "    # test_loader = DataLoader(train_data, sampler=valid_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True)\n",
        "\n",
        "    strain(ijepa, train_loader, optim)\n",
        "    ctrain(ijepa, classifier, train_loader, coptim)\n",
        "    test(ijepa, classifier, test_loader)\n",
        "\n",
        "    # strain(violet, train_loader, voptim)\n",
        "    # ctrain(violet, classifier, train_loader, coptim)\n",
        "    # test(violet, classifier, test_loader)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title supervised train test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "# def strain(model, dataloader, optim, scheduler=None):\n",
        "def strain(model, classifier, dataloader, optim, coptim, scheduler=None):\n",
        "    model.train()\n",
        "    classifier.train()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x = x.to(device)#.to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "        y = y.to(device)\n",
        "\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        # print(loss)\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # for p in list(filter(lambda p: p.grad is not None, model.parameters())):\n",
        "        #     print(p.grad.data.norm(2).item())\n",
        "        # print(\"max grad norm\", max([p.grad.data.norm(2).item() for p in list(filter(lambda p: p.grad is not None, model.parameters()))]))\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 10) # 0.5\n",
        "\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        print(\"strain\",loss.item())\n",
        "        # for param in ijepa.student.cls: print(param.data)\n",
        "        # for param in ijepa.predicter.cls: print(param.data)\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=50: break\n",
        "\n",
        "\n",
        "# def test(model, dataloader):\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        # .to(torch.bfloat16)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "        loss = F.cross_entropy(y_, y)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        print(correct/len(y))\n",
        "        try: wandb.log({\"correct\": correct/len(y), \"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "for i in range(1000):\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # np.random.shuffle(train_indices); np.random.shuffle(val_indices)\n",
        "    # train_sampler, valid_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(val_indices)\n",
        "    # # batch_size = 64 #512\n",
        "    # train_loader = DataLoader(train_data, sampler=train_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "    # test_loader = DataLoader(train_data, sampler=valid_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True)\n",
        "\n",
        "    # strain(ijepa, train_loader, optim)\n",
        "    strain(ijepa, classifier, train_loader, optim, coptim)\n",
        "    test(ijepa, classifier, test_loader)\n",
        "\n",
        "    # strain(violet, train_loader, voptim)\n",
        "    # test(violet, test_loader)\n"
      ],
      "metadata": {
        "id": "4J2ahp3wmqcg",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## drawer"
      ],
      "metadata": {
        "id": "aLT74ihtMnh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# @title test multiblock params\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "dataiter = iter(train_data)\n",
        "img,y = next(dataiter)\n",
        "img = img.unsqueeze(0)\n",
        "img = F.interpolate(img, (8,8))\n",
        "b,c,h,w = img.shape\n",
        "# img = img.flatten(2).transpose(-2,-1).to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "print(img.shape)\n",
        "# batch, seq,_ = img.shape\n",
        "\n",
        "\n",
        "# # target_mask = randpatch(seq, mask_size=8, gamma=.9) # 8.9 [seq]\n",
        "# target_mask = multiblock(seq, min_s=0.2, max_s=0.3, M=4).any(0) # best.2.3M4 og.15.2M4# mask out targets to be predicted # [M, seq]\n",
        "# target_mask = multiblock(seq, min_s=0.2, max_s=0.3, M=4).any(0)\n",
        "\n",
        "target_mask = multiblock2d((8,8), M=4).any(0).unsqueeze(0)\n",
        "\n",
        "\n",
        "# context_mask = ~multiblock(seq, min_s=0.85, max_s=1., M=1)|target_mask # [1, seq], True->Mask\n",
        "context_mask = ~multiblock2d((8,8), scale=(.85,.85), aspect_ratio=(.9,1.1), M=1)|target_mask # [1, seq], True->Mask\n",
        "\n",
        "# print(target_mask, context_mask)\n",
        "\n",
        "print(target_mask.shape, context_mask.shape)\n",
        "# target_img, context_img = img*target_mask.unsqueeze(-1), img*context_mask.unsqueeze(-1)\n",
        "# target_img, context_img = img*~target_mask.unsqueeze(-1), img*~context_mask.unsqueeze(-1)\n",
        "target_img, context_img = img*~target_mask.unsqueeze(0), img*~context_mask.unsqueeze(0)\n",
        "# print(target_img.shape, context_img.shape)\n",
        "target_img, context_img = target_img.transpose(-2,-1).reshape(b,c,h,w), context_img.transpose(-2,-1).reshape(b,c,h,w)\n",
        "target_img, context_img = target_img.float(), context_img.float()\n",
        "\n",
        "# imshow(out.detach().cpu())\n",
        "imshow(target_img[0])\n",
        "imshow(context_img[0])\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "3bpiybH7M0O1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 879
        },
        "outputId": "3d5e3a1a-7e09-4033-a6ad-5e68f9844da1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 8, 8])\n",
            "torch.Size([1, 8, 8]) torch.Size([1, 8, 8])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ViT me more\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "# class LayerNorm2d(nn.LayerNorm):\n",
        "class LayerNorm2d(nn.RMSNorm):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        x = super().forward(x)\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads=None, d_head=8, cond_dim=None, dropout=0.): # .1\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_head = d_head\n",
        "        self.n_heads = d_model // d_head\n",
        "        # self.d_head = d_model // n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or d_model, 2*d_model, bias=False)\n",
        "        # self.k = nn.Sequential(nn.Dropout(dropout), nn.Linear(cond_dim, d_model, bias=False))\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        # self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        # self.lin = nn.Sequential(nn.Dropout(dropout), zero_module(nn.Linear(d_model, d_model)))\n",
        "        self.drop = nn.Dropout(dropout) # indp before q,k,v; after linout\n",
        "        self.scale = self.d_head ** -.5\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [batch, T, d_model]=[batch, h*w, c], [batch, num_tok, cond_dim], [batch,T]\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        Q = self.q(x).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        # K = self.k(x).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2)\n",
        "        K, V = self.kv(cond).unflatten(-1, (self.n_heads, 2*self.d_head)).transpose(1, 2).chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # linear attention # Softmax(Q) @ (Softmax(K).T @ V)\n",
        "        if mask != None:\n",
        "            mask = mask[:, None, :, None] # [batch,T] -> [batch,1,T,1]\n",
        "            K, V = K.masked_fill(mask, -torch.finfo(x.dtype).max), V.masked_fill(mask, -torch.finfo(x.dtype).max)\n",
        "        Q, K = Q.softmax(dim=-1)*self.scale, K.softmax(dim=-2)\n",
        "        context = K.transpose(-2,-1) @ V # [batch, n_heads, d_head, d_head]\n",
        "        out = Q @ context # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # (quadratic) attention # Softmax(Q @ K.T) @ V\n",
        "        # attn = Q @ K.transpose(-2,-1) * self.scale # [batch, n_heads, T] # [batch, n_heads, T, T/num_tok]\n",
        "        # if mask != None: attn = attn.masked_fill(mask[:, None, :, None], -torch.finfo(attn.dtype).max) # [batch,T]->[batch,1,T,1]\n",
        "        # attention = torch.softmax(attn, dim=-1)\n",
        "        # out = self.drop(attention) @ V # [batch, n_heads, T, d_head]\n",
        "\n",
        "        out = out.transpose(1, 2).flatten(2)\n",
        "        return self.drop(self.lin(out)) # [batch, T, d_model]\n",
        "\n",
        "# if self, dont pass cond_dim in init, dont pass cond in fwd\n",
        "# Softmax(Q @ K.T) @ V ~ Softmax(Q) @ Softmax(K).T @ V\n",
        "\n",
        "# https://github.com/lucidrains/x-transformers/blob/main/x_transformers/x_transformers.py#L1855\n",
        "\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_head, cond_dim=None, mult=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.cond_dim = cond_dim\n",
        "        self.self = MultiHeadAttention(d_model, d_head=d_head, dropout=0) # 16448\n",
        "        # self.self = Pooling()\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        # self.ff = nn.Sequential(\n",
        "        #     *[nn.BatchNorm2d(d_model), act, SeparableConv2d(d_model, d_model),]*3\n",
        "        #     )\n",
        "        # self.ff = ResBlock(d_model) # 74112\n",
        "        # self.ff = UIB(d_model, mult=4) # uib m4 36992, m2 18944\n",
        "        ff_dim=d_model*mult\n",
        "        self.ff = nn.Sequential(\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(dropout), nn.Linear(d_model, ff_dim), act, # ReLU GELU\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(dropout), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            nn.RMSNorm(d_model), act, nn.Dropout(dropout), nn.Linear(d_model, ff_dim),\n",
        "            nn.RMSNorm(ff_dim), act, nn.Dropout(dropout), zero_module(nn.Linear(ff_dim, d_model))\n",
        "        )\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,c,h,w], [batch, num_tok, cond_dim], [batch,T]\n",
        "        bchw = x.shape\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "        # print('attnblk fwd',x.shape)\n",
        "\n",
        "        # if self.cond_dim==None: cond=None # is self attn\n",
        "        x = x + self.drop(self.self(self.norm(x)))\n",
        "        # x = x.transpose(1,2).reshape(*bchw)\n",
        "        x = x + self.ff(x)\n",
        "        # x = self.ff(x)\n",
        "        # x = x + self.drop(self.norm2(self.ff(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "import torch\n",
        "# def apply_masks(x, masks): # [b,t,d], [M,mask_size] # https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "#     all_x = []\n",
        "#     for m in masks: # [1,T]\n",
        "#         mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [1,T,dim]\n",
        "#         all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "#     return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "def apply_masks(x, mask): # [b,t,d], [mask_size] # https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "    mask_keep = mask.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [1,T,dim]\n",
        "    # return torch.cat(torch.gather(x, dim=1, index=mask_keep), dim=0)  # [M*batch,mask_size,dim]\n",
        "    return torch.gather(x, dim=1, index=mask_keep) # [M*batch,mask_size,dim]\n",
        "\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        # self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        self.positional_emb = nn.Parameter(torch.zeros(1, 8*8, d_model)) # positional_embedding == 'learnable'\n",
        "        # self.positional_emb = nn.Parameter(torch.zeros(1, dim, 8,8))\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.zeros(1,1,d_model)) # randn zeros\n",
        "        nn.init.trunc_normal_(self.cls, std=.02)\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model)\n",
        "        self.lin = nn.Linear(d_model, out_dim)# if out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = x.shape\n",
        "        # x = x * self.pos_encoder(context_indices)\n",
        "        x = x + self.positional_emb[:,context_indices]\n",
        "        # x = apply_masks(x, [context_indices])\n",
        "\n",
        "        # pred_tokens = self.cls * self.pos_encoder(trg_indices) # [M, num_trg_toks, d_model]\n",
        "        pred_tokens = self.cls + self.positional_emb[0,trg_indices]\n",
        "        print(self.cls.shape, self.positional_emb[0,trg_indices].shape, trg_indices.shape)\n",
        "        pred_tokens = pred_tokens.repeat(batch, 1, 1) # [batch*M, num_trg_toks, d_model]\n",
        "        # print(pred_tokens.requires_grad)\n",
        "        print(\"pred fwd\", x.shape, pred_tokens.shape)\n",
        "        # x = x.repeat_interleave(trg_indices.shape[0], dim=0) # [batch, seq_len, d_model] -> [batch*M, seq_len, d_model]\n",
        "        x = torch.cat([x, pred_tokens], dim=1) # [batch*M, seq_len+num_trg_toks, d_model]\n",
        "\n",
        "        # x = x.transpose(1,2).unflatten(-1, (8,8))#.reshape(*bchw)\n",
        "        out = self.transformer_encoder(x) # float [seq_len, batch_size, d_model]\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "\n",
        "        out = self.norm(out)\n",
        "        out = out[:,seq:] # [batch*M, num_trg_toks, d_model]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Sequential(\n",
        "            # # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.MaxPool1d(2,2), #nn.MaxPool1d(3, 2, 3//2),\n",
        "            # # nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.Conv1d(d_model, d_model,3,2,3//2)\n",
        "            # nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            # nn.Conv1d(d_model, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            # nn.Conv1d(d_model, d_model,3,2,3//2),\n",
        "            # # nn.Conv1d(in_dim, d_model,2,2,0), # like patch\n",
        "            nn.Conv2d(in_dim, d_model, kernel_size=7, stride=2, padding=7//2, bias=False), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "            )\n",
        "        # self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        self.positional_emb = nn.Parameter(torch.zeros(1, 8*8, d_model)) # positional_embedding == 'learnable'\n",
        "        # self.positional_emb = nn.Parameter(torch.zeros(1, dim, 8,8)) # positional_embedding == 'learnable'\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model)\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # x = self.embed(x.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "        x = self.embed(x)\n",
        "        # bchw = x.shape\n",
        "        x = x.flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c] # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "\n",
        "        # x = self.pos_encoder(x)\n",
        "        # x = x * self.pos_encoder(torch.arange(seq, device=device)).unsqueeze(0)\n",
        "        # print(x.shape, self.positional_emb.shape)\n",
        "        x = x + self.positional_emb\n",
        "        if context_indices != None:\n",
        "            # x = apply_masks(x, [context_indices])\n",
        "            x = apply_masks(x, context_indices)\n",
        "\n",
        "\n",
        "        # x = x.transpose(1,2).reshape(*bchw)\n",
        "        x = self.transformer_encoder(x) # float [seq_len, batch_size, d_model]\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "        out = self.norm(x)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "\n",
        "# pos_emb rope < learn < learned\n",
        "# conv > pixel?\n",
        "# droppath not required\n",
        "\n",
        "# norm,act,conv < conv,norm,act\n",
        "# 2*s1 < uib < resblock\n",
        "# gatedadaln 3 < 2 = 1 < ffmult4 = 2*gatedadaln\n",
        "# MaxPool2d(2,2) < MaxPool2d(3,2,3//2)\n",
        "\n",
        "\n",
        "# dim = 64\n",
        "# dim_head = 8\n",
        "# heads = dim // dim_head\n",
        "# num_classes = 10\n",
        "# # model = SimpleViT(image_size=32, patch_size=4, num_classes=num_classes, dim=dim, depth=1, heads=heads, mlp_dim=dim*4, channels = 3, dim_head = dim_head)\n",
        "# model = SimpleViT(in_dim=3, out_dim=num_classes, dim=dim, depth=1, heads=heads, mlp_dim=dim*4, channels = 3, dim_head = dim_head).to(device)\n",
        "# print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 59850\n",
        "# optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "# # print(images.shape) # [batch, 3, 32, 32]\n",
        "# logits = model(x)\n",
        "# print(logits.shape)\n",
        "# # print(logits[0])\n",
        "# # print(logits[0].argmax(1))\n",
        "# pred_probab = nn.Softmax(dim=1)(logits)\n",
        "# y_pred = pred_probab.argmax(1)\n",
        "# # print(f\"Predicted class: {y_pred}\")\n",
        "\n",
        "\n",
        "batch, seq_len, d_model = 4,1024,16\n",
        "in_dim = 3\n",
        "model = TransformerModel(in_dim, d_model, d_head=4, nlayers=2, dropout=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x = torch.rand((batch, in_dim, 32, 32), device=device)\n",
        "# x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "# # # print(out)\n",
        "# model = TransformerPredictor(in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1).to(device)\n",
        "# out = model(out)\n",
        "# print(out.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5554e8f3-71ec-4b70-c7d7-219b59e9a1d3",
        "cellView": "form",
        "id": "C1iZQ6UNwoty"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9920\n",
            "torch.Size([4, 64, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title simplex\n",
        "!pip install opensimplex\n",
        "from opensimplex import OpenSimplex\n",
        "\n",
        "seed=0\n",
        "noise = OpenSimplex(seed)  # Replace 'seed' with your starting value\n",
        "noise_scale = 10  # Adjust for desired noise range\n",
        "\n",
        "def get_noise(x, y):\n",
        "    global seed  # Assuming seed is a global variable\n",
        "    # noise_val = noise.noise2(x / noise_scale, y / noise_scale)\n",
        "    noise_val = noise.noise2(x, y)\n",
        "    # seed += 1  # Increment seed after each call\n",
        "    return noise_val\n",
        "\n",
        "x,y=0,0\n",
        "a=[[],[],[],[],[]]\n",
        "\n",
        "fps=2\n",
        "f=[]\n",
        "for i in range(10*fps):\n",
        "    f.append(i/fps)\n",
        "    x = x+0.3\n",
        "    # y = y+1\n",
        "    # noise_value = get_noise(x, y)\n",
        "    for k,j in enumerate([0,1,2,3,4]):\n",
        "        a[k].append(get_noise(x, y+j))\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "for aa in a:\n",
        "    plt.plot(f,aa)\n",
        "plt.show()\n",
        "\n",
        "# b,y,g,"
      ],
      "metadata": {
        "id": "e3dAhWh45F4M",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title pos_embed.py\n",
        "# https://github.com/facebookresearch/mae/blob/main/util/pos_embed.py#L20\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim=16, grid_size=8, cls_token=False): #\n",
        "    grid_h = np.arange(grid_size, dtype=np.float32)\n",
        "    grid_w = np.arange(grid_size, dtype=np.float32)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token: pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed # [(1+)grid_size*grid_size, embed_dim]\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    # assert embed_dim % 2 == 0\n",
        "\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1) # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos): #\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=float)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega  # (D/2,)\n",
        "\n",
        "    pos = pos.reshape(-1)  # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
        "\n",
        "    emb_sin = np.sin(out) # (M, D/2)\n",
        "    emb_cos = np.cos(out) # (M, D/2)\n",
        "\n",
        "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def interpolate_pos_embed(model, checkpoint_model):\n",
        "    if 'pos_embed' in checkpoint_model:\n",
        "        pos_embed_checkpoint = checkpoint_model['pos_embed']\n",
        "        embedding_size = pos_embed_checkpoint.shape[-1]\n",
        "        num_patches = model.patch_embed.num_patches\n",
        "        num_extra_tokens = model.pos_embed.shape[-2] - num_patches\n",
        "        # height (== width) for the checkpoint position embedding\n",
        "        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n",
        "        # height (== width) for the new position embedding\n",
        "        new_size = int(num_patches ** 0.5)\n",
        "        # class_token and dist_token are kept unchanged\n",
        "        if orig_size != new_size:\n",
        "            print(\"Position interpolate from %dx%d to %dx%d\" % (orig_size, orig_size, new_size, new_size))\n",
        "            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n",
        "            # only the position tokens are interpolated\n",
        "            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n",
        "            pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n",
        "            pos_tokens = torch.nn.functional.interpolate(\n",
        "                pos_tokens, size=(new_size, new_size), mode='bicubic', align_corners=False)\n",
        "            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n",
        "            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n",
        "            checkpoint_model['pos_embed'] = new_pos_embed\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "vtPSM8mbnJZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ijepa src.utils.tensors\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/utils/tensors.py\n",
        "\n",
        "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
        "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n",
        "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
        "    def norm_cdf(x):\n",
        "        # Computes standard normal cumulative distribution function\n",
        "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Values are generated by using a truncated uniform distribution and\n",
        "        # then using the inverse CDF for the normal distribution.\n",
        "        # Get upper and lower cdf values\n",
        "        l = norm_cdf((a - mean) / std)\n",
        "        u = norm_cdf((b - mean) / std)\n",
        "\n",
        "        # Uniformly fill tensor with values from [l, u], then translate to\n",
        "        # [2l-1, 2u-1].\n",
        "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
        "\n",
        "        # Use inverse cdf transform for normal distribution to get truncated\n",
        "        # standard normal\n",
        "        tensor.erfinv_()\n",
        "\n",
        "        # Transform to proper mean, std\n",
        "        tensor.mul_(std * math.sqrt(2.))\n",
        "        tensor.add_(mean)\n",
        "\n",
        "        # Clamp to ensure it's in the proper range\n",
        "        tensor.clamp_(min=a, max=b)\n",
        "        return tensor\n",
        "\n",
        "\n",
        "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
        "    # type: (Tensor, float, float, float, float) -> Tensor\n",
        "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n",
        "\n",
        "def repeat_interleave_batch(x, B, repeat): # [batch*M,...]? , M?\n",
        "    N = len(x) // B\n",
        "    x = torch.cat([\n",
        "        torch.cat([x[i*B:(i+1)*B] for _ in range(repeat)], dim=0)\n",
        "        for i in range(N)\n",
        "    ], dim=0)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "dLbXQ-3XXRMq",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ijepa multiblock down\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "import math\n",
        "from multiprocessing import Value\n",
        "import torch\n",
        "_GLOBAL_SEED = 0\n",
        "\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=(224, 224),\n",
        "        patch_size=16,\n",
        "        enc_mask_scale=(0.2, 0.8),\n",
        "        pred_mask_scale=(0.2, 0.8),\n",
        "        aspect_ratio=(0.3, 3.0),\n",
        "        nenc=1,\n",
        "        npred=2,\n",
        "        min_keep=4,\n",
        "        allow_overlap=False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if not isinstance(input_size, tuple):\n",
        "            input_size = (input_size, ) * 2\n",
        "        self.patch_size = patch_size\n",
        "        self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.aspect_ratio = aspect_ratio\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "        self._itr_counter = Value('i', -1)  # collator is shared across worker processes\n",
        "\n",
        "    def step(self):\n",
        "        i = self._itr_counter\n",
        "        with i.get_lock():\n",
        "            i.value += 1\n",
        "            v = i.value\n",
        "        return v\n",
        "\n",
        "    def _sample_block_size(self, generator, scale, aspect_ratio_scale):\n",
        "        _rand = torch.rand(1, generator=generator).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.height * self.width * mask_scale)\n",
        "        # -- Sample block aspect-ratio\n",
        "        min_ar, max_ar = aspect_ratio_scale\n",
        "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
        "        while h >= self.height:\n",
        "            h -= 1\n",
        "        while w >= self.width:\n",
        "            w -= 1\n",
        "        return (h, w)\n",
        "\n",
        "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
        "        h, w = b_size\n",
        "\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # --\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            top = torch.randint(0, self.height - h, (1,))\n",
        "            left = torch.randint(0, self.width - w, (1,))\n",
        "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
        "            mask[top:top+h, left:left+w] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
        "        mask_complement[top:top+h, left:left+w] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        B = len(batch)\n",
        "\n",
        "        collated_batch = torch.utils.data.default_collate(batch)\n",
        "\n",
        "        seed = self.step()\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(seed)\n",
        "        p_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.pred_mask_scale,\n",
        "            aspect_ratio_scale=self.aspect_ratio)\n",
        "        e_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.enc_mask_scale,\n",
        "            aspect_ratio_scale=(1., 1.))\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.height * self.width\n",
        "        min_keep_enc = self.height * self.width\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                print(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "        return collated_batch, collated_masks_enc, collated_masks_pred\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "COvEnBXPYth3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aH1UOD4WW8I2",
        "outputId": "e8671cd3-eba7-4649-84ca-94c6467ce62d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Mask generator says: \"Valid mask not found, decreasing acceptable-regions [1]\"\n",
            "WARNING:root:Mask generator says: \"Valid mask not found, decreasing acceptable-regions [2]\"\n",
            "WARNING:root:Mask generator says: \"Valid mask not found, decreasing acceptable-regions [3]\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vit fwd torch.Size([4, 3, 224, 224])\n",
            "vit fwd torch.Size([4, 196, 768])\n",
            "vit fwd torch.Size([4, 11, 768])\n",
            "predictor fwd1 torch.Size([4, 11, 384]) torch.Size([4, 196, 384])\n"
          ]
        }
      ],
      "source": [
        "# @title ijepa vision_transformer.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/models/vision_transformer.py\n",
        "import math\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# from src.utils.tensors import (trunc_normal_, repeat_interleave_batch)\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid height and width\n",
        "    return:\n",
        "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid_h = np.arange(grid_size, dtype=float)\n",
        "    grid_w = np.arange(grid_size, dtype=float)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0) # [1+h*w, dim]\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    assert embed_dim % 2 == 0\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid length\n",
        "    return:\n",
        "    pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid = np.arange(grid_size, dtype=float)\n",
        "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=float)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega   # (D/2,)\n",
        "    pos = pos.reshape(-1)   # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
        "    emb = np.concatenate([np.sin(out), np.cos(out)], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\"\"\"\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x, attn\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x, return_attention=False):\n",
        "        y, attn = self.attn(self.norm1(x))\n",
        "        if return_attention:\n",
        "            return attn\n",
        "        x = x + self.drop_path(y)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2) # ->[b,h*w,c]\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvEmbed(nn.Module):\n",
        "    \"\"\"3x3 Convolution stems for ViT following ViTC models\"\"\"\n",
        "    def __init__(self, channels, strides, img_size=224, in_chans=3, batch_norm=True):\n",
        "        super().__init__()\n",
        "        # Build the stems\n",
        "        stem = []\n",
        "        channels = [in_chans] + channels\n",
        "        for i in range(len(channels) - 2):\n",
        "            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3, stride=strides[i], padding=1, bias=(not batch_norm))]\n",
        "            if batch_norm:\n",
        "                stem += [nn.BatchNorm2d(channels[i+1])]\n",
        "            stem += [nn.ReLU(inplace=True)]\n",
        "        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n",
        "        self.stem = nn.Sequential(*stem)\n",
        "\n",
        "        # Comptute the number of patches\n",
        "        stride_prod = int(np.prod(strides))\n",
        "        self.num_patches = (img_size[0] // stride_prod)**2\n",
        "\n",
        "    def forward(self, x):\n",
        "        p = self.stem(x)\n",
        "        return p.flatten(2).transpose(1, 2)\n",
        "\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "def apply_masks(x, masks):\n",
        "    \"\"\":param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
        "    :param masks: list of tensors containing indices of patches in [N] to keep\"\"\"\n",
        "    all_x = []\n",
        "    for m in masks:\n",
        "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [batch,T,dim]\n",
        "        all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "    return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "\n",
        "class VisionTransformerPredictor(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, num_patches, embed_dim=768, predictor_embed_dim=384,\n",
        "        depth=6, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.init_std = init_std\n",
        "        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        # --\n",
        "        self.predictor_pos_embed = nn.Parameter(torch.zeros(1, num_patches, predictor_embed_dim), requires_grad=False)\n",
        "        predictor_pos_embed = get_2d_sincos_pos_embed(self.predictor_pos_embed.shape[-1], int(num_patches**.5), cls_token=False)\n",
        "        self.predictor_pos_embed.data.copy_(torch.from_numpy(predictor_pos_embed).float().unsqueeze(0)) # [1, num_patches, predictor_embed_dim]\n",
        "        # --\n",
        "        self.predictor_blocks = nn.ModuleList([Block(dim=predictor_embed_dim, num_heads=num_heads, drop=drop_rate, drop_path=dpr[i]) for i in range(depth)])\n",
        "        self.predictor_norm = nn.LayerNorm(predictor_embed_dim)\n",
        "        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n",
        "        # ------\n",
        "        # trunc_normal_(self.mask_token, std=self.init_std)\n",
        "\n",
        "    def forward(self, x, masks_x, masks): # [batch, num_context_patches, embed_dim], nenc*[batch, num_context_patches], npred*[batch, num_target_patches]\n",
        "        assert (masks is not None) and (masks_x is not None), 'Cannot run predictor without mask indices'\n",
        "        if not isinstance(masks_x, list): masks_x = [masks_x] # context mask\n",
        "        if not isinstance(masks, list): masks = [masks] # pred mask\n",
        "\n",
        "        # -- Batch Size\n",
        "        B = len(x) // len(masks_x)\n",
        "        # print(\"predictor fwd0\", x.shape) # [3, 121, 768])\n",
        "        # -- map from encoder-dim to pedictor-dim\n",
        "        x = self.predictor_embed(x) # [batch, num_patches, predictor_embed_dim]\n",
        "\n",
        "        # -- add positional embedding to x tokens\n",
        "        x_pos_embed = self.predictor_pos_embed.repeat(B, 1, 1) # [batch, num_patches, predictor_embed_dim]\n",
        "        print(\"predictor fwd1\", x.shape, x_pos_embed.shape) # [3, 121 or 11, 384], [3, 196, 384]\n",
        "        # print(\"predictor fwd masks_x\", masks_x)\n",
        "        x += apply_masks(x_pos_embed, masks_x) # get pos emb of context patches\n",
        "\n",
        "        # print(\"predictor fwd x\", x.shape) # [4, 11, 384])\n",
        "        _, N_ctxt, D = x.shape\n",
        "\n",
        "        # -- concat mask tokens to x\n",
        "        pos_embs = self.predictor_pos_embed.repeat(B, 1, 1) # [batch, num_patches, predictor_embed_dim]\n",
        "        pos_embs = apply_masks(pos_embs, masks) # [16, 104, predictor_embed_dim]\n",
        "        pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x)) # [16, 104, predictor_embed_dim]\n",
        "        # --\n",
        "        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1) # [16, 104, predictor_embed_dim]\n",
        "        # --\n",
        "        # print(\"predictor fwd pred_tokens\", pred_tokens.shape)\n",
        "\n",
        "        pred_tokens += pos_embs\n",
        "        x = x.repeat(len(masks), 1, 1)\n",
        "        x = torch.cat([x, pred_tokens], dim=1)\n",
        "\n",
        "        # -- fwd prop\n",
        "        for blk in self.predictor_blocks:\n",
        "            x = blk(x)\n",
        "        x = self.predictor_norm(x)\n",
        "\n",
        "        # -- return preds for mask tokens\n",
        "        x = x[:, N_ctxt:]\n",
        "        x = self.predictor_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, img_size=[224], patch_size=16, in_chans=3, embed_dim=768, predictor_embed_dim=384, depth=12, predictor_depth=12,\n",
        "        num_heads=12, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0, norm_layer=nn.LayerNorm, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_features = self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.init_std = init_std\n",
        "        # --\n",
        "        self.patch_embed = PatchEmbed(img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        # --\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=False)\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "        # --\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "    def forward(self, x, masks=None): # [batch,3,224,224]\n",
        "        if masks is not None:\n",
        "            if not isinstance(masks, list): masks = [masks]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        x = self.patch_embed(x)\n",
        "        B, N, D = x.shape # [batch, num_patches, embed_dim]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        # -- add positional embedding to x\n",
        "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
        "        x = x + pos_embed\n",
        "\n",
        "        if masks is not None: x = apply_masks(x, masks) # [batch, num_context_patches, embed_dim]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            x = blk(x)\n",
        "\n",
        "        if self.norm is not None: x = self.norm(x)\n",
        "        return x # [batch, num_context_patches, embed_dim]\n",
        "\n",
        "    def interpolate_pos_encoding(self, x, pos_embed):\n",
        "        npatch = x.shape[1] - 1\n",
        "        N = pos_embed.shape[1] - 1\n",
        "        if npatch == N: return pos_embed\n",
        "        class_emb, pos_embed = pos_embed[:, 0], pos_embed[:, 1:]\n",
        "        dim = x.shape[-1]\n",
        "        pos_embed = nn.functional.interpolate(pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2), scale_factor=math.sqrt(npatch / N), mode='bicubic',)\n",
        "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
        "\n",
        "# from functools import partial\n",
        "# def vit_predictor(**kwargs):\n",
        "#     model = VisionTransformerPredictor(\n",
        "#         mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "#     return model\n",
        "\n",
        "def vit(patch_size=16, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, # tiny\n",
        "        # patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, # small\n",
        "        # patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, # base\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "# encoder = vit()\n",
        "encoder = VisionTransformer(\n",
        "        patch_size=16, embed_dim=768, depth=1, num_heads=3, mlp_ratio=1,\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
        "\n",
        "# num_patches = (224/patch_size)^2\n",
        "# model = VisionTransformerPredictor(num_patches, embed_dim=768, predictor_embed_dim=384, depth=6, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02, **kwargs)\n",
        "model = VisionTransformerPredictor(num_patches=196, embed_dim=768, predictor_embed_dim=384, depth=1, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02)\n",
        "\n",
        "batch = 4\n",
        "img = torch.rand(batch, 3, 224, 224)\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/train.py\n",
        "mask_collator = MaskCollator(\n",
        "        input_size=(224, 224),\n",
        "        patch_size=16,\n",
        "        enc_mask_scale=(0.2, 0.8),\n",
        "        pred_mask_scale=(0.2, 0.8),\n",
        "        aspect_ratio=(0.3, 3.0),\n",
        "        nenc=1,\n",
        "        npred=4,\n",
        "        min_keep=4,\n",
        "        # allow_overlap=True)\n",
        "        allow_overlap=False)\n",
        "# self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
        "\n",
        "\n",
        "\n",
        "# v = mask_collator.step()\n",
        "# should pass the collater a list batch of idx?\n",
        "# collated_batch, collated_masks_enc, collated_masks_pred = mask_collator([batch,3,8])\n",
        "collated_batch, collated_masks_enc, collated_masks_pred = mask_collator([0,1,2,3])\n",
        "# print(v)\n",
        "\n",
        "\n",
        "def forward_target():\n",
        "    with torch.no_grad():\n",
        "        h = target_encoder(imgs)\n",
        "        h = F.layer_norm(h, (h.size(-1),))  # normalize over feature-dim\n",
        "        B = len(h)\n",
        "        # -- create targets (masked regions of h)\n",
        "        h = apply_masks(h, masks_pred)\n",
        "        h = repeat_interleave_batch(h, B, repeat=len(masks_enc))\n",
        "        return h\n",
        "\n",
        "def forward_context():\n",
        "    z = encoder(imgs, masks_enc)\n",
        "    z = predictor(z, masks_enc, masks_pred)\n",
        "    return z\n",
        "\n",
        "# # num_context_patches = 121 if allow_overlap=True else 11\n",
        "z = encoder(img, collated_masks_enc) # [batch, num_context_patches, embed_dim]\n",
        "# print(len(collated_masks_enc), len(collated_masks_pred)) # nenc, npred\n",
        "# print(collated_masks_enc[0].shape, collated_masks_pred[0].shape) # , [batch, num_context_patches = 121 or 11], [batch, num_target_patches]\n",
        "out = model(z, collated_masks_enc, collated_masks_pred)\n",
        "# # print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# print(224/16) # 14\n",
        "# print(len(collated_masks_enc), len(collated_masks_pred))\n",
        "print(collated_masks_enc, collated_masks_pred)\n",
        "# multiples of 14\n",
        "\n",
        "# print(collated_masks_pred[1])\n"
      ],
      "metadata": {
        "id": "Or3eQvUVg7l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(collated_masks_enc, collated_masks_pred)"
      ],
      "metadata": {
        "id": "u2mLjXAmXcwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ijepa multiblock.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "import math\n",
        "from multiprocessing import Value\n",
        "from logging import getLogger\n",
        "import torch\n",
        "_GLOBAL_SEED = 0\n",
        "logger = getLogger()\n",
        "\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=(224, 224),\n",
        "        patch_size=16,\n",
        "        enc_mask_scale=(0.2, 0.8),\n",
        "        pred_mask_scale=(0.2, 0.8),\n",
        "        aspect_ratio=(0.3, 3.0),\n",
        "        nenc=1,\n",
        "        npred=2,\n",
        "        min_keep=4,\n",
        "        allow_overlap=False\n",
        "    ):\n",
        "        super(MaskCollator, self).__init__()\n",
        "        if not isinstance(input_size, tuple):\n",
        "            input_size = (input_size, ) * 2\n",
        "        self.patch_size = patch_size\n",
        "        self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.aspect_ratio = aspect_ratio\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "        self._itr_counter = Value('i', -1)  # collator is shared across worker processes\n",
        "\n",
        "    def step(self):\n",
        "        i = self._itr_counter\n",
        "        with i.get_lock():\n",
        "            i.value += 1\n",
        "            v = i.value\n",
        "        return v\n",
        "\n",
        "    def _sample_block_size(self, generator, scale, aspect_ratio_scale):\n",
        "        _rand = torch.rand(1, generator=generator).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.height * self.width * mask_scale)\n",
        "        # -- Sample block aspect-ratio\n",
        "        min_ar, max_ar = aspect_ratio_scale\n",
        "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
        "        while h >= self.height:\n",
        "            h -= 1\n",
        "        while w >= self.width:\n",
        "            w -= 1\n",
        "\n",
        "        return (h, w)\n",
        "\n",
        "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
        "        h, w = b_size\n",
        "\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # --\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            top = torch.randint(0, self.height - h, (1,))\n",
        "            left = torch.randint(0, self.width - w, (1,))\n",
        "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
        "            mask[top:top+h, left:left+w] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "                    logger.warning(f'Mask generator says: \"Valid mask not found, decreasing acceptable-regions [{tries}]\"')\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
        "        mask_complement[top:top+h, left:left+w] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        B = len(batch)\n",
        "\n",
        "        collated_batch = torch.utils.data.default_collate(batch)\n",
        "\n",
        "        seed = self.step()\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(seed)\n",
        "        p_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.pred_mask_scale,\n",
        "            aspect_ratio_scale=self.aspect_ratio)\n",
        "        e_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.enc_mask_scale,\n",
        "            aspect_ratio_scale=(1., 1.))\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.height * self.width\n",
        "        min_keep_enc = self.height * self.width\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                logger.warning(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "\n",
        "        return collated_batch, collated_masks_enc, collated_masks_pred\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XQ7PxBMlsYCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ijepa train.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/train.py\n",
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "#\n",
        "\n",
        "import os\n",
        "\n",
        "# -- FOR DISTRIBUTED TRAINING ENSURE ONLY 1 DEVICE VISIBLE PER PROCESS\n",
        "try:\n",
        "    # -- WARNING: IF DOING DISTRIBUTED TRAINING ON A NON-SLURM CLUSTER, MAKE\n",
        "    # --          SURE TO UPDATE THIS TO GET LOCAL-RANK ON NODE, OR ENSURE\n",
        "    # --          THAT YOUR JOBS ARE LAUNCHED WITH ONLY 1 DEVICE VISIBLE\n",
        "    # --          TO EACH PROCESS\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = os.environ['SLURM_LOCALID']\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "import copy\n",
        "import logging\n",
        "import sys\n",
        "import yaml\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.multiprocessing as mp\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "\n",
        "from src.masks.multiblock import MaskCollator as MBMaskCollator\n",
        "from src.masks.utils import apply_masks\n",
        "from src.utils.distributed import (\n",
        "    init_distributed,\n",
        "    AllReduce\n",
        ")\n",
        "from src.utils.logging import (\n",
        "    CSVLogger,\n",
        "    gpu_timer,\n",
        "    grad_logger,\n",
        "    AverageMeter)\n",
        "from src.utils.tensors import repeat_interleave_batch\n",
        "from src.datasets.imagenet1k import make_imagenet1k\n",
        "\n",
        "from src.helper import (\n",
        "    load_checkpoint,\n",
        "    init_model,\n",
        "    init_opt)\n",
        "from src.transforms import make_transforms\n",
        "\n",
        "# --\n",
        "log_timings = True\n",
        "log_freq = 10\n",
        "checkpoint_freq = 50\n",
        "# --\n",
        "\n",
        "_GLOBAL_SEED = 0\n",
        "np.random.seed(_GLOBAL_SEED)\n",
        "torch.manual_seed(_GLOBAL_SEED)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logger = logging.getLogger()\n",
        "\n",
        "\n",
        "def main(args, resume_preempt=False):\n",
        "\n",
        "    # ----------------------------------------------------------------------- #\n",
        "    #  PASSED IN PARAMS FROM CONFIG FILE\n",
        "    # ----------------------------------------------------------------------- #\n",
        "\n",
        "    # -- META\n",
        "    use_bfloat16 = args['meta']['use_bfloat16']\n",
        "    model_name = args['meta']['model_name']\n",
        "    load_model = args['meta']['load_checkpoint'] or resume_preempt\n",
        "    r_file = args['meta']['read_checkpoint']\n",
        "    copy_data = args['meta']['copy_data']\n",
        "    pred_depth = args['meta']['pred_depth']\n",
        "    pred_emb_dim = args['meta']['pred_emb_dim']\n",
        "    if not torch.cuda.is_available():\n",
        "        device = torch.device('cpu')\n",
        "    else:\n",
        "        device = torch.device('cuda:0')\n",
        "        torch.cuda.set_device(device)\n",
        "\n",
        "    # -- DATA\n",
        "    use_gaussian_blur = args['data']['use_gaussian_blur']\n",
        "    use_horizontal_flip = args['data']['use_horizontal_flip']\n",
        "    use_color_distortion = args['data']['use_color_distortion']\n",
        "    color_jitter = args['data']['color_jitter_strength']\n",
        "    # --\n",
        "    batch_size = args['data']['batch_size']\n",
        "    pin_mem = args['data']['pin_mem']\n",
        "    num_workers = args['data']['num_workers']\n",
        "    root_path = args['data']['root_path']\n",
        "    image_folder = args['data']['image_folder']\n",
        "    crop_size = args['data']['crop_size']\n",
        "    crop_scale = args['data']['crop_scale']\n",
        "    # --\n",
        "\n",
        "    # -- MASK\n",
        "    allow_overlap = args['mask']['allow_overlap']  # whether to allow overlap b/w context and target blocks\n",
        "    patch_size = args['mask']['patch_size']  # patch-size for model training\n",
        "    num_enc_masks = args['mask']['num_enc_masks']  # number of context blocks\n",
        "    min_keep = args['mask']['min_keep']  # min number of patches in context block\n",
        "    enc_mask_scale = args['mask']['enc_mask_scale']  # scale of context blocks\n",
        "    num_pred_masks = args['mask']['num_pred_masks']  # number of target blocks\n",
        "    pred_mask_scale = args['mask']['pred_mask_scale']  # scale of target blocks\n",
        "    aspect_ratio = args['mask']['aspect_ratio']  # aspect ratio of target blocks\n",
        "    # --\n",
        "\n",
        "    # -- OPTIMIZATION\n",
        "    ema = args['optimization']['ema']\n",
        "    ipe_scale = args['optimization']['ipe_scale']  # scheduler scale factor (def: 1.0)\n",
        "    wd = float(args['optimization']['weight_decay'])\n",
        "    final_wd = float(args['optimization']['final_weight_decay'])\n",
        "    num_epochs = args['optimization']['epochs']\n",
        "    warmup = args['optimization']['warmup']\n",
        "    start_lr = args['optimization']['start_lr']\n",
        "    lr = args['optimization']['lr']\n",
        "    final_lr = args['optimization']['final_lr']\n",
        "\n",
        "    # -- LOGGING\n",
        "    folder = args['logging']['folder']\n",
        "    tag = args['logging']['write_tag']\n",
        "\n",
        "    dump = os.path.join(folder, 'params-ijepa.yaml')\n",
        "    with open(dump, 'w') as f:\n",
        "        yaml.dump(args, f)\n",
        "    # ----------------------------------------------------------------------- #\n",
        "\n",
        "    try:\n",
        "        mp.set_start_method('spawn')\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # -- init torch distributed backend\n",
        "    world_size, rank = init_distributed()\n",
        "    logger.info(f'Initialized (rank/world-size) {rank}/{world_size}')\n",
        "    if rank > 0:\n",
        "        logger.setLevel(logging.ERROR)\n",
        "\n",
        "    # -- log/checkpointing paths\n",
        "    log_file = os.path.join(folder, f'{tag}_r{rank}.csv')\n",
        "    save_path = os.path.join(folder, f'{tag}' + '-ep{epoch}.pth.tar')\n",
        "    latest_path = os.path.join(folder, f'{tag}-latest.pth.tar')\n",
        "    load_path = None\n",
        "    if load_model:\n",
        "        load_path = os.path.join(folder, r_file) if r_file is not None else latest_path\n",
        "\n",
        "    # -- make csv_logger\n",
        "    csv_logger = CSVLogger(log_file,\n",
        "                           ('%d', 'epoch'),\n",
        "                           ('%d', 'itr'),\n",
        "                           ('%.5f', 'loss'),\n",
        "                           ('%.5f', 'mask-A'),\n",
        "                           ('%.5f', 'mask-B'),\n",
        "                           ('%d', 'time (ms)'))\n",
        "\n",
        "    # -- init model\n",
        "    encoder, predictor = init_model(\n",
        "        device=device,\n",
        "        patch_size=patch_size,\n",
        "        crop_size=crop_size,\n",
        "        pred_depth=pred_depth,\n",
        "        pred_emb_dim=pred_emb_dim,\n",
        "        model_name=model_name)\n",
        "    target_encoder = copy.deepcopy(encoder)\n",
        "\n",
        "    # -- make data transforms\n",
        "    mask_collator = MBMaskCollator(\n",
        "        input_size=crop_size,\n",
        "        patch_size=patch_size,\n",
        "        pred_mask_scale=pred_mask_scale,\n",
        "        enc_mask_scale=enc_mask_scale,\n",
        "        aspect_ratio=aspect_ratio,\n",
        "        nenc=num_enc_masks,\n",
        "        npred=num_pred_masks,\n",
        "        allow_overlap=allow_overlap,\n",
        "        min_keep=min_keep)\n",
        "\n",
        "    transform = make_transforms(\n",
        "        crop_size=crop_size,\n",
        "        crop_scale=crop_scale,\n",
        "        gaussian_blur=use_gaussian_blur,\n",
        "        horizontal_flip=use_horizontal_flip,\n",
        "        color_distortion=use_color_distortion,\n",
        "        color_jitter=color_jitter)\n",
        "\n",
        "    # -- init data-loaders/samplers\n",
        "    _, unsupervised_loader, unsupervised_sampler = make_imagenet1k(\n",
        "            transform=transform,\n",
        "            batch_size=batch_size,\n",
        "            collator=mask_collator,\n",
        "            pin_mem=pin_mem,\n",
        "            training=True,\n",
        "            num_workers=num_workers,\n",
        "            world_size=world_size,\n",
        "            rank=rank,\n",
        "            root_path=root_path,\n",
        "            image_folder=image_folder,\n",
        "            copy_data=copy_data,\n",
        "            drop_last=True)\n",
        "    ipe = len(unsupervised_loader)\n",
        "\n",
        "    # -- init optimizer and scheduler\n",
        "    optimizer, scaler, scheduler, wd_scheduler = init_opt(\n",
        "        encoder=encoder,\n",
        "        predictor=predictor,\n",
        "        wd=wd,\n",
        "        final_wd=final_wd,\n",
        "        start_lr=start_lr,\n",
        "        ref_lr=lr,\n",
        "        final_lr=final_lr,\n",
        "        iterations_per_epoch=ipe,\n",
        "        warmup=warmup,\n",
        "        num_epochs=num_epochs,\n",
        "        ipe_scale=ipe_scale,\n",
        "        use_bfloat16=use_bfloat16)\n",
        "    encoder = DistributedDataParallel(encoder, static_graph=True)\n",
        "    predictor = DistributedDataParallel(predictor, static_graph=True)\n",
        "    target_encoder = DistributedDataParallel(target_encoder)\n",
        "    for p in target_encoder.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    # -- momentum schedule\n",
        "    momentum_scheduler = (ema[0] + i*(ema[1]-ema[0])/(ipe*num_epochs*ipe_scale)\n",
        "                          for i in range(int(ipe*num_epochs*ipe_scale)+1))\n",
        "\n",
        "    start_epoch = 0\n",
        "    # -- load training checkpoint\n",
        "    if load_model:\n",
        "        encoder, predictor, target_encoder, optimizer, scaler, start_epoch = load_checkpoint(\n",
        "            device=device,\n",
        "            r_path=load_path,\n",
        "            encoder=encoder,\n",
        "            predictor=predictor,\n",
        "            target_encoder=target_encoder,\n",
        "            opt=optimizer,\n",
        "            scaler=scaler)\n",
        "        for _ in range(start_epoch*ipe):\n",
        "            scheduler.step()\n",
        "            wd_scheduler.step()\n",
        "            next(momentum_scheduler)\n",
        "            mask_collator.step()\n",
        "\n",
        "    def save_checkpoint(epoch):\n",
        "        save_dict = {\n",
        "            'encoder': encoder.state_dict(),\n",
        "            'predictor': predictor.state_dict(),\n",
        "            'target_encoder': target_encoder.state_dict(),\n",
        "            'opt': optimizer.state_dict(),\n",
        "            'scaler': None if scaler is None else scaler.state_dict(),\n",
        "            'epoch': epoch,\n",
        "            'loss': loss_meter.avg,\n",
        "            'batch_size': batch_size,\n",
        "            'world_size': world_size,\n",
        "            'lr': lr\n",
        "        }\n",
        "        if rank == 0:\n",
        "            torch.save(save_dict, latest_path)\n",
        "            if (epoch + 1) % checkpoint_freq == 0:\n",
        "                torch.save(save_dict, save_path.format(epoch=f'{epoch + 1}'))\n",
        "\n",
        "    # -- TRAINING LOOP\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        logger.info('Epoch %d' % (epoch + 1))\n",
        "\n",
        "        # -- update distributed-data-loader epoch\n",
        "        unsupervised_sampler.set_epoch(epoch)\n",
        "\n",
        "        loss_meter = AverageMeter()\n",
        "        maskA_meter = AverageMeter()\n",
        "        maskB_meter = AverageMeter()\n",
        "        time_meter = AverageMeter()\n",
        "\n",
        "        for itr, (udata, masks_enc, masks_pred) in enumerate(unsupervised_loader):\n",
        "\n",
        "            def load_imgs():\n",
        "                # -- unsupervised imgs\n",
        "                imgs = udata[0].to(device, non_blocking=True)\n",
        "                masks_1 = [u.to(device, non_blocking=True) for u in masks_enc]\n",
        "                masks_2 = [u.to(device, non_blocking=True) for u in masks_pred]\n",
        "                return (imgs, masks_1, masks_2)\n",
        "            imgs, masks_enc, masks_pred = load_imgs()\n",
        "            maskA_meter.update(len(masks_enc[0][0]))\n",
        "            maskB_meter.update(len(masks_pred[0][0]))\n",
        "\n",
        "            def train_step():\n",
        "                _new_lr = scheduler.step()\n",
        "                _new_wd = wd_scheduler.step()\n",
        "                # --\n",
        "\n",
        "                def forward_target():\n",
        "                    with torch.no_grad():\n",
        "                        h = target_encoder(imgs)\n",
        "                        h = F.layer_norm(h, (h.size(-1),))  # normalize over feature-dim\n",
        "                        B = len(h)\n",
        "                        # -- create targets (masked regions of h)\n",
        "                        h = apply_masks(h, masks_pred)\n",
        "                        h = repeat_interleave_batch(h, B, repeat=len(masks_enc))\n",
        "                        return h\n",
        "\n",
        "                def forward_context():\n",
        "                    z = encoder(imgs, masks_enc)\n",
        "                    z = predictor(z, masks_enc, masks_pred)\n",
        "                    return z\n",
        "\n",
        "                def loss_fn(z, h):\n",
        "                    loss = F.smooth_l1_loss(z, h)\n",
        "                    loss = AllReduce.apply(loss)\n",
        "                    return loss\n",
        "\n",
        "                # Step 1. Forward\n",
        "                with torch.cuda.amp.autocast(dtype=torch.bfloat16, enabled=use_bfloat16):\n",
        "                    h = forward_target()\n",
        "                    z = forward_context()\n",
        "                    loss = loss_fn(z, h)\n",
        "\n",
        "                #  Step 2. Backward & step\n",
        "                if use_bfloat16:\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                else:\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                grad_stats = grad_logger(encoder.named_parameters())\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Step 3. momentum update of target encoder\n",
        "                with torch.no_grad():\n",
        "                    m = next(momentum_scheduler)\n",
        "                    for param_q, param_k in zip(encoder.parameters(), target_encoder.parameters()):\n",
        "                        param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "\n",
        "                return (float(loss), _new_lr, _new_wd, grad_stats)\n",
        "            (loss, _new_lr, _new_wd, grad_stats), etime = gpu_timer(train_step)\n",
        "            loss_meter.update(loss)\n",
        "            time_meter.update(etime)\n",
        "\n",
        "            # -- Logging\n",
        "            def log_stats():\n",
        "                csv_logger.log(epoch + 1, itr, loss, maskA_meter.val, maskB_meter.val, etime)\n",
        "                if (itr % log_freq == 0) or np.isnan(loss) or np.isinf(loss):\n",
        "                    logger.info('[%d, %5d] loss: %.3f '\n",
        "                                'masks: %.1f %.1f '\n",
        "                                '[wd: %.2e] [lr: %.2e] '\n",
        "                                '[mem: %.2e] '\n",
        "                                '(%.1f ms)'\n",
        "                                % (epoch + 1, itr,\n",
        "                                   loss_meter.avg,\n",
        "                                   maskA_meter.avg,\n",
        "                                   maskB_meter.avg,\n",
        "                                   _new_wd,\n",
        "                                   _new_lr,\n",
        "                                   torch.cuda.max_memory_allocated() / 1024.**2,\n",
        "                                   time_meter.avg))\n",
        "\n",
        "                    if grad_stats is not None:\n",
        "                        logger.info('[%d, %5d] grad_stats: [%.2e %.2e] (%.2e, %.2e)'\n",
        "                                    % (epoch + 1, itr,\n",
        "                                       grad_stats.first_layer,\n",
        "                                       grad_stats.last_layer,\n",
        "                                       grad_stats.min,\n",
        "                                       grad_stats.max))\n",
        "\n",
        "            log_stats()\n",
        "\n",
        "            assert not np.isnan(loss), 'loss is nan'\n",
        "\n",
        "        # -- Save Checkpoint after every epoch\n",
        "        logger.info('avg. loss %.3f' % loss_meter.avg)\n",
        "        save_checkpoint(epoch+1)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "iVUPGP93dpAN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}