{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "s3EO3PgMPH1x"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/I_JEPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LxACli7GdyGq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8c60d2e-3815-430c-c57b-6434cefd5816"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 60.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title data\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "transform = transforms.Compose([transforms.ToTensor(),])\n",
        "# transform = transforms.Compose([transforms.RandomResizedCrop((32,32), scale=(.3,1.)), transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "# CIFAR10: 60000 32x32 color images in 10 classes, with 6000 images per class\n",
        "train_data = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
        "test_data = torchvision.datasets.CIFAR10(root='data', train=False, download=True, transform=transform)\n",
        "batch_size = 64 # 4\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# dataiter = iter(train_loader) # get some random training images\n",
        "# images, labels = next(dataiter)\n",
        "# print(images.shape) # [batch, 3, 32, 32]\n",
        "# imshow(torchvision.utils.make_grid(images))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hiera"
      ],
      "metadata": {
        "id": "s3EO3PgMPH1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ResBlock\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters(): p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, kernel=3):\n",
        "        super().__init__()\n",
        "        out_ch = out_ch or in_ch\n",
        "        act = nn.SiLU() #\n",
        "        self.res_conv = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
        "        # self.block = nn.Sequential( # best?\n",
        "        #     nn.Conv2d(in_ch, out_ch, 3, padding=1), nn.BatchNorm2d(out_ch), act,\n",
        "        #     zero_module(nn.Conv2d(out_ch, out_ch, 3, padding=1)), nn.BatchNorm2d(out_ch), act,\n",
        "        #     )\n",
        "        self.block = nn.Sequential(\n",
        "            nn.BatchNorm2d(in_ch), act, nn.Conv2d(in_ch, out_ch, kernel, padding=kernel//2),\n",
        "            nn.BatchNorm2d(out_ch), act, zero_module(nn.Conv2d(out_ch, out_ch, kernel, padding=kernel//2)),\n",
        "            )\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w]\n",
        "        return self.block(x) + self.res_conv(x)\n"
      ],
      "metadata": {
        "id": "j3-vvMS1-gVn",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title UpDownBlock_me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class PixelShuffleConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, kernel=1, r=1):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        r = max(r, int(1/r))\n",
        "        out_ch = out_ch or in_ch\n",
        "        if self.r>1: self.net = nn.Sequential(ResBlock(in_ch, out_ch*r**2, kernel), nn.PixelShuffle(r))\n",
        "        # if self.r>1: self.net = nn.Sequential(Attention(in_ch, out_ch*r**2), nn.PixelShuffle(r))\n",
        "\n",
        "        elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), ResBlock(in_ch*r**2, out_ch, kernel))\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), Attention(in_ch*r**2, out_ch))\n",
        "        elif in_ch != out_ch: self.net = ResBlock(in_ch*r**2, out_ch, kernel)\n",
        "        else: self.net = lambda x: torch.zeros_like(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def AdaptiveAvgPool_nd(n, *args, **kwargs): return [nn.Identity, nn.AdaptiveAvgPool1d, nn.AdaptiveAvgPool2d, nn.AdaptiveAvgPool3d][n](*args, **kwargs)\n",
        "def AdaptiveMaxPool_nd(n, *args, **kwargs): return [nn.Identity, nn.AdaptiveMaxPool1d, nn.AdaptiveMaxPool2d, nn.AdaptiveMaxPool3d][n](*args, **kwargs)\n",
        "\n",
        "def adaptive_avg_pool_nd(n, x, output_size): return [nn.Identity, F.adaptive_avg_pool1d, F.adaptive_avg_pool2d, F.adaptive_avg_pool3d][n](x, output_size)\n",
        "def adaptive_max_pool_nd(n, x, output_size): return [nn.Identity, F.adaptive_max_pool1d, F.adaptive_max_pool2d, F.adaptive_max_pool3d][n](x, output_size)\n",
        "\n",
        "class AdaptivePool_at(nn.AdaptiveAvgPool1d): # AdaptiveAvgPool1d AdaptiveMaxPool1d\n",
        "    def __init__(self, dim=1, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.dim=dim\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(self.dim,-1)\n",
        "        shape = x.shape\n",
        "        return super().forward(x.flatten(0,-2)).unflatten(0, shape[:-1]).transpose(self.dim,-1)\n",
        "\n",
        "\n",
        "def adaptive_pool_at(x, dim, output_size, pool='avg'): # [b,c,h,w]\n",
        "    x = x.transpose(dim,-1)\n",
        "    shape = x.shape\n",
        "    parent={'avg':F.adaptive_avg_pool1d, 'max':F.adaptive_max_pool1d}[pool]\n",
        "    return parent(x.flatten(0,-2), output_size).unflatten(0, shape[:-1]).transpose(dim,-1)\n",
        "\n",
        "\n",
        "class ZeroExtend():\n",
        "    def __init__(self, dim=1, output_size=16):\n",
        "        self.dim, self.out = dim, output_size\n",
        "    def __call__(self, x): # [b,c,h,w]\n",
        "        return torch.cat((x, torch.zeros(*x.shape[:self.dim], self.out - x.shape[self.dim], *x.shape[self.dim+1:])), dim=self.dim)\n",
        "\n",
        "def make_pool_at(pool='avg', dim=1, output_size=5):\n",
        "    parent={'avg':nn.AdaptiveAvgPool1d, 'max':nn.AdaptiveMaxPool1d}[pool]\n",
        "    class AdaptivePool_at(parent): # AdaptiveAvgPool1d AdaptiveMaxPool1d\n",
        "        def __init__(self, dim=1, *args, **kwargs):\n",
        "            super().__init__(*args, **kwargs)\n",
        "            self.dim=dim\n",
        "        def forward(self, x):\n",
        "            x = x.transpose(self.dim,-1)\n",
        "            shape = x.shape\n",
        "            return super().forward(x.flatten(0,-2)).unflatten(0, shape[:-1]).transpose(self.dim,-1)\n",
        "    return AdaptivePool_at(dim, output_size=output_size)\n",
        "\n",
        "class Shortcut():\n",
        "    def __init__(self, dim=1, c=3, sp=(3,3), nd=2):\n",
        "        self.dim = dim\n",
        "        # self.ch_pool = make_pool_at(pool='avg', dim=dim, output_size=c)\n",
        "        self.ch_pool = make_pool_at(pool='max', dim=dim, output_size=c)\n",
        "        # self.ch_pool = ZeroExtend(dim, output_size=c) # only for out_dim>=in_dim\n",
        "        # self.sp_pool = AdaptiveAvgPool_nd(nd, sp)\n",
        "        self.sp_pool = AdaptiveMaxPool_nd(nd, sp)\n",
        "\n",
        "    def __call__(self, x): # [b,c,h,w]\n",
        "        x = self.sp_pool(x) # spatial first preserves spatial more?\n",
        "        x = self.ch_pool(x)\n",
        "        return x\n",
        "\n",
        "class UpDownBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, kernel=7, r=1):\n",
        "        super().__init__()\n",
        "        act = nn.SiLU()\n",
        "        self.r = r\n",
        "        self.block = PixelShuffleConv(in_ch, out_ch, kernel=kernel, r=r)\n",
        "        # self.block = nn.Sequential(\n",
        "        #     nn.BatchNorm2d(in_ch), act, PixelShuffleConv(in_ch, out_ch, kernel=kernel, r=r)\n",
        "        # )\n",
        "        # if self.r>1: self.res_conv = nn.Sequential(nn.ConvTranspose2d(in_ch, out_ch, kernel, 2, kernel//2, output_padding=1))\n",
        "        # if self.r>1: self.res_conv = nn.Sequential(nn.Upsample(scale_factor=2, mode='nearest'), nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity())\n",
        "        # if self.r>1: self.res_conv = nn.Sequential(nn.Upsample(scale_factor=2, mode='bilinear'), nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity())\n",
        "        # if self.r>1: self.res_conv = nn.Sequential(nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True), nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity())\n",
        "\n",
        "\n",
        "        # elif self.r<1: self.res_conv = nn.Sequential(nn.Conv2d(in_ch, out_ch, kernel, 2, kernel//2))\n",
        "        # elif self.r<1: self.res_conv = nn.Sequential(nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity(), nn.MaxPool2d(2,2))\n",
        "        # elif self.r<1: self.res_conv = nn.Sequential(nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity(), nn.AvgPool2d(2,2))\n",
        "        # elif self.r<1: self.res_conv = AttentionBlock(in_ch, out_ch, n_heads=4, q_stride=(2,2))\n",
        "\n",
        "        # else: self.res_conv = nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity()\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w]\n",
        "        b, num_tok, c, *win = x.shape\n",
        "        x = x.flatten(0,1)\n",
        "        out = self.block(x)\n",
        "        # # shortcut = F.interpolate(x.unsqueeze(1), size=out.shape[1:], mode='nearest-exact').squeeze(1) # pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html\n",
        "        # shortcut = F.adaptive_avg_pool3d(x, out.shape[1:]) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "        # # shortcut = F.adaptive_max_pool3d(x, out.shape[1:]) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "        # # shortcut = F.adaptive_avg_pool3d(x, out.shape[1:]) if out.shape[1]>=x.shape[1] else F.adaptive_max_pool3d(x, out.shape[1:])\n",
        "        # shortcut(x)\n",
        "        shortcut = Shortcut(dim=1, c=out.shape[1], sp=out.shape[-2:], nd=2)(x)\n",
        "        out = out + shortcut\n",
        "        out = out.unflatten(0, (b, num_tok))\n",
        "        return out\n",
        "\n",
        "        # return out + shortcut + self.res_conv(x)\n",
        "        # return out + self.res_conv(x)\n",
        "        # return self.res_conv(x)\n",
        "\n",
        "# if out>in, inter=max=ave=near.\n",
        "# if out<in, inter=ave. max=max\n",
        "\n",
        "# stride2\n",
        "# interconv/convpool\n",
        "# pixelconv\n",
        "# pixeluib\n",
        "# pixelres\n",
        "# shortcut\n",
        "\n",
        "# in_ch, out_ch = 16,3\n",
        "in_ch, out_ch = 3,16\n",
        "model = UpDownBlock(in_ch, out_ch, r=1/2).to(device)\n",
        "# model = UpDownBlock(in_ch, out_ch, r=2).to(device)\n",
        "\n",
        "x = torch.rand(12, in_ch, 64,64, device=device)\n",
        "x = torch.rand(12, 2, in_ch, 64,64, device=device)\n",
        "out = model(x)\n",
        "\n",
        "print(out.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "m0lclc9myo2c",
        "outputId": "4528f3c3-0b71-4f97-8b57-97934c6a80b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([12, 2, 16, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title maxpool path bchw\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def conv_nd(n, *args, **kwargs): return [nn.Identity, nn.Conv1d, nn.Conv2d, nn.Conv3d][n](*args, **kwargs)\n",
        "def maxpool_nd(n, *args, **kwargs): return [nn.Identity, nn.MaxPool1d, nn.MaxPool2d, nn.MaxPool3d][n](*args, **kwargs)\n",
        "def avgpool_nd(n, *args, **kwargs): return [nn.Identity, nn.AvgPool1d, nn.AvgPool2d, nn.AvgPool3d][n](*args, **kwargs)\n",
        "import math\n",
        "\n",
        "class MaskUnitAttention(nn.Module):\n",
        "    # def __init__(self, d_model=16, n_heads=4, q_stride=None, nd=2):\n",
        "    def __init__(self, in_dim, d_model=16, n_heads=4, q_stride=None, nd=2):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads, self.d_head = n_heads, d_model // n_heads\n",
        "        self.scale = self.d_head**-.5\n",
        "        # self.qkv = conv_nd(nd, d_model, 3*d_model, 1, bias=False)\n",
        "        self.qkv = conv_nd(nd, in_dim, 3*d_model, 1, bias=False)\n",
        "        # self.out = conv_nd(nd, d_model, d_model, 1)\n",
        "        self.out = nn.Linear(d_model, d_model, 1)\n",
        "        self.q_stride = q_stride # If greater than 1, pool q with this stride. The stride should be flattened (e.g., 2x2 = 4).\n",
        "        if q_stride:\n",
        "            self.q_stride = (q_stride,)*nd if type(q_stride)==int else q_stride\n",
        "            self.q_pool = maxpool_nd(nd, self.q_stride, stride=self.q_stride)\n",
        "\n",
        "    def forward(self, x): # [b,num_tok,c,win1,win2]\n",
        "        b, num_tok, c, *win = x.shape\n",
        "        # x = x.transpose(1,2).flatten(0,1) # [b*num_tok,c,win1,win2]\n",
        "        x = x.flatten(0,1) # [b*num_tok,c,win1,win2]\n",
        "        # print(x.shape)\n",
        "        # q, k, v = self.qkv(x).reshape(b, 3, self.n_heads, self.d_head, num_tok, win*win).permute(1,0,2,4,5,3) # [b,3*d_model,num_tok*win,win] -> 3* [b, n_heads, num_tok, win*win, d_head]\n",
        "        # self.qkv(x).flatten(1,-2).unflatten(-1, (self.heads,-1)).chunk(3, dim=-1) # [b,sp,n_heads,d_head]\n",
        "        q,k,v = self.qkv(x).chunk(3, dim=1) # [b*num_tok,d,win,win]\n",
        "        if self.q_stride:\n",
        "            q = self.q_pool(q)\n",
        "            win=[w//s for w,s in zip(win, self.q_stride)] # win = win/q_stride\n",
        "        if math.prod(win) >= 200:\n",
        "            print('MUattn', math.prod(win))\n",
        "            q, k, v = map(lambda t: t.reshape(b*num_tok, self.n_heads, self.d_head, -1).transpose(-2,-1), (q,k,v)) # [b*num_tok, n_heads, win*win, d_head]\n",
        "        else: q, k, v = map(lambda t: t.reshape(b, num_tok, self.n_heads, self.d_head, -1).permute(0,2,1,4,3).flatten(2,3), (q,k,v)) # [b, n_heads, num_tok*win*win, d_head]\n",
        "\n",
        "        q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2) # [b, n_heads, t(/pool), d_head]\n",
        "        context = k.transpose(-2,-1) @ v # [b, n_heads, d_head, d_head]\n",
        "        x = q @ context # [b, n_heads, t(/pool), d_head]\n",
        "\n",
        "        # print('attn fwd 2',x.shape)\n",
        "        # x = x.transpose(1, 3).reshape(B, -1, self.d_model)\n",
        "        x = x.transpose(1,2).reshape(b, -1, self.d_model) # [b,t,d]\n",
        "        # x = x.transpose(-2,-1).reshape(x.shape[0], self.d_model, ) # [b,t,d]\n",
        "        # [b, n_heads, num_tok, win*win, d_head] -> [b, n_heads, d_head, num_tok, win*win] -> [b,c,num_tok*win,win]\n",
        "        # x = x.permute(0,1,4,2,3).reshape(b, self.d_model, num_tok*win,win) # [b, n_heads, num_tok, win*win, d_head]\n",
        "        # x = x.transpose(-2,-1).reshape(b, self.d_model, num_tok, *win) # [b*num_tok, n_heads, win*win, d_head]\n",
        "        x = self.out(x)\n",
        "        L=len(win)\n",
        "        x = x.reshape(b, num_tok, *win, self.d_model).permute(0,1,L+2,*range(2,L+2)) # [b, num_tok, out_dim, *win]\n",
        "        # x = x.unflatten(0, (b, num_tok))\n",
        "        return x # [b,num_tok,c,win1,win2]\n",
        "\n",
        "d_model=16\n",
        "model = MaskUnitAttention(d_model, n_heads=4, q_stride=2)\n",
        "# MaskUnitAttention(d_model=16, n_heads=4, q_stride=None, nd=2)\n",
        "# x=torch.randn(2,4,4,3)\n",
        "# x=torch.randn(2,3,d_model,8,8)\n",
        "x=torch.randn(2,3,d_model,32,32)\n",
        "# [b*num_tok,c,win,win]\n",
        "\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "534cdcbc-fb3b-40e4-e037-32aebaecdac7",
        "id": "ZAyKHKivc0j7",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MUattn 256\n",
            "torch.Size([2, 3, 16, 16, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title hiera vit me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class LayerNorm_at(nn.RMSNorm): # LayerNorm RMSNorm\n",
        "    def __init__(self, dim=1, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.dim=dim\n",
        "    def forward(self, x):\n",
        "        return super().forward(x.transpose(self.dim,-1)).transpose(self.dim,-1)\n",
        "\n",
        "def unshuffle(x, window_shape): # [b,c,h,w] -> [b, h/win1* w/win2, c, win1,win2]\n",
        "    new_shape = list(x.shape[:2]) + [val for xx, win in zip(list(x.shape[2:]), window_shape) for val in [xx//win, win]] # [h,w]->[h/win1, win1, w/win2, win2]\n",
        "    x = x.reshape(new_shape) # [b, c, h/win1, win1, w/win2, win2]\n",
        "    # print('unsh',x.shape, window_shape, new_shape)\n",
        "    L = len(new_shape)\n",
        "    permute = ([0] + list(range(2, L - 1, 2)) + [1] + list(range(3, L, 2))) # [0,2,4,1,3,5] / [0,2,4,6,1,3,5,6]\n",
        "    return x.permute(permute).flatten(1, L//2-1) # [b, h/win1* w/win2, c, win1,win2]\n",
        "\n",
        "class HieraBlock(nn.Module):\n",
        "    # def __init__(self, d_model, n_heads, q_stride=None, mult=4, drop=0, nd=2):\n",
        "    def __init__(self, in_dim, d_model, n_heads, q_stride=None, mult=4, drop=0, nd=2):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        # self.norm = LayerNorm_at(2, d_model) # LayerNorm RMSNorm\n",
        "        self.norm = LayerNorm_at(2, in_dim) # LayerNorm RMSNorm\n",
        "        self.drop = nn.Dropout(drop)\n",
        "        # self.attn = MaskUnitAttention(d_model, n_heads, q_stride)\n",
        "        self.attn = MaskUnitAttention(in_dim, d_model, n_heads, q_stride)\n",
        "        ff_dim=d_model*mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), nn.Linear(d_model, ff_dim), nn.ReLU(), # ReLU GELU\n",
        "            nn.RMSNorm(ff_dim), nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "            # nn.RMSNorm(d_model), act, nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Linear(ff_dim, d_model)\n",
        "            # nn.RMSNorm(d_model), nn.Linear(d_model, ff_dim), nn.ReLU(), nn.Dropout(dropout), # ReLU GELU\n",
        "            # nn.Linear(ff_dim, d_model), nn.Dropout(dropout),\n",
        "        )\n",
        "        # self.res = conv_nd(nd, d_model, d_model, q_stride, q_stride) if q_stride else nn.Identity()\n",
        "\n",
        "        # self.res = nn.Sequential(\n",
        "        #     conv_nd(nd, in_dim, d_model, 1, 1) if in_dim!=d_model else nn.Identity(),\n",
        "        #     # maxpool_nd(nd, q_stride, q_stride) if q_stride else nn.Identity(),\n",
        "        #     avgpool_nd(nd, q_stride, q_stride) if q_stride else nn.Identity(),\n",
        "        # )\n",
        "        self.res = UpDownBlock(in_dim, d_model, kernel=1, r=1/2 if q_stride else 1)\n",
        "\n",
        "        # x = self.proj(x_norm).unflatten(1, (self.attn.q_stride, -1)).max(dim=1).values # pooling res # [b, (Sy, Sx, h/Sy, w/Sx), c] -> [b, (Sy, Sx), (h/Sy, w/Sx), c] -> [b, (h/Sy, w/Sx), c]\n",
        "\n",
        "\n",
        "    def forward(self, x): # [b, num_tok, c, *win]\n",
        "        b, num_tok, c, *win = x.shape\n",
        "        # print('attnblk fwd',x.shape)\n",
        "        # x = x + self.drop(self.self(self.norm(x)))\n",
        "        # print('attnblk fwd',self.res(x.flatten(0,1)).shape, self.drop(self.attn(self.norm(x))).flatten(0,1).shape)\n",
        "        # x = self.res(x.flatten(0,1)) + self.drop(self.attn(self.norm(x))).flatten(0,1) # [b*num_tok,c,win1,win2,win3]\n",
        "        x = self.res(x) + self.drop(self.attn(self.norm(x))) # [b*num_tok,c,win1,win2,win3]\n",
        "        # x = x + self.ff(x)\n",
        "        # x = x + self.ff(x.transpose(1,-1)).transpose(1,-1)\n",
        "        x = x + self.ff(x.transpose(2,-1)).transpose(2,-1)\n",
        "        # x = self.ff(x)\n",
        "        # x = x.unflatten(0, (b, num_tok))\n",
        "        return x\n",
        "\n",
        "    # def forward(self, x): # [b,t,c] # [b, (Sy, Sx, h/Sy, w/Sx), c]\n",
        "    #     # Attention + Q Pooling\n",
        "    #     x_norm = self.norm1(x)\n",
        "    #     if self.dim != self.dim_out:\n",
        "    #         x = self.proj(x_norm).unflatten(1, (self.attn.q_stride, -1)).max(dim=1).values # pooling res # [b, (Sy, Sx, h/Sy, w/Sx), c] -> [b, (Sy, Sx), (h/Sy, w/Sx), c] -> [b, (h/Sy, w/Sx), c]\n",
        "    #     x = x + self.drop_path(self.attn(x_norm))\n",
        "    #     x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "    #     return x\n",
        "\n",
        "\n",
        "class levelBlock(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, n_heads=None, depth=1, r=1):\n",
        "        super().__init__()\n",
        "        self.seq = nn.Sequential(\n",
        "            UpDownBlock(in_ch, out_ch, r=min(1,r)) if in_ch != out_ch or r<1 else nn.Identity(),\n",
        "            # AttentionBlock(d_model, d_model, n_heads, q_stride) if in_ch != out_ch or r<1 else nn.Identity(),\n",
        "            # AttentionBlock(d_model, d_model, n_heads, q_stride=(2,2)),\n",
        "            *[HieraBlock(d_model, d_model, n_heads) for i in range(1)],\n",
        "            # UpDownBlock(out_ch, out_ch, r=r) if r>1 else nn.Identity(),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.seq(x)\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, patch_size, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "    # def __init__(self, in_dim, out_dim, d_model, n_heads, depth):\n",
        "        # patch_size=4\n",
        "        super().__init__()\n",
        "        self.embed = nn.Sequential( # in, out, kernel, stride, pad\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.MaxPool2d(3,2,1), # nn.MaxPool2d(2,2)\n",
        "            nn.Conv2d(in_dim, d_model, 7, 1, 7//2, bias=False),\n",
        "            # nn.Conv2d(in_dim, d_model, patch_size, patch_size), # like patch\n",
        "            # nn.Conv2d(in_dim, d_model, 3, 2, 3//2, bias=False), nn.BatchNorm2d(d_model), nn.ReLU(),\n",
        "            # UpDownBlock(in_dim, dim, r=1/2, kernel=3), UpDownBlock(dim, dim, r=1/2, kernel=3)\n",
        "            # nn.PixelUnshuffle(2), nn.Conv2d(in_dim*2**2, dim, 1, bias=False),\n",
        "            # ResBlock(in_dim, d_model, kernel),\n",
        "            )\n",
        "        # # self.pos_emb = LearnedRoPE2D(dim) # LearnedRoPE2D, RoPE2D\n",
        "\n",
        "        emb_shape = (32,32)\n",
        "        # emb_shape = (8,32,32)\n",
        "        if len(emb_shape) == 3: # for video\n",
        "            pos_spatial = nn.Parameter(torch.randn(1, emb_shape[1]*emb_shape[2], d_model)*.02)\n",
        "            pos_temporal = nn.Parameter(torch.randn(1, emb_shape[0], d_model)*.02)\n",
        "            self.pos_emb = pos_spatial.repeat(1, emb_shape[0], 1) + torch.repeat_interleave(pos_temporal, emb_shape[1] * emb_shape[2], dim=1)\n",
        "        elif len(emb_shape) == 2: # for img\n",
        "            self.pos_emb = nn.Parameter(torch.randn(1, d_model, *emb_shape)*.02) # 56*56=3136\n",
        "        # self.pos_emb = self.pos_emb.flatten(2).transpose(-2,-1)\n",
        "\n",
        "        # self.blocks = nn.Sequential(*[AttentionBlock(d_model, n_heads, q_stride=(2,2) if i in [1,3] else None) for i in range(depth)])\n",
        "        # mult = [1,1,1,1]\n",
        "        mult = [1,2,4,8] # [1,2,3,4] [1,2,2,2]\n",
        "        ch_list = [d_model * m for m in mult] # [128, 256, 384, 512]\n",
        "\n",
        "        self.blocks = nn.Sequential(\n",
        "            # AttentionBlock(ch_list[0], ch_list[1], n_heads, q_stride=(2,2)),\n",
        "            UpDownBlock(ch_list[0], ch_list[1], kernel=1, r=1/2),\n",
        "            HieraBlock(ch_list[1], ch_list[1], n_heads),\n",
        "            # AttentionBlock(ch_list[1], ch_list[2], n_heads, q_stride=(2,2)),\n",
        "            UpDownBlock(ch_list[1], ch_list[2], kernel=1, r=1/2),\n",
        "            HieraBlock(ch_list[2], ch_list[2], n_heads),\n",
        "            )\n",
        "        self.norm = nn.RMSNorm(ch_list[2]) # LayerNorm RMSNorm\n",
        "        self.out = nn.Linear(ch_list[2], out_dim, bias=False) if out_dim and out_dim != ch_list[2] else None\n",
        "\n",
        "\n",
        "    def forward(self, x, context_indices=None): # [b,c,h,w], [b,num_tok]\n",
        "        x = self.embed(x)\n",
        "        # print('vit fwd', x.shape, self.pos_emb.shape)\n",
        "        x = x + self.pos_emb\n",
        "        x = unshuffle(x, (4,4)) # [b,num_tok,c,win1,win2,win3] or [b,1,c,f,h,w]\n",
        "        # if context_indices != None: print('vit fwd ctx', context_indices.shape)\n",
        "        # if context_indices != None: x = x[:,context_indices]\n",
        "        if context_indices != None: x = x[torch.arange(x.shape[0]).unsqueeze(-1), context_indices] # [batch, num_context_toks, d_model]\n",
        "\n",
        "        # print('vit fwd1', x.shape)\n",
        "\n",
        "        x = self.blocks(x) # [b,num_tok,c,1,1,1]\n",
        "        # print('vit fwd2', x.shape)\n",
        "        x = x.squeeze() # [b,num_tok,d]\n",
        "        out = self.norm(x)\n",
        "        if self.out: out = self.out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "# pos_emb rope < learn < learned\n",
        "# conv > pixel?\n",
        "# droppath not required\n",
        "\n",
        "# norm,act,conv < conv,norm,act\n",
        "# 2*s1 < uib < resblock\n",
        "# gatedadaln 3 < 2 = 1 < ffmult4 = 2*gatedadaln\n",
        "# MaxPool2d(2,2) < MaxPool2d(3,2,3//2)\n",
        "\n",
        "# patchattn only for\n",
        "\n",
        "# multiendfusion negligible diff\n",
        "\n",
        "\n",
        "d_model = 64\n",
        "in_dim = 3\n",
        "patch_size = 2\n",
        "model = TransformerModel(patch_size, in_dim, d_model, n_heads=4, nlayers=4, drop=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x = torch.rand((4, in_dim, 32, 32), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 59850\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lec1Nq7nkbgt",
        "outputId": "676e3be1-f1e0-4071-a920-76ed460de80e",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1475904\n",
            "torch.Size([4, 64, 256])\n",
            "1475904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## vit"
      ],
      "metadata": {
        "id": "g3XSZZyUPY1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title FSQ me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def ste_round(x): return x.round().detach() + x - x.detach()\n",
        "\n",
        "class FSQ(nn.Module):\n",
        "    def __init__(self, levels):\n",
        "        super().__init__()\n",
        "        self.levels = torch.tensor(levels, device=device)\n",
        "        self.basis = torch.cumprod(torch.tensor([*levels[1:], 1], device=device).flip(-1), dim=0).flip(-1)\n",
        "        self.half_width = (self.levels-1)/2\n",
        "        self.codebook_size = torch.prod(self.levels).item()\n",
        "        self.codebook = self.indexes_to_codes(torch.arange(self.codebook_size, device=device))\n",
        "\n",
        "    def forward(self, z, beta=1): # beta in (0,1). beta->0 => values more spread out\n",
        "        offset = (self.levels+1) % 2 /2 # .5 if even, 0 if odd\n",
        "        bound = (F.sigmoid(z)-1/2) * (self.levels-beta) + offset\n",
        "        # print('fwd', bound) #\n",
        "        quantized = ste_round(bound)\n",
        "        # print('fwd', quantized) # 4: -1012\n",
        "        return (quantized-offset) / self.half_width # split [-1,1]\n",
        "\n",
        "    def codes_to_indexes(self, zhat):\n",
        "        zhat = (zhat + 1) * self.half_width\n",
        "        return (zhat * self.basis).sum(axis=-1)#.int()\n",
        "\n",
        "    def indexes_to_codes(self, indices):\n",
        "        indices = indices.unsqueeze(-1)\n",
        "        codes = torch.remainder(indices//self.basis, self.levels)\n",
        "        # print(\"codes\",codes)\n",
        "        return codes / self.half_width - 1\n",
        "\n",
        "# fsq = FSQ(levels = [5,4,3,2])\n",
        "# # print(fsq.codebook)\n",
        "# batch_size, seq_len = 2, 4\n",
        "# # x = torch.rand((batch_size, seq_len,3),device=device)\n",
        "# x = torch.linspace(-2,2,7).repeat(4,1).T\n",
        "# la = fsq(x)\n",
        "# print(la)\n",
        "# lact = fsq.codes_to_indexes(la)\n",
        "# print(lact)\n",
        "# # la = fsq.indexes_to_codes(lact)\n",
        "# # print(la)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "eDqkaM2v0_zS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title RoPE2D & RotEmb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "def RoPE(dim, seq_len=512, base=10000):\n",
        "    theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "    pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "    angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "    rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [1, seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "    return rot_emb\n",
        "\n",
        "# class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "#     def __init__(self, dim, seq_len=512, base=10000):\n",
        "#         super().__init__()\n",
        "#         self.dim, self.base = dim, base\n",
        "#         theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "#         pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "#         angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "#         self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [1, seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "#     def forward(self, x): # [batch, T, dim] / [batch, T, n_heads, d_head]\n",
        "#         seq_len = x.size(1)\n",
        "#         if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.base)\n",
        "#         # print('rope fwd', x.shape, self.rot_emb.shape)\n",
        "#         if x.dim()==4: return x * self.rot_emb[:,:seq_len].unsqueeze(2)\n",
        "#         return x * self.rot_emb[:,:seq_len]\n",
        "\n",
        "\n",
        "def RoPE2D(dim=16, h=8, w=8, base=10000):\n",
        "    # theta = 1. / (base ** (torch.arange(0, dim, step=4) / dim))\n",
        "    # theta = 1. / (base**torch.linspace(0,1,dim//4)).unsqueeze(0)\n",
        "    theta = 1. / (base**torch.linspace(0,1,dim//2)).unsqueeze(0)\n",
        "    y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\") # print(y,x) # [h,w], y:row_num, x:col_num\n",
        "    y, x = (y.reshape(-1,1) * theta).unsqueeze(-1), (x.reshape(-1,1) * theta).unsqueeze(-1) # [h*w,1]*[1,dim//4] = [h*w, dim//4, 1]\n",
        "    # rot_emb = torch.cat([x.sin(), x.cos(), y.sin(), y.cos()], dim=-1).flatten(-2) # [h*w, dim//4 ,4] -> [h*w, dim]\n",
        "    # rot_emb = torch.cat([x.sin(), x.cos(), y.sin(), y.cos()], dim=-1)#.reshape(dim, h, w).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "    rot_emb = torch.cat([x.sin(), y.sin()], dim=-1).flatten(-2).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "    # rot_emb = torch.cat([x.cos(), y.cos()], dim=-1).reshape(h, w, dim).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "    return rot_emb\n",
        "\n",
        "\n",
        "\n",
        "# class RoPE2D(nn.Module): # Rotary Positional Embeddings\n",
        "#     def __init__(self, dim, h=224, w=224, base=10000):\n",
        "#         super().__init__()\n",
        "#         self.dim, self.h, self.w = dim, h, w\n",
        "#         # # theta = 1. / (base ** (torch.arange(0, dim, step=4) / dim))\n",
        "#         # theta = 1. / (base**torch.linspace(0,1,dim//4)).unsqueeze(0)\n",
        "#         theta = 1. / (base**torch.linspace(0,1,dim//2)).unsqueeze(0)\n",
        "#         y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\") # print(y,x) # [h,w], y:row_num, x:col_num\n",
        "#         y, x = (y.reshape(-1,1) * theta).unsqueeze(-1), (x.reshape(-1,1) * theta).unsqueeze(-1) # [h*w,1]*[1,dim//4] = [h*w, dim//4, 1]\n",
        "#         # # self.rot_emb = torch.cat([x.sin(), x.cos(), y.sin(), y.cos()], dim=-1).flatten(-2) # [h*w, dim//4 ,4] -> [h*w, dim]\n",
        "#         # self.rot_emb = torch.cat([x.sin(), x.cos(), y.sin(), y.cos()], dim=-1).reshape(h, w, dim).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "#         self.rot_emb = torch.cat([x.sin(), y.sin()], dim=-1).reshape(h, w, dim).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "#         # self.rot_emb = torch.cat([x.cos(), y.cos()], dim=-1).reshape(h, w, dim).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "\n",
        "#     def forward(self, img): #\n",
        "#         # batch, dim, h, w = img.shape\n",
        "#         # print(img.shape)\n",
        "#         hw = img.shape[1] # [b, hw, dim] / [b, hw, n_heads, d_head]\n",
        "#         h=w=int(hw**.5)\n",
        "#         if self.h < h or self.w < w: self.__init__(self.dim, h, w)\n",
        "#         # print(self.rot_emb.shape)\n",
        "#         # rot_emb = self.rot_emb[:, :h, :w].unsqueeze(0) # [1, h, w, dim]\n",
        "#         rot_emb = self.rot_emb[:h, :w] # [h, w, dim]\n",
        "#         # return img * rot_emb.flatten(end_dim=1).unsqueeze(0) # [b, hw, dim] * [1, hw, dim]\n",
        "#         return img * rot_emb.flatten(end_dim=1)[None,:,None,:] # [b, hw, n_heads, d_head] * [1, hw, 1, dim]\n",
        "#         # return img * self.rot_emb\n",
        "\n",
        "class RotEmb(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, top=torch.pi, base=10000):\n",
        "        super().__init__()\n",
        "        self.theta = top / (base ** (torch.arange(0, dim, step=2, device=device) / dim))\n",
        "        # self.theta = top / (base ** torch.linspace(0, 1, dim//2, device=device))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (pos.unsqueeze(-1) * self.theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [seq_len, dim]\n"
      ],
      "metadata": {
        "id": "ge36SCxOl2Oq",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title AttentionBlock\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class SelfAttn(nn.Module):\n",
        "    def __init__(self, dim, n_heads):\n",
        "        super().__init__()\n",
        "        self.dim, self.n_heads = dim, n_heads\n",
        "        d_head = dim//n_heads\n",
        "        self.qkv = nn.Linear(dim, dim*3, bias=False)\n",
        "        # self.lin = nn.Linear(dim, dim)\n",
        "        self.lin = zero_module(nn.Linear(dim, dim))\n",
        "        # self.rope = RoPE(d_head, seq_len=512, base=10000)\n",
        "        # self.rope = RoPE2D(d_head, h=64, w=64, base=100)\n",
        "        self.scale = d_head**-.5\n",
        "\n",
        "    def forward(self, x): # [b,t,d]\n",
        "        q,k,v = self.qkv(x).unflatten(-1, (self.n_heads,-1)).transpose(1,2).chunk(3, dim=-1) # [b, t, n_heads, d_head] -> [b, n_heads, t, d_head]\n",
        "        # q, k = self.rope(q), self.rope(k)\n",
        "        q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2)\n",
        "        context = k.transpose(-2,-1) @ v # [batch, n_heads, d_head, d_head]\n",
        "        x = q @ context # [b, n_heads, t, d_head]\n",
        "        # print('SelfAttn', x.shape)\n",
        "        x = x.transpose(1,2).flatten(2)\n",
        "        return self.lin(x)\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, mult=4, drop=0.):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        # self.norm = nn.RMSNorm(d_model, elementwise_affine=False) # LayerNorm RMSNorm\n",
        "        self.norm1, self.norm2 = nn.LayerNorm(d_model), nn.LayerNorm(d_model)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "        self.attn = SelfAttn(d_model, n_heads)\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        ff_dim=d_model*mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), act, nn.Dropout(drop), nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "            nn.RMSNorm(ff_dim), act, nn.Dropout(drop), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(drop), nn.Linear(d_model, ff_dim), act,\n",
        "            # nn.RMSNorm(ff_dim), nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x): # [batch, seq_len, d_model]\n",
        "        # print('attnblk fwd',x.shape)\n",
        "        x = x + self.drop(self.attn(self.norm1(x)))\n",
        "        x = x + self.ff(x)\n",
        "        # x = x + self.drop(self.ff(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, patch_size, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        patch_size=2\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Conv2d(in_dim, d_model, patch_size, patch_size), # like patch\n",
        "            nn.Conv2d(in_dim, d_model, 7, 1, 7//2, bias=False)\n",
        "            # nn.Conv2d(in_dim, d_model, kernel_size=7, stride=2, padding=7//2, bias=False), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "            # nn.Conv2d(in_dim, d_model, 3, 2, 3//2, bias=False), nn.BatchNorm2d(d_model), nn.ReLU(),\n",
        "            )\n",
        "        # self.embed.requires_grad=False\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, 32*32, d_model)*0.02)\n",
        "        # self.pos_emb = nn.Parameter(RoPE2D(dim=d_model, h=8, w=8, base=1000), requires_grad=False)\n",
        "\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=(32//patch_size)**2, base=10000), requires_grad=False)\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, d_model, n_heads) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x).flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c] # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        # x = self.pos_enc(x)\n",
        "        # print(\"TransformerModel\",x.shape, self.pos_emb.shape)\n",
        "        x = x + self.pos_emb[:,:x.shape[1]]\n",
        "        if context_indices != None: x = x[torch.arange(x.shape[0]).unsqueeze(-1), context_indices] # [batch, num_context_toks, d_model]\n",
        "\n",
        "        x = self.transformer(x)\n",
        "        out = self.norm(x)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "d_model = 64\n",
        "in_dim = 3\n",
        "patch_size = 2\n",
        "model = TransformerModel(patch_size, in_dim, d_model, n_heads=4, nlayers=4, drop=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x = torch.rand((4, in_dim, 32, 32), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "f6T4F651kmGh",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd4ba905-f524-46a4-ca12-8f706f4cf50b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "275456\n",
            "torch.Size([4, 1024, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title masks\n",
        "import torch\n",
        "\n",
        "def multiblock(seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "    return target_mask\n",
        "\n",
        "\n",
        "def multiblock2d(hw=(8,8), scale=(.15,.2), aspect_ratio=(.75,1.5), M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_aspect = torch.rand(1) * (aspect_ratio[1] - aspect_ratio[0]) + aspect_ratio[0] # in (min_s, max_s) # all blocks same size\n",
        "    mask_scale = torch.rand(1) * (scale[1] - scale[0]) + scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    h = (mask_scale/mask_aspect)**.5# h*(h*aspect) = scale\n",
        "    w = h * mask_aspect\n",
        "    h_pos, w_pos = torch.rand(M)*(1-w), torch.rand(M)*(1-h) # in (0, 1 - mask_len)\n",
        "    h_len, h_pos = (h*hw[0]).int(), h_pos*hw[0]\n",
        "    w_len, w_pos = (w*hw[1]).int(), w_pos*hw[1]\n",
        "    h_ind, w_ind = torch.arange(hw[0]).unsqueeze(0), torch.arange(hw[1]).unsqueeze(0) # [1, seq]\n",
        "    h_mask = (h_ind>=h_pos.unsqueeze(-1)) & (h_ind<(h_pos+h_len).unsqueeze(-1)) # [M, seq]\n",
        "    w_mask = (w_ind>=w_pos.unsqueeze(-1)) & (w_ind<(w_pos+w_len).unsqueeze(-1)) # [M, seq]\n",
        "    target_mask = h_mask.unsqueeze(-1) & w_mask.unsqueeze(-2) # [M, seq, seq]\n",
        "    return target_mask\n",
        "\n",
        "# https://arxiv.org/pdf/2210.07224\n",
        "def randpatch(seq, mask_size=8, gamma=0.9): # num patches of seq, mask patch size, masking ratio\n",
        "    # mask = torch.rand(seq//mask_size)<gamma\n",
        "    length = seq//mask_size\n",
        "    g = torch.normal(gamma, std=.1, size=(1,)).clamp(.5,.9)\n",
        "    # g = gamma\n",
        "    idx = torch.randperm(length)[:int(length*g)]\n",
        "    mask = torch.zeros(length, dtype=bool)\n",
        "    mask[idx] = True\n",
        "    mask = mask.repeat_interleave(mask_size, dim=-1)\n",
        "    return mask # [seq] , True -> mask\n",
        "\n",
        "\n",
        "import torch\n",
        "def apply_masks(x, mask): # [b,t,d], [mask_size] # https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "    mask_keep = mask.unsqueeze(-1).repeat(x.size(0), 1, x.size(-1)) # [batch,T,dim]\n",
        "    return torch.gather(x, dim=1, index=mask_keep) # [batch,mask_size,dim]\n",
        "\n",
        "\n",
        "# @title ijepa multiblock next\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "import math\n",
        "from multiprocessing import Value\n",
        "import torch\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(self, hw=(224, 224), enc_mask_scale=(.85,1), pred_mask_scale=(.15,.2), aspect_ratio=(.75,1.25),\n",
        "        nenc=1, npred=2, min_keep=4, allow_overlap=False):\n",
        "        super().__init__()\n",
        "        self.height, self.width = hw\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.aspect_ratio = aspect_ratio\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "\n",
        "    def _sample_block_size(self, scale, aspect_ratio_scale):\n",
        "        _rand = torch.rand(1).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.height * self.width * mask_scale) # num patches to keep\n",
        "        # -- Sample block aspect-ratio\n",
        "        min_ar, max_ar = aspect_ratio_scale\n",
        "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
        "        while h >= self.height: h -= 1 # crop mask to be smaller than img\n",
        "        while w >= self.width: w -= 1\n",
        "        return (h, w)\n",
        "\n",
        "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
        "        h, w = b_size\n",
        "\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # --\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            top = torch.randint(0, self.height - h, (1,))\n",
        "            left = torch.randint(0, self.width - w, (1,))\n",
        "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
        "            mask[top:top+h, left:left+w] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
        "        mask_complement[top:top+h, left:left+w] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, B):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        p_size = self._sample_block_size(scale=self.pred_mask_scale, aspect_ratio_scale=self.aspect_ratio)\n",
        "        e_size = self._sample_block_size(scale=self.enc_mask_scale, aspect_ratio_scale=(1., 1.))\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.height * self.width\n",
        "        min_keep_enc = self.height * self.width\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                print(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "        return collated_masks_enc, collated_masks_pred\n",
        "\n",
        "mask_collator = MaskCollator(hw=(32,32), enc_mask_scale=(.85, 1.), pred_mask_scale=(.15, .2), aspect_ratio=(.75, 1.5),\n",
        "        nenc=1, npred=4, min_keep=4,\n",
        "        # allow_overlap=True)\n",
        "        allow_overlap=False)\n",
        "\n",
        "collated_masks_enc, collated_masks_pred = mask_collator(4)\n",
        "ctx_index, trg_index = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "\n",
        "mask = torch.zeros(1 ,32*32)\n",
        "mask[:, trg_index[:1]] = 1\n",
        "mask[:, ctx_index[:1]] = .5\n",
        "mask = mask.reshape(1,32,32)\n",
        "\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "imshow(mask)\n",
        "\n"
      ],
      "metadata": {
        "id": "pQfM2fYvcTh6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "cellView": "form",
        "outputId": "422653db-4a89-4340-8c70-c41b480c1b2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title simplex\n",
        "# !pip install -q opensimplex\n",
        "import opensimplex\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def simplexmask2d(hw=(32,32), ctx_scale=(.85,1), trg_scale=(.6,.8), B=64, chaos=2):\n",
        "    seed = np.random.randint(1e10, size=B)\n",
        "    ix = iy = np.linspace(0, chaos, num=max(hw))\n",
        "    noise = opensimplex.noise3array(ix, iy, seed) # [b,h,w]\n",
        "    # plt.pcolormesh(noise[:1])\n",
        "    # plt.rcParams[\"figure.figsize\"] = (20,3)\n",
        "    # plt.show()\n",
        "    noise = torch.from_numpy(noise)\n",
        "    ctx_mask_scale = torch.rand(1) * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    trg_mask_scale = torch.rand(1) * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "    val, ind = noise.flatten(1).sort() # [b,h*w]\n",
        "    seq = hw[0]*hw[1]\n",
        "    trg_index = ind[:,-int(seq*trg_mask_scale):]\n",
        "    ctx_index = ind[:,-int(seq*ctx_mask_scale):-int(seq*trg_mask_scale)] # ctx wraps trg ; most similar to multiblock\n",
        "    # ctx_index = ind[:,:-(int(seq*ctx_mask_scale)-int(seq*trg_mask_scale))] # ctx hug bottom\n",
        "    return ctx_index, trg_index\n",
        "\n",
        "# def simplexmask3d(seq=512, ctx_scale=(.85,1), trg_scale=(.6,.8), B=64, chaos=2):\n",
        "def simplexmask3d(thw=(8,32,32), ctx_scale=(.85,1), trg_scale=(.6,.8), B=64, chaos=(.2,2)):\n",
        "    it = np.linspace(0, chaos[0], num=thw[0])\n",
        "    ix = iy = np.linspace(0, chaos[1], num=max(thw[1],thw[2]))\n",
        "    seed = np.random.randint(1e10, size=B)\n",
        "    noise = opensimplex.noise4array(ix, iy, it, seed) # [b,t,h,w]\n",
        "    # plt.pcolormesh(noise[:1])\n",
        "    # plt.rcParams[\"figure.figsize\"] = (20,3)\n",
        "    # plt.show()\n",
        "    noise = torch.from_numpy(noise)\n",
        "    ctx_mask_scale = torch.rand(1) * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    trg_mask_scale = torch.rand(1) * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "    # yy = y.flatten(2).sort()[0][int(hw[0]*hw[1]*mask_scale)]\n",
        "    val, ind = noise.flatten(2).sort() # [b,t,h*w]\n",
        "    # val, ind = noise.flatten(1).sort() # [b,t*h*w]\n",
        "    print(val.shape, ind.shape)\n",
        "    seq = thw[0]*thw[1]*thw[2]\n",
        "    trg_index = ind[:,:,-int(seq*trg_mask_scale):]\n",
        "    ctx_index = ind[:,:,-int(seq*ctx_mask_scale):-int(seq*trg_mask_scale)] # ctx wraps trg ; most similar to multiblock\n",
        "    # # ctx_index = ind[:,:-(int(seq*ctx_mask_scale)-int(seq*trg_mask_scale))] # ctx hug bottom\n",
        "    return ctx_index, trg_index\n",
        "\n",
        "\n",
        "# trg_index, ctx_index = simplexmask1d(seq=500, ctx_scale=(.85,1), trg_scale=(.6,.8), B=64, chaos=3)\n",
        "# simplexmask3d(thw=(8,32,32), ctx_scale=(.85,1), trg_scale=(.6,.8), B=64, chaos=(.2,2))\n",
        "\n",
        "ctx_index, trg_index = simplexmask2d(hw=(32,32), ctx_scale=(.85,1), trg_scale=(.6,.8), B=64, chaos=3)\n",
        "mask = torch.zeros(1 ,32*32)\n",
        "mask[:, trg_index[:1]] = 1\n",
        "mask[:, ctx_index[:1]] = .5\n",
        "mask = mask.reshape(1,32,32)\n",
        "# mask = mask.reshape(32,32)\n",
        "# print(mask)\n",
        "# plt.plot(mask)\n",
        "# from matplotlib import pyplot as plt\n",
        "# # # plt.pcolormesh(y)\n",
        "# plt.pcolormesh(mask)\n",
        "# plt.show()\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "imshow(mask)\n",
        "\n",
        "# from matplotlib import pyplot as plt\n",
        "# # plt.pcolormesh(y)\n",
        "# plt.pcolormesh(mask)\n",
        "# plt.show()\n",
        "\n",
        "# print(mask)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Fh9o__m2-j7K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "outputId": "e8beb64b-c140-4953-bcd9-bfe529c578e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Transformer Predictor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, 32*32, d_model)*0.02)\n",
        "        # self.pos_emb = nn.Parameter(RoPE2D(dim=d_model, h=8, w=8, base=1000), requires_grad=False)\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=32*32, base=10000), requires_grad=False)\n",
        "\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, d_model, n_heads) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.randn(1,1,d_model)*0.02) # randn zeros\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim)# if out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = x.shape\n",
        "        # x = x * self.pos_enc(context_indices)\n",
        "        x = x + self.pos_emb[0,context_indices]\n",
        "\n",
        "        # pred_tokens = self.cls * self.pos_enc(trg_indices) # [M, num_trg_toks, d_model]\n",
        "        pred_tokens = self.cls + self.pos_emb[0,trg_indices]\n",
        "        # print(\"pred fwd\", x.shape, pred_tokens.shape)\n",
        "        x = torch.cat([x, pred_tokens], dim=1) # [batch, seq_len+num_trg_toks, d_model]\n",
        "        out = self.transformer(x)\n",
        "\n",
        "        out = self.norm(out)\n",
        "        out = out[:,seq:] # [batch, num_trg_toks, d_model]\n",
        "        out = self.lin(out)\n",
        "        # print(\"pred fwd\", out.shape)\n",
        "        return out # [seq_len, batch_size, ntoken]\n"
      ],
      "metadata": {
        "id": "M-zdjdJixtOu",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title IJEPA\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class IJEPA(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, nlayers=2, n_heads=4):\n",
        "        super().__init__()\n",
        "        if out_dim is None: out_dim = d_model\n",
        "        self.patch_size = 4 # 4\n",
        "        self.student = TransformerModel(self.patch_size, in_dim, d_model, out_dim=out_dim, n_heads=n_heads, nlayers=nlayers, drop=0.)\n",
        "        # self.predicter = TransformerPredictor(out_dim, d_model//2, out_dim, n_heads=n_heads, nlayers=nlayers//2, drop=0.)\n",
        "        self.predicter = TransformerPredictor(out_dim, 3*d_model//8, out_dim, n_heads=4, nlayers=1, drop=0.)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "\n",
        "    def loss(self, x): #\n",
        "        b,c,h,w = x.shape\n",
        "        # # print(x.shape)\n",
        "\n",
        "        mask_collator = MaskCollator(hw=(8,8), enc_mask_scale=(.85, 1.), pred_mask_scale=(.15, .2), aspect_ratio=(.75, 1.5), nenc=1, npred=4, min_keep=4, allow_overlap=False)\n",
        "        collated_masks_enc, collated_masks_pred = mask_collator(b)\n",
        "        # print(torch.stack(collated_masks_enc).shape, torch.stack(collated_masks_pred).shape, collated_masks_pred[0])\n",
        "        context_indices, trg_indices = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "        # context_indices, trg_indices = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "        # print(context_indices.shape, trg_indices.shape)\n",
        "\n",
        "        # context_indices, trg_indices = simplexmask2d(hw=(8,8), ctx_scale=(.85,1), trg_scale=(.6,.8), B=b, chaos=3)\n",
        "\n",
        "\n",
        "        # print('ijepa loss x',x.shape)\n",
        "\n",
        "        # zero_mask = torch.zeros(b ,8*8, device=device)\n",
        "        # zero_mask[torch.arange(b).unsqueeze(-1), context_indices] = 1\n",
        "        # x_ = x * F.adaptive_avg_pool1d(zero_mask, h*w).unsqueeze(-1) # zero masked locations\n",
        "\n",
        "\n",
        "        # x = x * ~F.interpolate(mask.unsqueeze(1).float(), size=x.shape[2:], mode='nearest-exact').bool() # zero masked locations\n",
        "        # x_ = x * F.adaptive_avg_pool2d((~context_mask).float(), x.shape[1]).unsqueeze(-1) # zero masked locations\n",
        "\n",
        "        sx = self.student(x_, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "        # print('ijepa loss sx',sx.shape)\n",
        "\n",
        "        # print(trg_indices.shape)\n",
        "        sy_ = self.predicter(sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "        sy_ = F.layer_norm(sy_, (sy_.size(-1),))\n",
        "        with torch.no_grad():\n",
        "            sy = self.teacher(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "            sy = sy[torch.arange(sy.shape[0]).unsqueeze(-1), trg_indices] # [batch, num_context_toks, d_model] # nan bec len(trg_ind)==0 # print('loss sy',torch.isnan(sy).any())\n",
        "            sy = F.layer_norm(sy, (sy.size(-1),))\n",
        "        loss = F.mse_loss(sy, sy_)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        sx = self.student(x)\n",
        "        out = sx.mean(dim=1)\n",
        "        return out\n",
        "\n",
        "# min_s=0.15, max_s, M\n",
        "# trg.15.2M4 C.85 1\n",
        "# dont normalise data\n",
        "# randmsk < multiblk < simplex\n",
        "# teacher=trans ~? teacher=copy\n",
        "# learn convemb\n",
        "# lr pred,student: 3e-3/1e-2, 1e-3\n",
        "\n",
        "ijepa = IJEPA(in_dim=3, d_model=32, out_dim=16, nlayers=4, n_heads=4).to(device)#.to(torch.float)\n",
        "# ijepa = IJEPA(in_dim=3, d_model=1024, out_dim=16, nlayers=12, d_head=16).to(device)#.to(torch.float)\n",
        "# optim = torch.optim.AdamW(ijepa.parameters(), lr=1e-3) # 1e-3?\n",
        "optim = torch.optim.AdamW([{'params': ijepa.student.parameters()},\n",
        "    {'params': ijepa.predicter.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # good 3e-3,1e-3 ; default 1e-2, 5e-2\n",
        "    # {'params': ijepa.predicter.parameters(), 'lr': 1e-2}], lr=1e-3, weight_decay=1e-2)\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/configs/in1k_vith14_ep300.yaml\n",
        "# d_model 1024,384\n",
        "# depth 12,6/12\n",
        "# wd 5e-2 - 4e-1\n",
        "# adamw 1e-4 - 1e-3 - 1e-6\n",
        "# ema 0.996-1\n",
        "\n",
        "print(sum(p.numel() for p in ijepa.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in ijepa.parameters())) # 27584\n",
        "# print(sum(p.numel() for p in ijepa.predicter.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in ijepa.student.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in ijepa.teacher.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# d_model^2 * nlayers\n",
        "\n",
        "x = torch.rand((64,3,32,32), device=device)\n",
        "out = ijepa.loss(x)\n",
        "print(out.shape)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(16).to(device)\n",
        "# classifier = Classifier(16, 18).to(device)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n",
        "# optim = torch.optim.AdamW([{'params': ijepa.parameters()}, {'params': classifier.parameters(), 'lr': 1e-3}], lr=1e-3)\n",
        "\n"
      ],
      "metadata": {
        "id": "ardu1zJwdHM3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "3a5c1a6f-cbb5-49b3-f484-5af687f25be2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "406472\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (32) must match the size of tensor b (1024) at non-singleton dimension 2",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-162-37bfda2857ac>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mijepa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-162-37bfda2857ac>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mzero_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mzero_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_indices\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mx_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madaptive_avg_pool1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzero_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# zero masked locations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (1024) at non-singleton dimension 2"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randint(0,10,(64,3,32,32), device=device)"
      ],
      "metadata": {
        "id": "PT-gUOCH7xfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title TransformerVICReg\n",
        "import torch\n",
        "from torch import nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class TransformerVICReg(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, drop=0):\n",
        "        super().__init__()\n",
        "        act = nn.GELU()\n",
        "        patch_size=4\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Linear(in_dim, d_model), act\n",
        "            # nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            # nn.Conv1d(d_model, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            # nn.Conv1d(d_model, d_model,3,2,3//2),\n",
        "            nn.Conv2d(in_dim, d_model, patch_size, patch_size), # patch\n",
        "            )\n",
        "        self.pos_enc = RoPE(d_model, seq_len=200, base=10000)\n",
        "        # self.pos_emb = nn.Parameter(torch.randn(1, 8*8, d_model))\n",
        "        # self.pos_emb = nn.Parameter(RoPE2D(dim=d_model, h=8, w=8, base=10000).unsqueeze(0), requires_grad=False)\n",
        "\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "\n",
        "        # out_dim = out_dim or d_model\n",
        "        self.lin = nn.Linear(d_model, out_dim)\n",
        "        self.attn_pool = nn.Linear(d_model, 1, bias=False)\n",
        "\n",
        "        dim_v = d_model * 4\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(out_dim, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v, bias=False),\n",
        "            )\n",
        "\n",
        "    def forward(self, x): # [b,t,d] / [b,c,h,w]\n",
        "        x = self.embed(x).flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c]\n",
        "        # x = self.embed(x.transpose(-2,-1)).transpose(-2,-1) # [b,t,d]\n",
        "        x = self.pos_enc(x)\n",
        "        # x = x + self.pos_emb\n",
        "\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        attn = self.attn_pool(x).squeeze(-1) # [batch, seq] # seq_pool\n",
        "        out = (torch.softmax(attn, dim=-1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, seq] @ [batch, seq, dim] -> [batch, dim]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch, ntoken]\n",
        "\n",
        "    def expand(self, x):\n",
        "        sx = self.forward(x)\n",
        "        vx = self.exp(sx)\n",
        "        return vx\n",
        "\n",
        "batch, seq_len, d_model = 4,3500,512\n",
        "in_dim, out_dim=3,16\n",
        "model = TransformerVICReg(in_dim, d_model, out_dim, d_head=4, nlayers=2, drop=0.).to(device)\n",
        "# x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "x =  torch.rand((batch, in_dim, 32,32), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ],
      "metadata": {
        "id": "ODpKypTCsfIt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2f4380b-03a8-49be-95b5-eed2b564dc05",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Violet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class Violet(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, nlayers=2, d_head=4):\n",
        "        super().__init__()\n",
        "        out_dim = out_dim or d_model\n",
        "        self.student = TransformerVICReg(in_dim, d_model, out_dim=out_dim, d_head=d_head, nlayers=nlayers, drop=0.)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "\n",
        "        # vicreg\n",
        "        self.sim_coeff=25. # 10.0 # 25.0 # λ\n",
        "        self.std_coeff=25. # 1.0 # 25.0 # µ\n",
        "        self.cov_coeff=1. # 25.0 # 1.0 # ν\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]c/ [b,c,h,w]\n",
        "        # print(x.shape)\n",
        "        # mask = simplexmask(hw=(8,8), scale=(.7,.8)).unsqueeze(0) # .6.8\n",
        "        # vx = self.student.expand(x, mask) # [batch, num_context_toks, out_dim]\n",
        "\n",
        "        vx = self.student.expand(x) # [batch, num_context_toks, out_dim]\n",
        "\n",
        "        with torch.no_grad(): vy = self.teacher.expand(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "        loss = self.vicreg(vx, vy)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        return self.student(x)\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size, num_features = x.shape\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = self.sim_coeff * repr_loss + self.std_coeff * std_loss + self.cov_coeff * cov_loss\n",
        "        print(\"in vicreg \",self.sim_coeff * repr_loss.item() , self.std_coeff * std_loss.item() , self.cov_coeff * cov_loss.item())\n",
        "        # return loss\n",
        "        return repr_loss, std_loss, cov_loss\n",
        "\n",
        "\n",
        "violet = Violet(in_dim=3, d_model=32, out_dim=16, nlayers=2, d_head=4).to(device)\n",
        "voptim = torch.optim.AdamW(violet.parameters(), lr=1e-3) # 1e-3?\n",
        "# voptim = torch.optim.AdamW([{'params': violet.student.transformer.parameters()},\n",
        "#     {'params': violet.student.exp.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # default 1e-2\n",
        "print(sum(p.numel() for p in violet.parameters() if p.requires_grad)) # 27584\n",
        "\n",
        "# x = torch.rand((2,1000,3), device=device)\n",
        "x = torch.rand((2,3,32,32), device=device)\n",
        "# x = torch.rand((2,1,16), device=device)\n",
        "loss = violet.loss(x)\n",
        "# print(out.shape)\n",
        "print(loss)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(16).to(device)\n",
        "# classifier = Classifier(16, 18).to(device)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n",
        "\n"
      ],
      "metadata": {
        "id": "5qwg9dG4sQ3h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f0092f5-5bb7-4725-db2b-dcf7b0ba1a96",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "62850\n",
            "in vicreg  0.00019920778413506923 24.74469393491745 4.793645480560826e-09\n",
            "(tensor(7.9683e-06, grad_fn=<MseLossBackward0>), tensor(0.9898, grad_fn=<AddBackward0>), tensor(4.7936e-09, grad_fn=<AddBackward0>))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in ijepa.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name, param)"
      ],
      "metadata": {
        "id": "m_BFm8Kh-j3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title RankMe\n",
        "# RankMe: Assessing the Downstream Performance of Pretrained Self-Supervised Representations by Their Rank jun 2023 https://arxiv.org/pdf/2210.02885\n",
        "import torch\n",
        "\n",
        "# https://github.com/Spidartist/IJEPA_endoscopy/blob/main/src/helper.py#L22\n",
        "def RankMe(Z):\n",
        "    \"\"\"\n",
        "    Calculate the RankMe score (the higher, the better).\n",
        "    RankMe(Z) = exp(-sum_{k=1}^{min(N, K)} p_k * log(p_k)),\n",
        "    where p_k = sigma_k (Z) / ||sigma_k (Z)||_1 + epsilon\n",
        "    where sigma_k is the kth singular value of Z.\n",
        "    where Z is the matrix of embeddings (N × K)\n",
        "    \"\"\"\n",
        "    # compute the singular values of the embeddings\n",
        "    # _u, s, _vh = torch.linalg.svd(Z, full_matrices=False)  # s.shape = (min(N, K),)\n",
        "    # s = torch.linalg.svd(Z, full_matrices=False).S\n",
        "    s = torch.linalg.svdvals(Z)\n",
        "    p = s / torch.sum(s, axis=0) + 1e-7\n",
        "    return torch.exp(-torch.sum(p * torch.log(p)))\n",
        "\n",
        "# Z = torch.randn(5, 3)\n",
        "# rankme = RankMe(Z)\n",
        "# print(rankme)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "eb1n4BhimG_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title LiDAR\n",
        "# LiDAR: Sensing Linear Probing Performance In Joint Embedding SSL Architectures https://arxiv.org/pdf/2312.04000\n",
        "# https://github.com/rbalestr-lab/stable-ssl/blob/main/stable_ssl/monitors.py#L106\n",
        "\n",
        "def LiDAR(sx, eps=1e-7, delta=1e-3):\n",
        "    sx = sx.unflatten(0, (-1, 2))\n",
        "    n, q, d = sx.shape\n",
        "    mu_x = sx.mean(dim=1) # mu_x # [n,d]\n",
        "    mu = mu_x.mean(dim=0) # mu # [d]\n",
        "\n",
        "    diff_b = (mu_x - mu).unsqueeze(-1) # [n,d,1]\n",
        "    S_b = (diff_b @ diff_b.transpose(-2,-1)).sum(0) / (n - 1) # [n,d,d] -> [d,d]\n",
        "    diff_w = (sx - mu_x.unsqueeze(1)).reshape(-1,d,1) # [n,q,d] -> [nq,d,1]\n",
        "    S_w = (diff_w @ diff_w.transpose(-2,-1)).sum(0) / (n * (q - 1)) + delta * torch.eye(d, device=sx.device) # [nq,d,d] -> [d,d]\n",
        "\n",
        "    eigvals_w, eigvecs_w = torch.linalg.eigh(S_w)\n",
        "    eigvals_w = torch.clamp(eigvals_w, min=eps)\n",
        "\n",
        "    invsqrt_w = (eigvecs_w * (1. / torch.sqrt(eigvals_w))) @ eigvecs_w.transpose(-1, -2)\n",
        "    S_lidar = invsqrt_w @ S_b @ invsqrt_w\n",
        "    lam = torch.linalg.eigh(S_lidar)[0].clamp(min=0)\n",
        "    p = lam / lam.sum() + eps\n",
        "    return torch.exp(-torch.sum(p * torch.log(p)))\n",
        "\n",
        "# sx = torch.randn(32, 128)\n",
        "# print(sx.shape)\n",
        "# lidar = LiDAR(sx)\n",
        "# print(lidar.item())\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "y24ZNFqW7woQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34de8800-fa4b-4460-803c-3cd8dbfb6c70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ab7c7b6-8e8a-424e-ef38-a635df462805",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "strain 0.26996612548828125\n",
            "strain 0.370406836271286\n",
            "strain 0.4116763174533844\n",
            "strain 0.42137765884399414\n",
            "strain 0.39525631070137024\n",
            "strain 0.3397923409938812\n",
            "strain 0.2864193022251129\n",
            "strain 0.5152043700218201\n",
            "strain 0.3197158873081207\n",
            "strain 0.3295278251171112\n",
            "strain 0.3855665922164917\n",
            "strain 0.25959548354148865\n",
            "strain 0.39221489429473877\n",
            "strain 0.31160804629325867\n",
            "strain 0.27877041697502136\n",
            "strain 0.4111042320728302\n",
            "strain 0.36898073554039\n",
            "strain 0.4706434905529022\n",
            "strain 0.2586915194988251\n",
            "strain 0.39588865637779236\n",
            "classify 2.1385498046875\n",
            "classify 1.9569091796875\n",
            "classify 1.9822998046875\n",
            "classify 2.17779541015625\n",
            "classify 2.0545654296875\n",
            "classify 2.1124267578125\n",
            "classify 2.0196533203125\n",
            "classify 2.0400390625\n",
            "classify 2.0126953125\n",
            "classify 1.8173828125\n",
            "classify 2.0811767578125\n",
            "0.265625\n",
            "0.171875\n",
            "0.203125\n",
            "0.25\n",
            "0.25\n",
            "0.25\n",
            "0.28125\n",
            "0.3125\n",
            "0.15625\n",
            "0.25\n",
            "0.25\n",
            "433\n",
            "strain 0.28771308064460754\n",
            "strain 0.42915305495262146\n",
            "strain 0.28016144037246704\n",
            "strain 0.387515664100647\n",
            "strain 0.2679629921913147\n",
            "strain 0.286283016204834\n",
            "strain 0.5406668782234192\n",
            "strain 0.37790629267692566\n",
            "strain 0.405242383480072\n",
            "strain 0.42606228590011597\n",
            "strain 0.4102134108543396\n",
            "strain 0.3855912685394287\n",
            "strain 0.3209020495414734\n",
            "strain 0.4624849557876587\n",
            "strain 0.3633323907852173\n",
            "strain 0.2680973708629608\n",
            "strain 0.2831612527370453\n",
            "strain 0.3982699513435364\n",
            "strain 0.3094630837440491\n",
            "strain 0.3950338661670685\n",
            "strain 0.41430482268333435\n",
            "strain 0.3526526689529419\n",
            "strain 0.3248481750488281\n",
            "strain 0.3228369653224945\n",
            "strain 0.3100586533546448\n",
            "strain 0.24452568590641022\n",
            "strain 0.26331281661987305\n",
            "strain 0.4008632004261017\n",
            "strain 0.4413924217224121\n",
            "strain 0.2972029447555542\n",
            "strain 0.35276514291763306\n",
            "strain 0.3576434552669525\n",
            "strain 0.4366835951805115\n",
            "strain 0.33680102229118347\n",
            "strain 0.41827619075775146\n",
            "strain 0.28834301233291626\n",
            "strain 0.41498276591300964\n",
            "strain 0.41012075543403625\n",
            "strain 0.5018552541732788\n",
            "strain 0.3154374361038208\n",
            "strain 0.4830799400806427\n",
            "strain 0.2850795090198517\n",
            "strain 0.3184806704521179\n",
            "strain 0.501850426197052\n",
            "strain 0.3577440083026886\n",
            "strain 0.3935137391090393\n",
            "strain 0.3423580527305603\n",
            "strain 0.5360926985740662\n",
            "strain 0.37351563572883606\n",
            "strain 0.35259300470352173\n",
            "strain 0.2831799387931824\n",
            "classify 1.97607421875\n",
            "classify 2.0098876953125\n",
            "classify 1.9599609375\n",
            "classify 2.1192626953125\n",
            "classify 2.07427978515625\n",
            "classify 2.1407470703125\n",
            "classify 2.115478515625\n",
            "classify 2.01904296875\n",
            "classify 2.03619384765625\n",
            "classify 2.0216064453125\n",
            "classify 2.162841796875\n",
            "0.34375\n",
            "0.3125\n",
            "0.21875\n",
            "0.265625\n",
            "0.265625\n",
            "0.25\n",
            "0.25\n",
            "0.171875\n",
            "0.21875\n",
            "0.28125\n",
            "0.15625\n",
            "434\n",
            "strain 0.4941160976886749\n",
            "strain 0.3632166087627411\n",
            "strain 0.3589073717594147\n",
            "strain 0.32179751992225647\n",
            "strain 0.4028320014476776\n",
            "strain 0.3205658495426178\n",
            "strain 0.3844837248325348\n",
            "strain 0.3285672068595886\n",
            "strain 0.3305012285709381\n",
            "strain 0.28969576954841614\n",
            "strain 0.36485037207603455\n",
            "strain 0.4097318649291992\n",
            "strain 0.4337986707687378\n",
            "strain 0.28013861179351807\n",
            "strain 0.4078204929828644\n",
            "strain 0.268144428730011\n",
            "strain 0.31888195872306824\n",
            "strain 0.45146411657333374\n",
            "strain 0.2947285771369934\n",
            "strain 0.3988562822341919\n",
            "strain 0.42607560753822327\n",
            "strain 0.3359706401824951\n",
            "strain 0.32650861144065857\n",
            "strain 0.3435727655887604\n",
            "strain 0.24619877338409424\n",
            "strain 0.4277578592300415\n",
            "strain 0.2952843904495239\n",
            "strain 0.30305859446525574\n",
            "strain 0.28885480761528015\n",
            "strain 0.4119819104671478\n",
            "strain 0.4548793137073517\n",
            "strain 0.4502265155315399\n",
            "strain 0.4423622786998749\n",
            "strain 0.39048317074775696\n",
            "strain 0.3023991584777832\n",
            "strain 0.35054436326026917\n",
            "strain 0.3993665874004364\n",
            "strain 0.3576523959636688\n",
            "strain 0.4502842426300049\n",
            "strain 0.43570205569267273\n",
            "strain 0.25432825088500977\n",
            "strain 0.36160022020339966\n",
            "strain 0.4220603406429291\n",
            "strain 0.2531575858592987\n",
            "strain 0.42940181493759155\n",
            "strain 0.4046897888183594\n",
            "strain 0.38574883341789246\n",
            "strain 0.38409894704818726\n",
            "strain 0.2533853054046631\n",
            "strain 0.2744966149330139\n",
            "strain 0.29509884119033813\n",
            "classify 1.9737548828125\n",
            "classify 2.10357666015625\n",
            "classify 2.1409912109375\n",
            "classify 1.8961181640625\n",
            "classify 1.9886474609375\n",
            "classify 1.9154052734375\n",
            "classify 2.0687255859375\n",
            "classify 2.1390380859375\n",
            "classify 2.0604248046875\n",
            "classify 2.03045654296875\n",
            "classify 2.05206298828125\n",
            "0.171875\n",
            "0.3125\n",
            "0.1875\n",
            "0.359375\n",
            "0.203125\n",
            "0.171875\n",
            "0.34375\n",
            "0.25\n",
            "0.203125\n",
            "0.25\n",
            "0.3125\n",
            "435\n",
            "strain 0.4116714596748352\n",
            "strain 0.325996071100235\n",
            "strain 0.4598718285560608\n",
            "strain 0.3378031551837921\n",
            "strain 0.4378584325313568\n",
            "strain 0.2441428154706955\n",
            "strain 0.40358346700668335\n",
            "strain 0.4514942169189453\n",
            "strain 0.33106905221939087\n",
            "strain 0.3303575813770294\n",
            "strain 0.45384588837623596\n",
            "strain 0.2917709946632385\n",
            "strain 0.35134950280189514\n",
            "strain 0.4579693675041199\n",
            "strain 0.2665775418281555\n",
            "strain 0.2787075638771057\n",
            "strain 0.2243645340204239\n",
            "strain 0.3552159368991852\n",
            "strain 0.24684180319309235\n",
            "strain 0.40800297260284424\n",
            "strain 0.5107827186584473\n",
            "strain 0.351297527551651\n",
            "strain 0.46170568466186523\n",
            "strain 0.30167490243911743\n",
            "strain 0.2958124876022339\n",
            "strain 0.2475934475660324\n",
            "strain 0.28614526987075806\n",
            "strain 0.38537153601646423\n",
            "strain 0.24927301704883575\n",
            "strain 0.451940655708313\n",
            "strain 0.26813679933547974\n",
            "strain 0.509054958820343\n",
            "strain 0.4378223717212677\n",
            "strain 0.38035523891448975\n",
            "strain 0.4778633713722229\n",
            "strain 0.2936209738254547\n",
            "strain 0.33231568336486816\n",
            "strain 0.3409075438976288\n",
            "strain 0.27235326170921326\n",
            "strain 0.26782163977622986\n",
            "strain 0.36518654227256775\n",
            "strain 0.3887060582637787\n",
            "strain 0.3739575147628784\n",
            "strain 0.49292588233947754\n",
            "strain 0.4296940565109253\n",
            "strain 0.2675536870956421\n",
            "strain 0.3071438670158386\n",
            "strain 0.25725364685058594\n",
            "strain 0.35203802585601807\n",
            "strain 0.4151402413845062\n",
            "strain 0.2989073693752289\n",
            "classify 1.9615478515625\n",
            "classify 2.2548828125\n",
            "classify 2.07080078125\n",
            "classify 2.07666015625\n",
            "classify 1.9471435546875\n",
            "classify 2.01470947265625\n",
            "classify 2.0662841796875\n",
            "classify 2.062255859375\n",
            "classify 2.044921875\n",
            "classify 2.1778564453125\n",
            "classify 2.00347900390625\n",
            "0.171875\n",
            "0.25\n",
            "0.328125\n",
            "0.140625\n",
            "0.28125\n",
            "0.203125\n",
            "0.28125\n",
            "0.234375\n",
            "0.25\n",
            "0.25\n",
            "0.171875\n",
            "436\n",
            "strain 0.5592901706695557\n",
            "strain 0.4100596010684967\n",
            "strain 0.28474125266075134\n",
            "strain 0.32161810994148254\n",
            "strain 0.39598074555397034\n",
            "strain 0.41510993242263794\n",
            "strain 0.4506046175956726\n",
            "strain 0.27174481749534607\n",
            "strain 0.4189039170742035\n",
            "strain 0.2545969486236572\n",
            "strain 0.4731141924858093\n",
            "strain 0.26822128891944885\n",
            "strain 0.3077363073825836\n",
            "strain 0.28041526675224304\n",
            "strain 0.2871221601963043\n",
            "strain 0.2579757273197174\n",
            "strain 0.2956581115722656\n",
            "strain 0.4139860272407532\n",
            "strain 0.2945980727672577\n",
            "strain 0.295956552028656\n",
            "strain 0.3319578766822815\n",
            "strain 0.38913729786872864\n",
            "strain 0.5284516215324402\n",
            "strain 0.4009409546852112\n",
            "strain 0.4228402078151703\n",
            "strain 0.4618520140647888\n",
            "strain 0.29571881890296936\n",
            "strain 0.28503531217575073\n",
            "strain 0.3008902370929718\n",
            "strain 0.2950257956981659\n",
            "strain 0.45581957697868347\n",
            "strain 0.4538118243217468\n",
            "strain 0.4148593544960022\n",
            "strain 0.2587931156158447\n",
            "strain 0.441172331571579\n",
            "strain 0.35024720430374146\n",
            "strain 0.2660232484340668\n",
            "strain 0.36319613456726074\n",
            "strain 0.3911191523075104\n",
            "strain 0.27055248618125916\n",
            "strain 0.40945929288864136\n",
            "strain 0.5522927045822144\n",
            "strain 0.26583677530288696\n",
            "strain 0.452377051115036\n",
            "strain 0.5249684453010559\n",
            "strain 0.3833090662956238\n",
            "strain 0.41762834787368774\n",
            "strain 0.521639347076416\n",
            "strain 0.2713330388069153\n",
            "strain 0.29567405581474304\n",
            "strain 0.39940640330314636\n",
            "classify 2.08056640625\n",
            "classify 2.1427001953125\n",
            "classify 2.0042724609375\n",
            "classify 1.99700927734375\n",
            "classify 1.9974365234375\n",
            "classify 2.1270751953125\n",
            "classify 2.19158935546875\n",
            "classify 1.9830322265625\n",
            "classify 1.9349365234375\n",
            "classify 2.0416259765625\n",
            "classify 1.93798828125\n",
            "0.328125\n",
            "0.171875\n",
            "0.25\n",
            "0.1875\n",
            "0.328125\n",
            "0.203125\n",
            "0.296875\n",
            "0.296875\n",
            "0.1875\n",
            "0.359375\n",
            "0.21875\n",
            "437\n",
            "strain 0.3407830595970154\n",
            "strain 0.5440579652786255\n",
            "strain 0.4384251534938812\n",
            "strain 0.3691713809967041\n",
            "strain 0.543904721736908\n",
            "strain 0.42709705233573914\n",
            "strain 0.3457324504852295\n",
            "strain 0.3062301576137543\n",
            "strain 0.35157889127731323\n",
            "strain 0.48983538150787354\n",
            "strain 0.31148451566696167\n",
            "strain 0.2447802871465683\n",
            "strain 0.44361743330955505\n",
            "strain 0.35413169860839844\n",
            "strain 0.4499066174030304\n",
            "strain 0.2848370671272278\n",
            "strain 0.30702272057533264\n",
            "strain 0.3040848970413208\n",
            "strain 0.39961346983909607\n",
            "strain 0.4819779694080353\n",
            "strain 0.5344977974891663\n",
            "strain 0.43863600492477417\n",
            "strain 0.3393109142780304\n",
            "strain 0.2874653935432434\n",
            "strain 0.28976118564605713\n",
            "strain 0.2624324858188629\n",
            "strain 0.3976418077945709\n",
            "strain 0.2765858471393585\n",
            "strain 0.47623229026794434\n",
            "strain 0.3437444567680359\n",
            "strain 0.29919734597206116\n",
            "strain 0.31657910346984863\n",
            "strain 0.306010901927948\n",
            "strain 0.2643381357192993\n",
            "strain 0.4087914526462555\n",
            "strain 0.43018338084220886\n",
            "strain 0.33756694197654724\n",
            "strain 0.26710188388824463\n",
            "strain 0.30826956033706665\n",
            "strain 0.4126269519329071\n",
            "strain 0.27131474018096924\n",
            "strain 0.34629902243614197\n",
            "strain 0.2919132113456726\n",
            "strain 0.36374709010124207\n",
            "strain 0.399118572473526\n",
            "strain 0.31155842542648315\n",
            "strain 0.27884095907211304\n",
            "strain 0.4884164333343506\n",
            "strain 0.4438825249671936\n",
            "strain 0.2754081189632416\n",
            "strain 0.23656757175922394\n",
            "classify 2.02044677734375\n",
            "classify 2.09161376953125\n",
            "classify 2.10784912109375\n",
            "classify 2.03411865234375\n",
            "classify 2.02978515625\n",
            "classify 1.9962158203125\n",
            "classify 2.07659912109375\n",
            "classify 2.09320068359375\n",
            "classify 1.88836669921875\n",
            "classify 2.0230712890625\n",
            "classify 2.10211181640625\n",
            "0.21875\n",
            "0.34375\n",
            "0.28125\n",
            "0.203125\n",
            "0.28125\n",
            "0.15625\n",
            "0.203125\n",
            "0.25\n",
            "0.359375\n",
            "0.25\n",
            "0.15625\n",
            "438\n",
            "strain 0.39669474959373474\n",
            "strain 0.327945739030838\n",
            "strain 0.2758856415748596\n",
            "strain 0.29478490352630615\n",
            "strain 0.38784417510032654\n",
            "strain 0.28397369384765625\n",
            "strain 0.297378271818161\n",
            "strain 0.2402053028345108\n",
            "strain 0.3066953420639038\n",
            "strain 0.29594138264656067\n",
            "strain 0.3931732773780823\n",
            "strain 0.29331016540527344\n",
            "strain 0.47847121953964233\n",
            "strain 0.48062244057655334\n",
            "strain 0.40370774269104004\n",
            "strain 0.4010446071624756\n",
            "strain 0.47597694396972656\n",
            "strain 0.3012436032295227\n",
            "strain 0.29783397912979126\n",
            "strain 0.5155231952667236\n",
            "strain 0.36787959933280945\n",
            "strain 0.2264709174633026\n",
            "strain 0.29764753580093384\n",
            "strain 0.3912701904773712\n",
            "strain 0.3082554340362549\n",
            "strain 0.4888036847114563\n",
            "strain 0.4138434827327728\n",
            "strain 0.35554301738739014\n",
            "strain 0.32558637857437134\n",
            "strain 0.3712100386619568\n",
            "strain 0.34023261070251465\n",
            "strain 0.341181218624115\n",
            "strain 0.48193830251693726\n",
            "strain 0.2938844561576843\n",
            "strain 0.49954700469970703\n",
            "strain 0.40896379947662354\n",
            "strain 0.45163655281066895\n",
            "strain 0.25357872247695923\n",
            "strain 0.5744955539703369\n",
            "strain 0.3885161280632019\n",
            "strain 0.40890854597091675\n",
            "strain 0.3444965183734894\n",
            "strain 0.4235455095767975\n",
            "strain 0.3585732877254486\n",
            "strain 0.3163723349571228\n",
            "strain 0.5291376113891602\n",
            "strain 0.4537862241268158\n",
            "strain 0.3690507113933563\n",
            "strain 0.4290359914302826\n",
            "strain 0.255779892206192\n",
            "strain 0.27088192105293274\n",
            "classify 1.95306396484375\n",
            "classify 1.97979736328125\n",
            "classify 1.86444091796875\n",
            "classify 1.99188232421875\n",
            "classify 2.046142578125\n",
            "classify 2.0078125\n",
            "classify 2.111083984375\n",
            "classify 2.12603759765625\n",
            "classify 2.07568359375\n",
            "classify 1.894775390625\n",
            "classify 1.94744873046875\n",
            "0.15625\n",
            "0.296875\n",
            "0.296875\n",
            "0.203125\n",
            "0.25\n",
            "0.28125\n",
            "0.34375\n",
            "0.203125\n",
            "0.25\n",
            "0.328125\n",
            "0.3125\n",
            "439\n",
            "strain 0.5093322396278381\n",
            "strain 0.28089457750320435\n",
            "strain 0.4376429319381714\n",
            "strain 0.38063371181488037\n",
            "strain 0.31358036398887634\n",
            "strain 0.33168521523475647\n",
            "strain 0.3057102859020233\n",
            "strain 0.4158100485801697\n",
            "strain 0.29125794768333435\n",
            "strain 0.29160451889038086\n",
            "strain 0.3917515277862549\n",
            "strain 0.45711615681648254\n",
            "strain 0.33208799362182617\n",
            "strain 0.554652988910675\n",
            "strain 0.44098326563835144\n",
            "strain 0.4076061248779297\n",
            "strain 0.37010428309440613\n",
            "strain 0.31265074014663696\n",
            "strain 0.46402233839035034\n",
            "strain 0.4154360890388489\n",
            "strain 0.3304313123226166\n",
            "strain 0.29915449023246765\n",
            "strain 0.5081844925880432\n",
            "strain 0.391872376203537\n",
            "strain 0.3767802119255066\n",
            "strain 0.24288280308246613\n",
            "strain 0.4188386797904968\n",
            "strain 0.5205973386764526\n",
            "strain 0.44562041759490967\n",
            "strain 0.4820077419281006\n",
            "strain 0.43129563331604004\n",
            "strain 0.4589287340641022\n",
            "strain 0.46457141637802124\n",
            "strain 0.37561607360839844\n",
            "strain 0.3020707070827484\n",
            "strain 0.5489106774330139\n",
            "strain 0.39107486605644226\n",
            "strain 0.276639461517334\n",
            "strain 0.27261850237846375\n",
            "strain 0.39842018485069275\n",
            "strain 0.3436703383922577\n",
            "strain 0.4281482696533203\n",
            "strain 0.2633923292160034\n",
            "strain 0.28155478835105896\n",
            "strain 0.2871783375740051\n",
            "strain 0.4063554108142853\n",
            "strain 0.3328046500682831\n",
            "strain 0.4510338008403778\n",
            "strain 0.33957159519195557\n",
            "strain 0.4243764877319336\n",
            "strain 0.48563656210899353\n",
            "classify 2.05126953125\n",
            "classify 2.113037109375\n",
            "classify 1.9417724609375\n",
            "classify 1.95452880859375\n",
            "classify 2.1171875\n",
            "classify 1.8973388671875\n",
            "classify 1.97552490234375\n",
            "classify 2.03448486328125\n",
            "classify 2.1971435546875\n",
            "classify 2.11676025390625\n",
            "classify 2.06103515625\n",
            "0.21875\n",
            "0.21875\n",
            "0.21875\n",
            "0.21875\n",
            "0.265625\n",
            "0.21875\n",
            "0.171875\n",
            "0.296875\n",
            "0.1875\n",
            "0.1875\n",
            "0.296875\n",
            "440\n",
            "strain 0.3813056945800781\n",
            "strain 0.43305325508117676\n",
            "strain 0.24479258060455322\n",
            "strain 0.5107109546661377\n",
            "strain 0.34870463609695435\n",
            "strain 0.38490697741508484\n",
            "strain 0.36861714720726013\n",
            "strain 0.28600460290908813\n",
            "strain 0.45236554741859436\n",
            "strain 0.39055687189102173\n",
            "strain 0.32748931646347046\n",
            "strain 0.34263402223587036\n",
            "strain 0.30782726407051086\n",
            "strain 0.3794551193714142\n",
            "strain 0.3765072822570801\n",
            "strain 0.38406169414520264\n",
            "strain 0.3609236478805542\n",
            "strain 0.4498274326324463\n",
            "strain 0.3964080810546875\n",
            "strain 0.28439587354660034\n",
            "strain 0.3980071246623993\n",
            "strain 0.2734861373901367\n",
            "strain 0.35486137866973877\n",
            "strain 0.37898924946784973\n",
            "strain 0.2623842656612396\n",
            "strain 0.335778146982193\n",
            "strain 0.3107401430606842\n",
            "strain 0.3683203458786011\n",
            "strain 0.2502312958240509\n",
            "strain 0.25213515758514404\n",
            "strain 0.34945231676101685\n",
            "strain 0.28364598751068115\n",
            "strain 0.345182329416275\n",
            "strain 0.3260822296142578\n",
            "strain 0.22903212904930115\n",
            "strain 0.44585004448890686\n",
            "strain 0.317403107881546\n",
            "strain 0.26394546031951904\n",
            "strain 0.4199734330177307\n",
            "strain 0.3817233145236969\n",
            "strain 0.35992684960365295\n",
            "strain 0.4656912386417389\n",
            "strain 0.3109615743160248\n",
            "strain 0.4745315909385681\n",
            "strain 0.4593430459499359\n",
            "strain 0.2789855897426605\n",
            "strain 0.388448566198349\n",
            "strain 0.386674702167511\n",
            "strain 0.32764387130737305\n",
            "strain 0.43537309765815735\n",
            "strain 0.30946311354637146\n",
            "classify 2.1036376953125\n",
            "classify 2.130615234375\n",
            "classify 1.96075439453125\n",
            "classify 2.13623046875\n",
            "classify 2.0850830078125\n",
            "classify 2.076904296875\n",
            "classify 2.03485107421875\n",
            "classify 2.0543212890625\n",
            "classify 2.07098388671875\n",
            "classify 2.1517333984375\n",
            "classify 2.0869140625\n",
            "0.21875\n",
            "0.203125\n",
            "0.296875\n",
            "0.296875\n",
            "0.234375\n",
            "0.234375\n",
            "0.265625\n",
            "0.171875\n",
            "0.28125\n",
            "0.3125\n",
            "0.296875\n",
            "441\n",
            "strain 0.3114907145500183\n",
            "strain 0.32070139050483704\n",
            "strain 0.3665148615837097\n",
            "strain 0.3722037374973297\n",
            "strain 0.4951888918876648\n",
            "strain 0.32003313302993774\n",
            "strain 0.40062248706817627\n",
            "strain 0.4438443183898926\n",
            "strain 0.3101184070110321\n",
            "strain 0.3336988687515259\n",
            "strain 0.3660063147544861\n",
            "strain 0.4930999279022217\n",
            "strain 0.322225958108902\n",
            "strain 0.4366505444049835\n",
            "strain 0.3293083906173706\n",
            "strain 0.49286097288131714\n",
            "strain 0.3874289393424988\n",
            "strain 0.4807063341140747\n",
            "strain 0.3070032000541687\n",
            "strain 0.28261053562164307\n",
            "strain 0.44087520241737366\n",
            "strain 0.2408246099948883\n",
            "strain 0.5130909085273743\n",
            "strain 0.3651529848575592\n",
            "strain 0.36413317918777466\n",
            "strain 0.36052754521369934\n",
            "strain 0.4364815950393677\n",
            "strain 0.46942517161369324\n",
            "strain 0.3773512840270996\n",
            "strain 0.3713339567184448\n",
            "strain 0.29505524039268494\n",
            "strain 0.411791056394577\n",
            "strain 0.3713792562484741\n",
            "strain 0.30214959383010864\n",
            "strain 0.3291889727115631\n",
            "strain 0.4068388342857361\n",
            "strain 0.28614506125450134\n",
            "strain 0.3082698881626129\n",
            "strain 0.33577194809913635\n",
            "strain 0.3501347601413727\n",
            "strain 0.3582421541213989\n",
            "strain 0.4750816524028778\n",
            "strain 0.41384199261665344\n",
            "strain 0.4167897403240204\n",
            "strain 0.3450740873813629\n",
            "strain 0.3016152083873749\n",
            "strain 0.4457436501979828\n",
            "strain 0.28779760003089905\n",
            "strain 0.4942391812801361\n",
            "strain 0.262680321931839\n",
            "strain 0.3635304570198059\n",
            "classify 2.09100341796875\n",
            "classify 1.99688720703125\n",
            "classify 2.05029296875\n",
            "classify 2.056640625\n",
            "classify 1.986572265625\n",
            "classify 2.04193115234375\n",
            "classify 1.88397216796875\n",
            "classify 2.05352783203125\n",
            "classify 1.97784423828125\n",
            "classify 2.02410888671875\n",
            "classify 2.1082763671875\n",
            "0.203125\n",
            "0.203125\n",
            "0.234375\n",
            "0.234375\n",
            "0.265625\n",
            "0.234375\n",
            "0.265625\n",
            "0.203125\n",
            "0.28125\n",
            "0.15625\n",
            "0.1875\n",
            "442\n",
            "strain 0.3991794288158417\n",
            "strain 0.43989652395248413\n",
            "strain 0.41332095861434937\n",
            "strain 0.42501822113990784\n",
            "strain 0.39427268505096436\n",
            "strain 0.5632563233375549\n",
            "strain 0.43736934661865234\n",
            "strain 0.3455065190792084\n",
            "strain 0.27132290601730347\n",
            "strain 0.3087494969367981\n",
            "strain 0.49968183040618896\n",
            "strain 0.36544767022132874\n",
            "strain 0.47857406735420227\n",
            "strain 0.408718466758728\n",
            "strain 0.39534732699394226\n",
            "strain 0.4447493553161621\n",
            "strain 0.4073296785354614\n",
            "strain 0.3463457226753235\n",
            "strain 0.3073764145374298\n",
            "strain 0.3335212469100952\n",
            "strain 0.2860657572746277\n",
            "strain 0.339511513710022\n",
            "strain 0.3665197789669037\n",
            "strain 0.5236368775367737\n",
            "strain 0.3166514039039612\n",
            "strain 0.2529735863208771\n",
            "strain 0.47480034828186035\n",
            "strain 0.2851828634738922\n",
            "strain 0.3863266110420227\n",
            "strain 0.273307204246521\n",
            "strain 0.2490251213312149\n",
            "strain 0.2890496253967285\n",
            "strain 0.4494643807411194\n",
            "strain 0.28965750336647034\n",
            "strain 0.42184165120124817\n",
            "strain 0.2588176429271698\n",
            "strain 0.5685812830924988\n",
            "strain 0.24842414259910583\n",
            "strain 0.4346889555454254\n",
            "strain 0.4080014228820801\n",
            "strain 0.24374675750732422\n",
            "strain 0.2907085120677948\n",
            "strain 0.516465425491333\n",
            "strain 0.32698988914489746\n",
            "strain 0.3410658836364746\n",
            "strain 0.35354146361351013\n",
            "strain 0.2632976770401001\n",
            "strain 0.26762911677360535\n",
            "strain 0.5859688520431519\n",
            "strain 0.3285980820655823\n",
            "strain 0.41119182109832764\n",
            "classify 2.11248779296875\n",
            "classify 2.11199951171875\n",
            "classify 2.0052490234375\n",
            "classify 2.039306640625\n",
            "classify 2.088623046875\n",
            "classify 2.1231689453125\n",
            "classify 1.988037109375\n",
            "classify 1.97991943359375\n",
            "classify 1.981689453125\n",
            "classify 1.98486328125\n",
            "classify 1.9625244140625\n",
            "0.3125\n",
            "0.21875\n",
            "0.296875\n",
            "0.265625\n",
            "0.328125\n",
            "0.296875\n",
            "0.390625\n",
            "0.296875\n",
            "0.234375\n",
            "0.171875\n",
            "0.421875\n",
            "443\n",
            "strain 0.4701946973800659\n",
            "strain 0.3571580946445465\n",
            "strain 0.33706605434417725\n",
            "strain 0.2829391062259674\n",
            "strain 0.334850013256073\n",
            "strain 0.3646796643733978\n",
            "strain 0.26521337032318115\n",
            "strain 0.27582794427871704\n",
            "strain 0.4305286705493927\n",
            "strain 0.3524690568447113\n",
            "strain 0.3607841432094574\n",
            "strain 0.4730616509914398\n",
            "strain 0.3303978741168976\n",
            "strain 0.38987040519714355\n",
            "strain 0.3209264576435089\n",
            "strain 0.5070415139198303\n",
            "strain 0.32178881764411926\n",
            "strain 0.261675089597702\n",
            "strain 0.46252572536468506\n",
            "strain 0.36783072352409363\n",
            "strain 0.3277476131916046\n",
            "strain 0.2763994336128235\n",
            "strain 0.30716317892074585\n",
            "strain 0.44318708777427673\n",
            "strain 0.31031855940818787\n",
            "strain 0.24955087900161743\n",
            "strain 0.37150028347969055\n",
            "strain 0.3953990042209625\n",
            "strain 0.3780764043331146\n",
            "strain 0.36100178956985474\n",
            "strain 0.32907408475875854\n",
            "strain 0.28334805369377136\n",
            "strain 0.36844882369041443\n",
            "strain 0.3005293011665344\n",
            "strain 0.2547547221183777\n",
            "strain 0.25277209281921387\n",
            "strain 0.26413264870643616\n",
            "strain 0.4065324366092682\n",
            "strain 0.3562731444835663\n",
            "strain 0.2651523947715759\n",
            "strain 0.35689494013786316\n",
            "strain 0.4688358008861542\n",
            "strain 0.5507938265800476\n",
            "strain 0.24728937447071075\n",
            "strain 0.251698762178421\n",
            "strain 0.3497768044471741\n",
            "strain 0.4061894416809082\n",
            "strain 0.3638610243797302\n",
            "strain 0.4036003351211548\n",
            "strain 0.2913673520088196\n",
            "strain 0.4418632388114929\n",
            "classify 2.1087646484375\n",
            "classify 2.0615234375\n",
            "classify 1.96356201171875\n",
            "classify 2.00238037109375\n",
            "classify 2.0694580078125\n",
            "classify 2.073486328125\n",
            "classify 2.07769775390625\n",
            "classify 2.13671875\n",
            "classify 2.143310546875\n",
            "classify 2.1080322265625\n",
            "classify 2.049560546875\n",
            "0.296875\n",
            "0.1875\n",
            "0.265625\n",
            "0.296875\n",
            "0.25\n",
            "0.171875\n",
            "0.203125\n",
            "0.203125\n",
            "0.3125\n",
            "0.25\n",
            "0.234375\n",
            "444\n",
            "strain 0.303348571062088\n",
            "strain 0.2940274178981781\n",
            "strain 0.42433446645736694\n",
            "strain 0.34256553649902344\n",
            "strain 0.3071691393852234\n",
            "strain 0.4844639003276825\n",
            "strain 0.3836466073989868\n",
            "strain 0.5575959086418152\n",
            "strain 0.41981837153434753\n",
            "strain 0.4148595333099365\n",
            "strain 0.3354969024658203\n",
            "strain 0.31773826479911804\n",
            "strain 0.23560252785682678\n",
            "strain 0.3257423937320709\n",
            "strain 0.29400527477264404\n",
            "strain 0.4101051390171051\n",
            "strain 0.27296704053878784\n",
            "strain 0.2741330862045288\n",
            "strain 0.3367654085159302\n",
            "strain 0.29180094599723816\n",
            "strain 0.5063710808753967\n",
            "strain 0.3696702718734741\n",
            "strain 0.5254130959510803\n",
            "strain 0.27940836548805237\n",
            "strain 0.3240966200828552\n",
            "strain 0.48364314436912537\n",
            "strain 0.26277124881744385\n",
            "strain 0.4401419460773468\n",
            "strain 0.382623553276062\n",
            "strain 0.4174438416957855\n",
            "strain 0.357891708612442\n",
            "strain 0.28816065192222595\n",
            "strain 0.5428285598754883\n",
            "strain 0.44618842005729675\n",
            "strain 0.37439221143722534\n",
            "strain 0.309758722782135\n",
            "strain 0.2746485769748688\n",
            "strain 0.4874323904514313\n",
            "strain 0.41460099816322327\n",
            "strain 0.27491316199302673\n",
            "strain 0.2663841247558594\n",
            "strain 0.36861371994018555\n",
            "strain 0.29372119903564453\n",
            "strain 0.3313349187374115\n",
            "strain 0.40220603346824646\n",
            "strain 0.27537092566490173\n",
            "strain 0.3300230801105499\n",
            "strain 0.5469217896461487\n",
            "strain 0.2803037762641907\n",
            "strain 0.26330387592315674\n",
            "strain 0.37828513979911804\n",
            "classify 1.94891357421875\n",
            "classify 1.9962158203125\n",
            "classify 2.03497314453125\n",
            "classify 2.05322265625\n",
            "classify 2.104736328125\n",
            "classify 2.11785888671875\n",
            "classify 2.04486083984375\n",
            "classify 1.94232177734375\n",
            "classify 2.156494140625\n",
            "classify 2.0264892578125\n",
            "classify 2.13525390625\n",
            "0.359375\n",
            "0.3125\n",
            "0.234375\n",
            "0.171875\n",
            "0.25\n",
            "0.234375\n",
            "0.234375\n",
            "0.21875\n",
            "0.1875\n",
            "0.296875\n",
            "0.25\n",
            "445\n",
            "strain 0.3183499872684479\n",
            "strain 0.2700136601924896\n",
            "strain 0.3883349895477295\n",
            "strain 0.4147048890590668\n",
            "strain 0.3407334089279175\n",
            "strain 0.25060853362083435\n",
            "strain 0.42666253447532654\n",
            "strain 0.25811997056007385\n",
            "strain 0.2655622661113739\n",
            "strain 0.24364221096038818\n",
            "strain 0.3221817910671234\n",
            "strain 0.44134634733200073\n",
            "strain 0.4328027665615082\n",
            "strain 0.328467458486557\n",
            "strain 0.36452487111091614\n",
            "strain 0.27912279963493347\n",
            "strain 0.29950323700904846\n",
            "strain 0.31748166680336\n",
            "strain 0.4279484152793884\n",
            "strain 0.29176947474479675\n",
            "strain 0.3091748058795929\n",
            "strain 0.4128339886665344\n",
            "strain 0.3694661855697632\n",
            "strain 0.4404226839542389\n",
            "strain 0.39498019218444824\n",
            "strain 0.39238452911376953\n",
            "strain 0.40614810585975647\n",
            "strain 0.2598908841609955\n",
            "strain 0.3237348198890686\n",
            "strain 0.4314231872558594\n",
            "strain 0.29549458622932434\n",
            "strain 0.25853607058525085\n",
            "strain 0.42047128081321716\n",
            "strain 0.37863922119140625\n",
            "strain 0.38731271028518677\n",
            "strain 0.42378315329551697\n",
            "strain 0.4054962396621704\n",
            "strain 0.3472684621810913\n",
            "strain 0.3425258994102478\n",
            "strain 0.2957768142223358\n",
            "strain 0.322159081697464\n",
            "strain 0.4365222752094269\n",
            "strain 0.34525054693222046\n",
            "strain 0.2913135886192322\n",
            "strain 0.2864220142364502\n",
            "strain 0.2787682116031647\n",
            "strain 0.32744261622428894\n",
            "strain 0.44329163432121277\n",
            "strain 0.2701115310192108\n",
            "strain 0.552221417427063\n",
            "strain 0.3917858302593231\n",
            "classify 2.03424072265625\n",
            "classify 2.10693359375\n",
            "classify 2.0126953125\n",
            "classify 2.03265380859375\n",
            "classify 1.91412353515625\n",
            "classify 1.98980712890625\n",
            "classify 1.86358642578125\n",
            "classify 2.007080078125\n",
            "classify 2.25537109375\n",
            "classify 2.01275634765625\n",
            "classify 2.19573974609375\n",
            "0.28125\n",
            "0.25\n",
            "0.296875\n",
            "0.21875\n",
            "0.328125\n",
            "0.15625\n",
            "0.171875\n",
            "0.171875\n",
            "0.328125\n",
            "0.21875\n",
            "0.109375\n",
            "446\n",
            "strain 0.4337185025215149\n",
            "strain 0.3435016870498657\n",
            "strain 0.3638279438018799\n",
            "strain 0.37468990683555603\n",
            "strain 0.4171448349952698\n",
            "strain 0.2788565456867218\n",
            "strain 0.35554394125938416\n",
            "strain 0.2734214663505554\n",
            "strain 0.46017593145370483\n",
            "strain 0.4731242060661316\n",
            "strain 0.41819632053375244\n",
            "strain 0.3845878541469574\n",
            "strain 0.3254144787788391\n",
            "strain 0.30429351329803467\n",
            "strain 0.33128923177719116\n",
            "strain 0.29919540882110596\n",
            "strain 0.39427492022514343\n",
            "strain 0.27831944823265076\n",
            "strain 0.26528266072273254\n",
            "strain 0.3337264060974121\n",
            "strain 0.4467756450176239\n",
            "strain 0.2916974723339081\n",
            "strain 0.3020835220813751\n",
            "strain 0.4227394163608551\n",
            "strain 0.4146135151386261\n",
            "strain 0.2860865592956543\n",
            "strain 0.4006899297237396\n",
            "strain 0.3428575098514557\n",
            "strain 0.35661348700523376\n",
            "strain 0.28714489936828613\n",
            "strain 0.25513914227485657\n",
            "strain 0.2727878987789154\n",
            "strain 0.3983224034309387\n",
            "strain 0.532481849193573\n",
            "strain 0.3963908851146698\n",
            "strain 0.2755254805088043\n",
            "strain 0.3597775399684906\n",
            "strain 0.3206804692745209\n",
            "strain 0.32081732153892517\n",
            "strain 0.27991530299186707\n",
            "strain 0.2754901945590973\n",
            "strain 0.4442543685436249\n",
            "strain 0.38452911376953125\n",
            "strain 0.4905174672603607\n",
            "strain 0.33312320709228516\n",
            "strain 0.3585939407348633\n",
            "strain 0.2934771776199341\n",
            "strain 0.4060174524784088\n",
            "strain 0.24901993572711945\n",
            "strain 0.2960129976272583\n",
            "strain 0.40745970606803894\n",
            "classify 2.05474853515625\n",
            "classify 2.12347412109375\n",
            "classify 1.9588623046875\n",
            "classify 2.11456298828125\n",
            "classify 2.0811767578125\n",
            "classify 2.056640625\n",
            "classify 2.1527099609375\n",
            "classify 2.00164794921875\n",
            "classify 2.007568359375\n",
            "classify 2.01318359375\n",
            "classify 2.06744384765625\n",
            "0.203125\n",
            "0.21875\n",
            "0.21875\n",
            "0.15625\n",
            "0.21875\n",
            "0.34375\n",
            "0.203125\n",
            "0.21875\n",
            "0.203125\n",
            "0.234375\n",
            "0.25\n",
            "447\n",
            "strain 0.24442420899868011\n",
            "strain 0.4210478365421295\n",
            "strain 0.2581678032875061\n",
            "strain 0.40085485577583313\n",
            "strain 0.4294179677963257\n",
            "strain 0.31666022539138794\n",
            "strain 0.3618544042110443\n",
            "strain 0.29134002327919006\n",
            "strain 0.555027961730957\n",
            "strain 0.2851310670375824\n",
            "strain 0.32083624601364136\n",
            "strain 0.3234800100326538\n",
            "strain 0.4462067782878876\n",
            "strain 0.25803157687187195\n",
            "strain 0.35930562019348145\n",
            "strain 0.4196398854255676\n",
            "strain 0.26229754090309143\n",
            "strain 0.3008296489715576\n",
            "strain 0.3961173892021179\n",
            "strain 0.2906801104545593\n",
            "strain 0.4410991370677948\n",
            "strain 0.31123512983322144\n",
            "strain 0.3461708724498749\n",
            "strain 0.2580397427082062\n",
            "strain 0.4108966886997223\n",
            "strain 0.4214962422847748\n",
            "strain 0.38603296875953674\n",
            "strain 0.41898313164711\n",
            "strain 0.3506988286972046\n",
            "strain 0.3647807538509369\n",
            "strain 0.25860193371772766\n",
            "strain 0.3933876156806946\n",
            "strain 0.4964253604412079\n",
            "strain 0.3866565525531769\n",
            "strain 0.43544861674308777\n",
            "strain 0.29461830854415894\n",
            "strain 0.32150790095329285\n",
            "strain 0.3880266547203064\n",
            "strain 0.33863455057144165\n",
            "strain 0.32710227370262146\n",
            "strain 0.4442070722579956\n",
            "strain 0.40345779061317444\n",
            "strain 0.3478146195411682\n",
            "strain 0.43983665108680725\n",
            "strain 0.29909488558769226\n",
            "strain 0.4053274989128113\n",
            "strain 0.4649368226528168\n",
            "strain 0.49381059408187866\n",
            "strain 0.41044119000434875\n",
            "strain 0.27025964856147766\n",
            "strain 0.2909682095050812\n",
            "classify 1.9676513671875\n",
            "classify 1.994140625\n",
            "classify 2.05047607421875\n",
            "classify 2.0572509765625\n",
            "classify 2.00408935546875\n",
            "classify 2.0269775390625\n",
            "classify 2.04754638671875\n",
            "classify 2.095458984375\n",
            "classify 2.07470703125\n",
            "classify 2.00567626953125\n",
            "classify 1.92669677734375\n",
            "0.234375\n",
            "0.296875\n",
            "0.296875\n",
            "0.140625\n",
            "0.28125\n",
            "0.15625\n",
            "0.21875\n",
            "0.234375\n",
            "0.15625\n",
            "0.1875\n",
            "0.1875\n",
            "448\n",
            "strain 0.28260812163352966\n",
            "strain 0.49733760952949524\n",
            "strain 0.37532275915145874\n",
            "strain 0.29608091711997986\n",
            "strain 0.289337158203125\n",
            "strain 0.448665976524353\n",
            "strain 0.39595988392829895\n",
            "strain 0.2723071575164795\n",
            "strain 0.47950267791748047\n",
            "strain 0.4086819291114807\n",
            "strain 0.3453407883644104\n",
            "strain 0.27580487728118896\n",
            "strain 0.35087427496910095\n",
            "strain 0.34808555245399475\n",
            "strain 0.4300980269908905\n",
            "strain 0.25438040494918823\n",
            "strain 0.41472819447517395\n",
            "strain 0.4510272741317749\n",
            "strain 0.49445638060569763\n",
            "strain 0.2786576747894287\n",
            "strain 0.3376026451587677\n",
            "strain 0.3093193471431732\n",
            "strain 0.42266055941581726\n",
            "strain 0.4377038776874542\n",
            "strain 0.26000842452049255\n",
            "strain 0.3637465536594391\n",
            "strain 0.38193410634994507\n",
            "strain 0.3383671045303345\n",
            "strain 0.4458797574043274\n",
            "strain 0.4217314124107361\n",
            "strain 0.30669835209846497\n",
            "strain 0.40326377749443054\n",
            "strain 0.2964717447757721\n",
            "strain 0.3462177813053131\n",
            "strain 0.26776930689811707\n",
            "strain 0.35396966338157654\n",
            "strain 0.4888169765472412\n",
            "strain 0.24977849423885345\n",
            "strain 0.37252771854400635\n",
            "strain 0.45181894302368164\n",
            "strain 0.3969497084617615\n",
            "strain 0.3658582270145416\n",
            "strain 0.2513481378555298\n",
            "strain 0.2733713686466217\n",
            "strain 0.25000080466270447\n",
            "strain 0.30861973762512207\n",
            "strain 0.3407144844532013\n",
            "strain 0.34188148379325867\n",
            "strain 0.2887830436229706\n",
            "strain 0.33021193742752075\n",
            "strain 0.34693819284439087\n",
            "classify 1.962890625\n",
            "classify 2.1171875\n",
            "classify 1.93115234375\n",
            "classify 2.1661376953125\n",
            "classify 2.18994140625\n",
            "classify 2.1007080078125\n",
            "classify 1.98455810546875\n",
            "classify 2.01190185546875\n",
            "classify 2.06201171875\n",
            "classify 2.0330810546875\n",
            "classify 1.949951171875\n",
            "0.25\n",
            "0.234375\n",
            "0.234375\n",
            "0.203125\n",
            "0.125\n",
            "0.203125\n",
            "0.15625\n",
            "0.25\n",
            "0.328125\n",
            "0.25\n",
            "0.296875\n",
            "449\n",
            "strain 0.42420172691345215\n",
            "strain 0.45293736457824707\n",
            "strain 0.29263874888420105\n",
            "strain 0.23696747422218323\n",
            "strain 0.3185812830924988\n",
            "strain 0.3508397042751312\n",
            "strain 0.3661281168460846\n",
            "strain 0.2902471423149109\n",
            "strain 0.34454166889190674\n",
            "strain 0.2799420654773712\n",
            "strain 0.2928905189037323\n",
            "strain 0.4064326286315918\n",
            "strain 0.26344209909439087\n",
            "strain 0.43481627106666565\n",
            "strain 0.3540990948677063\n",
            "strain 0.3051067590713501\n",
            "strain 0.3128242492675781\n",
            "strain 0.35688701272010803\n",
            "strain 0.2772970497608185\n",
            "strain 0.45261773467063904\n",
            "strain 0.4807349741458893\n",
            "strain 0.2885591685771942\n",
            "strain 0.271679162979126\n",
            "strain 0.2814965546131134\n",
            "strain 0.25925958156585693\n",
            "strain 0.3030362129211426\n",
            "strain 0.3877021074295044\n",
            "strain 0.2792987823486328\n",
            "strain 0.3192105293273926\n",
            "strain 0.4484960734844208\n",
            "strain 0.3811098039150238\n",
            "strain 0.4085209369659424\n",
            "strain 0.286221981048584\n",
            "strain 0.4245028793811798\n",
            "strain 0.41875940561294556\n",
            "strain 0.2791857421398163\n",
            "strain 0.28219056129455566\n",
            "strain 0.42429453134536743\n",
            "strain 0.3945409655570984\n",
            "strain 0.32186833024024963\n",
            "strain 0.5135365128517151\n",
            "strain 0.40696588158607483\n",
            "strain 0.397240549325943\n",
            "strain 0.43452584743499756\n",
            "strain 0.35795801877975464\n",
            "strain 0.44371941685676575\n",
            "strain 0.4027927815914154\n",
            "strain 0.3927587866783142\n",
            "strain 0.3101789951324463\n",
            "strain 0.34392404556274414\n",
            "strain 0.2592124938964844\n",
            "classify 2.0589599609375\n",
            "classify 2.07330322265625\n",
            "classify 2.0919189453125\n",
            "classify 1.9810791015625\n",
            "classify 2.0421142578125\n",
            "classify 2.012939453125\n",
            "classify 2.1890869140625\n",
            "classify 2.02252197265625\n",
            "classify 2.0738525390625\n",
            "classify 1.981201171875\n",
            "classify 2.11102294921875\n",
            "0.21875\n",
            "0.25\n",
            "0.25\n",
            "0.234375\n",
            "0.21875\n",
            "0.25\n",
            "0.234375\n",
            "0.296875\n",
            "0.25\n",
            "0.296875\n",
            "0.296875\n",
            "450\n",
            "strain 0.2408900111913681\n",
            "strain 0.2698654532432556\n",
            "strain 0.31409311294555664\n",
            "strain 0.4336703419685364\n",
            "strain 0.40199142694473267\n",
            "strain 0.5171275734901428\n",
            "strain 0.2753847539424896\n",
            "strain 0.2695554792881012\n",
            "strain 0.3062692880630493\n",
            "strain 0.4167807400226593\n",
            "strain 0.2896023392677307\n",
            "strain 0.30152806639671326\n",
            "strain 0.4427002966403961\n",
            "strain 0.49881279468536377\n",
            "strain 0.42171186208724976\n",
            "strain 0.25405746698379517\n",
            "strain 0.2932261526584625\n",
            "strain 0.2783121168613434\n",
            "strain 0.2941056787967682\n",
            "strain 0.31049424409866333\n",
            "strain 0.314446359872818\n",
            "strain 0.3839665353298187\n",
            "strain 0.5190514922142029\n",
            "strain 0.4243493974208832\n",
            "strain 0.296726793050766\n",
            "strain 0.2605263888835907\n",
            "strain 0.33557385206222534\n",
            "strain 0.3870905339717865\n",
            "strain 0.362827867269516\n",
            "strain 0.36877521872520447\n",
            "strain 0.2448320835828781\n",
            "strain 0.2922097444534302\n",
            "strain 0.4353438913822174\n",
            "strain 0.3034132719039917\n",
            "strain 0.42967721819877625\n",
            "strain 0.29811903834342957\n",
            "strain 0.4216243624687195\n",
            "strain 0.4450416564941406\n",
            "strain 0.2889902889728546\n",
            "strain 0.4176824688911438\n",
            "strain 0.3531586527824402\n",
            "strain 0.3586398661136627\n",
            "strain 0.42212265729904175\n",
            "strain 0.3314600884914398\n",
            "strain 0.2971383333206177\n",
            "strain 0.30089253187179565\n",
            "strain 0.4063223898410797\n",
            "strain 0.25563541054725647\n",
            "strain 0.4356403350830078\n",
            "strain 0.4663386642932892\n",
            "strain 0.33585894107818604\n",
            "classify 2.07708740234375\n",
            "classify 1.924072265625\n",
            "classify 2.133056640625\n",
            "classify 2.06158447265625\n",
            "classify 2.053955078125\n",
            "classify 2.03466796875\n",
            "classify 2.009033203125\n",
            "classify 2.1005859375\n",
            "classify 1.966796875\n",
            "classify 2.0692138671875\n",
            "classify 1.94512939453125\n",
            "0.265625\n",
            "0.171875\n",
            "0.3125\n",
            "0.25\n",
            "0.25\n",
            "0.203125\n",
            "0.3125\n",
            "0.234375\n",
            "0.21875\n",
            "0.3125\n",
            "0.25\n",
            "451\n",
            "strain 0.41288888454437256\n",
            "strain 0.3313109278678894\n",
            "strain 0.24659621715545654\n",
            "strain 0.361430287361145\n",
            "strain 0.2955009639263153\n",
            "strain 0.2049942910671234\n",
            "strain 0.3319048583507538\n",
            "strain 0.3284265100955963\n",
            "strain 0.37752416729927063\n",
            "strain 0.39164215326309204\n",
            "strain 0.2692963778972626\n",
            "strain 0.22902949154376984\n",
            "strain 0.275042861700058\n",
            "strain 0.49668776988983154\n",
            "strain 0.46767473220825195\n",
            "strain 0.4158328175544739\n",
            "strain 0.41192683577537537\n",
            "strain 0.4872201979160309\n",
            "strain 0.3084135353565216\n",
            "strain 0.4920678436756134\n",
            "strain 0.37032580375671387\n",
            "strain 0.3362426161766052\n",
            "strain 0.33851975202560425\n",
            "strain 0.3999985158443451\n",
            "strain 0.31935328245162964\n",
            "strain 0.30379366874694824\n",
            "strain 0.30816200375556946\n",
            "strain 0.43110740184783936\n",
            "strain 0.4437001645565033\n",
            "strain 0.23704616725444794\n",
            "strain 0.4932619631290436\n",
            "strain 0.30490201711654663\n",
            "strain 0.4478813409805298\n",
            "strain 0.31840968132019043\n",
            "strain 0.5215637683868408\n",
            "strain 0.27918481826782227\n",
            "strain 0.265007883310318\n",
            "strain 0.26227307319641113\n",
            "strain 0.2808336913585663\n",
            "strain 0.42899230122566223\n",
            "strain 0.5428321957588196\n",
            "strain 0.3995070159435272\n",
            "strain 0.44947537779808044\n",
            "strain 0.27595046162605286\n",
            "strain 0.3033691942691803\n",
            "strain 0.4176543951034546\n",
            "strain 0.510065495967865\n",
            "strain 0.2824069559574127\n",
            "strain 0.3287395238876343\n",
            "strain 0.34908589720726013\n",
            "strain 0.45128437876701355\n",
            "classify 2.02752685546875\n",
            "classify 2.05084228515625\n",
            "classify 2.0316162109375\n",
            "classify 2.023681640625\n",
            "classify 2.08599853515625\n",
            "classify 2.05615234375\n",
            "classify 1.9422607421875\n",
            "classify 1.98980712890625\n",
            "classify 1.9969482421875\n",
            "classify 2.0252685546875\n",
            "classify 1.99810791015625\n",
            "0.28125\n",
            "0.296875\n",
            "0.234375\n",
            "0.328125\n",
            "0.234375\n",
            "0.171875\n",
            "0.265625\n",
            "0.1875\n",
            "0.25\n",
            "0.1875\n",
            "0.21875\n",
            "452\n",
            "strain 0.4033638834953308\n",
            "strain 0.4169439375400543\n",
            "strain 0.3820314109325409\n",
            "strain 0.3817911744117737\n",
            "strain 0.34106406569480896\n",
            "strain 0.3743496537208557\n",
            "strain 0.2954102158546448\n",
            "strain 0.24730679392814636\n",
            "strain 0.26121649146080017\n",
            "strain 0.28948503732681274\n",
            "strain 0.3499460816383362\n",
            "strain 0.2782266139984131\n",
            "strain 0.27720531821250916\n",
            "strain 0.27177920937538147\n",
            "strain 0.3916196823120117\n",
            "strain 0.3505079448223114\n",
            "strain 0.32735350728034973\n",
            "strain 0.3033040165901184\n",
            "strain 0.25045037269592285\n",
            "strain 0.3926534652709961\n",
            "strain 0.49508136510849\n",
            "strain 0.24744050204753876\n",
            "strain 0.30422818660736084\n",
            "strain 0.3852192759513855\n",
            "strain 0.26158276200294495\n",
            "strain 0.30538082122802734\n",
            "strain 0.2430054247379303\n",
            "strain 0.3213106691837311\n",
            "strain 0.3594458997249603\n",
            "strain 0.41469335556030273\n",
            "strain 0.32265332341194153\n",
            "strain 0.34103477001190186\n",
            "strain 0.41452011466026306\n",
            "strain 0.3843432068824768\n",
            "strain 0.45994821190834045\n",
            "strain 0.2708515524864197\n",
            "strain 0.23816967010498047\n",
            "strain 0.3427755832672119\n",
            "strain 0.4317145049571991\n",
            "strain 0.2906956374645233\n",
            "strain 0.4408531188964844\n",
            "strain 0.2971230149269104\n",
            "strain 0.5048831701278687\n",
            "strain 0.3917529582977295\n",
            "strain 0.2689148783683777\n",
            "strain 0.32869046926498413\n",
            "strain 0.4162514805793762\n",
            "strain 0.36231669783592224\n",
            "strain 0.27754294872283936\n",
            "strain 0.32962286472320557\n",
            "strain 0.46280211210250854\n",
            "classify 2.10748291015625\n",
            "classify 1.948486328125\n",
            "classify 2.04736328125\n",
            "classify 1.92938232421875\n",
            "classify 1.92681884765625\n",
            "classify 1.99029541015625\n",
            "classify 2.14373779296875\n",
            "classify 2.1602783203125\n",
            "classify 2.179443359375\n",
            "classify 2.0050048828125\n",
            "classify 1.98211669921875\n",
            "0.265625\n",
            "0.25\n",
            "0.1875\n",
            "0.34375\n",
            "0.28125\n",
            "0.359375\n",
            "0.296875\n",
            "0.109375\n",
            "0.28125\n",
            "0.21875\n",
            "0.234375\n",
            "453\n",
            "strain 0.3899837136268616\n",
            "strain 0.29164186120033264\n",
            "strain 0.25712671875953674\n",
            "strain 0.3843661844730377\n",
            "strain 0.23043029010295868\n",
            "strain 0.2715364992618561\n",
            "strain 0.33641543984413147\n",
            "strain 0.3067242205142975\n",
            "strain 0.4257199764251709\n",
            "strain 0.3070300221443176\n",
            "strain 0.3021192252635956\n",
            "strain 0.2702445685863495\n",
            "strain 0.3221244513988495\n",
            "strain 0.30173394083976746\n",
            "strain 0.4126494824886322\n",
            "strain 0.4804511070251465\n",
            "strain 0.26078927516937256\n",
            "strain 0.3203882575035095\n",
            "strain 0.24062924087047577\n",
            "strain 0.4105577766895294\n",
            "strain 0.25938957929611206\n",
            "strain 0.33792954683303833\n",
            "strain 0.36887386441230774\n",
            "strain 0.2444731891155243\n",
            "strain 0.44391292333602905\n",
            "strain 0.4541787803173065\n",
            "strain 0.29880809783935547\n",
            "strain 0.40678009390830994\n",
            "strain 0.2923148572444916\n",
            "strain 0.28927531838417053\n",
            "strain 0.39395204186439514\n",
            "strain 0.3584412634372711\n",
            "strain 0.2902834117412567\n",
            "strain 0.36042314767837524\n",
            "strain 0.30501627922058105\n",
            "strain 0.32498326897621155\n",
            "strain 0.27506062388420105\n",
            "strain 0.25787776708602905\n",
            "strain 0.3662494122982025\n",
            "strain 0.2990986108779907\n",
            "strain 0.429911345243454\n",
            "strain 0.3203777074813843\n",
            "strain 0.3383132815361023\n",
            "strain 0.5074257254600525\n",
            "strain 0.3290296196937561\n",
            "strain 0.42163702845573425\n",
            "strain 0.3055485486984253\n",
            "strain 0.4966011047363281\n",
            "strain 0.2586648464202881\n",
            "strain 0.3608885407447815\n",
            "strain 0.5615798234939575\n",
            "classify 2.01629638671875\n",
            "classify 2.0367431640625\n",
            "classify 1.9605712890625\n",
            "classify 2.09417724609375\n",
            "classify 1.89935302734375\n",
            "classify 2.1165771484375\n",
            "classify 2.0003662109375\n",
            "classify 2.02490234375\n",
            "classify 1.98895263671875\n",
            "classify 2.00994873046875\n",
            "classify 1.8916015625\n",
            "0.1875\n",
            "0.25\n",
            "0.25\n",
            "0.21875\n",
            "0.25\n",
            "0.359375\n",
            "0.21875\n",
            "0.234375\n",
            "0.234375\n",
            "0.28125\n",
            "0.1875\n",
            "454\n",
            "strain 0.3506769835948944\n",
            "strain 0.3673619031906128\n",
            "strain 0.405529648065567\n",
            "strain 0.2714851498603821\n",
            "strain 0.3186657130718231\n",
            "strain 0.4405449628829956\n",
            "strain 0.25819131731987\n",
            "strain 0.28814369440078735\n",
            "strain 0.44183528423309326\n",
            "strain 0.36665746569633484\n",
            "strain 0.3092069625854492\n",
            "strain 0.33367103338241577\n",
            "strain 0.30760765075683594\n",
            "strain 0.34862563014030457\n",
            "strain 0.28869375586509705\n",
            "strain 0.3760920763015747\n",
            "strain 0.38333776593208313\n",
            "strain 0.45393800735473633\n",
            "strain 0.3995784521102905\n",
            "strain 0.4561440944671631\n",
            "strain 0.2812693119049072\n",
            "strain 0.3418932259082794\n",
            "strain 0.24896357953548431\n",
            "strain 0.40722429752349854\n",
            "strain 0.47479569911956787\n",
            "strain 0.40385136008262634\n",
            "strain 0.4760902523994446\n",
            "strain 0.2841748595237732\n",
            "strain 0.2608075439929962\n",
            "strain 0.47293445467948914\n",
            "strain 0.27960821986198425\n",
            "strain 0.26747262477874756\n",
            "strain 0.36434826254844666\n",
            "strain 0.43302157521247864\n",
            "strain 0.3055849075317383\n",
            "strain 0.44872626662254333\n",
            "strain 0.40707793831825256\n",
            "strain 0.26119011640548706\n",
            "strain 0.31803110241889954\n",
            "strain 0.5284587740898132\n",
            "strain 0.3616093099117279\n",
            "strain 0.41133466362953186\n",
            "strain 0.35750123858451843\n",
            "strain 0.4985146224498749\n",
            "strain 0.32562384009361267\n",
            "strain 0.36563611030578613\n",
            "strain 0.3815743625164032\n",
            "strain 0.45200464129447937\n",
            "strain 0.3697137236595154\n",
            "strain 0.4618442952632904\n",
            "strain 0.46192193031311035\n",
            "classify 1.86920166015625\n",
            "classify 2.066650390625\n",
            "classify 2.141845703125\n",
            "classify 1.98602294921875\n",
            "classify 2.0943603515625\n",
            "classify 1.98968505859375\n",
            "classify 2.00115966796875\n",
            "classify 2.22705078125\n",
            "classify 1.8408203125\n",
            "classify 1.97845458984375\n",
            "classify 2.19085693359375\n",
            "0.203125\n",
            "0.28125\n",
            "0.203125\n",
            "0.234375\n",
            "0.3125\n",
            "0.15625\n",
            "0.25\n",
            "0.296875\n",
            "0.25\n",
            "0.328125\n",
            "0.28125\n",
            "455\n",
            "strain 0.2963707447052002\n",
            "strain 0.3199112117290497\n",
            "strain 0.32452088594436646\n",
            "strain 0.28886109590530396\n",
            "strain 0.3099290430545807\n",
            "strain 0.3762810528278351\n",
            "strain 0.4963143765926361\n",
            "strain 0.30317530035972595\n",
            "strain 0.3881799578666687\n",
            "strain 0.42534080147743225\n",
            "strain 0.3421144485473633\n",
            "strain 0.2694768011569977\n",
            "strain 0.27525582909584045\n",
            "strain 0.39782243967056274\n",
            "strain 0.4423539936542511\n",
            "strain 0.31799182295799255\n",
            "strain 0.4015471637248993\n",
            "strain 0.3870566189289093\n",
            "strain 0.4650549292564392\n",
            "strain 0.4583490192890167\n",
            "strain 0.38545578718185425\n",
            "strain 0.35472986102104187\n",
            "strain 0.4687339961528778\n",
            "strain 0.38198742270469666\n",
            "strain 0.2842422127723694\n",
            "strain 0.25567156076431274\n",
            "strain 0.4305247962474823\n",
            "strain 0.25040581822395325\n",
            "strain 0.2670910954475403\n",
            "strain 0.3704960346221924\n",
            "strain 0.30173349380493164\n",
            "strain 0.24783731997013092\n",
            "strain 0.39634642004966736\n",
            "strain 0.2826545834541321\n",
            "strain 0.3798205256462097\n",
            "strain 0.46229109168052673\n",
            "strain 0.4567936658859253\n",
            "strain 0.34702539443969727\n",
            "strain 0.4023473560810089\n",
            "strain 0.3001244366168976\n",
            "strain 0.5044435858726501\n",
            "strain 0.28201842308044434\n",
            "strain 0.26432329416275024\n",
            "strain 0.3618553578853607\n",
            "strain 0.35691940784454346\n",
            "strain 0.2390860617160797\n",
            "strain 0.3880603313446045\n",
            "strain 0.35218918323516846\n",
            "strain 0.29890358448028564\n",
            "strain 0.3779696226119995\n",
            "strain 0.2681371867656708\n",
            "classify 2.11962890625\n",
            "classify 2.052001953125\n",
            "classify 1.99713134765625\n",
            "classify 1.971923828125\n",
            "classify 1.8887939453125\n",
            "classify 2.10662841796875\n",
            "classify 2.047607421875\n",
            "classify 2.05548095703125\n",
            "classify 2.20489501953125\n",
            "classify 2.021484375\n",
            "classify 2.0994873046875\n",
            "0.203125\n",
            "0.21875\n",
            "0.203125\n",
            "0.1875\n",
            "0.28125\n",
            "0.203125\n",
            "0.328125\n",
            "0.34375\n",
            "0.21875\n",
            "0.203125\n",
            "0.203125\n",
            "456\n",
            "strain 0.2873428761959076\n",
            "strain 0.4201219975948334\n",
            "strain 0.3980967402458191\n",
            "strain 0.4383779466152191\n",
            "strain 0.40078988671302795\n",
            "strain 0.3787468671798706\n",
            "strain 0.2866249084472656\n",
            "strain 0.4781460762023926\n",
            "strain 0.34865641593933105\n",
            "strain 0.387307345867157\n",
            "strain 0.4798056483268738\n",
            "strain 0.38884830474853516\n",
            "strain 0.3109525144100189\n",
            "strain 0.41287413239479065\n",
            "strain 0.5041133165359497\n",
            "strain 0.32967209815979004\n",
            "strain 0.43536049127578735\n",
            "strain 0.2875365912914276\n",
            "strain 0.36158832907676697\n",
            "strain 0.3638625741004944\n",
            "strain 0.4291194677352905\n",
            "strain 0.42988303303718567\n",
            "strain 0.3634987771511078\n",
            "strain 0.3537035882472992\n",
            "strain 0.2560935318470001\n",
            "strain 0.38081133365631104\n",
            "strain 0.3504970371723175\n",
            "strain 0.2729285657405853\n",
            "strain 0.43857648968696594\n",
            "strain 0.24629473686218262\n",
            "strain 0.2754356861114502\n",
            "strain 0.3652392029762268\n",
            "strain 0.3749224841594696\n",
            "strain 0.30279862880706787\n",
            "strain 0.4220464527606964\n",
            "strain 0.28868386149406433\n",
            "strain 0.283675879240036\n",
            "strain 0.40081459283828735\n",
            "strain 0.5657720565795898\n",
            "strain 0.46144166588783264\n",
            "strain 0.2946907579898834\n",
            "strain 0.43396827578544617\n",
            "strain 0.2561168372631073\n",
            "strain 0.31356382369995117\n",
            "strain 0.26126325130462646\n",
            "strain 0.4922060966491699\n",
            "strain 0.347928911447525\n",
            "strain 0.26682329177856445\n",
            "strain 0.39696022868156433\n",
            "strain 0.3911476731300354\n",
            "strain 0.26866477727890015\n",
            "classify 2.0621337890625\n",
            "classify 2.0213623046875\n",
            "classify 2.04296875\n",
            "classify 1.953857421875\n",
            "classify 2.010009765625\n",
            "classify 1.95208740234375\n",
            "classify 2.19573974609375\n",
            "classify 2.17950439453125\n",
            "classify 2.060302734375\n",
            "classify 2.068115234375\n",
            "classify 2.1138916015625\n",
            "0.21875\n",
            "0.21875\n",
            "0.234375\n",
            "0.375\n",
            "0.296875\n",
            "0.203125\n",
            "0.234375\n",
            "0.234375\n",
            "0.234375\n",
            "0.234375\n",
            "0.234375\n",
            "457\n",
            "strain 0.2914551794528961\n",
            "strain 0.35566097497940063\n",
            "strain 0.2760465145111084\n",
            "strain 0.3235623836517334\n",
            "strain 0.3088839054107666\n",
            "strain 0.3715479075908661\n",
            "strain 0.2469627410173416\n",
            "strain 0.39591923356056213\n",
            "strain 0.4276506304740906\n",
            "strain 0.3025859594345093\n",
            "strain 0.34645912051200867\n",
            "strain 0.40267783403396606\n",
            "strain 0.24883593618869781\n",
            "strain 0.36577409505844116\n",
            "strain 0.27168041467666626\n",
            "strain 0.3807521462440491\n",
            "strain 0.44145917892456055\n",
            "strain 0.36387312412261963\n",
            "strain 0.2676772177219391\n",
            "strain 0.28969019651412964\n",
            "strain 0.3290627598762512\n",
            "strain 0.30757346749305725\n",
            "strain 0.4581775963306427\n",
            "strain 0.4384092390537262\n",
            "strain 0.33904993534088135\n",
            "strain 0.4330970346927643\n",
            "strain 0.32491040229797363\n",
            "strain 0.46172016859054565\n",
            "strain 0.2733822464942932\n",
            "strain 0.24589131772518158\n",
            "strain 0.36331668496131897\n",
            "strain 0.24812135100364685\n",
            "strain 0.30378320813179016\n",
            "strain 0.28052520751953125\n",
            "strain 0.2955094575881958\n",
            "strain 0.33978569507598877\n",
            "strain 0.2397521734237671\n",
            "strain 0.24491679668426514\n",
            "strain 0.23219190537929535\n",
            "strain 0.39604735374450684\n",
            "strain 0.43825626373291016\n",
            "strain 0.3990878462791443\n",
            "strain 0.3933006823062897\n",
            "strain 0.43175357580184937\n",
            "strain 0.2417498081922531\n",
            "strain 0.47021040320396423\n",
            "strain 0.30870962142944336\n",
            "strain 0.30836471915245056\n",
            "strain 0.3437725305557251\n",
            "strain 0.3988606035709381\n",
            "strain 0.2754248082637787\n",
            "classify 2.06488037109375\n",
            "classify 2.093994140625\n",
            "classify 2.12591552734375\n",
            "classify 2.05804443359375\n",
            "classify 1.96337890625\n",
            "classify 1.9918212890625\n",
            "classify 1.95660400390625\n",
            "classify 2.042236328125\n",
            "classify 2.144287109375\n",
            "classify 2.00885009765625\n",
            "classify 2.05419921875\n",
            "0.28125\n",
            "0.265625\n",
            "0.234375\n",
            "0.328125\n",
            "0.125\n",
            "0.21875\n",
            "0.1875\n",
            "0.25\n",
            "0.1875\n",
            "0.265625\n",
            "0.28125\n",
            "458\n",
            "strain 0.2933350205421448\n",
            "strain 0.5234937071800232\n",
            "strain 0.4342746138572693\n",
            "strain 0.3538971543312073\n",
            "strain 0.4141119122505188\n",
            "strain 0.2796412706375122\n",
            "strain 0.3001258671283722\n",
            "strain 0.23362675309181213\n",
            "strain 0.35191237926483154\n",
            "strain 0.37521201372146606\n",
            "strain 0.4305085837841034\n",
            "strain 0.3449932932853699\n",
            "strain 0.4196414351463318\n",
            "strain 0.3794756233692169\n",
            "strain 0.44243505597114563\n",
            "strain 0.27101147174835205\n",
            "strain 0.29054954648017883\n",
            "strain 0.25806787610054016\n",
            "strain 0.23948945105075836\n",
            "strain 0.26657813787460327\n",
            "strain 0.5079126358032227\n",
            "strain 0.3438760042190552\n",
            "strain 0.34601372480392456\n",
            "strain 0.33981597423553467\n",
            "strain 0.27422261238098145\n",
            "strain 0.3193483054637909\n",
            "strain 0.3766494393348694\n",
            "strain 0.39816346764564514\n",
            "strain 0.4974059760570526\n",
            "strain 0.3798792064189911\n",
            "strain 0.44755664467811584\n",
            "strain 0.25774839520454407\n",
            "strain 0.4412657618522644\n",
            "strain 0.23231273889541626\n",
            "strain 0.2499495893716812\n",
            "strain 0.4045247435569763\n",
            "strain 0.40952739119529724\n",
            "strain 0.3627530634403229\n",
            "strain 0.25575897097587585\n",
            "strain 0.34524548053741455\n",
            "strain 0.3911772668361664\n",
            "strain 0.216757133603096\n",
            "strain 0.3743574917316437\n",
            "strain 0.4292885661125183\n",
            "strain 0.329525887966156\n",
            "strain 0.23262885212898254\n",
            "strain 0.23978382349014282\n",
            "strain 0.3606567084789276\n",
            "strain 0.24565966427326202\n",
            "strain 0.2653697431087494\n",
            "strain 0.40798166394233704\n",
            "classify 2.0753173828125\n",
            "classify 2.017822265625\n",
            "classify 2.0111083984375\n",
            "classify 2.03460693359375\n",
            "classify 2.0548095703125\n",
            "classify 2.07958984375\n",
            "classify 2.08905029296875\n",
            "classify 1.98974609375\n",
            "classify 1.9881591796875\n",
            "classify 2.0721435546875\n",
            "classify 2.19384765625\n",
            "0.265625\n",
            "0.265625\n",
            "0.296875\n",
            "0.1875\n",
            "0.203125\n",
            "0.265625\n",
            "0.1875\n",
            "0.1875\n",
            "0.21875\n",
            "0.265625\n",
            "0.25\n",
            "459\n",
            "strain 0.30039888620376587\n",
            "strain 0.2953498661518097\n",
            "strain 0.2820017635822296\n",
            "strain 0.3153936564922333\n",
            "strain 0.2917700409889221\n",
            "strain 0.2539440989494324\n",
            "strain 0.2987546920776367\n",
            "strain 0.2947849631309509\n",
            "strain 0.2633678615093231\n",
            "strain 0.2512243986129761\n",
            "strain 0.2858199179172516\n",
            "strain 0.26956987380981445\n",
            "strain 0.4463922083377838\n",
            "strain 0.3058508634567261\n",
            "strain 0.24740836024284363\n",
            "strain 0.2836984694004059\n",
            "strain 0.2914316952228546\n",
            "strain 0.3896419107913971\n",
            "strain 0.4278745949268341\n",
            "strain 0.3841027319431305\n",
            "strain 0.30697351694107056\n",
            "strain 0.242952361702919\n",
            "strain 0.3027658760547638\n",
            "strain 0.31886303424835205\n",
            "strain 0.28497135639190674\n",
            "strain 0.39808300137519836\n",
            "strain 0.2678319215774536\n",
            "strain 0.3709850609302521\n",
            "strain 0.30655428767204285\n",
            "strain 0.501953661441803\n",
            "strain 0.39378127455711365\n",
            "strain 0.29665249586105347\n",
            "strain 0.465640664100647\n",
            "strain 0.3045668303966522\n",
            "strain 0.4128435552120209\n",
            "strain 0.2761373221874237\n",
            "strain 0.43531861901283264\n",
            "strain 0.41816896200180054\n",
            "strain 0.3275446593761444\n",
            "strain 0.3929806351661682\n",
            "strain 0.5305608510971069\n",
            "strain 0.3455868065357208\n",
            "strain 0.29336410760879517\n",
            "strain 0.2695632874965668\n",
            "strain 0.24911098182201385\n",
            "strain 0.422341525554657\n",
            "strain 0.2206069380044937\n",
            "strain 0.33553117513656616\n",
            "strain 0.2653915584087372\n",
            "strain 0.3494238257408142\n",
            "strain 0.3285462558269501\n",
            "classify 2.08245849609375\n",
            "classify 2.1229248046875\n",
            "classify 1.9786376953125\n",
            "classify 2.09130859375\n",
            "classify 2.143798828125\n",
            "classify 2.026123046875\n",
            "classify 1.949951171875\n",
            "classify 2.1324462890625\n",
            "classify 1.95880126953125\n",
            "classify 1.9971923828125\n",
            "classify 2.08740234375\n",
            "0.21875\n",
            "0.3125\n",
            "0.21875\n",
            "0.203125\n",
            "0.234375\n",
            "0.234375\n",
            "0.1875\n",
            "0.25\n",
            "0.25\n",
            "0.3125\n",
            "0.3125\n",
            "460\n",
            "strain 0.3867908716201782\n",
            "strain 0.35899704694747925\n",
            "strain 0.40571925044059753\n",
            "strain 0.24290911853313446\n",
            "strain 0.4074563980102539\n",
            "strain 0.31644168496131897\n",
            "strain 0.23880451917648315\n",
            "strain 0.3063744008541107\n",
            "strain 0.25630703568458557\n",
            "strain 0.4418276846408844\n",
            "strain 0.2925381362438202\n",
            "strain 0.3648349940776825\n",
            "strain 0.2874221205711365\n",
            "strain 0.2502378821372986\n",
            "strain 0.28552481532096863\n",
            "strain 0.3573155701160431\n",
            "strain 0.306021124124527\n",
            "strain 0.41294828057289124\n",
            "strain 0.40514686703681946\n",
            "strain 0.26308882236480713\n",
            "strain 0.2805043160915375\n",
            "strain 0.22420817613601685\n",
            "strain 0.35286709666252136\n",
            "strain 0.24926695227622986\n",
            "strain 0.27847880125045776\n",
            "strain 0.3800226151943207\n",
            "strain 0.38840341567993164\n",
            "strain 0.42915990948677063\n",
            "strain 0.3702571392059326\n",
            "strain 0.2662186026573181\n",
            "strain 0.38638073205947876\n",
            "strain 0.38331353664398193\n",
            "strain 0.39541736245155334\n",
            "strain 0.32243433594703674\n",
            "strain 0.43458160758018494\n",
            "strain 0.2593587338924408\n",
            "strain 0.46479302644729614\n",
            "strain 0.43214359879493713\n",
            "strain 0.25475531816482544\n",
            "strain 0.25863322615623474\n",
            "strain 0.3639706075191498\n",
            "strain 0.34587547183036804\n",
            "strain 0.41910091042518616\n",
            "strain 0.3078237473964691\n",
            "strain 0.3035751283168793\n",
            "strain 0.48498544096946716\n",
            "strain 0.3125901222229004\n",
            "strain 0.41228142380714417\n",
            "strain 0.3618258833885193\n",
            "strain 0.35534968972206116\n",
            "strain 0.2932422459125519\n",
            "classify 1.95233154296875\n",
            "classify 2.08941650390625\n",
            "classify 1.9423828125\n",
            "classify 2.0108642578125\n",
            "classify 2.0845947265625\n",
            "classify 1.9466552734375\n",
            "classify 2.0615234375\n",
            "classify 2.26611328125\n",
            "classify 1.916259765625\n",
            "classify 1.99884033203125\n",
            "classify 1.913330078125\n",
            "0.21875\n",
            "0.15625\n",
            "0.328125\n",
            "0.140625\n",
            "0.25\n",
            "0.21875\n",
            "0.25\n",
            "0.265625\n",
            "0.328125\n",
            "0.234375\n",
            "0.203125\n",
            "461\n",
            "strain 0.386764794588089\n",
            "strain 0.5240694284439087\n",
            "strain 0.3870757818222046\n",
            "strain 0.29966267943382263\n",
            "strain 0.424297571182251\n",
            "strain 0.39981722831726074\n",
            "strain 0.40329909324645996\n",
            "strain 0.4446944296360016\n",
            "strain 0.2781359553337097\n",
            "strain 0.2481885552406311\n",
            "strain 0.31919151544570923\n",
            "strain 0.24887613952159882\n",
            "strain 0.21861925721168518\n",
            "strain 0.4753812253475189\n",
            "strain 0.19974343478679657\n",
            "strain 0.34638017416000366\n",
            "strain 0.3120322823524475\n",
            "strain 0.23473379015922546\n",
            "strain 0.3210854232311249\n",
            "strain 0.22600136697292328\n",
            "strain 0.45193806290626526\n",
            "strain 0.3380318284034729\n",
            "strain 0.4389633536338806\n",
            "strain 0.3987179696559906\n",
            "strain 0.29640674591064453\n",
            "strain 0.3771224319934845\n",
            "strain 0.4297846555709839\n",
            "strain 0.4398592710494995\n",
            "strain 0.2711690366268158\n",
            "strain 0.521710991859436\n",
            "strain 0.4094502925872803\n",
            "strain 0.39158955216407776\n",
            "strain 0.2914607524871826\n",
            "strain 0.3302808105945587\n",
            "strain 0.2837063670158386\n",
            "strain 0.386707603931427\n",
            "strain 0.25111517310142517\n",
            "strain 0.4080077111721039\n",
            "strain 0.2576702833175659\n",
            "strain 0.4254480302333832\n",
            "strain 0.23628462851047516\n",
            "strain 0.46726036071777344\n",
            "strain 0.36059677600860596\n",
            "strain 0.2979477643966675\n",
            "strain 0.45924675464630127\n",
            "strain 0.24848899245262146\n",
            "strain 0.340032696723938\n",
            "strain 0.2872857451438904\n",
            "strain 0.2917652726173401\n",
            "strain 0.3725041449069977\n",
            "strain 0.2884601354598999\n",
            "classify 1.85791015625\n",
            "classify 2.04547119140625\n",
            "classify 1.93389892578125\n",
            "classify 2.1329345703125\n",
            "classify 2.01104736328125\n",
            "classify 2.0963134765625\n",
            "classify 2.110107421875\n",
            "classify 1.9749755859375\n",
            "classify 1.984619140625\n",
            "classify 1.999755859375\n",
            "classify 2.04541015625\n",
            "0.234375\n",
            "0.265625\n",
            "0.34375\n",
            "0.15625\n",
            "0.21875\n",
            "0.25\n",
            "0.25\n",
            "0.265625\n",
            "0.21875\n",
            "0.25\n",
            "0.328125\n",
            "462\n",
            "strain 0.3723924160003662\n",
            "strain 0.31987428665161133\n",
            "strain 0.23908449709415436\n",
            "strain 0.28317710757255554\n",
            "strain 0.43652647733688354\n",
            "strain 0.2646484076976776\n",
            "strain 0.38864171504974365\n",
            "strain 0.29858964681625366\n",
            "strain 0.29083332419395447\n",
            "strain 0.3855290114879608\n",
            "strain 0.24525153636932373\n",
            "strain 0.38305845856666565\n",
            "strain 0.43277639150619507\n",
            "strain 0.31204307079315186\n",
            "strain 0.25624409317970276\n",
            "strain 0.4159564971923828\n",
            "strain 0.4275156855583191\n",
            "strain 0.24352119863033295\n",
            "strain 0.3983670771121979\n",
            "strain 0.2570950388908386\n",
            "strain 0.4636874794960022\n",
            "strain 0.47344258427619934\n",
            "strain 0.2622012197971344\n",
            "strain 0.2938573956489563\n",
            "strain 0.23177433013916016\n",
            "strain 0.23955495655536652\n",
            "strain 0.32208582758903503\n",
            "strain 0.431441068649292\n",
            "strain 0.22311535477638245\n",
            "strain 0.40483009815216064\n",
            "strain 0.41711553931236267\n",
            "strain 0.41693100333213806\n",
            "strain 0.5074076056480408\n",
            "strain 0.37338581681251526\n",
            "strain 0.35389816761016846\n",
            "strain 0.30831241607666016\n",
            "strain 0.2805163264274597\n",
            "strain 0.2766838073730469\n",
            "strain 0.24765309691429138\n",
            "strain 0.45868119597435\n",
            "strain 0.4066880941390991\n",
            "strain 0.4136918783187866\n",
            "strain 0.3257194459438324\n",
            "strain 0.3383585810661316\n",
            "strain 0.3366733193397522\n",
            "strain 0.28582826256752014\n",
            "strain 0.2613160312175751\n",
            "strain 0.29148027300834656\n",
            "strain 0.4845942556858063\n",
            "strain 0.4800073206424713\n",
            "strain 0.33480703830718994\n",
            "classify 1.942626953125\n",
            "classify 1.9881591796875\n",
            "classify 2.05596923828125\n",
            "classify 2.03082275390625\n",
            "classify 1.99639892578125\n",
            "classify 1.95587158203125\n",
            "classify 1.9874267578125\n",
            "classify 2.0927734375\n",
            "classify 2.08612060546875\n",
            "classify 2.03778076171875\n",
            "classify 1.96820068359375\n",
            "0.28125\n",
            "0.25\n",
            "0.265625\n",
            "0.234375\n",
            "0.265625\n",
            "0.234375\n",
            "0.25\n",
            "0.171875\n",
            "0.234375\n",
            "0.3125\n",
            "0.25\n",
            "463\n",
            "strain 0.40721046924591064\n",
            "strain 0.4955158829689026\n",
            "strain 0.2721345126628876\n",
            "strain 0.2776947021484375\n",
            "strain 0.3233910799026489\n",
            "strain 0.2822640836238861\n",
            "strain 0.3114071488380432\n",
            "strain 0.26521188020706177\n",
            "strain 0.35334813594818115\n",
            "strain 0.3098636269569397\n",
            "strain 0.42958956956863403\n",
            "strain 0.2681882381439209\n",
            "strain 0.4620988070964813\n",
            "strain 0.2642725706100464\n",
            "strain 0.3906986713409424\n",
            "strain 0.28277191519737244\n",
            "strain 0.27506446838378906\n",
            "strain 0.36793580651283264\n",
            "strain 0.449317991733551\n",
            "strain 0.3140154778957367\n",
            "strain 0.4220467805862427\n",
            "strain 0.49318927526474\n",
            "strain 0.269976943731308\n",
            "strain 0.30715012550354004\n",
            "strain 0.38794979453086853\n",
            "strain 0.41390880942344666\n",
            "strain 0.4056910276412964\n",
            "strain 0.3094435930252075\n",
            "strain 0.29396697878837585\n",
            "strain 0.27878934144973755\n",
            "strain 0.3618778586387634\n",
            "strain 0.3435000777244568\n",
            "strain 0.3761836588382721\n",
            "strain 0.31933438777923584\n",
            "strain 0.3679788112640381\n",
            "strain 0.40221890807151794\n",
            "strain 0.3361261785030365\n",
            "strain 0.40323957800865173\n",
            "strain 0.35343828797340393\n",
            "strain 0.32781878113746643\n",
            "strain 0.31676939129829407\n",
            "strain 0.3654787242412567\n",
            "strain 0.37950170040130615\n",
            "strain 0.25454315543174744\n",
            "strain 0.3154672086238861\n",
            "strain 0.41468545794487\n",
            "strain 0.2629237771034241\n",
            "strain 0.29552534222602844\n",
            "strain 0.43408849835395813\n",
            "strain 0.290533185005188\n",
            "strain 0.39862415194511414\n",
            "classify 2.072509765625\n",
            "classify 2.1669921875\n",
            "classify 2.179443359375\n",
            "classify 2.0352783203125\n",
            "classify 1.87945556640625\n",
            "classify 2.20220947265625\n",
            "classify 2.011962890625\n",
            "classify 2.0863037109375\n",
            "classify 2.1541748046875\n",
            "classify 2.0526123046875\n",
            "classify 2.12451171875\n",
            "0.296875\n",
            "0.171875\n",
            "0.1875\n",
            "0.25\n",
            "0.265625\n",
            "0.28125\n",
            "0.234375\n",
            "0.15625\n",
            "0.28125\n",
            "0.203125\n",
            "0.328125\n",
            "464\n",
            "strain 0.3036290407180786\n",
            "strain 0.48351866006851196\n",
            "strain 0.4149256944656372\n",
            "strain 0.24775421619415283\n",
            "strain 0.26434779167175293\n",
            "strain 0.4173482358455658\n",
            "strain 0.38336026668548584\n",
            "strain 0.38587525486946106\n",
            "strain 0.23990470170974731\n",
            "strain 0.3543459475040436\n",
            "strain 0.26167455315589905\n",
            "strain 0.3837445080280304\n",
            "strain 0.4450501799583435\n",
            "strain 0.2703140676021576\n",
            "strain 0.3030093014240265\n",
            "strain 0.37913089990615845\n",
            "strain 0.2207609862089157\n",
            "strain 0.24685163795948029\n",
            "strain 0.5627574920654297\n",
            "strain 0.37866929173469543\n",
            "strain 0.25745701789855957\n",
            "strain 0.4283589720726013\n",
            "strain 0.4522154629230499\n",
            "strain 0.35491785407066345\n",
            "strain 0.24326491355895996\n",
            "strain 0.3878028094768524\n",
            "strain 0.2465948462486267\n",
            "strain 0.2704717218875885\n",
            "strain 0.3225129544734955\n",
            "strain 0.24736157059669495\n",
            "strain 0.3717203140258789\n",
            "strain 0.3651356101036072\n",
            "strain 0.28874191641807556\n",
            "strain 0.2958572804927826\n",
            "strain 0.31963539123535156\n",
            "strain 0.38378480076789856\n",
            "strain 0.4032787084579468\n",
            "strain 0.35625359416007996\n",
            "strain 0.33264052867889404\n",
            "strain 0.4037286043167114\n",
            "strain 0.3899790644645691\n",
            "strain 0.33741649985313416\n",
            "strain 0.2915603220462799\n",
            "strain 0.29402559995651245\n",
            "strain 0.3843051791191101\n",
            "strain 0.4305919408798218\n",
            "strain 0.3470669686794281\n",
            "strain 0.3044206202030182\n",
            "strain 0.33874571323394775\n",
            "strain 0.3825104236602783\n",
            "strain 0.4303816258907318\n",
            "classify 1.8739013671875\n",
            "classify 2.0560302734375\n",
            "classify 2.1036376953125\n",
            "classify 1.92474365234375\n",
            "classify 1.92645263671875\n",
            "classify 2.10382080078125\n",
            "classify 1.9581298828125\n",
            "classify 1.91107177734375\n",
            "classify 1.9163818359375\n",
            "classify 2.1112060546875\n",
            "classify 2.00518798828125\n",
            "0.1875\n",
            "0.34375\n",
            "0.25\n",
            "0.171875\n",
            "0.203125\n",
            "0.28125\n",
            "0.203125\n",
            "0.15625\n",
            "0.234375\n",
            "0.265625\n",
            "0.265625\n",
            "465\n",
            "strain 0.28700289130210876\n",
            "strain 0.40384477376937866\n",
            "strain 0.27748748660087585\n",
            "strain 0.3058271110057831\n",
            "strain 0.26059016585350037\n",
            "strain 0.45865508913993835\n",
            "strain 0.31262272596359253\n",
            "strain 0.33328282833099365\n",
            "strain 0.30025753378868103\n",
            "strain 0.3220803141593933\n",
            "strain 0.2811928689479828\n",
            "strain 0.3951975107192993\n",
            "strain 0.2578962445259094\n",
            "strain 0.41634708642959595\n",
            "strain 0.3891018331050873\n",
            "strain 0.3853552043437958\n",
            "strain 0.33201655745506287\n",
            "strain 0.4209733307361603\n",
            "strain 0.49203839898109436\n",
            "strain 0.3166482448577881\n",
            "strain 0.4167557954788208\n",
            "strain 0.2870095372200012\n",
            "strain 0.3648495674133301\n",
            "strain 0.3460143506526947\n",
            "strain 0.28761541843414307\n",
            "strain 0.38624438643455505\n",
            "strain 0.2623867988586426\n",
            "strain 0.3116644620895386\n",
            "strain 0.3918406665325165\n",
            "strain 0.27230846881866455\n",
            "strain 0.4079113006591797\n",
            "strain 0.4238702058792114\n",
            "strain 0.28730306029319763\n",
            "strain 0.3100022077560425\n",
            "strain 0.4245532751083374\n",
            "strain 0.33482831716537476\n",
            "strain 0.261369913816452\n",
            "strain 0.43512630462646484\n",
            "strain 0.3553979992866516\n",
            "strain 0.3003627359867096\n",
            "strain 0.43538814783096313\n",
            "strain 0.43458059430122375\n",
            "strain 0.3540356159210205\n",
            "strain 0.33980175852775574\n",
            "strain 0.3412989377975464\n",
            "strain 0.3287016749382019\n",
            "strain 0.28863823413848877\n",
            "strain 0.3354698717594147\n",
            "strain 0.3195251226425171\n",
            "strain 0.32625770568847656\n",
            "strain 0.30241233110427856\n",
            "classify 2.1336669921875\n",
            "classify 1.9786376953125\n",
            "classify 2.00286865234375\n",
            "classify 2.12432861328125\n",
            "classify 2.0828857421875\n",
            "classify 1.9735107421875\n",
            "classify 1.9827880859375\n",
            "classify 2.12030029296875\n",
            "classify 1.9677734375\n",
            "classify 2.0230712890625\n",
            "classify 2.0830078125\n",
            "0.234375\n",
            "0.3125\n",
            "0.265625\n",
            "0.234375\n",
            "0.25\n",
            "0.28125\n",
            "0.28125\n",
            "0.296875\n",
            "0.1875\n",
            "0.28125\n",
            "0.28125\n",
            "466\n",
            "strain 0.38627713918685913\n",
            "strain 0.25311797857284546\n",
            "strain 0.29822126030921936\n",
            "strain 0.28762930631637573\n",
            "strain 0.37097659707069397\n",
            "strain 0.39764609932899475\n",
            "strain 0.3287584185600281\n",
            "strain 0.292484313249588\n",
            "strain 0.32922276854515076\n",
            "strain 0.24369977414608002\n",
            "strain 0.2796286940574646\n",
            "strain 0.40544646978378296\n",
            "strain 0.2679075300693512\n",
            "strain 0.25512179732322693\n",
            "strain 0.2999626398086548\n",
            "strain 0.4159773588180542\n",
            "strain 0.33727169036865234\n",
            "strain 0.3661373257637024\n",
            "strain 0.43090304732322693\n",
            "strain 0.2376873940229416\n",
            "strain 0.2775735855102539\n",
            "strain 0.39267605543136597\n",
            "strain 0.23800814151763916\n",
            "strain 0.27338504791259766\n",
            "strain 0.30338239669799805\n",
            "strain 0.40241459012031555\n",
            "strain 0.26517266035079956\n",
            "strain 0.3888454735279083\n",
            "strain 0.37464475631713867\n",
            "strain 0.2706882357597351\n",
            "strain 0.3135698139667511\n",
            "strain 0.3487948179244995\n",
            "strain 0.3042329251766205\n",
            "strain 0.41842368245124817\n",
            "strain 0.33323389291763306\n",
            "strain 0.25924453139305115\n",
            "strain 0.35822752118110657\n",
            "strain 0.5289729833602905\n",
            "strain 0.3456715941429138\n",
            "strain 0.24775025248527527\n",
            "strain 0.3981553018093109\n",
            "strain 0.3809964060783386\n",
            "strain 0.35440537333488464\n",
            "strain 0.26287832856178284\n",
            "strain 0.24035169184207916\n",
            "strain 0.4696393609046936\n",
            "strain 0.33712220191955566\n",
            "strain 0.22650882601737976\n",
            "strain 0.4960681200027466\n",
            "strain 0.2926657497882843\n",
            "strain 0.31845447421073914\n",
            "classify 2.07952880859375\n",
            "classify 2.06591796875\n",
            "classify 2.044677734375\n",
            "classify 1.99078369140625\n",
            "classify 2.08856201171875\n",
            "classify 2.07611083984375\n",
            "classify 2.09649658203125\n",
            "classify 1.8551025390625\n",
            "classify 1.963623046875\n",
            "classify 1.94879150390625\n",
            "classify 1.94122314453125\n",
            "0.15625\n",
            "0.203125\n",
            "0.234375\n",
            "0.265625\n",
            "0.21875\n",
            "0.28125\n",
            "0.1875\n",
            "0.21875\n",
            "0.234375\n",
            "0.3125\n",
            "0.375\n",
            "467\n",
            "strain 0.28170228004455566\n",
            "strain 0.25309449434280396\n",
            "strain 0.39268240332603455\n",
            "strain 0.3431488573551178\n",
            "strain 0.24209779500961304\n",
            "strain 0.24101023375988007\n",
            "strain 0.3448231518268585\n",
            "strain 0.28317779302597046\n",
            "strain 0.4518583416938782\n",
            "strain 0.24331174790859222\n",
            "strain 0.3909370005130768\n",
            "strain 0.37440717220306396\n",
            "strain 0.2578239142894745\n",
            "strain 0.29850608110427856\n",
            "strain 0.3123167157173157\n",
            "strain 0.24971935153007507\n",
            "strain 0.4142172038555145\n",
            "strain 0.26351138949394226\n",
            "strain 0.35490018129348755\n",
            "strain 0.240326389670372\n",
            "strain 0.37419232726097107\n",
            "strain 0.35675060749053955\n",
            "strain 0.31518274545669556\n",
            "strain 0.4164202809333801\n",
            "strain 0.37280523777008057\n",
            "strain 0.35023847222328186\n",
            "strain 0.3454554080963135\n",
            "strain 0.3862136900424957\n",
            "strain 0.2621283531188965\n",
            "strain 0.33403533697128296\n",
            "strain 0.32368913292884827\n",
            "strain 0.3420216143131256\n",
            "strain 0.39089128375053406\n",
            "strain 0.32433775067329407\n",
            "strain 0.22261586785316467\n",
            "strain 0.24733887612819672\n",
            "strain 0.4621158242225647\n",
            "strain 0.4419742524623871\n",
            "strain 0.48912790417671204\n",
            "strain 0.21661822497844696\n",
            "strain 0.34267446398735046\n",
            "strain 0.37257829308509827\n",
            "strain 0.2867087721824646\n",
            "strain 0.24732650816440582\n",
            "strain 0.27951282262802124\n",
            "strain 0.34492266178131104\n",
            "strain 0.288289874792099\n",
            "strain 0.26660582423210144\n",
            "strain 0.32092729210853577\n",
            "strain 0.40877366065979004\n",
            "strain 0.37769269943237305\n",
            "classify 2.1485595703125\n",
            "classify 1.989990234375\n",
            "classify 2.116943359375\n",
            "classify 2.20892333984375\n",
            "classify 1.99786376953125\n",
            "classify 2.03515625\n",
            "classify 2.1392822265625\n",
            "classify 2.11474609375\n",
            "classify 2.0601806640625\n",
            "classify 2.003173828125\n",
            "classify 1.9761962890625\n",
            "0.359375\n",
            "0.140625\n",
            "0.125\n",
            "0.203125\n",
            "0.3125\n",
            "0.203125\n",
            "0.25\n",
            "0.15625\n",
            "0.296875\n",
            "0.21875\n",
            "0.140625\n",
            "468\n",
            "strain 0.25363659858703613\n",
            "strain 0.43769094347953796\n",
            "strain 0.24638357758522034\n",
            "strain 0.38511186838150024\n",
            "strain 0.3848452568054199\n",
            "strain 0.24461250007152557\n",
            "strain 0.44920113682746887\n",
            "strain 0.46676602959632874\n",
            "strain 0.3796829581260681\n",
            "strain 0.47127148509025574\n",
            "strain 0.26039475202560425\n",
            "strain 0.3819414973258972\n",
            "strain 0.2642527222633362\n",
            "strain 0.34599024057388306\n",
            "strain 0.260841429233551\n",
            "strain 0.39565327763557434\n",
            "strain 0.22781117260456085\n",
            "strain 0.2691897749900818\n",
            "strain 0.41426101326942444\n",
            "strain 0.41162970662117004\n",
            "strain 0.3317773640155792\n",
            "strain 0.32072946429252625\n",
            "strain 0.41499555110931396\n",
            "strain 0.42561331391334534\n",
            "strain 0.34739696979522705\n",
            "strain 0.2740887999534607\n",
            "strain 0.3812091052532196\n",
            "strain 0.3747316598892212\n",
            "strain 0.422200083732605\n",
            "strain 0.4428054392337799\n",
            "strain 0.28890618681907654\n",
            "strain 0.38261839747428894\n",
            "strain 0.2528444826602936\n",
            "strain 0.3130812346935272\n",
            "strain 0.34297358989715576\n",
            "strain 0.37630558013916016\n",
            "strain 0.40147700905799866\n",
            "strain 0.34715545177459717\n",
            "strain 0.292356938123703\n",
            "strain 0.41430366039276123\n",
            "strain 0.2692640721797943\n",
            "strain 0.5076403021812439\n",
            "strain 0.384182870388031\n",
            "strain 0.4424436390399933\n",
            "strain 0.45180439949035645\n",
            "strain 0.3889370262622833\n",
            "strain 0.3954288065433502\n",
            "strain 0.29698124527931213\n",
            "strain 0.2501230835914612\n",
            "strain 0.3795931339263916\n",
            "strain 0.2715229392051697\n",
            "classify 1.9783935546875\n",
            "classify 2.12176513671875\n",
            "classify 1.9859619140625\n",
            "classify 2.0889892578125\n",
            "classify 1.982666015625\n",
            "classify 2.0723876953125\n",
            "classify 2.1646728515625\n",
            "classify 2.1798095703125\n",
            "classify 2.15155029296875\n",
            "classify 2.0447998046875\n",
            "classify 2.1363525390625\n",
            "0.21875\n",
            "0.234375\n",
            "0.296875\n",
            "0.203125\n",
            "0.21875\n",
            "0.1875\n",
            "0.21875\n",
            "0.25\n",
            "0.25\n",
            "0.234375\n",
            "0.25\n",
            "469\n",
            "strain 0.3955457806587219\n",
            "strain 0.2542833089828491\n",
            "strain 0.3743956983089447\n",
            "strain 0.3738035559654236\n",
            "strain 0.34775882959365845\n",
            "strain 0.40941929817199707\n",
            "strain 0.3021233081817627\n",
            "strain 0.29768455028533936\n",
            "strain 0.39163529872894287\n",
            "strain 0.343710720539093\n",
            "strain 0.28968366980552673\n",
            "strain 0.34008359909057617\n",
            "strain 0.33356717228889465\n",
            "strain 0.2657865583896637\n",
            "strain 0.35202059149742126\n",
            "strain 0.32806408405303955\n",
            "strain 0.29526883363723755\n",
            "strain 0.3924729526042938\n",
            "strain 0.35311874747276306\n",
            "strain 0.3262965679168701\n",
            "strain 0.2533041834831238\n",
            "strain 0.4139254689216614\n",
            "strain 0.2965666949748993\n",
            "strain 0.2420128583908081\n",
            "strain 0.3334832489490509\n",
            "strain 0.3112832009792328\n",
            "strain 0.2986777722835541\n",
            "strain 0.3909120261669159\n",
            "strain 0.3297744393348694\n",
            "strain 0.4552251398563385\n",
            "strain 0.37233516573905945\n",
            "strain 0.37580761313438416\n",
            "strain 0.22798827290534973\n",
            "strain 0.29912838339805603\n",
            "strain 0.30725935101509094\n",
            "strain 0.4080083966255188\n",
            "strain 0.2998676598072052\n",
            "strain 0.30246227979660034\n",
            "strain 0.4515248239040375\n",
            "strain 0.3735969364643097\n",
            "strain 0.27521148324012756\n",
            "strain 0.3084133565425873\n",
            "strain 0.30069905519485474\n",
            "strain 0.343899130821228\n",
            "strain 0.28992775082588196\n",
            "strain 0.3404037058353424\n",
            "strain 0.39228355884552\n",
            "strain 0.3664328157901764\n",
            "strain 0.34350061416625977\n",
            "strain 0.24715307354927063\n",
            "strain 0.45159175992012024\n",
            "classify 2.04058837890625\n",
            "classify 2.022216796875\n",
            "classify 2.069580078125\n",
            "classify 2.0687255859375\n",
            "classify 1.9583740234375\n",
            "classify 2.1241455078125\n",
            "classify 2.1142578125\n",
            "classify 2.103271484375\n",
            "classify 2.11627197265625\n",
            "classify 2.1998291015625\n",
            "classify 1.885498046875\n",
            "0.25\n",
            "0.28125\n",
            "0.3125\n",
            "0.375\n",
            "0.28125\n",
            "0.21875\n",
            "0.15625\n",
            "0.28125\n",
            "0.296875\n",
            "0.203125\n",
            "0.25\n",
            "470\n",
            "strain 0.36139535903930664\n",
            "strain 0.36852023005485535\n",
            "strain 0.3435189723968506\n",
            "strain 0.22081594169139862\n",
            "strain 0.3984115421772003\n",
            "strain 0.2461155652999878\n",
            "strain 0.4445970356464386\n",
            "strain 0.3107720613479614\n",
            "strain 0.39760449528694153\n",
            "strain 0.2444765418767929\n",
            "strain 0.3387649655342102\n",
            "strain 0.3696373999118805\n",
            "strain 0.26657482981681824\n",
            "strain 0.27592334151268005\n",
            "strain 0.4092928469181061\n",
            "strain 0.4913441836833954\n",
            "strain 0.3090871274471283\n",
            "strain 0.3319295644760132\n",
            "strain 0.34810498356819153\n",
            "strain 0.3304252028465271\n",
            "strain 0.31951501965522766\n",
            "strain 0.30819669365882874\n",
            "strain 0.2685357928276062\n",
            "strain 0.39332178235054016\n",
            "strain 0.33998730778694153\n",
            "strain 0.2879257798194885\n",
            "strain 0.49348270893096924\n",
            "strain 0.32397642731666565\n",
            "strain 0.3550085127353668\n",
            "strain 0.2901800572872162\n",
            "strain 0.2744661867618561\n",
            "strain 0.263561874628067\n",
            "strain 0.27816903591156006\n",
            "strain 0.41298413276672363\n",
            "strain 0.2943881154060364\n",
            "strain 0.21484339237213135\n",
            "strain 0.26655739545822144\n",
            "strain 0.35808849334716797\n",
            "strain 0.28845515847206116\n",
            "strain 0.2265302836894989\n",
            "strain 0.2906116545200348\n",
            "strain 0.4059092700481415\n",
            "strain 0.2993333339691162\n",
            "strain 0.4211781919002533\n",
            "strain 0.40094345808029175\n",
            "strain 0.4637259840965271\n",
            "strain 0.4009971618652344\n",
            "strain 0.2557305097579956\n",
            "strain 0.3035852313041687\n",
            "strain 0.39596179127693176\n",
            "strain 0.3646034300327301\n",
            "classify 1.968994140625\n",
            "classify 2.14178466796875\n",
            "classify 1.9227294921875\n",
            "classify 2.025390625\n",
            "classify 1.9443359375\n",
            "classify 2.0648193359375\n",
            "classify 2.119384765625\n",
            "classify 1.94720458984375\n",
            "classify 1.93768310546875\n",
            "classify 1.9998779296875\n",
            "classify 2.007568359375\n",
            "0.328125\n",
            "0.234375\n",
            "0.25\n",
            "0.3125\n",
            "0.234375\n",
            "0.1875\n",
            "0.25\n",
            "0.28125\n",
            "0.1875\n",
            "0.21875\n",
            "0.28125\n",
            "471\n",
            "strain 0.308892160654068\n",
            "strain 0.2770882248878479\n",
            "strain 0.2452821433544159\n",
            "strain 0.2751093804836273\n",
            "strain 0.37585195899009705\n",
            "strain 0.30034536123275757\n",
            "strain 0.4066470265388489\n",
            "strain 0.31257158517837524\n",
            "strain 0.25780385732650757\n",
            "strain 0.273281067609787\n",
            "strain 0.5404719114303589\n",
            "strain 0.3890868127346039\n",
            "strain 0.3839627802371979\n",
            "strain 0.26190993189811707\n",
            "strain 0.2779957950115204\n",
            "strain 0.3716801404953003\n",
            "strain 0.3379061818122864\n",
            "strain 0.35130682587623596\n",
            "strain 0.35059985518455505\n",
            "strain 0.3434705138206482\n",
            "strain 0.3310495615005493\n",
            "strain 0.29363447427749634\n",
            "strain 0.43191590905189514\n",
            "strain 0.26457226276397705\n",
            "strain 0.3697802424430847\n",
            "strain 0.4933674931526184\n",
            "strain 0.45501795411109924\n",
            "strain 0.3060280680656433\n",
            "strain 0.33100536465644836\n",
            "strain 0.3974161744117737\n",
            "strain 0.25446856021881104\n",
            "strain 0.34350377321243286\n",
            "strain 0.3103275001049042\n",
            "strain 0.44604524970054626\n",
            "strain 0.4767916798591614\n",
            "strain 0.4031393229961395\n",
            "strain 0.3003268837928772\n",
            "strain 0.34095945954322815\n",
            "strain 0.3318948447704315\n",
            "strain 0.2719396650791168\n",
            "strain 0.24003492295742035\n",
            "strain 0.2344166487455368\n",
            "strain 0.2578400671482086\n",
            "strain 0.29197296500205994\n",
            "strain 0.3781546652317047\n",
            "strain 0.359124094247818\n",
            "strain 0.5236566662788391\n",
            "strain 0.34116047620773315\n",
            "strain 0.33668652176856995\n",
            "strain 0.45008087158203125\n",
            "strain 0.45783349871635437\n",
            "classify 1.952392578125\n",
            "classify 2.0894775390625\n",
            "classify 2.1640625\n",
            "classify 2.0899658203125\n",
            "classify 1.944091796875\n",
            "classify 2.0928955078125\n",
            "classify 2.05322265625\n",
            "classify 2.197509765625\n",
            "classify 2.0076904296875\n",
            "classify 2.050048828125\n",
            "classify 1.94268798828125\n",
            "0.1875\n",
            "0.25\n",
            "0.3125\n",
            "0.21875\n",
            "0.296875\n",
            "0.296875\n",
            "0.265625\n",
            "0.25\n",
            "0.359375\n",
            "0.234375\n",
            "0.265625\n",
            "472\n",
            "strain 0.3802933990955353\n",
            "strain 0.37804701924324036\n",
            "strain 0.350852370262146\n",
            "strain 0.2588502764701843\n",
            "strain 0.3639722466468811\n",
            "strain 0.45550453662872314\n",
            "strain 0.3831784427165985\n",
            "strain 0.39247769117355347\n",
            "strain 0.35747799277305603\n",
            "strain 0.3756229281425476\n",
            "strain 0.46519121527671814\n",
            "strain 0.45003625750541687\n",
            "strain 0.2618074119091034\n",
            "strain 0.41654983162879944\n",
            "strain 0.4211106300354004\n",
            "strain 0.3019194006919861\n",
            "strain 0.2501196563243866\n",
            "strain 0.4277094304561615\n",
            "strain 0.36809518933296204\n",
            "strain 0.3295865058898926\n",
            "strain 0.30762070417404175\n",
            "strain 0.40572619438171387\n",
            "strain 0.23598948121070862\n",
            "strain 0.3204316794872284\n",
            "strain 0.40061429142951965\n",
            "strain 0.23929545283317566\n",
            "strain 0.29425328969955444\n",
            "strain 0.24648380279541016\n",
            "strain 0.33091843128204346\n",
            "strain 0.31973540782928467\n",
            "strain 0.5138412714004517\n",
            "strain 0.30935442447662354\n",
            "strain 0.3757643699645996\n",
            "strain 0.3618662655353546\n",
            "strain 0.2546948194503784\n",
            "strain 0.2307891845703125\n",
            "strain 0.3442983329296112\n",
            "strain 0.27382194995880127\n",
            "strain 0.28487956523895264\n",
            "strain 0.3240894079208374\n",
            "strain 0.3623977303504944\n",
            "strain 0.4743785858154297\n",
            "strain 0.24101687967777252\n",
            "strain 0.43322908878326416\n",
            "strain 0.272196888923645\n",
            "strain 0.22508381307125092\n",
            "strain 0.3774961233139038\n",
            "strain 0.3012430667877197\n",
            "strain 0.2719281017780304\n",
            "strain 0.28970450162887573\n",
            "strain 0.3993905484676361\n",
            "classify 2.05755615234375\n",
            "classify 2.0172119140625\n",
            "classify 1.924072265625\n",
            "classify 2.02166748046875\n",
            "classify 1.9102783203125\n",
            "classify 1.829833984375\n",
            "classify 1.953125\n",
            "classify 2.0064697265625\n",
            "classify 2.07855224609375\n",
            "classify 2.08514404296875\n",
            "classify 2.04010009765625\n",
            "0.34375\n",
            "0.28125\n",
            "0.296875\n",
            "0.234375\n",
            "0.25\n",
            "0.3125\n",
            "0.171875\n",
            "0.3125\n",
            "0.234375\n",
            "0.296875\n",
            "0.234375\n",
            "473\n",
            "strain 0.3502645194530487\n",
            "strain 0.2702355980873108\n",
            "strain 0.47099393606185913\n",
            "strain 0.2679591178894043\n",
            "strain 0.29150640964508057\n",
            "strain 0.37850332260131836\n",
            "strain 0.3587678074836731\n",
            "strain 0.36543282866477966\n",
            "strain 0.26142850518226624\n",
            "strain 0.4284050166606903\n",
            "strain 0.3939436376094818\n",
            "strain 0.3698596656322479\n",
            "strain 0.27655503153800964\n",
            "strain 0.2644171714782715\n",
            "strain 0.5150341987609863\n",
            "strain 0.3564643859863281\n",
            "strain 0.39497724175453186\n",
            "strain 0.3785047233104706\n",
            "strain 0.4363655745983124\n",
            "strain 0.2455686777830124\n",
            "strain 0.3826914131641388\n",
            "strain 0.37165603041648865\n",
            "strain 0.42348769307136536\n",
            "strain 0.3963506817817688\n",
            "strain 0.32249966263771057\n",
            "strain 0.34471777081489563\n",
            "strain 0.2944057285785675\n",
            "strain 0.3142770826816559\n",
            "strain 0.38805076479911804\n",
            "strain 0.43000346422195435\n",
            "strain 0.3276180624961853\n",
            "strain 0.3486749231815338\n",
            "strain 0.47497689723968506\n",
            "strain 0.381376177072525\n",
            "strain 0.2539313733577728\n",
            "strain 0.27974799275398254\n",
            "strain 0.2622373700141907\n",
            "strain 0.30001503229141235\n",
            "strain 0.4558659791946411\n",
            "strain 0.3411291539669037\n",
            "strain 0.4019663631916046\n",
            "strain 0.25407832860946655\n",
            "strain 0.2694573998451233\n",
            "strain 0.2881428897380829\n",
            "strain 0.35372042655944824\n",
            "strain 0.43784579634666443\n",
            "strain 0.29161500930786133\n",
            "strain 0.24702385067939758\n",
            "strain 0.2949920892715454\n",
            "strain 0.28753653168678284\n",
            "strain 0.2606157958507538\n",
            "classify 2.09423828125\n",
            "classify 2.0723876953125\n",
            "classify 1.9827880859375\n",
            "classify 2.0272216796875\n",
            "classify 1.9576416015625\n",
            "classify 2.12542724609375\n",
            "classify 1.92144775390625\n",
            "classify 2.0477294921875\n",
            "classify 2.1322021484375\n",
            "classify 2.07757568359375\n",
            "classify 2.06396484375\n",
            "0.25\n",
            "0.203125\n",
            "0.28125\n",
            "0.234375\n",
            "0.171875\n",
            "0.3125\n",
            "0.28125\n",
            "0.234375\n",
            "0.296875\n",
            "0.328125\n",
            "0.328125\n",
            "474\n",
            "strain 0.33785706758499146\n",
            "strain 0.29178640246391296\n",
            "strain 0.40472716093063354\n",
            "strain 0.33063292503356934\n",
            "strain 0.4459874629974365\n",
            "strain 0.3697073459625244\n",
            "strain 0.40625056624412537\n",
            "strain 0.23480680584907532\n",
            "strain 0.26288309693336487\n",
            "strain 0.27595168352127075\n",
            "strain 0.4402601718902588\n",
            "strain 0.25838690996170044\n",
            "strain 0.46669718623161316\n",
            "strain 0.30331042408943176\n",
            "strain 0.43577319383621216\n",
            "strain 0.3264516592025757\n",
            "strain 0.3406246304512024\n",
            "strain 0.34938183426856995\n",
            "strain 0.2953217923641205\n",
            "strain 0.41431760787963867\n",
            "strain 0.3258019983768463\n",
            "strain 0.3648429214954376\n",
            "strain 0.2556467354297638\n",
            "strain 0.296928733587265\n",
            "strain 0.4744532108306885\n",
            "strain 0.2170565128326416\n",
            "strain 0.24808605015277863\n",
            "strain 0.3751526474952698\n",
            "strain 0.3106488287448883\n",
            "strain 0.32704028487205505\n",
            "strain 0.2517922818660736\n",
            "strain 0.35822394490242004\n",
            "strain 0.28026965260505676\n",
            "strain 0.30673888325691223\n",
            "strain 0.29563578963279724\n",
            "strain 0.47087669372558594\n",
            "strain 0.22256843745708466\n",
            "strain 0.39183127880096436\n",
            "strain 0.24741971492767334\n",
            "strain 0.3025462329387665\n",
            "strain 0.3331872224807739\n",
            "strain 0.37073013186454773\n",
            "strain 0.350382924079895\n",
            "strain 0.29034504294395447\n",
            "strain 0.33182772994041443\n",
            "strain 0.40704014897346497\n",
            "strain 0.3849058449268341\n",
            "strain 0.4534485936164856\n",
            "strain 0.2796666920185089\n",
            "strain 0.3963373005390167\n",
            "strain 0.31473028659820557\n",
            "classify 2.02532958984375\n",
            "classify 2.0537109375\n",
            "classify 2.134033203125\n",
            "classify 2.0194091796875\n",
            "classify 1.86602783203125\n",
            "classify 1.9814453125\n",
            "classify 1.85260009765625\n",
            "classify 2.0948486328125\n",
            "classify 1.981689453125\n",
            "classify 2.17041015625\n",
            "classify 2.13787841796875\n",
            "0.3125\n",
            "0.21875\n",
            "0.234375\n",
            "0.203125\n",
            "0.296875\n",
            "0.25\n",
            "0.28125\n",
            "0.3125\n",
            "0.28125\n",
            "0.3125\n",
            "0.203125\n",
            "475\n",
            "strain 0.43320807814598083\n",
            "strain 0.38205820322036743\n",
            "strain 0.3279959559440613\n",
            "strain 0.3065151274204254\n",
            "strain 0.3851185441017151\n",
            "strain 0.2557280361652374\n",
            "strain 0.44837328791618347\n",
            "strain 0.36376500129699707\n",
            "strain 0.29308822751045227\n",
            "strain 0.3795933127403259\n",
            "strain 0.33282166719436646\n",
            "strain 0.2747042775154114\n",
            "strain 0.47715622186660767\n",
            "strain 0.2616755962371826\n",
            "strain 0.45156314969062805\n",
            "strain 0.24587951600551605\n",
            "strain 0.2942502200603485\n",
            "strain 0.3067382574081421\n",
            "strain 0.4463529884815216\n",
            "strain 0.35935598611831665\n",
            "strain 0.27147677540779114\n",
            "strain 0.47073638439178467\n",
            "strain 0.25798192620277405\n",
            "strain 0.2955594062805176\n",
            "strain 0.23357385396957397\n",
            "strain 0.46765199303627014\n",
            "strain 0.325838565826416\n",
            "strain 0.2511141002178192\n",
            "strain 0.24932731688022614\n",
            "strain 0.23072996735572815\n",
            "strain 0.22081954777240753\n",
            "strain 0.3631783723831177\n",
            "strain 0.26491135358810425\n",
            "strain 0.33991554379463196\n",
            "strain 0.2530760169029236\n",
            "strain 0.2624887526035309\n",
            "strain 0.41022971272468567\n",
            "strain 0.427768737077713\n",
            "strain 0.37450993061065674\n",
            "strain 0.3886466324329376\n",
            "strain 0.4581025540828705\n",
            "strain 0.2641356587409973\n",
            "strain 0.3719808757305145\n",
            "strain 0.4032696783542633\n",
            "strain 0.4585689902305603\n",
            "strain 0.3512307107448578\n",
            "strain 0.3795173466205597\n",
            "strain 0.2534351646900177\n",
            "strain 0.28505170345306396\n",
            "strain 0.36944156885147095\n",
            "strain 0.47015541791915894\n",
            "classify 2.10589599609375\n",
            "classify 2.33447265625\n",
            "classify 2.08197021484375\n",
            "classify 2.05767822265625\n",
            "classify 1.93316650390625\n",
            "classify 1.995361328125\n",
            "classify 2.0045166015625\n",
            "classify 2.179443359375\n",
            "classify 2.07080078125\n",
            "classify 2.064208984375\n",
            "classify 2.07537841796875\n",
            "0.265625\n",
            "0.1875\n",
            "0.25\n",
            "0.25\n",
            "0.34375\n",
            "0.1875\n",
            "0.265625\n",
            "0.234375\n",
            "0.3125\n",
            "0.3125\n",
            "0.1875\n",
            "476\n",
            "strain 0.2527488172054291\n",
            "strain 0.3670591711997986\n",
            "strain 0.2770042419433594\n",
            "strain 0.2734818160533905\n",
            "strain 0.24141357839107513\n",
            "strain 0.28551408648490906\n",
            "strain 0.2463950365781784\n",
            "strain 0.240387424826622\n",
            "strain 0.3313123285770416\n",
            "strain 0.3989516794681549\n",
            "strain 0.43684762716293335\n",
            "strain 0.4248841404914856\n",
            "strain 0.2600572109222412\n",
            "strain 0.4025913178920746\n",
            "strain 0.29031726717948914\n",
            "strain 0.25048381090164185\n",
            "strain 0.3077305853366852\n",
            "strain 0.3410385251045227\n",
            "strain 0.40933945775032043\n",
            "strain 0.2666839361190796\n",
            "strain 0.28921568393707275\n",
            "strain 0.29985934495925903\n",
            "strain 0.3191908597946167\n",
            "strain 0.39472389221191406\n",
            "strain 0.26739561557769775\n",
            "strain 0.24598760902881622\n",
            "strain 0.252753347158432\n",
            "strain 0.546894907951355\n",
            "strain 0.3464259207248688\n",
            "strain 0.2912119925022125\n",
            "strain 0.2620489299297333\n",
            "strain 0.3726440370082855\n",
            "strain 0.3075878918170929\n",
            "strain 0.324445903301239\n",
            "strain 0.2689211070537567\n",
            "strain 0.411953330039978\n",
            "strain 0.4045262932777405\n",
            "strain 0.36937132477760315\n",
            "strain 0.29217347502708435\n",
            "strain 0.20791666209697723\n",
            "strain 0.30373579263687134\n",
            "strain 0.3828252851963043\n",
            "strain 0.30435043573379517\n",
            "strain 0.4644830524921417\n",
            "strain 0.2708716094493866\n",
            "strain 0.2738499939441681\n",
            "strain 0.43607985973358154\n",
            "strain 0.28444015979766846\n",
            "strain 0.40983495116233826\n",
            "strain 0.4001923203468323\n",
            "strain 0.2558967173099518\n",
            "classify 1.989501953125\n",
            "classify 2.123291015625\n",
            "classify 1.8531494140625\n",
            "classify 2.08447265625\n",
            "classify 2.0518798828125\n",
            "classify 2.0467529296875\n",
            "classify 1.9927978515625\n",
            "classify 2.04241943359375\n",
            "classify 2.014404296875\n",
            "classify 2.099853515625\n",
            "classify 2.1256103515625\n",
            "0.171875\n",
            "0.28125\n",
            "0.34375\n",
            "0.203125\n",
            "0.21875\n",
            "0.1875\n",
            "0.28125\n",
            "0.21875\n",
            "0.1875\n",
            "0.296875\n",
            "0.34375\n",
            "477\n",
            "strain 0.27203837037086487\n",
            "strain 0.36853328347206116\n",
            "strain 0.4012551009654999\n",
            "strain 0.39277926087379456\n",
            "strain 0.28656092286109924\n",
            "strain 0.5095227360725403\n",
            "strain 0.2702520191669464\n",
            "strain 0.2807573676109314\n",
            "strain 0.39577850699424744\n",
            "strain 0.42252984642982483\n",
            "strain 0.2884659171104431\n",
            "strain 0.31955933570861816\n",
            "strain 0.3474695384502411\n",
            "strain 0.32508766651153564\n",
            "strain 0.3678383231163025\n",
            "strain 0.45948824286460876\n",
            "strain 0.3818017244338989\n",
            "strain 0.38031747937202454\n",
            "strain 0.3783082067966461\n",
            "strain 0.2633293569087982\n",
            "strain 0.37361928820610046\n",
            "strain 0.3063446879386902\n",
            "strain 0.3634091913700104\n",
            "strain 0.3530847132205963\n",
            "strain 0.24293622374534607\n",
            "strain 0.4205707013607025\n",
            "strain 0.44123145937919617\n",
            "strain 0.26407527923583984\n",
            "strain 0.2600547671318054\n",
            "strain 0.42105311155319214\n",
            "strain 0.35142847895622253\n",
            "strain 0.3986806571483612\n",
            "strain 0.3368718922138214\n",
            "strain 0.4280853569507599\n",
            "strain 0.2906058430671692\n",
            "strain 0.2858583331108093\n",
            "strain 0.24859876930713654\n",
            "strain 0.4222702980041504\n",
            "strain 0.47889119386672974\n",
            "strain 0.38301682472229004\n",
            "strain 0.3725203275680542\n",
            "strain 0.3888823986053467\n",
            "strain 0.2837264835834503\n",
            "strain 0.312351256608963\n",
            "strain 0.3097763657569885\n",
            "strain 0.2477627694606781\n",
            "strain 0.41768401861190796\n",
            "strain 0.44009143114089966\n",
            "strain 0.287663996219635\n",
            "strain 0.30071040987968445\n",
            "strain 0.27973321080207825\n",
            "classify 2.10357666015625\n",
            "classify 1.9427490234375\n",
            "classify 1.96795654296875\n",
            "classify 2.05621337890625\n",
            "classify 1.921142578125\n",
            "classify 2.0791015625\n",
            "classify 2.09423828125\n",
            "classify 2.08502197265625\n",
            "classify 2.1156005859375\n",
            "classify 2.056396484375\n",
            "classify 2.099365234375\n",
            "0.234375\n",
            "0.1875\n",
            "0.234375\n",
            "0.1875\n",
            "0.1875\n",
            "0.296875\n",
            "0.1875\n",
            "0.234375\n",
            "0.203125\n",
            "0.140625\n",
            "0.265625\n",
            "478\n",
            "strain 0.347074031829834\n",
            "strain 0.2518247961997986\n",
            "strain 0.368888258934021\n",
            "strain 0.33465325832366943\n",
            "strain 0.30811405181884766\n",
            "strain 0.2672085165977478\n",
            "strain 0.2885337173938751\n",
            "strain 0.24889270961284637\n",
            "strain 0.31279540061950684\n",
            "strain 0.27849093079566956\n",
            "strain 0.27071413397789\n",
            "strain 0.2790449559688568\n",
            "strain 0.3641873598098755\n",
            "strain 0.4641417860984802\n",
            "strain 0.2938767671585083\n",
            "strain 0.41973766684532166\n",
            "strain 0.42059415578842163\n",
            "strain 0.381540983915329\n",
            "strain 0.45585086941719055\n",
            "strain 0.40381014347076416\n",
            "strain 0.499368816614151\n",
            "strain 0.39506274461746216\n",
            "strain 0.4173237979412079\n",
            "strain 0.2449612021446228\n",
            "strain 0.3379995822906494\n",
            "strain 0.2972569763660431\n",
            "strain 0.343819260597229\n",
            "strain 0.44521060585975647\n",
            "strain 0.31572669744491577\n",
            "strain 0.4116657078266144\n",
            "strain 0.4339772164821625\n",
            "strain 0.377621591091156\n",
            "strain 0.26176393032073975\n",
            "strain 0.3569481372833252\n",
            "strain 0.27710363268852234\n",
            "strain 0.32576102018356323\n",
            "strain 0.3064476549625397\n",
            "strain 0.40063709020614624\n",
            "strain 0.2849493622779846\n",
            "strain 0.34196197986602783\n",
            "strain 0.3017323315143585\n",
            "strain 0.422338604927063\n",
            "strain 0.4348309338092804\n",
            "strain 0.3705470561981201\n",
            "strain 0.46249818801879883\n",
            "strain 0.2644481658935547\n",
            "strain 0.3315548598766327\n",
            "strain 0.2773248851299286\n",
            "strain 0.27980655431747437\n",
            "strain 0.4133390784263611\n",
            "strain 0.3561001121997833\n",
            "classify 2.0277099609375\n",
            "classify 1.95855712890625\n",
            "classify 2.06927490234375\n",
            "classify 1.92218017578125\n",
            "classify 2.09814453125\n",
            "classify 2.00506591796875\n",
            "classify 2.1005859375\n",
            "classify 1.94244384765625\n",
            "classify 2.1475830078125\n",
            "classify 2.1019287109375\n",
            "classify 2.1173095703125\n",
            "0.25\n",
            "0.265625\n",
            "0.234375\n",
            "0.203125\n",
            "0.21875\n",
            "0.1875\n",
            "0.171875\n",
            "0.28125\n",
            "0.296875\n",
            "0.21875\n",
            "0.203125\n",
            "479\n",
            "strain 0.6343491077423096\n",
            "strain 0.24747014045715332\n",
            "strain 0.2801072597503662\n",
            "strain 0.3473135232925415\n",
            "strain 0.3430301249027252\n",
            "strain 0.40539318323135376\n",
            "strain 0.2993355989456177\n",
            "strain 0.3857406675815582\n",
            "strain 0.3049832880496979\n",
            "strain 0.29031285643577576\n",
            "strain 0.39367133378982544\n",
            "strain 0.4988313913345337\n",
            "strain 0.4513055384159088\n",
            "strain 0.2627217769622803\n",
            "strain 0.39640429615974426\n",
            "strain 0.3062843680381775\n",
            "strain 0.39881330728530884\n",
            "strain 0.2865847051143646\n",
            "strain 0.2887168228626251\n",
            "strain 0.4371662437915802\n",
            "strain 0.2820407748222351\n",
            "strain 0.45286327600479126\n",
            "strain 0.29233500361442566\n",
            "strain 0.24750961363315582\n",
            "strain 0.3619421720504761\n",
            "strain 0.5117397308349609\n",
            "strain 0.3204403519630432\n",
            "strain 0.2545858323574066\n",
            "strain 0.2985067367553711\n",
            "strain 0.34899330139160156\n",
            "strain 0.26560020446777344\n",
            "strain 0.3838365375995636\n",
            "strain 0.3157139718532562\n",
            "strain 0.28210535645484924\n",
            "strain 0.2554056942462921\n",
            "strain 0.25675511360168457\n",
            "strain 0.28174644708633423\n",
            "strain 0.28040921688079834\n",
            "strain 0.2879237234592438\n",
            "strain 0.46730753779411316\n",
            "strain 0.2551427483558655\n",
            "strain 0.5045531392097473\n",
            "strain 0.2457493245601654\n",
            "strain 0.41635507345199585\n",
            "strain 0.266384482383728\n",
            "strain 0.25290387868881226\n",
            "strain 0.43226808309555054\n",
            "strain 0.27201148867607117\n",
            "strain 0.29799309372901917\n",
            "strain 0.25005480647087097\n",
            "strain 0.23184947669506073\n",
            "classify 2.05340576171875\n",
            "classify 2.0721435546875\n",
            "classify 2.00689697265625\n",
            "classify 1.9407958984375\n",
            "classify 1.914306640625\n",
            "classify 2.00140380859375\n",
            "classify 2.059326171875\n",
            "classify 1.9981689453125\n",
            "classify 2.12908935546875\n",
            "classify 2.0775146484375\n",
            "classify 2.077880859375\n",
            "0.1875\n",
            "0.234375\n",
            "0.296875\n",
            "0.28125\n",
            "0.296875\n",
            "0.171875\n",
            "0.265625\n",
            "0.3125\n",
            "0.25\n",
            "0.234375\n",
            "0.234375\n",
            "480\n",
            "strain 0.23463180661201477\n",
            "strain 0.26033294200897217\n",
            "strain 0.2871193587779999\n",
            "strain 0.24733203649520874\n",
            "strain 0.3798687160015106\n",
            "strain 0.40798646211624146\n",
            "strain 0.49731338024139404\n",
            "strain 0.3141666054725647\n",
            "strain 0.2735796272754669\n",
            "strain 0.3784397840499878\n",
            "strain 0.2883470952510834\n",
            "strain 0.32511046528816223\n",
            "strain 0.24948367476463318\n",
            "strain 0.26213717460632324\n",
            "strain 0.25602492690086365\n",
            "strain 0.33721500635147095\n",
            "strain 0.37690919637680054\n",
            "strain 0.3149147629737854\n",
            "strain 0.3374289274215698\n",
            "strain 0.3766765892505646\n",
            "strain 0.2775720953941345\n",
            "strain 0.3150983452796936\n",
            "strain 0.28087666630744934\n",
            "strain 0.3035978376865387\n",
            "strain 0.26871415972709656\n",
            "strain 0.26745548844337463\n",
            "strain 0.36343151330947876\n",
            "strain 0.25976788997650146\n",
            "strain 0.2929011583328247\n",
            "strain 0.4151359498500824\n",
            "strain 0.2394711673259735\n",
            "strain 0.426860511302948\n",
            "strain 0.3971572816371918\n",
            "strain 0.22582484781742096\n",
            "strain 0.26771387457847595\n",
            "strain 0.23114317655563354\n",
            "strain 0.3507363796234131\n",
            "strain 0.2852376401424408\n",
            "strain 0.4214031398296356\n",
            "strain 0.2980605661869049\n",
            "strain 0.3214479386806488\n",
            "strain 0.4401838183403015\n",
            "strain 0.22904528677463531\n",
            "strain 0.3962459862232208\n",
            "strain 0.431854248046875\n",
            "strain 0.3632318079471588\n",
            "strain 0.35700663924217224\n",
            "strain 0.4454370439052582\n",
            "strain 0.20634736120700836\n",
            "strain 0.3220115005970001\n",
            "strain 0.32090428471565247\n",
            "classify 2.042724609375\n",
            "classify 2.154541015625\n",
            "classify 2.1239013671875\n",
            "classify 2.0133056640625\n",
            "classify 1.9072265625\n",
            "classify 2.251220703125\n",
            "classify 2.01611328125\n",
            "classify 1.870849609375\n",
            "classify 1.970947265625\n",
            "classify 2.14520263671875\n",
            "classify 1.97882080078125\n",
            "0.328125\n",
            "0.1875\n",
            "0.1875\n",
            "0.234375\n",
            "0.21875\n",
            "0.265625\n",
            "0.234375\n",
            "0.203125\n",
            "0.234375\n",
            "0.21875\n",
            "0.359375\n",
            "481\n",
            "strain 0.5111743807792664\n",
            "strain 0.45569923520088196\n",
            "strain 0.2817756235599518\n",
            "strain 0.3145768940448761\n",
            "strain 0.3767831325531006\n",
            "strain 0.4077307879924774\n",
            "strain 0.28874126076698303\n",
            "strain 0.29509881138801575\n",
            "strain 0.25722023844718933\n",
            "strain 0.376515656709671\n",
            "strain 0.3128899931907654\n",
            "strain 0.34349524974823\n",
            "strain 0.41523072123527527\n",
            "strain 0.2998996376991272\n",
            "strain 0.3206292390823364\n",
            "strain 0.3595294952392578\n",
            "strain 0.34718018770217896\n",
            "strain 0.24693846702575684\n",
            "strain 0.35466936230659485\n",
            "strain 0.3818257451057434\n",
            "strain 0.3847370445728302\n",
            "strain 0.29752016067504883\n",
            "strain 0.44481348991394043\n",
            "strain 0.37360259890556335\n",
            "strain 0.3384908139705658\n",
            "strain 0.38581740856170654\n",
            "strain 0.44393599033355713\n",
            "strain 0.33788540959358215\n",
            "strain 0.32228851318359375\n",
            "strain 0.3503854274749756\n",
            "strain 0.38492551445961\n",
            "strain 0.2572175860404968\n",
            "strain 0.2802022099494934\n",
            "strain 0.30916741490364075\n",
            "strain 0.26572728157043457\n",
            "strain 0.3464246988296509\n",
            "strain 0.3573060929775238\n",
            "strain 0.4832789897918701\n",
            "strain 0.3123705983161926\n",
            "strain 0.3490787446498871\n",
            "strain 0.2582104206085205\n",
            "strain 0.4418033957481384\n",
            "strain 0.41061943769454956\n",
            "strain 0.32745999097824097\n",
            "strain 0.24652467668056488\n",
            "strain 0.4089275598526001\n",
            "strain 0.2876042127609253\n",
            "strain 0.274415522813797\n",
            "strain 0.3829804062843323\n",
            "strain 0.4138197600841522\n",
            "strain 0.41404086351394653\n",
            "classify 2.0906982421875\n",
            "classify 1.98828125\n",
            "classify 1.8851318359375\n",
            "classify 2.017822265625\n",
            "classify 2.085693359375\n",
            "classify 1.9564208984375\n",
            "classify 2.168701171875\n",
            "classify 2.0089111328125\n",
            "classify 1.98187255859375\n",
            "classify 1.9906005859375\n",
            "classify 2.00518798828125\n",
            "0.265625\n",
            "0.234375\n",
            "0.375\n",
            "0.3125\n",
            "0.203125\n",
            "0.25\n",
            "0.25\n",
            "0.265625\n",
            "0.234375\n",
            "0.203125\n",
            "0.25\n",
            "482\n",
            "strain 0.40422025322914124\n",
            "strain 0.3766239881515503\n",
            "strain 0.25360801815986633\n",
            "strain 0.2820931077003479\n",
            "strain 0.45130112767219543\n",
            "strain 0.258755087852478\n",
            "strain 0.3762207627296448\n",
            "strain 0.3330151438713074\n",
            "strain 0.41825589537620544\n",
            "strain 0.24902145564556122\n",
            "strain 0.25680670142173767\n",
            "strain 0.39057686924934387\n",
            "strain 0.33698374032974243\n",
            "strain 0.37259045243263245\n",
            "strain 0.48500683903694153\n",
            "strain 0.4179185628890991\n",
            "strain 0.25141993165016174\n",
            "strain 0.3188314437866211\n",
            "strain 0.3207784593105316\n",
            "strain 0.2669909596443176\n",
            "strain 0.40662679076194763\n",
            "strain 0.30095839500427246\n",
            "strain 0.24354177713394165\n",
            "strain 0.4532252252101898\n",
            "strain 0.3142054080963135\n",
            "strain 0.40692123770713806\n",
            "strain 0.2511298954486847\n",
            "strain 0.39307382702827454\n",
            "strain 0.3344961106777191\n",
            "strain 0.41058704257011414\n",
            "strain 0.3407101035118103\n",
            "strain 0.38332200050354004\n",
            "strain 0.4076067805290222\n",
            "strain 0.28433915972709656\n",
            "strain 0.4229515790939331\n",
            "strain 0.2696954309940338\n",
            "strain 0.34246957302093506\n",
            "strain 0.32570621371269226\n",
            "strain 0.27687928080558777\n",
            "strain 0.3112856149673462\n",
            "strain 0.26585808396339417\n",
            "strain 0.3427318334579468\n",
            "strain 0.3792906701564789\n",
            "strain 0.5021997094154358\n",
            "strain 0.26014137268066406\n",
            "strain 0.24742287397384644\n",
            "strain 0.298552930355072\n",
            "strain 0.4482666850090027\n",
            "strain 0.32625168561935425\n",
            "strain 0.2468995451927185\n",
            "strain 0.3811963200569153\n",
            "classify 2.03997802734375\n",
            "classify 2.162841796875\n",
            "classify 2.07257080078125\n",
            "classify 1.92559814453125\n",
            "classify 2.0380859375\n",
            "classify 1.9022216796875\n",
            "classify 1.960205078125\n",
            "classify 2.03076171875\n",
            "classify 2.20703125\n",
            "classify 2.0855712890625\n",
            "classify 2.0477294921875\n",
            "0.25\n",
            "0.3125\n",
            "0.171875\n",
            "0.28125\n",
            "0.328125\n",
            "0.28125\n",
            "0.1875\n",
            "0.34375\n",
            "0.3125\n",
            "0.25\n",
            "0.34375\n",
            "483\n",
            "strain 0.2706809937953949\n",
            "strain 0.31572890281677246\n",
            "strain 0.2750568985939026\n",
            "strain 0.3064892888069153\n",
            "strain 0.3699078857898712\n",
            "strain 0.4885278344154358\n",
            "strain 0.434465616941452\n",
            "strain 0.34917813539505005\n",
            "strain 0.3945811688899994\n",
            "strain 0.3595369756221771\n",
            "strain 0.43828344345092773\n",
            "strain 0.42770498991012573\n",
            "strain 0.31495481729507446\n",
            "strain 0.36206021904945374\n",
            "strain 0.31373491883277893\n",
            "strain 0.3954339027404785\n",
            "strain 0.3646678626537323\n",
            "strain 0.32540857791900635\n",
            "strain 0.2750187814235687\n",
            "strain 0.31284651160240173\n",
            "strain 0.29535895586013794\n",
            "strain 0.24777273833751678\n",
            "strain 0.2581949532032013\n",
            "strain 0.4206976890563965\n",
            "strain 0.2703280448913574\n",
            "strain 0.45721936225891113\n",
            "strain 0.28892141580581665\n",
            "strain 0.4182961583137512\n",
            "strain 0.3617973029613495\n",
            "strain 0.3563867509365082\n",
            "strain 0.34480980038642883\n",
            "strain 0.2975139021873474\n",
            "strain 0.2470819354057312\n",
            "strain 0.43356654047966003\n",
            "strain 0.2846018671989441\n",
            "strain 0.2975112497806549\n",
            "strain 0.3817750811576843\n",
            "strain 0.3802163600921631\n",
            "strain 0.3653146028518677\n",
            "strain 0.28528451919555664\n",
            "strain 0.3806684911251068\n",
            "strain 0.23514817655086517\n",
            "strain 0.24300183355808258\n",
            "strain 0.4165284335613251\n",
            "strain 0.3482397496700287\n",
            "strain 0.3913145661354065\n",
            "strain 0.4349818527698517\n",
            "strain 0.28191515803337097\n",
            "strain 0.39064139127731323\n",
            "strain 0.31799986958503723\n",
            "strain 0.33243274688720703\n",
            "classify 2.1329345703125\n",
            "classify 2.1827392578125\n",
            "classify 2.07415771484375\n",
            "classify 2.113037109375\n",
            "classify 1.97064208984375\n",
            "classify 1.99176025390625\n",
            "classify 2.0296630859375\n",
            "classify 1.90496826171875\n",
            "classify 2.0533447265625\n",
            "classify 2.113037109375\n",
            "classify 2.0335693359375\n",
            "0.265625\n",
            "0.28125\n",
            "0.1875\n",
            "0.171875\n",
            "0.28125\n",
            "0.203125\n",
            "0.171875\n",
            "0.25\n",
            "0.359375\n",
            "0.234375\n",
            "0.3125\n",
            "484\n",
            "strain 0.4640561640262604\n",
            "strain 0.33608582615852356\n",
            "strain 0.21339105069637299\n",
            "strain 0.35798555612564087\n",
            "strain 0.34917908906936646\n",
            "strain 0.4457876980304718\n",
            "strain 0.2953152358531952\n",
            "strain 0.36082762479782104\n",
            "strain 0.2899872064590454\n",
            "strain 0.23711693286895752\n",
            "strain 0.26678961515426636\n",
            "strain 0.24675633013248444\n",
            "strain 0.36947041749954224\n",
            "strain 0.39154618978500366\n",
            "strain 0.3758866786956787\n",
            "strain 0.3716159164905548\n",
            "strain 0.28187885880470276\n",
            "strain 0.3692583739757538\n",
            "strain 0.3703382611274719\n",
            "strain 0.27626800537109375\n",
            "strain 0.3768145442008972\n",
            "strain 0.28763526678085327\n",
            "strain 0.3094943165779114\n",
            "strain 0.23577210307121277\n",
            "strain 0.40963214635849\n",
            "strain 0.25783854722976685\n",
            "strain 0.2699068486690521\n",
            "strain 0.2790728807449341\n",
            "strain 0.22899675369262695\n",
            "strain 0.29627180099487305\n",
            "strain 0.45794522762298584\n",
            "strain 0.39421263337135315\n",
            "strain 0.33627715706825256\n",
            "strain 0.46208375692367554\n",
            "strain 0.2449483573436737\n",
            "strain 0.4237801730632782\n",
            "strain 0.3172364830970764\n",
            "strain 0.24563181400299072\n",
            "strain 0.32591238617897034\n",
            "strain 0.24628804624080658\n",
            "strain 0.4533272981643677\n",
            "strain 0.31280097365379333\n",
            "strain 0.20883618295192719\n",
            "strain 0.2650701403617859\n",
            "strain 0.31414130330085754\n",
            "strain 0.2532378137111664\n",
            "strain 0.29244258999824524\n",
            "strain 0.23477669060230255\n",
            "strain 0.25562161207199097\n",
            "strain 0.2726973593235016\n",
            "strain 0.26450300216674805\n",
            "classify 2.14990234375\n",
            "classify 2.01507568359375\n",
            "classify 2.0010986328125\n",
            "classify 2.0037841796875\n",
            "classify 2.09625244140625\n",
            "classify 2.040283203125\n",
            "classify 2.06842041015625\n",
            "classify 2.07904052734375\n",
            "classify 2.089111328125\n",
            "classify 1.831787109375\n",
            "classify 2.04107666015625\n",
            "0.140625\n",
            "0.234375\n",
            "0.3125\n",
            "0.328125\n",
            "0.28125\n",
            "0.234375\n",
            "0.203125\n",
            "0.3125\n",
            "0.3125\n",
            "0.234375\n",
            "0.25\n",
            "485\n",
            "strain 0.4248979687690735\n",
            "strain 0.289490282535553\n",
            "strain 0.49978962540626526\n",
            "strain 0.3555842638015747\n",
            "strain 0.3749808073043823\n",
            "strain 0.4022313952445984\n",
            "strain 0.27219295501708984\n",
            "strain 0.3845749795436859\n",
            "strain 0.4383132755756378\n",
            "strain 0.27197813987731934\n",
            "strain 0.34836307168006897\n",
            "strain 0.3408433198928833\n",
            "strain 0.33842071890830994\n",
            "strain 0.2805342674255371\n",
            "strain 0.5068380832672119\n",
            "strain 0.28275004029273987\n",
            "strain 0.3127463757991791\n",
            "strain 0.3560616075992584\n",
            "strain 0.280696302652359\n",
            "strain 0.3653140366077423\n",
            "strain 0.2671198844909668\n",
            "strain 0.31416240334510803\n",
            "strain 0.4085533916950226\n",
            "strain 0.2819918692111969\n",
            "strain 0.2729375958442688\n",
            "strain 0.4916950762271881\n",
            "strain 0.41259345412254333\n",
            "strain 0.2878486216068268\n",
            "strain 0.292296439409256\n",
            "strain 0.22144776582717896\n",
            "strain 0.42468002438545227\n",
            "strain 0.2505493462085724\n",
            "strain 0.45040762424468994\n",
            "strain 0.4528222978115082\n",
            "strain 0.3017250895500183\n",
            "strain 0.26440903544425964\n",
            "strain 0.29015982151031494\n",
            "strain 0.2719476521015167\n",
            "strain 0.3562763035297394\n",
            "strain 0.444368839263916\n",
            "strain 0.4327796697616577\n",
            "strain 0.28496265411376953\n",
            "strain 0.35738807916641235\n",
            "strain 0.23624342679977417\n",
            "strain 0.2703841030597687\n",
            "strain 0.3022533655166626\n",
            "strain 0.2670101821422577\n",
            "strain 0.2846618592739105\n",
            "strain 0.4014342427253723\n",
            "strain 0.46065643429756165\n",
            "strain 0.3713962435722351\n",
            "classify 1.97412109375\n",
            "classify 2.05157470703125\n",
            "classify 2.093505859375\n",
            "classify 2.155029296875\n",
            "classify 2.08154296875\n",
            "classify 2.072998046875\n",
            "classify 2.02752685546875\n",
            "classify 2.0677490234375\n",
            "classify 2.029052734375\n",
            "classify 2.0712890625\n",
            "classify 1.9676513671875\n",
            "0.28125\n",
            "0.25\n",
            "0.28125\n",
            "0.265625\n",
            "0.296875\n",
            "0.25\n",
            "0.21875\n",
            "0.078125\n",
            "0.3125\n",
            "0.28125\n",
            "0.1875\n",
            "486\n",
            "strain 0.3989514708518982\n",
            "strain 0.29188236594200134\n",
            "strain 0.28826871514320374\n",
            "strain 0.27697092294692993\n",
            "strain 0.4237788915634155\n",
            "strain 0.26261085271835327\n",
            "strain 0.4147607982158661\n",
            "strain 0.48809918761253357\n",
            "strain 0.2438109964132309\n",
            "strain 0.2903423309326172\n",
            "strain 0.27003371715545654\n",
            "strain 0.21412675082683563\n",
            "strain 0.3016248345375061\n",
            "strain 0.30399221181869507\n",
            "strain 0.49515458941459656\n",
            "strain 0.24714887142181396\n",
            "strain 0.25148117542266846\n",
            "strain 0.2935757040977478\n",
            "strain 0.36408790946006775\n",
            "strain 0.4804382920265198\n",
            "strain 0.2618766129016876\n",
            "strain 0.42465490102767944\n",
            "strain 0.22852329909801483\n",
            "strain 0.46768930554389954\n",
            "strain 0.27703818678855896\n",
            "strain 0.3163308799266815\n",
            "strain 0.4003499150276184\n",
            "strain 0.29356077313423157\n",
            "strain 0.337820440530777\n",
            "strain 0.23925916850566864\n",
            "strain 0.26018020510673523\n",
            "strain 0.4411252439022064\n",
            "strain 0.2886054515838623\n",
            "strain 0.3202577531337738\n",
            "strain 0.31626656651496887\n",
            "strain 0.221210315823555\n",
            "strain 0.2812019884586334\n",
            "strain 0.29951921105384827\n",
            "strain 0.409604012966156\n",
            "strain 0.26906004548072815\n",
            "strain 0.2802175283432007\n",
            "strain 0.35565078258514404\n",
            "strain 0.32191362977027893\n",
            "strain 0.269644170999527\n",
            "strain 0.3451513946056366\n",
            "strain 0.34268173575401306\n",
            "strain 0.3107185661792755\n",
            "strain 0.42185530066490173\n",
            "strain 0.36688169836997986\n",
            "strain 0.3726857006549835\n",
            "strain 0.27162086963653564\n",
            "classify 2.060546875\n",
            "classify 2.06396484375\n",
            "classify 1.97625732421875\n",
            "classify 2.12353515625\n",
            "classify 2.00390625\n",
            "classify 1.97216796875\n",
            "classify 2.0589599609375\n",
            "classify 2.104736328125\n",
            "classify 2.00665283203125\n",
            "classify 1.959228515625\n",
            "classify 2.110107421875\n",
            "0.1875\n",
            "0.171875\n",
            "0.28125\n",
            "0.203125\n",
            "0.140625\n",
            "0.203125\n",
            "0.203125\n",
            "0.265625\n",
            "0.28125\n",
            "0.21875\n",
            "0.40625\n",
            "487\n",
            "strain 0.3445190191268921\n",
            "strain 0.4153274595737457\n",
            "strain 0.4335481524467468\n",
            "strain 0.4298734664916992\n",
            "strain 0.3246781826019287\n",
            "strain 0.25862205028533936\n",
            "strain 0.4004024863243103\n",
            "strain 0.37404772639274597\n",
            "strain 0.37359386682510376\n",
            "strain 0.34677654504776\n",
            "strain 0.32616519927978516\n",
            "strain 0.3406222462654114\n",
            "strain 0.2870735228061676\n",
            "strain 0.492931991815567\n",
            "strain 0.37596023082733154\n",
            "strain 0.3854096233844757\n",
            "strain 0.2988094985485077\n",
            "strain 0.3154025077819824\n",
            "strain 0.2722417116165161\n",
            "strain 0.20954447984695435\n",
            "strain 0.2832578122615814\n",
            "strain 0.3245958387851715\n",
            "strain 0.2422291785478592\n",
            "strain 0.24859336018562317\n",
            "strain 0.3009924590587616\n",
            "strain 0.39777860045433044\n",
            "strain 0.4349415898323059\n",
            "strain 0.30653584003448486\n",
            "strain 0.26434364914894104\n",
            "strain 0.36707669496536255\n",
            "strain 0.24782082438468933\n",
            "strain 0.527449369430542\n",
            "strain 0.29134076833724976\n",
            "strain 0.25085029006004333\n",
            "strain 0.30579307675361633\n",
            "strain 0.22430971264839172\n",
            "strain 0.49455714225769043\n",
            "strain 0.34555304050445557\n",
            "strain 0.27564236521720886\n",
            "strain 0.40359821915626526\n",
            "strain 0.41196784377098083\n",
            "strain 0.2641308605670929\n",
            "strain 0.23735305666923523\n",
            "strain 0.45687344670295715\n",
            "strain 0.2402961701154709\n",
            "strain 0.40504246950149536\n",
            "strain 0.3224744200706482\n",
            "strain 0.4449828565120697\n",
            "strain 0.27927839756011963\n",
            "strain 0.30154040455818176\n",
            "strain 0.35692864656448364\n",
            "classify 2.05224609375\n",
            "classify 1.984130859375\n",
            "classify 2.24542236328125\n",
            "classify 2.02069091796875\n",
            "classify 2.05657958984375\n",
            "classify 1.928466796875\n",
            "classify 2.1456298828125\n",
            "classify 1.96942138671875\n",
            "classify 1.987060546875\n",
            "classify 2.03472900390625\n",
            "classify 2.0047607421875\n",
            "0.296875\n",
            "0.171875\n",
            "0.234375\n",
            "0.28125\n",
            "0.203125\n",
            "0.21875\n",
            "0.234375\n",
            "0.28125\n",
            "0.265625\n",
            "0.265625\n",
            "0.3125\n",
            "488\n",
            "strain 0.4669382572174072\n",
            "strain 0.42281562089920044\n",
            "strain 0.3693629205226898\n",
            "strain 0.28173866868019104\n",
            "strain 0.3647083640098572\n",
            "strain 0.3592536747455597\n",
            "strain 0.26224997639656067\n",
            "strain 0.27901870012283325\n",
            "strain 0.35288557410240173\n",
            "strain 0.3002782166004181\n",
            "strain 0.290319561958313\n",
            "strain 0.2622467279434204\n",
            "strain 0.27357837557792664\n",
            "strain 0.36433035135269165\n",
            "strain 0.4121991693973541\n",
            "strain 0.4314933717250824\n",
            "strain 0.4183747470378876\n",
            "strain 0.4495196044445038\n",
            "strain 0.43869635462760925\n",
            "strain 0.2668943405151367\n",
            "strain 0.398550808429718\n",
            "strain 0.27610209584236145\n",
            "strain 0.3179020285606384\n",
            "strain 0.39550888538360596\n",
            "strain 0.4286418557167053\n",
            "strain 0.24634672701358795\n",
            "strain 0.2716793417930603\n",
            "strain 0.28145498037338257\n",
            "strain 0.2823885977268219\n",
            "strain 0.4702644646167755\n",
            "strain 0.3637010455131531\n",
            "strain 0.3792056441307068\n",
            "strain 0.31306564807891846\n",
            "strain 0.30858534574508667\n",
            "strain 0.3006528615951538\n",
            "strain 0.2869093120098114\n",
            "strain 0.30387094616889954\n",
            "strain 0.29921409487724304\n",
            "strain 0.29872632026672363\n",
            "strain 0.3666897714138031\n",
            "strain 0.28319185972213745\n",
            "strain 0.3241427540779114\n",
            "strain 0.441042423248291\n",
            "strain 0.2999109625816345\n",
            "strain 0.42741379141807556\n",
            "strain 0.2549683451652527\n",
            "strain 0.4422159492969513\n",
            "strain 0.3414188325405121\n",
            "strain 0.33445486426353455\n",
            "strain 0.3105230927467346\n",
            "strain 0.29198089241981506\n",
            "classify 2.1322021484375\n",
            "classify 2.300537109375\n",
            "classify 2.11944580078125\n",
            "classify 2.03497314453125\n",
            "classify 1.99884033203125\n",
            "classify 1.997314453125\n",
            "classify 2.07867431640625\n",
            "classify 1.9102783203125\n",
            "classify 2.16064453125\n",
            "classify 1.98150634765625\n",
            "classify 2.0931396484375\n",
            "0.375\n",
            "0.28125\n",
            "0.203125\n",
            "0.265625\n",
            "0.296875\n",
            "0.234375\n",
            "0.296875\n",
            "0.21875\n",
            "0.25\n",
            "0.265625\n",
            "0.296875\n",
            "489\n",
            "strain 0.3094201385974884\n",
            "strain 0.390714555978775\n",
            "strain 0.34601548314094543\n",
            "strain 0.5272650718688965\n",
            "strain 0.3505038619041443\n",
            "strain 0.3614274561405182\n",
            "strain 0.41601207852363586\n",
            "strain 0.4193858802318573\n",
            "strain 0.3353109359741211\n",
            "strain 0.3810052275657654\n",
            "strain 0.31322818994522095\n",
            "strain 0.3722141683101654\n",
            "strain 0.3453230559825897\n",
            "strain 0.37094801664352417\n",
            "strain 0.28463491797447205\n",
            "strain 0.3401545584201813\n",
            "strain 0.24775397777557373\n",
            "strain 0.42895999550819397\n",
            "strain 0.44713181257247925\n",
            "strain 0.3329354524612427\n",
            "strain 0.29383766651153564\n",
            "strain 0.31241050362586975\n",
            "strain 0.3994219899177551\n",
            "strain 0.35038501024246216\n",
            "strain 0.34660717844963074\n",
            "strain 0.2592827379703522\n",
            "strain 0.37294819951057434\n",
            "strain 0.46194618940353394\n",
            "strain 0.36284351348876953\n",
            "strain 0.42207369208335876\n",
            "strain 0.3859732151031494\n",
            "strain 0.31621190905570984\n",
            "strain 0.3655793070793152\n",
            "strain 0.24678198993206024\n",
            "strain 0.27271226048469543\n",
            "strain 0.372270792722702\n",
            "strain 0.25872015953063965\n",
            "strain 0.3959267735481262\n",
            "strain 0.23873673379421234\n",
            "strain 0.31369099020957947\n",
            "strain 0.2699800431728363\n",
            "strain 0.45648545026779175\n",
            "strain 0.399728924036026\n",
            "strain 0.35424381494522095\n",
            "strain 0.34253332018852234\n",
            "strain 0.4186495542526245\n",
            "strain 0.24212516844272614\n",
            "strain 0.33090585470199585\n",
            "strain 0.36990633606910706\n",
            "strain 0.28902530670166016\n",
            "strain 0.2726480960845947\n",
            "classify 1.94366455078125\n",
            "classify 2.03271484375\n",
            "classify 1.94903564453125\n",
            "classify 2.15863037109375\n",
            "classify 1.943603515625\n",
            "classify 2.19903564453125\n",
            "classify 2.027099609375\n",
            "classify 2.009765625\n",
            "classify 2.0802001953125\n",
            "classify 1.9761962890625\n",
            "classify 2.07330322265625\n",
            "0.1875\n",
            "0.234375\n",
            "0.140625\n",
            "0.28125\n",
            "0.234375\n",
            "0.171875\n",
            "0.328125\n",
            "0.21875\n",
            "0.203125\n",
            "0.328125\n",
            "0.25\n",
            "490\n",
            "strain 0.308810830116272\n",
            "strain 0.34857305884361267\n",
            "strain 0.2652677893638611\n",
            "strain 0.3833996653556824\n",
            "strain 0.29095715284347534\n",
            "strain 0.3048740029335022\n",
            "strain 0.2819232642650604\n",
            "strain 0.4146021902561188\n",
            "strain 0.43038660287857056\n",
            "strain 0.3347128629684448\n",
            "strain 0.34820979833602905\n",
            "strain 0.2830926775932312\n",
            "strain 0.4342438876628876\n",
            "strain 0.40239474177360535\n",
            "strain 0.3211667537689209\n",
            "strain 0.2586974501609802\n",
            "strain 0.4195682406425476\n",
            "strain 0.3882228434085846\n",
            "strain 0.2724432945251465\n",
            "strain 0.2955853044986725\n",
            "strain 0.4037959575653076\n",
            "strain 0.24961534142494202\n",
            "strain 0.37376391887664795\n",
            "strain 0.3670214116573334\n",
            "strain 0.3272361159324646\n",
            "strain 0.4337058663368225\n",
            "strain 0.2912594974040985\n",
            "strain 0.3125259578227997\n",
            "strain 0.23486505448818207\n",
            "strain 0.3880887031555176\n",
            "strain 0.3381567597389221\n",
            "strain 0.3632839620113373\n",
            "strain 0.4134635925292969\n",
            "strain 0.316334992647171\n",
            "strain 0.40031886100769043\n",
            "strain 0.3184274137020111\n",
            "strain 0.33595553040504456\n",
            "strain 0.3004106283187866\n",
            "strain 0.23250572383403778\n",
            "strain 0.23616676032543182\n",
            "strain 0.24467317759990692\n",
            "strain 0.3822176456451416\n",
            "strain 0.3684764802455902\n",
            "strain 0.3416163921356201\n",
            "strain 0.2470058798789978\n",
            "strain 0.2545715570449829\n",
            "strain 0.3089112639427185\n",
            "strain 0.27554190158843994\n",
            "strain 0.38908886909484863\n",
            "strain 0.2407771199941635\n",
            "strain 0.2682051360607147\n",
            "classify 2.08489990234375\n",
            "classify 2.119140625\n",
            "classify 2.0401611328125\n",
            "classify 2.1300048828125\n",
            "classify 2.190185546875\n",
            "classify 2.0831298828125\n",
            "classify 1.966064453125\n",
            "classify 2.1607666015625\n",
            "classify 1.9151611328125\n",
            "classify 1.9949951171875\n",
            "classify 2.122314453125\n",
            "0.296875\n",
            "0.21875\n",
            "0.203125\n",
            "0.34375\n",
            "0.21875\n",
            "0.28125\n",
            "0.171875\n",
            "0.28125\n",
            "0.203125\n",
            "0.140625\n",
            "0.265625\n",
            "491\n",
            "strain 0.27190670371055603\n",
            "strain 0.3412410020828247\n",
            "strain 0.38362717628479004\n",
            "strain 0.2614416182041168\n",
            "strain 0.33918172121047974\n",
            "strain 0.23272930085659027\n",
            "strain 0.2309243381023407\n",
            "strain 0.41007286310195923\n",
            "strain 0.2570788264274597\n",
            "strain 0.2657543420791626\n",
            "strain 0.3061470687389374\n",
            "strain 0.40424036979675293\n",
            "strain 0.3423336446285248\n",
            "strain 0.34425413608551025\n",
            "strain 0.3821156322956085\n",
            "strain 0.2987738847732544\n",
            "strain 0.345749169588089\n",
            "strain 0.23593835532665253\n",
            "strain 0.2633489966392517\n",
            "strain 0.2860041558742523\n",
            "strain 0.3700188994407654\n",
            "strain 0.4265143573284149\n",
            "strain 0.21354836225509644\n",
            "strain 0.41394275426864624\n",
            "strain 0.24191032350063324\n",
            "strain 0.4112705588340759\n",
            "strain 0.3123050928115845\n",
            "strain 0.24488316476345062\n",
            "strain 0.37232619524002075\n",
            "strain 0.3714808523654938\n",
            "strain 0.27655649185180664\n",
            "strain 0.2607453167438507\n",
            "strain 0.4066305160522461\n",
            "strain 0.34974586963653564\n",
            "strain 0.3902769088745117\n",
            "strain 0.2559727430343628\n",
            "strain 0.253812700510025\n",
            "strain 0.27047064900398254\n",
            "strain 0.3722750246524811\n",
            "strain 0.4234713017940521\n",
            "strain 0.31660905480384827\n",
            "strain 0.3178479075431824\n",
            "strain 0.2532426118850708\n",
            "strain 0.24094368517398834\n",
            "strain 0.47691109776496887\n",
            "strain 0.4300611913204193\n",
            "strain 0.25193536281585693\n",
            "strain 0.37931522727012634\n",
            "strain 0.36757156252861023\n",
            "strain 0.4251434803009033\n",
            "strain 0.43792209029197693\n",
            "classify 1.94903564453125\n",
            "classify 2.15283203125\n",
            "classify 2.049560546875\n",
            "classify 1.9373779296875\n",
            "classify 1.9718017578125\n",
            "classify 2.01263427734375\n",
            "classify 1.99462890625\n",
            "classify 2.01727294921875\n",
            "classify 2.02099609375\n",
            "classify 2.0888671875\n",
            "classify 2.0938720703125\n",
            "0.21875\n",
            "0.25\n",
            "0.28125\n",
            "0.25\n",
            "0.140625\n",
            "0.28125\n",
            "0.234375\n",
            "0.203125\n",
            "0.25\n",
            "0.203125\n",
            "0.28125\n",
            "492\n",
            "strain 0.2736368179321289\n",
            "strain 0.3904580771923065\n",
            "strain 0.3837311565876007\n",
            "strain 0.31347399950027466\n",
            "strain 0.34417638182640076\n",
            "strain 0.33168360590934753\n",
            "strain 0.2766352593898773\n",
            "strain 0.42310091853141785\n",
            "strain 0.373397558927536\n",
            "strain 0.3069964051246643\n",
            "strain 0.311356782913208\n",
            "strain 0.24396954476833344\n",
            "strain 0.2511321008205414\n",
            "strain 0.24918325245380402\n",
            "strain 0.3061734735965729\n",
            "strain 0.40797996520996094\n",
            "strain 0.37493371963500977\n",
            "strain 0.40879741311073303\n",
            "strain 0.23548172414302826\n",
            "strain 0.3038252890110016\n",
            "strain 0.2666548788547516\n",
            "strain 0.4283330738544464\n",
            "strain 0.2310846894979477\n",
            "strain 0.28526827692985535\n",
            "strain 0.371944397687912\n",
            "strain 0.30220866203308105\n",
            "strain 0.28270837664604187\n",
            "strain 0.39059075713157654\n",
            "strain 0.26648378372192383\n",
            "strain 0.30379432439804077\n",
            "strain 0.2878253161907196\n",
            "strain 0.2906551957130432\n",
            "strain 0.2452506124973297\n",
            "strain 0.47756239771842957\n",
            "strain 0.23338745534420013\n",
            "strain 0.3815508782863617\n",
            "strain 0.3590998351573944\n",
            "strain 0.2184438556432724\n",
            "strain 0.28463587164878845\n",
            "strain 0.28050360083580017\n",
            "strain 0.44110041856765747\n",
            "strain 0.28386190533638\n",
            "strain 0.3497723937034607\n",
            "strain 0.391276478767395\n",
            "strain 0.2841041684150696\n",
            "strain 0.3613671660423279\n",
            "strain 0.45377445220947266\n",
            "strain 0.26474398374557495\n",
            "strain 0.27056729793548584\n",
            "strain 0.3509729206562042\n",
            "strain 0.29911866784095764\n",
            "classify 2.16595458984375\n",
            "classify 2.04766845703125\n",
            "classify 1.90521240234375\n",
            "classify 2.125732421875\n",
            "classify 2.012451171875\n",
            "classify 1.98309326171875\n",
            "classify 2.0301513671875\n",
            "classify 1.96875\n",
            "classify 2.14923095703125\n",
            "classify 1.9266357421875\n",
            "classify 2.16845703125\n",
            "0.25\n",
            "0.1875\n",
            "0.265625\n",
            "0.28125\n",
            "0.1875\n",
            "0.203125\n",
            "0.171875\n",
            "0.203125\n",
            "0.234375\n",
            "0.25\n",
            "0.296875\n",
            "493\n",
            "strain 0.26022642850875854\n",
            "strain 0.2591418921947479\n",
            "strain 0.2871789038181305\n",
            "strain 0.3000338077545166\n",
            "strain 0.373445600271225\n",
            "strain 0.2789534628391266\n",
            "strain 0.2828301191329956\n",
            "strain 0.2572616636753082\n",
            "strain 0.3422231674194336\n",
            "strain 0.3329485356807709\n",
            "strain 0.43244457244873047\n",
            "strain 0.2384926676750183\n",
            "strain 0.3017370104789734\n",
            "strain 0.20213155448436737\n",
            "strain 0.3519909083843231\n",
            "strain 0.2449522465467453\n",
            "strain 0.2600918412208557\n",
            "strain 0.24840064346790314\n",
            "strain 0.2924708127975464\n",
            "strain 0.2766971290111542\n",
            "strain 0.31135424971580505\n",
            "strain 0.28411388397216797\n",
            "strain 0.2994338870048523\n",
            "strain 0.3184265196323395\n",
            "strain 0.2513393759727478\n",
            "strain 0.4021189510822296\n",
            "strain 0.2465078979730606\n",
            "strain 0.3138849139213562\n",
            "strain 0.45334577560424805\n",
            "strain 0.35464730858802795\n",
            "strain 0.23398855328559875\n",
            "strain 0.4023144841194153\n",
            "strain 0.31741708517074585\n",
            "strain 0.27281084656715393\n",
            "strain 0.2971292734146118\n",
            "strain 0.3940480649471283\n",
            "strain 0.2633594274520874\n",
            "strain 0.40003877878189087\n",
            "strain 0.36804842948913574\n",
            "strain 0.2780451774597168\n",
            "strain 0.32792794704437256\n",
            "strain 0.25356170535087585\n",
            "strain 0.32406535744667053\n",
            "strain 0.317678302526474\n",
            "strain 0.3671555519104004\n",
            "strain 0.25356051325798035\n",
            "strain 0.3970288336277008\n",
            "strain 0.5068089365959167\n",
            "strain 0.25217321515083313\n",
            "strain 0.2865184545516968\n",
            "strain 0.2310394048690796\n",
            "classify 1.833251953125\n",
            "classify 2.0914306640625\n",
            "classify 2.06207275390625\n",
            "classify 2.16552734375\n",
            "classify 2.10986328125\n",
            "classify 1.8729248046875\n",
            "classify 2.02899169921875\n",
            "classify 2.21661376953125\n",
            "classify 1.954345703125\n",
            "classify 2.1263427734375\n",
            "classify 2.11248779296875\n",
            "0.21875\n",
            "0.140625\n",
            "0.265625\n",
            "0.203125\n",
            "0.28125\n",
            "0.140625\n",
            "0.234375\n",
            "0.1875\n",
            "0.390625\n",
            "0.234375\n",
            "0.234375\n",
            "494\n",
            "strain 0.29069873690605164\n",
            "strain 0.21898549795150757\n",
            "strain 0.24181783199310303\n",
            "strain 0.2671005427837372\n",
            "strain 0.29080602526664734\n",
            "strain 0.3634583055973053\n",
            "strain 0.2862546741962433\n",
            "strain 0.2527104616165161\n",
            "strain 0.2343934327363968\n",
            "strain 0.2521127760410309\n",
            "strain 0.2764608561992645\n",
            "strain 0.42381006479263306\n",
            "strain 0.36650940775871277\n",
            "strain 0.35467085242271423\n",
            "strain 0.4917537569999695\n",
            "strain 0.27005037665367126\n",
            "strain 0.5049063563346863\n",
            "strain 0.3138301968574524\n",
            "strain 0.3053348660469055\n",
            "strain 0.3955097198486328\n",
            "strain 0.31320682168006897\n",
            "strain 0.36879733204841614\n",
            "strain 0.4758893549442291\n",
            "strain 0.4633883237838745\n",
            "strain 0.2749292254447937\n",
            "strain 0.29999247193336487\n",
            "strain 0.23123586177825928\n",
            "strain 0.2678586542606354\n",
            "strain 0.2848817706108093\n",
            "strain 0.4676437973976135\n",
            "strain 0.27758651971817017\n",
            "strain 0.31063348054885864\n",
            "strain 0.36497762799263\n",
            "strain 0.40381723642349243\n",
            "strain 0.2210884988307953\n",
            "strain 0.3549952507019043\n",
            "strain 0.28726691007614136\n",
            "strain 0.33929991722106934\n",
            "strain 0.24027439951896667\n",
            "strain 0.25010937452316284\n",
            "strain 0.33232802152633667\n",
            "strain 0.3757944107055664\n",
            "strain 0.39445608854293823\n",
            "strain 0.25401395559310913\n",
            "strain 0.27760767936706543\n",
            "strain 0.25732624530792236\n",
            "strain 0.3202882409095764\n",
            "strain 0.26026538014411926\n",
            "strain 0.2507097125053406\n",
            "strain 0.22668057680130005\n",
            "strain 0.2475292980670929\n",
            "classify 2.0311279296875\n",
            "classify 1.95849609375\n",
            "classify 1.9344482421875\n",
            "classify 2.0501708984375\n",
            "classify 1.97216796875\n",
            "classify 2.027099609375\n",
            "classify 1.89190673828125\n",
            "classify 2.1839599609375\n",
            "classify 2.028564453125\n",
            "classify 2.185546875\n",
            "classify 2.0780029296875\n",
            "0.25\n",
            "0.296875\n",
            "0.15625\n",
            "0.15625\n",
            "0.1875\n",
            "0.265625\n",
            "0.390625\n",
            "0.21875\n",
            "0.265625\n",
            "0.265625\n",
            "0.296875\n",
            "495\n",
            "strain 0.4452250301837921\n",
            "strain 0.3594096302986145\n",
            "strain 0.23375782370567322\n",
            "strain 0.3843742311000824\n",
            "strain 0.23889754712581635\n",
            "strain 0.3125395178794861\n",
            "strain 0.31420227885246277\n",
            "strain 0.48202186822891235\n",
            "strain 0.3616041839122772\n",
            "strain 0.3893839418888092\n",
            "strain 0.2630721926689148\n",
            "strain 0.34973394870758057\n",
            "strain 0.2625902593135834\n",
            "strain 0.36454856395721436\n",
            "strain 0.47221776843070984\n",
            "strain 0.3327164053916931\n",
            "strain 0.2741638422012329\n",
            "strain 0.41668766736984253\n",
            "strain 0.3890533447265625\n",
            "strain 0.2912355065345764\n",
            "strain 0.37102314829826355\n",
            "strain 0.3144238293170929\n",
            "strain 0.3576136529445648\n",
            "strain 0.2777916193008423\n",
            "strain 0.28539425134658813\n",
            "strain 0.4023706912994385\n",
            "strain 0.3988782465457916\n",
            "strain 0.2768884301185608\n",
            "strain 0.4070816934108734\n",
            "strain 0.4386572539806366\n",
            "strain 0.39858952164649963\n",
            "strain 0.39526835083961487\n",
            "strain 0.4285981059074402\n",
            "strain 0.27053770422935486\n",
            "strain 0.3914567232131958\n",
            "strain 0.27041304111480713\n",
            "strain 0.4279329180717468\n",
            "strain 0.5104183554649353\n",
            "strain 0.29224032163619995\n",
            "strain 0.2375984489917755\n",
            "strain 0.29995793104171753\n",
            "strain 0.26603052020072937\n",
            "strain 0.31064939498901367\n",
            "strain 0.4014474153518677\n",
            "strain 0.32849088311195374\n",
            "strain 0.2844204902648926\n",
            "strain 0.2905154824256897\n",
            "strain 0.24005460739135742\n",
            "strain 0.4541000425815582\n",
            "strain 0.3794375956058502\n",
            "strain 0.2518653869628906\n",
            "classify 2.135986328125\n",
            "classify 2.02398681640625\n",
            "classify 1.9532470703125\n",
            "classify 2.06915283203125\n",
            "classify 1.9892578125\n",
            "classify 2.0943603515625\n",
            "classify 2.1429443359375\n",
            "classify 2.117431640625\n",
            "classify 2.0985107421875\n",
            "classify 2.07769775390625\n",
            "classify 1.9954833984375\n",
            "0.234375\n",
            "0.296875\n",
            "0.28125\n",
            "0.171875\n",
            "0.28125\n",
            "0.234375\n",
            "0.234375\n",
            "0.21875\n",
            "0.328125\n",
            "0.234375\n",
            "0.234375\n",
            "496\n",
            "strain 0.26452794671058655\n",
            "strain 0.2560029625892639\n",
            "strain 0.3307287096977234\n",
            "strain 0.2890014946460724\n",
            "strain 0.44703465700149536\n",
            "strain 0.38636380434036255\n",
            "strain 0.42294836044311523\n",
            "strain 0.4556421637535095\n",
            "strain 0.36549362540245056\n",
            "strain 0.2879040539264679\n",
            "strain 0.47161051630973816\n",
            "strain 0.3523440659046173\n",
            "strain 0.5278353691101074\n",
            "strain 0.2648775577545166\n",
            "strain 0.33314743638038635\n",
            "strain 0.32474926114082336\n",
            "strain 0.3391229808330536\n",
            "strain 0.2955113649368286\n",
            "strain 0.2692564129829407\n",
            "strain 0.25113749504089355\n",
            "strain 0.3605033755302429\n",
            "strain 0.24356089532375336\n",
            "strain 0.2936886250972748\n",
            "strain 0.36905425786972046\n",
            "strain 0.42935609817504883\n",
            "strain 0.3583140969276428\n",
            "strain 0.31525465846061707\n",
            "strain 0.35491400957107544\n",
            "strain 0.4237974286079407\n",
            "strain 0.26452213525772095\n",
            "strain 0.26005735993385315\n",
            "strain 0.2841684818267822\n",
            "strain 0.33831173181533813\n",
            "strain 0.3015780746936798\n",
            "strain 0.3827148675918579\n",
            "strain 0.3278462588787079\n",
            "strain 0.3783600330352783\n",
            "strain 0.4432564675807953\n",
            "strain 0.3137189745903015\n",
            "strain 0.30394285917282104\n",
            "strain 0.3009951114654541\n",
            "strain 0.305224746465683\n",
            "strain 0.2754240036010742\n",
            "strain 0.36882686614990234\n",
            "strain 0.23467090725898743\n",
            "strain 0.4480883479118347\n",
            "strain 0.4410386383533478\n",
            "strain 0.25165948271751404\n",
            "strain 0.35027721524238586\n",
            "strain 0.3300344944000244\n",
            "strain 0.30560868978500366\n",
            "classify 2.104248046875\n",
            "classify 2.02825927734375\n",
            "classify 2.09765625\n",
            "classify 2.152099609375\n",
            "classify 2.00787353515625\n",
            "classify 1.97869873046875\n",
            "classify 1.9356689453125\n",
            "classify 2.0880126953125\n",
            "classify 1.9375\n",
            "classify 1.9971923828125\n",
            "classify 1.9935302734375\n",
            "0.25\n",
            "0.171875\n",
            "0.234375\n",
            "0.171875\n",
            "0.328125\n",
            "0.15625\n",
            "0.28125\n",
            "0.234375\n",
            "0.25\n",
            "0.25\n",
            "0.28125\n",
            "497\n",
            "strain 0.4220038056373596\n",
            "strain 0.23552826046943665\n",
            "strain 0.3025631308555603\n",
            "strain 0.44671109318733215\n",
            "strain 0.36188048124313354\n",
            "strain 0.33982959389686584\n",
            "strain 0.24351078271865845\n",
            "strain 0.4723835289478302\n",
            "strain 0.4339289367198944\n",
            "strain 0.45323243737220764\n",
            "strain 0.30277687311172485\n",
            "strain 0.4606614410877228\n",
            "strain 0.3711912930011749\n",
            "strain 0.3576720654964447\n",
            "strain 0.34086671471595764\n",
            "strain 0.23201845586299896\n",
            "strain 0.41139310598373413\n",
            "strain 0.3450546860694885\n",
            "strain 0.30274519324302673\n",
            "strain 0.37956514954566956\n",
            "strain 0.31881847977638245\n",
            "strain 0.3285622298717499\n",
            "strain 0.2953038215637207\n",
            "strain 0.418464720249176\n",
            "strain 0.2656516134738922\n",
            "strain 0.2571617066860199\n",
            "strain 0.36437639594078064\n",
            "strain 0.30763232707977295\n",
            "strain 0.32387980818748474\n",
            "strain 0.24607865512371063\n",
            "strain 0.24931733310222626\n",
            "strain 0.4478377103805542\n",
            "strain 0.2502192258834839\n",
            "strain 0.277040034532547\n",
            "strain 0.2596594989299774\n",
            "strain 0.2731030285358429\n",
            "strain 0.4166690409183502\n",
            "strain 0.5201602578163147\n",
            "strain 0.3109162151813507\n",
            "strain 0.2434994876384735\n",
            "strain 0.28599056601524353\n",
            "strain 0.2995510697364807\n",
            "strain 0.29183369874954224\n",
            "strain 0.35698235034942627\n",
            "strain 0.2648029923439026\n",
            "strain 0.5379021167755127\n",
            "strain 0.38574811816215515\n",
            "strain 0.44746556878089905\n",
            "strain 0.22010980546474457\n",
            "strain 0.2730422616004944\n",
            "strain 0.3616028428077698\n",
            "classify 2.09844970703125\n",
            "classify 2.19146728515625\n",
            "classify 2.1173095703125\n",
            "classify 2.130859375\n",
            "classify 2.0091552734375\n",
            "classify 2.14410400390625\n",
            "classify 2.0047607421875\n",
            "classify 2.05242919921875\n",
            "classify 2.1505126953125\n",
            "classify 1.95294189453125\n",
            "classify 1.9991455078125\n",
            "0.28125\n",
            "0.25\n",
            "0.1875\n",
            "0.1875\n",
            "0.171875\n",
            "0.296875\n",
            "0.15625\n",
            "0.15625\n",
            "0.328125\n",
            "0.234375\n",
            "0.296875\n",
            "498\n",
            "strain 0.3058922588825226\n",
            "strain 0.26999109983444214\n",
            "strain 0.41875842213630676\n",
            "strain 0.390941321849823\n",
            "strain 0.3485378623008728\n",
            "strain 0.286877304315567\n",
            "strain 0.3490317463874817\n",
            "strain 0.3590632677078247\n",
            "strain 0.36528274416923523\n",
            "strain 0.24637849628925323\n",
            "strain 0.49462026357650757\n",
            "strain 0.27671200037002563\n",
            "strain 0.3750420808792114\n",
            "strain 0.27858254313468933\n",
            "strain 0.4133521616458893\n",
            "strain 0.2846227288246155\n",
            "strain 0.43336641788482666\n",
            "strain 0.43275150656700134\n",
            "strain 0.3657658100128174\n",
            "strain 0.35778406262397766\n",
            "strain 0.363485723733902\n",
            "strain 0.24879704415798187\n",
            "strain 0.3770925998687744\n",
            "strain 0.23043327033519745\n",
            "strain 0.38035210967063904\n",
            "strain 0.4930696487426758\n",
            "strain 0.298811137676239\n",
            "strain 0.27149516344070435\n",
            "strain 0.36562907695770264\n",
            "strain 0.2597198188304901\n",
            "strain 0.2560323476791382\n",
            "strain 0.25479206442832947\n",
            "strain 0.28475072979927063\n",
            "strain 0.32399696111679077\n",
            "strain 0.3775485157966614\n",
            "strain 0.36636051535606384\n",
            "strain 0.4741697907447815\n",
            "strain 0.40110450983047485\n",
            "strain 0.3423122763633728\n",
            "strain 0.39378342032432556\n",
            "strain 0.3234280049800873\n",
            "strain 0.2758612632751465\n",
            "strain 0.2639174163341522\n",
            "strain 0.37201744318008423\n",
            "strain 0.3957650363445282\n",
            "strain 0.2325507402420044\n",
            "strain 0.25751662254333496\n",
            "strain 0.31888657808303833\n",
            "strain 0.2685869336128235\n",
            "strain 0.2753986120223999\n",
            "strain 0.3330107033252716\n",
            "classify 2.05194091796875\n",
            "classify 1.9239501953125\n",
            "classify 2.0726318359375\n",
            "classify 2.08465576171875\n",
            "classify 2.1544189453125\n",
            "classify 1.95166015625\n",
            "classify 2.1978759765625\n",
            "classify 2.12255859375\n",
            "classify 1.95001220703125\n",
            "classify 1.9652099609375\n",
            "classify 2.041748046875\n",
            "0.203125\n",
            "0.25\n",
            "0.25\n",
            "0.28125\n",
            "0.328125\n",
            "0.3125\n",
            "0.28125\n",
            "0.234375\n",
            "0.28125\n",
            "0.25\n",
            "0.3125\n",
            "499\n",
            "strain 0.31711044907569885\n",
            "strain 0.22032520174980164\n",
            "strain 0.4156794250011444\n",
            "strain 0.29357248544692993\n",
            "strain 0.36820006370544434\n",
            "strain 0.3071635961532593\n",
            "strain 0.35425758361816406\n",
            "strain 0.24673789739608765\n",
            "strain 0.38740137219429016\n",
            "strain 0.26721474528312683\n",
            "strain 0.3503561317920685\n",
            "strain 0.4228065609931946\n",
            "strain 0.28338468074798584\n",
            "strain 0.40051528811454773\n",
            "strain 0.3261832296848297\n",
            "strain 0.3612590432167053\n",
            "strain 0.39186978340148926\n",
            "strain 0.39266663789749146\n",
            "strain 0.33797547221183777\n",
            "strain 0.2460281103849411\n",
            "strain 0.27663809061050415\n",
            "strain 0.274893194437027\n",
            "strain 0.2562049329280853\n",
            "strain 0.3235093653202057\n",
            "strain 0.3151761591434479\n",
            "strain 0.32042163610458374\n",
            "strain 0.3682286739349365\n",
            "strain 0.3704448640346527\n",
            "strain 0.35856741666793823\n",
            "strain 0.29850438237190247\n",
            "strain 0.28017210960388184\n",
            "strain 0.3034016788005829\n",
            "strain 0.3748682737350464\n",
            "strain 0.36227947473526\n",
            "strain 0.2869139313697815\n",
            "strain 0.25062599778175354\n",
            "strain 0.3189420998096466\n",
            "strain 0.4056163728237152\n",
            "strain 0.2991397976875305\n",
            "strain 0.5028435587882996\n",
            "strain 0.34480372071266174\n",
            "strain 0.3835241198539734\n",
            "strain 0.41204121708869934\n",
            "strain 0.3275034427642822\n",
            "strain 0.28802525997161865\n",
            "strain 0.3844708502292633\n",
            "strain 0.35117074847221375\n",
            "strain 0.26306232810020447\n",
            "strain 0.29927191138267517\n",
            "strain 0.23443129658699036\n",
            "strain 0.4110099673271179\n",
            "classify 2.0709228515625\n",
            "classify 2.021240234375\n",
            "classify 2.06866455078125\n",
            "classify 2.00445556640625\n",
            "classify 2.0159912109375\n",
            "classify 2.0364990234375\n",
            "classify 2.0069580078125\n",
            "classify 1.9913330078125\n",
            "classify 2.0174560546875\n",
            "classify 1.95281982421875\n",
            "classify 2.01513671875\n",
            "0.21875\n",
            "0.265625\n",
            "0.234375\n",
            "0.25\n",
            "0.21875\n",
            "0.25\n",
            "0.25\n",
            "0.21875\n",
            "0.265625\n",
            "0.296875\n",
            "0.25\n"
          ]
        }
      ],
      "source": [
        "# @title strain ctrain test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "def strain(model, dataloader, optim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.train()\n",
        "    for i, (x, _) in enumerate(dataloader):\n",
        "        x = x.to(device)#.to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            loss = model.loss(x)\n",
        "\n",
        "            # repr_loss, std_loss, cov_loss = model.loss(x)\n",
        "            # loss = model.sim_coeff * repr_loss + model.std_coeff * std_loss + model.cov_coeff * cov_loss\n",
        "\n",
        "        optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 5) # 0.5\n",
        "\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            m=0.999 # 0.99 m = next(momentum_scheduler)\n",
        "            norms=[]\n",
        "            for param_q, param_k in zip(model.student.parameters(), model.teacher.parameters()):\n",
        "                param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        print(\"strain\",loss.item())\n",
        "        # try: wandb.log({\"loss\": loss.item(), \"repr/I\": repr_loss.item(), \"std/V\": std_loss.item(), \"cov/C\": cov_loss.item()})\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=50: break\n",
        "\n",
        "\n",
        "def ctrain(model, classifier, dataloader, coptim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.eval()\n",
        "    classifier.train()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        # x = x.flatten(2).transpose(-2,-1).to(torch.bfloat16)\n",
        "        # x, y = x[...,1:].to(device).to(torch.bfloat16), y[1].to(device) # [batch, ] # (id, activity)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            with torch.no_grad():\n",
        "                sx = model(x).detach()\n",
        "            # print(sx[0][0])\n",
        "            y_ = classifier(sx)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "\n",
        "        coptim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(coptim)\n",
        "        scaler.update()\n",
        "        print(\"classify\",loss.item())\n",
        "        try: wandb.log({\"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        # x = x.flatten(2).transpose(-2,-1)#.to(torch.bfloat16)\n",
        "        # x, y = x[...,1:].to(device).to(torch.float), y[1].to(device) # [batch, ] # (id, activity)\n",
        "        with torch.no_grad():\n",
        "            sx = model(x)\n",
        "            rankme = RankMe(sx).item()\n",
        "            lidar = LiDAR(sx).item()\n",
        "            y_ = classifier(sx)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        print(correct/len(y))\n",
        "        # try: wandb.log({\"correct\": correct/len(y)})\n",
        "        try: wandb.log({\"correct\": correct/len(y), \"rankme\": rankme, \"lidar\": lidar})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "# for i in range(1):\n",
        "for i in range(500):\n",
        "    print(i)\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # np.random.shuffle(train_indices); np.random.shuffle(val_indices)\n",
        "    # train_sampler, valid_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(val_indices)\n",
        "    # # batch_size = 64 #512\n",
        "    # train_loader = DataLoader(train_data, sampler=train_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "    # test_loader = DataLoader(train_data, sampler=valid_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True)\n",
        "\n",
        "    strain(ijepa, train_loader, optim)\n",
        "    ctrain(ijepa, classifier, train_loader, coptim)\n",
        "    test(ijepa, classifier, test_loader)\n",
        "\n",
        "    # strain(violet, train_loader, voptim)\n",
        "    # ctrain(violet, classifier, train_loader, coptim)\n",
        "    # test(violet, classifier, test_loader)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Nd-sGe6Ku4S",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "outputId": "5eb52636-6b5f-4c93-8557-a23f9e25b0a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>██▆▆██▅▇▆▇▆▅▄▅▅▅▅▂▄▅▅▃▅▄▁▃▃▅▃▄▃▂▃▃▅▄▅▄▅▃</td></tr><tr><td>correct</td><td>▄▄▂▁▃▂▄▃▆▆▄▄▆▃▅▃▄▄▂▅▅▂█▇▅▅▃▅▄▇▃▂▂▃▆▄▃▆▄▂</td></tr><tr><td>loss</td><td>▁▁▂▂▃▄▃▄▄▃▃▃▂▂▂▃▇▅▄█▃▄▅▄▃▃▄▅▇▃▄▃▃▄▃▄▄▃▄▃</td></tr><tr><td>rankme</td><td>▁▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▄▅▅▅▅▆▆▆▆▆▆▆▆▆▆▇▆▆▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>2.21686</td></tr><tr><td>correct</td><td>0.17188</td></tr><tr><td>loss</td><td>0.18593</td></tr><tr><td>rankme</td><td>4.88123</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fluent-disco-60</strong> at: <a href='https://wandb.ai/bobdole/ijepa/runs/8diybe87' target=\"_blank\">https://wandb.ai/bobdole/ijepa/runs/8diybe87</a><br> View project at: <a href='https://wandb.ai/bobdole/ijepa' target=\"_blank\">https://wandb.ai/bobdole/ijepa</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250401_054523-8diybe87/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250401_060831-tvl0pi0a</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/ijepa/runs/tvl0pi0a' target=\"_blank\">fluent-oath-61</a></strong> to <a href='https://wandb.ai/bobdole/ijepa' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/ijepa' target=\"_blank\">https://wandb.ai/bobdole/ijepa</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/ijepa/runs/tvl0pi0a' target=\"_blank\">https://wandb.ai/bobdole/ijepa/runs/tvl0pi0a</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "!pip install -q wandb\n",
        "import wandb # https://docs.wandb.ai/quickstart\n",
        "wandb.login(key='487a2109e55dce4e13fc70681781de9f50f27be7')\n",
        "try: run.finish()\n",
        "except NameError: pass\n",
        "run = wandb.init(project=\"ijepa\", config={\"model\": \"res18\",})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title supervised train test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "# def strain(model, dataloader, optim, scheduler=None):\n",
        "def strain(model, classifier, dataloader, optim, coptim, scheduler=None):\n",
        "    model.train()\n",
        "    classifier.train()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x = x.to(device)#.to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "        y = y.to(device)\n",
        "\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        # print(loss)\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # for p in list(filter(lambda p: p.grad is not None, model.parameters())):\n",
        "        #     print(p.grad.data.norm(2).item())\n",
        "        # print(\"max grad norm\", max([p.grad.data.norm(2).item() for p in list(filter(lambda p: p.grad is not None, model.parameters()))]))\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 10) # 0.5\n",
        "\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        print(\"strain\",loss.item())\n",
        "        # for param in ijepa.student.cls: print(param.data)\n",
        "        # for param in ijepa.predicter.cls: print(param.data)\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=50: break\n",
        "\n",
        "\n",
        "# def test(model, dataloader):\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        # .to(torch.bfloat16)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "        loss = F.cross_entropy(y_, y)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        print(correct/len(y))\n",
        "        try: wandb.log({\"correct\": correct/len(y), \"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "for i in range(1000):\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # np.random.shuffle(train_indices); np.random.shuffle(val_indices)\n",
        "    # train_sampler, valid_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(val_indices)\n",
        "    # # batch_size = 64 #512\n",
        "    # train_loader = DataLoader(train_data, sampler=train_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "    # test_loader = DataLoader(train_data, sampler=valid_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True)\n",
        "\n",
        "    # strain(ijepa, train_loader, optim)\n",
        "    strain(ijepa, classifier, train_loader, optim, coptim)\n",
        "    test(ijepa, classifier, test_loader)\n",
        "\n",
        "    # strain(violet, train_loader, voptim)\n",
        "    # test(violet, test_loader)\n"
      ],
      "metadata": {
        "id": "4J2ahp3wmqcg",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## drawer"
      ],
      "metadata": {
        "id": "aLT74ihtMnh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# @title test multiblock params\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "dataiter = iter(train_data)\n",
        "img,y = next(dataiter)\n",
        "img = img.unsqueeze(0)\n",
        "img = F.interpolate(img, (8,8))\n",
        "b,c,h,w = img.shape\n",
        "# img = img.flatten(2).transpose(-2,-1).to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "print(img.shape)\n",
        "# batch, seq,_ = img.shape\n",
        "\n",
        "\n",
        "# # target_mask = randpatch(seq, mask_size=8, gamma=.9) # 8.9 [seq]\n",
        "# target_mask = multiblock(seq, min_s=0.2, max_s=0.3, M=4).any(0) # best.2.3M4 og.15.2M4# mask out targets to be predicted # [M, seq]\n",
        "# target_mask = multiblock(seq, min_s=0.2, max_s=0.3, M=4).any(0)\n",
        "\n",
        "target_mask = multiblock2d((8,8), M=4).any(0).unsqueeze(0)\n",
        "\n",
        "\n",
        "# context_mask = ~multiblock(seq, min_s=0.85, max_s=1., M=1)|target_mask # [1, seq], True->Mask\n",
        "context_mask = ~multiblock2d((8,8), scale=(.85,.85), aspect_ratio=(.9,1.1), M=1)|target_mask # [1, seq], True->Mask\n",
        "\n",
        "# print(target_mask, context_mask)\n",
        "\n",
        "print(target_mask.shape, context_mask.shape)\n",
        "# target_img, context_img = img*target_mask.unsqueeze(-1), img*context_mask.unsqueeze(-1)\n",
        "# target_img, context_img = img*~target_mask.unsqueeze(-1), img*~context_mask.unsqueeze(-1)\n",
        "target_img, context_img = img*~target_mask.unsqueeze(0), img*~context_mask.unsqueeze(0)\n",
        "# print(target_img.shape, context_img.shape)\n",
        "target_img, context_img = target_img.transpose(-2,-1).reshape(b,c,h,w), context_img.transpose(-2,-1).reshape(b,c,h,w)\n",
        "target_img, context_img = target_img.float(), context_img.float()\n",
        "\n",
        "# imshow(out.detach().cpu())\n",
        "imshow(target_img[0])\n",
        "imshow(context_img[0])\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "3bpiybH7M0O1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 879
        },
        "outputId": "3d5e3a1a-7e09-4033-a6ad-5e68f9844da1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 8, 8])\n",
            "torch.Size([1, 8, 8]) torch.Size([1, 8, 8])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ViT me more\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "# class LayerNorm2d(nn.LayerNorm):\n",
        "class LayerNorm2d(nn.RMSNorm):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        x = super().forward(x)\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads=None, d_head=8, cond_dim=None, dropout=0.): # .1\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_head = d_head\n",
        "        self.n_heads = d_model // d_head\n",
        "        # self.d_head = d_model // n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or d_model, 2*d_model, bias=False)\n",
        "        # self.k = nn.Sequential(nn.Dropout(dropout), nn.Linear(cond_dim, d_model, bias=False))\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        # self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        # self.lin = nn.Sequential(nn.Dropout(dropout), zero_module(nn.Linear(d_model, d_model)))\n",
        "        self.drop = nn.Dropout(dropout) # indp before q,k,v; after linout\n",
        "        self.scale = self.d_head ** -.5\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [batch, T, d_model]=[batch, h*w, c], [batch, num_tok, cond_dim], [batch,T]\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        Q = self.q(x).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        # K = self.k(x).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2)\n",
        "        K, V = self.kv(cond).unflatten(-1, (self.n_heads, 2*self.d_head)).transpose(1, 2).chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # linear attention # Softmax(Q) @ (Softmax(K).T @ V)\n",
        "        if mask != None:\n",
        "            mask = mask[:, None, :, None] # [batch,T] -> [batch,1,T,1]\n",
        "            K, V = K.masked_fill(mask, -torch.finfo(x.dtype).max), V.masked_fill(mask, -torch.finfo(x.dtype).max)\n",
        "        Q, K = Q.softmax(dim=-1)*self.scale, K.softmax(dim=-2)\n",
        "        context = K.transpose(-2,-1) @ V # [batch, n_heads, d_head, d_head]\n",
        "        out = Q @ context # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # (quadratic) attention # Softmax(Q @ K.T) @ V\n",
        "        # attn = Q @ K.transpose(-2,-1) * self.scale # [batch, n_heads, T] # [batch, n_heads, T, T/num_tok]\n",
        "        # if mask != None: attn = attn.masked_fill(mask[:, None, :, None], -torch.finfo(attn.dtype).max) # [batch,T]->[batch,1,T,1]\n",
        "        # attention = torch.softmax(attn, dim=-1)\n",
        "        # out = self.drop(attention) @ V # [batch, n_heads, T, d_head]\n",
        "\n",
        "        out = out.transpose(1, 2).flatten(2)\n",
        "        return self.drop(self.lin(out)) # [batch, T, d_model]\n",
        "\n",
        "# if self, dont pass cond_dim in init, dont pass cond in fwd\n",
        "# Softmax(Q @ K.T) @ V ~ Softmax(Q) @ Softmax(K).T @ V\n",
        "\n",
        "# https://github.com/lucidrains/x-transformers/blob/main/x_transformers/x_transformers.py#L1855\n",
        "\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_head, cond_dim=None, mult=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.cond_dim = cond_dim\n",
        "        self.self = MultiHeadAttention(d_model, d_head=d_head, dropout=0) # 16448\n",
        "        # self.self = Pooling()\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        # self.ff = nn.Sequential(\n",
        "        #     *[nn.BatchNorm2d(d_model), act, SeparableConv2d(d_model, d_model),]*3\n",
        "        #     )\n",
        "        # self.ff = ResBlock(d_model) # 74112\n",
        "        # self.ff = UIB(d_model, mult=4) # uib m4 36992, m2 18944\n",
        "        ff_dim=d_model*mult\n",
        "        self.ff = nn.Sequential(\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(dropout), nn.Linear(d_model, ff_dim), act, # ReLU GELU\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(dropout), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            nn.RMSNorm(d_model), act, nn.Dropout(dropout), nn.Linear(d_model, ff_dim),\n",
        "            nn.RMSNorm(ff_dim), act, nn.Dropout(dropout), zero_module(nn.Linear(ff_dim, d_model))\n",
        "        )\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,c,h,w], [batch, num_tok, cond_dim], [batch,T]\n",
        "        bchw = x.shape\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "        # print('attnblk fwd',x.shape)\n",
        "\n",
        "        # if self.cond_dim==None: cond=None # is self attn\n",
        "        x = x + self.drop(self.self(self.norm(x)))\n",
        "        # x = x.transpose(1,2).reshape(*bchw)\n",
        "        x = x + self.ff(x)\n",
        "        # x = self.ff(x)\n",
        "        # x = x + self.drop(self.norm2(self.ff(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "import torch\n",
        "# def apply_masks(x, masks): # [b,t,d], [M,mask_size] # https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "#     all_x = []\n",
        "#     for m in masks: # [1,T]\n",
        "#         mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [1,T,dim]\n",
        "#         all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "#     return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "def apply_masks(x, mask): # [b,t,d], [mask_size] # https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "    mask_keep = mask.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [1,T,dim]\n",
        "    # return torch.cat(torch.gather(x, dim=1, index=mask_keep), dim=0)  # [M*batch,mask_size,dim]\n",
        "    return torch.gather(x, dim=1, index=mask_keep) # [M*batch,mask_size,dim]\n",
        "\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        # self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        self.positional_emb = nn.Parameter(torch.zeros(1, 8*8, d_model)) # positional_embedding == 'learnable'\n",
        "        # self.positional_emb = nn.Parameter(torch.zeros(1, dim, 8,8))\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.zeros(1,1,d_model)) # randn zeros\n",
        "        nn.init.trunc_normal_(self.cls, std=.02)\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model)\n",
        "        self.lin = nn.Linear(d_model, out_dim)# if out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = x.shape\n",
        "        # x = x * self.pos_encoder(context_indices)\n",
        "        x = x + self.positional_emb[:,context_indices]\n",
        "        # x = apply_masks(x, [context_indices])\n",
        "\n",
        "        # pred_tokens = self.cls * self.pos_encoder(trg_indices) # [M, num_trg_toks, d_model]\n",
        "        pred_tokens = self.cls + self.positional_emb[0,trg_indices]\n",
        "        print(self.cls.shape, self.positional_emb[0,trg_indices].shape, trg_indices.shape)\n",
        "        pred_tokens = pred_tokens.repeat(batch, 1, 1) # [batch*M, num_trg_toks, d_model]\n",
        "        # print(pred_tokens.requires_grad)\n",
        "        print(\"pred fwd\", x.shape, pred_tokens.shape)\n",
        "        # x = x.repeat_interleave(trg_indices.shape[0], dim=0) # [batch, seq_len, d_model] -> [batch*M, seq_len, d_model]\n",
        "        x = torch.cat([x, pred_tokens], dim=1) # [batch*M, seq_len+num_trg_toks, d_model]\n",
        "\n",
        "        # x = x.transpose(1,2).unflatten(-1, (8,8))#.reshape(*bchw)\n",
        "        out = self.transformer_encoder(x) # float [seq_len, batch_size, d_model]\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "\n",
        "        out = self.norm(out)\n",
        "        out = out[:,seq:] # [batch*M, num_trg_toks, d_model]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Sequential(\n",
        "            # # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.MaxPool1d(2,2), #nn.MaxPool1d(3, 2, 3//2),\n",
        "            # # nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.Conv1d(d_model, d_model,3,2,3//2)\n",
        "            # nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            # nn.Conv1d(d_model, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            # nn.Conv1d(d_model, d_model,3,2,3//2),\n",
        "            # # nn.Conv1d(in_dim, d_model,2,2,0), # like patch\n",
        "            nn.Conv2d(in_dim, d_model, kernel_size=7, stride=2, padding=7//2, bias=False), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "            )\n",
        "        # self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        self.positional_emb = nn.Parameter(torch.zeros(1, 8*8, d_model)) # positional_embedding == 'learnable'\n",
        "        # self.positional_emb = nn.Parameter(torch.zeros(1, dim, 8,8)) # positional_embedding == 'learnable'\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model)\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # x = self.embed(x.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "        x = self.embed(x)\n",
        "        # bchw = x.shape\n",
        "        x = x.flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c] # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "\n",
        "        # x = self.pos_encoder(x)\n",
        "        # x = x * self.pos_encoder(torch.arange(seq, device=device)).unsqueeze(0)\n",
        "        # print(x.shape, self.positional_emb.shape)\n",
        "        x = x + self.positional_emb\n",
        "        if context_indices != None:\n",
        "            # x = apply_masks(x, [context_indices])\n",
        "            x = apply_masks(x, context_indices)\n",
        "\n",
        "\n",
        "        # x = x.transpose(1,2).reshape(*bchw)\n",
        "        x = self.transformer_encoder(x) # float [seq_len, batch_size, d_model]\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "        out = self.norm(x)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "\n",
        "# pos_emb rope < learn < learned\n",
        "# conv > pixel?\n",
        "# droppath not required\n",
        "\n",
        "# norm,act,conv < conv,norm,act\n",
        "# 2*s1 < uib < resblock\n",
        "# gatedadaln 3 < 2 = 1 < ffmult4 = 2*gatedadaln\n",
        "# MaxPool2d(2,2) < MaxPool2d(3,2,3//2)\n",
        "\n",
        "\n",
        "# dim = 64\n",
        "# dim_head = 8\n",
        "# heads = dim // dim_head\n",
        "# num_classes = 10\n",
        "# # model = SimpleViT(image_size=32, patch_size=4, num_classes=num_classes, dim=dim, depth=1, heads=heads, mlp_dim=dim*4, channels = 3, dim_head = dim_head)\n",
        "# model = SimpleViT(in_dim=3, out_dim=num_classes, dim=dim, depth=1, heads=heads, mlp_dim=dim*4, channels = 3, dim_head = dim_head).to(device)\n",
        "# print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 59850\n",
        "# optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "# # print(images.shape) # [batch, 3, 32, 32]\n",
        "# logits = model(x)\n",
        "# print(logits.shape)\n",
        "# # print(logits[0])\n",
        "# # print(logits[0].argmax(1))\n",
        "# pred_probab = nn.Softmax(dim=1)(logits)\n",
        "# y_pred = pred_probab.argmax(1)\n",
        "# # print(f\"Predicted class: {y_pred}\")\n",
        "\n",
        "\n",
        "batch, seq_len, d_model = 4,1024,16\n",
        "in_dim = 3\n",
        "model = TransformerModel(in_dim, d_model, d_head=4, nlayers=2, dropout=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x = torch.rand((batch, in_dim, 32, 32), device=device)\n",
        "# x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "# # # print(out)\n",
        "# model = TransformerPredictor(in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1).to(device)\n",
        "# out = model(out)\n",
        "# print(out.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5554e8f3-71ec-4b70-c7d7-219b59e9a1d3",
        "cellView": "form",
        "id": "C1iZQ6UNwoty"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9920\n",
            "torch.Size([4, 64, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title simplex\n",
        "!pip install opensimplex\n",
        "from opensimplex import OpenSimplex\n",
        "\n",
        "seed=0\n",
        "noise = OpenSimplex(seed)  # Replace 'seed' with your starting value\n",
        "noise_scale = 10  # Adjust for desired noise range\n",
        "\n",
        "def get_noise(x, y):\n",
        "    global seed  # Assuming seed is a global variable\n",
        "    # noise_val = noise.noise2(x / noise_scale, y / noise_scale)\n",
        "    noise_val = noise.noise2(x, y)\n",
        "    # seed += 1  # Increment seed after each call\n",
        "    return noise_val\n",
        "\n",
        "x,y=0,0\n",
        "a=[[],[],[],[],[]]\n",
        "\n",
        "fps=2\n",
        "f=[]\n",
        "for i in range(10*fps):\n",
        "    f.append(i/fps)\n",
        "    x = x+0.3\n",
        "    # y = y+1\n",
        "    # noise_value = get_noise(x, y)\n",
        "    for k,j in enumerate([0,1,2,3,4]):\n",
        "        a[k].append(get_noise(x, y+j))\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "for aa in a:\n",
        "    plt.plot(f,aa)\n",
        "plt.show()\n",
        "\n",
        "# b,y,g,"
      ],
      "metadata": {
        "id": "e3dAhWh45F4M",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title pos_embed.py\n",
        "# https://github.com/facebookresearch/mae/blob/main/util/pos_embed.py#L20\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim=16, grid_size=8, cls_token=False): #\n",
        "    grid_h = np.arange(grid_size, dtype=np.float32)\n",
        "    grid_w = np.arange(grid_size, dtype=np.float32)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token: pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed # [(1+)grid_size*grid_size, embed_dim]\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    # assert embed_dim % 2 == 0\n",
        "\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1) # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos): #\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=float)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega  # (D/2,)\n",
        "\n",
        "    pos = pos.reshape(-1)  # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
        "\n",
        "    emb_sin = np.sin(out) # (M, D/2)\n",
        "    emb_cos = np.cos(out) # (M, D/2)\n",
        "\n",
        "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def interpolate_pos_embed(model, checkpoint_model):\n",
        "    if 'pos_embed' in checkpoint_model:\n",
        "        pos_embed_checkpoint = checkpoint_model['pos_embed']\n",
        "        embedding_size = pos_embed_checkpoint.shape[-1]\n",
        "        num_patches = model.patch_embed.num_patches\n",
        "        num_extra_tokens = model.pos_embed.shape[-2] - num_patches\n",
        "        # height (== width) for the checkpoint position embedding\n",
        "        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n",
        "        # height (== width) for the new position embedding\n",
        "        new_size = int(num_patches ** 0.5)\n",
        "        # class_token and dist_token are kept unchanged\n",
        "        if orig_size != new_size:\n",
        "            print(\"Position interpolate from %dx%d to %dx%d\" % (orig_size, orig_size, new_size, new_size))\n",
        "            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n",
        "            # only the position tokens are interpolated\n",
        "            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n",
        "            pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n",
        "            pos_tokens = torch.nn.functional.interpolate(\n",
        "                pos_tokens, size=(new_size, new_size), mode='bicubic', align_corners=False)\n",
        "            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n",
        "            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n",
        "            checkpoint_model['pos_embed'] = new_pos_embed\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "vtPSM8mbnJZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ijepa src.utils.tensors\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/utils/tensors.py\n",
        "\n",
        "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
        "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n",
        "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
        "    def norm_cdf(x):\n",
        "        # Computes standard normal cumulative distribution function\n",
        "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Values are generated by using a truncated uniform distribution and\n",
        "        # then using the inverse CDF for the normal distribution.\n",
        "        # Get upper and lower cdf values\n",
        "        l = norm_cdf((a - mean) / std)\n",
        "        u = norm_cdf((b - mean) / std)\n",
        "\n",
        "        # Uniformly fill tensor with values from [l, u], then translate to\n",
        "        # [2l-1, 2u-1].\n",
        "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
        "\n",
        "        # Use inverse cdf transform for normal distribution to get truncated\n",
        "        # standard normal\n",
        "        tensor.erfinv_()\n",
        "\n",
        "        # Transform to proper mean, std\n",
        "        tensor.mul_(std * math.sqrt(2.))\n",
        "        tensor.add_(mean)\n",
        "\n",
        "        # Clamp to ensure it's in the proper range\n",
        "        tensor.clamp_(min=a, max=b)\n",
        "        return tensor\n",
        "\n",
        "\n",
        "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
        "    # type: (Tensor, float, float, float, float) -> Tensor\n",
        "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n",
        "\n",
        "def repeat_interleave_batch(x, B, repeat): # [batch*M,...]? , M?\n",
        "    N = len(x) // B\n",
        "    x = torch.cat([\n",
        "        torch.cat([x[i*B:(i+1)*B] for _ in range(repeat)], dim=0)\n",
        "        for i in range(N)\n",
        "    ], dim=0)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "dLbXQ-3XXRMq",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ijepa multiblock down\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "import math\n",
        "from multiprocessing import Value\n",
        "import torch\n",
        "_GLOBAL_SEED = 0\n",
        "\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=(224, 224),\n",
        "        patch_size=16,\n",
        "        enc_mask_scale=(0.2, 0.8),\n",
        "        pred_mask_scale=(0.2, 0.8),\n",
        "        aspect_ratio=(0.3, 3.0),\n",
        "        nenc=1,\n",
        "        npred=2,\n",
        "        min_keep=4,\n",
        "        allow_overlap=False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if not isinstance(input_size, tuple):\n",
        "            input_size = (input_size, ) * 2\n",
        "        self.patch_size = patch_size\n",
        "        self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.aspect_ratio = aspect_ratio\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "        self._itr_counter = Value('i', -1)  # collator is shared across worker processes\n",
        "\n",
        "    def step(self):\n",
        "        i = self._itr_counter\n",
        "        with i.get_lock():\n",
        "            i.value += 1\n",
        "            v = i.value\n",
        "        return v\n",
        "\n",
        "    def _sample_block_size(self, generator, scale, aspect_ratio_scale):\n",
        "        _rand = torch.rand(1, generator=generator).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.height * self.width * mask_scale)\n",
        "        # -- Sample block aspect-ratio\n",
        "        min_ar, max_ar = aspect_ratio_scale\n",
        "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
        "        while h >= self.height:\n",
        "            h -= 1\n",
        "        while w >= self.width:\n",
        "            w -= 1\n",
        "        return (h, w)\n",
        "\n",
        "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
        "        h, w = b_size\n",
        "\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # --\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            top = torch.randint(0, self.height - h, (1,))\n",
        "            left = torch.randint(0, self.width - w, (1,))\n",
        "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
        "            mask[top:top+h, left:left+w] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
        "        mask_complement[top:top+h, left:left+w] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        B = len(batch)\n",
        "\n",
        "        collated_batch = torch.utils.data.default_collate(batch)\n",
        "\n",
        "        seed = self.step()\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(seed)\n",
        "        p_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.pred_mask_scale,\n",
        "            aspect_ratio_scale=self.aspect_ratio)\n",
        "        e_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.enc_mask_scale,\n",
        "            aspect_ratio_scale=(1., 1.))\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.height * self.width\n",
        "        min_keep_enc = self.height * self.width\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                print(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "        return collated_batch, collated_masks_enc, collated_masks_pred\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "COvEnBXPYth3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aH1UOD4WW8I2",
        "outputId": "e8671cd3-eba7-4649-84ca-94c6467ce62d",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Mask generator says: \"Valid mask not found, decreasing acceptable-regions [1]\"\n",
            "WARNING:root:Mask generator says: \"Valid mask not found, decreasing acceptable-regions [2]\"\n",
            "WARNING:root:Mask generator says: \"Valid mask not found, decreasing acceptable-regions [3]\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vit fwd torch.Size([4, 3, 224, 224])\n",
            "vit fwd torch.Size([4, 196, 768])\n",
            "vit fwd torch.Size([4, 11, 768])\n",
            "predictor fwd1 torch.Size([4, 11, 384]) torch.Size([4, 196, 384])\n"
          ]
        }
      ],
      "source": [
        "# @title ijepa vision_transformer.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/models/vision_transformer.py\n",
        "import math\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# from src.utils.tensors import (trunc_normal_, repeat_interleave_batch)\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid height and width\n",
        "    return:\n",
        "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid_h = np.arange(grid_size, dtype=float)\n",
        "    grid_w = np.arange(grid_size, dtype=float)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0) # [1+h*w, dim]\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    assert embed_dim % 2 == 0\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid length\n",
        "    return:\n",
        "    pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid = np.arange(grid_size, dtype=float)\n",
        "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=float)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega   # (D/2,)\n",
        "    pos = pos.reshape(-1)   # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
        "    emb = np.concatenate([np.sin(out), np.cos(out)], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\"\"\"\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x, attn\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x, return_attention=False):\n",
        "        y, attn = self.attn(self.norm1(x))\n",
        "        if return_attention:\n",
        "            return attn\n",
        "        x = x + self.drop_path(y)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2) # ->[b,h*w,c]\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvEmbed(nn.Module):\n",
        "    \"\"\"3x3 Convolution stems for ViT following ViTC models\"\"\"\n",
        "    def __init__(self, channels, strides, img_size=224, in_chans=3, batch_norm=True):\n",
        "        super().__init__()\n",
        "        # Build the stems\n",
        "        stem = []\n",
        "        channels = [in_chans] + channels\n",
        "        for i in range(len(channels) - 2):\n",
        "            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3, stride=strides[i], padding=1, bias=(not batch_norm))]\n",
        "            if batch_norm:\n",
        "                stem += [nn.BatchNorm2d(channels[i+1])]\n",
        "            stem += [nn.ReLU(inplace=True)]\n",
        "        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n",
        "        self.stem = nn.Sequential(*stem)\n",
        "\n",
        "        # Comptute the number of patches\n",
        "        stride_prod = int(np.prod(strides))\n",
        "        self.num_patches = (img_size[0] // stride_prod)**2\n",
        "\n",
        "    def forward(self, x):\n",
        "        p = self.stem(x)\n",
        "        return p.flatten(2).transpose(1, 2)\n",
        "\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "def apply_masks(x, masks):\n",
        "    \"\"\":param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
        "    :param masks: list of tensors containing indices of patches in [N] to keep\"\"\"\n",
        "    all_x = []\n",
        "    for m in masks:\n",
        "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [batch,T,dim]\n",
        "        all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "    return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "\n",
        "class VisionTransformerPredictor(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, num_patches, embed_dim=768, predictor_embed_dim=384,\n",
        "        depth=6, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.init_std = init_std\n",
        "        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        # --\n",
        "        self.predictor_pos_embed = nn.Parameter(torch.zeros(1, num_patches, predictor_embed_dim), requires_grad=False)\n",
        "        predictor_pos_embed = get_2d_sincos_pos_embed(self.predictor_pos_embed.shape[-1], int(num_patches**.5), cls_token=False)\n",
        "        self.predictor_pos_embed.data.copy_(torch.from_numpy(predictor_pos_embed).float().unsqueeze(0)) # [1, num_patches, predictor_embed_dim]\n",
        "        # --\n",
        "        self.predictor_blocks = nn.ModuleList([Block(dim=predictor_embed_dim, num_heads=num_heads, drop=drop_rate, drop_path=dpr[i]) for i in range(depth)])\n",
        "        self.predictor_norm = nn.LayerNorm(predictor_embed_dim)\n",
        "        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n",
        "        # ------\n",
        "        # trunc_normal_(self.mask_token, std=self.init_std)\n",
        "\n",
        "    def forward(self, x, masks_x, masks): # [batch, num_context_patches, embed_dim], nenc*[batch, num_context_patches], npred*[batch, num_target_patches]\n",
        "        assert (masks is not None) and (masks_x is not None), 'Cannot run predictor without mask indices'\n",
        "        if not isinstance(masks_x, list): masks_x = [masks_x] # context mask\n",
        "        if not isinstance(masks, list): masks = [masks] # pred mask\n",
        "\n",
        "        # -- Batch Size\n",
        "        B = len(x) // len(masks_x)\n",
        "        # print(\"predictor fwd0\", x.shape) # [3, 121, 768])\n",
        "        # -- map from encoder-dim to pedictor-dim\n",
        "        x = self.predictor_embed(x) # [batch, num_patches, predictor_embed_dim]\n",
        "\n",
        "        # -- add positional embedding to x tokens\n",
        "        x_pos_embed = self.predictor_pos_embed.repeat(B, 1, 1) # [batch, num_patches, predictor_embed_dim]\n",
        "        print(\"predictor fwd1\", x.shape, x_pos_embed.shape) # [3, 121 or 11, 384], [3, 196, 384]\n",
        "        # print(\"predictor fwd masks_x\", masks_x)\n",
        "        x += apply_masks(x_pos_embed, masks_x) # get pos emb of context patches\n",
        "\n",
        "        # print(\"predictor fwd x\", x.shape) # [4, 11, 384])\n",
        "        _, N_ctxt, D = x.shape\n",
        "\n",
        "        # -- concat mask tokens to x\n",
        "        pos_embs = self.predictor_pos_embed.repeat(B, 1, 1) # [batch, num_patches, predictor_embed_dim]\n",
        "        pos_embs = apply_masks(pos_embs, masks) # [16, 104, predictor_embed_dim]\n",
        "        pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x)) # [16, 104, predictor_embed_dim]\n",
        "        # --\n",
        "        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1) # [16, 104, predictor_embed_dim]\n",
        "        # --\n",
        "        # print(\"predictor fwd pred_tokens\", pred_tokens.shape)\n",
        "\n",
        "        pred_tokens += pos_embs\n",
        "        x = x.repeat(len(masks), 1, 1)\n",
        "        x = torch.cat([x, pred_tokens], dim=1)\n",
        "\n",
        "        # -- fwd prop\n",
        "        for blk in self.predictor_blocks:\n",
        "            x = blk(x)\n",
        "        x = self.predictor_norm(x)\n",
        "\n",
        "        # -- return preds for mask tokens\n",
        "        x = x[:, N_ctxt:]\n",
        "        x = self.predictor_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, img_size=[224], patch_size=16, in_chans=3, embed_dim=768, predictor_embed_dim=384, depth=12, predictor_depth=12,\n",
        "        num_heads=12, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0, norm_layer=nn.LayerNorm, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_features = self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.init_std = init_std\n",
        "        # --\n",
        "        self.patch_embed = PatchEmbed(img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        # --\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=False)\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "        # --\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "    def forward(self, x, masks=None): # [batch,3,224,224]\n",
        "        if masks is not None:\n",
        "            if not isinstance(masks, list): masks = [masks]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        x = self.patch_embed(x)\n",
        "        B, N, D = x.shape # [batch, num_patches, embed_dim]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        # -- add positional embedding to x\n",
        "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
        "        x = x + pos_embed\n",
        "\n",
        "        if masks is not None: x = apply_masks(x, masks) # [batch, num_context_patches, embed_dim]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            x = blk(x)\n",
        "\n",
        "        if self.norm is not None: x = self.norm(x)\n",
        "        return x # [batch, num_context_patches, embed_dim]\n",
        "\n",
        "    def interpolate_pos_encoding(self, x, pos_embed):\n",
        "        npatch = x.shape[1] - 1\n",
        "        N = pos_embed.shape[1] - 1\n",
        "        if npatch == N: return pos_embed\n",
        "        class_emb, pos_embed = pos_embed[:, 0], pos_embed[:, 1:]\n",
        "        dim = x.shape[-1]\n",
        "        pos_embed = nn.functional.interpolate(pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2), scale_factor=math.sqrt(npatch / N), mode='bicubic',)\n",
        "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
        "\n",
        "# from functools import partial\n",
        "# def vit_predictor(**kwargs):\n",
        "#     model = VisionTransformerPredictor(\n",
        "#         mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "#     return model\n",
        "\n",
        "def vit(patch_size=16, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, # tiny\n",
        "        # patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, # small\n",
        "        # patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, # base\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "# encoder = vit()\n",
        "encoder = VisionTransformer(\n",
        "        patch_size=16, embed_dim=768, depth=1, num_heads=3, mlp_ratio=1,\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
        "\n",
        "# num_patches = (224/patch_size)^2\n",
        "# model = VisionTransformerPredictor(num_patches, embed_dim=768, predictor_embed_dim=384, depth=6, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02, **kwargs)\n",
        "model = VisionTransformerPredictor(num_patches=196, embed_dim=768, predictor_embed_dim=384, depth=1, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02)\n",
        "\n",
        "batch = 4\n",
        "img = torch.rand(batch, 3, 224, 224)\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/train.py\n",
        "mask_collator = MaskCollator(\n",
        "        input_size=(224, 224),\n",
        "        patch_size=16,\n",
        "        enc_mask_scale=(0.2, 0.8),\n",
        "        pred_mask_scale=(0.2, 0.8),\n",
        "        aspect_ratio=(0.3, 3.0),\n",
        "        nenc=1,\n",
        "        npred=4,\n",
        "        min_keep=4,\n",
        "        # allow_overlap=True)\n",
        "        allow_overlap=False)\n",
        "# self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
        "\n",
        "\n",
        "\n",
        "# v = mask_collator.step()\n",
        "# should pass the collater a list batch of idx?\n",
        "# collated_batch, collated_masks_enc, collated_masks_pred = mask_collator([batch,3,8])\n",
        "collated_batch, collated_masks_enc, collated_masks_pred = mask_collator([0,1,2,3])\n",
        "# print(v)\n",
        "\n",
        "\n",
        "def forward_target():\n",
        "    with torch.no_grad():\n",
        "        h = target_encoder(imgs)\n",
        "        h = F.layer_norm(h, (h.size(-1),))  # normalize over feature-dim\n",
        "        B = len(h)\n",
        "        # -- create targets (masked regions of h)\n",
        "        h = apply_masks(h, masks_pred)\n",
        "        h = repeat_interleave_batch(h, B, repeat=len(masks_enc))\n",
        "        return h\n",
        "\n",
        "def forward_context():\n",
        "    z = encoder(imgs, masks_enc)\n",
        "    z = predictor(z, masks_enc, masks_pred)\n",
        "    return z\n",
        "\n",
        "# # num_context_patches = 121 if allow_overlap=True else 11\n",
        "z = encoder(img, collated_masks_enc) # [batch, num_context_patches, embed_dim]\n",
        "# print(len(collated_masks_enc), len(collated_masks_pred)) # nenc, npred\n",
        "# print(collated_masks_enc[0].shape, collated_masks_pred[0].shape) # , [batch, num_context_patches = 121 or 11], [batch, num_target_patches]\n",
        "out = model(z, collated_masks_enc, collated_masks_pred)\n",
        "# # print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# print(224/16) # 14\n",
        "# print(len(collated_masks_enc), len(collated_masks_pred))\n",
        "print(collated_masks_enc, collated_masks_pred)\n",
        "# multiples of 14\n",
        "\n",
        "# print(collated_masks_pred[1])\n"
      ],
      "metadata": {
        "id": "Or3eQvUVg7l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(collated_masks_enc, collated_masks_pred)"
      ],
      "metadata": {
        "id": "u2mLjXAmXcwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ijepa multiblock.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "import math\n",
        "from multiprocessing import Value\n",
        "from logging import getLogger\n",
        "import torch\n",
        "_GLOBAL_SEED = 0\n",
        "logger = getLogger()\n",
        "\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=(224, 224),\n",
        "        patch_size=16,\n",
        "        enc_mask_scale=(0.2, 0.8),\n",
        "        pred_mask_scale=(0.2, 0.8),\n",
        "        aspect_ratio=(0.3, 3.0),\n",
        "        nenc=1,\n",
        "        npred=2,\n",
        "        min_keep=4,\n",
        "        allow_overlap=False\n",
        "    ):\n",
        "        super(MaskCollator, self).__init__()\n",
        "        if not isinstance(input_size, tuple):\n",
        "            input_size = (input_size, ) * 2\n",
        "        self.patch_size = patch_size\n",
        "        self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.aspect_ratio = aspect_ratio\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "        self._itr_counter = Value('i', -1)  # collator is shared across worker processes\n",
        "\n",
        "    def step(self):\n",
        "        i = self._itr_counter\n",
        "        with i.get_lock():\n",
        "            i.value += 1\n",
        "            v = i.value\n",
        "        return v\n",
        "\n",
        "    def _sample_block_size(self, generator, scale, aspect_ratio_scale):\n",
        "        _rand = torch.rand(1, generator=generator).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.height * self.width * mask_scale)\n",
        "        # -- Sample block aspect-ratio\n",
        "        min_ar, max_ar = aspect_ratio_scale\n",
        "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
        "        while h >= self.height:\n",
        "            h -= 1\n",
        "        while w >= self.width:\n",
        "            w -= 1\n",
        "\n",
        "        return (h, w)\n",
        "\n",
        "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
        "        h, w = b_size\n",
        "\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # --\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            top = torch.randint(0, self.height - h, (1,))\n",
        "            left = torch.randint(0, self.width - w, (1,))\n",
        "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
        "            mask[top:top+h, left:left+w] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "                    logger.warning(f'Mask generator says: \"Valid mask not found, decreasing acceptable-regions [{tries}]\"')\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
        "        mask_complement[top:top+h, left:left+w] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        B = len(batch)\n",
        "\n",
        "        collated_batch = torch.utils.data.default_collate(batch)\n",
        "\n",
        "        seed = self.step()\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(seed)\n",
        "        p_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.pred_mask_scale,\n",
        "            aspect_ratio_scale=self.aspect_ratio)\n",
        "        e_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.enc_mask_scale,\n",
        "            aspect_ratio_scale=(1., 1.))\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.height * self.width\n",
        "        min_keep_enc = self.height * self.width\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                logger.warning(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "\n",
        "        return collated_batch, collated_masks_enc, collated_masks_pred\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XQ7PxBMlsYCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ijepa train.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/train.py\n",
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "#\n",
        "\n",
        "import os\n",
        "\n",
        "# -- FOR DISTRIBUTED TRAINING ENSURE ONLY 1 DEVICE VISIBLE PER PROCESS\n",
        "try:\n",
        "    # -- WARNING: IF DOING DISTRIBUTED TRAINING ON A NON-SLURM CLUSTER, MAKE\n",
        "    # --          SURE TO UPDATE THIS TO GET LOCAL-RANK ON NODE, OR ENSURE\n",
        "    # --          THAT YOUR JOBS ARE LAUNCHED WITH ONLY 1 DEVICE VISIBLE\n",
        "    # --          TO EACH PROCESS\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = os.environ['SLURM_LOCALID']\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "import copy\n",
        "import logging\n",
        "import sys\n",
        "import yaml\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.multiprocessing as mp\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "\n",
        "from src.masks.multiblock import MaskCollator as MBMaskCollator\n",
        "from src.masks.utils import apply_masks\n",
        "from src.utils.distributed import (\n",
        "    init_distributed,\n",
        "    AllReduce\n",
        ")\n",
        "from src.utils.logging import (\n",
        "    CSVLogger,\n",
        "    gpu_timer,\n",
        "    grad_logger,\n",
        "    AverageMeter)\n",
        "from src.utils.tensors import repeat_interleave_batch\n",
        "from src.datasets.imagenet1k import make_imagenet1k\n",
        "\n",
        "from src.helper import (\n",
        "    load_checkpoint,\n",
        "    init_model,\n",
        "    init_opt)\n",
        "from src.transforms import make_transforms\n",
        "\n",
        "# --\n",
        "log_timings = True\n",
        "log_freq = 10\n",
        "checkpoint_freq = 50\n",
        "# --\n",
        "\n",
        "_GLOBAL_SEED = 0\n",
        "np.random.seed(_GLOBAL_SEED)\n",
        "torch.manual_seed(_GLOBAL_SEED)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logger = logging.getLogger()\n",
        "\n",
        "\n",
        "def main(args, resume_preempt=False):\n",
        "\n",
        "    # ----------------------------------------------------------------------- #\n",
        "    #  PASSED IN PARAMS FROM CONFIG FILE\n",
        "    # ----------------------------------------------------------------------- #\n",
        "\n",
        "    # -- META\n",
        "    use_bfloat16 = args['meta']['use_bfloat16']\n",
        "    model_name = args['meta']['model_name']\n",
        "    load_model = args['meta']['load_checkpoint'] or resume_preempt\n",
        "    r_file = args['meta']['read_checkpoint']\n",
        "    copy_data = args['meta']['copy_data']\n",
        "    pred_depth = args['meta']['pred_depth']\n",
        "    pred_emb_dim = args['meta']['pred_emb_dim']\n",
        "    if not torch.cuda.is_available():\n",
        "        device = torch.device('cpu')\n",
        "    else:\n",
        "        device = torch.device('cuda:0')\n",
        "        torch.cuda.set_device(device)\n",
        "\n",
        "    # -- DATA\n",
        "    use_gaussian_blur = args['data']['use_gaussian_blur']\n",
        "    use_horizontal_flip = args['data']['use_horizontal_flip']\n",
        "    use_color_distortion = args['data']['use_color_distortion']\n",
        "    color_jitter = args['data']['color_jitter_strength']\n",
        "    # --\n",
        "    batch_size = args['data']['batch_size']\n",
        "    pin_mem = args['data']['pin_mem']\n",
        "    num_workers = args['data']['num_workers']\n",
        "    root_path = args['data']['root_path']\n",
        "    image_folder = args['data']['image_folder']\n",
        "    crop_size = args['data']['crop_size']\n",
        "    crop_scale = args['data']['crop_scale']\n",
        "    # --\n",
        "\n",
        "    # -- MASK\n",
        "    allow_overlap = args['mask']['allow_overlap']  # whether to allow overlap b/w context and target blocks\n",
        "    patch_size = args['mask']['patch_size']  # patch-size for model training\n",
        "    num_enc_masks = args['mask']['num_enc_masks']  # number of context blocks\n",
        "    min_keep = args['mask']['min_keep']  # min number of patches in context block\n",
        "    enc_mask_scale = args['mask']['enc_mask_scale']  # scale of context blocks\n",
        "    num_pred_masks = args['mask']['num_pred_masks']  # number of target blocks\n",
        "    pred_mask_scale = args['mask']['pred_mask_scale']  # scale of target blocks\n",
        "    aspect_ratio = args['mask']['aspect_ratio']  # aspect ratio of target blocks\n",
        "    # --\n",
        "\n",
        "    # -- OPTIMIZATION\n",
        "    ema = args['optimization']['ema']\n",
        "    ipe_scale = args['optimization']['ipe_scale']  # scheduler scale factor (def: 1.0)\n",
        "    wd = float(args['optimization']['weight_decay'])\n",
        "    final_wd = float(args['optimization']['final_weight_decay'])\n",
        "    num_epochs = args['optimization']['epochs']\n",
        "    warmup = args['optimization']['warmup']\n",
        "    start_lr = args['optimization']['start_lr']\n",
        "    lr = args['optimization']['lr']\n",
        "    final_lr = args['optimization']['final_lr']\n",
        "\n",
        "    # -- LOGGING\n",
        "    folder = args['logging']['folder']\n",
        "    tag = args['logging']['write_tag']\n",
        "\n",
        "    dump = os.path.join(folder, 'params-ijepa.yaml')\n",
        "    with open(dump, 'w') as f:\n",
        "        yaml.dump(args, f)\n",
        "    # ----------------------------------------------------------------------- #\n",
        "\n",
        "    try:\n",
        "        mp.set_start_method('spawn')\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # -- init torch distributed backend\n",
        "    world_size, rank = init_distributed()\n",
        "    logger.info(f'Initialized (rank/world-size) {rank}/{world_size}')\n",
        "    if rank > 0:\n",
        "        logger.setLevel(logging.ERROR)\n",
        "\n",
        "    # -- log/checkpointing paths\n",
        "    log_file = os.path.join(folder, f'{tag}_r{rank}.csv')\n",
        "    save_path = os.path.join(folder, f'{tag}' + '-ep{epoch}.pth.tar')\n",
        "    latest_path = os.path.join(folder, f'{tag}-latest.pth.tar')\n",
        "    load_path = None\n",
        "    if load_model:\n",
        "        load_path = os.path.join(folder, r_file) if r_file is not None else latest_path\n",
        "\n",
        "    # -- make csv_logger\n",
        "    csv_logger = CSVLogger(log_file,\n",
        "                           ('%d', 'epoch'),\n",
        "                           ('%d', 'itr'),\n",
        "                           ('%.5f', 'loss'),\n",
        "                           ('%.5f', 'mask-A'),\n",
        "                           ('%.5f', 'mask-B'),\n",
        "                           ('%d', 'time (ms)'))\n",
        "\n",
        "    # -- init model\n",
        "    encoder, predictor = init_model(\n",
        "        device=device,\n",
        "        patch_size=patch_size,\n",
        "        crop_size=crop_size,\n",
        "        pred_depth=pred_depth,\n",
        "        pred_emb_dim=pred_emb_dim,\n",
        "        model_name=model_name)\n",
        "    target_encoder = copy.deepcopy(encoder)\n",
        "\n",
        "    # -- make data transforms\n",
        "    mask_collator = MBMaskCollator(\n",
        "        input_size=crop_size,\n",
        "        patch_size=patch_size,\n",
        "        pred_mask_scale=pred_mask_scale,\n",
        "        enc_mask_scale=enc_mask_scale,\n",
        "        aspect_ratio=aspect_ratio,\n",
        "        nenc=num_enc_masks,\n",
        "        npred=num_pred_masks,\n",
        "        allow_overlap=allow_overlap,\n",
        "        min_keep=min_keep)\n",
        "\n",
        "    transform = make_transforms(\n",
        "        crop_size=crop_size,\n",
        "        crop_scale=crop_scale,\n",
        "        gaussian_blur=use_gaussian_blur,\n",
        "        horizontal_flip=use_horizontal_flip,\n",
        "        color_distortion=use_color_distortion,\n",
        "        color_jitter=color_jitter)\n",
        "\n",
        "    # -- init data-loaders/samplers\n",
        "    _, unsupervised_loader, unsupervised_sampler = make_imagenet1k(\n",
        "            transform=transform,\n",
        "            batch_size=batch_size,\n",
        "            collator=mask_collator,\n",
        "            pin_mem=pin_mem,\n",
        "            training=True,\n",
        "            num_workers=num_workers,\n",
        "            world_size=world_size,\n",
        "            rank=rank,\n",
        "            root_path=root_path,\n",
        "            image_folder=image_folder,\n",
        "            copy_data=copy_data,\n",
        "            drop_last=True)\n",
        "    ipe = len(unsupervised_loader)\n",
        "\n",
        "    # -- init optimizer and scheduler\n",
        "    optimizer, scaler, scheduler, wd_scheduler = init_opt(\n",
        "        encoder=encoder,\n",
        "        predictor=predictor,\n",
        "        wd=wd,\n",
        "        final_wd=final_wd,\n",
        "        start_lr=start_lr,\n",
        "        ref_lr=lr,\n",
        "        final_lr=final_lr,\n",
        "        iterations_per_epoch=ipe,\n",
        "        warmup=warmup,\n",
        "        num_epochs=num_epochs,\n",
        "        ipe_scale=ipe_scale,\n",
        "        use_bfloat16=use_bfloat16)\n",
        "    encoder = DistributedDataParallel(encoder, static_graph=True)\n",
        "    predictor = DistributedDataParallel(predictor, static_graph=True)\n",
        "    target_encoder = DistributedDataParallel(target_encoder)\n",
        "    for p in target_encoder.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    # -- momentum schedule\n",
        "    momentum_scheduler = (ema[0] + i*(ema[1]-ema[0])/(ipe*num_epochs*ipe_scale)\n",
        "                          for i in range(int(ipe*num_epochs*ipe_scale)+1))\n",
        "\n",
        "    start_epoch = 0\n",
        "    # -- load training checkpoint\n",
        "    if load_model:\n",
        "        encoder, predictor, target_encoder, optimizer, scaler, start_epoch = load_checkpoint(\n",
        "            device=device,\n",
        "            r_path=load_path,\n",
        "            encoder=encoder,\n",
        "            predictor=predictor,\n",
        "            target_encoder=target_encoder,\n",
        "            opt=optimizer,\n",
        "            scaler=scaler)\n",
        "        for _ in range(start_epoch*ipe):\n",
        "            scheduler.step()\n",
        "            wd_scheduler.step()\n",
        "            next(momentum_scheduler)\n",
        "            mask_collator.step()\n",
        "\n",
        "    def save_checkpoint(epoch):\n",
        "        save_dict = {\n",
        "            'encoder': encoder.state_dict(),\n",
        "            'predictor': predictor.state_dict(),\n",
        "            'target_encoder': target_encoder.state_dict(),\n",
        "            'opt': optimizer.state_dict(),\n",
        "            'scaler': None if scaler is None else scaler.state_dict(),\n",
        "            'epoch': epoch,\n",
        "            'loss': loss_meter.avg,\n",
        "            'batch_size': batch_size,\n",
        "            'world_size': world_size,\n",
        "            'lr': lr\n",
        "        }\n",
        "        if rank == 0:\n",
        "            torch.save(save_dict, latest_path)\n",
        "            if (epoch + 1) % checkpoint_freq == 0:\n",
        "                torch.save(save_dict, save_path.format(epoch=f'{epoch + 1}'))\n",
        "\n",
        "    # -- TRAINING LOOP\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        logger.info('Epoch %d' % (epoch + 1))\n",
        "\n",
        "        # -- update distributed-data-loader epoch\n",
        "        unsupervised_sampler.set_epoch(epoch)\n",
        "\n",
        "        loss_meter = AverageMeter()\n",
        "        maskA_meter = AverageMeter()\n",
        "        maskB_meter = AverageMeter()\n",
        "        time_meter = AverageMeter()\n",
        "\n",
        "        for itr, (udata, masks_enc, masks_pred) in enumerate(unsupervised_loader):\n",
        "\n",
        "            def load_imgs():\n",
        "                # -- unsupervised imgs\n",
        "                imgs = udata[0].to(device, non_blocking=True)\n",
        "                masks_1 = [u.to(device, non_blocking=True) for u in masks_enc]\n",
        "                masks_2 = [u.to(device, non_blocking=True) for u in masks_pred]\n",
        "                return (imgs, masks_1, masks_2)\n",
        "            imgs, masks_enc, masks_pred = load_imgs()\n",
        "            maskA_meter.update(len(masks_enc[0][0]))\n",
        "            maskB_meter.update(len(masks_pred[0][0]))\n",
        "\n",
        "            def train_step():\n",
        "                _new_lr = scheduler.step()\n",
        "                _new_wd = wd_scheduler.step()\n",
        "                # --\n",
        "\n",
        "                def forward_target():\n",
        "                    with torch.no_grad():\n",
        "                        h = target_encoder(imgs)\n",
        "                        h = F.layer_norm(h, (h.size(-1),))  # normalize over feature-dim\n",
        "                        B = len(h)\n",
        "                        # -- create targets (masked regions of h)\n",
        "                        h = apply_masks(h, masks_pred)\n",
        "                        h = repeat_interleave_batch(h, B, repeat=len(masks_enc))\n",
        "                        return h\n",
        "\n",
        "                def forward_context():\n",
        "                    z = encoder(imgs, masks_enc)\n",
        "                    z = predictor(z, masks_enc, masks_pred)\n",
        "                    return z\n",
        "\n",
        "                def loss_fn(z, h):\n",
        "                    loss = F.smooth_l1_loss(z, h)\n",
        "                    loss = AllReduce.apply(loss)\n",
        "                    return loss\n",
        "\n",
        "                # Step 1. Forward\n",
        "                with torch.cuda.amp.autocast(dtype=torch.bfloat16, enabled=use_bfloat16):\n",
        "                    h = forward_target()\n",
        "                    z = forward_context()\n",
        "                    loss = loss_fn(z, h)\n",
        "\n",
        "                #  Step 2. Backward & step\n",
        "                if use_bfloat16:\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                else:\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                grad_stats = grad_logger(encoder.named_parameters())\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Step 3. momentum update of target encoder\n",
        "                with torch.no_grad():\n",
        "                    m = next(momentum_scheduler)\n",
        "                    for param_q, param_k in zip(encoder.parameters(), target_encoder.parameters()):\n",
        "                        param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "\n",
        "                return (float(loss), _new_lr, _new_wd, grad_stats)\n",
        "            (loss, _new_lr, _new_wd, grad_stats), etime = gpu_timer(train_step)\n",
        "            loss_meter.update(loss)\n",
        "            time_meter.update(etime)\n",
        "\n",
        "            # -- Logging\n",
        "            def log_stats():\n",
        "                csv_logger.log(epoch + 1, itr, loss, maskA_meter.val, maskB_meter.val, etime)\n",
        "                if (itr % log_freq == 0) or np.isnan(loss) or np.isinf(loss):\n",
        "                    logger.info('[%d, %5d] loss: %.3f '\n",
        "                                'masks: %.1f %.1f '\n",
        "                                '[wd: %.2e] [lr: %.2e] '\n",
        "                                '[mem: %.2e] '\n",
        "                                '(%.1f ms)'\n",
        "                                % (epoch + 1, itr,\n",
        "                                   loss_meter.avg,\n",
        "                                   maskA_meter.avg,\n",
        "                                   maskB_meter.avg,\n",
        "                                   _new_wd,\n",
        "                                   _new_lr,\n",
        "                                   torch.cuda.max_memory_allocated() / 1024.**2,\n",
        "                                   time_meter.avg))\n",
        "\n",
        "                    if grad_stats is not None:\n",
        "                        logger.info('[%d, %5d] grad_stats: [%.2e %.2e] (%.2e, %.2e)'\n",
        "                                    % (epoch + 1, itr,\n",
        "                                       grad_stats.first_layer,\n",
        "                                       grad_stats.last_layer,\n",
        "                                       grad_stats.min,\n",
        "                                       grad_stats.max))\n",
        "\n",
        "            log_stats()\n",
        "\n",
        "            assert not np.isnan(loss), 'loss is nan'\n",
        "\n",
        "        # -- Save Checkpoint after every epoch\n",
        "        logger.info('avg. loss %.3f' % loss_meter.avg)\n",
        "        save_checkpoint(epoch+1)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "iVUPGP93dpAN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}