{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/I_JEPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxACli7GdyGq",
        "outputId": "9dbfa734-b34a-4e6e-8085-291ee6e9283b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:13<00:00, 12.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title data\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "transform = transforms.Compose([transforms.ToTensor(),])\n",
        "# transform = transforms.Compose([transforms.RandomResizedCrop((32,32), scale=(.3,1.)), transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "# CIFAR10: 60000 32x32 color images in 10 classes, with 6000 images per class\n",
        "train_data = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
        "test_data = torchvision.datasets.CIFAR10(root='data', train=False, download=True, transform=transform)\n",
        "batch_size = 64 # 4\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# dataiter = iter(train_loader) # get some random training images\n",
        "# images, labels = next(dataiter)\n",
        "# print(images.shape) # [batch, 3, 32, 32]\n",
        "# imshow(torchvision.utils.make_grid(images))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3EO3PgMPH1x"
      },
      "source": [
        "## hiera"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "j3-vvMS1-gVn"
      },
      "outputs": [],
      "source": [
        "# @title ResBlock\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters(): p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, kernel=3):\n",
        "        super().__init__()\n",
        "        out_ch = out_ch or in_ch\n",
        "        act = nn.SiLU() #\n",
        "        self.res_conv = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
        "        # self.block = nn.Sequential( # best?\n",
        "        #     nn.Conv2d(in_ch, out_ch, 3, padding=1), nn.BatchNorm2d(out_ch), act,\n",
        "        #     zero_module(nn.Conv2d(out_ch, out_ch, 3, padding=1)), nn.BatchNorm2d(out_ch), act,\n",
        "        #     )\n",
        "        self.block = nn.Sequential(\n",
        "            nn.BatchNorm2d(in_ch), act, nn.Conv2d(in_ch, out_ch, kernel, padding=kernel//2),\n",
        "            nn.BatchNorm2d(out_ch), act, zero_module(nn.Conv2d(out_ch, out_ch, kernel, padding=kernel//2)),\n",
        "            )\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w]\n",
        "        return self.block(x) + self.res_conv(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0lclc9myo2c",
        "outputId": "35ea2b51-c2dc-4d96-f2ae-c27bd760dcd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([12, 2, 16, 32, 32])\n"
          ]
        }
      ],
      "source": [
        "# @title UpDownBlock_me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class PixelShuffleConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, kernel=1, r=1):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        r = max(r, int(1/r))\n",
        "        out_ch = out_ch or in_ch\n",
        "        if self.r>1: self.net = nn.Sequential(ResBlock(in_ch, out_ch*r**2, kernel), nn.PixelShuffle(r))\n",
        "        # if self.r>1: self.net = nn.Sequential(Attention(in_ch, out_ch*r**2), nn.PixelShuffle(r))\n",
        "\n",
        "        elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), ResBlock(in_ch*r**2, out_ch, kernel))\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), Attention(in_ch*r**2, out_ch))\n",
        "        elif in_ch != out_ch: self.net = ResBlock(in_ch*r**2, out_ch, kernel)\n",
        "        else: self.net = lambda x: torch.zeros_like(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def AdaptiveAvgPool_nd(n, *args, **kwargs): return [nn.Identity, nn.AdaptiveAvgPool1d, nn.AdaptiveAvgPool2d, nn.AdaptiveAvgPool3d][n](*args, **kwargs)\n",
        "def AdaptiveMaxPool_nd(n, *args, **kwargs): return [nn.Identity, nn.AdaptiveMaxPool1d, nn.AdaptiveMaxPool2d, nn.AdaptiveMaxPool3d][n](*args, **kwargs)\n",
        "\n",
        "def adaptive_avg_pool_nd(n, x, output_size): return [nn.Identity, F.adaptive_avg_pool1d, F.adaptive_avg_pool2d, F.adaptive_avg_pool3d][n](x, output_size)\n",
        "def adaptive_max_pool_nd(n, x, output_size): return [nn.Identity, F.adaptive_max_pool1d, F.adaptive_max_pool2d, F.adaptive_max_pool3d][n](x, output_size)\n",
        "\n",
        "class AdaptivePool_at(nn.AdaptiveAvgPool1d): # AdaptiveAvgPool1d AdaptiveMaxPool1d\n",
        "    def __init__(self, dim=1, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.dim=dim\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(self.dim,-1)\n",
        "        shape = x.shape\n",
        "        return super().forward(x.flatten(0,-2)).unflatten(0, shape[:-1]).transpose(self.dim,-1)\n",
        "\n",
        "\n",
        "def adaptive_pool_at(x, dim, output_size, pool='avg'): # [b,c,h,w]\n",
        "    x = x.transpose(dim,-1)\n",
        "    shape = x.shape\n",
        "    parent={'avg':F.adaptive_avg_pool1d, 'max':F.adaptive_max_pool1d}[pool]\n",
        "    return parent(x.flatten(0,-2), output_size).unflatten(0, shape[:-1]).transpose(dim,-1)\n",
        "\n",
        "\n",
        "class ZeroExtend():\n",
        "    def __init__(self, dim=1, output_size=16):\n",
        "        self.dim, self.out = dim, output_size\n",
        "    def __call__(self, x): # [b,c,h,w]\n",
        "        return torch.cat((x, torch.zeros(*x.shape[:self.dim], self.out - x.shape[self.dim], *x.shape[self.dim+1:])), dim=self.dim)\n",
        "\n",
        "def make_pool_at(pool='avg', dim=1, output_size=5):\n",
        "    parent={'avg':nn.AdaptiveAvgPool1d, 'max':nn.AdaptiveMaxPool1d}[pool]\n",
        "    class AdaptivePool_at(parent): # AdaptiveAvgPool1d AdaptiveMaxPool1d\n",
        "        def __init__(self, dim=1, *args, **kwargs):\n",
        "            super().__init__(*args, **kwargs)\n",
        "            self.dim=dim\n",
        "        def forward(self, x):\n",
        "            x = x.transpose(self.dim,-1)\n",
        "            shape = x.shape\n",
        "            return super().forward(x.flatten(0,-2)).unflatten(0, shape[:-1]).transpose(self.dim,-1)\n",
        "    return AdaptivePool_at(dim, output_size=output_size)\n",
        "\n",
        "class Shortcut():\n",
        "    def __init__(self, dim=1, c=3, sp=(3,3), nd=2):\n",
        "        self.dim = dim\n",
        "        # self.ch_pool = make_pool_at(pool='avg', dim=dim, output_size=c)\n",
        "        self.ch_pool = make_pool_at(pool='max', dim=dim, output_size=c)\n",
        "        # self.ch_pool = ZeroExtend(dim, output_size=c) # only for out_dim>=in_dim\n",
        "        # self.sp_pool = AdaptiveAvgPool_nd(nd, sp)\n",
        "        self.sp_pool = AdaptiveMaxPool_nd(nd, sp)\n",
        "\n",
        "    def __call__(self, x): # [b,c,h,w]\n",
        "        x = self.sp_pool(x) # spatial first preserves spatial more?\n",
        "        x = self.ch_pool(x)\n",
        "        return x\n",
        "\n",
        "class UpDownBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, kernel=7, r=1):\n",
        "        super().__init__()\n",
        "        act = nn.SiLU()\n",
        "        self.r = r\n",
        "        self.block = PixelShuffleConv(in_ch, out_ch, kernel=kernel, r=r)\n",
        "        # self.block = nn.Sequential(\n",
        "        #     nn.BatchNorm2d(in_ch), act, PixelShuffleConv(in_ch, out_ch, kernel=kernel, r=r)\n",
        "        # )\n",
        "        # if self.r>1: self.res_conv = nn.Sequential(nn.ConvTranspose2d(in_ch, out_ch, kernel, 2, kernel//2, output_padding=1))\n",
        "        # if self.r>1: self.res_conv = nn.Sequential(nn.Upsample(scale_factor=2, mode='nearest'), nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity())\n",
        "        # if self.r>1: self.res_conv = nn.Sequential(nn.Upsample(scale_factor=2, mode='bilinear'), nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity())\n",
        "        # if self.r>1: self.res_conv = nn.Sequential(nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True), nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity())\n",
        "\n",
        "\n",
        "        # elif self.r<1: self.res_conv = nn.Sequential(nn.Conv2d(in_ch, out_ch, kernel, 2, kernel//2))\n",
        "        # elif self.r<1: self.res_conv = nn.Sequential(nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity(), nn.MaxPool2d(2,2))\n",
        "        # elif self.r<1: self.res_conv = nn.Sequential(nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity(), nn.AvgPool2d(2,2))\n",
        "        # elif self.r<1: self.res_conv = AttentionBlock(in_ch, out_ch, n_heads=4, q_stride=(2,2))\n",
        "\n",
        "        # else: self.res_conv = nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity()\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w]\n",
        "        b, num_tok, c, *win = x.shape\n",
        "        x = x.flatten(0,1)\n",
        "        out = self.block(x)\n",
        "        # # shortcut = F.interpolate(x.unsqueeze(1), size=out.shape[1:], mode='nearest-exact').squeeze(1) # pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html\n",
        "        # shortcut = F.adaptive_avg_pool3d(x, out.shape[1:]) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "        # # shortcut = F.adaptive_max_pool3d(x, out.shape[1:]) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "        # # shortcut = F.adaptive_avg_pool3d(x, out.shape[1:]) if out.shape[1]>=x.shape[1] else F.adaptive_max_pool3d(x, out.shape[1:])\n",
        "        # shortcut(x)\n",
        "        shortcut = Shortcut(dim=1, c=out.shape[1], sp=out.shape[-2:], nd=2)(x)\n",
        "        out = out + shortcut\n",
        "        out = out.unflatten(0, (b, num_tok))\n",
        "        return out\n",
        "\n",
        "        # return out + shortcut + self.res_conv(x)\n",
        "        # return out + self.res_conv(x)\n",
        "        # return self.res_conv(x)\n",
        "\n",
        "# if out>in, inter=max=ave=near.\n",
        "# if out<in, inter=ave. max=max\n",
        "\n",
        "# stride2\n",
        "# interconv/convpool\n",
        "# pixelconv\n",
        "# pixeluib\n",
        "# pixelres\n",
        "# shortcut\n",
        "\n",
        "# in_ch, out_ch = 16,3\n",
        "in_ch, out_ch = 3,16\n",
        "model = UpDownBlock(in_ch, out_ch, r=1/2).to(device)\n",
        "# model = UpDownBlock(in_ch, out_ch, r=2).to(device)\n",
        "\n",
        "x = torch.rand(12, in_ch, 64,64, device=device)\n",
        "x = torch.rand(12, 2, in_ch, 64,64, device=device)\n",
        "out = model(x)\n",
        "\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAyKHKivc0j7",
        "outputId": "e66e47ef-f7e7-44c1-f15f-168e0adf1807"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MUattn 256\n",
            "torch.Size([2, 3, 16, 16, 16])\n"
          ]
        }
      ],
      "source": [
        "# @title maxpool path bchw\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def conv_nd(n, *args, **kwargs): return [nn.Identity, nn.Conv1d, nn.Conv2d, nn.Conv3d][n](*args, **kwargs)\n",
        "def maxpool_nd(n, *args, **kwargs): return [nn.Identity, nn.MaxPool1d, nn.MaxPool2d, nn.MaxPool3d][n](*args, **kwargs)\n",
        "def avgpool_nd(n, *args, **kwargs): return [nn.Identity, nn.AvgPool1d, nn.AvgPool2d, nn.AvgPool3d][n](*args, **kwargs)\n",
        "\n",
        "import math\n",
        "class MaskUnitAttention(nn.Module):\n",
        "    # def __init__(self, d_model=16, n_heads=4, q_stride=None, nd=2):\n",
        "    def __init__(self, in_dim, d_model=16, n_heads=4, q_stride=None, nd=2):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads, self.d_head = n_heads, d_model // n_heads\n",
        "        self.scale = self.d_head**-.5\n",
        "        # self.qkv = conv_nd(nd, d_model, 3*d_model, 1, bias=False)\n",
        "        self.qkv = conv_nd(nd, in_dim, 3*d_model, 1, bias=False)\n",
        "        # self.out = conv_nd(nd, d_model, d_model, 1)\n",
        "        self.out = nn.Linear(d_model, d_model, 1)\n",
        "        self.q_stride = q_stride # If greater than 1, pool q with this stride. The stride should be flattened (e.g., 2x2 = 4).\n",
        "        if q_stride:\n",
        "            self.q_stride = (q_stride,)*nd if type(q_stride)==int else q_stride\n",
        "            self.q_pool = maxpool_nd(nd, self.q_stride, stride=self.q_stride)\n",
        "\n",
        "    def forward(self, x): # [b,num_tok,c,win1,win2]\n",
        "        b, num_tok, c, *win = x.shape\n",
        "        x = x.flatten(0,1) # [b*num_tok,c,win1,win2]\n",
        "        # print(x.shape)\n",
        "        # q, k, v = self.qkv(x).reshape(b, 3, self.n_heads, self.d_head, num_tok, win*win).permute(1,0,2,4,5,3) # [b,3*d_model,num_tok*win,win] -> 3* [b, n_heads, num_tok, win*win, d_head]\n",
        "        # self.qkv(x).flatten(1,-2).unflatten(-1, (self.heads,-1)).chunk(3, dim=-1) # [b,sp,n_heads,d_head]\n",
        "        q,k,v = self.qkv(x).chunk(3, dim=1) # [b*num_tok,d,win,win]\n",
        "        if self.q_stride:\n",
        "            q = self.q_pool(q)\n",
        "            win=[w//s for w,s in zip(win, self.q_stride)] # win = win/q_stride\n",
        "        if math.prod(win) >= 200:\n",
        "            print('MUattn', math.prod(win))\n",
        "            q, k, v = map(lambda t: t.reshape(b*num_tok, self.n_heads, self.d_head, -1).transpose(-2,-1), (q,k,v)) # [b*num_tok, n_heads, win*win, d_head]\n",
        "        else:\n",
        "            # q, k, v = map(lambda t: t.reshape(b, num_tok, self.n_heads, self.d_head, -1).permute(0,2,4,1,3), (q,k,v)) # downsampling attention # [b, n_heads, win*win, num_tok, d_head]\n",
        "            q, k, v = map(lambda t: t.reshape(b, num_tok, self.n_heads, self.d_head, -1).permute(0,2,1,4,3).flatten(2,3), (q,k,v)) # [b, n_heads, num_tok*win*win, d_head]\n",
        "\n",
        "        # x = F.scaled_dot_product_attention(q,k,v, attn_mask=None) # https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html\n",
        "        q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2) # [b, n_heads, t(/pool), d_head]\n",
        "        context = k.transpose(-2,-1) @ v # [b, n_heads, d_head, d_head]\n",
        "        x = q @ context # [b, n_heads, t(/pool), d_head]\n",
        "\n",
        "        # print('attn fwd 2',x.shape)\n",
        "        # x = x.transpose(1, 3).reshape(B, -1, self.d_model)\n",
        "        x = x.transpose(1,2).reshape(b, -1, self.d_model) # [b,t,d]\n",
        "        # x = x.transpose(-2,-1).reshape(x.shape[0], self.d_model, ) # [b,t,d]\n",
        "        # [b, n_heads, num_tok, win*win, d_head] -> [b, n_heads, d_head, num_tok, win*win] -> [b,c,num_tok*win,win]\n",
        "        # x = x.permute(0,1,4,2,3).reshape(b, self.d_model, num_tok*win,win) # [b, n_heads, num_tok, win*win, d_head]\n",
        "        # x = x.transpose(-2,-1).reshape(b, self.d_model, num_tok, *win) # [b*num_tok, n_heads, win*win, d_head]\n",
        "        x = self.out(x)\n",
        "        L=len(win)\n",
        "        x = x.reshape(b, num_tok, *win, self.d_model).permute(0,1,L+2,*range(2,L+2)) # [b, num_tok, out_dim, *win]\n",
        "        # x = x.unflatten(0, (b, num_tok))\n",
        "        return x # [b,num_tok,c,win1,win2]\n",
        "\n",
        "d_model=16\n",
        "model = MaskUnitAttention(d_model, n_heads=4, q_stride=2)\n",
        "# MaskUnitAttention(d_model=16, n_heads=4, q_stride=None, nd=2)\n",
        "# x=torch.randn(2,4,4,3)\n",
        "# x=torch.randn(2,3,d_model,8,8)\n",
        "x=torch.randn(2,3,d_model,32,32)\n",
        "# [b*num_tok,c,win,win]\n",
        "\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lec1Nq7nkbgt",
        "outputId": "bd142f82-f226-4884-ab9a-33d38e8b9c67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "124928\n",
            "torch.Size([4, 64, 64])\n",
            "124928\n"
          ]
        }
      ],
      "source": [
        "# @title hiera vit me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class LayerNorm_at(nn.RMSNorm): # LayerNorm RMSNorm\n",
        "    def __init__(self, dim=1, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.dim=dim\n",
        "    def forward(self, x):\n",
        "        return super().forward(x.transpose(self.dim,-1)).transpose(self.dim,-1)\n",
        "\n",
        "# # [b, h/win1* w/win2, c, win1,win2] -> [b,c,h,w]\n",
        "# def shuffle(x, window_shape): # [b,win*win, c, h/win, w/win] -> [b,1,c,h,w]\n",
        "#     # out_shape = [x.shape[0], -1, x.shape[2]] + [x*w for x,w in zip(x.shape[3:], window_shape)] # [b,1,c,h/win*wim, w/win*wim]\n",
        "#     out_shape = [x.shape[0], -1, x.shape[2]] + [x*w for x,w in zip(x.shape[3:], window_shape)] # [b,1,c,h/win*wim, w/win*wim]\n",
        "#     x = x.unflatten(1, (-1, *window_shape)) # [b,num_tok,win,win, c, h/win, w/win]\n",
        "#     D=x.dim()+1\n",
        "#     permute = [0,1,D//2] + [val for tup in zip(range(1+D//2, D), range(2, D//2)) for val in tup]\n",
        "#     x = x.permute(permute).reshape(out_shape)\n",
        "#     return x\n",
        "\n",
        "def unshuffle(x, window_shape): # [b,c,h,w] -> [b, h/win1* w/win2, c, win1,win2]\n",
        "    new_shape = list(x.shape[:2]) + [val for xx, win in zip(list(x.shape[2:]), window_shape) for val in [xx//win, win]] # [h,w]->[h/win1, win1, w/win2, win2]\n",
        "    x = x.reshape(new_shape) # [b, c, h/win1, win1, w/win2, win2]\n",
        "    # print('unsh',x.shape, window_shape, new_shape)\n",
        "    L = len(new_shape)\n",
        "    permute = ([0] + list(range(2, L - 1, 2)) + [1] + list(range(3, L, 2))) # [0,2,4,1,3,5] / [0,2,4,6,1,3,5,6]\n",
        "    return x.permute(permute).flatten(1, L//2-1) # [b, h/win1* w/win2, c, win1,win2]\n",
        "\n",
        "class HieraBlock(nn.Module):\n",
        "    # def __init__(self, d_model, n_heads, q_stride=None, mult=4, drop=0, nd=2):\n",
        "    def __init__(self, in_dim, d_model, n_heads, q_stride=None, mult=4, drop=0, nd=2):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        # self.norm = LayerNorm_at(2, d_model) # LayerNorm RMSNorm\n",
        "        self.norm = LayerNorm_at(2, in_dim) # LayerNorm RMSNorm\n",
        "        self.drop = nn.Dropout(drop)\n",
        "        # self.attn = MaskUnitAttention(d_model, n_heads, q_stride)\n",
        "        self.attn = MaskUnitAttention(in_dim, d_model, n_heads, q_stride)\n",
        "        ff_dim=d_model*mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), nn.Linear(d_model, ff_dim), nn.ReLU(), # ReLU GELU\n",
        "            nn.RMSNorm(ff_dim), nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "            # nn.RMSNorm(d_model), act, nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Linear(ff_dim, d_model)\n",
        "            # nn.RMSNorm(d_model), nn.Linear(d_model, ff_dim), nn.ReLU(), nn.Dropout(dropout), # ReLU GELU\n",
        "            # nn.Linear(ff_dim, d_model), nn.Dropout(dropout),\n",
        "        )\n",
        "        # self.res = conv_nd(nd, d_model, d_model, q_stride, q_stride) if q_stride else nn.Identity()\n",
        "\n",
        "        # self.res = nn.Sequential(\n",
        "        #     conv_nd(nd, in_dim, d_model, 1, 1) if in_dim!=d_model else nn.Identity(),\n",
        "        #     # maxpool_nd(nd, q_stride, q_stride) if q_stride else nn.Identity(),\n",
        "        #     avgpool_nd(nd, q_stride, q_stride) if q_stride else nn.Identity(),\n",
        "        # )\n",
        "        self.res = UpDownBlock(in_dim, d_model, kernel=1, r=1/2 if q_stride else 1)\n",
        "\n",
        "        # x = self.proj(x_norm).unflatten(1, (self.attn.q_stride, -1)).max(dim=1).values # pooling res # [b, (Sy, Sx, h/Sy, w/Sx), c] -> [b, (Sy, Sx), (h/Sy, w/Sx), c] -> [b, (h/Sy, w/Sx), c]\n",
        "\n",
        "\n",
        "    def forward(self, x): # [b, num_tok, c, *win]\n",
        "        b, num_tok, c, *win = x.shape\n",
        "        # print('attnblk fwd',x.shape)\n",
        "        # x = x + self.drop(self.self(self.norm(x)))\n",
        "        # print('attnblk fwd',self.res(x.flatten(0,1)).shape, self.drop(self.attn(self.norm(x))).flatten(0,1).shape)\n",
        "        # x = self.res(x.flatten(0,1)) + self.drop(self.attn(self.norm(x))).flatten(0,1) # [b*num_tok,c,win1,win2,win3]\n",
        "        x = self.res(x) + self.drop(self.attn(self.norm(x))) # [b*num_tok,c,win1,win2,win3]\n",
        "        # x = x + self.ff(x)\n",
        "        # x = x + self.ff(x.transpose(1,-1)).transpose(1,-1)\n",
        "        x = x + self.ff(x.transpose(2,-1)).transpose(2,-1)\n",
        "        # x = self.ff(x)\n",
        "        # x = x.unflatten(0, (b, num_tok))\n",
        "        return x\n",
        "\n",
        "    # def forward(self, x): # [b,t,c] # [b, (Sy, Sx, h/Sy, w/Sx), c]\n",
        "    #     # Attention + Q Pooling\n",
        "    #     x_norm = self.norm1(x)\n",
        "    #     if self.dim != self.dim_out:\n",
        "    #         x = self.proj(x_norm).unflatten(1, (self.attn.q_stride, -1)).max(dim=1).values # pooling res # [b, (Sy, Sx, h/Sy, w/Sx), c] -> [b, (Sy, Sx), (h/Sy, w/Sx), c] -> [b, (h/Sy, w/Sx), c]\n",
        "    #     x = x + self.drop_path(self.attn(x_norm))\n",
        "    #     x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "    #     return x\n",
        "\n",
        "\n",
        "class levelBlock(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, n_heads=None, depth=1, r=1):\n",
        "        super().__init__()\n",
        "        self.seq = nn.Sequential(\n",
        "            UpDownBlock(in_ch, out_ch, r=min(1,r)) if in_ch != out_ch or r<1 else nn.Identity(),\n",
        "            # AttentionBlock(d_model, d_model, n_heads, q_stride) if in_ch != out_ch or r<1 else nn.Identity(),\n",
        "            # AttentionBlock(d_model, d_model, n_heads, q_stride=(2,2)),\n",
        "            *[HieraBlock(d_model, d_model, n_heads) for i in range(1)],\n",
        "            # UpDownBlock(out_ch, out_ch, r=r) if r>1 else nn.Identity(),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.seq(x)\n",
        "\n",
        "\n",
        "class Hiera(nn.Module):\n",
        "    def __init__(self, patch_size, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "    # def __init__(self, in_dim, out_dim, d_model, n_heads, depth):\n",
        "        # patch_size=4\n",
        "        super().__init__()\n",
        "        self.embed = nn.Sequential( # in, out, kernel, stride, pad\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.MaxPool2d(3,2,1), # nn.MaxPool2d(2,2)\n",
        "            nn.Conv2d(in_dim, d_model, 7, 1, 7//2, bias=False),\n",
        "            # nn.Conv2d(in_dim, d_model, patch_size, patch_size), # like patch\n",
        "            # nn.Conv2d(in_dim, d_model, 3, 2, 3//2, bias=False), nn.BatchNorm2d(d_model), nn.ReLU(),\n",
        "            # UpDownBlock(in_dim, dim, r=1/2, kernel=3), UpDownBlock(dim, dim, r=1/2, kernel=3)\n",
        "            # nn.PixelUnshuffle(2), nn.Conv2d(in_dim*2**2, dim, 1, bias=False),\n",
        "            # ResBlock(in_dim, d_model, kernel),\n",
        "            )\n",
        "        # # self.pos_emb = LearnedRoPE2D(dim) # LearnedRoPE2D, RoPE2D\n",
        "\n",
        "        emb_shape = (32,32)\n",
        "        # emb_shape = (8,32,32)\n",
        "        if len(emb_shape) == 3: # for video\n",
        "            pos_spatial = nn.Parameter(torch.randn(1, emb_shape[1]*emb_shape[2], d_model)*.02)\n",
        "            pos_temporal = nn.Parameter(torch.randn(1, emb_shape[0], d_model)*.02)\n",
        "            self.pos_emb = pos_spatial.repeat(1, emb_shape[0], 1) + torch.repeat_interleave(pos_temporal, emb_shape[1] * emb_shape[2], dim=1)\n",
        "        elif len(emb_shape) == 2: # for img\n",
        "            self.pos_emb = nn.Parameter(torch.randn(1, d_model, *emb_shape)*.02) # 56*56=3136\n",
        "        # self.pos_emb = self.pos_emb.flatten(2).transpose(-2,-1)\n",
        "\n",
        "        # self.blocks = nn.Sequential(*[AttentionBlock(d_model, n_heads, q_stride=(2,2) if i in [1,3] else None) for i in range(depth)])\n",
        "        mult = [1,1,1,1]\n",
        "        # mult = [1,2,4,8] # [1,2,3,4] [1,2,2,2]\n",
        "        ch_list = [d_model * m for m in mult] # [128, 256, 384, 512]\n",
        "\n",
        "        self.blocks = nn.Sequential(\n",
        "            # HieraBlock(ch_list[0], ch_list[1], n_heads, q_stride=(2,2)),\n",
        "            # UpDownBlock(ch_list[0], ch_list[1], kernel=1, r=1/2),\n",
        "            HieraBlock(ch_list[1], ch_list[1], n_heads),\n",
        "            # HieraBlock(ch_list[1], ch_list[2], n_heads, q_stride=(2,2)),\n",
        "            # UpDownBlock(ch_list[1], ch_list[2], kernel=1, r=1/2),\n",
        "            # HieraBlock(ch_list[2], ch_list[2], n_heads),\n",
        "            )\n",
        "        self.norm = nn.RMSNorm(ch_list[2]) # LayerNorm RMSNorm\n",
        "        self.out = nn.Linear(ch_list[2], out_dim, bias=False) if out_dim and out_dim != ch_list[2] else None\n",
        "\n",
        "\n",
        "    def forward(self, x, context_indices=None): # [b,c,h,w], [b,num_tok]\n",
        "        x = self.embed(x)\n",
        "        # print('vit fwd', x.shape, self.pos_emb.shape)\n",
        "        x = x + self.pos_emb\n",
        "        x = unshuffle(x, (4,4)) # [b,num_tok,c,win1,win2,win3] or [b,1,c,f,h,w]\n",
        "        if context_indices != None: x = x[torch.arange(x.shape[0]).unsqueeze(-1), context_indices] # [batch, num_context_toks, d_model]\n",
        "\n",
        "        # print('vit fwd1', x.shape)\n",
        "        x = self.blocks(x) # [b,num_tok,c,1,1,1]\n",
        "        # print('vit fwd2', x.shape)\n",
        "        # x = x.mean(-1).mean(-1)\n",
        "        x = x.flatten(-2).max(-1)[0]\n",
        "        # print('vit fwd3', x.shape)\n",
        "        x = x.squeeze() # [b,num_tok,d]\n",
        "        out = self.norm(x)\n",
        "        if self.out: out = self.out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "# pos_emb rope < learn < learned\n",
        "# conv > pixel?\n",
        "# droppath not required\n",
        "\n",
        "# norm,act,conv < conv,norm,act\n",
        "# 2*s1 < uib < resblock\n",
        "# gatedadaln 3 < 2 = 1 < ffmult4 = 2*gatedadaln\n",
        "# MaxPool2d(2,2) < MaxPool2d(3,2,3//2)\n",
        "\n",
        "# patchattn only for\n",
        "\n",
        "# multiendfusion negligible diff\n",
        "\n",
        "\n",
        "d_model = 64\n",
        "in_dim = 3\n",
        "patch_size = 2\n",
        "# model = Hiera(patch_size, in_dim, d_model, n_heads=4, nlayers=4, drop=0.).to(device)\n",
        "model = Hiera(patch_size, in_dim, d_model, n_heads=4, nlayers=1, drop=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x = torch.rand((4, in_dim, 32, 32), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 59850\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3XSZZyUPY1i"
      },
      "source": [
        "## vit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "eDqkaM2v0_zS"
      },
      "outputs": [],
      "source": [
        "# @title FSQ me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def ste_round(x): return x.round().detach() + x - x.detach()\n",
        "\n",
        "class FSQ(nn.Module):\n",
        "    def __init__(self, levels):\n",
        "        super().__init__()\n",
        "        self.levels = torch.tensor(levels, device=device)\n",
        "        self.basis = torch.cumprod(torch.tensor([*levels[1:], 1], device=device).flip(-1), dim=0).flip(-1)\n",
        "        self.half_width = (self.levels-1)/2\n",
        "        self.codebook_size = torch.prod(self.levels).item()\n",
        "        self.codebook = self.indexes_to_codes(torch.arange(self.codebook_size, device=device))\n",
        "\n",
        "    def forward(self, z, beta=1): # beta in (0,1). beta->0 => values more spread out\n",
        "        offset = (self.levels+1) % 2 /2 # .5 if even, 0 if odd\n",
        "        bound = (F.sigmoid(z)-1/2) * (self.levels-beta) + offset\n",
        "        # print('fwd', bound) #\n",
        "        quantized = ste_round(bound)\n",
        "        # print('fwd', quantized) # 4: -1012\n",
        "        return (quantized-offset) / self.half_width # split [-1,1]\n",
        "\n",
        "    def codes_to_indexes(self, zhat):\n",
        "        zhat = (zhat + 1) * self.half_width\n",
        "        return (zhat * self.basis).sum(axis=-1)#.int()\n",
        "\n",
        "    def indexes_to_codes(self, indices):\n",
        "        indices = indices.unsqueeze(-1)\n",
        "        codes = torch.remainder(indices//self.basis, self.levels)\n",
        "        # print(\"codes\",codes)\n",
        "        return codes / self.half_width - 1\n",
        "\n",
        "# fsq = FSQ(levels = [5,4,3,2])\n",
        "# # print(fsq.codebook)\n",
        "# batch_size, seq_len = 2, 4\n",
        "# # x = torch.rand((batch_size, seq_len,3),device=device)\n",
        "# x = torch.linspace(-2,2,7).repeat(4,1).T\n",
        "# la = fsq(x)\n",
        "# print(la)\n",
        "# lact = fsq.codes_to_indexes(la)\n",
        "# print(lact)\n",
        "# # la = fsq.indexes_to_codes(lact)\n",
        "# # print(la)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6T4F651kmGh",
        "outputId": "f9bf5345-35f8-41bf-c35e-ad2d187aa325"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "162112\n",
            "torch.Size([5, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# @title AttentionBlock\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class SelfAttn(nn.Module):\n",
        "    def __init__(self, dim, n_heads):\n",
        "        super().__init__()\n",
        "        self.dim, self.n_heads = dim, n_heads\n",
        "        d_head = dim//n_heads\n",
        "        self.qkv = nn.Linear(dim, dim*3, bias=False)\n",
        "        # self.lin = nn.Linear(dim, dim)\n",
        "        self.lin = zero_module(nn.Linear(dim, dim))\n",
        "        # self.rope = RoPE(d_head, seq_len=512, base=10000)\n",
        "        # self.rope = RoPE2D(d_head, h=64, w=64, base=100)\n",
        "        self.scale = d_head**-.5\n",
        "\n",
        "    def forward(self, x): # [b,t,d]\n",
        "        q,k,v = self.qkv(x).unflatten(-1, (self.n_heads,-1)).transpose(1,2).chunk(3, dim=-1) # [b, t, n_heads, d_head] -> [b, n_heads, t, d_head]\n",
        "        # q, k = self.rope(q), self.rope(k)\n",
        "        # x = F.scaled_dot_product_attention(q,k,v, attn_mask=None) # https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html\n",
        "        q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2)\n",
        "        context = k.transpose(-2,-1) @ v # [batch, n_heads, d_head, d_head]\n",
        "        x = q @ context # [b, n_heads, t, d_head]\n",
        "        # # print('SelfAttn', x.shape)\n",
        "        x = x.transpose(1,2).flatten(2)\n",
        "        return self.lin(x)\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, mult=4, drop=0.):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        # self.norm = nn.RMSNorm(d_model, elementwise_affine=False) # LayerNorm RMSNorm\n",
        "        self.norm1, self.norm2 = nn.LayerNorm(d_model), nn.LayerNorm(d_model)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "        self.attn = SelfAttn(d_model, n_heads)\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        ff_dim=d_model*mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), act, nn.Dropout(drop), nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "            nn.RMSNorm(ff_dim), act, nn.Dropout(drop), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(drop), nn.Linear(d_model, ff_dim), act,\n",
        "            # nn.RMSNorm(ff_dim), nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x): # [batch, seq_len, d_model]\n",
        "        # print('attnblk fwd',x.shape)\n",
        "        x = x + self.drop(self.attn(self.norm1(x)))\n",
        "        x = x + self.ff(x)\n",
        "        # x = x + self.drop(self.ff(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, patch_size, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        patch_size=2\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Conv2d(in_dim, d_model, patch_size, patch_size), # like patch\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 1, 7//2, bias=False)\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.MaxPool2d(3,2,1)\n",
        "            nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.BatchNorm2d(d_model), nn.ReLU(),\n",
        "            nn.Conv2d(d_model, d_model, 3, 2, 3//2, bias=False)\n",
        "            )\n",
        "        # self.embed.requires_grad=False\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, 32*32, d_model)*.02)\n",
        "        # self.pos_emb = nn.Parameter(RoPE2D(dim=d_model, h=8, w=8, base=1000), requires_grad=False)\n",
        "\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=(32//patch_size)**2, base=10000), requires_grad=False)\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, d_model, n_heads) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x).flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c] # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        # x = self.pos_enc(x)\n",
        "        # print(\"TransformerModel\",x.shape, self.pos_emb.shape)\n",
        "        x = x + self.pos_emb[:,:x.shape[1]]\n",
        "        if context_indices != None: x = x[torch.arange(x.shape[0]).unsqueeze(-1), context_indices] # [batch, num_context_toks, d_model]\n",
        "\n",
        "        x = self.transformer(x)\n",
        "        out = self.norm(x)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "d_model = 64\n",
        "in_dim = 3\n",
        "patch_size = 2\n",
        "# model = ViT(patch_size, in_dim, d_model, n_heads=4, nlayers=4, drop=0.).to(device)\n",
        "model = ViT(patch_size, in_dim, d_model, n_heads=4, nlayers=1, drop=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x = torch.rand((5, in_dim, 32, 32), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "Fh9o__m2-j7K",
        "outputId": "a217bb8c-8bfb-4683-8059-27e0f3ee24dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/268.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.0/268.0 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title simplex\n",
        "# !pip install -q opensimplex\n",
        "import opensimplex\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def simplexmask2d(hw=(32,32), ctx_scale=(.85,1.), trg_scale=(.6,.8), B=64, chaos=[1,.5]):\n",
        "    ix, iy = np.split(np.linspace(0, chaos[0], num=sum(hw)), [hw[0]])\n",
        "    noise = opensimplex.noise3array(ix, iy, np.random.randint(1e10, size=B)) # [b,h,w]\n",
        "    noise = torch.from_numpy(noise).flatten(1)\n",
        "    trunc_normal = torch.fmod(torch.randn(2)*.3,1)/2 + .5\n",
        "    ctx_mask_scale = torch.rand(1) * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    trg_mask_scale = torch.rand(1) * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "    # ctx_mask_scale = trunc_normal[0] * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    # trg_mask_scale = trunc_normal[1] * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "    seq = hw[0]*hw[1]\n",
        "    ctx_len, trg_len = int(seq*ctx_mask_scale), int(seq*trg_mask_scale)\n",
        "    val, trg_index = torch.topk(noise, trg_len, dim=1, sorted=False)\n",
        "    # ctx_len = ctx_len - trg_len\n",
        "    ctx_len = min(ctx_len, seq-trg_len)\n",
        "\n",
        "    remove_mask = torch.ones((B,seq), dtype=bool) # [B, S]\n",
        "    remove_mask.scatter_(1, trg_index, False).flatten()\n",
        "    ind = torch.arange(seq).unsqueeze(0).repeat(B,1)[remove_mask].reshape(B, -1)\n",
        "\n",
        "    ix, iy = np.split(np.linspace(0, chaos[1], num=sum(hw)), [hw[0]])\n",
        "    noise = opensimplex.noise3array(ix, iy, np.random.randint(1e10, size=B)) # [b,h,w]\n",
        "    noise = torch.from_numpy(noise).flatten(1)[remove_mask].reshape(B, -1)\n",
        "    val, ctx_ind = torch.topk(noise, ctx_len, dim=1, sorted=False)\n",
        "    ctx_index = ind[torch.arange(B).unsqueeze(-1), ctx_ind]\n",
        "    return ctx_index, trg_index\n",
        "\n",
        "b=16\n",
        "# ctx_index, trg_index = simplexmask2d(hw=(32,32), ctx_scale=(.85,1), trg_scale=(.7,.8), B=b, chaos=[3,.5])\n",
        "# ctx_index, trg_index = simplexmask2d(hw=(32,32), ctx_scale=(.85,1), trg_scale=(.7,.8), B=b, chaos=[3,1])\n",
        "# ctx_index, trg_index = simplexmask2d(hw=(32,32), ctx_scale=(.85,1), trg_scale=(.2,.8), B=b, chaos=[3,1])\n",
        "ctx_index, trg_index = simplexmask2d(hw=(32,32), ctx_scale=(.05,.2), trg_scale=(.7,.8), B=b, chaos=[3,1])\n",
        "# ctx_index, trg_index = simplexmask2d(hw=(32,32), ctx_scale=(.85,1), trg_scale=(.5,.5), B=b, chaos=[.01,.5])\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "# imshow(mask)\n",
        "\n",
        "mask = torch.zeros(b ,32*32)\n",
        "# mask = torch.ones(b ,32*32)*.3\n",
        "mask[torch.arange(b).unsqueeze(-1), trg_index] = 1\n",
        "mask[torch.arange(b).unsqueeze(-1), ctx_index] = .5\n",
        "# print((mask==1).sum(1))\n",
        "# print((mask==.5).sum(1))\n",
        "mask = mask.reshape(b,1,32,32)\n",
        "import torchvision\n",
        "imshow(torchvision.utils.make_grid(mask, nrow=8))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "pQfM2fYvcTh6",
        "outputId": "5067de41-f150-40ca-c9bf-003494493fbc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title masks\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def multiblock(seq, min_s, max_s, M=1):\n",
        "    mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "    return target_mask\n",
        "\n",
        "\n",
        "def multiblock2d(hw=(8,8), scale=(.15,.2), aspect_ratio=(.75,1.5), M=1):\n",
        "    mask_aspect = torch.rand(1) * (aspect_ratio[1] - aspect_ratio[0]) + aspect_ratio[0] # in (min_s, max_s) # all blocks same size\n",
        "    mask_scale = torch.rand(1) * (scale[1] - scale[0]) + scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    h = (mask_scale/mask_aspect)**.5# h*(h*aspect) = scale\n",
        "    w = h * mask_aspect\n",
        "    h_pos, w_pos = torch.rand(M)*(1-w), torch.rand(M)*(1-h) # in (0, 1 - mask_len)\n",
        "    h_len, h_pos = (h*hw[0]).int(), h_pos*hw[0]\n",
        "    w_len, w_pos = (w*hw[1]).int(), w_pos*hw[1]\n",
        "    h_ind, w_ind = torch.arange(hw[0]).unsqueeze(0), torch.arange(hw[1]).unsqueeze(0) # [1, seq]\n",
        "    h_mask = (h_ind>=h_pos.unsqueeze(-1)) & (h_ind<(h_pos+h_len).unsqueeze(-1)) # [M, seq]\n",
        "    w_mask = (w_ind>=w_pos.unsqueeze(-1)) & (w_ind<(w_pos+w_len).unsqueeze(-1)) # [M, seq]\n",
        "    target_mask = h_mask.unsqueeze(-1) & w_mask.unsqueeze(-2) # [M, seq, seq]\n",
        "    return target_mask\n",
        "\n",
        "# https://arxiv.org/pdf/2210.07224\n",
        "def randpatch(seq, mask_size=8, gamma=0.9): # num patches of seq, mask patch size, masking ratio\n",
        "    # mask = torch.rand(seq//mask_size)<gamma\n",
        "    length = seq//mask_size\n",
        "    g = torch.normal(gamma, std=.1, size=(1,)).clamp(.5,.9)\n",
        "    # g = gamma\n",
        "    idx = torch.randperm(length)[:int(length*g)]\n",
        "    mask = torch.zeros(length, dtype=bool)\n",
        "    mask[idx] = True\n",
        "    mask = mask.repeat_interleave(mask_size, dim=-1)\n",
        "    return mask # [seq] , True -> mask\n",
        "\n",
        "\n",
        "import torch\n",
        "def apply_masks(x, mask): # [b,t,d], [mask_size] # https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "    mask_keep = mask.unsqueeze(-1).repeat(x.size(0), 1, x.size(-1)) # [batch,T,dim]\n",
        "    return torch.gather(x, dim=1, index=mask_keep) # [batch,mask_size,dim]\n",
        "\n",
        "\n",
        "# @title ijepa multiblock next\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "import math\n",
        "from multiprocessing import Value\n",
        "import torch\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(self, hw=(224, 224), enc_mask_scale=(.85,1), pred_mask_scale=(.15,.2), aspect_ratio=(.75,1.25),\n",
        "        nenc=1, npred=2, min_keep=4, allow_overlap=False):\n",
        "        super().__init__()\n",
        "        self.height, self.width = hw\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.aspect_ratio = aspect_ratio\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "\n",
        "    def _sample_block_size(self, scale, aspect_ratio_scale):\n",
        "        _rand = torch.rand(1).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.height * self.width * mask_scale) # num patches to keep\n",
        "        # -- Sample block aspect-ratio\n",
        "        min_ar, max_ar = aspect_ratio_scale\n",
        "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
        "        while h >= self.height: h -= 1 # crop mask to be smaller than img\n",
        "        while w >= self.width: w -= 1\n",
        "        return (h, w)\n",
        "\n",
        "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
        "        h, w = b_size\n",
        "\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # --\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            top = torch.randint(0, self.height - h, (1,))\n",
        "            left = torch.randint(0, self.width - w, (1,))\n",
        "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
        "            mask[top:top+h, left:left+w] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
        "        mask_complement[top:top+h, left:left+w] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, B):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        p_size = self._sample_block_size(scale=self.pred_mask_scale, aspect_ratio_scale=self.aspect_ratio)\n",
        "        e_size = self._sample_block_size(scale=self.enc_mask_scale, aspect_ratio_scale=(1., 1.))\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.height * self.width\n",
        "        min_keep_enc = self.height * self.width\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                print(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "        return collated_masks_enc, collated_masks_pred\n",
        "\n",
        "mask_collator = MaskCollator(hw=(32,32), enc_mask_scale=(.85, 1.), pred_mask_scale=(.15, .2), aspect_ratio=(.75, 1.5),\n",
        "        nenc=1, npred=4, min_keep=4,\n",
        "        # allow_overlap=True)\n",
        "        allow_overlap=False)\n",
        "\n",
        "b=16\n",
        "collated_masks_enc, collated_masks_pred = mask_collator(b)\n",
        "ctx_index, trg_index = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "\n",
        "# mask = torch.zeros(1 ,32*32)\n",
        "# mask[:, trg_index[:1]] = 1\n",
        "# mask[:, ctx_index[:1]] = .5\n",
        "# mask = mask.reshape(1,32,32)\n",
        "\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "# imshow(mask)\n",
        "\n",
        "mask = torch.zeros(b ,32*32)\n",
        "mask[torch.arange(b).unsqueeze(-1), trg_index] = 1\n",
        "mask[torch.arange(b).unsqueeze(-1), ctx_index] = .5\n",
        "mask = mask.reshape(b,1,32,32)\n",
        "import torchvision\n",
        "imshow(torchvision.utils.make_grid(mask, nrow=8))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask_collator = MaskCollator(hw=(1024,1024), enc_mask_scale=(.85, 1.), pred_mask_scale=(.15, .2), aspect_ratio=(.75, 1.5), nenc=1, npred=4, min_keep=4, allow_overlap=False)\n",
        "# %timeit collated_masks_enc, collated_masks_pred = mask_collator(64) # 225 ms 1024:4.79 s\n",
        "# %timeit ctx_index, trg_index = simplexmask2d(hw=(1024,1024), ctx_scale=(.85,1), trg_scale=(.5,.6), B=b, chaos=.5) # 265 ms ;topk 203 ms 1024:4.27 s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nj5bI5hSEdJ1",
        "outputId": "6a7d20de-cdae-47f0-82ac-64546f16f829"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slowest run took 5.27 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "4.79 s ± 3.03 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ttc=[]\n",
        "ttt=[]\n",
        "def mean(x): return sum(x)/len(x)\n",
        "\n",
        "for i in range(1000):\n",
        "    # collated_masks_enc, collated_masks_pred = mask_collator(1)\n",
        "    # context_indices, trg_indices = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "    # context_indices, trg_indices = simplexmask2d(hw=(32,32), ctx_scale=(.85,1.), trg_scale=(.7,.8), B=1, chaos=[3,1])\n",
        "    context_indices, trg_indices = simplexmask2d(hw=(32,32), ctx_scale=(.05,.5), trg_scale=(.2,.8), B=1, chaos=[3,1])\n",
        "    ttc.append(context_indices.shape[-1])\n",
        "    ttt.append(trg_indices.shape[-1])\n",
        "\n",
        "print(mean(ttc), mean(ttt))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"figure.figsize\"] = (4,4)\n",
        "plt.hist(ttc, bins=20, alpha=.5, label='context mask')\n",
        "plt.hist(ttt, bins=20, alpha=.5, label='target mask')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "bC6elHNF5xzL",
        "outputId": "03d9bedf-8c95-4bca-f1b3-f3c851dfc6c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "258.673 499.481\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "M-zdjdJixtOu"
      },
      "outputs": [],
      "source": [
        "# @title Transformer Predictor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, 32*32, d_model)*0.02)\n",
        "        # self.pos_emb = nn.Parameter(RoPE2D(dim=d_model, h=8, w=8, base=1000), requires_grad=False)\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=32*32, base=10000), requires_grad=False)\n",
        "\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, d_model, n_heads) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.randn(1,1,d_model)*0.02) # randn zeros\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim)# if out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = x.shape\n",
        "        # x = x * self.pos_enc(context_indices)\n",
        "        x = x + self.pos_emb[0,context_indices]\n",
        "\n",
        "        # pred_tokens = self.cls * self.pos_enc(trg_indices) # [M, num_trg_toks, d_model]\n",
        "        pred_tokens = self.cls + self.pos_emb[0,trg_indices]\n",
        "        # print(\"pred fwd\", x.shape, pred_tokens.shape)\n",
        "        x = torch.cat([x, pred_tokens], dim=1) # [batch, seq_len+num_trg_toks, d_model]\n",
        "        out = self.transformer(x)\n",
        "\n",
        "        out = self.norm(out)\n",
        "\n",
        "        out = out[:,seq:] # [batch, num_trg_toks, d_model]\n",
        "        # ind = torch.cat([context_indices, trg_indices], dim=-1).unsqueeze(-1).repeat(1,1,x.shape[-1]) # [b,t]->[b,t,d]\n",
        "        # out=torch.gather(out, 1, ind)\n",
        "\n",
        "        # print(\"pred fwd\", context_indices.shape, trg_indices.shape, out.shape)\n",
        "\n",
        "        out = self.lin(out)\n",
        "        # print(\"pred fwd\", out.shape)\n",
        "        return out # [seq_len, batch_size, ntoken]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ardu1zJwdHM3",
        "outputId": "7cdcf6fd-bb9b-47ef-e2ac-7976d1182ab7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "245184\n",
            "torch.Size([])\n"
          ]
        }
      ],
      "source": [
        "# @title IJEPA\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class IJEPA(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=64, out_dim=None, nlayers=1, n_heads=4):\n",
        "        super().__init__()\n",
        "        self.out_dim = out_dim = out_dim or d_model\n",
        "        self.patch_size = 4 # 4\n",
        "        # self.student = Hiera(self.patch_size, in_dim, d_model, out_dim=out_dim, n_heads=n_heads, nlayers=nlayers, drop=0.)\n",
        "        self.student = ViT(self.patch_size, in_dim, d_model, out_dim=out_dim, n_heads=n_heads, nlayers=nlayers, drop=0.)\n",
        "        self.predicter = TransformerPredictor(out_dim, d_model//2, out_dim, n_heads=4, nlayers=1, drop=0.)\n",
        "        # self.predicter = TransformerPredictor(out_dim, 3*d_model//8, out_dim, n_heads=4, nlayers=1, drop=0.)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "\n",
        "    def loss(self, x): #\n",
        "        b,c,h,w = x.shape\n",
        "        # print('ijepa loss x',x.shape)\n",
        "        hw=(8,8)\n",
        "        # hw=(32,32)\n",
        "        mask_collator = MaskCollator(hw, enc_mask_scale=(.85, 1.), pred_mask_scale=(.15, .2), aspect_ratio=(.75, 1.5), nenc=1, npred=4, min_keep=4, allow_overlap=False)\n",
        "        collated_masks_enc, collated_masks_pred = mask_collator(b)\n",
        "        context_indices, trg_indices = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "        # print(context_indices.shape, trg_indices.shape)\n",
        "\n",
        "        # context_indices, trg_indices = simplexmask2d(hw, ctx_scale=(.85,1), trg_scale=(.7,.8), B=b, chaos=[3,.5])\n",
        "        # context_indices, trg_indices = simplexmask2d(hw, ctx_scale=(.85,1), trg_scale=(.7,.8), B=b, chaos=[5,.5])\n",
        "        # context_indices, trg_indices = simplexmask2d(hw, ctx_scale=(.05,.2), trg_scale=(.7,.8), B=b, chaos=[3,1])\n",
        "        # context_indices, trg_indices = context_indices.repeat(b,1), trg_indices.repeat(b,1)\n",
        "\n",
        "        # zero_mask = torch.zeros(b ,*hw, device=device).flatten(1)\n",
        "        # zero_mask[torch.arange(b).unsqueeze(-1), context_indices] = 1\n",
        "        # x_ = x * F.interpolate(zero_mask.reshape(b,1,*hw), size=x.shape[2:], mode='nearest-exact') # zero masked locations\n",
        "\n",
        "        sx = self.student(x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "        # print('ijepa loss sx',sx.shape)\n",
        "        sy_ = self.predicter(sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "        sy_ = F.layer_norm(sy_, (sy_.size(-1),))\n",
        "        with torch.no_grad():\n",
        "            sy = self.teacher(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "            sy = sy[torch.arange(sy.shape[0]).unsqueeze(-1), trg_indices] # [batch, num_context_toks, d_model] # nan bec len(trg_ind)==0 # print('loss sy',torch.isnan(sy).any())\n",
        "            sy = F.layer_norm(sy, (sy.size(-1),))\n",
        "        loss = F.mse_loss(sy, sy_)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        sx = self.student(x)\n",
        "        out = sx.mean(dim=1)\n",
        "        return out\n",
        "\n",
        "# min_s=0.15, max_s, M\n",
        "# trg.15.2M4 C.85 1\n",
        "# dont normalise data\n",
        "# randmsk < simplex 1msk < multiblk\n",
        "# teacher=trans ~? teacher=copy\n",
        "# learn convemb\n",
        "# lr pred,student: 3e-3/1e-2, 1e-3\n",
        "\n",
        "# lr 1e-3 < 1e-2,1e-3? slower but dun plateau\n",
        "\n",
        "# vit 1lyr 15.4sec 15.6\n",
        "# hiera downsampling attn 20.5\n",
        "\n",
        "# ijepa = IJEPA(in_dim=3, d_model=64, out_dim=64, nlayers=4, n_heads=4).to(device)\n",
        "ijepa = IJEPA(in_dim=3, d_model=64, out_dim=64, nlayers=1, n_heads=8).to(device)\n",
        "# ijepa = IJEPA(in_dim=3, d_model=32, out_dim=32, nlayers=1, n_heads=4).to(device)\n",
        "# optim = torch.optim.AdamW(ijepa.parameters(), lr=1e-3) # 1e-3?\n",
        "optim = torch.optim.AdamW([{'params': ijepa.student.parameters()},\n",
        "#     {'params': ijepa.predicter.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # good 3e-3,1e-3 ; default 1e-2, 5e-2\n",
        "    {'params': ijepa.predicter.parameters(), 'lr': 1e-2}], lr=1e-3, weight_decay=1e-2)\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/configs/in1k_vith14_ep300.yaml\n",
        "# d_model 1024,384\n",
        "# depth 12,6/12\n",
        "# wd 5e-2 - 4e-1\n",
        "# adamw 1e-4 - 1e-3 - 1e-6\n",
        "# ema 0.996-1\n",
        "\n",
        "print(sum(p.numel() for p in ijepa.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in ijepa.parameters())) # 27584\n",
        "# print(sum(p.numel() for p in ijepa.predicter.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# d_model^2 * nlayers\n",
        "\n",
        "x = torch.rand((64,3,32,32), device=device)\n",
        "out = ijepa.loss(x)\n",
        "print(out.shape)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(ijepa.out_dim, 10).to(device)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n",
        "# optim = torch.optim.AdamW([{'params': ijepa.parameters()}, {'params': classifier.parameters(), 'lr': 1e-3}], lr=1e-3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODpKypTCsfIt",
        "outputId": "b2f4380b-03a8-49be-95b5-eed2b564dc05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 16])\n"
          ]
        }
      ],
      "source": [
        "# @title TransformerVICReg\n",
        "import torch\n",
        "from torch import nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class TransformerVICReg(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, drop=0):\n",
        "        super().__init__()\n",
        "        act = nn.GELU()\n",
        "        patch_size=4\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Linear(in_dim, d_model), act\n",
        "            # nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            # nn.Conv1d(d_model, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            # nn.Conv1d(d_model, d_model,3,2,3//2),\n",
        "            nn.Conv2d(in_dim, d_model, patch_size, patch_size), # patch\n",
        "            )\n",
        "        self.pos_enc = RoPE(d_model, seq_len=200, base=10000)\n",
        "        # self.pos_emb = nn.Parameter(torch.randn(1, 8*8, d_model))\n",
        "        # self.pos_emb = nn.Parameter(RoPE2D(dim=d_model, h=8, w=8, base=10000).unsqueeze(0), requires_grad=False)\n",
        "\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "\n",
        "        # out_dim = out_dim or d_model\n",
        "        self.lin = nn.Linear(d_model, out_dim)\n",
        "        self.attn_pool = nn.Linear(d_model, 1, bias=False)\n",
        "\n",
        "        dim_v = d_model * 4\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(out_dim, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v, bias=False),\n",
        "            )\n",
        "\n",
        "    def forward(self, x): # [b,t,d] / [b,c,h,w]\n",
        "        x = self.embed(x).flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c]\n",
        "        # x = self.embed(x.transpose(-2,-1)).transpose(-2,-1) # [b,t,d]\n",
        "        x = self.pos_enc(x)\n",
        "        # x = x + self.pos_emb\n",
        "\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        attn = self.attn_pool(x).squeeze(-1) # [batch, seq] # seq_pool\n",
        "        out = (torch.softmax(attn, dim=-1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, seq] @ [batch, seq, dim] -> [batch, dim]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch, ntoken]\n",
        "\n",
        "    def expand(self, x):\n",
        "        sx = self.forward(x)\n",
        "        vx = self.exp(sx)\n",
        "        return vx\n",
        "\n",
        "batch, seq_len, d_model = 4,3500,512\n",
        "in_dim, out_dim=3,16\n",
        "model = TransformerVICReg(in_dim, d_model, out_dim, d_head=4, nlayers=2, drop=0.).to(device)\n",
        "# x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "x =  torch.rand((batch, in_dim, 32,32), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qwg9dG4sQ3h",
        "outputId": "2f0092f5-5bb7-4725-db2b-dcf7b0ba1a96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "62850\n",
            "in vicreg  0.00019920778413506923 24.74469393491745 4.793645480560826e-09\n",
            "(tensor(7.9683e-06, grad_fn=<MseLossBackward0>), tensor(0.9898, grad_fn=<AddBackward0>), tensor(4.7936e-09, grad_fn=<AddBackward0>))\n"
          ]
        }
      ],
      "source": [
        "# @title Violet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class Violet(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, nlayers=2, d_head=4):\n",
        "        super().__init__()\n",
        "        out_dim = out_dim or d_model\n",
        "        self.student = TransformerVICReg(in_dim, d_model, out_dim=out_dim, d_head=d_head, nlayers=nlayers, drop=0.)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "\n",
        "        # vicreg\n",
        "        self.sim_coeff=25. # 10.0 # 25.0 # λ\n",
        "        self.std_coeff=25. # 1.0 # 25.0 # µ\n",
        "        self.cov_coeff=1. # 25.0 # 1.0 # ν\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]c/ [b,c,h,w]\n",
        "        # print(x.shape)\n",
        "        # mask = simplexmask(hw=(8,8), scale=(.7,.8)).unsqueeze(0) # .6.8\n",
        "        # vx = self.student.expand(x, mask) # [batch, num_context_toks, out_dim]\n",
        "\n",
        "        vx = self.student.expand(x) # [batch, num_context_toks, out_dim]\n",
        "\n",
        "        with torch.no_grad(): vy = self.teacher.expand(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "        loss = self.vicreg(vx, vy)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        return self.student(x)\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size, num_features = x.shape\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = self.sim_coeff * repr_loss + self.std_coeff * std_loss + self.cov_coeff * cov_loss\n",
        "        print(\"in vicreg \",self.sim_coeff * repr_loss.item() , self.std_coeff * std_loss.item() , self.cov_coeff * cov_loss.item())\n",
        "        # return loss\n",
        "        return repr_loss, std_loss, cov_loss\n",
        "\n",
        "\n",
        "violet = Violet(in_dim=3, d_model=32, out_dim=16, nlayers=2, d_head=4).to(device)\n",
        "voptim = torch.optim.AdamW(violet.parameters(), lr=1e-3) # 1e-3?\n",
        "# voptim = torch.optim.AdamW([{'params': violet.student.transformer.parameters()},\n",
        "#     {'params': violet.student.exp.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # default 1e-2\n",
        "print(sum(p.numel() for p in violet.parameters() if p.requires_grad)) # 27584\n",
        "\n",
        "# x = torch.rand((2,1000,3), device=device)\n",
        "x = torch.rand((2,3,32,32), device=device)\n",
        "# x = torch.rand((2,1,16), device=device)\n",
        "loss = violet.loss(x)\n",
        "# print(out.shape)\n",
        "print(loss)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(16).to(device)\n",
        "# classifier = Classifier(16, 18).to(device)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_BFm8Kh-j3u"
      },
      "outputs": [],
      "source": [
        "for name, param in ijepa.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name, param)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "eb1n4BhimG_N"
      },
      "outputs": [],
      "source": [
        "# @title RankMe\n",
        "# RankMe: Assessing the Downstream Performance of Pretrained Self-Supervised Representations by Their Rank jun 2023 https://arxiv.org/pdf/2210.02885\n",
        "import torch\n",
        "\n",
        "# https://github.com/Spidartist/IJEPA_endoscopy/blob/main/src/helper.py#L22\n",
        "def RankMe(Z):\n",
        "    \"\"\"\n",
        "    Calculate the RankMe score (the higher, the better).\n",
        "    RankMe(Z) = exp(-sum_{k=1}^{min(N, K)} p_k * log(p_k)),\n",
        "    where p_k = sigma_k (Z) / ||sigma_k (Z)||_1 + epsilon\n",
        "    where sigma_k is the kth singular value of Z.\n",
        "    where Z is the matrix of embeddings (N × K)\n",
        "    \"\"\"\n",
        "    # compute the singular values of the embeddings\n",
        "    # _u, s, _vh = torch.linalg.svd(Z, full_matrices=False)  # s.shape = (min(N, K),)\n",
        "    # s = torch.linalg.svd(Z, full_matrices=False).S\n",
        "    s = torch.linalg.svdvals(Z)\n",
        "    p = s / torch.sum(s, axis=0) + 1e-7\n",
        "    return torch.exp(-torch.sum(p * torch.log(p)))\n",
        "\n",
        "# Z = torch.randn(5, 3)\n",
        "# rankme = RankMe(Z)\n",
        "# print(rankme)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "y24ZNFqW7woQ"
      },
      "outputs": [],
      "source": [
        "# @title LiDAR\n",
        "# LiDAR: Sensing Linear Probing Performance In Joint Embedding SSL Architectures https://arxiv.org/pdf/2312.04000\n",
        "# https://github.com/rbalestr-lab/stable-ssl/blob/main/stable_ssl/monitors.py#L106\n",
        "\n",
        "def LiDAR(sx, eps=1e-7, delta=1e-3):\n",
        "    sx = sx.unflatten(0, (-1, 2))\n",
        "    n, q, d = sx.shape\n",
        "    mu_x = sx.mean(dim=1) # mu_x # [n,d]\n",
        "    mu = mu_x.mean(dim=0) # mu # [d]\n",
        "\n",
        "    diff_b = (mu_x - mu).unsqueeze(-1) # [n,d,1]\n",
        "    S_b = (diff_b @ diff_b.transpose(-2,-1)).sum(0) / (n - 1) # [n,d,d] -> [d,d]\n",
        "    diff_w = (sx - mu_x.unsqueeze(1)).reshape(-1,d,1) # [n,q,d] -> [nq,d,1]\n",
        "    S_w = (diff_w @ diff_w.transpose(-2,-1)).sum(0) / (n * (q - 1)) + delta * torch.eye(d, device=sx.device) # [nq,d,d] -> [d,d]\n",
        "\n",
        "    eigvals_w, eigvecs_w = torch.linalg.eigh(S_w)\n",
        "    eigvals_w = torch.clamp(eigvals_w, min=eps)\n",
        "\n",
        "    invsqrt_w = (eigvecs_w * (1. / torch.sqrt(eigvals_w))) @ eigvecs_w.transpose(-1, -2)\n",
        "    S_lidar = invsqrt_w @ S_b @ invsqrt_w\n",
        "    lam = torch.linalg.eigh(S_lidar)[0].clamp(min=0)\n",
        "    p = lam / lam.sum() + eps\n",
        "    return torch.exp(-torch.sum(p * torch.log(p)))\n",
        "\n",
        "# sx = torch.randn(32, 128)\n",
        "# print(sx.shape)\n",
        "# lidar = LiDAR(sx)\n",
        "# print(lidar.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "2Nd-sGe6Ku4S",
        "outputId": "e3dcd0a7-7036-4165-f7d1-f06f85022d87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbobdole\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250425_054118-56tkdoh5</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/ijepa/runs/56tkdoh5' target=\"_blank\">rosy-lion-92</a></strong> to <a href='https://wandb.ai/bobdole/ijepa' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/ijepa' target=\"_blank\">https://wandb.ai/bobdole/ijepa</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/ijepa/runs/56tkdoh5' target=\"_blank\">https://wandb.ai/bobdole/ijepa/runs/56tkdoh5</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "!pip install -q wandb\n",
        "import wandb # https://docs.wandb.ai/quickstart\n",
        "wandb.login(key='487a2109e55dce4e13fc70681781de9f50f27be7')\n",
        "try: run.finish()\n",
        "except NameError: pass\n",
        "run = wandb.init(project=\"ijepa\", config={\"model\": \"res18\",})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "34de8800-fa4b-4460-803c-3cd8dbfb6c70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a3e7101-7bef-4479-e669-252bcc8d8836"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "classify 2.0167236328125\n",
            "classify 2.0855712890625\n",
            "classify 1.9647216796875\n",
            "classify 1.976318359375\n",
            "classify 2.046142578125\n",
            "classify 1.9697265625\n",
            "classify 2.0433349609375\n",
            "classify 1.9512939453125\n",
            "0.390625\n",
            "0.453125\n",
            "0.265625\n",
            "0.25\n",
            "0.34375\n",
            "0.203125\n",
            "0.375\n",
            "0.265625\n",
            "0.328125\n",
            "0.328125\n",
            "0.34375\n",
            "time: 3.81715989112854 4.082977388974288\n",
            "834\n",
            "strain 0.23869021236896515\n",
            "strain 0.2295597642660141\n",
            "strain 0.20958562195301056\n",
            "strain 0.2896016836166382\n",
            "strain 0.2604771554470062\n",
            "strain 0.2397172451019287\n",
            "classify 1.9007568359375\n",
            "classify 1.990478515625\n",
            "classify 2.021484375\n",
            "classify 2.010498046875\n",
            "classify 2.0006103515625\n",
            "classify 2.0599365234375\n",
            "classify 2.0028076171875\n",
            "classify 1.9781494140625\n",
            "classify 2.0323486328125\n",
            "classify 1.8548583984375\n",
            "classify 1.97802734375\n",
            "0.3125\n",
            "0.421875\n",
            "0.390625\n",
            "0.328125\n",
            "0.296875\n",
            "0.265625\n",
            "0.390625\n",
            "0.234375\n",
            "0.328125\n",
            "0.21875\n",
            "0.21875\n",
            "time: 4.692733526229858 4.083708222611936\n",
            "835\n",
            "strain 0.31268802285194397\n",
            "strain 0.25268077850341797\n",
            "strain 0.3272973895072937\n",
            "strain 0.2420344352722168\n",
            "strain 0.215169757604599\n",
            "strain 0.20490844547748566\n",
            "classify 1.978271484375\n",
            "classify 1.962890625\n",
            "classify 2.010009765625\n",
            "classify 1.9481201171875\n",
            "classify 1.969482421875\n",
            "classify 1.986328125\n",
            "classify 2.016357421875\n",
            "classify 1.8968505859375\n",
            "classify 1.936279296875\n",
            "classify 2.0054931640625\n",
            "classify 1.9520263671875\n",
            "0.265625\n",
            "0.359375\n",
            "0.296875\n",
            "0.28125\n",
            "0.390625\n",
            "0.390625\n",
            "0.28125\n",
            "0.390625\n",
            "0.421875\n",
            "0.40625\n",
            "0.453125\n",
            "time: 3.8571720123291016 4.083437760766043\n",
            "836\n",
            "strain 0.2371879667043686\n",
            "strain 0.22028589248657227\n",
            "strain 0.28837358951568604\n",
            "strain 0.23631498217582703\n",
            "strain 0.28172895312309265\n",
            "strain 0.28563791513442993\n",
            "classify 1.993896484375\n",
            "classify 1.9158935546875\n",
            "classify 1.95172119140625\n",
            "classify 2.081787109375\n",
            "classify 1.9327392578125\n",
            "classify 1.991943359375\n",
            "classify 1.95361328125\n",
            "classify 2.06689453125\n",
            "classify 2.0880126953125\n",
            "classify 1.9483642578125\n",
            "classify 1.920654296875\n",
            "0.25\n",
            "0.375\n",
            "0.375\n",
            "0.375\n",
            "0.25\n",
            "0.296875\n",
            "0.3125\n",
            "0.296875\n",
            "0.3125\n",
            "0.34375\n",
            "0.296875\n",
            "time: 3.8464765548706055 4.083155205315301\n",
            "837\n",
            "strain 0.29049861431121826\n",
            "strain 0.21302908658981323\n",
            "strain 0.3074931800365448\n",
            "strain 0.31215959787368774\n",
            "strain 0.20173294842243195\n",
            "strain 0.28305667638778687\n",
            "classify 2.078125\n",
            "classify 1.9835205078125\n",
            "classify 1.957275390625\n",
            "classify 1.9658203125\n",
            "classify 1.9432373046875\n",
            "classify 2.0130615234375\n",
            "classify 1.95068359375\n",
            "classify 1.9559326171875\n",
            "classify 1.9390869140625\n",
            "classify 1.951416015625\n",
            "classify 1.95751953125\n",
            "0.28125\n",
            "0.3125\n",
            "0.265625\n",
            "0.390625\n",
            "0.328125\n",
            "0.234375\n",
            "0.296875\n",
            "0.34375\n",
            "0.359375\n",
            "0.359375\n",
            "0.203125\n",
            "time: 4.460749626159668 4.083606306863571\n",
            "838\n",
            "strain 0.20414699614048004\n",
            "strain 0.2250896841287613\n",
            "strain 0.22278963029384613\n",
            "strain 0.2793150544166565\n",
            "strain 0.28606879711151123\n",
            "strain 0.27556005120277405\n",
            "classify 1.985107421875\n",
            "classify 2.065185546875\n",
            "classify 1.94775390625\n",
            "classify 2.0029296875\n",
            "classify 1.881103515625\n",
            "classify 2.029052734375\n",
            "classify 1.9847412109375\n",
            "classify 1.9691162109375\n",
            "classify 2.037353515625\n",
            "classify 2.0975341796875\n",
            "classify 2.068603515625\n",
            "0.34375\n",
            "0.296875\n",
            "0.359375\n",
            "0.28125\n",
            "0.265625\n",
            "0.359375\n",
            "0.390625\n",
            "0.296875\n",
            "0.328125\n",
            "0.296875\n",
            "0.359375\n",
            "time: 3.7816948890686035 4.083247002600487\n",
            "839\n",
            "strain 0.29398852586746216\n",
            "strain 0.28399965167045593\n",
            "strain 0.2910141348838806\n",
            "strain 0.24937155842781067\n",
            "strain 0.2115841805934906\n",
            "strain 0.3194512128829956\n",
            "classify 1.9024658203125\n",
            "classify 2.0126953125\n",
            "classify 2.037353515625\n",
            "classify 1.922607421875\n",
            "classify 2.01123046875\n",
            "classify 1.9879150390625\n",
            "classify 2.008056640625\n",
            "classify 1.9798583984375\n",
            "classify 1.9515380859375\n",
            "classify 1.947509765625\n",
            "classify 1.875\n",
            "0.3125\n",
            "0.203125\n",
            "0.375\n",
            "0.265625\n",
            "0.3125\n",
            "0.46875\n",
            "0.234375\n",
            "0.359375\n",
            "0.3125\n",
            "0.359375\n",
            "0.328125\n",
            "time: 3.7828752994537354 4.082889968724478\n",
            "840\n",
            "strain 0.25978168845176697\n",
            "strain 0.3565426766872406\n",
            "strain 0.2914915680885315\n",
            "strain 0.3034428656101227\n",
            "strain 0.29518207907676697\n",
            "strain 0.23193775117397308\n",
            "classify 1.933837890625\n",
            "classify 1.9415283203125\n",
            "classify 1.959228515625\n",
            "classify 1.953369140625\n",
            "classify 1.9437255859375\n",
            "classify 2.041015625\n",
            "classify 1.9473876953125\n",
            "classify 1.9544677734375\n",
            "classify 2.0003662109375\n",
            "classify 1.904541015625\n",
            "classify 1.9443359375\n",
            "0.375\n",
            "0.328125\n",
            "0.328125\n",
            "0.25\n",
            "0.34375\n",
            "0.234375\n",
            "0.390625\n",
            "0.453125\n",
            "0.390625\n",
            "0.25\n",
            "0.265625\n",
            "time: 4.57634162902832 4.0834772802278065\n",
            "841\n",
            "strain 0.2600639760494232\n",
            "strain 0.21294108033180237\n",
            "strain 0.24701181054115295\n",
            "strain 0.2640094757080078\n",
            "strain 0.27002662420272827\n",
            "strain 0.22542807459831238\n",
            "classify 1.937255859375\n",
            "classify 1.9677734375\n",
            "classify 1.9434814453125\n",
            "classify 1.966064453125\n",
            "classify 1.996337890625\n",
            "classify 1.9005126953125\n",
            "classify 2.0128173828125\n",
            "classify 1.967529296875\n",
            "classify 2.0169677734375\n",
            "classify 2.0260009765625\n",
            "classify 1.922607421875\n",
            "0.25\n",
            "0.375\n",
            "0.34375\n",
            "0.328125\n",
            "0.359375\n",
            "0.453125\n",
            "0.28125\n",
            "0.3125\n",
            "0.328125\n",
            "0.25\n",
            "0.265625\n",
            "time: 3.8764772415161133 4.083231958810621\n",
            "842\n",
            "strain 0.21265694499015808\n",
            "strain 0.23156949877738953\n",
            "strain 0.2364206612110138\n",
            "strain 0.24659988284111023\n",
            "strain 0.25462228059768677\n",
            "strain 0.2821428179740906\n",
            "classify 1.980712890625\n",
            "classify 2.064208984375\n",
            "classify 2.1036376953125\n",
            "classify 1.85760498046875\n",
            "classify 1.9429931640625\n",
            "classify 2.1197509765625\n",
            "classify 1.9869384765625\n",
            "classify 2.0743408203125\n",
            "classify 1.976806640625\n",
            "classify 2.0260009765625\n",
            "classify 2.00799560546875\n",
            "0.328125\n",
            "0.40625\n",
            "0.390625\n",
            "0.25\n",
            "0.265625\n",
            "0.296875\n",
            "0.359375\n",
            "0.328125\n",
            "0.25\n",
            "0.390625\n",
            "0.296875\n",
            "time: 3.8452084064483643 4.082950144334479\n",
            "843\n",
            "strain 0.2817550599575043\n",
            "strain 0.30613183975219727\n",
            "strain 0.22474265098571777\n",
            "strain 0.1944827288389206\n",
            "strain 0.27339622378349304\n",
            "strain 0.28959429264068604\n",
            "classify 1.9781494140625\n",
            "classify 2.0458984375\n",
            "classify 2.026611328125\n",
            "classify 2.0362548828125\n",
            "classify 1.9501953125\n",
            "classify 1.9259033203125\n",
            "classify 1.98291015625\n",
            "classify 1.994873046875\n",
            "classify 1.9932861328125\n",
            "classify 1.854736328125\n",
            "classify 1.9971923828125\n",
            "0.359375\n",
            "0.3125\n",
            "0.359375\n",
            "0.296875\n",
            "0.34375\n",
            "0.296875\n",
            "0.25\n",
            "0.328125\n",
            "0.34375\n",
            "0.34375\n",
            "0.234375\n",
            "time: 4.45073390007019 4.083386466683935\n",
            "844\n",
            "strain 0.2277887612581253\n",
            "strain 0.2580299377441406\n",
            "strain 0.20053686201572418\n",
            "strain 0.2872011065483093\n",
            "strain 0.25946205854415894\n",
            "strain 0.2150328904390335\n",
            "classify 1.96942138671875\n",
            "classify 1.8797607421875\n",
            "classify 1.9395751953125\n",
            "classify 2.035400390625\n",
            "classify 1.93109130859375\n",
            "classify 2.003662109375\n",
            "classify 2.0328369140625\n",
            "classify 1.97564697265625\n",
            "classify 1.9864501953125\n",
            "classify 2.04638671875\n",
            "classify 1.986083984375\n",
            "0.203125\n",
            "0.203125\n",
            "0.34375\n",
            "0.25\n",
            "0.359375\n",
            "0.328125\n",
            "0.296875\n",
            "0.296875\n",
            "0.375\n",
            "0.34375\n",
            "0.34375\n",
            "time: 3.8431737422943115 4.0831027239737425\n",
            "845\n",
            "strain 0.29802799224853516\n",
            "strain 0.23041190207004547\n",
            "strain 0.31217822432518005\n",
            "strain 0.20623145997524261\n",
            "strain 0.22542572021484375\n",
            "strain 0.2812282145023346\n",
            "classify 2.1351318359375\n",
            "classify 1.914794921875\n",
            "classify 2.0101318359375\n",
            "classify 1.9759521484375\n",
            "classify 1.9056396484375\n",
            "classify 1.9293212890625\n",
            "classify 2.0487060546875\n",
            "classify 1.9888916015625\n",
            "classify 1.8670654296875\n",
            "classify 2.0732421875\n",
            "classify 1.9600830078125\n",
            "0.40625\n",
            "0.265625\n",
            "0.234375\n",
            "0.40625\n",
            "0.328125\n",
            "0.375\n",
            "0.3125\n",
            "0.1875\n",
            "0.296875\n",
            "0.234375\n",
            "0.34375\n",
            "time: 3.822376251220703 4.082795063372763\n",
            "846\n",
            "strain 0.24891768395900726\n",
            "strain 0.23671671748161316\n",
            "strain 0.29392656683921814\n",
            "strain 0.29665952920913696\n",
            "strain 0.2819085717201233\n",
            "strain 0.24993649125099182\n",
            "classify 1.960205078125\n",
            "classify 1.9549560546875\n",
            "classify 2.018798828125\n",
            "classify 1.9136962890625\n",
            "classify 1.97845458984375\n",
            "classify 1.9757080078125\n",
            "classify 1.9866943359375\n",
            "classify 2.0157470703125\n",
            "classify 1.938232421875\n",
            "classify 1.9036865234375\n",
            "classify 1.9375\n",
            "0.390625\n",
            "0.296875\n",
            "0.234375\n",
            "0.234375\n",
            "0.265625\n",
            "0.40625\n",
            "0.28125\n",
            "0.25\n",
            "0.265625\n",
            "0.359375\n",
            "0.234375\n",
            "time: 4.4537904262542725 4.083233563649471\n",
            "847\n",
            "strain 0.36708688735961914\n",
            "strain 0.26128190755844116\n",
            "strain 0.20654377341270447\n",
            "strain 0.24062640964984894\n",
            "strain 0.3086979389190674\n",
            "strain 0.22414898872375488\n",
            "classify 2.0284423828125\n",
            "classify 2.02972412109375\n",
            "classify 1.9324951171875\n",
            "classify 2.1046142578125\n",
            "classify 1.925048828125\n",
            "classify 1.99896240234375\n",
            "classify 1.9805908203125\n",
            "classify 1.9073486328125\n",
            "classify 1.9417724609375\n",
            "classify 1.976318359375\n",
            "classify 1.9503173828125\n",
            "0.28125\n",
            "0.28125\n",
            "0.328125\n",
            "0.296875\n",
            "0.28125\n",
            "0.34375\n",
            "0.359375\n",
            "0.328125\n",
            "0.328125\n",
            "0.21875\n",
            "0.375\n",
            "time: 3.8077900409698486 4.082910031923708\n",
            "848\n",
            "strain 0.2253846377134323\n",
            "strain 0.28569507598876953\n",
            "strain 0.2451356202363968\n",
            "strain 0.29788339138031006\n",
            "strain 0.22593948245048523\n",
            "strain 0.30061423778533936\n",
            "classify 2.088134765625\n",
            "classify 1.95782470703125\n",
            "classify 1.9609375\n",
            "classify 1.9608154296875\n",
            "classify 2.01025390625\n",
            "classify 1.839599609375\n",
            "classify 1.9493408203125\n",
            "classify 1.924072265625\n",
            "classify 2.0035400390625\n",
            "classify 1.89044189453125\n",
            "classify 1.9244384765625\n",
            "0.34375\n",
            "0.390625\n",
            "0.328125\n",
            "0.28125\n",
            "0.34375\n",
            "0.328125\n",
            "0.25\n",
            "0.375\n",
            "0.328125\n",
            "0.28125\n",
            "0.328125\n",
            "time: 3.802081346511841 4.082579819979179\n",
            "849\n",
            "strain 0.25482437014579773\n",
            "strain 0.30193984508514404\n",
            "strain 0.23694844543933868\n",
            "strain 0.37263453006744385\n",
            "strain 0.21283788979053497\n",
            "strain 0.3853694796562195\n",
            "classify 1.9566650390625\n",
            "classify 1.981201171875\n",
            "classify 1.9615478515625\n",
            "classify 1.9425048828125\n",
            "classify 1.964599609375\n",
            "classify 1.9183349609375\n",
            "classify 2.0269775390625\n",
            "classify 2.0167236328125\n",
            "classify 2.0166015625\n",
            "classify 1.9674072265625\n",
            "classify 2.00927734375\n",
            "0.328125\n",
            "0.296875\n",
            "0.1875\n",
            "0.328125\n",
            "0.34375\n",
            "0.359375\n",
            "0.296875\n",
            "0.265625\n",
            "0.34375\n",
            "0.265625\n",
            "0.328125\n",
            "time: 4.477146625518799 4.083044359263252\n",
            "850\n",
            "strain 0.29460567235946655\n",
            "strain 0.24701619148254395\n",
            "strain 0.2225009948015213\n",
            "strain 0.2937024235725403\n",
            "strain 0.2617780268192291\n",
            "strain 0.30884823203086853\n",
            "classify 1.9034423828125\n",
            "classify 1.97564697265625\n",
            "classify 1.9932861328125\n",
            "classify 2.02142333984375\n",
            "classify 2.0130615234375\n",
            "classify 1.9671630859375\n",
            "classify 1.990966796875\n",
            "classify 1.930908203125\n",
            "classify 1.923583984375\n",
            "classify 1.966552734375\n",
            "classify 1.907958984375\n",
            "0.296875\n",
            "0.21875\n",
            "0.15625\n",
            "0.296875\n",
            "0.359375\n",
            "0.265625\n",
            "0.3125\n",
            "0.359375\n",
            "0.40625\n",
            "0.3125\n",
            "0.3125\n",
            "time: 3.819830894470215 4.082735616648099\n",
            "851\n",
            "strain 0.2916906774044037\n",
            "strain 0.20906445384025574\n",
            "strain 0.20100003480911255\n",
            "strain 0.2858424782752991\n",
            "strain 0.23870849609375\n",
            "strain 0.2857798933982849\n",
            "classify 2.03057861328125\n",
            "classify 2.0040283203125\n",
            "classify 1.9932861328125\n",
            "classify 1.9984130859375\n",
            "classify 1.8780517578125\n",
            "classify 1.95166015625\n",
            "classify 2.0064697265625\n",
            "classify 1.9481201171875\n",
            "classify 1.9395751953125\n",
            "classify 1.9942626953125\n",
            "classify 2.017822265625\n",
            "0.421875\n",
            "0.34375\n",
            "0.359375\n",
            "0.328125\n",
            "0.234375\n",
            "0.359375\n",
            "0.296875\n",
            "0.34375\n",
            "0.421875\n",
            "0.25\n",
            "0.40625\n",
            "time: 3.868919610977173 4.082485176867722\n",
            "852\n",
            "strain 0.2551507353782654\n",
            "strain 0.21894998848438263\n",
            "strain 0.2486484795808792\n",
            "strain 0.27284303307533264\n",
            "strain 0.2492390275001526\n",
            "strain 0.30296748876571655\n",
            "classify 1.904296875\n",
            "classify 2.0960693359375\n",
            "classify 1.997314453125\n",
            "classify 2.05596923828125\n",
            "classify 1.9619140625\n",
            "classify 1.9140625\n",
            "classify 1.9371337890625\n",
            "classify 2.0625\n",
            "classify 2.0087890625\n",
            "classify 1.94140625\n",
            "classify 1.8951416015625\n",
            "0.296875\n",
            "0.265625\n",
            "0.453125\n",
            "0.21875\n",
            "0.421875\n",
            "0.359375\n",
            "0.25\n",
            "0.3125\n",
            "0.3125\n",
            "0.3125\n",
            "0.28125\n",
            "time: 4.480973243713379 4.08295285575974\n",
            "853\n",
            "strain 0.25619834661483765\n",
            "strain 0.23517479002475739\n",
            "strain 0.20842581987380981\n",
            "strain 0.3709096908569336\n",
            "strain 0.22641825675964355\n",
            "strain 0.244609996676445\n",
            "classify 1.937744140625\n",
            "classify 2.0828857421875\n",
            "classify 1.978271484375\n",
            "classify 1.90380859375\n",
            "classify 1.85888671875\n",
            "classify 2.0194091796875\n",
            "classify 1.96337890625\n",
            "classify 1.9854736328125\n",
            "classify 1.9083251953125\n",
            "classify 1.9652099609375\n",
            "classify 2.0303955078125\n",
            "0.28125\n",
            "0.375\n",
            "0.328125\n",
            "0.328125\n",
            "0.296875\n",
            "0.390625\n",
            "0.3125\n",
            "0.3125\n",
            "0.40625\n",
            "0.203125\n",
            "0.46875\n",
            "time: 3.8494391441345215 4.082679957639976\n",
            "854\n",
            "strain 0.22793401777744293\n",
            "strain 0.2747824490070343\n",
            "strain 0.3216477334499359\n",
            "strain 0.24094517529010773\n",
            "strain 0.30535781383514404\n",
            "strain 0.3060239255428314\n",
            "classify 1.94287109375\n",
            "classify 2.021728515625\n",
            "classify 1.994873046875\n",
            "classify 1.943359375\n",
            "classify 1.87890625\n",
            "classify 2.000732421875\n",
            "classify 1.95068359375\n",
            "classify 2.06005859375\n",
            "classify 2.0018310546875\n",
            "classify 1.92919921875\n",
            "classify 2.08544921875\n",
            "0.265625\n",
            "0.34375\n",
            "0.28125\n",
            "0.25\n",
            "0.296875\n",
            "0.28125\n",
            "0.3125\n",
            "0.265625\n",
            "0.265625\n",
            "0.28125\n",
            "0.296875\n",
            "time: 3.8080785274505615 4.082359322888112\n",
            "855\n",
            "strain 0.2702409625053406\n",
            "strain 0.3390565812587738\n",
            "strain 0.24727940559387207\n",
            "strain 0.29230988025665283\n",
            "strain 0.32579562067985535\n",
            "strain 0.2428542524576187\n",
            "classify 1.90814208984375\n",
            "classify 1.9859619140625\n",
            "classify 1.9627685546875\n",
            "classify 2.080322265625\n",
            "classify 2.10205078125\n",
            "classify 1.9632568359375\n",
            "classify 1.9290771484375\n",
            "classify 1.959228515625\n",
            "classify 2.0263671875\n",
            "classify 1.9794921875\n",
            "classify 1.9813232421875\n",
            "0.390625\n",
            "0.296875\n",
            "0.359375\n",
            "0.328125\n",
            "0.265625\n",
            "0.3125\n",
            "0.390625\n",
            "0.25\n",
            "0.328125\n",
            "0.390625\n",
            "0.28125\n",
            "time: 4.441699743270874 4.082779666530752\n",
            "856\n",
            "strain 0.29108747839927673\n",
            "strain 0.3957013487815857\n",
            "strain 0.23407427966594696\n",
            "strain 0.227362260222435\n",
            "strain 0.3097984194755554\n",
            "strain 0.22419235110282898\n",
            "classify 1.9365234375\n",
            "classify 2.0001220703125\n",
            "classify 2.00567626953125\n",
            "classify 2.0257568359375\n",
            "classify 2.02587890625\n",
            "classify 1.9144287109375\n",
            "classify 2.01318359375\n",
            "classify 2.0164794921875\n",
            "classify 1.912109375\n",
            "classify 1.84521484375\n",
            "classify 2.0185546875\n",
            "0.296875\n",
            "0.3125\n",
            "0.328125\n",
            "0.234375\n",
            "0.3125\n",
            "0.3125\n",
            "0.296875\n",
            "0.234375\n",
            "0.328125\n",
            "0.375\n",
            "0.25\n",
            "time: 3.861785650253296 4.082522296571676\n",
            "857\n",
            "strain 0.3892473876476288\n",
            "strain 0.22120033204555511\n",
            "strain 0.21828599274158478\n",
            "strain 0.30004042387008667\n",
            "strain 0.22166487574577332\n",
            "strain 0.2186010479927063\n",
            "classify 2.025146484375\n",
            "classify 1.94873046875\n",
            "classify 1.9854736328125\n",
            "classify 1.862548828125\n",
            "classify 1.9898681640625\n",
            "classify 2.0599365234375\n",
            "classify 1.9765625\n",
            "classify 2.0101318359375\n",
            "classify 1.9114990234375\n",
            "classify 1.9337158203125\n",
            "classify 2.0528564453125\n",
            "0.234375\n",
            "0.296875\n",
            "0.265625\n",
            "0.328125\n",
            "0.3125\n",
            "0.390625\n",
            "0.328125\n",
            "0.421875\n",
            "0.234375\n",
            "0.34375\n",
            "0.375\n",
            "time: 3.817369222640991 4.082213852233265\n",
            "858\n",
            "strain 0.225228950381279\n",
            "strain 0.2695339620113373\n",
            "strain 0.2080751359462738\n",
            "strain 0.22989924252033234\n",
            "strain 0.20529715716838837\n",
            "strain 0.2674303352832794\n",
            "classify 2.0087890625\n",
            "classify 2.019287109375\n",
            "classify 1.9703369140625\n",
            "classify 1.994873046875\n",
            "classify 1.992431640625\n",
            "classify 1.9501953125\n",
            "classify 2.0191650390625\n",
            "classify 1.93157958984375\n",
            "classify 2.0447998046875\n",
            "classify 1.9515380859375\n",
            "classify 2.0213623046875\n",
            "0.34375\n",
            "0.390625\n",
            "0.3125\n",
            "0.453125\n",
            "0.359375\n",
            "0.25\n",
            "0.25\n",
            "0.28125\n",
            "0.234375\n",
            "0.3125\n",
            "0.28125\n",
            "time: 4.253186225891113 4.082413395014576\n",
            "859\n",
            "strain 0.2209920734167099\n",
            "strain 0.28162598609924316\n",
            "strain 0.2776038646697998\n",
            "strain 0.25833749771118164\n",
            "strain 0.24020586907863617\n",
            "strain 0.22372868657112122\n",
            "classify 1.927734375\n",
            "classify 2.0001220703125\n",
            "classify 1.967529296875\n",
            "classify 1.8597412109375\n",
            "classify 1.957275390625\n",
            "classify 1.9921875\n",
            "classify 1.935302734375\n",
            "classify 2.006591796875\n",
            "classify 1.8809814453125\n",
            "classify 1.9478759765625\n",
            "classify 1.86767578125\n",
            "0.25\n",
            "0.34375\n",
            "0.296875\n",
            "0.3125\n",
            "0.40625\n",
            "0.28125\n",
            "0.34375\n",
            "0.265625\n",
            "0.453125\n",
            "0.421875\n",
            "0.234375\n",
            "time: 4.039075136184692 4.082363631836204\n",
            "860\n",
            "strain 0.20919187366962433\n",
            "strain 0.23673105239868164\n",
            "strain 0.24703386425971985\n",
            "strain 0.25560563802719116\n",
            "strain 0.2608765661716461\n",
            "strain 0.3537938594818115\n",
            "classify 2.007568359375\n",
            "classify 1.98779296875\n",
            "classify 1.976318359375\n",
            "classify 1.9893798828125\n",
            "classify 2.007568359375\n",
            "classify 1.98828125\n",
            "classify 1.94287109375\n",
            "classify 2.0234375\n",
            "classify 2.044921875\n",
            "classify 2.0023193359375\n",
            "classify 2.07763671875\n",
            "0.390625\n",
            "0.375\n",
            "0.28125\n",
            "0.28125\n",
            "0.296875\n",
            "0.296875\n",
            "0.3125\n",
            "0.28125\n",
            "0.296875\n",
            "0.25\n",
            "0.296875\n",
            "time: 3.789368152618408 4.082023828010249\n",
            "861\n",
            "strain 0.2824532091617584\n",
            "strain 0.2375374436378479\n",
            "strain 0.2639116048812866\n",
            "strain 0.2391243577003479\n",
            "strain 0.26716259121894836\n",
            "strain 0.26997941732406616\n",
            "classify 2.0198974609375\n",
            "classify 1.9757080078125\n",
            "classify 1.903564453125\n",
            "classify 1.962158203125\n",
            "classify 2.0726318359375\n",
            "classify 2.0185546875\n",
            "classify 2.0079345703125\n",
            "classify 2.01153564453125\n",
            "classify 2.0582275390625\n",
            "classify 1.979736328125\n",
            "classify 1.9969482421875\n",
            "0.390625\n",
            "0.21875\n",
            "0.3125\n",
            "0.28125\n",
            "0.25\n",
            "0.328125\n",
            "0.265625\n",
            "0.296875\n",
            "0.375\n",
            "0.203125\n",
            "0.34375\n",
            "time: 4.169088363647461 4.082125315810024\n",
            "862\n",
            "strain 0.21290618181228638\n",
            "strain 0.2821651101112366\n",
            "strain 0.2974596917629242\n",
            "strain 0.22625786066055298\n",
            "strain 0.2509324252605438\n",
            "strain 0.21710388362407684\n",
            "classify 1.87548828125\n",
            "classify 2.031982421875\n",
            "classify 1.9759521484375\n",
            "classify 1.968017578125\n",
            "classify 1.9990234375\n",
            "classify 1.96533203125\n",
            "classify 2.0029296875\n",
            "classify 1.8702392578125\n",
            "classify 1.9410400390625\n",
            "classify 1.9056396484375\n",
            "classify 1.9454345703125\n",
            "0.234375\n",
            "0.265625\n",
            "0.390625\n",
            "0.234375\n",
            "0.28125\n",
            "0.265625\n",
            "0.296875\n",
            "0.25\n",
            "0.390625\n",
            "0.25\n",
            "0.234375\n",
            "time: 4.158546686172485 4.082215334835914\n",
            "863\n",
            "strain 0.27836036682128906\n",
            "strain 0.2153756469488144\n",
            "strain 0.2678591012954712\n",
            "strain 0.22383339703083038\n",
            "strain 0.2640188932418823\n",
            "strain 0.23356495797634125\n",
            "classify 1.9388427734375\n",
            "classify 1.9766845703125\n",
            "classify 1.8905029296875\n",
            "classify 1.9058837890625\n",
            "classify 1.961181640625\n",
            "classify 2.0167236328125\n",
            "classify 1.987548828125\n",
            "classify 1.9881591796875\n",
            "classify 1.8594970703125\n",
            "classify 1.9879150390625\n",
            "classify 2.021728515625\n",
            "0.265625\n",
            "0.25\n",
            "0.328125\n",
            "0.203125\n",
            "0.328125\n",
            "0.21875\n",
            "0.203125\n",
            "0.34375\n",
            "0.203125\n",
            "0.21875\n",
            "0.28125\n",
            "time: 3.861830711364746 4.081960799793403\n",
            "864\n",
            "strain 0.2960032522678375\n",
            "strain 0.2798008322715759\n",
            "strain 0.19759516417980194\n",
            "strain 0.22768667340278625\n",
            "strain 0.25815466046333313\n",
            "strain 0.26320549845695496\n",
            "classify 2.04736328125\n",
            "classify 1.95849609375\n",
            "classify 1.9986572265625\n",
            "classify 1.89788818359375\n",
            "classify 1.9815673828125\n",
            "classify 2.0015869140625\n",
            "classify 1.964599609375\n",
            "classify 1.9254150390625\n",
            "classify 1.89996337890625\n",
            "classify 1.9053955078125\n",
            "classify 2.1026611328125\n",
            "0.328125\n",
            "0.328125\n",
            "0.296875\n",
            "0.203125\n",
            "0.25\n",
            "0.375\n",
            "0.21875\n",
            "0.3125\n",
            "0.296875\n",
            "0.3125\n",
            "0.375\n",
            "time: 4.138711214065552 4.082026920153226\n",
            "865\n",
            "strain 0.2379685789346695\n",
            "strain 0.3540630042552948\n",
            "strain 0.23363304138183594\n",
            "strain 0.29851385951042175\n",
            "strain 0.28899383544921875\n",
            "strain 0.21636679768562317\n",
            "classify 2.033447265625\n",
            "classify 1.986572265625\n",
            "classify 2.0816650390625\n",
            "classify 2.005126953125\n",
            "classify 2.0616455078125\n",
            "classify 2.03173828125\n",
            "classify 1.9761962890625\n",
            "classify 1.947998046875\n",
            "classify 1.8802490234375\n",
            "classify 1.990478515625\n",
            "classify 1.9610595703125\n",
            "0.28125\n",
            "0.25\n",
            "0.3125\n",
            "0.28125\n",
            "0.34375\n",
            "0.375\n",
            "0.3125\n",
            "0.328125\n",
            "0.328125\n",
            "0.203125\n",
            "0.40625\n",
            "time: 4.258387804031372 4.082231057158244\n",
            "866\n",
            "strain 0.2566894292831421\n",
            "strain 0.21528302133083344\n",
            "strain 0.20681904256343842\n",
            "strain 0.36585551500320435\n",
            "strain 0.227548286318779\n",
            "strain 0.22061417996883392\n",
            "classify 1.96343994140625\n",
            "classify 1.98291015625\n",
            "classify 1.99200439453125\n",
            "classify 1.9410400390625\n",
            "classify 1.989990234375\n",
            "classify 1.9188232421875\n",
            "classify 1.9686279296875\n",
            "classify 1.939208984375\n",
            "classify 1.8673095703125\n",
            "classify 1.91131591796875\n",
            "classify 2.0301513671875\n",
            "0.296875\n",
            "0.296875\n",
            "0.3125\n",
            "0.453125\n",
            "0.40625\n",
            "0.375\n",
            "0.234375\n",
            "0.296875\n",
            "0.296875\n",
            "0.234375\n",
            "0.28125\n",
            "time: 3.8161113262176514 4.081924743443349\n",
            "867\n",
            "strain 0.2820911407470703\n",
            "strain 0.2839897871017456\n",
            "strain 0.2381068468093872\n",
            "strain 0.2831980288028717\n",
            "strain 0.23838621377944946\n",
            "strain 0.32878783345222473\n",
            "classify 1.966796875\n",
            "classify 2.0174560546875\n",
            "classify 1.948486328125\n",
            "classify 2.0169677734375\n",
            "classify 1.9505615234375\n",
            "classify 2.0511474609375\n",
            "classify 1.898681640625\n",
            "classify 2.0257568359375\n",
            "classify 1.937744140625\n",
            "classify 1.9072265625\n",
            "classify 1.9864501953125\n",
            "0.265625\n",
            "0.3125\n",
            "0.28125\n",
            "0.3125\n",
            "0.375\n",
            "0.375\n",
            "0.34375\n",
            "0.28125\n",
            "0.328125\n",
            "0.296875\n",
            "0.28125\n",
            "time: 4.0394909381866455 4.0818764042744435\n",
            "868\n",
            "strain 0.2274845689535141\n",
            "strain 0.31070953607559204\n",
            "strain 0.2842956483364105\n",
            "strain 0.3119933605194092\n",
            "strain 0.2494390308856964\n",
            "strain 0.2861100137233734\n",
            "classify 1.988037109375\n",
            "classify 1.9420166015625\n",
            "classify 1.995361328125\n",
            "classify 2.0008544921875\n",
            "classify 1.9713134765625\n",
            "classify 1.9169921875\n",
            "classify 1.947021484375\n",
            "classify 1.9610595703125\n",
            "classify 1.955810546875\n",
            "classify 1.96923828125\n",
            "classify 1.9813232421875\n",
            "0.21875\n",
            "0.40625\n",
            "0.375\n",
            "0.1875\n",
            "0.328125\n",
            "0.25\n",
            "0.234375\n",
            "0.3125\n",
            "0.3125\n",
            "0.234375\n",
            "0.265625\n",
            "time: 4.3045642375946045 4.082134262190315\n",
            "869\n",
            "strain 0.2850773334503174\n",
            "strain 0.23468425869941711\n",
            "strain 0.28069010376930237\n",
            "strain 0.2873547077178955\n",
            "strain 0.3571421802043915\n",
            "strain 0.21518434584140778\n",
            "classify 1.9508056640625\n",
            "classify 2.011962890625\n",
            "classify 2.08203125\n",
            "classify 2.0074462890625\n",
            "classify 1.9459228515625\n",
            "classify 1.9776611328125\n",
            "classify 2.0323486328125\n",
            "classify 1.9415283203125\n",
            "classify 1.9656982421875\n",
            "classify 1.944091796875\n",
            "classify 2.0003662109375\n",
            "0.375\n",
            "0.375\n",
            "0.265625\n",
            "0.359375\n",
            "0.265625\n",
            "0.203125\n",
            "0.421875\n",
            "0.265625\n",
            "0.328125\n",
            "0.28125\n",
            "0.234375\n",
            "time: 3.7831103801727295 4.081791069589812\n",
            "870\n",
            "strain 0.22253496944904327\n",
            "strain 0.23361583054065704\n",
            "strain 0.3372221887111664\n",
            "strain 0.22943738102912903\n",
            "strain 0.27940839529037476\n",
            "strain 0.28211572766304016\n",
            "classify 1.968505859375\n",
            "classify 2.0250244140625\n",
            "classify 1.9171142578125\n",
            "classify 1.9764404296875\n",
            "classify 1.8631591796875\n",
            "classify 2.0386962890625\n",
            "classify 1.931884765625\n",
            "classify 2.0157470703125\n",
            "classify 1.9268798828125\n",
            "classify 1.990478515625\n",
            "classify 1.9163818359375\n",
            "0.328125\n",
            "0.296875\n",
            "0.328125\n",
            "0.15625\n",
            "0.28125\n",
            "0.34375\n",
            "0.296875\n",
            "0.296875\n",
            "0.265625\n",
            "0.296875\n",
            "0.3125\n",
            "time: 3.9417428970336914 4.08163082065867\n",
            "871\n",
            "strain 0.3190159797668457\n",
            "strain 0.26045650243759155\n",
            "strain 0.22084684669971466\n",
            "strain 0.2275896668434143\n",
            "strain 0.35734423995018005\n",
            "strain 0.23017358779907227\n",
            "classify 2.0765380859375\n",
            "classify 1.90643310546875\n",
            "classify 2.00445556640625\n",
            "classify 1.964111328125\n",
            "classify 1.957763671875\n",
            "classify 2.066162109375\n",
            "classify 1.9801025390625\n",
            "classify 2.016845703125\n",
            "classify 1.9390869140625\n",
            "classify 1.93011474609375\n",
            "classify 1.946044921875\n",
            "0.3125\n",
            "0.421875\n",
            "0.234375\n",
            "0.3125\n",
            "0.3125\n",
            "0.265625\n",
            "0.34375\n",
            "0.34375\n",
            "0.265625\n",
            "0.171875\n",
            "0.40625\n",
            "time: 4.382305383682251 4.0819766433960805\n",
            "872\n",
            "strain 0.22969084978103638\n",
            "strain 0.26732343435287476\n",
            "strain 0.3040786385536194\n",
            "strain 0.2960669994354248\n",
            "strain 0.22197474539279938\n",
            "strain 0.3027603328227997\n",
            "classify 2.04052734375\n",
            "classify 1.95751953125\n",
            "classify 2.0047607421875\n",
            "classify 2.0225830078125\n",
            "classify 2.020263671875\n",
            "classify 2.0546875\n",
            "classify 2.031494140625\n",
            "classify 2.095458984375\n",
            "classify 2.031494140625\n",
            "classify 1.90576171875\n",
            "classify 2.0008544921875\n",
            "0.296875\n",
            "0.265625\n",
            "0.265625\n",
            "0.375\n",
            "0.328125\n",
            "0.328125\n",
            "0.328125\n",
            "0.234375\n",
            "0.390625\n",
            "0.265625\n",
            "0.25\n",
            "time: 4.157262563705444 4.082063437054384\n",
            "873\n",
            "strain 0.2185804843902588\n",
            "strain 0.3002314865589142\n",
            "strain 0.2642076909542084\n",
            "strain 0.3044956922531128\n",
            "strain 0.24658162891864777\n",
            "strain 0.3097163736820221\n",
            "classify 1.9556884765625\n",
            "classify 1.9365234375\n",
            "classify 1.943359375\n",
            "classify 1.9063720703125\n",
            "classify 1.97021484375\n",
            "classify 1.9510498046875\n",
            "classify 2.0799560546875\n",
            "classify 1.9298095703125\n",
            "classify 1.9974365234375\n",
            "classify 1.9482421875\n",
            "classify 2.046630859375\n",
            "0.375\n",
            "0.265625\n",
            "0.28125\n",
            "0.453125\n",
            "0.296875\n",
            "0.328125\n",
            "0.375\n",
            "0.234375\n",
            "0.34375\n",
            "0.34375\n",
            "0.265625\n",
            "time: 4.010850667953491 4.081982483307339\n",
            "874\n",
            "strain 0.24675700068473816\n",
            "strain 0.2421550303697586\n",
            "strain 0.24135835468769073\n",
            "strain 0.2054501324892044\n",
            "strain 0.2843928337097168\n",
            "strain 0.28634241223335266\n",
            "classify 2.0689697265625\n",
            "classify 1.997314453125\n",
            "classify 2.0726318359375\n",
            "classify 2.0638427734375\n",
            "classify 1.9599609375\n",
            "classify 1.8795166015625\n",
            "classify 2.0804443359375\n",
            "classify 2.03662109375\n",
            "classify 2.0145263671875\n",
            "classify 1.927734375\n",
            "classify 1.9635009765625\n",
            "0.296875\n",
            "0.28125\n",
            "0.296875\n",
            "0.28125\n",
            "0.25\n",
            "0.328125\n",
            "0.328125\n",
            "0.3125\n",
            "0.34375\n",
            "0.265625\n",
            "0.265625\n",
            "time: 4.4631242752075195 4.082419180733817\n",
            "875\n",
            "strain 0.25627025961875916\n",
            "strain 0.22506186366081238\n",
            "strain 0.20810839533805847\n",
            "strain 0.24625535309314728\n",
            "strain 0.2526189088821411\n",
            "strain 0.25695323944091797\n",
            "classify 1.9959716796875\n",
            "classify 2.0555419921875\n",
            "classify 1.947021484375\n",
            "classify 1.963623046875\n",
            "classify 1.964111328125\n",
            "classify 2.0050048828125\n",
            "classify 2.0135498046875\n",
            "classify 1.8636474609375\n",
            "classify 2.0155029296875\n",
            "classify 1.978515625\n",
            "classify 1.9951171875\n",
            "0.25\n",
            "0.234375\n",
            "0.34375\n",
            "0.34375\n",
            "0.328125\n",
            "0.34375\n",
            "0.3125\n",
            "0.296875\n",
            "0.296875\n",
            "0.34375\n",
            "0.28125\n",
            "time: 3.8766531944274902 4.082184877841985\n",
            "876\n",
            "strain 0.21444562077522278\n",
            "strain 0.27380937337875366\n",
            "strain 0.22638355195522308\n",
            "strain 0.2808939218521118\n",
            "strain 0.24152016639709473\n",
            "strain 0.2606121897697449\n",
            "classify 1.9910888671875\n",
            "classify 1.9791259765625\n",
            "classify 1.91180419921875\n",
            "classify 1.9154052734375\n",
            "classify 1.8270263671875\n",
            "classify 1.962158203125\n",
            "classify 1.9178466796875\n",
            "classify 2.0179443359375\n",
            "classify 2.0496826171875\n",
            "classify 1.9840087890625\n",
            "classify 1.86395263671875\n",
            "0.375\n",
            "0.3125\n",
            "0.3125\n",
            "0.359375\n",
            "0.3125\n",
            "0.359375\n",
            "0.34375\n",
            "0.328125\n",
            "0.3125\n",
            "0.234375\n",
            "0.421875\n",
            "time: 4.003044605255127 4.082095196200886\n",
            "877\n",
            "strain 0.3598299026489258\n",
            "strain 0.25611412525177\n",
            "strain 0.25249627232551575\n",
            "strain 0.3150644302368164\n",
            "strain 0.20352891087532043\n",
            "strain 0.28386759757995605\n",
            "classify 2.0255126953125\n",
            "classify 1.96484375\n",
            "classify 1.9437255859375\n",
            "classify 1.9818115234375\n",
            "classify 2.050537109375\n",
            "classify 1.962158203125\n",
            "classify 2.040771484375\n",
            "classify 1.93212890625\n",
            "classify 2.1036376953125\n",
            "classify 2.0162353515625\n",
            "classify 1.9178466796875\n",
            "0.28125\n",
            "0.4375\n",
            "0.359375\n",
            "0.375\n",
            "0.203125\n",
            "0.375\n",
            "0.359375\n",
            "0.28125\n",
            "0.34375\n",
            "0.34375\n",
            "0.390625\n",
            "time: 4.390500545501709 4.082447545131953\n",
            "878\n",
            "strain 0.2636643350124359\n",
            "strain 0.23629339039325714\n",
            "strain 0.35616159439086914\n",
            "strain 0.3136155307292938\n",
            "strain 0.2580375373363495\n",
            "strain 0.2435130625963211\n",
            "classify 1.8612060546875\n",
            "classify 2.021728515625\n",
            "classify 1.9388427734375\n",
            "classify 2.009521484375\n",
            "classify 1.92706298828125\n",
            "classify 2.044677734375\n",
            "classify 1.9139404296875\n",
            "classify 1.9093017578125\n",
            "classify 2.0179443359375\n",
            "classify 1.919921875\n",
            "classify 2.046875\n",
            "0.25\n",
            "0.28125\n",
            "0.390625\n",
            "0.28125\n",
            "0.265625\n",
            "0.3125\n",
            "0.34375\n",
            "0.375\n",
            "0.203125\n",
            "0.203125\n",
            "0.5\n",
            "time: 3.8472676277160645 4.082180478602682\n",
            "879\n",
            "strain 0.2617669701576233\n",
            "strain 0.23477904498577118\n",
            "strain 0.28162825107574463\n",
            "strain 0.3425792157649994\n",
            "strain 0.21602599322795868\n",
            "strain 0.21779917180538177\n",
            "classify 1.9962158203125\n",
            "classify 1.8824462890625\n",
            "classify 1.98974609375\n",
            "classify 1.9736328125\n",
            "classify 2.009521484375\n",
            "classify 1.921630859375\n",
            "classify 2.0126953125\n",
            "classify 2.0654296875\n",
            "classify 1.98779296875\n",
            "classify 1.953857421875\n",
            "classify 1.922607421875\n",
            "0.296875\n",
            "0.234375\n",
            "0.21875\n",
            "0.34375\n",
            "0.203125\n",
            "0.265625\n",
            "0.25\n",
            "0.375\n",
            "0.328125\n",
            "0.296875\n",
            "0.328125\n",
            "time: 3.8955483436584473 4.081968879157847\n",
            "880\n",
            "strain 0.2751748859882355\n",
            "strain 0.3131229281425476\n",
            "strain 0.263136625289917\n",
            "strain 0.32182401418685913\n",
            "strain 0.25084128975868225\n",
            "strain 0.30576568841934204\n",
            "classify 1.8507080078125\n",
            "classify 1.9659423828125\n",
            "classify 1.931640625\n",
            "classify 1.96630859375\n",
            "classify 1.950927734375\n",
            "classify 2.013916015625\n",
            "classify 1.924560546875\n",
            "classify 1.8699951171875\n",
            "classify 1.954833984375\n",
            "classify 1.9217529296875\n",
            "classify 1.98492431640625\n",
            "0.21875\n",
            "0.265625\n",
            "0.453125\n",
            "0.3125\n",
            "0.265625\n",
            "0.3125\n",
            "0.3125\n",
            "0.328125\n",
            "0.375\n",
            "0.34375\n",
            "0.40625\n",
            "time: 4.423586130142212 4.082357711607969\n",
            "881\n",
            "strain 0.23980951309204102\n",
            "strain 0.2969224452972412\n",
            "strain 0.24108248949050903\n",
            "strain 0.23466749489307404\n",
            "strain 0.3638908863067627\n",
            "strain 0.217241108417511\n",
            "classify 2.03955078125\n",
            "classify 1.9317626953125\n",
            "classify 1.92529296875\n",
            "classify 2.00958251953125\n",
            "classify 1.9375\n",
            "classify 2.0413818359375\n",
            "classify 1.9398193359375\n",
            "classify 1.939697265625\n",
            "classify 1.9947509765625\n",
            "classify 1.911376953125\n",
            "classify 1.952880859375\n",
            "0.3125\n",
            "0.421875\n",
            "0.375\n",
            "0.25\n",
            "0.265625\n",
            "0.25\n",
            "0.3125\n",
            "0.28125\n",
            "0.28125\n",
            "0.375\n",
            "0.3125\n",
            "time: 4.514633655548096 4.0828484915822\n",
            "882\n",
            "strain 0.2650282680988312\n",
            "strain 0.2611370086669922\n",
            "strain 0.26434266567230225\n",
            "strain 0.2856433391571045\n",
            "strain 0.2940334677696228\n",
            "strain 0.23122180998325348\n",
            "classify 1.964599609375\n",
            "classify 1.9151611328125\n",
            "classify 2.0087890625\n",
            "classify 1.9334716796875\n",
            "classify 1.88427734375\n",
            "classify 1.9478759765625\n",
            "classify 2.048095703125\n",
            "classify 1.88525390625\n",
            "classify 2.094482421875\n",
            "classify 1.9840087890625\n",
            "classify 1.982177734375\n",
            "0.375\n",
            "0.34375\n",
            "0.390625\n",
            "0.25\n",
            "0.1875\n",
            "0.359375\n",
            "0.359375\n",
            "0.328125\n",
            "0.40625\n",
            "0.296875\n",
            "0.328125\n",
            "time: 3.9376060962677 4.082684498546078\n",
            "883\n",
            "strain 0.30923980474472046\n",
            "strain 0.3844667673110962\n",
            "strain 0.34630727767944336\n",
            "strain 0.22301806509494781\n",
            "strain 0.22164678573608398\n",
            "strain 0.28639930486679077\n",
            "classify 1.99615478515625\n",
            "classify 1.962158203125\n",
            "classify 1.9847412109375\n",
            "classify 1.9039306640625\n",
            "classify 1.9617919921875\n",
            "classify 2.017822265625\n",
            "classify 1.8900146484375\n",
            "classify 1.8741455078125\n",
            "classify 1.9754638671875\n",
            "classify 1.9876708984375\n",
            "classify 1.924560546875\n",
            "0.234375\n",
            "0.375\n",
            "0.296875\n",
            "0.265625\n",
            "0.21875\n",
            "0.421875\n",
            "0.34375\n",
            "0.375\n",
            "0.359375\n",
            "0.234375\n",
            "0.390625\n",
            "time: 4.3225998878479 4.08295689233288\n",
            "884\n",
            "strain 0.2593095004558563\n",
            "strain 0.25869879126548767\n",
            "strain 0.354277104139328\n",
            "strain 0.25668102502822876\n",
            "strain 0.3207870423793793\n",
            "strain 0.21626725792884827\n",
            "classify 1.9287109375\n",
            "classify 2.0423583984375\n",
            "classify 1.9576416015625\n",
            "classify 1.9417724609375\n",
            "classify 1.9771728515625\n",
            "classify 1.92706298828125\n",
            "classify 1.93505859375\n",
            "classify 1.9698486328125\n",
            "classify 2.0235595703125\n",
            "classify 2.044677734375\n",
            "classify 1.98138427734375\n",
            "0.265625\n",
            "0.28125\n",
            "0.328125\n",
            "0.21875\n",
            "0.296875\n",
            "0.25\n",
            "0.328125\n",
            "0.375\n",
            "0.21875\n",
            "0.328125\n",
            "0.265625\n",
            "time: 3.7885918617248535 4.08262478941578\n",
            "885\n",
            "strain 0.23814190924167633\n",
            "strain 0.21630766987800598\n",
            "strain 0.3304148316383362\n",
            "strain 0.23174479603767395\n",
            "strain 0.2037259191274643\n",
            "strain 0.2536088228225708\n",
            "classify 1.934814453125\n",
            "classify 1.9173583984375\n",
            "classify 1.8477783203125\n",
            "classify 2.001708984375\n",
            "classify 1.88568115234375\n",
            "classify 1.9686279296875\n",
            "classify 1.91748046875\n",
            "classify 1.9061279296875\n",
            "classify 1.9466552734375\n",
            "classify 1.942626953125\n",
            "classify 2.052490234375\n",
            "0.359375\n",
            "0.328125\n",
            "0.34375\n",
            "0.375\n",
            "0.25\n",
            "0.375\n",
            "0.296875\n",
            "0.3125\n",
            "0.28125\n",
            "0.328125\n",
            "0.3125\n",
            "time: 3.876858949661255 4.082393061764892\n",
            "886\n",
            "strain 0.29893770813941956\n",
            "strain 0.2878649830818176\n",
            "strain 0.24165357649326324\n",
            "strain 0.29038840532302856\n",
            "strain 0.2730863690376282\n",
            "strain 0.23434443771839142\n",
            "classify 1.989501953125\n",
            "classify 1.974609375\n",
            "classify 1.953125\n",
            "classify 1.904296875\n",
            "classify 1.9676513671875\n",
            "classify 2.0064697265625\n",
            "classify 1.8753662109375\n",
            "classify 1.953857421875\n",
            "classify 2.0260009765625\n",
            "classify 2.0750732421875\n",
            "classify 2.0113525390625\n",
            "0.265625\n",
            "0.40625\n",
            "0.296875\n",
            "0.328125\n",
            "0.25\n",
            "0.375\n",
            "0.21875\n",
            "0.34375\n",
            "0.296875\n",
            "0.171875\n",
            "0.375\n",
            "time: 4.385592937469482 4.082736170950441\n",
            "887\n",
            "strain 0.19484834372997284\n",
            "strain 0.24270455539226532\n",
            "strain 0.2955501079559326\n",
            "strain 0.3532254695892334\n",
            "strain 0.28278717398643494\n",
            "strain 0.2206006497144699\n",
            "classify 2.029296875\n",
            "classify 1.961669921875\n",
            "classify 1.8494873046875\n",
            "classify 2.0242919921875\n",
            "classify 1.9261474609375\n",
            "classify 1.982666015625\n",
            "classify 1.91943359375\n",
            "classify 1.9217529296875\n",
            "classify 2.0167236328125\n",
            "classify 1.899658203125\n",
            "classify 1.97265625\n",
            "0.3125\n",
            "0.421875\n",
            "0.375\n",
            "0.328125\n",
            "0.296875\n",
            "0.328125\n",
            "0.328125\n",
            "0.234375\n",
            "0.390625\n",
            "0.390625\n",
            "0.265625\n",
            "time: 3.855578660964966 4.08248087468448\n",
            "888\n",
            "strain 0.28005048632621765\n",
            "strain 0.2667013108730316\n",
            "strain 0.2703402042388916\n",
            "strain 0.25143900513648987\n",
            "strain 0.2220473438501358\n",
            "strain 0.22005295753479004\n",
            "classify 1.9022216796875\n",
            "classify 2.013916015625\n",
            "classify 1.94378662109375\n",
            "classify 2.030517578125\n",
            "classify 2.0377197265625\n",
            "classify 2.03564453125\n",
            "classify 1.9315185546875\n",
            "classify 2.04931640625\n",
            "classify 1.9107666015625\n",
            "classify 1.96337890625\n",
            "classify 1.9254150390625\n",
            "0.390625\n",
            "0.296875\n",
            "0.296875\n",
            "0.21875\n",
            "0.21875\n",
            "0.375\n",
            "0.25\n",
            "0.296875\n",
            "0.28125\n",
            "0.375\n",
            "0.25\n",
            "time: 3.8088760375976562 4.082173439461415\n",
            "889\n",
            "strain 0.21502292156219482\n",
            "strain 0.28245145082473755\n",
            "strain 0.37056490778923035\n",
            "strain 0.21336843073368073\n",
            "strain 0.22157099843025208\n",
            "strain 0.31293702125549316\n",
            "classify 1.8604736328125\n",
            "classify 1.950927734375\n",
            "classify 1.9835205078125\n",
            "classify 1.9361572265625\n",
            "classify 2.037841796875\n",
            "classify 1.830322265625\n",
            "classify 1.9315185546875\n",
            "classify 1.9415283203125\n",
            "classify 1.9808349609375\n",
            "classify 1.8385009765625\n",
            "classify 1.97119140625\n",
            "0.3125\n",
            "0.34375\n",
            "0.21875\n",
            "0.28125\n",
            "0.46875\n",
            "0.34375\n",
            "0.34375\n",
            "0.359375\n",
            "0.390625\n",
            "0.296875\n",
            "0.34375\n",
            "time: 4.459157943725586 4.082597550381435\n",
            "890\n",
            "strain 0.2422182857990265\n",
            "strain 0.27783745527267456\n",
            "strain 0.22741109132766724\n",
            "strain 0.22654278576374054\n",
            "strain 0.26838934421539307\n",
            "strain 0.2759394645690918\n",
            "classify 1.83868408203125\n",
            "classify 2.0198974609375\n",
            "classify 1.9744873046875\n",
            "classify 1.9495849609375\n",
            "classify 1.9013671875\n",
            "classify 2.00927734375\n",
            "classify 1.9412841796875\n",
            "classify 1.98553466796875\n",
            "classify 2.068603515625\n",
            "classify 1.8753662109375\n",
            "classify 1.9732666015625\n",
            "0.296875\n",
            "0.28125\n",
            "0.3125\n",
            "0.390625\n",
            "0.296875\n",
            "0.359375\n",
            "0.40625\n",
            "0.3125\n",
            "0.421875\n",
            "0.34375\n",
            "0.328125\n",
            "time: 3.8082175254821777 4.08229048484905\n",
            "891\n",
            "strain 0.25647181272506714\n",
            "strain 0.3030683994293213\n",
            "strain 0.2624549865722656\n",
            "strain 0.29835590720176697\n",
            "strain 0.22152462601661682\n",
            "strain 0.2252674400806427\n",
            "classify 2.04345703125\n",
            "classify 2.011962890625\n",
            "classify 2.0687255859375\n",
            "classify 2.047119140625\n",
            "classify 1.8831787109375\n",
            "classify 2.0247802734375\n",
            "classify 1.9737548828125\n",
            "classify 1.9825439453125\n",
            "classify 1.986328125\n",
            "classify 1.96746826171875\n",
            "classify 1.9638671875\n",
            "0.3125\n",
            "0.265625\n",
            "0.359375\n",
            "0.328125\n",
            "0.40625\n",
            "0.375\n",
            "0.375\n",
            "0.265625\n",
            "0.328125\n",
            "0.234375\n",
            "0.234375\n",
            "time: 3.817347526550293 4.081993989078454\n",
            "892\n",
            "strain 0.26250573992729187\n",
            "strain 0.2630176842212677\n",
            "strain 0.2623955011367798\n",
            "strain 0.21425558626651764\n",
            "strain 0.20268656313419342\n",
            "strain 0.229717418551445\n",
            "classify 1.9781494140625\n",
            "classify 1.9107666015625\n",
            "classify 1.9951171875\n",
            "classify 1.930419921875\n",
            "classify 1.962890625\n",
            "classify 1.9033203125\n",
            "classify 1.9482421875\n",
            "classify 1.9649658203125\n",
            "classify 1.9697265625\n",
            "classify 1.957763671875\n",
            "classify 1.9766845703125\n",
            "0.390625\n",
            "0.3125\n",
            "0.328125\n",
            "0.3125\n",
            "0.34375\n",
            "0.234375\n",
            "0.28125\n",
            "0.25\n",
            "0.328125\n",
            "0.359375\n",
            "0.328125\n",
            "time: 4.476964950561523 4.082436814687287\n",
            "893\n",
            "strain 0.3309480547904968\n",
            "strain 0.266529381275177\n",
            "strain 0.2285126894712448\n",
            "strain 0.27334022521972656\n",
            "strain 0.2429058849811554\n",
            "strain 0.24698492884635925\n",
            "classify 1.96240234375\n",
            "classify 2.156982421875\n",
            "classify 1.9666748046875\n",
            "classify 1.9384765625\n",
            "classify 1.9901123046875\n",
            "classify 1.9617919921875\n",
            "classify 1.9720458984375\n",
            "classify 1.9598388671875\n",
            "classify 1.939697265625\n",
            "classify 2.01312255859375\n",
            "classify 1.97149658203125\n",
            "0.359375\n",
            "0.3125\n",
            "0.375\n",
            "0.34375\n",
            "0.28125\n",
            "0.25\n",
            "0.390625\n",
            "0.25\n",
            "0.375\n",
            "0.359375\n",
            "0.34375\n",
            "time: 3.90927791595459 4.0822436276164895\n",
            "894\n",
            "strain 0.2242862731218338\n",
            "strain 0.3969396948814392\n",
            "strain 0.2872868478298187\n",
            "strain 0.28729718923568726\n",
            "strain 0.3236899971961975\n",
            "strain 0.22095566987991333\n",
            "classify 2.0494384765625\n",
            "classify 1.9769287109375\n",
            "classify 2.0064697265625\n",
            "classify 1.8759765625\n",
            "classify 2.0166015625\n",
            "classify 1.9163818359375\n",
            "classify 2.064208984375\n",
            "classify 2.0513916015625\n",
            "classify 2.0321044921875\n",
            "classify 1.90771484375\n",
            "classify 1.9500732421875\n",
            "0.28125\n",
            "0.234375\n",
            "0.484375\n",
            "0.421875\n",
            "0.375\n",
            "0.3125\n",
            "0.40625\n",
            "0.296875\n",
            "0.28125\n",
            "0.359375\n",
            "0.234375\n",
            "time: 3.90081524848938 4.082041464704375\n",
            "895\n",
            "strain 0.36910754442214966\n",
            "strain 0.23845764994621277\n",
            "strain 0.1943645030260086\n",
            "strain 0.3300316333770752\n",
            "strain 0.23096343874931335\n",
            "strain 0.30433928966522217\n",
            "classify 1.927734375\n",
            "classify 1.99212646484375\n",
            "classify 1.96142578125\n",
            "classify 2.0684814453125\n",
            "classify 2.0126953125\n",
            "classify 1.9844970703125\n",
            "classify 1.9556884765625\n",
            "classify 1.99627685546875\n",
            "classify 1.934814453125\n",
            "classify 2.0029296875\n",
            "classify 1.99053955078125\n",
            "0.359375\n",
            "0.34375\n",
            "0.3125\n",
            "0.4375\n",
            "0.375\n",
            "0.34375\n",
            "0.359375\n",
            "0.25\n",
            "0.25\n",
            "0.421875\n",
            "0.3125\n",
            "time: 4.509358882904053 4.082518908594336\n",
            "896\n",
            "strain 0.27696549892425537\n",
            "strain 0.32614123821258545\n",
            "strain 0.22632142901420593\n",
            "strain 0.26705601811408997\n",
            "strain 0.32641321420669556\n",
            "strain 0.22720175981521606\n",
            "classify 2.0670166015625\n",
            "classify 1.9599609375\n",
            "classify 1.9154052734375\n",
            "classify 1.9549560546875\n",
            "classify 1.9473876953125\n",
            "classify 1.8917236328125\n",
            "classify 2.005126953125\n",
            "classify 1.9622802734375\n",
            "classify 1.9398193359375\n",
            "classify 1.9495849609375\n",
            "classify 1.9554443359375\n",
            "0.28125\n",
            "0.28125\n",
            "0.28125\n",
            "0.265625\n",
            "0.359375\n",
            "0.1875\n",
            "0.359375\n",
            "0.265625\n",
            "0.3125\n",
            "0.296875\n",
            "0.328125\n",
            "time: 3.8750030994415283 4.082288199576778\n",
            "897\n",
            "strain 0.36706265807151794\n",
            "strain 0.21160821616649628\n",
            "strain 0.293465256690979\n",
            "strain 0.22948023676872253\n",
            "strain 0.22858288884162903\n",
            "strain 0.22845454514026642\n",
            "classify 1.9842529296875\n",
            "classify 1.921142578125\n",
            "classify 1.9801025390625\n",
            "classify 1.93328857421875\n",
            "classify 1.98126220703125\n",
            "classify 1.9752197265625\n",
            "classify 1.9542236328125\n",
            "classify 2.009033203125\n",
            "classify 1.9586181640625\n",
            "classify 1.9517822265625\n",
            "classify 2.02178955078125\n",
            "0.34375\n",
            "0.265625\n",
            "0.28125\n",
            "0.28125\n",
            "0.328125\n",
            "0.1875\n",
            "0.203125\n",
            "0.34375\n",
            "0.328125\n",
            "0.375\n",
            "0.296875\n",
            "time: 3.7860424518585205 4.0819587755309445\n",
            "898\n",
            "strain 0.24325884878635406\n",
            "strain 0.2147005796432495\n",
            "strain 0.22162920236587524\n",
            "strain 0.21673519909381866\n",
            "strain 0.2708366811275482\n",
            "strain 0.20015393197536469\n",
            "classify 2.046630859375\n",
            "classify 2.0078125\n",
            "classify 1.909423828125\n",
            "classify 2.02294921875\n",
            "classify 2.0057373046875\n",
            "classify 1.9761962890625\n",
            "classify 1.9898681640625\n",
            "classify 2.0511474609375\n",
            "classify 2.0921630859375\n",
            "classify 1.8740234375\n",
            "classify 1.9029541015625\n",
            "0.390625\n",
            "0.34375\n",
            "0.40625\n",
            "0.375\n",
            "0.28125\n",
            "0.3125\n",
            "0.359375\n",
            "0.34375\n",
            "0.296875\n",
            "0.328125\n",
            "0.359375\n",
            "time: 4.461722135543823 4.082381738571489\n",
            "899\n",
            "strain 0.23206393420696259\n",
            "strain 0.25194185972213745\n",
            "strain 0.3198276162147522\n",
            "strain 0.2668026089668274\n",
            "strain 0.2954390048980713\n",
            "strain 0.28611814975738525\n",
            "classify 2.0089111328125\n",
            "classify 1.9490966796875\n",
            "classify 1.932861328125\n",
            "classify 1.9364013671875\n",
            "classify 1.966064453125\n",
            "classify 1.9403076171875\n",
            "classify 1.9742431640625\n",
            "classify 1.8779296875\n",
            "classify 1.9622802734375\n",
            "classify 1.966552734375\n",
            "classify 1.9620361328125\n",
            "0.375\n",
            "0.21875\n",
            "0.375\n",
            "0.390625\n",
            "0.359375\n",
            "0.3125\n",
            "0.15625\n",
            "0.296875\n",
            "0.375\n",
            "0.296875\n",
            "0.359375\n",
            "time: 3.8164114952087402 4.082086698744032\n",
            "900\n",
            "strain 0.28313836455345154\n",
            "strain 0.339876651763916\n",
            "strain 0.27013900876045227\n",
            "strain 0.26871681213378906\n",
            "strain 0.3303977847099304\n",
            "strain 0.3087596595287323\n",
            "classify 1.8558349609375\n",
            "classify 1.864501953125\n",
            "classify 1.9813232421875\n",
            "classify 1.9022216796875\n",
            "classify 1.9698486328125\n",
            "classify 1.9185791015625\n",
            "classify 2.038330078125\n",
            "classify 1.97308349609375\n",
            "classify 2.0111083984375\n",
            "classify 1.9229736328125\n",
            "classify 1.9853515625\n",
            "0.203125\n",
            "0.40625\n",
            "0.265625\n",
            "0.265625\n",
            "0.375\n",
            "0.28125\n",
            "0.34375\n",
            "0.3125\n",
            "0.3125\n",
            "0.359375\n",
            "0.34375\n",
            "time: 3.8098666667938232 4.081785071570918\n",
            "901\n",
            "strain 0.2641521394252777\n",
            "strain 0.2880755066871643\n",
            "strain 0.24871599674224854\n",
            "strain 0.20554420351982117\n",
            "strain 0.259958952665329\n",
            "strain 0.25306496024131775\n",
            "classify 2.065673828125\n",
            "classify 2.016357421875\n",
            "classify 1.9593505859375\n",
            "classify 2.0013427734375\n",
            "classify 2.0081787109375\n",
            "classify 1.982421875\n",
            "classify 2.03515625\n",
            "classify 1.882568359375\n",
            "classify 1.874267578125\n",
            "classify 1.94000244140625\n",
            "classify 2.00341796875\n",
            "0.375\n",
            "0.296875\n",
            "0.3125\n",
            "0.3125\n",
            "0.265625\n",
            "0.34375\n",
            "0.375\n",
            "0.375\n",
            "0.28125\n",
            "0.21875\n",
            "0.21875\n",
            "time: 4.485992193222046 4.0822336668450125\n",
            "902\n",
            "strain 0.39237555861473083\n",
            "strain 0.3388403654098511\n",
            "strain 0.20897549390792847\n",
            "strain 0.2953967750072479\n",
            "strain 0.21885724365711212\n",
            "strain 0.23463095724582672\n",
            "classify 1.9639892578125\n",
            "classify 1.9825439453125\n",
            "classify 1.903076171875\n",
            "classify 2.0137939453125\n",
            "classify 1.8875732421875\n",
            "classify 2.0262451171875\n",
            "classify 1.908203125\n",
            "classify 1.89178466796875\n",
            "classify 2.048828125\n",
            "classify 1.84210205078125\n",
            "classify 1.8934326171875\n",
            "0.296875\n",
            "0.359375\n",
            "0.359375\n",
            "0.390625\n",
            "0.375\n",
            "0.4375\n",
            "0.25\n",
            "0.375\n",
            "0.359375\n",
            "0.296875\n",
            "0.34375\n",
            "time: 3.8140296936035156 4.081937171560058\n",
            "903\n",
            "strain 0.2353861927986145\n",
            "strain 0.2242211252450943\n",
            "strain 0.21094441413879395\n",
            "strain 0.22857628762722015\n",
            "strain 0.250173956155777\n",
            "strain 0.21423567831516266\n",
            "classify 1.994384765625\n",
            "classify 1.990966796875\n",
            "classify 2.1033935546875\n",
            "classify 2.001220703125\n",
            "classify 1.890380859375\n",
            "classify 1.8646240234375\n",
            "classify 1.85443115234375\n",
            "classify 2.0279541015625\n",
            "classify 2.0926513671875\n",
            "classify 2.0281982421875\n",
            "classify 1.9993896484375\n",
            "0.375\n",
            "0.25\n",
            "0.25\n",
            "0.21875\n",
            "0.265625\n",
            "0.328125\n",
            "0.34375\n",
            "0.34375\n",
            "0.328125\n",
            "0.40625\n",
            "0.5\n",
            "time: 3.765693426132202 4.081587823882567\n",
            "904\n",
            "strain 0.2768859565258026\n",
            "strain 0.26959818601608276\n",
            "strain 0.3030332326889038\n",
            "strain 0.27587980031967163\n",
            "strain 0.26510292291641235\n",
            "strain 0.23642471432685852\n",
            "classify 1.93280029296875\n",
            "classify 1.9742431640625\n",
            "classify 1.9107666015625\n",
            "classify 2.052001953125\n",
            "classify 1.9654541015625\n",
            "classify 1.9642333984375\n",
            "classify 1.972412109375\n",
            "classify 1.9608154296875\n",
            "classify 1.928955078125\n",
            "classify 1.93798828125\n",
            "classify 1.9681396484375\n",
            "0.359375\n",
            "0.296875\n",
            "0.453125\n",
            "0.234375\n",
            "0.265625\n",
            "0.21875\n",
            "0.3125\n",
            "0.203125\n",
            "0.296875\n",
            "0.265625\n",
            "0.34375\n",
            "time: 4.473128080368042 4.082020884719343\n",
            "905\n",
            "strain 0.2448674887418747\n",
            "strain 0.2468489706516266\n",
            "strain 0.37515613436698914\n",
            "strain 0.24349944293498993\n",
            "strain 0.2663336992263794\n",
            "strain 0.26250994205474854\n",
            "classify 1.947265625\n",
            "classify 1.9560546875\n",
            "classify 1.8082275390625\n",
            "classify 1.981689453125\n",
            "classify 2.02227783203125\n",
            "classify 1.8905029296875\n",
            "classify 2.0201416015625\n",
            "classify 2.028076171875\n",
            "classify 2.0220947265625\n",
            "classify 2.00146484375\n",
            "classify 1.9615478515625\n",
            "0.265625\n",
            "0.203125\n",
            "0.296875\n",
            "0.203125\n",
            "0.296875\n",
            "0.296875\n",
            "0.40625\n",
            "0.265625\n",
            "0.28125\n",
            "0.328125\n",
            "0.296875\n",
            "time: 3.8303496837615967 4.0817435861423315\n",
            "906\n",
            "strain 0.20668572187423706\n",
            "strain 0.2624329924583435\n",
            "strain 0.24299748241901398\n",
            "strain 0.22313693165779114\n",
            "strain 0.2573900818824768\n",
            "strain 0.2382017821073532\n",
            "classify 1.962890625\n",
            "classify 2.017822265625\n",
            "classify 1.889404296875\n",
            "classify 1.9249267578125\n",
            "classify 1.9703369140625\n",
            "classify 2.011962890625\n",
            "classify 1.7545166015625\n",
            "classify 2.062255859375\n",
            "classify 1.96533203125\n",
            "classify 1.9964599609375\n",
            "classify 1.9453125\n",
            "0.25\n",
            "0.234375\n",
            "0.28125\n",
            "0.171875\n",
            "0.375\n",
            "0.296875\n",
            "0.296875\n",
            "0.296875\n",
            "0.25\n",
            "0.25\n",
            "0.203125\n",
            "time: 3.77424693107605 4.08140506817987\n",
            "907\n",
            "strain 0.22808606922626495\n",
            "strain 0.32504531741142273\n",
            "strain 0.28164348006248474\n",
            "strain 0.23316335678100586\n",
            "strain 0.21653874218463898\n",
            "strain 0.23912541568279266\n",
            "classify 1.943359375\n",
            "classify 1.923583984375\n",
            "classify 1.91748046875\n",
            "classify 1.97528076171875\n",
            "classify 1.85791015625\n",
            "classify 2.01611328125\n",
            "classify 1.96075439453125\n",
            "classify 1.9676513671875\n",
            "classify 1.9244384765625\n",
            "classify 1.9384765625\n",
            "classify 2.05322265625\n",
            "0.3125\n",
            "0.28125\n",
            "0.234375\n",
            "0.390625\n",
            "0.328125\n",
            "0.359375\n",
            "0.28125\n",
            "0.34375\n",
            "0.359375\n",
            "0.34375\n",
            "0.328125\n",
            "time: 4.416331052780151 4.081774401769764\n",
            "908\n",
            "strain 0.27364373207092285\n",
            "strain 0.29034677147865295\n",
            "strain 0.2988014221191406\n",
            "strain 0.27892717719078064\n",
            "strain 0.2841690480709076\n",
            "strain 0.3746732473373413\n",
            "classify 1.9237060546875\n",
            "classify 1.886474609375\n",
            "classify 1.8994140625\n",
            "classify 1.9141845703125\n",
            "classify 1.97607421875\n",
            "classify 1.9862060546875\n",
            "classify 1.9749755859375\n",
            "classify 1.9373779296875\n",
            "classify 1.9833984375\n",
            "classify 2.0172119140625\n",
            "classify 1.9554443359375\n",
            "0.328125\n",
            "0.359375\n",
            "0.390625\n",
            "0.359375\n",
            "0.3125\n",
            "0.46875\n",
            "0.25\n",
            "0.359375\n",
            "0.359375\n",
            "0.34375\n",
            "0.234375\n",
            "time: 3.8828938007354736 4.081556002561277\n",
            "909\n",
            "strain 0.324933260679245\n",
            "strain 0.2788844704627991\n",
            "strain 0.2601357400417328\n",
            "strain 0.2852569818496704\n",
            "strain 0.23862583935260773\n",
            "strain 0.2969457507133484\n",
            "classify 1.93017578125\n",
            "classify 2.010986328125\n",
            "classify 1.9869384765625\n",
            "classify 2.06005859375\n",
            "classify 1.914794921875\n",
            "classify 2.0101318359375\n",
            "classify 1.966552734375\n",
            "classify 1.915771484375\n",
            "classify 1.9498291015625\n",
            "classify 1.9501953125\n",
            "classify 2.0279541015625\n",
            "0.21875\n",
            "0.359375\n",
            "0.3125\n",
            "0.40625\n",
            "0.34375\n",
            "0.453125\n",
            "0.359375\n",
            "0.25\n",
            "0.4375\n",
            "0.296875\n",
            "0.296875\n",
            "time: 3.788179636001587 4.081234081498869\n",
            "910\n",
            "strain 0.3067913055419922\n",
            "strain 0.23927757143974304\n",
            "strain 0.23038141429424286\n",
            "strain 0.2898624539375305\n",
            "strain 0.2528645992279053\n",
            "strain 0.2108146697282791\n",
            "classify 1.937255859375\n",
            "classify 1.901123046875\n",
            "classify 1.991943359375\n",
            "classify 1.9005126953125\n",
            "classify 1.927734375\n",
            "classify 1.992431640625\n",
            "classify 1.9715576171875\n",
            "classify 1.9736328125\n",
            "classify 1.9525146484375\n",
            "classify 1.992431640625\n",
            "classify 1.9769287109375\n",
            "0.328125\n",
            "0.234375\n",
            "0.296875\n",
            "0.3125\n",
            "0.265625\n",
            "0.25\n",
            "0.28125\n",
            "0.34375\n",
            "0.3125\n",
            "0.28125\n",
            "0.234375\n",
            "time: 4.564362287521362 4.081764889077481\n",
            "911\n",
            "strain 0.3111810088157654\n",
            "strain 0.2275107502937317\n",
            "strain 0.3403099477291107\n",
            "strain 0.22455157339572906\n",
            "strain 0.2606603801250458\n",
            "strain 0.22869446873664856\n",
            "classify 1.9454345703125\n",
            "classify 1.90087890625\n",
            "classify 1.91455078125\n",
            "classify 1.88232421875\n",
            "classify 2.0841064453125\n",
            "classify 1.9674072265625\n",
            "classify 1.87847900390625\n",
            "classify 1.98223876953125\n",
            "classify 2.0919189453125\n",
            "classify 1.917236328125\n",
            "classify 2.0025634765625\n",
            "0.296875\n",
            "0.359375\n",
            "0.296875\n",
            "0.265625\n",
            "0.296875\n",
            "0.203125\n",
            "0.359375\n",
            "0.390625\n",
            "0.328125\n",
            "0.234375\n",
            "0.375\n",
            "time: 3.967944383621216 4.0816414392831035\n",
            "912\n",
            "strain 0.2033100724220276\n",
            "strain 0.20084135234355927\n",
            "strain 0.21690961718559265\n",
            "strain 0.22893166542053223\n",
            "strain 0.2903493642807007\n",
            "strain 0.3177904784679413\n",
            "classify 1.89208984375\n",
            "classify 1.9176025390625\n",
            "classify 1.958251953125\n",
            "classify 1.90997314453125\n",
            "classify 1.930419921875\n",
            "classify 2.060791015625\n",
            "classify 1.80078125\n",
            "classify 2.0203857421875\n",
            "classify 1.9796142578125\n",
            "classify 1.9378662109375\n",
            "classify 2.003662109375\n",
            "0.34375\n",
            "0.40625\n",
            "0.40625\n",
            "0.3125\n",
            "0.359375\n",
            "0.265625\n",
            "0.34375\n",
            "0.296875\n",
            "0.359375\n",
            "0.3125\n",
            "0.296875\n",
            "time: 3.7834277153015137 4.0813153351515306\n",
            "913\n",
            "strain 0.27358636260032654\n",
            "strain 0.3345533609390259\n",
            "strain 0.24378478527069092\n",
            "strain 0.2840614914894104\n",
            "strain 0.33161264657974243\n",
            "strain 0.24500656127929688\n",
            "classify 1.8861083984375\n",
            "classify 1.89117431640625\n",
            "classify 1.938720703125\n",
            "classify 1.9298095703125\n",
            "classify 1.8880615234375\n",
            "classify 1.9578857421875\n",
            "classify 2.045166015625\n",
            "classify 2.00616455078125\n",
            "classify 1.8948974609375\n",
            "classify 2.0445556640625\n",
            "classify 1.967529296875\n",
            "0.25\n",
            "0.34375\n",
            "0.296875\n",
            "0.234375\n",
            "0.421875\n",
            "0.296875\n",
            "0.359375\n",
            "0.375\n",
            "0.375\n",
            "0.3125\n",
            "0.328125\n",
            "time: 4.181017875671387 4.081424881645052\n",
            "914\n",
            "strain 0.2833317220211029\n",
            "strain 0.22133328020572662\n",
            "strain 0.3291776180267334\n",
            "strain 0.2439184933900833\n",
            "strain 0.22639791667461395\n",
            "strain 0.28432515263557434\n",
            "classify 1.96160888671875\n",
            "classify 2.003173828125\n",
            "classify 1.946044921875\n",
            "classify 2.0074462890625\n",
            "classify 1.91748046875\n",
            "classify 1.964599609375\n",
            "classify 1.909423828125\n",
            "classify 2.0040283203125\n",
            "classify 2.003173828125\n",
            "classify 1.957275390625\n",
            "classify 1.9718017578125\n",
            "0.34375\n",
            "0.28125\n",
            "0.359375\n",
            "0.359375\n",
            "0.3125\n",
            "0.375\n",
            "0.265625\n",
            "0.1875\n",
            "0.25\n",
            "0.34375\n",
            "0.265625\n",
            "time: 4.053327798843384 4.0813947768810666\n",
            "915\n",
            "strain 0.29482221603393555\n",
            "strain 0.20909449458122253\n",
            "strain 0.32828423380851746\n",
            "strain 0.24065262079238892\n",
            "strain 0.31642240285873413\n",
            "strain 0.21192698180675507\n",
            "classify 1.9459228515625\n",
            "classify 1.9324951171875\n",
            "classify 2.0040283203125\n",
            "classify 1.9339599609375\n",
            "classify 1.878173828125\n",
            "classify 1.9930419921875\n",
            "classify 1.943603515625\n",
            "classify 1.9759521484375\n",
            "classify 1.9569091796875\n",
            "classify 1.902587890625\n",
            "classify 2.040283203125\n",
            "0.296875\n",
            "0.390625\n",
            "0.34375\n",
            "0.328125\n",
            "0.296875\n",
            "0.359375\n",
            "0.34375\n",
            "0.40625\n",
            "0.328125\n",
            "0.34375\n",
            "0.296875\n",
            "time: 3.8188095092773438 4.08110858285271\n",
            "916\n",
            "strain 0.2776058614253998\n",
            "strain 0.2707012891769409\n",
            "strain 0.20507659018039703\n",
            "strain 0.23881883919239044\n",
            "strain 0.33726632595062256\n",
            "strain 0.2037218064069748\n",
            "classify 1.980712890625\n",
            "classify 1.938720703125\n",
            "classify 1.9505615234375\n",
            "classify 2.03076171875\n",
            "classify 2.078369140625\n",
            "classify 1.95953369140625\n",
            "classify 1.9718017578125\n",
            "classify 1.96875\n",
            "classify 1.89599609375\n",
            "classify 1.917236328125\n",
            "classify 2.044921875\n",
            "0.265625\n",
            "0.375\n",
            "0.34375\n",
            "0.3125\n",
            "0.296875\n",
            "0.34375\n",
            "0.296875\n",
            "0.40625\n",
            "0.265625\n",
            "0.328125\n",
            "0.34375\n",
            "time: 4.104110479354858 4.081134209326075\n",
            "917\n",
            "strain 0.31443750858306885\n",
            "strain 0.27330222725868225\n",
            "strain 0.3056965470314026\n",
            "strain 0.2322992980480194\n",
            "strain 0.22620421648025513\n",
            "strain 0.2959083914756775\n",
            "classify 2.0390625\n",
            "classify 1.8685302734375\n",
            "classify 1.9739990234375\n",
            "classify 1.9296875\n",
            "classify 1.96142578125\n",
            "classify 1.907470703125\n",
            "classify 1.9884033203125\n",
            "classify 1.9720458984375\n",
            "classify 2.02008056640625\n",
            "classify 1.977294921875\n",
            "classify 1.9979248046875\n",
            "0.25\n",
            "0.296875\n",
            "0.28125\n",
            "0.28125\n",
            "0.3125\n",
            "0.328125\n",
            "0.34375\n",
            "0.296875\n",
            "0.28125\n",
            "0.3125\n",
            "0.21875\n",
            "time: 4.24644660949707 4.081314923456811\n",
            "918\n",
            "strain 0.21736127138137817\n",
            "strain 0.27022072672843933\n",
            "strain 0.25316426157951355\n",
            "strain 0.23513714969158173\n",
            "strain 0.2670277953147888\n",
            "strain 0.21529221534729004\n",
            "classify 1.8902587890625\n",
            "classify 1.98291015625\n",
            "classify 1.9857177734375\n",
            "classify 1.9737548828125\n",
            "classify 1.8817138671875\n",
            "classify 1.9498291015625\n",
            "classify 2.072998046875\n",
            "classify 1.9722900390625\n",
            "classify 1.9058837890625\n",
            "classify 1.9124755859375\n",
            "classify 1.864990234375\n",
            "0.40625\n",
            "0.265625\n",
            "0.3125\n",
            "0.328125\n",
            "0.296875\n",
            "0.25\n",
            "0.34375\n",
            "0.328125\n",
            "0.328125\n",
            "0.40625\n",
            "0.375\n",
            "time: 3.8064942359924316 4.081016380976282\n",
            "919\n",
            "strain 0.27973777055740356\n",
            "strain 0.27267584204673767\n",
            "strain 0.21740013360977173\n",
            "strain 0.29958033561706543\n",
            "strain 0.29111534357070923\n",
            "strain 0.2677455246448517\n",
            "classify 2.0263671875\n",
            "classify 1.87506103515625\n",
            "classify 1.998779296875\n",
            "classify 1.9171142578125\n",
            "classify 1.9296875\n",
            "classify 1.9578857421875\n",
            "classify 1.973876953125\n",
            "classify 1.954345703125\n",
            "classify 1.98681640625\n",
            "classify 2.0386962890625\n",
            "classify 1.9005126953125\n",
            "0.21875\n",
            "0.328125\n",
            "0.3125\n",
            "0.265625\n",
            "0.359375\n",
            "0.3125\n",
            "0.328125\n",
            "0.375\n",
            "0.21875\n",
            "0.328125\n",
            "0.421875\n",
            "time: 3.965076208114624 4.080890865170438\n",
            "920\n",
            "strain 0.22245581448078156\n",
            "strain 0.2646830976009369\n",
            "strain 0.29482531547546387\n",
            "strain 0.26772284507751465\n",
            "strain 0.1943323314189911\n",
            "strain 0.2307984083890915\n",
            "classify 1.895263671875\n",
            "classify 1.9039306640625\n",
            "classify 2.077392578125\n",
            "classify 1.9622802734375\n",
            "classify 1.985595703125\n",
            "classify 2.00146484375\n",
            "classify 1.984619140625\n",
            "classify 1.9219970703125\n",
            "classify 2.01043701171875\n",
            "classify 1.9149169921875\n",
            "classify 2.028564453125\n",
            "0.328125\n",
            "0.359375\n",
            "0.390625\n",
            "0.171875\n",
            "0.390625\n",
            "0.265625\n",
            "0.390625\n",
            "0.265625\n",
            "0.375\n",
            "0.421875\n",
            "0.328125\n",
            "time: 4.313003301620483 4.081143473957571\n",
            "921\n",
            "strain 0.2858960032463074\n",
            "strain 0.286266028881073\n",
            "strain 0.2754940390586853\n",
            "strain 0.23330283164978027\n",
            "strain 0.2244516760110855\n",
            "strain 0.20859713852405548\n",
            "classify 1.922119140625\n",
            "classify 2.0181884765625\n",
            "classify 2.0299072265625\n",
            "classify 1.9853515625\n",
            "classify 1.9482421875\n",
            "classify 1.96484375\n",
            "classify 1.9478759765625\n",
            "classify 1.9508056640625\n",
            "classify 1.9869384765625\n",
            "classify 2.0418701171875\n",
            "classify 1.8868408203125\n",
            "0.296875\n",
            "0.390625\n",
            "0.359375\n",
            "0.390625\n",
            "0.3125\n",
            "0.3125\n",
            "0.296875\n",
            "0.234375\n",
            "0.328125\n",
            "0.234375\n",
            "0.21875\n",
            "time: 3.7634918689727783 4.080799443085644\n",
            "922\n",
            "strain 0.30727285146713257\n",
            "strain 0.35135897994041443\n",
            "strain 0.3136647045612335\n",
            "strain 0.22473716735839844\n",
            "strain 0.27986663579940796\n",
            "strain 0.28323429822921753\n",
            "classify 1.95562744140625\n",
            "classify 1.97265625\n",
            "classify 1.967529296875\n",
            "classify 2.052734375\n",
            "classify 1.941162109375\n",
            "classify 2.06402587890625\n",
            "classify 1.98828125\n",
            "classify 2.0181884765625\n",
            "classify 1.947265625\n",
            "classify 1.85821533203125\n",
            "classify 2.005126953125\n",
            "0.359375\n",
            "0.296875\n",
            "0.28125\n",
            "0.25\n",
            "0.40625\n",
            "0.21875\n",
            "0.3125\n",
            "0.234375\n",
            "0.421875\n",
            "0.359375\n",
            "0.296875\n",
            "time: 3.8516628742218018 4.080551653745244\n",
            "923\n",
            "strain 0.269747257232666\n",
            "strain 0.241507425904274\n",
            "strain 0.3613801896572113\n",
            "strain 0.2976970076560974\n",
            "strain 0.2565113306045532\n",
            "strain 0.2488330453634262\n",
            "classify 1.8790283203125\n",
            "classify 2.01171875\n",
            "classify 1.9547119140625\n",
            "classify 1.8948974609375\n",
            "classify 1.8350830078125\n",
            "classify 1.98883056640625\n",
            "classify 2.051513671875\n",
            "classify 2.099853515625\n",
            "classify 1.9271240234375\n",
            "classify 2.0291748046875\n",
            "classify 1.9385986328125\n",
            "0.21875\n",
            "0.359375\n",
            "0.296875\n",
            "0.25\n",
            "0.375\n",
            "0.34375\n",
            "0.328125\n",
            "0.25\n",
            "0.28125\n",
            "0.296875\n",
            "0.34375\n",
            "time: 4.386096477508545 4.080882808346769\n",
            "924\n",
            "strain 0.2017163336277008\n",
            "strain 0.2770814895629883\n",
            "strain 0.29148921370506287\n",
            "strain 0.2729107737541199\n",
            "strain 0.23891589045524597\n",
            "strain 0.30762726068496704\n",
            "classify 1.921142578125\n",
            "classify 2.021484375\n",
            "classify 2.0252685546875\n",
            "classify 1.957763671875\n",
            "classify 1.9442138671875\n",
            "classify 2.0338134765625\n",
            "classify 2.0030517578125\n",
            "classify 1.939208984375\n",
            "classify 1.920654296875\n",
            "classify 2.08984375\n",
            "classify 1.8858642578125\n",
            "0.375\n",
            "0.296875\n",
            "0.390625\n",
            "0.34375\n",
            "0.3125\n",
            "0.375\n",
            "0.296875\n",
            "0.265625\n",
            "0.234375\n",
            "0.4375\n",
            "0.28125\n",
            "time: 3.862179756164551 4.0806468654323265\n",
            "925\n",
            "strain 0.29035699367523193\n",
            "strain 0.36336204409599304\n",
            "strain 0.3029761016368866\n",
            "strain 0.2086232602596283\n",
            "strain 0.28766387701034546\n",
            "strain 0.32028526067733765\n",
            "classify 1.9071044921875\n",
            "classify 1.9989013671875\n",
            "classify 1.84326171875\n",
            "classify 1.9134521484375\n",
            "classify 1.9412841796875\n",
            "classify 1.8895263671875\n",
            "classify 2.0048828125\n",
            "classify 1.98779296875\n",
            "classify 1.91064453125\n",
            "classify 1.910400390625\n",
            "classify 1.9888916015625\n",
            "0.296875\n",
            "0.265625\n",
            "0.34375\n",
            "0.28125\n",
            "0.265625\n",
            "0.34375\n",
            "0.25\n",
            "0.34375\n",
            "0.375\n",
            "0.328125\n",
            "0.25\n",
            "time: 3.7646703720092773 4.080306104913386\n",
            "926\n",
            "strain 0.27360451221466064\n",
            "strain 0.27443668246269226\n",
            "strain 0.27641725540161133\n",
            "strain 0.26762330532073975\n",
            "strain 0.30552539229393005\n",
            "strain 0.21392706036567688\n",
            "classify 2.0047607421875\n",
            "classify 2.1400146484375\n",
            "classify 1.9456787109375\n",
            "classify 1.9300537109375\n",
            "classify 2.0390625\n",
            "classify 1.94873046875\n",
            "classify 1.9664306640625\n",
            "classify 1.98773193359375\n",
            "classify 2.0380859375\n",
            "classify 1.99176025390625\n",
            "classify 1.9229736328125\n",
            "0.265625\n",
            "0.34375\n",
            "0.375\n",
            "0.296875\n",
            "0.234375\n",
            "0.328125\n",
            "0.25\n",
            "0.3125\n",
            "0.234375\n",
            "0.296875\n",
            "0.34375\n",
            "time: 4.471883773803711 4.080729029325219\n",
            "927\n",
            "strain 0.22204360365867615\n",
            "strain 0.24442465603351593\n",
            "strain 0.25063222646713257\n",
            "strain 0.23979023098945618\n",
            "strain 0.20404966175556183\n",
            "strain 0.28694605827331543\n",
            "classify 1.943359375\n",
            "classify 1.9588623046875\n",
            "classify 1.8648681640625\n",
            "classify 1.978515625\n",
            "classify 1.947509765625\n",
            "classify 1.931396484375\n",
            "classify 1.9666748046875\n",
            "classify 1.9248046875\n",
            "classify 1.993896484375\n",
            "classify 1.905029296875\n",
            "classify 2.0498046875\n",
            "0.34375\n",
            "0.3125\n",
            "0.3125\n",
            "0.34375\n",
            "0.296875\n",
            "0.328125\n",
            "0.3125\n",
            "0.296875\n",
            "0.3125\n",
            "0.296875\n",
            "0.28125\n",
            "time: 3.815539598464966 4.080443761728961\n",
            "928\n",
            "strain 0.22436852753162384\n",
            "strain 0.2140006572008133\n",
            "strain 0.27406734228134155\n",
            "strain 0.23990915715694427\n",
            "strain 0.3099924921989441\n",
            "strain 0.2129979431629181\n",
            "classify 1.9686279296875\n",
            "classify 1.9571533203125\n",
            "classify 1.924560546875\n",
            "classify 2.00286865234375\n",
            "classify 2.0052490234375\n",
            "classify 1.94427490234375\n",
            "classify 1.9813232421875\n",
            "classify 2.0120849609375\n",
            "classify 1.9359130859375\n",
            "classify 1.925537109375\n",
            "classify 1.9520263671875\n",
            "0.3125\n",
            "0.296875\n",
            "0.328125\n",
            "0.25\n",
            "0.34375\n",
            "0.3125\n",
            "0.234375\n",
            "0.234375\n",
            "0.375\n",
            "0.28125\n",
            "0.234375\n",
            "time: 3.770979881286621 4.080111124030484\n",
            "929\n",
            "strain 0.2825643718242645\n",
            "strain 0.20610465109348297\n",
            "strain 0.3644615411758423\n",
            "strain 0.27110716700553894\n",
            "strain 0.26823943853378296\n",
            "strain 0.26389604806900024\n",
            "classify 1.878662109375\n",
            "classify 2.0252685546875\n",
            "classify 1.8658447265625\n",
            "classify 1.92822265625\n",
            "classify 1.9375\n",
            "classify 2.012451171875\n",
            "classify 1.9210205078125\n",
            "classify 2.05419921875\n",
            "classify 2.0081787109375\n",
            "classify 1.92822265625\n",
            "classify 1.92236328125\n",
            "0.28125\n",
            "0.328125\n",
            "0.3125\n",
            "0.40625\n",
            "0.328125\n",
            "0.265625\n",
            "0.265625\n",
            "0.28125\n",
            "0.34375\n",
            "0.25\n",
            "0.328125\n",
            "time: 4.437903642654419 4.0804963324659616\n",
            "930\n",
            "strain 0.22794461250305176\n",
            "strain 0.24696438014507294\n",
            "strain 0.26402246952056885\n",
            "strain 0.2686764597892761\n",
            "strain 0.20936381816864014\n",
            "strain 0.28841328620910645\n",
            "classify 1.95513916015625\n",
            "classify 2.01361083984375\n",
            "classify 1.9232177734375\n",
            "classify 2.010986328125\n",
            "classify 1.9620361328125\n",
            "classify 2.063232421875\n",
            "classify 1.927001953125\n",
            "classify 1.92645263671875\n",
            "classify 1.96435546875\n",
            "classify 1.98974609375\n",
            "classify 2.0118408203125\n",
            "0.375\n",
            "0.328125\n",
            "0.34375\n",
            "0.265625\n",
            "0.375\n",
            "0.3125\n",
            "0.3125\n",
            "0.296875\n",
            "0.28125\n",
            "0.3125\n",
            "0.25\n",
            "time: 3.8386991024017334 4.0802371192055285\n",
            "931\n",
            "strain 0.29157984256744385\n",
            "strain 0.3006955683231354\n",
            "strain 0.30992892384529114\n",
            "strain 0.25353682041168213\n",
            "strain 0.24158035218715668\n",
            "strain 0.21376851201057434\n",
            "classify 2.02392578125\n",
            "classify 1.998291015625\n",
            "classify 1.93505859375\n",
            "classify 2.0384521484375\n",
            "classify 2.06787109375\n",
            "classify 2.005859375\n",
            "classify 1.9771728515625\n",
            "classify 1.9459228515625\n",
            "classify 2.0079345703125\n",
            "classify 1.9598388671875\n",
            "classify 1.9820556640625\n",
            "0.296875\n",
            "0.171875\n",
            "0.28125\n",
            "0.359375\n",
            "0.28125\n",
            "0.328125\n",
            "0.328125\n",
            "0.359375\n",
            "0.390625\n",
            "0.359375\n",
            "0.28125\n",
            "time: 3.800947666168213 4.07993794996851\n",
            "932\n",
            "strain 0.2619251310825348\n",
            "strain 0.23532137274742126\n",
            "strain 0.3041786849498749\n",
            "strain 0.33045899868011475\n",
            "strain 0.2394946664571762\n",
            "strain 0.33824366331100464\n",
            "classify 2.00244140625\n",
            "classify 1.9293212890625\n",
            "classify 1.908935546875\n",
            "classify 2.0489501953125\n",
            "classify 1.9832763671875\n",
            "classify 1.93798828125\n",
            "classify 2.0223388671875\n",
            "classify 1.9600830078125\n",
            "classify 1.9462890625\n",
            "classify 1.9962158203125\n",
            "classify 1.915283203125\n",
            "0.3125\n",
            "0.296875\n",
            "0.234375\n",
            "0.21875\n",
            "0.34375\n",
            "0.3125\n",
            "0.28125\n",
            "0.34375\n",
            "0.265625\n",
            "0.4375\n",
            "0.3125\n",
            "time: 4.519836664199829 4.08040993364422\n",
            "933\n",
            "strain 0.2350183129310608\n",
            "strain 0.29019665718078613\n",
            "strain 0.2716009020805359\n",
            "strain 0.30548372864723206\n",
            "strain 0.20781567692756653\n",
            "strain 0.2037198543548584\n",
            "classify 1.91748046875\n",
            "classify 1.909912109375\n",
            "classify 1.98828125\n",
            "classify 1.9521484375\n",
            "classify 2.005859375\n",
            "classify 1.957275390625\n",
            "classify 1.95361328125\n",
            "classify 2.00244140625\n",
            "classify 1.90771484375\n",
            "classify 1.9090576171875\n",
            "classify 1.948486328125\n",
            "0.390625\n",
            "0.375\n",
            "0.25\n",
            "0.21875\n",
            "0.3125\n",
            "0.390625\n",
            "0.234375\n",
            "0.328125\n",
            "0.375\n",
            "0.28125\n",
            "0.375\n",
            "time: 3.835223436355591 4.080147936972102\n",
            "934\n",
            "strain 0.3766361474990845\n",
            "strain 0.2600886821746826\n",
            "strain 0.25645920634269714\n",
            "strain 0.2800670266151428\n",
            "strain 0.34636616706848145\n",
            "strain 0.2412349432706833\n",
            "classify 1.893310546875\n",
            "classify 1.975830078125\n",
            "classify 1.87353515625\n",
            "classify 2.001220703125\n",
            "classify 1.928955078125\n",
            "classify 1.9239501953125\n",
            "classify 2.01507568359375\n",
            "classify 1.9639892578125\n",
            "classify 1.928955078125\n",
            "classify 1.9832763671875\n",
            "classify 1.9427490234375\n",
            "0.359375\n",
            "0.34375\n",
            "0.359375\n",
            "0.421875\n",
            "0.25\n",
            "0.328125\n",
            "0.359375\n",
            "0.234375\n",
            "0.359375\n",
            "0.390625\n",
            "0.3125\n",
            "time: 3.8013689517974854 4.079850248856978\n",
            "935\n",
            "strain 0.32805538177490234\n",
            "strain 0.3788849711418152\n",
            "strain 0.2852057218551636\n",
            "strain 0.22285641729831696\n",
            "strain 0.2754397690296173\n",
            "strain 0.31478795409202576\n",
            "classify 2.0853271484375\n",
            "classify 1.939208984375\n",
            "classify 2.0289306640625\n",
            "classify 1.9937744140625\n",
            "classify 1.9508056640625\n",
            "classify 1.9144287109375\n",
            "classify 1.976318359375\n",
            "classify 1.9130859375\n",
            "classify 1.9466552734375\n",
            "classify 1.9295654296875\n",
            "classify 1.966064453125\n",
            "0.296875\n",
            "0.453125\n",
            "0.234375\n",
            "0.21875\n",
            "0.28125\n",
            "0.265625\n",
            "0.296875\n",
            "0.265625\n",
            "0.234375\n",
            "0.3125\n",
            "0.28125\n",
            "time: 4.459195613861084 4.080256799602101\n",
            "936\n",
            "strain 0.2894355058670044\n",
            "strain 0.21862339973449707\n",
            "strain 0.23507778346538544\n",
            "strain 0.2618751525878906\n",
            "strain 0.20756129920482635\n",
            "strain 0.2215467244386673\n",
            "classify 1.9876708984375\n",
            "classify 1.9249267578125\n",
            "classify 2.0018310546875\n",
            "classify 2.0423583984375\n",
            "classify 2.000732421875\n",
            "classify 1.958740234375\n",
            "classify 1.9423828125\n",
            "classify 1.944091796875\n",
            "classify 1.9527587890625\n",
            "classify 1.9293212890625\n",
            "classify 1.927978515625\n",
            "0.375\n",
            "0.328125\n",
            "0.203125\n",
            "0.28125\n",
            "0.25\n",
            "0.3125\n",
            "0.234375\n",
            "0.34375\n",
            "0.34375\n",
            "0.265625\n",
            "0.3125\n",
            "time: 3.7914741039276123 4.07994905963397\n",
            "937\n",
            "strain 0.3336303234100342\n",
            "strain 0.2958037853240967\n",
            "strain 0.38928717374801636\n",
            "strain 0.22046810388565063\n",
            "strain 0.25334107875823975\n",
            "strain 0.23440609872341156\n",
            "classify 1.976318359375\n",
            "classify 1.87353515625\n",
            "classify 1.9755859375\n",
            "classify 1.951171875\n",
            "classify 1.9698486328125\n",
            "classify 1.9468994140625\n",
            "classify 2.04425048828125\n",
            "classify 1.9559326171875\n",
            "classify 2.0419921875\n",
            "classify 2.02197265625\n",
            "classify 2.0146484375\n",
            "0.296875\n",
            "0.328125\n",
            "0.28125\n",
            "0.34375\n",
            "0.25\n",
            "0.25\n",
            "0.3125\n",
            "0.25\n",
            "0.359375\n",
            "0.28125\n",
            "0.359375\n",
            "time: 3.818938970565796 4.079671294450251\n",
            "938\n",
            "strain 0.26180997490882874\n",
            "strain 0.2868768274784088\n",
            "strain 0.26141178607940674\n",
            "strain 0.21037444472312927\n",
            "strain 0.36704906821250916\n",
            "strain 0.3872341811656952\n",
            "classify 2.1142578125\n",
            "classify 1.9530029296875\n",
            "classify 2.000244140625\n",
            "classify 1.921142578125\n",
            "classify 1.894287109375\n",
            "classify 1.9735107421875\n",
            "classify 1.95263671875\n",
            "classify 1.995849609375\n",
            "classify 1.934326171875\n",
            "classify 2.0057373046875\n",
            "classify 2.0426025390625\n",
            "0.203125\n",
            "0.265625\n",
            "0.40625\n",
            "0.3125\n",
            "0.203125\n",
            "0.21875\n",
            "0.3125\n",
            "0.28125\n",
            "0.359375\n",
            "0.34375\n",
            "0.28125\n",
            "time: 4.501684904098511 4.080121361028653\n",
            "939\n",
            "strain 0.23340106010437012\n",
            "strain 0.22529427707195282\n",
            "strain 0.23369979858398438\n",
            "strain 0.2349083423614502\n",
            "strain 0.3620602488517761\n",
            "strain 0.2604259252548218\n",
            "classify 1.9278564453125\n",
            "classify 1.989013671875\n",
            "classify 2.0103759765625\n",
            "classify 1.93524169921875\n",
            "classify 1.97998046875\n",
            "classify 2.017578125\n",
            "classify 1.8990478515625\n",
            "classify 1.95068359375\n",
            "classify 1.9840087890625\n",
            "classify 1.9537353515625\n",
            "classify 2.04736328125\n",
            "0.25\n",
            "0.234375\n",
            "0.3125\n",
            "0.234375\n",
            "0.25\n",
            "0.203125\n",
            "0.3125\n",
            "0.28125\n",
            "0.375\n",
            "0.359375\n",
            "0.25\n",
            "time: 3.86667799949646 4.079894739262601\n",
            "940\n",
            "strain 0.2143765091896057\n",
            "strain 0.19292619824409485\n",
            "strain 0.29463768005371094\n",
            "strain 0.2256636619567871\n",
            "strain 0.3019787669181824\n",
            "strain 0.19919027388095856\n",
            "classify 1.9769287109375\n",
            "classify 1.8863525390625\n",
            "classify 1.9002685546875\n",
            "classify 1.989990234375\n",
            "classify 1.99853515625\n",
            "classify 1.9454345703125\n",
            "classify 2.08447265625\n",
            "classify 1.9864501953125\n",
            "classify 1.907958984375\n",
            "classify 2.0313720703125\n",
            "classify 2.1131591796875\n",
            "0.34375\n",
            "0.375\n",
            "0.28125\n",
            "0.359375\n",
            "0.34375\n",
            "0.34375\n",
            "0.25\n",
            "0.28125\n",
            "0.40625\n",
            "0.40625\n",
            "0.359375\n",
            "time: 3.851217269897461 4.079652247342749\n",
            "941\n",
            "strain 0.24467124044895172\n",
            "strain 0.19799768924713135\n",
            "strain 0.30554887652397156\n",
            "strain 0.2652008831501007\n",
            "strain 0.3029431998729706\n",
            "strain 0.2383228987455368\n",
            "classify 2.0147705078125\n",
            "classify 1.9107666015625\n",
            "classify 1.89874267578125\n",
            "classify 1.9649658203125\n",
            "classify 1.92724609375\n",
            "classify 1.93408203125\n",
            "classify 2.035888671875\n",
            "classify 1.947998046875\n",
            "classify 1.903564453125\n",
            "classify 1.9713134765625\n",
            "classify 1.89898681640625\n",
            "0.296875\n",
            "0.328125\n",
            "0.359375\n",
            "0.375\n",
            "0.484375\n",
            "0.3125\n",
            "0.296875\n",
            "0.328125\n",
            "0.359375\n",
            "0.359375\n",
            "0.296875\n",
            "time: 4.496436595916748 4.080095214955113\n",
            "942\n",
            "strain 0.2863345146179199\n",
            "strain 0.2980601191520691\n",
            "strain 0.2589823305606842\n",
            "strain 0.21368558704853058\n",
            "strain 0.2896057665348053\n",
            "strain 0.21875299513339996\n",
            "classify 1.986328125\n",
            "classify 1.9510498046875\n",
            "classify 2.01513671875\n",
            "classify 1.9437255859375\n",
            "classify 1.9561767578125\n",
            "classify 1.96337890625\n",
            "classify 1.961669921875\n",
            "classify 2.0518798828125\n",
            "classify 2.0184326171875\n",
            "classify 1.903076171875\n",
            "classify 1.9046630859375\n",
            "0.296875\n",
            "0.375\n",
            "0.28125\n",
            "0.21875\n",
            "0.34375\n",
            "0.359375\n",
            "0.28125\n",
            "0.265625\n",
            "0.3125\n",
            "0.375\n",
            "0.34375\n",
            "time: 3.8836982250213623 4.079887484695095\n",
            "943\n",
            "strain 0.27130037546157837\n",
            "strain 0.2381090670824051\n",
            "strain 0.21074917912483215\n",
            "strain 0.2278365045785904\n",
            "strain 0.30803871154785156\n",
            "strain 0.24694731831550598\n",
            "classify 1.90771484375\n",
            "classify 2.0108642578125\n",
            "classify 2.01513671875\n",
            "classify 2.063232421875\n",
            "classify 2.0435791015625\n",
            "classify 2.0384521484375\n",
            "classify 1.9515380859375\n",
            "classify 1.919921875\n",
            "classify 1.938232421875\n",
            "classify 1.96728515625\n",
            "classify 1.978271484375\n",
            "0.1875\n",
            "0.25\n",
            "0.296875\n",
            "0.40625\n",
            "0.359375\n",
            "0.28125\n",
            "0.234375\n",
            "0.21875\n",
            "0.28125\n",
            "0.328125\n",
            "0.296875\n",
            "time: 3.7707879543304443 4.079560509677661\n",
            "944\n",
            "strain 0.29103583097457886\n",
            "strain 0.2373914271593094\n",
            "strain 0.1877841055393219\n",
            "strain 0.2283201962709427\n",
            "strain 0.24126937985420227\n",
            "strain 0.3271493911743164\n",
            "classify 1.94775390625\n",
            "classify 1.95379638671875\n",
            "classify 2.017578125\n",
            "classify 2.0159912109375\n",
            "classify 1.9185791015625\n",
            "classify 1.9691162109375\n",
            "classify 1.9603271484375\n",
            "classify 1.9810791015625\n",
            "classify 1.9708251953125\n",
            "classify 1.96142578125\n",
            "classify 1.95953369140625\n",
            "0.390625\n",
            "0.34375\n",
            "0.328125\n",
            "0.390625\n",
            "0.234375\n",
            "0.28125\n",
            "0.234375\n",
            "0.328125\n",
            "0.375\n",
            "0.359375\n",
            "0.390625\n",
            "time: 4.293227195739746 4.079787094883187\n",
            "945\n",
            "strain 0.2521921396255493\n",
            "strain 0.2684849500656128\n",
            "strain 0.35340094566345215\n",
            "strain 0.2538691461086273\n",
            "strain 0.2555564045906067\n",
            "strain 0.2300221174955368\n",
            "classify 1.962890625\n",
            "classify 2.0302734375\n",
            "classify 1.9207763671875\n",
            "classify 1.956298828125\n",
            "classify 1.916748046875\n",
            "classify 2.000244140625\n",
            "classify 2.012451171875\n",
            "classify 1.934326171875\n",
            "classify 1.945556640625\n",
            "classify 1.960693359375\n",
            "classify 1.90771484375\n",
            "0.328125\n",
            "0.34375\n",
            "0.359375\n",
            "0.25\n",
            "0.234375\n",
            "0.296875\n",
            "0.34375\n",
            "0.265625\n",
            "0.265625\n",
            "0.28125\n",
            "0.359375\n",
            "time: 4.045150279998779 4.079751481717535\n",
            "946\n",
            "strain 0.21925747394561768\n",
            "strain 0.22467835247516632\n",
            "strain 0.24902860820293427\n",
            "strain 0.22092662751674652\n",
            "strain 0.2271278202533722\n",
            "strain 0.24797140061855316\n",
            "classify 2.0301513671875\n",
            "classify 1.9022216796875\n",
            "classify 2.030517578125\n",
            "classify 2.014404296875\n",
            "classify 1.875244140625\n",
            "classify 1.98779296875\n",
            "classify 1.98919677734375\n",
            "classify 1.9224853515625\n",
            "classify 1.996826171875\n",
            "classify 1.900390625\n",
            "classify 1.8890380859375\n",
            "0.375\n",
            "0.28125\n",
            "0.25\n",
            "0.34375\n",
            "0.328125\n",
            "0.4375\n",
            "0.28125\n",
            "0.28125\n",
            "0.390625\n",
            "0.390625\n",
            "0.40625\n",
            "time: 3.8502559661865234 4.079509628108587\n",
            "947\n",
            "strain 0.23233355581760406\n",
            "strain 0.27797847986221313\n",
            "strain 0.20599018037319183\n",
            "strain 0.22454187273979187\n",
            "strain 0.2586456835269928\n",
            "strain 0.22367341816425323\n",
            "classify 1.986328125\n",
            "classify 1.93310546875\n",
            "classify 2.0435791015625\n",
            "classify 1.9425048828125\n",
            "classify 1.9539794921875\n",
            "classify 1.89453125\n",
            "classify 1.9129638671875\n",
            "classify 1.935302734375\n",
            "classify 1.90869140625\n",
            "classify 1.926025390625\n",
            "classify 1.927734375\n",
            "0.328125\n",
            "0.265625\n",
            "0.203125\n",
            "0.421875\n",
            "0.265625\n",
            "0.3125\n",
            "0.328125\n",
            "0.3125\n",
            "0.3125\n",
            "0.25\n",
            "0.265625\n",
            "time: 4.219456911087036 4.0796577311769315\n",
            "948\n",
            "strain 0.3703864514827728\n",
            "strain 0.24749340116977692\n",
            "strain 0.23509611189365387\n",
            "strain 0.24212421476840973\n",
            "strain 0.2645478844642639\n",
            "strain 0.26130300760269165\n",
            "classify 2.015625\n",
            "classify 2.03759765625\n",
            "classify 1.9903564453125\n",
            "classify 2.0595703125\n",
            "classify 1.8204345703125\n",
            "classify 1.959716796875\n",
            "classify 1.9205322265625\n",
            "classify 1.9149169921875\n",
            "classify 1.9893798828125\n",
            "classify 1.9715576171875\n",
            "classify 1.92529296875\n",
            "0.359375\n",
            "0.34375\n",
            "0.34375\n",
            "0.421875\n",
            "0.390625\n",
            "0.296875\n",
            "0.3125\n",
            "0.328125\n",
            "0.421875\n",
            "0.15625\n",
            "0.328125\n",
            "time: 4.31334662437439 4.079904452515854\n",
            "949\n",
            "strain 0.2912166118621826\n",
            "strain 0.21896333992481232\n",
            "strain 0.2626955211162567\n",
            "strain 0.22040283679962158\n",
            "strain 0.2986071705818176\n",
            "strain 0.21818874776363373\n",
            "classify 1.9842529296875\n",
            "classify 1.8914794921875\n",
            "classify 1.957275390625\n",
            "classify 1.956298828125\n",
            "classify 1.9744873046875\n",
            "classify 1.93017578125\n",
            "classify 1.99041748046875\n",
            "classify 1.97705078125\n",
            "classify 1.912353515625\n",
            "classify 1.8446044921875\n",
            "classify 2.00146484375\n",
            "0.234375\n",
            "0.234375\n",
            "0.359375\n",
            "0.3125\n",
            "0.28125\n",
            "0.421875\n",
            "0.40625\n",
            "0.328125\n",
            "0.203125\n",
            "0.296875\n",
            "0.34375\n",
            "time: 3.8260550498962402 4.079637850460253\n",
            "950\n",
            "strain 0.19590264558792114\n",
            "strain 0.2920760214328766\n",
            "strain 0.2766008973121643\n",
            "strain 0.20808100700378418\n",
            "strain 0.27774327993392944\n",
            "strain 0.2616216838359833\n",
            "classify 2.057373046875\n",
            "classify 2.0767822265625\n",
            "classify 1.9473876953125\n",
            "classify 2.0247802734375\n",
            "classify 2.00146484375\n",
            "classify 1.9453125\n",
            "classify 2.0059814453125\n",
            "classify 2.0174560546875\n",
            "classify 1.98712158203125\n",
            "classify 2.044189453125\n",
            "classify 1.9141845703125\n",
            "0.359375\n",
            "0.265625\n",
            "0.375\n",
            "0.25\n",
            "0.21875\n",
            "0.328125\n",
            "0.3125\n",
            "0.359375\n",
            "0.375\n",
            "0.328125\n",
            "0.1875\n",
            "time: 4.2155139446258545 4.0797812630325465\n",
            "951\n",
            "strain 0.27788612246513367\n",
            "strain 0.24335342645645142\n",
            "strain 0.24623475968837738\n",
            "strain 0.28120699524879456\n",
            "strain 0.22326618432998657\n",
            "strain 0.23185589909553528\n",
            "classify 1.92724609375\n",
            "classify 2.0072021484375\n",
            "classify 2.0216064453125\n",
            "classify 1.93994140625\n",
            "classify 1.9888916015625\n",
            "classify 1.84375\n",
            "classify 1.9766845703125\n",
            "classify 2.00927734375\n",
            "classify 1.94189453125\n",
            "classify 2.03369140625\n",
            "classify 1.9019775390625\n",
            "0.375\n",
            "0.359375\n",
            "0.375\n",
            "0.3125\n",
            "0.3125\n",
            "0.359375\n",
            "0.234375\n",
            "0.40625\n",
            "0.265625\n",
            "0.375\n",
            "0.3125\n",
            "time: 4.0908074378967285 4.079793841147623\n",
            "952\n",
            "strain 0.22479383647441864\n",
            "strain 0.25425466895103455\n",
            "strain 0.17705023288726807\n",
            "strain 0.35180315375328064\n",
            "strain 0.28663361072540283\n",
            "strain 0.21917803585529327\n",
            "classify 1.98602294921875\n",
            "classify 1.992919921875\n",
            "classify 1.97216796875\n",
            "classify 1.92822265625\n",
            "classify 2.0438232421875\n",
            "classify 1.9566650390625\n",
            "classify 1.9580078125\n",
            "classify 1.9134521484375\n",
            "classify 1.98974609375\n",
            "classify 1.99462890625\n",
            "classify 2.00048828125\n",
            "0.375\n",
            "0.25\n",
            "0.296875\n",
            "0.234375\n",
            "0.203125\n",
            "0.390625\n",
            "0.296875\n",
            "0.265625\n",
            "0.296875\n",
            "0.34375\n",
            "0.359375\n",
            "time: 3.848351001739502 4.079551462861949\n",
            "953\n",
            "strain 0.2321711629629135\n",
            "strain 0.34159573912620544\n",
            "strain 0.2044685333967209\n",
            "strain 0.26861876249313354\n",
            "strain 0.2983630299568176\n",
            "strain 0.2069423347711563\n",
            "classify 2.0584716796875\n",
            "classify 1.9334716796875\n",
            "classify 1.856689453125\n",
            "classify 2.0933837890625\n",
            "classify 2.008056640625\n",
            "classify 1.932861328125\n",
            "classify 1.9171142578125\n",
            "classify 1.95703125\n",
            "classify 1.915283203125\n",
            "classify 1.8863525390625\n",
            "classify 1.950439453125\n",
            "0.21875\n",
            "0.3125\n",
            "0.359375\n",
            "0.3125\n",
            "0.28125\n",
            "0.3125\n",
            "0.25\n",
            "0.375\n",
            "0.296875\n",
            "0.296875\n",
            "0.296875\n",
            "time: 4.1192145347595215 4.079593511247535\n",
            "954\n",
            "strain 0.27017784118652344\n",
            "strain 0.2595040202140808\n",
            "strain 0.22855336964130402\n",
            "strain 0.2937909960746765\n",
            "strain 0.29537975788116455\n",
            "strain 0.2617432475090027\n",
            "classify 1.93896484375\n",
            "classify 1.9307861328125\n",
            "classify 2.0020751953125\n",
            "classify 1.8541259765625\n",
            "classify 2.056396484375\n",
            "classify 1.9549560546875\n",
            "classify 1.9979248046875\n",
            "classify 1.94287109375\n",
            "classify 1.8760986328125\n",
            "classify 2.0411376953125\n",
            "classify 1.949462890625\n",
            "0.265625\n",
            "0.328125\n",
            "0.3125\n",
            "0.328125\n",
            "0.390625\n",
            "0.265625\n",
            "0.296875\n",
            "0.265625\n",
            "0.34375\n",
            "0.34375\n",
            "0.46875\n",
            "time: 4.266584873199463 4.079789852222223\n",
            "955\n",
            "strain 0.23161575198173523\n",
            "strain 0.21939289569854736\n",
            "strain 0.20411956310272217\n",
            "strain 0.21328072249889374\n",
            "strain 0.2841026186943054\n",
            "strain 0.26444923877716064\n",
            "classify 2.0145263671875\n",
            "classify 1.9156494140625\n",
            "classify 2.024169921875\n",
            "classify 1.955322265625\n",
            "classify 1.9940185546875\n",
            "classify 2.0076904296875\n",
            "classify 1.9686279296875\n",
            "classify 1.8798828125\n",
            "classify 1.9940185546875\n",
            "classify 1.932373046875\n",
            "classify 1.94970703125\n",
            "0.296875\n",
            "0.4375\n",
            "0.34375\n",
            "0.34375\n",
            "0.3125\n",
            "0.28125\n",
            "0.328125\n",
            "0.34375\n",
            "0.296875\n",
            "0.265625\n",
            "0.21875\n",
            "time: 3.823051929473877 4.0795217286093965\n",
            "956\n",
            "strain 0.21700771152973175\n",
            "strain 0.20545946061611176\n",
            "strain 0.3128398060798645\n",
            "strain 0.21210874617099762\n",
            "strain 0.2125949114561081\n",
            "strain 0.3078692555427551\n",
            "classify 1.9566650390625\n",
            "classify 1.97119140625\n",
            "classify 1.8984375\n",
            "classify 1.9281005859375\n",
            "classify 2.01318359375\n",
            "classify 1.8851318359375\n",
            "classify 1.9521484375\n",
            "classify 1.96240234375\n",
            "classify 1.88720703125\n",
            "classify 1.964599609375\n",
            "classify 1.993896484375\n",
            "0.375\n",
            "0.359375\n",
            "0.375\n",
            "0.40625\n",
            "0.421875\n",
            "0.390625\n",
            "0.484375\n",
            "0.328125\n",
            "0.3125\n",
            "0.390625\n",
            "0.359375\n",
            "time: 4.076630353927612 4.079519212308224\n",
            "957\n",
            "strain 0.2487373948097229\n",
            "strain 0.21549932658672333\n",
            "strain 0.21646015346050262\n",
            "strain 0.2656053304672241\n",
            "strain 0.24193120002746582\n",
            "strain 0.3100467026233673\n",
            "classify 2.0238037109375\n",
            "classify 1.956787109375\n",
            "classify 1.9622802734375\n",
            "classify 2.0408935546875\n",
            "classify 1.958984375\n",
            "classify 1.968505859375\n",
            "classify 1.956787109375\n",
            "classify 1.9598388671875\n",
            "classify 1.9521484375\n",
            "classify 1.991455078125\n",
            "classify 1.9739990234375\n",
            "0.28125\n",
            "0.390625\n",
            "0.34375\n",
            "0.390625\n",
            "0.3125\n",
            "0.1875\n",
            "0.28125\n",
            "0.453125\n",
            "0.375\n",
            "0.28125\n",
            "0.234375\n",
            "time: 4.246841907501221 4.079694482379268\n",
            "958\n",
            "strain 0.28478413820266724\n",
            "strain 0.316742479801178\n",
            "strain 0.2592451572418213\n",
            "strain 0.3604829013347626\n",
            "strain 0.3325763940811157\n",
            "strain 0.3415488600730896\n",
            "classify 2.113525390625\n",
            "classify 1.9073486328125\n",
            "classify 1.9462890625\n",
            "classify 2.087890625\n",
            "classify 1.94287109375\n",
            "classify 2.0277099609375\n",
            "classify 2.004150390625\n",
            "classify 1.9443359375\n",
            "classify 1.9451904296875\n",
            "classify 1.9825439453125\n",
            "classify 1.97467041015625\n",
            "0.3125\n",
            "0.171875\n",
            "0.296875\n",
            "0.28125\n",
            "0.4375\n",
            "0.296875\n",
            "0.515625\n",
            "0.34375\n",
            "0.234375\n",
            "0.328125\n",
            "0.28125\n",
            "time: 3.8661742210388184 4.0794722877279685\n",
            "959\n",
            "strain 0.19945238530635834\n",
            "strain 0.225249245762825\n",
            "strain 0.211877703666687\n",
            "strain 0.264605849981308\n",
            "strain 0.26950275897979736\n",
            "strain 0.20068958401679993\n",
            "classify 1.8653564453125\n",
            "classify 1.928466796875\n",
            "classify 1.8568115234375\n",
            "classify 1.9718017578125\n",
            "classify 1.9744873046875\n",
            "classify 1.9599609375\n",
            "classify 1.90771484375\n",
            "classify 1.95849609375\n",
            "classify 1.9893798828125\n",
            "classify 2.046630859375\n",
            "classify 1.8919677734375\n",
            "0.34375\n",
            "0.296875\n",
            "0.390625\n",
            "0.359375\n",
            "0.234375\n",
            "0.3125\n",
            "0.4375\n",
            "0.390625\n",
            "0.265625\n",
            "0.3125\n",
            "0.296875\n",
            "time: 3.9068045616149902 4.07929289812843\n",
            "960\n",
            "strain 0.23057831823825836\n",
            "strain 0.20956334471702576\n",
            "strain 0.1934775859117508\n",
            "strain 0.26113560795783997\n",
            "strain 0.21772034466266632\n",
            "strain 0.23181724548339844\n",
            "classify 1.8988037109375\n",
            "classify 1.9368896484375\n",
            "classify 1.92822265625\n",
            "classify 2.0443115234375\n",
            "classify 1.9071044921875\n",
            "classify 1.99169921875\n",
            "classify 1.97265625\n",
            "classify 1.953369140625\n",
            "classify 1.96533203125\n",
            "classify 2.0264892578125\n",
            "classify 1.8826904296875\n",
            "0.3125\n",
            "0.28125\n",
            "0.234375\n",
            "0.21875\n",
            "0.421875\n",
            "0.296875\n",
            "0.234375\n",
            "0.296875\n",
            "0.34375\n",
            "0.328125\n",
            "0.234375\n",
            "time: 4.343420028686523 4.079568211171431\n",
            "961\n",
            "strain 0.2644306421279907\n",
            "strain 0.3108634948730469\n",
            "strain 0.24244137108325958\n",
            "strain 0.26324182748794556\n",
            "strain 0.2730432152748108\n",
            "strain 0.310352623462677\n",
            "classify 1.965087890625\n",
            "classify 1.9708251953125\n",
            "classify 1.899658203125\n",
            "classify 1.913330078125\n",
            "classify 1.995361328125\n",
            "classify 1.961181640625\n",
            "classify 1.9755859375\n",
            "classify 1.937255859375\n",
            "classify 1.96087646484375\n",
            "classify 1.94970703125\n",
            "classify 1.9075927734375\n",
            "0.296875\n",
            "0.3125\n",
            "0.296875\n",
            "0.234375\n",
            "0.3125\n",
            "0.375\n",
            "0.3125\n",
            "0.3125\n",
            "0.234375\n",
            "0.265625\n",
            "0.28125\n",
            "time: 3.829786777496338 4.0793090374702725\n",
            "962\n",
            "strain 0.3593021631240845\n",
            "strain 0.3414551615715027\n",
            "strain 0.2508409321308136\n",
            "strain 0.22163204848766327\n",
            "strain 0.25972118973731995\n",
            "strain 0.27594685554504395\n",
            "classify 1.966552734375\n",
            "classify 1.9962158203125\n",
            "classify 1.9986572265625\n",
            "classify 1.9693603515625\n",
            "classify 1.969482421875\n",
            "classify 1.9991455078125\n",
            "classify 1.9102783203125\n",
            "classify 1.9984130859375\n",
            "classify 2.02197265625\n",
            "classify 1.971923828125\n",
            "classify 1.9747314453125\n",
            "0.265625\n",
            "0.34375\n",
            "0.265625\n",
            "0.3125\n",
            "0.265625\n",
            "0.265625\n",
            "0.375\n",
            "0.296875\n",
            "0.328125\n",
            "0.34375\n",
            "0.3125\n",
            "time: 3.785963773727417 4.079004859874801\n",
            "963\n",
            "strain 0.3091046214103699\n",
            "strain 0.2679481506347656\n",
            "strain 0.3136948347091675\n",
            "strain 0.28653597831726074\n",
            "strain 0.231077641248703\n",
            "strain 0.2840103209018707\n",
            "classify 2.0096435546875\n",
            "classify 1.9786376953125\n",
            "classify 2.0474853515625\n",
            "classify 1.96240234375\n",
            "classify 1.9930419921875\n",
            "classify 1.990234375\n",
            "classify 1.9814453125\n",
            "classify 1.9615478515625\n",
            "classify 1.9305419921875\n",
            "classify 1.9541015625\n",
            "classify 1.90283203125\n",
            "0.375\n",
            "0.34375\n",
            "0.296875\n",
            "0.296875\n",
            "0.3125\n",
            "0.28125\n",
            "0.34375\n",
            "0.21875\n",
            "0.25\n",
            "0.28125\n",
            "0.328125\n",
            "time: 4.472273826599121 4.079414072373101\n",
            "964\n",
            "strain 0.3130151033401489\n",
            "strain 0.20266278088092804\n",
            "strain 0.22933383285999298\n",
            "strain 0.3015594482421875\n",
            "strain 0.2190275937318802\n",
            "strain 0.2641602158546448\n",
            "classify 1.963134765625\n",
            "classify 1.98583984375\n",
            "classify 1.893798828125\n",
            "classify 2.0478515625\n",
            "classify 1.9112548828125\n",
            "classify 1.8160400390625\n",
            "classify 1.9027099609375\n",
            "classify 1.94775390625\n",
            "classify 1.9788818359375\n",
            "classify 1.930908203125\n",
            "classify 1.8880615234375\n",
            "0.359375\n",
            "0.265625\n",
            "0.265625\n",
            "0.359375\n",
            "0.3125\n",
            "0.3125\n",
            "0.40625\n",
            "0.40625\n",
            "0.34375\n",
            "0.375\n",
            "0.28125\n",
            "time: 3.7928965091705322 4.079117605229116\n",
            "965\n",
            "strain 0.21752293407917023\n",
            "strain 0.2574408948421478\n",
            "strain 0.29660657048225403\n",
            "strain 0.30026209354400635\n",
            "strain 0.2557682991027832\n",
            "strain 0.23876187205314636\n",
            "classify 1.9791259765625\n",
            "classify 2.025146484375\n",
            "classify 1.9697265625\n",
            "classify 1.9193115234375\n",
            "classify 1.943603515625\n",
            "classify 1.9769287109375\n",
            "classify 2.066162109375\n",
            "classify 1.9468994140625\n",
            "classify 1.9378662109375\n",
            "classify 1.9781494140625\n",
            "classify 1.9105224609375\n",
            "0.3125\n",
            "0.359375\n",
            "0.34375\n",
            "0.3125\n",
            "0.359375\n",
            "0.328125\n",
            "0.203125\n",
            "0.359375\n",
            "0.328125\n",
            "0.25\n",
            "0.265625\n",
            "time: 3.7966864109039307 4.078825705046486\n",
            "966\n",
            "strain 0.2788380980491638\n",
            "strain 0.35601386427879333\n",
            "strain 0.3778010308742523\n",
            "strain 0.21104545891284943\n",
            "strain 0.24879346787929535\n",
            "strain 0.30109453201293945\n",
            "classify 1.96875\n",
            "classify 1.9708251953125\n",
            "classify 1.928955078125\n",
            "classify 1.9736328125\n",
            "classify 2.019775390625\n",
            "classify 2.0618896484375\n",
            "classify 2.006591796875\n",
            "classify 1.96044921875\n",
            "classify 1.93701171875\n",
            "classify 2.0325927734375\n",
            "classify 1.9666748046875\n",
            "0.28125\n",
            "0.28125\n",
            "0.234375\n",
            "0.234375\n",
            "0.3125\n",
            "0.28125\n",
            "0.3125\n",
            "0.296875\n",
            "0.4375\n",
            "0.390625\n",
            "0.265625\n",
            "time: 4.49575138092041 4.079257309251882\n",
            "967\n",
            "strain 0.23092058300971985\n",
            "strain 0.2261480987071991\n",
            "strain 0.2779279351234436\n",
            "strain 0.2974255084991455\n",
            "strain 0.27616453170776367\n",
            "strain 0.3422090709209442\n",
            "classify 1.9674072265625\n",
            "classify 2.01171875\n",
            "classify 1.908447265625\n",
            "classify 1.9334716796875\n",
            "classify 1.8226318359375\n",
            "classify 1.94091796875\n",
            "classify 1.944091796875\n",
            "classify 1.9898681640625\n",
            "classify 1.9190673828125\n",
            "classify 2.0145263671875\n",
            "classify 1.97357177734375\n",
            "0.296875\n",
            "0.21875\n",
            "0.296875\n",
            "0.296875\n",
            "0.25\n",
            "0.3125\n",
            "0.4375\n",
            "0.203125\n",
            "0.40625\n",
            "0.328125\n",
            "0.328125\n",
            "time: 4.179826021194458 4.079361692933012\n",
            "968\n",
            "strain 0.22755533456802368\n",
            "strain 0.2591822147369385\n",
            "strain 0.2367635816335678\n",
            "strain 0.30572086572647095\n",
            "strain 0.3333474099636078\n",
            "strain 0.2876396179199219\n",
            "classify 2.0921630859375\n",
            "classify 1.9239501953125\n",
            "classify 1.8582763671875\n",
            "classify 1.84942626953125\n",
            "classify 1.9503173828125\n",
            "classify 1.998046875\n",
            "classify 2.0443115234375\n",
            "classify 1.9991455078125\n",
            "classify 1.8272705078125\n",
            "classify 2.036376953125\n",
            "classify 1.865966796875\n",
            "0.3125\n",
            "0.296875\n",
            "0.3125\n",
            "0.421875\n",
            "0.296875\n",
            "0.34375\n",
            "0.296875\n",
            "0.265625\n",
            "0.375\n",
            "0.265625\n",
            "0.21875\n",
            "time: 4.143267393112183 4.0794282739499526\n",
            "969\n",
            "strain 0.23664861917495728\n",
            "strain 0.2997351884841919\n",
            "strain 0.30895209312438965\n",
            "strain 0.27319014072418213\n",
            "strain 0.24771475791931152\n",
            "strain 0.28734129667282104\n",
            "classify 1.917724609375\n",
            "classify 1.90380859375\n",
            "classify 1.9471435546875\n",
            "classify 1.9610595703125\n",
            "classify 2.0032958984375\n",
            "classify 1.9427490234375\n",
            "classify 1.98052978515625\n",
            "classify 2.0052490234375\n",
            "classify 1.941162109375\n",
            "classify 1.9759521484375\n",
            "classify 2.0357666015625\n",
            "0.421875\n",
            "0.359375\n",
            "0.359375\n",
            "0.359375\n",
            "0.328125\n",
            "0.28125\n",
            "0.328125\n",
            "0.34375\n",
            "0.359375\n",
            "0.34375\n",
            "0.25\n",
            "time: 4.424564599990845 4.079784408058088\n",
            "970\n",
            "strain 0.32714661955833435\n",
            "strain 0.2876157760620117\n",
            "strain 0.3056495785713196\n",
            "strain 0.24674150347709656\n",
            "strain 0.32766568660736084\n",
            "strain 0.3085573613643646\n",
            "classify 2.0498046875\n",
            "classify 1.9222412109375\n",
            "classify 1.9210205078125\n",
            "classify 1.963134765625\n",
            "classify 1.97216796875\n",
            "classify 1.9652099609375\n",
            "classify 1.962158203125\n",
            "classify 1.92578125\n",
            "classify 1.99365234375\n",
            "classify 2.104248046875\n",
            "classify 1.9583740234375\n",
            "0.328125\n",
            "0.40625\n",
            "0.359375\n",
            "0.328125\n",
            "0.265625\n",
            "0.359375\n",
            "0.28125\n",
            "0.390625\n",
            "0.296875\n",
            "0.34375\n",
            "0.34375\n",
            "time: 3.8370590209960938 4.079534891827835\n",
            "971\n",
            "strain 0.25038599967956543\n",
            "strain 0.290824294090271\n",
            "strain 0.2193402647972107\n",
            "strain 0.2591843903064728\n",
            "strain 0.34912174940109253\n",
            "strain 0.29421162605285645\n",
            "classify 1.9586181640625\n",
            "classify 1.9827880859375\n",
            "classify 2.050537109375\n",
            "classify 1.9522705078125\n",
            "classify 1.9710693359375\n",
            "classify 2.0220947265625\n",
            "classify 1.9417724609375\n",
            "classify 1.9720458984375\n",
            "classify 2.0006103515625\n",
            "classify 1.8985595703125\n",
            "classify 1.9517822265625\n",
            "0.3125\n",
            "0.296875\n",
            "0.390625\n",
            "0.25\n",
            "0.3125\n",
            "0.28125\n",
            "0.328125\n",
            "0.359375\n",
            "0.3125\n",
            "0.359375\n",
            "0.34375\n",
            "time: 3.7971203327178955 4.079244865556803\n",
            "972\n",
            "strain 0.26953428983688354\n",
            "strain 0.23856186866760254\n",
            "strain 0.2601633369922638\n",
            "strain 0.2446766495704651\n",
            "strain 0.36099904775619507\n",
            "strain 0.2963173985481262\n",
            "classify 1.865966796875\n",
            "classify 1.9486083984375\n",
            "classify 1.9066162109375\n",
            "classify 1.890380859375\n",
            "classify 1.8975830078125\n",
            "classify 1.9425048828125\n",
            "classify 1.985107421875\n",
            "classify 1.85394287109375\n",
            "classify 1.9898681640625\n",
            "classify 1.91455078125\n",
            "classify 1.896728515625\n",
            "0.375\n",
            "0.296875\n",
            "0.328125\n",
            "0.234375\n",
            "0.34375\n",
            "0.390625\n",
            "0.328125\n",
            "0.3125\n",
            "0.296875\n",
            "0.28125\n",
            "0.296875\n",
            "time: 4.4704430103302 4.07964738716447\n",
            "973\n",
            "strain 0.2791207432746887\n",
            "strain 0.3008226752281189\n",
            "strain 0.23845450580120087\n",
            "strain 0.3024154603481293\n",
            "strain 0.2537376582622528\n",
            "strain 0.35715121030807495\n",
            "classify 1.940185546875\n",
            "classify 2.02734375\n",
            "classify 1.9779052734375\n",
            "classify 1.9200439453125\n",
            "classify 1.94189453125\n",
            "classify 1.974853515625\n",
            "classify 1.97998046875\n",
            "classify 1.9635009765625\n",
            "classify 1.9490966796875\n",
            "classify 2.1024169921875\n",
            "classify 1.98291015625\n",
            "0.46875\n",
            "0.25\n",
            "0.296875\n",
            "0.328125\n",
            "0.328125\n",
            "0.359375\n",
            "0.375\n",
            "0.25\n",
            "0.234375\n",
            "0.25\n",
            "0.34375\n",
            "time: 3.7960402965545654 4.079356700487939\n",
            "974\n",
            "strain 0.30376315116882324\n",
            "strain 0.35015392303466797\n",
            "strain 0.239401176571846\n",
            "strain 0.29925447702407837\n",
            "strain 0.233598530292511\n",
            "strain 0.21517610549926758\n",
            "classify 1.925048828125\n",
            "classify 2.0067138671875\n",
            "classify 1.9984130859375\n",
            "classify 1.9365234375\n",
            "classify 2.0213623046875\n",
            "classify 1.857666015625\n",
            "classify 1.9078369140625\n",
            "classify 1.93450927734375\n",
            "classify 2.093994140625\n",
            "classify 2.0076904296875\n",
            "classify 1.90087890625\n",
            "0.390625\n",
            "0.34375\n",
            "0.375\n",
            "0.390625\n",
            "0.359375\n",
            "0.1875\n",
            "0.25\n",
            "0.40625\n",
            "0.265625\n",
            "0.34375\n",
            "0.296875\n",
            "time: 3.8147318363189697 4.079085790927594\n",
            "975\n",
            "strain 0.2538807988166809\n",
            "strain 0.25559377670288086\n",
            "strain 0.21422281861305237\n",
            "strain 0.26211079955101013\n",
            "strain 0.23326802253723145\n",
            "strain 0.3410680294036865\n",
            "classify 2.0648193359375\n",
            "classify 1.9200439453125\n",
            "classify 1.9119873046875\n",
            "classify 1.9149169921875\n",
            "classify 1.943603515625\n",
            "classify 1.9237060546875\n",
            "classify 1.9139404296875\n",
            "classify 1.9632568359375\n",
            "classify 1.9632568359375\n",
            "classify 1.965576171875\n",
            "classify 2.03759765625\n",
            "0.328125\n",
            "0.34375\n",
            "0.28125\n",
            "0.34375\n",
            "0.21875\n",
            "0.25\n",
            "0.40625\n",
            "0.390625\n",
            "0.3125\n",
            "0.4375\n",
            "0.3125\n",
            "time: 4.463531732559204 4.079480171936457\n",
            "976\n",
            "strain 0.21945513784885406\n",
            "strain 0.21684131026268005\n",
            "strain 0.31502604484558105\n",
            "strain 0.23561429977416992\n",
            "strain 0.28927749395370483\n",
            "strain 0.3593805730342865\n",
            "classify 1.82586669921875\n",
            "classify 1.9276123046875\n",
            "classify 1.996337890625\n",
            "classify 2.015380859375\n",
            "classify 1.9029541015625\n",
            "classify 2.0101318359375\n",
            "classify 1.9012451171875\n",
            "classify 1.9102783203125\n",
            "classify 1.915283203125\n",
            "classify 1.978515625\n",
            "classify 1.996826171875\n",
            "0.203125\n",
            "0.296875\n",
            "0.203125\n",
            "0.34375\n",
            "0.328125\n",
            "0.3125\n",
            "0.34375\n",
            "0.328125\n",
            "0.390625\n",
            "0.328125\n",
            "0.34375\n",
            "time: 3.8272223472595215 4.079222639849144\n",
            "977\n",
            "strain 0.2934618890285492\n",
            "strain 0.3223855793476105\n",
            "strain 0.29103022813796997\n",
            "strain 0.22323031723499298\n",
            "strain 0.2472122460603714\n",
            "strain 0.2732468843460083\n",
            "classify 1.9029541015625\n",
            "classify 1.9959716796875\n",
            "classify 2.0447998046875\n",
            "classify 1.96435546875\n",
            "classify 1.9014892578125\n",
            "classify 2.05859375\n",
            "classify 1.9412841796875\n",
            "classify 1.94677734375\n",
            "classify 1.9307861328125\n",
            "classify 2.009033203125\n",
            "classify 1.8843994140625\n",
            "0.25\n",
            "0.265625\n",
            "0.234375\n",
            "0.21875\n",
            "0.25\n",
            "0.328125\n",
            "0.296875\n",
            "0.421875\n",
            "0.3125\n",
            "0.28125\n",
            "0.296875\n",
            "time: 3.8078644275665283 4.078945700863875\n",
            "978\n",
            "strain 0.2404053956270218\n",
            "strain 0.21976031363010406\n",
            "strain 0.20362263917922974\n",
            "strain 0.19181407988071442\n",
            "strain 0.3293970227241516\n",
            "strain 0.23954017460346222\n",
            "classify 1.9559326171875\n",
            "classify 1.9149169921875\n",
            "classify 1.9544677734375\n",
            "classify 1.9822998046875\n",
            "classify 1.9376220703125\n",
            "classify 1.9525146484375\n",
            "classify 1.89892578125\n",
            "classify 1.9874267578125\n",
            "classify 1.9644775390625\n",
            "classify 1.94384765625\n",
            "classify 1.8482666015625\n",
            "0.359375\n",
            "0.453125\n",
            "0.375\n",
            "0.3125\n",
            "0.328125\n",
            "0.4375\n",
            "0.171875\n",
            "0.28125\n",
            "0.3125\n",
            "0.265625\n",
            "0.328125\n",
            "time: 4.4690141677856445 4.079344568018772\n",
            "979\n",
            "strain 0.28662464022636414\n",
            "strain 0.313537061214447\n",
            "strain 0.28581473231315613\n",
            "strain 0.2198379784822464\n",
            "strain 0.28002601861953735\n",
            "strain 0.305741548538208\n",
            "classify 1.8179931640625\n",
            "classify 1.970947265625\n",
            "classify 2.1007080078125\n",
            "classify 1.9854736328125\n",
            "classify 1.9940185546875\n",
            "classify 1.954833984375\n",
            "classify 1.968505859375\n",
            "classify 1.979248046875\n",
            "classify 2.044677734375\n",
            "classify 1.9178466796875\n",
            "classify 1.885009765625\n",
            "0.46875\n",
            "0.265625\n",
            "0.296875\n",
            "0.40625\n",
            "0.296875\n",
            "0.25\n",
            "0.21875\n",
            "0.34375\n",
            "0.3125\n",
            "0.40625\n",
            "0.265625\n",
            "time: 3.8163986206054688 4.079076707119844\n",
            "980\n",
            "strain 0.2572622299194336\n",
            "strain 0.20770592987537384\n",
            "strain 0.2799971401691437\n",
            "strain 0.19823701679706573\n",
            "strain 0.22798758745193481\n",
            "strain 0.2219281643629074\n",
            "classify 1.9566650390625\n",
            "classify 1.919921875\n",
            "classify 2.0198974609375\n",
            "classify 1.9908447265625\n",
            "classify 1.957763671875\n",
            "classify 1.91064453125\n",
            "classify 2.0052490234375\n",
            "classify 2.0703125\n",
            "classify 1.98822021484375\n",
            "classify 1.8785400390625\n",
            "classify 1.947265625\n",
            "0.265625\n",
            "0.328125\n",
            "0.21875\n",
            "0.25\n",
            "0.359375\n",
            "0.265625\n",
            "0.34375\n",
            "0.265625\n",
            "0.375\n",
            "0.34375\n",
            "0.4375\n",
            "time: 3.80614972114563 4.078799009809193\n",
            "981\n",
            "strain 0.23228518664836884\n",
            "strain 0.25980937480926514\n",
            "strain 0.2812880873680115\n",
            "strain 0.3594759404659271\n",
            "strain 0.29144448041915894\n",
            "strain 0.2494978904724121\n",
            "classify 1.958984375\n",
            "classify 1.91656494140625\n",
            "classify 2.077880859375\n",
            "classify 1.9071044921875\n",
            "classify 1.9453125\n",
            "classify 1.8265380859375\n",
            "classify 1.831298828125\n",
            "classify 1.921630859375\n",
            "classify 2.0294189453125\n",
            "classify 1.930419921875\n",
            "classify 1.92791748046875\n",
            "0.265625\n",
            "0.3125\n",
            "0.359375\n",
            "0.40625\n",
            "0.359375\n",
            "0.25\n",
            "0.28125\n",
            "0.40625\n",
            "0.328125\n",
            "0.296875\n",
            "0.390625\n",
            "time: 4.487876653671265 4.079215911643811\n",
            "982\n",
            "strain 0.24483975768089294\n",
            "strain 0.21233458817005157\n",
            "strain 0.26262322068214417\n",
            "strain 0.2772630453109741\n",
            "strain 0.31021150946617126\n",
            "strain 0.26725780963897705\n",
            "classify 1.8826904296875\n",
            "classify 1.93505859375\n",
            "classify 1.9371337890625\n",
            "classify 1.985107421875\n",
            "classify 1.9388427734375\n",
            "classify 1.8922119140625\n",
            "classify 1.9251708984375\n",
            "classify 2.03369140625\n",
            "classify 1.9130859375\n",
            "classify 1.9334716796875\n",
            "classify 1.9259033203125\n",
            "0.328125\n",
            "0.328125\n",
            "0.359375\n",
            "0.34375\n",
            "0.21875\n",
            "0.328125\n",
            "0.3125\n",
            "0.328125\n",
            "0.40625\n",
            "0.375\n",
            "0.390625\n",
            "time: 3.7864911556243896 4.078918606436992\n",
            "983\n",
            "strain 0.20369639992713928\n",
            "strain 0.24485450983047485\n",
            "strain 0.34379586577415466\n",
            "strain 0.3879585862159729\n",
            "strain 0.21657024323940277\n",
            "strain 0.23386093974113464\n",
            "classify 1.937255859375\n",
            "classify 2.02685546875\n",
            "classify 2.0255126953125\n",
            "classify 1.88916015625\n",
            "classify 1.9954833984375\n",
            "classify 2.0333251953125\n",
            "classify 1.927734375\n",
            "classify 2.0465087890625\n",
            "classify 1.9078369140625\n",
            "classify 1.989501953125\n",
            "classify 1.955322265625\n",
            "0.234375\n",
            "0.265625\n",
            "0.234375\n",
            "0.40625\n",
            "0.25\n",
            "0.34375\n",
            "0.4375\n",
            "0.34375\n",
            "0.296875\n",
            "0.375\n",
            "0.34375\n",
            "time: 3.8403143882751465 4.078676744689786\n",
            "984\n",
            "strain 0.2554914057254791\n",
            "strain 0.23750726878643036\n",
            "strain 0.26694202423095703\n",
            "strain 0.3604608476161957\n",
            "strain 0.23306751251220703\n",
            "strain 0.30542922019958496\n",
            "classify 1.97412109375\n",
            "classify 1.912353515625\n",
            "classify 1.955078125\n",
            "classify 1.9273681640625\n",
            "classify 1.9119873046875\n",
            "classify 2.0206298828125\n",
            "classify 1.917724609375\n",
            "classify 1.9796142578125\n",
            "classify 2.0108642578125\n",
            "classify 1.9794921875\n",
            "classify 1.86767578125\n",
            "0.359375\n",
            "0.40625\n",
            "0.296875\n",
            "0.3125\n",
            "0.328125\n",
            "0.328125\n",
            "0.390625\n",
            "0.3125\n",
            "0.296875\n",
            "0.3125\n",
            "0.34375\n",
            "time: 4.4890923500061035 4.079093865331659\n",
            "985\n",
            "strain 0.3034306466579437\n",
            "strain 0.2600069046020508\n",
            "strain 0.24376074969768524\n",
            "strain 0.29689982533454895\n",
            "strain 0.3177284002304077\n",
            "strain 0.21285291016101837\n",
            "classify 2.0323486328125\n",
            "classify 1.9771728515625\n",
            "classify 1.9342041015625\n",
            "classify 2.018798828125\n",
            "classify 1.9041748046875\n",
            "classify 1.9024658203125\n",
            "classify 1.9354248046875\n",
            "classify 2.077392578125\n",
            "classify 2.020263671875\n",
            "classify 2.032470703125\n",
            "classify 1.9052734375\n",
            "0.328125\n",
            "0.234375\n",
            "0.265625\n",
            "0.40625\n",
            "0.3125\n",
            "0.40625\n",
            "0.328125\n",
            "0.359375\n",
            "0.296875\n",
            "0.328125\n",
            "0.296875\n",
            "time: 3.9078338146209717 4.0789206615567934\n",
            "986\n",
            "strain 0.33751603960990906\n",
            "strain 0.23050446808338165\n",
            "strain 0.2765118181705475\n",
            "strain 0.25586998462677\n",
            "strain 0.23878955841064453\n",
            "strain 0.2725790739059448\n",
            "classify 1.990234375\n",
            "classify 1.9798583984375\n",
            "classify 1.94580078125\n",
            "classify 1.83392333984375\n",
            "classify 1.98779296875\n",
            "classify 1.9656982421875\n",
            "classify 2.01708984375\n",
            "classify 2.01806640625\n",
            "classify 1.92822265625\n",
            "classify 1.8515625\n",
            "classify 2.0169677734375\n",
            "0.3125\n",
            "0.28125\n",
            "0.28125\n",
            "0.203125\n",
            "0.296875\n",
            "0.34375\n",
            "0.296875\n",
            "0.296875\n",
            "0.34375\n",
            "0.25\n",
            "0.3125\n",
            "time: 4.0648462772369385 4.078906862689852\n",
            "987\n",
            "strain 0.22727161645889282\n",
            "strain 0.22030961513519287\n",
            "strain 0.2855769991874695\n",
            "strain 0.251327782869339\n",
            "strain 0.31078261137008667\n",
            "strain 0.2578513026237488\n",
            "classify 1.9405517578125\n",
            "classify 1.900146484375\n",
            "classify 2.005615234375\n",
            "classify 1.9625244140625\n",
            "classify 1.95703125\n",
            "classify 1.9835205078125\n",
            "classify 2.0028076171875\n",
            "classify 1.975341796875\n",
            "classify 1.91510009765625\n",
            "classify 2.0330810546875\n",
            "classify 1.9910888671875\n",
            "0.34375\n",
            "0.390625\n",
            "0.3125\n",
            "0.328125\n",
            "0.265625\n",
            "0.25\n",
            "0.4375\n",
            "0.40625\n",
            "0.34375\n",
            "0.359375\n",
            "0.375\n",
            "time: 4.479881286621094 4.079313157058438\n",
            "988\n",
            "strain 0.3535153865814209\n",
            "strain 0.2873156666755676\n",
            "strain 0.2403927445411682\n",
            "strain 0.21494168043136597\n",
            "strain 0.372880220413208\n",
            "strain 0.2324310690164566\n",
            "classify 1.93017578125\n",
            "classify 1.998779296875\n",
            "classify 2.011962890625\n",
            "classify 1.9478759765625\n",
            "classify 1.9542236328125\n",
            "classify 1.9693603515625\n",
            "classify 2.0167236328125\n",
            "classify 1.929931640625\n",
            "classify 1.9197998046875\n",
            "classify 1.8980712890625\n",
            "classify 1.9852294921875\n",
            "0.359375\n",
            "0.296875\n",
            "0.3125\n",
            "0.328125\n",
            "0.375\n",
            "0.25\n",
            "0.484375\n",
            "0.265625\n",
            "0.234375\n",
            "0.21875\n",
            "0.28125\n",
            "time: 3.871344804763794 4.079103416331014\n",
            "989\n",
            "strain 0.29400917887687683\n",
            "strain 0.20173890888690948\n",
            "strain 0.3276694715023041\n",
            "strain 0.27361828088760376\n",
            "strain 0.21412400901317596\n",
            "strain 0.2254214584827423\n",
            "classify 1.9625244140625\n",
            "classify 1.909423828125\n",
            "classify 1.9466552734375\n",
            "classify 1.986083984375\n",
            "classify 1.998779296875\n",
            "classify 1.97216796875\n",
            "classify 2.0057373046875\n",
            "classify 1.971435546875\n",
            "classify 1.9876708984375\n",
            "classify 1.975830078125\n",
            "classify 1.844970703125\n",
            "0.28125\n",
            "0.3125\n",
            "0.359375\n",
            "0.34375\n",
            "0.3125\n",
            "0.203125\n",
            "0.265625\n",
            "0.3125\n",
            "0.359375\n",
            "0.296875\n",
            "0.359375\n",
            "time: 3.8106281757354736 4.078832686308658\n",
            "990\n",
            "strain 0.2835395932197571\n",
            "strain 0.22166317701339722\n",
            "strain 0.3265302777290344\n",
            "strain 0.30694806575775146\n",
            "strain 0.24034219980239868\n",
            "strain 0.30026280879974365\n",
            "classify 2.018310546875\n",
            "classify 1.9569091796875\n",
            "classify 2.003173828125\n",
            "classify 1.959228515625\n",
            "classify 1.9163818359375\n",
            "classify 1.929443359375\n",
            "classify 1.9703369140625\n",
            "classify 2.0106201171875\n",
            "classify 2.0272216796875\n",
            "classify 2.057373046875\n",
            "classify 2.0628662109375\n",
            "0.328125\n",
            "0.234375\n",
            "0.40625\n",
            "0.359375\n",
            "0.234375\n",
            "0.3125\n",
            "0.25\n",
            "0.375\n",
            "0.34375\n",
            "0.296875\n",
            "0.40625\n",
            "time: 4.436065196990967 4.079193602416638\n",
            "991\n",
            "strain 0.21694311499595642\n",
            "strain 0.22842714190483093\n",
            "strain 0.24825136363506317\n",
            "strain 0.3065911531448364\n",
            "strain 0.31285756826400757\n",
            "strain 0.21972137689590454\n",
            "classify 1.9395751953125\n",
            "classify 1.96875\n",
            "classify 1.9752197265625\n",
            "classify 2.01953125\n",
            "classify 2.033935546875\n",
            "classify 1.988525390625\n",
            "classify 1.8804931640625\n",
            "classify 1.9461669921875\n",
            "classify 1.914794921875\n",
            "classify 1.9542236328125\n",
            "classify 2.0240478515625\n",
            "0.25\n",
            "0.234375\n",
            "0.375\n",
            "0.375\n",
            "0.375\n",
            "0.40625\n",
            "0.3125\n",
            "0.296875\n",
            "0.40625\n",
            "0.171875\n",
            "0.25\n",
            "time: 3.9365200996398926 4.079050950225322\n",
            "992\n",
            "strain 0.20687277615070343\n",
            "strain 0.2562582492828369\n",
            "strain 0.20973417162895203\n",
            "strain 0.23824946582317352\n",
            "strain 0.31299084424972534\n",
            "strain 0.24176600575447083\n",
            "classify 2.0213623046875\n",
            "classify 1.93316650390625\n",
            "classify 1.9959716796875\n",
            "classify 2.0013427734375\n",
            "classify 2.0699462890625\n",
            "classify 1.872314453125\n",
            "classify 2.112548828125\n",
            "classify 1.9874267578125\n",
            "classify 1.973876953125\n",
            "classify 1.96630859375\n",
            "classify 1.966064453125\n",
            "0.265625\n",
            "0.25\n",
            "0.296875\n",
            "0.34375\n",
            "0.28125\n",
            "0.28125\n",
            "0.265625\n",
            "0.296875\n",
            "0.359375\n",
            "0.265625\n",
            "0.328125\n",
            "time: 3.803571939468384 4.078774007785716\n",
            "993\n",
            "strain 0.2857482135295868\n",
            "strain 0.22621896862983704\n",
            "strain 0.28238213062286377\n",
            "strain 0.2585618197917938\n",
            "strain 0.3040022552013397\n",
            "strain 0.3218303918838501\n",
            "classify 1.9107666015625\n",
            "classify 1.8873291015625\n",
            "classify 1.931640625\n",
            "classify 1.9156494140625\n",
            "classify 1.948974609375\n",
            "classify 1.9747314453125\n",
            "classify 2.0509033203125\n",
            "classify 1.8978271484375\n",
            "classify 1.9766845703125\n",
            "classify 1.94268798828125\n",
            "classify 1.9381103515625\n",
            "0.359375\n",
            "0.265625\n",
            "0.25\n",
            "0.28125\n",
            "0.203125\n",
            "0.359375\n",
            "0.34375\n",
            "0.3125\n",
            "0.40625\n",
            "0.28125\n",
            "0.296875\n",
            "time: 4.269105672836304 4.07896591935839\n",
            "994\n",
            "strain 0.245921328663826\n",
            "strain 0.24721169471740723\n",
            "strain 0.21379722654819489\n",
            "strain 0.22749890387058258\n",
            "strain 0.27014365792274475\n",
            "strain 0.20851753652095795\n",
            "classify 2.1041259765625\n",
            "classify 1.93603515625\n",
            "classify 1.87744140625\n",
            "classify 1.9627685546875\n",
            "classify 2.0430908203125\n",
            "classify 1.964111328125\n",
            "classify 1.987548828125\n",
            "classify 1.949462890625\n",
            "classify 1.9384765625\n",
            "classify 1.935302734375\n",
            "classify 2.0338134765625\n",
            "0.265625\n",
            "0.28125\n",
            "0.34375\n",
            "0.34375\n",
            "0.375\n",
            "0.21875\n",
            "0.34375\n",
            "0.25\n",
            "0.296875\n",
            "0.234375\n",
            "0.296875\n",
            "time: 4.061037063598633 4.078948378922353\n",
            "995\n",
            "strain 0.394258588552475\n",
            "strain 0.2524486482143402\n",
            "strain 0.2507147490978241\n",
            "strain 0.27976760268211365\n",
            "strain 0.22285304963588715\n",
            "strain 0.3452327251434326\n",
            "classify 2.012451171875\n",
            "classify 1.967529296875\n",
            "classify 1.900390625\n",
            "classify 1.951171875\n",
            "classify 1.9034423828125\n",
            "classify 1.994384765625\n",
            "classify 1.9122314453125\n",
            "classify 1.953369140625\n",
            "classify 1.9630126953125\n",
            "classify 1.9898681640625\n",
            "classify 1.948486328125\n",
            "0.28125\n",
            "0.25\n",
            "0.21875\n",
            "0.296875\n",
            "0.359375\n",
            "0.390625\n",
            "0.21875\n",
            "0.328125\n",
            "0.34375\n",
            "0.3125\n",
            "0.296875\n",
            "time: 3.7980308532714844 4.078666830637369\n",
            "996\n",
            "strain 0.2688741981983185\n",
            "strain 0.3904244899749756\n",
            "strain 0.27950406074523926\n",
            "strain 0.22831140458583832\n",
            "strain 0.2662051022052765\n",
            "strain 0.3043364882469177\n",
            "classify 1.8978271484375\n",
            "classify 1.896240234375\n",
            "classify 2.0341796875\n",
            "classify 1.9599609375\n",
            "classify 1.9263916015625\n",
            "classify 2.0113525390625\n",
            "classify 1.977783203125\n",
            "classify 1.944091796875\n",
            "classify 1.9620361328125\n",
            "classify 1.9613037109375\n",
            "classify 2.0367431640625\n",
            "0.265625\n",
            "0.46875\n",
            "0.40625\n",
            "0.421875\n",
            "0.265625\n",
            "0.234375\n",
            "0.25\n",
            "0.390625\n",
            "0.265625\n",
            "0.390625\n",
            "0.234375\n",
            "time: 4.155184507369995 4.07874404056859\n",
            "997\n",
            "strain 0.21862494945526123\n",
            "strain 0.2939193844795227\n",
            "strain 0.2405778020620346\n",
            "strain 0.2811136245727539\n",
            "strain 0.24612052738666534\n",
            "strain 0.2486603856086731\n",
            "classify 2.0052490234375\n",
            "classify 1.9893798828125\n",
            "classify 1.946533203125\n",
            "classify 1.945556640625\n",
            "classify 2.0281982421875\n",
            "classify 1.9224853515625\n",
            "classify 2.014892578125\n",
            "classify 1.9110107421875\n",
            "classify 1.965087890625\n",
            "classify 2.030029296875\n",
            "classify 2.0059814453125\n",
            "0.390625\n",
            "0.359375\n",
            "0.296875\n",
            "0.453125\n",
            "0.234375\n",
            "0.421875\n",
            "0.328125\n",
            "0.390625\n",
            "0.25\n",
            "0.28125\n",
            "0.296875\n",
            "time: 4.187767505645752 4.078853723520267\n",
            "998\n",
            "strain 0.20995886623859406\n",
            "strain 0.25592201948165894\n",
            "strain 0.23340384662151337\n",
            "strain 0.24395757913589478\n",
            "strain 0.30595502257347107\n",
            "strain 0.22244593501091003\n",
            "classify 1.975830078125\n",
            "classify 1.96234130859375\n",
            "classify 1.9381103515625\n",
            "classify 1.9176025390625\n",
            "classify 2.04052734375\n",
            "classify 1.8465576171875\n",
            "classify 1.9935302734375\n",
            "classify 1.98095703125\n",
            "classify 2.0037841796875\n",
            "classify 2.0345458984375\n",
            "classify 1.9825439453125\n",
            "0.34375\n",
            "0.3125\n",
            "0.453125\n",
            "0.3125\n",
            "0.25\n",
            "0.3125\n",
            "0.3125\n",
            "0.265625\n",
            "0.25\n",
            "0.34375\n",
            "0.4375\n",
            "time: 3.8229944705963135 4.078598056588922\n",
            "999\n",
            "strain 0.2772834897041321\n",
            "strain 0.3408656120300293\n",
            "strain 0.24714013934135437\n",
            "strain 0.30106568336486816\n",
            "strain 0.37784942984580994\n",
            "strain 0.287384569644928\n",
            "classify 1.986083984375\n",
            "classify 1.9608154296875\n",
            "classify 1.8980712890625\n",
            "classify 1.953369140625\n",
            "classify 1.9593505859375\n",
            "classify 1.9906005859375\n",
            "classify 1.9385986328125\n",
            "classify 1.974609375\n",
            "classify 1.9677734375\n",
            "classify 2.0570068359375\n",
            "classify 1.9901123046875\n",
            "0.28125\n",
            "0.421875\n",
            "0.140625\n",
            "0.28125\n",
            "0.296875\n",
            "0.359375\n",
            "0.28125\n",
            "0.203125\n",
            "0.390625\n",
            "0.359375\n",
            "0.296875\n",
            "time: 4.077869176864624 4.078597799301147\n"
          ]
        }
      ],
      "source": [
        "# @title strain ctrain test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "def strain(model, dataloader, optim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.train()\n",
        "    for i, (x, _) in enumerate(dataloader):\n",
        "        x = x.to(device)#.to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            loss = model.loss(x)\n",
        "\n",
        "            # repr_loss, std_loss, cov_loss = model.loss(x)\n",
        "            # j_loss, (repr_loss, std_loss, cov_loss) = model.loss(x)\n",
        "            # loss = model.sim_coeff * repr_loss + model.std_coeff * std_loss + model.cov_coeff * cov_loss\n",
        "            # loss = j_loss + loss\n",
        "\n",
        "        optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 5) # 0.5\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            m=0.999 # 0.99 m = next(momentum_scheduler)\n",
        "            norms=[]\n",
        "            for param_q, param_k in zip(model.student.parameters(), model.teacher.parameters()):\n",
        "                param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        if i%10==0: print(\"strain\",loss.item())\n",
        "        # try: wandb.log({\"loss\": loss.item(), \"repr/I\": repr_loss.item(), \"std/V\": std_loss.item(), \"cov/C\": cov_loss.item()})\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=50: break\n",
        "\n",
        "\n",
        "def ctrain(model, classifier, dataloader, coptim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.eval()\n",
        "    classifier.train()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            with torch.no_grad():\n",
        "                sx = model(x).detach()\n",
        "            y_ = classifier(sx)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "        coptim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(coptim)\n",
        "        scaler.update()\n",
        "        print(\"classify\",loss.item())\n",
        "        try: wandb.log({\"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        with torch.no_grad():\n",
        "            sx = model(x)\n",
        "            rankme = RankMe(sx).item()\n",
        "            lidar = LiDAR(sx).item()\n",
        "            y_ = classifier(sx)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        print(correct/len(y))\n",
        "        # try: wandb.log({\"correct\": correct/len(y)})\n",
        "        try: wandb.log({\"correct\": correct/len(y), \"rankme\": rankme, \"lidar\": lidar})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "import time\n",
        "start = begin = time.time()\n",
        "# for i in range(1):\n",
        "for i in range(1000): # 1000\n",
        "    print(i)\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    strain(ijepa, train_loader, optim)\n",
        "    ctrain(ijepa, classifier, train_loader, coptim)\n",
        "    test(ijepa, classifier, test_loader)\n",
        "\n",
        "    # strain(violet, train_loader, voptim)\n",
        "    # ctrain(violet, classifier, train_loader, coptim)\n",
        "    # test(violet, classifier, test_loader)\n",
        "\n",
        "    print('time:',time.time() - start, (time.time()-begin)/(i+1))\n",
        "    start = time.time()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4J2ahp3wmqcg"
      },
      "outputs": [],
      "source": [
        "# @title supervised train test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "# def strain(model, dataloader, optim, scheduler=None):\n",
        "def strain(model, classifier, dataloader, optim, coptim, scheduler=None):\n",
        "    model.train()\n",
        "    classifier.train()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x = x.to(device)#.to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "        y = y.to(device)\n",
        "\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        # print(loss)\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # for p in list(filter(lambda p: p.grad is not None, model.parameters())):\n",
        "        #     print(p.grad.data.norm(2).item())\n",
        "        # print(\"max grad norm\", max([p.grad.data.norm(2).item() for p in list(filter(lambda p: p.grad is not None, model.parameters()))]))\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 10) # 0.5\n",
        "\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        print(\"strain\",loss.item())\n",
        "        # for param in ijepa.student.cls: print(param.data)\n",
        "        # for param in ijepa.predicter.cls: print(param.data)\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=50: break\n",
        "\n",
        "\n",
        "# def test(model, dataloader):\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        # .to(torch.bfloat16)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "        loss = F.cross_entropy(y_, y)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        print(correct/len(y))\n",
        "        try: wandb.log({\"correct\": correct/len(y), \"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "for i in range(1000):\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # np.random.shuffle(train_indices); np.random.shuffle(val_indices)\n",
        "    # train_sampler, valid_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(val_indices)\n",
        "    # # batch_size = 64 #512\n",
        "    # train_loader = DataLoader(train_data, sampler=train_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "    # test_loader = DataLoader(train_data, sampler=valid_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True)\n",
        "\n",
        "    # strain(ijepa, train_loader, optim)\n",
        "    strain(ijepa, classifier, train_loader, optim, coptim)\n",
        "    test(ijepa, classifier, test_loader)\n",
        "\n",
        "    # strain(violet, train_loader, voptim)\n",
        "    # test(violet, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzo9DMDPcOxu",
        "outputId": "2aa950ff-7a1c-46e0-924c-c6922b2ca522"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# @title save/load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder='/content/drive/MyDrive/jepa/'\n",
        "\n",
        "# # modelsd, optimsd = torch.load(folder+'SeqJEPA.pkl', map_location=device).values()\n",
        "# modelsd, optimsd = torch.load('SeqJEPA.pkl', map_location=device).values()\n",
        "# seq_jepa.load_state_dict(modelsd, strict=False)\n",
        "# optim.load_state_dict(optimsd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "DNNPOuUmcSNf",
        "outputId": "9c0fdeca-f315-457a-a102-ff87f9290f75"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'seq_jepa' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-06e8be6b0f78>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mseq_jepa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'IJEPA.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# torch.save(checkpoint, 'IJEPA.pkl')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'seq_jepa' is not defined"
          ]
        }
      ],
      "source": [
        "checkpoint = {'model': seq_jepa.state_dict(), 'optimizer': optim.state_dict()}\n",
        "torch.save(checkpoint, folder+'IJEPA.pkl')\n",
        "# torch.save(checkpoint, 'IJEPA.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLT74ihtMnh3"
      },
      "source": [
        "## drawer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMTvHTtTRInh",
        "outputId": "91ca330e-fd81-42c6-af3c-d4e5316df589"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "345200\n",
            "(tensor(1.9985, device='cuda:0', grad_fn=<MseLossBackward0>), (tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>), tensor(0.9865, device='cuda:0', grad_fn=<AddBackward0>), tensor(7.4222e-06, device='cuda:0', grad_fn=<AddBackward0>)))\n"
          ]
        }
      ],
      "source": [
        "# @title IJEPA + VICReg\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "    def __init__(self, patch_size, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        patch_size=4\n",
        "        act = nn.GELU()\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Conv2d(in_dim, d_model, patch_size, patch_size), # like patch\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 1, 7//2, bias=False)\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.MaxPool2d(3,2,1)\n",
        "            nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.BatchNorm2d(d_model), nn.ReLU(),\n",
        "            nn.Conv2d(d_model, d_model, 3, 2, 3//2, bias=False)\n",
        "            )\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, 32*32, d_model)*.02)\n",
        "        # self.pos_emb = nn.Parameter(RoPE2D(dim=d_model, h=8, w=8, base=1000), requires_grad=False)\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=(32//patch_size)**2, base=10000), requires_grad=False)\n",
        "\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, d_model, n_heads) for _ in range(nlayers)])\n",
        "\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "        # self.lin = nn.Linear(d_model, out_dim)\n",
        "        self.attn_pool = nn.Linear(d_model, 1, bias=False)\n",
        "\n",
        "        dim_v = d_model * 4\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(out_dim, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v, bias=False),\n",
        "            )\n",
        "\n",
        "    # def forward(self, x, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "    def jepa_fwd(self, x, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x).flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c] # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        # x = self.pos_enc(x)\n",
        "        # print(\"TransformerModel\",x.shape, self.pos_emb.shape)\n",
        "        x = x + self.pos_emb[:,:x.shape[1]]\n",
        "        if context_indices != None: x = x[torch.arange(x.shape[0]).unsqueeze(-1), context_indices] # [batch, num_context_toks, d_model]\n",
        "\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        out = self.norm(x)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out\n",
        "\n",
        "    def pool(self, x):\n",
        "        # sx = self.forward(x)\n",
        "        # attn = self.attn_pool(x).squeeze(-1) # [batch, seq] # seq_pool\n",
        "        # out = (torch.softmax(attn, dim=-1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, seq] @ [batch, seq, dim] -> [batch, dim]\n",
        "        # if self.lin: out = self.lin(out)\n",
        "        out = x.mean(dim=1)\n",
        "        return out\n",
        "\n",
        "    def forward(self, x, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.jepa_fwd(x, context_indices=None)\n",
        "        out = self.pool(x)\n",
        "        return out\n",
        "\n",
        "    def expand(self, x):\n",
        "        # sx = self.forward(x)\n",
        "        sx = self.pool(x)\n",
        "        vx = self.exp(sx)\n",
        "        return vx\n",
        "\n",
        "\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class IJEPAVICReg(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=64, out_dim=None, nlayers=1, n_heads=4):\n",
        "        super().__init__()\n",
        "        self.out_dim = out_dim = out_dim or d_model\n",
        "        self.patch_size = 4 # 4\n",
        "        self.student = ViT(self.patch_size, in_dim, d_model, out_dim=out_dim, n_heads=n_heads, nlayers=nlayers, drop=0.)\n",
        "        self.predicter = TransformerPredictor(out_dim, 3*d_model//8, out_dim, n_heads=4, nlayers=1, drop=0.)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "\n",
        "        # vicreg\n",
        "        self.sim_coeff=25. # 10.0 # 25.0 # λ\n",
        "        self.std_coeff=25. # 1.0 # 25.0 # µ\n",
        "        self.cov_coeff=1. # 25.0 # 1.0 # ν\n",
        "\n",
        "\n",
        "    def loss(self, x): #\n",
        "        b,c,h,w = x.shape\n",
        "        # print('ijepa loss x',x.shape)\n",
        "        hw=(8,8)\n",
        "        # hw=(32,32)\n",
        "        mask_collator = MaskCollator(hw, enc_mask_scale=(.85, 1.), pred_mask_scale=(.15, .2), aspect_ratio=(.75, 1.5), nenc=1, npred=4, min_keep=4, allow_overlap=False)\n",
        "        collated_masks_enc, collated_masks_pred = mask_collator(b)\n",
        "        context_indices, trg_indices = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "        # print(context_indices.shape, trg_indices.shape)\n",
        "\n",
        "        # context_indices, trg_indices = simplexmask2d(hw, ctx_scale=(.85,1), trg_scale=(.6,.8), B=b, chaos=3)\n",
        "\n",
        "        # zero_mask = torch.zeros(b ,*hw, device=device).flatten(1)\n",
        "        # zero_mask[torch.arange(b).unsqueeze(-1), context_indices] = 1\n",
        "        # x_ = x * F.interpolate(zero_mask.reshape(b,1,*hw), size=x.shape[2:], mode='nearest-exact') # zero masked locations\n",
        "\n",
        "        sx = self.student.jepa_fwd(x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "\n",
        "        # print('ijepa loss sx',sx.shape)\n",
        "        sy_ = self.predicter(sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "        sy_ = F.layer_norm(sy_, (sy_.size(-1),))\n",
        "        vx = self.student.expand(sx) # [batch, num_context_toks, out_dim]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            sy = self.teacher.jepa_fwd(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "            sy = sy[torch.arange(sy.shape[0]).unsqueeze(-1), trg_indices] # [batch, num_context_toks, d_model] # nan bec len(trg_ind)==0 # print('loss sy',torch.isnan(sy).any())\n",
        "            sy = F.layer_norm(sy, (sy.size(-1),))\n",
        "\n",
        "            vy = self.teacher.expand(sy.detach()) # [batch, num_trg_toks, out_dim]\n",
        "\n",
        "        vic_loss = self.vicreg(vx, vy)\n",
        "        j_loss = F.mse_loss(sy, sy_)\n",
        "        # return loss\n",
        "        return j_loss, vic_loss\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        out = self.student(x)\n",
        "        return out\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size, num_features = x.shape\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = self.sim_coeff * repr_loss + self.std_coeff * std_loss + self.cov_coeff * cov_loss\n",
        "        # print(\"in vicreg \",self.sim_coeff * repr_loss.item() , self.std_coeff * std_loss.item() , self.cov_coeff * cov_loss.item())\n",
        "        # return loss\n",
        "        return repr_loss, std_loss, cov_loss\n",
        "\n",
        "# for jepa+vic, attnpool < meanpool\n",
        "# jepa+vic < jepa\n",
        "\n",
        "\n",
        "ijepa = IJEPAVICReg(in_dim=3, d_model=64, out_dim=64, nlayers=1, n_heads=4).to(device)\n",
        "print(sum(p.numel() for p in ijepa.parameters() if p.requires_grad)) # 27584\n",
        "optim = torch.optim.AdamW(ijepa.parameters(), lr=1e-3) # 1e-3?\n",
        "# optim = torch.optim.AdamW([{'params': ijepa.student.parameters()},\n",
        "#     {'params': ijepa.predicter.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # good 3e-3,1e-3 ; default 1e-2, 5e-2\n",
        "#     # {'params': ijepa.predicter.parameters(), 'lr': 1e-2}], lr=1e-3, weight_decay=1e-2)\n",
        "\n",
        "x = torch.rand((2,3,32,32), device=device)\n",
        "loss = ijepa.loss(x)\n",
        "print(loss)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(ijepa.out_dim, 10).to(device)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ge36SCxOl2Oq"
      },
      "outputs": [],
      "source": [
        "# @title RoPE2D & RotEmb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "def RoPE(dim, seq_len=512, base=10000):\n",
        "    theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "    pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "    angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "    rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [1, seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "    return rot_emb\n",
        "\n",
        "# class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "#     def __init__(self, dim, seq_len=512, base=10000):\n",
        "#         super().__init__()\n",
        "#         self.dim, self.base = dim, base\n",
        "#         theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "#         pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "#         angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "#         self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [1, seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "#     def forward(self, x): # [batch, T, dim] / [batch, T, n_heads, d_head]\n",
        "#         seq_len = x.size(1)\n",
        "#         if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.base)\n",
        "#         # print('rope fwd', x.shape, self.rot_emb.shape)\n",
        "#         if x.dim()==4: return x * self.rot_emb[:,:seq_len].unsqueeze(2)\n",
        "#         return x * self.rot_emb[:,:seq_len]\n",
        "\n",
        "\n",
        "def RoPE2D(dim=16, h=8, w=8, base=10000):\n",
        "    # theta = 1. / (base ** (torch.arange(0, dim, step=4) / dim))\n",
        "    # theta = 1. / (base**torch.linspace(0,1,dim//4)).unsqueeze(0)\n",
        "    theta = 1. / (base**torch.linspace(0,1,dim//2)).unsqueeze(0)\n",
        "    y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\") # print(y,x) # [h,w], y:row_num, x:col_num\n",
        "    y, x = (y.reshape(-1,1) * theta).unsqueeze(-1), (x.reshape(-1,1) * theta).unsqueeze(-1) # [h*w,1]*[1,dim//4] = [h*w, dim//4, 1]\n",
        "    # rot_emb = torch.cat([x.sin(), x.cos(), y.sin(), y.cos()], dim=-1).flatten(-2) # [h*w, dim//4 ,4] -> [h*w, dim]\n",
        "    # rot_emb = torch.cat([x.sin(), x.cos(), y.sin(), y.cos()], dim=-1)#.reshape(dim, h, w).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "    rot_emb = torch.cat([x.sin(), y.sin()], dim=-1).flatten(-2).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "    # rot_emb = torch.cat([x.cos(), y.cos()], dim=-1).reshape(h, w, dim).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "    return rot_emb\n",
        "\n",
        "\n",
        "\n",
        "# class RoPE2D(nn.Module): # Rotary Positional Embeddings\n",
        "#     def __init__(self, dim, h=224, w=224, base=10000):\n",
        "#         super().__init__()\n",
        "#         self.dim, self.h, self.w = dim, h, w\n",
        "#         # # theta = 1. / (base ** (torch.arange(0, dim, step=4) / dim))\n",
        "#         # theta = 1. / (base**torch.linspace(0,1,dim//4)).unsqueeze(0)\n",
        "#         theta = 1. / (base**torch.linspace(0,1,dim//2)).unsqueeze(0)\n",
        "#         y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\") # print(y,x) # [h,w], y:row_num, x:col_num\n",
        "#         y, x = (y.reshape(-1,1) * theta).unsqueeze(-1), (x.reshape(-1,1) * theta).unsqueeze(-1) # [h*w,1]*[1,dim//4] = [h*w, dim//4, 1]\n",
        "#         # # self.rot_emb = torch.cat([x.sin(), x.cos(), y.sin(), y.cos()], dim=-1).flatten(-2) # [h*w, dim//4 ,4] -> [h*w, dim]\n",
        "#         # self.rot_emb = torch.cat([x.sin(), x.cos(), y.sin(), y.cos()], dim=-1).reshape(h, w, dim).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "#         self.rot_emb = torch.cat([x.sin(), y.sin()], dim=-1).reshape(h, w, dim).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "#         # self.rot_emb = torch.cat([x.cos(), y.cos()], dim=-1).reshape(h, w, dim).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "\n",
        "#     def forward(self, img): #\n",
        "#         # batch, dim, h, w = img.shape\n",
        "#         # print(img.shape)\n",
        "#         hw = img.shape[1] # [b, hw, dim] / [b, hw, n_heads, d_head]\n",
        "#         h=w=int(hw**.5)\n",
        "#         if self.h < h or self.w < w: self.__init__(self.dim, h, w)\n",
        "#         # print(self.rot_emb.shape)\n",
        "#         # rot_emb = self.rot_emb[:, :h, :w].unsqueeze(0) # [1, h, w, dim]\n",
        "#         rot_emb = self.rot_emb[:h, :w] # [h, w, dim]\n",
        "#         # return img * rot_emb.flatten(end_dim=1).unsqueeze(0) # [b, hw, dim] * [1, hw, dim]\n",
        "#         return img * rot_emb.flatten(end_dim=1)[None,:,None,:] # [b, hw, n_heads, d_head] * [1, hw, 1, dim]\n",
        "#         # return img * self.rot_emb\n",
        "\n",
        "class RotEmb(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, top=torch.pi, base=10000):\n",
        "        super().__init__()\n",
        "        self.theta = top / (base ** (torch.arange(0, dim, step=2, device=device) / dim))\n",
        "        # self.theta = top / (base ** torch.linspace(0, 1, dim//2, device=device))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (pos.unsqueeze(-1) * self.theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [seq_len, dim]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3bpiybH7M0O1"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# @title test multiblock params\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "dataiter = iter(train_data)\n",
        "img,y = next(dataiter)\n",
        "img = img.unsqueeze(0)\n",
        "img = F.interpolate(img, (8,8))\n",
        "b,c,h,w = img.shape\n",
        "# img = img.flatten(2).transpose(-2,-1).to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "print(img.shape)\n",
        "# batch, seq,_ = img.shape\n",
        "\n",
        "\n",
        "# # target_mask = randpatch(seq, mask_size=8, gamma=.9) # 8.9 [seq]\n",
        "# target_mask = multiblock(seq, min_s=0.2, max_s=0.3, M=4).any(0) # best.2.3M4 og.15.2M4# mask out targets to be predicted # [M, seq]\n",
        "# target_mask = multiblock(seq, min_s=0.2, max_s=0.3, M=4).any(0)\n",
        "\n",
        "target_mask = multiblock2d((8,8), M=4).any(0).unsqueeze(0)\n",
        "\n",
        "\n",
        "# context_mask = ~multiblock(seq, min_s=0.85, max_s=1., M=1)|target_mask # [1, seq], True->Mask\n",
        "context_mask = ~multiblock2d((8,8), scale=(.85,.85), aspect_ratio=(.9,1.1), M=1)|target_mask # [1, seq], True->Mask\n",
        "\n",
        "# print(target_mask, context_mask)\n",
        "\n",
        "print(target_mask.shape, context_mask.shape)\n",
        "# target_img, context_img = img*target_mask.unsqueeze(-1), img*context_mask.unsqueeze(-1)\n",
        "# target_img, context_img = img*~target_mask.unsqueeze(-1), img*~context_mask.unsqueeze(-1)\n",
        "target_img, context_img = img*~target_mask.unsqueeze(0), img*~context_mask.unsqueeze(0)\n",
        "# print(target_img.shape, context_img.shape)\n",
        "target_img, context_img = target_img.transpose(-2,-1).reshape(b,c,h,w), context_img.transpose(-2,-1).reshape(b,c,h,w)\n",
        "target_img, context_img = target_img.float(), context_img.float()\n",
        "\n",
        "# imshow(out.detach().cpu())\n",
        "imshow(target_img[0])\n",
        "imshow(context_img[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1iZQ6UNwoty",
        "outputId": "5554e8f3-71ec-4b70-c7d7-219b59e9a1d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9920\n",
            "torch.Size([4, 64, 16])\n"
          ]
        }
      ],
      "source": [
        "# @title ViT me more\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "# class LayerNorm2d(nn.LayerNorm):\n",
        "class LayerNorm2d(nn.RMSNorm):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        x = super().forward(x)\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads=None, d_head=8, cond_dim=None, dropout=0.): # .1\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_head = d_head\n",
        "        self.n_heads = d_model // d_head\n",
        "        # self.d_head = d_model // n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or d_model, 2*d_model, bias=False)\n",
        "        # self.k = nn.Sequential(nn.Dropout(dropout), nn.Linear(cond_dim, d_model, bias=False))\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        # self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        # self.lin = nn.Sequential(nn.Dropout(dropout), zero_module(nn.Linear(d_model, d_model)))\n",
        "        self.drop = nn.Dropout(dropout) # indp before q,k,v; after linout\n",
        "        self.scale = self.d_head ** -.5\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [batch, T, d_model]=[batch, h*w, c], [batch, num_tok, cond_dim], [batch,T]\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        Q = self.q(x).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        # K = self.k(x).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2)\n",
        "        K, V = self.kv(cond).unflatten(-1, (self.n_heads, 2*self.d_head)).transpose(1, 2).chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # linear attention # Softmax(Q) @ (Softmax(K).T @ V)\n",
        "        if mask != None:\n",
        "            mask = mask[:, None, :, None] # [batch,T] -> [batch,1,T,1]\n",
        "            K, V = K.masked_fill(mask, -torch.finfo(x.dtype).max), V.masked_fill(mask, -torch.finfo(x.dtype).max)\n",
        "        Q, K = Q.softmax(dim=-1)*self.scale, K.softmax(dim=-2)\n",
        "        context = K.transpose(-2,-1) @ V # [batch, n_heads, d_head, d_head]\n",
        "        out = Q @ context # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # (quadratic) attention # Softmax(Q @ K.T) @ V\n",
        "        # attn = Q @ K.transpose(-2,-1) * self.scale # [batch, n_heads, T] # [batch, n_heads, T, T/num_tok]\n",
        "        # if mask != None: attn = attn.masked_fill(mask[:, None, :, None], -torch.finfo(attn.dtype).max) # [batch,T]->[batch,1,T,1]\n",
        "        # attention = torch.softmax(attn, dim=-1)\n",
        "        # out = self.drop(attention) @ V # [batch, n_heads, T, d_head]\n",
        "\n",
        "        out = out.transpose(1, 2).flatten(2)\n",
        "        return self.drop(self.lin(out)) # [batch, T, d_model]\n",
        "\n",
        "# if self, dont pass cond_dim in init, dont pass cond in fwd\n",
        "# Softmax(Q @ K.T) @ V ~ Softmax(Q) @ Softmax(K).T @ V\n",
        "\n",
        "# https://github.com/lucidrains/x-transformers/blob/main/x_transformers/x_transformers.py#L1855\n",
        "\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_head, cond_dim=None, mult=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.cond_dim = cond_dim\n",
        "        self.self = MultiHeadAttention(d_model, d_head=d_head, dropout=0) # 16448\n",
        "        # self.self = Pooling()\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        # self.ff = nn.Sequential(\n",
        "        #     *[nn.BatchNorm2d(d_model), act, SeparableConv2d(d_model, d_model),]*3\n",
        "        #     )\n",
        "        # self.ff = ResBlock(d_model) # 74112\n",
        "        # self.ff = UIB(d_model, mult=4) # uib m4 36992, m2 18944\n",
        "        ff_dim=d_model*mult\n",
        "        self.ff = nn.Sequential(\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(dropout), nn.Linear(d_model, ff_dim), act, # ReLU GELU\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(dropout), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            nn.RMSNorm(d_model), act, nn.Dropout(dropout), nn.Linear(d_model, ff_dim),\n",
        "            nn.RMSNorm(ff_dim), act, nn.Dropout(dropout), zero_module(nn.Linear(ff_dim, d_model))\n",
        "        )\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,c,h,w], [batch, num_tok, cond_dim], [batch,T]\n",
        "        bchw = x.shape\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "        # print('attnblk fwd',x.shape)\n",
        "\n",
        "        # if self.cond_dim==None: cond=None # is self attn\n",
        "        x = x + self.drop(self.self(self.norm(x)))\n",
        "        # x = x.transpose(1,2).reshape(*bchw)\n",
        "        x = x + self.ff(x)\n",
        "        # x = self.ff(x)\n",
        "        # x = x + self.drop(self.norm2(self.ff(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "import torch\n",
        "# def apply_masks(x, masks): # [b,t,d], [M,mask_size] # https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "#     all_x = []\n",
        "#     for m in masks: # [1,T]\n",
        "#         mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [1,T,dim]\n",
        "#         all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "#     return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "def apply_masks(x, mask): # [b,t,d], [mask_size] # https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "    mask_keep = mask.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [1,T,dim]\n",
        "    # return torch.cat(torch.gather(x, dim=1, index=mask_keep), dim=0)  # [M*batch,mask_size,dim]\n",
        "    return torch.gather(x, dim=1, index=mask_keep) # [M*batch,mask_size,dim]\n",
        "\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        # self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        self.positional_emb = nn.Parameter(torch.zeros(1, 8*8, d_model)) # positional_embedding == 'learnable'\n",
        "        # self.positional_emb = nn.Parameter(torch.zeros(1, dim, 8,8))\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.zeros(1,1,d_model)) # randn zeros\n",
        "        nn.init.trunc_normal_(self.cls, std=.02)\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model)\n",
        "        self.lin = nn.Linear(d_model, out_dim)# if out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = x.shape\n",
        "        # x = x * self.pos_encoder(context_indices)\n",
        "        x = x + self.positional_emb[:,context_indices]\n",
        "        # x = apply_masks(x, [context_indices])\n",
        "\n",
        "        # pred_tokens = self.cls * self.pos_encoder(trg_indices) # [M, num_trg_toks, d_model]\n",
        "        pred_tokens = self.cls + self.positional_emb[0,trg_indices]\n",
        "        print(self.cls.shape, self.positional_emb[0,trg_indices].shape, trg_indices.shape)\n",
        "        pred_tokens = pred_tokens.repeat(batch, 1, 1) # [batch*M, num_trg_toks, d_model]\n",
        "        # print(pred_tokens.requires_grad)\n",
        "        print(\"pred fwd\", x.shape, pred_tokens.shape)\n",
        "        # x = x.repeat_interleave(trg_indices.shape[0], dim=0) # [batch, seq_len, d_model] -> [batch*M, seq_len, d_model]\n",
        "        x = torch.cat([x, pred_tokens], dim=1) # [batch*M, seq_len+num_trg_toks, d_model]\n",
        "\n",
        "        # x = x.transpose(1,2).unflatten(-1, (8,8))#.reshape(*bchw)\n",
        "        out = self.transformer_encoder(x) # float [seq_len, batch_size, d_model]\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "\n",
        "        out = self.norm(out)\n",
        "        out = out[:,seq:] # [batch*M, num_trg_toks, d_model]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Sequential(\n",
        "            # # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.MaxPool1d(2,2), #nn.MaxPool1d(3, 2, 3//2),\n",
        "            # # nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.Conv1d(d_model, d_model,3,2,3//2)\n",
        "            # nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            # nn.Conv1d(d_model, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            # nn.Conv1d(d_model, d_model,3,2,3//2),\n",
        "            # # nn.Conv1d(in_dim, d_model,2,2,0), # like patch\n",
        "            nn.Conv2d(in_dim, d_model, kernel_size=7, stride=2, padding=7//2, bias=False), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "            )\n",
        "        # self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        self.positional_emb = nn.Parameter(torch.zeros(1, 8*8, d_model)) # positional_embedding == 'learnable'\n",
        "        # self.positional_emb = nn.Parameter(torch.zeros(1, dim, 8,8)) # positional_embedding == 'learnable'\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model)\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # x = self.embed(x.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "        x = self.embed(x)\n",
        "        # bchw = x.shape\n",
        "        x = x.flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c] # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "\n",
        "        # x = self.pos_encoder(x)\n",
        "        # x = x * self.pos_encoder(torch.arange(seq, device=device)).unsqueeze(0)\n",
        "        # print(x.shape, self.positional_emb.shape)\n",
        "        x = x + self.positional_emb\n",
        "        if context_indices != None:\n",
        "            # x = apply_masks(x, [context_indices])\n",
        "            x = apply_masks(x, context_indices)\n",
        "\n",
        "\n",
        "        # x = x.transpose(1,2).reshape(*bchw)\n",
        "        x = self.transformer_encoder(x) # float [seq_len, batch_size, d_model]\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "        out = self.norm(x)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "\n",
        "# pos_emb rope < learn < learned\n",
        "# conv > pixel?\n",
        "# droppath not required\n",
        "\n",
        "# norm,act,conv < conv,norm,act\n",
        "# 2*s1 < uib < resblock\n",
        "# gatedadaln 3 < 2 = 1 < ffmult4 = 2*gatedadaln\n",
        "# MaxPool2d(2,2) < MaxPool2d(3,2,3//2)\n",
        "\n",
        "\n",
        "# dim = 64\n",
        "# dim_head = 8\n",
        "# heads = dim // dim_head\n",
        "# num_classes = 10\n",
        "# # model = SimpleViT(image_size=32, patch_size=4, num_classes=num_classes, dim=dim, depth=1, heads=heads, mlp_dim=dim*4, channels = 3, dim_head = dim_head)\n",
        "# model = SimpleViT(in_dim=3, out_dim=num_classes, dim=dim, depth=1, heads=heads, mlp_dim=dim*4, channels = 3, dim_head = dim_head).to(device)\n",
        "# print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 59850\n",
        "# optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "# # print(images.shape) # [batch, 3, 32, 32]\n",
        "# logits = model(x)\n",
        "# print(logits.shape)\n",
        "# # print(logits[0])\n",
        "# # print(logits[0].argmax(1))\n",
        "# pred_probab = nn.Softmax(dim=1)(logits)\n",
        "# y_pred = pred_probab.argmax(1)\n",
        "# # print(f\"Predicted class: {y_pred}\")\n",
        "\n",
        "\n",
        "batch, seq_len, d_model = 4,1024,16\n",
        "in_dim = 3\n",
        "model = TransformerModel(in_dim, d_model, d_head=4, nlayers=2, dropout=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x = torch.rand((batch, in_dim, 32, 32), device=device)\n",
        "# x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "# # # print(out)\n",
        "# model = TransformerPredictor(in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1).to(device)\n",
        "# out = model(out)\n",
        "# print(out.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "e3dAhWh45F4M"
      },
      "outputs": [],
      "source": [
        "# @title simplex\n",
        "# !pip install -q opensimplex\n",
        "import opensimplex\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def simplexmask2d(hw=(32,32), ctx_scale=(.85,1), trg_scale=(.6,.8), B=64, chaos=2):\n",
        "    seed = np.random.randint(1e10, size=B)\n",
        "    ix = iy = np.linspace(0, chaos, num=max(hw))\n",
        "    noise = opensimplex.noise3array(ix, iy, seed) # [b,h,w]\n",
        "    # plt.pcolormesh(noise[:1])\n",
        "    # plt.rcParams[\"figure.figsize\"] = (20,3)\n",
        "    # plt.show()\n",
        "    noise = torch.from_numpy(noise)\n",
        "    ctx_mask_scale = torch.rand(1) * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    trg_mask_scale = torch.rand(1) * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "    val, ind = noise.flatten(1).sort() # [b,h*w]\n",
        "    seq = hw[0]*hw[1]\n",
        "    # trg_index = ind[:,-int(seq*trg_mask_scale):]\n",
        "    # ctx_index = ind[:,-int(seq*ctx_mask_scale):-int(seq*trg_mask_scale)] # ctx wraps trg ; most similar to multiblock\n",
        "    # ctx_index = ind[:,:int(seq*ctx_mask_scale)-int(seq*trg_mask_scale)] # ctx hug bottom\n",
        "    # ctx_index = ind[:,-int(seq*ctx_mask_scale)-int(seq*trg_mask_scale):-int(seq*trg_mask_scale)] # ctx wraps trg ; most similar to multiblock\n",
        "    # ctx_index = ind[:,:int(seq*ctx_mask_scale)] # ctx hug bottom\n",
        "    return ctx_index, trg_index\n",
        "\n",
        "\n",
        "def simplexmask2d(hw=(32,32), ctx_scale=(.85,1), trg_scale=(.6,.8), B=64, chaos=2):\n",
        "    seed = np.random.randint(1e10, size=B)\n",
        "    ix = np.linspace(0, chaos, num=hw[1])\n",
        "    iy = np.linspace(0, chaos, num=hw[0])\n",
        "    noise = opensimplex.noise3array(ix, iy, seed) # [b,h,w]\n",
        "    noise = torch.from_numpy(noise)\n",
        "    ctx_mask_scale = torch.rand(1) * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    trg_mask_scale = torch.rand(1) * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "    val, ind = noise.flatten(1).sort() # [b,h*w]\n",
        "\n",
        "    seq = hw[0]*hw[1]\n",
        "\n",
        "    ctx_len = int(seq*ctx_mask_scale)\n",
        "    trg_len = int(seq*trg_mask_scale)\n",
        "    trg_pos = (torch.rand(B) * (seq-trg_len)).int()\n",
        "    # print(trg_pos)\n",
        "\n",
        "    trg_ind = trg_pos.unsqueeze(-1) + torch.arange(trg_len).unsqueeze(0)\n",
        "    trg_index = ind[torch.arange(B).unsqueeze(-1), trg_ind]\n",
        "    ctx_len = ctx_len - trg_len\n",
        "\n",
        "    remove_mask = torch.ones((B,seq), dtype=bool) # [B, S]\n",
        "    remove_mask.scatter_(1, trg_ind, False).flatten()\n",
        "    ctx_ind = torch.arange(seq).unsqueeze(0).repeat(B,1)[remove_mask].reshape(B, -1)\n",
        "    ctx_index = ind[torch.arange(B).unsqueeze(-1), ctx_ind[:,-ctx_len:]]\n",
        "    return ctx_index, trg_index\n",
        "\n",
        "\n",
        "# @title simplex\n",
        "!pip install opensimplex\n",
        "from opensimplex import OpenSimplex\n",
        "\n",
        "seed=0\n",
        "noise = OpenSimplex(seed)  # Replace 'seed' with your starting value\n",
        "noise_scale = 10  # Adjust for desired noise range\n",
        "\n",
        "def get_noise(x, y):\n",
        "    global seed  # Assuming seed is a global variable\n",
        "    # noise_val = noise.noise2(x / noise_scale, y / noise_scale)\n",
        "    noise_val = noise.noise2(x, y)\n",
        "    # seed += 1  # Increment seed after each call\n",
        "    return noise_val\n",
        "\n",
        "x,y=0,0\n",
        "a=[[],[],[],[],[]]\n",
        "\n",
        "fps=2\n",
        "f=[]\n",
        "for i in range(10*fps):\n",
        "    f.append(i/fps)\n",
        "    x = x+0.3\n",
        "    # y = y+1\n",
        "    # noise_value = get_noise(x, y)\n",
        "    for k,j in enumerate([0,1,2,3,4]):\n",
        "        a[k].append(get_noise(x, y+j))\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "for aa in a:\n",
        "    plt.plot(f,aa)\n",
        "plt.show()\n",
        "\n",
        "# b,y,g,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vtPSM8mbnJZy"
      },
      "outputs": [],
      "source": [
        "# @title pos_embed.py\n",
        "# https://github.com/facebookresearch/mae/blob/main/util/pos_embed.py#L20\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim=16, grid_size=8, cls_token=False): #\n",
        "    grid_h = np.arange(grid_size, dtype=np.float32)\n",
        "    grid_w = np.arange(grid_size, dtype=np.float32)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token: pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed # [(1+)grid_size*grid_size, embed_dim]\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    # assert embed_dim % 2 == 0\n",
        "\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1) # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos): #\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=float)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega  # (D/2,)\n",
        "\n",
        "    pos = pos.reshape(-1)  # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
        "\n",
        "    emb_sin = np.sin(out) # (M, D/2)\n",
        "    emb_cos = np.cos(out) # (M, D/2)\n",
        "\n",
        "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def interpolate_pos_embed(model, checkpoint_model):\n",
        "    if 'pos_embed' in checkpoint_model:\n",
        "        pos_embed_checkpoint = checkpoint_model['pos_embed']\n",
        "        embedding_size = pos_embed_checkpoint.shape[-1]\n",
        "        num_patches = model.patch_embed.num_patches\n",
        "        num_extra_tokens = model.pos_embed.shape[-2] - num_patches\n",
        "        # height (== width) for the checkpoint position embedding\n",
        "        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n",
        "        # height (== width) for the new position embedding\n",
        "        new_size = int(num_patches ** 0.5)\n",
        "        # class_token and dist_token are kept unchanged\n",
        "        if orig_size != new_size:\n",
        "            print(\"Position interpolate from %dx%d to %dx%d\" % (orig_size, orig_size, new_size, new_size))\n",
        "            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n",
        "            # only the position tokens are interpolated\n",
        "            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n",
        "            pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n",
        "            pos_tokens = torch.nn.functional.interpolate(\n",
        "                pos_tokens, size=(new_size, new_size), mode='bicubic', align_corners=False)\n",
        "            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n",
        "            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n",
        "            checkpoint_model['pos_embed'] = new_pos_embed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "dLbXQ-3XXRMq"
      },
      "outputs": [],
      "source": [
        "# @title ijepa src.utils.tensors\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/utils/tensors.py\n",
        "\n",
        "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
        "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n",
        "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
        "    def norm_cdf(x):\n",
        "        # Computes standard normal cumulative distribution function\n",
        "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Values are generated by using a truncated uniform distribution and\n",
        "        # then using the inverse CDF for the normal distribution.\n",
        "        # Get upper and lower cdf values\n",
        "        l = norm_cdf((a - mean) / std)\n",
        "        u = norm_cdf((b - mean) / std)\n",
        "\n",
        "        # Uniformly fill tensor with values from [l, u], then translate to\n",
        "        # [2l-1, 2u-1].\n",
        "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
        "\n",
        "        # Use inverse cdf transform for normal distribution to get truncated\n",
        "        # standard normal\n",
        "        tensor.erfinv_()\n",
        "\n",
        "        # Transform to proper mean, std\n",
        "        tensor.mul_(std * math.sqrt(2.))\n",
        "        tensor.add_(mean)\n",
        "\n",
        "        # Clamp to ensure it's in the proper range\n",
        "        tensor.clamp_(min=a, max=b)\n",
        "        return tensor\n",
        "\n",
        "\n",
        "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
        "    # type: (Tensor, float, float, float, float) -> Tensor\n",
        "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n",
        "\n",
        "def repeat_interleave_batch(x, B, repeat): # [batch*M,...]? , M?\n",
        "    N = len(x) // B\n",
        "    x = torch.cat([\n",
        "        torch.cat([x[i*B:(i+1)*B] for _ in range(repeat)], dim=0)\n",
        "        for i in range(N)\n",
        "    ], dim=0)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "COvEnBXPYth3"
      },
      "outputs": [],
      "source": [
        "# @title ijepa multiblock down\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "import math\n",
        "from multiprocessing import Value\n",
        "import torch\n",
        "_GLOBAL_SEED = 0\n",
        "\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=(224, 224),\n",
        "        patch_size=16,\n",
        "        enc_mask_scale=(0.2, 0.8),\n",
        "        pred_mask_scale=(0.2, 0.8),\n",
        "        aspect_ratio=(0.3, 3.0),\n",
        "        nenc=1,\n",
        "        npred=2,\n",
        "        min_keep=4,\n",
        "        allow_overlap=False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if not isinstance(input_size, tuple):\n",
        "            input_size = (input_size, ) * 2\n",
        "        self.patch_size = patch_size\n",
        "        self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.aspect_ratio = aspect_ratio\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "        self._itr_counter = Value('i', -1)  # collator is shared across worker processes\n",
        "\n",
        "    def step(self):\n",
        "        i = self._itr_counter\n",
        "        with i.get_lock():\n",
        "            i.value += 1\n",
        "            v = i.value\n",
        "        return v\n",
        "\n",
        "    def _sample_block_size(self, generator, scale, aspect_ratio_scale):\n",
        "        _rand = torch.rand(1, generator=generator).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.height * self.width * mask_scale)\n",
        "        # -- Sample block aspect-ratio\n",
        "        min_ar, max_ar = aspect_ratio_scale\n",
        "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
        "        while h >= self.height:\n",
        "            h -= 1\n",
        "        while w >= self.width:\n",
        "            w -= 1\n",
        "        return (h, w)\n",
        "\n",
        "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
        "        h, w = b_size\n",
        "\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # --\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            top = torch.randint(0, self.height - h, (1,))\n",
        "            left = torch.randint(0, self.width - w, (1,))\n",
        "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
        "            mask[top:top+h, left:left+w] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
        "        mask_complement[top:top+h, left:left+w] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        B = len(batch)\n",
        "\n",
        "        collated_batch = torch.utils.data.default_collate(batch)\n",
        "\n",
        "        seed = self.step()\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(seed)\n",
        "        p_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.pred_mask_scale,\n",
        "            aspect_ratio_scale=self.aspect_ratio)\n",
        "        e_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.enc_mask_scale,\n",
        "            aspect_ratio_scale=(1., 1.))\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.height * self.width\n",
        "        min_keep_enc = self.height * self.width\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                print(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "        return collated_batch, collated_masks_enc, collated_masks_pred\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aH1UOD4WW8I2",
        "outputId": "e8671cd3-eba7-4649-84ca-94c6467ce62d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Mask generator says: \"Valid mask not found, decreasing acceptable-regions [1]\"\n",
            "WARNING:root:Mask generator says: \"Valid mask not found, decreasing acceptable-regions [2]\"\n",
            "WARNING:root:Mask generator says: \"Valid mask not found, decreasing acceptable-regions [3]\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vit fwd torch.Size([4, 3, 224, 224])\n",
            "vit fwd torch.Size([4, 196, 768])\n",
            "vit fwd torch.Size([4, 11, 768])\n",
            "predictor fwd1 torch.Size([4, 11, 384]) torch.Size([4, 196, 384])\n"
          ]
        }
      ],
      "source": [
        "# @title ijepa vision_transformer.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/models/vision_transformer.py\n",
        "import math\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# from src.utils.tensors import (trunc_normal_, repeat_interleave_batch)\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid height and width\n",
        "    return:\n",
        "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid_h = np.arange(grid_size, dtype=float)\n",
        "    grid_w = np.arange(grid_size, dtype=float)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0) # [1+h*w, dim]\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    assert embed_dim % 2 == 0\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid length\n",
        "    return:\n",
        "    pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid = np.arange(grid_size, dtype=float)\n",
        "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=float)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega   # (D/2,)\n",
        "    pos = pos.reshape(-1)   # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
        "    emb = np.concatenate([np.sin(out), np.cos(out)], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\"\"\"\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x, attn\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x, return_attention=False):\n",
        "        y, attn = self.attn(self.norm1(x))\n",
        "        if return_attention:\n",
        "            return attn\n",
        "        x = x + self.drop_path(y)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2) # ->[b,h*w,c]\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvEmbed(nn.Module):\n",
        "    \"\"\"3x3 Convolution stems for ViT following ViTC models\"\"\"\n",
        "    def __init__(self, channels, strides, img_size=224, in_chans=3, batch_norm=True):\n",
        "        super().__init__()\n",
        "        # Build the stems\n",
        "        stem = []\n",
        "        channels = [in_chans] + channels\n",
        "        for i in range(len(channels) - 2):\n",
        "            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3, stride=strides[i], padding=1, bias=(not batch_norm))]\n",
        "            if batch_norm:\n",
        "                stem += [nn.BatchNorm2d(channels[i+1])]\n",
        "            stem += [nn.ReLU(inplace=True)]\n",
        "        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n",
        "        self.stem = nn.Sequential(*stem)\n",
        "\n",
        "        # Comptute the number of patches\n",
        "        stride_prod = int(np.prod(strides))\n",
        "        self.num_patches = (img_size[0] // stride_prod)**2\n",
        "\n",
        "    def forward(self, x):\n",
        "        p = self.stem(x)\n",
        "        return p.flatten(2).transpose(1, 2)\n",
        "\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "def apply_masks(x, masks):\n",
        "    \"\"\":param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
        "    :param masks: list of tensors containing indices of patches in [N] to keep\"\"\"\n",
        "    all_x = []\n",
        "    for m in masks:\n",
        "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [batch,T,dim]\n",
        "        all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "    return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "\n",
        "class VisionTransformerPredictor(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, num_patches, embed_dim=768, predictor_embed_dim=384,\n",
        "        depth=6, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.init_std = init_std\n",
        "        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        # --\n",
        "        self.predictor_pos_embed = nn.Parameter(torch.zeros(1, num_patches, predictor_embed_dim), requires_grad=False)\n",
        "        predictor_pos_embed = get_2d_sincos_pos_embed(self.predictor_pos_embed.shape[-1], int(num_patches**.5), cls_token=False)\n",
        "        self.predictor_pos_embed.data.copy_(torch.from_numpy(predictor_pos_embed).float().unsqueeze(0)) # [1, num_patches, predictor_embed_dim]\n",
        "        # --\n",
        "        self.predictor_blocks = nn.ModuleList([Block(dim=predictor_embed_dim, num_heads=num_heads, drop=drop_rate, drop_path=dpr[i]) for i in range(depth)])\n",
        "        self.predictor_norm = nn.LayerNorm(predictor_embed_dim)\n",
        "        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n",
        "        # ------\n",
        "        # trunc_normal_(self.mask_token, std=self.init_std)\n",
        "\n",
        "    def forward(self, x, masks_x, masks): # [batch, num_context_patches, embed_dim], nenc*[batch, num_context_patches], npred*[batch, num_target_patches]\n",
        "        assert (masks is not None) and (masks_x is not None), 'Cannot run predictor without mask indices'\n",
        "        if not isinstance(masks_x, list): masks_x = [masks_x] # context mask\n",
        "        if not isinstance(masks, list): masks = [masks] # pred mask\n",
        "\n",
        "        # -- Batch Size\n",
        "        B = len(x) // len(masks_x)\n",
        "        # print(\"predictor fwd0\", x.shape) # [3, 121, 768])\n",
        "        # -- map from encoder-dim to pedictor-dim\n",
        "        x = self.predictor_embed(x) # [batch, num_patches, predictor_embed_dim]\n",
        "\n",
        "        # -- add positional embedding to x tokens\n",
        "        x_pos_embed = self.predictor_pos_embed.repeat(B, 1, 1) # [batch, num_patches, predictor_embed_dim]\n",
        "        print(\"predictor fwd1\", x.shape, x_pos_embed.shape) # [3, 121 or 11, 384], [3, 196, 384]\n",
        "        # print(\"predictor fwd masks_x\", masks_x)\n",
        "        x += apply_masks(x_pos_embed, masks_x) # get pos emb of context patches\n",
        "\n",
        "        # print(\"predictor fwd x\", x.shape) # [4, 11, 384])\n",
        "        _, N_ctxt, D = x.shape\n",
        "\n",
        "        # -- concat mask tokens to x\n",
        "        pos_embs = self.predictor_pos_embed.repeat(B, 1, 1) # [batch, num_patches, predictor_embed_dim]\n",
        "        pos_embs = apply_masks(pos_embs, masks) # [16, 104, predictor_embed_dim]\n",
        "        pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x)) # [16, 104, predictor_embed_dim]\n",
        "        # --\n",
        "        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1) # [16, 104, predictor_embed_dim]\n",
        "        # --\n",
        "        # print(\"predictor fwd pred_tokens\", pred_tokens.shape)\n",
        "\n",
        "        pred_tokens += pos_embs\n",
        "        x = x.repeat(len(masks), 1, 1)\n",
        "        x = torch.cat([x, pred_tokens], dim=1)\n",
        "\n",
        "        # -- fwd prop\n",
        "        for blk in self.predictor_blocks:\n",
        "            x = blk(x)\n",
        "        x = self.predictor_norm(x)\n",
        "\n",
        "        # -- return preds for mask tokens\n",
        "        x = x[:, N_ctxt:]\n",
        "        x = self.predictor_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, img_size=[224], patch_size=16, in_chans=3, embed_dim=768, predictor_embed_dim=384, depth=12, predictor_depth=12,\n",
        "        num_heads=12, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0, norm_layer=nn.LayerNorm, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_features = self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.init_std = init_std\n",
        "        # --\n",
        "        self.patch_embed = PatchEmbed(img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        # --\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=False)\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "        # --\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "    def forward(self, x, masks=None): # [batch,3,224,224]\n",
        "        if masks is not None:\n",
        "            if not isinstance(masks, list): masks = [masks]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        x = self.patch_embed(x)\n",
        "        B, N, D = x.shape # [batch, num_patches, embed_dim]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        # -- add positional embedding to x\n",
        "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
        "        x = x + pos_embed\n",
        "\n",
        "        if masks is not None: x = apply_masks(x, masks) # [batch, num_context_patches, embed_dim]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            x = blk(x)\n",
        "\n",
        "        if self.norm is not None: x = self.norm(x)\n",
        "        return x # [batch, num_context_patches, embed_dim]\n",
        "\n",
        "    def interpolate_pos_encoding(self, x, pos_embed):\n",
        "        npatch = x.shape[1] - 1\n",
        "        N = pos_embed.shape[1] - 1\n",
        "        if npatch == N: return pos_embed\n",
        "        class_emb, pos_embed = pos_embed[:, 0], pos_embed[:, 1:]\n",
        "        dim = x.shape[-1]\n",
        "        pos_embed = nn.functional.interpolate(pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2), scale_factor=math.sqrt(npatch / N), mode='bicubic',)\n",
        "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
        "\n",
        "# from functools import partial\n",
        "# def vit_predictor(**kwargs):\n",
        "#     model = VisionTransformerPredictor(\n",
        "#         mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "#     return model\n",
        "\n",
        "def vit(patch_size=16, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, # tiny\n",
        "        # patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, # small\n",
        "        # patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, # base\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "# encoder = vit()\n",
        "encoder = VisionTransformer(\n",
        "        patch_size=16, embed_dim=768, depth=1, num_heads=3, mlp_ratio=1,\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
        "\n",
        "# num_patches = (224/patch_size)^2\n",
        "# model = VisionTransformerPredictor(num_patches, embed_dim=768, predictor_embed_dim=384, depth=6, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02, **kwargs)\n",
        "model = VisionTransformerPredictor(num_patches=196, embed_dim=768, predictor_embed_dim=384, depth=1, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02)\n",
        "\n",
        "batch = 4\n",
        "img = torch.rand(batch, 3, 224, 224)\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/train.py\n",
        "mask_collator = MaskCollator(\n",
        "        input_size=(224, 224),\n",
        "        patch_size=16,\n",
        "        enc_mask_scale=(0.2, 0.8),\n",
        "        pred_mask_scale=(0.2, 0.8),\n",
        "        aspect_ratio=(0.3, 3.0),\n",
        "        nenc=1,\n",
        "        npred=4,\n",
        "        min_keep=4,\n",
        "        # allow_overlap=True)\n",
        "        allow_overlap=False)\n",
        "# self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
        "\n",
        "\n",
        "\n",
        "# v = mask_collator.step()\n",
        "# should pass the collater a list batch of idx?\n",
        "# collated_batch, collated_masks_enc, collated_masks_pred = mask_collator([batch,3,8])\n",
        "collated_batch, collated_masks_enc, collated_masks_pred = mask_collator([0,1,2,3])\n",
        "# print(v)\n",
        "\n",
        "\n",
        "def forward_target():\n",
        "    with torch.no_grad():\n",
        "        h = target_encoder(imgs)\n",
        "        h = F.layer_norm(h, (h.size(-1),))  # normalize over feature-dim\n",
        "        B = len(h)\n",
        "        # -- create targets (masked regions of h)\n",
        "        h = apply_masks(h, masks_pred)\n",
        "        h = repeat_interleave_batch(h, B, repeat=len(masks_enc))\n",
        "        return h\n",
        "\n",
        "def forward_context():\n",
        "    z = encoder(imgs, masks_enc)\n",
        "    z = predictor(z, masks_enc, masks_pred)\n",
        "    return z\n",
        "\n",
        "# # num_context_patches = 121 if allow_overlap=True else 11\n",
        "z = encoder(img, collated_masks_enc) # [batch, num_context_patches, embed_dim]\n",
        "# print(len(collated_masks_enc), len(collated_masks_pred)) # nenc, npred\n",
        "# print(collated_masks_enc[0].shape, collated_masks_pred[0].shape) # , [batch, num_context_patches = 121 or 11], [batch, num_target_patches]\n",
        "out = model(z, collated_masks_enc, collated_masks_pred)\n",
        "# # print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Or3eQvUVg7l_"
      },
      "outputs": [],
      "source": [
        "\n",
        "# print(224/16) # 14\n",
        "# print(len(collated_masks_enc), len(collated_masks_pred))\n",
        "print(collated_masks_enc, collated_masks_pred)\n",
        "# multiples of 14\n",
        "\n",
        "# print(collated_masks_pred[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2mLjXAmXcwJ"
      },
      "outputs": [],
      "source": [
        "print(collated_masks_enc, collated_masks_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XQ7PxBMlsYCI"
      },
      "outputs": [],
      "source": [
        "# @title ijepa multiblock.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "import math\n",
        "from multiprocessing import Value\n",
        "from logging import getLogger\n",
        "import torch\n",
        "_GLOBAL_SEED = 0\n",
        "logger = getLogger()\n",
        "\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=(224, 224),\n",
        "        patch_size=16,\n",
        "        enc_mask_scale=(0.2, 0.8),\n",
        "        pred_mask_scale=(0.2, 0.8),\n",
        "        aspect_ratio=(0.3, 3.0),\n",
        "        nenc=1,\n",
        "        npred=2,\n",
        "        min_keep=4,\n",
        "        allow_overlap=False\n",
        "    ):\n",
        "        super(MaskCollator, self).__init__()\n",
        "        if not isinstance(input_size, tuple):\n",
        "            input_size = (input_size, ) * 2\n",
        "        self.patch_size = patch_size\n",
        "        self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.aspect_ratio = aspect_ratio\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "        self._itr_counter = Value('i', -1)  # collator is shared across worker processes\n",
        "\n",
        "    def step(self):\n",
        "        i = self._itr_counter\n",
        "        with i.get_lock():\n",
        "            i.value += 1\n",
        "            v = i.value\n",
        "        return v\n",
        "\n",
        "    def _sample_block_size(self, generator, scale, aspect_ratio_scale):\n",
        "        _rand = torch.rand(1, generator=generator).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.height * self.width * mask_scale)\n",
        "        # -- Sample block aspect-ratio\n",
        "        min_ar, max_ar = aspect_ratio_scale\n",
        "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
        "        while h >= self.height:\n",
        "            h -= 1\n",
        "        while w >= self.width:\n",
        "            w -= 1\n",
        "\n",
        "        return (h, w)\n",
        "\n",
        "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
        "        h, w = b_size\n",
        "\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # --\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            top = torch.randint(0, self.height - h, (1,))\n",
        "            left = torch.randint(0, self.width - w, (1,))\n",
        "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
        "            mask[top:top+h, left:left+w] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "                    logger.warning(f'Mask generator says: \"Valid mask not found, decreasing acceptable-regions [{tries}]\"')\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
        "        mask_complement[top:top+h, left:left+w] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        B = len(batch)\n",
        "\n",
        "        collated_batch = torch.utils.data.default_collate(batch)\n",
        "\n",
        "        seed = self.step()\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(seed)\n",
        "        p_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.pred_mask_scale,\n",
        "            aspect_ratio_scale=self.aspect_ratio)\n",
        "        e_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.enc_mask_scale,\n",
        "            aspect_ratio_scale=(1., 1.))\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.height * self.width\n",
        "        min_keep_enc = self.height * self.width\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                logger.warning(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "\n",
        "        return collated_batch, collated_masks_enc, collated_masks_pred\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "iVUPGP93dpAN"
      },
      "outputs": [],
      "source": [
        "# @title ijepa train.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/train.py\n",
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "#\n",
        "\n",
        "import os\n",
        "\n",
        "# -- FOR DISTRIBUTED TRAINING ENSURE ONLY 1 DEVICE VISIBLE PER PROCESS\n",
        "try:\n",
        "    # -- WARNING: IF DOING DISTRIBUTED TRAINING ON A NON-SLURM CLUSTER, MAKE\n",
        "    # --          SURE TO UPDATE THIS TO GET LOCAL-RANK ON NODE, OR ENSURE\n",
        "    # --          THAT YOUR JOBS ARE LAUNCHED WITH ONLY 1 DEVICE VISIBLE\n",
        "    # --          TO EACH PROCESS\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = os.environ['SLURM_LOCALID']\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "import copy\n",
        "import logging\n",
        "import sys\n",
        "import yaml\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.multiprocessing as mp\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "\n",
        "from src.masks.multiblock import MaskCollator as MBMaskCollator\n",
        "from src.masks.utils import apply_masks\n",
        "from src.utils.distributed import (\n",
        "    init_distributed,\n",
        "    AllReduce\n",
        ")\n",
        "from src.utils.logging import (\n",
        "    CSVLogger,\n",
        "    gpu_timer,\n",
        "    grad_logger,\n",
        "    AverageMeter)\n",
        "from src.utils.tensors import repeat_interleave_batch\n",
        "from src.datasets.imagenet1k import make_imagenet1k\n",
        "\n",
        "from src.helper import (\n",
        "    load_checkpoint,\n",
        "    init_model,\n",
        "    init_opt)\n",
        "from src.transforms import make_transforms\n",
        "\n",
        "# --\n",
        "log_timings = True\n",
        "log_freq = 10\n",
        "checkpoint_freq = 50\n",
        "# --\n",
        "\n",
        "_GLOBAL_SEED = 0\n",
        "np.random.seed(_GLOBAL_SEED)\n",
        "torch.manual_seed(_GLOBAL_SEED)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logger = logging.getLogger()\n",
        "\n",
        "\n",
        "def main(args, resume_preempt=False):\n",
        "\n",
        "    # ----------------------------------------------------------------------- #\n",
        "    #  PASSED IN PARAMS FROM CONFIG FILE\n",
        "    # ----------------------------------------------------------------------- #\n",
        "\n",
        "    # -- META\n",
        "    use_bfloat16 = args['meta']['use_bfloat16']\n",
        "    model_name = args['meta']['model_name']\n",
        "    load_model = args['meta']['load_checkpoint'] or resume_preempt\n",
        "    r_file = args['meta']['read_checkpoint']\n",
        "    copy_data = args['meta']['copy_data']\n",
        "    pred_depth = args['meta']['pred_depth']\n",
        "    pred_emb_dim = args['meta']['pred_emb_dim']\n",
        "    if not torch.cuda.is_available():\n",
        "        device = torch.device('cpu')\n",
        "    else:\n",
        "        device = torch.device('cuda:0')\n",
        "        torch.cuda.set_device(device)\n",
        "\n",
        "    # -- DATA\n",
        "    use_gaussian_blur = args['data']['use_gaussian_blur']\n",
        "    use_horizontal_flip = args['data']['use_horizontal_flip']\n",
        "    use_color_distortion = args['data']['use_color_distortion']\n",
        "    color_jitter = args['data']['color_jitter_strength']\n",
        "    # --\n",
        "    batch_size = args['data']['batch_size']\n",
        "    pin_mem = args['data']['pin_mem']\n",
        "    num_workers = args['data']['num_workers']\n",
        "    root_path = args['data']['root_path']\n",
        "    image_folder = args['data']['image_folder']\n",
        "    crop_size = args['data']['crop_size']\n",
        "    crop_scale = args['data']['crop_scale']\n",
        "    # --\n",
        "\n",
        "    # -- MASK\n",
        "    allow_overlap = args['mask']['allow_overlap']  # whether to allow overlap b/w context and target blocks\n",
        "    patch_size = args['mask']['patch_size']  # patch-size for model training\n",
        "    num_enc_masks = args['mask']['num_enc_masks']  # number of context blocks\n",
        "    min_keep = args['mask']['min_keep']  # min number of patches in context block\n",
        "    enc_mask_scale = args['mask']['enc_mask_scale']  # scale of context blocks\n",
        "    num_pred_masks = args['mask']['num_pred_masks']  # number of target blocks\n",
        "    pred_mask_scale = args['mask']['pred_mask_scale']  # scale of target blocks\n",
        "    aspect_ratio = args['mask']['aspect_ratio']  # aspect ratio of target blocks\n",
        "    # --\n",
        "\n",
        "    # -- OPTIMIZATION\n",
        "    ema = args['optimization']['ema']\n",
        "    ipe_scale = args['optimization']['ipe_scale']  # scheduler scale factor (def: 1.0)\n",
        "    wd = float(args['optimization']['weight_decay'])\n",
        "    final_wd = float(args['optimization']['final_weight_decay'])\n",
        "    num_epochs = args['optimization']['epochs']\n",
        "    warmup = args['optimization']['warmup']\n",
        "    start_lr = args['optimization']['start_lr']\n",
        "    lr = args['optimization']['lr']\n",
        "    final_lr = args['optimization']['final_lr']\n",
        "\n",
        "    # -- LOGGING\n",
        "    folder = args['logging']['folder']\n",
        "    tag = args['logging']['write_tag']\n",
        "\n",
        "    dump = os.path.join(folder, 'params-ijepa.yaml')\n",
        "    with open(dump, 'w') as f:\n",
        "        yaml.dump(args, f)\n",
        "    # ----------------------------------------------------------------------- #\n",
        "\n",
        "    try:\n",
        "        mp.set_start_method('spawn')\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # -- init torch distributed backend\n",
        "    world_size, rank = init_distributed()\n",
        "    logger.info(f'Initialized (rank/world-size) {rank}/{world_size}')\n",
        "    if rank > 0:\n",
        "        logger.setLevel(logging.ERROR)\n",
        "\n",
        "    # -- log/checkpointing paths\n",
        "    log_file = os.path.join(folder, f'{tag}_r{rank}.csv')\n",
        "    save_path = os.path.join(folder, f'{tag}' + '-ep{epoch}.pth.tar')\n",
        "    latest_path = os.path.join(folder, f'{tag}-latest.pth.tar')\n",
        "    load_path = None\n",
        "    if load_model:\n",
        "        load_path = os.path.join(folder, r_file) if r_file is not None else latest_path\n",
        "\n",
        "    # -- make csv_logger\n",
        "    csv_logger = CSVLogger(log_file,\n",
        "                           ('%d', 'epoch'),\n",
        "                           ('%d', 'itr'),\n",
        "                           ('%.5f', 'loss'),\n",
        "                           ('%.5f', 'mask-A'),\n",
        "                           ('%.5f', 'mask-B'),\n",
        "                           ('%d', 'time (ms)'))\n",
        "\n",
        "    # -- init model\n",
        "    encoder, predictor = init_model(\n",
        "        device=device,\n",
        "        patch_size=patch_size,\n",
        "        crop_size=crop_size,\n",
        "        pred_depth=pred_depth,\n",
        "        pred_emb_dim=pred_emb_dim,\n",
        "        model_name=model_name)\n",
        "    target_encoder = copy.deepcopy(encoder)\n",
        "\n",
        "    # -- make data transforms\n",
        "    mask_collator = MBMaskCollator(\n",
        "        input_size=crop_size,\n",
        "        patch_size=patch_size,\n",
        "        pred_mask_scale=pred_mask_scale,\n",
        "        enc_mask_scale=enc_mask_scale,\n",
        "        aspect_ratio=aspect_ratio,\n",
        "        nenc=num_enc_masks,\n",
        "        npred=num_pred_masks,\n",
        "        allow_overlap=allow_overlap,\n",
        "        min_keep=min_keep)\n",
        "\n",
        "    transform = make_transforms(\n",
        "        crop_size=crop_size,\n",
        "        crop_scale=crop_scale,\n",
        "        gaussian_blur=use_gaussian_blur,\n",
        "        horizontal_flip=use_horizontal_flip,\n",
        "        color_distortion=use_color_distortion,\n",
        "        color_jitter=color_jitter)\n",
        "\n",
        "    # -- init data-loaders/samplers\n",
        "    _, unsupervised_loader, unsupervised_sampler = make_imagenet1k(\n",
        "            transform=transform,\n",
        "            batch_size=batch_size,\n",
        "            collator=mask_collator,\n",
        "            pin_mem=pin_mem,\n",
        "            training=True,\n",
        "            num_workers=num_workers,\n",
        "            world_size=world_size,\n",
        "            rank=rank,\n",
        "            root_path=root_path,\n",
        "            image_folder=image_folder,\n",
        "            copy_data=copy_data,\n",
        "            drop_last=True)\n",
        "    ipe = len(unsupervised_loader)\n",
        "\n",
        "    # -- init optimizer and scheduler\n",
        "    optimizer, scaler, scheduler, wd_scheduler = init_opt(\n",
        "        encoder=encoder,\n",
        "        predictor=predictor,\n",
        "        wd=wd,\n",
        "        final_wd=final_wd,\n",
        "        start_lr=start_lr,\n",
        "        ref_lr=lr,\n",
        "        final_lr=final_lr,\n",
        "        iterations_per_epoch=ipe,\n",
        "        warmup=warmup,\n",
        "        num_epochs=num_epochs,\n",
        "        ipe_scale=ipe_scale,\n",
        "        use_bfloat16=use_bfloat16)\n",
        "    encoder = DistributedDataParallel(encoder, static_graph=True)\n",
        "    predictor = DistributedDataParallel(predictor, static_graph=True)\n",
        "    target_encoder = DistributedDataParallel(target_encoder)\n",
        "    for p in target_encoder.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    # -- momentum schedule\n",
        "    momentum_scheduler = (ema[0] + i*(ema[1]-ema[0])/(ipe*num_epochs*ipe_scale)\n",
        "                          for i in range(int(ipe*num_epochs*ipe_scale)+1))\n",
        "\n",
        "    start_epoch = 0\n",
        "    # -- load training checkpoint\n",
        "    if load_model:\n",
        "        encoder, predictor, target_encoder, optimizer, scaler, start_epoch = load_checkpoint(\n",
        "            device=device,\n",
        "            r_path=load_path,\n",
        "            encoder=encoder,\n",
        "            predictor=predictor,\n",
        "            target_encoder=target_encoder,\n",
        "            opt=optimizer,\n",
        "            scaler=scaler)\n",
        "        for _ in range(start_epoch*ipe):\n",
        "            scheduler.step()\n",
        "            wd_scheduler.step()\n",
        "            next(momentum_scheduler)\n",
        "            mask_collator.step()\n",
        "\n",
        "    def save_checkpoint(epoch):\n",
        "        save_dict = {\n",
        "            'encoder': encoder.state_dict(),\n",
        "            'predictor': predictor.state_dict(),\n",
        "            'target_encoder': target_encoder.state_dict(),\n",
        "            'opt': optimizer.state_dict(),\n",
        "            'scaler': None if scaler is None else scaler.state_dict(),\n",
        "            'epoch': epoch,\n",
        "            'loss': loss_meter.avg,\n",
        "            'batch_size': batch_size,\n",
        "            'world_size': world_size,\n",
        "            'lr': lr\n",
        "        }\n",
        "        if rank == 0:\n",
        "            torch.save(save_dict, latest_path)\n",
        "            if (epoch + 1) % checkpoint_freq == 0:\n",
        "                torch.save(save_dict, save_path.format(epoch=f'{epoch + 1}'))\n",
        "\n",
        "    # -- TRAINING LOOP\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        logger.info('Epoch %d' % (epoch + 1))\n",
        "\n",
        "        # -- update distributed-data-loader epoch\n",
        "        unsupervised_sampler.set_epoch(epoch)\n",
        "\n",
        "        loss_meter = AverageMeter()\n",
        "        maskA_meter = AverageMeter()\n",
        "        maskB_meter = AverageMeter()\n",
        "        time_meter = AverageMeter()\n",
        "\n",
        "        for itr, (udata, masks_enc, masks_pred) in enumerate(unsupervised_loader):\n",
        "\n",
        "            def load_imgs():\n",
        "                # -- unsupervised imgs\n",
        "                imgs = udata[0].to(device, non_blocking=True)\n",
        "                masks_1 = [u.to(device, non_blocking=True) for u in masks_enc]\n",
        "                masks_2 = [u.to(device, non_blocking=True) for u in masks_pred]\n",
        "                return (imgs, masks_1, masks_2)\n",
        "            imgs, masks_enc, masks_pred = load_imgs()\n",
        "            maskA_meter.update(len(masks_enc[0][0]))\n",
        "            maskB_meter.update(len(masks_pred[0][0]))\n",
        "\n",
        "            def train_step():\n",
        "                _new_lr = scheduler.step()\n",
        "                _new_wd = wd_scheduler.step()\n",
        "                # --\n",
        "\n",
        "                def forward_target():\n",
        "                    with torch.no_grad():\n",
        "                        h = target_encoder(imgs)\n",
        "                        h = F.layer_norm(h, (h.size(-1),))  # normalize over feature-dim\n",
        "                        B = len(h)\n",
        "                        # -- create targets (masked regions of h)\n",
        "                        h = apply_masks(h, masks_pred)\n",
        "                        h = repeat_interleave_batch(h, B, repeat=len(masks_enc))\n",
        "                        return h\n",
        "\n",
        "                def forward_context():\n",
        "                    z = encoder(imgs, masks_enc)\n",
        "                    z = predictor(z, masks_enc, masks_pred)\n",
        "                    return z\n",
        "\n",
        "                def loss_fn(z, h):\n",
        "                    loss = F.smooth_l1_loss(z, h)\n",
        "                    loss = AllReduce.apply(loss)\n",
        "                    return loss\n",
        "\n",
        "                # Step 1. Forward\n",
        "                with torch.cuda.amp.autocast(dtype=torch.bfloat16, enabled=use_bfloat16):\n",
        "                    h = forward_target()\n",
        "                    z = forward_context()\n",
        "                    loss = loss_fn(z, h)\n",
        "\n",
        "                #  Step 2. Backward & step\n",
        "                if use_bfloat16:\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                else:\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                grad_stats = grad_logger(encoder.named_parameters())\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Step 3. momentum update of target encoder\n",
        "                with torch.no_grad():\n",
        "                    m = next(momentum_scheduler)\n",
        "                    for param_q, param_k in zip(encoder.parameters(), target_encoder.parameters()):\n",
        "                        param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "\n",
        "                return (float(loss), _new_lr, _new_wd, grad_stats)\n",
        "            (loss, _new_lr, _new_wd, grad_stats), etime = gpu_timer(train_step)\n",
        "            loss_meter.update(loss)\n",
        "            time_meter.update(etime)\n",
        "\n",
        "            # -- Logging\n",
        "            def log_stats():\n",
        "                csv_logger.log(epoch + 1, itr, loss, maskA_meter.val, maskB_meter.val, etime)\n",
        "                if (itr % log_freq == 0) or np.isnan(loss) or np.isinf(loss):\n",
        "                    logger.info('[%d, %5d] loss: %.3f '\n",
        "                                'masks: %.1f %.1f '\n",
        "                                '[wd: %.2e] [lr: %.2e] '\n",
        "                                '[mem: %.2e] '\n",
        "                                '(%.1f ms)'\n",
        "                                % (epoch + 1, itr,\n",
        "                                   loss_meter.avg,\n",
        "                                   maskA_meter.avg,\n",
        "                                   maskB_meter.avg,\n",
        "                                   _new_wd,\n",
        "                                   _new_lr,\n",
        "                                   torch.cuda.max_memory_allocated() / 1024.**2,\n",
        "                                   time_meter.avg))\n",
        "\n",
        "                    if grad_stats is not None:\n",
        "                        logger.info('[%d, %5d] grad_stats: [%.2e %.2e] (%.2e, %.2e)'\n",
        "                                    % (epoch + 1, itr,\n",
        "                                       grad_stats.first_layer,\n",
        "                                       grad_stats.last_layer,\n",
        "                                       grad_stats.min,\n",
        "                                       grad_stats.max))\n",
        "\n",
        "            log_stats()\n",
        "\n",
        "            assert not np.isnan(loss), 'loss is nan'\n",
        "\n",
        "        # -- Save Checkpoint after every epoch\n",
        "        logger.info('avg. loss %.3f' % loss_meter.avg)\n",
        "        save_checkpoint(epoch+1)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "s3EO3PgMPH1x"
      ],
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}