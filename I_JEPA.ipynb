{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/I_JEPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "LxACli7GdyGq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69636b34-567e-4e1e-f7fc-df7314e8d237"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:05<00:00, 29.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title data\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "transform = transforms.Compose([transforms.ToTensor(),])\n",
        "# transform = transforms.Compose([transforms.RandomResizedCrop((32,32), scale=(.3,1.)), transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "# CIFAR10: 60000 32x32 color images in 10 classes, with 6000 images per class\n",
        "train_data = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
        "test_data = torchvision.datasets.CIFAR10(root='data', train=False, download=True, transform=transform)\n",
        "batch_size = 64 # 4\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# dataiter = iter(train_loader) # get some random training images\n",
        "# images, labels = next(dataiter)\n",
        "# print(images.shape) # [batch, 3, 32, 32]\n",
        "# imshow(torchvision.utils.make_grid(images))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hiera"
      ],
      "metadata": {
        "id": "s3EO3PgMPH1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ResBlock\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters(): p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, kernel=3):\n",
        "        super().__init__()\n",
        "        out_ch = out_ch or in_ch\n",
        "        act = nn.SiLU() #\n",
        "        self.res_conv = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
        "        # self.block = nn.Sequential( # best?\n",
        "        #     nn.Conv2d(in_ch, out_ch, 3, padding=1), nn.BatchNorm2d(out_ch), act,\n",
        "        #     zero_module(nn.Conv2d(out_ch, out_ch, 3, padding=1)), nn.BatchNorm2d(out_ch), act,\n",
        "        #     )\n",
        "        self.block = nn.Sequential(\n",
        "            nn.BatchNorm2d(in_ch), act, nn.Conv2d(in_ch, out_ch, kernel, padding=kernel//2),\n",
        "            nn.BatchNorm2d(out_ch), act, zero_module(nn.Conv2d(out_ch, out_ch, kernel, padding=kernel//2)),\n",
        "            )\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w]\n",
        "        return self.block(x) + self.res_conv(x)\n"
      ],
      "metadata": {
        "id": "j3-vvMS1-gVn",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title UpDownBlock_me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class PixelShuffleConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, kernel=1, r=1):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        r = max(r, int(1/r))\n",
        "        out_ch = out_ch or in_ch\n",
        "        if self.r>1: self.net = nn.Sequential(ResBlock(in_ch, out_ch*r**2, kernel), nn.PixelShuffle(r))\n",
        "        # if self.r>1: self.net = nn.Sequential(Attention(in_ch, out_ch*r**2), nn.PixelShuffle(r))\n",
        "\n",
        "        elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), ResBlock(in_ch*r**2, out_ch, kernel))\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), Attention(in_ch*r**2, out_ch))\n",
        "        elif in_ch != out_ch: self.net = ResBlock(in_ch*r**2, out_ch, kernel)\n",
        "        else: self.net = lambda x: torch.zeros_like(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def AdaptiveAvgPool_nd(n, *args, **kwargs): return [nn.Identity, nn.AdaptiveAvgPool1d, nn.AdaptiveAvgPool2d, nn.AdaptiveAvgPool3d][n](*args, **kwargs)\n",
        "def AdaptiveMaxPool_nd(n, *args, **kwargs): return [nn.Identity, nn.AdaptiveMaxPool1d, nn.AdaptiveMaxPool2d, nn.AdaptiveMaxPool3d][n](*args, **kwargs)\n",
        "\n",
        "def adaptive_avg_pool_nd(n, x, output_size): return [nn.Identity, F.adaptive_avg_pool1d, F.adaptive_avg_pool2d, F.adaptive_avg_pool3d][n](x, output_size)\n",
        "def adaptive_max_pool_nd(n, x, output_size): return [nn.Identity, F.adaptive_max_pool1d, F.adaptive_max_pool2d, F.adaptive_max_pool3d][n](x, output_size)\n",
        "\n",
        "class AdaptivePool_at(nn.AdaptiveAvgPool1d): # AdaptiveAvgPool1d AdaptiveMaxPool1d\n",
        "    def __init__(self, dim=1, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.dim=dim\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(self.dim,-1)\n",
        "        shape = x.shape\n",
        "        return super().forward(x.flatten(0,-2)).unflatten(0, shape[:-1]).transpose(self.dim,-1)\n",
        "\n",
        "\n",
        "def adaptive_pool_at(x, dim, output_size, pool='avg'): # [b,c,h,w]\n",
        "    x = x.transpose(dim,-1)\n",
        "    shape = x.shape\n",
        "    parent={'avg':F.adaptive_avg_pool1d, 'max':F.adaptive_max_pool1d}[pool]\n",
        "    return parent(x.flatten(0,-2), output_size).unflatten(0, shape[:-1]).transpose(dim,-1)\n",
        "\n",
        "\n",
        "class ZeroExtend():\n",
        "    def __init__(self, dim=1, output_size=16):\n",
        "        self.dim, self.out = dim, output_size\n",
        "    def __call__(self, x): # [b,c,h,w]\n",
        "        return torch.cat((x, torch.zeros(*x.shape[:self.dim], self.out - x.shape[self.dim], *x.shape[self.dim+1:])), dim=self.dim)\n",
        "\n",
        "def make_pool_at(pool='avg', dim=1, output_size=5):\n",
        "    parent={'avg':nn.AdaptiveAvgPool1d, 'max':nn.AdaptiveMaxPool1d}[pool]\n",
        "    class AdaptivePool_at(parent): # AdaptiveAvgPool1d AdaptiveMaxPool1d\n",
        "        def __init__(self, dim=1, *args, **kwargs):\n",
        "            super().__init__(*args, **kwargs)\n",
        "            self.dim=dim\n",
        "        def forward(self, x):\n",
        "            x = x.transpose(self.dim,-1)\n",
        "            shape = x.shape\n",
        "            return super().forward(x.flatten(0,-2)).unflatten(0, shape[:-1]).transpose(self.dim,-1)\n",
        "    return AdaptivePool_at(dim, output_size=output_size)\n",
        "\n",
        "class Shortcut():\n",
        "    def __init__(self, dim=1, c=3, sp=(3,3), nd=2):\n",
        "        self.dim = dim\n",
        "        # self.ch_pool = make_pool_at(pool='avg', dim=dim, output_size=c)\n",
        "        self.ch_pool = make_pool_at(pool='max', dim=dim, output_size=c)\n",
        "        # self.ch_pool = ZeroExtend(dim, output_size=c) # only for out_dim>=in_dim\n",
        "        # self.sp_pool = AdaptiveAvgPool_nd(nd, sp)\n",
        "        self.sp_pool = AdaptiveMaxPool_nd(nd, sp)\n",
        "\n",
        "    def __call__(self, x): # [b,c,h,w]\n",
        "        x = self.sp_pool(x) # spatial first preserves spatial more?\n",
        "        x = self.ch_pool(x)\n",
        "        return x\n",
        "\n",
        "class UpDownBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, kernel=7, r=1):\n",
        "        super().__init__()\n",
        "        act = nn.SiLU()\n",
        "        self.r = r\n",
        "        self.block = PixelShuffleConv(in_ch, out_ch, kernel=kernel, r=r)\n",
        "        # self.block = nn.Sequential(\n",
        "        #     nn.BatchNorm2d(in_ch), act, PixelShuffleConv(in_ch, out_ch, kernel=kernel, r=r)\n",
        "        # )\n",
        "        # if self.r>1: self.res_conv = nn.Sequential(nn.ConvTranspose2d(in_ch, out_ch, kernel, 2, kernel//2, output_padding=1))\n",
        "        # if self.r>1: self.res_conv = nn.Sequential(nn.Upsample(scale_factor=2, mode='nearest'), nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity())\n",
        "        # if self.r>1: self.res_conv = nn.Sequential(nn.Upsample(scale_factor=2, mode='bilinear'), nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity())\n",
        "        # if self.r>1: self.res_conv = nn.Sequential(nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True), nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity())\n",
        "\n",
        "\n",
        "        # elif self.r<1: self.res_conv = nn.Sequential(nn.Conv2d(in_ch, out_ch, kernel, 2, kernel//2))\n",
        "        # elif self.r<1: self.res_conv = nn.Sequential(nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity(), nn.MaxPool2d(2,2))\n",
        "        # elif self.r<1: self.res_conv = nn.Sequential(nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity(), nn.AvgPool2d(2,2))\n",
        "        # elif self.r<1: self.res_conv = AttentionBlock(in_ch, out_ch, n_heads=4, q_stride=(2,2))\n",
        "\n",
        "        # else: self.res_conv = nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity()\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w]\n",
        "        b, num_tok, c, *win = x.shape\n",
        "        x = x.flatten(0,1)\n",
        "        out = self.block(x)\n",
        "        # # shortcut = F.interpolate(x.unsqueeze(1), size=out.shape[1:], mode='nearest-exact').squeeze(1) # pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html\n",
        "        # shortcut = F.adaptive_avg_pool3d(x, out.shape[1:]) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "        # # shortcut = F.adaptive_max_pool3d(x, out.shape[1:]) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "        # # shortcut = F.adaptive_avg_pool3d(x, out.shape[1:]) if out.shape[1]>=x.shape[1] else F.adaptive_max_pool3d(x, out.shape[1:])\n",
        "        # shortcut(x)\n",
        "        shortcut = Shortcut(dim=1, c=out.shape[1], sp=out.shape[-2:], nd=2)(x)\n",
        "        out = out + shortcut\n",
        "        out = out.unflatten(0, (b, num_tok))\n",
        "        return out\n",
        "\n",
        "        # return out + shortcut + self.res_conv(x)\n",
        "        # return out + self.res_conv(x)\n",
        "        # return self.res_conv(x)\n",
        "\n",
        "# if out>in, inter=max=ave=near.\n",
        "# if out<in, inter=ave. max=max\n",
        "\n",
        "# stride2\n",
        "# interconv/convpool\n",
        "# pixelconv\n",
        "# pixeluib\n",
        "# pixelres\n",
        "# shortcut\n",
        "\n",
        "# in_ch, out_ch = 16,3\n",
        "in_ch, out_ch = 3,16\n",
        "model = UpDownBlock(in_ch, out_ch, r=1/2).to(device)\n",
        "# model = UpDownBlock(in_ch, out_ch, r=2).to(device)\n",
        "\n",
        "x = torch.rand(12, in_ch, 64,64, device=device)\n",
        "x = torch.rand(12, 2, in_ch, 64,64, device=device)\n",
        "out = model(x)\n",
        "\n",
        "print(out.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0lclc9myo2c",
        "outputId": "35ea2b51-c2dc-4d96-f2ae-c27bd760dcd9",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([12, 2, 16, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title maxpool path bchw\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def conv_nd(n, *args, **kwargs): return [nn.Identity, nn.Conv1d, nn.Conv2d, nn.Conv3d][n](*args, **kwargs)\n",
        "def maxpool_nd(n, *args, **kwargs): return [nn.Identity, nn.MaxPool1d, nn.MaxPool2d, nn.MaxPool3d][n](*args, **kwargs)\n",
        "def avgpool_nd(n, *args, **kwargs): return [nn.Identity, nn.AvgPool1d, nn.AvgPool2d, nn.AvgPool3d][n](*args, **kwargs)\n",
        "\n",
        "import math\n",
        "class MaskUnitAttention(nn.Module):\n",
        "    # def __init__(self, d_model=16, n_heads=4, q_stride=None, nd=2):\n",
        "    def __init__(self, in_dim, d_model=16, n_heads=4, q_stride=None, nd=2):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads, self.d_head = n_heads, d_model // n_heads\n",
        "        self.scale = self.d_head**-.5\n",
        "        # self.qkv = conv_nd(nd, d_model, 3*d_model, 1, bias=False)\n",
        "        self.qkv = conv_nd(nd, in_dim, 3*d_model, 1, bias=False)\n",
        "        # self.out = conv_nd(nd, d_model, d_model, 1)\n",
        "        self.out = nn.Linear(d_model, d_model, 1)\n",
        "        self.q_stride = q_stride # If greater than 1, pool q with this stride. The stride should be flattened (e.g., 2x2 = 4).\n",
        "        if q_stride:\n",
        "            self.q_stride = (q_stride,)*nd if type(q_stride)==int else q_stride\n",
        "            self.q_pool = maxpool_nd(nd, self.q_stride, stride=self.q_stride)\n",
        "\n",
        "    def forward(self, x): # [b,num_tok,c,win1,win2]\n",
        "        b, num_tok, c, *win = x.shape\n",
        "        x = x.flatten(0,1) # [b*num_tok,c,win1,win2]\n",
        "        # print(x.shape)\n",
        "        # q, k, v = self.qkv(x).reshape(b, 3, self.n_heads, self.d_head, num_tok, win*win).permute(1,0,2,4,5,3) # [b,3*d_model,num_tok*win,win] -> 3* [b, n_heads, num_tok, win*win, d_head]\n",
        "        # self.qkv(x).flatten(1,-2).unflatten(-1, (self.heads,-1)).chunk(3, dim=-1) # [b,sp,n_heads,d_head]\n",
        "        q,k,v = self.qkv(x).chunk(3, dim=1) # [b*num_tok,d,win,win]\n",
        "        if self.q_stride:\n",
        "            q = self.q_pool(q)\n",
        "            win=[w//s for w,s in zip(win, self.q_stride)] # win = win/q_stride\n",
        "        if math.prod(win) >= 200:\n",
        "            print('MUattn', math.prod(win))\n",
        "            q, k, v = map(lambda t: t.reshape(b*num_tok, self.n_heads, self.d_head, -1).transpose(-2,-1), (q,k,v)) # [b*num_tok, n_heads, win*win, d_head]\n",
        "        else:\n",
        "            # q, k, v = map(lambda t: t.reshape(b, num_tok, self.n_heads, self.d_head, -1).permute(0,2,4,1,3), (q,k,v)) # downsampling attention # [b, n_heads, win*win, num_tok, d_head]\n",
        "            q, k, v = map(lambda t: t.reshape(b, num_tok, self.n_heads, self.d_head, -1).permute(0,2,1,4,3).flatten(2,3), (q,k,v)) # [b, n_heads, num_tok*win*win, d_head]\n",
        "\n",
        "        # x = F.scaled_dot_product_attention(q,k,v, attn_mask=None) # https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html\n",
        "        q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2) # [b, n_heads, t(/pool), d_head]\n",
        "        context = k.transpose(-2,-1) @ v # [b, n_heads, d_head, d_head]\n",
        "        x = q @ context # [b, n_heads, t(/pool), d_head]\n",
        "\n",
        "        # print('attn fwd 2',x.shape)\n",
        "        # x = x.transpose(1, 3).reshape(B, -1, self.d_model)\n",
        "        x = x.transpose(1,2).reshape(b, -1, self.d_model) # [b,t,d]\n",
        "        # x = x.transpose(-2,-1).reshape(x.shape[0], self.d_model, ) # [b,t,d]\n",
        "        # [b, n_heads, num_tok, win*win, d_head] -> [b, n_heads, d_head, num_tok, win*win] -> [b,c,num_tok*win,win]\n",
        "        # x = x.permute(0,1,4,2,3).reshape(b, self.d_model, num_tok*win,win) # [b, n_heads, num_tok, win*win, d_head]\n",
        "        # x = x.transpose(-2,-1).reshape(b, self.d_model, num_tok, *win) # [b*num_tok, n_heads, win*win, d_head]\n",
        "        x = self.out(x)\n",
        "        L=len(win)\n",
        "        x = x.reshape(b, num_tok, *win, self.d_model).permute(0,1,L+2,*range(2,L+2)) # [b, num_tok, out_dim, *win]\n",
        "        # x = x.unflatten(0, (b, num_tok))\n",
        "        return x # [b,num_tok,c,win1,win2]\n",
        "\n",
        "d_model=16\n",
        "model = MaskUnitAttention(d_model, n_heads=4, q_stride=2)\n",
        "# MaskUnitAttention(d_model=16, n_heads=4, q_stride=None, nd=2)\n",
        "# x=torch.randn(2,4,4,3)\n",
        "# x=torch.randn(2,3,d_model,8,8)\n",
        "x=torch.randn(2,3,d_model,32,32)\n",
        "# [b*num_tok,c,win,win]\n",
        "\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e66e47ef-f7e7-44c1-f15f-168e0adf1807",
        "id": "ZAyKHKivc0j7",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MUattn 256\n",
            "torch.Size([2, 3, 16, 16, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title hiera vit me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class LayerNorm_at(nn.RMSNorm): # LayerNorm RMSNorm\n",
        "    def __init__(self, dim=1, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.dim=dim\n",
        "    def forward(self, x):\n",
        "        return super().forward(x.transpose(self.dim,-1)).transpose(self.dim,-1)\n",
        "\n",
        "# # [b, h/win1* w/win2, c, win1,win2] -> [b,c,h,w]\n",
        "# def shuffle(x, window_shape): # [b,win*win, c, h/win, w/win] -> [b,1,c,h,w]\n",
        "#     # out_shape = [x.shape[0], -1, x.shape[2]] + [x*w for x,w in zip(x.shape[3:], window_shape)] # [b,1,c,h/win*wim, w/win*wim]\n",
        "#     out_shape = [x.shape[0], -1, x.shape[2]] + [x*w for x,w in zip(x.shape[3:], window_shape)] # [b,1,c,h/win*wim, w/win*wim]\n",
        "#     x = x.unflatten(1, (-1, *window_shape)) # [b,num_tok,win,win, c, h/win, w/win]\n",
        "#     D=x.dim()+1\n",
        "#     permute = [0,1,D//2] + [val for tup in zip(range(1+D//2, D), range(2, D//2)) for val in tup]\n",
        "#     x = x.permute(permute).reshape(out_shape)\n",
        "#     return x\n",
        "\n",
        "def unshuffle(x, window_shape): # [b,c,h,w] -> [b, h/win1* w/win2, c, win1,win2]\n",
        "    new_shape = list(x.shape[:2]) + [val for xx, win in zip(list(x.shape[2:]), window_shape) for val in [xx//win, win]] # [h,w]->[h/win1, win1, w/win2, win2]\n",
        "    x = x.reshape(new_shape) # [b, c, h/win1, win1, w/win2, win2]\n",
        "    # print('unsh',x.shape, window_shape, new_shape)\n",
        "    L = len(new_shape)\n",
        "    permute = ([0] + list(range(2, L - 1, 2)) + [1] + list(range(3, L, 2))) # [0,2,4,1,3,5] / [0,2,4,6,1,3,5,6]\n",
        "    return x.permute(permute).flatten(1, L//2-1) # [b, h/win1* w/win2, c, win1,win2]\n",
        "\n",
        "class HieraBlock(nn.Module):\n",
        "    # def __init__(self, d_model, n_heads, q_stride=None, mult=4, drop=0, nd=2):\n",
        "    def __init__(self, in_dim, d_model, n_heads, q_stride=None, mult=4, drop=0, nd=2):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        # self.norm = LayerNorm_at(2, d_model) # LayerNorm RMSNorm\n",
        "        self.norm = LayerNorm_at(2, in_dim) # LayerNorm RMSNorm\n",
        "        self.drop = nn.Dropout(drop)\n",
        "        # self.attn = MaskUnitAttention(d_model, n_heads, q_stride)\n",
        "        self.attn = MaskUnitAttention(in_dim, d_model, n_heads, q_stride)\n",
        "        ff_dim=d_model*mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), nn.Linear(d_model, ff_dim), nn.ReLU(), # ReLU GELU\n",
        "            nn.RMSNorm(ff_dim), nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "            # nn.RMSNorm(d_model), act, nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Linear(ff_dim, d_model)\n",
        "            # nn.RMSNorm(d_model), nn.Linear(d_model, ff_dim), nn.ReLU(), nn.Dropout(dropout), # ReLU GELU\n",
        "            # nn.Linear(ff_dim, d_model), nn.Dropout(dropout),\n",
        "        )\n",
        "        # self.res = conv_nd(nd, d_model, d_model, q_stride, q_stride) if q_stride else nn.Identity()\n",
        "\n",
        "        # self.res = nn.Sequential(\n",
        "        #     conv_nd(nd, in_dim, d_model, 1, 1) if in_dim!=d_model else nn.Identity(),\n",
        "        #     # maxpool_nd(nd, q_stride, q_stride) if q_stride else nn.Identity(),\n",
        "        #     avgpool_nd(nd, q_stride, q_stride) if q_stride else nn.Identity(),\n",
        "        # )\n",
        "        self.res = UpDownBlock(in_dim, d_model, kernel=1, r=1/2 if q_stride else 1)\n",
        "\n",
        "        # x = self.proj(x_norm).unflatten(1, (self.attn.q_stride, -1)).max(dim=1).values # pooling res # [b, (Sy, Sx, h/Sy, w/Sx), c] -> [b, (Sy, Sx), (h/Sy, w/Sx), c] -> [b, (h/Sy, w/Sx), c]\n",
        "\n",
        "\n",
        "    def forward(self, x): # [b, num_tok, c, *win]\n",
        "        b, num_tok, c, *win = x.shape\n",
        "        # print('attnblk fwd',x.shape)\n",
        "        # x = x + self.drop(self.self(self.norm(x)))\n",
        "        # print('attnblk fwd',self.res(x.flatten(0,1)).shape, self.drop(self.attn(self.norm(x))).flatten(0,1).shape)\n",
        "        # x = self.res(x.flatten(0,1)) + self.drop(self.attn(self.norm(x))).flatten(0,1) # [b*num_tok,c,win1,win2,win3]\n",
        "        x = self.res(x) + self.drop(self.attn(self.norm(x))) # [b*num_tok,c,win1,win2,win3]\n",
        "        # x = x + self.ff(x)\n",
        "        # x = x + self.ff(x.transpose(1,-1)).transpose(1,-1)\n",
        "        x = x + self.ff(x.transpose(2,-1)).transpose(2,-1)\n",
        "        # x = self.ff(x)\n",
        "        # x = x.unflatten(0, (b, num_tok))\n",
        "        return x\n",
        "\n",
        "    # def forward(self, x): # [b,t,c] # [b, (Sy, Sx, h/Sy, w/Sx), c]\n",
        "    #     # Attention + Q Pooling\n",
        "    #     x_norm = self.norm1(x)\n",
        "    #     if self.dim != self.dim_out:\n",
        "    #         x = self.proj(x_norm).unflatten(1, (self.attn.q_stride, -1)).max(dim=1).values # pooling res # [b, (Sy, Sx, h/Sy, w/Sx), c] -> [b, (Sy, Sx), (h/Sy, w/Sx), c] -> [b, (h/Sy, w/Sx), c]\n",
        "    #     x = x + self.drop_path(self.attn(x_norm))\n",
        "    #     x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "    #     return x\n",
        "\n",
        "\n",
        "class levelBlock(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, n_heads=None, depth=1, r=1):\n",
        "        super().__init__()\n",
        "        self.seq = nn.Sequential(\n",
        "            UpDownBlock(in_ch, out_ch, r=min(1,r)) if in_ch != out_ch or r<1 else nn.Identity(),\n",
        "            # AttentionBlock(d_model, d_model, n_heads, q_stride) if in_ch != out_ch or r<1 else nn.Identity(),\n",
        "            # AttentionBlock(d_model, d_model, n_heads, q_stride=(2,2)),\n",
        "            *[HieraBlock(d_model, d_model, n_heads) for i in range(1)],\n",
        "            # UpDownBlock(out_ch, out_ch, r=r) if r>1 else nn.Identity(),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.seq(x)\n",
        "\n",
        "\n",
        "class Hiera(nn.Module):\n",
        "    def __init__(self, patch_size, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "    # def __init__(self, in_dim, out_dim, d_model, n_heads, depth):\n",
        "        # patch_size=4\n",
        "        super().__init__()\n",
        "        self.embed = nn.Sequential( # in, out, kernel, stride, pad\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.MaxPool2d(3,2,1), # nn.MaxPool2d(2,2)\n",
        "            nn.Conv2d(in_dim, d_model, 7, 1, 7//2, bias=False),\n",
        "            # nn.Conv2d(in_dim, d_model, patch_size, patch_size), # like patch\n",
        "            # nn.Conv2d(in_dim, d_model, 3, 2, 3//2, bias=False), nn.BatchNorm2d(d_model), nn.ReLU(),\n",
        "            # UpDownBlock(in_dim, dim, r=1/2, kernel=3), UpDownBlock(dim, dim, r=1/2, kernel=3)\n",
        "            # nn.PixelUnshuffle(2), nn.Conv2d(in_dim*2**2, dim, 1, bias=False),\n",
        "            # ResBlock(in_dim, d_model, kernel),\n",
        "            )\n",
        "        # # self.pos_emb = LearnedRoPE2D(dim) # LearnedRoPE2D, RoPE2D\n",
        "\n",
        "        emb_shape = (32,32)\n",
        "        # emb_shape = (8,32,32)\n",
        "        if len(emb_shape) == 3: # for video\n",
        "            pos_spatial = nn.Parameter(torch.randn(1, emb_shape[1]*emb_shape[2], d_model)*.02)\n",
        "            pos_temporal = nn.Parameter(torch.randn(1, emb_shape[0], d_model)*.02)\n",
        "            self.pos_emb = pos_spatial.repeat(1, emb_shape[0], 1) + torch.repeat_interleave(pos_temporal, emb_shape[1] * emb_shape[2], dim=1)\n",
        "        elif len(emb_shape) == 2: # for img\n",
        "            self.pos_emb = nn.Parameter(torch.randn(1, d_model, *emb_shape)*.02) # 56*56=3136\n",
        "        # self.pos_emb = self.pos_emb.flatten(2).transpose(-2,-1)\n",
        "\n",
        "        # self.blocks = nn.Sequential(*[AttentionBlock(d_model, n_heads, q_stride=(2,2) if i in [1,3] else None) for i in range(depth)])\n",
        "        mult = [1,1,1,1]\n",
        "        # mult = [1,2,4,8] # [1,2,3,4] [1,2,2,2]\n",
        "        ch_list = [d_model * m for m in mult] # [128, 256, 384, 512]\n",
        "\n",
        "        self.blocks = nn.Sequential(\n",
        "            # HieraBlock(ch_list[0], ch_list[1], n_heads, q_stride=(2,2)),\n",
        "            # UpDownBlock(ch_list[0], ch_list[1], kernel=1, r=1/2),\n",
        "            HieraBlock(ch_list[1], ch_list[1], n_heads),\n",
        "            # HieraBlock(ch_list[1], ch_list[2], n_heads, q_stride=(2,2)),\n",
        "            # UpDownBlock(ch_list[1], ch_list[2], kernel=1, r=1/2),\n",
        "            # HieraBlock(ch_list[2], ch_list[2], n_heads),\n",
        "            )\n",
        "        self.norm = nn.RMSNorm(ch_list[2]) # LayerNorm RMSNorm\n",
        "        self.out = nn.Linear(ch_list[2], out_dim, bias=False) if out_dim and out_dim != ch_list[2] else None\n",
        "\n",
        "\n",
        "    def forward(self, x, context_indices=None): # [b,c,h,w], [b,num_tok]\n",
        "        x = self.embed(x)\n",
        "        # print('vit fwd', x.shape, self.pos_emb.shape)\n",
        "        x = x + self.pos_emb\n",
        "        x = unshuffle(x, (4,4)) # [b,num_tok,c,win1,win2,win3] or [b,1,c,f,h,w]\n",
        "        if context_indices != None: x = x[torch.arange(x.shape[0]).unsqueeze(-1), context_indices] # [batch, num_context_toks, d_model]\n",
        "\n",
        "        # print('vit fwd1', x.shape)\n",
        "        x = self.blocks(x) # [b,num_tok,c,1,1,1]\n",
        "        # print('vit fwd2', x.shape)\n",
        "        # x = x.mean(-1).mean(-1)\n",
        "        x = x.flatten(-2).max(-1)[0]\n",
        "        # print('vit fwd3', x.shape)\n",
        "        x = x.squeeze() # [b,num_tok,d]\n",
        "        out = self.norm(x)\n",
        "        if self.out: out = self.out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "# pos_emb rope < learn < learned\n",
        "# conv > pixel?\n",
        "# droppath not required\n",
        "\n",
        "# norm,act,conv < conv,norm,act\n",
        "# 2*s1 < uib < resblock\n",
        "# gatedadaln 3 < 2 = 1 < ffmult4 = 2*gatedadaln\n",
        "# MaxPool2d(2,2) < MaxPool2d(3,2,3//2)\n",
        "\n",
        "# patchattn only for\n",
        "\n",
        "# multiendfusion negligible diff\n",
        "\n",
        "\n",
        "d_model = 64\n",
        "in_dim = 3\n",
        "patch_size = 2\n",
        "# model = Hiera(patch_size, in_dim, d_model, n_heads=4, nlayers=4, drop=0.).to(device)\n",
        "model = Hiera(patch_size, in_dim, d_model, n_heads=4, nlayers=1, drop=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x = torch.rand((4, in_dim, 32, 32), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 59850\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lec1Nq7nkbgt",
        "outputId": "bd142f82-f226-4884-ab9a-33d38e8b9c67",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "124928\n",
            "torch.Size([4, 64, 64])\n",
            "124928\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## vit"
      ],
      "metadata": {
        "id": "g3XSZZyUPY1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title FSQ me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def ste_round(x): return x.round().detach() + x - x.detach()\n",
        "\n",
        "class FSQ(nn.Module):\n",
        "    def __init__(self, levels):\n",
        "        super().__init__()\n",
        "        self.levels = torch.tensor(levels, device=device)\n",
        "        self.basis = torch.cumprod(torch.tensor([*levels[1:], 1], device=device).flip(-1), dim=0).flip(-1)\n",
        "        self.half_width = (self.levels-1)/2\n",
        "        self.codebook_size = torch.prod(self.levels).item()\n",
        "        self.codebook = self.indexes_to_codes(torch.arange(self.codebook_size, device=device))\n",
        "\n",
        "    def forward(self, z, beta=1): # beta in (0,1). beta->0 => values more spread out\n",
        "        offset = (self.levels+1) % 2 /2 # .5 if even, 0 if odd\n",
        "        bound = (F.sigmoid(z)-1/2) * (self.levels-beta) + offset\n",
        "        # print('fwd', bound) #\n",
        "        quantized = ste_round(bound)\n",
        "        # print('fwd', quantized) # 4: -1012\n",
        "        return (quantized-offset) / self.half_width # split [-1,1]\n",
        "\n",
        "    def codes_to_indexes(self, zhat):\n",
        "        zhat = (zhat + 1) * self.half_width\n",
        "        return (zhat * self.basis).sum(axis=-1)#.int()\n",
        "\n",
        "    def indexes_to_codes(self, indices):\n",
        "        indices = indices.unsqueeze(-1)\n",
        "        codes = torch.remainder(indices//self.basis, self.levels)\n",
        "        # print(\"codes\",codes)\n",
        "        return codes / self.half_width - 1\n",
        "\n",
        "# fsq = FSQ(levels = [5,4,3,2])\n",
        "# # print(fsq.codebook)\n",
        "# batch_size, seq_len = 2, 4\n",
        "# # x = torch.rand((batch_size, seq_len,3),device=device)\n",
        "# x = torch.linspace(-2,2,7).repeat(4,1).T\n",
        "# la = fsq(x)\n",
        "# print(la)\n",
        "# lact = fsq.codes_to_indexes(la)\n",
        "# print(lact)\n",
        "# # la = fsq.indexes_to_codes(lact)\n",
        "# # print(la)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "eDqkaM2v0_zS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title AttentionBlock\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class SelfAttn(nn.Module):\n",
        "    def __init__(self, dim, n_heads):\n",
        "        super().__init__()\n",
        "        self.dim, self.n_heads = dim, n_heads\n",
        "        d_head = dim//n_heads\n",
        "        self.qkv = nn.Linear(dim, dim*3, bias=False)\n",
        "        # self.lin = nn.Linear(dim, dim)\n",
        "        self.lin = zero_module(nn.Linear(dim, dim))\n",
        "        # self.rope = RoPE(d_head, seq_len=512, base=10000)\n",
        "        # self.rope = RoPE2D(d_head, h=64, w=64, base=100)\n",
        "        self.scale = d_head**-.5\n",
        "\n",
        "    def forward(self, x): # [b,t,d]\n",
        "        q,k,v = self.qkv(x).unflatten(-1, (self.n_heads,-1)).transpose(1,2).chunk(3, dim=-1) # [b, t, n_heads, d_head] -> [b, n_heads, t, d_head]\n",
        "        # q, k = self.rope(q), self.rope(k)\n",
        "        # x = F.scaled_dot_product_attention(q,k,v, attn_mask=None) # https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html\n",
        "        q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2)\n",
        "        context = k.transpose(-2,-1) @ v # [batch, n_heads, d_head, d_head]\n",
        "        x = q @ context # [b, n_heads, t, d_head]\n",
        "        # # print('SelfAttn', x.shape)\n",
        "        x = x.transpose(1,2).flatten(2)\n",
        "        return self.lin(x)\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, mult=4, drop=0.):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        # self.norm = nn.RMSNorm(d_model, elementwise_affine=False) # LayerNorm RMSNorm\n",
        "        self.norm1, self.norm2 = nn.LayerNorm(d_model), nn.LayerNorm(d_model)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "        self.attn = SelfAttn(d_model, n_heads)\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        ff_dim=d_model*mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), act, nn.Dropout(drop), nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "            nn.RMSNorm(ff_dim), act, nn.Dropout(drop), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(drop), nn.Linear(d_model, ff_dim), act,\n",
        "            # nn.RMSNorm(ff_dim), nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x): # [batch, seq_len, d_model]\n",
        "        # print('attnblk fwd',x.shape)\n",
        "        x = x + self.drop(self.attn(self.norm1(x)))\n",
        "        x = x + self.ff(x)\n",
        "        # x = x + self.drop(self.ff(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, patch_size, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        patch_size=2\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Conv2d(in_dim, d_model, patch_size, patch_size), # like patch\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 1, 7//2, bias=False)\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.MaxPool2d(3,2,1)\n",
        "            nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.BatchNorm2d(d_model), nn.ReLU(),\n",
        "            nn.Conv2d(d_model, d_model, 3, 2, 3//2, bias=False)\n",
        "            )\n",
        "        # self.embed.requires_grad=False\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, 32*32, d_model)*.02)\n",
        "        # self.pos_emb = nn.Parameter(RoPE2D(dim=d_model, h=8, w=8, base=1000), requires_grad=False)\n",
        "\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=(32//patch_size)**2, base=10000), requires_grad=False)\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, d_model, n_heads) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x).flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c] # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        # x = self.pos_enc(x)\n",
        "        # print(\"TransformerModel\",x.shape, self.pos_emb.shape)\n",
        "        x = x + self.pos_emb[:,:x.shape[1]]\n",
        "        if context_indices != None: x = x[torch.arange(x.shape[0]).unsqueeze(-1), context_indices] # [batch, num_context_toks, d_model]\n",
        "\n",
        "        x = self.transformer(x)\n",
        "        out = self.norm(x)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "d_model = 64\n",
        "in_dim = 3\n",
        "patch_size = 2\n",
        "# model = ViT(patch_size, in_dim, d_model, n_heads=4, nlayers=4, drop=0.).to(device)\n",
        "model = ViT(patch_size, in_dim, d_model, n_heads=4, nlayers=1, drop=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x = torch.rand((5, in_dim, 32, 32), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "f6T4F651kmGh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "112db928-e57b-4575-cf9b-b71923c12417"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "162112\n",
            "torch.Size([5, 64, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title simplex\n",
        "# !pip install -q opensimplex\n",
        "import opensimplex\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def simplexmask2d(hw=(32,32), ctx_scale=(.85,1), trg_scale=(.6,.8), B=64, chaos=2):\n",
        "    seed = np.random.randint(1e10, size=B)\n",
        "    ix = iy = np.linspace(0, chaos, num=max(hw))\n",
        "    noise = opensimplex.noise3array(ix, iy, seed) # [b,h,w]\n",
        "    # plt.pcolormesh(noise[:1])\n",
        "    # plt.rcParams[\"figure.figsize\"] = (20,3)\n",
        "    # plt.show()\n",
        "    noise = torch.from_numpy(noise)\n",
        "    ctx_mask_scale = torch.rand(1) * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    trg_mask_scale = torch.rand(1) * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "    val, ind = noise.flatten(1).sort() # [b,h*w]\n",
        "    seq = hw[0]*hw[1]\n",
        "    trg_index = ind[:,-int(seq*trg_mask_scale):]\n",
        "    ctx_index = ind[:,-int(seq*ctx_mask_scale):-int(seq*trg_mask_scale)] # ctx wraps trg ; most similar to multiblock\n",
        "    # ctx_index = ind[:,:int(seq*ctx_mask_scale)-int(seq*trg_mask_scale)] # ctx hug bottom\n",
        "    # ctx_index = ind[:,-int(seq*ctx_mask_scale)-int(seq*trg_mask_scale):-int(seq*trg_mask_scale)] # ctx wraps trg ; most similar to multiblock\n",
        "    # ctx_index = ind[:,:int(seq*ctx_mask_scale)] # ctx hug bottom\n",
        "    return ctx_index, trg_index\n",
        "\n",
        "b=16\n",
        "# trg_index, ctx_index = simplexmask1d(seq=500, ctx_scale=(.85,1), trg_scale=(.6,.8), B=b, chaos=3)\n",
        "\n",
        "# ctx_index, trg_index = simplexmask2d(hw=(32,32), ctx_scale=(.6,.7), trg_scale=(.4,.5), B=b, chaos=2)\n",
        "# ctx_index, trg_index = simplexmask2d(hw=(32,32), ctx_scale=(.2,.3), trg_scale=(.4,.5), B=b, chaos=3)\n",
        "ctx_index, trg_index = simplexmask2d(hw=(32,32), ctx_scale=(.85,1), trg_scale=(.7,.8), B=b, chaos=1)\n",
        "# mask = torch.zeros(1 ,32*32)\n",
        "# mask[:, trg_index[:1]] = 1\n",
        "# mask[:, ctx_index[:1]] = .5\n",
        "# mask = mask.reshape(1,32,32)\n",
        "# print(mask)\n",
        "\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "# imshow(mask)\n",
        "\n",
        "mask = torch.zeros(b ,32*32)\n",
        "mask[torch.arange(b).unsqueeze(-1), trg_index] = 1\n",
        "mask[torch.arange(b).unsqueeze(-1), ctx_index] = .5\n",
        "mask = mask.reshape(b,1,32,32)\n",
        "import torchvision\n",
        "imshow(torchvision.utils.make_grid(mask, nrow=8))\n"
      ],
      "metadata": {
        "id": "Fh9o__m2-j7K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "f726ccd8-f21e-4409-b054-54a866017570",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/268.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m266.2/268.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.0/268.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title masks\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def multiblock(seq, min_s, max_s, M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "    mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "    mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "    indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "    target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "    return target_mask\n",
        "\n",
        "\n",
        "def multiblock2d(hw=(8,8), scale=(.15,.2), aspect_ratio=(.75,1.5), M=1): # https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "    mask_aspect = torch.rand(1) * (aspect_ratio[1] - aspect_ratio[0]) + aspect_ratio[0] # in (min_s, max_s) # all blocks same size\n",
        "    mask_scale = torch.rand(1) * (scale[1] - scale[0]) + scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    h = (mask_scale/mask_aspect)**.5# h*(h*aspect) = scale\n",
        "    w = h * mask_aspect\n",
        "    h_pos, w_pos = torch.rand(M)*(1-w), torch.rand(M)*(1-h) # in (0, 1 - mask_len)\n",
        "    h_len, h_pos = (h*hw[0]).int(), h_pos*hw[0]\n",
        "    w_len, w_pos = (w*hw[1]).int(), w_pos*hw[1]\n",
        "    h_ind, w_ind = torch.arange(hw[0]).unsqueeze(0), torch.arange(hw[1]).unsqueeze(0) # [1, seq]\n",
        "    h_mask = (h_ind>=h_pos.unsqueeze(-1)) & (h_ind<(h_pos+h_len).unsqueeze(-1)) # [M, seq]\n",
        "    w_mask = (w_ind>=w_pos.unsqueeze(-1)) & (w_ind<(w_pos+w_len).unsqueeze(-1)) # [M, seq]\n",
        "    target_mask = h_mask.unsqueeze(-1) & w_mask.unsqueeze(-2) # [M, seq, seq]\n",
        "    return target_mask\n",
        "\n",
        "# https://arxiv.org/pdf/2210.07224\n",
        "def randpatch(seq, mask_size=8, gamma=0.9): # num patches of seq, mask patch size, masking ratio\n",
        "    # mask = torch.rand(seq//mask_size)<gamma\n",
        "    length = seq//mask_size\n",
        "    g = torch.normal(gamma, std=.1, size=(1,)).clamp(.5,.9)\n",
        "    # g = gamma\n",
        "    idx = torch.randperm(length)[:int(length*g)]\n",
        "    mask = torch.zeros(length, dtype=bool)\n",
        "    mask[idx] = True\n",
        "    mask = mask.repeat_interleave(mask_size, dim=-1)\n",
        "    return mask # [seq] , True -> mask\n",
        "\n",
        "\n",
        "import torch\n",
        "def apply_masks(x, mask): # [b,t,d], [mask_size] # https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "    mask_keep = mask.unsqueeze(-1).repeat(x.size(0), 1, x.size(-1)) # [batch,T,dim]\n",
        "    return torch.gather(x, dim=1, index=mask_keep) # [batch,mask_size,dim]\n",
        "\n",
        "\n",
        "# @title ijepa multiblock next\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "import math\n",
        "from multiprocessing import Value\n",
        "import torch\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(self, hw=(224, 224), enc_mask_scale=(.85,1), pred_mask_scale=(.15,.2), aspect_ratio=(.75,1.25),\n",
        "        nenc=1, npred=2, min_keep=4, allow_overlap=False):\n",
        "        super().__init__()\n",
        "        self.height, self.width = hw\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.aspect_ratio = aspect_ratio\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "\n",
        "    def _sample_block_size(self, scale, aspect_ratio_scale):\n",
        "        _rand = torch.rand(1).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.height * self.width * mask_scale) # num patches to keep\n",
        "        # -- Sample block aspect-ratio\n",
        "        min_ar, max_ar = aspect_ratio_scale\n",
        "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
        "        while h >= self.height: h -= 1 # crop mask to be smaller than img\n",
        "        while w >= self.width: w -= 1\n",
        "        return (h, w)\n",
        "\n",
        "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
        "        h, w = b_size\n",
        "\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # --\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            top = torch.randint(0, self.height - h, (1,))\n",
        "            left = torch.randint(0, self.width - w, (1,))\n",
        "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
        "            mask[top:top+h, left:left+w] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
        "        mask_complement[top:top+h, left:left+w] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, B):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        p_size = self._sample_block_size(scale=self.pred_mask_scale, aspect_ratio_scale=self.aspect_ratio)\n",
        "        e_size = self._sample_block_size(scale=self.enc_mask_scale, aspect_ratio_scale=(1., 1.))\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.height * self.width\n",
        "        min_keep_enc = self.height * self.width\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                print(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "        return collated_masks_enc, collated_masks_pred\n",
        "\n",
        "mask_collator = MaskCollator(hw=(32,32), enc_mask_scale=(.85, 1.), pred_mask_scale=(.15, .2), aspect_ratio=(.75, 1.5),\n",
        "        nenc=1, npred=4, min_keep=4,\n",
        "        # allow_overlap=True)\n",
        "        allow_overlap=False)\n",
        "\n",
        "b=16\n",
        "collated_masks_enc, collated_masks_pred = mask_collator(b)\n",
        "ctx_index, trg_index = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "\n",
        "# mask = torch.zeros(1 ,32*32)\n",
        "# mask[:, trg_index[:1]] = 1\n",
        "# mask[:, ctx_index[:1]] = .5\n",
        "# mask = mask.reshape(1,32,32)\n",
        "\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "# imshow(mask)\n",
        "\n",
        "mask = torch.zeros(b ,32*32)\n",
        "mask[torch.arange(b).unsqueeze(-1), trg_index] = 1\n",
        "mask[torch.arange(b).unsqueeze(-1), ctx_index] = .5\n",
        "mask = mask.reshape(b,1,32,32)\n",
        "import torchvision\n",
        "imshow(torchvision.utils.make_grid(mask, nrow=8))\n"
      ],
      "metadata": {
        "id": "pQfM2fYvcTh6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "24ff1aa4-115b-4744-87dd-2005f5949a31",
        "cellView": "form"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Transformer Predictor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, 32*32, d_model)*0.02)\n",
        "        # self.pos_emb = nn.Parameter(RoPE2D(dim=d_model, h=8, w=8, base=1000), requires_grad=False)\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=32*32, base=10000), requires_grad=False)\n",
        "\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, d_model, n_heads) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.randn(1,1,d_model)*0.02) # randn zeros\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim)# if out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = x.shape\n",
        "        # x = x * self.pos_enc(context_indices)\n",
        "        x = x + self.pos_emb[0,context_indices]\n",
        "\n",
        "        # pred_tokens = self.cls * self.pos_enc(trg_indices) # [M, num_trg_toks, d_model]\n",
        "        pred_tokens = self.cls + self.pos_emb[0,trg_indices]\n",
        "        # print(\"pred fwd\", x.shape, pred_tokens.shape)\n",
        "        x = torch.cat([x, pred_tokens], dim=1) # [batch, seq_len+num_trg_toks, d_model]\n",
        "        out = self.transformer(x)\n",
        "\n",
        "        out = self.norm(out)\n",
        "\n",
        "        out = out[:,seq:] # [batch, num_trg_toks, d_model]\n",
        "        # ind = torch.cat([context_indices, trg_indices], dim=-1).unsqueeze(-1).repeat(1,1,x.shape[-1]) # [b,t]->[b,t,d]\n",
        "        # out=torch.gather(out, 1, ind)\n",
        "\n",
        "        # print(\"pred fwd\", context_indices.shape, trg_indices.shape, out.shape)\n",
        "\n",
        "        out = self.lin(out)\n",
        "        # print(\"pred fwd\", out.shape)\n",
        "        return out # [seq_len, batch_size, ntoken]\n"
      ],
      "metadata": {
        "id": "M-zdjdJixtOu",
        "cellView": "form"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title IJEPA\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class IJEPA(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=64, out_dim=None, nlayers=1, n_heads=4):\n",
        "        super().__init__()\n",
        "        self.out_dim = out_dim = out_dim or d_model\n",
        "        self.patch_size = 4 # 4\n",
        "        # self.student = Hiera(self.patch_size, in_dim, d_model, out_dim=out_dim, n_heads=n_heads, nlayers=nlayers, drop=0.)\n",
        "        self.student = ViT(self.patch_size, in_dim, d_model, out_dim=out_dim, n_heads=n_heads, nlayers=nlayers, drop=0.)\n",
        "        # self.predicter = TransformerPredictor(out_dim, d_model//2, out_dim, n_heads=n_heads, nlayers=nlayers//2, drop=0.)\n",
        "        self.predicter = TransformerPredictor(out_dim, 3*d_model//8, out_dim, n_heads=4, nlayers=1, drop=0.)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "\n",
        "    def loss(self, x): #\n",
        "        b,c,h,w = x.shape\n",
        "        # print('ijepa loss x',x.shape)\n",
        "        hw=(8,8)\n",
        "        # hw=(32,32)\n",
        "        mask_collator = MaskCollator(hw, enc_mask_scale=(.85, 1.), pred_mask_scale=(.15, .2), aspect_ratio=(.75, 1.5), nenc=1, npred=4, min_keep=4, allow_overlap=False)\n",
        "        collated_masks_enc, collated_masks_pred = mask_collator(b)\n",
        "        context_indices, trg_indices = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "        # print(context_indices.shape, trg_indices.shape)\n",
        "\n",
        "        # context_indices, trg_indices = simplexmask2d(hw, ctx_scale=(.85,1), trg_scale=(.6,.8), B=b, chaos=3)\n",
        "\n",
        "        # zero_mask = torch.zeros(b ,*hw, device=device).flatten(1)\n",
        "        # zero_mask[torch.arange(b).unsqueeze(-1), context_indices] = 1\n",
        "        # x_ = x * F.interpolate(zero_mask.reshape(b,1,*hw), size=x.shape[2:], mode='nearest-exact') # zero masked locations\n",
        "\n",
        "        sx = self.student(x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "        # print('ijepa loss sx',sx.shape)\n",
        "        sy_ = self.predicter(sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "        sy_ = F.layer_norm(sy_, (sy_.size(-1),))\n",
        "        with torch.no_grad():\n",
        "            sy = self.teacher(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "            sy = sy[torch.arange(sy.shape[0]).unsqueeze(-1), trg_indices] # [batch, num_context_toks, d_model] # nan bec len(trg_ind)==0 # print('loss sy',torch.isnan(sy).any())\n",
        "            sy = F.layer_norm(sy, (sy.size(-1),))\n",
        "        loss = F.mse_loss(sy, sy_)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        sx = self.student(x)\n",
        "        out = sx.mean(dim=1)\n",
        "        return out\n",
        "\n",
        "# min_s=0.15, max_s, M\n",
        "# trg.15.2M4 C.85 1\n",
        "# dont normalise data\n",
        "# randmsk < simplex 1msk < multiblk\n",
        "# teacher=trans ~? teacher=copy\n",
        "# learn convemb\n",
        "# lr pred,student: 3e-3/1e-2, 1e-3\n",
        "\n",
        "# vit 1lyr 15.4sec 15.6\n",
        "# hiera downsampling attn 20.5\n",
        "\n",
        "# ijepa = IJEPA(in_dim=3, d_model=64, out_dim=64, nlayers=4, n_heads=4).to(device)\n",
        "ijepa = IJEPA(in_dim=3, d_model=64, out_dim=64, nlayers=1, n_heads=4).to(device)\n",
        "# ijepa = IJEPA(in_dim=3, d_model=32, out_dim=32, nlayers=1, n_heads=4).to(device)\n",
        "# optim = torch.optim.AdamW(ijepa.parameters(), lr=1e-3) # 1e-3?\n",
        "optim = torch.optim.AdamW([{'params': ijepa.student.parameters()},\n",
        "#     {'params': ijepa.predicter.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # good 3e-3,1e-3 ; default 1e-2, 5e-2\n",
        "    {'params': ijepa.predicter.parameters(), 'lr': 1e-2}], lr=1e-3, weight_decay=1e-2)\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/configs/in1k_vith14_ep300.yaml\n",
        "# d_model 1024,384\n",
        "# depth 12,6/12\n",
        "# wd 5e-2 - 4e-1\n",
        "# adamw 1e-4 - 1e-3 - 1e-6\n",
        "# ema 0.996-1\n",
        "\n",
        "print(sum(p.numel() for p in ijepa.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in ijepa.parameters())) # 27584\n",
        "# print(sum(p.numel() for p in ijepa.predicter.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# d_model^2 * nlayers\n",
        "\n",
        "x = torch.rand((64,3,32,32), device=device)\n",
        "out = ijepa.loss(x)\n",
        "print(out.shape)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(ijepa.out_dim, 10).to(device)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n",
        "# optim = torch.optim.AdamW([{'params': ijepa.parameters()}, {'params': classifier.parameters(), 'lr': 1e-3}], lr=1e-3)\n",
        "\n"
      ],
      "metadata": {
        "id": "ardu1zJwdHM3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "035ebea4-7ea5-46af-d6de-fa2bfa59a2e3"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "197168\n",
            "torch.Size([])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title TransformerVICReg\n",
        "import torch\n",
        "from torch import nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class TransformerVICReg(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, drop=0):\n",
        "        super().__init__()\n",
        "        act = nn.GELU()\n",
        "        patch_size=4\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Linear(in_dim, d_model), act\n",
        "            # nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            # nn.Conv1d(d_model, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            # nn.Conv1d(d_model, d_model,3,2,3//2),\n",
        "            nn.Conv2d(in_dim, d_model, patch_size, patch_size), # patch\n",
        "            )\n",
        "        self.pos_enc = RoPE(d_model, seq_len=200, base=10000)\n",
        "        # self.pos_emb = nn.Parameter(torch.randn(1, 8*8, d_model))\n",
        "        # self.pos_emb = nn.Parameter(RoPE2D(dim=d_model, h=8, w=8, base=10000).unsqueeze(0), requires_grad=False)\n",
        "\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "\n",
        "        # out_dim = out_dim or d_model\n",
        "        self.lin = nn.Linear(d_model, out_dim)\n",
        "        self.attn_pool = nn.Linear(d_model, 1, bias=False)\n",
        "\n",
        "        dim_v = d_model * 4\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(out_dim, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v, bias=False),\n",
        "            )\n",
        "\n",
        "    def forward(self, x): # [b,t,d] / [b,c,h,w]\n",
        "        x = self.embed(x).flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c]\n",
        "        # x = self.embed(x.transpose(-2,-1)).transpose(-2,-1) # [b,t,d]\n",
        "        x = self.pos_enc(x)\n",
        "        # x = x + self.pos_emb\n",
        "\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        attn = self.attn_pool(x).squeeze(-1) # [batch, seq] # seq_pool\n",
        "        out = (torch.softmax(attn, dim=-1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, seq] @ [batch, seq, dim] -> [batch, dim]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch, ntoken]\n",
        "\n",
        "    def expand(self, x):\n",
        "        sx = self.forward(x)\n",
        "        vx = self.exp(sx)\n",
        "        return vx\n",
        "\n",
        "batch, seq_len, d_model = 4,3500,512\n",
        "in_dim, out_dim=3,16\n",
        "model = TransformerVICReg(in_dim, d_model, out_dim, d_head=4, nlayers=2, drop=0.).to(device)\n",
        "# x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "x =  torch.rand((batch, in_dim, 32,32), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ],
      "metadata": {
        "id": "ODpKypTCsfIt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2f4380b-03a8-49be-95b5-eed2b564dc05",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Violet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class Violet(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=32, out_dim=None, nlayers=2, d_head=4):\n",
        "        super().__init__()\n",
        "        out_dim = out_dim or d_model\n",
        "        self.student = TransformerVICReg(in_dim, d_model, out_dim=out_dim, d_head=d_head, nlayers=nlayers, drop=0.)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "\n",
        "        # vicreg\n",
        "        self.sim_coeff=25. # 10.0 # 25.0 # λ\n",
        "        self.std_coeff=25. # 1.0 # 25.0 # µ\n",
        "        self.cov_coeff=1. # 25.0 # 1.0 # ν\n",
        "\n",
        "    def loss(self, x): # [batch, T, 3]c/ [b,c,h,w]\n",
        "        # print(x.shape)\n",
        "        # mask = simplexmask(hw=(8,8), scale=(.7,.8)).unsqueeze(0) # .6.8\n",
        "        # vx = self.student.expand(x, mask) # [batch, num_context_toks, out_dim]\n",
        "\n",
        "        vx = self.student.expand(x) # [batch, num_context_toks, out_dim]\n",
        "\n",
        "        with torch.no_grad(): vy = self.teacher.expand(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "        loss = self.vicreg(vx, vy)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        return self.student(x)\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size, num_features = x.shape\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = self.sim_coeff * repr_loss + self.std_coeff * std_loss + self.cov_coeff * cov_loss\n",
        "        print(\"in vicreg \",self.sim_coeff * repr_loss.item() , self.std_coeff * std_loss.item() , self.cov_coeff * cov_loss.item())\n",
        "        # return loss\n",
        "        return repr_loss, std_loss, cov_loss\n",
        "\n",
        "\n",
        "violet = Violet(in_dim=3, d_model=32, out_dim=16, nlayers=2, d_head=4).to(device)\n",
        "voptim = torch.optim.AdamW(violet.parameters(), lr=1e-3) # 1e-3?\n",
        "# voptim = torch.optim.AdamW([{'params': violet.student.transformer.parameters()},\n",
        "#     {'params': violet.student.exp.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # default 1e-2\n",
        "print(sum(p.numel() for p in violet.parameters() if p.requires_grad)) # 27584\n",
        "\n",
        "# x = torch.rand((2,1000,3), device=device)\n",
        "x = torch.rand((2,3,32,32), device=device)\n",
        "# x = torch.rand((2,1,16), device=device)\n",
        "loss = violet.loss(x)\n",
        "# print(out.shape)\n",
        "print(loss)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(16).to(device)\n",
        "# classifier = Classifier(16, 18).to(device)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n",
        "\n"
      ],
      "metadata": {
        "id": "5qwg9dG4sQ3h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f0092f5-5bb7-4725-db2b-dcf7b0ba1a96",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "62850\n",
            "in vicreg  0.00019920778413506923 24.74469393491745 4.793645480560826e-09\n",
            "(tensor(7.9683e-06, grad_fn=<MseLossBackward0>), tensor(0.9898, grad_fn=<AddBackward0>), tensor(4.7936e-09, grad_fn=<AddBackward0>))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title IJEPA + VICReg\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "    def __init__(self, patch_size, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        patch_size=4\n",
        "        act = nn.GELU()\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Conv2d(in_dim, d_model, patch_size, patch_size), # like patch\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 1, 7//2, bias=False)\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.MaxPool2d(3,2,1)\n",
        "            nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.BatchNorm2d(d_model), nn.ReLU(),\n",
        "            nn.Conv2d(d_model, d_model, 3, 2, 3//2, bias=False)\n",
        "            )\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, 32*32, d_model)*.02)\n",
        "        # self.pos_emb = nn.Parameter(RoPE2D(dim=d_model, h=8, w=8, base=1000), requires_grad=False)\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=(32//patch_size)**2, base=10000), requires_grad=False)\n",
        "\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, d_model, n_heads) for _ in range(nlayers)])\n",
        "\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "        # self.lin = nn.Linear(d_model, out_dim)\n",
        "        self.attn_pool = nn.Linear(d_model, 1, bias=False)\n",
        "\n",
        "        dim_v = d_model * 4\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(out_dim, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v), act,\n",
        "            nn.Linear(dim_v, dim_v, bias=False),\n",
        "            )\n",
        "\n",
        "    # def forward(self, x, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "    def jepa_fwd(self, x, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x).flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c] # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        # x = self.pos_enc(x)\n",
        "        # print(\"TransformerModel\",x.shape, self.pos_emb.shape)\n",
        "        x = x + self.pos_emb[:,:x.shape[1]]\n",
        "        if context_indices != None: x = x[torch.arange(x.shape[0]).unsqueeze(-1), context_indices] # [batch, num_context_toks, d_model]\n",
        "\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        out = self.norm(x)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out\n",
        "\n",
        "    def pool(self, x):\n",
        "        # sx = self.forward(x)\n",
        "        # attn = self.attn_pool(x).squeeze(-1) # [batch, seq] # seq_pool\n",
        "        # out = (torch.softmax(attn, dim=-1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, seq] @ [batch, seq, dim] -> [batch, dim]\n",
        "        # if self.lin: out = self.lin(out)\n",
        "        out = x.mean(dim=1)\n",
        "        return out\n",
        "\n",
        "    def forward(self, x, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.jepa_fwd(x, context_indices=None)\n",
        "        out = self.pool(x)\n",
        "        return out\n",
        "\n",
        "    def expand(self, x):\n",
        "        # sx = self.forward(x)\n",
        "        sx = self.pool(x)\n",
        "        vx = self.exp(sx)\n",
        "        return vx\n",
        "\n",
        "\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class IJEPAVICReg(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=64, out_dim=None, nlayers=1, n_heads=4):\n",
        "        super().__init__()\n",
        "        self.out_dim = out_dim = out_dim or d_model\n",
        "        self.patch_size = 4 # 4\n",
        "        self.student = ViT(self.patch_size, in_dim, d_model, out_dim=out_dim, n_heads=n_heads, nlayers=nlayers, drop=0.)\n",
        "        self.predicter = TransformerPredictor(out_dim, 3*d_model//8, out_dim, n_heads=4, nlayers=1, drop=0.)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "\n",
        "        # vicreg\n",
        "        self.sim_coeff=25. # 10.0 # 25.0 # λ\n",
        "        self.std_coeff=25. # 1.0 # 25.0 # µ\n",
        "        self.cov_coeff=1. # 25.0 # 1.0 # ν\n",
        "\n",
        "\n",
        "    def loss(self, x): #\n",
        "        b,c,h,w = x.shape\n",
        "        # print('ijepa loss x',x.shape)\n",
        "        hw=(8,8)\n",
        "        # hw=(32,32)\n",
        "        mask_collator = MaskCollator(hw, enc_mask_scale=(.85, 1.), pred_mask_scale=(.15, .2), aspect_ratio=(.75, 1.5), nenc=1, npred=4, min_keep=4, allow_overlap=False)\n",
        "        collated_masks_enc, collated_masks_pred = mask_collator(b)\n",
        "        context_indices, trg_indices = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "        # print(context_indices.shape, trg_indices.shape)\n",
        "\n",
        "        # context_indices, trg_indices = simplexmask2d(hw, ctx_scale=(.85,1), trg_scale=(.6,.8), B=b, chaos=3)\n",
        "\n",
        "        # zero_mask = torch.zeros(b ,*hw, device=device).flatten(1)\n",
        "        # zero_mask[torch.arange(b).unsqueeze(-1), context_indices] = 1\n",
        "        # x_ = x * F.interpolate(zero_mask.reshape(b,1,*hw), size=x.shape[2:], mode='nearest-exact') # zero masked locations\n",
        "\n",
        "        sx = self.student.jepa_fwd(x, context_indices=context_indices) # [batch, num_context_toks, out_dim]\n",
        "\n",
        "        # print('ijepa loss sx',sx.shape)\n",
        "        sy_ = self.predicter(sx, context_indices=context_indices, trg_indices=trg_indices) # [batch*M, num_trg_toks, out_dim]\n",
        "        sy_ = F.layer_norm(sy_, (sy_.size(-1),))\n",
        "        vx = self.student.expand(sx) # [batch, num_context_toks, out_dim]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            sy = self.teacher.jepa_fwd(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "            sy = sy[torch.arange(sy.shape[0]).unsqueeze(-1), trg_indices] # [batch, num_context_toks, d_model] # nan bec len(trg_ind)==0 # print('loss sy',torch.isnan(sy).any())\n",
        "            sy = F.layer_norm(sy, (sy.size(-1),))\n",
        "\n",
        "            vy = self.teacher.expand(sy.detach()) # [batch, num_trg_toks, out_dim]\n",
        "\n",
        "        vic_loss = self.vicreg(vx, vy)\n",
        "        j_loss = F.mse_loss(sy, sy_)\n",
        "        # return loss\n",
        "        return j_loss, vic_loss\n",
        "\n",
        "    def forward(self, x): # [batch, T, 3]\n",
        "        out = self.student(x)\n",
        "        return out\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size, num_features = x.shape\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = self.sim_coeff * repr_loss + self.std_coeff * std_loss + self.cov_coeff * cov_loss\n",
        "        # print(\"in vicreg \",self.sim_coeff * repr_loss.item() , self.std_coeff * std_loss.item() , self.cov_coeff * cov_loss.item())\n",
        "        # return loss\n",
        "        return repr_loss, std_loss, cov_loss\n",
        "\n",
        "# for jepa+vic, attnpool < meanpool\n",
        "# jepa+vic < jepa\n",
        "\n",
        "\n",
        "ijepa = IJEPAVICReg(in_dim=3, d_model=64, out_dim=64, nlayers=1, n_heads=4).to(device)\n",
        "print(sum(p.numel() for p in ijepa.parameters() if p.requires_grad)) # 27584\n",
        "optim = torch.optim.AdamW(ijepa.parameters(), lr=1e-3) # 1e-3?\n",
        "# optim = torch.optim.AdamW([{'params': ijepa.student.parameters()},\n",
        "#     {'params': ijepa.predicter.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # good 3e-3,1e-3 ; default 1e-2, 5e-2\n",
        "#     # {'params': ijepa.predicter.parameters(), 'lr': 1e-2}], lr=1e-3, weight_decay=1e-2)\n",
        "\n",
        "x = torch.rand((2,3,32,32), device=device)\n",
        "loss = ijepa.loss(x)\n",
        "print(loss)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(ijepa.out_dim, 10).to(device)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n"
      ],
      "metadata": {
        "id": "hMTvHTtTRInh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91ca330e-fd81-42c6-af3c-d4e5316df589",
        "cellView": "form"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "345200\n",
            "(tensor(1.9985, device='cuda:0', grad_fn=<MseLossBackward0>), (tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>), tensor(0.9865, device='cuda:0', grad_fn=<AddBackward0>), tensor(7.4222e-06, device='cuda:0', grad_fn=<AddBackward0>)))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in ijepa.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name, param)"
      ],
      "metadata": {
        "id": "m_BFm8Kh-j3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title RankMe\n",
        "# RankMe: Assessing the Downstream Performance of Pretrained Self-Supervised Representations by Their Rank jun 2023 https://arxiv.org/pdf/2210.02885\n",
        "import torch\n",
        "\n",
        "# https://github.com/Spidartist/IJEPA_endoscopy/blob/main/src/helper.py#L22\n",
        "def RankMe(Z):\n",
        "    \"\"\"\n",
        "    Calculate the RankMe score (the higher, the better).\n",
        "    RankMe(Z) = exp(-sum_{k=1}^{min(N, K)} p_k * log(p_k)),\n",
        "    where p_k = sigma_k (Z) / ||sigma_k (Z)||_1 + epsilon\n",
        "    where sigma_k is the kth singular value of Z.\n",
        "    where Z is the matrix of embeddings (N × K)\n",
        "    \"\"\"\n",
        "    # compute the singular values of the embeddings\n",
        "    # _u, s, _vh = torch.linalg.svd(Z, full_matrices=False)  # s.shape = (min(N, K),)\n",
        "    # s = torch.linalg.svd(Z, full_matrices=False).S\n",
        "    s = torch.linalg.svdvals(Z)\n",
        "    p = s / torch.sum(s, axis=0) + 1e-7\n",
        "    return torch.exp(-torch.sum(p * torch.log(p)))\n",
        "\n",
        "# Z = torch.randn(5, 3)\n",
        "# rankme = RankMe(Z)\n",
        "# print(rankme)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "eb1n4BhimG_N"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title LiDAR\n",
        "# LiDAR: Sensing Linear Probing Performance In Joint Embedding SSL Architectures https://arxiv.org/pdf/2312.04000\n",
        "# https://github.com/rbalestr-lab/stable-ssl/blob/main/stable_ssl/monitors.py#L106\n",
        "\n",
        "def LiDAR(sx, eps=1e-7, delta=1e-3):\n",
        "    sx = sx.unflatten(0, (-1, 2))\n",
        "    n, q, d = sx.shape\n",
        "    mu_x = sx.mean(dim=1) # mu_x # [n,d]\n",
        "    mu = mu_x.mean(dim=0) # mu # [d]\n",
        "\n",
        "    diff_b = (mu_x - mu).unsqueeze(-1) # [n,d,1]\n",
        "    S_b = (diff_b @ diff_b.transpose(-2,-1)).sum(0) / (n - 1) # [n,d,d] -> [d,d]\n",
        "    diff_w = (sx - mu_x.unsqueeze(1)).reshape(-1,d,1) # [n,q,d] -> [nq,d,1]\n",
        "    S_w = (diff_w @ diff_w.transpose(-2,-1)).sum(0) / (n * (q - 1)) + delta * torch.eye(d, device=sx.device) # [nq,d,d] -> [d,d]\n",
        "\n",
        "    eigvals_w, eigvecs_w = torch.linalg.eigh(S_w)\n",
        "    eigvals_w = torch.clamp(eigvals_w, min=eps)\n",
        "\n",
        "    invsqrt_w = (eigvecs_w * (1. / torch.sqrt(eigvals_w))) @ eigvecs_w.transpose(-1, -2)\n",
        "    S_lidar = invsqrt_w @ S_b @ invsqrt_w\n",
        "    lam = torch.linalg.eigh(S_lidar)[0].clamp(min=0)\n",
        "    p = lam / lam.sum() + eps\n",
        "    return torch.exp(-torch.sum(p * torch.log(p)))\n",
        "\n",
        "# sx = torch.randn(32, 128)\n",
        "# print(sx.shape)\n",
        "# lidar = LiDAR(sx)\n",
        "# print(lidar.item())\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "y24ZNFqW7woQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "2Nd-sGe6Ku4S",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "outputId": "4663e9dc-af43-4a8d-951a-f7c41c71f787"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>██▇▇▇▇▇▇▇▆▆▆▅▆▅▅▅▅▆▄▄▄▅▄▃▄▃▄▄▃▁▁▂▃▂▁▂▃▂▁</td></tr><tr><td>correct</td><td>▁▁▂▁▂▄▅▄▄▄▃▄▅▅▅▇▇▅▇█▅▄▇▅▇▆▇▅▆▇▆▄▇▅▆▅▆▅▇▆</td></tr><tr><td>lidar</td><td>▃▂▆▆▄▃▆▅▅▇▅▂▄▃▇▅▃▄█▇▂▅▆▅▃▃▁▄▂▆▅▄▅▃█▂▄▅▄▃</td></tr><tr><td>loss</td><td>▁▂▁▁▂▂▂▄▁▂▃▄▆▁▁▃▃▂▂▂▃▆▄▅▃▃▂▅▆▅▄▂▄▇▆▅█▃▂▃</td></tr><tr><td>rankme</td><td>▁▁▂▃▂▃▄▄▄▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>1.78607</td></tr><tr><td>correct</td><td>0.23438</td></tr><tr><td>lidar</td><td>14.54945</td></tr><tr><td>loss</td><td>0.26696</td></tr><tr><td>rankme</td><td>22.73201</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trim-puddle-79</strong> at: <a href='https://wandb.ai/bobdole/ijepa/runs/pmrnhy58' target=\"_blank\">https://wandb.ai/bobdole/ijepa/runs/pmrnhy58</a><br> View project at: <a href='https://wandb.ai/bobdole/ijepa' target=\"_blank\">https://wandb.ai/bobdole/ijepa</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250414_021849-pmrnhy58/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250414_033715-fsd0o2yk</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/ijepa/runs/fsd0o2yk' target=\"_blank\">eternal-frost-80</a></strong> to <a href='https://wandb.ai/bobdole/ijepa' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/ijepa' target=\"_blank\">https://wandb.ai/bobdole/ijepa</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/ijepa/runs/fsd0o2yk' target=\"_blank\">https://wandb.ai/bobdole/ijepa/runs/fsd0o2yk</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "!pip install -q wandb\n",
        "import wandb # https://docs.wandb.ai/quickstart\n",
        "wandb.login(key='487a2109e55dce4e13fc70681781de9f50f27be7')\n",
        "try: run.finish()\n",
        "except NameError: pass\n",
        "run = wandb.init(project=\"ijepa\", config={\"model\": \"res18\",})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34de8800-fa4b-4460-803c-3cd8dbfb6c70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f18003f7-a108-4b98-d946-5861a01a52bf",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "strain 2.1782681941986084\n",
            "strain 0.9828532338142395\n",
            "strain 0.9428795576095581\n",
            "strain 0.9278950095176697\n",
            "strain 0.8584551811218262\n",
            "strain 0.8069671392440796\n",
            "classify 2.39453125\n",
            "classify 2.4312744140625\n",
            "classify 2.410888671875\n",
            "classify 2.4522705078125\n",
            "classify 2.2838134765625\n",
            "classify 2.39599609375\n",
            "classify 2.4072265625\n",
            "classify 2.3787841796875\n",
            "classify 2.4407958984375\n",
            "classify 2.300537109375\n",
            "classify 2.30908203125\n",
            "0.140625\n",
            "0.15625\n",
            "0.140625\n",
            "0.078125\n",
            "0.109375\n",
            "0.125\n",
            "0.171875\n",
            "0.09375\n",
            "0.109375\n",
            "0.125\n",
            "0.171875\n",
            "time: 5.060026168823242 5.0600268840789795\n",
            "1\n",
            "strain 0.8335343599319458\n",
            "strain 0.7619064450263977\n",
            "strain 0.7876648902893066\n",
            "strain 0.7278911471366882\n",
            "strain 0.6789777874946594\n",
            "strain 0.6973310708999634\n",
            "classify 2.41015625\n",
            "classify 2.38916015625\n",
            "classify 2.3173828125\n",
            "classify 2.333740234375\n",
            "classify 2.337890625\n",
            "classify 2.31396484375\n",
            "classify 2.3489990234375\n",
            "classify 2.4718017578125\n",
            "classify 2.30029296875\n",
            "classify 2.3380126953125\n",
            "classify 2.3433837890625\n",
            "0.140625\n",
            "0.09375\n",
            "0.0625\n",
            "0.15625\n",
            "0.171875\n",
            "0.09375\n",
            "0.140625\n",
            "0.125\n",
            "0.078125\n",
            "0.078125\n",
            "0.15625\n",
            "time: 4.06578803062439 4.563116908073425\n",
            "2\n",
            "strain 0.6713978052139282\n",
            "strain 0.7273394465446472\n",
            "strain 0.685651957988739\n",
            "strain 0.6595648527145386\n",
            "strain 0.6737430095672607\n",
            "strain 0.6274166107177734\n",
            "classify 2.3375244140625\n",
            "classify 2.296630859375\n",
            "classify 2.2977294921875\n",
            "classify 2.242431640625\n",
            "classify 2.3109130859375\n",
            "classify 2.2308349609375\n",
            "classify 2.3111572265625\n",
            "classify 2.3519287109375\n",
            "classify 2.3280029296875\n",
            "classify 2.2886962890625\n",
            "classify 2.3031005859375\n",
            "0.109375\n",
            "0.109375\n",
            "0.09375\n",
            "0.09375\n",
            "0.109375\n",
            "0.15625\n",
            "0.140625\n",
            "0.125\n",
            "0.140625\n",
            "0.09375\n",
            "0.09375\n",
            "time: 4.073628664016724 4.40014402071635\n",
            "3\n",
            "strain 0.6277245879173279\n",
            "strain 0.6010007262229919\n",
            "strain 0.5983944535255432\n",
            "strain 0.591130256652832\n",
            "strain 0.5705755352973938\n",
            "strain 0.5451286435127258\n",
            "classify 2.2178955078125\n",
            "classify 2.3511962890625\n",
            "classify 2.311279296875\n",
            "classify 2.3587646484375\n",
            "classify 2.27783203125\n",
            "classify 2.39599609375\n",
            "classify 2.3367919921875\n",
            "classify 2.455322265625\n",
            "classify 2.37744140625\n",
            "classify 2.32958984375\n",
            "classify 2.34375\n",
            "0.140625\n",
            "0.140625\n",
            "0.140625\n",
            "0.140625\n",
            "0.125\n",
            "0.125\n",
            "0.125\n",
            "0.109375\n",
            "0.140625\n",
            "0.078125\n",
            "0.203125\n",
            "time: 4.8478429317474365 4.512181878089905\n",
            "4\n",
            "strain 0.541098952293396\n",
            "strain 0.5630078315734863\n",
            "strain 0.5273250341415405\n",
            "strain 0.5258330702781677\n",
            "strain 0.4872693419456482\n",
            "strain 0.5042347311973572\n",
            "classify 2.400146484375\n",
            "classify 2.2967529296875\n",
            "classify 2.288330078125\n",
            "classify 2.3150634765625\n",
            "classify 2.276123046875\n",
            "classify 2.30615234375\n",
            "classify 2.3209228515625\n",
            "classify 2.3026123046875\n",
            "classify 2.302734375\n",
            "classify 2.24658203125\n",
            "classify 2.2855224609375\n",
            "0.09375\n",
            "0.15625\n",
            "0.15625\n",
            "0.09375\n",
            "0.15625\n",
            "0.09375\n",
            "0.09375\n",
            "0.0625\n",
            "0.140625\n",
            "0.078125\n",
            "0.09375\n",
            "time: 4.123581647872925 4.4345519065856935\n",
            "5\n",
            "strain 0.504020094871521\n",
            "strain 0.47533130645751953\n",
            "strain 0.4616093039512634\n",
            "strain 0.42332565784454346\n",
            "strain 0.4278174936771393\n",
            "strain 0.4525918960571289\n",
            "classify 2.3377685546875\n",
            "classify 2.306884765625\n",
            "classify 2.2086181640625\n",
            "classify 2.2646484375\n",
            "classify 2.369873046875\n",
            "classify 2.244873046875\n",
            "classify 2.301513671875\n",
            "classify 2.2791748046875\n",
            "classify 2.2520751953125\n",
            "classify 2.2227783203125\n",
            "classify 2.3302001953125\n",
            "0.078125\n",
            "0.0625\n",
            "0.125\n",
            "0.171875\n",
            "0.078125\n",
            "0.125\n",
            "0.15625\n",
            "0.140625\n",
            "0.171875\n",
            "0.140625\n",
            "0.15625\n",
            "time: 4.27277398109436 4.407670338948567\n",
            "6\n",
            "strain 0.43287476897239685\n",
            "strain 0.4291767477989197\n",
            "strain 0.4454532563686371\n",
            "strain 0.39496541023254395\n",
            "strain 0.37814465165138245\n",
            "strain 0.4134076237678528\n",
            "classify 2.3441162109375\n",
            "classify 2.3565673828125\n",
            "classify 2.247802734375\n",
            "classify 2.3328857421875\n",
            "classify 2.3026123046875\n",
            "classify 2.299072265625\n",
            "classify 2.287353515625\n",
            "classify 2.2740478515625\n",
            "classify 2.2503662109375\n",
            "classify 2.2674560546875\n",
            "classify 2.2891845703125\n",
            "0.203125\n",
            "0.140625\n",
            "0.15625\n",
            "0.109375\n",
            "0.140625\n",
            "0.09375\n",
            "0.09375\n",
            "0.21875\n",
            "0.125\n",
            "0.03125\n",
            "0.109375\n",
            "time: 4.847486257553101 4.470799309866769\n",
            "7\n",
            "strain 0.3698755204677582\n",
            "strain 0.36101943254470825\n",
            "strain 0.37841030955314636\n",
            "strain 0.3712787628173828\n",
            "strain 0.3670492172241211\n",
            "strain 0.3858121335506439\n",
            "classify 2.3565673828125\n",
            "classify 2.3150634765625\n",
            "classify 2.228271484375\n",
            "classify 2.270751953125\n",
            "classify 2.3056640625\n",
            "classify 2.3197021484375\n",
            "classify 2.3453369140625\n",
            "classify 2.2012939453125\n",
            "classify 2.2806396484375\n",
            "classify 2.3521728515625\n",
            "classify 2.301513671875\n",
            "0.125\n",
            "0.109375\n",
            "0.140625\n",
            "0.125\n",
            "0.171875\n",
            "0.09375\n",
            "0.109375\n",
            "0.140625\n",
            "0.125\n",
            "0.078125\n",
            "0.21875\n",
            "time: 4.048951625823975 4.418128818273544\n",
            "8\n",
            "strain 0.3823510706424713\n",
            "strain 0.3605724573135376\n",
            "strain 0.3769802451133728\n",
            "strain 0.34257152676582336\n",
            "strain 0.354256808757782\n",
            "strain 0.35296717286109924\n",
            "classify 2.2672119140625\n",
            "classify 2.2501220703125\n",
            "classify 2.2469482421875\n",
            "classify 2.3177490234375\n",
            "classify 2.30078125\n",
            "classify 2.29296875\n",
            "classify 2.353271484375\n",
            "classify 2.2862548828125\n",
            "classify 2.3133544921875\n",
            "classify 2.34326171875\n",
            "classify 2.2747802734375\n",
            "0.1875\n",
            "0.0625\n",
            "0.15625\n",
            "0.0625\n",
            "0.109375\n",
            "0.09375\n",
            "0.125\n",
            "0.109375\n",
            "0.125\n",
            "0.1875\n",
            "0.15625\n",
            "time: 4.173611402511597 4.391008456548055\n",
            "9\n",
            "strain 0.3400557339191437\n",
            "strain 0.346027672290802\n",
            "strain 0.3293689787387848\n",
            "strain 0.3396596908569336\n",
            "strain 0.3140229284763336\n",
            "strain 0.3232761025428772\n",
            "classify 2.3165283203125\n",
            "classify 2.3790283203125\n",
            "classify 2.2733154296875\n",
            "classify 2.313232421875\n",
            "classify 2.288818359375\n",
            "classify 2.3404541015625\n",
            "classify 2.266845703125\n",
            "classify 2.332275390625\n",
            "classify 2.266845703125\n",
            "classify 2.346923828125\n",
            "classify 2.3187255859375\n",
            "0.1875\n",
            "0.109375\n",
            "0.171875\n",
            "0.203125\n",
            "0.109375\n",
            "0.09375\n",
            "0.140625\n",
            "0.078125\n",
            "0.078125\n",
            "0.078125\n",
            "0.234375\n",
            "time: 4.61039137840271 4.413069462776184\n",
            "10\n",
            "strain 0.3671643137931824\n",
            "strain 0.3441316485404968\n",
            "strain 0.33082064986228943\n",
            "strain 0.2920983135700226\n",
            "strain 0.3176528811454773\n",
            "strain 0.3267970085144043\n",
            "classify 2.3642578125\n",
            "classify 2.3282470703125\n",
            "classify 2.3267822265625\n",
            "classify 2.23974609375\n",
            "classify 2.232177734375\n",
            "classify 2.2713623046875\n",
            "classify 2.30517578125\n",
            "classify 2.26025390625\n",
            "classify 2.2730712890625\n",
            "classify 2.31201171875\n",
            "classify 2.2796630859375\n",
            "0.171875\n",
            "0.171875\n",
            "0.0625\n",
            "0.15625\n",
            "0.046875\n",
            "0.109375\n",
            "0.171875\n",
            "0.15625\n",
            "0.09375\n",
            "0.109375\n",
            "0.109375\n",
            "time: 4.0115931034088135 4.376612554896962\n",
            "11\n",
            "strain 0.34409961104393005\n",
            "strain 0.3222423195838928\n",
            "strain 0.321561336517334\n",
            "strain 0.2589879035949707\n",
            "strain 0.3184932768344879\n",
            "strain 0.29226750135421753\n",
            "classify 2.2786865234375\n",
            "classify 2.403076171875\n",
            "classify 2.302490234375\n",
            "classify 2.3939208984375\n",
            "classify 2.3092041015625\n",
            "classify 2.28271484375\n",
            "classify 2.3260498046875\n",
            "classify 2.3082275390625\n",
            "classify 2.32861328125\n",
            "classify 2.28173828125\n",
            "classify 2.2064208984375\n",
            "0.171875\n",
            "0.1875\n",
            "0.125\n",
            "0.171875\n",
            "0.09375\n",
            "0.125\n",
            "0.15625\n",
            "0.140625\n",
            "0.078125\n",
            "0.125\n",
            "0.125\n",
            "time: 4.302806615829468 4.370501637458801\n",
            "12\n",
            "strain 0.3085249364376068\n",
            "strain 0.33404669165611267\n",
            "strain 0.2860271632671356\n",
            "strain 0.26927557587623596\n",
            "strain 0.29343730211257935\n",
            "strain 0.29028961062431335\n",
            "classify 2.3382568359375\n",
            "classify 2.18408203125\n",
            "classify 2.3201904296875\n",
            "classify 2.3590087890625\n",
            "classify 2.2564697265625\n",
            "classify 2.3028564453125\n",
            "classify 2.280517578125\n",
            "classify 2.2484130859375\n",
            "classify 2.2718505859375\n",
            "classify 2.298828125\n",
            "classify 2.305419921875\n",
            "0.171875\n",
            "0.0625\n",
            "0.109375\n",
            "0.125\n",
            "0.140625\n",
            "0.1875\n",
            "0.15625\n",
            "0.125\n",
            "0.15625\n",
            "0.09375\n",
            "0.125\n",
            "time: 4.502954721450806 4.380729730312641\n",
            "13\n",
            "strain 0.30848070979118347\n",
            "strain 0.30584943294525146\n",
            "strain 0.2860623896121979\n",
            "strain 0.29507383704185486\n",
            "strain 0.317099392414093\n",
            "strain 0.2678197920322418\n",
            "classify 2.3330078125\n",
            "classify 2.250244140625\n",
            "classify 2.300537109375\n",
            "classify 2.2894287109375\n",
            "classify 2.3162841796875\n",
            "classify 2.298095703125\n",
            "classify 2.3253173828125\n",
            "classify 2.3094482421875\n",
            "classify 2.2615966796875\n",
            "classify 2.32666015625\n",
            "classify 2.285888671875\n",
            "0.1875\n",
            "0.140625\n",
            "0.046875\n",
            "0.09375\n",
            "0.203125\n",
            "0.140625\n",
            "0.171875\n",
            "0.125\n",
            "0.078125\n",
            "0.078125\n",
            "0.171875\n",
            "time: 3.9522974491119385 4.350164754050119\n",
            "14\n",
            "strain 0.29724743962287903\n",
            "strain 0.28216004371643066\n",
            "strain 0.3125006854534149\n",
            "strain 0.2983638346195221\n",
            "strain 0.30513736605644226\n",
            "strain 0.3059779703617096\n",
            "classify 2.2510986328125\n",
            "classify 2.291748046875\n",
            "classify 2.202880859375\n",
            "classify 2.2550048828125\n",
            "classify 2.2799072265625\n",
            "classify 2.2896728515625\n",
            "classify 2.285400390625\n",
            "classify 2.26611328125\n",
            "classify 2.3106689453125\n",
            "classify 2.272705078125\n",
            "classify 2.31005859375\n",
            "0.140625\n",
            "0.140625\n",
            "0.09375\n",
            "0.09375\n",
            "0.078125\n",
            "0.15625\n",
            "0.078125\n",
            "0.140625\n",
            "0.15625\n",
            "0.09375\n",
            "0.09375\n",
            "time: 4.16013765335083 4.337529309590658\n",
            "15\n",
            "strain 0.323975145816803\n",
            "strain 0.3161783218383789\n",
            "strain 0.3093526065349579\n",
            "strain 0.313604474067688\n",
            "strain 0.300009548664093\n",
            "strain 0.3402886390686035\n",
            "classify 2.261962890625\n",
            "classify 2.28369140625\n",
            "classify 2.2529296875\n",
            "classify 2.2000732421875\n",
            "classify 2.280029296875\n",
            "classify 2.2977294921875\n",
            "classify 2.34814453125\n",
            "classify 2.3070068359375\n",
            "classify 2.332763671875\n",
            "classify 2.2952880859375\n",
            "classify 2.1826171875\n",
            "0.125\n",
            "0.09375\n",
            "0.109375\n",
            "0.125\n",
            "0.078125\n",
            "0.140625\n",
            "0.109375\n",
            "0.046875\n",
            "0.109375\n",
            "0.09375\n",
            "0.109375\n",
            "time: 4.5183610916137695 4.348907858133316\n",
            "16\n",
            "strain 0.3152207136154175\n",
            "strain 0.32169127464294434\n",
            "strain 0.2982942759990692\n",
            "strain 0.32075929641723633\n",
            "strain 0.2761802077293396\n",
            "strain 0.3241196870803833\n",
            "classify 2.3421630859375\n",
            "classify 2.2825927734375\n",
            "classify 2.279541015625\n",
            "classify 2.3558349609375\n",
            "classify 2.3358154296875\n",
            "classify 2.2869873046875\n",
            "classify 2.2628173828125\n",
            "classify 2.2652587890625\n",
            "classify 2.228515625\n",
            "classify 2.3148193359375\n",
            "classify 2.30712890625\n",
            "0.1875\n",
            "0.1875\n",
            "0.125\n",
            "0.15625\n",
            "0.1875\n",
            "0.09375\n",
            "0.15625\n",
            "0.125\n",
            "0.125\n",
            "0.109375\n",
            "0.140625\n",
            "time: 3.9409737586975098 4.324939685709336\n",
            "17\n",
            "strain 0.3040052652359009\n",
            "strain 0.299618661403656\n",
            "strain 0.2907028794288635\n",
            "strain 0.32660695910453796\n",
            "strain 0.2838084101676941\n",
            "strain 0.3324854373931885\n",
            "classify 2.35546875\n",
            "classify 2.2291259765625\n",
            "classify 2.250244140625\n",
            "classify 2.3389892578125\n",
            "classify 2.315673828125\n",
            "classify 2.276611328125\n",
            "classify 2.2872314453125\n",
            "classify 2.23095703125\n",
            "classify 2.352783203125\n",
            "classify 2.3494873046875\n",
            "classify 2.26318359375\n",
            "0.140625\n",
            "0.078125\n",
            "0.09375\n",
            "0.1875\n",
            "0.21875\n",
            "0.265625\n",
            "0.140625\n",
            "0.125\n",
            "0.171875\n",
            "0.046875\n",
            "0.140625\n",
            "time: 4.223878622055054 4.319352626800537\n",
            "18\n",
            "strain 0.2696296274662018\n",
            "strain 0.28888756036758423\n",
            "strain 0.2842889726161957\n",
            "strain 0.34043896198272705\n",
            "strain 0.293967068195343\n",
            "strain 0.26451465487480164\n",
            "classify 2.27880859375\n",
            "classify 2.26171875\n",
            "classify 2.30810546875\n",
            "classify 2.24365234375\n",
            "classify 2.288818359375\n",
            "classify 2.3367919921875\n",
            "classify 2.2615966796875\n",
            "classify 2.360595703125\n",
            "classify 2.337646484375\n",
            "classify 2.3077392578125\n",
            "classify 2.339111328125\n",
            "0.125\n",
            "0.109375\n",
            "0.125\n",
            "0.078125\n",
            "0.09375\n",
            "0.125\n",
            "0.0625\n",
            "0.140625\n",
            "0.109375\n",
            "0.078125\n",
            "0.1875\n",
            "time: 4.796299934387207 4.3444805270747135\n",
            "19\n",
            "strain 0.31833702325820923\n",
            "strain 0.28444424271583557\n",
            "strain 0.3585898280143738\n",
            "strain 0.2609427869319916\n",
            "strain 0.32908567786216736\n",
            "strain 0.31789132952690125\n",
            "classify 2.366943359375\n",
            "classify 2.2408447265625\n",
            "classify 2.219482421875\n",
            "classify 2.281494140625\n",
            "classify 2.313720703125\n",
            "classify 2.3203125\n",
            "classify 2.304443359375\n",
            "classify 2.2403564453125\n",
            "classify 2.2845458984375\n",
            "classify 2.2647705078125\n",
            "classify 2.2568359375\n",
            "0.09375\n",
            "0.09375\n",
            "0.109375\n",
            "0.078125\n",
            "0.09375\n",
            "0.109375\n",
            "0.125\n",
            "0.109375\n",
            "0.171875\n",
            "0.25\n",
            "0.140625\n",
            "time: 4.040108680725098 4.329287123680115\n",
            "20\n",
            "strain 0.28017744421958923\n",
            "strain 0.28797048330307007\n",
            "strain 0.2921038866043091\n",
            "strain 0.30174630880355835\n",
            "strain 0.2644217610359192\n",
            "strain 0.2787163257598877\n",
            "classify 2.3094482421875\n",
            "classify 2.2967529296875\n",
            "classify 2.25927734375\n",
            "classify 2.2293701171875\n",
            "classify 2.3218994140625\n",
            "classify 2.2332763671875\n",
            "classify 2.3262939453125\n",
            "classify 2.2890625\n",
            "classify 2.2769775390625\n",
            "classify 2.208251953125\n",
            "classify 2.19482421875\n",
            "0.1875\n",
            "0.140625\n",
            "0.125\n",
            "0.09375\n",
            "0.109375\n",
            "0.203125\n",
            "0.109375\n",
            "0.109375\n",
            "0.140625\n",
            "0.1875\n",
            "0.125\n",
            "time: 4.38192081451416 4.331818398975191\n",
            "21\n",
            "strain 0.2564522325992584\n",
            "strain 0.2841813564300537\n",
            "strain 0.27727794647216797\n",
            "strain 0.28362491726875305\n",
            "strain 0.3167230784893036\n",
            "strain 0.30677035450935364\n",
            "classify 2.26220703125\n",
            "classify 2.3291015625\n",
            "classify 2.2896728515625\n",
            "classify 2.275146484375\n",
            "classify 2.2744140625\n",
            "classify 2.3372802734375\n",
            "classify 2.25927734375\n",
            "classify 2.2945556640625\n",
            "classify 2.318359375\n",
            "classify 2.289306640625\n",
            "classify 2.3065185546875\n",
            "0.15625\n",
            "0.15625\n",
            "0.171875\n",
            "0.125\n",
            "0.21875\n",
            "0.109375\n",
            "0.0625\n",
            "0.171875\n",
            "0.09375\n",
            "0.125\n",
            "0.171875\n",
            "time: 4.398672580718994 4.334900238297203\n",
            "22\n",
            "strain 0.2994585633277893\n",
            "strain 0.25502434372901917\n",
            "strain 0.30919745564460754\n",
            "strain 0.27342307567596436\n",
            "strain 0.27639883756637573\n",
            "strain 0.2565959692001343\n",
            "classify 2.32177734375\n",
            "classify 2.2545166015625\n",
            "classify 2.267822265625\n",
            "classify 2.246337890625\n",
            "classify 2.2886962890625\n",
            "classify 2.2347412109375\n",
            "classify 2.3043212890625\n",
            "classify 2.241455078125\n",
            "classify 2.271728515625\n",
            "classify 2.2713623046875\n",
            "classify 2.2275390625\n",
            "0.125\n",
            "0.140625\n",
            "0.09375\n",
            "0.21875\n",
            "0.09375\n",
            "0.15625\n",
            "0.125\n",
            "0.09375\n",
            "0.078125\n",
            "0.125\n",
            "0.125\n",
            "time: 4.009692907333374 4.32078158337137\n",
            "23\n",
            "strain 0.31565532088279724\n",
            "strain 0.2700485289096832\n",
            "strain 0.26473113894462585\n",
            "strain 0.24178548157215118\n",
            "strain 0.2870911657810211\n",
            "strain 0.3778817057609558\n",
            "classify 2.278564453125\n",
            "classify 2.2449951171875\n",
            "classify 2.239013671875\n",
            "classify 2.295166015625\n",
            "classify 2.293701171875\n",
            "classify 2.271240234375\n",
            "classify 2.2078857421875\n",
            "classify 2.240234375\n",
            "classify 2.21435546875\n",
            "classify 2.305419921875\n",
            "classify 2.3343505859375\n",
            "0.09375\n",
            "0.09375\n",
            "0.0625\n",
            "0.09375\n",
            "0.21875\n",
            "0.15625\n",
            "0.1875\n",
            "0.09375\n",
            "0.171875\n",
            "0.15625\n",
            "0.125\n",
            "time: 4.426372528076172 4.32520051797231\n",
            "24\n",
            "strain 0.3378821015357971\n",
            "strain 0.27972736954689026\n",
            "strain 0.2771693468093872\n",
            "strain 0.28816378116607666\n",
            "strain 0.24683701992034912\n",
            "strain 0.3493926227092743\n",
            "classify 2.296630859375\n",
            "classify 2.2965087890625\n",
            "classify 2.272216796875\n",
            "classify 2.319091796875\n",
            "classify 2.2415771484375\n",
            "classify 2.2645263671875\n",
            "classify 2.2652587890625\n",
            "classify 2.3140869140625\n",
            "classify 2.218505859375\n",
            "classify 2.3157958984375\n",
            "classify 2.1953125\n",
            "0.0625\n",
            "0.078125\n",
            "0.203125\n",
            "0.109375\n",
            "0.09375\n",
            "0.09375\n",
            "0.15625\n",
            "0.109375\n",
            "0.0625\n",
            "0.09375\n",
            "0.09375\n",
            "time: 4.4098060131073 4.32861026763916\n",
            "25\n",
            "strain 0.27278685569763184\n",
            "strain 0.30478715896606445\n",
            "strain 0.29717928171157837\n",
            "strain 0.3088533282279968\n",
            "strain 0.24313394725322723\n",
            "strain 0.31532782316207886\n",
            "classify 2.237060546875\n",
            "classify 2.3319091796875\n",
            "classify 2.285888671875\n",
            "classify 2.1767578125\n",
            "classify 2.24658203125\n",
            "classify 2.3143310546875\n",
            "classify 2.2528076171875\n",
            "classify 2.2845458984375\n",
            "classify 2.26708984375\n",
            "classify 2.267822265625\n",
            "classify 2.298095703125\n",
            "0.140625\n",
            "0.078125\n",
            "0.109375\n",
            "0.140625\n",
            "0.15625\n",
            "0.109375\n",
            "0.0625\n",
            "0.109375\n",
            "0.078125\n",
            "0.125\n",
            "0.078125\n",
            "time: 4.093143939971924 4.31957234786107\n",
            "26\n",
            "strain 0.2798311114311218\n",
            "strain 0.3038490414619446\n",
            "strain 0.29968082904815674\n",
            "strain 0.2824549674987793\n",
            "strain 0.35805392265319824\n",
            "strain 0.2812443673610687\n",
            "classify 2.3074951171875\n",
            "classify 2.331787109375\n",
            "classify 2.294189453125\n",
            "classify 2.2548828125\n",
            "classify 2.2977294921875\n",
            "classify 2.2486572265625\n",
            "classify 2.3109130859375\n",
            "classify 2.285888671875\n",
            "classify 2.284423828125\n",
            "classify 2.2523193359375\n",
            "classify 2.27734375\n",
            "0.203125\n",
            "0.1875\n",
            "0.0625\n",
            "0.125\n",
            "0.078125\n",
            "0.078125\n",
            "0.140625\n",
            "0.21875\n",
            "0.109375\n",
            "0.1875\n",
            "0.171875\n",
            "time: 4.587353944778442 4.329507209636547\n",
            "27\n",
            "strain 0.2913588583469391\n",
            "strain 0.30053848028182983\n",
            "strain 0.27148786187171936\n",
            "strain 0.27087900042533875\n",
            "strain 0.24491973221302032\n",
            "strain 0.26861780881881714\n",
            "classify 2.272216796875\n",
            "classify 2.27587890625\n",
            "classify 2.2886962890625\n",
            "classify 2.2939453125\n",
            "classify 2.29248046875\n",
            "classify 2.27685546875\n",
            "classify 2.1885986328125\n",
            "classify 2.261474609375\n",
            "classify 2.26025390625\n",
            "classify 2.27099609375\n",
            "classify 2.31005859375\n",
            "0.15625\n",
            "0.0625\n",
            "0.125\n",
            "0.15625\n",
            "0.234375\n",
            "0.0625\n",
            "0.15625\n",
            "0.125\n",
            "0.125\n",
            "0.125\n",
            "0.109375\n",
            "time: 4.269989490509033 4.327399321964809\n",
            "28\n",
            "strain 0.2679859399795532\n",
            "strain 0.26259568333625793\n",
            "strain 0.2669593393802643\n",
            "strain 0.2870483100414276\n",
            "strain 0.3110678791999817\n",
            "strain 0.27009910345077515\n",
            "classify 2.2950439453125\n",
            "classify 2.30810546875\n",
            "classify 2.237548828125\n",
            "classify 2.3255615234375\n",
            "classify 2.2811279296875\n",
            "classify 2.3009033203125\n",
            "classify 2.3370361328125\n",
            "classify 2.33056640625\n",
            "classify 2.2303466796875\n",
            "classify 2.2462158203125\n",
            "classify 2.248291015625\n",
            "0.265625\n",
            "0.234375\n",
            "0.171875\n",
            "0.15625\n",
            "0.109375\n",
            "0.15625\n",
            "0.140625\n",
            "0.203125\n",
            "0.109375\n",
            "0.109375\n",
            "0.125\n",
            "time: 3.9624533653259277 4.314831051333197\n",
            "29\n",
            "strain 0.2799825668334961\n",
            "strain 0.27339035272598267\n",
            "strain 0.28439316153526306\n",
            "strain 0.24403493106365204\n",
            "strain 0.26992368698120117\n",
            "strain 0.2683660089969635\n",
            "classify 2.29736328125\n",
            "classify 2.2772216796875\n",
            "classify 2.2198486328125\n",
            "classify 2.28271484375\n",
            "classify 2.3076171875\n",
            "classify 2.31103515625\n",
            "classify 2.3304443359375\n",
            "classify 2.3594970703125\n",
            "classify 2.323486328125\n",
            "classify 2.3267822265625\n",
            "classify 2.2996826171875\n",
            "0.109375\n",
            "0.0625\n",
            "0.09375\n",
            "0.09375\n",
            "0.171875\n",
            "0.171875\n",
            "0.078125\n",
            "0.15625\n",
            "0.15625\n",
            "0.078125\n",
            "0.09375\n",
            "time: 4.529715538024902 4.322008816401164\n",
            "30\n",
            "strain 0.28567996621131897\n",
            "strain 0.2772175073623657\n",
            "strain 0.2879801392555237\n",
            "strain 0.28314313292503357\n",
            "strain 0.26115670800209045\n",
            "strain 0.2720845937728882\n",
            "classify 2.229248046875\n",
            "classify 2.29296875\n",
            "classify 2.2587890625\n",
            "classify 2.3056640625\n",
            "classify 2.2940673828125\n",
            "classify 2.262939453125\n",
            "classify 2.266357421875\n",
            "classify 2.19140625\n",
            "classify 2.26220703125\n",
            "classify 2.2713623046875\n",
            "classify 2.2584228515625\n",
            "0.21875\n",
            "0.21875\n",
            "0.078125\n",
            "0.09375\n",
            "0.09375\n",
            "0.234375\n",
            "0.09375\n",
            "0.171875\n",
            "0.03125\n",
            "0.0625\n",
            "0.140625\n",
            "time: 4.197319984436035 4.3179960404672935\n",
            "31\n",
            "strain 0.2571171820163727\n",
            "strain 0.2490403950214386\n",
            "strain 0.264731228351593\n",
            "strain 0.26896557211875916\n",
            "strain 0.27414676547050476\n",
            "strain 0.258498877286911\n",
            "classify 2.2425537109375\n",
            "classify 2.2904052734375\n",
            "classify 2.283203125\n",
            "classify 2.292724609375\n",
            "classify 2.293212890625\n",
            "classify 2.2928466796875\n",
            "classify 2.2508544921875\n",
            "classify 2.2415771484375\n",
            "classify 2.227294921875\n",
            "classify 2.2923583984375\n",
            "classify 2.310791015625\n",
            "0.125\n",
            "0.078125\n",
            "0.078125\n",
            "0.125\n",
            "0.0625\n",
            "0.140625\n",
            "0.171875\n",
            "0.078125\n",
            "0.203125\n",
            "0.15625\n",
            "0.078125\n",
            "time: 3.935023069381714 4.306042477488518\n",
            "32\n",
            "strain 0.22980529069900513\n",
            "strain 0.307384192943573\n",
            "strain 0.2413381189107895\n",
            "strain 0.27645355463027954\n",
            "strain 0.26439130306243896\n",
            "strain 0.2615537941455841\n",
            "classify 2.29443359375\n",
            "classify 2.2725830078125\n",
            "classify 2.286865234375\n",
            "classify 2.2884521484375\n",
            "classify 2.31640625\n",
            "classify 2.250244140625\n",
            "classify 2.2864990234375\n",
            "classify 2.287109375\n",
            "classify 2.278076171875\n",
            "classify 2.2823486328125\n",
            "classify 2.32421875\n",
            "0.109375\n",
            "0.171875\n",
            "0.109375\n",
            "0.109375\n",
            "0.15625\n",
            "0.046875\n",
            "0.0625\n",
            "0.09375\n",
            "0.15625\n",
            "0.140625\n",
            "0.140625\n",
            "time: 4.570890188217163 4.314084024140329\n",
            "33\n",
            "strain 0.21800597012043\n",
            "strain 0.2434372454881668\n",
            "strain 0.2687082886695862\n",
            "strain 0.2779872417449951\n",
            "strain 0.24064254760742188\n",
            "strain 0.30082806944847107\n",
            "classify 2.2679443359375\n",
            "classify 2.2041015625\n",
            "classify 2.3375244140625\n",
            "classify 2.292236328125\n",
            "classify 2.27734375\n",
            "classify 2.300537109375\n",
            "classify 2.2783203125\n",
            "classify 2.3116455078125\n",
            "classify 2.2969970703125\n",
            "classify 2.29052734375\n",
            "classify 2.2177734375\n",
            "0.125\n",
            "0.125\n",
            "0.1875\n",
            "0.203125\n",
            "0.140625\n",
            "0.1875\n",
            "0.125\n",
            "0.1875\n",
            "0.140625\n",
            "0.125\n",
            "0.09375\n",
            "time: 4.247362375259399 4.312160288586336\n",
            "34\n",
            "strain 0.2841078042984009\n",
            "strain 0.2597976624965668\n",
            "strain 0.23802787065505981\n",
            "strain 0.2943427264690399\n",
            "strain 0.23493963479995728\n",
            "strain 0.27007269859313965\n",
            "classify 2.313232421875\n",
            "classify 2.30517578125\n",
            "classify 2.27587890625\n",
            "classify 2.2841796875\n",
            "classify 2.2705078125\n",
            "classify 2.2388916015625\n",
            "classify 2.27294921875\n",
            "classify 2.31201171875\n",
            "classify 2.2427978515625\n",
            "classify 2.26904296875\n",
            "classify 2.2459716796875\n",
            "0.125\n",
            "0.140625\n",
            "0.09375\n",
            "0.09375\n",
            "0.109375\n",
            "0.109375\n",
            "0.171875\n",
            "0.078125\n",
            "0.171875\n",
            "0.125\n",
            "0.15625\n",
            "time: 4.019871950149536 4.3038251536233085\n",
            "35\n",
            "strain 0.27273717522621155\n",
            "strain 0.29689061641693115\n",
            "strain 0.2555943727493286\n",
            "strain 0.30280113220214844\n",
            "strain 0.2589416205883026\n",
            "strain 0.2692438066005707\n",
            "classify 2.2850341796875\n",
            "classify 2.2432861328125\n",
            "classify 2.2908935546875\n",
            "classify 2.2349853515625\n",
            "classify 2.320556640625\n",
            "classify 2.247314453125\n",
            "classify 2.257568359375\n",
            "classify 2.2677001953125\n",
            "classify 2.2850341796875\n",
            "classify 2.2593994140625\n",
            "classify 2.31103515625\n",
            "0.140625\n",
            "0.078125\n",
            "0.140625\n",
            "0.140625\n",
            "0.109375\n",
            "0.09375\n",
            "0.15625\n",
            "0.125\n",
            "0.140625\n",
            "0.1875\n",
            "0.1875\n",
            "time: 4.613863229751587 4.31244930293825\n",
            "36\n",
            "strain 0.34646329283714294\n",
            "strain 0.2942512035369873\n",
            "strain 0.25385770201683044\n",
            "strain 0.27409881353378296\n",
            "strain 0.23221096396446228\n",
            "strain 0.2395154982805252\n",
            "classify 2.3514404296875\n",
            "classify 2.3079833984375\n",
            "classify 2.263427734375\n",
            "classify 2.2474365234375\n",
            "classify 2.3314208984375\n",
            "classify 2.276123046875\n",
            "classify 2.2740478515625\n",
            "classify 2.26513671875\n",
            "classify 2.2996826171875\n",
            "classify 2.3104248046875\n",
            "classify 2.338623046875\n",
            "0.15625\n",
            "0.21875\n",
            "0.109375\n",
            "0.078125\n",
            "0.140625\n",
            "0.09375\n",
            "0.125\n",
            "0.171875\n",
            "0.09375\n",
            "0.125\n",
            "0.125\n",
            "time: 4.149493455886841 4.308078263257001\n",
            "37\n",
            "strain 0.2657112777233124\n",
            "strain 0.23154674470424652\n",
            "strain 0.25226539373397827\n",
            "strain 0.2507534623146057\n",
            "strain 0.23043441772460938\n",
            "strain 0.24169877171516418\n",
            "classify 2.2813720703125\n",
            "classify 2.296142578125\n",
            "classify 2.2589111328125\n",
            "classify 2.25732421875\n",
            "classify 2.2763671875\n",
            "classify 2.2376708984375\n",
            "classify 2.25244140625\n",
            "classify 2.27685546875\n",
            "classify 2.2813720703125\n",
            "classify 2.2603759765625\n",
            "classify 2.2581787109375\n",
            "0.140625\n",
            "0.171875\n",
            "0.140625\n",
            "0.140625\n",
            "0.109375\n",
            "0.078125\n",
            "0.078125\n",
            "0.15625\n",
            "0.125\n",
            "0.140625\n",
            "0.109375\n",
            "time: 4.068673133850098 4.301790689167223\n",
            "38\n",
            "strain 0.2690953016281128\n",
            "strain 0.2711723744869232\n",
            "strain 0.27346593141555786\n",
            "strain 0.2571272552013397\n",
            "strain 0.23248913884162903\n",
            "strain 0.24871158599853516\n",
            "classify 2.320068359375\n",
            "classify 2.2559814453125\n",
            "classify 2.2445068359375\n",
            "classify 2.33642578125\n",
            "classify 2.267333984375\n",
            "classify 2.2681884765625\n",
            "classify 2.25244140625\n",
            "classify 2.3106689453125\n",
            "classify 2.3221435546875\n",
            "classify 2.2750244140625\n",
            "classify 2.3092041015625\n",
            "0.125\n",
            "0.1875\n",
            "0.1875\n",
            "0.09375\n",
            "0.125\n",
            "0.15625\n",
            "0.09375\n",
            "0.203125\n",
            "0.1875\n",
            "0.15625\n",
            "0.171875\n",
            "time: 4.67522120475769 4.3113780816396075\n",
            "39\n",
            "strain 0.2544528543949127\n",
            "strain 0.24532683193683624\n",
            "strain 0.29835063219070435\n",
            "strain 0.2531328797340393\n",
            "strain 0.2546985447406769\n",
            "strain 0.232796773314476\n",
            "classify 2.2674560546875\n",
            "classify 2.2813720703125\n",
            "classify 2.2508544921875\n",
            "classify 2.2802734375\n",
            "classify 2.267333984375\n",
            "classify 2.2808837890625\n",
            "classify 2.2794189453125\n",
            "classify 2.241943359375\n",
            "classify 2.301513671875\n",
            "classify 2.2967529296875\n",
            "classify 2.2569580078125\n",
            "0.21875\n",
            "0.171875\n",
            "0.109375\n",
            "0.125\n",
            "0.203125\n",
            "0.09375\n",
            "0.015625\n",
            "0.09375\n",
            "0.15625\n",
            "0.09375\n",
            "0.140625\n",
            "time: 4.1309123039245605 4.3068783223629\n",
            "40\n",
            "strain 0.269351601600647\n",
            "strain 0.290550172328949\n",
            "strain 0.2558101713657379\n",
            "strain 0.3010707199573517\n",
            "strain 0.23122289776802063\n",
            "strain 0.22710227966308594\n",
            "classify 2.259521484375\n",
            "classify 2.2667236328125\n",
            "classify 2.255859375\n",
            "classify 2.2332763671875\n",
            "classify 2.2884521484375\n",
            "classify 2.2947998046875\n",
            "classify 2.182861328125\n",
            "classify 2.3192138671875\n",
            "classify 2.255615234375\n",
            "classify 2.216796875\n",
            "classify 2.26123046875\n",
            "0.125\n",
            "0.140625\n",
            "0.15625\n",
            "0.09375\n",
            "0.109375\n",
            "0.09375\n",
            "0.125\n",
            "0.109375\n",
            "0.09375\n",
            "0.171875\n",
            "0.203125\n",
            "time: 3.936014413833618 4.297840781328155\n",
            "41\n",
            "strain 0.25708645582199097\n",
            "strain 0.2397294044494629\n",
            "strain 0.24200034141540527\n",
            "strain 0.234770730137825\n",
            "strain 0.2334434986114502\n",
            "strain 0.26828324794769287\n",
            "classify 2.1776123046875\n",
            "classify 2.2838134765625\n",
            "classify 2.273193359375\n",
            "classify 2.304931640625\n",
            "classify 2.2542724609375\n",
            "classify 2.2734375\n",
            "classify 2.2686767578125\n",
            "classify 2.28759765625\n",
            "classify 2.281494140625\n",
            "classify 2.317138671875\n",
            "classify 2.226806640625\n",
            "0.203125\n",
            "0.140625\n",
            "0.171875\n",
            "0.078125\n",
            "0.15625\n",
            "0.1875\n",
            "0.109375\n",
            "0.328125\n",
            "0.21875\n",
            "0.171875\n",
            "0.1875\n",
            "time: 4.777049541473389 4.309262922831944\n",
            "42\n",
            "strain 0.2549748420715332\n",
            "strain 0.23012995719909668\n",
            "strain 0.21014362573623657\n",
            "strain 0.23613479733467102\n",
            "strain 0.24773505330085754\n",
            "strain 0.22995086014270782\n",
            "classify 2.279541015625\n",
            "classify 2.26025390625\n",
            "classify 2.2789306640625\n",
            "classify 2.241943359375\n",
            "classify 2.308837890625\n",
            "classify 2.273681640625\n",
            "classify 2.31201171875\n",
            "classify 2.2734375\n",
            "classify 2.2303466796875\n",
            "classify 2.294921875\n",
            "classify 2.3516845703125\n",
            "0.0625\n",
            "0.078125\n",
            "0.09375\n",
            "0.078125\n",
            "0.15625\n",
            "0.125\n",
            "0.140625\n",
            "0.171875\n",
            "0.109375\n",
            "0.171875\n",
            "0.09375\n",
            "time: 4.0803916454315186 4.303951784621837\n",
            "43\n",
            "strain 0.28265416622161865\n",
            "strain 0.27833297848701477\n",
            "strain 0.258431077003479\n",
            "strain 0.23404563963413239\n",
            "strain 0.25003284215927124\n",
            "strain 0.2367946356534958\n",
            "classify 2.2359619140625\n",
            "classify 2.324462890625\n",
            "classify 2.242919921875\n",
            "classify 2.2850341796875\n",
            "classify 2.32080078125\n",
            "classify 2.3004150390625\n",
            "classify 2.289794921875\n",
            "classify 2.2601318359375\n",
            "classify 2.3310546875\n",
            "classify 2.2479248046875\n",
            "classify 2.309326171875\n",
            "0.1875\n",
            "0.203125\n",
            "0.15625\n",
            "0.125\n",
            "0.109375\n",
            "0.109375\n",
            "0.171875\n",
            "0.09375\n",
            "0.109375\n",
            "0.140625\n",
            "0.09375\n",
            "time: 4.046296834945679 4.2981057437983425\n",
            "44\n",
            "strain 0.2685753107070923\n",
            "strain 0.22694964706897736\n",
            "strain 0.2370225042104721\n",
            "strain 0.2509462833404541\n",
            "strain 0.2860122621059418\n",
            "strain 0.26432275772094727\n",
            "classify 2.279052734375\n",
            "classify 2.2308349609375\n",
            "classify 2.21923828125\n",
            "classify 2.287841796875\n",
            "classify 2.2586669921875\n",
            "classify 2.2774658203125\n",
            "classify 2.2633056640625\n",
            "classify 2.2979736328125\n",
            "classify 2.2843017578125\n",
            "classify 2.268798828125\n",
            "classify 2.277099609375\n",
            "0.109375\n",
            "0.171875\n",
            "0.1875\n",
            "0.1875\n",
            "0.109375\n",
            "0.140625\n",
            "0.171875\n",
            "0.109375\n",
            "0.171875\n",
            "0.140625\n",
            "0.125\n",
            "time: 4.7074620723724365 4.307212612364027\n",
            "45\n",
            "strain 0.3000708818435669\n",
            "strain 0.23368827998638153\n",
            "strain 0.2413046658039093\n",
            "strain 0.25142204761505127\n",
            "strain 0.21644186973571777\n",
            "strain 0.25332918763160706\n",
            "classify 2.258056640625\n",
            "classify 2.2376708984375\n",
            "classify 2.2213134765625\n",
            "classify 2.2713623046875\n",
            "classify 2.2723388671875\n",
            "classify 2.30322265625\n",
            "classify 2.28125\n",
            "classify 2.240234375\n",
            "classify 2.3057861328125\n",
            "classify 2.2398681640625\n",
            "classify 2.2215576171875\n",
            "0.1875\n",
            "0.21875\n",
            "0.234375\n",
            "0.171875\n",
            "0.1875\n",
            "0.140625\n",
            "0.21875\n",
            "0.171875\n",
            "0.265625\n",
            "0.21875\n",
            "0.1875\n",
            "time: 3.995689868927002 4.300452097602513\n",
            "46\n",
            "strain 0.2682740092277527\n",
            "strain 0.25126543641090393\n",
            "strain 0.246299609541893\n",
            "strain 0.23506702482700348\n",
            "strain 0.27226686477661133\n",
            "strain 0.25040343403816223\n",
            "classify 2.34912109375\n",
            "classify 2.197509765625\n",
            "classify 2.31396484375\n",
            "classify 2.2733154296875\n",
            "classify 2.279052734375\n",
            "classify 2.2825927734375\n",
            "classify 2.2183837890625\n",
            "classify 2.2894287109375\n",
            "classify 2.24658203125\n",
            "classify 2.239013671875\n",
            "classify 2.2139892578125\n",
            "0.15625\n",
            "0.078125\n",
            "0.140625\n",
            "0.09375\n",
            "0.109375\n",
            "0.234375\n",
            "0.078125\n",
            "0.125\n",
            "0.21875\n",
            "0.15625\n",
            "0.078125\n",
            "time: 3.9716742038726807 4.293467054975793\n",
            "47\n",
            "strain 0.2263490855693817\n",
            "strain 0.26444685459136963\n",
            "strain 0.2601297199726105\n",
            "strain 0.26467564702033997\n",
            "strain 0.27572259306907654\n",
            "strain 0.22805114090442657\n",
            "classify 2.25634765625\n",
            "classify 2.260986328125\n",
            "classify 2.311279296875\n",
            "classify 2.31298828125\n",
            "classify 2.246337890625\n",
            "classify 2.2696533203125\n",
            "classify 2.2406005859375\n",
            "classify 2.2388916015625\n",
            "classify 2.268310546875\n",
            "classify 2.2777099609375\n",
            "classify 2.237060546875\n",
            "0.125\n",
            "0.1875\n",
            "0.1875\n",
            "0.09375\n",
            "0.140625\n",
            "0.203125\n",
            "0.109375\n",
            "0.109375\n",
            "0.15625\n",
            "0.15625\n",
            "0.109375\n",
            "time: 4.7096333503723145 4.302146951357524\n",
            "48\n",
            "strain 0.25413626432418823\n",
            "strain 0.2860974073410034\n",
            "strain 0.2666170001029968\n",
            "strain 0.25002795457839966\n",
            "strain 0.22337426245212555\n",
            "strain 0.24283432960510254\n",
            "classify 2.26904296875\n",
            "classify 2.23828125\n",
            "classify 2.2891845703125\n",
            "classify 2.3189697265625\n",
            "classify 2.272705078125\n",
            "classify 2.2669677734375\n",
            "classify 2.2633056640625\n",
            "classify 2.2335205078125\n",
            "classify 2.321044921875\n",
            "classify 2.2384033203125\n",
            "classify 2.2745361328125\n",
            "0.171875\n",
            "0.125\n",
            "0.140625\n",
            "0.171875\n",
            "0.171875\n",
            "0.21875\n",
            "0.125\n",
            "0.1875\n",
            "0.109375\n",
            "0.125\n",
            "0.109375\n",
            "time: 4.254590272903442 4.30118619179239\n",
            "49\n",
            "strain 0.25236576795578003\n",
            "strain 0.25782522559165955\n",
            "strain 0.23319083452224731\n",
            "strain 0.21158818900585175\n",
            "strain 0.2759034335613251\n",
            "strain 0.23776817321777344\n",
            "classify 2.3028564453125\n",
            "classify 2.2744140625\n",
            "classify 2.2257080078125\n",
            "classify 2.2564697265625\n",
            "classify 2.263427734375\n",
            "classify 2.32421875\n",
            "classify 2.2908935546875\n",
            "classify 2.22509765625\n",
            "classify 2.317626953125\n",
            "classify 2.2933349609375\n",
            "classify 2.25048828125\n",
            "0.21875\n",
            "0.140625\n",
            "0.125\n",
            "0.203125\n",
            "0.171875\n",
            "0.09375\n",
            "0.234375\n",
            "0.109375\n",
            "0.140625\n",
            "0.21875\n",
            "0.140625\n",
            "time: 4.039615631103516 4.2959634923934935\n",
            "50\n",
            "strain 0.25171807408332825\n",
            "strain 0.2419106662273407\n",
            "strain 0.27350184321403503\n",
            "strain 0.20325443148612976\n",
            "strain 0.24858912825584412\n",
            "strain 0.2595413029193878\n",
            "classify 2.2421875\n",
            "classify 2.259765625\n",
            "classify 2.2679443359375\n",
            "classify 2.2490234375\n",
            "classify 2.2568359375\n",
            "classify 2.307373046875\n",
            "classify 2.2657470703125\n",
            "classify 2.20703125\n",
            "classify 2.2802734375\n",
            "classify 2.210693359375\n",
            "classify 2.2166748046875\n",
            "0.109375\n",
            "0.140625\n",
            "0.109375\n",
            "0.15625\n",
            "0.265625\n",
            "0.125\n",
            "0.125\n",
            "0.171875\n",
            "0.109375\n",
            "0.171875\n",
            "0.234375\n",
            "time: 4.793081283569336 4.305720282535927\n",
            "51\n",
            "strain 0.23807531595230103\n",
            "strain 0.24563033878803253\n",
            "strain 0.2495461106300354\n",
            "strain 0.24037346243858337\n",
            "strain 0.2493813931941986\n",
            "strain 0.2643951177597046\n",
            "classify 2.2979736328125\n",
            "classify 2.2615966796875\n",
            "classify 2.29638671875\n",
            "classify 2.2489013671875\n",
            "classify 2.2540283203125\n",
            "classify 2.2640380859375\n",
            "classify 2.2552490234375\n",
            "classify 2.2230224609375\n",
            "classify 2.23974609375\n",
            "classify 2.29638671875\n",
            "classify 2.2601318359375\n",
            "0.171875\n",
            "0.078125\n",
            "0.125\n",
            "0.125\n",
            "0.203125\n",
            "0.15625\n",
            "0.0625\n",
            "0.1875\n",
            "0.203125\n",
            "0.078125\n",
            "0.171875\n",
            "time: 4.0685577392578125 4.301167841141041\n",
            "52\n",
            "strain 0.25210925936698914\n",
            "strain 0.28285640478134155\n",
            "strain 0.2331010103225708\n",
            "strain 0.25398245453834534\n",
            "strain 0.27762851119041443\n",
            "strain 0.22109480202198029\n",
            "classify 2.2586669921875\n",
            "classify 2.252197265625\n",
            "classify 2.24267578125\n",
            "classify 2.25146484375\n",
            "classify 2.240966796875\n",
            "classify 2.2811279296875\n",
            "classify 2.27001953125\n",
            "classify 2.2578125\n",
            "classify 2.2001953125\n",
            "classify 2.25048828125\n",
            "classify 2.292724609375\n",
            "0.140625\n",
            "0.078125\n",
            "0.1875\n",
            "0.109375\n",
            "0.203125\n",
            "0.1875\n",
            "0.0625\n",
            "0.1875\n",
            "0.15625\n",
            "0.203125\n",
            "0.140625\n",
            "time: 3.9756150245666504 4.2950336168397145\n",
            "53\n",
            "strain 0.23068992793560028\n",
            "strain 0.24113570153713226\n",
            "strain 0.23560793697834015\n",
            "strain 0.24719023704528809\n",
            "strain 0.24957895278930664\n",
            "strain 0.22867447137832642\n",
            "classify 2.290283203125\n",
            "classify 2.2589111328125\n",
            "classify 2.3065185546875\n",
            "classify 2.288330078125\n",
            "classify 2.2698974609375\n",
            "classify 2.2091064453125\n",
            "classify 2.263671875\n",
            "classify 2.322021484375\n",
            "classify 2.235595703125\n",
            "classify 2.2586669921875\n",
            "classify 2.260986328125\n",
            "0.1875\n",
            "0.1875\n",
            "0.15625\n",
            "0.21875\n",
            "0.15625\n",
            "0.078125\n",
            "0.203125\n",
            "0.203125\n",
            "0.15625\n",
            "0.171875\n",
            "0.171875\n",
            "time: 4.819819211959839 4.3047608225433915\n",
            "54\n",
            "strain 0.23255109786987305\n",
            "strain 0.22518552839756012\n",
            "strain 0.27696406841278076\n",
            "strain 0.2655743360519409\n",
            "strain 0.2519565522670746\n",
            "strain 0.28208547830581665\n",
            "classify 2.2530517578125\n",
            "classify 2.261474609375\n",
            "classify 2.258544921875\n",
            "classify 2.3363037109375\n",
            "classify 2.2603759765625\n",
            "classify 2.1939697265625\n",
            "classify 2.2137451171875\n",
            "classify 2.2984619140625\n",
            "classify 2.2396240234375\n",
            "classify 2.2607421875\n",
            "classify 2.26806640625\n",
            "0.125\n",
            "0.265625\n",
            "0.1875\n",
            "0.1875\n",
            "0.21875\n",
            "0.09375\n",
            "0.203125\n",
            "0.203125\n",
            "0.1875\n",
            "0.203125\n",
            "0.140625\n",
            "time: 3.991508722305298 4.299074151299217\n",
            "55\n",
            "strain 0.2936078608036041\n",
            "strain 0.2440597414970398\n",
            "strain 0.2689363360404968\n",
            "strain 0.25518807768821716\n",
            "strain 0.2638782262802124\n",
            "strain 0.2892618179321289\n",
            "classify 2.2100830078125\n",
            "classify 2.298095703125\n",
            "classify 2.2216796875\n",
            "classify 2.2318115234375\n",
            "classify 2.221923828125\n",
            "classify 2.2894287109375\n",
            "classify 2.17138671875\n",
            "classify 2.2435302734375\n",
            "classify 2.25634765625\n",
            "classify 2.264892578125\n",
            "classify 2.2735595703125\n",
            "0.25\n",
            "0.109375\n",
            "0.1875\n",
            "0.140625\n",
            "0.125\n",
            "0.09375\n",
            "0.1875\n",
            "0.171875\n",
            "0.234375\n",
            "0.125\n",
            "0.203125\n",
            "time: 4.031329154968262 4.294302310262408\n",
            "56\n",
            "strain 0.2858130931854248\n",
            "strain 0.21290223300457\n",
            "strain 0.22015105187892914\n",
            "strain 0.2792242169380188\n",
            "strain 0.2713290750980377\n",
            "strain 0.23301739990711212\n",
            "classify 2.215576171875\n",
            "classify 2.29345703125\n",
            "classify 2.2542724609375\n",
            "classify 2.22900390625\n",
            "classify 2.3111572265625\n",
            "classify 2.2611083984375\n",
            "classify 2.2713623046875\n",
            "classify 2.2645263671875\n",
            "classify 2.2857666015625\n",
            "classify 2.260986328125\n",
            "classify 2.2537841796875\n",
            "0.15625\n",
            "0.15625\n",
            "0.1875\n",
            "0.171875\n",
            "0.125\n",
            "0.109375\n",
            "0.140625\n",
            "0.171875\n",
            "0.203125\n",
            "0.125\n",
            "0.1875\n",
            "time: 4.792490720748901 4.303050735540557\n",
            "57\n",
            "strain 0.23509910702705383\n",
            "strain 0.21694836020469666\n",
            "strain 0.23810148239135742\n",
            "strain 0.29303789138793945\n",
            "strain 0.32548022270202637\n",
            "strain 0.22734075784683228\n",
            "classify 2.231689453125\n",
            "classify 2.2958984375\n",
            "classify 2.2974853515625\n",
            "classify 2.2755126953125\n",
            "classify 2.199462890625\n",
            "classify 2.1962890625\n",
            "classify 2.2684326171875\n",
            "classify 2.2685546875\n",
            "classify 2.2723388671875\n",
            "classify 2.193603515625\n",
            "classify 2.2950439453125\n",
            "0.203125\n",
            "0.15625\n",
            "0.171875\n",
            "0.1875\n",
            "0.09375\n",
            "0.21875\n",
            "0.09375\n",
            "0.125\n",
            "0.15625\n",
            "0.140625\n",
            "0.140625\n",
            "time: 4.051493406295776 4.298720947627364\n",
            "58\n",
            "strain 0.27544069290161133\n",
            "strain 0.26212579011917114\n"
          ]
        }
      ],
      "source": [
        "# @title strain ctrain test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "def strain(model, dataloader, optim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.train()\n",
        "    for i, (x, _) in enumerate(dataloader):\n",
        "        x = x.to(device)#.to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            loss = model.loss(x)\n",
        "\n",
        "            # repr_loss, std_loss, cov_loss = model.loss(x)\n",
        "            # j_loss, (repr_loss, std_loss, cov_loss) = model.loss(x)\n",
        "            # loss = model.sim_coeff * repr_loss + model.std_coeff * std_loss + model.cov_coeff * cov_loss\n",
        "            # loss = j_loss + loss\n",
        "\n",
        "        optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 5) # 0.5\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            m=0.999 # 0.99 m = next(momentum_scheduler)\n",
        "            norms=[]\n",
        "            for param_q, param_k in zip(model.student.parameters(), model.teacher.parameters()):\n",
        "                param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        if i%10==0: print(\"strain\",loss.item())\n",
        "        # try: wandb.log({\"loss\": loss.item(), \"repr/I\": repr_loss.item(), \"std/V\": std_loss.item(), \"cov/C\": cov_loss.item()})\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=50: break\n",
        "\n",
        "\n",
        "def ctrain(model, classifier, dataloader, coptim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.eval()\n",
        "    classifier.train()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            with torch.no_grad():\n",
        "                sx = model(x).detach()\n",
        "            y_ = classifier(sx)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "        coptim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(coptim)\n",
        "        scaler.update()\n",
        "        print(\"classify\",loss.item())\n",
        "        try: wandb.log({\"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        with torch.no_grad():\n",
        "            sx = model(x)\n",
        "            rankme = RankMe(sx).item()\n",
        "            lidar = LiDAR(sx).item()\n",
        "            y_ = classifier(sx)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        print(correct/len(y))\n",
        "        # try: wandb.log({\"correct\": correct/len(y)})\n",
        "        try: wandb.log({\"correct\": correct/len(y), \"rankme\": rankme, \"lidar\": lidar})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "import time\n",
        "start = begin = time.time()\n",
        "# for i in range(1):\n",
        "for i in range(500):\n",
        "    print(i)\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    strain(ijepa, train_loader, optim)\n",
        "    ctrain(ijepa, classifier, train_loader, coptim)\n",
        "    test(ijepa, classifier, test_loader)\n",
        "\n",
        "    # strain(violet, train_loader, voptim)\n",
        "    # ctrain(violet, classifier, train_loader, coptim)\n",
        "    # test(violet, classifier, test_loader)\n",
        "\n",
        "    print('time:',time.time() - start, (time.time()-begin)/(i+1))\n",
        "    start = time.time()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title supervised train test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "# def strain(model, dataloader, optim, scheduler=None):\n",
        "def strain(model, classifier, dataloader, optim, coptim, scheduler=None):\n",
        "    model.train()\n",
        "    classifier.train()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x = x.to(device)#.to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "        y = y.to(device)\n",
        "\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        # print(loss)\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # for p in list(filter(lambda p: p.grad is not None, model.parameters())):\n",
        "        #     print(p.grad.data.norm(2).item())\n",
        "        # print(\"max grad norm\", max([p.grad.data.norm(2).item() for p in list(filter(lambda p: p.grad is not None, model.parameters()))]))\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 10) # 0.5\n",
        "\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        print(\"strain\",loss.item())\n",
        "        # for param in ijepa.student.cls: print(param.data)\n",
        "        # for param in ijepa.predicter.cls: print(param.data)\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=50: break\n",
        "\n",
        "\n",
        "# def test(model, dataloader):\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        # .to(torch.bfloat16)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "        loss = F.cross_entropy(y_, y)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        print(correct/len(y))\n",
        "        try: wandb.log({\"correct\": correct/len(y), \"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "for i in range(1000):\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # np.random.shuffle(train_indices); np.random.shuffle(val_indices)\n",
        "    # train_sampler, valid_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(val_indices)\n",
        "    # # batch_size = 64 #512\n",
        "    # train_loader = DataLoader(train_data, sampler=train_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "    # test_loader = DataLoader(train_data, sampler=valid_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True)\n",
        "\n",
        "    # strain(ijepa, train_loader, optim)\n",
        "    strain(ijepa, classifier, train_loader, optim, coptim)\n",
        "    test(ijepa, classifier, test_loader)\n",
        "\n",
        "    # strain(violet, train_loader, voptim)\n",
        "    # test(violet, test_loader)\n"
      ],
      "metadata": {
        "id": "4J2ahp3wmqcg",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title save/load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder='/content/drive/MyDrive/jepa/'\n",
        "\n",
        "# # modelsd, optimsd = torch.load(folder+'SeqJEPA.pkl', map_location=device).values()\n",
        "# modelsd, optimsd = torch.load('SeqJEPA.pkl', map_location=device).values()\n",
        "# seq_jepa.load_state_dict(modelsd, strict=False)\n",
        "# optim.load_state_dict(optimsd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzo9DMDPcOxu",
        "outputId": "2aa950ff-7a1c-46e0-924c-c6922b2ca522"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = {'model': seq_jepa.state_dict(), 'optimizer': optim.state_dict()}\n",
        "torch.save(checkpoint, folder+'IJEPA.pkl')\n",
        "# torch.save(checkpoint, 'IJEPA.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "DNNPOuUmcSNf",
        "outputId": "9c0fdeca-f315-457a-a102-ff87f9290f75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'seq_jepa' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-06e8be6b0f78>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mseq_jepa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'IJEPA.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# torch.save(checkpoint, 'IJEPA.pkl')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'seq_jepa' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## drawer"
      ],
      "metadata": {
        "id": "aLT74ihtMnh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title RoPE2D & RotEmb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "def RoPE(dim, seq_len=512, base=10000):\n",
        "    theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "    pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "    angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "    rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [1, seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "    return rot_emb\n",
        "\n",
        "# class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "#     def __init__(self, dim, seq_len=512, base=10000):\n",
        "#         super().__init__()\n",
        "#         self.dim, self.base = dim, base\n",
        "#         theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "#         pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "#         angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "#         self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [1, seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "#     def forward(self, x): # [batch, T, dim] / [batch, T, n_heads, d_head]\n",
        "#         seq_len = x.size(1)\n",
        "#         if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.base)\n",
        "#         # print('rope fwd', x.shape, self.rot_emb.shape)\n",
        "#         if x.dim()==4: return x * self.rot_emb[:,:seq_len].unsqueeze(2)\n",
        "#         return x * self.rot_emb[:,:seq_len]\n",
        "\n",
        "\n",
        "def RoPE2D(dim=16, h=8, w=8, base=10000):\n",
        "    # theta = 1. / (base ** (torch.arange(0, dim, step=4) / dim))\n",
        "    # theta = 1. / (base**torch.linspace(0,1,dim//4)).unsqueeze(0)\n",
        "    theta = 1. / (base**torch.linspace(0,1,dim//2)).unsqueeze(0)\n",
        "    y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\") # print(y,x) # [h,w], y:row_num, x:col_num\n",
        "    y, x = (y.reshape(-1,1) * theta).unsqueeze(-1), (x.reshape(-1,1) * theta).unsqueeze(-1) # [h*w,1]*[1,dim//4] = [h*w, dim//4, 1]\n",
        "    # rot_emb = torch.cat([x.sin(), x.cos(), y.sin(), y.cos()], dim=-1).flatten(-2) # [h*w, dim//4 ,4] -> [h*w, dim]\n",
        "    # rot_emb = torch.cat([x.sin(), x.cos(), y.sin(), y.cos()], dim=-1)#.reshape(dim, h, w).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "    rot_emb = torch.cat([x.sin(), y.sin()], dim=-1).flatten(-2).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "    # rot_emb = torch.cat([x.cos(), y.cos()], dim=-1).reshape(h, w, dim).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "    return rot_emb\n",
        "\n",
        "\n",
        "\n",
        "# class RoPE2D(nn.Module): # Rotary Positional Embeddings\n",
        "#     def __init__(self, dim, h=224, w=224, base=10000):\n",
        "#         super().__init__()\n",
        "#         self.dim, self.h, self.w = dim, h, w\n",
        "#         # # theta = 1. / (base ** (torch.arange(0, dim, step=4) / dim))\n",
        "#         # theta = 1. / (base**torch.linspace(0,1,dim//4)).unsqueeze(0)\n",
        "#         theta = 1. / (base**torch.linspace(0,1,dim//2)).unsqueeze(0)\n",
        "#         y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\") # print(y,x) # [h,w], y:row_num, x:col_num\n",
        "#         y, x = (y.reshape(-1,1) * theta).unsqueeze(-1), (x.reshape(-1,1) * theta).unsqueeze(-1) # [h*w,1]*[1,dim//4] = [h*w, dim//4, 1]\n",
        "#         # # self.rot_emb = torch.cat([x.sin(), x.cos(), y.sin(), y.cos()], dim=-1).flatten(-2) # [h*w, dim//4 ,4] -> [h*w, dim]\n",
        "#         # self.rot_emb = torch.cat([x.sin(), x.cos(), y.sin(), y.cos()], dim=-1).reshape(h, w, dim).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "#         self.rot_emb = torch.cat([x.sin(), y.sin()], dim=-1).reshape(h, w, dim).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "#         # self.rot_emb = torch.cat([x.cos(), y.cos()], dim=-1).reshape(h, w, dim).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "\n",
        "#     def forward(self, img): #\n",
        "#         # batch, dim, h, w = img.shape\n",
        "#         # print(img.shape)\n",
        "#         hw = img.shape[1] # [b, hw, dim] / [b, hw, n_heads, d_head]\n",
        "#         h=w=int(hw**.5)\n",
        "#         if self.h < h or self.w < w: self.__init__(self.dim, h, w)\n",
        "#         # print(self.rot_emb.shape)\n",
        "#         # rot_emb = self.rot_emb[:, :h, :w].unsqueeze(0) # [1, h, w, dim]\n",
        "#         rot_emb = self.rot_emb[:h, :w] # [h, w, dim]\n",
        "#         # return img * rot_emb.flatten(end_dim=1).unsqueeze(0) # [b, hw, dim] * [1, hw, dim]\n",
        "#         return img * rot_emb.flatten(end_dim=1)[None,:,None,:] # [b, hw, n_heads, d_head] * [1, hw, 1, dim]\n",
        "#         # return img * self.rot_emb\n",
        "\n",
        "class RotEmb(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, top=torch.pi, base=10000):\n",
        "        super().__init__()\n",
        "        self.theta = top / (base ** (torch.arange(0, dim, step=2, device=device) / dim))\n",
        "        # self.theta = top / (base ** torch.linspace(0, 1, dim//2, device=device))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (pos.unsqueeze(-1) * self.theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [seq_len, dim]\n"
      ],
      "metadata": {
        "id": "ge36SCxOl2Oq",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# @title test multiblock params\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "dataiter = iter(train_data)\n",
        "img,y = next(dataiter)\n",
        "img = img.unsqueeze(0)\n",
        "img = F.interpolate(img, (8,8))\n",
        "b,c,h,w = img.shape\n",
        "# img = img.flatten(2).transpose(-2,-1).to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "print(img.shape)\n",
        "# batch, seq,_ = img.shape\n",
        "\n",
        "\n",
        "# # target_mask = randpatch(seq, mask_size=8, gamma=.9) # 8.9 [seq]\n",
        "# target_mask = multiblock(seq, min_s=0.2, max_s=0.3, M=4).any(0) # best.2.3M4 og.15.2M4# mask out targets to be predicted # [M, seq]\n",
        "# target_mask = multiblock(seq, min_s=0.2, max_s=0.3, M=4).any(0)\n",
        "\n",
        "target_mask = multiblock2d((8,8), M=4).any(0).unsqueeze(0)\n",
        "\n",
        "\n",
        "# context_mask = ~multiblock(seq, min_s=0.85, max_s=1., M=1)|target_mask # [1, seq], True->Mask\n",
        "context_mask = ~multiblock2d((8,8), scale=(.85,.85), aspect_ratio=(.9,1.1), M=1)|target_mask # [1, seq], True->Mask\n",
        "\n",
        "# print(target_mask, context_mask)\n",
        "\n",
        "print(target_mask.shape, context_mask.shape)\n",
        "# target_img, context_img = img*target_mask.unsqueeze(-1), img*context_mask.unsqueeze(-1)\n",
        "# target_img, context_img = img*~target_mask.unsqueeze(-1), img*~context_mask.unsqueeze(-1)\n",
        "target_img, context_img = img*~target_mask.unsqueeze(0), img*~context_mask.unsqueeze(0)\n",
        "# print(target_img.shape, context_img.shape)\n",
        "target_img, context_img = target_img.transpose(-2,-1).reshape(b,c,h,w), context_img.transpose(-2,-1).reshape(b,c,h,w)\n",
        "target_img, context_img = target_img.float(), context_img.float()\n",
        "\n",
        "# imshow(out.detach().cpu())\n",
        "imshow(target_img[0])\n",
        "imshow(context_img[0])\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "3bpiybH7M0O1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 879
        },
        "outputId": "3d5e3a1a-7e09-4033-a6ad-5e68f9844da1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 8, 8])\n",
            "torch.Size([1, 8, 8]) torch.Size([1, 8, 8])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ViT me more\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "# class LayerNorm2d(nn.LayerNorm):\n",
        "class LayerNorm2d(nn.RMSNorm):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        x = super().forward(x)\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads=None, d_head=8, cond_dim=None, dropout=0.): # .1\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_head = d_head\n",
        "        self.n_heads = d_model // d_head\n",
        "        # self.d_head = d_model // n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or d_model, 2*d_model, bias=False)\n",
        "        # self.k = nn.Sequential(nn.Dropout(dropout), nn.Linear(cond_dim, d_model, bias=False))\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        # self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        # self.lin = nn.Sequential(nn.Dropout(dropout), zero_module(nn.Linear(d_model, d_model)))\n",
        "        self.drop = nn.Dropout(dropout) # indp before q,k,v; after linout\n",
        "        self.scale = self.d_head ** -.5\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [batch, T, d_model]=[batch, h*w, c], [batch, num_tok, cond_dim], [batch,T]\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        Q = self.q(x).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        # K = self.k(x).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2)\n",
        "        K, V = self.kv(cond).unflatten(-1, (self.n_heads, 2*self.d_head)).transpose(1, 2).chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # linear attention # Softmax(Q) @ (Softmax(K).T @ V)\n",
        "        if mask != None:\n",
        "            mask = mask[:, None, :, None] # [batch,T] -> [batch,1,T,1]\n",
        "            K, V = K.masked_fill(mask, -torch.finfo(x.dtype).max), V.masked_fill(mask, -torch.finfo(x.dtype).max)\n",
        "        Q, K = Q.softmax(dim=-1)*self.scale, K.softmax(dim=-2)\n",
        "        context = K.transpose(-2,-1) @ V # [batch, n_heads, d_head, d_head]\n",
        "        out = Q @ context # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # (quadratic) attention # Softmax(Q @ K.T) @ V\n",
        "        # attn = Q @ K.transpose(-2,-1) * self.scale # [batch, n_heads, T] # [batch, n_heads, T, T/num_tok]\n",
        "        # if mask != None: attn = attn.masked_fill(mask[:, None, :, None], -torch.finfo(attn.dtype).max) # [batch,T]->[batch,1,T,1]\n",
        "        # attention = torch.softmax(attn, dim=-1)\n",
        "        # out = self.drop(attention) @ V # [batch, n_heads, T, d_head]\n",
        "\n",
        "        out = out.transpose(1, 2).flatten(2)\n",
        "        return self.drop(self.lin(out)) # [batch, T, d_model]\n",
        "\n",
        "# if self, dont pass cond_dim in init, dont pass cond in fwd\n",
        "# Softmax(Q @ K.T) @ V ~ Softmax(Q) @ Softmax(K).T @ V\n",
        "\n",
        "# https://github.com/lucidrains/x-transformers/blob/main/x_transformers/x_transformers.py#L1855\n",
        "\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_head, cond_dim=None, mult=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.cond_dim = cond_dim\n",
        "        self.self = MultiHeadAttention(d_model, d_head=d_head, dropout=0) # 16448\n",
        "        # self.self = Pooling()\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        # self.ff = nn.Sequential(\n",
        "        #     *[nn.BatchNorm2d(d_model), act, SeparableConv2d(d_model, d_model),]*3\n",
        "        #     )\n",
        "        # self.ff = ResBlock(d_model) # 74112\n",
        "        # self.ff = UIB(d_model, mult=4) # uib m4 36992, m2 18944\n",
        "        ff_dim=d_model*mult\n",
        "        self.ff = nn.Sequential(\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(dropout), nn.Linear(d_model, ff_dim), act, # ReLU GELU\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(dropout), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            nn.RMSNorm(d_model), act, nn.Dropout(dropout), nn.Linear(d_model, ff_dim),\n",
        "            nn.RMSNorm(ff_dim), act, nn.Dropout(dropout), zero_module(nn.Linear(ff_dim, d_model))\n",
        "        )\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,c,h,w], [batch, num_tok, cond_dim], [batch,T]\n",
        "        bchw = x.shape\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "        # print('attnblk fwd',x.shape)\n",
        "\n",
        "        # if self.cond_dim==None: cond=None # is self attn\n",
        "        x = x + self.drop(self.self(self.norm(x)))\n",
        "        # x = x.transpose(1,2).reshape(*bchw)\n",
        "        x = x + self.ff(x)\n",
        "        # x = self.ff(x)\n",
        "        # x = x + self.drop(self.norm2(self.ff(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "import torch\n",
        "# def apply_masks(x, masks): # [b,t,d], [M,mask_size] # https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "#     all_x = []\n",
        "#     for m in masks: # [1,T]\n",
        "#         mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [1,T,dim]\n",
        "#         all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "#     return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "def apply_masks(x, mask): # [b,t,d], [mask_size] # https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "    mask_keep = mask.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [1,T,dim]\n",
        "    # return torch.cat(torch.gather(x, dim=1, index=mask_keep), dim=0)  # [M*batch,mask_size,dim]\n",
        "    return torch.gather(x, dim=1, index=mask_keep) # [M*batch,mask_size,dim]\n",
        "\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        # self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        self.positional_emb = nn.Parameter(torch.zeros(1, 8*8, d_model)) # positional_embedding == 'learnable'\n",
        "        # self.positional_emb = nn.Parameter(torch.zeros(1, dim, 8,8))\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.zeros(1,1,d_model)) # randn zeros\n",
        "        nn.init.trunc_normal_(self.cls, std=.02)\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model)\n",
        "        self.lin = nn.Linear(d_model, out_dim)# if out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = x.shape\n",
        "        # x = x * self.pos_encoder(context_indices)\n",
        "        x = x + self.positional_emb[:,context_indices]\n",
        "        # x = apply_masks(x, [context_indices])\n",
        "\n",
        "        # pred_tokens = self.cls * self.pos_encoder(trg_indices) # [M, num_trg_toks, d_model]\n",
        "        pred_tokens = self.cls + self.positional_emb[0,trg_indices]\n",
        "        print(self.cls.shape, self.positional_emb[0,trg_indices].shape, trg_indices.shape)\n",
        "        pred_tokens = pred_tokens.repeat(batch, 1, 1) # [batch*M, num_trg_toks, d_model]\n",
        "        # print(pred_tokens.requires_grad)\n",
        "        print(\"pred fwd\", x.shape, pred_tokens.shape)\n",
        "        # x = x.repeat_interleave(trg_indices.shape[0], dim=0) # [batch, seq_len, d_model] -> [batch*M, seq_len, d_model]\n",
        "        x = torch.cat([x, pred_tokens], dim=1) # [batch*M, seq_len+num_trg_toks, d_model]\n",
        "\n",
        "        # x = x.transpose(1,2).unflatten(-1, (8,8))#.reshape(*bchw)\n",
        "        out = self.transformer_encoder(x) # float [seq_len, batch_size, d_model]\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "\n",
        "        out = self.norm(out)\n",
        "        out = out[:,seq:] # [batch*M, num_trg_toks, d_model]\n",
        "        out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Sequential(\n",
        "            # # nn.Conv1d(in_dim, d_model,7,2,7//2), nn.MaxPool1d(2,2), #nn.MaxPool1d(3, 2, 3//2),\n",
        "            # # nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.Conv1d(d_model, d_model,3,2,3//2)\n",
        "            # nn.Conv1d(in_dim, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            # nn.Conv1d(d_model, d_model,3,2,3//2), nn.BatchNorm1d(d_model), nn.ReLU(), nn.MaxPool1d(2,2),\n",
        "            # nn.Conv1d(d_model, d_model,3,2,3//2),\n",
        "            # # nn.Conv1d(in_dim, d_model,2,2,0), # like patch\n",
        "            nn.Conv2d(in_dim, d_model, kernel_size=7, stride=2, padding=7//2, bias=False), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "            )\n",
        "        # self.pos_encoder = RotEmb(d_model, top=1, base=10000)\n",
        "        self.positional_emb = nn.Parameter(torch.zeros(1, 8*8, d_model)) # positional_embedding == 'learnable'\n",
        "        # self.positional_emb = nn.Parameter(torch.zeros(1, dim, 8,8)) # positional_embedding == 'learnable'\n",
        "        self.transformer_encoder = nn.Sequential(*[AttentionBlock(d_model, d_head=d_head) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model)\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        # x = self.embed(x.transpose(-2,-1)).transpose(-2,-1) # [batch, T, d_model]\n",
        "        x = self.embed(x)\n",
        "        # bchw = x.shape\n",
        "        x = x.flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c] # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "\n",
        "        # x = self.pos_encoder(x)\n",
        "        # x = x * self.pos_encoder(torch.arange(seq, device=device)).unsqueeze(0)\n",
        "        # print(x.shape, self.positional_emb.shape)\n",
        "        x = x + self.positional_emb\n",
        "        if context_indices != None:\n",
        "            # x = apply_masks(x, [context_indices])\n",
        "            x = apply_masks(x, context_indices)\n",
        "\n",
        "\n",
        "        # x = x.transpose(1,2).reshape(*bchw)\n",
        "        x = self.transformer_encoder(x) # float [seq_len, batch_size, d_model]\n",
        "        # x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "        out = self.norm(x)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out # [seq_len, batch_size, ntoken]\n",
        "\n",
        "\n",
        "\n",
        "# pos_emb rope < learn < learned\n",
        "# conv > pixel?\n",
        "# droppath not required\n",
        "\n",
        "# norm,act,conv < conv,norm,act\n",
        "# 2*s1 < uib < resblock\n",
        "# gatedadaln 3 < 2 = 1 < ffmult4 = 2*gatedadaln\n",
        "# MaxPool2d(2,2) < MaxPool2d(3,2,3//2)\n",
        "\n",
        "\n",
        "# dim = 64\n",
        "# dim_head = 8\n",
        "# heads = dim // dim_head\n",
        "# num_classes = 10\n",
        "# # model = SimpleViT(image_size=32, patch_size=4, num_classes=num_classes, dim=dim, depth=1, heads=heads, mlp_dim=dim*4, channels = 3, dim_head = dim_head)\n",
        "# model = SimpleViT(in_dim=3, out_dim=num_classes, dim=dim, depth=1, heads=heads, mlp_dim=dim*4, channels = 3, dim_head = dim_head).to(device)\n",
        "# print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 59850\n",
        "# optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "# # print(images.shape) # [batch, 3, 32, 32]\n",
        "# logits = model(x)\n",
        "# print(logits.shape)\n",
        "# # print(logits[0])\n",
        "# # print(logits[0].argmax(1))\n",
        "# pred_probab = nn.Softmax(dim=1)(logits)\n",
        "# y_pred = pred_probab.argmax(1)\n",
        "# # print(f\"Predicted class: {y_pred}\")\n",
        "\n",
        "\n",
        "batch, seq_len, d_model = 4,1024,16\n",
        "in_dim = 3\n",
        "model = TransformerModel(in_dim, d_model, d_head=4, nlayers=2, dropout=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x = torch.rand((batch, in_dim, 32, 32), device=device)\n",
        "# x =  torch.rand((batch, seq_len, in_dim), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "# # # print(out)\n",
        "# model = TransformerPredictor(in_dim, d_model, out_dim=None, d_head=4, d_hid=None, nlayers=1).to(device)\n",
        "# out = model(out)\n",
        "# print(out.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5554e8f3-71ec-4b70-c7d7-219b59e9a1d3",
        "cellView": "form",
        "id": "C1iZQ6UNwoty"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9920\n",
            "torch.Size([4, 64, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title simplex\n",
        "!pip install opensimplex\n",
        "from opensimplex import OpenSimplex\n",
        "\n",
        "seed=0\n",
        "noise = OpenSimplex(seed)  # Replace 'seed' with your starting value\n",
        "noise_scale = 10  # Adjust for desired noise range\n",
        "\n",
        "def get_noise(x, y):\n",
        "    global seed  # Assuming seed is a global variable\n",
        "    # noise_val = noise.noise2(x / noise_scale, y / noise_scale)\n",
        "    noise_val = noise.noise2(x, y)\n",
        "    # seed += 1  # Increment seed after each call\n",
        "    return noise_val\n",
        "\n",
        "x,y=0,0\n",
        "a=[[],[],[],[],[]]\n",
        "\n",
        "fps=2\n",
        "f=[]\n",
        "for i in range(10*fps):\n",
        "    f.append(i/fps)\n",
        "    x = x+0.3\n",
        "    # y = y+1\n",
        "    # noise_value = get_noise(x, y)\n",
        "    for k,j in enumerate([0,1,2,3,4]):\n",
        "        a[k].append(get_noise(x, y+j))\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "for aa in a:\n",
        "    plt.plot(f,aa)\n",
        "plt.show()\n",
        "\n",
        "# b,y,g,"
      ],
      "metadata": {
        "id": "e3dAhWh45F4M",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title pos_embed.py\n",
        "# https://github.com/facebookresearch/mae/blob/main/util/pos_embed.py#L20\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim=16, grid_size=8, cls_token=False): #\n",
        "    grid_h = np.arange(grid_size, dtype=np.float32)\n",
        "    grid_w = np.arange(grid_size, dtype=np.float32)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token: pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed # [(1+)grid_size*grid_size, embed_dim]\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    # assert embed_dim % 2 == 0\n",
        "\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1) # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos): #\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=float)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega  # (D/2,)\n",
        "\n",
        "    pos = pos.reshape(-1)  # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
        "\n",
        "    emb_sin = np.sin(out) # (M, D/2)\n",
        "    emb_cos = np.cos(out) # (M, D/2)\n",
        "\n",
        "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def interpolate_pos_embed(model, checkpoint_model):\n",
        "    if 'pos_embed' in checkpoint_model:\n",
        "        pos_embed_checkpoint = checkpoint_model['pos_embed']\n",
        "        embedding_size = pos_embed_checkpoint.shape[-1]\n",
        "        num_patches = model.patch_embed.num_patches\n",
        "        num_extra_tokens = model.pos_embed.shape[-2] - num_patches\n",
        "        # height (== width) for the checkpoint position embedding\n",
        "        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n",
        "        # height (== width) for the new position embedding\n",
        "        new_size = int(num_patches ** 0.5)\n",
        "        # class_token and dist_token are kept unchanged\n",
        "        if orig_size != new_size:\n",
        "            print(\"Position interpolate from %dx%d to %dx%d\" % (orig_size, orig_size, new_size, new_size))\n",
        "            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n",
        "            # only the position tokens are interpolated\n",
        "            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n",
        "            pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n",
        "            pos_tokens = torch.nn.functional.interpolate(\n",
        "                pos_tokens, size=(new_size, new_size), mode='bicubic', align_corners=False)\n",
        "            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n",
        "            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n",
        "            checkpoint_model['pos_embed'] = new_pos_embed\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "vtPSM8mbnJZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ijepa src.utils.tensors\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/utils/tensors.py\n",
        "\n",
        "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
        "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n",
        "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
        "    def norm_cdf(x):\n",
        "        # Computes standard normal cumulative distribution function\n",
        "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Values are generated by using a truncated uniform distribution and\n",
        "        # then using the inverse CDF for the normal distribution.\n",
        "        # Get upper and lower cdf values\n",
        "        l = norm_cdf((a - mean) / std)\n",
        "        u = norm_cdf((b - mean) / std)\n",
        "\n",
        "        # Uniformly fill tensor with values from [l, u], then translate to\n",
        "        # [2l-1, 2u-1].\n",
        "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
        "\n",
        "        # Use inverse cdf transform for normal distribution to get truncated\n",
        "        # standard normal\n",
        "        tensor.erfinv_()\n",
        "\n",
        "        # Transform to proper mean, std\n",
        "        tensor.mul_(std * math.sqrt(2.))\n",
        "        tensor.add_(mean)\n",
        "\n",
        "        # Clamp to ensure it's in the proper range\n",
        "        tensor.clamp_(min=a, max=b)\n",
        "        return tensor\n",
        "\n",
        "\n",
        "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
        "    # type: (Tensor, float, float, float, float) -> Tensor\n",
        "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n",
        "\n",
        "def repeat_interleave_batch(x, B, repeat): # [batch*M,...]? , M?\n",
        "    N = len(x) // B\n",
        "    x = torch.cat([\n",
        "        torch.cat([x[i*B:(i+1)*B] for _ in range(repeat)], dim=0)\n",
        "        for i in range(N)\n",
        "    ], dim=0)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "dLbXQ-3XXRMq",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ijepa multiblock down\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "import math\n",
        "from multiprocessing import Value\n",
        "import torch\n",
        "_GLOBAL_SEED = 0\n",
        "\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=(224, 224),\n",
        "        patch_size=16,\n",
        "        enc_mask_scale=(0.2, 0.8),\n",
        "        pred_mask_scale=(0.2, 0.8),\n",
        "        aspect_ratio=(0.3, 3.0),\n",
        "        nenc=1,\n",
        "        npred=2,\n",
        "        min_keep=4,\n",
        "        allow_overlap=False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if not isinstance(input_size, tuple):\n",
        "            input_size = (input_size, ) * 2\n",
        "        self.patch_size = patch_size\n",
        "        self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.aspect_ratio = aspect_ratio\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "        self._itr_counter = Value('i', -1)  # collator is shared across worker processes\n",
        "\n",
        "    def step(self):\n",
        "        i = self._itr_counter\n",
        "        with i.get_lock():\n",
        "            i.value += 1\n",
        "            v = i.value\n",
        "        return v\n",
        "\n",
        "    def _sample_block_size(self, generator, scale, aspect_ratio_scale):\n",
        "        _rand = torch.rand(1, generator=generator).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.height * self.width * mask_scale)\n",
        "        # -- Sample block aspect-ratio\n",
        "        min_ar, max_ar = aspect_ratio_scale\n",
        "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
        "        while h >= self.height:\n",
        "            h -= 1\n",
        "        while w >= self.width:\n",
        "            w -= 1\n",
        "        return (h, w)\n",
        "\n",
        "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
        "        h, w = b_size\n",
        "\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # --\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            top = torch.randint(0, self.height - h, (1,))\n",
        "            left = torch.randint(0, self.width - w, (1,))\n",
        "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
        "            mask[top:top+h, left:left+w] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
        "        mask_complement[top:top+h, left:left+w] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        B = len(batch)\n",
        "\n",
        "        collated_batch = torch.utils.data.default_collate(batch)\n",
        "\n",
        "        seed = self.step()\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(seed)\n",
        "        p_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.pred_mask_scale,\n",
        "            aspect_ratio_scale=self.aspect_ratio)\n",
        "        e_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.enc_mask_scale,\n",
        "            aspect_ratio_scale=(1., 1.))\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.height * self.width\n",
        "        min_keep_enc = self.height * self.width\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                print(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "        return collated_batch, collated_masks_enc, collated_masks_pred\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "COvEnBXPYth3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aH1UOD4WW8I2",
        "outputId": "e8671cd3-eba7-4649-84ca-94c6467ce62d",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Mask generator says: \"Valid mask not found, decreasing acceptable-regions [1]\"\n",
            "WARNING:root:Mask generator says: \"Valid mask not found, decreasing acceptable-regions [2]\"\n",
            "WARNING:root:Mask generator says: \"Valid mask not found, decreasing acceptable-regions [3]\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vit fwd torch.Size([4, 3, 224, 224])\n",
            "vit fwd torch.Size([4, 196, 768])\n",
            "vit fwd torch.Size([4, 11, 768])\n",
            "predictor fwd1 torch.Size([4, 11, 384]) torch.Size([4, 196, 384])\n"
          ]
        }
      ],
      "source": [
        "# @title ijepa vision_transformer.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/models/vision_transformer.py\n",
        "import math\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# from src.utils.tensors import (trunc_normal_, repeat_interleave_batch)\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid height and width\n",
        "    return:\n",
        "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid_h = np.arange(grid_size, dtype=float)\n",
        "    grid_w = np.arange(grid_size, dtype=float)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0) # [1+h*w, dim]\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    assert embed_dim % 2 == 0\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid length\n",
        "    return:\n",
        "    pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid = np.arange(grid_size, dtype=float)\n",
        "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=float)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega   # (D/2,)\n",
        "    pos = pos.reshape(-1)   # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
        "    emb = np.concatenate([np.sin(out), np.cos(out)], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\"\"\"\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x, attn\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x, return_attention=False):\n",
        "        y, attn = self.attn(self.norm1(x))\n",
        "        if return_attention:\n",
        "            return attn\n",
        "        x = x + self.drop_path(y)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2) # ->[b,h*w,c]\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvEmbed(nn.Module):\n",
        "    \"\"\"3x3 Convolution stems for ViT following ViTC models\"\"\"\n",
        "    def __init__(self, channels, strides, img_size=224, in_chans=3, batch_norm=True):\n",
        "        super().__init__()\n",
        "        # Build the stems\n",
        "        stem = []\n",
        "        channels = [in_chans] + channels\n",
        "        for i in range(len(channels) - 2):\n",
        "            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3, stride=strides[i], padding=1, bias=(not batch_norm))]\n",
        "            if batch_norm:\n",
        "                stem += [nn.BatchNorm2d(channels[i+1])]\n",
        "            stem += [nn.ReLU(inplace=True)]\n",
        "        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n",
        "        self.stem = nn.Sequential(*stem)\n",
        "\n",
        "        # Comptute the number of patches\n",
        "        stride_prod = int(np.prod(strides))\n",
        "        self.num_patches = (img_size[0] // stride_prod)**2\n",
        "\n",
        "    def forward(self, x):\n",
        "        p = self.stem(x)\n",
        "        return p.flatten(2).transpose(1, 2)\n",
        "\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "def apply_masks(x, masks):\n",
        "    \"\"\":param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
        "    :param masks: list of tensors containing indices of patches in [N] to keep\"\"\"\n",
        "    all_x = []\n",
        "    for m in masks:\n",
        "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1)) # [batch,T,dim]\n",
        "        all_x += [torch.gather(x, dim=1, index=mask_keep)] # M * [batch,mask_size,dim]\n",
        "    return torch.cat(all_x, dim=0)  # [M*batch,mask_size,dim]\n",
        "\n",
        "\n",
        "class VisionTransformerPredictor(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, num_patches, embed_dim=768, predictor_embed_dim=384,\n",
        "        depth=6, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.init_std = init_std\n",
        "        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        # --\n",
        "        self.predictor_pos_embed = nn.Parameter(torch.zeros(1, num_patches, predictor_embed_dim), requires_grad=False)\n",
        "        predictor_pos_embed = get_2d_sincos_pos_embed(self.predictor_pos_embed.shape[-1], int(num_patches**.5), cls_token=False)\n",
        "        self.predictor_pos_embed.data.copy_(torch.from_numpy(predictor_pos_embed).float().unsqueeze(0)) # [1, num_patches, predictor_embed_dim]\n",
        "        # --\n",
        "        self.predictor_blocks = nn.ModuleList([Block(dim=predictor_embed_dim, num_heads=num_heads, drop=drop_rate, drop_path=dpr[i]) for i in range(depth)])\n",
        "        self.predictor_norm = nn.LayerNorm(predictor_embed_dim)\n",
        "        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n",
        "        # ------\n",
        "        # trunc_normal_(self.mask_token, std=self.init_std)\n",
        "\n",
        "    def forward(self, x, masks_x, masks): # [batch, num_context_patches, embed_dim], nenc*[batch, num_context_patches], npred*[batch, num_target_patches]\n",
        "        assert (masks is not None) and (masks_x is not None), 'Cannot run predictor without mask indices'\n",
        "        if not isinstance(masks_x, list): masks_x = [masks_x] # context mask\n",
        "        if not isinstance(masks, list): masks = [masks] # pred mask\n",
        "\n",
        "        # -- Batch Size\n",
        "        B = len(x) // len(masks_x)\n",
        "        # print(\"predictor fwd0\", x.shape) # [3, 121, 768])\n",
        "        # -- map from encoder-dim to pedictor-dim\n",
        "        x = self.predictor_embed(x) # [batch, num_patches, predictor_embed_dim]\n",
        "\n",
        "        # -- add positional embedding to x tokens\n",
        "        x_pos_embed = self.predictor_pos_embed.repeat(B, 1, 1) # [batch, num_patches, predictor_embed_dim]\n",
        "        print(\"predictor fwd1\", x.shape, x_pos_embed.shape) # [3, 121 or 11, 384], [3, 196, 384]\n",
        "        # print(\"predictor fwd masks_x\", masks_x)\n",
        "        x += apply_masks(x_pos_embed, masks_x) # get pos emb of context patches\n",
        "\n",
        "        # print(\"predictor fwd x\", x.shape) # [4, 11, 384])\n",
        "        _, N_ctxt, D = x.shape\n",
        "\n",
        "        # -- concat mask tokens to x\n",
        "        pos_embs = self.predictor_pos_embed.repeat(B, 1, 1) # [batch, num_patches, predictor_embed_dim]\n",
        "        pos_embs = apply_masks(pos_embs, masks) # [16, 104, predictor_embed_dim]\n",
        "        pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x)) # [16, 104, predictor_embed_dim]\n",
        "        # --\n",
        "        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1) # [16, 104, predictor_embed_dim]\n",
        "        # --\n",
        "        # print(\"predictor fwd pred_tokens\", pred_tokens.shape)\n",
        "\n",
        "        pred_tokens += pos_embs\n",
        "        x = x.repeat(len(masks), 1, 1)\n",
        "        x = torch.cat([x, pred_tokens], dim=1)\n",
        "\n",
        "        # -- fwd prop\n",
        "        for blk in self.predictor_blocks:\n",
        "            x = blk(x)\n",
        "        x = self.predictor_norm(x)\n",
        "\n",
        "        # -- return preds for mask tokens\n",
        "        x = x[:, N_ctxt:]\n",
        "        x = self.predictor_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer \"\"\"\n",
        "    def __init__(self, img_size=[224], patch_size=16, in_chans=3, embed_dim=768, predictor_embed_dim=384, depth=12, predictor_depth=12,\n",
        "        num_heads=12, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0, norm_layer=nn.LayerNorm, init_std=0.02, **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_features = self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.init_std = init_std\n",
        "        # --\n",
        "        self.patch_embed = PatchEmbed(img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        # --\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=False)\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "        # --\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "    def forward(self, x, masks=None): # [batch,3,224,224]\n",
        "        if masks is not None:\n",
        "            if not isinstance(masks, list): masks = [masks]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        x = self.patch_embed(x)\n",
        "        B, N, D = x.shape # [batch, num_patches, embed_dim]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        # -- add positional embedding to x\n",
        "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
        "        x = x + pos_embed\n",
        "\n",
        "        if masks is not None: x = apply_masks(x, masks) # [batch, num_context_patches, embed_dim]\n",
        "\n",
        "        print(\"vit fwd\", x.shape)\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            x = blk(x)\n",
        "\n",
        "        if self.norm is not None: x = self.norm(x)\n",
        "        return x # [batch, num_context_patches, embed_dim]\n",
        "\n",
        "    def interpolate_pos_encoding(self, x, pos_embed):\n",
        "        npatch = x.shape[1] - 1\n",
        "        N = pos_embed.shape[1] - 1\n",
        "        if npatch == N: return pos_embed\n",
        "        class_emb, pos_embed = pos_embed[:, 0], pos_embed[:, 1:]\n",
        "        dim = x.shape[-1]\n",
        "        pos_embed = nn.functional.interpolate(pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2), scale_factor=math.sqrt(npatch / N), mode='bicubic',)\n",
        "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n",
        "\n",
        "# from functools import partial\n",
        "# def vit_predictor(**kwargs):\n",
        "#     model = VisionTransformerPredictor(\n",
        "#         mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "#     return model\n",
        "\n",
        "def vit(patch_size=16, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, # tiny\n",
        "        # patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, # small\n",
        "        # patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, # base\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "# encoder = vit()\n",
        "encoder = VisionTransformer(\n",
        "        patch_size=16, embed_dim=768, depth=1, num_heads=3, mlp_ratio=1,\n",
        "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
        "\n",
        "# num_patches = (224/patch_size)^2\n",
        "# model = VisionTransformerPredictor(num_patches, embed_dim=768, predictor_embed_dim=384, depth=6, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02, **kwargs)\n",
        "model = VisionTransformerPredictor(num_patches=196, embed_dim=768, predictor_embed_dim=384, depth=1, num_heads=12, drop_rate=0.0, drop_path_rate=0.0, init_std=0.02)\n",
        "\n",
        "batch = 4\n",
        "img = torch.rand(batch, 3, 224, 224)\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/train.py\n",
        "mask_collator = MaskCollator(\n",
        "        input_size=(224, 224),\n",
        "        patch_size=16,\n",
        "        enc_mask_scale=(0.2, 0.8),\n",
        "        pred_mask_scale=(0.2, 0.8),\n",
        "        aspect_ratio=(0.3, 3.0),\n",
        "        nenc=1,\n",
        "        npred=4,\n",
        "        min_keep=4,\n",
        "        # allow_overlap=True)\n",
        "        allow_overlap=False)\n",
        "# self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
        "\n",
        "\n",
        "\n",
        "# v = mask_collator.step()\n",
        "# should pass the collater a list batch of idx?\n",
        "# collated_batch, collated_masks_enc, collated_masks_pred = mask_collator([batch,3,8])\n",
        "collated_batch, collated_masks_enc, collated_masks_pred = mask_collator([0,1,2,3])\n",
        "# print(v)\n",
        "\n",
        "\n",
        "def forward_target():\n",
        "    with torch.no_grad():\n",
        "        h = target_encoder(imgs)\n",
        "        h = F.layer_norm(h, (h.size(-1),))  # normalize over feature-dim\n",
        "        B = len(h)\n",
        "        # -- create targets (masked regions of h)\n",
        "        h = apply_masks(h, masks_pred)\n",
        "        h = repeat_interleave_batch(h, B, repeat=len(masks_enc))\n",
        "        return h\n",
        "\n",
        "def forward_context():\n",
        "    z = encoder(imgs, masks_enc)\n",
        "    z = predictor(z, masks_enc, masks_pred)\n",
        "    return z\n",
        "\n",
        "# # num_context_patches = 121 if allow_overlap=True else 11\n",
        "z = encoder(img, collated_masks_enc) # [batch, num_context_patches, embed_dim]\n",
        "# print(len(collated_masks_enc), len(collated_masks_pred)) # nenc, npred\n",
        "# print(collated_masks_enc[0].shape, collated_masks_pred[0].shape) # , [batch, num_context_patches = 121 or 11], [batch, num_target_patches]\n",
        "out = model(z, collated_masks_enc, collated_masks_pred)\n",
        "# # print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# print(224/16) # 14\n",
        "# print(len(collated_masks_enc), len(collated_masks_pred))\n",
        "print(collated_masks_enc, collated_masks_pred)\n",
        "# multiples of 14\n",
        "\n",
        "# print(collated_masks_pred[1])\n"
      ],
      "metadata": {
        "id": "Or3eQvUVg7l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(collated_masks_enc, collated_masks_pred)"
      ],
      "metadata": {
        "id": "u2mLjXAmXcwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ijepa multiblock.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "import math\n",
        "from multiprocessing import Value\n",
        "from logging import getLogger\n",
        "import torch\n",
        "_GLOBAL_SEED = 0\n",
        "logger = getLogger()\n",
        "\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=(224, 224),\n",
        "        patch_size=16,\n",
        "        enc_mask_scale=(0.2, 0.8),\n",
        "        pred_mask_scale=(0.2, 0.8),\n",
        "        aspect_ratio=(0.3, 3.0),\n",
        "        nenc=1,\n",
        "        npred=2,\n",
        "        min_keep=4,\n",
        "        allow_overlap=False\n",
        "    ):\n",
        "        super(MaskCollator, self).__init__()\n",
        "        if not isinstance(input_size, tuple):\n",
        "            input_size = (input_size, ) * 2\n",
        "        self.patch_size = patch_size\n",
        "        self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.aspect_ratio = aspect_ratio\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "        self._itr_counter = Value('i', -1)  # collator is shared across worker processes\n",
        "\n",
        "    def step(self):\n",
        "        i = self._itr_counter\n",
        "        with i.get_lock():\n",
        "            i.value += 1\n",
        "            v = i.value\n",
        "        return v\n",
        "\n",
        "    def _sample_block_size(self, generator, scale, aspect_ratio_scale):\n",
        "        _rand = torch.rand(1, generator=generator).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.height * self.width * mask_scale)\n",
        "        # -- Sample block aspect-ratio\n",
        "        min_ar, max_ar = aspect_ratio_scale\n",
        "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
        "        while h >= self.height:\n",
        "            h -= 1\n",
        "        while w >= self.width:\n",
        "            w -= 1\n",
        "\n",
        "        return (h, w)\n",
        "\n",
        "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
        "        h, w = b_size\n",
        "\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # --\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            top = torch.randint(0, self.height - h, (1,))\n",
        "            left = torch.randint(0, self.width - w, (1,))\n",
        "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
        "            mask[top:top+h, left:left+w] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "                    logger.warning(f'Mask generator says: \"Valid mask not found, decreasing acceptable-regions [{tries}]\"')\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
        "        mask_complement[top:top+h, left:left+w] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        B = len(batch)\n",
        "\n",
        "        collated_batch = torch.utils.data.default_collate(batch)\n",
        "\n",
        "        seed = self.step()\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(seed)\n",
        "        p_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.pred_mask_scale,\n",
        "            aspect_ratio_scale=self.aspect_ratio)\n",
        "        e_size = self._sample_block_size(\n",
        "            generator=g,\n",
        "            scale=self.enc_mask_scale,\n",
        "            aspect_ratio_scale=(1., 1.))\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.height * self.width\n",
        "        min_keep_enc = self.height * self.width\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                logger.warning(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "\n",
        "        return collated_batch, collated_masks_enc, collated_masks_pred\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XQ7PxBMlsYCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ijepa train.py\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/train.py\n",
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "#\n",
        "\n",
        "import os\n",
        "\n",
        "# -- FOR DISTRIBUTED TRAINING ENSURE ONLY 1 DEVICE VISIBLE PER PROCESS\n",
        "try:\n",
        "    # -- WARNING: IF DOING DISTRIBUTED TRAINING ON A NON-SLURM CLUSTER, MAKE\n",
        "    # --          SURE TO UPDATE THIS TO GET LOCAL-RANK ON NODE, OR ENSURE\n",
        "    # --          THAT YOUR JOBS ARE LAUNCHED WITH ONLY 1 DEVICE VISIBLE\n",
        "    # --          TO EACH PROCESS\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = os.environ['SLURM_LOCALID']\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "import copy\n",
        "import logging\n",
        "import sys\n",
        "import yaml\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.multiprocessing as mp\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "\n",
        "from src.masks.multiblock import MaskCollator as MBMaskCollator\n",
        "from src.masks.utils import apply_masks\n",
        "from src.utils.distributed import (\n",
        "    init_distributed,\n",
        "    AllReduce\n",
        ")\n",
        "from src.utils.logging import (\n",
        "    CSVLogger,\n",
        "    gpu_timer,\n",
        "    grad_logger,\n",
        "    AverageMeter)\n",
        "from src.utils.tensors import repeat_interleave_batch\n",
        "from src.datasets.imagenet1k import make_imagenet1k\n",
        "\n",
        "from src.helper import (\n",
        "    load_checkpoint,\n",
        "    init_model,\n",
        "    init_opt)\n",
        "from src.transforms import make_transforms\n",
        "\n",
        "# --\n",
        "log_timings = True\n",
        "log_freq = 10\n",
        "checkpoint_freq = 50\n",
        "# --\n",
        "\n",
        "_GLOBAL_SEED = 0\n",
        "np.random.seed(_GLOBAL_SEED)\n",
        "torch.manual_seed(_GLOBAL_SEED)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logger = logging.getLogger()\n",
        "\n",
        "\n",
        "def main(args, resume_preempt=False):\n",
        "\n",
        "    # ----------------------------------------------------------------------- #\n",
        "    #  PASSED IN PARAMS FROM CONFIG FILE\n",
        "    # ----------------------------------------------------------------------- #\n",
        "\n",
        "    # -- META\n",
        "    use_bfloat16 = args['meta']['use_bfloat16']\n",
        "    model_name = args['meta']['model_name']\n",
        "    load_model = args['meta']['load_checkpoint'] or resume_preempt\n",
        "    r_file = args['meta']['read_checkpoint']\n",
        "    copy_data = args['meta']['copy_data']\n",
        "    pred_depth = args['meta']['pred_depth']\n",
        "    pred_emb_dim = args['meta']['pred_emb_dim']\n",
        "    if not torch.cuda.is_available():\n",
        "        device = torch.device('cpu')\n",
        "    else:\n",
        "        device = torch.device('cuda:0')\n",
        "        torch.cuda.set_device(device)\n",
        "\n",
        "    # -- DATA\n",
        "    use_gaussian_blur = args['data']['use_gaussian_blur']\n",
        "    use_horizontal_flip = args['data']['use_horizontal_flip']\n",
        "    use_color_distortion = args['data']['use_color_distortion']\n",
        "    color_jitter = args['data']['color_jitter_strength']\n",
        "    # --\n",
        "    batch_size = args['data']['batch_size']\n",
        "    pin_mem = args['data']['pin_mem']\n",
        "    num_workers = args['data']['num_workers']\n",
        "    root_path = args['data']['root_path']\n",
        "    image_folder = args['data']['image_folder']\n",
        "    crop_size = args['data']['crop_size']\n",
        "    crop_scale = args['data']['crop_scale']\n",
        "    # --\n",
        "\n",
        "    # -- MASK\n",
        "    allow_overlap = args['mask']['allow_overlap']  # whether to allow overlap b/w context and target blocks\n",
        "    patch_size = args['mask']['patch_size']  # patch-size for model training\n",
        "    num_enc_masks = args['mask']['num_enc_masks']  # number of context blocks\n",
        "    min_keep = args['mask']['min_keep']  # min number of patches in context block\n",
        "    enc_mask_scale = args['mask']['enc_mask_scale']  # scale of context blocks\n",
        "    num_pred_masks = args['mask']['num_pred_masks']  # number of target blocks\n",
        "    pred_mask_scale = args['mask']['pred_mask_scale']  # scale of target blocks\n",
        "    aspect_ratio = args['mask']['aspect_ratio']  # aspect ratio of target blocks\n",
        "    # --\n",
        "\n",
        "    # -- OPTIMIZATION\n",
        "    ema = args['optimization']['ema']\n",
        "    ipe_scale = args['optimization']['ipe_scale']  # scheduler scale factor (def: 1.0)\n",
        "    wd = float(args['optimization']['weight_decay'])\n",
        "    final_wd = float(args['optimization']['final_weight_decay'])\n",
        "    num_epochs = args['optimization']['epochs']\n",
        "    warmup = args['optimization']['warmup']\n",
        "    start_lr = args['optimization']['start_lr']\n",
        "    lr = args['optimization']['lr']\n",
        "    final_lr = args['optimization']['final_lr']\n",
        "\n",
        "    # -- LOGGING\n",
        "    folder = args['logging']['folder']\n",
        "    tag = args['logging']['write_tag']\n",
        "\n",
        "    dump = os.path.join(folder, 'params-ijepa.yaml')\n",
        "    with open(dump, 'w') as f:\n",
        "        yaml.dump(args, f)\n",
        "    # ----------------------------------------------------------------------- #\n",
        "\n",
        "    try:\n",
        "        mp.set_start_method('spawn')\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # -- init torch distributed backend\n",
        "    world_size, rank = init_distributed()\n",
        "    logger.info(f'Initialized (rank/world-size) {rank}/{world_size}')\n",
        "    if rank > 0:\n",
        "        logger.setLevel(logging.ERROR)\n",
        "\n",
        "    # -- log/checkpointing paths\n",
        "    log_file = os.path.join(folder, f'{tag}_r{rank}.csv')\n",
        "    save_path = os.path.join(folder, f'{tag}' + '-ep{epoch}.pth.tar')\n",
        "    latest_path = os.path.join(folder, f'{tag}-latest.pth.tar')\n",
        "    load_path = None\n",
        "    if load_model:\n",
        "        load_path = os.path.join(folder, r_file) if r_file is not None else latest_path\n",
        "\n",
        "    # -- make csv_logger\n",
        "    csv_logger = CSVLogger(log_file,\n",
        "                           ('%d', 'epoch'),\n",
        "                           ('%d', 'itr'),\n",
        "                           ('%.5f', 'loss'),\n",
        "                           ('%.5f', 'mask-A'),\n",
        "                           ('%.5f', 'mask-B'),\n",
        "                           ('%d', 'time (ms)'))\n",
        "\n",
        "    # -- init model\n",
        "    encoder, predictor = init_model(\n",
        "        device=device,\n",
        "        patch_size=patch_size,\n",
        "        crop_size=crop_size,\n",
        "        pred_depth=pred_depth,\n",
        "        pred_emb_dim=pred_emb_dim,\n",
        "        model_name=model_name)\n",
        "    target_encoder = copy.deepcopy(encoder)\n",
        "\n",
        "    # -- make data transforms\n",
        "    mask_collator = MBMaskCollator(\n",
        "        input_size=crop_size,\n",
        "        patch_size=patch_size,\n",
        "        pred_mask_scale=pred_mask_scale,\n",
        "        enc_mask_scale=enc_mask_scale,\n",
        "        aspect_ratio=aspect_ratio,\n",
        "        nenc=num_enc_masks,\n",
        "        npred=num_pred_masks,\n",
        "        allow_overlap=allow_overlap,\n",
        "        min_keep=min_keep)\n",
        "\n",
        "    transform = make_transforms(\n",
        "        crop_size=crop_size,\n",
        "        crop_scale=crop_scale,\n",
        "        gaussian_blur=use_gaussian_blur,\n",
        "        horizontal_flip=use_horizontal_flip,\n",
        "        color_distortion=use_color_distortion,\n",
        "        color_jitter=color_jitter)\n",
        "\n",
        "    # -- init data-loaders/samplers\n",
        "    _, unsupervised_loader, unsupervised_sampler = make_imagenet1k(\n",
        "            transform=transform,\n",
        "            batch_size=batch_size,\n",
        "            collator=mask_collator,\n",
        "            pin_mem=pin_mem,\n",
        "            training=True,\n",
        "            num_workers=num_workers,\n",
        "            world_size=world_size,\n",
        "            rank=rank,\n",
        "            root_path=root_path,\n",
        "            image_folder=image_folder,\n",
        "            copy_data=copy_data,\n",
        "            drop_last=True)\n",
        "    ipe = len(unsupervised_loader)\n",
        "\n",
        "    # -- init optimizer and scheduler\n",
        "    optimizer, scaler, scheduler, wd_scheduler = init_opt(\n",
        "        encoder=encoder,\n",
        "        predictor=predictor,\n",
        "        wd=wd,\n",
        "        final_wd=final_wd,\n",
        "        start_lr=start_lr,\n",
        "        ref_lr=lr,\n",
        "        final_lr=final_lr,\n",
        "        iterations_per_epoch=ipe,\n",
        "        warmup=warmup,\n",
        "        num_epochs=num_epochs,\n",
        "        ipe_scale=ipe_scale,\n",
        "        use_bfloat16=use_bfloat16)\n",
        "    encoder = DistributedDataParallel(encoder, static_graph=True)\n",
        "    predictor = DistributedDataParallel(predictor, static_graph=True)\n",
        "    target_encoder = DistributedDataParallel(target_encoder)\n",
        "    for p in target_encoder.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    # -- momentum schedule\n",
        "    momentum_scheduler = (ema[0] + i*(ema[1]-ema[0])/(ipe*num_epochs*ipe_scale)\n",
        "                          for i in range(int(ipe*num_epochs*ipe_scale)+1))\n",
        "\n",
        "    start_epoch = 0\n",
        "    # -- load training checkpoint\n",
        "    if load_model:\n",
        "        encoder, predictor, target_encoder, optimizer, scaler, start_epoch = load_checkpoint(\n",
        "            device=device,\n",
        "            r_path=load_path,\n",
        "            encoder=encoder,\n",
        "            predictor=predictor,\n",
        "            target_encoder=target_encoder,\n",
        "            opt=optimizer,\n",
        "            scaler=scaler)\n",
        "        for _ in range(start_epoch*ipe):\n",
        "            scheduler.step()\n",
        "            wd_scheduler.step()\n",
        "            next(momentum_scheduler)\n",
        "            mask_collator.step()\n",
        "\n",
        "    def save_checkpoint(epoch):\n",
        "        save_dict = {\n",
        "            'encoder': encoder.state_dict(),\n",
        "            'predictor': predictor.state_dict(),\n",
        "            'target_encoder': target_encoder.state_dict(),\n",
        "            'opt': optimizer.state_dict(),\n",
        "            'scaler': None if scaler is None else scaler.state_dict(),\n",
        "            'epoch': epoch,\n",
        "            'loss': loss_meter.avg,\n",
        "            'batch_size': batch_size,\n",
        "            'world_size': world_size,\n",
        "            'lr': lr\n",
        "        }\n",
        "        if rank == 0:\n",
        "            torch.save(save_dict, latest_path)\n",
        "            if (epoch + 1) % checkpoint_freq == 0:\n",
        "                torch.save(save_dict, save_path.format(epoch=f'{epoch + 1}'))\n",
        "\n",
        "    # -- TRAINING LOOP\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        logger.info('Epoch %d' % (epoch + 1))\n",
        "\n",
        "        # -- update distributed-data-loader epoch\n",
        "        unsupervised_sampler.set_epoch(epoch)\n",
        "\n",
        "        loss_meter = AverageMeter()\n",
        "        maskA_meter = AverageMeter()\n",
        "        maskB_meter = AverageMeter()\n",
        "        time_meter = AverageMeter()\n",
        "\n",
        "        for itr, (udata, masks_enc, masks_pred) in enumerate(unsupervised_loader):\n",
        "\n",
        "            def load_imgs():\n",
        "                # -- unsupervised imgs\n",
        "                imgs = udata[0].to(device, non_blocking=True)\n",
        "                masks_1 = [u.to(device, non_blocking=True) for u in masks_enc]\n",
        "                masks_2 = [u.to(device, non_blocking=True) for u in masks_pred]\n",
        "                return (imgs, masks_1, masks_2)\n",
        "            imgs, masks_enc, masks_pred = load_imgs()\n",
        "            maskA_meter.update(len(masks_enc[0][0]))\n",
        "            maskB_meter.update(len(masks_pred[0][0]))\n",
        "\n",
        "            def train_step():\n",
        "                _new_lr = scheduler.step()\n",
        "                _new_wd = wd_scheduler.step()\n",
        "                # --\n",
        "\n",
        "                def forward_target():\n",
        "                    with torch.no_grad():\n",
        "                        h = target_encoder(imgs)\n",
        "                        h = F.layer_norm(h, (h.size(-1),))  # normalize over feature-dim\n",
        "                        B = len(h)\n",
        "                        # -- create targets (masked regions of h)\n",
        "                        h = apply_masks(h, masks_pred)\n",
        "                        h = repeat_interleave_batch(h, B, repeat=len(masks_enc))\n",
        "                        return h\n",
        "\n",
        "                def forward_context():\n",
        "                    z = encoder(imgs, masks_enc)\n",
        "                    z = predictor(z, masks_enc, masks_pred)\n",
        "                    return z\n",
        "\n",
        "                def loss_fn(z, h):\n",
        "                    loss = F.smooth_l1_loss(z, h)\n",
        "                    loss = AllReduce.apply(loss)\n",
        "                    return loss\n",
        "\n",
        "                # Step 1. Forward\n",
        "                with torch.cuda.amp.autocast(dtype=torch.bfloat16, enabled=use_bfloat16):\n",
        "                    h = forward_target()\n",
        "                    z = forward_context()\n",
        "                    loss = loss_fn(z, h)\n",
        "\n",
        "                #  Step 2. Backward & step\n",
        "                if use_bfloat16:\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                else:\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                grad_stats = grad_logger(encoder.named_parameters())\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Step 3. momentum update of target encoder\n",
        "                with torch.no_grad():\n",
        "                    m = next(momentum_scheduler)\n",
        "                    for param_q, param_k in zip(encoder.parameters(), target_encoder.parameters()):\n",
        "                        param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "\n",
        "                return (float(loss), _new_lr, _new_wd, grad_stats)\n",
        "            (loss, _new_lr, _new_wd, grad_stats), etime = gpu_timer(train_step)\n",
        "            loss_meter.update(loss)\n",
        "            time_meter.update(etime)\n",
        "\n",
        "            # -- Logging\n",
        "            def log_stats():\n",
        "                csv_logger.log(epoch + 1, itr, loss, maskA_meter.val, maskB_meter.val, etime)\n",
        "                if (itr % log_freq == 0) or np.isnan(loss) or np.isinf(loss):\n",
        "                    logger.info('[%d, %5d] loss: %.3f '\n",
        "                                'masks: %.1f %.1f '\n",
        "                                '[wd: %.2e] [lr: %.2e] '\n",
        "                                '[mem: %.2e] '\n",
        "                                '(%.1f ms)'\n",
        "                                % (epoch + 1, itr,\n",
        "                                   loss_meter.avg,\n",
        "                                   maskA_meter.avg,\n",
        "                                   maskB_meter.avg,\n",
        "                                   _new_wd,\n",
        "                                   _new_lr,\n",
        "                                   torch.cuda.max_memory_allocated() / 1024.**2,\n",
        "                                   time_meter.avg))\n",
        "\n",
        "                    if grad_stats is not None:\n",
        "                        logger.info('[%d, %5d] grad_stats: [%.2e %.2e] (%.2e, %.2e)'\n",
        "                                    % (epoch + 1, itr,\n",
        "                                       grad_stats.first_layer,\n",
        "                                       grad_stats.last_layer,\n",
        "                                       grad_stats.min,\n",
        "                                       grad_stats.max))\n",
        "\n",
        "            log_stats()\n",
        "\n",
        "            assert not np.isnan(loss), 'loss is nan'\n",
        "\n",
        "        # -- Save Checkpoint after every epoch\n",
        "        logger.info('avg. loss %.3f' % loss_meter.avg)\n",
        "        save_checkpoint(epoch+1)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "iVUPGP93dpAN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}