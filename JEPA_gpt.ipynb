{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMR2BrDsDh5/n+KfpzVJn+/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/JEPA_gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### setup"
      ],
      "metadata": {
        "id": "4w5m_JAyK1wW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://openreview.net/pdf?id=BZ5a1r-kVsf\n",
        "# import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim\n",
        "import collections\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "!pip install optuna\n",
        "import optuna\n"
      ],
      "metadata": {
        "id": "btQjTpnUd95O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c406a033-75b5-48ba-e27c-86704c2f026b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optuna\n",
            "  Downloading optuna-2.10.1-py3-none-any.whl (308 kB)\n",
            "\u001b[K     |████████████████████████████████| 308 kB 27.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (3.13)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-6.6.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.64.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.39)\n",
            "Collecting alembic\n",
            "  Downloading alembic-1.8.1-py3-none-any.whl (209 kB)\n",
            "\u001b[K     |████████████████████████████████| 209 kB 62.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.3)\n",
            "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.7.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.21.6)\n",
            "Collecting cliff\n",
            "  Downloading cliff-3.10.1-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 11.5 MB/s \n",
            "\u001b[?25hCollecting cmaes>=0.8.2\n",
            "  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (3.0.9)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (1.1.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (4.12.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (5.9.0)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.1-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 9.2 MB/s \n",
            "\u001b[?25hCollecting stevedore>=2.0.1\n",
            "  Downloading stevedore-3.5.0-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 8.4 MB/s \n",
            "\u001b[?25hCollecting cmd2>=1.0.0\n",
            "  Downloading cmd2-2.4.2-py3-none-any.whl (147 kB)\n",
            "\u001b[K     |████████████████████████████████| 147 kB 66.6 MB/s \n",
            "\u001b[?25hCollecting autopage>=0.4.0\n",
            "  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.3.0)\n",
            "Collecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.9.0-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 35.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (4.1.1)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (22.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna) (3.8.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna) (2.0.1)\n",
            "Building wheels for collected packages: pyperclip\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=ff97763653417815ede39f5e1dcd89f7e65fdf15010e49992b3672b4127706e4\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n",
            "Successfully built pyperclip\n",
            "Installing collected packages: pyperclip, pbr, stevedore, Mako, cmd2, autopage, colorlog, cmaes, cliff, alembic, optuna\n",
            "Successfully installed Mako-1.2.1 alembic-1.8.1 autopage-0.5.1 cliff-3.10.1 cmaes-0.8.2 cmd2-2.4.2 colorlog-6.6.0 optuna-2.10.1 pbr-5.9.0 pyperclip-1.8.2 stevedore-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### dataset"
      ],
      "metadata": {
        "id": "03KL_vt18kl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchdata\n",
        "\n",
        "# https://github.com/Sam-Armstrong/tinyGPT/blob/main/Training.py\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torchtext.datasets import WikiText2\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class WikiDataset(Dataset):\n",
        "    \"\"\"Dataset functionality from https://github.com/karpathy/minGPT\"\"\"\n",
        "    def __init__(self, data, block_size):\n",
        "        chars = sorted(list(set(data)))\n",
        "        vocab_size = len(chars)\n",
        "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
        "        self.block_size = block_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.data = data\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        chunk = self.data[idx:idx + self.block_size + 1]\n",
        "        dix = [self.stoi[s] for s in chunk]\n",
        "        x = torch.tensor(dix[:-1], dtype = torch.long)\n",
        "        y = torch.tensor(dix[1:], dtype = torch.long)\n",
        "        return x, y\n",
        "\n",
        "\n",
        "seq_len = 128\n",
        "# Code for using a text corpus contained within a .txt file\n",
        "# data = open('input.txt', 'r').read()\n",
        "# data = list(data)\n",
        "\n",
        "text_iter = WikiText2(split = 'train')\n",
        "train_data = []\n",
        "for i, sequence in enumerate(text_iter):\n",
        "    train_data += list(sequence)\n",
        "# print(len(train_data)) # 10780437\n",
        "\n",
        "# train_dataset = WikiDataset(train_data[:10000], seq_len) # one line of poem is roughly 50 characters\n",
        "train_dataset = WikiDataset(train_data, seq_len) # one line of poem is roughly 50 characters\n",
        "text_iter = WikiText2(split = 'test')\n",
        "test_data = []\n",
        "for i, sequence in enumerate(text_iter):\n",
        "    test_data += list(sequence)\n",
        "# print(len(data)) # 10780437\n",
        "# test_dataset = WikiDataset(test_data[:1000], seq_len) # one line of poem is roughly 50 characters\n",
        "test_dataset = WikiDataset(test_data, seq_len) # one line of poem is roughly 50 characters\n",
        "\n",
        "vocab_size = train_dataset.vocab_size\n",
        "# print(\"vocab_size\",vocab_size) #283\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6sPjtK08Lzk",
        "outputId": "4cb1da18-193c-4016-e6b0-eee0d6145458"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchdata\n",
            "  Downloading torchdata-0.4.0-cp37-cp37m-manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 10.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch==1.12.0 in /usr/local/lib/python3.7/dist-packages (from torchdata) (1.12.0+cu113)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchdata) (2.23.0)\n",
            "Collecting urllib3>=1.25\n",
            "  Downloading urllib3-1.26.11-py2.py3-none-any.whl (139 kB)\n",
            "\u001b[K     |████████████████████████████████| 139 kB 66.6 MB/s \n",
            "\u001b[?25hCollecting portalocker>=2.0.0\n",
            "  Downloading portalocker-2.5.1-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.12.0->torchdata) (4.1.1)\n",
            "Collecting urllib3>=1.25\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 67.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (2.10)\n",
            "Installing collected packages: urllib3, portalocker, torchdata\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed portalocker-2.5.1 torchdata-0.4.0 urllib3-1.25.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### functions"
      ],
      "metadata": {
        "id": "BLW1TAw6K7s5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def off_diagonal(x):\n",
        "    # print(\"off_diagonal\",x.shape)\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n"
      ],
      "metadata": {
        "id": "QHmNYpcUK95w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### jepa"
      ],
      "metadata": {
        "id": "75SRBabzK-6P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_Xjga__gdeY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "outputId": "18bd9632-9091-4a55-c853-ed481cd596f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-08-03 05:39:18,944]\u001b[0m Trial 0 failed because of the following error: RuntimeError('Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument tensors in method wrapper_cat)')\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/optuna/study/_optimize.py\", line 213, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "  File \"<ipython-input-25-4aee1e424150>\", line 97, in objective\n",
            "    sxz = torch.cat([sx, z], dim=-1)\n",
            "RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument tensors in method wrapper_cat)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-4aee1e424150>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;31m# print(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m \u001b[0minv_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minv_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-4aee1e424150>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0msy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# print(\"sx, sy\",sx.shape, sy.shape) #10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# z=np.array(z)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-4aee1e424150>\u001b[0m in \u001b[0;36margm\u001b[0;34m(self, SX, SY)\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0mmseloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msy_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmseloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m             \u001b[0mst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;31m# print(\"st\",st['z'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgc_after_trial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m             \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         )\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mreseed_sampler_rng\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mtime_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0mprogress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             )\n\u001b[1;32m     78\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTrialState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFAIL\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc_err\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-4aee1e424150>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0;31m# print(\"z trail\",sx,z)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0mz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                 \u001b[0msxz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m                 \u001b[0msy_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msxz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0mmseloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msy_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument tensors in method wrapper_cat)"
          ]
        }
      ],
      "source": [
        "# self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "# self.position_embedding = nn.Parameter(torch.zeros(1, seq_len, embed_dim))\n",
        "\n",
        "# embedding = self.embedding(idx)\n",
        "# position_embedding = self.position_embedding[:, :seq_len, :].repeat(batch_size, 1, 1)\n",
        "# # Concats token and position and embeddings then projects them onto the embedding dimension\n",
        "# x = torch.concat((embedding, position_embedding), dim = -1)\n",
        "\n",
        "\n",
        "class JEPA(nn.Module):\n",
        "    # def __init__(self, xin_channels, dim_sx, dim_sy, dim_z, dim_v, n_actions, space_dims, hidden_dims):\n",
        "    def __init__(self, vocab_size, embed_size, in_dimx, in_dimy, dim_sx, dim_sy, dim_z, dim_v):\n",
        "        super(JEPA, self).__init__()\n",
        "        self.enc_x = nn.Sequential(\n",
        "            nn.Embedding(vocab_size, embed_size),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            nn.Linear(in_dimx*embed_size, dim_sx),\n",
        "            )\n",
        "        self.enc_y = nn.Sequential(\n",
        "            nn.Embedding(vocab_size, embed_size),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            nn.Linear(in_dimx*embed_size, dim_sx),\n",
        "            )\n",
        "        self.pred = nn.Sequential(\n",
        "            nn.Linear(dim_sx + dim_z, dim_sy),\n",
        "            nn.ReLU(True),\n",
        "            )\n",
        "        self.exp_x = nn.Sequential(\n",
        "            nn.Linear(dim_sx, dim_v),\n",
        "            nn.ReLU(True),\n",
        "            )\n",
        "        self.exp_y = nn.Sequential(\n",
        "            nn.Linear(dim_sy, dim_v),\n",
        "            nn.ReLU(True),\n",
        "            )    \n",
        "\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y)\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001)\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        # # covariance loss\n",
        "        # cov_x = (x.T @ x) / (self.args.batch_size - 1)\n",
        "        # cov_y = (y.T @ y) / (self.args.batch_size - 1)\n",
        "        # cov_loss = off_diagonal(cov_x).pow_(2).sum().div(self.num_features)\\\n",
        "        #  + off_diagonal(cov_y).pow_(2).sum().div(self.num_features)\n",
        "\n",
        "        # loss = (self.args.sim_coeff * repr_loss + self.args.std_coeff * std_loss + self.args.cov_coeff * cov_loss)\n",
        "\n",
        "        batch_size=3\n",
        "        num_features=3\n",
        "        sim_coeff=1\n",
        "        std_coeff=1\n",
        "        cov_coeff=1\n",
        "\n",
        "        # print(\"x.dim()\",x.dim())\n",
        "        if x.dim() == 1:\n",
        "            x = x.view(-1, 1)\n",
        "        \n",
        "        if y.dim() == 1:\n",
        "            y = y.view(-1, 1)\n",
        "        x=x.T\n",
        "        y=y.T\n",
        "        # print(\"x\",x.shape)\n",
        "        cov_x = (x.T @ x) / (batch_size - 1)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        # print(\"cov_x\",cov_x.shape)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features)\n",
        "\n",
        "        loss = (sim_coeff * repr_loss + std_coeff * std_loss + cov_coeff * cov_loss)\n",
        "        return loss\n",
        "\n",
        "    # def argm(self, sx, sy):\n",
        "    def argm(self, SX, SY):\n",
        "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "        sampler = optuna.samplers.NSGAIISampler()\n",
        "        # sampler = optuna.samplers.MOTPESampler()\n",
        "        pruner = optuna.pruners.MedianPruner()\n",
        "        batch_size=1\n",
        "        # if sx.dim() == 2: batch_size,_=sx.shape\n",
        "        if SX.dim() == 2: batch_size,_=SX.shape\n",
        "        s=[]\n",
        "        for i in range(batch_size):\n",
        "            sx, sy = SX[i],SY[i]\n",
        "            study = optuna.create_study(direction=\"minimize\", sampler=sampler, pruner=pruner)\n",
        "            # study = optuna.create_study()\n",
        "            def objective(trial):\n",
        "                z = trial.suggest_uniform('z', -1, 1)\n",
        "                # print(\"z trail\",sx,z)\n",
        "                # z=torch.tensor([z]).to(device)\n",
        "                z=z.view(1,-1).to(device)\n",
        "                sxz = torch.cat([sx, z], dim=-1)\n",
        "                sy_ = self.pred(sxz)\n",
        "                mseloss = nn.MSELoss()(sy, sy_)\n",
        "                return mseloss\n",
        "            study.optimize(objective, n_trials=10)\n",
        "            st=study.best_params\n",
        "            # print(\"st\",st['z'])\n",
        "            st=torch.tensor([st['z']])\n",
        "            s.append(st)\n",
        "        return torch.tensor(s)\n",
        "\n",
        "    def loss(self, x, y):\n",
        "        if x.dim()==2: batch_size,_=x.shape\n",
        "        # print(\"loss\",x,x.shape,x.dtype)\n",
        "        sx = self.enc_x(x)\n",
        "        sy = self.enc_y(y)\n",
        "        sx=sx.flatten(start_dim=1)\n",
        "        sy=sy.flatten(start_dim=1)\n",
        "        # print(\"sx, sy\",sx.shape, sy.shape) #10000\n",
        "        z = self.argm(sx, sy).to(device)\n",
        "        # z=np.array(z)\n",
        "        z=torch.tensor(z).view(-1,1)\n",
        "        # print(\"sx, z\",sx.device, z.device) #[500, 20] [1]\n",
        "        sxz = torch.cat([sx, z], dim=-1)\n",
        "        sy_ = self.pred(sxz)\n",
        "        # loss(sy, sy_)\n",
        "        mseloss = nn.MSELoss()(sy, sy_)\n",
        "\n",
        "        vx = self.exp_x(sx)\n",
        "        vy = self.exp_y(sy)\n",
        "        # print(\"vx\",vx.shape) #[40]\n",
        "        vicloss = self.vicreg(vx, vy)\n",
        "        return mseloss + vicloss\n",
        "\n",
        "    def forward(self, sx, a):\n",
        "        # sx = self.enc_x(x)\n",
        "        # sx=sx.flatten()\n",
        "        sxz = torch.cat([sx, a], dim=-1)\n",
        "        sy_ = self.pred(sxz)\n",
        "        return sy_\n",
        "\n",
        "in_dimx=128 # embedding size\n",
        "in_dimy=128\n",
        "dim_sx=20\n",
        "dim_sy=20\n",
        "dim_z=1\n",
        "dim_v=40\n",
        "# vocab_size=283\n",
        "embed_size=4\n",
        "\n",
        "# model = JEPA(in_dimx, in_dimy, dim_sx, dim_sy, dim_z, dim_v)\n",
        "model = JEPA(vocab_size, embed_size, in_dimx, in_dimy, dim_sx, dim_sy, dim_z, dim_v)\n",
        "\n",
        "\n",
        "# x=torch.rand(3, 210, 160)\n",
        "# print(\"x\",x.dtype) #\n",
        "\n",
        "# y=torch.rand(3, 210, 160)\n",
        "# x=torch.rand(in_dimx)\n",
        "# y=torch.rand(in_dimy)\n",
        "x=torch.randint(0,vocab_size,(500, 128), dtype = torch.long)\n",
        "y=torch.randint(0,vocab_size,(500, 128), dtype = torch.long)\n",
        "# print(x)\n",
        "inv_loss = model.loss(x,y)\n",
        "print(inv_loss)\n",
        "\n",
        "\n",
        "# # # lin=nn.Linear(dim_sx + dim_z, dim_sy)\n",
        "# # lin=nn.Embedding(283, dim_sx)\n",
        "# lin=nn.Embedding(vocab_size, embed_size)\n",
        "# now=nn.Linear(in_dimx*embed_size, dim_sx)\n",
        "# # now=nn.Linear((in_dimx+dim_z)*embed_size, dim_sx)\n",
        "\n",
        "# x=torch.randint(0,128,(500, 21), dtype = torch.long)\n",
        "# print(\"x\",x.dtype) #torch.float64\n",
        "# print(\"x\",x.shape,x.dtype) #\n",
        "# x=lin(x)\n",
        "# print(\"x\",x.shape,x.dtype) #\n",
        "# x=x.flatten(start_dim=1)\n",
        "# print(\"x\",x.shape,x.dtype) #\n",
        "# x=now(x)\n",
        "# print(\"x\",x.shape,x.dtype) #\n",
        "\n",
        "# x=torch.rand(500, 21)\n",
        "# print(\"x\",x.shape) #\n",
        "# lop=nn.Linear(dim_sx + dim_z, dim_sy)\n",
        "# x=lop(x)\n",
        "# print(\"x\",x.shape) #\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# enc_x = nn.Sequential( # embed pi (240, 256, 3) -> 256 when flattened\n",
        "#             nn.Conv2d(xin_channels, 8, 3, stride=2, padding=1), nn.ELU(),\n",
        "#             # nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "#             nn.Conv2d(8, 16, 5, stride=2, padding=2), nn.ELU(),\n",
        "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "#             nn.Conv2d(16, 8, 7, stride=2, padding=3), nn.ELU(),\n",
        "#             nn.Conv2d(8, 1, 5, stride=2, padding=2), nn.ELU(),\n",
        "#             nn.AdaptiveAvgPool2d((5,4)),\n",
        "#             # # nn.Conv2d(in_channels, out_channels=1, kernel_size=3, stride=2, padding=1),\n",
        "#             # nn.ReLU(),\n",
        "#             )\n",
        "# x=torch.rand(3, 210, 160)\n",
        "# sx=enc_x(x)\n",
        "# print(\"sx.shape\",sx.shape) # [1, 256] [1, 5, 4]\n",
        "# sx=sx.flatten()\n",
        "# print(\"sx.shape\",sx.shape) # [20]\n",
        "\n",
        "\n",
        "# pred = nn.Sequential(\n",
        "#             nn.Linear(dim_sx + dim_z, dim_sy),\n",
        "#             nn.ReLU(True),\n",
        "#             )\n",
        "\n",
        "# # sx =torch.rand(1, 16, 16)\n",
        "# z =torch.rand(1)\n",
        "# z=torch.tensor(z)\n",
        "# sxz = torch.cat([sx, z], dim=-1)\n",
        "# print(sxz.shape) #257\n",
        "# sy_ = pred(sxz)\n",
        "\n",
        "\n",
        "grad_clip_norm=1\n",
        "lr=1e-4\n",
        "betas=(0.9, 0.95)\n",
        "batch_size=500\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### data train eval"
      ],
      "metadata": {
        "id": "oEr6soVkqbQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train(loader, model, loss_fn, optimizer):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    pbar = tqdm(enumerate(loader), total = len(loader))\n",
        "    for it, (x, y) in pbar:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        # print(\"x,y\",x.shape,y.shape)\n",
        "        # print(\"x,y\",x.dtype,y.dtype) #torch.int64\n",
        "        # print(\"x\",x)\n",
        "        with torch.set_grad_enabled(True):\n",
        "            # logits = model(x)\n",
        "            # # loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
        "            # loss = loss_fn(logits, y)\n",
        "            loss = model.loss(x,y)\n",
        "            losses.append(loss.item())\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)\n",
        "        optimizer.step()\n",
        "        # lr = lr\n",
        "        # pbar.set_description(f\"epoch {epoch + 1} iter {it}: train loss {loss.item():.5f}. lr {lr:e}\")\n",
        "        pbar.set_description(f\"epoch {epoch + 1} iter {it}: train loss {loss.item():.5f}\")\n",
        "\n",
        "\n",
        "def eval(loader, model, loss_fn):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    pbar = enumerate(loader)\n",
        "    for it, (x, y) in pbar:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        with torch.set_grad_enabled(False):\n",
        "            # logits = model(x)\n",
        "            # # loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
        "            # loss = loss_fn(logits, y)\n",
        "            loss = model.loss(x,y)\n",
        "            losses.append(loss.item())\n",
        "    test_loss = float(np.mean(losses))\n",
        "    logger.info(\"test loss: %f\", test_loss)\n",
        "    return test_loss\n"
      ],
      "metadata": {
        "id": "V_EUHNNBgxuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### wwwwwwwwwww"
      ],
      "metadata": {
        "id": "nsKk_OKSqfTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# in_dimx=128 # embedding size\n",
        "# in_dimy=128\n",
        "# dim_sx=20\n",
        "# dim_sy=20\n",
        "# dim_z=1\n",
        "# dim_v=40\n",
        "# # vocab_size=283\n",
        "# embed_size=4\n",
        "# model = JEPA(in_dimx, in_dimy, dim_sx, dim_sy, dim_z, dim_v)\n",
        "model = JEPA(vocab_size, embed_size, in_dimx, in_dimy, dim_sx, dim_sy, dim_z, dim_v).to(device)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr = lr, betas = betas)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def loss_fn(logits, y):\n",
        "    return criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
        "\n"
      ],
      "metadata": {
        "id": "Yu4L88zkjA79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 4)\n",
        "test_loader = DataLoader(test_dataset, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 0)\n",
        "\n",
        "epochs=1\n",
        "for epoch in range(epochs):\n",
        "    # run_epoch(train_loader)\n",
        "    train(train_loader, model, loss_fn, optimizer)\n",
        "    if test is not None:\n",
        "        test_loss = eval(test_loader, model, loss_fn)\n",
        "        print('Test Loss:', test_loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 991
        },
        "id": "WNxuQST8jGeu",
        "outputId": "a4c44128-5c3a-4a7b-a50e-01b234eb796c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "  0%|          | 0/21561 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "epoch 1 iter 0: train loss 5834.47021:   0%|          | 1/21561 [00:03<19:42:35,  3.29s/it]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "epoch 1 iter 1: train loss 5660.86377:   0%|          | 2/21561 [00:06<19:01:07,  3.18s/it]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "epoch 1 iter 2: train loss 5580.06934:   0%|          | 3/21561 [00:09<18:40:48,  3.12s/it]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "epoch 1 iter 3: train loss 5313.14404:   0%|          | 4/21561 [00:12<18:30:28,  3.09s/it]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "epoch 1 iter 4: train loss 5194.05908:   0%|          | 5/21561 [00:15<18:25:53,  3.08s/it]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "epoch 1 iter 5: train loss 5029.72314:   0%|          | 6/21561 [00:18<18:24:45,  3.08s/it]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "epoch 1 iter 6: train loss 4435.02930:   0%|          | 7/21561 [00:21<18:22:28,  3.07s/it]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "epoch 1 iter 7: train loss 4605.11523:   0%|          | 8/21561 [00:24<18:41:08,  3.12s/it]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "epoch 1 iter 8: train loss 4425.74658:   0%|          | 9/21561 [00:27<18:35:39,  3.11s/it]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "epoch 1 iter 9: train loss 4297.64160:   0%|          | 10/21561 [00:31<18:29:57,  3.09s/it]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "epoch 1 iter 10: train loss 4282.19141:   0%|          | 11/21561 [00:34<18:27:19,  3.08s/it]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "epoch 1 iter 11: train loss 4343.83789:   0%|          | 12/21561 [00:37<18:27:13,  3.08s/it]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "epoch 1 iter 12: train loss 4235.39551:   0%|          | 13/21561 [00:40<18:26:42,  3.08s/it]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "epoch 1 iter 13: train loss 4247.17920:   0%|          | 14/21561 [00:43<18:26:17,  3.08s/it]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "epoch 1 iter 14: train loss 3871.43750:   0%|          | 15/21561 [00:46<18:26:27,  3.08s/it]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "epoch 1 iter 15: train loss 3557.42603:   0%|          | 16/21561 [00:49<18:25:12,  3.08s/it]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "epoch 1 iter 16: train loss 3389.95776:   0%|          | 17/21561 [00:52<18:25:09,  3.08s/it]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "epoch 1 iter 17: train loss 3421.51489:   0%|          | 18/21561 [00:55<18:27:27,  3.08s/it]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "epoch 1 iter 18: train loss 3570.78809:   0%|          | 19/21561 [00:58<18:52:49,  3.16s/it]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "epoch 1 iter 19: train loss 3292.06445:   0%|          | 20/21561 [01:02<19:11:57,  3.21s/it]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "epoch 1 iter 20: train loss 3252.52881:   0%|          | 21/21561 [01:07<23:36:10,  3.94s/it]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "epoch 1 iter 21: train loss 2984.16455:   0%|          | 22/21561 [01:11<23:13:05,  3.88s/it]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "epoch 1 iter 22: train loss 3125.85303:   0%|          | 23/21561 [01:14<21:47:37,  3.64s/it]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "epoch 1 iter 23: train loss 2822.14380:   0%|          | 24/21561 [01:18<22:18:17,  3.73s/it]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "epoch 1 iter 24: train loss 2858.36499:   0%|          | 25/21561 [01:21<21:26:29,  3.58s/it]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "epoch 1 iter 25: train loss 2808.32568:   0%|          | 26/21561 [01:25<20:45:01,  3.47s/it]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "epoch 1 iter 26: train loss 2662.70728:   0%|          | 27/21561 [01:28<20:22:53,  3.41s/it]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "epoch 1 iter 27: train loss 2712.75708:   0%|          | 28/21561 [01:31<19:53:02,  3.32s/it]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "epoch 1 iter 28: train loss 2592.90015:   0%|          | 29/21561 [01:34<19:32:10,  3.27s/it]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "epoch 1 iter 29: train loss 2476.69312:   0%|          | 30/21561 [01:37<19:33:55,  3.27s/it]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "epoch 1 iter 30: train loss 2186.90161:   0%|          | 31/21561 [01:42<19:48:27,  3.31s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-8841f117017d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# run_epoch(train_loader)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-fc3eb936df04>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(loader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;31m# # loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;31m# loss = loss_fn(logits, y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-4aee1e424150>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0msy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# print(\"sx, sy\",sx.shape, sy.shape) #10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# z=np.array(z)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-4aee1e424150>\u001b[0m in \u001b[0;36margm\u001b[0;34m(self, SX, SY)\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0mmseloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msy_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmseloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m             \u001b[0mst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;31m# print(\"st\",st['z'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgc_after_trial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m             \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         )\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mreseed_sampler_rng\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mtime_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0mprogress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             )\n\u001b[1;32m     78\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprogress_bar\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m             \u001b[0mprogress_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_seconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optuna/progress_bar.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, elapsed_seconds)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0moptuna_logging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_library_root_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_tqdm_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melapsed_seconds\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \"\"\"Update the progress bars if ``is_valid`` is :obj:`True`.\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### inference"
      ],
      "metadata": {
        "id": "2gLKjKZej5L_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def inference(model, x, max_steps = 512, seq_len=seq_len):\n",
        "    # seq_len = model.seq_len\n",
        "    # seq_len = seq_len\n",
        "    model.eval()\n",
        "    print(\"test\",x)\n",
        "    for n in range(max_steps):\n",
        "        if x.shape[1] <= seq_len:\n",
        "            x_bar = x\n",
        "        else:\n",
        "            x_bar = x[:, -seq_len:]\n",
        "        # print(\"test\",x_bar)\n",
        "        # output = model(x_bar)\n",
        "        a=torch.zeros(1,1)\n",
        "        print(x_bar.shape, a.shape)\n",
        "        output = model(x_bar, a)\n",
        "        # print(\"output\",output)\n",
        "        output = output[:, -1, :]\n",
        "        output = F.softmax(output, dim = -1)\n",
        "        ix = torch.multinomial(output, num_samples = 1)\n",
        "        x = torch.cat((x, ix), dim = 1)\n",
        "    return x\n",
        "\n",
        "context = \"This is what \"\n",
        "#context = 'There are many things about horses that have been discovered in recent'\n",
        "# print([train_dataset.stoi[s] for s in context])\n",
        "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype = torch.long)[None,...].to(device)\n",
        "y = inference(model, x)[0]\n",
        "completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
        "print(completion)\n",
        "\n"
      ],
      "metadata": {
        "id": "TZTuCdvLjprk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 512\n",
        "\n",
        "Embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "position_embedding = torch.zeros(1, seq_len, embed_dim)\n",
        "lin=nn.Linear(embed_dim * 2, embed_dim)\n",
        "\n",
        "\n",
        "context = \"This is what \"\n",
        "#context = 'There are many things about horses that have been discovered in recent'\n",
        "# print([train_dataset.stoi[s] for s in context])\n",
        "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype = torch.long)[None,...].to(device)\n",
        "max_steps = 20#512\n",
        "x=x.cpu()\n",
        "# model.eval()\n",
        "print(\"test\",x)\n",
        "for n in range(max_steps):\n",
        "    if x.shape[1] <= seq_len:\n",
        "        x_bar = x\n",
        "    else:\n",
        "        x_bar = x[:, -seq_len:]\n",
        "    # print(\"test\",x_bar)\n",
        "    # output = model(x_bar)\n",
        "    a=torch.zeros(1,1)\n",
        "    print(x_bar.shape, a.shape)\n",
        "    # output = mod(x_bar)\n",
        "\n",
        "    idx=x_bar\n",
        "    batch_size = idx.shape[0] # 1\n",
        "    seq_len = idx.shape[1] # len(contex)+\n",
        "    print(\"Embedding\",Embedding)\n",
        "    embedding = Embedding(idx) # [1, len(contex)+, embed_size]\n",
        "    position_embedding = position_embedding[:, :seq_len, :].repeat(batch_size, 1, 1)\n",
        "    # Concats token and position and embeddings then projects them onto the embedding dimension\n",
        "    x = torch.concat((embedding, position_embedding), dim = -1)\n",
        "    print(\"wp.s\",embedding.shape, position_embedding.shape)\n",
        "    # output=x[0]\n",
        "    x=lin(x)\n",
        "    print(x.shape)\n",
        "    output=x\n",
        "    # # output = mod(x_bar, a)\n",
        "    # print(\"test\",x_bar.shape) # [1, len(contex)+] int to 277+\n",
        "    # output = model(x_bar)\n",
        "    # print(\"output\",output.shape) # [1, len(contex)+, vocab_size=283] float\n",
        "    output = output[:, -1, :] #get logit for last character\n",
        "    output = F.softmax(output, dim = -1) #vocab_size to char\n",
        "    ix = torch.multinomial(output, num_samples = 1)\n",
        "    x = torch.cat((x, ix), dim = 1)\n",
        "\n",
        "# y = inference(model, x)[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "D7JjUZlo8Gst",
        "outputId": "1f455408-614f-45c1-8738-37fe987cbffc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test tensor([[53, 72, 73, 83,  1, 73, 83,  1, 87, 72, 65, 84,  1]])\n",
            "torch.Size([1, 13]) torch.Size([1, 1])\n",
            "Embedding Embedding(283, 512)\n",
            "wp.s torch.Size([1, 13, 512]) torch.Size([1, 13, 512])\n",
            "torch.Size([1, 13, 512])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-a58ec7e1be30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#vocab_size to char\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# y = inference(model, x)[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 3 and 2"
          ]
        }
      ]
    }
  ]
}