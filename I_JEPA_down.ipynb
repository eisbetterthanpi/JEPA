{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/I_JEPA_down.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "LxACli7GdyGq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceedae52-09f6-435e-a5f6-c5be9d1e354f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170M/170M [05:05<00:00, 557kB/s] \n"
          ]
        }
      ],
      "source": [
        "# @title data\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "transform = transforms.Compose([transforms.ToTensor(),])\n",
        "# transform = transforms.Compose([transforms.RandomResizedCrop((32,32), scale=(.3,1.)), transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "# CIFAR10: 60000 32x32 color images in 10 classes, with 6000 images per class\n",
        "train_data = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
        "test_data = torchvision.datasets.CIFAR10(root='data', train=False, download=True, transform=transform)\n",
        "batch_size = 64 # 4\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# dataiter = iter(train_loader) # get some random training images\n",
        "# images, labels = next(dataiter)\n",
        "# print(images.shape) # [batch, 3, 32, 32]\n",
        "# imshow(torchvision.utils.make_grid(images))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3XSZZyUPY1i"
      },
      "source": [
        "## vit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "GD7ezZzmhTHU"
      },
      "outputs": [],
      "source": [
        "# @title RoPE\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def RoPE(dim, seq_len=512, top=torch.pi, base=1000):\n",
        "    theta = top / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "    pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "    angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim//2] -> [1, seq_len, dim//2, 1]\n",
        "    rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [1, seq_len, dim//2, 2] -> [1, seq_len, dim]\n",
        "    return rot_emb\n",
        "\n",
        "\n",
        "# class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "#     def __init__(self, dim, seq_len=512, top=torch.pi, base=1000):\n",
        "#         super().__init__()\n",
        "#         self.dim, self.base = dim, base\n",
        "#         theta = top / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "#         pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "#         angles = (pos * theta)[None,None,...,None] # [seq_len, 1] * [dim//2] -> [1, 1, seq_len, dim//2, 1]\n",
        "#         self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [1, 1, seq_len, dim//2, 2] -> [1, 1, seq_len, dim]\n",
        "\n",
        "#     def forward(self, x): # [b,h,t,d]\n",
        "#         seq_len = x.size(-2)\n",
        "#         if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.base)\n",
        "#         return x * self.rot_emb#[:,:,:seq_len]\n",
        "\n",
        "\n",
        "# @title proper rope\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class RoPE(nn.Module):\n",
        "    def __init__(self, dim, seq_len=512, top=torch.pi, base=1000):\n",
        "    # def __init__(self, dim, seq_len=512, min_freq=1, max_freq=400, n_zero_freqs=0):\n",
        "        super().__init__()\n",
        "        speed = top / (base ** (torch.arange(0, dim, step=2) / dim)) # [dim//2]\n",
        "        pos = torch.arange(seq_len).unsqueeze(-1) # [t,1]\n",
        "        # speed = torch.cat([torch.zeros(n_zero_freqs), min_freq * (max_freq/min_freq) ** torch.linspace(0,1,dim//2-n_zero_freqs)]) # [dim//2]\n",
        "        # pos = torch.linspace(0, 1, seq_len).unsqueeze(-1) # [t,1]\n",
        "\n",
        "        theta = (speed*pos) # [t,1]*[dim//2]=[t,d//2]\n",
        "        self.theta = theta\n",
        "        cos, sin = torch.cos(theta), torch.sin(theta)\n",
        "        self.affine = torch.stack([cos, -sin, sin, cos], dim=-1).unflatten(-1,(2,2)).to(device) # [t,d//2,4]-> [1,1,t,d//2,2,2]\n",
        "\n",
        "    def forward(self, x): # [b,h,t,d]\n",
        "        return (self.affine @ x.unflatten(-1, (-1,2)).unsqueeze(-1)).flatten(-3) # [1,1,t,d//2,2,2] @ [b,h,t,d//2,2,1] = [b,h,t,d]\n",
        "\n",
        "# dim=64\n",
        "# n_heads=4\n",
        "# seq_len=64\n",
        "# rope = RoPE(dim, seq_len, top=torch.pi, base=100)\n",
        "# # rope = RoPE(dim, seq_len, min_freq=1, max_freq=200, n_zero_freqs=0)\n",
        "\n",
        "# x = torch.rand(2, n_heads, seq_len, dim, device=device)\n",
        "# out = rope(x)\n",
        "# print(out.shape)\n",
        "\n",
        "# theta = rope.theta # [t,d//2]\n",
        "# sim = torch.cos(theta-theta[0].unsqueeze(0)).T\n",
        "# # print(sim.shape)\n",
        "\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# def imshow(img):\n",
        "#     npimg = img.numpy()\n",
        "#     print(npimg.shape)\n",
        "#     plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "#     plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "#     plt.show()\n",
        "\n",
        "# import torchvision\n",
        "# imshow(torchvision.utils.make_grid(sim, nrow=dim//2))\n",
        "\n",
        "\n",
        "\n",
        "class RotEmb(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, top=torch.pi, base=1000):\n",
        "        super().__init__()\n",
        "        self.theta = top / (base ** (torch.arange(0, dim, step=2, device=device) / dim))\n",
        "        # self.theta = top / (base ** torch.linspace(0, 1, dim//2, device=device))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (pos.unsqueeze(-1) * self.theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [seq_len, dim]\n",
        "\n",
        "# emb = RotEmb(dim, top=torch.pi, base=1000)\n",
        "\n",
        "# theta = emb.theta\n",
        "# print(theta)\n",
        "# pos = torch.arange(0,200,1)\n",
        "# angles = (pos.unsqueeze(-1) * theta).T # [b,t]\n",
        "# sim = torch.cos(angles-angles[:,0].unsqueeze(-1))\n",
        "# print(sim.shape)\n",
        "\n",
        "# imshow(torchvision.utils.make_grid(sim, nrow=dim//2))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "f6T4F651kmGh"
      },
      "outputs": [],
      "source": [
        "# @title AttentionBlock\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class SelfAttn(nn.Module):\n",
        "    def __init__(self, dim, n_heads):\n",
        "        super().__init__()\n",
        "        self.dim, self.n_heads = dim, n_heads\n",
        "        d_head = dim//n_heads\n",
        "        self.rope = RoPE(d_head, seq_len=64, top=torch.pi, base=1000)\n",
        "        # self.rope = RoPE(d_head, seq_len=512, base=1000)\n",
        "        # self.rope = RoPE2D(d_head, h=64, w=64, base=100)\n",
        "        # self.rope[0] = 1 # id for smry h\n",
        "        self.qkv = nn.Linear(dim, dim*3, bias=False)\n",
        "        # self.lin = nn.Linear(dim, dim)\n",
        "        self.lin = zero_module(nn.Linear(dim, dim))\n",
        "\n",
        "        self.scale = d_head**-.5\n",
        "\n",
        "    # def forward(self, x): # [b,t,d]\n",
        "    def forward(self, x, pos=None): # [b,t,d]\n",
        "        q,k,v = self.qkv(x).unflatten(-1, (self.n_heads,-1)).transpose(1,2).chunk(3, dim=-1) # [b, t, n_heads, d_head] -> [b, n_heads, t, d_head]\n",
        "        # q, k = self.rope(q), self.rope(k)\n",
        "        if pos != None:\n",
        "            # rope = self.rope[pos]\n",
        "            # q, k = q*rope, k*rope\n",
        "            q, k = self.rope(q, pos), self.rope(k, pos)\n",
        "\n",
        "        # x = F.scaled_dot_product_attention(q,k,v, attn_mask=None) # https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html\n",
        "        q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2)\n",
        "        context = k.transpose(-2,-1) @ v # [batch, n_heads, d_head, d_head]\n",
        "        x = q @ context # [b, n_heads, t, d_head]\n",
        "        # # print('SelfAttn', x.shape)\n",
        "        x = x.transpose(1,2).flatten(2)\n",
        "        return self.lin(x)\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, mult=4, drop=0.):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        # self.norm = nn.RMSNorm(d_model, elementwise_affine=False) # LayerNorm RMSNorm\n",
        "        self.norm1, self.norm2 = nn.LayerNorm(d_model), nn.LayerNorm(d_model)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "        self.attn = SelfAttn(d_model, n_heads)\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        ff_dim=d_model*mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), act, nn.Dropout(drop), nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "            nn.RMSNorm(ff_dim), act, nn.Dropout(drop), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(drop), nn.Linear(d_model, ff_dim), act,\n",
        "            # nn.RMSNorm(ff_dim), nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "\n",
        "    # def forward(self, x): # [b,t,d]\n",
        "    def forward(self, x, pos=None): # [b,t,d]\n",
        "        # print('attnblk fwd',x.shape)\n",
        "        # x = x + self.drop(self.attn(self.norm1(x)))\n",
        "        x = x + self.drop(self.attn(self.norm1(x), pos))\n",
        "        x = x + self.ff(x)\n",
        "        # x = x + self.drop(self.ff(self.norm2(x)))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gdtmiB_Zzn5u"
      },
      "outputs": [],
      "source": [
        "# @title ssd me\n",
        "# https://goombalab.github.io/blog/2024/mamba2-part4-systems/\n",
        "# https://tridao.me/blog/2024/mamba2-part3-algorithm/\n",
        "# # https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/triton/ssd_combined.py\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def segsum(x): # [...,c] # Naive segment sum calculation. exp(segsum(A)) produces a 1-SS matrix, which is equivalent to a scalar SSM.\n",
        "    T = x.size(-1)\n",
        "    x_cumsum = torch.cumsum(x, dim=-1)\n",
        "    x_segsum = x_cumsum.unsqueeze(-1) - x_cumsum.unsqueeze(-2) # [...,c,c] # vert-hori\n",
        "    mask = torch.triu(torch.ones(T, T, device=x.device, dtype=bool), diagonal=1)\n",
        "    return x_segsum.masked_fill(mask, -torch.inf) # [...,c,c]\n",
        "\n",
        "# def ssd(X, A, B, C, h0=None, b_ind=None, chunk=64): # X:[b,h,t,d], A:[b,h,t], B/C:[b,h,t,s], h0:[b,h,d,s], b_ind:[b]\n",
        "def ssd(X, A, B, C, h0=None, msk=None, b_ind=None, chunk=64): # X:[b,h,t,d], A:[b,h,t], B/C:[b,h,t,s], h0:[b,h,d,s], msk:[b,t], b_ind:[b]\n",
        "    # print('ssd', X.dtype, A.dtype, B.dtype)\n",
        "    # assert X.dtype == A.dtype == B.dtype == C.dtype\n",
        "    assert X.shape[:-1] == A.shape == B.shape[:-1]\n",
        "    assert h0==None or b_ind==None\n",
        "    # print('ssd', X.shape, A.shape, B.shape)\n",
        "    if b_ind!=None: A[:,:,b_ind[1:-1]] = 0 # at boundaries, A=0\n",
        "    if X.shape[2] % chunk != 0: X, A, B, C = [x.unsqueeze(2) for x in (X, A, B, C)]\n",
        "    else: X, A, B, C = [x.unflatten(2, (-1,chunk)) for x in (X, A, B, C)] # [b,h,t/c,c(,d/s)]\n",
        "\n",
        "    # 1. Compute the output for each intra-chunk (diagonal blocks)\n",
        "    L = torch.exp(segsum(A)) # [b,h,t/c,c,c]\n",
        "    # if msk!=None: L[msk] = 0 # L = L.masked_fill(msk, 0)\n",
        "    if msk!=None: # this only saves computation from ssd. full info leakage, full compute in in_proj\n",
        "        b,h,l,c,c = L.shape\n",
        "        assert msk.shape==(b,l*c)\n",
        "        msk = msk.reshape(b,1,l,c)\n",
        "        msk = msk.unsqueeeze(-2) | msk.unsqueeze(-1) # [b,1,t/c,c,c]\n",
        "        L[msk] = 0 # L = L.masked_fill(msk, 0)\n",
        "    Y_diag  = torch.einsum(\"...cs,...ks,...ck,...kd->...cd\", C, B, L, X) # bhlcs,bhlks,bhlck,bhlkd->bhlcd # full CA...ABX? for chunks\n",
        "\n",
        "    # 2. Compute the state for each intra-chunk # (right term of low-rank factorization of off-diagonal blocks; B terms)\n",
        "    A_cumsum = torch.cumsum(A, dim=-1) # [b,h,t/c,c]\n",
        "    decay_states = torch.exp((A_cumsum[...,-1:] - A_cumsum)) # [b,h,t/c,c] # Ai+1...T\n",
        "    states = torch.einsum(\"...cs,...c,...cd->...ds\", B, decay_states, X) # bhlcs,bhlc,bhlcd->bhlds # BiXiAi+1...T\n",
        "\n",
        "    # 3. Compute the inter-chunk SSM recurrence; produces correct SSM states at chunk boundaries # (middle term of factorization of off-diag blocks; A terms)\n",
        "    if h0==None: h0 = torch.zeros_like(states[:,:,0], device=states.device) # [b,h,d,s]\n",
        "    states = torch.cat([h0.unsqueeze(2), states], dim=2) # [b,1+t/c,h,d,s] # h0,hc,h2c,...,ht\n",
        "    decay_chunk = torch.exp(segsum(F.pad(A_cumsum[...,-1], (1,0)))) # [b,h,1+t/c]-> # [b,h,1+t/c,1+t/c] # 1,A1...1t/c,A1t/c+1...2t/c,...,A(c-1)t/c...At\n",
        "    new_states = torch.einsum(\"...tl,...lds->...tds\", decay_chunk, states) # bhtl,bhlds->bhtds # h0, BiXi/A1...i-1,\n",
        "\n",
        "    # 4. Compute state -> output conversion per chunk # (left term of low-rank factorization of off-diagonal blocks; C terms)\n",
        "    Y_off = torch.einsum('...cs,...ds,...c->...cd', C, new_states[:,:,:-1], torch.exp(A_cumsum)) # bhlcs,bhlds,bhlc->bhlcd # offset for each chunk # C1h0A1, Ci BiXi/A1...i-1,\n",
        "    Y = (Y_diag+Y_off).flatten(2,3)\n",
        "    return Y, new_states[:,:,-1] # [b,t,h,d], [b,h,d,s]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "E9ri_f5IGL5T"
      },
      "outputs": [],
      "source": [
        "# @title euler zoh bilinear diagonal A\n",
        "import torch\n",
        "\n",
        "# Euler: Abar = 1 + âˆ†A ; Bbar = âˆ†B\n",
        "def euler(A, B, dt): # bts, btsd, bt / bths, bthsd, bt\n",
        "    # return 1+dt.unsqueeze(-1)*A, dt[...,None,None]*B\n",
        "    return 1+torch.einsum('bt,bt...s->bt...s', dt, A), torch.einsum('bt,bt...sd->bt...sd', dt, B)\n",
        "\n",
        "# ZOH: Abar = exp(âˆ†A) ; Bbar = (exp(âˆ†A)-I)/A Â· B ~ (1+A/2)B # For numerical stability as A->0 # (e^x -1)/x ~ 1+(x/2) # from taylor expansion, e^x ~ 1 + x + x^2/2 + x^3/6 + x^4/24 + ...\n",
        "def zoh(A, B, dt): # btd, btds, bt\n",
        "    # A_bar = torch.exp(dt.unsqueeze(-1)*A)\n",
        "    A_bar = torch.exp(torch.einsum('bt,bt...s->bt...s', dt, A))\n",
        "    return A_bar, ((A_bar-1)/A).unsqueeze(-1) * B\n",
        "    # return torch.exp(dt.unsqueeze(-1)*A), (1+A/2).unsqueeze(-1) * B # For numerical stability\n",
        "\n",
        "# Bilinear: Abar = (1+âˆ†A/2) / (1-âˆ†A/2) ; Bbar = âˆ†B/(1-âˆ†A/2)\n",
        "def bilinear(A, B, dt): # btd, btds, bt\n",
        "    # dA_2 = dt.unsqueeze(-1)*A/2\n",
        "    dA_2 = torch.einsum('bt,bt...s->bt...s', dt, A/2)\n",
        "    # return (1+dA_2)/(1-dA_2), (dt.unsqueeze(-1)/(1-dA_2)).unsqueeze(-1) * B\n",
        "    return (1+dA_2)/(1-dA_2), torch.einsum('bt,bt...s->bt...s', dt, 1-dA_2).unsqueeze(-1) * B\n",
        "\n",
        "# b,t,d,s = 2,5,8,4\n",
        "# h=3\n",
        "# A = -torch.randn(b,t,d) # Negative for stability?\n",
        "# B = torch.randn(b,t,d,s)\n",
        "# dt = torch.rand(b,t)*.1\n",
        "# # A_bar, B_bar = euler(A, B, dt)\n",
        "# # A_bar, B_bar = zoh(A, B, dt)\n",
        "# A_bar, B_bar = bilinear(A, B, dt)\n",
        "# print(A_bar.shape, B_bar.shape)\n",
        "\n",
        "# A = -torch.randn(b,t,h,d) # Negative for stability?\n",
        "# B = torch.randn(b,t,h,d,s)\n",
        "\n",
        "# # A_bar, B_bar = euler(A, B, dt)\n",
        "# # print(A_bar.shape, B_bar.shape)\n",
        "# # A_bar, B_bar = zoh(A, B, dt)\n",
        "# # print(A_bar.shape, B_bar.shape) # [2, 5, 8, 8], [2, 5, 8, 4]\n",
        "# A_bar, B_bar = bilinear(A, B, dt)\n",
        "# print(A_bar.shape, B_bar.shape) # [2, 5, 8, 8], [2, 5, 8, 4]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "6eeu0m-bUSdh"
      },
      "outputs": [],
      "source": [
        "# @title hydra me\n",
        "# https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba2_simple.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "# @torch.compile()\n",
        "class Hydra(nn.Module):\n",
        "    def __init__(self, d_model, expand=3, n_heads=8, n_groups=8, d_state=8, d_conv=7):\n",
        "        super().__init__()\n",
        "        n_groups = min(n_heads, n_groups)\n",
        "        assert n_heads % n_groups == 0, \"nheads must be divisible by ngroups\"\n",
        "        self.d_model, self.n_groups, self.d_state, self.d_conv = d_model, n_groups, d_state, d_conv\n",
        "        self.d_inner = expand * self.d_model\n",
        "        self.n_heads, self.d_head = n_heads, self.d_inner//n_heads\n",
        "\n",
        "        # self.in_proj = nn.Linear(self.d_model, 2* self.d_inner + 2* self.n_groups*self.d_state + self.n_heads, bias=False) # z,x,B,C,A\n",
        "        self.in_proj = nn.Linear(self.d_model, 2* self.d_inner + 2* self.n_groups*self.d_state + 2* self.n_heads, bias=True) # z,x,B,C,A\n",
        "        # with torch.no_grad(): self.in_proj.weight[self.d_inner:] = .1*self.in_proj.weight[self.d_inner:]\n",
        "        # conv_dim = self.d_inner + 2*self.n_groups*self.d_state # for x,B,C\n",
        "        # self.conv1d = nn.Conv1d(conv_dim, conv_dim, kernel_size=d_conv, groups=conv_dim, padding=d_conv//2, bias=True)\n",
        "        self.conv1d = nn.Conv1d(d_model, d_model, kernel_size=d_conv, groups=d_model, padding=d_conv//2, bias=True)\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "        self.A_log = nn.Parameter(torch.log(torch.rand(self.n_heads)))\n",
        "        # self.h0 = nn.Parameter(torch.zeros(self.n_heads, self.d_head, self.d_state))\n",
        "        # self.D = nn.Parameter(torch.ones(self.n_heads)) # from mamba\n",
        "        self.D = nn.Linear(self.d_inner, self.n_heads) # og\n",
        "        self.norm = nn.RMSNorm(self.d_inner)\n",
        "        self.out = zero_module(nn.Linear(self.d_inner, self.d_model, bias=False))\n",
        "\n",
        "    # def forward(self, u, h=None, mask=None, b_ind=None): # [b,t,d], ([b,k-1,xbc], [b,h,d,s]), [b+1]\n",
        "    # def forward(self, u, h=None, ctx_idx= None, mask=None, b_ind=None): # [b,t,d], ([b,k-1,xbc], [b,h,d,s]), [b+1]\n",
        "    def forward(self, u, h=None, step= None, mask=None, b_ind=None): # [b,t,d], ([b,k-1,xbc], [b,h,d,s]), [b+1]\n",
        "        b, t = u.shape[:2]\n",
        "        if h==None: h_conv, h_ssm = None, None\n",
        "        else: h_conv, h_ssm = h\n",
        "        # ctx_idx = torch.cat([torch.zeros(b,1, device=u.device), ctx_idx], dim=-1) # [b,1+t]\n",
        "        # step = ctx_idx[:,1:] - ctx_idx[:,:-1] # [b,t]\n",
        "        # step = torch.ones(b,t, device=u.device) #\n",
        "        # step = torch.cat((step, torch.flip(step,(1,))), dim=0) # [b,t]->[2b,t]\n",
        "        if step==None: step = torch.ones(2*b,t, device=u.device) # [b,t]\n",
        "        else: step = torch.cat((step[:,:-1], torch.flip(step[:,1:],(1,))), dim=0).to(device) # [b,t+1]->[2b,t]\n",
        "\n",
        "        # u = self.act(self.conv1d(u.transpose(-2,-1)).transpose(-2,-1)) # [b,t,inr+2gs]\n",
        "        # z, xBC, A = self.in_proj(u).split([self.d_inner, self.d_inner + 2*self.n_groups*self.d_state, self.n_heads], dim=-1)\n",
        "        # z, xBC, A, dt = self.in_proj(u).split([self.d_inner, self.d_inner + 2*self.n_groups*self.d_state, self.n_heads, self.n_heads], dim=-1)\n",
        "        z, to_flip = self.in_proj(u).split([self.d_inner, self.d_inner + 2*self.n_groups*self.d_state + 2*self.n_heads], dim=-1)\n",
        "        # z, x, B, C, A, dt = self.in_proj(u).split([self.d_inner, self.d_inner, self.n_groups*self.d_state, self.n_groups*self.d_state, self.n_heads, self.n_heads], dim=-1)\n",
        "\n",
        "        to_flip = torch.cat((to_flip, torch.flip(to_flip,(1,))), dim=0) # [b,t,xbc]->[2b,t,xbc]\n",
        "        # x, B, C = to_flip.split([self.d_inner, self.n_groups*self.d_state, self.n_groups*self.d_state], dim=-1)\n",
        "        x, B, C, A, dt = to_flip.split([self.d_inner, self.n_groups*self.d_state, self.n_groups*self.d_state, self.n_heads, self.n_heads], dim=-1)\n",
        "\n",
        "        # dt = torch.cat((dt, torch.flip(dt,(1,))), dim=0) # [b,t,h]->[2b,t,h]\n",
        "        dt = F.softplus(dt)\n",
        "        # A = torch.cat((A, torch.flip(A,(1,))), dim=0) # [b,t,h]->[2b,t,h]\n",
        "        # A = -torch.exp(self.A_log) # mamba2\n",
        "        # A = -torch.exp(A) #\n",
        "        # A = -F.softplus(-A) # log(1+e^x)\n",
        "        A = -F.sigmoid(-A) # 1/(1+e^-x)\n",
        "        # print('mamba fwd0', A[-1,-3:,-5:])\n",
        "\n",
        "        # print('mamba fwd', xBC.shape, h_conv.shape)\n",
        "        # xBC = self.act(self.conv1d(xBC.transpose(-2,-1)).transpose(-2,-1)) # [b,t,inr+2gs]\n",
        "\n",
        "\n",
        "        x_og = x[:b] # [b,t,inr]\n",
        "        x, B, C = x.unflatten(-1, (self.n_heads, self.d_head)), B.unflatten(-1, (self.n_groups, self.d_state)), C.unflatten(-1, (self.n_groups, self.d_state)) # x:[2b,t,h,d], B/C:[2b,t,g,s]\n",
        "        # y_diag = (x[:b] * self.D.unsqueeze(-1)).flatten(2) # bthd*h1\n",
        "\n",
        "        h_g = self.n_heads//self.n_groups\n",
        "        if h_g>1: B, C = B.repeat_interleave(h_g, dim=-2), C.repeat_interleave(h_g, dim=-2) # [b,t,g,s]->[b,t,h,s]\n",
        "\n",
        "        # # A, x = (A*step[...,*[None]*(A.ndim-2)]).exp(), x*step[...,None,None] # bthd*bt11=bthd # sdd discretization\n",
        "        A, B = zoh(A, B, step) # euler zoh bilinear # bth,bths\n",
        "        # print('step', step.max())\n",
        "        # print('step', step)\n",
        "        # # print('mamba fwd', A.shape, B.shape, dt.shape)\n",
        "        # # A, B = zoh(A, B*dt.unsqueeze(-1), step) # euler zoh bilinear # bth,bths\n",
        "        # # A, B = zoh(A*dt, B*dt.unsqueeze(-1), step) # euler zoh bilinear # bth,bths\n",
        "        # # print('mamba fwd1', A[-1,-3:,-5:])\n",
        "        # dt = dt * step.unsqueeze(-1) # [2b,t,h]*[2b,t,1]\n",
        "\n",
        "# x:bthd, dt:bthd, A.exp:hds, B/C:bts, D:hd # mamba1\n",
        "# x:bhtd, dt:bht, A:h, B/C:bgts-repeat>bhts, D:h # mamba2\n",
        "# mamba(ssd):\n",
        "# h = Ah + Bx : A*h + x@B = 1/ds*ds + d1@1s = ds\n",
        "# y = Ch + Dx : h@C + D*x = ds@s1 + 1/d1*d1 = d1\n",
        "\n",
        "        # print('mamba fwd', x.shape, A.shape, B.shape)\n",
        "        x, A, B, C = [a.transpose(1,2) for a in (x, A, B, C)] # X:[b,h,t,d], A:[b,h,t], B/C:[b,h,t,s]\n",
        "        dt = dt.transpose(1,2) # [b,h,t]\n",
        "# x:bhtd, A:bht, B/C:bhts, 10.5s\n",
        "        y, h_ssm = ssd(x, A.log(), B, C, h_ssm, msk=mask, b_ind=b_ind, chunk=64) #\n",
        "        # y, h_ssm = ssd(x*dt.unsqueeze(-1), A*dt, B, C, h_ssm, msk=mask, b_ind=b_ind, chunk=64) # 256\n",
        "\n",
        "        y = y.transpose(1,2).flatten(2) # [b,t,d]/[1,b*t,d]\n",
        "\n",
        "        y = torch.roll(y, shifts=1, dims=1) # 123...l -> l12...l-1\n",
        "        y[:,0] = 0 # 012...l-1\n",
        "        y = y[:b] + torch.flip(y[b:], (1,)) + x_og * self.D(x_og).repeat(1,1,self.d_head) # [b,t,inr]\n",
        "        # y = y[:b] + torch.flip(y[b:], (1,)) +  y_diag # [b,t,inr]\n",
        "        y = self.norm(y * self.act(z)) # [b,t,inr] # norm(x)*silu(z) if norm_before_gate, else norm(x*silu(z)) # https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/triton/layernorm_gated.py#L18\n",
        "        out = self.out(y)\n",
        "        return out#, (h_conv, h_ssm) # [b,t,in], ([b,k-1,xbc], [b,h,d,s])\n",
        "\n",
        "\n",
        "b,t,d_model=5,256,32\n",
        "# b,t,d_model=5,7,32\n",
        "u = torch.randn(b,t,d_model, device=device)\n",
        "model = Hydra(d_model).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) #\n",
        "# out, h = model(u)\n",
        "out = model(u)\n",
        "# h0 = torch.randn(b, model.n_heads, model.d_head, model.d_state)\n",
        "# print(out.shape)\n",
        "# print(out.shape, h.shape)\n",
        "# print(out[0,-3:,:5], h[0,:2,:5,:5])\n",
        "# out, h = model(u, h)\n",
        "\n",
        "print(out.shape)\n",
        "print(out[0,-3:,:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pQfM2fYvcTh6"
      },
      "outputs": [],
      "source": [
        "# @title masks\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# def multiblock(seq, min_s, max_s, M=1):\n",
        "#     mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "#     mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "#     mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "#     indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "#     target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "#     return target_mask\n",
        "\n",
        "\n",
        "# def multiblock2d(hw=(8,8), scale=(.15,.2), aspect_ratio=(.75,1.5), M=1):\n",
        "#     mask_aspect = torch.rand(1) * (aspect_ratio[1] - aspect_ratio[0]) + aspect_ratio[0] # in (min_s, max_s) # all blocks same size\n",
        "#     mask_scale = torch.rand(1) * (scale[1] - scale[0]) + scale[0] # in (min_s, max_s) # all blocks same size\n",
        "#     h = (mask_scale/mask_aspect)**.5# h*(h*aspect) = scale\n",
        "#     w = h * mask_aspect\n",
        "#     h_pos, w_pos = torch.rand(M)*(1-w), torch.rand(M)*(1-h) # in (0, 1 - mask_len)\n",
        "#     h_len, h_pos = (h*hw[0]).int(), h_pos*hw[0]\n",
        "#     w_len, w_pos = (w*hw[1]).int(), w_pos*hw[1]\n",
        "#     h_ind, w_ind = torch.arange(hw[0]).unsqueeze(0), torch.arange(hw[1]).unsqueeze(0) # [1, seq]\n",
        "#     h_mask = (h_ind>=h_pos.unsqueeze(-1)) & (h_ind<(h_pos+h_len).unsqueeze(-1)) # [M, seq]\n",
        "#     w_mask = (w_ind>=w_pos.unsqueeze(-1)) & (w_ind<(w_pos+w_len).unsqueeze(-1)) # [M, seq]\n",
        "#     target_mask = h_mask.unsqueeze(-1) & w_mask.unsqueeze(-2) # [M, seq, seq]\n",
        "#     return target_mask\n",
        "\n",
        "# # https://arxiv.org/pdf/2210.07224\n",
        "# def randpatch(seq, mask_size=8, gamma=0.9): # num patches of seq, mask patch size, masking ratio\n",
        "#     # mask = torch.rand(seq//mask_size)<gamma\n",
        "#     length = seq//mask_size\n",
        "#     g = torch.normal(gamma, std=.1, size=(1,)).clamp(.5,.9)\n",
        "#     # g = gamma\n",
        "#     idx = torch.randperm(length)[:int(length*g)]\n",
        "#     mask = torch.zeros(length, dtype=bool)\n",
        "#     mask[idx] = True\n",
        "#     mask = mask.repeat_interleave(mask_size, dim=-1)\n",
        "#     return mask # [seq] , True -> mask\n",
        "\n",
        "\n",
        "# import torch\n",
        "# def apply_masks(x, mask): # [b,t,d], [mask_size] # https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "#     mask_keep = mask.unsqueeze(-1).repeat(x.size(0), 1, x.size(-1)) # [batch,T,dim]\n",
        "#     return torch.gather(x, dim=1, index=mask_keep) # [batch,mask_size,dim]\n",
        "\n",
        "\n",
        "# @title ijepa multiblock next\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "import math\n",
        "from multiprocessing import Value\n",
        "import torch\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(self, hw=(224, 224), enc_mask_scale=(.85,1), pred_mask_scale=(.15,.2), aspect_ratio=(.75,1.25),\n",
        "        nenc=1, npred=2, min_keep=4, allow_overlap=False):\n",
        "        super().__init__()\n",
        "        self.height, self.width = hw\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.aspect_ratio = aspect_ratio\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "\n",
        "    def _sample_block_size(self, scale, aspect_ratio_scale):\n",
        "        _rand = torch.rand(1).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.height * self.width * mask_scale) # num patches to keep\n",
        "        # -- Sample block aspect-ratio\n",
        "        min_ar, max_ar = aspect_ratio_scale\n",
        "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
        "        while h >= self.height: h -= 1 # crop mask to be smaller than img\n",
        "        while w >= self.width: w -= 1\n",
        "        return (h, w)\n",
        "\n",
        "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
        "        h, w = b_size\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            top = torch.randint(0, self.height - h, (1,))\n",
        "            left = torch.randint(0, self.width - w, (1,))\n",
        "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
        "            mask[top:top+h, left:left+w] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
        "        mask_complement[top:top+h, left:left+w] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, B):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        p_size = self._sample_block_size(scale=self.pred_mask_scale, aspect_ratio_scale=self.aspect_ratio)\n",
        "        e_size = self._sample_block_size(scale=self.enc_mask_scale, aspect_ratio_scale=(1., 1.))\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.height * self.width\n",
        "        min_keep_enc = self.height * self.width\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                print(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "        return collated_masks_enc, collated_masks_pred\n",
        "\n",
        "mask_collator = MaskCollator(hw=(32,32), enc_mask_scale=(.85, 1.), pred_mask_scale=(.15, .2), aspect_ratio=(.75, 1.5),\n",
        "        nenc=1, npred=4, min_keep=4,\n",
        "        # allow_overlap=True)\n",
        "        allow_overlap=False)\n",
        "\n",
        "b=16\n",
        "collated_masks_enc, collated_masks_pred = mask_collator(b)\n",
        "ctx_index, trg_index = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "# imshow(mask)\n",
        "\n",
        "mask = torch.zeros(b ,32*32)\n",
        "mask[torch.arange(b).unsqueeze(-1), trg_index] = 1\n",
        "mask[torch.arange(b).unsqueeze(-1), ctx_index] = .5\n",
        "mask = mask.reshape(b,1,32,32)\n",
        "print((mask==1).flatten(1).sum(-1))\n",
        "print((mask==.5).flatten(1).sum(-1))\n",
        "import torchvision\n",
        "imshow(torchvision.utils.make_grid(mask, nrow=8))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "collapsed": true,
        "id": "Nd1GqX3PYpIk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "outputId": "e57c241b-bee9-4b2c-c7f3-a26c6e7c337b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title Random Fourier Features noise\n",
        "import math\n",
        "import torch\n",
        "\n",
        "# Random Fourier Features: âˆ‘_k] a_k * cos(Ï‰_k â‹… x + ðœ™_k)\n",
        "def rff_noise(x, out_dim, n_freqs=128, scale=1): # [...,n_Dim]\n",
        "    space, in_dim, device = x.shape[:-1], x.shape[-1], x.device\n",
        "    x = x.flatten(0,-2) # [N,in]\n",
        "    w = torch.randn(out_dim, n_freqs, in_dim, device=device) * scale\n",
        "    phi = torch.empty(out_dim, n_freqs, device=device).uniform_(0,2*math.pi)\n",
        "    a = torch.randn(out_dim, n_freqs, device=device) / math.sqrt(n_freqs)\n",
        "    y = torch.cos(torch.einsum(\"ofd,...d->...of\", w, x) + phi) # [N,out,freq]\n",
        "    return torch.einsum(\"of,...of->...o\", a, y).unflatten(0,space) # [...,out]\n",
        "\n",
        "def rffmask2d(bhw, ctx_scale=(.85,1.), trg_scale=(.6,.8), chaos=[1,.5]): # ctx_scale > trg_scale\n",
        "# def rffmask2d(bhw, ctx_scale=(.05,.5), trg_scale=(.2,.8), chaos=[1,.5],): # ctx_scale > trg_scale\n",
        "    b,h,w = bhw\n",
        "    ctx_mask_scale = torch.rand(1) * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    trg_mask_scale = torch.rand(1) * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "    seq = h*w\n",
        "    ctx_len, trg_len = int(seq*ctx_mask_scale), int(seq*trg_mask_scale)\n",
        "    # print(ctx_len, trg_len)\n",
        "    # ctx_len = ctx_len - trg_len\n",
        "    ctx_len = min(ctx_len, seq-trg_len)\n",
        "\n",
        "    ix, iy = torch.linspace(0, chaos[0], h), torch.linspace(0, chaos[0], w)\n",
        "    x = torch.stack(torch.meshgrid(ix, iy, indexing=\"ij\"), dim=-1) # [t,h,w,b] / [h,w,b]\n",
        "    noise = rff_noise(x, b).flatten(0,-2).T # [h,w,b]->[h*w,b]->[b,h*w]\n",
        "    _, trg_ind = torch.topk(noise, trg_len, dim=1, sorted=False)\n",
        "\n",
        "    ix, iy = torch.linspace(0, chaos[1], h), torch.linspace(0, chaos[1], w)\n",
        "    x = torch.stack(torch.meshgrid(ix, iy, indexing=\"ij\"), dim=-1) # [t,h,w,b] / [h,w,b]\n",
        "    noise = rff_noise(x, b).flatten(0,-2).T\n",
        "    noise.scatter_(1, trg_ind, -torch.inf).flatten()\n",
        "    _, ctx_ind = torch.topk(noise, ctx_len, dim=1, sorted=False)\n",
        "    return ctx_ind, trg_ind\n",
        "\n",
        "b=16\n",
        "hw=(8,8)\n",
        "# hw=(32,32)\n",
        "# cxt_inds, trg_inds = rffmask2d((b,)+hw, ctx_scale=(.85,1.), trg_scale=(.2,.8), chaos=[3,1])\n",
        "# cxt_inds, trg_inds = rffmask2d((b,)+hw, ctx_scale=(.85,1.), trg_scale=(.2,.8), chaos=[1,.5])\n",
        "cxt_inds, trg_inds = rffmask2d((b,)+hw, ctx_scale=(.05,.5), trg_scale=(.2,.8), chaos=[4,1])\n",
        "# print(cxt_inds, trg_inds)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "# imshow(mask)\n",
        "\n",
        "mask = torch.zeros(b ,math.prod(hw))\n",
        "mask[torch.arange(b).unsqueeze(-1), trg_inds] = 1\n",
        "mask[torch.arange(b).unsqueeze(-1), cxt_inds] = .5\n",
        "# print((mask==1).sum(1))\n",
        "# print((mask==.5).sum(1))\n",
        "mask = mask.reshape(b,1,*hw)\n",
        "import torchvision\n",
        "imshow(torchvision.utils.make_grid(mask, nrow=8))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ivqYjBIVrB3e",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "feeddce7-43a6-4069-aec7-70e1e8d70969"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100672\n",
            "torch.Size([5, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# @title ViT\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        self._kwargs = [[name for name, p in inspect.signature(layer.forward).parameters.items()] for layer in self]\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        for layer, _kwargs in zip(self, self._kwargs):\n",
        "            x = layer(x, *args, **{k: v for k, v in kwargs.items() if k in _kwargs})\n",
        "        return x\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, patch_size, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        patch_size=2\n",
        "        self.hw=(8,8) # (8,8) (32,32)\n",
        "        self.seq = self.hw[0]*self.hw[1]\n",
        "\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Conv2d(in_dim, d_model, patch_size, patch_size), # like patch\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 1, 7//2, bias=False)\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.MaxPool2d(3,2,1)\n",
        "            nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.BatchNorm2d(d_model), nn.ReLU(),\n",
        "            nn.Conv2d(d_model, d_model, 3, 2, 3//2, bias=False)\n",
        "            )\n",
        "        # self.embed.requires_grad=False\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, self.seq, d_model)*.02)\n",
        "        # self.pos_emb = nn.Parameter(RoPE2D(dim=d_model, h=8, w=8, base=1000), requires_grad=False)\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=(32//patch_size)**2, base=10000), requires_grad=False)\n",
        "\n",
        "        self.transformer = Seq(*[AttentionBlock(d_model, d_model, n_heads) for _ in range(nlayers)])\n",
        "        # self.transformer = Seq(*[Hydra(d_model, n_heads) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, cxt_inds=None): # [b,ctx,3], [b,ctx] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x).flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c] # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "\n",
        "        # x = self.pos_enc(x)\n",
        "        # print(\"TransformerModel\",x.shape, self.pos_emb.shape)\n",
        "        x = x + self.pos_emb[:,:x.shape[1]]\n",
        "        if cxt_inds != None: x = x[torch.arange(x.shape[0]).unsqueeze(-1), cxt_inds] # [batch, num_context_toks, d_model]\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        # if cxt_inds != None: # for pos\n",
        "        #     # x = self.transformer(x[torch.arange(x.shape[0]).unsqueeze(-1), cxt_inds], cxt_inds) # for pos attn\n",
        "        #     b = x.shape[0]\n",
        "        #     # print('vit fwd', cxt_inds)\n",
        "        #     ctx_idx = torch.cat([torch.full((b,1),-1), cxt_inds, torch.full((b,1),self.seq)], dim=-1) # [b,1+t+1]\n",
        "        #     step = ctx_idx[:,1:] - ctx_idx[:,:-1] # [b,t+1]\n",
        "        #     x = self.transformer(x[torch.arange(x.shape[0]).unsqueeze(-1), cxt_inds], step=step)\n",
        "        # else: x = self.transformer(x)\n",
        "\n",
        "        out = self.norm(x)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out\n",
        "\n",
        "d_model = 64\n",
        "in_dim = 3\n",
        "patch_size = 2\n",
        "# model = ViT(patch_size, in_dim, d_model, n_heads=4, nlayers=4, drop=0.).to(device)\n",
        "model = ViT(patch_size, in_dim, d_model, n_heads=4, nlayers=1, drop=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x = torch.rand((5, in_dim, 32, 32), device=device) # [b,c,h,w]\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "M-zdjdJixtOu",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Transformer Predictor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        self.hw=(8,8) # (8,8) (32,32)\n",
        "        self.seq = self.hw[0]*self.hw[1]\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, self.seq, d_model)*0.02)\n",
        "\n",
        "        self.transformer = Seq(*[AttentionBlock(d_model, n_heads) for _ in range(nlayers)])\n",
        "        # self.transformer = Seq(*[Hydra(d_model, n_heads) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.randn(1,1,d_model)*.02) # randn zeros\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim)# if out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, cxt_inds, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        b, seq, dim = x.shape\n",
        "\n",
        "        # # x = x * self.pos_enc(cxt_inds)\n",
        "        # # pred_tokens = self.cls * self.pos_enc(trg_indices) # [M, num_trg_toks, d_model]\n",
        "        x = x + self.pos_emb[0,cxt_inds]\n",
        "        pred_tokens = self.cls + self.pos_emb[0,trg_indices]\n",
        "        # # print(\"pred fwd\", x.shape, pred_tokens.shape)\n",
        "        x = torch.cat([x, pred_tokens], dim=1) # [batch, seq_len+num_trg_toks, d_model]\n",
        "        out = self.transformer(x)[:,seq:] # [b,trg,d]\n",
        "\n",
        "        # # for ssm\n",
        "        # x = torch.cat([x, self.cls.expand(*trg_indices.shape,-1)], dim=1) # [batch, num_ctx_toks+num_trg_toks, d_model]\n",
        "        # val, idx = torch.sort(torch.cat([cxt_inds, trg_indices], dim=-1)) # [b,ctx+trg]\n",
        "        # x = x[torch.arange(b).unsqueeze(-1), idx]\n",
        "        # indices = torch.cat([torch.full((b,1), -1), val, torch.full((b,1), self.seq)], dim=-1) # [b,1+t+1]\n",
        "        # step = indices[:,1:] - indices[:,:-1] # [b,t+1]\n",
        "        # out = self.transformer(x, step=step)\n",
        "        # back_idx = idx[idx>=cxt_inds.shape[-1]].reshape(b,-1) # [b,trg]\n",
        "        # out = out[torch.arange(b).unsqueeze(-1), back_idx] # [b,trg,d]\n",
        "\n",
        "        # # for pos attn\n",
        "        # x = torch.cat([x, self.cls.expand(*trg_indices.shape,-1)], dim=1) # [batch, num_ctx_toks+num_trg_toks, d_model]\n",
        "        # out = self.transformer(x, torch.cat([cxt_inds, trg_indices], dim=-1))[:,seq:] # [b,trg,d]\n",
        "\n",
        "        out = self.norm(out)\n",
        "        out = self.lin(out)\n",
        "        # print(\"pred fwd\", out.shape)\n",
        "        return out # [seq_len, batch_size, ntoken]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ardu1zJwdHM3",
        "outputId": "0c9d78ae-0388-4144-8f17-fc015bc70d99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "153024\n",
            "torch.Size([])\n"
          ]
        }
      ],
      "source": [
        "# @title IJEPA\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "@torch.compile\n",
        "class IJEPA(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=64, out_dim=None, nlayers=1, n_heads=4):\n",
        "        super().__init__()\n",
        "        self.out_dim = out_dim = out_dim or d_model\n",
        "        self.patch_size = 4 # 4\n",
        "        self.student = ViT(self.patch_size, in_dim, d_model, out_dim=out_dim, n_heads=n_heads, nlayers=nlayers, drop=0.)\n",
        "        self.predicter = TransformerPredictor(out_dim, d_model//2, out_dim, n_heads=4, nlayers=1, drop=0.)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "        self.hw=(8,8) # (8,8) (32,32)\n",
        "        # self.mask_collator = MaskCollator(self.hw, enc_mask_scale=(.85, 1.), pred_mask_scale=(.15, .2), aspect_ratio=(.75, 1.5), nenc=1, npred=4, min_keep=4, allow_overlap=False) # og\n",
        "\n",
        "    def loss(self, x): #\n",
        "        b,c,h,w = x.shape\n",
        "        # collated_masks_enc, collated_masks_pred = self.mask_collator(b)\n",
        "        # cxt_inds, trg_inds = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "\n",
        "        # cxt_inds, trg_inds = rffmask2d((1,)+self.hw, ctx_scale=(.85,1.), trg_scale=(.2,.8), chaos=[3,1])\n",
        "        cxt_inds, trg_inds = rffmask2d((1,)+self.hw, ctx_scale=(.05,.5), trg_scale=(.4,.8), chaos=[4,1])\n",
        "        # cxt_inds, trg_inds = rffmask2d((1,)+self.hw, ctx_scale=(.05,.5), trg_scale=(.15,.2), chaos=[3,1])\n",
        "        # cxt_inds, trg_inds = simplexmask2d(hw, ctx_scale=(.85,1), trg_scale=(.7,.8), B=b, chaos=[3,.5])\n",
        "        cxt_inds, trg_inds = cxt_inds.repeat(b,1), trg_inds.repeat(b,1)\n",
        "        # cxt_inds = cxt_inds.sort(-1)[0]\n",
        "\n",
        "        # zero_mask = torch.zeros(b ,*self.hw, device=device).flatten(1)\n",
        "        # zero_mask[torch.arange(b).unsqueeze(-1), cxt_inds] = 1\n",
        "        # x_ = x * F.interpolate(zero_mask.reshape(b,1,*self.hw), size=x.shape[2:], mode='nearest-exact') # zero masked locations\n",
        "        # sx = self.student(x_, cxt_inds=cxt_inds) # [batch, num_context_toks, out_dim]\n",
        "\n",
        "        sx = self.student(x, cxt_inds=cxt_inds) # [batch, num_context_toks, out_dim]\n",
        "        # print('ijepa loss sx',sx.shape)\n",
        "        sy_ = self.predicter(sx, cxt_inds=cxt_inds, trg_indices=trg_inds) # [batch*M, num_trg_toks, out_dim]\n",
        "        sy_ = F.layer_norm(sy_, (sy_.size(-1),))\n",
        "        with torch.no_grad():\n",
        "            sy = self.teacher(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "            # print('ijepa loss sy',sy.shape)\n",
        "            sy = sy[torch.arange(sy.shape[0]).unsqueeze(-1), trg_inds] # [batch, num_context_toks, d_model] # nan bec len(trg_ind)==0 # print('loss sy',torch.isnan(sy).any())\n",
        "            sy = F.layer_norm(sy, (sy.size(-1),))\n",
        "        loss = F.mse_loss(sy, sy_)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x): # [b,t,3]\n",
        "        out = self.student(x).mean(1)\n",
        "        return out\n",
        "\n",
        "# min_s=0.15, max_s, M\n",
        "# trg.15.2M4 C.85 1\n",
        "# dont normalise data\n",
        "# randmsk < simplex 1msk < multiblk\n",
        "# teacher=trans ~? teacher=copy\n",
        "# learn convemb\n",
        "# lr pred,student: 3e-3/1e-2, 1e-3\n",
        "\n",
        "# lr 1e-4<<<1e-3 < 1e-2,1e-3? slower but dun plateau\n",
        "# zeromsk impt, else closs start increasing\n",
        "# l1rge trg helps delay increase?\n",
        "\n",
        "# vit 1lyr 15.4sec 15.6\n",
        "# hiera downsampling attn 20.5\n",
        "\n",
        "# ijepa = IJEPA(in_dim=3, d_model=64, out_dim=64, nlayers=4, n_heads=4).to(device)\n",
        "ijepa = IJEPA(in_dim=3, d_model=64, out_dim=64, nlayers=1, n_heads=8).to(device)\n",
        "# ijepa = IJEPA(in_dim=3, d_model=32, out_dim=32, nlayers=1, n_heads=4).to(device)\n",
        "# optim = torch.optim.AdamW(ijepa.parameters(), lr=1e-3)#, weight_decay=0) # 1e-3?\n",
        "optim = torch.optim.AdamW([{'params': ijepa.student.parameters()},\n",
        "#     {'params': ijepa.predicter.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # good 3e-3,1e-3 ; default 1e-2, 5e-2\n",
        "    {'params': ijepa.predicter.parameters(), 'lr': 1e-2}], lr=1e-3, weight_decay=1e-2)\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/configs/in1k_vith14_ep300.yaml\n",
        "# d_model 1024,384\n",
        "# depth 12,6/12\n",
        "# wd 5e-2 - 4e-1\n",
        "# adamw 1e-4 - 1e-3 - 1e-6\n",
        "# ema 0.996-1\n",
        "\n",
        "print(sum(p.numel() for p in ijepa.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in ijepa.parameters())) # 27584\n",
        "# print(sum(p.numel() for p in ijepa.predicter.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# d_model^2 * nlayers\n",
        "\n",
        "x = torch.rand((64,3,32,32), device=device)\n",
        "out = ijepa.loss(x)\n",
        "print(out.shape)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(ijepa.out_dim, 10).to(device)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n",
        "# optim = torch.optim.AdamW([{'params': ijepa.parameters()}, {'params': classifier.parameters(), 'lr': 1e-3}], lr=1e-3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "2Nd-sGe6Ku4S",
        "outputId": "c7a27ff5-2464-45ff-b861-5ba461262aeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>â–ˆâ–‡â–‡â–†â–†â–…â–…â–…â–„â–„â–„â–…â–ƒâ–„â–„â–„â–…â–ƒâ–„â–ƒâ–„â–„â–ƒâ–ƒâ–…â–ƒâ–„â–„â–ƒâ–ƒâ–‚â–â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–â–â–‚</td></tr><tr><td>correct</td><td>â–‚â–â–‚â–‚â–‚â–ƒâ–†â–‚â–‚â–‡â–…â–†â–„â–„â–ˆâ–ƒâ–†â–…â–†â–„â–…â–…â–†â–†â–†â–ˆâ–‡â–†â–ˆâ–…â–ˆâ–…â–‡â–…â–ˆâ–‡â–…â–†â–…â–ˆ</td></tr><tr><td>loss</td><td>â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–…â–„â–‡â–‚â–‚â–â–â–ƒâ–â–‚â–â–‚â–„â–ƒâ–â–‚â–„â–ƒâ–…â–‡â–â–…â–‚â–‚â–…â–‚â–…â–‚â–‚â–‚â–ˆâ–‚</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>1.68398</td></tr><tr><td>correct</td><td>0.40625</td></tr><tr><td>loss</td><td>0.3876</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">sleek-galaxy-10</strong> at: <a href='https://wandb.ai/bobdole/rff/runs/c4yga5sf' target=\"_blank\">https://wandb.ai/bobdole/rff/runs/c4yga5sf</a><br> View project at: <a href='https://wandb.ai/bobdole/rff' target=\"_blank\">https://wandb.ai/bobdole/rff</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20260205_061723-c4yga5sf/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.24.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20260205_070058-hul5b50w</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/rff/runs/hul5b50w' target=\"_blank\">volcanic-disco-11</a></strong> to <a href='https://wandb.ai/bobdole/rff' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/rff' target=\"_blank\">https://wandb.ai/bobdole/rff</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/rff/runs/hul5b50w' target=\"_blank\">https://wandb.ai/bobdole/rff/runs/hul5b50w</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "!pip install -q wandb\n",
        "import wandb # https://docs.wandb.ai/quickstart\n",
        "wandb.login(key='487a2109e55dce4e13fc70681781de9f50f27be7')\n",
        "try: run.finish()\n",
        "except NameError: pass\n",
        "run = wandb.init(project=\"rff\", config={\"model\": \"res18\",})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34de8800-fa4b-4460-803c-3cd8dbfb6c70",
        "outputId": "d36a2c5e-7aae-4329-80fa-2a851c0d0a6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 sloss, closs, correct 0.23769031465053558 2.299105167388916 0.0625\n",
            "time: 23.265898942947388 23.265899896621704\n",
            "1 sloss, closs, correct 0.23099415004253387 2.341782569885254 0.1875\n",
            "time: 12.818718433380127 18.042577505111694\n",
            "2 sloss, closs, correct 0.2617381811141968 2.198543071746826 0.15625\n",
            "time: 12.830862522125244 16.305521249771118\n",
            "3 sloss, closs, correct 0.23363907635211945 2.2750048637390137 0.125\n",
            "time: 12.653909683227539 15.39273589849472\n",
            "4 sloss, closs, correct 0.3918825387954712 2.2808263301849365 0.140625\n",
            "time: 12.748950242996216 14.864121580123902\n",
            "5 sloss, closs, correct 0.3881267309188843 2.26395320892334 0.109375\n",
            "time: 12.723624229431152 14.507456382115683\n",
            "6 sloss, closs, correct 0.31299492716789246 2.2071468830108643 0.1875\n",
            "time: 12.828648090362549 14.268017326082502\n",
            "7 sloss, closs, correct 0.2555914521217346 2.1971445083618164 0.21875\n",
            "time: 14.39554476737976 14.28403615951538\n",
            "8 sloss, closs, correct 0.35867539048194885 2.1873602867126465 0.296875\n",
            "time: 13.00919246673584 14.142468399471706\n",
            "9 sloss, closs, correct 0.2511358857154846 2.1322829723358154 0.15625\n",
            "time: 12.834636926651001 14.011739373207092\n",
            "10 sloss, closs, correct 0.26766642928123474 2.134791612625122 0.203125\n",
            "time: 12.81708812713623 13.903181119398637\n",
            "11 sloss, closs, correct 0.2175656259059906 2.1804912090301514 0.21875\n",
            "time: 12.730667352676392 13.805520335833231\n",
            "12 sloss, closs, correct 0.19252197444438934 2.0468153953552246 0.265625\n",
            "time: 12.90251088142395 13.736112833023071\n",
            "13 sloss, closs, correct 0.21066048741340637 2.151256561279297 0.203125\n",
            "time: 12.758421421051025 13.666314908436366\n",
            "14 sloss, closs, correct 0.2503378093242645 2.0996382236480713 0.203125\n",
            "time: 12.903581380844116 13.61551570892334\n",
            "15 sloss, closs, correct 0.20503759384155273 2.035331964492798 0.34375\n",
            "time: 12.91368579864502 13.571685045957565\n",
            "16 sloss, closs, correct 0.20011281967163086 2.083500385284424 0.15625\n",
            "time: 12.726349353790283 13.522000929888558\n",
            "17 sloss, closs, correct 0.26488783955574036 2.087769031524658 0.25\n",
            "time: 12.64638352394104 13.473384684986538\n",
            "18 sloss, closs, correct 0.37943795323371887 2.1218514442443848 0.21875\n",
            "time: 12.70661997795105 13.43306693277861\n",
            "19 sloss, closs, correct 0.3084098994731903 2.113607883453369 0.328125\n",
            "time: 12.690306186676025 13.395955169200898\n",
            "20 sloss, closs, correct 0.2689196765422821 2.1247711181640625 0.28125\n",
            "time: 12.855465173721313 13.370242868150983\n",
            "21 sloss, closs, correct 0.30873632431030273 2.040818214416504 0.234375\n",
            "time: 12.703530550003052 13.340024785561996\n",
            "22 sloss, closs, correct 0.24062508344650269 2.0185041427612305 0.265625\n",
            "time: 12.638519525527954 13.309546512106191\n",
            "23 sloss, closs, correct 0.206009179353714 1.989084005355835 0.203125\n",
            "time: 12.769474744796753 13.28707226117452\n",
            "24 sloss, closs, correct 0.30505484342575073 1.8764022588729858 0.234375\n",
            "time: 12.680599689483643 13.262833642959595\n",
            "25 sloss, closs, correct 0.19611114263534546 2.024768590927124 0.203125\n",
            "time: 12.820710182189941 13.245858284143301\n",
            "26 sloss, closs, correct 0.22448450326919556 1.9596176147460938 0.34375\n",
            "time: 12.741266250610352 13.227189064025879\n",
            "27 sloss, closs, correct 0.2993983328342438 1.93267822265625 0.3125\n",
            "time: 12.728237867355347 13.20939517872674\n",
            "28 sloss, closs, correct 0.416481614112854 1.9201500415802002 0.265625\n",
            "time: 12.9022216796875 13.19882108425272\n",
            "29 sloss, closs, correct 0.2887929081916809 1.9198858737945557 0.1875\n",
            "time: 13.218851327896118 13.199513292312622\n",
            "30 sloss, closs, correct 0.3568682372570038 2.0035367012023926 0.234375\n",
            "time: 13.830034017562866 13.219868829173427\n",
            "31 sloss, closs, correct 0.25169098377227783 1.9577763080596924 0.296875\n",
            "time: 13.394438982009888 13.225336335599422\n",
            "32 sloss, closs, correct 0.23623886704444885 1.8597207069396973 0.296875\n",
            "time: 13.035183429718018 13.21963508201368\n",
            "33 sloss, closs, correct 0.310455858707428 2.021939277648926 0.34375\n",
            "time: 12.884341478347778 13.209785412339603\n",
            "34 sloss, closs, correct 0.43404000997543335 1.854264736175537 0.3125\n",
            "time: 12.786578893661499 13.19771113395691\n",
            "35 sloss, closs, correct 0.3149816691875458 1.9315950870513916 0.328125\n",
            "time: 12.789477348327637 13.186381883091396\n",
            "36 sloss, closs, correct 0.2707764804363251 1.8819591999053955 0.28125\n",
            "time: 13.197951078414917 13.186760045386649\n"
          ]
        }
      ],
      "source": [
        "# @title strain ctrain test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "def strain(model, dataloader, optim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.train()\n",
        "    for i, (x, _) in enumerate(dataloader):\n",
        "        x = x.to(device)\n",
        "        loss = model.loss(x)\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        # grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 5) # 0.5\n",
        "        # print('grad_norm', grad_norm.item())\n",
        "        optim.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            m=0.999 # 0.99 m = next(momentum_scheduler)\n",
        "            for param_q, param_k in zip(model.student.parameters(), model.teacher.parameters()):\n",
        "                param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        # if i%10==0: print(\"strain\",loss.item())\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=500: break\n",
        "    return loss.item()\n",
        "\n",
        "def ctrain(model, classifier, dataloader, coptim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.eval()\n",
        "    classifier.train()\n",
        "    # closs = 0\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        with torch.no_grad(): sx = model(x).detach()\n",
        "        y_ = classifier(sx)\n",
        "        loss = F.cross_entropy(y_, y)\n",
        "        coptim.zero_grad()\n",
        "        loss.backward()\n",
        "        coptim.step()\n",
        "        # print(\"classify\",loss.item())\n",
        "        # closs += loss.item()\n",
        "        try: wandb.log({\"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=100: break\n",
        "    # return closs/\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        with torch.no_grad():\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        # print(correct/len(y))\n",
        "        try: wandb.log({\"correct\": correct/len(y)})\n",
        "        # try: wandb.log({\"correct\": correct/len(y), \"rankme\": rankme, \"lidar\": lidar})\n",
        "        except NameError: pass\n",
        "        if i>=100: break\n",
        "    return correct/len(y)\n",
        "\n",
        "import time\n",
        "start = begin = time.time()\n",
        "# for i in range(1):\n",
        "for i in range(100): # 1000\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    sloss = strain(ijepa, train_loader, optim)\n",
        "    closs = ctrain(ijepa, classifier, train_loader, coptim)\n",
        "    correct = test(ijepa, classifier, test_loader)\n",
        "    print(i, 'sloss, closs, correct', sloss, closs, correct)\n",
        "    print('time:',time.time() - start, (time.time()-begin)/(i+1))\n",
        "    start = time.time()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phs2ERy_RmMh"
      },
      "outputs": [],
      "source": [
        "# print(ijepa.student.transformer[0].in_proj.weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jzo9DMDPcOxu"
      },
      "outputs": [],
      "source": [
        "# @title save/load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder='/content/drive/MyDrive/jepa/'\n",
        "\n",
        "# # modelsd, optimsd = torch.load(folder+'SeqJEPA.pkl', map_location=device).values()\n",
        "# modelsd, optimsd = torch.load('SeqJEPA.pkl', map_location=device).values()\n",
        "# seq_jepa.load_state_dict(modelsd, strict=False)\n",
        "# optim.load_state_dict(optimsd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "DNNPOuUmcSNf"
      },
      "outputs": [],
      "source": [
        "checkpoint = {'model': ijepa.state_dict(), 'optimizer': optim.state_dict()}\n",
        "torch.save(checkpoint, folder+'ijepa.pkl')\n",
        "# torch.save(checkpoint, 'IJEPA.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yl-xtHthFl0M"
      },
      "source": [
        "## store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "e7HYQxn6n6iD"
      },
      "outputs": [],
      "source": [
        "# @title GoldenGateRoPE2d\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class GoldenGateRoPE2d(nn.Module): # jerryxio.ng/posts/nd-rope\n",
        "    def __init__(self, image_size, n_heads, n_freqs, min_freq=.8, max_freq=10, n_zero_freqs=0):\n",
        "        super().__init__()\n",
        "        intv = math.pi * (math.sqrt(5)-1)/2 # mod pi instead of 2pi # pi*(sqrt5+-1)/2 ; + and - are equivalent bec mod pi\n",
        "        # intv = math.pi * (math.sqrt(5)-1) # https://en.wikipedia.org/wiki/Golden_angle\n",
        "        speed = torch.cat([torch.zeros(n_zero_freqs), min_freq * (max_freq/min_freq) ** torch.linspace(0,1,n_freqs-n_zero_freqs)]).unsqueeze(-1) # [n_freqs,1] # og\n",
        "        angle = torch.arange(n_heads*n_freqs).reshape(n_heads, n_freqs) * intv # [n_heads, n_freqs]\n",
        "        direction = torch.stack([torch.cos(angle), torch.sin(angle)], dim=-1) # [n_heads, n_freqs, 2]\n",
        "        h, w = image_size\n",
        "        xlim, ylim = w/h, h/w\n",
        "        y, x = torch.meshgrid(torch.linspace(-ylim, ylim, h), torch.linspace(-xlim, xlim, w), indexing=\"ij\") # [h,w], y:row_num, x:col_num\n",
        "        pos = torch.stack([x, y], dim=-1).reshape(-1,1,1,2) # [h*w,1,1,2] cartesian coords\n",
        "        theta = (speed*direction*pos).sum(dim=-1) # [t,n_heads,n_freqs,2]->[t,n_heads,d_head]\n",
        "        self.theta = theta\n",
        "        cos, sin = torch.cos(theta), torch.sin(theta)\n",
        "        self.affine = torch.stack([cos, -sin, sin, cos], dim=-1).transpose(0,1).reshape(1,n_heads,h*w,n_freqs,2,2).to(device) # [t,n_heads,n_freqs,4]->[1,n_heads,t,n_freqs,2,2]\n",
        "\n",
        "    def forward(self, x): # [b,h,t,d]\n",
        "        return (self.affine @ x.unflatten(-1, (-1,2)).unsqueeze(-1)).flatten(-3) # @ [b,h,t,d_head//2,2,1]\n",
        "\n",
        "# /2 better\n",
        "# speed [n_freqs,1] # og best\n",
        "# w/h best\n",
        "# rope < ggrope < learned\n",
        "\n",
        "# image_size=(8,8)\n",
        "image_size=(20,30)\n",
        "# image_size=(90,120)\n",
        "n_heads=4\n",
        "n_freqs=6\n",
        "ggrope = GoldenGateRoPE2d(image_size, n_heads, n_freqs)\n",
        "\n",
        "# x = torch.rand(2, *image_size, n_heads, n_freqs*2)\n",
        "x = torch.rand(2, n_heads, image_size[0]*image_size[1], n_freqs*2)\n",
        "out = ggrope(x)\n",
        "print(out.shape)\n",
        "# # print(out[0])\n",
        "# theta = ggrope.theta.flatten(-2).permute(2,0,1).unsqueeze(1) # [t,n_heads,d_head][b,1,h,w]\n",
        "theta = ggrope.theta.flatten(-2).T.reshape(n_heads*n_freqs, 1, *image_size) # [t,n_heads,d_head]->[d,1,h,w]\n",
        "cy, cx = image_size[0]//2, image_size[1]//2\n",
        "sim = torch.cos(theta-theta[...,cy,cx][...,None,None]) # [b,1,h,w]\n",
        "# sim = sim.unflatten(0, (n_heads, n_freqs)).mean(1)\n",
        "# print(sim.shape)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    print(npimg.shape)\n",
        "    plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "# bhwc\n",
        "\n",
        "import torchvision\n",
        "imshow(torchvision.utils.make_grid(sim, nrow=n_freqs))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "eA2R-JZ7G67P"
      },
      "outputs": [],
      "source": [
        "# @title RoPE pos\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class RoPE(nn.Module):\n",
        "    def __init__(self, dim, seq_len=512, top=torch.pi, base=10000):\n",
        "    # def __init__(self, dim, seq_len=512, min_freq=1, max_freq=400, n_zero_freqs=0):\n",
        "        super().__init__()\n",
        "        self.dim, self.top, self.base = dim, top, base\n",
        "        speed = top / (base ** (torch.arange(0, dim, step=2) / dim)) # [dim//2]\n",
        "        pos = torch.arange(seq_len).unsqueeze(-1) # [t,1]\n",
        "        # speed = torch.cat([torch.zeros(n_zero_freqs), min_freq * (max_freq/min_freq) ** torch.linspace(0,1,dim//2-n_zero_freqs)]) # [dim//2]\n",
        "        # pos = torch.linspace(0, 1, seq_len).unsqueeze(-1) # [t,1]\n",
        "        theta = (speed*pos) # [t,1]*[dim//2]=[t,d//2]\n",
        "        self.theta = theta\n",
        "        cos, sin = torch.cos(theta), torch.sin(theta)\n",
        "        self.affine = torch.stack([cos, -sin, sin, cos], dim=-1).unflatten(-1,(2,2)).to(device)#[None,None,...] # [t,d//2,4]->[t,d//2,2,2] # [1,1,t,d//2,2,2]\n",
        "\n",
        "\n",
        "        # angles = theta[None,...,None] # [seq_len, 1] * [dim//2] -> [1, 1, seq_len, dim//2, 1]\n",
        "        # self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [1, 1, seq_len, dim//2, 2] -> [1, 1, seq_len, dim]\n",
        "        # # self.rot_emb = torch.cat([sin, cos], dim=-1).flatten(-2).to(device) # [1, 1, seq_len, dim//2, 2] -> [1,1,seq_len,dim]\n",
        "        # print('self.rot_emb', self.rot_emb.shape)\n",
        "\n",
        "    def forward(self, x, ind=None): # [b,h,t,d], [b,t]\n",
        "        # seq_len = x.size(-2)\n",
        "        b,_,seq_len,_ = x.shape\n",
        "        if ind!=None: seq_len = max(seq_len, ind.max()+1)\n",
        "        if self.affine.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.top, self.base)\n",
        "        if ind!=None:\n",
        "            # print(\"if affine, x\",self.affine.shape, x.shape)\n",
        "            return (self.affine.unsqueeze(0).expand(b,-1,-1,-1,-1)[torch.arange(b, device=device).unsqueeze(-1), ind].unsqueeze(1) @ x.unflatten(-1, (-1,2)).unsqueeze(-1)).flatten(-3) # @ [b,h,t,d_head//2,2,1]\n",
        "        else:\n",
        "            # print(\"else affine, x\",self.affine.shape, x.shape) # [64, 4, 2, 2], [64, 8, 10, 8]\n",
        "            return (self.affine[None,None,...] @ x.unflatten(-1, (-1,2)).unsqueeze(-1)).flatten(-3) # @ [b,h,t,d_head//2,2,1]\n",
        "\n",
        "\n",
        "        # # if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.base)\n",
        "        # if ind==None:\n",
        "        #     # print(\"if rot, x\",self.rot_emb.shape, x.shape)\n",
        "        #     return x * self.rot_emb.unsqueeze(1)#[None,None,...]\n",
        "        # else:\n",
        "        #     return x * (self.rot_emb.expand(b,-1,-1)[torch.arange(b, device=device).unsqueeze(-1), ind].unsqueeze(1))\n",
        "\n",
        "\n",
        "\n",
        "dim=64\n",
        "seq_len=56\n",
        "rope = RoPE(dim, seq_len, top=torch.pi, base=100)\n",
        "# rope = RoPE(dim, seq_len, min_freq=1, max_freq=200, n_zero_freqs=0)\n",
        "\n",
        "batch=2\n",
        "t=50\n",
        "x = torch.rand(batch, 4, seq_len, dim, device=device)\n",
        "out = rope(x)\n",
        "print(\"out1\", out.shape)\n",
        "x = torch.rand(batch, 4, t, dim, device=device)\n",
        "pos = torch.randint(0,seq_len,(batch,t), device=device)\n",
        "out = rope(x, pos)\n",
        "print(\"out2\", out.shape)\n",
        "\n",
        "# theta = rope.theta # [t,d//2]\n",
        "# sim = torch.cos(theta-theta[0].unsqueeze(0)).T\n",
        "# # print(sim.shape)\n",
        "\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# def imshow(img):\n",
        "#     npimg = img.numpy()\n",
        "#     print(npimg.shape)\n",
        "#     plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "#     plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "#     plt.show()\n",
        "\n",
        "# import torchvision\n",
        "# imshow(torchvision.utils.make_grid(sim, nrow=dim//2))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "k3nfeLM4wtkJ"
      },
      "outputs": [],
      "source": [
        "# @title random_masking\n",
        "import torch\n",
        "\n",
        "def random_masking(length, mask_ratio, b=64):\n",
        "    noise = torch.rand(b, length)\n",
        "    len_mask = int(length * mask_ratio)\n",
        "    _, msk_ind = torch.topk(noise, k=len_mask, dim=-1, sorted=False) # val, ind -> [b,len_mask]\n",
        "    _, keep_ind = torch.topk(noise, k=length-len_mask, largest=False, dim=-1, sorted=False) # val, ind -> [b,len_keep]\n",
        "    return msk_ind, keep_ind\n",
        "\n",
        "# msk_ind, keep_ind = random_masking(10, .3, b=2)\n",
        "\n",
        "# x_ = torch.rand(4, 3, 2)\n",
        "# print(x_)\n",
        "# # ids = torch.tensor([0, 2, 1])[None,:,None]\n",
        "# # ids = torch.tensor([0, 2, 1])[None,:,None].repeat(4,1,2)\n",
        "# ids = torch.tensor([1, 2, 0])[None,:,None].repeat(4,1,2)\n",
        "# # o = torch.gather(x_, dim=1, index=ids)\n",
        "# o = torch.zeros_like(x_).scatter_(dim=1, index=ids, src=x_)\n",
        "# print(o)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "62UPGVucmGNe"
      },
      "outputs": [],
      "source": [
        "# @title AttentionBlock pos\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "# import inspect\n",
        "# class Seq(nn.Sequential):\n",
        "#     def __init__(self, *args):\n",
        "#         super().__init__(*args)\n",
        "#         for layer in self:\n",
        "#             params = inspect.signature(layer.forward).parameters.keys()\n",
        "#             layer._fwdparams = ','.join(params)\n",
        "\n",
        "#     def forward(self, x, pos=None, masks=None):\n",
        "#         arg_map = {'pos':pos, 'masks':masks}\n",
        "#         for layer in self:\n",
        "#             args = [x]\n",
        "#             # if 'masks' in layer._fwdparams: args.append(masks)\n",
        "#             args.extend(arg_map[p] for p in arg_map if p in layer._fwdparams)\n",
        "#             # print(layer._fwdparams, args)\n",
        "#             x = layer(*args)\n",
        "#         return x\n",
        "\n",
        "# import inspect\n",
        "# class Seq(nn.Sequential):\n",
        "#     def __init__(self, *args):\n",
        "#         super().__init__(*args)\n",
        "#         # for layer in self:\n",
        "#         #     params = inspect.signature(layer.forward).parameters.keys()\n",
        "#         #     layer._fwdparams = ','.join(params)\n",
        "\n",
        "#     # def forward(self, x, pos=None, masks=None):\n",
        "#     def forward(self, x, *args, **kwargs):\n",
        "#         # arg_map = {'pos':pos, 'masks':masks}\n",
        "#         for layer in self:\n",
        "#             # args = [x]\n",
        "#             # if 'masks' in layer._fwdparams: args.append(masks)\n",
        "#             # args.extend(arg_map[p] for p in arg_map if p in layer._fwdparams)\n",
        "#             # print(layer._fwdparams, args)\n",
        "#             x = layer(x, *args)\n",
        "#         return x\n",
        "\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        self._kwargs = [[name for name, p in inspect.signature(layer.forward).parameters.items()] for layer in self]\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        for layer, _kwargs in zip(self, self._kwargs):\n",
        "            x = layer(x, *args, **{k: v for k, v in kwargs.items() if k in _kwargs})\n",
        "        return x\n",
        "\n",
        "class SelfAttn(nn.Module):\n",
        "    def __init__(self, dim, n_heads):\n",
        "        super().__init__()\n",
        "        self.dim, self.n_heads = dim, n_heads\n",
        "        d_head = dim//n_heads\n",
        "        self.rope = RoPE(d_head, seq_len=64, top=torch.pi, base=1000)\n",
        "        # self.rope = RoPE(d_head, seq_len=512, base=1000)\n",
        "        # self.rope = RoPE2D(d_head, h=64, w=64, base=100)\n",
        "        # self.rope[0] = 1 # id for smry h\n",
        "        self.qkv = nn.Linear(dim, dim*3, bias=False)\n",
        "        # self.lin = nn.Linear(dim, dim)\n",
        "        self.lin = zero_module(nn.Linear(dim, dim))\n",
        "        self.scale = d_head**-.5\n",
        "\n",
        "    # def forward(self, x): # [b,t,d]\n",
        "    def forward(self, x, pos=None): # [b,t,d]\n",
        "        # print('SelfAttn', x.shape)\n",
        "        q,k,v = self.qkv(x).unflatten(-1, (self.n_heads,-1)).transpose(1,2).chunk(3, dim=-1) # [b, t, n_heads, d_head] -> [b, n_heads, t, d_head]\n",
        "        if pos==None: q, k = self.rope(q), self.rope(k)\n",
        "        else:\n",
        "            # print('SelfAttn fwd', x.shape, pos.shape)\n",
        "            q, k = self.rope(q, pos), self.rope(k, pos)\n",
        "\n",
        "        x = F.scaled_dot_product_attention(q,k,v, attn_mask=None) # https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html\n",
        "        # q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2)\n",
        "        # context = k.transpose(-2,-1) @ v # [batch, n_heads, d_head, d_head]\n",
        "        # x = q @ context # [b,n_heads,t,d_head]\n",
        "        x = x.transpose(1,2).flatten(2)\n",
        "        return self.lin(x)\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, mult=4, drop=0.):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        # self.norm = nn.RMSNorm(d_model, elementwise_affine=False) # LayerNorm RMSNorm\n",
        "        self.norm1, self.norm2 = nn.LayerNorm(d_model), nn.LayerNorm(d_model)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "        self.attn = SelfAttn(d_model, n_heads)\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        ff_dim=d_model*mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), act, nn.Dropout(drop), nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "            nn.RMSNorm(ff_dim), act, nn.Dropout(drop), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(drop), nn.Linear(d_model, ff_dim), act,\n",
        "            # nn.RMSNorm(ff_dim), nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "\n",
        "    # def forward(self, x): # [b,t,d]\n",
        "    def forward(self, x, pos=None): # [b,t,d]\n",
        "        # print('attnblk fwd',x.shape)\n",
        "        # x = x + self.drop(self.attn(self.norm1(x)))\n",
        "        x = x + self.drop(self.attn(self.norm1(x), pos))\n",
        "        x = x + self.ff(x)\n",
        "        # x = x + self.drop(self.ff(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, patch_size, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        patch_size=2\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Conv2d(in_dim, d_model, patch_size, patch_size), # like patch\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 1, 7//2, bias=False)\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.MaxPool2d(3,2,1)\n",
        "            nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.BatchNorm2d(d_model), nn.ReLU(),\n",
        "            nn.Conv2d(d_model, d_model, 3, 2, 3//2, bias=False)\n",
        "            )\n",
        "        # self.embed.requires_grad=False\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, 32*32, d_model)*.02)\n",
        "        # self.pos_emb = nn.Parameter(RoPE2D(dim=d_model, h=8, w=8, base=1000), requires_grad=False)\n",
        "\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=(32//patch_size)**2, base=10000), requires_grad=False)\n",
        "        # self.transformer = nn.Sequential(*[AttentionBlock(d_model, d_model, n_heads) for _ in range(nlayers)])\n",
        "        self.transformer = Seq(*[AttentionBlock(d_model, n_heads) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, cxt_inds=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x).flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c] # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        # x = self.pos_enc(x)\n",
        "        # print(\"TransformerModel\",x.shape, self.pos_emb.shape)\n",
        "        # print(\"TransformerModel\",x.shape)\n",
        "        # x = x + self.pos_emb[:,:x.shape[1]]\n",
        "        # if cxt_inds != None: x = x[torch.arange(x.shape[0]).unsqueeze(-1), cxt_inds] # [batch, num_context_toks, d_model]\n",
        "        # x = self.transformer(x)\n",
        "\n",
        "        if cxt_inds != None:\n",
        "            # print('vit transformer',x[torch.arange(x.shape[0]).unsqueeze(-1), cxt_inds].shape, cxt_inds.shape)\n",
        "            x = self.transformer(x[torch.arange(x.shape[0]).unsqueeze(-1), cxt_inds], cxt_inds)\n",
        "        else: x = self.transformer(x)\n",
        "\n",
        "        out = self.norm(x)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out\n",
        "\n",
        "d_model = 64\n",
        "in_dim = 3\n",
        "patch_size = 2\n",
        "# model = ViT(patch_size, in_dim, d_model, n_heads=4, nlayers=4, drop=0.).to(device)\n",
        "model = ViT(patch_size, in_dim, d_model, n_heads=4, nlayers=1, drop=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x = torch.rand((5, in_dim, 32, 32), device=device) # [b,c,h,w]\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "OgZ6d59vOQil"
      },
      "outputs": [],
      "source": [
        "# @title test seq\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        # for layer in self:\n",
        "        #     params = inspect.signature(layer.forward).parameters.keys()\n",
        "        #     layer._fwdparams = ','.join(params)\n",
        "\n",
        "    # def forward(self, x, pos=None, masks=None):\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        # arg_map = {'pos':pos, 'masks':masks}\n",
        "        for layer in self:\n",
        "            # args = [x]\n",
        "            # if 'masks' in layer._fwdparams: args.append(masks)\n",
        "            # args.extend(arg_map[p] for p in arg_map if p in layer._fwdparams)\n",
        "            # print(layer._fwdparams, args)\n",
        "            x = layer(x, *args, **kwargs)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        self._accepted_kwargs = []\n",
        "        for layer in self:\n",
        "            sig = inspect.signature(layer.forward)\n",
        "            print(sig.parameters.items())\n",
        "            self._accepted_kwargs.append(\n",
        "                [name for name, p in sig.parameters.items()]\n",
        "                # {name for name, p in sig.parameters.items()\n",
        "                #  if p.kind in (p.POSITIONAL_OR_KEYWORD, p.KEYWORD_ONLY)}\n",
        "            )\n",
        "\n",
        "            # if any(p.kind == p.VAR_KEYWORD for p in sig.parameters.values()):\n",
        "            #     accepted = None  # means \"pass everything\"\n",
        "\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        # for layer in self:\n",
        "        #     x = layer(x, *args, **kwargs)\n",
        "\n",
        "        # for layer in self:\n",
        "        #     sig = inspect.signature(layer.forward)\n",
        "        #     # keep only accepted keyword arguments\n",
        "        #     filtered_kwargs = {\n",
        "        #         k: v for k, v in kwargs.items()\n",
        "        #         if k in sig.parameters\n",
        "        #     }\n",
        "        #     x = layer(x, *args, **filtered_kwargs)\n",
        "\n",
        "        for layer, accepted in zip(self, self._accepted_kwargs):\n",
        "            filtered_kwargs = {k: v for k, v in kwargs.items() if k in accepted}\n",
        "            x = layer(x, *args, **filtered_kwargs)\n",
        "        # if accepted is None:\n",
        "        #     x = layer(x, *args, **kwargs)\n",
        "        # else:\n",
        "        #     filtered = {k: v for k, v in kwargs.items() if k in accepted}\n",
        "        #     x = layer(x, *args, **filtered)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        self._kwargs = [[name for name, p in inspect.signature(layer.forward).parameters.items()] for layer in self]\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        for layer, _kwargs in zip(self, self._kwargs):\n",
        "            x = layer(x, *args, **{k: v for k, v in kwargs.items() if k in _kwargs})\n",
        "        return x\n",
        "\n",
        "\n",
        "class SS(nn.Module):\n",
        "    def __init__(self, d):\n",
        "        super().__init__()\n",
        "        self.d = d\n",
        "    def forward(self, x, y, pos=None, bos=None):\n",
        "        # print('SS', x, pos, bos)\n",
        "        print('SS', x, y, pos, bos)\n",
        "        return x+1\n",
        "\n",
        "class PP(nn.Module):\n",
        "    def __init__(self, d):\n",
        "        super().__init__()\n",
        "        self.d = d\n",
        "    def forward(self, x, pos=None, dos=None):\n",
        "        print('PP', x, pos, dos)\n",
        "    # def forward(self, x, y, pos=None, dos=None):\n",
        "        # print('PP', x, y, pos, dos)\n",
        "        return x+1\n",
        "\n",
        "d=2\n",
        "# TT = Seq(*[SS(d) for _ in range(2)])\n",
        "TT = Seq(*[SS(d), PP(d)])\n",
        "# out = TT(3, a=2, pos='p')\n",
        "# out = TT(3, a=2, bos='b')\n",
        "# out = TT(3, a=2, dos='d')\n",
        "out = TT(3, 'q','w')\n",
        "# out = TT(3, 'q','w', pos=2, dos='d')\n",
        "# out = TT(3, 'q', pos=2, dos='d')\n",
        "print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4J2ahp3wmqcg"
      },
      "outputs": [],
      "source": [
        "# @title supervised train test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "# def strain(model, dataloader, optim, scheduler=None):\n",
        "def strain(model, classifier, dataloader, optim, coptim, scheduler=None):\n",
        "    model.train()\n",
        "    classifier.train()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x = x.to(device)#.to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "        y = y.to(device)\n",
        "\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        # print(loss)\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # for p in list(filter(lambda p: p.grad is not None, model.parameters())):\n",
        "        #     print(p.grad.data.norm(2).item())\n",
        "        # print(\"max grad norm\", max([p.grad.data.norm(2).item() for p in list(filter(lambda p: p.grad is not None, model.parameters()))]))\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 10) # 0.5\n",
        "\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        print(\"strain\",loss.item())\n",
        "        # for param in ijepa.student.cls: print(param.data)\n",
        "        # for param in ijepa.predicter.cls: print(param.data)\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=50: break\n",
        "\n",
        "\n",
        "# def test(model, dataloader):\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        # .to(torch.bfloat16)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "        loss = F.cross_entropy(y_, y)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        print(correct/len(y))\n",
        "        try: wandb.log({\"correct\": correct/len(y), \"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "for i in range(1000):\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # np.random.shuffle(train_indices); np.random.shuffle(val_indices)\n",
        "    # train_sampler, valid_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(val_indices)\n",
        "    # # batch_size = 64 #512\n",
        "    # train_loader = DataLoader(train_data, sampler=train_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "    # test_loader = DataLoader(train_data, sampler=valid_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True)\n",
        "\n",
        "    # strain(ijepa, train_loader, optim)\n",
        "    strain(ijepa, classifier, train_loader, optim, coptim)\n",
        "    test(ijepa, classifier, test_loader)\n",
        "\n",
        "    # strain(violet, train_loader, voptim)\n",
        "    # test(violet, test_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e1Yi9JrLPLd"
      },
      "source": [
        "## trash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KUvHrTi4LSEe"
      },
      "outputs": [],
      "source": [
        "# @title GoldenGateRoPE2d\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class GoldenGateRoPE2d(nn.Module): # jerryxio.ng/posts/nd-rope\n",
        "    def __init__(self, image_size: tuple[int, int], n_heads, n_freqs):\n",
        "        super().__init__()\n",
        "        n_zero_freqs = 1\n",
        "        min_freq, max_freq = .2, 20\n",
        "        speed = torch.cat([torch.zeros(n_zero_freqs), min_freq * (max_freq/min_freq) ** torch.linspace(0,1,n_freqs-n_zero_freqs)]) # [n_freqs]\n",
        "        direction_spacing = math.pi * (math.sqrt(5)-1)/2\n",
        "        phi = torch.arange(n_heads * n_freqs).reshape(n_heads, n_freqs) * direction_spacing # [n_heads, n_freqs]\n",
        "        directions = torch.stack((torch.cos(phi), torch.sin(phi)), dim=-1) # [n_heads, n_freqs, 2]\n",
        "        # print('directions', directions)\n",
        "        vel = speed.unsqueeze(-1) * directions # speed in direction[n_heads, n_freqs, 2]\n",
        "\n",
        "        H, W = image_size\n",
        "        xlim, ylim = math.sqrt(W / H), math.sqrt(H / W)\n",
        "        print(xlim, ylim)\n",
        "        y, x = torch.meshgrid(torch.linspace(-ylim, ylim, H), torch.linspace(-xlim, xlim, W), indexing=\"ij\") # [h,w], y:row_num, x:col_num\n",
        "        # y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\") # print(y,x) # [h,w], y:row_num, x:col_num\n",
        "\n",
        "        pos = torch.stack((x, y), dim=-1).reshape(H, W, 1, 1, 2) # cartesian coords\n",
        "\n",
        "        theta = (vel*pos).sum(dim=-1) # [h,w,n_heads,n_freqs,2]->[h,w,n_heads,d_head]\n",
        "        print('theta', theta.shape)\n",
        "        self.cos, self.sin = torch.cos(theta), torch.sin(theta)\n",
        "        print('self.cos', self.cos.shape)\n",
        "\n",
        "    def forward(self, input): # [b,h,w,n_head,d_head]\n",
        "        # x, y = input.float().chunk(2, dim=-1) # [b,h,w,n_head,n_freqs]\n",
        "        x, y = input.float().unflatten(-1, (-1,2)).chunk(2, dim=-1)\n",
        "        x, y = x.squeeze(-1), y.squeeze(-1)\n",
        "        x_out = x * self.cos - y * self.sin\n",
        "        y_out = x * self.sin + y * self.cos\n",
        "        # output = torch.cat((x_out, y_out), dim=-1)\n",
        "        output = torch.stack((x_out, y_out), dim=-1).flatten(-2)\n",
        "        return output.type_as(input)\n",
        "\n",
        "\n",
        "\n",
        "class GoldenGateRoPE2d2(nn.Module): # jerryxio.ng/posts/nd-rope\n",
        "    def __init__(self, image_size, n_heads, n_freqs):\n",
        "        super().__init__()\n",
        "        n_zero_freqs = 1\n",
        "        min_freq, max_freq = .2, 20\n",
        "        intv = math.pi * (math.sqrt(5)-1)/2\n",
        "        speed = torch.cat([torch.zeros(n_zero_freqs), min_freq * (max_freq/min_freq) ** torch.linspace(0,1,n_freqs-n_zero_freqs)]) # [n_freqs]\n",
        "        phi = torch.arange(n_heads*n_freqs).reshape(n_heads, n_freqs) * intv # [n_heads, n_freqs]\n",
        "        direction = torch.stack((torch.cos(phi), torch.sin(phi)), dim=-1) # [n_heads, n_freqs, 2]\n",
        "        vel = speed.unsqueeze(-1) * direction # speed in direction[n_heads, n_freqs, 2]\n",
        "        h, w = image_size\n",
        "        xlim, ylim = math.sqrt(w/h), math.sqrt(h/w)\n",
        "        y, x = torch.meshgrid(torch.linspace(-ylim, ylim, h), torch.linspace(-xlim, xlim, w), indexing=\"ij\") # [h,w], y:row_num, x:col_num\n",
        "        pos = torch.stack([x, y], dim=-1)[...,None,None,:] # [h,w,1,1,2] cartesian coords\n",
        "        theta = (vel*pos).sum(dim=-1) # [h,w,n_heads,n_freqs,2]->[h,w,n_heads,d_head]\n",
        "        cos, sin = torch.cos(theta), torch.sin(theta)\n",
        "        self.affine = torch.stack([cos, -sin, sin, cos], dim=-1).unflatten(-1, (2,2))\n",
        "\n",
        "    def forward(self, input): # [b,h,w,n_head,d_head]\n",
        "        return (self.affine @ input.unflatten(-1, (-1,2)).unsqueeze(-1)).flatten(-3)\n",
        "\n",
        "\n",
        "\n",
        "image_size=(5,7)\n",
        "n_heads=4\n",
        "d_head=16\n",
        "ggrope = GoldenGateRoPE2d(image_size, n_heads, d_head//2)\n",
        "ggrope2 = GoldenGateRoPE2d2(image_size, n_heads, d_head//2)\n",
        "\n",
        "# x = torch.rand(2, *image_size, n_heads, d_head)\n",
        "out = ggrope(x)\n",
        "out2 = ggrope2(x)\n",
        "print(out.shape)\n",
        "print(out2.shape)\n",
        "# print(out[0])\n",
        "# print(out2[0])\n",
        "# print((out==out2)[0])\n",
        "print((out==out2).all())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "K6saOuAX-FV8"
      },
      "outputs": [],
      "source": [
        "# @title test gather\n",
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "l=56\n",
        "b,t,d = 2,50,64\n",
        "src = torch.rand(b,l,d, device=device)\n",
        "idx = torch.randint(0,l,(b,t), device=device)\n",
        "out = src[torch.arange(b).unsqueeze(-1), idx]\n",
        "# out = torch.take_along_dim(src, idx.unsqueeze(-1).expand(-1,-1,d), dim=1)\n",
        "# out = src.index_select(0, idx).reshape(b, t, d) # == src[idx]\n",
        "# out = src.gather(1, idx.unsqueeze(-1).expand(-1, -1, d))\n",
        "print(out.shape)\n",
        "\n",
        "# %timeit out = src.expand(b,-1,-1).gather(1, idx.unsqueeze(-1).expand(-1, -1, src.size(-1))) # 2.53 ms\n",
        "# %timeit out = src.expand(b,-1,-1)[torch.arange(b, device=device).unsqueeze(-1), idx] # 1.39 ms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNt7SRjj9g_l"
      },
      "outputs": [],
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "1e1Yi9JrLPLd"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}