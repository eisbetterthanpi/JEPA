{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/I_JEPA_down.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "LxACli7GdyGq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6308ff0-a49c-4af4-b360-4158cb6b3426"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:19<00:00, 8.93MB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title data\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "transform = transforms.Compose([transforms.ToTensor(),])\n",
        "# transform = transforms.Compose([transforms.RandomResizedCrop((32,32), scale=(.3,1.)), transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "# CIFAR10: 60000 32x32 color images in 10 classes, with 6000 images per class\n",
        "train_data = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
        "test_data = torchvision.datasets.CIFAR10(root='data', train=False, download=True, transform=transform)\n",
        "batch_size = 64 # 4\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# dataiter = iter(train_loader) # get some random training images\n",
        "# images, labels = next(dataiter)\n",
        "# print(images.shape) # [batch, 3, 32, 32]\n",
        "# imshow(torchvision.utils.make_grid(images))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3XSZZyUPY1i"
      },
      "source": [
        "## vit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GD7ezZzmhTHU",
        "collapsed": true,
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "24d7aaf0-312f-4ee9-8af9-a8f594e55b5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0..1.0].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 4, 64, 64])\n",
            "(3, 32, 64)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title RoPE\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def RoPE(dim, seq_len=512, top=torch.pi, base=1000):\n",
        "    theta = top / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "    pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "    angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim//2] -> [1, seq_len, dim//2, 1]\n",
        "    rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [1, seq_len, dim//2, 2] -> [1, seq_len, dim]\n",
        "    return rot_emb\n",
        "\n",
        "\n",
        "# class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "#     def __init__(self, dim, seq_len=512, top=torch.pi, base=1000):\n",
        "#         super().__init__()\n",
        "#         self.dim, self.base = dim, base\n",
        "#         theta = top / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "#         pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "#         angles = (pos * theta)[None,None,...,None] # [seq_len, 1] * [dim//2] -> [1, 1, seq_len, dim//2, 1]\n",
        "#         self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [1, 1, seq_len, dim//2, 2] -> [1, 1, seq_len, dim]\n",
        "\n",
        "#     def forward(self, x): # [b,h,t,d]\n",
        "#         seq_len = x.size(-2)\n",
        "#         if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.base)\n",
        "#         return x * self.rot_emb#[:,:,:seq_len]\n",
        "\n",
        "\n",
        "# @title proper rope\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class RoPE(nn.Module):\n",
        "    def __init__(self, dim, seq_len=512, top=torch.pi, base=1000):\n",
        "    # def __init__(self, dim, seq_len=512, min_freq=1, max_freq=400, n_zero_freqs=0):\n",
        "        super().__init__()\n",
        "        speed = top / (base ** (torch.arange(0, dim, step=2) / dim)) # [dim//2]\n",
        "        pos = torch.arange(seq_len).unsqueeze(-1) # [t,1]\n",
        "        # speed = torch.cat([torch.zeros(n_zero_freqs), min_freq * (max_freq/min_freq) ** torch.linspace(0,1,dim//2-n_zero_freqs)]) # [dim//2]\n",
        "        # pos = torch.linspace(0, 1, seq_len).unsqueeze(-1) # [t,1]\n",
        "\n",
        "        theta = (speed*pos) # [t,1]*[dim//2]=[t,d//2]\n",
        "        self.theta = theta\n",
        "        cos, sin = torch.cos(theta), torch.sin(theta)\n",
        "        self.affine = torch.stack([cos, -sin, sin, cos], dim=-1).unflatten(-1,(2,2)).to(device) # [t,d//2,4]-> [1,1,t,d//2,2,2]\n",
        "\n",
        "    def forward(self, x): # [b,h,t,d]\n",
        "        return (self.affine @ x.unflatten(-1, (-1,2)).unsqueeze(-1)).flatten(-3) # [1,1,t,d//2,2,2] @ [b,h,t,d//2,2,1] = [b,h,t,d]\n",
        "\n",
        "dim=64\n",
        "n_heads=4\n",
        "seq_len=64\n",
        "rope = RoPE(dim, seq_len, top=torch.pi, base=100)\n",
        "# rope = RoPE(dim, seq_len, min_freq=1, max_freq=200, n_zero_freqs=0)\n",
        "\n",
        "x = torch.rand(2, n_heads, seq_len, dim, device=device)\n",
        "out = rope(x)\n",
        "print(out.shape)\n",
        "\n",
        "theta = rope.theta # [t,d//2]\n",
        "sim = torch.cos(theta-theta[0].unsqueeze(0)).T\n",
        "# print(sim.shape)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    print(npimg.shape)\n",
        "    plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "import torchvision\n",
        "imshow(torchvision.utils.make_grid(sim, nrow=dim//2))\n",
        "\n",
        "\n",
        "\n",
        "class RotEmb(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, top=torch.pi, base=1000):\n",
        "        super().__init__()\n",
        "        self.theta = top / (base ** (torch.arange(0, dim, step=2, device=device) / dim))\n",
        "        # self.theta = top / (base ** torch.linspace(0, 1, dim//2, device=device))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (pos.unsqueeze(-1) * self.theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [seq_len, dim]\n",
        "\n",
        "# emb = RotEmb(dim, top=torch.pi, base=1000)\n",
        "\n",
        "# theta = emb.theta\n",
        "# print(theta)\n",
        "# pos = torch.arange(0,200,1)\n",
        "# angles = (pos.unsqueeze(-1) * theta).T # [b,t]\n",
        "# sim = torch.cos(angles-angles[:,0].unsqueeze(-1))\n",
        "# print(sim.shape)\n",
        "\n",
        "# imshow(torchvision.utils.make_grid(sim, nrow=dim//2))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "f6T4F651kmGh",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title AttentionBlock\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class SelfAttn(nn.Module):\n",
        "    def __init__(self, dim, n_heads):\n",
        "        super().__init__()\n",
        "        self.dim, self.n_heads = dim, n_heads\n",
        "        d_head = dim//n_heads\n",
        "        self.rope = RoPE(d_head, seq_len=64, top=torch.pi, base=1000)\n",
        "        # self.rope = RoPE(d_head, seq_len=512, base=1000)\n",
        "        # self.rope = RoPE2D(d_head, h=64, w=64, base=100)\n",
        "        # self.rope[0] = 1 # id for smry h\n",
        "        self.qkv = nn.Linear(dim, dim*3, bias=False)\n",
        "        # self.lin = nn.Linear(dim, dim)\n",
        "        self.lin = zero_module(nn.Linear(dim, dim))\n",
        "\n",
        "        self.scale = d_head**-.5\n",
        "\n",
        "    # def forward(self, x): # [b,t,d]\n",
        "    def forward(self, x, pos=None): # [b,t,d]\n",
        "        q,k,v = self.qkv(x).unflatten(-1, (self.n_heads,-1)).transpose(1,2).chunk(3, dim=-1) # [b, t, n_heads, d_head] -> [b, n_heads, t, d_head]\n",
        "        # q, k = self.rope(q), self.rope(k)\n",
        "        if pos != None:\n",
        "            # rope = self.rope[pos]\n",
        "            # q, k = q*rope, k*rope\n",
        "            q, k = self.rope(q, pos), self.rope(k, pos)\n",
        "\n",
        "        # x = F.scaled_dot_product_attention(q,k,v, attn_mask=None) # https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html\n",
        "        q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2)\n",
        "        context = k.transpose(-2,-1) @ v # [batch, n_heads, d_head, d_head]\n",
        "        x = q @ context # [b, n_heads, t, d_head]\n",
        "        # # print('SelfAttn', x.shape)\n",
        "        x = x.transpose(1,2).flatten(2)\n",
        "        return self.lin(x)\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, mult=4, drop=0.):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        # self.norm = nn.RMSNorm(d_model, elementwise_affine=False) # LayerNorm RMSNorm\n",
        "        self.norm1, self.norm2 = nn.LayerNorm(d_model), nn.LayerNorm(d_model)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "        self.attn = SelfAttn(d_model, n_heads)\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        ff_dim=d_model*mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), act, nn.Dropout(drop), nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "            nn.RMSNorm(ff_dim), act, nn.Dropout(drop), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(drop), nn.Linear(d_model, ff_dim), act,\n",
        "            # nn.RMSNorm(ff_dim), nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "\n",
        "    # def forward(self, x): # [b,t,d]\n",
        "    def forward(self, x, pos=None): # [b,t,d]\n",
        "        # print('attnblk fwd',x.shape)\n",
        "        # x = x + self.drop(self.attn(self.norm1(x)))\n",
        "        x = x + self.drop(self.attn(self.norm1(x), pos))\n",
        "        x = x + self.ff(x)\n",
        "        # x = x + self.drop(self.ff(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "# class ViT(nn.Module):\n",
        "#     # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "#     def __init__(self, patch_size, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "#         super().__init__()\n",
        "#         patch_size=2\n",
        "#         self.embed = nn.Sequential(\n",
        "#             # nn.Conv2d(in_dim, d_model, patch_size, patch_size), # like patch\n",
        "#             # nn.Conv2d(in_dim, d_model, 7, 1, 7//2, bias=False)\n",
        "#             # nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.MaxPool2d(3,2,1)\n",
        "#             nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.BatchNorm2d(d_model), nn.ReLU(),\n",
        "#             nn.Conv2d(d_model, d_model, 3, 2, 3//2, bias=False)\n",
        "#             )\n",
        "#         # self.embed.requires_grad=False\n",
        "#         # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "#         self.pos_emb = nn.Parameter(torch.randn(1, 32*32, d_model)*.02)\n",
        "#         # self.pos_emb = nn.Parameter(RoPE2D(dim=d_model, h=8, w=8, base=1000), requires_grad=False)\n",
        "\n",
        "#         # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=(32//patch_size)**2, base=10000), requires_grad=False)\n",
        "#         self.transformer = nn.Sequential(*[AttentionBlock(d_model, d_model, n_heads) for _ in range(nlayers)])\n",
        "#         self.transformer = Seq(*[AttentionBlock(d_model, d_model, n_heads) for _ in range(nlayers)])\n",
        "#         self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "#         self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "#     def forward(self, x, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "#         x = self.embed(x).flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c] # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "#         # x = self.pos_enc(x)\n",
        "#         # print(\"TransformerModel\",x.shape, self.pos_emb.shape)\n",
        "#         # x = x + self.pos_emb[:,:x.shape[1]]\n",
        "#         # if context_indices != None: x = x[torch.arange(x.shape[0]).unsqueeze(-1), context_indices] # [batch, num_context_toks, d_model]\n",
        "#         # x = self.transformer(x)\n",
        "\n",
        "#         x = self.transformer(x, context_indices)\n",
        "#         out = self.norm(x)\n",
        "#         if self.lin: out = self.lin(out)\n",
        "#         return out\n",
        "\n",
        "\n",
        "# d_model = 64\n",
        "# in_dim = 3\n",
        "# patch_size = 2\n",
        "# # model = ViT(patch_size, in_dim, d_model, n_heads=4, nlayers=4, drop=0.).to(device)\n",
        "# model = ViT(patch_size, in_dim, d_model, n_heads=4, nlayers=1, drop=0.).to(device)\n",
        "# print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "# x = torch.rand((5, in_dim, 32, 32), device=device)\n",
        "# out = model(x)\n",
        "# print(out.shape)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gdtmiB_Zzn5u"
      },
      "outputs": [],
      "source": [
        "# @title ssd me\n",
        "# https://goombalab.github.io/blog/2024/mamba2-part4-systems/\n",
        "# https://tridao.me/blog/2024/mamba2-part3-algorithm/\n",
        "# # https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/triton/ssd_combined.py\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def segsum(x): # [...,c] # Naive segment sum calculation. exp(segsum(A)) produces a 1-SS matrix, which is equivalent to a scalar SSM.\n",
        "    T = x.size(-1)\n",
        "    x_cumsum = torch.cumsum(x, dim=-1)\n",
        "    x_segsum = x_cumsum.unsqueeze(-1) - x_cumsum.unsqueeze(-2) # [...,c,c] # vert-hori\n",
        "    mask = torch.triu(torch.ones(T, T, device=x.device, dtype=bool), diagonal=1)\n",
        "    return x_segsum.masked_fill(mask, -torch.inf) # [...,c,c]\n",
        "\n",
        "# def ssd(X, A, B, C, h0=None, b_ind=None, chunk=64): # X:[b,h,t,d], A:[b,h,t], B/C:[b,h,t,s], h0:[b,h,d,s], b_ind:[b]\n",
        "def ssd(X, A, B, C, h0=None, msk=None, b_ind=None, chunk=64): # X:[b,h,t,d], A:[b,h,t], B/C:[b,h,t,s], h0:[b,h,d,s], msk:[b,t], b_ind:[b]\n",
        "    # print('ssd', X.dtype, A.dtype, B.dtype)\n",
        "    # assert X.dtype == A.dtype == B.dtype == C.dtype\n",
        "    assert X.shape[:-1] == A.shape == B.shape[:-1]\n",
        "    assert h0==None or b_ind==None\n",
        "    # print('ssd', X.shape, A.shape, B.shape)\n",
        "    if b_ind!=None: A[:,:,b_ind[1:-1]] = 0 # at boundaries, A=0\n",
        "    if X.shape[2] % chunk != 0: X, A, B, C = [x.unsqueeze(2) for x in (X, A, B, C)]\n",
        "    else: X, A, B, C = [x.unflatten(2, (-1,chunk)) for x in (X, A, B, C)] # [b,h,t/c,c(,d/s)]\n",
        "\n",
        "    # 1. Compute the output for each intra-chunk (diagonal blocks)\n",
        "    L = torch.exp(segsum(A)) # [b,h,t/c,c,c]\n",
        "    # if msk!=None: L[msk] = 0 # L = L.masked_fill(msk, 0)\n",
        "    if msk!=None: # this only saves computation from ssd. full info leakage, full compute in in_proj\n",
        "        b,h,l,c,c = L.shape\n",
        "        assert msk.shape==(b,l*c)\n",
        "        msk = msk.reshape(b,1,l,c)\n",
        "        msk = msk.unsqueeeze(-2) | msk.unsqueeze(-1) # [b,1,t/c,c,c]\n",
        "        L[msk] = 0 # L = L.masked_fill(msk, 0)\n",
        "    Y_diag  = torch.einsum(\"...cs,...ks,...ck,...kd->...cd\", C, B, L, X) # bhlcs,bhlks,bhlck,bhlkd->bhlcd # full CA...ABX? for chunks\n",
        "\n",
        "    # 2. Compute the state for each intra-chunk # (right term of low-rank factorization of off-diagonal blocks; B terms)\n",
        "    A_cumsum = torch.cumsum(A, dim=-1) # [b,h,t/c,c]\n",
        "    decay_states = torch.exp((A_cumsum[...,-1:] - A_cumsum)) # [b,h,t/c,c] # Ai+1...T\n",
        "    states = torch.einsum(\"...cs,...c,...cd->...ds\", B, decay_states, X) # bhlcs,bhlc,bhlcd->bhlds # BiXiAi+1...T\n",
        "\n",
        "    # 3. Compute the inter-chunk SSM recurrence; produces correct SSM states at chunk boundaries # (middle term of factorization of off-diag blocks; A terms)\n",
        "    if h0==None: h0 = torch.zeros_like(states[:,:,0], device=states.device) # [b,h,d,s]\n",
        "    states = torch.cat([h0.unsqueeze(2), states], dim=2) # [b,1+t/c,h,d,s] # h0,hc,h2c,...,ht\n",
        "    decay_chunk = torch.exp(segsum(F.pad(A_cumsum[...,-1], (1,0)))) # [b,h,1+t/c]-> # [b,h,1+t/c,1+t/c] # 1,A1...1t/c,A1t/c+1...2t/c,...,A(c-1)t/c...At\n",
        "    new_states = torch.einsum(\"...tl,...lds->...tds\", decay_chunk, states) # bhtl,bhlds->bhtds # h0, BiXi/A1...i-1,\n",
        "\n",
        "    # 4. Compute state -> output conversion per chunk # (left term of low-rank factorization of off-diagonal blocks; C terms)\n",
        "    Y_off = torch.einsum('...cs,...ds,...c->...cd', C, new_states[:,:,:-1], torch.exp(A_cumsum)) # bhlcs,bhlds,bhlc->bhlcd # offset for each chunk # C1h0A1, Ci BiXi/A1...i-1,\n",
        "    Y = (Y_diag+Y_off).flatten(2,3)\n",
        "    return Y, new_states[:,:,-1] # [b,t,h,d], [b,h,d,s]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eeu0m-bUSdh",
        "collapsed": true,
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Hydra one BCdt\n",
        "# https://github.com/goombalab/hydra/blob/main/hydra/modules/hydra.py\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "# @torch.compile()\n",
        "class Hydra(nn.Module):\n",
        "    # def __init__(self, d_model, expand=2, n_heads=8, n_groups=8, d_state=8, d_conv=7):\n",
        "    def __init__(self, d_model, expand=3, n_heads=8, n_groups=8, d_state=8, d_conv=7):\n",
        "    # def __init__(self, d_model, expand=4, n_heads=8, n_groups=8, d_state=8, d_conv=7):\n",
        "        super().__init__()\n",
        "        n_groups = min(n_heads, n_groups)\n",
        "        self.d_model, self.n_groups, self.d_state, self.d_conv = d_model, n_groups, d_state, d_conv\n",
        "        self.d_inner = expand * self.d_model\n",
        "        self.n_heads, self.d_head = n_heads, self.d_inner//n_heads\n",
        "\n",
        "        self.in_proj = nn.Linear(self.d_model, 2* self.d_inner + 2*self.n_groups*self.d_state + self.n_heads, bias=False) # z,x,B,C,dt\n",
        "        conv_dim = self.d_inner + 2*self.n_groups*self.d_state # for x,B,C\n",
        "        self.conv1d = nn.Conv1d(conv_dim, conv_dim, kernel_size=d_conv, groups=conv_dim, padding=d_conv//2, bias=True)\n",
        "        # self.conv1d = nn.Conv1d(conv_dim, conv_dim, kernel_size=d_conv, groups=conv_dim, padding=d_conv-1, bias=True)\n",
        "        # self.conv1d.weight._no_weight_decay = True\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "        # self.h0 = nn.Parameter(torch.zeros(self.n_heads, self.d_head, self.d_state))\n",
        "        # self.h0._no_weight_decay = True\n",
        "        dt_min, dt_max = .001, .1\n",
        "        dt = torch.exp(torch.rand(self.n_heads) * (math.log(dt_max)-math.log(dt_min)) + math.log(dt_min)).clamp(min=1e-4)\n",
        "        self.dt_bias = nn.Parameter(dt + torch.log(-torch.expm1(-dt))) # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n",
        "        self.dt_bias._no_weight_decay = True\n",
        "        # A = torch.empty(self.n_heads, dtype=torch.float32).uniform_(1,16) # og\n",
        "        A = torch.empty(self.n_heads).uniform_(1,16)\n",
        "        # A = torch.ones(self.n_heads, dtype=torch.float32)\n",
        "        self.A_log = nn.Parameter(torch.log(A))\n",
        "        self.A_log._no_weight_decay = True\n",
        "\n",
        "        self.D = nn.Linear(self.d_inner, self.n_heads)\n",
        "        # nn.init.constant_(self.D.weight, 1)\n",
        "        self.D.bias._no_weight_decay = True\n",
        "\n",
        "        self.norm = nn.RMSNorm(self.d_inner)\n",
        "        self.out = zero_module(nn.Linear(self.d_inner, self.d_model, bias=False))\n",
        "\n",
        "    # def forward(self, u, h=None): # [b,t,d]\n",
        "    # def forward(self, u, h=None, b_ind=None): # [b,t,d]\n",
        "    def forward(self, u, h=None, ctx_ind=None, b_ind=None): # [b,t,d]\n",
        "        b = u.shape[0]\n",
        "        # print('Hydra u', u.shape)\n",
        "        # print('Hydra u', u.dtype)\n",
        "        A = -torch.exp(self.A_log) # [n_heads]\n",
        "        z, xBC, dt = self.in_proj(u).split([self.d_inner, self.d_inner + 2*self.n_groups*self.d_state, self.n_heads], dim=-1)\n",
        "        dt = torch.cat((dt, torch.flip(dt,(1,))), dim=0) # [b,t,h]->[2b,t,h]\n",
        "        dt = F.softplus(dt+self.dt_bias) # [2b,t,h]+[h]\n",
        "        xBC = self.act(self.conv1d(xBC.transpose(-2,-1)).transpose(-2,-1))  # [b,t, d_inner + 2* n_groups*d_state]\n",
        "        x, BC = xBC.split([self.d_inner, 2*self.n_groups*self.d_state], dim=-1)\n",
        "\n",
        "        x_og = x # [b,t,inr]\n",
        "        x = torch.cat((x, torch.flip(x,(1,))), dim=0).unflatten(-1, (self.n_heads, self.d_head)) # [2b,t,inr]->[2b,t,h,d]\n",
        "        B, C = torch.cat((BC, torch.flip(BC,(1,))), dim=0).chunk(2, dim=-1) # [b,t,2gs]->[2b,t,2gs]->[2b,t,gs]\n",
        "        B, C = B.unflatten(-1, (self.n_groups, self.d_state)), C.unflatten(-1, (self.n_groups, self.d_state)) # x:[2b,t,h,d], B/C:[2b,t,g,s]\n",
        "        h_g = self.n_heads//self.n_groups\n",
        "        if h_g>1: B, C = B.repeat_interleave(h_g, dim=-2), C.repeat_interleave(h_g, dim=-2) # [b,t,g,s]->[b,t,h,s]\n",
        "        x, B, C, dt = [a.transpose(1,2) for a in (x, B, C, dt)] # X:[b,h,t,d], B/C:[b,h,t,s], dt:[b,h,t]\n",
        "\n",
        "        # h0 = self.h0.expand(u.size(0),-1,-1,-1) # [b,n,d,s]\n",
        "        # print('x dt a b', x.shape, dt.shape, A.shape, B.shape)\n",
        "        y, h = ssd(x*dt.unsqueeze(-1), A.unsqueeze(-1)*dt, B, C, h, b_ind=b_ind, chunk=64) # 256\n",
        "        y = y.transpose(1,2).flatten(2) # [2b,t,d]/[1,2b*t,d]\n",
        "\n",
        "        y = torch.roll(y, shifts=1, dims=1) # 123...l -> l12...l-1\n",
        "        y[:,0] = 0 # 012...l-1\n",
        "        y = y[:b] + torch.flip(y[b:], (1,)) + x_og * self.D(x_og).repeat(1,1,self.d_head) # [b,t,h*inr]\n",
        "\n",
        "        y = self.norm(y * self.act(z)) # [b,t,d_inner] # norm(x)*silu(z) if norm_before_gate, else norm(x*silu(z)) # https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/triton/layernorm_gated.py#L18\n",
        "        # y = self.norm(y) * self.act(z)\n",
        "        out = self.out(y)\n",
        "        # return out, h # [b,t,in]\n",
        "        return out # [b,t,in]\n",
        "\n",
        "\n",
        "b,t,d_model=5,256,32\n",
        "# b,t,d_model=5,7,32\n",
        "u = torch.randn(b,t,d_model, device=device)\n",
        "model = Hydra(d_model).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 59850\n",
        "# out, h = model(u)\n",
        "out = model(u)\n",
        "# h0 = torch.randn(b, model.n_heads, model.d_head, model.d_state)\n",
        "# print(out.shape)\n",
        "# print(out.shape, h.shape)\n",
        "# print(out[0,-3:,:5], h[0,:2,:5,:5])\n",
        "# out, h = model(u, h)\n",
        "\n",
        "print(out.shape)\n",
        "# print(out[0,-3:,:5], h[0,:2,:5,:5])\n",
        "\n",
        "# noroll is still valid\n",
        "# increase d_state to d_model*expand//n_groups? = d_inner//n_groups\n",
        "# causal conv by slicing\n",
        "\n",
        "# dt=dt*step then unk&step=1\n",
        "# zoh(step then unk&step=1\n",
        "# x=x+pos_emb then unk+pos_emb (same as trans)\n",
        "# x=rope(x) then rope(unk) (attn pos)\n",
        "# unk[ctx]=x (+pos?) (no drop toks)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ViT\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        self._kwargs = [[name for name, p in inspect.signature(layer.forward).parameters.items()] for layer in self]\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        for layer, _kwargs in zip(self, self._kwargs):\n",
        "            x = layer(x, *args, **{k: v for k, v in kwargs.items() if k in _kwargs})\n",
        "        return x\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, patch_size, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        patch_size=2\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Conv2d(in_dim, d_model, patch_size, patch_size), # like patch\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 1, 7//2, bias=False)\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.MaxPool2d(3,2,1)\n",
        "            nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.BatchNorm2d(d_model), nn.ReLU(),\n",
        "            nn.Conv2d(d_model, d_model, 3, 2, 3//2, bias=False)\n",
        "            )\n",
        "        # self.embed.requires_grad=False\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, 32*32, d_model)*.02)\n",
        "        # self.pos_emb = nn.Parameter(RoPE2D(dim=d_model, h=8, w=8, base=1000), requires_grad=False)\n",
        "\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=(32//patch_size)**2, base=10000), requires_grad=False)\n",
        "        self.transformer = nn.Sequential(*[AttentionBlock(d_model, d_model, n_heads) for _ in range(nlayers)])\n",
        "        # self.transformer = Seq(*[Hydra(d_model, n_heads) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices=None): # [b,ctx,3], [b,ctx] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x).flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c] # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        # x = self.pos_enc(x)\n",
        "        # print(\"TransformerModel\",x.shape, self.pos_emb.shape)\n",
        "        # print(\"TransformerModel\",x.shape)\n",
        "        x = x + self.pos_emb[:,:x.shape[1]]\n",
        "        # if context_indices != None: x = x[torch.arange(x.shape[0]).unsqueeze(-1), context_indices] # [batch, num_context_toks, d_model]\n",
        "        # x = self.transformer(x)\n",
        "\n",
        "        if context_indices != None:\n",
        "            # print('vit fwd', context_indices)\n",
        "            # print('vit transformer',x[torch.arange(x.shape[0]).unsqueeze(-1), context_indices].shape, context_indices.shape)\n",
        "            # x = self.transformer(x[torch.arange(x.shape[0]).unsqueeze(-1), context_indices], context_indices)\n",
        "            x = self.transformer(x[torch.arange(x.shape[0]).unsqueeze(-1), context_indices])\n",
        "        else: x = self.transformer(x)\n",
        "\n",
        "        out = self.norm(x)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out\n",
        "\n",
        "d_model = 64\n",
        "in_dim = 3\n",
        "patch_size = 2\n",
        "# model = ViT(patch_size, in_dim, d_model, n_heads=4, nlayers=4, drop=0.).to(device)\n",
        "model = ViT(patch_size, in_dim, d_model, n_heads=4, nlayers=1, drop=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x = torch.rand((5, in_dim, 32, 32), device=device) # [b,c,h,w]\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ],
      "metadata": {
        "id": "ivqYjBIVrB3e",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9349df78-2ee8-42aa-c23e-0ce24dd6ee18"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "162112\n",
            "torch.Size([5, 64, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pQfM2fYvcTh6",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "8d70ca52-b45c-46c0-bdc3-d10dbfb1506a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([455, 583, 382, 543, 509, 473, 413, 507, 336, 459, 567, 497, 464, 591,\n",
            "        528, 523])\n",
            "tensor([309, 309, 309, 309, 309, 309, 309, 309, 309, 309, 309, 309, 309, 309,\n",
            "        309, 309])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAApsAAADOCAYAAABxTskJAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAM4dJREFUeJzt3XlwHNWdB/Bvz2hmNLrv05It4QMbH/hGNjGX1zIxKVhcWZJlax2KggrYLGA2C07CWak4y1YRKok5kk0ZtnYJhN0FB7w2GOMDbFk+5Es+ZMuXztE9M5Lmnn77h6NJhG6pjzm+nypXWd2t934zb7r1m+53SEIIASIiIiIiFRj0DoCIiIiIoheTTSIiIiJSDZNNIiIiIlINk00iIiIiUg2TTSIiIiJSDZNNIiIiIlINk00iIiIiUg2TTSIiIiJSDZNNIiIiIlINk00iIiIiUo1qyeaWLVswZcoUxMfHY+nSpTh8+LBaVRERERFRmFIl2fzggw+wceNGvPjii6iqqsK8efNQXl6O1tZWNaojIiIiojAlCSGE0oUuXboUixcvxm9+8xsAgCzLKCoqwhNPPIHnnntu2N+VZRlNTU1ITk6GJElKh0ZEREREEySEQHd3NwoKCmAwDH/vMk7pyn0+H44dO4ZNmzaFthkMBqxcuRIVFRUDjvd6vfB6vaGfGxsbMWvWLKXDIiIiIiKF1dfXY9KkScMeo/hj9Pb2dgSDQeTm5vbbnpubC5vNNuD4zZs3IzU1NfSPiSYRERFRZEhOTh7xGN1Ho2/atAkOhyP0r76+Xu+QiIiIiGgURtPlUfHH6FlZWTAajWhpaem3vaWlBXl5eQOOt1gssFgsw5ZZVFSE4uJi9uHE9T4SV65cQVNTk96hEBERRQWr1Yrp06eP6i5dLOju7saFCxfgdrsVKU/xZNNsNmPhwoXYvXs37rvvPgDXB/3s3r0bGzZsGFeZU6ZMwW233cZkE0AwGITf72eySUREpBCr1YpFixahoKBA71DCQlNTE+rr68M32QSAjRs3Yt26dVi0aBGWLFmC119/Hb29vXjooYfGVZ4kSTAYDEw2cf3O5mjeh6ysLKSnp2sQ0dAkSUJ2djbS0tI0qa+rqwvt7e345gQLQgi0trbC6XSOqpzU1FRkZ2fz8/ZnnZ2d6OjoGPaYkpISzJw5U5P3rKenBy0tLQgGg6rXNV4dHR3o7Owc9piMjAxkZmaGfjabzcjPzx/xSY8SgsEgbDYbent7Va9rNIQQaG9vh91uH/a4cLiuhYvRXNckSUJWVpZm1+BwJ8syWltb0d3dPWCfJEmhXENPFosFeXl5mlwHAoEAbDYbXC7XgH1974dSVEk2H3jgAbS1teGFF16AzWbDzTffjJ07dw4YNETqkCQJs2bNwuLFi3WNIy4uDrfddhvmz5+vSX2HDx/G119/DVmW+233+/348ssvUV1dPapyiouLceeddyIuTpXTI+JUVlbiwIEDA5L4v7Zq1Sr85Cc/gclkUj2empoa7Nq1Cx6PR/W6xkMIgQMHDqCysnLY46ZPn45ly5aFLuiZmZn49re/rcl10uVyYefOnbh8+bLqdY1GMBjE/v37UVVVNeQx4XJdCxejua4ZDAbMnTsXN998s3aBhTGv14tdu3ahpqZG71CGlJKSgttvvx3Z2dmq19XT04MdO3bg2rVrqtel2l/TDRs2jPuxOU2c2WxGUlKSrjEYjUZkZGQgNzdX9TteQghkZGQgKSlp0GRzLEmQyWRCYmKiJolTuBNCwGw2j3hcQkICcnNzR3XsRLW3tyMpKWnYLwNmsxmJiYma3p0WQqCnpwder3dUn52+c7QvxuTkZGRnZw/at11pPT09SE1N7XeN0PI9CwQC6OnpCZ2rwWBwTO8Zjf66xvfsL+Li4sL+JoLBYEBCQoImbSbLMoxGo+r1AComm0REesnPz8ett96K+Ph4zep0u9346quvNLlLoIaioiIsW7ZMky8Mzc3N2L9/P3p6elSvi4j0F9PJpiRJMBqNmt39CAaDA+66qcVoNCIuLk6T1ybLclj3n6PYEx8fj4KCAiQmJmpWZ09PD6xWq2b1Kc1qtaKwsFCTBD0QCIzpjorZbIbJZILFYlH9uqbldZooVsR0smmxWDB//nxkZWWpXlcgEMDp06fR0NCgel0GgwHTp0/H6tWrNUk2r1y5grNnz/ICTUSKS0lJwT/+4z9i7ty5KCwsVLVPazAYRHV1Nerq6lSrgygWxXSyaTabMW3aNJSWlqpel9frRWNjoybJJgAUFhZi4cKFmvSVDAQCOHfunKr1EFFsslqtWL16NdasWaN6XT6fDzabjckmkcJiOtnUktFoRGlpKeLj49Hc3IyGhoZhR/cSERERRQPdl6uMFUajEXPmzMHf/M3fYPr06ZzDkYiIiGIC72xqpG8wksFg0H3SWCIiIiKtMOshIiIiItXwzibRGFgsFk3mIQSuD1bwer2a1EXakCQJaWlpsFqtyMzMRHJycqhLTWJiYsw89TAajUhMTEQwGITb7ebUaURRjskm0SgZDAbcdNNNmDVrlib1nT9/HlVVVZxSKookJibi8ccfx4oVK5CWloaMjIzQPrPZHDNrWGdnZ6O8vBwOhwMHDx5EY2Oj3iERkYqYbBKNQWZmJm644QZNppRqb2/nQLIoYzKZMG/ePKxatUrvUHRltVoxefJkOBwOHD9+XO9wJkySpNA/tQkhOJMJRRwmm0RERBOQmpqK2bNna7KClMvlQnV1Ndrb21Wvi0gpTDaJiIgmIDk5GQsWLEB6errqdXV2duLatWtMNimiMNmkCcvKysKcOXMG9C00GAyaLAVKRBQO2O2FaHBMNmlCJEnClClTMGnSpEH3x8XF8QJMREQUw5hs0oQZjUYYjUa9wyAiIqIwFBuTuhERERGRLnhnk2gIBoMBZrM51A3AYDAgLo6nDBHFBkmSYDKZNHty5ff7EQgENKmLtMW/nERDyMjIwJIlS5CYmAjg+oU3JydH56iIiLRhNpuxcOFCFBYWql6XLMs4ceIELl26pHpdpD0mmzqYyATAHGyjnYSEBEydOrXfKi9a++bnhJM5E5FWjEYjioqKMHPmTNXrCgQCqKurw+XLlxUtl9fM8MBkUweTJk3CrbfeOq6TwO/348yZMypEReGmsLAQt956a2hKKa/Xi/Pnz8PhcOgcGRHpxWq14uabb0ZBQQGqqqr0DkcxkiRh6tSpik6M39bWhosXL/LRfBhgsqkxSZJQVFQ05FRBI/H5fNi+fbvCUVG4kSQJkyZN6vf4yuFwwGazMdkkimFWqxULFixAb28v/vu//1vvcBRjMBgwffp0TJs2TbEyz5w5gytXrjDZDANMNnUwkTV0tVp/l/T3zbZmuxNR33XBYDBE1TWh77Uo+Zqi6f2JdJz6iIiIiIhUwzubREREoyRJEhISEhAfHx/alpqaGnELWxgMBiQlJcFkMg15jNVqhdls1jAqZfl8PnR2dsLtdgO43u/d5/PpHFVsYrJJREQ0SgaDAXPnzsVNN90U2mY2m0NTpEWK+Ph4LFu2bNjxAwaDAenp6RpGpazGxkZs37491M9dlmXY7XZ9g4pRTDaJiIjGIC0tDYWFhRHRJzAYDMLv9w/YLssyMjIyxj1YNRK4XC40NTWNKsGUJAlxcXHD3unVgslkiojP1Vgx2SQiIopCsizj7NmzaGtrG7AvKysLK1aswPTp03WILPwkJiZi2bJl/e5Y68FqtSI5OVnXGNTAZJOIiCgKCSHQ0NCAhoaGAfvy8/PhdDp1iCo8WSwWTJ8+HQsWLNA7lKjE0ehEREREpBomm0RERESkGj5Gp7AnhIDdbh+xk3dXV9e418HtWyqtsLAQ06dPR2lpKfLy8nTvLD4YIQQ6OzvR3d2tSV1dXV0jHud0OnH16lVN3q+2trbQEp40tL7PyWCPSj0eT2g6GCKKbEIIOJ3OMf8NdLlc8Hg8Kkb2F0w2KewJIXD+/HlUVlYOeyJ5vd5xJ5smkwn/8A//gHXr1sFsNiM+Ph4mk0nRdXqVEgwGcfLkSZw6dUqT+kbzvtbW1uJPf/qTJnMN+v3+QUfXUn+yLOP06dM4ceLEgPYTQjDZJIoitbW1OHDgAILB4Kh/RwgBl8ulYlR/wWSTIoLH44HD4Rh3MjkSSZKQkZGB4uLisJ12QpZl9Pb2wul0wul0htUa6V6vFw6HI2wmtg4EAuju7oYQAlarNWzi0prb7R71vIJ971kgEIjp94woEvVdg8eSbGppTH02N2/ejMWLFyM5ORk5OTm47777UFNT0+8Yj8eD9evXIzMzE0lJSVi7di1aWloUDZooFrndbnz99df45JNPcPnyZb3DCWvNzc3YsWMHvvjii1F1AyCgoaEB27dvx549ezhKmYgUNaZkc9++fVi/fj0OHTqEXbt2we/3Y9WqVejt7Q0d8/TTT+OTTz7Bhx9+iH379qGpqQn333+/4oErRZZlBIPBqOsDJoRAMBjs90+tu4KkjUAggKamJly9elWT/pqRzOVy4dq1a2hoaIDX69U7nIjQ09ODq1evorGxMayW9BNCIBAIwOfzwe/3D7iuKflPlmVeJ4lUMKbH6Dt37uz38zvvvIOcnBwcO3YMK1asgMPhwO9//3u89957uPPOOwEAW7duxcyZM3Ho0CHccsstykWuAI/Hg6qqKly+fBkzZswI60eoYyGEwMWLF/HFF1+EXk9ycjLmzp2LpKQknaMjIhq9np4evPPOO9i/fz+mTZuGkpIS1eoKBoNobGxUrXyiWDWhPpt9fcYyMjIAAMeOHYPf78fKlStDx9x4440oLi5GRUXFoMmm1+vtd+dBy8c3Pp8PZ8+ehdFoREpKCoqLizWrW01CCNTV1eHQoUOhbfn5+Zg6dSqTTSKKKC6XCx9//DEMBgPuvPNOLF++XO+QiGiMxp1syrKMp556CsuXL8fs2bMBADabDWazGWlpaf2Ozc3Nhc1mG7SczZs34+WXXx5VnU6nE/X19QgEAuMNe1BxcXGYP3++omUSEZFy+HibKHKNO9lcv349qqur8fXXX08ogE2bNmHjxo2hn51OJ4qKigY9tqWlBbt27VJ8yg6z2YzbbrtN0TKJiIiIaJzJ5oYNG/Dpp59i//79mDRpUmh7Xl4efD4f7HZ7v7ubLS0tyMvLG7Qsi8UCi8UybH3d3d1oampCe3s7PB6P4p3X+wbTEBEREUWLvgnf/3og92i0t7cr+hR5TMmmEAJPPPEEPvroI+zdu3dAR+2FCxfCZDJh9+7dWLt2LQCgpqYGdXV1KCsrG3eQ58+fR319PXw+X1iNkiQiIiIKV8FgEMePH8eZM2fG9HuBQAA9PT2KxTGmZHP9+vV47733sG3bNiQnJ4f6YaampsJqtSI1NRUPP/wwNm7ciIyMDKSkpOCJJ55AWVnZhEaiu91urnZBRBFPCIHu7m50dHQgLi5O1eU9g8HguO5MyLIMr9erycoiHo8n6qadI9JDIBCAy+UacD4FAgHY7Xa0t7frFNl1Y0o233zzTQDA7bff3m/71q1b8YMf/AAA8Mtf/hIGgwFr166F1+tFeXk53njjDUWCJSKKZL29vXj77bexfft2zJw5E7Nnz1ZtujUhBFpbW8f8e06nE3v37h2xe5MS/H6/7n8EiaLBpUuXsH379gFL+QohwmJhnTE/Rh9JfHw8tmzZgi1btow7KCKiaOT3+3H48GEcPnwYK1asgNFoDLu5fT0eD1eoIoowHR0dOH/+/IBkM1yMaQUhIiIiIqKxYLJJRERERKqZ0ApCRIPx+/1obm4e81QLw2G/LiIiosjEZJMU53a7cfDgQdTV1SlWpt/v5woiREREEYjJJilOCAGPx6PJ1ClEREQU3thnk4iIiIhUwzubREQUU4LB4LiXKI6Li+PyxkRjxGSTiIhiyuXLl3Hu3Llx9QM3mUyYOXPmhJZgJoo1TDZ1wIEuNJ7PAD83RNfPg4lMhN+3stLJkyfHtVSm2WzWdHaMoc57IUTUXhOi9XXFMiabGhNC4NixYzh48OC4LnSBQADHjx9XITLSihACR44cQUVFxZguqp2dnWhsbFQxMqLwVldXB6PRqEg5kZDQCCFw9OhRHDx4cEC8Xq8XZ8+e1SkydZ0+fRr79+9HIBCYUDnHjx+H1+tVKCqaCCabOti/fz+ef/75cZ9IEz0BSV9CCOzZswcvv/zymPp+CSHY9hSzhBC4ePEiLl26NOGyZFmOiGQTAPbu3YuXXnpp0HM/Wq8HlZWV+OlPfwq32z2hcmRZjtr3KNIw2dRBMBiEz+fjSRDD+j4DHGhA0cLhcKCrq2tcSZwsy3A4HCMeJ4SIuXMmGAzC6/XG1OuWZRk+nw8+n0/vUEghTDaJiGjCLl68iAMHDoyrexAAeDwehSMionDBZJOIdCHLMtxut6p3+C0Wi2Z3hIQQ6O3thdPpHFU/MY/HA4fDMaHBLuHE6XTC6XSOO9mMJF1dXairq1O97YQQsNvtqpQdDAbR2tqq6EpvSuns7IyYbg40Okw2iUgXHo8HBw8eRHNzs2p1lJaWYs2aNSguLlatjj5erxeVlZXYvXs3urq6Rjz+woULaG1tjYpkUwgBh8MRE4mm3+/Hf/7nf2Lv3r2aJJt1dXWqfGGy2+147bXX8O677ype9kQ1NjbyEXqUYbIZI/r6OmnRTzSW+hbRdePpS+fz+dDc3IwrV66oFNX1aWq0Go0aDAZhs9lw9erVUR1vt9tVu2tF6ukbqHTx4kW9Q5kQn8+HkydP6h1G2BBCwO/3K3q9iIuLg8FgiIovlBPFZDNGdHd3o6KiAlarVfW6fD4fOjs7Va+Hwse1a9fwxRdfwGAY/Qq4Pp8PHR0dKkZFRDQ6bW1t+PWvf428vDxFyjMajfjOd76D22+/XZHyIh2TzRjR29uLU6dO6R0GRSmbzQabzaZ3GERE42K32/HHP/5RsfJMJhMmTZrEZPPPmGxS2AoEAqivrx/VlCgjlcM7aERERPpgsklhy+/3o6qqCjU1NRMui/1IiYiI9MFkE38Z8Xf06FFN6qqvr1d9Wgen04nGxsaI7pjs8XjQ09MDv9+vel2yLGv6GWhsbOTUHhrweDyaLenX2dk54bvwRBQd+v7WHzlyRJNZCxoaGsJ6NghJhNlfPKfTidTUVM3rzcnJQXp6uiZ1dXZ2oq2tTdU6EhISNBkMpCYhBHp6ejSZAkOSJGRnZ2v2Gejo6EB7e7smdcUys9mM/Px8xMfHq15XMBhEc3Mzent7Va+LiMJfdnY2MjIyNKlLi7xiKA6HAykpKcMew2STiIiIiMZlNMnm6OcpISIiIiIaIyabRERERKQaJptEREREpBomm0RERESkmoia+igpKQn5+fkwGo2a1903WjkzM1OT+rq6utDS0jLs9Dh2u33EY1JSUpCXlzemZQSjWVdXF1pbWwe8Z5IkaTojQbiTZRnNzc3o7u7WOxSKcUajEfn5+UhKStI1DpPJhIKCAiQkJKhelyzLsNlsg06l5ff70djYCI/HM2I5vK71x+uafiIq2czLy0N5ebkmJ/s3SZKEb33rW1i6dKkmc1dWVVVhz549w05GfvLkSezevRuBQGDIYwoLC7Fq1SpYLBY1wowoQggcO3YMbW1tA5JNg8GAOXPmYMGCBTpFF148Hg8+//xznD9/Xu9QKMZZLBYsXboUN9xwg65xJCcnY82aNSguLla9Lq/Xi88//xznzp0bsK+rqwuffvopmpubRyyH17X+eF3TT0Qlm0ajEVarVZf5Iw0GA1JTU5GZmanJBK2pqamwWq3DJptms3nEWOLi4mC1Wpls/tlw75nJZIr4uUmVIknSmJ4gWK1WJCYmqhhRZOnt7YXb7R72mISEBF2+OIerod4zSZJgsVh0PzcTExORlpamydMtj8eDpKSkQV+z2+0e05Oq8VzX4uLikJKSostTxLEKBAJwOp2jWiVutNe15ORk/s38MyEEuru7JzzfdUQlm0QUnqZOnYqlS5eyuwauP6o7evQoTpw4MexxM2bMwKJFiyJ6lS+lBINBVFZWorq6Wu9QCEB6ejruuOMOpKWl6R3KiNra2rBnzx7Y7XZFyouLi8OCBQswY8YMRcqLdG63G/v27UNdXd2EymGySUQTlpiYiLy8vIi4E6I2WZZH1b8wOTkZ+fn5TDZxPdnknfHwYTKZkJ2djezsbL1DGZEQAnFxyqUykiQhLS0N+fn5ipU5HKPRqNl1U5blYbvdDaa3t1eRu7xMNomIiIg0JkkSZsyYgenTp2vypfPy5cuorq4eVZcDpTHZJCIiIgI0fdIgSRLy8vIwb948TcaC+Hw+nDlzRtV6hsJkk2gIycnJmDp1KuLj41Wvy+12o7a2Fj09PaFt+fn5KC4u1qQfZEdHBy5fvjzmRyxEFBlycnJQUlIyqutJSkqK7gOytJaeno7Vq1ejsLAQM2fORGFhoWp1+f1+XLp0adCpraLVhJLNX/ziF9i0aROefPJJvP766wCuj6J75pln8P7778Pr9aK8vBxvvPEGcnNzlYiXSDNpaWlYvny5JnPUdXR0oK2trV+yWVxcjLvuukuT/jxnz55FXV0dk02iKFVYWIg77rgDJpNpVMfH2mC/rKwsrF+/HosXL4bBYFD1TmNvby+cTieTzdE4cuQI3n77bcydO7ff9qeffhrbt2/Hhx9+iNTUVGzYsAH3338/Dhw4MOFgY0lKSgpKS0sH7VvR0dERUx9SvUiSBIPBoEmyZzQaB1zc+qbp0KJ+DlIhim591zIO4hucJEmIi4uD2WxWva7BrvfRblzJZk9PDx588EH87ne/w89+9rPQdofDgd///vd47733cOeddwIAtm7dipkzZ+LQoUO45ZZblIk6ykmShNLSUhQUFAzYFwwGsX//fhw/flyHyIiIiIjGZlzJ5vr167FmzRqsXLmyX7J57Ngx+P1+rFy5MrTtxhtvRHFxMSoqKgZNNr1eL7xeb+hnp9M5npBUI0kSEhISYDabNZ3k1Ww2D/oNKxAIaPLNiyhSSJIEq9U66seDE+XxePpds4iIaHhjTjbff/99VFVV4ciRIwP22Ww2mM3mARPB5ubmwmazDVre5s2b8fLLL481DM1YLBaUlZVh0qRJXF+WKAyZTCYsWbIEU6ZMUb2uvgnbz549q3pdRETRYkzJZn19PZ588kns2rVLsRG6mzZtwsaNG0M/O51OFBUVKVK2EoxGI3JycjT5Q0axzWg09pucONY66I+XwWBAVlaWJudoMBjEhQsXVK+HBmcwGDQ7L2RZhizLmtRFFO3GlGweO3YMra2tWLBgQWhbXx/C3/zmN/jss8/g8/lgt9v73d1saWlBXl7eoGVaLBauQUoxLzExEUuWLOk3Gj0vLy/mOpETDUWSJEybNg2lpaWa1Hft2jWcP3+eCSeRAsaUbN511104ffp0v20PPfQQbrzxRjz77LMoKiqCyWTC7t27sXbtWgBATU0N6urqUFZWplzURFHGYrFg1qxZA7Yz2SS6TpIkFBUVYcmSJZrVWVNTo1ldRNFsTMlmcnIyZs+e3W9bYmIiMjMzQ9sffvhhbNy4ERkZGUhJScETTzyBsrKyiBuJnpiYiMmTJyM1NRWpqal6hxM1EhMTMWXKFE3uZnu9Xly9ehW9vb2q1zVRTCqJRk+L1VaISDmKryD0y1/+EgaDAWvXru03qXukSU1NxYoVK5CZmcl5yRSUnp6OFStWaDLYqqurC11dXRGRbBIREUWrCSebe/fu7fdzfHw8tmzZgi1btky0aF0ZDAaYTCbNplOJFZIkwWQyaTJ9U1xcHAfZEBFRWDEYDMjMzITL5UJycrLe4WiCa6MTERERacRiseCWW27BwoULkZCQoHc4mmCySUREA0iSBLPZ3G86LrUEAgFN6iEKBwaDASkpKXqHoSme3URENIDJZMLixYs1mffY5/Ph3LlzOHDggOp1EZH2mGzGAEmSNOm7KITgKE6iKGE0GlFQUIAZM2aoXpfX6+UKbURRjMlmlMvNzcW3vvUtJCYmql5XV1cXzp07B4/Ho3pdREREFBmYbEa5nJwclJWVaTJX6JUrV3Dp0iUmm0RERBTCZDMGSJLEScOJiIhIF5yEkIiIiIhUwzubREQxrLe3Fy6Xa8DgvsTERHi9Xp2iIqJowmSTiChGybKM6upqnDhxYsC+tLQ03HbbbZg/f772gRFRVGGySYoxGAywWCywWCxDHmM2m9l/lAjXJzL3er26ng+yLMNut8Nmsw3Y5/P5ONiPiBTBZJMUk5WVhTvvvBN+v3/IYxITE2NmeS6i4dTW1sLpdOqabAoh0NLSolv9RBQbmGySYpKSkjBz5ky9wyCKCK2trWhtbdU7DApTfAJE0YTJJhERURiJi4vDtGnTkJycPGBfU1MTdu3apUNUROPHZJOIiCiMGI1GzJ49GzfddNOAfbW1tUhJSdEhKqLxY7JJ9A0pKSnIyspCbm4uTCaT3uHEJK/Xi5aWlmH7//ZJSEhAb2+vBlFd7+PY3t6OS5cuDXtMV1eXIvVNnjwZ06ZNg8Gg/pTIzc3NOHfuHAKBgOp1RaJAIICmpiYEg8EB++Lj4xW9Xgy3EIfBYOAjdoo4TDaJvmHy5Mm44447EB8fj/j4eL3DiUldXV3YvXv3qJK21NRUlJeXaxDVX6YK2rZt25DHCCHg8/kUqW/16tXYtGkT4uLUv1Rv27YNzz33HLq7u1WvKxK53W4cOHAARqNxwL6CggLcfffdSEtL0z4wogjAZJPoG8xmM5KTk3lXcwSSJCEzMxPJyckoKChAenr6oH+Ix8Pn8yEzM3NU5aWkpAw73ZbSPB6PZglZUlISCgoKNPksTpo0CSUlJaHXlpqaisTERNXrjRSyLMPlcg26z+VyQZZljSOKDsFgEC6Xa9x31Lu7uwe920zhhckmEY2LxWLBww8/jG9/+9tITk5Genq6Yo/3vF4v7rnnnlH9AeobTEETc8stt+CNN94I/eHm+0pacLvd+Oqrr8Y9BZfP50NPT4/CUZHSmGwS0bgYjUbMmDEDK1asUKX86dOnq1IuDS4nJwc5OTl6h0ExJhAIoKWlBdeuXdM7lLAkhIAQQpE753reAWaySURERBSmLl++jAsXLkAIMaFyWltbdevuwWSTiIiIKEw1NTXh6NGjEd0vmMkmkY7sdjuuXbsGo9GIkpISTQZk+P1+1NXVwW63h7YNNaULEUWH9vZ2HD9+fFSD7hISEjBlyhRYrVYNIgsPDocD27ZtQ3V1NYqKipCZmTngGLPZjClTpgw62T4Nj8kmkY7a2tqwZ88eWCwWpKena5ZsVlVV4cKFC6Ftsiwz2SSKYvX19WhqahrVsfn5+cjOzo6pZLO1tRWvvfYa4uPjsWrVKsybN2/AMampqUhNTWWyOQ5MNsOILMuw2+3DTlAdDAY58i6KCCHg9/thNBon3B9nLHUGAoFRTZhORNFBluVRP4YNBAKaXY/ChRACXq8XsizD5/MNen2MxfdFKUw2w0gwGMTx48dx5syZYY8baq43IiIionDDZDMMyLIMr9cLj8cDh8OBzs5OvUMiIiIiUgSTzTDQ29uLiooKtLa2oq2tTe9wiIiIiBQTNclm38SnStFyioG+0cF1dXVj+j32HYkefZP2avG5i+TpM9TU1wbDnVfss0VENHZRk2zW19ejtrZWsT8Es2bNwt133z3o9AdKc7vdOHbsGKqrq8f0ezabjSOIo0TfZ+DixYuq1+X3+9He3q56PZEmEAhg+/btOHz48JDHBINBVFRUaBgVEVHki5pks7GxEQcOHFDsro3f74fb7VakrJG43W6cOnUKBw8e1KQ+Cj9utxsnT55UbG3xkfDu3ECBQAA7d+7Eb3/722GP43tHRDQ2EZVsOp1OnD9/HmazecC+5uZmRf8IdHR0YPv27aiqqlKszKG0tLTEzJ0ml8uFhoYG+Hw+RcoLBoNoamqCw+EYsK+npweHDx/uN+BK6c+J0sI5tligdHccIiKKsGSzpaUFn3/++aB3f5TuS3XlyhW89NJLMBgMipU5FFmWh51bM5p0dnZiz5496OrqUqQ8n8+Hzz77DGfPnh2wr28Oy7++2x0IBNhnkYiISEMRlWz2TRGkhWAwCKfTqUldscTr9aKxsVGxUfc+nw+tra39ll4kIiJSWjAYRGdnJ6xWK5KSkpCQkKB3SBFjzMlmY2Mjnn32WezYsQMulwtTp07F1q1bsWjRIgDX7ya9+OKL+N3vfge73Y7ly5fjzTffxLRp0xQPniJPZ2cnPv/8c1y9elWR8mRZRnd3tyJlERERDcXlcuGrr76C1WrFsmXLMGvWLM362Ue6MSWbXV1dWL58Oe644w7s2LED2dnZuHjxItLT00PHvPrqq/jVr36Fd999FyUlJXj++edRXl6Os2fPIj4+XvEXQJGlbyR0S0uL3qEQkU4CgQBcLleoS4vP59PsqRXRePXd2YyLi+NKfmM0pmTzX//1X1FUVIStW7eGtpWUlIT+L4TA66+/jp/+9Ke49957AQD/8R//gdzcXHz88cf43ve+p1DYREQUqWpqavDWW2+FBkbKsowTJ07oGxQRqWZMyeaf/vQnlJeX47vf/S727duHwsJCPP7443jkkUcAXB9UY7PZsHLlytDvpKamYunSpaioqBg02fR6vf2+0bKfpPJGGmHLxwBEpKXW1lZs27YN9fX1eodCRBoYU7J5+fJlvPnmm9i4cSN+/OMf48iRI/inf/onmM1mrFu3DjabDQCQm5vb7/dyc3ND+75p8+bNePnll8cZPo3k7NmzeP311wd0YZAkCcuXL8eyZcuYbBIREZFqxpRsyrKMRYsW4ec//zkAYP78+aiursZbb72FdevWjSuATZs2YePGjaGfnU4nioqKxlUWDXTy5EmcPn16QEJpMBjwk5/8BGVlZTpFRkRERLFgTMlmfn4+Zs2a1W/bzJkz8T//8z8AgLy8PADX58PMz88PHdPS0oKbb7550DItFgssFstYwqAxEEIMuqSlwWDgfJNEFDGEEOjo6EBtba0m9bW1tXGCf515PB7YbDYEAgFFyvP5fOjp6VGkLBqbMSWby5cvR01NTb9tFy5cwOTJkwFcHyyUl5eH3bt3h5JLp9OJyspKPPbYY8pETEREMUcIgTNnzuDixYua1PfNBSFIex0dHfjiiy8GXSFuPIQQnPVAJ2NKNp9++mksW7YMP//5z/F3f/d3OHz4MH7729+G1hKWJAlPPfUUfvazn2HatGmhqY8KCgpw3333qRE/ERHFCJ/Pp9hSt7HC5XKNecW2pKSkQZ+IqaFvruTBYrTb7eju7tbtbqTH44Hb7R6w3WQyaZq0ejwedHV16fLlx+Vywe/3T7icMSWbixcvxkcffYRNmzbhlVdeQUlJCV5//XU8+OCDoWP+5V/+Bb29vXj00Udht9tx6623YufOnZxjk4iISEPBYBCnTp3C5cuXx/R7c+fO1ewGUd9E6ZWVlQP2eb1eXZdyrq2txdGjRwckefHx8Zg3bx6WL1+uegxCCNTU1ODjjz/W7AvAX5NlGR0dHRMuZ8wrCN1zzz245557htwvSRJeeeUVvPLKKxMKjKJLIBCA3++Hx+PhoymNybIMWZbH1P8sEAiwvxpRFOjq6hrznc2cnBxF7maNRjAYREtLS1hNgxUMBuH3++FwOFBfXz9osqnl3Van04n6+npdkk2lRNTa6BS5Kisr8cEHH6C+vl6Rb0k0ek1NTaiurh7ThSoQCKC5uVnFqIiIwk8wGER1dTVsNhtsNhu/dCuEySZp4ty5c/j3f//3Qfu/kLo6Ojpw/Phx9nUjIhqBLMu4evUqrl69qncoUYXJZowSQuDo0aN4++23NZnU/cCBA4pNX6EGIQTq6upgMBiQmZmp2aN+u92OkydPwm63q1ZHQ0NDRD9+IQp3PT09OH36NBISElSvq6GhQdd+jETjwWQzRgkh8Nlnn2H37t2a1NfXByZcybKMc+fOoaamBpMmTdIsOWtra8OePXvQ2tqqWh19fTaJSB12ux1fffWVJnV1dnaq+uWUSA1MNmNYIBAI67uNWutLypqamlBRUaHJYgNnzpyB2+1mO9CgGhsbcfDgQRiNxgmVk5ubi9LS0gmXo4dgMIi2tjYu/vFnTqdT9Wl3enp6UFVVpUn/epvNBqfTqXo9SpJlGZcuXcKBAwdUr0sIgWvXrkV831Emm0TfsHfvXpw+fRoGg0H1unp7exWbsJiiz44dO1BZWTnhri7f/e538cILL2jymFdpXq8XFRUVMJlMeocSFoLBoOqP0a9cuYLnnnsOZrNZ1XqA65Pnt7W1qV6Pkvx+P95991189NFHqtclhIDdbo/4p1NMNom+obu7G93d3XqHEfb6LoJNTU16h6IIr9cLl8uldxj9OBwORb6MXLt2Dc3NzbBarQpENXEdHR2j/uMphOASgxrz+XxoaGjQO4ywJYRAe3s72tvb9Q4lYkgizO7NOp1OpKam6h0GEY3AaDRi1qxZKCws1DsURfT12w2n+f6UMmnSJMycOTNsHqN3dnbi1KlT8Hg8eodCRBPkcDiQkpIy7DFMNomIiIhoXEaTbKrfKY2IiIiIYhaTTSIiIiJSDZNNIiIiIlINk00iIiIiUk3YJZthNl6JiIiIiIYwmrwt7JJNzm9IREREFBlGk7eF3dRHsiyjpqYGs2bNQn19/YjD6Sl8OZ1OFBUVsR0jGNsw8rENIx/bMPJFYxsKIdDd3Y2CgoIRV9wLuxWEDAZDaJLolJSUqGmUWMZ2jHxsw8jHNox8bMPIF21tONp50cPuMToRERERRQ8mm0RERESkmrBMNi0WC1588UVYLBa9Q6EJYDtGPrZh5GMbRj62YeSL9TYMuwFCRERERBQ9wvLOJhERERFFByabRERERKQaJptEREREpBomm0RERESkmrBMNrds2YIpU6YgPj4eS5cuxeHDh/UOiYbw0ksvQZKkfv9uvPHG0H6Px4P169cjMzMTSUlJWLt2LVpaWnSMmPbv34/vfOc7KCgogCRJ+Pjjj/vtF0LghRdeQH5+PqxWK1auXImLFy/2O6azsxMPPvggUlJSkJaWhocffhg9PT0avorYNlIb/uAHPxhwXq5evbrfMWxDfW3evBmLFy9GcnIycnJycN9996GmpqbfMaO5ftbV1WHNmjVISEhATk4OfvSjHyEQCGj5UmLWaNrw9ttvH3Au/vCHP+x3TCy0Ydglmx988AE2btyIF198EVVVVZg3bx7Ky8vR2tqqd2g0hJtuugnNzc2hf19//XVo39NPP41PPvkEH374Ifbt24empibcf//9OkZLvb29mDdvHrZs2TLo/ldffRW/+tWv8NZbb6GyshKJiYkoLy+Hx+MJHfPggw/izJkz2LVrFz799FPs378fjz76qFYvIeaN1IYAsHr16n7n5R/+8Id++9mG+tq3bx/Wr1+PQ4cOYdeuXfD7/Vi1ahV6e3tDx4x0/QwGg1izZg18Ph8OHjyId999F++88w5eeOEFPV5SzBlNGwLAI4880u9cfPXVV0P7YqYNRZhZsmSJWL9+fejnYDAoCgoKxObNm3WMioby4osvinnz5g26z263C5PJJD788MPQtnPnzgkAoqKiQqMIaTgAxEcffRT6WZZlkZeXJ/7t3/4ttM1utwuLxSL+8Ic/CCGEOHv2rAAgjhw5Ejpmx44dQpIk0djYqFnsdN0321AIIdatWyfuvffeIX+HbRh+WltbBQCxb98+IcTorp//93//JwwGg7DZbKFj3nzzTZGSkiK8Xq+2L4AGtKEQQtx2223iySefHPJ3YqUNw+rOps/nw7Fjx7By5crQNoPBgJUrV6KiokLHyGg4Fy9eREFBAUpLS/Hggw+irq4OAHDs2DH4/f5+7XnjjTeiuLiY7Rmmrly5ApvN1q/NUlNTsXTp0lCbVVRUIC0tDYsWLQods3LlShgMBlRWVmoeMw1u7969yMnJwYwZM/DYY4+ho6MjtI9tGH4cDgcAICMjA8Dorp8VFRWYM2cOcnNzQ8eUl5fD6XTizJkzGkZPwMA27PNf//VfyMrKwuzZs7Fp0ya4XK7Qvlhpwzi9A/hr7e3tCAaD/d50AMjNzcX58+d1ioqGs3TpUrzzzjuYMWMGmpub8fLLL+Nb3/oWqqurYbPZYDabkZaW1u93cnNzYbPZ9AmYhtXXLoOdg337bDYbcnJy+u2Pi4tDRkYG2zVMrF69Gvfffz9KSkpw6dIl/PjHP8bdd9+NiooKGI1GtmGYkWUZTz31FJYvX47Zs2cDwKiunzabbdBztW8faWewNgSAv//7v8fkyZNRUFCAU6dO4dlnn0VNTQ3+93//F0DstGFYJZsUee6+++7Q/+fOnYulS5di8uTJ+OMf/wir1apjZESx63vf+17o/3PmzMHcuXNxww03YO/evbjrrrt0jIwGs379elRXV/fr706RZag2/Ot+0HPmzEF+fj7uuusuXLp0CTfccIPWYeomrB6jZ2VlwWg0Dhht19LSgry8PJ2iorFIS0vD9OnTUVtbi7y8PPh8Ptjt9n7HsD3DV1+7DHcO5uXlDRiwFwgE0NnZyXYNU6WlpcjKykJtbS0AtmE42bBhAz799FPs2bMHkyZNCm0fzfUzLy9v0HO1bx9pY6g2HMzSpUsBoN+5GAttGFbJptlsxsKFC7F79+7QNlmWsXv3bpSVlekYGY1WT08PLl26hPz8fCxcuBAmk6lfe9bU1KCuro7tGaZKSkqQl5fXr82cTicqKytDbVZWVga73Y5jx46Fjvnyyy8hy3LoQkrhpaGhAR0dHcjPzwfANgwHQghs2LABH330Eb788kuUlJT02z+a62dZWRlOnz7d74vDrl27kJKSglmzZmnzQmLYSG04mBMnTgBAv3MxJtpQ7xFK3/T+++8Li8Ui3nnnHXH27Fnx6KOPirS0tH4jtSh8PPPMM2Lv3r3iypUr4sCBA2LlypUiKytLtLa2CiGE+OEPfyiKi4vFl19+KY4ePSrKyspEWVmZzlHHtu7ubnH8+HFx/PhxAUC89tpr4vjx4+LatWtCCCF+8YtfiLS0NLFt2zZx6tQpce+994qSkhLhdrtDZaxevVrMnz9fVFZWiq+//lpMmzZNfP/739frJcWc4dqwu7tb/PM//7OoqKgQV65cEV988YVYsGCBmDZtmvB4PKEy2Ib6euyxx0RqaqrYu3evaG5uDv1zuVyhY0a6fgYCATF79myxatUqceLECbFz506RnZ0tNm3apMdLijkjtWFtba145ZVXxNGjR8WVK1fEtm3bRGlpqVixYkWojFhpw7BLNoUQ4te//rUoLi4WZrNZLFmyRBw6dEjvkGgIDzzwgMjPzxdms1kUFhaKBx54QNTW1ob2u91u8fjjj4v09HSRkJAg/vZv/1Y0NzfrGDHt2bNHABjwb926dUKI69MfPf/88yI3N1dYLBZx1113iZqamn5ldHR0iO9///siKSlJpKSkiIceekh0d3fr8Gpi03Bt6HK5xKpVq0R2drYwmUxi8uTJ4pFHHhnwhZ1tqK/B2g+A2Lp1a+iY0Vw/r169Ku6++25htVpFVlaWeOaZZ4Tf79f41cSmkdqwrq5OrFixQmRkZAiLxSKmTp0qfvSjHwmHw9GvnFhoQ0kIIbS7j0pEREREsSSs+mwSERERUXRhsklEREREqmGySURERESqYbJJRERERKphsklEREREqmGySURERESqYbJJRERERKphsklEREREqmGySURERESqYbJJRERERKphsklEREREqmGySURERESq+X8OzOnSvrGcCAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title masks\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# def multiblock(seq, min_s, max_s, M=1):\n",
        "#     mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "#     mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "#     mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "#     indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "#     target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "#     return target_mask\n",
        "\n",
        "\n",
        "# def multiblock2d(hw=(8,8), scale=(.15,.2), aspect_ratio=(.75,1.5), M=1):\n",
        "#     mask_aspect = torch.rand(1) * (aspect_ratio[1] - aspect_ratio[0]) + aspect_ratio[0] # in (min_s, max_s) # all blocks same size\n",
        "#     mask_scale = torch.rand(1) * (scale[1] - scale[0]) + scale[0] # in (min_s, max_s) # all blocks same size\n",
        "#     h = (mask_scale/mask_aspect)**.5# h*(h*aspect) = scale\n",
        "#     w = h * mask_aspect\n",
        "#     h_pos, w_pos = torch.rand(M)*(1-w), torch.rand(M)*(1-h) # in (0, 1 - mask_len)\n",
        "#     h_len, h_pos = (h*hw[0]).int(), h_pos*hw[0]\n",
        "#     w_len, w_pos = (w*hw[1]).int(), w_pos*hw[1]\n",
        "#     h_ind, w_ind = torch.arange(hw[0]).unsqueeze(0), torch.arange(hw[1]).unsqueeze(0) # [1, seq]\n",
        "#     h_mask = (h_ind>=h_pos.unsqueeze(-1)) & (h_ind<(h_pos+h_len).unsqueeze(-1)) # [M, seq]\n",
        "#     w_mask = (w_ind>=w_pos.unsqueeze(-1)) & (w_ind<(w_pos+w_len).unsqueeze(-1)) # [M, seq]\n",
        "#     target_mask = h_mask.unsqueeze(-1) & w_mask.unsqueeze(-2) # [M, seq, seq]\n",
        "#     return target_mask\n",
        "\n",
        "# # https://arxiv.org/pdf/2210.07224\n",
        "# def randpatch(seq, mask_size=8, gamma=0.9): # num patches of seq, mask patch size, masking ratio\n",
        "#     # mask = torch.rand(seq//mask_size)<gamma\n",
        "#     length = seq//mask_size\n",
        "#     g = torch.normal(gamma, std=.1, size=(1,)).clamp(.5,.9)\n",
        "#     # g = gamma\n",
        "#     idx = torch.randperm(length)[:int(length*g)]\n",
        "#     mask = torch.zeros(length, dtype=bool)\n",
        "#     mask[idx] = True\n",
        "#     mask = mask.repeat_interleave(mask_size, dim=-1)\n",
        "#     return mask # [seq] , True -> mask\n",
        "\n",
        "\n",
        "# import torch\n",
        "# def apply_masks(x, mask): # [b,t,d], [mask_size] # https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "#     mask_keep = mask.unsqueeze(-1).repeat(x.size(0), 1, x.size(-1)) # [batch,T,dim]\n",
        "#     return torch.gather(x, dim=1, index=mask_keep) # [batch,mask_size,dim]\n",
        "\n",
        "\n",
        "# @title ijepa multiblock next\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "import math\n",
        "from multiprocessing import Value\n",
        "import torch\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(self, hw=(224, 224), enc_mask_scale=(.85,1), pred_mask_scale=(.15,.2), aspect_ratio=(.75,1.25),\n",
        "        nenc=1, npred=2, min_keep=4, allow_overlap=False):\n",
        "        super().__init__()\n",
        "        self.height, self.width = hw\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.aspect_ratio = aspect_ratio\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "\n",
        "    def _sample_block_size(self, scale, aspect_ratio_scale):\n",
        "        _rand = torch.rand(1).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.height * self.width * mask_scale) # num patches to keep\n",
        "        # -- Sample block aspect-ratio\n",
        "        min_ar, max_ar = aspect_ratio_scale\n",
        "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
        "        while h >= self.height: h -= 1 # crop mask to be smaller than img\n",
        "        while w >= self.width: w -= 1\n",
        "        return (h, w)\n",
        "\n",
        "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
        "        h, w = b_size\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            top = torch.randint(0, self.height - h, (1,))\n",
        "            left = torch.randint(0, self.width - w, (1,))\n",
        "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
        "            mask[top:top+h, left:left+w] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
        "        mask_complement[top:top+h, left:left+w] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, B):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        p_size = self._sample_block_size(scale=self.pred_mask_scale, aspect_ratio_scale=self.aspect_ratio)\n",
        "        e_size = self._sample_block_size(scale=self.enc_mask_scale, aspect_ratio_scale=(1., 1.))\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.height * self.width\n",
        "        min_keep_enc = self.height * self.width\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                print(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "        return collated_masks_enc, collated_masks_pred\n",
        "\n",
        "mask_collator = MaskCollator(hw=(32,32), enc_mask_scale=(.85, 1.), pred_mask_scale=(.15, .2), aspect_ratio=(.75, 1.5),\n",
        "        nenc=1, npred=4, min_keep=4,\n",
        "        # allow_overlap=True)\n",
        "        allow_overlap=False)\n",
        "\n",
        "b=16\n",
        "collated_masks_enc, collated_masks_pred = mask_collator(b)\n",
        "ctx_index, trg_index = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "# imshow(mask)\n",
        "\n",
        "mask = torch.zeros(b ,32*32)\n",
        "mask[torch.arange(b).unsqueeze(-1), trg_index] = 1\n",
        "mask[torch.arange(b).unsqueeze(-1), ctx_index] = .5\n",
        "mask = mask.reshape(b,1,32,32)\n",
        "print((mask==1).flatten(1).sum(-1))\n",
        "print((mask==.5).flatten(1).sum(-1))\n",
        "import torchvision\n",
        "imshow(torchvision.utils.make_grid(mask, nrow=8))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Random Fourier Features noise\n",
        "import math\n",
        "import torch\n",
        "\n",
        "# Random Fourier Features: ∑_k] a_k * cos(ω_k ⋅ x + 𝜙_k)\n",
        "def rff_noise(x, out_dim, n_freqs=64, scale=1): # [...,n_Dim]\n",
        "    space, in_dim, device = x.shape[:-1], x.shape[-1], x.device\n",
        "    x = x.flatten(0,-2) # [N,in]\n",
        "    w = torch.randn(out_dim, n_freqs, in_dim, device=device) * scale\n",
        "    phi = torch.empty(out_dim, n_freqs, device=device).uniform_(0,2*math.pi)\n",
        "    a = torch.randn(out_dim, n_freqs, device=device) / math.sqrt(n_freqs)\n",
        "    y = torch.cos(torch.einsum(\"ofd,...d->...of\", w, x) + phi) # [N,out,freq]\n",
        "    return torch.einsum(\"of,...of->...o\", a, y).unflatten(0,space) # [...,out]\n",
        "\n",
        "def rffmask2d(bhw, ctx_scale=(.85,1.), trg_scale=(.6,.8), chaos=[1,.5], sort=False): # ctx_scale > trg_scale\n",
        "# def rffmask2d(bhw, ctx_scale=(.05,.5), trg_scale=(.2,.8), chaos=[1,.5], sort=False): # ctx_scale > trg_scale\n",
        "    b,h,w = bhw\n",
        "    ix, iy = torch.linspace(0, chaos[0], h), torch.linspace(0, chaos[1], w)\n",
        "    x = torch.stack(torch.meshgrid(ix, iy, indexing=\"ij\"), dim=-1) # [t,h,w,b] / [h,w,b]\n",
        "    noise = rff_noise(x, b, n_freqs=16).flatten(0,-2).T # [h,w,b]->[h*w,b]->[b,h*w]\n",
        "    ctx_mask_scale = torch.rand(1) * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    trg_mask_scale = torch.rand(1) * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "    seq = h*w\n",
        "    ctx_len, trg_len = int(seq*ctx_mask_scale), int(seq*trg_mask_scale)\n",
        "    # print(ctx_len, trg_len)\n",
        "    # ctx_len = ctx_len - trg_len\n",
        "    ctx_len = min(ctx_len, seq-trg_len)\n",
        "\n",
        "    _, trg_ind = torch.topk(noise, trg_len, dim=1, sorted=sort)\n",
        "    noise = rff_noise(x, b, n_freqs=16).flatten(0,-2).T\n",
        "    noise.scatter_(1, trg_ind, -10).flatten()\n",
        "    _, ctx_ind = torch.topk(noise, ctx_len, dim=1, sorted=sort)\n",
        "    return ctx_ind, trg_ind\n",
        "\n",
        "b=16\n",
        "hw=(8,8)\n",
        "# cxt_inds, trg_inds = rffmask2d((b,)+hw, ctx_scale=(.85,1.), trg_scale=(.2,.8), chaos=[3,1], sort=True)\n",
        "# cxt_inds, trg_inds = rffmask2d((b,)+hw, ctx_scale=(.85,1.), trg_scale=(.2,.8), chaos=[1,.5], sort=True)\n",
        "cxt_inds, trg_inds = rffmask2d((b,)+hw, ctx_scale=(.05,.5), trg_scale=(.2,.8), chaos=[3,1], sort=True)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "# imshow(mask)\n",
        "\n",
        "mask = torch.zeros(b ,math.prod(hw))\n",
        "# mask = torch.ones(b ,32*32)*.3\n",
        "mask[torch.arange(b).unsqueeze(-1), trg_inds] = 1\n",
        "mask[torch.arange(b).unsqueeze(-1), cxt_inds] = .5\n",
        "# print((mask==1).sum(1))\n",
        "# print((mask==.5).sum(1))\n",
        "mask = mask.reshape(b,1,*hw)\n",
        "import torchvision\n",
        "imshow(torchvision.utils.make_grid(mask, nrow=8))\n"
      ],
      "metadata": {
        "id": "Nd1GqX3PYpIk",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "a2f29a9d-2fdd-4d70-d5d1-41b56c0dfc53"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "M-zdjdJixtOu",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Transformer Predictor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, 32*32, d_model)*0.02)\n",
        "        # self.pos_emb = nn.Parameter(RoPE2D(dim=d_model, h=8, w=8, base=1000), requires_grad=False)\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=32*32, base=10000), requires_grad=False)\n",
        "\n",
        "        # self.transformer = nn.Sequential(*[AttentionBlock(d_model, d_model, n_heads) for _ in range(nlayers)])\n",
        "        self.transformer = Seq(*[AttentionBlock(d_model, n_heads) for _ in range(nlayers)])\n",
        "        # self.transformer = Seq(*[Hydra(d_model, n_heads) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.randn(1,1,d_model)*.02) # randn zeros\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim)# if out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        batch, seq, dim = x.shape\n",
        "        # x = x * self.pos_enc(context_indices)\n",
        "        # x = x + self.pos_emb[0,context_indices]\n",
        "\n",
        "        # # pred_tokens = self.cls * self.pos_enc(trg_indices) # [M, num_trg_toks, d_model]\n",
        "        # pred_tokens = self.cls + self.pos_emb[0,trg_indices]\n",
        "        # # print(\"pred fwd\", x.shape, pred_tokens.shape)\n",
        "        # x = torch.cat([x, pred_tokens], dim=1) # [batch, seq_len+num_trg_toks, d_model]\n",
        "        # out = self.transformer(x)\n",
        "\n",
        "        # x = torch.cat([x, self.cls.expand(*trg_indices.shape,-1)], dim=1) # [batch, num_ctx_toks+num_trg_toks, d_model]\n",
        "        # out = self.transformer(x, torch.cat([context_indices, trg_indices], dim=-1))\n",
        "\n",
        "        x = torch.cat([x, self.cls.expand(*trg_indices.shape,-1)], dim=1) # [batch, num_ctx_toks+num_trg_toks, d_model]\n",
        "        _, idx = torch.sort(torch.cat([context_indices, trg_indices], dim=-1)) # [b,ctx+trg]\n",
        "        x = x[torch.arange(x.shape[0]).unsqueeze(-1), idx]\n",
        "        out = self.transformer(x)\n",
        "\n",
        "        out = self.norm(out)\n",
        "        out = out[:,seq:] # [batch, num_trg_toks, d_model]\n",
        "        out = self.lin(out)\n",
        "        # print(\"pred fwd\", out.shape)\n",
        "        return out # [seq_len, batch_size, ntoken]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "ardu1zJwdHM3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a65d0d12-20d6-487c-a64f-d5ec8660c468"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "245184\n",
            "torch.Size([])\n"
          ]
        }
      ],
      "source": [
        "# @title IJEPA\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "@torch.compile\n",
        "class IJEPA(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=64, out_dim=None, nlayers=1, n_heads=4):\n",
        "        super().__init__()\n",
        "        self.out_dim = out_dim = out_dim or d_model\n",
        "        self.patch_size = 4 # 4\n",
        "        self.student = ViT(self.patch_size, in_dim, d_model, out_dim=out_dim, n_heads=n_heads, nlayers=nlayers, drop=0.)\n",
        "        self.predicter = TransformerPredictor(out_dim, d_model//2, out_dim, n_heads=4, nlayers=1, drop=0.)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "        self.hw=(8,8) # (8,8) (32,32)\n",
        "        # self.mask_collator = MaskCollator(self.hw, enc_mask_scale=(.85, 1.), pred_mask_scale=(.15, .2), aspect_ratio=(.75, 1.5), nenc=1, npred=4, min_keep=4, allow_overlap=False) # og\n",
        "\n",
        "    def loss(self, x): #\n",
        "        b,c,h,w = x.shape\n",
        "        # collated_masks_enc, collated_masks_pred = self.mask_collator(b)\n",
        "        # cxt_inds, trg_inds = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "\n",
        "        cxt_inds, trg_inds = rffmask2d((1,)+self.hw, ctx_scale=(.85,1.), trg_scale=(.2,.8), chaos=[3,1], sort=True)\n",
        "        # cxt_inds, trg_inds = rffmask2d((1,)+self.hw, ctx_scale=(.05,.5), trg_scale=(.15,.2), chaos=[3,1], sort=True)\n",
        "        # cxt_inds, trg_inds = simplexmask2d(hw, ctx_scale=(.85,1), trg_scale=(.7,.8), B=b, chaos=[3,.5])\n",
        "        cxt_inds, trg_inds = cxt_inds.repeat(b,1), trg_inds.repeat(b,1)\n",
        "\n",
        "        # zero_mask = torch.zeros(b ,*self.hw, device=device).flatten(1)\n",
        "        # zero_mask[torch.arange(b).unsqueeze(-1), cxt_inds] = 1\n",
        "        # x_ = x * F.interpolate(zero_mask.reshape(b,1,*self.hw), size=x.shape[2:], mode='nearest-exact') # zero masked locations\n",
        "        # sx = self.student(x_, context_indices=cxt_inds) # [batch, num_context_toks, out_dim]\n",
        "\n",
        "        sx = self.student(x, context_indices=cxt_inds) # [batch, num_context_toks, out_dim]\n",
        "        # print('ijepa loss sx',sx.shape)\n",
        "        sy_ = self.predicter(sx, context_indices=cxt_inds, trg_indices=trg_inds) # [batch*M, num_trg_toks, out_dim]\n",
        "        sy_ = F.layer_norm(sy_, (sy_.size(-1),))\n",
        "        with torch.no_grad():\n",
        "            sy = self.teacher(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "            # print('ijepa loss sy',sy.shape)\n",
        "            sy = sy[torch.arange(sy.shape[0]).unsqueeze(-1), trg_inds] # [batch, num_context_toks, d_model] # nan bec len(trg_ind)==0 # print('loss sy',torch.isnan(sy).any())\n",
        "            sy = F.layer_norm(sy, (sy.size(-1),))\n",
        "        loss = F.mse_loss(sy, sy_)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x): # [b,t,3]\n",
        "        out = self.student(x).mean(dim=1)\n",
        "        return out\n",
        "\n",
        "# min_s=0.15, max_s, M\n",
        "# trg.15.2M4 C.85 1\n",
        "# dont normalise data\n",
        "# randmsk < simplex 1msk < multiblk\n",
        "# teacher=trans ~? teacher=copy\n",
        "# learn convemb\n",
        "# lr pred,student: 3e-3/1e-2, 1e-3\n",
        "\n",
        "# lr 1e-3 < 1e-2,1e-3? slower but dun plateau\n",
        "# zeromsk impt, else closs start increasing\n",
        "# l1rge trg helps delay increase?\n",
        "\n",
        "# vit 1lyr 15.4sec 15.6\n",
        "# hiera downsampling attn 20.5\n",
        "\n",
        "# ijepa = IJEPA(in_dim=3, d_model=64, out_dim=64, nlayers=4, n_heads=4).to(device)\n",
        "ijepa = IJEPA(in_dim=3, d_model=64, out_dim=64, nlayers=1, n_heads=8).to(device)\n",
        "# ijepa = IJEPA(in_dim=3, d_model=32, out_dim=32, nlayers=1, n_heads=4).to(device)\n",
        "optim = torch.optim.AdamW(ijepa.parameters(), lr=1e-3)#, weight_decay=0) # 1e-3?\n",
        "# optim = torch.optim.AdamW([{'params': ijepa.student.parameters()},\n",
        "# #     {'params': ijepa.predicter.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # good 3e-3,1e-3 ; default 1e-2, 5e-2\n",
        "#     {'params': ijepa.predicter.parameters(), 'lr': 1e-2}], lr=1e-3, weight_decay=1e-2)\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/configs/in1k_vith14_ep300.yaml\n",
        "# d_model 1024,384\n",
        "# depth 12,6/12\n",
        "# wd 5e-2 - 4e-1\n",
        "# adamw 1e-4 - 1e-3 - 1e-6\n",
        "# ema 0.996-1\n",
        "\n",
        "print(sum(p.numel() for p in ijepa.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in ijepa.parameters())) # 27584\n",
        "# print(sum(p.numel() for p in ijepa.predicter.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# d_model^2 * nlayers\n",
        "\n",
        "x = torch.rand((64,3,32,32), device=device)\n",
        "out = ijepa.loss(x)\n",
        "print(out.shape)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(ijepa.out_dim, 10).to(device)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n",
        "# optim = torch.optim.AdamW([{'params': ijepa.parameters()}, {'params': classifier.parameters(), 'lr': 1e-3}], lr=1e-3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "cellView": "form",
        "id": "2Nd-sGe6Ku4S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "outputId": "6662f9d3-c39e-4adb-8f01-33b21c9d5044"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>▇██▆▆▅▅▅▅▆▅▆▅▅▄▅▄▄▃▄▄▃▅▂▃▃▃▃▂▂▄▄▃▃▁▃▃▃▃▃</td></tr><tr><td>correct</td><td>▁▂▂▂▃▂▃▂▆▄▇▆█▅▅▆▆▅█▆▇▆▅▆▆▅▆▆▆█▆▆▇██▆▃▇▇▆</td></tr><tr><td>loss</td><td>█▂▁▂▁▂▁▁▁▂▁▂▂▁▂▂▁▂▃▁▁▁▁▃▂▂▂▃▂▃▁▁▂▃▂▁▁▁▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>1.82904</td></tr><tr><td>correct</td><td>0.42188</td></tr><tr><td>loss</td><td>0.05904</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">laced-resonance-115</strong> at: <a href='https://wandb.ai/bobdole/ijepa/runs/zmc5u70b' target=\"_blank\">https://wandb.ai/bobdole/ijepa/runs/zmc5u70b</a><br> View project at: <a href='https://wandb.ai/bobdole/ijepa' target=\"_blank\">https://wandb.ai/bobdole/ijepa</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20260202_063326-zmc5u70b/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.24.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20260202_072007-mlhh9qow</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/ijepa/runs/mlhh9qow' target=\"_blank\">fluent-yogurt-117</a></strong> to <a href='https://wandb.ai/bobdole/ijepa' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/ijepa' target=\"_blank\">https://wandb.ai/bobdole/ijepa</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/ijepa/runs/mlhh9qow' target=\"_blank\">https://wandb.ai/bobdole/ijepa/runs/mlhh9qow</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "!pip install -q wandb\n",
        "import wandb # https://docs.wandb.ai/quickstart\n",
        "wandb.login(key='487a2109e55dce4e13fc70681781de9f50f27be7')\n",
        "try: run.finish()\n",
        "except NameError: pass\n",
        "run = wandb.init(project=\"ijepa\", config={\"model\": \"res18\",})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34de8800-fa4b-4460-803c-3cd8dbfb6c70",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdcb6220-5634-43bb-e567-13e63ba91334"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "classify 2.007080078125\n",
            "classify 2.0814208984375\n",
            "0.234375\n",
            "0.3125\n",
            "0.140625\n",
            "0.140625\n",
            "0.234375\n",
            "0.21875\n",
            "0.140625\n",
            "0.171875\n",
            "0.203125\n",
            "0.25\n",
            "0.1875\n",
            "time: 2.4143049716949463 2.500626894934424\n",
            "116\n",
            "strain 0.04952884837985039\n",
            "strain 0.06368880718946457\n",
            "strain 0.08494727313518524\n",
            "strain 0.05067498981952667\n",
            "strain 0.07501504570245743\n",
            "strain 0.15976302325725555\n",
            "classify 2.041748046875\n",
            "classify 2.19879150390625\n",
            "classify 2.10382080078125\n",
            "classify 2.0950927734375\n",
            "classify 2.1217041015625\n",
            "classify 2.093505859375\n",
            "classify 2.1075439453125\n",
            "classify 2.15362548828125\n",
            "classify 2.1942138671875\n",
            "classify 2.0794677734375\n",
            "classify 2.055908203125\n",
            "0.171875\n",
            "0.265625\n",
            "0.1875\n",
            "0.171875\n",
            "0.203125\n",
            "0.15625\n",
            "0.15625\n",
            "0.234375\n",
            "0.15625\n",
            "0.171875\n",
            "0.21875\n",
            "time: 2.3960227966308594 2.4997373992561274\n",
            "117\n",
            "strain 0.07382393628358841\n",
            "strain 0.04498496279120445\n",
            "strain 0.21108299493789673\n",
            "strain 0.05674690008163452\n",
            "strain 0.05989784002304077\n",
            "strain 0.08305159956216812\n",
            "classify 2.1173095703125\n",
            "classify 2.09423828125\n",
            "classify 2.156005859375\n",
            "classify 2.032470703125\n",
            "classify 2.1551513671875\n",
            "classify 2.2115478515625\n",
            "classify 2.092529296875\n",
            "classify 2.170654296875\n",
            "classify 2.134521484375\n",
            "classify 2.08721923828125\n",
            "classify 2.0181884765625\n",
            "0.3125\n",
            "0.171875\n",
            "0.234375\n",
            "0.21875\n",
            "0.203125\n",
            "0.109375\n",
            "0.234375\n",
            "0.265625\n",
            "0.21875\n",
            "0.25\n",
            "0.25\n",
            "time: 2.429337501525879 2.499145596714343\n",
            "118\n",
            "strain 0.14679168164730072\n",
            "strain 0.0426698662340641\n",
            "strain 0.06814493983983994\n",
            "strain 0.04198894277215004\n",
            "strain 0.05044442042708397\n",
            "strain 0.17848314344882965\n",
            "classify 2.1202392578125\n",
            "classify 2.11474609375\n",
            "classify 2.1820068359375\n",
            "classify 2.0355224609375\n",
            "classify 2.181884765625\n",
            "classify 2.0205078125\n",
            "classify 2.16632080078125\n",
            "classify 2.0244140625\n",
            "classify 2.11541748046875\n",
            "classify 2.120361328125\n",
            "classify 2.089111328125\n",
            "0.1875\n",
            "0.1875\n",
            "0.234375\n",
            "0.265625\n",
            "0.171875\n",
            "0.140625\n",
            "0.21875\n",
            "0.25\n",
            "0.25\n",
            "0.265625\n",
            "0.15625\n",
            "time: 2.633740186691284 2.5002813579655494\n",
            "119\n",
            "strain 0.03672756627202034\n",
            "strain 0.039741795510053635\n",
            "strain 0.04553205519914627\n",
            "strain 0.2604796290397644\n",
            "strain 0.08126741647720337\n",
            "strain 0.10909862071275711\n",
            "classify 2.2325439453125\n",
            "classify 2.0882568359375\n",
            "classify 2.138671875\n",
            "classify 2.0360107421875\n",
            "classify 2.08795166015625\n",
            "classify 2.0595703125\n",
            "classify 2.05828857421875\n",
            "classify 2.1868896484375\n",
            "classify 2.1585693359375\n",
            "classify 2.1181640625\n",
            "classify 2.0738525390625\n",
            "0.203125\n",
            "0.25\n",
            "0.328125\n",
            "0.21875\n",
            "0.203125\n",
            "0.234375\n",
            "0.21875\n",
            "0.21875\n",
            "0.203125\n",
            "0.265625\n",
            "0.203125\n",
            "time: 2.638718605041504 2.5014433999856314\n",
            "120\n",
            "strain 0.03927961364388466\n",
            "strain 0.046706318855285645\n",
            "strain 0.051892321556806564\n",
            "strain 0.07742509245872498\n",
            "strain 0.04392590746283531\n",
            "strain 0.07214487344026566\n",
            "classify 2.06201171875\n",
            "classify 2.009521484375\n",
            "classify 2.09417724609375\n",
            "classify 1.99566650390625\n",
            "classify 2.089111328125\n",
            "classify 2.101318359375\n",
            "classify 2.184814453125\n",
            "classify 2.13470458984375\n",
            "classify 2.056640625\n",
            "classify 2.139892578125\n",
            "classify 2.017578125\n",
            "0.171875\n",
            "0.203125\n",
            "0.125\n",
            "0.1875\n",
            "0.3125\n",
            "0.28125\n",
            "0.265625\n",
            "0.21875\n",
            "0.265625\n",
            "0.21875\n",
            "0.265625\n",
            "time: 2.4222006797790527 2.500793685597822\n",
            "121\n",
            "strain 0.05977964028716087\n",
            "strain 0.04316708445549011\n",
            "strain 0.10065066814422607\n",
            "strain 0.05251208320260048\n",
            "strain 0.06882408261299133\n",
            "strain 0.05028712749481201\n",
            "classify 2.0574951171875\n",
            "classify 2.1004638671875\n",
            "classify 2.2579345703125\n",
            "classify 2.1607666015625\n",
            "classify 2.0584716796875\n",
            "classify 2.0985107421875\n",
            "classify 2.055419921875\n",
            "classify 2.1751708984375\n",
            "classify 2.1168212890625\n",
            "classify 2.1019287109375\n",
            "classify 2.028564453125\n",
            "0.171875\n",
            "0.203125\n",
            "0.1875\n",
            "0.171875\n",
            "0.265625\n",
            "0.3125\n",
            "0.25\n",
            "0.203125\n",
            "0.171875\n",
            "0.21875\n",
            "0.203125\n",
            "time: 2.4135243892669678 2.500083170953344\n",
            "122\n",
            "strain 0.07920993864536285\n",
            "strain 0.04180428758263588\n",
            "strain 0.059884071350097656\n",
            "strain 0.1141555979847908\n",
            "strain 0.06696438789367676\n",
            "strain 0.06825857609510422\n",
            "classify 2.1737060546875\n",
            "classify 2.151611328125\n",
            "classify 2.081298828125\n",
            "classify 2.119384765625\n",
            "classify 2.0509033203125\n",
            "classify 2.1102294921875\n",
            "classify 2.123046875\n",
            "classify 2.1773681640625\n",
            "classify 2.0352783203125\n",
            "classify 2.05059814453125\n",
            "classify 2.1619873046875\n",
            "0.21875\n",
            "0.234375\n",
            "0.203125\n",
            "0.265625\n",
            "0.171875\n",
            "0.171875\n",
            "0.28125\n",
            "0.21875\n",
            "0.203125\n",
            "0.1875\n",
            "0.265625\n",
            "time: 2.456479072570801 2.4997333452953554\n",
            "123\n",
            "strain 0.07793651521205902\n",
            "strain 0.04012877121567726\n",
            "strain 0.05798321217298508\n",
            "strain 0.1127212643623352\n",
            "strain 0.04390908405184746\n",
            "strain 0.06745028495788574\n",
            "classify 2.08642578125\n",
            "classify 2.06146240234375\n",
            "classify 2.134765625\n",
            "classify 2.1268310546875\n",
            "classify 2.111083984375\n",
            "classify 2.171142578125\n",
            "classify 2.1737060546875\n",
            "classify 2.07135009765625\n",
            "classify 2.1572265625\n",
            "classify 2.128662109375\n",
            "classify 1.984130859375\n",
            "0.203125\n",
            "0.203125\n",
            "0.125\n",
            "0.1875\n",
            "0.234375\n",
            "0.28125\n",
            "0.171875\n",
            "0.25\n",
            "0.203125\n",
            "0.28125\n",
            "0.234375\n",
            "time: 2.670501232147217 2.5011153240357675\n",
            "124\n",
            "strain 0.10467486083507538\n",
            "strain 0.04079456627368927\n",
            "strain 0.04895700514316559\n",
            "strain 0.048903997987508774\n",
            "strain 0.07717394083738327\n",
            "strain 0.22254855930805206\n",
            "classify 2.0753173828125\n",
            "classify 2.058349609375\n",
            "classify 1.997802734375\n",
            "classify 2.0968017578125\n",
            "classify 2.16943359375\n",
            "classify 2.066650390625\n",
            "classify 2.1082763671875\n",
            "classify 2.090576171875\n",
            "classify 2.1075439453125\n",
            "classify 2.10894775390625\n",
            "classify 2.1146240234375\n",
            "0.296875\n",
            "0.171875\n",
            "0.15625\n",
            "0.203125\n",
            "0.203125\n",
            "0.203125\n",
            "0.234375\n",
            "0.25\n",
            "0.234375\n",
            "0.265625\n",
            "0.21875\n",
            "time: 2.5660128593444824 2.501642234802246\n",
            "125\n",
            "strain 0.11981412023305893\n",
            "strain 0.08555349707603455\n",
            "strain 0.06299013644456863\n",
            "strain 0.05493824556469917\n",
            "strain 0.06372592598199844\n",
            "strain 0.09034109860658646\n",
            "classify 2.1011962890625\n",
            "classify 2.0194091796875\n",
            "classify 2.0499267578125\n",
            "classify 2.1280517578125\n",
            "classify 2.06048583984375\n",
            "classify 2.08447265625\n",
            "classify 2.0966796875\n",
            "classify 2.0511474609375\n",
            "classify 2.093505859375\n",
            "classify 2.053466796875\n",
            "classify 2.0076904296875\n",
            "0.1875\n",
            "0.171875\n",
            "0.234375\n",
            "0.1875\n",
            "0.1875\n",
            "0.171875\n",
            "0.171875\n",
            "0.265625\n",
            "0.25\n",
            "0.109375\n",
            "0.296875\n",
            "time: 2.409334421157837 2.5009145755616444\n",
            "126\n",
            "strain 0.04298071563243866\n",
            "strain 0.1506396234035492\n",
            "strain 0.0964689776301384\n",
            "strain 0.04538438841700554\n",
            "strain 0.07167309522628784\n",
            "strain 0.16016586124897003\n",
            "classify 2.066650390625\n",
            "classify 2.0814208984375\n",
            "classify 2.1392822265625\n",
            "classify 2.0592041015625\n",
            "classify 2.111572265625\n",
            "classify 2.24609375\n",
            "classify 2.1292724609375\n",
            "classify 2.0628662109375\n",
            "classify 2.0513916015625\n",
            "classify 2.177734375\n",
            "classify 2.070068359375\n",
            "0.203125\n",
            "0.171875\n",
            "0.296875\n",
            "0.1875\n",
            "0.15625\n",
            "0.1875\n",
            "0.25\n",
            "0.234375\n",
            "0.265625\n",
            "0.265625\n",
            "0.28125\n",
            "time: 2.4061386585235596 2.5001729754951056\n",
            "127\n",
            "strain 0.4671332538127899\n",
            "strain 0.05774589255452156\n",
            "strain 0.11168905347585678\n",
            "strain 0.068637415766716\n",
            "strain 0.10672450810670853\n",
            "strain 0.057141613215208054\n",
            "classify 2.12158203125\n",
            "classify 2.15673828125\n",
            "classify 2.025390625\n",
            "classify 2.0933837890625\n",
            "classify 2.12353515625\n",
            "classify 2.1256103515625\n",
            "classify 2.113037109375\n",
            "classify 2.01861572265625\n",
            "classify 2.0733642578125\n",
            "classify 2.1575927734375\n",
            "classify 2.0716552734375\n",
            "0.15625\n",
            "0.1875\n",
            "0.25\n",
            "0.296875\n",
            "0.21875\n",
            "0.1875\n",
            "0.234375\n",
            "0.171875\n",
            "0.1875\n",
            "0.234375\n",
            "0.28125\n",
            "time: 2.398223876953125 2.4993808045983315\n",
            "128\n",
            "strain 0.12694644927978516\n",
            "strain 0.12724728882312775\n",
            "strain 0.06630110740661621\n",
            "strain 0.03623143583536148\n",
            "strain 0.09633679687976837\n",
            "strain 0.1690300703048706\n",
            "classify 2.0986328125\n",
            "classify 2.08349609375\n",
            "classify 2.03466796875\n",
            "classify 2.0474853515625\n",
            "classify 2.04095458984375\n",
            "classify 1.9881591796875\n",
            "classify 2.20355224609375\n",
            "classify 2.0386962890625\n",
            "classify 2.161865234375\n",
            "classify 2.044677734375\n",
            "classify 2.0426025390625\n",
            "0.234375\n",
            "0.21875\n",
            "0.25\n",
            "0.265625\n",
            "0.375\n",
            "0.234375\n",
            "0.21875\n",
            "0.21875\n",
            "0.15625\n",
            "0.140625\n",
            "0.25\n",
            "time: 2.652198076248169 2.5005695893782978\n",
            "129\n",
            "strain 0.052487052977085114\n",
            "strain 0.04823405668139458\n",
            "strain 0.04409638047218323\n",
            "strain 0.04529732093214989\n",
            "strain 0.08350834995508194\n",
            "strain 0.04841506481170654\n",
            "classify 2.0802001953125\n",
            "classify 2.154296875\n",
            "classify 2.1536865234375\n",
            "classify 2.1405029296875\n",
            "classify 2.1036376953125\n",
            "classify 2.1439208984375\n",
            "classify 2.11181640625\n",
            "classify 2.1341552734375\n",
            "classify 2.0826416015625\n",
            "classify 2.12115478515625\n",
            "classify 2.1636962890625\n",
            "0.28125\n",
            "0.3125\n",
            "0.265625\n",
            "0.234375\n",
            "0.296875\n",
            "0.25\n",
            "0.171875\n",
            "0.203125\n",
            "0.1875\n",
            "0.1875\n",
            "0.21875\n",
            "time: 2.585834503173828 2.5012315676762507\n",
            "130\n",
            "strain 0.12803831696510315\n",
            "strain 0.1004965752363205\n",
            "strain 0.05183260887861252\n",
            "strain 0.05784072354435921\n",
            "strain 0.04441133141517639\n",
            "strain 0.08801441639661789\n",
            "classify 2.01971435546875\n",
            "classify 2.0439453125\n",
            "classify 2.087890625\n",
            "classify 2.04010009765625\n",
            "classify 1.98291015625\n",
            "classify 2.1304931640625\n",
            "classify 2.0767822265625\n",
            "classify 2.181640625\n",
            "classify 2.19921875\n",
            "classify 2.10595703125\n",
            "classify 2.101318359375\n",
            "0.171875\n",
            "0.203125\n",
            "0.21875\n",
            "0.1875\n",
            "0.21875\n",
            "0.25\n",
            "0.1875\n",
            "0.15625\n",
            "0.265625\n",
            "0.21875\n",
            "0.234375\n",
            "time: 2.39788556098938 2.5004475335128435\n",
            "131\n",
            "strain 0.04896010830998421\n",
            "strain 0.046864740550518036\n",
            "strain 0.046253081411123276\n",
            "strain 0.04081200063228607\n",
            "strain 0.12588411569595337\n",
            "strain 0.06913171708583832\n",
            "classify 2.200439453125\n",
            "classify 2.185302734375\n",
            "classify 2.1697998046875\n",
            "classify 2.10302734375\n",
            "classify 2.13232421875\n",
            "classify 2.096923828125\n",
            "classify 2.074462890625\n",
            "classify 2.064453125\n",
            "classify 2.1324462890625\n",
            "classify 2.0177001953125\n",
            "classify 2.02392578125\n",
            "0.234375\n",
            "0.234375\n",
            "0.21875\n",
            "0.234375\n",
            "0.265625\n",
            "0.15625\n",
            "0.28125\n",
            "0.109375\n",
            "0.28125\n",
            "0.1875\n",
            "0.203125\n",
            "time: 2.41629958152771 2.49981412562457\n",
            "132\n",
            "strain 0.05386734753847122\n",
            "strain 0.08663137257099152\n",
            "strain 0.09980149567127228\n",
            "strain 0.046893950551748276\n",
            "strain 0.1829286813735962\n",
            "strain 0.06811156868934631\n",
            "classify 2.1453857421875\n",
            "classify 2.06781005859375\n",
            "classify 2.1634521484375\n",
            "classify 2.15057373046875\n",
            "classify 2.0914306640625\n",
            "classify 2.0771484375\n",
            "classify 2.052001953125\n",
            "classify 1.98199462890625\n",
            "classify 2.0498046875\n",
            "classify 2.1534423828125\n",
            "classify 2.017822265625\n",
            "0.125\n",
            "0.171875\n",
            "0.140625\n",
            "0.203125\n",
            "0.140625\n",
            "0.28125\n",
            "0.328125\n",
            "0.28125\n",
            "0.171875\n",
            "0.234375\n",
            "0.15625\n",
            "time: 2.438861608505249 2.499360125763972\n",
            "133\n",
            "strain 0.09611668437719345\n",
            "strain 0.12575505673885345\n",
            "strain 0.05574573203921318\n",
            "strain 0.07274125516414642\n",
            "strain 0.09519707411527634\n",
            "strain 0.04894649237394333\n",
            "classify 2.0498046875\n",
            "classify 2.0772705078125\n",
            "classify 2.081298828125\n",
            "classify 2.107177734375\n",
            "classify 2.0528564453125\n",
            "classify 2.23822021484375\n",
            "classify 2.14697265625\n",
            "classify 2.1015625\n",
            "classify 2.0284423828125\n",
            "classify 2.12103271484375\n",
            "classify 2.06103515625\n",
            "0.203125\n",
            "0.234375\n",
            "0.265625\n",
            "0.25\n",
            "0.21875\n",
            "0.21875\n",
            "0.21875\n",
            "0.296875\n",
            "0.25\n",
            "0.265625\n",
            "0.25\n",
            "time: 2.650059938430786 2.50049049996618\n",
            "134\n",
            "strain 0.10971751809120178\n",
            "strain 0.0485694594681263\n",
            "strain 0.16594383120536804\n",
            "strain 0.05487372353672981\n",
            "strain 0.03774242848157883\n",
            "strain 0.06829731166362762\n",
            "classify 2.2091064453125\n",
            "classify 2.1270751953125\n",
            "classify 2.0972900390625\n",
            "classify 2.0936279296875\n",
            "classify 2.0205078125\n",
            "classify 2.1134033203125\n",
            "classify 2.11090087890625\n",
            "classify 2.0596923828125\n",
            "classify 2.080810546875\n",
            "classify 2.01153564453125\n",
            "classify 2.0692138671875\n",
            "0.328125\n",
            "0.140625\n",
            "0.234375\n",
            "0.140625\n",
            "0.15625\n",
            "0.25\n",
            "0.21875\n",
            "0.203125\n",
            "0.15625\n",
            "0.171875\n",
            "0.234375\n",
            "time: 2.607759952545166 2.5012887248286493\n",
            "135\n",
            "strain 0.034622952342033386\n",
            "strain 0.03901151940226555\n",
            "strain 0.056062210351228714\n",
            "strain 0.07235190272331238\n",
            "strain 0.2737346589565277\n",
            "strain 0.08231205493211746\n",
            "classify 2.159423828125\n",
            "classify 2.1136474609375\n",
            "classify 2.148193359375\n",
            "classify 2.0087890625\n",
            "classify 2.084228515625\n",
            "classify 2.01507568359375\n",
            "classify 2.153076171875\n",
            "classify 2.114501953125\n",
            "classify 2.098876953125\n",
            "classify 2.08892822265625\n",
            "classify 2.06549072265625\n",
            "0.265625\n",
            "0.15625\n",
            "0.328125\n",
            "0.171875\n",
            "0.265625\n",
            "0.234375\n",
            "0.234375\n",
            "0.328125\n",
            "0.171875\n",
            "0.234375\n",
            "0.1875\n",
            "time: 2.384080648422241 2.5004309485940373\n",
            "136\n",
            "strain 0.10908547788858414\n",
            "strain 0.0966615378856659\n",
            "strain 0.10249730944633484\n",
            "strain 0.04943061247467995\n",
            "strain 0.08499743789434433\n",
            "strain 0.06833071261644363\n",
            "classify 2.00927734375\n",
            "classify 2.0059814453125\n",
            "classify 1.99774169921875\n",
            "classify 2.03662109375\n",
            "classify 2.2640380859375\n",
            "classify 2.07080078125\n",
            "classify 2.0572509765625\n",
            "classify 2.16265869140625\n",
            "classify 2.194091796875\n",
            "classify 2.07080078125\n",
            "classify 2.123291015625\n",
            "0.203125\n",
            "0.359375\n",
            "0.28125\n",
            "0.265625\n",
            "0.1875\n",
            "0.1875\n",
            "0.1875\n",
            "0.1875\n",
            "0.265625\n",
            "0.3125\n",
            "0.21875\n",
            "time: 2.4034337997436523 2.49972696896017\n",
            "137\n",
            "strain 0.14823417365550995\n",
            "strain 0.03336089104413986\n",
            "strain 0.10487842559814453\n",
            "strain 0.07225953042507172\n",
            "strain 0.048948660492897034\n",
            "strain 0.08853812515735626\n",
            "classify 2.2423095703125\n",
            "classify 2.145263671875\n",
            "classify 2.08837890625\n",
            "classify 2.1634521484375\n",
            "classify 2.035400390625\n",
            "classify 2.1636962890625\n",
            "classify 2.141845703125\n",
            "classify 2.0946044921875\n",
            "classify 2.07867431640625\n",
            "classify 2.1265869140625\n",
            "classify 2.0732421875\n",
            "0.296875\n",
            "0.25\n",
            "0.28125\n",
            "0.203125\n",
            "0.1875\n",
            "0.234375\n",
            "0.1875\n",
            "0.25\n",
            "0.25\n",
            "0.296875\n",
            "0.21875\n",
            "time: 2.425023317337036 2.499189553053483\n",
            "138\n",
            "strain 0.048127882182598114\n",
            "strain 0.08086410164833069\n",
            "strain 0.07441697269678116\n",
            "strain 0.17336703836917877\n",
            "strain 0.05194234475493431\n",
            "strain 0.03662635758519173\n",
            "classify 2.056640625\n",
            "classify 2.204833984375\n",
            "classify 2.162109375\n",
            "classify 2.136474609375\n",
            "classify 2.16064453125\n",
            "classify 2.1009521484375\n",
            "classify 2.111328125\n",
            "classify 2.159423828125\n",
            "classify 2.114013671875\n",
            "classify 2.0296630859375\n",
            "classify 2.135009765625\n",
            "0.3125\n",
            "0.328125\n",
            "0.171875\n",
            "0.265625\n",
            "0.265625\n",
            "0.234375\n",
            "0.171875\n",
            "0.21875\n",
            "0.21875\n",
            "0.28125\n",
            "0.25\n",
            "time: 2.6723010540008545 2.5004391412940814\n",
            "139\n",
            "strain 0.05099176615476608\n",
            "strain 0.13991191983222961\n",
            "strain 0.055127549916505814\n",
            "strain 0.11368019878864288\n",
            "strain 0.07615547627210617\n",
            "strain 0.0503731295466423\n",
            "classify 2.08251953125\n",
            "classify 2.08203125\n",
            "classify 1.9810791015625\n",
            "classify 2.1571044921875\n",
            "classify 2.0950927734375\n",
            "classify 2.1884765625\n",
            "classify 2.021240234375\n",
            "classify 2.07952880859375\n",
            "classify 2.0823974609375\n",
            "classify 2.124755859375\n",
            "classify 2.05322265625\n",
            "0.21875\n",
            "0.25\n",
            "0.25\n",
            "0.203125\n",
            "0.125\n",
            "0.21875\n",
            "0.1875\n",
            "0.125\n",
            "0.171875\n",
            "0.234375\n",
            "0.140625\n",
            "time: 2.601168155670166 2.501163237435477\n",
            "140\n",
            "strain 0.09213621914386749\n",
            "strain 0.041333384811878204\n",
            "strain 0.054912272840738297\n",
            "strain 0.08914872258901596\n",
            "strain 0.06518304347991943\n",
            "strain 0.05282513424754143\n",
            "classify 2.10009765625\n",
            "classify 2.1475830078125\n",
            "classify 2.0950927734375\n",
            "classify 2.115966796875\n",
            "classify 2.08349609375\n",
            "classify 2.08868408203125\n",
            "classify 2.0223388671875\n",
            "classify 2.107666015625\n",
            "classify 2.09930419921875\n",
            "classify 2.00390625\n",
            "classify 2.052978515625\n",
            "0.203125\n",
            "0.203125\n",
            "0.21875\n",
            "0.1875\n",
            "0.234375\n",
            "0.203125\n",
            "0.265625\n",
            "0.25\n",
            "0.296875\n",
            "0.25\n",
            "0.296875\n",
            "time: 2.3912572860717773 2.5003880152465605\n",
            "141\n",
            "strain 0.15059584379196167\n",
            "strain 0.035915255546569824\n",
            "strain 0.17411759495735168\n",
            "strain 0.05160056799650192\n",
            "strain 0.04443228617310524\n",
            "strain 0.04798123240470886\n",
            "classify 2.1312255859375\n",
            "classify 2.1336669921875\n",
            "classify 2.04693603515625\n",
            "classify 2.08990478515625\n",
            "classify 2.022705078125\n",
            "classify 2.1317138671875\n",
            "classify 2.02044677734375\n",
            "classify 2.09185791015625\n",
            "classify 2.0699462890625\n",
            "classify 2.078857421875\n",
            "classify 2.0902099609375\n",
            "0.375\n",
            "0.234375\n",
            "0.296875\n",
            "0.234375\n",
            "0.21875\n",
            "0.21875\n",
            "0.25\n",
            "0.25\n",
            "0.203125\n",
            "0.203125\n",
            "0.265625\n",
            "time: 2.391923427581787 2.4996281089917036\n",
            "142\n",
            "strain 0.07376059144735336\n",
            "strain 0.058250751346349716\n",
            "strain 0.15226779878139496\n",
            "strain 0.08506118506193161\n",
            "strain 0.06236884742975235\n",
            "strain 0.08985323458909988\n",
            "classify 2.1878662109375\n",
            "classify 2.1519775390625\n",
            "classify 2.1317138671875\n",
            "classify 2.1688232421875\n",
            "classify 2.0447998046875\n",
            "classify 2.0772705078125\n",
            "classify 2.044189453125\n",
            "classify 2.03466796875\n",
            "classify 2.0869140625\n",
            "classify 2.162353515625\n",
            "classify 2.1239013671875\n",
            "0.203125\n",
            "0.203125\n",
            "0.21875\n",
            "0.171875\n",
            "0.21875\n",
            "0.171875\n",
            "0.21875\n",
            "0.28125\n",
            "0.25\n",
            "0.171875\n",
            "0.296875\n",
            "time: 2.431617021560669 2.499156918559041\n",
            "143\n",
            "strain 0.05326573923230171\n",
            "strain 0.13913562893867493\n",
            "strain 0.22535088658332825\n",
            "strain 0.040117137134075165\n",
            "strain 0.13539065420627594\n",
            "strain 0.11558479815721512\n",
            "classify 2.05035400390625\n",
            "classify 2.01739501953125\n",
            "classify 2.0758056640625\n",
            "classify 2.08929443359375\n",
            "classify 2.0323486328125\n",
            "classify 2.09326171875\n",
            "classify 2.2069091796875\n",
            "classify 2.147216796875\n",
            "classify 2.091064453125\n",
            "classify 2.067138671875\n",
            "classify 2.1114501953125\n",
            "0.25\n",
            "0.21875\n",
            "0.234375\n",
            "0.171875\n",
            "0.28125\n",
            "0.1875\n",
            "0.1875\n",
            "0.234375\n",
            "0.21875\n",
            "0.15625\n",
            "0.140625\n",
            "time: 2.6483442783355713 2.5001968327495785\n",
            "144\n",
            "strain 0.045448947697877884\n",
            "strain 0.04892588406801224\n",
            "strain 0.04655785113573074\n",
            "strain 0.03944125398993492\n",
            "strain 0.040644384920597076\n",
            "strain 0.04797978326678276\n",
            "classify 2.08154296875\n",
            "classify 2.131103515625\n",
            "classify 2.13702392578125\n",
            "classify 2.1900634765625\n",
            "classify 2.05810546875\n",
            "classify 2.05377197265625\n",
            "classify 1.9884033203125\n",
            "classify 2.04736328125\n",
            "classify 2.080078125\n",
            "classify 2.1214599609375\n",
            "classify 2.1070556640625\n",
            "0.296875\n",
            "0.21875\n",
            "0.171875\n",
            "0.203125\n",
            "0.1875\n",
            "0.25\n",
            "0.234375\n",
            "0.25\n",
            "0.234375\n",
            "0.28125\n",
            "0.21875\n",
            "time: 2.593435049057007 2.50084516262186\n",
            "145\n",
            "strain 0.14963464438915253\n",
            "strain 0.13045570254325867\n",
            "strain 0.05529237538576126\n",
            "strain 0.04308854788541794\n",
            "strain 0.043236397206783295\n",
            "strain 0.3029862344264984\n",
            "classify 2.1104736328125\n",
            "classify 2.1663818359375\n",
            "classify 2.0218505859375\n",
            "classify 2.03662109375\n",
            "classify 2.0491943359375\n",
            "classify 2.136962890625\n",
            "classify 2.1719970703125\n",
            "classify 2.1497802734375\n",
            "classify 2.1099853515625\n",
            "classify 2.03497314453125\n",
            "classify 2.1083984375\n",
            "0.203125\n",
            "0.21875\n",
            "0.234375\n",
            "0.1875\n",
            "0.234375\n",
            "0.15625\n",
            "0.25\n",
            "0.21875\n",
            "0.1875\n",
            "0.21875\n",
            "0.171875\n",
            "time: 2.3948090076446533 2.5001226418638884\n",
            "146\n",
            "strain 0.12033410370349884\n",
            "strain 0.13257558643817902\n",
            "strain 0.10703378170728683\n",
            "strain 0.12223703414201736\n",
            "strain 0.04710618779063225\n",
            "strain 0.12259332835674286\n",
            "classify 2.1656494140625\n",
            "classify 2.162353515625\n",
            "classify 2.0797119140625\n",
            "classify 2.09130859375\n",
            "classify 2.1573486328125\n",
            "classify 2.162109375\n",
            "classify 2.0859375\n",
            "classify 1.980224609375\n",
            "classify 2.07568359375\n",
            "classify 2.0087890625\n",
            "classify 2.0267333984375\n",
            "0.234375\n",
            "0.265625\n",
            "0.21875\n",
            "0.171875\n",
            "0.234375\n",
            "0.25\n",
            "0.296875\n",
            "0.265625\n",
            "0.234375\n",
            "0.21875\n",
            "0.203125\n",
            "time: 2.4126358032226562 2.499532568211458\n",
            "147\n",
            "strain 0.07648640125989914\n",
            "strain 0.06661483645439148\n",
            "strain 0.07685955613851547\n",
            "strain 0.1106235608458519\n",
            "strain 0.047731515020132065\n",
            "strain 0.21521805226802826\n",
            "classify 2.0452880859375\n",
            "classify 2.1136474609375\n",
            "classify 2.03924560546875\n",
            "classify 2.04351806640625\n",
            "classify 2.0230712890625\n",
            "classify 2.0645751953125\n",
            "classify 2.04193115234375\n",
            "classify 1.9654541015625\n",
            "classify 1.95684814453125\n",
            "classify 2.1556396484375\n",
            "classify 2.1644287109375\n",
            "0.25\n",
            "0.25\n",
            "0.109375\n",
            "0.15625\n",
            "0.296875\n",
            "0.328125\n",
            "0.203125\n",
            "0.171875\n",
            "0.265625\n",
            "0.203125\n",
            "0.171875\n",
            "time: 2.4350318908691406 2.499100635180602\n",
            "148\n",
            "strain 0.11039642244577408\n",
            "strain 0.17152534425258636\n",
            "strain 0.18767055869102478\n",
            "strain 0.056452132761478424\n",
            "strain 0.11864272505044937\n",
            "strain 0.05502038076519966\n",
            "classify 2.10009765625\n",
            "classify 2.024169921875\n",
            "classify 2.077392578125\n",
            "classify 2.1123046875\n",
            "classify 2.1160888671875\n",
            "classify 2.0616455078125\n",
            "classify 2.1043701171875\n",
            "classify 2.10272216796875\n",
            "classify 1.95654296875\n",
            "classify 2.07025146484375\n",
            "classify 1.9993896484375\n",
            "0.265625\n",
            "0.28125\n",
            "0.171875\n",
            "0.25\n",
            "0.28125\n",
            "0.265625\n",
            "0.125\n",
            "0.28125\n",
            "0.328125\n",
            "0.234375\n",
            "0.234375\n",
            "time: 2.6804511547088623 2.500321778675054\n",
            "149\n",
            "strain 0.0708603635430336\n",
            "strain 0.06094950810074806\n",
            "strain 0.05877402052283287\n",
            "strain 0.047477755695581436\n",
            "strain 0.3199857473373413\n",
            "strain 0.1437520533800125\n",
            "classify 2.077392578125\n",
            "classify 2.137451171875\n",
            "classify 2.0836181640625\n",
            "classify 2.11859130859375\n",
            "classify 2.13134765625\n",
            "classify 2.025634765625\n",
            "classify 2.03271484375\n",
            "classify 2.1468505859375\n",
            "classify 2.10504150390625\n",
            "classify 2.0250244140625\n",
            "classify 2.019775390625\n",
            "0.203125\n",
            "0.234375\n",
            "0.265625\n",
            "0.25\n",
            "0.171875\n",
            "0.3125\n",
            "0.203125\n",
            "0.265625\n",
            "0.21875\n",
            "0.296875\n",
            "0.234375\n",
            "time: 2.6191599369049072 2.501120270093282\n",
            "150\n",
            "strain 0.06698143482208252\n",
            "strain 0.07804995030164719\n",
            "strain 0.05613670498132706\n",
            "strain 0.06486804783344269\n",
            "strain 0.06739335507154465\n",
            "strain 0.05844192951917648\n",
            "classify 2.048583984375\n",
            "classify 2.1187744140625\n",
            "classify 2.1993408203125\n",
            "classify 2.0517578125\n",
            "classify 2.0291748046875\n",
            "classify 2.0740966796875\n",
            "classify 2.1363525390625\n",
            "classify 2.02471923828125\n",
            "classify 2.03857421875\n",
            "classify 2.01220703125\n",
            "classify 2.21826171875\n",
            "0.28125\n",
            "0.265625\n",
            "0.28125\n",
            "0.1875\n",
            "0.28125\n",
            "0.3125\n",
            "0.203125\n",
            "0.15625\n",
            "0.21875\n",
            "0.25\n",
            "0.21875\n",
            "time: 2.4003162384033203 2.5004565889472206\n",
            "151\n",
            "strain 0.05192159488797188\n",
            "strain 0.052738916128873825\n",
            "strain 0.11742252856492996\n",
            "strain 0.15192613005638123\n",
            "strain 0.062068089842796326\n",
            "strain 0.24898555874824524\n",
            "classify 2.0872802734375\n",
            "classify 2.08160400390625\n",
            "classify 2.06982421875\n",
            "classify 2.068359375\n",
            "classify 2.023681640625\n",
            "classify 2.0543212890625\n",
            "classify 2.0877685546875\n",
            "classify 2.064453125\n",
            "classify 2.1016845703125\n",
            "classify 2.1143798828125\n",
            "classify 2.139404296875\n",
            "0.265625\n",
            "0.25\n",
            "0.203125\n",
            "0.203125\n",
            "0.265625\n",
            "0.15625\n",
            "0.234375\n",
            "0.3125\n",
            "0.203125\n",
            "0.25\n",
            "0.28125\n",
            "time: 2.4142532348632812 2.49989326847227\n",
            "152\n",
            "strain 0.05482672154903412\n",
            "strain 0.04369550198316574\n",
            "strain 0.06578381359577179\n",
            "strain 0.046544045209884644\n",
            "strain 0.054118551313877106\n",
            "strain 0.04869688302278519\n",
            "classify 2.00201416015625\n",
            "classify 2.1497802734375\n",
            "classify 2.107666015625\n",
            "classify 2.14459228515625\n",
            "classify 2.0751953125\n",
            "classify 2.0062255859375\n",
            "classify 1.98529052734375\n",
            "classify 1.98779296875\n",
            "classify 2.124267578125\n",
            "classify 2.0716552734375\n",
            "classify 1.97857666015625\n",
            "0.203125\n",
            "0.25\n",
            "0.28125\n",
            "0.3125\n",
            "0.25\n",
            "0.296875\n",
            "0.203125\n",
            "0.25\n",
            "0.234375\n",
            "0.265625\n",
            "0.234375\n",
            "time: 2.3936684131622314 2.499202394797132\n",
            "153\n",
            "strain 0.07870214432477951\n",
            "strain 0.10166312754154205\n",
            "strain 0.0719015970826149\n",
            "strain 0.04690582677721977\n",
            "strain 0.08845612406730652\n",
            "strain 0.11191337555646896\n",
            "classify 2.080322265625\n",
            "classify 2.087158203125\n",
            "classify 2.0396728515625\n",
            "classify 2.0394287109375\n",
            "classify 2.115966796875\n",
            "classify 2.149169921875\n",
            "classify 2.1170654296875\n",
            "classify 2.1512451171875\n",
            "classify 2.130126953125\n",
            "classify 2.1064453125\n",
            "classify 1.9853515625\n",
            "0.234375\n",
            "0.265625\n",
            "0.203125\n",
            "0.21875\n",
            "0.25\n",
            "0.25\n",
            "0.265625\n",
            "0.21875\n",
            "0.28125\n",
            "0.28125\n",
            "0.203125\n",
            "time: 2.6669774055480957 2.5002955461477305\n",
            "154\n",
            "strain 0.04313790053129196\n",
            "strain 0.06014486402273178\n",
            "strain 0.1600928008556366\n",
            "strain 0.03958018496632576\n",
            "strain 0.04643021151423454\n",
            "strain 0.24444416165351868\n",
            "classify 1.97869873046875\n",
            "classify 2.13134765625\n",
            "classify 1.9716796875\n",
            "classify 2.040283203125\n",
            "classify 2.0341796875\n",
            "classify 2.2418212890625\n",
            "classify 2.1754150390625\n",
            "classify 2.1275634765625\n",
            "classify 2.0616455078125\n",
            "classify 2.124267578125\n",
            "classify 2.11846923828125\n",
            "0.28125\n",
            "0.34375\n",
            "0.140625\n",
            "0.3125\n",
            "0.25\n",
            "0.28125\n",
            "0.203125\n",
            "0.1875\n",
            "0.265625\n",
            "0.234375\n",
            "0.25\n",
            "time: 2.589247226715088 2.5008787755043276\n",
            "155\n",
            "strain 0.037501923739910126\n",
            "strain 0.12942063808441162\n",
            "strain 0.2722044587135315\n",
            "strain 0.06441846489906311\n",
            "strain 0.052930522710084915\n",
            "strain 0.07730178534984589\n",
            "classify 2.1556396484375\n",
            "classify 2.1275634765625\n",
            "classify 2.0394287109375\n",
            "classify 2.1966552734375\n",
            "classify 2.08392333984375\n",
            "classify 1.993896484375\n",
            "classify 2.0704345703125\n",
            "classify 2.11505126953125\n",
            "classify 2.1063232421875\n",
            "classify 2.0501708984375\n",
            "classify 2.02081298828125\n",
            "0.203125\n",
            "0.21875\n",
            "0.25\n",
            "0.140625\n",
            "0.234375\n",
            "0.265625\n",
            "0.328125\n",
            "0.234375\n",
            "0.265625\n",
            "0.21875\n",
            "0.15625\n",
            "time: 2.402472972869873 2.500251808227637\n",
            "156\n",
            "strain 0.04813462123274803\n",
            "strain 0.11127346754074097\n",
            "strain 0.04182800278067589\n",
            "strain 0.16394107043743134\n",
            "strain 0.04483051225543022\n",
            "strain 0.08797231316566467\n",
            "classify 2.048095703125\n",
            "classify 2.2515869140625\n",
            "classify 1.980712890625\n",
            "classify 2.031005859375\n",
            "classify 2.160400390625\n",
            "classify 2.022216796875\n",
            "classify 2.0712890625\n",
            "classify 2.0675048828125\n",
            "classify 2.1513671875\n",
            "classify 2.1009521484375\n",
            "classify 2.01904296875\n",
            "0.25\n",
            "0.28125\n",
            "0.1875\n",
            "0.21875\n",
            "0.234375\n",
            "0.34375\n",
            "0.234375\n",
            "0.15625\n",
            "0.25\n",
            "0.234375\n",
            "0.234375\n",
            "time: 2.4028046131134033 2.499635095049621\n",
            "157\n",
            "strain 0.20791561901569366\n",
            "strain 0.03624033182859421\n",
            "strain 0.14507102966308594\n",
            "strain 0.06205498054623604\n",
            "strain 0.040607232600450516\n",
            "strain 0.08066955953836441\n",
            "classify 2.07177734375\n",
            "classify 2.03369140625\n",
            "classify 2.146240234375\n",
            "classify 1.97216796875\n",
            "classify 1.99658203125\n",
            "classify 2.06298828125\n",
            "classify 1.982177734375\n",
            "classify 2.070556640625\n",
            "classify 2.04541015625\n",
            "classify 2.0673828125\n",
            "classify 1.97027587890625\n",
            "0.203125\n",
            "0.265625\n",
            "0.234375\n",
            "0.28125\n",
            "0.21875\n",
            "0.296875\n",
            "0.234375\n",
            "0.296875\n",
            "0.234375\n",
            "0.140625\n",
            "0.234375\n",
            "time: 2.418696641921997 2.4991274634494056\n",
            "158\n",
            "strain 0.08681868016719818\n",
            "strain 0.05444030836224556\n",
            "strain 0.08740619570016861\n",
            "strain 0.050821177661418915\n",
            "strain 0.07279298454523087\n",
            "strain 0.06788695603609085\n",
            "classify 2.2215576171875\n",
            "classify 2.049072265625\n",
            "classify 2.1739501953125\n",
            "classify 2.0240478515625\n",
            "classify 2.098388671875\n",
            "classify 2.04339599609375\n",
            "classify 2.1322021484375\n",
            "classify 2.136474609375\n",
            "classify 2.0692138671875\n",
            "classify 2.065673828125\n",
            "classify 2.0836181640625\n",
            "0.203125\n",
            "0.296875\n",
            "0.1875\n",
            "0.25\n",
            "0.25\n",
            "0.234375\n",
            "0.171875\n",
            "0.203125\n",
            "0.34375\n",
            "0.140625\n",
            "0.234375\n",
            "time: 2.6794044971466064 2.500265284904144\n",
            "159\n",
            "strain 0.05095658451318741\n",
            "strain 0.10097868740558624\n",
            "strain 0.10597895085811615\n",
            "strain 0.11303763091564178\n",
            "strain 0.04199006035923958\n",
            "strain 0.05357439070940018\n",
            "classify 2.04656982421875\n",
            "classify 2.033935546875\n",
            "classify 1.98626708984375\n",
            "classify 2.1005859375\n",
            "classify 2.11407470703125\n",
            "classify 1.97198486328125\n",
            "classify 2.1634521484375\n",
            "classify 1.992431640625\n",
            "classify 2.1182861328125\n",
            "classify 2.0762939453125\n",
            "classify 2.08154296875\n",
            "0.25\n",
            "0.296875\n",
            "0.234375\n",
            "0.265625\n",
            "0.203125\n",
            "0.296875\n",
            "0.21875\n",
            "0.21875\n",
            "0.140625\n",
            "0.234375\n",
            "0.171875\n",
            "time: 2.6094436645507812 2.500953511893749\n",
            "160\n",
            "strain 0.05226637050509453\n",
            "strain 0.0511152409017086\n",
            "strain 0.07394597679376602\n",
            "strain 0.06580022722482681\n",
            "strain 0.051421452313661575\n",
            "strain 0.045743267983198166\n",
            "classify 2.171142578125\n",
            "classify 2.1131591796875\n",
            "classify 2.00830078125\n",
            "classify 2.1309814453125\n",
            "classify 1.9854736328125\n",
            "classify 2.116943359375\n",
            "classify 2.0482177734375\n",
            "classify 2.1015625\n",
            "classify 2.1544189453125\n",
            "classify 2.06439208984375\n",
            "classify 1.9952392578125\n",
            "0.265625\n",
            "0.34375\n",
            "0.28125\n",
            "0.15625\n",
            "0.171875\n",
            "0.171875\n",
            "0.203125\n",
            "0.1875\n",
            "0.265625\n",
            "0.265625\n",
            "0.21875\n",
            "time: 2.4087510108947754 2.5003842804002465\n",
            "161\n",
            "strain 0.1991506814956665\n",
            "strain 0.056801002472639084\n",
            "strain 0.3078749179840088\n",
            "strain 0.04035632312297821\n",
            "strain 0.03976660966873169\n",
            "strain 0.15926183760166168\n",
            "classify 2.1220703125\n",
            "classify 2.01385498046875\n",
            "classify 2.2318115234375\n",
            "classify 2.04302978515625\n",
            "classify 2.0272216796875\n",
            "classify 2.1026611328125\n",
            "classify 2.0704345703125\n",
            "classify 2.0244140625\n",
            "classify 2.04766845703125\n",
            "classify 2.00103759765625\n",
            "classify 2.010009765625\n",
            "0.28125\n",
            "0.203125\n",
            "0.25\n",
            "0.171875\n",
            "0.265625\n",
            "0.21875\n",
            "0.203125\n",
            "0.109375\n",
            "0.171875\n",
            "0.234375\n",
            "0.203125\n",
            "time: 2.411860466003418 2.499841613534056\n",
            "162\n",
            "strain 0.05264164134860039\n",
            "strain 0.03134109452366829\n",
            "strain 0.058044418692588806\n",
            "strain 0.10256852209568024\n",
            "strain 0.06196844205260277\n",
            "strain 0.1763901561498642\n",
            "classify 2.1180419921875\n",
            "classify 2.0367431640625\n",
            "classify 2.054443359375\n",
            "classify 2.12652587890625\n",
            "classify 2.140869140625\n",
            "classify 2.1141357421875\n",
            "classify 2.0662841796875\n",
            "classify 2.0946044921875\n",
            "classify 2.10302734375\n",
            "classify 2.0526123046875\n",
            "classify 2.103759765625\n",
            "0.40625\n",
            "0.1875\n",
            "0.265625\n",
            "0.234375\n",
            "0.171875\n",
            "0.234375\n",
            "0.171875\n",
            "0.21875\n",
            "0.203125\n",
            "0.296875\n",
            "0.234375\n",
            "time: 2.4230434894561768 2.499374418902251\n",
            "163\n",
            "strain 0.14861661195755005\n",
            "strain 0.05722092092037201\n",
            "strain 0.15544593334197998\n",
            "strain 0.07897205650806427\n",
            "strain 0.10885147005319595\n",
            "strain 0.14628608524799347\n",
            "classify 2.0428466796875\n",
            "classify 1.97418212890625\n",
            "classify 2.118896484375\n",
            "classify 2.0313720703125\n",
            "classify 2.05999755859375\n",
            "classify 2.02215576171875\n",
            "classify 1.9788818359375\n",
            "classify 2.048583984375\n",
            "classify 2.03594970703125\n",
            "classify 2.1439208984375\n",
            "classify 1.98876953125\n",
            "0.21875\n",
            "0.21875\n",
            "0.15625\n",
            "0.3125\n",
            "0.265625\n",
            "0.21875\n",
            "0.296875\n",
            "0.21875\n",
            "0.359375\n",
            "0.359375\n",
            "0.234375\n",
            "time: 2.6502370834350586 2.500297975249407\n",
            "164\n",
            "strain 0.039223652333021164\n",
            "strain 0.047924913465976715\n",
            "strain 0.11766955256462097\n",
            "strain 0.06288197636604309\n",
            "strain 0.05112861841917038\n",
            "strain 0.16376300156116486\n",
            "classify 2.0421142578125\n",
            "classify 2.2232666015625\n",
            "classify 2.054443359375\n",
            "classify 2.1109619140625\n",
            "classify 2.08447265625\n",
            "classify 2.1717529296875\n",
            "classify 2.07733154296875\n",
            "classify 2.129150390625\n",
            "classify 2.042724609375\n",
            "classify 2.06353759765625\n",
            "classify 2.042236328125\n",
            "0.25\n",
            "0.28125\n",
            "0.1875\n",
            "0.21875\n",
            "0.140625\n",
            "0.15625\n",
            "0.171875\n",
            "0.15625\n",
            "0.265625\n",
            "0.3125\n",
            "0.15625\n",
            "time: 2.577878713607788 2.5007732449155866\n",
            "165\n",
            "strain 0.03460261970758438\n",
            "strain 0.052853573113679886\n",
            "strain 0.03864465653896332\n",
            "strain 0.048310887068510056\n",
            "strain 0.1502426713705063\n",
            "strain 0.31776341795921326\n",
            "classify 2.0262451171875\n",
            "classify 2.00762939453125\n",
            "classify 2.0390625\n",
            "classify 2.0416259765625\n",
            "classify 2.02587890625\n",
            "classify 2.2366943359375\n",
            "classify 2.1466064453125\n",
            "classify 1.906494140625\n",
            "classify 2.0848388671875\n",
            "classify 1.99755859375\n",
            "classify 2.0009765625\n",
            "0.234375\n",
            "0.296875\n",
            "0.3125\n",
            "0.1875\n",
            "0.21875\n",
            "0.203125\n",
            "0.15625\n",
            "0.328125\n",
            "0.265625\n",
            "0.21875\n",
            "0.234375\n",
            "time: 2.399329423904419 2.5001663544091834\n",
            "166\n",
            "strain 0.05295366048812866\n",
            "strain 0.10885804891586304\n",
            "strain 0.11597765237092972\n",
            "strain 0.14014925062656403\n",
            "strain 0.10159875452518463\n",
            "strain 0.060872599482536316\n",
            "classify 2.12408447265625\n",
            "classify 2.0079345703125\n",
            "classify 2.1251220703125\n",
            "classify 2.0322265625\n",
            "classify 2.1405029296875\n",
            "classify 2.110107421875\n",
            "classify 2.04461669921875\n",
            "classify 1.97802734375\n",
            "classify 2.1143798828125\n",
            "classify 2.0341796875\n",
            "classify 2.10302734375\n",
            "0.21875\n",
            "0.390625\n",
            "0.203125\n",
            "0.3125\n",
            "0.28125\n",
            "0.25\n",
            "0.328125\n",
            "0.1875\n",
            "0.171875\n",
            "0.34375\n",
            "0.203125\n",
            "time: 2.4235455989837646 2.49971159061272\n",
            "167\n",
            "strain 0.2682730555534363\n",
            "strain 0.08330643177032471\n",
            "strain 0.10903391242027283\n",
            "strain 0.06988908350467682\n",
            "strain 0.05378054454922676\n",
            "strain 0.32583561539649963\n",
            "classify 2.1793212890625\n",
            "classify 1.9949951171875\n",
            "classify 2.0418701171875\n",
            "classify 2.072509765625\n",
            "classify 2.07177734375\n",
            "classify 2.1591796875\n",
            "classify 2.17462158203125\n",
            "classify 2.1673583984375\n",
            "classify 2.02630615234375\n",
            "classify 2.13232421875\n",
            "classify 2.096923828125\n",
            "0.203125\n",
            "0.21875\n",
            "0.265625\n",
            "0.25\n",
            "0.234375\n",
            "0.125\n",
            "0.21875\n",
            "0.3125\n",
            "0.265625\n",
            "0.203125\n",
            "0.203125\n",
            "time: 2.3924858570098877 2.4990764274483634\n",
            "168\n",
            "strain 0.22921480238437653\n",
            "strain 0.059962715953588486\n",
            "strain 0.03977961838245392\n",
            "strain 0.07426753640174866\n",
            "strain 0.10790447890758514\n",
            "strain 0.047453347593545914\n",
            "classify 2.1173095703125\n",
            "classify 2.16864013671875\n",
            "classify 2.119140625\n",
            "classify 2.06524658203125\n",
            "classify 2.072265625\n",
            "classify 2.14080810546875\n",
            "classify 2.0638427734375\n",
            "classify 2.08056640625\n",
            "classify 2.093017578125\n",
            "classify 2.105712890625\n",
            "classify 2.052734375\n",
            "0.28125\n",
            "0.296875\n",
            "0.21875\n",
            "0.3125\n",
            "0.203125\n",
            "0.21875\n",
            "0.1875\n",
            "0.28125\n",
            "0.203125\n",
            "0.28125\n",
            "0.21875\n",
            "time: 2.668278932571411 2.500080744895709\n",
            "169\n",
            "strain 0.04584990441799164\n",
            "strain 0.04865278676152229\n",
            "strain 0.39987263083457947\n",
            "strain 0.14591068029403687\n",
            "strain 0.05532761663198471\n",
            "strain 0.11662691086530685\n",
            "classify 2.1456298828125\n",
            "classify 1.8935546875\n",
            "classify 2.07183837890625\n",
            "classify 2.12353515625\n",
            "classify 2.0885009765625\n",
            "classify 2.12353515625\n",
            "classify 2.1810302734375\n",
            "classify 2.12091064453125\n",
            "classify 1.93701171875\n",
            "classify 2.1412353515625\n",
            "classify 2.0550537109375\n",
            "0.28125\n",
            "0.203125\n",
            "0.25\n",
            "0.21875\n",
            "0.1875\n",
            "0.203125\n",
            "0.28125\n",
            "0.25\n",
            "0.21875\n",
            "0.1875\n",
            "0.21875\n",
            "time: 2.5558595657348633 2.5004150671117444\n",
            "170\n",
            "strain 0.155799001455307\n",
            "strain 0.0780537948012352\n",
            "strain 0.12539413571357727\n",
            "strain 0.059717100113630295\n",
            "strain 0.06294742226600647\n",
            "strain 0.04647539556026459\n",
            "classify 2.09320068359375\n",
            "classify 2.0665283203125\n",
            "classify 2.1759033203125\n",
            "classify 2.018310546875\n",
            "classify 2.17919921875\n",
            "classify 2.107421875\n",
            "classify 2.029296875\n",
            "classify 2.079833984375\n",
            "classify 2.09503173828125\n",
            "classify 2.06005859375\n",
            "classify 2.1729736328125\n",
            "0.234375\n",
            "0.1875\n",
            "0.21875\n",
            "0.328125\n",
            "0.25\n",
            "0.28125\n",
            "0.203125\n",
            "0.296875\n",
            "0.265625\n",
            "0.25\n",
            "0.171875\n",
            "time: 2.419248104095459 2.4999434222951966\n",
            "171\n",
            "strain 0.0834229364991188\n",
            "strain 0.07855503261089325\n",
            "strain 0.038863953202962875\n",
            "strain 0.12967169284820557\n",
            "strain 0.1020161435008049\n",
            "strain 0.06302439421415329\n",
            "classify 1.9833984375\n",
            "classify 1.9803466796875\n",
            "classify 2.103515625\n",
            "classify 2.0374755859375\n",
            "classify 2.06201171875\n",
            "classify 2.0740966796875\n",
            "classify 2.041259765625\n",
            "classify 1.9967041015625\n",
            "classify 2.081787109375\n",
            "classify 2.0360107421875\n",
            "classify 2.01654052734375\n",
            "0.1875\n",
            "0.234375\n",
            "0.3125\n",
            "0.28125\n",
            "0.234375\n",
            "0.21875\n",
            "0.234375\n",
            "0.125\n",
            "0.28125\n",
            "0.25\n",
            "0.28125\n",
            "time: 2.4405858516693115 2.4996018174082733\n",
            "172\n",
            "strain 0.04830325022339821\n",
            "strain 0.08168797194957733\n",
            "strain 0.06599429994821548\n",
            "strain 0.03880617395043373\n",
            "strain 0.049617987126111984\n",
            "strain 0.034697141498327255\n",
            "classify 2.1409912109375\n",
            "classify 2.02276611328125\n",
            "classify 1.98834228515625\n",
            "classify 2.2781982421875\n",
            "classify 2.042236328125\n",
            "classify 2.1475830078125\n",
            "classify 2.05615234375\n",
            "classify 2.0838623046875\n",
            "classify 2.0491943359375\n",
            "classify 1.978515625\n",
            "classify 2.02099609375\n",
            "0.265625\n",
            "0.21875\n",
            "0.3125\n",
            "0.265625\n",
            "0.171875\n",
            "0.1875\n",
            "0.25\n",
            "0.265625\n",
            "0.34375\n",
            "0.25\n",
            "0.1875\n",
            "time: 2.4210808277130127 2.4991510851534806\n",
            "173\n",
            "strain 0.07087769359350204\n",
            "strain 0.08858492225408554\n",
            "strain 0.08966071903705597\n",
            "strain 0.056060921400785446\n",
            "strain 0.20323209464550018\n",
            "strain 0.06211695820093155\n",
            "classify 1.9276123046875\n",
            "classify 2.0869140625\n",
            "classify 2.0028076171875\n",
            "classify 2.1309814453125\n",
            "classify 2.0687255859375\n",
            "classify 2.04931640625\n",
            "classify 1.90380859375\n",
            "classify 2.0943603515625\n",
            "classify 1.9652099609375\n",
            "classify 1.9666748046875\n",
            "classify 1.96044921875\n",
            "0.25\n",
            "0.234375\n",
            "0.28125\n",
            "0.203125\n",
            "0.25\n",
            "0.28125\n",
            "0.234375\n",
            "0.296875\n",
            "0.203125\n",
            "0.234375\n",
            "0.171875\n",
            "time: 2.7056305408477783 2.5003411578035903\n",
            "174\n",
            "strain 0.1767147034406662\n",
            "strain 0.21119393408298492\n",
            "strain 0.08191417157649994\n",
            "strain 0.08655949681997299\n",
            "strain 0.07053875923156738\n",
            "strain 0.08508464694023132\n",
            "classify 1.86090087890625\n",
            "classify 1.9576416015625\n",
            "classify 2.0626220703125\n",
            "classify 2.011474609375\n",
            "classify 2.0904541015625\n",
            "classify 2.026123046875\n",
            "classify 2.11102294921875\n",
            "classify 1.94854736328125\n",
            "classify 2.05810546875\n",
            "classify 2.07000732421875\n",
            "classify 1.9566650390625\n",
            "0.359375\n",
            "0.171875\n",
            "0.203125\n",
            "0.203125\n",
            "0.28125\n",
            "0.203125\n",
            "0.28125\n",
            "0.265625\n",
            "0.234375\n",
            "0.21875\n",
            "0.15625\n",
            "time: 2.5774576663970947 2.500788324901036\n",
            "175\n",
            "strain 0.037947878241539\n",
            "strain 0.05698820948600769\n",
            "strain 0.07427509129047394\n",
            "strain 0.08036120980978012\n",
            "strain 0.06932826340198517\n",
            "strain 0.06022898852825165\n",
            "classify 2.10791015625\n",
            "classify 2.135009765625\n",
            "classify 2.00762939453125\n",
            "classify 1.9813232421875\n",
            "classify 2.0242919921875\n",
            "classify 1.9891357421875\n",
            "classify 2.04718017578125\n",
            "classify 2.20458984375\n",
            "classify 1.9852294921875\n",
            "classify 1.957275390625\n",
            "classify 2.171142578125\n",
            "0.296875\n",
            "0.21875\n",
            "0.21875\n",
            "0.234375\n",
            "0.171875\n",
            "0.265625\n",
            "0.171875\n",
            "0.296875\n",
            "0.28125\n",
            "0.21875\n",
            "0.28125\n",
            "time: 2.3979246616363525 2.5002071153033865\n",
            "176\n",
            "strain 0.07015647739171982\n",
            "strain 0.0754317194223404\n",
            "strain 0.04050374776124954\n",
            "strain 0.05146526172757149\n",
            "strain 0.047969747334718704\n",
            "strain 0.07820411771535873\n",
            "classify 2.0435791015625\n",
            "classify 2.09063720703125\n",
            "classify 2.0499267578125\n",
            "classify 2.1070556640625\n",
            "classify 2.03460693359375\n",
            "classify 2.0382080078125\n",
            "classify 2.00567626953125\n",
            "classify 2.161865234375\n",
            "classify 2.2041015625\n",
            "classify 2.08258056640625\n",
            "classify 2.151611328125\n",
            "0.28125\n",
            "0.28125\n",
            "0.28125\n",
            "0.171875\n",
            "0.234375\n",
            "0.3125\n",
            "0.328125\n",
            "0.296875\n",
            "0.203125\n",
            "0.25\n",
            "0.296875\n",
            "time: 2.4057300090789795 2.4996767259587003\n",
            "177\n",
            "strain 0.048199236392974854\n",
            "strain 0.09471941739320755\n",
            "strain 0.10343946516513824\n",
            "strain 0.08010266721248627\n",
            "strain 0.045226965099573135\n",
            "strain 0.044672925025224686\n",
            "classify 2.1114501953125\n",
            "classify 2.0198974609375\n",
            "classify 2.0010986328125\n",
            "classify 2.102783203125\n",
            "classify 2.157958984375\n",
            "classify 2.078857421875\n",
            "classify 2.0645751953125\n",
            "classify 2.039794921875\n",
            "classify 1.986083984375\n",
            "classify 2.145263671875\n",
            "classify 2.0411376953125\n",
            "0.21875\n",
            "0.34375\n",
            "0.15625\n",
            "0.296875\n",
            "0.25\n",
            "0.265625\n",
            "0.28125\n",
            "0.265625\n",
            "0.25\n",
            "0.390625\n",
            "0.296875\n",
            "time: 2.4118640422821045 2.4991867729787076\n",
            "178\n",
            "strain 0.0652824118733406\n",
            "strain 0.2704523503780365\n",
            "strain 0.046197421848773956\n",
            "strain 0.05429110303521156\n",
            "strain 0.05071577802300453\n",
            "strain 0.040250591933727264\n",
            "classify 2.2169189453125\n",
            "classify 2.04351806640625\n",
            "classify 2.1549072265625\n",
            "classify 2.2337646484375\n",
            "classify 2.09130859375\n",
            "classify 1.96197509765625\n",
            "classify 2.120849609375\n",
            "classify 2.13214111328125\n",
            "classify 2.0274658203125\n",
            "classify 2.039794921875\n",
            "classify 2.04296875\n",
            "0.171875\n",
            "0.25\n",
            "0.140625\n",
            "0.265625\n",
            "0.1875\n",
            "0.3125\n",
            "0.265625\n",
            "0.265625\n",
            "0.359375\n",
            "0.265625\n",
            "0.359375\n",
            "time: 2.706555128097534 2.5003488809702783\n",
            "179\n",
            "strain 0.036214184015989304\n",
            "strain 0.0829245001077652\n",
            "strain 0.04533812031149864\n",
            "strain 0.056035853922367096\n",
            "strain 0.33667200803756714\n",
            "strain 0.08458107709884644\n",
            "classify 1.9912109375\n",
            "classify 2.00250244140625\n",
            "classify 2.1146240234375\n",
            "classify 1.9940185546875\n",
            "classify 2.0013427734375\n",
            "classify 2.0616455078125\n",
            "classify 1.9566650390625\n",
            "classify 1.8460693359375\n",
            "classify 2.100830078125\n",
            "classify 2.06195068359375\n",
            "classify 2.079833984375\n",
            "0.25\n",
            "0.21875\n",
            "0.28125\n",
            "0.3125\n",
            "0.28125\n",
            "0.21875\n",
            "0.3125\n",
            "0.234375\n",
            "0.296875\n",
            "0.265625\n",
            "0.234375\n",
            "time: 2.5517642498016357 2.500643797715505\n",
            "180\n",
            "strain 0.07239603996276855\n",
            "strain 0.12817175686359406\n",
            "strain 0.046012841165065765\n",
            "strain 0.05852923169732094\n",
            "strain 0.1587044894695282\n",
            "strain 0.10794999450445175\n",
            "classify 2.128662109375\n",
            "classify 2.10626220703125\n",
            "classify 2.0086669921875\n",
            "classify 2.052734375\n",
            "classify 2.0941162109375\n",
            "classify 2.0360107421875\n",
            "classify 2.0582275390625\n",
            "classify 2.13916015625\n",
            "classify 2.107177734375\n",
            "classify 2.0091552734375\n",
            "classify 2.102294921875\n",
            "0.265625\n",
            "0.15625\n",
            "0.296875\n",
            "0.328125\n",
            "0.25\n",
            "0.28125\n",
            "0.234375\n",
            "0.28125\n",
            "0.3125\n",
            "0.21875\n",
            "0.265625\n",
            "time: 2.395982027053833 2.5000686395234166\n",
            "181\n",
            "strain 0.04674345627427101\n",
            "strain 0.04283886030316353\n",
            "strain 0.052876535803079605\n",
            "strain 0.33721068501472473\n",
            "strain 0.047090064734220505\n",
            "strain 0.043308548629283905\n",
            "classify 2.15643310546875\n",
            "classify 2.1082763671875\n",
            "classify 2.023193359375\n",
            "classify 1.96142578125\n",
            "classify 1.92425537109375\n",
            "classify 2.1175537109375\n",
            "classify 2.0511474609375\n",
            "classify 2.051025390625\n",
            "classify 1.931640625\n",
            "classify 1.9625244140625\n",
            "classify 2.0765380859375\n",
            "0.234375\n",
            "0.25\n",
            "0.203125\n",
            "0.15625\n",
            "0.296875\n",
            "0.25\n",
            "0.265625\n",
            "0.21875\n",
            "0.375\n",
            "0.203125\n",
            "0.25\n",
            "time: 2.3936469554901123 2.4994871603263604\n",
            "182\n",
            "strain 0.09335261583328247\n",
            "strain 0.20095588266849518\n",
            "strain 0.04695575684309006\n",
            "strain 0.10107070207595825\n",
            "strain 0.06178809702396393\n",
            "strain 0.0961577519774437\n",
            "classify 2.111083984375\n",
            "classify 2.1781005859375\n",
            "classify 1.978759765625\n",
            "classify 2.045654296875\n",
            "classify 2.015625\n",
            "classify 2.03839111328125\n",
            "classify 2.0006103515625\n",
            "classify 2.01678466796875\n",
            "classify 2.158203125\n",
            "classify 2.0831298828125\n",
            "classify 2.0560302734375\n",
            "0.21875\n",
            "0.296875\n",
            "0.296875\n",
            "0.28125\n",
            "0.296875\n",
            "0.328125\n",
            "0.203125\n",
            "0.296875\n",
            "0.171875\n",
            "0.21875\n",
            "0.328125\n",
            "time: 2.3888487815856934 2.4988855364544142\n",
            "183\n",
            "strain 0.16183780133724213\n",
            "strain 0.042456287890672684\n",
            "strain 0.07970872521400452\n",
            "strain 0.04198472946882248\n",
            "strain 0.08992254734039307\n",
            "strain 0.11911351978778839\n",
            "classify 2.12255859375\n",
            "classify 1.989501953125\n",
            "classify 1.979248046875\n",
            "classify 1.992919921875\n",
            "classify 2.0677490234375\n",
            "classify 1.9791259765625\n",
            "classify 1.9896240234375\n",
            "classify 2.00433349609375\n",
            "classify 2.063720703125\n",
            "classify 2.103759765625\n",
            "classify 2.03924560546875\n",
            "0.171875\n",
            "0.328125\n",
            "0.3125\n",
            "0.25\n",
            "0.25\n",
            "0.265625\n",
            "0.328125\n",
            "0.140625\n",
            "0.140625\n",
            "0.34375\n",
            "0.28125\n",
            "time: 2.7425522804260254 2.5002129388892134\n",
            "184\n",
            "strain 0.05855763331055641\n",
            "strain 0.10188629478216171\n",
            "strain 0.061248522251844406\n",
            "strain 0.21856607496738434\n",
            "strain 0.1141340509057045\n",
            "strain 0.0573582760989666\n",
            "classify 2.03460693359375\n",
            "classify 2.0244140625\n",
            "classify 2.0499267578125\n",
            "classify 1.99713134765625\n",
            "classify 2.1427001953125\n",
            "classify 2.03155517578125\n",
            "classify 2.1719970703125\n",
            "classify 2.0313720703125\n",
            "classify 2.0013427734375\n",
            "classify 1.9979248046875\n",
            "classify 2.16259765625\n",
            "0.375\n",
            "0.25\n",
            "0.234375\n",
            "0.25\n",
            "0.1875\n",
            "0.28125\n",
            "0.234375\n",
            "0.3125\n",
            "0.1875\n",
            "0.28125\n",
            "0.21875\n",
            "time: 2.531223773956299 2.500384121972161\n",
            "185\n",
            "strain 0.14669394493103027\n",
            "strain 0.08134452253580093\n",
            "strain 0.037546783685684204\n",
            "strain 0.09018654376268387\n",
            "strain 0.049257129430770874\n",
            "strain 0.08822772651910782\n",
            "classify 1.9735107421875\n",
            "classify 2.1197509765625\n",
            "classify 2.17919921875\n",
            "classify 2.072265625\n",
            "classify 2.077880859375\n",
            "classify 2.1221923828125\n",
            "classify 2.12744140625\n",
            "classify 2.139404296875\n",
            "classify 2.1611328125\n",
            "classify 2.076416015625\n",
            "classify 2.0477294921875\n",
            "0.296875\n",
            "0.25\n",
            "0.296875\n",
            "0.21875\n",
            "0.21875\n",
            "0.25\n",
            "0.234375\n",
            "0.3125\n",
            "0.15625\n",
            "0.203125\n",
            "0.296875\n",
            "time: 2.3559465408325195 2.4996106675876084\n",
            "186\n",
            "strain 0.0382603295147419\n",
            "strain 0.0592997744679451\n",
            "strain 0.34290215373039246\n",
            "strain 0.07570510357618332\n",
            "strain 0.05576276034116745\n",
            "strain 0.17642389237880707\n",
            "classify 2.02392578125\n",
            "classify 2.1656494140625\n",
            "classify 2.00262451171875\n",
            "classify 1.960205078125\n",
            "classify 2.0357666015625\n",
            "classify 1.91961669921875\n",
            "classify 2.005126953125\n",
            "classify 2.021484375\n",
            "classify 2.0032958984375\n",
            "classify 1.9879150390625\n",
            "classify 2.1676025390625\n",
            "0.25\n",
            "0.25\n",
            "0.234375\n",
            "0.296875\n",
            "0.25\n",
            "0.171875\n",
            "0.234375\n",
            "0.171875\n",
            "0.171875\n",
            "0.296875\n",
            "0.1875\n",
            "time: 2.42114520072937 2.4991940508551775\n",
            "187\n",
            "strain 0.04484167695045471\n",
            "strain 0.048952169716358185\n",
            "strain 0.05407233163714409\n",
            "strain 0.055936455726623535\n",
            "strain 0.05611991137266159\n",
            "strain 0.12137316912412643\n",
            "classify 2.0177001953125\n",
            "classify 1.9327392578125\n",
            "classify 2.090087890625\n",
            "classify 2.0494384765625\n",
            "classify 1.98907470703125\n",
            "classify 2.0269775390625\n",
            "classify 2.1162109375\n",
            "classify 2.06201171875\n",
            "classify 2.1458740234375\n",
            "classify 2.03399658203125\n",
            "classify 1.98138427734375\n",
            "0.234375\n",
            "0.203125\n",
            "0.40625\n",
            "0.25\n",
            "0.25\n",
            "0.15625\n",
            "0.25\n",
            "0.203125\n",
            "0.296875\n",
            "0.234375\n",
            "0.34375\n",
            "time: 2.386681079864502 2.498598575592041\n",
            "188\n",
            "strain 0.05034526437520981\n",
            "strain 0.04311193898320198\n",
            "strain 0.047012414783239365\n",
            "strain 0.07102604955434799\n",
            "strain 0.1715802103281021\n",
            "strain 0.07829049229621887\n",
            "classify 1.9508056640625\n",
            "classify 2.07470703125\n",
            "classify 1.9632568359375\n",
            "classify 1.9000244140625\n",
            "classify 2.09259033203125\n",
            "classify 2.1549072265625\n",
            "classify 2.09326171875\n",
            "classify 1.91461181640625\n",
            "classify 1.9735107421875\n",
            "classify 1.82373046875\n",
            "classify 2.055908203125\n",
            "0.296875\n",
            "0.265625\n",
            "0.28125\n",
            "0.25\n",
            "0.390625\n",
            "0.296875\n",
            "0.21875\n",
            "0.203125\n",
            "0.234375\n",
            "0.28125\n",
            "0.3125\n",
            "time: 2.6690642833709717 2.4995034094210022\n",
            "189\n",
            "strain 0.11545910686254501\n",
            "strain 0.059713102877140045\n",
            "strain 0.08801741153001785\n",
            "strain 0.1021743044257164\n",
            "strain 0.16184870898723602\n",
            "strain 0.055152591317892075\n",
            "classify 1.95989990234375\n",
            "classify 2.1500244140625\n",
            "classify 2.09326171875\n",
            "classify 2.02435302734375\n",
            "classify 2.0345458984375\n",
            "classify 2.0806884765625\n",
            "classify 2.10565185546875\n",
            "classify 2.0980224609375\n",
            "classify 2.0123291015625\n",
            "classify 2.04150390625\n",
            "classify 1.97833251953125\n",
            "0.3125\n",
            "0.296875\n",
            "0.265625\n",
            "0.234375\n",
            "0.28125\n",
            "0.265625\n",
            "0.296875\n",
            "0.359375\n",
            "0.3125\n",
            "0.3125\n",
            "0.328125\n",
            "time: 2.5862655639648438 2.49996472408897\n",
            "190\n",
            "strain 0.4295577108860016\n",
            "strain 0.07791637629270554\n",
            "strain 0.124823197722435\n",
            "strain 0.05343189463019371\n",
            "strain 0.07502655684947968\n",
            "strain 0.14373183250427246\n",
            "classify 2.00152587890625\n",
            "classify 2.06689453125\n",
            "classify 1.94354248046875\n",
            "classify 2.055419921875\n",
            "classify 2.021728515625\n",
            "classify 2.08868408203125\n",
            "classify 1.9677734375\n",
            "classify 1.9683837890625\n",
            "classify 1.96490478515625\n",
            "classify 2.1243896484375\n",
            "classify 2.05169677734375\n",
            "0.328125\n",
            "0.28125\n",
            "0.265625\n",
            "0.25\n",
            "0.234375\n",
            "0.3125\n",
            "0.21875\n",
            "0.15625\n",
            "0.265625\n",
            "0.21875\n",
            "0.296875\n",
            "time: 2.38710880279541 2.4993769198812115\n",
            "191\n",
            "strain 0.08197428286075592\n",
            "strain 0.03932072967290878\n",
            "strain 0.14253878593444824\n",
            "strain 0.3370921015739441\n",
            "strain 0.05791129171848297\n",
            "strain 0.04417438805103302\n",
            "classify 1.9949951171875\n",
            "classify 2.025634765625\n",
            "classify 2.0330810546875\n",
            "classify 1.9361572265625\n",
            "classify 2.108642578125\n",
            "classify 2.11138916015625\n",
            "classify 2.0440673828125\n",
            "classify 2.08197021484375\n",
            "classify 2.0816650390625\n",
            "classify 2.0557861328125\n",
            "classify 2.1707763671875\n",
            "0.203125\n",
            "0.25\n",
            "0.203125\n",
            "0.21875\n",
            "0.1875\n",
            "0.234375\n",
            "0.234375\n",
            "0.234375\n",
            "0.21875\n",
            "0.3125\n",
            "0.1875\n",
            "time: 2.3835513591766357 2.498776869227489\n",
            "192\n",
            "strain 0.04866751655936241\n",
            "strain 0.04348080977797508\n",
            "strain 0.0695435181260109\n",
            "strain 0.04479273036122322\n",
            "strain 0.11838226020336151\n",
            "strain 0.22054694592952728\n",
            "classify 2.029296875\n",
            "classify 1.959228515625\n",
            "classify 2.10601806640625\n",
            "classify 2.0950927734375\n",
            "classify 2.007568359375\n",
            "classify 2.1239013671875\n",
            "classify 1.9764404296875\n",
            "classify 2.187744140625\n",
            "classify 2.018310546875\n",
            "classify 2.16796875\n",
            "classify 2.0240478515625\n",
            "0.21875\n",
            "0.328125\n",
            "0.28125\n",
            "0.21875\n",
            "0.171875\n",
            "0.1875\n",
            "0.3125\n",
            "0.265625\n",
            "0.203125\n",
            "0.25\n",
            "0.296875\n",
            "time: 2.4214322566986084 2.4983791403202193\n",
            "193\n",
            "strain 0.14970102906227112\n",
            "strain 0.14787811040878296\n",
            "strain 0.04445793852210045\n",
            "strain 0.18712686002254486\n",
            "strain 0.04473065212368965\n",
            "strain 0.08241254091262817\n",
            "classify 2.042724609375\n",
            "classify 2.04656982421875\n",
            "classify 2.02978515625\n",
            "classify 2.0601806640625\n",
            "classify 2.0130615234375\n",
            "classify 1.9713134765625\n",
            "classify 1.89849853515625\n",
            "classify 2.1092529296875\n",
            "classify 2.091796875\n",
            "classify 2.0228271484375\n",
            "classify 2.063232421875\n",
            "0.15625\n",
            "0.25\n",
            "0.3125\n",
            "0.265625\n",
            "0.265625\n",
            "0.25\n",
            "0.28125\n",
            "0.28125\n",
            "0.234375\n",
            "0.28125\n",
            "0.265625\n",
            "time: 2.626957654953003 2.4990449050038133\n",
            "194\n",
            "strain 0.0898226648569107\n",
            "strain 0.048409152776002884\n",
            "strain 0.0547461062669754\n",
            "strain 0.056522294878959656\n",
            "strain 0.36701518297195435\n",
            "strain 0.06643908470869064\n",
            "classify 2.00970458984375\n",
            "classify 1.939208984375\n",
            "classify 2.0087890625\n",
            "classify 2.1451416015625\n",
            "classify 2.0245361328125\n",
            "classify 2.0330810546875\n",
            "classify 2.01727294921875\n",
            "classify 2.12939453125\n",
            "classify 2.0140380859375\n",
            "classify 1.9818115234375\n",
            "classify 2.0712890625\n",
            "0.203125\n",
            "0.34375\n",
            "0.203125\n",
            "0.25\n",
            "0.265625\n",
            "0.28125\n",
            "0.21875\n",
            "0.328125\n",
            "0.234375\n",
            "0.28125\n",
            "0.203125\n",
            "time: 2.5831592082977295 2.499478704501421\n",
            "195\n",
            "strain 0.16241095960140228\n",
            "strain 0.046840693801641464\n",
            "strain 0.07520352303981781\n",
            "strain 0.04730512201786041\n",
            "strain 0.04895574226975441\n",
            "strain 0.05028166249394417\n",
            "classify 2.0455322265625\n",
            "classify 2.0269775390625\n",
            "classify 2.0799560546875\n",
            "classify 2.03411865234375\n",
            "classify 2.10205078125\n",
            "classify 2.06646728515625\n",
            "classify 1.992919921875\n",
            "classify 2.01348876953125\n",
            "classify 2.0931396484375\n",
            "classify 1.980224609375\n",
            "classify 1.99029541015625\n",
            "0.265625\n",
            "0.328125\n",
            "0.234375\n",
            "0.25\n",
            "0.296875\n",
            "0.25\n",
            "0.3125\n",
            "0.234375\n",
            "0.375\n",
            "0.265625\n",
            "0.25\n",
            "time: 2.4072301387786865 2.499011373033329\n",
            "196\n",
            "strain 0.06593012064695358\n",
            "strain 0.07423067092895508\n",
            "strain 0.12158433347940445\n",
            "strain 0.14930255711078644\n",
            "strain 0.050229091197252274\n",
            "strain 0.05802204832434654\n",
            "classify 2.0126953125\n",
            "classify 2.076171875\n",
            "classify 1.982177734375\n",
            "classify 1.9327392578125\n",
            "classify 1.93707275390625\n",
            "classify 2.052978515625\n",
            "classify 1.889892578125\n",
            "classify 1.84735107421875\n",
            "classify 2.052001953125\n",
            "classify 1.96124267578125\n",
            "classify 1.9549560546875\n",
            "0.28125\n",
            "0.21875\n",
            "0.359375\n",
            "0.3125\n",
            "0.3125\n",
            "0.1875\n",
            "0.328125\n",
            "0.3125\n",
            "0.28125\n",
            "0.34375\n",
            "0.1875\n",
            "time: 2.4046742916107178 2.4985355580518696\n",
            "197\n",
            "strain 0.05380583927035332\n",
            "strain 0.0405307337641716\n",
            "strain 0.10155098885297775\n",
            "strain 0.05791167542338371\n",
            "strain 0.05871199443936348\n",
            "strain 0.08103328198194504\n",
            "classify 2.12982177734375\n",
            "classify 2.1138916015625\n",
            "classify 2.05816650390625\n",
            "classify 2.167236328125\n",
            "classify 2.1043701171875\n",
            "classify 1.98309326171875\n",
            "classify 2.1090087890625\n",
            "classify 2.009033203125\n",
            "classify 1.97369384765625\n",
            "classify 1.98565673828125\n",
            "classify 2.0833740234375\n",
            "0.25\n",
            "0.28125\n",
            "0.359375\n",
            "0.3125\n",
            "0.234375\n",
            "0.34375\n",
            "0.25\n",
            "0.15625\n",
            "0.3125\n",
            "0.21875\n",
            "0.234375\n",
            "time: 2.3858256340026855 2.4979690409669972\n",
            "198\n",
            "strain 0.15995872020721436\n",
            "strain 0.05032240226864815\n",
            "strain 0.21681319177150726\n",
            "strain 0.045583903789520264\n",
            "strain 0.05587898939847946\n",
            "strain 0.25296542048454285\n",
            "classify 1.97686767578125\n",
            "classify 1.974609375\n",
            "classify 2.146728515625\n",
            "classify 1.93402099609375\n",
            "classify 1.987060546875\n",
            "classify 1.9598388671875\n",
            "classify 1.95233154296875\n",
            "classify 2.06024169921875\n",
            "classify 2.048828125\n",
            "classify 2.0836181640625\n",
            "classify 1.94732666015625\n",
            "0.171875\n",
            "0.28125\n",
            "0.1875\n",
            "0.296875\n",
            "0.34375\n",
            "0.234375\n",
            "0.265625\n",
            "0.3125\n",
            "0.265625\n",
            "0.328125\n",
            "0.234375\n",
            "time: 2.689802885055542 2.4989371012203656\n",
            "199\n",
            "strain 0.06516344845294952\n",
            "strain 0.0605865903198719\n",
            "strain 0.1036515086889267\n",
            "strain 0.054134417325258255\n",
            "strain 0.04841665178537369\n",
            "strain 0.12805186212062836\n",
            "classify 2.0877685546875\n",
            "classify 2.0411376953125\n",
            "classify 2.0634765625\n",
            "classify 2.0186767578125\n",
            "classify 2.07122802734375\n",
            "classify 2.10595703125\n",
            "classify 2.0338134765625\n",
            "classify 2.021240234375\n",
            "classify 1.93963623046875\n",
            "classify 2.11590576171875\n",
            "classify 1.990234375\n",
            "0.140625\n",
            "0.21875\n",
            "0.203125\n",
            "0.328125\n",
            "0.1875\n",
            "0.3125\n",
            "0.109375\n",
            "0.28125\n",
            "0.328125\n",
            "0.25\n",
            "0.21875\n",
            "time: 2.5730807781219482 2.499310337305069\n",
            "200\n",
            "strain 0.03582129627466202\n",
            "strain 0.03780283033847809\n",
            "strain 0.044501177966594696\n",
            "strain 0.06769915670156479\n",
            "strain 0.05525318905711174\n",
            "strain 0.041837986558675766\n",
            "classify 2.0430908203125\n",
            "classify 2.070556640625\n",
            "classify 2.0528564453125\n",
            "classify 2.068603515625\n",
            "classify 2.16015625\n",
            "classify 2.1248779296875\n",
            "classify 1.9549560546875\n",
            "classify 2.145263671875\n",
            "classify 2.00634765625\n",
            "classify 1.93994140625\n",
            "classify 1.9473876953125\n",
            "0.234375\n",
            "0.296875\n",
            "0.328125\n",
            "0.171875\n",
            "0.328125\n",
            "0.359375\n",
            "0.1875\n",
            "0.25\n",
            "0.265625\n",
            "0.296875\n",
            "0.1875\n",
            "time: 2.384232997894287 2.4987405615659495\n",
            "201\n",
            "strain 0.08775091171264648\n",
            "strain 0.109716035425663\n",
            "strain 0.04185262694954872\n",
            "strain 0.22751639783382416\n",
            "strain 0.07841078191995621\n",
            "strain 0.03926091268658638\n",
            "classify 2.00799560546875\n",
            "classify 2.0489501953125\n",
            "classify 2.0576171875\n",
            "classify 2.0069580078125\n",
            "classify 1.9521484375\n",
            "classify 2.036865234375\n",
            "classify 2.0531005859375\n",
            "classify 2.041259765625\n",
            "classify 2.1611328125\n",
            "classify 2.0526123046875\n",
            "classify 2.0174560546875\n",
            "0.28125\n",
            "0.1875\n",
            "0.203125\n",
            "0.1875\n",
            "0.3125\n",
            "0.265625\n",
            "0.28125\n",
            "0.28125\n",
            "0.21875\n",
            "0.25\n",
            "0.3125\n",
            "time: 2.4211676120758057 2.498359145504413\n",
            "202\n",
            "strain 0.03629500791430473\n",
            "strain 0.10661442577838898\n",
            "strain 0.04275595024228096\n",
            "strain 0.06637091934680939\n",
            "strain 0.10497891157865524\n",
            "strain 0.05854099243879318\n",
            "classify 1.9554443359375\n",
            "classify 1.96209716796875\n",
            "classify 2.160888671875\n",
            "classify 1.993408203125\n",
            "classify 2.12091064453125\n",
            "classify 2.0577392578125\n",
            "classify 1.898193359375\n",
            "classify 2.015380859375\n",
            "classify 2.1168212890625\n",
            "classify 2.0517578125\n",
            "classify 2.03485107421875\n",
            "0.21875\n",
            "0.3125\n",
            "0.296875\n",
            "0.21875\n",
            "0.3125\n",
            "0.296875\n",
            "0.265625\n",
            "0.203125\n",
            "0.296875\n",
            "0.390625\n",
            "0.234375\n",
            "time: 2.4350790977478027 2.4980500120247524\n",
            "203\n",
            "strain 0.08805623650550842\n",
            "strain 0.12946444749832153\n",
            "strain 0.11330098658800125\n",
            "strain 0.04826392978429794\n",
            "strain 0.07262694835662842\n",
            "strain 0.07877466827630997\n",
            "classify 2.041015625\n",
            "classify 1.94415283203125\n",
            "classify 2.006591796875\n",
            "classify 2.15435791015625\n",
            "classify 2.00238037109375\n",
            "classify 2.14349365234375\n",
            "classify 2.00634765625\n",
            "classify 2.064208984375\n",
            "classify 2.003173828125\n",
            "classify 2.1307373046875\n",
            "classify 1.9869384765625\n",
            "0.296875\n",
            "0.25\n",
            "0.28125\n",
            "0.21875\n",
            "0.28125\n",
            "0.3125\n",
            "0.21875\n",
            "0.21875\n",
            "0.234375\n",
            "0.15625\n",
            "0.265625\n",
            "time: 2.6449170112609863 2.4987728852851716\n",
            "204\n",
            "strain 0.0554584302008152\n",
            "strain 0.18882717192173004\n",
            "strain 0.04331246018409729\n",
            "strain 0.047627709805965424\n",
            "strain 0.09593997895717621\n",
            "strain 0.04085668548941612\n",
            "classify 1.95574951171875\n",
            "classify 2.07562255859375\n",
            "classify 2.00115966796875\n",
            "classify 1.982177734375\n",
            "classify 2.10296630859375\n",
            "classify 1.984130859375\n",
            "classify 2.1263427734375\n",
            "classify 2.017822265625\n",
            "classify 1.980224609375\n",
            "classify 2.1407470703125\n",
            "classify 2.0440673828125\n",
            "0.3125\n",
            "0.203125\n",
            "0.34375\n",
            "0.1875\n",
            "0.25\n",
            "0.296875\n",
            "0.28125\n",
            "0.3125\n",
            "0.296875\n",
            "0.171875\n",
            "0.265625\n",
            "time: 2.5898735523223877 2.4992218924731744\n",
            "205\n",
            "strain 0.045123402029275894\n",
            "strain 0.11604045331478119\n",
            "strain 0.050273068249225616\n",
            "strain 0.12779447436332703\n",
            "strain 0.05949632078409195\n",
            "strain 0.03978810831904411\n",
            "classify 2.0570068359375\n",
            "classify 2.040283203125\n",
            "classify 1.9324951171875\n",
            "classify 2.14288330078125\n",
            "classify 2.0211181640625\n",
            "classify 2.0472412109375\n",
            "classify 2.14599609375\n",
            "classify 2.02069091796875\n",
            "classify 1.99542236328125\n",
            "classify 2.05859375\n",
            "classify 1.950927734375\n",
            "0.28125\n",
            "0.28125\n",
            "0.25\n",
            "0.21875\n",
            "0.25\n",
            "0.34375\n",
            "0.25\n",
            "0.28125\n",
            "0.28125\n",
            "0.34375\n",
            "0.234375\n",
            "time: 2.4142889976501465 2.4988123213203206\n",
            "206\n",
            "strain 0.04824382811784744\n",
            "strain 0.06372897326946259\n",
            "strain 0.07778080552816391\n",
            "strain 0.08312145620584488\n",
            "strain 0.055801015347242355\n",
            "strain 0.10609088093042374\n",
            "classify 1.9718017578125\n",
            "classify 2.097900390625\n",
            "classify 1.93206787109375\n",
            "classify 1.88702392578125\n",
            "classify 2.15185546875\n",
            "classify 2.17822265625\n",
            "classify 2.1654052734375\n",
            "classify 2.00341796875\n",
            "classify 1.8787841796875\n",
            "classify 1.9365234375\n",
            "classify 1.935302734375\n",
            "0.265625\n",
            "0.375\n",
            "0.328125\n",
            "0.234375\n",
            "0.265625\n",
            "0.28125\n",
            "0.140625\n",
            "0.21875\n",
            "0.296875\n",
            "0.265625\n",
            "0.265625\n",
            "time: 2.403859853744507 2.4983564482794867\n",
            "207\n",
            "strain 0.09379315376281738\n",
            "strain 0.04941483959555626\n",
            "strain 0.09219682216644287\n",
            "strain 0.05982447788119316\n",
            "strain 0.09692980349063873\n",
            "strain 0.04099803790450096\n",
            "classify 2.0189208984375\n",
            "classify 1.93695068359375\n",
            "classify 2.085693359375\n",
            "classify 2.0208740234375\n",
            "classify 2.05389404296875\n",
            "classify 2.104736328125\n",
            "classify 1.9818115234375\n",
            "classify 2.149169921875\n",
            "classify 2.08831787109375\n",
            "classify 2.1080322265625\n",
            "classify 2.034423828125\n",
            "0.296875\n",
            "0.34375\n",
            "0.34375\n",
            "0.296875\n",
            "0.21875\n",
            "0.265625\n",
            "0.296875\n",
            "0.265625\n",
            "0.171875\n",
            "0.359375\n",
            "0.34375\n",
            "time: 2.416292428970337 2.4979645048196497\n",
            "208\n",
            "strain 0.08195211738348007\n",
            "strain 0.06502678990364075\n",
            "strain 0.0738319382071495\n",
            "strain 0.07207662612199783\n",
            "strain 0.06097884848713875\n",
            "strain 0.2581775486469269\n",
            "classify 2.028564453125\n",
            "classify 2.083984375\n",
            "classify 1.9541015625\n",
            "classify 2.02783203125\n",
            "classify 1.9637451171875\n",
            "classify 2.15472412109375\n",
            "classify 1.96240234375\n",
            "classify 2.074951171875\n",
            "classify 2.110595703125\n",
            "classify 1.991943359375\n",
            "classify 1.93328857421875\n",
            "0.125\n",
            "0.296875\n",
            "0.296875\n",
            "0.296875\n",
            "0.1875\n",
            "0.21875\n",
            "0.171875\n",
            "0.25\n",
            "0.34375\n",
            "0.265625\n",
            "0.3125\n",
            "time: 2.6575183868408203 2.498730499778638\n",
            "209\n",
            "strain 0.1280188411474228\n",
            "strain 0.04635994881391525\n",
            "strain 0.3730444312095642\n",
            "strain 0.037208136171102524\n",
            "strain 0.05793799087405205\n",
            "strain 0.16608843207359314\n",
            "classify 2.0037841796875\n",
            "classify 2.0172119140625\n",
            "classify 1.9752197265625\n",
            "classify 2.00439453125\n",
            "classify 1.834716796875\n",
            "classify 2.2523193359375\n",
            "classify 2.0250244140625\n",
            "classify 2.0682373046875\n",
            "classify 2.0665283203125\n",
            "classify 1.947265625\n",
            "classify 2.1805419921875\n",
            "0.28125\n",
            "0.25\n",
            "0.328125\n",
            "0.28125\n",
            "0.21875\n",
            "0.1875\n",
            "0.234375\n",
            "0.234375\n",
            "0.359375\n",
            "0.265625\n",
            "0.265625\n",
            "time: 2.594987392425537 2.4991937943867275\n",
            "210\n",
            "strain 0.2007448673248291\n",
            "strain 0.0782792791724205\n",
            "strain 0.10700021684169769\n",
            "strain 0.07430455088615417\n",
            "strain 0.07700604945421219\n",
            "strain 0.07399771362543106\n",
            "classify 2.1424560546875\n",
            "classify 1.95855712890625\n",
            "classify 1.98016357421875\n",
            "classify 1.911376953125\n",
            "classify 2.04071044921875\n",
            "classify 1.9669189453125\n",
            "classify 1.9449462890625\n",
            "classify 2.0035400390625\n",
            "classify 2.082763671875\n",
            "classify 2.04248046875\n",
            "classify 1.96807861328125\n",
            "0.328125\n",
            "0.203125\n",
            "0.25\n",
            "0.3125\n",
            "0.25\n",
            "0.203125\n",
            "0.25\n",
            "0.296875\n",
            "0.265625\n",
            "0.15625\n",
            "0.25\n",
            "time: 2.4040451049804688 2.498745466295577\n",
            "211\n",
            "strain 0.21122360229492188\n",
            "strain 0.1076686754822731\n",
            "strain 0.047775283455848694\n",
            "strain 0.12359132617712021\n",
            "strain 0.17263153195381165\n",
            "strain 0.1372450441122055\n",
            "classify 2.0638427734375\n",
            "classify 2.128173828125\n",
            "classify 2.041259765625\n",
            "classify 1.991455078125\n",
            "classify 1.98406982421875\n",
            "classify 2.0679931640625\n",
            "classify 2.0531005859375\n",
            "classify 1.93023681640625\n",
            "classify 2.049072265625\n",
            "classify 2.06451416015625\n",
            "classify 2.04754638671875\n",
            "0.25\n",
            "0.15625\n",
            "0.296875\n",
            "0.3125\n",
            "0.375\n",
            "0.265625\n",
            "0.3125\n",
            "0.234375\n",
            "0.3125\n",
            "0.265625\n",
            "0.28125\n",
            "time: 2.4357285499572754 2.4984509146438456\n",
            "212\n",
            "strain 0.06253604590892792\n",
            "strain 0.06683338433504105\n",
            "strain 0.1201070174574852\n",
            "strain 0.06498827040195465\n",
            "strain 0.18222084641456604\n",
            "strain 0.04989177733659744\n",
            "classify 2.0994873046875\n",
            "classify 2.02935791015625\n",
            "classify 1.9835205078125\n",
            "classify 1.9454345703125\n",
            "classify 2.058349609375\n",
            "classify 2.148681640625\n",
            "classify 1.89788818359375\n",
            "classify 2.0281982421875\n",
            "classify 1.939697265625\n",
            "classify 2.0565185546875\n",
            "classify 1.990234375\n",
            "0.359375\n",
            "0.203125\n",
            "0.171875\n",
            "0.28125\n",
            "0.203125\n",
            "0.21875\n",
            "0.28125\n",
            "0.296875\n",
            "0.25\n",
            "0.15625\n",
            "0.28125\n",
            "time: 2.397817611694336 2.4979810815461923\n",
            "213\n",
            "strain 0.06477504223585129\n",
            "strain 0.1890668123960495\n",
            "strain 0.048864323645830154\n",
            "strain 0.04366479814052582\n",
            "strain 0.25579261779785156\n",
            "strain 0.03015274368226528\n",
            "classify 2.022216796875\n",
            "classify 1.91815185546875\n",
            "classify 2.0252685546875\n",
            "classify 2.037109375\n",
            "classify 2.11175537109375\n",
            "classify 2.103271484375\n",
            "classify 2.017822265625\n",
            "classify 2.0205078125\n",
            "classify 2.0819091796875\n",
            "classify 2.1361083984375\n",
            "classify 2.0308837890625\n",
            "0.265625\n",
            "0.3125\n",
            "0.21875\n",
            "0.1875\n",
            "0.15625\n",
            "0.296875\n",
            "0.21875\n",
            "0.34375\n",
            "0.234375\n",
            "0.28125\n",
            "0.359375\n",
            "time: 2.661102771759033 2.4987459728650956\n",
            "214\n",
            "strain 0.06866675615310669\n",
            "strain 0.16552190482616425\n",
            "strain 0.08957437425851822\n",
            "strain 0.04206784814596176\n",
            "strain 0.0643734335899353\n",
            "strain 0.04403608664870262\n",
            "classify 2.00677490234375\n",
            "classify 2.061279296875\n",
            "classify 1.98712158203125\n",
            "classify 2.02667236328125\n",
            "classify 2.00384521484375\n",
            "classify 2.0537109375\n",
            "classify 1.998046875\n",
            "classify 2.10028076171875\n",
            "classify 1.997314453125\n",
            "classify 2.10894775390625\n",
            "classify 1.955078125\n",
            "0.1875\n",
            "0.3125\n",
            "0.28125\n",
            "0.234375\n",
            "0.296875\n",
            "0.203125\n",
            "0.171875\n",
            "0.25\n",
            "0.234375\n",
            "0.234375\n",
            "0.25\n",
            "time: 2.598073959350586 2.4992104242014332\n",
            "215\n",
            "strain 0.1542101353406906\n",
            "strain 0.04063670337200165\n",
            "strain 0.04508187621831894\n",
            "strain 0.2588595151901245\n",
            "strain 0.050731707364320755\n",
            "strain 0.10559990257024765\n",
            "classify 1.935546875\n",
            "classify 2.07696533203125\n",
            "classify 2.0919189453125\n",
            "classify 2.11322021484375\n",
            "classify 2.0274658203125\n",
            "classify 2.0643310546875\n",
            "classify 1.9503173828125\n",
            "classify 1.9779052734375\n",
            "classify 1.96197509765625\n",
            "classify 2.01995849609375\n",
            "classify 2.0628662109375\n",
            "0.3125\n",
            "0.171875\n",
            "0.234375\n",
            "0.265625\n",
            "0.171875\n",
            "0.328125\n",
            "0.265625\n",
            "0.359375\n",
            "0.109375\n",
            "0.34375\n",
            "0.3125\n",
            "time: 2.3998725414276123 2.498753215427752\n",
            "216\n",
            "strain 0.08442050963640213\n",
            "strain 0.0465356782078743\n",
            "strain 0.04137992486357689\n",
            "strain 0.1374049186706543\n",
            "strain 0.13390856981277466\n",
            "strain 0.09222406893968582\n",
            "classify 2.0103759765625\n",
            "classify 1.9136962890625\n",
            "classify 1.9837646484375\n",
            "classify 1.8848876953125\n",
            "classify 2.056884765625\n",
            "classify 1.9871826171875\n",
            "classify 2.0804443359375\n",
            "classify 1.97509765625\n",
            "classify 2.0030517578125\n",
            "classify 1.9931640625\n",
            "classify 1.9674072265625\n",
            "0.25\n",
            "0.3125\n",
            "0.265625\n",
            "0.3125\n",
            "0.25\n",
            "0.265625\n",
            "0.25\n",
            "0.28125\n",
            "0.1875\n",
            "0.21875\n",
            "0.328125\n",
            "time: 2.433931589126587 2.498457018680836\n",
            "217\n",
            "strain 0.10337665677070618\n",
            "strain 0.06952717900276184\n",
            "strain 0.04519965499639511\n",
            "strain 0.1541730761528015\n",
            "strain 0.04738842695951462\n",
            "strain 0.03784766420722008\n",
            "classify 2.04644775390625\n",
            "classify 1.97137451171875\n",
            "classify 2.076416015625\n",
            "classify 2.065185546875\n",
            "classify 2.02685546875\n",
            "classify 2.11572265625\n",
            "classify 1.796630859375\n",
            "classify 2.0037841796875\n",
            "classify 1.9283447265625\n",
            "classify 2.02691650390625\n",
            "classify 1.97308349609375\n",
            "0.28125\n",
            "0.328125\n",
            "0.234375\n",
            "0.328125\n",
            "0.21875\n",
            "0.359375\n",
            "0.265625\n",
            "0.296875\n",
            "0.3125\n",
            "0.203125\n",
            "0.3125\n",
            "time: 2.3934550285339355 2.497977973124303\n",
            "218\n",
            "strain 0.15774719417095184\n",
            "strain 0.10986776649951935\n",
            "strain 0.059963688254356384\n",
            "strain 0.05336787924170494\n",
            "strain 0.040359288454055786\n",
            "strain 0.05834995210170746\n",
            "classify 2.0797119140625\n",
            "classify 2.1092529296875\n",
            "classify 2.0162353515625\n",
            "classify 2.00238037109375\n",
            "classify 1.9737548828125\n",
            "classify 1.79705810546875\n",
            "classify 2.0146484375\n",
            "classify 1.9617919921875\n",
            "classify 2.016357421875\n",
            "classify 2.04449462890625\n",
            "classify 2.03955078125\n",
            "0.203125\n",
            "0.328125\n",
            "0.28125\n",
            "0.296875\n",
            "0.296875\n",
            "0.28125\n",
            "0.25\n",
            "0.265625\n",
            "0.3125\n",
            "0.28125\n",
            "0.296875\n",
            "time: 2.6651604175567627 2.4987440512060575\n",
            "219\n",
            "strain 0.1075831726193428\n",
            "strain 0.052551791071891785\n",
            "strain 0.26166772842407227\n",
            "strain 0.06192110478878021\n",
            "strain 0.15265321731567383\n",
            "strain 0.09296588599681854\n",
            "classify 1.92205810546875\n",
            "classify 1.99053955078125\n",
            "classify 2.0029296875\n",
            "classify 2.067138671875\n",
            "classify 2.09625244140625\n",
            "classify 1.88262939453125\n",
            "classify 1.9678955078125\n",
            "classify 2.0174560546875\n",
            "classify 2.03570556640625\n",
            "classify 1.97509765625\n",
            "classify 2.00341796875\n",
            "0.25\n",
            "0.328125\n",
            "0.296875\n",
            "0.28125\n",
            "0.34375\n",
            "0.296875\n",
            "0.375\n",
            "0.234375\n",
            "0.25\n",
            "0.203125\n",
            "0.375\n",
            "time: 2.6371231079101562 2.499377302689986\n",
            "220\n",
            "strain 0.0672462210059166\n",
            "strain 0.04224345460534096\n",
            "strain 0.0632503405213356\n",
            "strain 0.15282011032104492\n",
            "strain 0.04733175039291382\n",
            "strain 0.10887712240219116\n",
            "classify 2.15338134765625\n",
            "classify 2.02880859375\n",
            "classify 1.91217041015625\n",
            "classify 2.067626953125\n",
            "classify 2.08331298828125\n",
            "classify 2.01983642578125\n",
            "classify 2.11474609375\n",
            "classify 1.91986083984375\n",
            "classify 2.023193359375\n",
            "classify 1.998046875\n",
            "classify 2.01361083984375\n",
            "0.328125\n",
            "0.3125\n",
            "0.21875\n",
            "0.25\n",
            "0.171875\n",
            "0.375\n",
            "0.21875\n",
            "0.3125\n",
            "0.3125\n",
            "0.28125\n",
            "0.25\n",
            "time: 2.4141547679901123 2.4989946740784794\n",
            "221\n",
            "strain 0.06625572592020035\n",
            "strain 0.04473921284079552\n",
            "strain 0.042113881558179855\n",
            "strain 0.2497979998588562\n",
            "strain 0.09779141843318939\n",
            "strain 0.15660341084003448\n",
            "classify 1.937255859375\n",
            "classify 2.0301513671875\n",
            "classify 1.94586181640625\n",
            "classify 2.0892333984375\n",
            "classify 2.06402587890625\n",
            "classify 2.1400146484375\n",
            "classify 2.124755859375\n",
            "classify 2.03289794921875\n",
            "classify 1.9432373046875\n",
            "classify 2.1163330078125\n",
            "classify 2.015380859375\n",
            "0.09375\n",
            "0.21875\n",
            "0.28125\n",
            "0.25\n",
            "0.328125\n",
            "0.25\n",
            "0.3125\n",
            "0.359375\n",
            "0.296875\n",
            "0.296875\n",
            "0.25\n",
            "time: 2.4189209938049316 2.498636483072161\n",
            "222\n",
            "strain 0.20945264399051666\n",
            "strain 0.1053699478507042\n",
            "strain 0.11402279138565063\n",
            "strain 0.10948888212442398\n",
            "strain 0.048273663967847824\n",
            "strain 0.06597931683063507\n",
            "classify 2.07568359375\n",
            "classify 2.00848388671875\n",
            "classify 2.08935546875\n",
            "classify 2.10980224609375\n",
            "classify 1.93572998046875\n",
            "classify 2.0760498046875\n",
            "classify 2.04168701171875\n",
            "classify 1.99871826171875\n",
            "classify 1.9471435546875\n",
            "classify 2.00592041015625\n",
            "classify 1.982421875\n",
            "0.265625\n",
            "0.375\n",
            "0.265625\n",
            "0.234375\n",
            "0.265625\n",
            "0.25\n",
            "0.140625\n",
            "0.3125\n",
            "0.359375\n",
            "0.296875\n",
            "0.265625\n",
            "time: 2.4089460372924805 2.4982371148507156\n",
            "223\n",
            "strain 0.07296601682901382\n",
            "strain 0.09768001735210419\n",
            "strain 0.13093967735767365\n",
            "strain 0.08267352730035782\n",
            "strain 0.22105610370635986\n",
            "strain 0.07544346153736115\n",
            "classify 2.00933837890625\n",
            "classify 2.152587890625\n",
            "classify 1.986328125\n",
            "classify 2.043212890625\n",
            "classify 1.96380615234375\n",
            "classify 2.1322021484375\n",
            "classify 1.97216796875\n",
            "classify 1.99737548828125\n",
            "classify 1.95306396484375\n",
            "classify 2.0228271484375\n",
            "classify 1.96142578125\n",
            "0.265625\n",
            "0.265625\n",
            "0.25\n",
            "0.296875\n",
            "0.28125\n",
            "0.203125\n",
            "0.296875\n",
            "0.359375\n",
            "0.265625\n",
            "0.171875\n",
            "0.234375\n",
            "time: 2.627448320388794 2.498816452920437\n",
            "224\n",
            "strain 0.07099433988332748\n",
            "strain 0.2868766486644745\n",
            "strain 0.051123637706041336\n",
            "strain 0.1646190732717514\n",
            "strain 0.040355194360017776\n",
            "strain 0.1058647632598877\n",
            "classify 2.035888671875\n",
            "classify 2.0103759765625\n",
            "classify 2.06396484375\n",
            "classify 2.05419921875\n",
            "classify 2.076416015625\n",
            "classify 2.0426025390625\n",
            "classify 2.1131591796875\n",
            "classify 2.0\n",
            "classify 1.9970703125\n",
            "classify 2.0274658203125\n",
            "classify 2.006591796875\n",
            "0.203125\n",
            "0.3125\n",
            "0.265625\n",
            "0.21875\n",
            "0.234375\n",
            "0.1875\n",
            "0.3125\n",
            "0.3125\n",
            "0.34375\n",
            "0.171875\n",
            "0.234375\n",
            "time: 2.5650880336761475 2.499116094377306\n",
            "225\n",
            "strain 0.12025895714759827\n",
            "strain 0.07975724339485168\n",
            "strain 0.031045101583003998\n",
            "strain 0.04801030457019806\n",
            "strain 0.08280372619628906\n",
            "strain 0.043961577117443085\n",
            "classify 2.090087890625\n",
            "classify 1.98858642578125\n",
            "classify 1.98284912109375\n",
            "classify 2.0755615234375\n",
            "classify 2.1055908203125\n",
            "classify 2.068359375\n",
            "classify 2.0272216796875\n",
            "classify 1.951171875\n",
            "classify 1.9959716796875\n",
            "classify 2.0987548828125\n",
            "classify 2.01544189453125\n",
            "0.3125\n",
            "0.21875\n",
            "0.15625\n",
            "0.28125\n",
            "0.234375\n",
            "0.3125\n",
            "0.1875\n",
            "0.21875\n",
            "0.3125\n",
            "0.34375\n",
            "0.328125\n",
            "time: 2.3755550384521484 2.498572081591176\n",
            "226\n",
            "strain 0.05138905346393585\n",
            "strain 0.06242923438549042\n",
            "strain 0.052866339683532715\n",
            "strain 0.053734224289655685\n",
            "strain 0.15809546411037445\n",
            "strain 0.1336340457201004\n",
            "classify 1.84857177734375\n",
            "classify 1.95172119140625\n",
            "classify 2.0897216796875\n",
            "classify 2.04833984375\n",
            "classify 2.060302734375\n",
            "classify 1.92578125\n",
            "classify 1.9931640625\n",
            "classify 2.043701171875\n",
            "classify 2.0557861328125\n",
            "classify 1.97943115234375\n",
            "classify 1.9410400390625\n",
            "0.21875\n",
            "0.28125\n",
            "0.359375\n",
            "0.328125\n",
            "0.265625\n",
            "0.3125\n",
            "0.234375\n",
            "0.265625\n",
            "0.1875\n",
            "0.28125\n",
            "0.25\n",
            "time: 2.397386074066162 2.4981288290233863\n",
            "227\n",
            "strain 0.1852211356163025\n",
            "strain 0.0681597888469696\n",
            "strain 0.2057451605796814\n",
            "strain 0.037307433784008026\n",
            "strain 0.1009630337357521\n",
            "strain 0.04668675363063812\n",
            "classify 2.06085205078125\n",
            "classify 2.125\n",
            "classify 2.10003662109375\n",
            "classify 2.1097412109375\n",
            "classify 2.0552978515625\n",
            "classify 2.0352783203125\n",
            "classify 2.02716064453125\n",
            "classify 2.0341796875\n",
            "classify 1.9676513671875\n",
            "classify 2.02703857421875\n",
            "classify 2.0477294921875\n",
            "0.25\n",
            "0.25\n",
            "0.359375\n",
            "0.1875\n",
            "0.34375\n",
            "0.3125\n",
            "0.375\n",
            "0.25\n",
            "0.296875\n",
            "0.15625\n",
            "0.296875\n",
            "time: 2.384429693222046 2.4976326959174977\n",
            "228\n",
            "strain 0.20944365859031677\n",
            "strain 0.04930713027715683\n",
            "strain 0.0766516625881195\n",
            "strain 0.05923181027173996\n",
            "strain 0.2899051010608673\n",
            "strain 0.07027753442525864\n",
            "classify 2.021728515625\n",
            "classify 1.996826171875\n",
            "classify 2.0054931640625\n",
            "classify 2.0555419921875\n",
            "classify 2.078857421875\n",
            "classify 2.10430908203125\n",
            "classify 1.9354248046875\n",
            "classify 2.092529296875\n",
            "classify 2.1068115234375\n",
            "classify 1.92041015625\n",
            "classify 2.140869140625\n",
            "0.28125\n",
            "0.265625\n",
            "0.25\n",
            "0.265625\n",
            "0.296875\n",
            "0.3125\n",
            "0.1875\n",
            "0.25\n",
            "0.328125\n",
            "0.28125\n",
            "0.21875\n",
            "time: 2.6126770973205566 2.4981375625560376\n",
            "229\n",
            "strain 0.061590906232595444\n",
            "strain 0.06298145651817322\n",
            "strain 0.04848514497280121\n",
            "strain 0.08558731526136398\n",
            "strain 0.0513480119407177\n",
            "strain 0.2894391715526581\n",
            "classify 2.0198974609375\n",
            "classify 1.982421875\n",
            "classify 2.10675048828125\n",
            "classify 1.9473876953125\n",
            "classify 2.110595703125\n",
            "classify 1.9598388671875\n",
            "classify 2.032958984375\n",
            "classify 2.12457275390625\n",
            "classify 2.03717041015625\n",
            "classify 2.04254150390625\n",
            "classify 2.0499267578125\n",
            "0.265625\n",
            "0.28125\n",
            "0.28125\n",
            "0.328125\n",
            "0.28125\n",
            "0.359375\n",
            "0.234375\n",
            "0.359375\n",
            "0.25\n",
            "0.359375\n",
            "0.28125\n",
            "time: 2.557258367538452 2.4983967501184217\n",
            "230\n",
            "strain 0.04705708473920822\n",
            "strain 0.059480197727680206\n",
            "strain 0.2568270266056061\n",
            "strain 0.05390298739075661\n",
            "strain 0.11226154118776321\n",
            "strain 0.035733647644519806\n",
            "classify 1.88232421875\n",
            "classify 2.0711669921875\n",
            "classify 2.04034423828125\n",
            "classify 2.09478759765625\n",
            "classify 2.057861328125\n",
            "classify 1.978271484375\n",
            "classify 2.04150390625\n",
            "classify 1.924072265625\n",
            "classify 2.0072021484375\n",
            "classify 1.96728515625\n",
            "classify 1.94219970703125\n",
            "0.171875\n",
            "0.3125\n",
            "0.453125\n",
            "0.390625\n",
            "0.28125\n",
            "0.328125\n",
            "0.375\n",
            "0.265625\n",
            "0.171875\n",
            "0.3125\n",
            "0.234375\n",
            "time: 2.3818552494049072 2.4978945637162115\n",
            "231\n",
            "strain 0.044929876923561096\n",
            "strain 0.0592777393758297\n",
            "strain 0.053627047687768936\n",
            "strain 0.050330858677625656\n",
            "strain 0.06468697637319565\n",
            "strain 0.10474789887666702\n",
            "classify 2.0955810546875\n",
            "classify 2.0235595703125\n",
            "classify 2.07403564453125\n",
            "classify 2.10797119140625\n",
            "classify 2.1435546875\n",
            "classify 1.855712890625\n",
            "classify 2.01171875\n",
            "classify 1.994140625\n",
            "classify 1.96044921875\n",
            "classify 1.9068603515625\n",
            "classify 2.0775146484375\n",
            "0.359375\n",
            "0.1875\n",
            "0.28125\n",
            "0.21875\n",
            "0.234375\n",
            "0.28125\n",
            "0.265625\n",
            "0.28125\n",
            "0.1875\n",
            "0.21875\n",
            "0.359375\n",
            "time: 2.406751871109009 2.4975040873576857\n",
            "232\n",
            "strain 0.05369232967495918\n",
            "strain 0.07195786386728287\n",
            "strain 0.042174339294433594\n",
            "strain 0.1243683472275734\n",
            "strain 0.1819695681333542\n",
            "strain 0.10790511220693588\n",
            "classify 2.04638671875\n",
            "classify 2.0107421875\n",
            "classify 2.0032958984375\n",
            "classify 2.03912353515625\n",
            "classify 1.97491455078125\n",
            "classify 2.06689453125\n",
            "classify 2.0224609375\n",
            "classify 1.90283203125\n",
            "classify 2.1065673828125\n",
            "classify 2.0819091796875\n",
            "classify 2.04736328125\n",
            "0.359375\n",
            "0.234375\n",
            "0.3125\n",
            "0.25\n",
            "0.28125\n",
            "0.34375\n",
            "0.234375\n",
            "0.203125\n",
            "0.265625\n",
            "0.28125\n",
            "0.203125\n",
            "time: 2.434776544570923 2.497237503272781\n",
            "233\n",
            "strain 0.06503526866436005\n",
            "strain 0.04834423586726189\n",
            "strain 0.0599619559943676\n",
            "strain 0.06527061760425568\n",
            "strain 0.0486818291246891\n",
            "strain 0.07502622902393341\n",
            "classify 2.003662109375\n",
            "classify 2.13916015625\n",
            "classify 2.0074462890625\n",
            "classify 2.01007080078125\n",
            "classify 1.90313720703125\n",
            "classify 2.03155517578125\n",
            "classify 1.8385009765625\n",
            "classify 1.99078369140625\n",
            "classify 2.02362060546875\n",
            "classify 2.0118408203125\n",
            "classify 1.9306640625\n",
            "0.21875\n",
            "0.171875\n",
            "0.265625\n",
            "0.3125\n",
            "0.21875\n",
            "0.203125\n",
            "0.328125\n",
            "0.28125\n",
            "0.265625\n",
            "0.296875\n",
            "0.21875\n",
            "time: 2.620776653289795 2.4977679293379826\n",
            "234\n",
            "strain 0.06338976323604584\n",
            "strain 0.07806526124477386\n",
            "strain 0.061766382306814194\n",
            "strain 0.13777269423007965\n",
            "strain 0.06042955070734024\n",
            "strain 0.08821569383144379\n",
            "classify 2.087158203125\n",
            "classify 2.1466064453125\n",
            "classify 2.03094482421875\n",
            "classify 2.03594970703125\n",
            "classify 2.08331298828125\n",
            "classify 1.9228515625\n",
            "classify 1.93499755859375\n",
            "classify 1.86151123046875\n",
            "classify 1.9224853515625\n",
            "classify 1.926025390625\n",
            "classify 1.8653564453125\n",
            "0.34375\n",
            "0.265625\n",
            "0.25\n",
            "0.34375\n",
            "0.3125\n",
            "0.296875\n",
            "0.1875\n",
            "0.21875\n",
            "0.265625\n",
            "0.296875\n",
            "0.296875\n",
            "time: 2.669605016708374 2.49850180808534\n",
            "235\n",
            "strain 0.09499190002679825\n",
            "strain 0.10457374155521393\n",
            "strain 0.05121266096830368\n",
            "strain 0.047275468707084656\n",
            "strain 0.09210233390331268\n",
            "strain 0.21934835612773895\n",
            "classify 1.9520263671875\n",
            "classify 2.0445556640625\n",
            "classify 2.0120849609375\n",
            "classify 1.989990234375\n",
            "classify 1.937744140625\n",
            "classify 2.023681640625\n",
            "classify 2.088134765625\n",
            "classify 2.07403564453125\n",
            "classify 2.0535888671875\n",
            "classify 1.98358154296875\n",
            "classify 2.0042724609375\n",
            "0.265625\n",
            "0.21875\n",
            "0.265625\n",
            "0.1875\n",
            "0.25\n",
            "0.3125\n",
            "0.21875\n",
            "0.25\n",
            "0.3125\n",
            "0.234375\n",
            "0.234375\n",
            "time: 2.4038689136505127 2.498103319588354\n",
            "236\n",
            "strain 0.061889223754405975\n",
            "strain 0.126522496342659\n",
            "strain 0.08290228247642517\n",
            "strain 0.15121197700500488\n",
            "strain 0.07276249676942825\n",
            "strain 0.06982345134019852\n",
            "classify 2.07794189453125\n",
            "classify 1.89044189453125\n",
            "classify 1.963134765625\n",
            "classify 1.94091796875\n",
            "classify 1.93182373046875\n",
            "classify 2.00048828125\n",
            "classify 2.1063232421875\n",
            "classify 1.92730712890625\n",
            "classify 2.03466796875\n",
            "classify 2.07464599609375\n",
            "classify 1.919189453125\n",
            "0.28125\n",
            "0.40625\n",
            "0.359375\n",
            "0.296875\n",
            "0.234375\n",
            "0.21875\n",
            "0.28125\n",
            "0.21875\n",
            "0.28125\n",
            "0.265625\n",
            "0.25\n",
            "time: 2.4146482944488525 2.4977538374405874\n",
            "237\n",
            "strain 0.0390319749712944\n",
            "strain 0.0475475937128067\n",
            "strain 0.037568099796772\n",
            "strain 0.072455033659935\n",
            "strain 0.05164971202611923\n",
            "strain 0.13026872277259827\n",
            "classify 2.1483154296875\n",
            "classify 1.947265625\n",
            "classify 2.1422119140625\n",
            "classify 2.0323486328125\n",
            "classify 2.0582275390625\n",
            "classify 1.878173828125\n",
            "classify 2.0220947265625\n",
            "classify 2.0826416015625\n",
            "classify 2.0389404296875\n",
            "classify 2.02410888671875\n",
            "classify 1.8656005859375\n",
            "0.21875\n",
            "0.34375\n",
            "0.296875\n",
            "0.25\n",
            "0.28125\n",
            "0.375\n",
            "0.359375\n",
            "0.34375\n",
            "0.296875\n",
            "0.15625\n",
            "0.359375\n",
            "time: 2.4026565551757812 2.4973565830903897\n",
            "238\n",
            "strain 0.062265072017908096\n",
            "strain 0.07849332690238953\n",
            "strain 0.04528714343905449\n",
            "strain 0.1502271294593811\n",
            "strain 0.15274135768413544\n",
            "strain 0.09627478569746017\n",
            "classify 2.0760498046875\n",
            "classify 1.9964599609375\n",
            "classify 2.0216064453125\n",
            "classify 2.0325927734375\n",
            "classify 2.0556640625\n",
            "classify 1.918701171875\n",
            "classify 2.0703125\n",
            "classify 2.0185546875\n",
            "classify 2.02459716796875\n",
            "classify 2.09747314453125\n",
            "classify 2.0128173828125\n",
            "0.265625\n",
            "0.3125\n",
            "0.296875\n",
            "0.234375\n",
            "0.25\n",
            "0.234375\n",
            "0.328125\n",
            "0.28125\n",
            "0.234375\n",
            "0.3125\n",
            "0.328125\n",
            "time: 2.6164703369140625 2.4978573541760944\n",
            "239\n",
            "strain 0.042375609278678894\n",
            "strain 0.047412432730197906\n",
            "strain 0.08997577428817749\n",
            "strain 0.12781351804733276\n",
            "strain 0.1453542560338974\n",
            "strain 0.058537937700748444\n",
            "classify 1.91876220703125\n",
            "classify 2.04229736328125\n",
            "classify 1.996337890625\n",
            "classify 1.9185791015625\n",
            "classify 2.025146484375\n",
            "classify 2.068115234375\n",
            "classify 1.91546630859375\n",
            "classify 1.9310302734375\n",
            "classify 1.935546875\n",
            "classify 1.83746337890625\n",
            "classify 2.0224609375\n",
            "0.34375\n",
            "0.359375\n",
            "0.328125\n",
            "0.25\n",
            "0.234375\n",
            "0.328125\n",
            "0.28125\n",
            "0.296875\n",
            "0.234375\n",
            "0.28125\n",
            "0.3125\n",
            "time: 2.644735336303711 2.4984757820765178\n",
            "240\n",
            "strain 0.052913591265678406\n",
            "strain 0.06931336969137192\n",
            "strain 0.041082944720983505\n",
            "strain 0.18119759857654572\n",
            "strain 0.07136289775371552\n",
            "strain 0.37567007541656494\n",
            "classify 2.0537109375\n",
            "classify 1.9168701171875\n",
            "classify 1.9781494140625\n",
            "classify 2.1282958984375\n",
            "classify 2.1053466796875\n",
            "classify 1.9853515625\n",
            "classify 1.90765380859375\n",
            "classify 1.99444580078125\n",
            "classify 2.0078125\n",
            "classify 2.07830810546875\n",
            "classify 1.96820068359375\n",
            "0.296875\n",
            "0.265625\n",
            "0.203125\n",
            "0.390625\n",
            "0.171875\n",
            "0.203125\n",
            "0.25\n",
            "0.265625\n",
            "0.28125\n",
            "0.21875\n",
            "0.234375\n",
            "time: 2.409797191619873 2.4981103546886523\n",
            "241\n",
            "strain 0.03537049517035484\n",
            "strain 0.055608827620744705\n",
            "strain 0.049474772065877914\n",
            "strain 0.07506837695837021\n",
            "strain 0.04379308223724365\n",
            "strain 0.05059676617383957\n",
            "classify 1.9227294921875\n",
            "classify 2.1348876953125\n",
            "classify 2.0576171875\n",
            "classify 2.0086669921875\n",
            "classify 2.05242919921875\n",
            "classify 2.0513916015625\n",
            "classify 1.95355224609375\n",
            "classify 1.90069580078125\n",
            "classify 2.02655029296875\n",
            "classify 1.91845703125\n",
            "classify 2.0771484375\n",
            "0.203125\n",
            "0.203125\n",
            "0.296875\n",
            "0.203125\n",
            "0.203125\n",
            "0.265625\n",
            "0.203125\n",
            "0.265625\n",
            "0.28125\n",
            "0.296875\n",
            "0.21875\n",
            "time: 2.433098554611206 2.497843978818783\n",
            "242\n",
            "strain 0.04233229160308838\n",
            "strain 0.09306299686431885\n",
            "strain 0.05216427519917488\n",
            "strain 0.12824462354183197\n",
            "strain 0.054385580122470856\n",
            "strain 0.05234389752149582\n",
            "classify 1.89996337890625\n",
            "classify 2.086669921875\n",
            "classify 2.11492919921875\n",
            "classify 1.91845703125\n",
            "classify 2.06591796875\n",
            "classify 1.9847412109375\n",
            "classify 2.06707763671875\n",
            "classify 2.0595703125\n",
            "classify 2.02392578125\n",
            "classify 2.06719970703125\n",
            "classify 2.00848388671875\n",
            "0.3125\n",
            "0.25\n",
            "0.234375\n",
            "0.25\n",
            "0.171875\n",
            "0.34375\n",
            "0.328125\n",
            "0.296875\n",
            "0.328125\n",
            "0.140625\n",
            "0.203125\n",
            "time: 2.4023919105529785 2.497453557120429\n",
            "243\n",
            "strain 0.04818384721875191\n",
            "strain 0.03635622560977936\n",
            "strain 0.27273106575012207\n",
            "strain 0.061593227088451385\n",
            "strain 0.03647953271865845\n",
            "strain 0.05405445396900177\n",
            "classify 2.018310546875\n",
            "classify 1.997314453125\n",
            "classify 1.93853759765625\n",
            "classify 1.88397216796875\n",
            "classify 1.98748779296875\n",
            "classify 2.0748291015625\n",
            "classify 2.041748046875\n",
            "classify 2.0400390625\n",
            "classify 2.03729248046875\n",
            "classify 1.95086669921875\n",
            "classify 2.08123779296875\n",
            "0.3125\n",
            "0.25\n",
            "0.3125\n",
            "0.296875\n",
            "0.328125\n",
            "0.265625\n",
            "0.234375\n",
            "0.265625\n",
            "0.140625\n",
            "0.328125\n",
            "0.328125\n",
            "time: 2.6470963954925537 2.4980693097974433\n",
            "244\n",
            "strain 0.04292306676506996\n",
            "strain 0.04714669659733772\n",
            "strain 0.05032883584499359\n",
            "strain 0.054478343576192856\n",
            "strain 0.056969426572322845\n",
            "strain 0.04226916283369064\n",
            "classify 1.9869384765625\n",
            "classify 1.92889404296875\n",
            "classify 2.00982666015625\n",
            "classify 1.9754638671875\n",
            "classify 1.8961181640625\n",
            "classify 2.03594970703125\n",
            "classify 1.9739990234375\n",
            "classify 1.975830078125\n",
            "classify 1.9892578125\n",
            "classify 1.89422607421875\n",
            "classify 1.9267578125\n",
            "0.296875\n",
            "0.3125\n",
            "0.21875\n",
            "0.359375\n",
            "0.21875\n",
            "0.203125\n",
            "0.21875\n",
            "0.34375\n",
            "0.21875\n",
            "0.328125\n",
            "0.25\n",
            "time: 2.6518478393554688 2.498707592244051\n",
            "245\n",
            "strain 0.03854725509881973\n",
            "strain 0.09425201267004013\n",
            "strain 0.05441945791244507\n",
            "strain 0.0404837429523468\n",
            "strain 0.07824087142944336\n",
            "strain 0.13474997878074646\n",
            "classify 1.871337890625\n",
            "classify 1.8955078125\n",
            "classify 1.967041015625\n",
            "classify 1.975830078125\n",
            "classify 1.8541259765625\n",
            "classify 2.0029296875\n",
            "classify 1.9459228515625\n",
            "classify 1.994873046875\n",
            "classify 2.0201416015625\n",
            "classify 2.06329345703125\n",
            "classify 1.91802978515625\n",
            "0.328125\n",
            "0.359375\n",
            "0.3125\n",
            "0.3125\n",
            "0.328125\n",
            "0.234375\n",
            "0.28125\n",
            "0.375\n",
            "0.328125\n",
            "0.296875\n",
            "0.234375\n",
            "time: 2.4088258743286133 2.4983446937266405\n",
            "246\n",
            "strain 0.15145887434482574\n",
            "strain 0.045825134962797165\n",
            "strain 0.034339502453804016\n",
            "strain 0.3139100968837738\n",
            "strain 0.14525845646858215\n",
            "strain 0.05359361693263054\n",
            "classify 2.058349609375\n",
            "classify 2.0213623046875\n",
            "classify 1.950927734375\n",
            "classify 2.0220947265625\n",
            "classify 2.0423583984375\n",
            "classify 2.027099609375\n",
            "classify 1.984375\n",
            "classify 1.96533203125\n",
            "classify 2.03472900390625\n",
            "classify 1.9940185546875\n",
            "classify 2.01190185546875\n",
            "0.25\n",
            "0.296875\n",
            "0.234375\n",
            "0.265625\n",
            "0.375\n",
            "0.3125\n",
            "0.3125\n",
            "0.203125\n",
            "0.265625\n",
            "0.25\n",
            "0.234375\n",
            "time: 2.414874315261841 2.4980089732027246\n",
            "247\n",
            "strain 0.05931854993104935\n",
            "strain 0.22456730902194977\n",
            "strain 0.07437922060489655\n",
            "strain 0.07173772901296616\n",
            "strain 0.2201455980539322\n",
            "strain 0.08213931322097778\n",
            "classify 1.98052978515625\n",
            "classify 2.04962158203125\n",
            "classify 2.0333251953125\n",
            "classify 2.012451171875\n",
            "classify 1.9791259765625\n",
            "classify 2.025390625\n",
            "classify 1.9383544921875\n",
            "classify 2.0423583984375\n",
            "classify 1.88677978515625\n",
            "classify 2.0986328125\n",
            "classify 2.01739501953125\n",
            "0.25\n",
            "0.265625\n",
            "0.25\n",
            "0.171875\n",
            "0.21875\n",
            "0.34375\n",
            "0.21875\n",
            "0.328125\n",
            "0.234375\n",
            "0.28125\n",
            "0.28125\n",
            "time: 2.402188301086426 2.497625039469811\n",
            "248\n",
            "strain 0.03550579771399498\n",
            "strain 0.12292058020830154\n",
            "strain 0.25008684396743774\n",
            "strain 0.1548161804676056\n",
            "strain 0.39655864238739014\n",
            "strain 0.05970197170972824\n",
            "classify 1.9974365234375\n",
            "classify 1.9649658203125\n",
            "classify 2.1300048828125\n",
            "classify 1.977294921875\n",
            "classify 1.7965087890625\n",
            "classify 1.92010498046875\n",
            "classify 1.9649658203125\n",
            "classify 1.94818115234375\n",
            "classify 2.00152587890625\n",
            "classify 1.91046142578125\n",
            "classify 1.8524169921875\n",
            "0.375\n",
            "0.265625\n",
            "0.328125\n",
            "0.328125\n",
            "0.3125\n",
            "0.28125\n",
            "0.328125\n",
            "0.3125\n",
            "0.21875\n",
            "0.40625\n",
            "0.328125\n",
            "time: 2.6287989616394043 2.4981541547430566\n",
            "249\n",
            "strain 0.06769542396068573\n",
            "strain 0.04478892311453819\n",
            "strain 0.08831468969583511\n",
            "strain 0.0733509361743927\n",
            "strain 0.08170101791620255\n",
            "strain 0.044371411204338074\n",
            "classify 2.04949951171875\n",
            "classify 2.02899169921875\n",
            "classify 2.1600341796875\n",
            "classify 1.95904541015625\n",
            "classify 1.953369140625\n",
            "classify 1.94964599609375\n",
            "classify 1.96942138671875\n",
            "classify 2.052490234375\n",
            "classify 2.1044921875\n",
            "classify 2.0487060546875\n",
            "classify 2.00677490234375\n",
            "0.25\n",
            "0.234375\n",
            "0.34375\n",
            "0.1875\n",
            "0.265625\n",
            "0.3125\n",
            "0.34375\n",
            "0.1875\n",
            "0.21875\n",
            "0.265625\n",
            "0.34375\n",
            "time: 2.627253532409668 2.4986734580993653\n",
            "250\n",
            "strain 0.04535182937979698\n",
            "strain 0.060400571674108505\n",
            "strain 0.06474430114030838\n",
            "strain 0.05898197367787361\n",
            "strain 0.05058996006846428\n",
            "strain 0.07467629015445709\n",
            "classify 1.9122314453125\n",
            "classify 2.0546875\n",
            "classify 2.1021728515625\n",
            "classify 1.8956298828125\n",
            "classify 2.00445556640625\n",
            "classify 1.9541015625\n",
            "classify 2.015380859375\n",
            "classify 1.99481201171875\n",
            "classify 2.02215576171875\n",
            "classify 2.02239990234375\n",
            "classify 1.999755859375\n",
            "0.234375\n",
            "0.234375\n",
            "0.375\n",
            "0.234375\n",
            "0.21875\n",
            "0.21875\n",
            "0.3125\n",
            "0.234375\n",
            "0.265625\n",
            "0.3125\n",
            "0.265625\n",
            "time: 2.3976805210113525 2.4982734210937623\n",
            "251\n",
            "strain 0.06891807913780212\n",
            "strain 0.1811259388923645\n",
            "strain 0.07034796476364136\n",
            "strain 0.048672083765268326\n",
            "strain 0.05545045807957649\n",
            "strain 0.05280471593141556\n",
            "classify 2.100341796875\n",
            "classify 2.03497314453125\n",
            "classify 2.0718994140625\n",
            "classify 2.0103759765625\n",
            "classify 2.082763671875\n",
            "classify 2.0242919921875\n",
            "classify 2.19232177734375\n",
            "classify 2.084228515625\n",
            "classify 1.9874267578125\n",
            "classify 1.95501708984375\n",
            "classify 2.079345703125\n",
            "0.21875\n",
            "0.28125\n",
            "0.390625\n",
            "0.234375\n",
            "0.296875\n",
            "0.25\n",
            "0.265625\n",
            "0.203125\n",
            "0.296875\n",
            "0.421875\n",
            "0.265625\n",
            "time: 2.4346635341644287 2.4980231504591686\n",
            "252\n",
            "strain 0.04331774264574051\n",
            "strain 0.042801667004823685\n",
            "strain 0.045404817909002304\n",
            "strain 0.17758332192897797\n",
            "strain 0.09762172400951385\n",
            "strain 0.04962984845042229\n",
            "classify 2.030517578125\n",
            "classify 2.007080078125\n",
            "classify 2.0513916015625\n",
            "classify 2.01116943359375\n",
            "classify 2.0025634765625\n",
            "classify 2.0845947265625\n",
            "classify 2.14208984375\n",
            "classify 2.08441162109375\n",
            "classify 2.05615234375\n",
            "classify 2.0390625\n",
            "classify 1.9957275390625\n",
            "0.3125\n",
            "0.234375\n",
            "0.1875\n",
            "0.234375\n",
            "0.296875\n",
            "0.234375\n",
            "0.25\n",
            "0.203125\n",
            "0.359375\n",
            "0.234375\n",
            "0.28125\n",
            "time: 2.41117787361145 2.4976821473464663\n",
            "253\n",
            "strain 0.06516362726688385\n",
            "strain 0.07149051129817963\n",
            "strain 0.16005989909172058\n",
            "strain 0.0442679189145565\n",
            "strain 0.10194165259599686\n",
            "strain 0.0442163422703743\n",
            "classify 1.9671630859375\n",
            "classify 1.860107421875\n",
            "classify 1.97930908203125\n",
            "classify 2.1126708984375\n",
            "classify 1.998046875\n",
            "classify 1.91973876953125\n",
            "classify 2.0155029296875\n",
            "classify 2.0350341796875\n",
            "classify 1.9859619140625\n",
            "classify 2.05889892578125\n",
            "classify 1.8995361328125\n",
            "0.28125\n",
            "0.328125\n",
            "0.25\n",
            "0.234375\n",
            "0.25\n",
            "0.1875\n",
            "0.3125\n",
            "0.25\n",
            "0.265625\n",
            "0.296875\n",
            "0.328125\n",
            "time: 2.6138110160827637 2.4981415093414427\n",
            "254\n",
            "strain 0.03986303508281708\n",
            "strain 0.08533735573291779\n",
            "strain 0.0423579104244709\n",
            "strain 0.039893195033073425\n",
            "strain 0.041021738201379776\n",
            "strain 0.06594382971525192\n",
            "classify 1.99481201171875\n",
            "classify 1.99542236328125\n",
            "classify 2.1524658203125\n",
            "classify 2.03839111328125\n",
            "classify 1.9808349609375\n",
            "classify 1.9888916015625\n",
            "classify 1.9471435546875\n",
            "classify 1.919921875\n",
            "classify 1.98883056640625\n",
            "classify 2.0291748046875\n",
            "classify 2.060791015625\n",
            "0.25\n",
            "0.28125\n",
            "0.234375\n",
            "0.203125\n",
            "0.234375\n",
            "0.296875\n",
            "0.265625\n",
            "0.25\n",
            "0.25\n",
            "0.265625\n",
            "0.21875\n",
            "time: 2.6360411643981934 2.4986872168148264\n",
            "255\n",
            "strain 0.038019873201847076\n",
            "strain 0.05260682851076126\n",
            "strain 0.13770529627799988\n",
            "strain 0.051188014447689056\n",
            "strain 0.03798821568489075\n",
            "strain 0.049052927643060684\n",
            "classify 1.9693603515625\n",
            "classify 2.01409912109375\n",
            "classify 1.86328125\n",
            "classify 2.005126953125\n",
            "classify 1.9954833984375\n",
            "classify 2.03851318359375\n",
            "classify 2.111328125\n",
            "classify 2.0491943359375\n",
            "classify 1.843017578125\n",
            "classify 2.0968017578125\n",
            "classify 2.0687255859375\n",
            "0.234375\n",
            "0.328125\n",
            "0.171875\n",
            "0.21875\n",
            "0.328125\n",
            "0.328125\n",
            "0.375\n",
            "0.3125\n",
            "0.4375\n",
            "0.265625\n",
            "0.1875\n",
            "time: 2.420869827270508 2.4983856081962585\n",
            "256\n",
            "strain 0.037704434245824814\n",
            "strain 0.1104932650923729\n",
            "strain 0.16685502231121063\n",
            "strain 0.0659969300031662\n",
            "strain 0.0815219134092331\n",
            "strain 0.20303528010845184\n",
            "classify 1.8958740234375\n",
            "classify 1.98834228515625\n",
            "classify 1.9869384765625\n",
            "classify 1.92901611328125\n",
            "classify 1.9305419921875\n",
            "classify 2.0625\n",
            "classify 2.031982421875\n",
            "classify 1.959716796875\n",
            "classify 1.95379638671875\n",
            "classify 2.04498291015625\n",
            "classify 1.93280029296875\n",
            "0.265625\n",
            "0.34375\n",
            "0.296875\n",
            "0.3125\n",
            "0.28125\n",
            "0.359375\n",
            "0.25\n",
            "0.25\n",
            "0.28125\n",
            "0.21875\n",
            "0.3125\n",
            "time: 2.4293227195739746 2.498119159431309\n",
            "257\n",
            "strain 0.06273604184389114\n",
            "strain 0.08551789075136185\n",
            "strain 0.06546381115913391\n",
            "strain 0.09799604117870331\n",
            "strain 0.201374813914299\n",
            "strain 0.1394045054912567\n",
            "classify 2.06964111328125\n",
            "classify 1.9923095703125\n",
            "classify 1.95947265625\n",
            "classify 2.0155029296875\n",
            "classify 1.8846435546875\n",
            "classify 1.99359130859375\n",
            "classify 2.00994873046875\n",
            "classify 1.982177734375\n",
            "classify 2.0140380859375\n",
            "classify 2.07733154296875\n",
            "classify 1.99517822265625\n",
            "0.265625\n",
            "0.265625\n",
            "0.296875\n",
            "0.328125\n",
            "0.203125\n",
            "0.28125\n",
            "0.265625\n",
            "0.375\n",
            "0.25\n",
            "0.3125\n",
            "0.203125\n",
            "time: 2.424717664718628 2.49783684301746\n",
            "258\n",
            "strain 0.14402902126312256\n",
            "strain 0.042923521250486374\n",
            "strain 0.05557715520262718\n",
            "strain 0.10250003635883331\n",
            "strain 0.05382230132818222\n",
            "strain 0.08528462052345276\n",
            "classify 2.0814208984375\n",
            "classify 1.97564697265625\n",
            "classify 2.0272216796875\n",
            "classify 2.1317138671875\n",
            "classify 2.0281982421875\n",
            "classify 1.88995361328125\n",
            "classify 2.103271484375\n",
            "classify 2.1109619140625\n",
            "classify 2.04913330078125\n",
            "classify 2.0045166015625\n",
            "classify 1.956787109375\n",
            "0.265625\n",
            "0.25\n",
            "0.3125\n",
            "0.234375\n",
            "0.234375\n",
            "0.34375\n",
            "0.28125\n",
            "0.296875\n",
            "0.28125\n",
            "0.25\n",
            "0.171875\n",
            "time: 2.649843454360962 2.498425817857838\n",
            "259\n",
            "strain 0.08647767454385757\n",
            "strain 0.057226672768592834\n",
            "strain 0.04288101941347122\n",
            "strain 0.2445114552974701\n",
            "strain 0.06916215270757675\n",
            "strain 0.15819516777992249\n",
            "classify 1.9666748046875\n",
            "classify 2.052734375\n",
            "classify 2.03436279296875\n",
            "classify 2.0467529296875\n",
            "classify 1.91021728515625\n",
            "classify 1.9906005859375\n",
            "classify 2.0653076171875\n",
            "classify 2.07781982421875\n",
            "classify 1.9434814453125\n",
            "classify 1.9571533203125\n",
            "classify 2.0015869140625\n",
            "0.25\n",
            "0.25\n",
            "0.265625\n",
            "0.25\n",
            "0.296875\n",
            "0.296875\n",
            "0.28125\n",
            "0.3125\n",
            "0.34375\n",
            "0.265625\n",
            "0.3125\n",
            "time: 2.613443374633789 2.498870654289539\n",
            "260\n",
            "strain 0.04644687473773956\n",
            "strain 0.05478522181510925\n",
            "strain 0.04853346571326256\n",
            "strain 0.10888181626796722\n",
            "strain 0.09406977891921997\n",
            "strain 0.19268400967121124\n",
            "classify 1.8782958984375\n",
            "classify 1.986328125\n",
            "classify 2.057373046875\n",
            "classify 2.06463623046875\n",
            "classify 1.92645263671875\n",
            "classify 2.0125732421875\n",
            "classify 2.01708984375\n",
            "classify 2.01080322265625\n",
            "classify 1.99591064453125\n",
            "classify 2.05810546875\n",
            "classify 2.0162353515625\n",
            "0.421875\n",
            "0.3125\n",
            "0.328125\n",
            "0.34375\n",
            "0.328125\n",
            "0.234375\n",
            "0.25\n",
            "0.296875\n",
            "0.34375\n",
            "0.1875\n",
            "0.234375\n",
            "time: 2.4177491664886475 2.4985623734207447\n",
            "261\n",
            "strain 0.09079040586948395\n",
            "strain 0.11041891574859619\n",
            "strain 0.06771013140678406\n",
            "strain 0.35923895239830017\n",
            "strain 0.08059021830558777\n",
            "strain 0.06058844178915024\n",
            "classify 2.0379638671875\n",
            "classify 1.9222412109375\n",
            "classify 2.0462646484375\n",
            "classify 1.95611572265625\n",
            "classify 1.99725341796875\n",
            "classify 1.97186279296875\n",
            "classify 2.0804443359375\n",
            "classify 1.9974365234375\n",
            "classify 2.07177734375\n",
            "classify 1.99725341796875\n",
            "classify 1.82330322265625\n",
            "0.328125\n",
            "0.25\n",
            "0.171875\n",
            "0.265625\n",
            "0.265625\n",
            "0.234375\n",
            "0.25\n",
            "0.25\n",
            "0.40625\n",
            "0.25\n",
            "0.359375\n",
            "time: 2.414581775665283 2.498243909755736\n",
            "262\n",
            "strain 0.06112087145447731\n",
            "strain 0.1075778678059578\n",
            "strain 0.03555901348590851\n",
            "strain 0.04020657390356064\n",
            "strain 0.0671871155500412\n",
            "strain 0.08511386066675186\n",
            "classify 1.98883056640625\n",
            "classify 1.94464111328125\n",
            "classify 1.9478759765625\n",
            "classify 2.01025390625\n",
            "classify 2.0030517578125\n",
            "classify 1.98162841796875\n",
            "classify 2.12213134765625\n",
            "classify 1.8875732421875\n",
            "classify 1.87689208984375\n",
            "classify 2.04425048828125\n",
            "classify 1.94964599609375\n",
            "0.328125\n",
            "0.1875\n",
            "0.296875\n",
            "0.203125\n",
            "0.28125\n",
            "0.296875\n",
            "0.328125\n",
            "0.375\n",
            "0.21875\n",
            "0.25\n",
            "0.28125\n",
            "time: 2.419461965560913 2.4979465687682874\n",
            "263\n",
            "strain 0.04478633031249046\n",
            "strain 0.20203842222690582\n",
            "strain 0.15259240567684174\n",
            "strain 0.17905676364898682\n",
            "strain 0.06153715029358864\n",
            "strain 0.10647519677877426\n",
            "classify 2.04681396484375\n",
            "classify 2.0623779296875\n",
            "classify 1.985595703125\n",
            "classify 2.06243896484375\n",
            "classify 2.046875\n",
            "classify 1.912109375\n",
            "classify 1.9967041015625\n",
            "classify 2.025146484375\n",
            "classify 1.9453125\n",
            "classify 2.052001953125\n",
            "classify 1.9727783203125\n",
            "0.234375\n",
            "0.25\n",
            "0.359375\n",
            "0.1875\n",
            "0.328125\n",
            "0.171875\n",
            "0.359375\n",
            "0.28125\n",
            "0.390625\n",
            "0.171875\n",
            "0.28125\n",
            "time: 2.6472253799438477 2.498514760624279\n",
            "264\n",
            "strain 0.16406311094760895\n",
            "strain 0.05248206853866577\n",
            "strain 0.12599124014377594\n",
            "strain 0.09758466482162476\n",
            "strain 0.0494530126452446\n",
            "strain 0.040978241711854935\n",
            "classify 2.0087890625\n",
            "classify 1.95379638671875\n",
            "classify 2.08966064453125\n",
            "classify 2.0262451171875\n",
            "classify 2.011962890625\n",
            "classify 2.04949951171875\n",
            "classify 2.035400390625\n",
            "classify 1.9766845703125\n",
            "classify 2.07977294921875\n",
            "classify 1.8775634765625\n",
            "classify 2.001708984375\n",
            "0.296875\n",
            "0.265625\n",
            "0.296875\n",
            "0.25\n",
            "0.21875\n",
            "0.1875\n",
            "0.15625\n",
            "0.359375\n",
            "0.171875\n",
            "0.28125\n",
            "0.25\n",
            "time: 2.630356788635254 2.4990142309440757\n",
            "265\n",
            "strain 0.11662201583385468\n",
            "strain 0.05230524763464928\n",
            "strain 0.04588514193892479\n",
            "strain 0.12506814301013947\n",
            "strain 0.10197855532169342\n",
            "strain 0.05986862629652023\n",
            "classify 2.07476806640625\n",
            "classify 2.1065673828125\n",
            "classify 1.92486572265625\n",
            "classify 2.070068359375\n",
            "classify 2.05908203125\n",
            "classify 2.02197265625\n",
            "classify 2.08294677734375\n",
            "classify 1.981201171875\n",
            "classify 2.02392578125\n",
            "classify 2.1583251953125\n",
            "classify 1.91558837890625\n",
            "0.265625\n",
            "0.34375\n",
            "0.203125\n",
            "0.34375\n",
            "0.21875\n",
            "0.3125\n",
            "0.25\n",
            "0.28125\n",
            "0.3125\n",
            "0.28125\n",
            "0.25\n",
            "time: 2.413390874862671 2.498694366082213\n",
            "266\n",
            "strain 0.04470129311084747\n",
            "strain 0.04137415066361427\n",
            "strain 0.062404386699199677\n",
            "strain 0.044947464019060135\n",
            "strain 0.04536721110343933\n",
            "strain 0.041634250432252884\n",
            "classify 2.0477294921875\n",
            "classify 1.99652099609375\n",
            "classify 2.061279296875\n",
            "classify 1.95355224609375\n",
            "classify 1.8831787109375\n",
            "classify 1.92437744140625\n",
            "classify 1.9134521484375\n",
            "classify 2.0166015625\n",
            "classify 2.1083984375\n",
            "classify 2.0477294921875\n",
            "classify 2.0286865234375\n",
            "0.328125\n",
            "0.21875\n",
            "0.328125\n",
            "0.203125\n",
            "0.296875\n",
            "0.21875\n",
            "0.203125\n",
            "0.328125\n",
            "0.375\n",
            "0.28125\n",
            "0.328125\n",
            "time: 2.4612605571746826 2.4985561978057977\n",
            "267\n",
            "strain 0.08213729411363602\n",
            "strain 0.044400203973054886\n",
            "strain 0.07290186733007431\n",
            "strain 0.04237614944577217\n",
            "strain 0.17770375311374664\n",
            "strain 0.07149062305688858\n",
            "classify 1.9940185546875\n",
            "classify 1.99102783203125\n",
            "classify 1.9888916015625\n",
            "classify 1.9071044921875\n",
            "classify 2.07708740234375\n",
            "classify 2.0372314453125\n",
            "classify 1.97796630859375\n",
            "classify 2.0311279296875\n",
            "classify 1.96649169921875\n",
            "classify 1.9190673828125\n",
            "classify 1.98809814453125\n",
            "0.265625\n",
            "0.3125\n",
            "0.296875\n",
            "0.3125\n",
            "0.375\n",
            "0.34375\n",
            "0.203125\n",
            "0.296875\n",
            "0.359375\n",
            "0.25\n",
            "0.203125\n",
            "time: 2.428565502166748 2.498297194046761\n",
            "268\n",
            "strain 0.05501120910048485\n",
            "strain 0.07678614556789398\n",
            "strain 0.07298773527145386\n",
            "strain 0.056684527546167374\n",
            "strain 0.04463396966457367\n",
            "strain 0.14030984044075012\n",
            "classify 2.0504150390625\n",
            "classify 2.0360107421875\n",
            "classify 1.95404052734375\n",
            "classify 1.91558837890625\n",
            "classify 2.0638427734375\n",
            "classify 2.044189453125\n",
            "classify 2.02130126953125\n",
            "classify 2.018310546875\n",
            "classify 1.8184814453125\n",
            "classify 1.92413330078125\n",
            "classify 2.00732421875\n",
            "0.203125\n",
            "0.234375\n",
            "0.25\n",
            "0.296875\n",
            "0.203125\n",
            "0.328125\n",
            "0.34375\n",
            "0.265625\n",
            "0.265625\n",
            "0.28125\n",
            "0.234375\n",
            "time: 2.6722636222839355 2.4989459798238536\n",
            "269\n",
            "strain 0.0553421713411808\n",
            "strain 0.04524613544344902\n",
            "strain 0.0767907053232193\n",
            "strain 0.042811717838048935\n",
            "strain 0.07689189165830612\n",
            "strain 0.38823986053466797\n",
            "classify 2.1287841796875\n",
            "classify 2.037353515625\n",
            "classify 1.9453125\n",
            "classify 1.96929931640625\n",
            "classify 2.0546875\n",
            "classify 2.00543212890625\n",
            "classify 1.9808349609375\n",
            "classify 2.00177001953125\n",
            "classify 1.9967041015625\n",
            "classify 1.92193603515625\n",
            "classify 2.078125\n",
            "0.390625\n",
            "0.328125\n",
            "0.265625\n",
            "0.296875\n",
            "0.203125\n",
            "0.3125\n",
            "0.265625\n",
            "0.3125\n",
            "0.265625\n",
            "0.203125\n",
            "0.296875\n",
            "time: 2.6199655532836914 2.4993960830900406\n",
            "270\n",
            "strain 0.16454562544822693\n",
            "strain 0.16524964570999146\n",
            "strain 0.03795343637466431\n",
            "strain 0.1278626024723053\n",
            "strain 0.10356941819190979\n",
            "strain 0.18297399580478668\n",
            "classify 1.97308349609375\n",
            "classify 2.0377197265625\n",
            "classify 2.07269287109375\n",
            "classify 1.891357421875\n",
            "classify 1.91424560546875\n",
            "classify 2.0101318359375\n",
            "classify 2.10113525390625\n",
            "classify 2.0015869140625\n",
            "classify 2.00604248046875\n",
            "classify 1.81536865234375\n",
            "classify 2.02325439453125\n",
            "0.265625\n",
            "0.375\n",
            "0.25\n",
            "0.28125\n",
            "0.1875\n",
            "0.265625\n",
            "0.25\n",
            "0.234375\n",
            "0.34375\n",
            "0.34375\n",
            "0.21875\n",
            "time: 2.4078855514526367 2.499061142826432\n",
            "271\n",
            "strain 0.08418755978345871\n",
            "strain 0.17245525121688843\n",
            "strain 0.06847093999385834\n",
            "strain 0.2032865583896637\n",
            "strain 0.07848609983921051\n",
            "strain 0.05969604104757309\n",
            "classify 1.97808837890625\n",
            "classify 2.0262451171875\n",
            "classify 2.04974365234375\n",
            "classify 1.874755859375\n",
            "classify 1.9637451171875\n",
            "classify 1.98223876953125\n",
            "classify 2.1031494140625\n",
            "classify 2.0048828125\n",
            "classify 2.08245849609375\n",
            "classify 2.0428466796875\n",
            "classify 1.950439453125\n",
            "0.234375\n",
            "0.3125\n",
            "0.25\n",
            "0.28125\n",
            "0.3125\n",
            "0.234375\n",
            "0.375\n",
            "0.15625\n",
            "0.328125\n",
            "0.25\n",
            "0.25\n",
            "time: 2.400686264038086 2.498701474245857\n",
            "272\n",
            "strain 0.04167982190847397\n",
            "strain 0.0708576962351799\n",
            "strain 0.1448713093996048\n",
            "strain 0.22719207406044006\n",
            "strain 0.11239863187074661\n",
            "strain 0.09091842919588089\n",
            "classify 1.96661376953125\n",
            "classify 2.1065673828125\n",
            "classify 2.12811279296875\n",
            "classify 2.018798828125\n",
            "classify 2.0079345703125\n",
            "classify 2.03955078125\n",
            "classify 1.91876220703125\n",
            "classify 1.96307373046875\n",
            "classify 2.0546875\n",
            "classify 2.005615234375\n",
            "classify 1.98779296875\n",
            "0.3125\n",
            "0.3125\n",
            "0.328125\n",
            "0.21875\n",
            "0.171875\n",
            "0.21875\n",
            "0.328125\n",
            "0.3125\n",
            "0.3125\n",
            "0.1875\n",
            "0.265625\n",
            "time: 2.4042115211486816 2.498357346643022\n",
            "273\n",
            "strain 0.03624889999628067\n",
            "strain 0.03800594061613083\n",
            "strain 0.034082140773534775\n",
            "strain 0.05437993258237839\n",
            "strain 0.03983777016401291\n",
            "strain 0.0492088682949543\n",
            "classify 1.980712890625\n",
            "classify 1.98529052734375\n",
            "classify 2.02008056640625\n",
            "classify 1.9027099609375\n",
            "classify 1.956787109375\n",
            "classify 1.793212890625\n",
            "classify 1.9683837890625\n",
            "classify 1.98065185546875\n",
            "classify 1.91925048828125\n",
            "classify 2.0994873046875\n",
            "classify 1.925048828125\n",
            "0.1875\n",
            "0.28125\n",
            "0.25\n",
            "0.296875\n",
            "0.25\n",
            "0.296875\n",
            "0.265625\n",
            "0.203125\n",
            "0.171875\n",
            "0.265625\n",
            "0.3125\n",
            "time: 2.659461736679077 2.4989474611560794\n",
            "274\n",
            "strain 0.0998571515083313\n",
            "strain 0.05552401766180992\n",
            "strain 0.08363046497106552\n",
            "strain 0.07273467630147934\n",
            "strain 0.0689501166343689\n",
            "strain 0.18177826702594757\n",
            "classify 1.9737548828125\n",
            "classify 2.0047607421875\n",
            "classify 2.0257568359375\n",
            "classify 1.96905517578125\n",
            "classify 2.05194091796875\n",
            "classify 1.98846435546875\n",
            "classify 1.85223388671875\n",
            "classify 2.06719970703125\n",
            "classify 1.982177734375\n",
            "classify 2.03570556640625\n",
            "classify 1.90802001953125\n",
            "0.296875\n",
            "0.203125\n",
            "0.21875\n",
            "0.21875\n",
            "0.25\n",
            "0.328125\n",
            "0.296875\n",
            "0.265625\n",
            "0.3125\n",
            "0.3125\n",
            "0.359375\n",
            "time: 2.5776679515838623 2.499235578883778\n",
            "275\n",
            "strain 0.07510291785001755\n",
            "strain 0.05361208692193031\n",
            "strain 0.15579284727573395\n",
            "strain 0.04732127860188484\n",
            "strain 0.057991452515125275\n",
            "strain 0.045224811881780624\n",
            "classify 1.978759765625\n",
            "classify 1.98162841796875\n",
            "classify 2.0792236328125\n",
            "classify 1.94708251953125\n",
            "classify 2.109130859375\n",
            "classify 1.98052978515625\n",
            "classify 1.9422607421875\n",
            "classify 2.0543212890625\n",
            "classify 1.91741943359375\n",
            "classify 1.9154052734375\n",
            "classify 1.96490478515625\n",
            "0.21875\n",
            "0.390625\n",
            "0.40625\n",
            "0.375\n",
            "0.171875\n",
            "0.28125\n",
            "0.328125\n",
            "0.28125\n",
            "0.234375\n",
            "0.328125\n",
            "0.359375\n",
            "time: 2.4099881649017334 2.498914099257925\n",
            "276\n",
            "strain 0.11092130839824677\n",
            "strain 0.10670650005340576\n",
            "strain 0.09334823489189148\n",
            "strain 0.15442663431167603\n",
            "strain 0.26839640736579895\n",
            "strain 0.0797523483633995\n",
            "classify 1.90875244140625\n",
            "classify 1.98321533203125\n",
            "classify 2.11181640625\n",
            "classify 2.1297607421875\n",
            "classify 2.0013427734375\n",
            "classify 1.8924560546875\n",
            "classify 1.9893798828125\n",
            "classify 2.124267578125\n",
            "classify 1.9974365234375\n",
            "classify 2.0753173828125\n",
            "classify 1.9183349609375\n",
            "0.234375\n",
            "0.3125\n",
            "0.34375\n",
            "0.359375\n",
            "0.203125\n",
            "0.328125\n",
            "0.171875\n",
            "0.265625\n",
            "0.328125\n",
            "0.28125\n",
            "0.328125\n",
            "time: 2.4047393798828125 2.49857619350998\n",
            "277\n",
            "strain 0.04719749093055725\n",
            "strain 0.04996940866112709\n",
            "strain 0.11599191278219223\n",
            "strain 0.3361833393573761\n",
            "strain 0.04567818343639374\n",
            "strain 0.04827310889959335\n",
            "classify 2.1058349609375\n",
            "classify 1.92950439453125\n",
            "classify 1.86383056640625\n",
            "classify 2.0419921875\n",
            "classify 1.9739990234375\n",
            "classify 2.1097412109375\n",
            "classify 2.065673828125\n",
            "classify 1.93609619140625\n",
            "classify 1.94232177734375\n",
            "classify 1.9384765625\n",
            "classify 1.96295166015625\n",
            "0.34375\n",
            "0.328125\n",
            "0.265625\n",
            "0.328125\n",
            "0.328125\n",
            "0.359375\n",
            "0.3125\n",
            "0.265625\n",
            "0.234375\n",
            "0.28125\n",
            "0.203125\n",
            "time: 2.4069297313690186 2.4982497606346077\n",
            "278\n",
            "strain 0.03845665603876114\n",
            "strain 0.10392218828201294\n",
            "strain 0.08163768798112869\n",
            "strain 0.09798870980739594\n",
            "strain 0.2218390554189682\n",
            "strain 0.04464180767536163\n",
            "classify 1.9554443359375\n",
            "classify 1.9444580078125\n",
            "classify 1.95318603515625\n",
            "classify 2.0101318359375\n",
            "classify 2.2420654296875\n",
            "classify 1.9775390625\n",
            "classify 1.9910888671875\n",
            "classify 2.06561279296875\n",
            "classify 1.9700927734375\n",
            "classify 1.98138427734375\n",
            "classify 2.10064697265625\n",
            "0.390625\n",
            "0.3125\n",
            "0.359375\n",
            "0.328125\n",
            "0.3125\n",
            "0.234375\n",
            "0.203125\n",
            "0.28125\n",
            "0.3125\n",
            "0.1875\n",
            "0.3125\n",
            "time: 2.672269344329834 2.4988755923445507\n",
            "279\n",
            "strain 0.048562586307525635\n",
            "strain 0.07777439802885056\n",
            "strain 0.042207472026348114\n",
            "strain 0.0489744134247303\n",
            "strain 0.10356079041957855\n",
            "strain 0.04301849752664566\n",
            "classify 2.017822265625\n",
            "classify 2.0443115234375\n",
            "classify 1.93597412109375\n",
            "classify 2.0269775390625\n",
            "classify 2.1207275390625\n",
            "classify 1.99755859375\n",
            "classify 1.9866943359375\n",
            "classify 2.14111328125\n",
            "classify 1.9930419921875\n",
            "classify 1.94854736328125\n",
            "classify 2.0225830078125\n",
            "0.296875\n",
            "0.25\n",
            "0.203125\n",
            "0.265625\n",
            "0.234375\n",
            "0.234375\n",
            "0.25\n",
            "0.328125\n",
            "0.28125\n",
            "0.25\n",
            "0.28125\n",
            "time: 2.5790460109710693 2.4991716504096986\n",
            "280\n",
            "strain 0.11555426567792892\n",
            "strain 0.11043328046798706\n",
            "strain 0.07454526424407959\n",
            "strain 0.07673861086368561\n",
            "strain 0.20433709025382996\n",
            "strain 0.03632204234600067\n",
            "classify 2.02734375\n",
            "classify 1.820068359375\n",
            "classify 1.9482421875\n",
            "classify 2.074462890625\n",
            "classify 1.91259765625\n",
            "classify 2.0989990234375\n",
            "classify 2.0328369140625\n",
            "classify 2.1673583984375\n",
            "classify 2.03326416015625\n",
            "classify 2.02581787109375\n",
            "classify 2.0726318359375\n",
            "0.296875\n",
            "0.328125\n",
            "0.28125\n",
            "0.359375\n",
            "0.375\n",
            "0.234375\n",
            "0.234375\n",
            "0.296875\n",
            "0.265625\n",
            "0.234375\n",
            "0.25\n",
            "time: 2.4055519104003906 2.498840517845018\n",
            "281\n",
            "strain 0.04083404690027237\n",
            "strain 0.04117985814809799\n",
            "strain 0.04804863780736923\n",
            "strain 0.031916115432977676\n",
            "strain 0.14278344810009003\n",
            "strain 0.1410641074180603\n",
            "classify 2.07928466796875\n",
            "classify 1.96649169921875\n",
            "classify 2.01318359375\n",
            "classify 2.01165771484375\n",
            "classify 2.03350830078125\n",
            "classify 1.9974365234375\n",
            "classify 2.01727294921875\n",
            "classify 2.07720947265625\n",
            "classify 2.02728271484375\n",
            "classify 1.9847412109375\n",
            "classify 2.0838623046875\n",
            "0.359375\n",
            "0.359375\n",
            "0.234375\n",
            "0.328125\n",
            "0.25\n",
            "0.234375\n",
            "0.3125\n",
            "0.3125\n",
            "0.3125\n",
            "0.359375\n",
            "0.25\n",
            "time: 2.42828631401062 2.4985923741726164\n",
            "282\n",
            "strain 0.35864531993865967\n",
            "strain 0.04415067285299301\n",
            "strain 0.10435372591018677\n",
            "strain 0.05082330480217934\n",
            "strain 0.058572735637426376\n"
          ]
        }
      ],
      "source": [
        "# @title strain ctrain test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter()\n",
        "\n",
        "def strain(model, dataloader, optim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.train()\n",
        "    for i, (x, _) in enumerate(dataloader):\n",
        "        x = x.to(device)#.to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            loss = model.loss(x)\n",
        "        optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 5) # 0.5\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            m=0.999 # 0.99 m = next(momentum_scheduler)\n",
        "            norms=[]\n",
        "            for param_q, param_k in zip(model.student.parameters(), model.teacher.parameters()):\n",
        "                param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        if i%10==0: print(\"strain\",loss.item())\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "\n",
        "        if i>=50: break\n",
        "    writer.close()\n",
        "\n",
        "def ctrain(model, classifier, dataloader, coptim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.eval()\n",
        "    classifier.train()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            with torch.no_grad():\n",
        "                sx = model(x).detach()\n",
        "            y_ = classifier(sx)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "        coptim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(coptim)\n",
        "        scaler.update()\n",
        "        print(\"classify\",loss.item())\n",
        "        try: wandb.log({\"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        with torch.no_grad():\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        print(correct/len(y))\n",
        "        try: wandb.log({\"correct\": correct/len(y)})\n",
        "        # try: wandb.log({\"correct\": correct/len(y), \"rankme\": rankme, \"lidar\": lidar})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "import time\n",
        "start = begin = time.time()\n",
        "# for i in range(1):\n",
        "for i in range(1000): # 1000\n",
        "    print(i)\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    strain(ijepa, train_loader, optim)\n",
        "    ctrain(ijepa, classifier, train_loader, coptim)\n",
        "    test(ijepa, classifier, test_loader)\n",
        "\n",
        "    print('time:',time.time() - start, (time.time()-begin)/(i+1))\n",
        "    start = time.time()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzo9DMDPcOxu",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title save/load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder='/content/drive/MyDrive/jepa/'\n",
        "\n",
        "# # modelsd, optimsd = torch.load(folder+'SeqJEPA.pkl', map_location=device).values()\n",
        "# modelsd, optimsd = torch.load('SeqJEPA.pkl', map_location=device).values()\n",
        "# seq_jepa.load_state_dict(modelsd, strict=False)\n",
        "# optim.load_state_dict(optimsd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNNPOuUmcSNf",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "checkpoint = {'model': ijepa.state_dict(), 'optimizer': optim.state_dict()}\n",
        "torch.save(checkpoint, folder+'ijepa.pkl')\n",
        "# torch.save(checkpoint, 'IJEPA.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## store"
      ],
      "metadata": {
        "id": "yl-xtHthFl0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title GoldenGateRoPE2d\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class GoldenGateRoPE2d(nn.Module): # jerryxio.ng/posts/nd-rope\n",
        "    def __init__(self, image_size, n_heads, n_freqs, min_freq=.8, max_freq=10, n_zero_freqs=0):\n",
        "        super().__init__()\n",
        "        intv = math.pi * (math.sqrt(5)-1)/2 # mod pi instead of 2pi # pi*(sqrt5+-1)/2 ; + and - are equivalent bec mod pi\n",
        "        # intv = math.pi * (math.sqrt(5)-1) # https://en.wikipedia.org/wiki/Golden_angle\n",
        "        speed = torch.cat([torch.zeros(n_zero_freqs), min_freq * (max_freq/min_freq) ** torch.linspace(0,1,n_freqs-n_zero_freqs)]).unsqueeze(-1) # [n_freqs,1] # og\n",
        "        angle = torch.arange(n_heads*n_freqs).reshape(n_heads, n_freqs) * intv # [n_heads, n_freqs]\n",
        "        direction = torch.stack([torch.cos(angle), torch.sin(angle)], dim=-1) # [n_heads, n_freqs, 2]\n",
        "        h, w = image_size\n",
        "        xlim, ylim = w/h, h/w\n",
        "        y, x = torch.meshgrid(torch.linspace(-ylim, ylim, h), torch.linspace(-xlim, xlim, w), indexing=\"ij\") # [h,w], y:row_num, x:col_num\n",
        "        pos = torch.stack([x, y], dim=-1).reshape(-1,1,1,2) # [h*w,1,1,2] cartesian coords\n",
        "        theta = (speed*direction*pos).sum(dim=-1) # [t,n_heads,n_freqs,2]->[t,n_heads,d_head]\n",
        "        self.theta = theta\n",
        "        cos, sin = torch.cos(theta), torch.sin(theta)\n",
        "        self.affine = torch.stack([cos, -sin, sin, cos], dim=-1).transpose(0,1).reshape(1,n_heads,h*w,n_freqs,2,2).to(device) # [t,n_heads,n_freqs,4]->[1,n_heads,t,n_freqs,2,2]\n",
        "\n",
        "    def forward(self, x): # [b,h,t,d]\n",
        "        return (self.affine @ x.unflatten(-1, (-1,2)).unsqueeze(-1)).flatten(-3) # @ [b,h,t,d_head//2,2,1]\n",
        "\n",
        "# /2 better\n",
        "# speed [n_freqs,1] # og best\n",
        "# w/h best\n",
        "# rope < ggrope < learned\n",
        "\n",
        "# image_size=(8,8)\n",
        "image_size=(20,30)\n",
        "# image_size=(90,120)\n",
        "n_heads=4\n",
        "n_freqs=6\n",
        "ggrope = GoldenGateRoPE2d(image_size, n_heads, n_freqs)\n",
        "\n",
        "# x = torch.rand(2, *image_size, n_heads, n_freqs*2)\n",
        "x = torch.rand(2, n_heads, image_size[0]*image_size[1], n_freqs*2)\n",
        "out = ggrope(x)\n",
        "print(out.shape)\n",
        "# # print(out[0])\n",
        "# theta = ggrope.theta.flatten(-2).permute(2,0,1).unsqueeze(1) # [t,n_heads,d_head][b,1,h,w]\n",
        "theta = ggrope.theta.flatten(-2).T.reshape(n_heads*n_freqs, 1, *image_size) # [t,n_heads,d_head]->[d,1,h,w]\n",
        "cy, cx = image_size[0]//2, image_size[1]//2\n",
        "sim = torch.cos(theta-theta[...,cy,cx][...,None,None]) # [b,1,h,w]\n",
        "# sim = sim.unflatten(0, (n_heads, n_freqs)).mean(1)\n",
        "# print(sim.shape)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    print(npimg.shape)\n",
        "    plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "# bhwc\n",
        "\n",
        "import torchvision\n",
        "imshow(torchvision.utils.make_grid(sim, nrow=n_freqs))\n",
        "\n"
      ],
      "metadata": {
        "id": "e7HYQxn6n6iD",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title RoPE pos\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class RoPE(nn.Module):\n",
        "    def __init__(self, dim, seq_len=512, top=torch.pi, base=10000):\n",
        "    # def __init__(self, dim, seq_len=512, min_freq=1, max_freq=400, n_zero_freqs=0):\n",
        "        super().__init__()\n",
        "        self.dim, self.top, self.base = dim, top, base\n",
        "        speed = top / (base ** (torch.arange(0, dim, step=2) / dim)) # [dim//2]\n",
        "        pos = torch.arange(seq_len).unsqueeze(-1) # [t,1]\n",
        "        # speed = torch.cat([torch.zeros(n_zero_freqs), min_freq * (max_freq/min_freq) ** torch.linspace(0,1,dim//2-n_zero_freqs)]) # [dim//2]\n",
        "        # pos = torch.linspace(0, 1, seq_len).unsqueeze(-1) # [t,1]\n",
        "        theta = (speed*pos) # [t,1]*[dim//2]=[t,d//2]\n",
        "        self.theta = theta\n",
        "        cos, sin = torch.cos(theta), torch.sin(theta)\n",
        "        self.affine = torch.stack([cos, -sin, sin, cos], dim=-1).unflatten(-1,(2,2)).to(device)#[None,None,...] # [t,d//2,4]->[t,d//2,2,2] # [1,1,t,d//2,2,2]\n",
        "\n",
        "\n",
        "        # angles = theta[None,...,None] # [seq_len, 1] * [dim//2] -> [1, 1, seq_len, dim//2, 1]\n",
        "        # self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [1, 1, seq_len, dim//2, 2] -> [1, 1, seq_len, dim]\n",
        "        # # self.rot_emb = torch.cat([sin, cos], dim=-1).flatten(-2).to(device) # [1, 1, seq_len, dim//2, 2] -> [1,1,seq_len,dim]\n",
        "        # print('self.rot_emb', self.rot_emb.shape)\n",
        "\n",
        "    def forward(self, x, ind=None): # [b,h,t,d], [b,t]\n",
        "        # seq_len = x.size(-2)\n",
        "        b,_,seq_len,_ = x.shape\n",
        "        if ind!=None: seq_len = max(seq_len, ind.max()+1)\n",
        "        if self.affine.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.top, self.base)\n",
        "        if ind!=None:\n",
        "            # print(\"if affine, x\",self.affine.shape, x.shape)\n",
        "            return (self.affine.unsqueeze(0).expand(b,-1,-1,-1,-1)[torch.arange(b, device=device).unsqueeze(-1), ind].unsqueeze(1) @ x.unflatten(-1, (-1,2)).unsqueeze(-1)).flatten(-3) # @ [b,h,t,d_head//2,2,1]\n",
        "        else:\n",
        "            # print(\"else affine, x\",self.affine.shape, x.shape) # [64, 4, 2, 2], [64, 8, 10, 8]\n",
        "            return (self.affine[None,None,...] @ x.unflatten(-1, (-1,2)).unsqueeze(-1)).flatten(-3) # @ [b,h,t,d_head//2,2,1]\n",
        "\n",
        "\n",
        "        # # if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.base)\n",
        "        # if ind==None:\n",
        "        #     # print(\"if rot, x\",self.rot_emb.shape, x.shape)\n",
        "        #     return x * self.rot_emb.unsqueeze(1)#[None,None,...]\n",
        "        # else:\n",
        "        #     return x * (self.rot_emb.expand(b,-1,-1)[torch.arange(b, device=device).unsqueeze(-1), ind].unsqueeze(1))\n",
        "\n",
        "\n",
        "\n",
        "dim=64\n",
        "seq_len=56\n",
        "rope = RoPE(dim, seq_len, top=torch.pi, base=100)\n",
        "# rope = RoPE(dim, seq_len, min_freq=1, max_freq=200, n_zero_freqs=0)\n",
        "\n",
        "batch=2\n",
        "t=50\n",
        "x = torch.rand(batch, 4, seq_len, dim, device=device)\n",
        "out = rope(x)\n",
        "print(\"out1\", out.shape)\n",
        "x = torch.rand(batch, 4, t, dim, device=device)\n",
        "pos = torch.randint(0,seq_len,(batch,t), device=device)\n",
        "out = rope(x, pos)\n",
        "print(\"out2\", out.shape)\n",
        "\n",
        "# theta = rope.theta # [t,d//2]\n",
        "# sim = torch.cos(theta-theta[0].unsqueeze(0)).T\n",
        "# # print(sim.shape)\n",
        "\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# def imshow(img):\n",
        "#     npimg = img.numpy()\n",
        "#     print(npimg.shape)\n",
        "#     plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "#     plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "#     plt.show()\n",
        "\n",
        "# import torchvision\n",
        "# imshow(torchvision.utils.make_grid(sim, nrow=dim//2))\n",
        "\n"
      ],
      "metadata": {
        "id": "eA2R-JZ7G67P",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title random_masking\n",
        "import torch\n",
        "\n",
        "def random_masking(length, mask_ratio, b=64):\n",
        "    noise = torch.rand(b, length)\n",
        "    len_mask = int(length * mask_ratio)\n",
        "    _, msk_ind = torch.topk(noise, k=len_mask, dim=-1, sorted=False) # val, ind -> [b,len_mask]\n",
        "    _, keep_ind = torch.topk(noise, k=length-len_mask, largest=False, dim=-1, sorted=False) # val, ind -> [b,len_keep]\n",
        "    return msk_ind, keep_ind\n",
        "\n",
        "# msk_ind, keep_ind = random_masking(10, .3, b=2)\n",
        "\n",
        "# x_ = torch.rand(4, 3, 2)\n",
        "# print(x_)\n",
        "# # ids = torch.tensor([0, 2, 1])[None,:,None]\n",
        "# # ids = torch.tensor([0, 2, 1])[None,:,None].repeat(4,1,2)\n",
        "# ids = torch.tensor([1, 2, 0])[None,:,None].repeat(4,1,2)\n",
        "# # o = torch.gather(x_, dim=1, index=ids)\n",
        "# o = torch.zeros_like(x_).scatter_(dim=1, index=ids, src=x_)\n",
        "# print(o)\n"
      ],
      "metadata": {
        "id": "k3nfeLM4wtkJ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62UPGVucmGNe",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title AttentionBlock pos\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "# import inspect\n",
        "# class Seq(nn.Sequential):\n",
        "#     def __init__(self, *args):\n",
        "#         super().__init__(*args)\n",
        "#         for layer in self:\n",
        "#             params = inspect.signature(layer.forward).parameters.keys()\n",
        "#             layer._fwdparams = ','.join(params)\n",
        "\n",
        "#     def forward(self, x, pos=None, masks=None):\n",
        "#         arg_map = {'pos':pos, 'masks':masks}\n",
        "#         for layer in self:\n",
        "#             args = [x]\n",
        "#             # if 'masks' in layer._fwdparams: args.append(masks)\n",
        "#             args.extend(arg_map[p] for p in arg_map if p in layer._fwdparams)\n",
        "#             # print(layer._fwdparams, args)\n",
        "#             x = layer(*args)\n",
        "#         return x\n",
        "\n",
        "# import inspect\n",
        "# class Seq(nn.Sequential):\n",
        "#     def __init__(self, *args):\n",
        "#         super().__init__(*args)\n",
        "#         # for layer in self:\n",
        "#         #     params = inspect.signature(layer.forward).parameters.keys()\n",
        "#         #     layer._fwdparams = ','.join(params)\n",
        "\n",
        "#     # def forward(self, x, pos=None, masks=None):\n",
        "#     def forward(self, x, *args, **kwargs):\n",
        "#         # arg_map = {'pos':pos, 'masks':masks}\n",
        "#         for layer in self:\n",
        "#             # args = [x]\n",
        "#             # if 'masks' in layer._fwdparams: args.append(masks)\n",
        "#             # args.extend(arg_map[p] for p in arg_map if p in layer._fwdparams)\n",
        "#             # print(layer._fwdparams, args)\n",
        "#             x = layer(x, *args)\n",
        "#         return x\n",
        "\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        self._kwargs = [[name for name, p in inspect.signature(layer.forward).parameters.items()] for layer in self]\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        for layer, _kwargs in zip(self, self._kwargs):\n",
        "            x = layer(x, *args, **{k: v for k, v in kwargs.items() if k in _kwargs})\n",
        "        return x\n",
        "\n",
        "class SelfAttn(nn.Module):\n",
        "    def __init__(self, dim, n_heads):\n",
        "        super().__init__()\n",
        "        self.dim, self.n_heads = dim, n_heads\n",
        "        d_head = dim//n_heads\n",
        "        self.rope = RoPE(d_head, seq_len=64, top=torch.pi, base=1000)\n",
        "        # self.rope = RoPE(d_head, seq_len=512, base=1000)\n",
        "        # self.rope = RoPE2D(d_head, h=64, w=64, base=100)\n",
        "        # self.rope[0] = 1 # id for smry h\n",
        "        self.qkv = nn.Linear(dim, dim*3, bias=False)\n",
        "        # self.lin = nn.Linear(dim, dim)\n",
        "        self.lin = zero_module(nn.Linear(dim, dim))\n",
        "        self.scale = d_head**-.5\n",
        "\n",
        "    # def forward(self, x): # [b,t,d]\n",
        "    def forward(self, x, pos=None): # [b,t,d]\n",
        "        # print('SelfAttn', x.shape)\n",
        "        q,k,v = self.qkv(x).unflatten(-1, (self.n_heads,-1)).transpose(1,2).chunk(3, dim=-1) # [b, t, n_heads, d_head] -> [b, n_heads, t, d_head]\n",
        "        if pos==None: q, k = self.rope(q), self.rope(k)\n",
        "        else:\n",
        "            # print('SelfAttn fwd', x.shape, pos.shape)\n",
        "            q, k = self.rope(q, pos), self.rope(k, pos)\n",
        "\n",
        "        x = F.scaled_dot_product_attention(q,k,v, attn_mask=None) # https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html\n",
        "        # q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2)\n",
        "        # context = k.transpose(-2,-1) @ v # [batch, n_heads, d_head, d_head]\n",
        "        # x = q @ context # [b,n_heads,t,d_head]\n",
        "        x = x.transpose(1,2).flatten(2)\n",
        "        return self.lin(x)\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, mult=4, drop=0.):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        # self.norm = nn.RMSNorm(d_model, elementwise_affine=False) # LayerNorm RMSNorm\n",
        "        self.norm1, self.norm2 = nn.LayerNorm(d_model), nn.LayerNorm(d_model)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "        self.attn = SelfAttn(d_model, n_heads)\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        ff_dim=d_model*mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), act, nn.Dropout(drop), nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "            nn.RMSNorm(ff_dim), act, nn.Dropout(drop), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(drop), nn.Linear(d_model, ff_dim), act,\n",
        "            # nn.RMSNorm(ff_dim), nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "\n",
        "    # def forward(self, x): # [b,t,d]\n",
        "    def forward(self, x, pos=None): # [b,t,d]\n",
        "        # print('attnblk fwd',x.shape)\n",
        "        # x = x + self.drop(self.attn(self.norm1(x)))\n",
        "        x = x + self.drop(self.attn(self.norm1(x), pos))\n",
        "        x = x + self.ff(x)\n",
        "        # x = x + self.drop(self.ff(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, patch_size, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        patch_size=2\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Conv2d(in_dim, d_model, patch_size, patch_size), # like patch\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 1, 7//2, bias=False)\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.MaxPool2d(3,2,1)\n",
        "            nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.BatchNorm2d(d_model), nn.ReLU(),\n",
        "            nn.Conv2d(d_model, d_model, 3, 2, 3//2, bias=False)\n",
        "            )\n",
        "        # self.embed.requires_grad=False\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, 32*32, d_model)*.02)\n",
        "        # self.pos_emb = nn.Parameter(RoPE2D(dim=d_model, h=8, w=8, base=1000), requires_grad=False)\n",
        "\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=(32//patch_size)**2, base=10000), requires_grad=False)\n",
        "        # self.transformer = nn.Sequential(*[AttentionBlock(d_model, d_model, n_heads) for _ in range(nlayers)])\n",
        "        self.transformer = Seq(*[AttentionBlock(d_model, n_heads) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, context_indices=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x).flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c] # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        # x = self.pos_enc(x)\n",
        "        # print(\"TransformerModel\",x.shape, self.pos_emb.shape)\n",
        "        # print(\"TransformerModel\",x.shape)\n",
        "        # x = x + self.pos_emb[:,:x.shape[1]]\n",
        "        # if context_indices != None: x = x[torch.arange(x.shape[0]).unsqueeze(-1), context_indices] # [batch, num_context_toks, d_model]\n",
        "        # x = self.transformer(x)\n",
        "\n",
        "        if context_indices != None:\n",
        "            # print('vit transformer',x[torch.arange(x.shape[0]).unsqueeze(-1), context_indices].shape, context_indices.shape)\n",
        "            x = self.transformer(x[torch.arange(x.shape[0]).unsqueeze(-1), context_indices], context_indices)\n",
        "        else: x = self.transformer(x)\n",
        "\n",
        "        out = self.norm(x)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out\n",
        "\n",
        "d_model = 64\n",
        "in_dim = 3\n",
        "patch_size = 2\n",
        "# model = ViT(patch_size, in_dim, d_model, n_heads=4, nlayers=4, drop=0.).to(device)\n",
        "model = ViT(patch_size, in_dim, d_model, n_heads=4, nlayers=1, drop=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x = torch.rand((5, in_dim, 32, 32), device=device) # [b,c,h,w]\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test seq\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        # for layer in self:\n",
        "        #     params = inspect.signature(layer.forward).parameters.keys()\n",
        "        #     layer._fwdparams = ','.join(params)\n",
        "\n",
        "    # def forward(self, x, pos=None, masks=None):\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        # arg_map = {'pos':pos, 'masks':masks}\n",
        "        for layer in self:\n",
        "            # args = [x]\n",
        "            # if 'masks' in layer._fwdparams: args.append(masks)\n",
        "            # args.extend(arg_map[p] for p in arg_map if p in layer._fwdparams)\n",
        "            # print(layer._fwdparams, args)\n",
        "            x = layer(x, *args, **kwargs)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        self._accepted_kwargs = []\n",
        "        for layer in self:\n",
        "            sig = inspect.signature(layer.forward)\n",
        "            print(sig.parameters.items())\n",
        "            self._accepted_kwargs.append(\n",
        "                [name for name, p in sig.parameters.items()]\n",
        "                # {name for name, p in sig.parameters.items()\n",
        "                #  if p.kind in (p.POSITIONAL_OR_KEYWORD, p.KEYWORD_ONLY)}\n",
        "            )\n",
        "\n",
        "            # if any(p.kind == p.VAR_KEYWORD for p in sig.parameters.values()):\n",
        "            #     accepted = None  # means \"pass everything\"\n",
        "\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        # for layer in self:\n",
        "        #     x = layer(x, *args, **kwargs)\n",
        "\n",
        "        # for layer in self:\n",
        "        #     sig = inspect.signature(layer.forward)\n",
        "        #     # keep only accepted keyword arguments\n",
        "        #     filtered_kwargs = {\n",
        "        #         k: v for k, v in kwargs.items()\n",
        "        #         if k in sig.parameters\n",
        "        #     }\n",
        "        #     x = layer(x, *args, **filtered_kwargs)\n",
        "\n",
        "        for layer, accepted in zip(self, self._accepted_kwargs):\n",
        "            filtered_kwargs = {k: v for k, v in kwargs.items() if k in accepted}\n",
        "            x = layer(x, *args, **filtered_kwargs)\n",
        "        # if accepted is None:\n",
        "        #     x = layer(x, *args, **kwargs)\n",
        "        # else:\n",
        "        #     filtered = {k: v for k, v in kwargs.items() if k in accepted}\n",
        "        #     x = layer(x, *args, **filtered)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        self._kwargs = [[name for name, p in inspect.signature(layer.forward).parameters.items()] for layer in self]\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        for layer, _kwargs in zip(self, self._kwargs):\n",
        "            x = layer(x, *args, **{k: v for k, v in kwargs.items() if k in _kwargs})\n",
        "        return x\n",
        "\n",
        "\n",
        "class SS(nn.Module):\n",
        "    def __init__(self, d):\n",
        "        super().__init__()\n",
        "        self.d = d\n",
        "    def forward(self, x, y, pos=None, bos=None):\n",
        "        # print('SS', x, pos, bos)\n",
        "        print('SS', x, y, pos, bos)\n",
        "        return x+1\n",
        "\n",
        "class PP(nn.Module):\n",
        "    def __init__(self, d):\n",
        "        super().__init__()\n",
        "        self.d = d\n",
        "    def forward(self, x, pos=None, dos=None):\n",
        "        print('PP', x, pos, dos)\n",
        "    # def forward(self, x, y, pos=None, dos=None):\n",
        "        # print('PP', x, y, pos, dos)\n",
        "        return x+1\n",
        "\n",
        "d=2\n",
        "# TT = Seq(*[SS(d) for _ in range(2)])\n",
        "TT = Seq(*[SS(d), PP(d)])\n",
        "# out = TT(3, a=2, pos='p')\n",
        "# out = TT(3, a=2, bos='b')\n",
        "# out = TT(3, a=2, dos='d')\n",
        "out = TT(3, 'q','w')\n",
        "# out = TT(3, 'q','w', pos=2, dos='d')\n",
        "# out = TT(3, 'q', pos=2, dos='d')\n",
        "print(out)\n"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "OgZ6d59vOQil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4J2ahp3wmqcg"
      },
      "outputs": [],
      "source": [
        "# @title supervised train test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "# def strain(model, dataloader, optim, scheduler=None):\n",
        "def strain(model, classifier, dataloader, optim, coptim, scheduler=None):\n",
        "    model.train()\n",
        "    classifier.train()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x = x.to(device)#.to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "        y = y.to(device)\n",
        "\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        # print(loss)\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # for p in list(filter(lambda p: p.grad is not None, model.parameters())):\n",
        "        #     print(p.grad.data.norm(2).item())\n",
        "        # print(\"max grad norm\", max([p.grad.data.norm(2).item() for p in list(filter(lambda p: p.grad is not None, model.parameters()))]))\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 10) # 0.5\n",
        "\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        print(\"strain\",loss.item())\n",
        "        # for param in ijepa.student.cls: print(param.data)\n",
        "        # for param in ijepa.predicter.cls: print(param.data)\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=50: break\n",
        "\n",
        "\n",
        "# def test(model, dataloader):\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        # .to(torch.bfloat16)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "        loss = F.cross_entropy(y_, y)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        print(correct/len(y))\n",
        "        try: wandb.log({\"correct\": correct/len(y), \"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "for i in range(1000):\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # np.random.shuffle(train_indices); np.random.shuffle(val_indices)\n",
        "    # train_sampler, valid_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(val_indices)\n",
        "    # # batch_size = 64 #512\n",
        "    # train_loader = DataLoader(train_data, sampler=train_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "    # test_loader = DataLoader(train_data, sampler=valid_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True)\n",
        "\n",
        "    # strain(ijepa, train_loader, optim)\n",
        "    strain(ijepa, classifier, train_loader, optim, coptim)\n",
        "    test(ijepa, classifier, test_loader)\n",
        "\n",
        "    # strain(violet, train_loader, voptim)\n",
        "    # test(violet, test_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## trash"
      ],
      "metadata": {
        "id": "1e1Yi9JrLPLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title GoldenGateRoPE2d\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class GoldenGateRoPE2d(nn.Module): # jerryxio.ng/posts/nd-rope\n",
        "    def __init__(self, image_size: tuple[int, int], n_heads, n_freqs):\n",
        "        super().__init__()\n",
        "        n_zero_freqs = 1\n",
        "        min_freq, max_freq = .2, 20\n",
        "        speed = torch.cat([torch.zeros(n_zero_freqs), min_freq * (max_freq/min_freq) ** torch.linspace(0,1,n_freqs-n_zero_freqs)]) # [n_freqs]\n",
        "        direction_spacing = math.pi * (math.sqrt(5)-1)/2\n",
        "        phi = torch.arange(n_heads * n_freqs).reshape(n_heads, n_freqs) * direction_spacing # [n_heads, n_freqs]\n",
        "        directions = torch.stack((torch.cos(phi), torch.sin(phi)), dim=-1) # [n_heads, n_freqs, 2]\n",
        "        # print('directions', directions)\n",
        "        vel = speed.unsqueeze(-1) * directions # speed in direction[n_heads, n_freqs, 2]\n",
        "\n",
        "        H, W = image_size\n",
        "        xlim, ylim = math.sqrt(W / H), math.sqrt(H / W)\n",
        "        print(xlim, ylim)\n",
        "        y, x = torch.meshgrid(torch.linspace(-ylim, ylim, H), torch.linspace(-xlim, xlim, W), indexing=\"ij\") # [h,w], y:row_num, x:col_num\n",
        "        # y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\") # print(y,x) # [h,w], y:row_num, x:col_num\n",
        "\n",
        "        pos = torch.stack((x, y), dim=-1).reshape(H, W, 1, 1, 2) # cartesian coords\n",
        "\n",
        "        theta = (vel*pos).sum(dim=-1) # [h,w,n_heads,n_freqs,2]->[h,w,n_heads,d_head]\n",
        "        print('theta', theta.shape)\n",
        "        self.cos, self.sin = torch.cos(theta), torch.sin(theta)\n",
        "        print('self.cos', self.cos.shape)\n",
        "\n",
        "    def forward(self, input): # [b,h,w,n_head,d_head]\n",
        "        # x, y = input.float().chunk(2, dim=-1) # [b,h,w,n_head,n_freqs]\n",
        "        x, y = input.float().unflatten(-1, (-1,2)).chunk(2, dim=-1)\n",
        "        x, y = x.squeeze(-1), y.squeeze(-1)\n",
        "        x_out = x * self.cos - y * self.sin\n",
        "        y_out = x * self.sin + y * self.cos\n",
        "        # output = torch.cat((x_out, y_out), dim=-1)\n",
        "        output = torch.stack((x_out, y_out), dim=-1).flatten(-2)\n",
        "        return output.type_as(input)\n",
        "\n",
        "\n",
        "\n",
        "class GoldenGateRoPE2d2(nn.Module): # jerryxio.ng/posts/nd-rope\n",
        "    def __init__(self, image_size, n_heads, n_freqs):\n",
        "        super().__init__()\n",
        "        n_zero_freqs = 1\n",
        "        min_freq, max_freq = .2, 20\n",
        "        intv = math.pi * (math.sqrt(5)-1)/2\n",
        "        speed = torch.cat([torch.zeros(n_zero_freqs), min_freq * (max_freq/min_freq) ** torch.linspace(0,1,n_freqs-n_zero_freqs)]) # [n_freqs]\n",
        "        phi = torch.arange(n_heads*n_freqs).reshape(n_heads, n_freqs) * intv # [n_heads, n_freqs]\n",
        "        direction = torch.stack((torch.cos(phi), torch.sin(phi)), dim=-1) # [n_heads, n_freqs, 2]\n",
        "        vel = speed.unsqueeze(-1) * direction # speed in direction[n_heads, n_freqs, 2]\n",
        "        h, w = image_size\n",
        "        xlim, ylim = math.sqrt(w/h), math.sqrt(h/w)\n",
        "        y, x = torch.meshgrid(torch.linspace(-ylim, ylim, h), torch.linspace(-xlim, xlim, w), indexing=\"ij\") # [h,w], y:row_num, x:col_num\n",
        "        pos = torch.stack([x, y], dim=-1)[...,None,None,:] # [h,w,1,1,2] cartesian coords\n",
        "        theta = (vel*pos).sum(dim=-1) # [h,w,n_heads,n_freqs,2]->[h,w,n_heads,d_head]\n",
        "        cos, sin = torch.cos(theta), torch.sin(theta)\n",
        "        self.affine = torch.stack([cos, -sin, sin, cos], dim=-1).unflatten(-1, (2,2))\n",
        "\n",
        "    def forward(self, input): # [b,h,w,n_head,d_head]\n",
        "        return (self.affine @ input.unflatten(-1, (-1,2)).unsqueeze(-1)).flatten(-3)\n",
        "\n",
        "\n",
        "\n",
        "image_size=(5,7)\n",
        "n_heads=4\n",
        "d_head=16\n",
        "ggrope = GoldenGateRoPE2d(image_size, n_heads, d_head//2)\n",
        "ggrope2 = GoldenGateRoPE2d2(image_size, n_heads, d_head//2)\n",
        "\n",
        "# x = torch.rand(2, *image_size, n_heads, d_head)\n",
        "out = ggrope(x)\n",
        "out2 = ggrope2(x)\n",
        "print(out.shape)\n",
        "print(out2.shape)\n",
        "# print(out[0])\n",
        "# print(out2[0])\n",
        "# print((out==out2)[0])\n",
        "print((out==out2).all())\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "KUvHrTi4LSEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test gather\n",
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "l=56\n",
        "b,t,d = 2,50,64\n",
        "src = torch.rand(b,l,d, device=device)\n",
        "idx = torch.randint(0,l,(b,t), device=device)\n",
        "out = src[torch.arange(b).unsqueeze(-1), idx]\n",
        "# out = torch.take_along_dim(src, idx.unsqueeze(-1).expand(-1,-1,d), dim=1)\n",
        "# out = src.index_select(0, idx).reshape(b, t, d) # == src[idx]\n",
        "# out = src.gather(1, idx.unsqueeze(-1).expand(-1, -1, d))\n",
        "print(out.shape)\n",
        "\n",
        "# %timeit out = src.expand(b,-1,-1).gather(1, idx.unsqueeze(-1).expand(-1, -1, src.size(-1))) # 2.53 ms\n",
        "# %timeit out = src.expand(b,-1,-1)[torch.arange(b, device=device).unsqueeze(-1), idx] # 1.39 ms\n"
      ],
      "metadata": {
        "id": "K6saOuAX-FV8",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ],
      "metadata": {
        "id": "QNt7SRjj9g_l"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}