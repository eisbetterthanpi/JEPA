{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/I_JEPA_down.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LxACli7GdyGq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e38d20f-d159-4099-e54a-3b02bf98dfba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170M/170M [00:05<00:00, 30.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title data\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "transform = transforms.Compose([transforms.ToTensor(),])\n",
        "# transform = transforms.Compose([transforms.RandomResizedCrop((32,32), scale=(.3,1.)), transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "# CIFAR10: 60000 32x32 color images in 10 classes, with 6000 images per class\n",
        "train_data = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
        "test_data = torchvision.datasets.CIFAR10(root='data', train=False, download=True, transform=transform)\n",
        "batch_size = 64 # 4\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# dataiter = iter(train_loader) # get some random training images\n",
        "# images, labels = next(dataiter)\n",
        "# print(images.shape) # [batch, 3, 32, 32]\n",
        "# imshow(torchvision.utils.make_grid(images))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3XSZZyUPY1i"
      },
      "source": [
        "## vit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "GD7ezZzmhTHU"
      },
      "outputs": [],
      "source": [
        "# @title RoPE\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def RoPE(dim, seq_len=512, top=torch.pi, base=1000):\n",
        "    theta = top / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "    pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "    angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim//2] -> [1, seq_len, dim//2, 1]\n",
        "    rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [1, seq_len, dim//2, 2] -> [1, seq_len, dim]\n",
        "    return rot_emb\n",
        "\n",
        "\n",
        "# class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "#     def __init__(self, dim, seq_len=512, top=torch.pi, base=1000):\n",
        "#         super().__init__()\n",
        "#         self.dim, self.base = dim, base\n",
        "#         theta = top / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "#         pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "#         angles = (pos * theta)[None,None,...,None] # [seq_len, 1] * [dim//2] -> [1, 1, seq_len, dim//2, 1]\n",
        "#         self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [1, 1, seq_len, dim//2, 2] -> [1, 1, seq_len, dim]\n",
        "\n",
        "#     def forward(self, x): # [b,h,t,d]\n",
        "#         seq_len = x.size(-2)\n",
        "#         if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.base)\n",
        "#         return x * self.rot_emb#[:,:,:seq_len]\n",
        "\n",
        "\n",
        "# @title proper rope\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class RoPE(nn.Module):\n",
        "    def __init__(self, dim, seq_len=512, top=torch.pi, base=1000):\n",
        "    # def __init__(self, dim, seq_len=512, min_freq=1, max_freq=400, n_zero_freqs=0):\n",
        "        super().__init__()\n",
        "        speed = top / (base ** (torch.arange(0, dim, step=2) / dim)) # [dim//2]\n",
        "        pos = torch.arange(seq_len).unsqueeze(-1) # [t,1]\n",
        "        # speed = torch.cat([torch.zeros(n_zero_freqs), min_freq * (max_freq/min_freq) ** torch.linspace(0,1,dim//2-n_zero_freqs)]) # [dim//2]\n",
        "        # pos = torch.linspace(0, 1, seq_len).unsqueeze(-1) # [t,1]\n",
        "\n",
        "        theta = (speed*pos) # [t,1]*[dim//2]=[t,d//2]\n",
        "        self.theta = theta\n",
        "        cos, sin = torch.cos(theta), torch.sin(theta)\n",
        "        self.affine = torch.stack([cos, -sin, sin, cos], dim=-1).unflatten(-1,(2,2)).to(device) # [t,d//2,4]-> [1,1,t,d//2,2,2]\n",
        "\n",
        "    def forward(self, x): # [b,h,t,d]\n",
        "        return (self.affine @ x.unflatten(-1, (-1,2)).unsqueeze(-1)).flatten(-3) # [1,1,t,d//2,2,2] @ [b,h,t,d//2,2,1] = [b,h,t,d]\n",
        "\n",
        "dim=64\n",
        "n_heads=4\n",
        "seq_len=64\n",
        "rope = RoPE(dim, seq_len, top=torch.pi, base=100)\n",
        "# rope = RoPE(dim, seq_len, min_freq=1, max_freq=200, n_zero_freqs=0)\n",
        "\n",
        "x = torch.rand(2, n_heads, seq_len, dim, device=device)\n",
        "out = rope(x)\n",
        "print(out.shape)\n",
        "\n",
        "theta = rope.theta # [t,d//2]\n",
        "sim = torch.cos(theta-theta[0].unsqueeze(0)).T\n",
        "# print(sim.shape)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    print(npimg.shape)\n",
        "    plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "import torchvision\n",
        "imshow(torchvision.utils.make_grid(sim, nrow=dim//2))\n",
        "\n",
        "\n",
        "\n",
        "class RotEmb(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, top=torch.pi, base=1000):\n",
        "        super().__init__()\n",
        "        self.theta = top / (base ** (torch.arange(0, dim, step=2, device=device) / dim))\n",
        "        # self.theta = top / (base ** torch.linspace(0, 1, dim//2, device=device))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (pos.unsqueeze(-1) * self.theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [seq_len, dim]\n",
        "\n",
        "# emb = RotEmb(dim, top=torch.pi, base=1000)\n",
        "\n",
        "# theta = emb.theta\n",
        "# print(theta)\n",
        "# pos = torch.arange(0,200,1)\n",
        "# angles = (pos.unsqueeze(-1) * theta).T # [b,t]\n",
        "# sim = torch.cos(angles-angles[:,0].unsqueeze(-1))\n",
        "# print(sim.shape)\n",
        "\n",
        "# imshow(torchvision.utils.make_grid(sim, nrow=dim//2))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "f6T4F651kmGh"
      },
      "outputs": [],
      "source": [
        "# @title AttentionBlock\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class SelfAttn(nn.Module):\n",
        "    def __init__(self, dim, n_heads):\n",
        "        super().__init__()\n",
        "        self.dim, self.n_heads = dim, n_heads\n",
        "        d_head = dim//n_heads\n",
        "        self.rope = RoPE(d_head, seq_len=64, top=torch.pi, base=1000)\n",
        "        # self.rope = RoPE(d_head, seq_len=512, base=1000)\n",
        "        # self.rope = RoPE2D(d_head, h=64, w=64, base=100)\n",
        "        # self.rope[0] = 1 # id for smry h\n",
        "        self.qkv = nn.Linear(dim, dim*3, bias=False)\n",
        "        # self.lin = nn.Linear(dim, dim)\n",
        "        self.lin = zero_module(nn.Linear(dim, dim))\n",
        "\n",
        "        self.scale = d_head**-.5\n",
        "\n",
        "    # def forward(self, x): # [b,t,d]\n",
        "    def forward(self, x, pos=None): # [b,t,d]\n",
        "        q,k,v = self.qkv(x).unflatten(-1, (self.n_heads,-1)).transpose(1,2).chunk(3, dim=-1) # [b, t, n_heads, d_head] -> [b, n_heads, t, d_head]\n",
        "        # q, k = self.rope(q), self.rope(k)\n",
        "        if pos != None:\n",
        "            # rope = self.rope[pos]\n",
        "            # q, k = q*rope, k*rope\n",
        "            q, k = self.rope(q, pos), self.rope(k, pos)\n",
        "\n",
        "        # x = F.scaled_dot_product_attention(q,k,v, attn_mask=None) # https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html\n",
        "        q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2)\n",
        "        context = k.transpose(-2,-1) @ v # [batch, n_heads, d_head, d_head]\n",
        "        x = q @ context # [b, n_heads, t, d_head]\n",
        "        # # print('SelfAttn', x.shape)\n",
        "        x = x.transpose(1,2).flatten(2)\n",
        "        return self.lin(x)\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, mult=4, drop=0.):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        # self.norm = nn.RMSNorm(d_model, elementwise_affine=False) # LayerNorm RMSNorm\n",
        "        self.norm1, self.norm2 = nn.LayerNorm(d_model), nn.LayerNorm(d_model)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "        self.attn = SelfAttn(d_model, n_heads)\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        ff_dim=d_model*mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), act, nn.Dropout(drop), nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "            nn.RMSNorm(ff_dim), act, nn.Dropout(drop), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(drop), nn.Linear(d_model, ff_dim), act,\n",
        "            # nn.RMSNorm(ff_dim), nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "\n",
        "    # def forward(self, x): # [b,t,d]\n",
        "    def forward(self, x, pos=None): # [b,t,d]\n",
        "        # print('attnblk fwd',x.shape)\n",
        "        # x = x + self.drop(self.attn(self.norm1(x)))\n",
        "        x = x + self.drop(self.attn(self.norm1(x), pos))\n",
        "        x = x + self.ff(x)\n",
        "        # x = x + self.drop(self.ff(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "# class ViT(nn.Module):\n",
        "#     # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "#     def __init__(self, patch_size, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "#         super().__init__()\n",
        "#         patch_size=2\n",
        "#         self.embed = nn.Sequential(\n",
        "#             # nn.Conv2d(in_dim, d_model, patch_size, patch_size), # like patch\n",
        "#             # nn.Conv2d(in_dim, d_model, 7, 1, 7//2, bias=False)\n",
        "#             # nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.MaxPool2d(3,2,1)\n",
        "#             nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.BatchNorm2d(d_model), nn.ReLU(),\n",
        "#             nn.Conv2d(d_model, d_model, 3, 2, 3//2, bias=False)\n",
        "#             )\n",
        "#         # self.embed.requires_grad=False\n",
        "#         # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "#         self.pos_emb = nn.Parameter(torch.randn(1, 32*32, d_model)*.02)\n",
        "#         # self.pos_emb = nn.Parameter(RoPE2D(dim=d_model, h=8, w=8, base=1000), requires_grad=False)\n",
        "\n",
        "#         # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=(32//patch_size)**2, base=10000), requires_grad=False)\n",
        "#         self.transformer = nn.Sequential(*[AttentionBlock(d_model, d_model, n_heads) for _ in range(nlayers)])\n",
        "#         self.transformer = Seq(*[AttentionBlock(d_model, d_model, n_heads) for _ in range(nlayers)])\n",
        "#         self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "#         self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "#     def forward(self, x, cxt_inds=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "#         x = self.embed(x).flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c] # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "#         # x = self.pos_enc(x)\n",
        "#         # print(\"TransformerModel\",x.shape, self.pos_emb.shape)\n",
        "#         # x = x + self.pos_emb[:,:x.shape[1]]\n",
        "#         # if cxt_inds != None: x = x[torch.arange(x.shape[0]).unsqueeze(-1), cxt_inds] # [batch, num_context_toks, d_model]\n",
        "#         # x = self.transformer(x)\n",
        "\n",
        "#         x = self.transformer(x, cxt_inds)\n",
        "#         out = self.norm(x)\n",
        "#         if self.lin: out = self.lin(out)\n",
        "#         return out\n",
        "\n",
        "\n",
        "# d_model = 64\n",
        "# in_dim = 3\n",
        "# patch_size = 2\n",
        "# # model = ViT(patch_size, in_dim, d_model, n_heads=4, nlayers=4, drop=0.).to(device)\n",
        "# model = ViT(patch_size, in_dim, d_model, n_heads=4, nlayers=1, drop=0.).to(device)\n",
        "# print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "# x = torch.rand((5, in_dim, 32, 32), device=device)\n",
        "# out = model(x)\n",
        "# print(out.shape)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gdtmiB_Zzn5u"
      },
      "outputs": [],
      "source": [
        "# @title ssd me\n",
        "# https://goombalab.github.io/blog/2024/mamba2-part4-systems/\n",
        "# https://tridao.me/blog/2024/mamba2-part3-algorithm/\n",
        "# # https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/triton/ssd_combined.py\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def segsum(x): # [...,c] # Naive segment sum calculation. exp(segsum(A)) produces a 1-SS matrix, which is equivalent to a scalar SSM.\n",
        "    T = x.size(-1)\n",
        "    x_cumsum = torch.cumsum(x, dim=-1)\n",
        "    x_segsum = x_cumsum.unsqueeze(-1) - x_cumsum.unsqueeze(-2) # [...,c,c] # vert-hori\n",
        "    mask = torch.triu(torch.ones(T, T, device=x.device, dtype=bool), diagonal=1)\n",
        "    return x_segsum.masked_fill(mask, -torch.inf) # [...,c,c]\n",
        "\n",
        "# def ssd(X, A, B, C, h0=None, b_ind=None, chunk=64): # X:[b,h,t,d], A:[b,h,t], B/C:[b,h,t,s], h0:[b,h,d,s], b_ind:[b]\n",
        "def ssd(X, A, B, C, h0=None, msk=None, b_ind=None, chunk=64): # X:[b,h,t,d], A:[b,h,t], B/C:[b,h,t,s], h0:[b,h,d,s], msk:[b,t], b_ind:[b]\n",
        "    # print('ssd', X.dtype, A.dtype, B.dtype)\n",
        "    # assert X.dtype == A.dtype == B.dtype == C.dtype\n",
        "    assert X.shape[:-1] == A.shape == B.shape[:-1]\n",
        "    assert h0==None or b_ind==None\n",
        "    # print('ssd', X.shape, A.shape, B.shape)\n",
        "    if b_ind!=None: A[:,:,b_ind[1:-1]] = 0 # at boundaries, A=0\n",
        "    if X.shape[2] % chunk != 0: X, A, B, C = [x.unsqueeze(2) for x in (X, A, B, C)]\n",
        "    else: X, A, B, C = [x.unflatten(2, (-1,chunk)) for x in (X, A, B, C)] # [b,h,t/c,c(,d/s)]\n",
        "\n",
        "    # 1. Compute the output for each intra-chunk (diagonal blocks)\n",
        "    L = torch.exp(segsum(A)) # [b,h,t/c,c,c]\n",
        "    # if msk!=None: L[msk] = 0 # L = L.masked_fill(msk, 0)\n",
        "    if msk!=None: # this only saves computation from ssd. full info leakage, full compute in in_proj\n",
        "        b,h,l,c,c = L.shape\n",
        "        assert msk.shape==(b,l*c)\n",
        "        msk = msk.reshape(b,1,l,c)\n",
        "        msk = msk.unsqueeeze(-2) | msk.unsqueeze(-1) # [b,1,t/c,c,c]\n",
        "        L[msk] = 0 # L = L.masked_fill(msk, 0)\n",
        "    Y_diag  = torch.einsum(\"...cs,...ks,...ck,...kd->...cd\", C, B, L, X) # bhlcs,bhlks,bhlck,bhlkd->bhlcd # full CA...ABX? for chunks\n",
        "\n",
        "    # 2. Compute the state for each intra-chunk # (right term of low-rank factorization of off-diagonal blocks; B terms)\n",
        "    A_cumsum = torch.cumsum(A, dim=-1) # [b,h,t/c,c]\n",
        "    decay_states = torch.exp((A_cumsum[...,-1:] - A_cumsum)) # [b,h,t/c,c] # Ai+1...T\n",
        "    states = torch.einsum(\"...cs,...c,...cd->...ds\", B, decay_states, X) # bhlcs,bhlc,bhlcd->bhlds # BiXiAi+1...T\n",
        "\n",
        "    # 3. Compute the inter-chunk SSM recurrence; produces correct SSM states at chunk boundaries # (middle term of factorization of off-diag blocks; A terms)\n",
        "    if h0==None: h0 = torch.zeros_like(states[:,:,0], device=states.device) # [b,h,d,s]\n",
        "    states = torch.cat([h0.unsqueeze(2), states], dim=2) # [b,1+t/c,h,d,s] # h0,hc,h2c,...,ht\n",
        "    decay_chunk = torch.exp(segsum(F.pad(A_cumsum[...,-1], (1,0)))) # [b,h,1+t/c]-> # [b,h,1+t/c,1+t/c] # 1,A1...1t/c,A1t/c+1...2t/c,...,A(c-1)t/c...At\n",
        "    new_states = torch.einsum(\"...tl,...lds->...tds\", decay_chunk, states) # bhtl,bhlds->bhtds # h0, BiXi/A1...i-1,\n",
        "\n",
        "    # 4. Compute state -> output conversion per chunk # (left term of low-rank factorization of off-diagonal blocks; C terms)\n",
        "    Y_off = torch.einsum('...cs,...ds,...c->...cd', C, new_states[:,:,:-1], torch.exp(A_cumsum)) # bhlcs,bhlds,bhlc->bhlcd # offset for each chunk # C1h0A1, Ci BiXi/A1...i-1,\n",
        "    Y = (Y_diag+Y_off).flatten(2,3)\n",
        "    return Y, new_states[:,:,-1] # [b,t,h,d], [b,h,d,s]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9ri_f5IGL5T",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title euler zoh bilinear diagonal A\n",
        "import torch\n",
        "\n",
        "# Euler: Abar = 1 + âˆ†A ; Bbar = âˆ†B\n",
        "def euler(A, B, dt): # bts, btsd, bt / bths, bthsd, bt\n",
        "    # return 1+dt.unsqueeze(-1)*A, dt[...,None,None]*B\n",
        "    return 1+torch.einsum('bt,bt...s->bt...s', dt, A), torch.einsum('bt,bt...sd->bt...sd', dt, B)\n",
        "\n",
        "# ZOH: Abar = exp(âˆ†A) ; Bbar = (exp(âˆ†A)-I)/A Â· B ~ (1+A/2)B # For numerical stability as A->0 # (e^x -1)/x ~ 1+(x/2) # from taylor expansion, e^x ~ 1 + x + x^2/2 + x^3/6 + x^4/24 + ...\n",
        "def zoh(A, B, dt): # btd, btds, bt\n",
        "    # A_bar = torch.exp(dt.unsqueeze(-1)*A)\n",
        "    A_bar = torch.exp(torch.einsum('bt,bt...s->bt...s', dt, A))\n",
        "    return A_bar, ((A_bar-1)/A).unsqueeze(-1) * B\n",
        "    # return torch.exp(dt.unsqueeze(-1)*A), (1+A/2).unsqueeze(-1) * B # For numerical stability\n",
        "\n",
        "# Bilinear: Abar = (1+âˆ†A/2) / (1-âˆ†A/2) ; Bbar = âˆ†B/(1-âˆ†A/2)\n",
        "def bilinear(A, B, dt): # btd, btds, bt\n",
        "    # dA_2 = dt.unsqueeze(-1)*A/2\n",
        "    dA_2 = torch.einsum('bt,bt...s->bt...s', dt, A/2)\n",
        "    # return (1+dA_2)/(1-dA_2), (dt.unsqueeze(-1)/(1-dA_2)).unsqueeze(-1) * B\n",
        "    return (1+dA_2)/(1-dA_2), torch.einsum('bt,bt...s->bt...s', dt, 1-dA_2).unsqueeze(-1) * B\n",
        "\n",
        "# b,t,d,s = 2,5,8,4\n",
        "# h=3\n",
        "# A = -torch.randn(b,t,d) # Negative for stability?\n",
        "# B = torch.randn(b,t,d,s)\n",
        "# dt = torch.rand(b,t)*.1\n",
        "# # A_bar, B_bar = euler(A, B, dt)\n",
        "# # A_bar, B_bar = zoh(A, B, dt)\n",
        "# A_bar, B_bar = bilinear(A, B, dt)\n",
        "# print(A_bar.shape, B_bar.shape)\n",
        "\n",
        "# A = -torch.randn(b,t,h,d) # Negative for stability?\n",
        "# B = torch.randn(b,t,h,d,s)\n",
        "\n",
        "# # A_bar, B_bar = euler(A, B, dt)\n",
        "# # print(A_bar.shape, B_bar.shape)\n",
        "# # A_bar, B_bar = zoh(A, B, dt)\n",
        "# # print(A_bar.shape, B_bar.shape) # [2, 5, 8, 8], [2, 5, 8, 4]\n",
        "# A_bar, B_bar = bilinear(A, B, dt)\n",
        "# print(A_bar.shape, B_bar.shape) # [2, 5, 8, 8], [2, 5, 8, 4]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pQfM2fYvcTh6"
      },
      "outputs": [],
      "source": [
        "# @title masks\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# def multiblock(seq, min_s, max_s, M=1):\n",
        "#     mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "#     mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "#     mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "#     indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "#     target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "#     return target_mask\n",
        "\n",
        "\n",
        "# def multiblock2d(hw=(8,8), scale=(.15,.2), aspect_ratio=(.75,1.5), M=1):\n",
        "#     mask_aspect = torch.rand(1) * (aspect_ratio[1] - aspect_ratio[0]) + aspect_ratio[0] # in (min_s, max_s) # all blocks same size\n",
        "#     mask_scale = torch.rand(1) * (scale[1] - scale[0]) + scale[0] # in (min_s, max_s) # all blocks same size\n",
        "#     h = (mask_scale/mask_aspect)**.5# h*(h*aspect) = scale\n",
        "#     w = h * mask_aspect\n",
        "#     h_pos, w_pos = torch.rand(M)*(1-w), torch.rand(M)*(1-h) # in (0, 1 - mask_len)\n",
        "#     h_len, h_pos = (h*hw[0]).int(), h_pos*hw[0]\n",
        "#     w_len, w_pos = (w*hw[1]).int(), w_pos*hw[1]\n",
        "#     h_ind, w_ind = torch.arange(hw[0]).unsqueeze(0), torch.arange(hw[1]).unsqueeze(0) # [1, seq]\n",
        "#     h_mask = (h_ind>=h_pos.unsqueeze(-1)) & (h_ind<(h_pos+h_len).unsqueeze(-1)) # [M, seq]\n",
        "#     w_mask = (w_ind>=w_pos.unsqueeze(-1)) & (w_ind<(w_pos+w_len).unsqueeze(-1)) # [M, seq]\n",
        "#     target_mask = h_mask.unsqueeze(-1) & w_mask.unsqueeze(-2) # [M, seq, seq]\n",
        "#     return target_mask\n",
        "\n",
        "# # https://arxiv.org/pdf/2210.07224\n",
        "# def randpatch(seq, mask_size=8, gamma=0.9): # num patches of seq, mask patch size, masking ratio\n",
        "#     # mask = torch.rand(seq//mask_size)<gamma\n",
        "#     length = seq//mask_size\n",
        "#     g = torch.normal(gamma, std=.1, size=(1,)).clamp(.5,.9)\n",
        "#     # g = gamma\n",
        "#     idx = torch.randperm(length)[:int(length*g)]\n",
        "#     mask = torch.zeros(length, dtype=bool)\n",
        "#     mask[idx] = True\n",
        "#     mask = mask.repeat_interleave(mask_size, dim=-1)\n",
        "#     return mask # [seq] , True -> mask\n",
        "\n",
        "\n",
        "# import torch\n",
        "# def apply_masks(x, mask): # [b,t,d], [mask_size] # https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "#     mask_keep = mask.unsqueeze(-1).repeat(x.size(0), 1, x.size(-1)) # [batch,T,dim]\n",
        "#     return torch.gather(x, dim=1, index=mask_keep) # [batch,mask_size,dim]\n",
        "\n",
        "\n",
        "# @title ijepa multiblock next\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "import math\n",
        "from multiprocessing import Value\n",
        "import torch\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(self, hw=(224, 224), enc_mask_scale=(.85,1), pred_mask_scale=(.15,.2), aspect_ratio=(.75,1.25),\n",
        "        nenc=1, npred=2, min_keep=4, allow_overlap=False):\n",
        "        super().__init__()\n",
        "        self.height, self.width = hw\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.aspect_ratio = aspect_ratio\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "\n",
        "    def _sample_block_size(self, scale, aspect_ratio_scale):\n",
        "        _rand = torch.rand(1).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.height * self.width * mask_scale) # num patches to keep\n",
        "        # -- Sample block aspect-ratio\n",
        "        min_ar, max_ar = aspect_ratio_scale\n",
        "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
        "        while h >= self.height: h -= 1 # crop mask to be smaller than img\n",
        "        while w >= self.width: w -= 1\n",
        "        return (h, w)\n",
        "\n",
        "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
        "        h, w = b_size\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            top = torch.randint(0, self.height - h, (1,))\n",
        "            left = torch.randint(0, self.width - w, (1,))\n",
        "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
        "            mask[top:top+h, left:left+w] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
        "        mask_complement[top:top+h, left:left+w] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, B):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        p_size = self._sample_block_size(scale=self.pred_mask_scale, aspect_ratio_scale=self.aspect_ratio)\n",
        "        e_size = self._sample_block_size(scale=self.enc_mask_scale, aspect_ratio_scale=(1., 1.))\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.height * self.width\n",
        "        min_keep_enc = self.height * self.width\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                print(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "        return collated_masks_enc, collated_masks_pred\n",
        "\n",
        "mask_collator = MaskCollator(hw=(32,32), enc_mask_scale=(.85, 1.), pred_mask_scale=(.15, .2), aspect_ratio=(.75, 1.5),\n",
        "        nenc=1, npred=4, min_keep=4,\n",
        "        # allow_overlap=True)\n",
        "        allow_overlap=False)\n",
        "\n",
        "b=16\n",
        "collated_masks_enc, collated_masks_pred = mask_collator(b)\n",
        "ctx_index, trg_index = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "# imshow(mask)\n",
        "\n",
        "mask = torch.zeros(b ,32*32)\n",
        "mask[torch.arange(b).unsqueeze(-1), trg_index] = 1\n",
        "mask[torch.arange(b).unsqueeze(-1), ctx_index] = .5\n",
        "mask = mask.reshape(b,1,32,32)\n",
        "print((mask==1).flatten(1).sum(-1))\n",
        "print((mask==.5).flatten(1).sum(-1))\n",
        "import torchvision\n",
        "imshow(torchvision.utils.make_grid(mask, nrow=8))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Nd1GqX3PYpIk",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# @title Random Fourier Features noise\n",
        "import math\n",
        "import torch\n",
        "\n",
        "# Random Fourier Features: âˆ‘_k] a_k * cos(Ï‰_k â‹… x + ðœ™_k)\n",
        "def rff_noise(x, out_dim, n_freqs=64, scale=1): # [...,n_Dim]\n",
        "    space, in_dim, device = x.shape[:-1], x.shape[-1], x.device\n",
        "    x = x.flatten(0,-2) # [N,in]\n",
        "    w = torch.randn(out_dim, n_freqs, in_dim, device=device) * scale\n",
        "    phi = torch.empty(out_dim, n_freqs, device=device).uniform_(0,2*math.pi)\n",
        "    a = torch.randn(out_dim, n_freqs, device=device) / math.sqrt(n_freqs)\n",
        "    y = torch.cos(torch.einsum(\"ofd,...d->...of\", w, x) + phi) # [N,out,freq]\n",
        "    return torch.einsum(\"of,...of->...o\", a, y).unflatten(0,space) # [...,out]\n",
        "\n",
        "def rffmask2d(bhw, ctx_scale=(.85,1.), trg_scale=(.6,.8), chaos=[1,.5]): # ctx_scale > trg_scale\n",
        "# def rffmask2d(bhw, ctx_scale=(.05,.5), trg_scale=(.2,.8), chaos=[1,.5],): # ctx_scale > trg_scale\n",
        "    b,h,w = bhw\n",
        "    ctx_mask_scale = torch.rand(1) * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    trg_mask_scale = torch.rand(1) * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "    seq = h*w\n",
        "    ctx_len, trg_len = int(seq*ctx_mask_scale), int(seq*trg_mask_scale)\n",
        "    # print(ctx_len, trg_len)\n",
        "    # ctx_len = ctx_len - trg_len\n",
        "    ctx_len = min(ctx_len, seq-trg_len)\n",
        "\n",
        "    ix, iy = torch.linspace(0, chaos[0], h), torch.linspace(0, chaos[0], w)\n",
        "    x = torch.stack(torch.meshgrid(ix, iy, indexing=\"ij\"), dim=-1) # [t,h,w,b] / [h,w,b]\n",
        "    noise = rff_noise(x, b, n_freqs=16).flatten(0,-2).T # [h,w,b]->[h*w,b]->[b,h*w]\n",
        "    _, trg_ind = torch.topk(noise, trg_len, dim=1, sorted=False)\n",
        "\n",
        "    ix, iy = torch.linspace(0, chaos[1], h), torch.linspace(0, chaos[1], w)\n",
        "    x = torch.stack(torch.meshgrid(ix, iy, indexing=\"ij\"), dim=-1) # [t,h,w,b] / [h,w,b]\n",
        "    noise = rff_noise(x, b, n_freqs=16).flatten(0,-2).T\n",
        "    noise.scatter_(1, trg_ind, -10).flatten()\n",
        "    _, ctx_ind = torch.topk(noise, ctx_len, dim=1, sorted=False)\n",
        "    return ctx_ind, trg_ind\n",
        "\n",
        "# b=16\n",
        "# # hw=(8,8)\n",
        "# hw=(32,32)\n",
        "# # cxt_inds, trg_inds = rffmask2d((b,)+hw, ctx_scale=(.85,1.), trg_scale=(.2,.8), chaos=[3,1])\n",
        "# # cxt_inds, trg_inds = rffmask2d((b,)+hw, ctx_scale=(.85,1.), trg_scale=(.2,.8), chaos=[1,.5])\n",
        "# cxt_inds, trg_inds = rffmask2d((b,)+hw, ctx_scale=(.05,.5), trg_scale=(.2,.8), chaos=[3,1])\n",
        "# print(cxt_inds, trg_inds)\n",
        "\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# def imshow(img):\n",
        "#     npimg = img.numpy()\n",
        "#     plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "#     plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "#     plt.show()\n",
        "# # imshow(mask)\n",
        "\n",
        "# mask = torch.zeros(b ,math.prod(hw))\n",
        "# mask[torch.arange(b).unsqueeze(-1), trg_inds] = 1\n",
        "# mask[torch.arange(b).unsqueeze(-1), cxt_inds] = .5\n",
        "# # print((mask==1).sum(1))\n",
        "# # print((mask==.5).sum(1))\n",
        "# mask = mask.reshape(b,1,*hw)\n",
        "# import torchvision\n",
        "# imshow(torchvision.utils.make_grid(mask, nrow=8))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6eeu0m-bUSdh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0343baf7-f453-4039-801c-ac34eba6918c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15296\n",
            "torch.Size([5, 256, 32])\n",
            "tensor([[0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.]], device='cuda:0', grad_fn=<SliceBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# @title hydra me\n",
        "# https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba2_simple.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "# @torch.compile()\n",
        "class Hydra(nn.Module):\n",
        "    def __init__(self, d_model, expand=3, n_heads=8, n_groups=8, d_state=8, d_conv=7):\n",
        "        super().__init__()\n",
        "        n_groups = min(n_heads, n_groups)\n",
        "        assert n_heads % n_groups == 0, \"nheads must be divisible by ngroups\"\n",
        "        self.d_model, self.n_groups, self.d_state, self.d_conv = d_model, n_groups, d_state, d_conv\n",
        "        self.d_inner = expand * self.d_model\n",
        "        self.n_heads, self.d_head = n_heads, self.d_inner//n_heads\n",
        "\n",
        "        # self.in_proj = nn.Linear(self.d_model, 2* self.d_inner + 2* self.n_groups*self.d_state + self.n_heads, bias=False) # z,x,B,C,A\n",
        "        self.in_proj = nn.Linear(self.d_model, 2* self.d_inner + 2* self.n_groups*self.d_state + 2* self.n_heads, bias=True) # z,x,B,C,A\n",
        "        # with torch.no_grad(): self.in_proj.weight[self.d_inner:] = .1*self.in_proj.weight[self.d_inner:]\n",
        "        # conv_dim = self.d_inner + 2*self.n_groups*self.d_state # for x,B,C\n",
        "        # self.conv1d = nn.Conv1d(conv_dim, conv_dim, kernel_size=d_conv, groups=conv_dim, padding=d_conv//2, bias=True)\n",
        "        self.conv1d = nn.Conv1d(d_model, d_model, kernel_size=d_conv, groups=d_model, padding=d_conv//2, bias=True)\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "        self.A_log = nn.Parameter(torch.log(torch.rand(self.n_heads)))\n",
        "        # self.h0 = nn.Parameter(torch.zeros(self.n_heads, self.d_head, self.d_state))\n",
        "        # self.D = nn.Parameter(torch.ones(self.n_heads)) # from mamba\n",
        "        self.D = nn.Linear(self.d_inner, self.n_heads) # og\n",
        "        self.norm = nn.RMSNorm(self.d_inner)\n",
        "        self.out = zero_module(nn.Linear(self.d_inner, self.d_model, bias=False))\n",
        "\n",
        "    # def forward(self, u, h=None, mask=None, b_ind=None): # [b,t,d], ([b,k-1,xbc], [b,h,d,s]), [b+1]\n",
        "    # def forward(self, u, h=None, ctx_idx= None, mask=None, b_ind=None): # [b,t,d], ([b,k-1,xbc], [b,h,d,s]), [b+1]\n",
        "    def forward(self, u, h=None, step= None, mask=None, b_ind=None): # [b,t,d], ([b,k-1,xbc], [b,h,d,s]), [b+1]\n",
        "        b, t = u.shape[:2]\n",
        "        if h==None: h_conv, h_ssm = None, None\n",
        "        else: h_conv, h_ssm = h\n",
        "        # ctx_idx = torch.cat([torch.zeros(b,1, device=u.device), ctx_idx], dim=-1) # [b,1+t]\n",
        "        # step = ctx_idx[:,1:] - ctx_idx[:,:-1] # [b,t]\n",
        "        # step = torch.ones(b,t, device=u.device) #\n",
        "        # step = torch.cat((step, torch.flip(step,(1,))), dim=0) # [b,t]->[2b,t]\n",
        "        if step==None: step = torch.ones(2*b,t, device=u.device) # [b,t]\n",
        "        else: step = torch.cat((step[:,:-1], torch.flip(step[:,1:],(1,))), dim=0).to(device) # [b,t+1]->[2b,t]\n",
        "\n",
        "        # u = self.act(self.conv1d(u.transpose(-2,-1)).transpose(-2,-1)) # [b,t,inr+2gs]\n",
        "        # z, xBC, A = self.in_proj(u).split([self.d_inner, self.d_inner + 2*self.n_groups*self.d_state, self.n_heads], dim=-1)\n",
        "        # z, xBC, A, dt = self.in_proj(u).split([self.d_inner, self.d_inner + 2*self.n_groups*self.d_state, self.n_heads, self.n_heads], dim=-1)\n",
        "        z, to_flip = self.in_proj(u).split([self.d_inner, self.d_inner + 2*self.n_groups*self.d_state + 2*self.n_heads], dim=-1)\n",
        "        # z, x, B, C, A, dt = self.in_proj(u).split([self.d_inner, self.d_inner, self.n_groups*self.d_state, self.n_groups*self.d_state, self.n_heads, self.n_heads], dim=-1)\n",
        "\n",
        "        to_flip = torch.cat((to_flip, torch.flip(to_flip,(1,))), dim=0) # [b,t,xbc]->[2b,t,xbc]\n",
        "        # x, B, C = to_flip.split([self.d_inner, self.n_groups*self.d_state, self.n_groups*self.d_state], dim=-1)\n",
        "        x, B, C, A, dt = to_flip.split([self.d_inner, self.n_groups*self.d_state, self.n_groups*self.d_state, self.n_heads, self.n_heads], dim=-1)\n",
        "\n",
        "        # dt = torch.cat((dt, torch.flip(dt,(1,))), dim=0) # [b,t,h]->[2b,t,h]\n",
        "        dt = F.softplus(dt)\n",
        "        # A = torch.cat((A, torch.flip(A,(1,))), dim=0) # [b,t,h]->[2b,t,h]\n",
        "        # A = -torch.exp(self.A_log) # mamba2\n",
        "        # A = -torch.exp(A) #\n",
        "        # A = -F.softplus(-A) # log(1+e^x)\n",
        "        A = -F.sigmoid(-A) # 1/(1+e^-x)\n",
        "        # print('mamba fwd0', A[-1,-3:,-5:])\n",
        "\n",
        "        # print('mamba fwd', xBC.shape, h_conv.shape)\n",
        "        # xBC = self.act(self.conv1d(xBC.transpose(-2,-1)).transpose(-2,-1)) # [b,t,inr+2gs]\n",
        "\n",
        "\n",
        "        x_og = x[:b] # [b,t,inr]\n",
        "        x, B, C = x.unflatten(-1, (self.n_heads, self.d_head)), B.unflatten(-1, (self.n_groups, self.d_state)), C.unflatten(-1, (self.n_groups, self.d_state)) # x:[2b,t,h,d], B/C:[2b,t,g,s]\n",
        "        # y_diag = (x[:b] * self.D.unsqueeze(-1)).flatten(2) # bthd*h1\n",
        "\n",
        "        h_g = self.n_heads//self.n_groups\n",
        "        if h_g>1: B, C = B.repeat_interleave(h_g, dim=-2), C.repeat_interleave(h_g, dim=-2) # [b,t,g,s]->[b,t,h,s]\n",
        "\n",
        "        # # A, x = (A*step[...,*[None]*(A.ndim-2)]).exp(), x*step[...,None,None] # bthd*bt11=bthd # sdd discretization\n",
        "        A, B = zoh(A, B, step) # euler zoh bilinear # bth,bths\n",
        "        # print('step', step.max())\n",
        "        # print('step', step)\n",
        "        # # print('mamba fwd', A.shape, B.shape, dt.shape)\n",
        "        # # A, B = zoh(A, B*dt.unsqueeze(-1), step) # euler zoh bilinear # bth,bths\n",
        "        # # A, B = zoh(A*dt, B*dt.unsqueeze(-1), step) # euler zoh bilinear # bth,bths\n",
        "        # # print('mamba fwd1', A[-1,-3:,-5:])\n",
        "        # dt = dt * step.unsqueeze(-1) # [2b,t,h]*[2b,t,1]\n",
        "\n",
        "# x:bthd, dt:bthd, A.exp:hds, B/C:bts, D:hd # mamba1\n",
        "# x:bhtd, dt:bht, A:h, B/C:bgts-repeat>bhts, D:h # mamba2\n",
        "# mamba(ssd):\n",
        "# h = Ah + Bx : A*h + x@B = 1/ds*ds + d1@1s = ds\n",
        "# y = Ch + Dx : h@C + D*x = ds@s1 + 1/d1*d1 = d1\n",
        "\n",
        "        # print('mamba fwd', x.shape, A.shape, B.shape)\n",
        "        x, A, B, C = [a.transpose(1,2) for a in (x, A, B, C)] # X:[b,h,t,d], A:[b,h,t], B/C:[b,h,t,s]\n",
        "        dt = dt.transpose(1,2) # [b,h,t]\n",
        "# x:bhtd, A:bht, B/C:bhts, 10.5s\n",
        "        y, h_ssm = ssd(x, A.log(), B, C, h_ssm, msk=mask, b_ind=b_ind, chunk=64) #\n",
        "        # y, h_ssm = ssd(x*dt.unsqueeze(-1), A*dt, B, C, h_ssm, msk=mask, b_ind=b_ind, chunk=64) # 256\n",
        "\n",
        "        y = y.transpose(1,2).flatten(2) # [b,t,d]/[1,b*t,d]\n",
        "\n",
        "        y = torch.roll(y, shifts=1, dims=1) # 123...l -> l12...l-1\n",
        "        y[:,0] = 0 # 012...l-1\n",
        "        y = y[:b] + torch.flip(y[b:], (1,)) + x_og * self.D(x_og).repeat(1,1,self.d_head) # [b,t,inr]\n",
        "        # y = y[:b] + torch.flip(y[b:], (1,)) +  y_diag # [b,t,inr]\n",
        "        y = self.norm(y * self.act(z)) # [b,t,inr] # norm(x)*silu(z) if norm_before_gate, else norm(x*silu(z)) # https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/triton/layernorm_gated.py#L18\n",
        "        out = self.out(y)\n",
        "        return out#, (h_conv, h_ssm) # [b,t,in], ([b,k-1,xbc], [b,h,d,s])\n",
        "\n",
        "\n",
        "b,t,d_model=5,256,32\n",
        "# b,t,d_model=5,7,32\n",
        "u = torch.randn(b,t,d_model, device=device)\n",
        "model = Hydra(d_model).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) #\n",
        "# out, h = model(u)\n",
        "out = model(u)\n",
        "# h0 = torch.randn(b, model.n_heads, model.d_head, model.d_state)\n",
        "# print(out.shape)\n",
        "# print(out.shape, h.shape)\n",
        "# print(out[0,-3:,:5], h[0,:2,:5,:5])\n",
        "# out, h = model(u, h)\n",
        "\n",
        "print(out.shape)\n",
        "print(out[0,-3:,:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivqYjBIVrB3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd4541fd-669b-4335-f1a8-d12e143432bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "112416\n",
            "torch.Size([5, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# @title ViT\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        self._kwargs = [[name for name, p in inspect.signature(layer.forward).parameters.items()] for layer in self]\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        for layer, _kwargs in zip(self, self._kwargs):\n",
        "            x = layer(x, *args, **{k: v for k, v in kwargs.items() if k in _kwargs})\n",
        "        return x\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, patch_size, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        patch_size=2\n",
        "        self.hw=(8,8) # (8,8) (32,32)\n",
        "        self.seq = self.hw[0]*self.hw[1]\n",
        "\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Conv2d(in_dim, d_model, patch_size, patch_size), # like patch\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 1, 7//2, bias=False)\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.MaxPool2d(3,2,1)\n",
        "            nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.BatchNorm2d(d_model), nn.ReLU(),\n",
        "            nn.Conv2d(d_model, d_model, 3, 2, 3//2, bias=False)\n",
        "            )\n",
        "        # self.embed.requires_grad=False\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, self.seq, d_model)*.02)\n",
        "        # self.pos_emb = nn.Parameter(RoPE2D(dim=d_model, h=8, w=8, base=1000), requires_grad=False)\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=(32//patch_size)**2, base=10000), requires_grad=False)\n",
        "\n",
        "        self.transformer = Seq(*[AttentionBlock(d_model, d_model, n_heads) for _ in range(nlayers)])\n",
        "        # self.transformer = Seq(*[Hydra(d_model, n_heads) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, cxt_inds=None): # [b,ctx,3], [b,ctx] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x).flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c] # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "\n",
        "        # x = self.pos_enc(x)\n",
        "        # print(\"TransformerModel\",x.shape, self.pos_emb.shape)\n",
        "        x = x + self.pos_emb[:,:x.shape[1]]\n",
        "        if cxt_inds != None: x = x[torch.arange(x.shape[0]).unsqueeze(-1), cxt_inds] # [batch, num_context_toks, d_model]\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        # if cxt_inds != None: # for pos\n",
        "        #     # x = self.transformer(x[torch.arange(x.shape[0]).unsqueeze(-1), cxt_inds], cxt_inds) # for pos attn\n",
        "        #     b = x.shape[0]\n",
        "        #     # print('vit fwd', cxt_inds)\n",
        "        #     ctx_idx = torch.cat([torch.full((b,1),-1), cxt_inds, torch.full((b,1),self.seq)], dim=-1) # [b,1+t+1]\n",
        "        #     step = ctx_idx[:,1:] - ctx_idx[:,:-1] # [b,t+1]\n",
        "        #     x = self.transformer(x[torch.arange(x.shape[0]).unsqueeze(-1), cxt_inds], step=step)\n",
        "        # else: x = self.transformer(x)\n",
        "\n",
        "        out = self.norm(x)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out\n",
        "\n",
        "d_model = 64\n",
        "in_dim = 3\n",
        "patch_size = 2\n",
        "# model = ViT(patch_size, in_dim, d_model, n_heads=4, nlayers=4, drop=0.).to(device)\n",
        "model = ViT(patch_size, in_dim, d_model, n_heads=4, nlayers=1, drop=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x = torch.rand((5, in_dim, 32, 32), device=device) # [b,c,h,w]\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-zdjdJixtOu"
      },
      "outputs": [],
      "source": [
        "# @title Transformer Predictor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        self.hw=(8,8) # (8,8) (32,32)\n",
        "        self.seq = self.hw[0]*self.hw[1]\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, self.seq, d_model)*0.02)\n",
        "\n",
        "        # self.transformer = Seq(*[AttentionBlock(d_model, n_heads) for _ in range(nlayers)])\n",
        "        self.transformer = Seq(*[Hydra(d_model, n_heads) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.randn(1,1,d_model)*.02) # randn zeros\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim)# if out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, cxt_inds, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        b, seq, dim = x.shape\n",
        "\n",
        "        # # x = x * self.pos_enc(cxt_inds)\n",
        "        # # pred_tokens = self.cls * self.pos_enc(trg_indices) # [M, num_trg_toks, d_model]\n",
        "        x = x + self.pos_emb[0,cxt_inds]\n",
        "        pred_tokens = self.cls + self.pos_emb[0,trg_indices]\n",
        "        # # print(\"pred fwd\", x.shape, pred_tokens.shape)\n",
        "        x = torch.cat([x, pred_tokens], dim=1) # [batch, seq_len+num_trg_toks, d_model]\n",
        "        out = self.transformer(x)[:,seq:] # [b,trg,d]\n",
        "\n",
        "        # # for ssm\n",
        "        # x = torch.cat([x, self.cls.expand(*trg_indices.shape,-1)], dim=1) # [batch, num_ctx_toks+num_trg_toks, d_model]\n",
        "        # val, idx = torch.sort(torch.cat([cxt_inds, trg_indices], dim=-1)) # [b,ctx+trg]\n",
        "        # x = x[torch.arange(b).unsqueeze(-1), idx]\n",
        "        # indices = torch.cat([torch.full((b,1), -1), val, torch.full((b,1), self.seq)], dim=-1) # [b,1+t+1]\n",
        "        # step = indices[:,1:] - indices[:,:-1] # [b,t+1]\n",
        "        # out = self.transformer(x, step=step)\n",
        "        # back_idx = idx[idx>=cxt_inds.shape[-1]].reshape(b,-1) # [b,trg]\n",
        "        # out = out[torch.arange(b).unsqueeze(-1), back_idx] # [b,trg,d]\n",
        "\n",
        "        # # for pos attn\n",
        "        # x = torch.cat([x, self.cls.expand(*trg_indices.shape,-1)], dim=1) # [batch, num_ctx_toks+num_trg_toks, d_model]\n",
        "        # out = self.transformer(x, torch.cat([cxt_inds, trg_indices], dim=-1))[:,seq:] # [b,trg,d]\n",
        "\n",
        "        out = self.norm(out)\n",
        "        out = self.lin(out)\n",
        "        # print(\"pred fwd\", out.shape)\n",
        "        return out # [seq_len, batch_size, ntoken]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ardu1zJwdHM3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20955d3b-8ea7-4c45-e001-d0e7383d8b4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "189408\n",
            "torch.Size([])\n"
          ]
        }
      ],
      "source": [
        "# @title IJEPA\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "@torch.compile\n",
        "class IJEPA(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=64, out_dim=None, nlayers=1, n_heads=4):\n",
        "        super().__init__()\n",
        "        self.out_dim = out_dim = out_dim or d_model\n",
        "        self.patch_size = 4 # 4\n",
        "        self.student = ViT(self.patch_size, in_dim, d_model, out_dim=out_dim, n_heads=n_heads, nlayers=nlayers, drop=0.)\n",
        "        self.predicter = TransformerPredictor(out_dim, d_model//2, out_dim, n_heads=4, nlayers=1, drop=0.)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "        self.hw=(8,8) # (8,8) (32,32)\n",
        "        # self.mask_collator = MaskCollator(self.hw, enc_mask_scale=(.85, 1.), pred_mask_scale=(.15, .2), aspect_ratio=(.75, 1.5), nenc=1, npred=4, min_keep=4, allow_overlap=False) # og\n",
        "\n",
        "    def loss(self, x): #\n",
        "        b,c,h,w = x.shape\n",
        "        # collated_masks_enc, collated_masks_pred = self.mask_collator(b)\n",
        "        # cxt_inds, trg_inds = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "\n",
        "        # cxt_inds, trg_inds = rffmask2d((1,)+self.hw, ctx_scale=(.85,1.), trg_scale=(.2,.8), chaos=[3,1])\n",
        "        cxt_inds, trg_inds = rffmask2d((b,)+self.hw, ctx_scale=(.05,.5), trg_scale=(.2,.8), chaos=[3,1])\n",
        "        # cxt_inds, trg_inds = rffmask2d((1,)+self.hw, ctx_scale=(.05,.5), trg_scale=(.15,.2), chaos=[3,1])\n",
        "        # cxt_inds, trg_inds = simplexmask2d(hw, ctx_scale=(.85,1), trg_scale=(.7,.8), B=b, chaos=[3,.5])\n",
        "        # cxt_inds, trg_inds = cxt_inds.repeat(b,1), trg_inds.repeat(b,1)\n",
        "        cxt_inds = cxt_inds.sort(-1)[0]\n",
        "\n",
        "        # zero_mask = torch.zeros(b ,*self.hw, device=device).flatten(1)\n",
        "        # zero_mask[torch.arange(b).unsqueeze(-1), cxt_inds] = 1\n",
        "        # x_ = x * F.interpolate(zero_mask.reshape(b,1,*self.hw), size=x.shape[2:], mode='nearest-exact') # zero masked locations\n",
        "        # sx = self.student(x_, cxt_inds=cxt_inds) # [batch, num_context_toks, out_dim]\n",
        "\n",
        "        sx = self.student(x, cxt_inds=cxt_inds) # [batch, num_context_toks, out_dim]\n",
        "        # print('ijepa loss sx',sx.shape)\n",
        "        sy_ = self.predicter(sx, cxt_inds=cxt_inds, trg_indices=trg_inds) # [batch*M, num_trg_toks, out_dim]\n",
        "        sy_ = F.layer_norm(sy_, (sy_.size(-1),))\n",
        "        with torch.no_grad():\n",
        "            sy = self.teacher(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "            # print('ijepa loss sy',sy.shape)\n",
        "            sy = sy[torch.arange(sy.shape[0]).unsqueeze(-1), trg_inds] # [batch, num_context_toks, d_model] # nan bec len(trg_ind)==0 # print('loss sy',torch.isnan(sy).any())\n",
        "            sy = F.layer_norm(sy, (sy.size(-1),))\n",
        "        loss = F.mse_loss(sy, sy_)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x): # [b,t,3]\n",
        "        out = self.student(x).mean(1)\n",
        "        return out\n",
        "\n",
        "# min_s=0.15, max_s, M\n",
        "# trg.15.2M4 C.85 1\n",
        "# dont normalise data\n",
        "# randmsk < simplex 1msk < multiblk\n",
        "# teacher=trans ~? teacher=copy\n",
        "# learn convemb\n",
        "# lr pred,student: 3e-3/1e-2, 1e-3\n",
        "\n",
        "# lr 1e-4<<<1e-3 < 1e-2,1e-3? slower but dun plateau\n",
        "# zeromsk impt, else closs start increasing\n",
        "# l1rge trg helps delay increase?\n",
        "\n",
        "# vit 1lyr 15.4sec 15.6\n",
        "# hiera downsampling attn 20.5\n",
        "\n",
        "# ijepa = IJEPA(in_dim=3, d_model=64, out_dim=64, nlayers=4, n_heads=4).to(device)\n",
        "ijepa = IJEPA(in_dim=3, d_model=64, out_dim=64, nlayers=1, n_heads=8).to(device)\n",
        "# ijepa = IJEPA(in_dim=3, d_model=32, out_dim=32, nlayers=1, n_heads=4).to(device)\n",
        "optim = torch.optim.AdamW(ijepa.parameters(), lr=1e-3)#, weight_decay=0) # 1e-3?\n",
        "# optim = torch.optim.AdamW([{'params': ijepa.student.parameters()},\n",
        "# #     {'params': ijepa.predicter.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # good 3e-3,1e-3 ; default 1e-2, 5e-2\n",
        "#     {'params': ijepa.predicter.parameters(), 'lr': 1e-2}], lr=1e-3, weight_decay=1e-2)\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/configs/in1k_vith14_ep300.yaml\n",
        "# d_model 1024,384\n",
        "# depth 12,6/12\n",
        "# wd 5e-2 - 4e-1\n",
        "# adamw 1e-4 - 1e-3 - 1e-6\n",
        "# ema 0.996-1\n",
        "\n",
        "print(sum(p.numel() for p in ijepa.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in ijepa.parameters())) # 27584\n",
        "# print(sum(p.numel() for p in ijepa.predicter.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# d_model^2 * nlayers\n",
        "\n",
        "x = torch.rand((64,3,32,32), device=device)\n",
        "out = ijepa.loss(x)\n",
        "print(out.shape)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(ijepa.out_dim, 10).to(device)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n",
        "# optim = torch.optim.AdamW([{'params': ijepa.parameters()}, {'params': classifier.parameters(), 'lr': 1e-3}], lr=1e-3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2Nd-sGe6Ku4S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "outputId": "c348ea57-59f3-45a5-b41e-203031d2287f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>â–‡â–ˆâ–…â–„â–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–â–‚â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–„â–„â–„â–ƒâ–ƒâ–ƒ</td></tr><tr><td>correct</td><td>â–…â–„â–â–…â–ˆâ–ƒâ–…â–‡â–ƒâ–†â–ƒâ–„â–†â–ˆâ–…â–ƒâ–„â–†â–ƒâ–‡â–„â–†â–†â–…â–ƒâ–‡â–ƒâ–„â–…â–„â–†â–†â–ˆâ–…â–†â–†â–ˆâ–†â–„â–„</td></tr><tr><td>loss</td><td>â–â–â–â–ƒâ–‚â–ƒâ–‚â–‚â–„â–„â–…â–ƒâ–ƒâ–„â–ƒâ–„â–„â–„â–†â–„â–…â–…â–…â–…â–„â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>2.11142</td></tr><tr><td>correct</td><td>0.25</td></tr><tr><td>loss</td><td>0.44192</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lunar-sun-129</strong> at: <a href='https://wandb.ai/bobdole/ijepa/runs/dx9opkzs' target=\"_blank\">https://wandb.ai/bobdole/ijepa/runs/dx9opkzs</a><br> View project at: <a href='https://wandb.ai/bobdole/ijepa' target=\"_blank\">https://wandb.ai/bobdole/ijepa</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20260204_071954-dx9opkzs/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.24.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20260204_074520-cnzev4z5</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/ijepa/runs/cnzev4z5' target=\"_blank\">different-dream-131</a></strong> to <a href='https://wandb.ai/bobdole/ijepa' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/ijepa' target=\"_blank\">https://wandb.ai/bobdole/ijepa</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/ijepa/runs/cnzev4z5' target=\"_blank\">https://wandb.ai/bobdole/ijepa/runs/cnzev4z5</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "!pip install -q wandb\n",
        "import wandb # https://docs.wandb.ai/quickstart\n",
        "wandb.login(key='487a2109e55dce4e13fc70681781de9f50f27be7')\n",
        "try: run.finish()\n",
        "except NameError: pass\n",
        "run = wandb.init(project=\"ijepa\", config={\"model\": \"res18\",})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "34de8800-fa4b-4460-803c-3cd8dbfb6c70",
        "outputId": "73e099f3-4fd6-49d1-c88f-5fab034f292a",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i, sloss, closs, correct 0 0.6004719734191895 2.491990089416504 0.078125\n",
            "time: 5.014026403427124 5.01402735710144\n",
            "i, sloss, closs, correct 1 0.3642529845237732 2.460205554962158 0.09375\n",
            "time: 2.3198766708374023 3.6683143377304077\n",
            "i, sloss, closs, correct 2 0.17256581783294678 2.3843932151794434 0.046875\n",
            "time: 2.145686388015747 3.1615182558695474\n",
            "i, sloss, closs, correct 3 0.09287141263484955 2.44989013671875 0.078125\n",
            "time: 1.9136383533477783 2.8496703505516052\n",
            "i, sloss, closs, correct 4 0.07864677906036377 2.379855155944824 0.125\n",
            "time: 1.903547763824463 2.6605443000793456\n",
            "i, sloss, closs, correct 5 0.06194238364696503 2.464533805847168 0.0625\n",
            "time: 1.9071075916290283 2.5350519816080728\n",
            "i, sloss, closs, correct 6 0.05828384310007095 2.411741256713867 0.078125\n",
            "time: 1.9091477394104004 2.4457477842058455\n",
            "i, sloss, closs, correct 7 0.05293658748269081 2.3087821006774902 0.0625\n",
            "time: 2.1881418228149414 2.4136056900024414\n",
            "i, sloss, closs, correct 8 0.054366935044527054 2.347996711730957 0.09375\n",
            "time: 2.282512903213501 2.3990832964579263\n",
            "i, sloss, closs, correct 9 0.052190475165843964 2.366960048675537 0.109375\n",
            "time: 1.9230551719665527 2.3515605688095094\n",
            "i, sloss, closs, correct 10 0.051549281924963 2.31897234916687 0.046875\n",
            "time: 1.8929083347320557 2.3099085200916636\n",
            "i, sloss, closs, correct 11 0.059928301721811295 2.4127769470214844 0.03125\n",
            "time: 1.8816733360290527 2.2742973963419595\n",
            "i, sloss, closs, correct 12 0.09197570383548737 2.4114465713500977 0.109375\n",
            "time: 1.8984224796295166 2.245421207868136\n",
            "i, sloss, closs, correct 13 0.07420778274536133 2.3578481674194336 0.015625\n",
            "time: 2.0076842308044434 2.228473322732108\n",
            "i, sloss, closs, correct 14 0.08133967965841293 2.4143447875976562 0.046875\n",
            "time: 2.4838433265686035 2.245523484547933\n",
            "i, sloss, closs, correct 15 0.08038971573114395 2.506287097930908 0.03125\n",
            "time: 1.9563803672790527 2.227480873465538\n",
            "i, sloss, closs, correct 16 0.0966612845659256 2.4735758304595947 0.0625\n",
            "time: 1.907362699508667 2.208677151623894\n",
            "i, sloss, closs, correct 17 0.09484592080116272 2.356388568878174 0.078125\n",
            "time: 1.9096589088439941 2.1920904053582086\n",
            "i, sloss, closs, correct 18 0.07896321266889572 2.3273134231567383 0.125\n",
            "time: 1.8680107593536377 2.175058628383436\n",
            "i, sloss, closs, correct 19 0.13317368924617767 2.344809055328369 0.09375\n",
            "time: 1.8961801528930664 2.1611570000648497\n",
            "i, sloss, closs, correct 20 0.13144077360630035 2.346127986907959 0.078125\n",
            "time: 2.506779432296753 2.177638916742234\n",
            "i, sloss, closs, correct 21 0.10046801716089249 2.387786865234375 0.078125\n",
            "time: 1.9673922061920166 2.1681484200737695\n",
            "i, sloss, closs, correct 22 0.13634073734283447 2.385033130645752 0.078125\n",
            "time: 1.9342982769012451 2.1580013917840044\n",
            "i, sloss, closs, correct 23 0.12231721729040146 2.3892593383789062 0.09375\n",
            "time: 1.93990159034729 2.1489372551441193\n",
            "i, sloss, closs, correct 24 0.13179104030132294 2.3364052772521973 0.03125\n",
            "time: 1.878763198852539 2.138162794113159\n",
            "i, sloss, closs, correct 25 0.10894588381052017 2.316744327545166 0.0625\n",
            "time: 1.8742523193359375 2.1280315747627845\n",
            "i, sloss, closs, correct 26 0.1428239792585373 2.377657413482666 0.0625\n",
            "time: 2.2902495861053467 2.134057742578012\n",
            "i, sloss, closs, correct 27 0.14516018331050873 2.359915256500244 0.078125\n",
            "time: 2.179560899734497 2.135696606976645\n",
            "i, sloss, closs, correct 28 0.184628427028656 2.3930296897888184 0.078125\n",
            "time: 1.9098377227783203 2.127924919128418\n",
            "i, sloss, closs, correct 29 0.14456899464130402 2.378225564956665 0.015625\n",
            "time: 1.943453311920166 2.121804777781169\n",
            "i, sloss, closs, correct 30 0.1406765878200531 2.3299143314361572 0.078125\n",
            "time: 1.902524709701538 2.1147787032588834\n",
            "i, sloss, closs, correct 31 0.16958799958229065 2.3303537368774414 0.078125\n",
            "time: 1.8746669292449951 2.107290804386139\n",
            "i, sloss, closs, correct 32 0.23526734113693237 2.322352409362793 0.03125\n",
            "time: 2.2180747985839844 2.110662843241836\n",
            "i, sloss, closs, correct 33 0.12496515363454819 2.2867093086242676 0.078125\n",
            "time: 2.248554229736328 2.1147298953112434\n",
            "i, sloss, closs, correct 34 0.15438194572925568 2.2929232120513916 0.09375\n",
            "time: 1.8928983211517334 2.1084063734327043\n",
            "i, sloss, closs, correct 35 0.20066402852535248 2.344292402267456 0.0625\n",
            "time: 1.890723705291748 2.1023738053109913\n",
            "i, sloss, closs, correct 36 0.2097322791814804 2.3199007511138916 0.125\n",
            "time: 1.8738186359405518 2.096219610523533\n",
            "i, sloss, closs, correct 37 0.18739154934883118 2.27146577835083 0.078125\n",
            "time: 1.888608455657959 2.090782366300884\n",
            "i, sloss, closs, correct 38 0.14945605397224426 2.275815010070801 0.03125\n",
            "time: 2.033263683319092 2.0893247983394523\n",
            "i, sloss, closs, correct 39 0.22493867576122284 2.28395414352417 0.0625\n",
            "time: 2.416961193084717 2.097565990686417\n",
            "i, sloss, closs, correct 40 0.16724470257759094 2.293959140777588 0.078125\n",
            "time: 1.892106056213379 2.092567083312244\n",
            "i, sloss, closs, correct 41 0.15451651811599731 2.2641422748565674 0.09375\n",
            "time: 1.8885128498077393 2.0877201103028797\n",
            "i, sloss, closs, correct 42 0.21947912871837616 2.336513042449951 0.140625\n",
            "time: 1.8879163265228271 2.0831017106078393\n",
            "i, sloss, closs, correct 43 0.19866466522216797 2.3554890155792236 0.046875\n",
            "time: 1.8926715850830078 2.078784866766496\n",
            "i, sloss, closs, correct 44 0.1653817743062973 2.308443546295166 0.125\n",
            "time: 1.9027855396270752 2.0748840702904596\n",
            "i, sloss, closs, correct 45 0.18474359810352325 2.3123209476470947 0.0625\n",
            "time: 2.5010268688201904 2.084158726360487\n",
            "i, sloss, closs, correct 46 0.13632552325725555 2.2419867515563965 0.0625\n",
            "time: 1.9481909275054932 2.081276127632628\n",
            "i, sloss, closs, correct 47 0.16556139290332794 2.324941396713257 0.09375\n",
            "time: 1.8930742740631104 2.0773656417926154\n",
            "i, sloss, closs, correct 48 0.25204429030418396 2.2783937454223633 0.109375\n",
            "time: 1.8540236949920654 2.072817797563514\n",
            "i, sloss, closs, correct 49 0.13810387253761292 2.2724952697753906 0.046875\n",
            "time: 1.879244327545166 2.068956208229065\n",
            "i, sloss, closs, correct 50 0.20975267887115479 2.2542243003845215 0.15625\n",
            "time: 1.866286039352417 2.0649918060676726\n",
            "i, sloss, closs, correct 51 0.18420612812042236 2.265894889831543 0.171875\n",
            "time: 2.284578323364258 2.069223614839407\n",
            "i, sloss, closs, correct 52 0.1788107007741928 2.2984671592712402 0.09375\n",
            "time: 2.1819727420806885 2.0713580914263456\n",
            "i, sloss, closs, correct 53 0.18773356080055237 2.211268663406372 0.0625\n",
            "time: 1.8628818988800049 2.067506030753807\n",
            "i, sloss, closs, correct 54 0.1722872406244278 2.2846274375915527 0.09375\n",
            "time: 1.8811092376708984 2.0641320185227827\n",
            "i, sloss, closs, correct 55 0.19850757718086243 2.270782947540283 0.09375\n",
            "time: 1.8578686714172363 2.0604571189199175\n",
            "i, sloss, closs, correct 56 0.15470591187477112 2.2004308700561523 0.203125\n",
            "time: 1.8634192943572998 2.0570083417390523\n",
            "i, sloss, closs, correct 57 0.18879923224449158 2.16159725189209 0.015625\n",
            "time: 2.1526362895965576 2.058669427345539\n",
            "i, sloss, closs, correct 58 0.24514922499656677 2.247192621231079 0.0625\n",
            "time: 2.333237648010254 2.063331131207741\n",
            "i, sloss, closs, correct 59 0.22250190377235413 2.2597782611846924 0.078125\n",
            "time: 1.9184589385986328 2.0609251459439597\n",
            "i, sloss, closs, correct 60 0.18412604928016663 2.2829737663269043 0.125\n",
            "time: 1.8782849311828613 2.0579389275097455\n",
            "i, sloss, closs, correct 61 0.2220970243215561 2.2324910163879395 0.125\n",
            "time: 1.8578002452850342 2.0547187020701747\n",
            "i, sloss, closs, correct 62 0.15148301422595978 2.209674835205078 0.078125\n",
            "time: 1.8548343181610107 2.051553525621929\n",
            "i, sloss, closs, correct 63 0.20796088874340057 2.2818655967712402 0.140625\n",
            "time: 1.9635238647460938 2.0501855090260506\n",
            "i, sloss, closs, correct 64 0.2123994380235672 2.3060343265533447 0.125\n",
            "time: 2.4926018714904785 2.0569981464972864\n",
            "i, sloss, closs, correct 65 0.2482888251543045 2.2021915912628174 0.15625\n",
            "time: 1.8855011463165283 2.0544066429138184\n",
            "i, sloss, closs, correct 66 0.16039496660232544 2.2855112552642822 0.140625\n",
            "time: 1.885883092880249 2.051903628591281\n",
            "i, sloss, closs, correct 67 0.20093944668769836 2.2134058475494385 0.078125\n",
            "time: 1.8485395908355713 2.048920245731578\n",
            "i, sloss, closs, correct 68 0.28075161576271057 2.2327799797058105 0.1875\n",
            "time: 1.885213851928711 2.0465555985768638\n",
            "i, sloss, closs, correct 69 0.19400490820407867 2.2038424015045166 0.109375\n",
            "time: 1.9013540744781494 2.0444884708949496\n",
            "i, sloss, closs, correct 70 0.19380630552768707 2.2042574882507324 0.125\n",
            "time: 2.3515493869781494 2.0488299517564372\n",
            "i, sloss, closs, correct 71 0.1399385780096054 2.2000620365142822 0.09375\n",
            "time: 2.0358550548553467 2.0486564106411405\n",
            "i, sloss, closs, correct 72 0.2552465796470642 2.220773696899414 0.140625\n",
            "time: 1.8688342571258545 2.0461997953179765\n",
            "i, sloss, closs, correct 73 0.2853989005088806 2.250579595565796 0.1875\n",
            "time: 1.9034605026245117 2.0442774070275798\n",
            "i, sloss, closs, correct 74 0.2171211838722229 2.1915981769561768 0.203125\n",
            "time: 1.897219181060791 2.042327489852905\n",
            "i, sloss, closs, correct 75 0.1506429761648178 2.205395221710205 0.125\n",
            "time: 1.9069664478302002 2.0405527572882804\n",
            "i, sloss, closs, correct 76 0.2717636227607727 2.252925157546997 0.09375\n",
            "time: 2.24025821685791 2.0431568963187083\n",
            "i, sloss, closs, correct 77 0.28591272234916687 2.14998459815979 0.140625\n",
            "time: 2.1762781143188477 2.0448936376816187\n",
            "i, sloss, closs, correct 78 0.20195603370666504 2.195617198944092 0.125\n",
            "time: 1.9111592769622803 2.043206827549995\n",
            "i, sloss, closs, correct 79 0.1869838386774063 2.2163634300231934 0.125\n",
            "time: 1.895585060119629 2.041372302174568\n",
            "i, sloss, closs, correct 80 0.22168408334255219 2.1491761207580566 0.203125\n",
            "time: 1.9021656513214111 2.0396595266130237\n",
            "i, sloss, closs, correct 81 0.15706036984920502 2.2158665657043457 0.1875\n",
            "time: 1.923506736755371 2.038252551381181\n",
            "i, sloss, closs, correct 82 0.3247477114200592 2.228456497192383 0.203125\n",
            "time: 2.0719411373138428 2.0386643725705436\n",
            "i, sloss, closs, correct 83 0.20091022551059723 2.253822088241577 0.171875\n",
            "time: 2.4596967697143555 2.0436926938238598\n",
            "i, sloss, closs, correct 84 0.2897757887840271 2.205068588256836 0.140625\n",
            "time: 1.9116864204406738 2.0421457178452433\n",
            "i, sloss, closs, correct 85 0.26654139161109924 2.2112932205200195 0.25\n",
            "time: 1.866692304611206 2.0401111979817235\n",
            "i, sloss, closs, correct 86 0.18506118655204773 2.257449150085449 0.171875\n",
            "time: 1.8794641494750977 2.038274241589952\n",
            "i, sloss, closs, correct 87 0.14605464041233063 2.2470011711120605 0.203125\n",
            "time: 1.9028525352478027 2.036740955981341\n",
            "i, sloss, closs, correct 88 0.2447168231010437 2.1724767684936523 0.1875\n",
            "time: 1.9465930461883545 2.0357376859429177\n",
            "i, sloss, closs, correct 89 0.20536838471889496 2.133829116821289 0.171875\n",
            "time: 2.5326735973358154 2.041264714135064\n",
            "i, sloss, closs, correct 90 0.14531607925891876 2.218637228012085 0.15625\n",
            "time: 1.8971261978149414 2.0396864728613213\n",
            "i, sloss, closs, correct 91 0.14264370501041412 2.189053535461426 0.171875\n",
            "time: 1.881680965423584 2.0379779934883118\n",
            "i, sloss, closs, correct 92 0.22084718942642212 2.1664299964904785 0.1875\n",
            "time: 1.8793702125549316 2.0362896867977676\n",
            "i, sloss, closs, correct 93 0.16934452950954437 2.1951637268066406 0.09375\n",
            "time: 1.920480728149414 2.0350628381079816\n",
            "i, sloss, closs, correct 94 0.16223779320716858 2.2216155529022217 0.21875\n",
            "time: 1.888906717300415 2.033531412325407\n",
            "i, sloss, closs, correct 95 0.14182552695274353 2.189133644104004 0.125\n",
            "time: 2.3619635105133057 2.0369574452439942\n",
            "i, sloss, closs, correct 96 0.15677542984485626 2.101503610610962 0.1875\n",
            "time: 2.040477752685547 2.0369981121771112\n",
            "i, sloss, closs, correct 97 0.29844585061073303 2.1205272674560547 0.125\n",
            "time: 1.8922815322875977 2.0355260907387245\n",
            "i, sloss, closs, correct 98 0.17274604737758636 2.16428542137146 0.21875\n",
            "time: 1.9136910438537598 2.0343000551666877\n",
            "i, sloss, closs, correct 99 0.22235436737537384 2.1882777214050293 0.1875\n",
            "time: 1.8743152618408203 2.032704999446869\n",
            "i, sloss, closs, correct 100 0.16145756840705872 2.162386417388916 0.171875\n",
            "time: 1.8783297538757324 2.0311813519732786\n",
            "i, sloss, closs, correct 101 0.19958730041980743 2.184993028640747 0.203125\n",
            "time: 2.253700017929077 2.03337011384029\n",
            "i, sloss, closs, correct 102 0.13427288830280304 2.150920867919922 0.25\n",
            "time: 2.1987926959991455 2.03498080633219\n",
            "i, sloss, closs, correct 103 0.22335846722126007 2.1578407287597656 0.15625\n",
            "time: 1.9162096977233887 2.0338435769081116\n",
            "i, sloss, closs, correct 104 0.1336970329284668 2.1323513984680176 0.234375\n",
            "time: 1.944869041442871 2.03300316901434\n",
            "i, sloss, closs, correct 105 0.13512219488620758 2.1928253173828125 0.171875\n",
            "time: 1.9245507717132568 2.0319844776729368\n",
            "i, sloss, closs, correct 106 0.1752214878797531 2.1935269832611084 0.21875\n",
            "time: 1.8869845867156982 2.0306337882425183\n",
            "i, sloss, closs, correct 107 0.1789938509464264 2.1593234539031982 0.171875\n",
            "time: 2.1752264499664307 2.031977088363082\n",
            "i, sloss, closs, correct 108 0.14855961501598358 2.2836766242980957 0.1875\n",
            "time: 2.29984712600708 2.0344389184899287\n",
            "i, sloss, closs, correct 109 0.1689900904893875 2.15787672996521 0.21875\n",
            "time: 1.884563684463501 2.0330807490782306\n",
            "i, sloss, closs, correct 110 0.1406291425228119 2.198232650756836 0.1875\n",
            "time: 1.9480149745941162 2.0323187982713855\n",
            "i, sloss, closs, correct 111 0.17270106077194214 2.1937785148620605 0.171875\n",
            "time: 1.905357837677002 2.0311924644878934\n",
            "i, sloss, closs, correct 112 0.1801336407661438 2.18961238861084 0.171875\n",
            "time: 1.8690204620361328 2.029761613997738\n",
            "i, sloss, closs, correct 113 0.1534561812877655 2.1848604679107666 0.140625\n",
            "time: 2.0102336406707764 2.0295946911761633\n",
            "i, sloss, closs, correct 114 0.15882505476474762 2.1288912296295166 0.125\n",
            "time: 2.449510097503662 2.033249452839727\n",
            "i, sloss, closs, correct 115 0.21731378138065338 2.1633830070495605 0.203125\n",
            "time: 1.8936903476715088 2.0320505869799645\n",
            "i, sloss, closs, correct 116 0.15831595659255981 2.1404459476470947 0.1875\n",
            "time: 1.893662929534912 2.030871845718123\n",
            "i, sloss, closs, correct 117 0.15749263763427734 2.1781554222106934 0.234375\n",
            "time: 1.8895246982574463 2.02968453754813\n",
            "i, sloss, closs, correct 118 0.18298479914665222 2.1614980697631836 0.25\n",
            "time: 1.880347728729248 2.02843361141301\n",
            "i, sloss, closs, correct 119 0.1530173420906067 2.1901187896728516 0.25\n",
            "time: 1.8764069080352783 2.0271733005841575\n",
            "i, sloss, closs, correct 120 0.18149980902671814 2.162759780883789 0.234375\n",
            "time: 2.4249980449676514 2.030465173327233\n",
            "i, sloss, closs, correct 121 0.19492416083812714 2.213653087615967 0.171875\n",
            "time: 1.983546495437622 2.0301067965929627\n",
            "i, sloss, closs, correct 122 0.15810735523700714 2.1729254722595215 0.203125\n",
            "time: 1.9123742580413818 2.029153680413719\n",
            "i, sloss, closs, correct 123 0.13768784701824188 2.1610870361328125 0.203125\n",
            "time: 1.9081270694732666 2.028181597109764\n",
            "i, sloss, closs, correct 124 0.19370897114276886 2.1672189235687256 0.15625\n",
            "time: 1.9009933471679688 2.0271679039001467\n",
            "i, sloss, closs, correct 125 0.17443522810935974 2.2060070037841797 0.203125\n",
            "time: 1.8708312511444092 2.0259310499070184\n",
            "i, sloss, closs, correct 126 0.18543346226215363 2.1625900268554688 0.1875\n",
            "time: 2.2612144947052 2.027789551442064\n",
            "i, sloss, closs, correct 127 0.1376388669013977 2.216874122619629 0.203125\n",
            "time: 2.1473000049591064 2.0287269074469805\n",
            "i, sloss, closs, correct 128 0.16536015272140503 2.2051897048950195 0.09375\n",
            "time: 1.8993003368377686 2.027728333953739\n",
            "i, sloss, closs, correct 129 0.18380102515220642 2.1494898796081543 0.171875\n",
            "time: 1.906247615814209 2.026800381220304\n",
            "i, sloss, closs, correct 130 0.1701556146144867 2.2256453037261963 0.21875\n",
            "time: 1.9040749073028564 2.025867316559071\n",
            "i, sloss, closs, correct 131 0.24283795058727264 2.177400588989258 0.140625\n",
            "time: 1.886427640914917 2.024814470247789\n",
            "i, sloss, closs, correct 132 0.18286734819412231 2.1758341789245605 0.1875\n",
            "time: 2.1110987663269043 2.025467067732847\n",
            "i, sloss, closs, correct 133 0.15559762716293335 2.1468935012817383 0.28125\n",
            "time: 2.3055078983306885 2.0275599992097315\n",
            "i, sloss, closs, correct 134 0.1322614997625351 2.1647377014160156 0.15625\n",
            "time: 1.930039882659912 2.026852035522461\n",
            "i, sloss, closs, correct 135 0.18077555298805237 2.137129068374634 0.21875\n",
            "time: 1.9093937873840332 2.025991992038839\n",
            "i, sloss, closs, correct 136 0.12283563613891602 2.186356544494629 0.21875\n",
            "time: 1.9111368656158447 2.0251570061175492\n",
            "i, sloss, closs, correct 137 0.15349465608596802 2.229400634765625 0.1875\n",
            "time: 1.860297679901123 2.023965609246406\n",
            "i, sloss, closs, correct 138 0.15282055735588074 2.215419292449951 0.25\n",
            "time: 1.9814376831054688 2.0236629496375436\n",
            "i, sloss, closs, correct 139 0.1471724808216095 2.1949539184570312 0.21875\n",
            "time: 2.4189631938934326 2.026514506340027\n",
            "i, sloss, closs, correct 140 0.21500298380851746 2.183389902114868 0.296875\n",
            "time: 1.9203293323516846 2.0257676763737456\n",
            "i, sloss, closs, correct 141 0.13648895919322968 2.1313657760620117 0.171875\n",
            "time: 1.9014277458190918 2.02489757202041\n",
            "i, sloss, closs, correct 142 0.22024451196193695 2.171309471130371 0.203125\n",
            "time: 1.8838727474212646 2.0239145422315263\n",
            "i, sloss, closs, correct 143 0.15212106704711914 2.1966264247894287 0.21875\n",
            "time: 1.8980045318603516 2.02304347190592\n",
            "i, sloss, closs, correct 144 0.17252086102962494 2.187680244445801 0.171875\n",
            "time: 1.8718671798706055 2.0220041603877625\n",
            "i, sloss, closs, correct 145 0.20255601406097412 2.1921491622924805 0.203125\n",
            "time: 2.419013023376465 2.0247265165799284\n",
            "i, sloss, closs, correct 146 0.13118360936641693 2.2004642486572266 0.171875\n",
            "time: 2.012012004852295 2.0246589897441214\n",
            "i, sloss, closs, correct 147 0.13784600794315338 2.154883861541748 0.140625\n",
            "time: 1.8993210792541504 2.0238153628400855\n",
            "i, sloss, closs, correct 148 0.12649092078208923 2.212566375732422 0.125\n",
            "time: 1.9092142581939697 2.023049352953098\n",
            "i, sloss, closs, correct 149 0.14309553802013397 2.119788646697998 0.140625\n",
            "time: 1.8721864223480225 2.022048986752828\n",
            "i, sloss, closs, correct 150 0.17544332146644592 2.1675519943237305 0.234375\n",
            "time: 1.8735501766204834 2.021068681944285\n",
            "i, sloss, closs, correct 151 0.1599891483783722 2.139014720916748 0.09375\n",
            "time: 2.2613329887390137 2.0226542777136753\n",
            "i, sloss, closs, correct 152 0.15116894245147705 2.2136354446411133 0.21875\n",
            "time: 2.1540112495422363 2.02351519329096\n",
            "i, sloss, closs, correct 153 0.13789834082126617 2.1458895206451416 0.21875\n",
            "time: 1.9093420505523682 2.0227769473930457\n",
            "i, sloss, closs, correct 154 0.23441635072231293 2.1701622009277344 0.21875\n",
            "time: 1.9092986583709717 2.0220479334554367\n",
            "i, sloss, closs, correct 155 0.14775250852108002 2.1317105293273926 0.234375\n",
            "time: 1.885225534439087 2.021173866895529\n",
            "i, sloss, closs, correct 156 0.1845298856496811 2.148317337036133 0.171875\n",
            "time: 1.868539810180664 2.0202062509621785\n",
            "i, sloss, closs, correct 157 0.19495749473571777 2.2352285385131836 0.296875\n",
            "time: 2.1254024505615234 2.0208753468115117\n",
            "i, sloss, closs, correct 158 0.12961003184318542 2.1159069538116455 0.1875\n",
            "time: 2.3086066246032715 2.0226890860863453\n",
            "i, sloss, closs, correct 159 0.17026974260807037 2.142383575439453 0.171875\n",
            "time: 1.9017362594604492 2.0219360813498497\n",
            "i, sloss, closs, correct 160 0.14223136007785797 2.162797212600708 0.1875\n",
            "time: 1.919863224029541 2.0213050116663394\n",
            "i, sloss, closs, correct 161 0.12766270339488983 2.134265899658203 0.203125\n",
            "time: 1.8734426498413086 2.0203953113084956\n",
            "i, sloss, closs, correct 162 0.12642934918403625 2.163882255554199 0.15625\n",
            "time: 1.8976352214813232 2.0196450239310235\n",
            "i, sloss, closs, correct 163 0.20081526041030884 2.138654947280884 0.171875\n",
            "time: 1.983302354812622 2.0194264330515046\n",
            "i, sloss, closs, correct 164 0.15761291980743408 2.180363178253174 0.328125\n",
            "time: 2.433549404144287 2.0219493157935866\n",
            "i, sloss, closs, correct 165 0.13537383079528809 2.133897304534912 0.1875\n",
            "time: 1.9433870315551758 2.02147895744048\n",
            "i, sloss, closs, correct 166 0.18549180030822754 2.1578335762023926 0.21875\n",
            "time: 1.8955509662628174 2.0207279870610035\n",
            "i, sloss, closs, correct 167 0.13143840432167053 2.154689311981201 0.203125\n",
            "time: 1.8719329833984375 2.0198451436701275\n",
            "i, sloss, closs, correct 168 0.1332153081893921 2.162889003753662 0.171875\n",
            "time: 1.9234132766723633 2.0192774245019494\n",
            "i, sloss, closs, correct 169 0.16278716921806335 2.1491425037384033 0.140625\n",
            "time: 1.8881218433380127 2.0185109587276684\n",
            "i, sloss, closs, correct 170 0.14786572754383087 2.219010829925537 0.1875\n",
            "time: 2.480984926223755 2.021218270586248\n",
            "i, sloss, closs, correct 171 0.13766688108444214 2.205502510070801 0.203125\n",
            "time: 1.933631181716919 2.020712956439617\n",
            "i, sloss, closs, correct 172 0.14858049154281616 2.1435728073120117 0.15625\n",
            "time: 1.891756296157837 2.0199702599145084\n",
            "i, sloss, closs, correct 173 0.2142045497894287 2.145416259765625 0.21875\n",
            "time: 1.9240779876708984 2.019422383143984\n",
            "i, sloss, closs, correct 174 0.13839146494865417 2.120044708251953 0.1875\n",
            "time: 1.8765368461608887 2.018608636856079\n",
            "i, sloss, closs, correct 175 0.13430942595005035 2.1348466873168945 0.28125\n",
            "time: 1.893279790878296 2.0178991556167603\n",
            "i, sloss, closs, correct 176 0.20213718712329865 2.0950326919555664 0.203125\n",
            "time: 2.301360845565796 2.0195047006768694\n",
            "i, sloss, closs, correct 177 0.19989588856697083 2.1432385444641113 0.1875\n",
            "time: 2.1127936840057373 2.020030978020657\n",
            "i, sloss, closs, correct 178 0.1223117783665657 2.1510510444641113 0.1875\n",
            "time: 1.9127612113952637 2.019434365480306\n",
            "i, sloss, closs, correct 179 0.20326009392738342 2.1426522731781006 0.265625\n",
            "time: 1.8856611251831055 2.0186957332823012\n",
            "i, sloss, closs, correct 180 0.1120699867606163 2.1905508041381836 0.15625\n",
            "time: 1.8946127891540527 2.0180123379217325\n",
            "i, sloss, closs, correct 181 0.17546238005161285 2.099116086959839 0.203125\n",
            "time: 1.8872416019439697 2.0172964413087446\n",
            "i, sloss, closs, correct 182 0.13241516053676605 2.198399305343628 0.171875\n",
            "time: 2.16575288772583 2.0181114256707695\n",
            "i, sloss, closs, correct 183 0.14267076551914215 2.2006876468658447 0.171875\n",
            "time: 2.2418553829193115 2.0193294532921002\n",
            "i, sloss, closs, correct 184 0.11762344092130661 2.1782875061035156 0.203125\n",
            "time: 1.8962314128875732 2.0186665650960562\n",
            "i, sloss, closs, correct 185 0.1757269650697708 2.1450865268707275 0.25\n",
            "time: 1.8999814987182617 2.018031225409559\n",
            "i, sloss, closs, correct 186 0.1291438788175583 2.1456029415130615 0.171875\n",
            "time: 1.864678144454956 2.017217295692566\n",
            "i, sloss, closs, correct 187 0.1387285441160202 2.11832332611084 0.125\n",
            "time: 1.8577210903167725 2.01637132497544\n",
            "i, sloss, closs, correct 188 0.1368652731180191 2.217869758605957 0.109375\n",
            "time: 2.026606321334839 2.0164279761137784\n",
            "i, sloss, closs, correct 189 0.14012043178081512 2.1806867122650146 0.234375\n",
            "time: 2.410921812057495 2.0185254059339823\n",
            "i, sloss, closs, correct 190 0.12183626741170883 2.1725692749023438 0.25\n",
            "time: 1.91123366355896 2.017966155606415\n",
            "i, sloss, closs, correct 191 0.13133129477500916 2.133946180343628 0.21875\n",
            "time: 1.903639316558838 2.0173732141653695\n",
            "i, sloss, closs, correct 192 0.13024428486824036 2.2717952728271484 0.21875\n",
            "time: 1.8590106964111328 2.0165551017603107\n",
            "i, sloss, closs, correct 193 0.14762084186077118 2.2030115127563477 0.265625\n",
            "time: 1.8725242614746094 2.0158153194742106\n",
            "i, sloss, closs, correct 194 0.1607542783021927 2.223999500274658 0.234375\n",
            "time: 1.8746204376220703 2.015093821745652\n",
            "i, sloss, closs, correct 195 0.12327534705400467 2.178382158279419 0.1875\n",
            "time: 2.5088164806365967 2.0176153389775022\n",
            "i, sloss, closs, correct 196 0.14502352476119995 2.0973706245422363 0.1875\n",
            "time: 1.964634656906128 2.0173482967512255\n",
            "i, sloss, closs, correct 197 0.15950803458690643 2.1966891288757324 0.328125\n",
            "time: 1.8935019969940186 2.016725227086231\n",
            "i, sloss, closs, correct 198 0.13553814589977264 2.189321517944336 0.21875\n",
            "time: 1.8881795406341553 2.016081983719639\n",
            "i, sloss, closs, correct 199 0.11042753607034683 2.153521776199341 0.171875\n",
            "time: 1.8684184551239014 2.015347045660019\n",
            "i, sloss, closs, correct 200 0.14911100268363953 2.1821746826171875 0.21875\n",
            "time: 1.875182867050171 2.0146519817523103\n",
            "i, sloss, closs, correct 201 0.14809975028038025 2.1386289596557617 0.25\n",
            "time: 2.2840871810913086 2.015988326308751\n",
            "i, sloss, closs, correct 202 0.17217813432216644 2.140841484069824 0.203125\n",
            "time: 2.1200478076934814 2.016502821973979\n",
            "i, sloss, closs, correct 203 0.13210386037826538 2.0907444953918457 0.171875\n",
            "time: 1.8827040195465088 2.015849604326136\n",
            "i, sloss, closs, correct 204 0.1316073089838028 2.171430826187134 0.171875\n",
            "time: 1.8645412921905518 2.0151138794131396\n",
            "i, sloss, closs, correct 205 0.17212146520614624 2.133729934692383 0.359375\n",
            "time: 1.8710486888885498 2.0144189920240234\n",
            "i, sloss, closs, correct 206 0.13212166726589203 2.1806957721710205 0.21875\n",
            "time: 1.8917489051818848 2.013828770549977\n",
            "i, sloss, closs, correct 207 0.18579305708408356 2.1845784187316895 0.203125\n",
            "time: 2.115452289581299 2.0143255385068746\n",
            "i, sloss, closs, correct 208 0.20728342235088348 2.1311726570129395 0.15625\n",
            "time: 2.2761166095733643 2.015579983378141\n",
            "i, sloss, closs, correct 209 0.1345268189907074 2.216769218444824 0.171875\n",
            "time: 1.9041054248809814 2.01505161012922\n",
            "i, sloss, closs, correct 210 0.13435587286949158 2.1335158348083496 0.234375\n",
            "time: 1.9055814743041992 2.01453521240379\n",
            "i, sloss, closs, correct 211 0.11293778568506241 2.116652488708496 0.265625\n",
            "time: 1.8775129318237305 2.0138911796065995\n",
            "i, sloss, closs, correct 212 0.12640736997127533 2.1240434646606445 0.203125\n",
            "time: 1.8727474212646484 2.0132322927036195\n",
            "i, sloss, closs, correct 213 0.133210688829422 2.147700309753418 0.203125\n",
            "time: 2.008802652359009 2.0132137938080548\n",
            "i, sloss, closs, correct 214 0.12748612463474274 2.108706474304199 0.21875\n",
            "time: 2.4329795837402344 2.015167923860772\n",
            "i, sloss, closs, correct 215 0.10888123512268066 2.1372389793395996 0.21875\n",
            "time: 1.894277811050415 2.0146103744153625\n",
            "i, sloss, closs, correct 216 0.12892219424247742 2.149780035018921 0.171875\n",
            "time: 1.8675498962402344 2.0139348473966394\n",
            "i, sloss, closs, correct 217 0.10853733122348785 2.1818385124206543 0.25\n",
            "time: 1.8660740852355957 2.0132586562305415\n",
            "i, sloss, closs, correct 218 0.11505991965532303 2.245784044265747 0.203125\n",
            "time: 1.8856351375579834 2.012677944958482\n",
            "i, sloss, closs, correct 219 0.1790345311164856 2.082364797592163 0.234375\n",
            "time: 1.9286127090454102 2.0122978568077086\n",
            "i, sloss, closs, correct 220 0.15305782854557037 2.1136536598205566 0.28125\n",
            "time: 2.4145727157592773 2.014120221677409\n",
            "i, sloss, closs, correct 221 0.1446959376335144 2.148733615875244 0.234375\n",
            "time: 2.0265960693359375 2.014178622950305\n",
            "i, sloss, closs, correct 222 0.1251775324344635 2.2280614376068115 0.25\n",
            "time: 1.9085664749145508 2.0137070968012103\n",
            "i, sloss, closs, correct 223 0.13066284358501434 2.1508312225341797 0.171875\n",
            "time: 1.8842196464538574 2.013131317283426\n",
            "i, sloss, closs, correct 224 0.1463637501001358 2.1500191688537598 0.203125\n",
            "time: 1.8687543869018555 2.0124931695726183\n",
            "i, sloss, closs, correct 225 0.08979937434196472 2.1600723266601562 0.21875\n",
            "time: 1.9258065223693848 2.012111706016338\n",
            "i, sloss, closs, correct 226 0.11974072456359863 2.1487061977386475 0.234375\n",
            "time: 2.280886173248291 2.013297838261474\n",
            "i, sloss, closs, correct 227 0.12541380524635315 2.080557346343994 0.265625\n",
            "time: 2.1619644165039062 2.0139545170884383\n",
            "i, sloss, closs, correct 228 0.12381341308355331 2.1878409385681152 0.171875\n",
            "time: 1.8820395469665527 2.013380466069717\n",
            "i, sloss, closs, correct 229 0.14460507035255432 2.2510995864868164 0.1875\n",
            "time: 1.8943946361541748 2.0128652676292087\n",
            "i, sloss, closs, correct 230 0.11407804489135742 2.165048122406006 0.1875\n",
            "time: 1.8621275424957275 2.0122148371362067\n",
            "i, sloss, closs, correct 231 0.12695181369781494 2.094162940979004 0.140625\n",
            "time: 1.891704797744751 2.011697455726821\n",
            "i, sloss, closs, correct 232 0.14523166418075562 2.156902313232422 0.125\n",
            "time: 2.1075830459594727 2.0121109465161107\n",
            "i, sloss, closs, correct 233 0.1253320425748825 2.1660654544830322 0.203125\n",
            "time: 2.3214097023010254 2.013434409076332\n",
            "i, sloss, closs, correct 234 0.14747750759124756 2.179229497909546 0.234375\n",
            "time: 1.9070603847503662 2.012983756369733\n",
            "i, sloss, closs, correct 235 0.18927332758903503 2.157824993133545 0.25\n",
            "time: 1.8853647708892822 2.0124450719962685\n",
            "i, sloss, closs, correct 236 0.09379831701517105 2.1271181106567383 0.15625\n",
            "time: 1.884087085723877 2.011905534357964\n",
            "i, sloss, closs, correct 237 0.09661352634429932 2.1852879524230957 0.265625\n",
            "time: 1.8640635013580322 2.0112887620925903\n",
            "i, sloss, closs, correct 238 0.12681844830513 2.1353180408477783 0.15625\n",
            "time: 1.9301338195800781 2.010951193805519\n",
            "i, sloss, closs, correct 239 0.1297735720872879 2.1380536556243896 0.28125\n",
            "time: 2.451820135116577 2.01279050608476\n",
            "i, sloss, closs, correct 240 0.1121823787689209 2.1626930236816406 0.1875\n",
            "time: 1.8957676887512207 2.0123068959881163\n",
            "i, sloss, closs, correct 241 0.15033365786075592 2.128636360168457 0.21875\n",
            "time: 1.858328104019165 2.011672615019743\n",
            "i, sloss, closs, correct 242 0.14578422904014587 2.1773571968078613 0.296875\n",
            "time: 1.8833062648773193 2.0111466150715516\n",
            "i, sloss, closs, correct 243 0.11373881250619888 2.213717460632324 0.25\n",
            "time: 1.8780279159545898 2.0106030008831963\n",
            "i, sloss, closs, correct 244 0.1116519644856453 2.1351819038391113 0.21875\n",
            "time: 1.880300760269165 2.0100735206993257\n",
            "i, sloss, closs, correct 245 0.09431511908769608 2.189488649368286 0.25\n",
            "time: 2.3030054569244385 2.011266280965107\n",
            "i, sloss, closs, correct 246 0.13675250113010406 2.1264400482177734 0.265625\n",
            "time: 2.071610450744629 2.011512307502963\n",
            "i, sloss, closs, correct 247 0.11320168524980545 2.139580488204956 0.234375\n",
            "time: 1.8722882270812988 2.0109588138518797\n",
            "i, sloss, closs, correct 248 0.14618396759033203 2.1176390647888184 0.15625\n",
            "time: 1.8749768733978271 2.010416989345627\n",
            "i, sloss, closs, correct 249 0.11172804981470108 2.117692470550537 0.15625\n",
            "time: 1.8999860286712646 2.0099771432876588\n",
            "i, sloss, closs, correct 250 0.09494797140359879 2.1291537284851074 0.25\n",
            "time: 1.9150571823120117 2.009600845466097\n",
            "i, sloss, closs, correct 251 0.11272464692592621 2.160459518432617 0.203125\n",
            "time: 2.189380168914795 2.010316118361458\n",
            "i, sloss, closs, correct 252 0.17780236899852753 2.168717384338379 0.265625\n",
            "time: 2.192758321762085 2.0110462955806567\n",
            "i, sloss, closs, correct 253 0.16937938332557678 2.1421890258789062 0.203125\n",
            "time: 1.864424705505371 2.010470916905741\n",
            "i, sloss, closs, correct 254 0.16082578897476196 2.184704303741455 0.34375\n",
            "time: 1.9081363677978516 2.010072789472692\n",
            "i, sloss, closs, correct 255 0.10684752464294434 2.164597272872925 0.203125\n",
            "time: 1.8989965915679932 2.009640689007938\n",
            "i, sloss, closs, correct 256 0.1441105753183365 2.1566452980041504 0.234375\n",
            "time: 1.8950822353363037 2.0091967026083384\n",
            "i, sloss, closs, correct 257 0.09419968724250793 2.183089256286621 0.15625\n",
            "time: 2.0637736320495605 2.009410203889359\n",
            "i, sloss, closs, correct 258 0.11035359650850296 2.1887974739074707 0.296875\n",
            "time: 2.4258978366851807 2.011025704932489\n",
            "i, sloss, closs, correct 259 0.10423295199871063 2.1794967651367188 0.140625\n",
            "time: 1.9157185554504395 2.0106642310435956\n",
            "i, sloss, closs, correct 260 0.11981937289237976 2.1233069896698 0.171875\n",
            "time: 1.92268967628479 2.010329848505071\n",
            "i, sloss, closs, correct 261 0.09697244316339493 2.147871255874634 0.25\n",
            "time: 1.9211947917938232 2.009994539595742\n",
            "i, sloss, closs, correct 262 0.10358478873968124 2.156203269958496 0.21875\n",
            "time: 1.8805077075958252 2.0095039655953757\n",
            "i, sloss, closs, correct 263 0.15688246488571167 2.117122173309326 0.171875\n",
            "time: 2.059213876724243 2.0096939820231814\n",
            "i, sloss, closs, correct 264 0.1018538549542427 2.1630349159240723 0.265625\n",
            "time: 2.4486465454101562 2.011355203052737\n",
            "i, sloss, closs, correct 265 0.16355465352535248 2.1943843364715576 0.21875\n",
            "time: 1.8711388111114502 2.0108298296318914\n",
            "i, sloss, closs, correct 266 0.09491924196481705 2.188316822052002 0.28125\n",
            "time: 1.915863037109375 2.0104762168412798\n",
            "i, sloss, closs, correct 267 0.12772361934185028 2.2206363677978516 0.171875\n",
            "time: 1.891005039215088 2.0100334064284366\n",
            "i, sloss, closs, correct 268 0.10684443265199661 2.2307028770446777 0.265625\n",
            "time: 1.8440914154052734 2.009418299650171\n",
            "i, sloss, closs, correct 269 0.10191751271486282 2.227879047393799 0.28125\n",
            "time: 1.884389877319336 2.00895811981625\n",
            "i, sloss, closs, correct 270 0.09649739414453506 2.11042857170105 0.25\n",
            "time: 2.369335889816284 2.0102896575998117\n",
            "i, sloss, closs, correct 271 0.08506017178297043 2.1918280124664307 0.296875\n",
            "time: 2.049713373184204 2.0104364796596417\n",
            "i, sloss, closs, correct 272 0.1189117357134819 2.1662561893463135 0.15625\n",
            "time: 1.8908209800720215 2.010000107489226\n",
            "i, sloss, closs, correct 273 0.09905631840229034 2.182478904724121 0.265625\n",
            "time: 1.868927001953125 2.0094869563179296\n",
            "i, sloss, closs, correct 274 0.10911013931035995 2.1631317138671875 0.28125\n",
            "time: 1.8768136501312256 2.009007474725897\n",
            "i, sloss, closs, correct 275 0.12715189158916473 2.1462550163269043 0.171875\n",
            "time: 1.8606433868408203 2.00847174551176\n",
            "i, sloss, closs, correct 276 0.1102517694234848 2.1374855041503906 0.203125\n",
            "time: 2.194952964782715 2.0091467945153965\n",
            "i, sloss, closs, correct 277 0.15900641679763794 2.1873481273651123 0.1875\n",
            "time: 2.1925976276397705 2.0098080506427682\n",
            "i, sloss, closs, correct 278 0.0839509591460228 2.211181163787842 0.1875\n",
            "time: 1.9303638935089111 2.0095250110899676\n",
            "i, sloss, closs, correct 279 0.1040218397974968 2.102168560028076 0.296875\n",
            "time: 1.876129150390625 2.0090502790042333\n",
            "i, sloss, closs, correct 280 0.10053848475217819 2.0990328788757324 0.21875\n",
            "time: 1.8964691162109375 2.008651396557954\n",
            "i, sloss, closs, correct 281 0.08572530746459961 2.1358530521392822 0.3125\n",
            "time: 1.8559534549713135 2.0081116941803736\n",
            "i, sloss, closs, correct 282 0.10491292923688889 2.1870932579040527 0.125\n",
            "time: 2.0188300609588623 2.008151254889822\n",
            "i, sloss, closs, correct 283 0.14407218992710114 2.188699245452881 0.3125\n",
            "time: 2.361280679702759 2.009395996449699\n",
            "i, sloss, closs, correct 284 0.12642690539360046 2.137338399887085 0.171875\n",
            "time: 1.8623683452606201 2.0088816726416874\n",
            "i, sloss, closs, correct 285 0.12812499701976776 2.1354711055755615 0.25\n",
            "time: 1.88383150100708 2.0084461949088355\n",
            "i, sloss, closs, correct 286 0.12021944671869278 2.1850807666778564 0.25\n",
            "time: 1.918816328048706 2.0081355463752346\n",
            "i, sloss, closs, correct 287 0.11212203651666641 2.1375796794891357 0.25\n",
            "time: 1.8846571445465088 2.007709677848551\n",
            "i, sloss, closs, correct 288 0.11311577260494232 2.148758888244629 0.21875\n",
            "time: 1.8598337173461914 2.007199647104864\n",
            "i, sloss, closs, correct 289 0.10412751138210297 2.1159486770629883 0.15625\n",
            "time: 2.4326937198638916 2.0086685591730578\n",
            "i, sloss, closs, correct 290 0.09120554476976395 2.13484525680542 0.21875\n",
            "time: 1.997258186340332 2.008633048264022\n",
            "i, sloss, closs, correct 291 0.14052139222621918 2.213174819946289 0.296875\n",
            "time: 1.8940811157226562 2.008242428302765\n",
            "i, sloss, closs, correct 292 0.14292651414871216 2.1583995819091797 0.234375\n",
            "time: 1.883108377456665 2.0078169965906763\n",
            "i, sloss, closs, correct 293 0.1655093878507614 2.1403608322143555 0.234375\n",
            "time: 1.9184648990631104 2.007514913065904\n",
            "i, sloss, closs, correct 294 0.14928901195526123 2.1548805236816406 0.1875\n",
            "time: 1.8534464836120605 2.006995125140174\n",
            "i, sloss, closs, correct 295 0.08927815407514572 2.1370980739593506 0.203125\n",
            "time: 2.2918813228607178 2.0079604765853367\n",
            "i, sloss, closs, correct 296 0.15022845566272736 2.2004027366638184 0.234375\n",
            "time: 2.0951802730560303 2.0082554961695815\n",
            "i, sloss, closs, correct 297 0.10421667248010635 2.1381444931030273 0.25\n",
            "time: 1.8885767459869385 2.007855544154276\n",
            "i, sloss, closs, correct 298 0.10847921669483185 2.138211965560913 0.25\n",
            "time: 1.907855749130249 2.007522667530787\n",
            "i, sloss, closs, correct 299 0.11351079493761063 2.211787700653076 0.25\n",
            "time: 1.8734092712402344 2.0070781906445823\n",
            "i, sloss, closs, correct 300 0.13534921407699585 2.1594061851501465 0.28125\n",
            "time: 1.8763599395751953 2.0066453491730547\n",
            "i, sloss, closs, correct 301 0.09113536775112152 2.2241082191467285 0.1875\n",
            "time: 2.212926149368286 2.0073299107962095\n",
            "i, sloss, closs, correct 302 0.16724912822246552 2.1625494956970215 0.234375\n",
            "time: 2.2807230949401855 2.0082334346897137\n",
            "i, sloss, closs, correct 303 0.11494038999080658 2.219668388366699 0.25\n",
            "time: 1.8912689685821533 2.007850139548904\n",
            "i, sloss, closs, correct 304 0.13010384142398834 2.1425576210021973 0.203125\n",
            "time: 1.8853907585144043 2.007450962848351\n",
            "i, sloss, closs, correct 305 0.09289765357971191 2.149719715118408 0.34375\n",
            "time: 1.8885247707366943 2.0070639264349843\n",
            "i, sloss, closs, correct 306 0.11547265946865082 2.156867504119873 0.21875\n",
            "time: 1.8572635650634766 2.0065774894304305\n",
            "i, sloss, closs, correct 307 0.1000998467206955 2.141939163208008 0.203125\n",
            "time: 2.0587191581726074 2.0067494728348474\n",
            "i, sloss, closs, correct 308 0.14354686439037323 2.1476409435272217 0.234375\n",
            "time: 2.455444574356079 2.008202772696041\n",
            "i, sloss, closs, correct 309 0.10572923719882965 2.1807327270507812 0.25\n",
            "time: 1.9270410537719727 2.0079424981148013\n",
            "i, sloss, closs, correct 310 0.11910511553287506 2.1794869899749756 0.25\n",
            "time: 1.8698673248291016 2.007500249856538\n",
            "i, sloss, closs, correct 311 0.11071636527776718 2.1390562057495117 0.25\n",
            "time: 1.8814983367919922 2.007097928187786\n",
            "i, sloss, closs, correct 312 0.10171795636415482 2.16817569732666 0.359375\n",
            "time: 1.8807005882263184 2.0066956087423207\n",
            "i, sloss, closs, correct 313 0.10495822876691818 2.1859214305877686 0.21875\n",
            "time: 1.8784661293029785 2.006288691690773\n",
            "i, sloss, closs, correct 314 0.10277890413999557 2.195932388305664 0.28125\n",
            "time: 2.5214345455169678 2.0079255656590536\n",
            "i, sloss, closs, correct 315 0.1357397884130478 2.176539421081543 0.203125\n",
            "time: 1.8973867893218994 2.0075772495209416\n",
            "i, sloss, closs, correct 316 0.10339988768100739 2.116771936416626 0.1875\n",
            "time: 1.8791601657867432 2.0071736277089887\n",
            "i, sloss, closs, correct 317 0.1274186074733734 2.141525983810425 0.203125\n",
            "time: 1.8960540294647217 2.0068310926545343\n",
            "i, sloss, closs, correct 318 0.10754687339067459 2.1620566844940186 0.25\n",
            "time: 1.8705427646636963 2.0064053759679523\n",
            "i, sloss, closs, correct 319 0.17356976866722107 2.1594786643981934 0.296875\n",
            "time: 1.8690831661224365 2.005977936834097\n",
            "i, sloss, closs, correct 320 0.10979252308607101 2.1161789894104004 0.25\n",
            "time: 2.293170213699341 2.0068742151943693\n",
            "i, sloss, closs, correct 321 0.11462332308292389 2.1614298820495605 0.234375\n",
            "time: 2.08203125 2.007108809785073\n",
            "i, sloss, closs, correct 322 0.10284410417079926 2.1805520057678223 0.15625\n",
            "time: 1.8698642253875732 2.0066854296823036\n",
            "i, sloss, closs, correct 323 0.12939028441905975 2.2218027114868164 0.265625\n",
            "time: 1.9152905941009521 2.0064047619148537\n",
            "i, sloss, closs, correct 324 0.13207921385765076 2.2028908729553223 0.171875\n",
            "time: 1.8404099941253662 2.005895485511193\n",
            "i, sloss, closs, correct 325 0.12922099232673645 2.168691635131836 0.28125\n",
            "time: 1.8684239387512207 2.005475242445074\n",
            "i, sloss, closs, correct 326 0.1549237221479416 2.1591320037841797 0.21875\n",
            "time: 2.1103389263153076 2.005797380336563\n",
            "i, sloss, closs, correct 327 0.11412092298269272 2.2106244564056396 0.1875\n",
            "time: 2.2515101432800293 2.006547642917168\n",
            "i, sloss, closs, correct 328 0.12388831377029419 2.214301586151123 0.28125\n",
            "time: 1.9026329517364502 2.0062332059112364\n",
            "i, sloss, closs, correct 329 0.12816599011421204 2.1663272380828857 0.265625\n",
            "time: 1.8766343593597412 2.005842017404961\n",
            "i, sloss, closs, correct 330 0.11656767129898071 2.194931983947754 0.265625\n",
            "time: 1.8544683456420898 2.005386090350655\n",
            "i, sloss, closs, correct 331 0.13372156023979187 2.148866653442383 0.171875\n",
            "time: 1.9655957221984863 2.005267669637519\n",
            "i, sloss, closs, correct 332 0.11271350830793381 2.0895416736602783 0.25\n",
            "time: 1.9632513523101807 2.0051438579330214\n",
            "i, sloss, closs, correct 333 0.15361063182353973 2.1833443641662598 0.203125\n",
            "time: 2.4107372760772705 2.0063593694549833\n",
            "i, sloss, closs, correct 334 0.1240672692656517 2.1783783435821533 0.3125\n",
            "time: 1.8911027908325195 2.0060168180892717\n",
            "i, sloss, closs, correct 335 0.14516808092594147 2.1741647720336914 0.265625\n",
            "time: 1.9069104194641113 2.005724315132414\n",
            "i, sloss, closs, correct 336 0.1066509485244751 2.2112975120544434 0.25\n",
            "time: 1.8800392150878906 2.0053527843350825\n",
            "i, sloss, closs, correct 337 0.13529223203659058 2.134197235107422 0.25\n",
            "time: 1.8800115585327148 2.004983277715875\n",
            "i, sloss, closs, correct 338 0.13979777693748474 2.168191432952881 0.21875\n",
            "time: 1.8696322441101074 2.0045854349051955\n",
            "i, sloss, closs, correct 339 0.13832387328147888 2.2171573638916016 0.21875\n",
            "time: 2.421700954437256 2.0058135684798746\n",
            "i, sloss, closs, correct 340 0.14319507777690887 2.180906057357788 0.21875\n",
            "time: 1.976252794265747 2.0057326824434343\n",
            "i, sloss, closs, correct 341 0.12130583822727203 2.215524673461914 0.234375\n",
            "time: 1.8988258838653564 2.0054214349267077\n",
            "i, sloss, closs, correct 342 0.10659285634756088 2.207818031311035 0.28125\n",
            "time: 1.8676722049713135 2.0050221017776355\n",
            "i, sloss, closs, correct 343 0.15215466916561127 2.1544060707092285 0.171875\n",
            "time: 1.8700346946716309 2.0046311866405397\n",
            "i, sloss, closs, correct 344 0.128017395734787 2.230961799621582 0.234375\n",
            "time: 1.8930432796478271 2.0043091932932535\n",
            "i, sloss, closs, correct 345 0.1306961476802826 2.184257745742798 0.296875\n",
            "time: 2.2360639572143555 2.0049820690485785\n",
            "i, sloss, closs, correct 346 0.10085819661617279 2.177635669708252 0.203125\n",
            "time: 2.162877321243286 2.005438149834229\n",
            "i, sloss, closs, correct 347 0.11551310867071152 2.1753525733947754 0.203125\n",
            "time: 1.940610408782959 2.0052542830335685\n",
            "i, sloss, closs, correct 348 0.1317974179983139 2.161454200744629 0.21875\n",
            "time: 1.8653857707977295 2.004855555586282\n",
            "i, sloss, closs, correct 349 0.1848989874124527 2.225902557373047 0.125\n",
            "time: 1.8489105701446533 2.004411361558097\n",
            "i, sloss, closs, correct 350 0.15675778687000275 2.180197238922119 0.203125\n",
            "time: 1.8695294857025146 2.004028484692261\n",
            "i, sloss, closs, correct 351 0.1820553094148636 2.1997804641723633 0.15625\n",
            "time: 2.1246702671051025 2.00437261367386\n",
            "i, sloss, closs, correct 352 0.13410423696041107 2.1797380447387695 0.25\n",
            "time: 2.2841663360595703 2.005167224927259\n",
            "i, sloss, closs, correct 353 0.14022698998451233 2.208055019378662 0.15625\n",
            "time: 1.9160981178283691 2.004916978421184\n",
            "i, sloss, closs, correct 354 0.16666463017463684 2.122225522994995 0.21875\n",
            "time: 1.8926877975463867 2.004602218010056\n",
            "i, sloss, closs, correct 355 0.11742698401212692 2.195127010345459 0.140625\n",
            "time: 1.8850758075714111 2.004267797041475\n",
            "i, sloss, closs, correct 356 0.10076873749494553 2.1510379314422607 0.21875\n",
            "time: 1.8705403804779053 2.00389520460818\n",
            "i, sloss, closs, correct 357 0.13341861963272095 2.177868366241455 0.21875\n",
            "time: 1.9679186344146729 2.0037968265277715\n",
            "i, sloss, closs, correct 358 0.13041828572750092 2.2233388423919678 0.28125\n",
            "time: 2.4192111492156982 2.0049579249783145\n",
            "i, sloss, closs, correct 359 0.16488555073738098 2.142712354660034 0.15625\n",
            "time: 1.9096486568450928 2.004694610171848\n",
            "i, sloss, closs, correct 360 0.16138537228107452 2.186605453491211 0.234375\n",
            "time: 1.8928437232971191 2.004386994977407\n",
            "i, sloss, closs, correct 361 0.14029096066951752 2.162104845046997 0.203125\n",
            "time: 1.9154057502746582 2.0041424813191533\n",
            "i, sloss, closs, correct 362 0.11658734828233719 2.210813522338867 0.21875\n",
            "time: 1.8813159465789795 2.0038060649367404\n",
            "i, sloss, closs, correct 363 0.15119916200637817 2.213052749633789 0.296875\n",
            "time: 1.8924834728240967 2.003501548216893\n",
            "i, sloss, closs, correct 364 0.12710058689117432 2.158287763595581 0.21875\n",
            "time: 2.437546730041504 2.004691996639722\n",
            "i, sloss, closs, correct 365 0.14331313967704773 2.241436719894409 0.21875\n",
            "time: 1.98836350440979 2.0046488561265456\n",
            "i, sloss, closs, correct 366 0.13957011699676514 2.2133517265319824 0.25\n",
            "time: 1.9098100662231445 2.0043918066518507\n",
            "i, sloss, closs, correct 367 0.11358612030744553 2.160737991333008 0.296875\n",
            "time: 1.8387115001678467 2.0039445090553034\n",
            "i, sloss, closs, correct 368 0.14793391525745392 2.1824851036071777 0.25\n",
            "time: 1.8864972591400146 2.0036274889297276\n",
            "i, sloss, closs, correct 369 0.09943653643131256 2.1949682235717773 0.328125\n",
            "time: 1.9090723991394043 2.0033731872970995\n",
            "i, sloss, closs, correct 370 0.13465295732021332 2.189972162246704 0.28125\n",
            "time: 2.265460252761841 2.004081205537698\n",
            "i, sloss, closs, correct 371 0.17436248064041138 2.210833787918091 0.234375\n",
            "time: 2.1792964935302734 2.0045563181241355\n",
            "i, sloss, closs, correct 372 0.11123435199260712 2.177473783493042 0.171875\n",
            "time: 1.9003360271453857 2.004278218138953\n",
            "i, sloss, closs, correct 373 0.17105455696582794 2.2257800102233887 0.25\n",
            "time: 1.8659095764160156 2.0039095132746163\n",
            "i, sloss, closs, correct 374 0.12973466515541077 2.174713134765625 0.21875\n",
            "time: 1.898404836654663 2.0036294600168865\n",
            "i, sloss, closs, correct 375 0.1634649783372879 2.202087163925171 0.140625\n",
            "time: 1.8804755210876465 2.0033042843037463\n",
            "i, sloss, closs, correct 376 0.11843797564506531 2.2211248874664307 0.125\n",
            "time: 2.121030807495117 2.003617812847269\n",
            "i, sloss, closs, correct 377 0.16642004251480103 2.2434768676757812 0.203125\n",
            "time: 2.3139724731445312 2.004444230170477\n",
            "i, sloss, closs, correct 378 0.15773674845695496 2.197733163833618 0.1875\n",
            "time: 1.8818471431732178 2.0041253856115415\n",
            "i, sloss, closs, correct 379 0.11501366645097733 2.1540493965148926 0.1875\n",
            "time: 1.8688693046569824 2.0037706142977663\n",
            "i, sloss, closs, correct 380 0.12587696313858032 2.189650058746338 0.171875\n",
            "time: 1.8600819110870361 2.003399718777714\n",
            "i, sloss, closs, correct 381 0.11248639225959778 2.21836519241333 0.1875\n",
            "time: 1.8670427799224854 2.0030440216913274\n",
            "i, sloss, closs, correct 382 0.1577964425086975 2.1726839542388916 0.328125\n",
            "time: 1.9307262897491455 2.0028563989982904\n",
            "i, sloss, closs, correct 383 0.15677884221076965 2.1482884883880615 0.203125\n",
            "time: 2.4903736114501953 2.0041281692683697\n",
            "i, sloss, closs, correct 384 0.16798101365566254 2.1562037467956543 0.125\n",
            "time: 1.8985397815704346 2.0038553615669152\n",
            "i, sloss, closs, correct 385 0.14367367327213287 2.224637508392334 0.140625\n",
            "time: 1.8638195991516113 2.003493843300973\n",
            "i, sloss, closs, correct 386 0.1285082846879959 2.2307424545288086 0.234375\n",
            "time: 1.8815600872039795 2.003180612889371\n",
            "i, sloss, closs, correct 387 0.15421733260154724 2.2067689895629883 0.375\n",
            "time: 1.9223377704620361 2.0029742385923246\n",
            "i, sloss, closs, correct 388 0.1464366614818573 2.210869312286377 0.21875\n",
            "time: 1.831517219543457 2.0025347511075764\n",
            "i, sloss, closs, correct 389 0.11748353391885757 2.216533660888672 0.234375\n",
            "time: 2.452941417694092 2.0036908797728707\n",
            "i, sloss, closs, correct 390 0.12190540879964828 2.159330368041992 0.1875\n",
            "time: 2.0606613159179688 2.003838767176089\n",
            "i, sloss, closs, correct 391 0.15968860685825348 2.18833065032959 0.1875\n",
            "time: 1.9361183643341064 2.0036672554454027\n",
            "i, sloss, closs, correct 392 0.19577203691005707 2.2433886528015137 0.234375\n",
            "time: 1.9434404373168945 2.003515895693053\n",
            "i, sloss, closs, correct 393 0.11628254503011703 2.244020462036133 0.28125\n",
            "time: 1.9111804962158203 2.003282813251321\n",
            "i, sloss, closs, correct 394 0.1244279071688652 2.173140048980713 0.171875\n",
            "time: 1.8704686164855957 2.00294777049294\n",
            "i, sloss, closs, correct 395 0.1886049211025238 2.177349328994751 0.171875\n",
            "time: 2.2434425354003906 2.0035562750064964\n",
            "i, sloss, closs, correct 396 0.1287236362695694 2.1768908500671387 0.125\n",
            "time: 2.1356234550476074 2.0038899534595402\n",
            "i, sloss, closs, correct 397 0.17541536688804626 2.189735174179077 0.203125\n",
            "time: 1.8901073932647705 2.003605296264342\n",
            "i, sloss, closs, correct 398 0.13459233939647675 2.186605453491211 0.140625\n",
            "time: 1.892942190170288 2.003329244771398\n",
            "i, sloss, closs, correct 399 0.12568898499011993 2.1735095977783203 0.1875\n",
            "time: 1.918102741241455 2.0031173545122147\n",
            "i, sloss, closs, correct 400 0.14124619960784912 2.175525665283203 0.3125\n",
            "time: 2.233215808868408 2.0036932429173344\n",
            "i, sloss, closs, correct 401 0.16771548986434937 2.2371017932891846 0.15625\n",
            "time: 2.2676401138305664 2.004351038244826\n",
            "i, sloss, closs, correct 402 0.18300370872020721 2.1550512313842773 0.25\n",
            "time: 2.1646206378936768 2.0047498157539083\n",
            "i, sloss, closs, correct 403 0.13233962655067444 2.175091505050659 0.234375\n",
            "time: 1.925645112991333 2.0045552295033295\n",
            "i, sloss, closs, correct 404 0.14209698140621185 2.2028279304504395 0.21875\n",
            "time: 1.901216983795166 2.0043012736756123\n",
            "i, sloss, closs, correct 405 0.18559139966964722 2.166830539703369 0.25\n",
            "time: 1.892287015914917 2.004026713042424\n",
            "i, sloss, closs, correct 406 0.19119244813919067 2.1510438919067383 0.203125\n",
            "time: 1.8890256881713867 2.0037453420624978\n",
            "i, sloss, closs, correct 407 0.14050926268100739 2.158066987991333 0.28125\n",
            "time: 2.191877841949463 2.0042076531578514\n",
            "i, sloss, closs, correct 408 0.1479857712984085 2.1810665130615234 0.1875\n",
            "time: 2.271674633026123 2.0048625754843714\n",
            "i, sloss, closs, correct 409 0.1603260040283203 2.1650643348693848 0.25\n",
            "time: 1.8813841342926025 2.004562611114688\n",
            "i, sloss, closs, correct 410 0.13941684365272522 2.157395362854004 0.203125\n",
            "time: 1.9094996452331543 2.0043328829345333\n",
            "i, sloss, closs, correct 411 0.1832091361284256 2.2338480949401855 0.15625\n",
            "time: 1.874861717224121 2.0040198217317897\n",
            "i, sloss, closs, correct 412 0.17052587866783142 2.1909589767456055 0.21875\n",
            "time: 1.9609651565551758 2.003916743303904\n",
            "i, sloss, closs, correct 413 0.16960160434246063 2.219949960708618 0.234375\n",
            "time: 2.0576508045196533 2.0040476708020565\n",
            "i, sloss, closs, correct 414 0.12633521854877472 2.194514274597168 0.21875\n",
            "time: 2.442357063293457 2.0051051363887558\n",
            "i, sloss, closs, correct 415 0.15688763558864594 2.1619458198547363 0.265625\n",
            "time: 1.9048585891723633 2.004866561637475\n",
            "i, sloss, closs, correct 416 0.13064774870872498 2.1960103511810303 0.3125\n",
            "time: 1.9175286293029785 2.004658286805919\n",
            "i, sloss, closs, correct 417 0.14150428771972656 2.2033748626708984 0.21875\n",
            "time: 1.939201831817627 2.004504854028875\n",
            "i, sloss, closs, correct 418 0.14160358905792236 2.1307239532470703 0.265625\n",
            "time: 1.9117064476013184 2.004284607197618\n",
            "i, sloss, closs, correct 419 0.12938269972801208 2.2095210552215576 0.234375\n",
            "time: 1.9731817245483398 2.0042117010979426\n",
            "i, sloss, closs, correct 420 0.1437092274427414 2.1422338485717773 0.171875\n",
            "time: 2.527778148651123 2.0054565387780197\n",
            "i, sloss, closs, correct 421 0.15322723984718323 2.1630332469940186 0.234375\n",
            "time: 1.9550361633300781 2.005338696506916\n",
            "i, sloss, closs, correct 422 0.16596829891204834 2.228684663772583 0.28125\n",
            "time: 1.9284422397613525 2.005160764599523\n",
            "i, sloss, closs, correct 423 0.16531825065612793 2.230693817138672 0.265625\n",
            "time: 1.9360849857330322 2.0049989735180476\n",
            "i, sloss, closs, correct 424 0.14375944435596466 2.1795477867126465 0.1875\n",
            "time: 1.8942396640777588 2.004739497128655\n",
            "i, sloss, closs, correct 425 0.14200563728809357 2.21526837348938 0.25\n",
            "time: 1.8728735446929932 2.0044311206665397\n",
            "i, sloss, closs, correct 426 0.1384037286043167 2.190749168395996 0.203125\n",
            "time: 2.4534661769866943 2.005483859037627\n",
            "i, sloss, closs, correct 427 0.19817598164081573 2.185072422027588 0.265625\n",
            "time: 1.9814095497131348 2.0054327299661727\n",
            "i, sloss, closs, correct 428 0.12000162899494171 2.2114779949188232 0.25\n",
            "time: 1.972978115081787 2.0053584853530206\n",
            "i, sloss, closs, correct 429 0.1575780212879181 2.1900556087493896 0.25\n",
            "time: 1.8793118000030518 2.0050665001536525\n",
            "i, sloss, closs, correct 430 0.14265310764312744 2.1474196910858154 0.21875\n",
            "time: 1.8815279006958008 2.0047809646190458\n",
            "i, sloss, closs, correct 431 0.11215797811746597 2.190279006958008 0.125\n",
            "time: 1.8812789916992188 2.0044962813456855\n",
            "i, sloss, closs, correct 432 0.1283232420682907 2.1904854774475098 0.234375\n",
            "time: 2.2989394664764404 2.005178093084287\n",
            "i, sloss, closs, correct 433 0.15434853732585907 2.221888542175293 0.25\n",
            "time: 2.1534817218780518 2.005520733270777\n",
            "i, sloss, closs, correct 434 0.1576845496892929 2.1729328632354736 0.203125\n",
            "time: 1.9256477355957031 2.0053383509318032\n",
            "i, sloss, closs, correct 435 0.14234665036201477 2.2002553939819336 0.25\n",
            "time: 1.8738739490509033 2.0050380366657854\n",
            "i, sloss, closs, correct 436 0.19154895842075348 2.1947038173675537 0.125\n",
            "time: 1.9135174751281738 2.004829714991135\n",
            "i, sloss, closs, correct 437 0.22570133209228516 2.244157314300537 0.28125\n",
            "time: 1.8853874206542969 2.004558962774059\n",
            "i, sloss, closs, correct 438 0.17645962536334991 2.2082653045654297 0.28125\n",
            "time: 2.2029061317443848 2.005011876243122\n",
            "i, sloss, closs, correct 439 0.2079802006483078 2.180668354034424 0.25\n",
            "time: 2.2036447525024414 2.00546423738653\n",
            "i, sloss, closs, correct 440 0.14416228234767914 2.199089288711548 0.28125\n",
            "time: 1.9149150848388672 2.0052600479990987\n",
            "i, sloss, closs, correct 441 0.15756909549236298 2.1734812259674072 0.15625\n",
            "time: 1.8885693550109863 2.004997079728416\n",
            "i, sloss, closs, correct 442 0.22791099548339844 2.201028823852539 0.25\n",
            "time: 1.8415336608886719 2.004629197561983\n",
            "i, sloss, closs, correct 443 0.15620510280132294 2.180752754211426 0.296875\n",
            "time: 1.9167146682739258 2.004432292671891\n",
            "i, sloss, closs, correct 444 0.15548646450042725 2.2021355628967285 0.1875\n",
            "time: 2.0375912189483643 2.0045080120643872\n",
            "i, sloss, closs, correct 445 0.1778886467218399 2.18162202835083 0.359375\n",
            "time: 2.370030403137207 2.0053293870703524\n",
            "i, sloss, closs, correct 446 0.1890176385641098 2.215970039367676 0.3125\n",
            "time: 1.900437831878662 2.005095911239351\n",
            "i, sloss, closs, correct 447 0.23696710169315338 2.2153403759002686 0.296875\n",
            "time: 1.8542110919952393 2.0047609428209916\n",
            "i, sloss, closs, correct 448 0.22047114372253418 2.2029762268066406 0.234375\n",
            "time: 1.8605620861053467 2.0044409405680703\n",
            "i, sloss, closs, correct 449 0.1671571284532547 2.200416088104248 0.1875\n",
            "time: 1.863645076751709 2.004129169782003\n",
            "i, sloss, closs, correct 450 0.20214416086673737 2.2411646842956543 0.09375\n",
            "time: 1.8952031135559082 2.0038894417544957\n",
            "i, sloss, closs, correct 451 0.14940166473388672 2.1934139728546143 0.25\n",
            "time: 2.497520685195923 2.0049826355107063\n",
            "i, sloss, closs, correct 452 0.14384044706821442 2.191559076309204 0.28125\n",
            "time: 1.9389724731445312 2.0048407187261854\n",
            "i, sloss, closs, correct 453 0.17048300802707672 2.161280632019043 0.25\n",
            "time: 1.9019875526428223 2.004615219679173\n",
            "i, sloss, closs, correct 454 0.18742898106575012 2.175157070159912 0.265625\n",
            "time: 1.8850622177124023 2.004353541594285\n",
            "i, sloss, closs, correct 455 0.1461009979248047 2.2362022399902344 0.171875\n",
            "time: 1.8601794242858887 2.004038486041521\n",
            "i, sloss, closs, correct 456 0.19570472836494446 2.2107250690460205 0.28125\n",
            "time: 1.8786919116973877 2.003765229323239\n",
            "i, sloss, closs, correct 457 0.16641953587532043 2.195183038711548 0.203125\n",
            "time: 2.255547046661377 2.004318542876098\n",
            "i, sloss, closs, correct 458 0.1810215711593628 2.1935606002807617 0.265625\n",
            "time: 2.1564362049102783 2.0046508452471565\n",
            "i, sloss, closs, correct 459 0.15971310436725616 2.1869454383850098 0.234375\n",
            "time: 1.92171311378479 2.0044715611831\n",
            "i, sloss, closs, correct 460 0.19039800763130188 2.188478469848633 0.328125\n",
            "time: 1.891869068145752 2.00422834263965\n",
            "i, sloss, closs, correct 461 0.17257364094257355 2.2062742710113525 0.203125\n",
            "time: 1.8691730499267578 2.0039370163178547\n",
            "i, sloss, closs, correct 462 0.2151012122631073 2.1863327026367188 0.328125\n",
            "time: 1.9030187129974365 2.003720799996065\n",
            "i, sloss, closs, correct 463 0.18002949655056 2.1793365478515625 0.265625\n",
            "time: 2.1538727283477783 2.0040454694937013\n",
            "i, sloss, closs, correct 464 0.1615755707025528 2.164968252182007 0.15625\n",
            "time: 2.2583425045013428 2.0045932872321015\n",
            "i, sloss, closs, correct 465 0.17113831639289856 2.194225788116455 0.203125\n",
            "time: 1.8837344646453857 2.004335067814512\n",
            "i, sloss, closs, correct 466 0.1498284488916397 2.2331008911132812 0.171875\n",
            "time: 1.9116952419281006 2.004137739634871\n",
            "i, sloss, closs, correct 467 0.2525518536567688 2.2108592987060547 0.171875\n",
            "time: 1.9112002849578857 2.00394089303465\n",
            "i, sloss, closs, correct 468 0.15814459323883057 2.210742473602295 0.21875\n",
            "time: 1.9013876914978027 2.003723326014049\n",
            "i, sloss, closs, correct 469 0.14694486558437347 2.1894640922546387 0.234375\n",
            "time: 2.0134387016296387 2.0037450338931793\n",
            "i, sloss, closs, correct 470 0.16603867709636688 2.1821017265319824 0.265625\n",
            "time: 2.410236120223999 2.0046102499506278\n",
            "i, sloss, closs, correct 471 0.1600119173526764 2.1997146606445312 0.25\n",
            "time: 1.863640308380127 2.0043125930479015\n",
            "i, sloss, closs, correct 472 0.18375521898269653 2.1545653343200684 0.28125\n",
            "time: 1.8827309608459473 2.0040565595566346\n",
            "i, sloss, closs, correct 473 0.14194627106189728 2.1969070434570312 0.234375\n",
            "time: 1.9273464679718018 2.0038957465047074\n",
            "i, sloss, closs, correct 474 0.15081824362277985 2.118772506713867 0.234375\n",
            "time: 1.9199581146240234 2.0037201083333867\n",
            "i, sloss, closs, correct 475 0.21536646783351898 2.202761650085449 0.234375\n",
            "time: 1.8850843906402588 2.0034725615958204\n",
            "i, sloss, closs, correct 476 0.15471003949642181 2.226132869720459 0.25\n",
            "time: 2.516350269317627 2.0045488225589008\n",
            "i, sloss, closs, correct 477 0.16602276265621185 2.234527349472046 0.25\n",
            "time: 1.8880062103271484 2.0043060904267445\n",
            "i, sloss, closs, correct 478 0.13549675047397614 2.193443775177002 0.265625\n",
            "time: 1.8762485980987549 2.0040398017349723\n",
            "i, sloss, closs, correct 479 0.1378956437110901 2.2242720127105713 0.234375\n",
            "time: 1.8953418731689453 2.0038143371542296\n",
            "i, sloss, closs, correct 480 0.17453919351100922 2.1728920936584473 0.15625\n",
            "time: 1.8733584880828857 2.003544175451362\n",
            "i, sloss, closs, correct 481 0.171453058719635 2.1848361492156982 0.171875\n",
            "time: 1.8830556869506836 2.00329520445147\n",
            "i, sloss, closs, correct 482 0.2317875176668167 2.1792821884155273 0.25\n",
            "time: 2.3323471546173096 2.0039774892739874\n",
            "i, sloss, closs, correct 483 0.1454535275697708 2.1784732341766357 0.359375\n",
            "time: 2.1500606536865234 2.004282544959675\n",
            "i, sloss, closs, correct 484 0.25550106167793274 2.182997941970825 0.25\n",
            "time: 1.885183334350586 2.004037996174134\n",
            "i, sloss, closs, correct 485 0.17194029688835144 2.196145534515381 0.3125\n",
            "time: 1.883049488067627 2.0037900268295665\n",
            "i, sloss, closs, correct 486 0.21720187366008759 2.1969680786132812 0.203125\n",
            "time: 1.8811287879943848 2.003539200436163\n",
            "i, sloss, closs, correct 487 0.21678094565868378 2.2383227348327637 0.296875\n",
            "time: 1.872225046157837 2.0032710989967724\n",
            "i, sloss, closs, correct 488 0.15078265964984894 2.167947292327881 0.21875\n",
            "time: 2.1963517665863037 2.0036669684334036\n",
            "i, sloss, closs, correct 489 0.17242851853370667 2.203798770904541 0.25\n",
            "time: 2.239720582962036 2.004149625739273\n",
            "i, sloss, closs, correct 490 0.17140237987041473 2.1961708068847656 0.171875\n",
            "time: 1.8956377506256104 2.003930204026325\n",
            "i, sloss, closs, correct 491 0.15005730092525482 2.1601462364196777 0.125\n",
            "time: 1.891005516052246 2.003701742587051\n",
            "i, sloss, closs, correct 492 0.1697758585214615 2.1977243423461914 0.125\n",
            "time: 1.8866722583770752 2.0034660001797318\n",
            "i, sloss, closs, correct 493 0.18615655601024628 2.1916425228118896 0.296875\n",
            "time: 1.874605655670166 2.0032061999626007\n",
            "i, sloss, closs, correct 494 0.1678256392478943 2.136521816253662 0.203125\n",
            "time: 2.0344207286834717 2.00327022388728\n",
            "i, sloss, closs, correct 495 0.2687382102012634 2.1769471168518066 0.28125\n",
            "time: 2.370636463165283 2.0040116579301896\n",
            "i, sloss, closs, correct 496 0.22825764119625092 2.2270402908325195 0.3125\n",
            "time: 1.9100456237792969 2.003823561447727\n",
            "i, sloss, closs, correct 497 0.2521076798439026 2.2223894596099854 0.265625\n",
            "time: 1.961808681488037 2.0037401828421166\n",
            "i, sloss, closs, correct 498 0.16313551366329193 2.1643810272216797 0.28125\n",
            "time: 1.8814218044281006 2.003495980838019\n",
            "i, sloss, closs, correct 499 0.20092473924160004 2.193988800048828 0.21875\n",
            "time: 1.865572214126587 2.0032211108207703\n",
            "i, sloss, closs, correct 500 0.16207337379455566 2.1842870712280273 0.109375\n",
            "time: 1.8805313110351562 2.0029771518326567\n",
            "i, sloss, closs, correct 501 0.1820775717496872 2.175342559814453 0.234375\n",
            "time: 2.517451763153076 2.004002905461893\n",
            "i, sloss, closs, correct 502 0.17744781076908112 2.2203845977783203 0.28125\n",
            "time: 1.9304702281951904 2.0038580562674975\n",
            "i, sloss, closs, correct 503 0.1943770796060562 2.135864734649658 0.265625\n",
            "time: 1.9204580783843994 2.003693606172289\n",
            "i, sloss, closs, correct 504 0.18538135290145874 2.200230121612549 0.25\n",
            "time: 1.8879969120025635 2.0034655934513204\n",
            "i, sloss, closs, correct 505 0.21310535073280334 2.1417486667633057 0.1875\n",
            "time: 1.876908540725708 2.0032171307816338\n",
            "i, sloss, closs, correct 506 0.1873946338891983 2.1628832817077637 0.234375\n",
            "time: 1.8847720623016357 2.0029845091953313\n",
            "i, sloss, closs, correct 507 0.22912028431892395 2.2380995750427246 0.265625\n",
            "time: 2.3306000232696533 2.003633120867211\n",
            "i, sloss, closs, correct 508 0.20842137932777405 2.169949769973755 0.296875\n",
            "time: 2.090036392211914 2.003803514544294\n",
            "i, sloss, closs, correct 509 0.16431425511837006 2.2014970779418945 0.1875\n",
            "time: 1.8876800537109375 2.0035768144270953\n",
            "i, sloss, closs, correct 510 0.17344127595424652 2.1799187660217285 0.3125\n",
            "time: 1.8881301879882812 2.0033519869914493\n",
            "i, sloss, closs, correct 511 0.22035011649131775 2.187920331954956 0.265625\n",
            "time: 1.884375810623169 2.003120551817119\n",
            "i, sloss, closs, correct 512 0.20203858613967896 2.1998186111450195 0.1875\n",
            "time: 1.9306161403656006 2.002980767634877\n",
            "i, sloss, closs, correct 513 0.24180710315704346 2.1814115047454834 0.234375\n",
            "time: 2.2091739177703857 2.003382904984144\n",
            "i, sloss, closs, correct 514 0.2027929425239563 2.197061061859131 0.15625\n",
            "time: 2.19610857963562 2.0037578911457246\n",
            "i, sloss, closs, correct 515 0.19798222184181213 2.2169761657714844 0.125\n",
            "time: 1.8867342472076416 2.0035320866939634\n",
            "i, sloss, closs, correct 516 0.16772782802581787 2.197427272796631 0.296875\n",
            "time: 1.9104278087615967 2.003352936739156\n",
            "i, sloss, closs, correct 517 0.17852213978767395 2.185180425643921 0.28125\n",
            "time: 1.8923518657684326 2.0031395874428473\n",
            "i, sloss, closs, correct 518 0.1962040662765503 2.1981170177459717 0.21875\n",
            "time: 1.8939361572265625 2.002930143665027\n",
            "i, sloss, closs, correct 519 0.18102452158927917 2.205108642578125 0.203125\n",
            "time: 2.129676103591919 2.0031748436964474\n",
            "i, sloss, closs, correct 520 0.16304515302181244 2.2104785442352295 0.1875\n",
            "time: 2.3566536903381348 2.0038541118380206\n",
            "i, sloss, closs, correct 521 0.18321990966796875 2.1569032669067383 0.234375\n",
            "time: 1.8798539638519287 2.003617525100708\n",
            "i, sloss, closs, correct 522 0.19472016394138336 2.1966450214385986 0.328125\n",
            "time: 1.8604326248168945 2.003344707233728\n",
            "i, sloss, closs, correct 523 0.27007198333740234 2.2008750438690186 0.265625\n",
            "time: 1.8806092739105225 2.00311143889682\n",
            "i, sloss, closs, correct 524 0.23508885502815247 2.202277183532715 0.28125\n",
            "time: 1.9027247428894043 2.0029211371285576\n",
            "i, sloss, closs, correct 525 0.20157043635845184 2.186591148376465 0.25\n",
            "time: 1.9887542724609375 2.0028951136331594\n",
            "i, sloss, closs, correct 526 0.16463574767112732 2.2217655181884766 0.265625\n",
            "time: 2.4293160438537598 2.0037049962175866\n",
            "i, sloss, closs, correct 527 0.18006868660449982 2.1992685794830322 0.34375\n",
            "time: 1.9230408668518066 2.003553731874986\n",
            "i, sloss, closs, correct 528 0.25030168890953064 2.2198715209960938 0.21875\n",
            "time: 1.8928196430206299 2.003345277223785\n",
            "i, sloss, closs, correct 529 0.2042129635810852 2.192082643508911 0.265625\n",
            "time: 1.8983566761016846 2.0031481009609293\n",
            "i, sloss, closs, correct 530 0.24108636379241943 2.1823947429656982 0.328125\n",
            "time: 1.8764104843139648 2.002910975907033\n",
            "i, sloss, closs, correct 531 0.2032233327627182 2.168452501296997 0.1875\n",
            "time: 1.8747854232788086 2.0026711207583436\n",
            "i, sloss, closs, correct 532 0.25604864954948425 2.2125790119171143 0.390625\n",
            "time: 2.37591290473938 2.003372823096127\n",
            "i, sloss, closs, correct 533 0.2954951226711273 2.162421703338623 0.265625\n",
            "time: 2.1033990383148193 2.003560843092672\n",
            "i, sloss, closs, correct 534 0.1691141277551651 2.187835454940796 0.3125\n",
            "time: 1.9133632183074951 2.003393115729929\n",
            "i, sloss, closs, correct 535 0.19509105384349823 2.1702969074249268 0.21875\n",
            "time: 1.882094144821167 2.0031684063263793\n",
            "i, sloss, closs, correct 536 0.21376387774944305 2.188441038131714 0.203125\n",
            "time: 1.8900010585784912 2.002958532152229\n",
            "i, sloss, closs, correct 537 0.18968115746974945 2.2200584411621094 0.25\n",
            "time: 1.8996729850769043 2.002767452963223\n",
            "i, sloss, closs, correct 538 0.18525683879852295 2.220534563064575 0.171875\n",
            "time: 2.273620367050171 2.0032709485303495\n",
            "i, sloss, closs, correct 539 0.2247825264930725 2.2010531425476074 0.25\n",
            "time: 2.1232643127441406 2.003493927584754\n",
            "i, sloss, closs, correct 540 0.1659230887889862 2.166321277618408 0.140625\n",
            "time: 1.876020908355713 2.0032592097404045\n",
            "i, sloss, closs, correct 541 0.2429109811782837 2.1858510971069336 0.15625\n",
            "time: 1.8966126441955566 2.003063303958006\n",
            "i, sloss, closs, correct 542 0.18608850240707397 2.2015254497528076 0.109375\n",
            "time: 1.9354865550994873 2.002940376819168\n",
            "i, sloss, closs, correct 543 0.27604857087135315 2.2036523818969727 0.234375\n",
            "time: 1.8646094799041748 2.002686940571841\n",
            "i, sloss, closs, correct 544 0.2140936553478241 2.162050724029541 0.265625\n",
            "time: 2.1326370239257812 2.002926431883366\n",
            "i, sloss, closs, correct 545 0.24402062594890594 2.1550581455230713 0.203125\n",
            "time: 2.255356788635254 2.0033897257549858\n",
            "i, sloss, closs, correct 546 0.2510605454444885 2.2095537185668945 0.203125\n",
            "time: 1.8956897258758545 2.0031937306280327\n",
            "i, sloss, closs, correct 547 0.2553972005844116 2.2193589210510254 0.28125\n",
            "time: 1.8858356475830078 2.002980498066784\n",
            "i, sloss, closs, correct 548 0.18146346509456635 2.176849365234375 0.265625\n",
            "time: 1.9137756824493408 2.0028189123219695\n",
            "i, sloss, closs, correct 549 0.20327195525169373 2.2350869178771973 0.234375\n",
            "time: 1.9073874950408936 2.002646271098744\n",
            "i, sloss, closs, correct 550 0.2902498245239258 2.1839327812194824 0.203125\n",
            "time: 1.981290340423584 2.0026098851067187\n",
            "i, sloss, closs, correct 551 0.174832284450531 2.1645803451538086 0.234375\n",
            "time: 2.4203054904937744 2.003371997156005\n",
            "i, sloss, closs, correct 552 0.2763918936252594 2.186095952987671 0.296875\n",
            "time: 1.909020185470581 2.0032023460912445\n",
            "i, sloss, closs, correct 553 0.18872907757759094 2.2023303508758545 0.21875\n",
            "time: 1.921055793762207 2.0030549648436398\n",
            "i, sloss, closs, correct 554 0.23108097910881042 2.192448139190674 0.28125\n",
            "time: 1.9216904640197754 2.002909227749249\n",
            "i, sloss, closs, correct 555 0.2366434782743454 2.177086353302002 0.3125\n",
            "time: 1.8971753120422363 2.0027208997191286\n",
            "i, sloss, closs, correct 556 0.21889840066432953 2.193715810775757 0.21875\n",
            "time: 1.890650749206543 2.002520570207113\n",
            "i, sloss, closs, correct 557 0.2972847819328308 2.224236488342285 0.265625\n",
            "time: 2.5040886402130127 2.003420881900309\n",
            "i, sloss, closs, correct 558 0.22449488937854767 2.20493745803833 0.3125\n",
            "time: 1.9294521808624268 2.0032911040488637\n",
            "i, sloss, closs, correct 559 0.2443237602710724 2.1737327575683594 0.28125\n",
            "time: 1.926238775253296 2.0031543872186117\n",
            "i, sloss, closs, correct 560 0.2146078497171402 2.1697402000427246 0.265625\n",
            "time: 1.9815340042114258 2.003116860959194\n",
            "i, sloss, closs, correct 561 0.3187086582183838 2.2051491737365723 0.25\n",
            "time: 1.8757469654083252 2.0028910641144178\n",
            "i, sloss, closs, correct 562 0.20072706043720245 2.2217979431152344 0.3125\n",
            "time: 1.8824920654296875 2.002678062733706\n",
            "i, sloss, closs, correct 563 0.20795170962810516 2.2121968269348145 0.1875\n",
            "time: 2.3268749713897705 2.00325371483539\n",
            "i, sloss, closs, correct 564 0.2928759455680847 2.152862548828125 0.125\n",
            "time: 2.0947823524475098 2.003416453842568\n",
            "i, sloss, closs, correct 565 0.22389216721057892 2.179960250854492 0.15625\n",
            "time: 1.8947019577026367 2.0032267321967403\n",
            "i, sloss, closs, correct 566 0.25177717208862305 2.1814005374908447 0.25\n",
            "time: 1.8990840911865234 2.003043940458348\n",
            "i, sloss, closs, correct 567 0.3124810457229614 2.1908440589904785 0.078125\n",
            "time: 1.8668780326843262 2.0028050978418808\n",
            "i, sloss, closs, correct 568 0.24833571910858154 2.2305140495300293 0.296875\n",
            "time: 1.8760707378387451 2.0025832342975796\n",
            "i, sloss, closs, correct 569 0.25243330001831055 2.2130446434020996 0.1875\n",
            "time: 2.1966230869293213 2.0029246459927474\n",
            "i, sloss, closs, correct 570 0.20929460227489471 2.1773855686187744 0.21875\n",
            "time: 2.2410805225372314 2.0033431286987198\n",
            "i, sloss, closs, correct 571 0.20628635585308075 2.1955208778381348 0.234375\n",
            "time: 1.9156534671783447 2.0031907291679114\n",
            "i, sloss, closs, correct 572 0.2183598130941391 2.2185635566711426 0.375\n",
            "time: 1.9488632678985596 2.003097410185383\n",
            "i, sloss, closs, correct 573 0.29250568151474 2.245649576187134 0.21875\n",
            "time: 1.8865196704864502 2.0028951820180807\n",
            "i, sloss, closs, correct 574 0.22022242844104767 2.2077183723449707 0.3125\n",
            "time: 1.927332878112793 2.0027646255493163\n",
            "i, sloss, closs, correct 575 0.2185709923505783 2.1859731674194336 0.15625\n",
            "time: 2.110874652862549 2.0029531829059124\n",
            "i, sloss, closs, correct 576 0.29243260622024536 2.191220760345459 0.203125\n",
            "time: 2.329648017883301 2.0035224956697566\n",
            "i, sloss, closs, correct 577 0.23995286226272583 2.220928192138672 0.265625\n",
            "time: 1.8966436386108398 2.003339022913606\n",
            "i, sloss, closs, correct 578 0.27390727400779724 2.20745587348938 0.25\n",
            "time: 1.9294276237487793 2.0032159306224764\n",
            "i, sloss, closs, correct 579 0.22716885805130005 2.191227674484253 0.3125\n",
            "time: 1.902564525604248 2.0030432799766804\n",
            "i, sloss, closs, correct 580 0.22203204035758972 2.1964151859283447 0.234375\n",
            "time: 1.8814165592193604 2.0028347534074635\n",
            "i, sloss, closs, correct 581 0.30673038959503174 2.228482246398926 0.21875\n",
            "time: 1.9477732181549072 2.0027409574829838\n",
            "i, sloss, closs, correct 582 0.24724945425987244 2.213468313217163 0.28125\n",
            "time: 2.4608705043792725 2.003527457268324\n",
            "i, sloss, closs, correct 583 0.2318790704011917 2.2062997817993164 0.21875\n",
            "time: 1.9169871807098389 2.0033800581546677\n",
            "i, sloss, closs, correct 584 0.25736671686172485 2.214789390563965 0.296875\n",
            "time: 1.9259819984436035 2.0032486194219343\n",
            "i, sloss, closs, correct 585 0.2710011601448059 2.194807529449463 0.328125\n",
            "time: 1.913336992263794 2.003096001140087\n",
            "i, sloss, closs, correct 586 0.20799703896045685 2.227484703063965 0.265625\n",
            "time: 1.882908821105957 2.0028921168748117\n",
            "i, sloss, closs, correct 587 0.24215060472488403 2.1686201095581055 0.328125\n",
            "time: 1.9236323833465576 2.0027591379321352\n",
            "i, sloss, closs, correct 588 0.24530406296253204 2.199859857559204 0.203125\n",
            "time: 2.4762461185455322 2.0035638537997906\n",
            "i, sloss, closs, correct 589 0.2745193541049957 2.2100653648376465 0.15625\n",
            "time: 1.9516940116882324 2.0034792302018505\n",
            "i, sloss, closs, correct 590 0.2511974275112152 2.1918935775756836 0.265625\n",
            "time: 1.8832886219024658 2.003276751932961\n",
            "i, sloss, closs, correct 591 0.24355128407478333 2.200778007507324 0.265625\n",
            "time: 1.8838460445404053 2.0030758433245324\n",
            "i, sloss, closs, correct 592 0.24639664590358734 2.2528131008148193 0.3125\n",
            "time: 1.9150993824005127 2.0029289991819437\n",
            "i, sloss, closs, correct 593 0.23161739110946655 2.1903157234191895 0.1875\n",
            "time: 1.8977744579315186 2.0027528310865668\n",
            "i, sloss, closs, correct 594 0.23517674207687378 2.160374164581299 0.21875\n",
            "time: 2.30155348777771 2.00325600800394\n",
            "i, sloss, closs, correct 595 0.20430557429790497 2.1970272064208984 0.21875\n",
            "time: 2.127284049987793 2.0034647787977384\n",
            "i, sloss, closs, correct 596 0.22457659244537354 2.161726951599121 0.140625\n",
            "time: 1.8653314113616943 2.0032350110448585\n",
            "i, sloss, closs, correct 597 0.3204444646835327 2.1749377250671387 0.296875\n",
            "time: 1.8738183975219727 2.003019427774742\n",
            "i, sloss, closs, correct 598 0.21203388273715973 2.191220760345459 0.234375\n",
            "time: 1.891099214553833 2.002833392265842\n",
            "i, sloss, closs, correct 599 0.2981657385826111 2.193748950958252 0.34375\n",
            "time: 1.8970072269439697 2.002657867272695\n",
            "i, sloss, closs, correct 600 0.23525317013263702 2.2228500843048096 0.171875\n",
            "time: 2.1619648933410645 2.002923744887163\n",
            "i, sloss, closs, correct 601 0.29237157106399536 2.189751148223877 0.21875\n",
            "time: 2.2694575786590576 2.003367301633588\n",
            "i, sloss, closs, correct 602 0.2722702920436859 2.1936047077178955 0.34375\n",
            "time: 1.915787935256958 2.003222897278136\n",
            "i, sloss, closs, correct 603 0.2270418256521225 2.1836342811584473 0.359375\n",
            "time: 1.8743226528167725 2.003010284426986\n",
            "i, sloss, closs, correct 604 0.24914124608039856 2.2310378551483154 0.171875\n",
            "time: 1.8586883544921875 2.0027725917248684\n",
            "i, sloss, closs, correct 605 0.23280423879623413 2.1526002883911133 0.203125\n",
            "time: 1.9278907775878906 2.002650714156651\n",
            "i, sloss, closs, correct 606 0.23442870378494263 2.172806978225708 0.21875\n",
            "time: 2.0023155212402344 2.002650964004793\n",
            "i, sloss, closs, correct 607 0.31677621603012085 2.1649603843688965 0.265625\n",
            "time: 2.3917391300201416 2.0032915955311372\n",
            "i, sloss, closs, correct 608 0.2838255763053894 2.199462890625 0.25\n",
            "time: 1.8967387676239014 2.0031174611184004\n",
            "i, sloss, closs, correct 609 0.2536294460296631 2.168328285217285 0.21875\n",
            "time: 1.897897481918335 2.0029457479226784\n",
            "i, sloss, closs, correct 610 0.27763164043426514 2.1893272399902344 0.328125\n",
            "time: 1.8784279823303223 2.002742805184397\n",
            "i, sloss, closs, correct 611 0.22709301114082336 2.2252197265625 0.21875\n",
            "time: 1.9131650924682617 2.002597234997095\n",
            "i, sloss, closs, correct 612 0.23436978459358215 2.1895484924316406 0.328125\n",
            "time: 1.8877196311950684 2.0024110504698016\n",
            "i, sloss, closs, correct 613 0.28997528553009033 2.192505359649658 0.25\n",
            "time: 2.5078110694885254 2.0032349980226947\n",
            "i, sloss, closs, correct 614 0.2767427861690521 2.213651657104492 0.265625\n",
            "time: 1.9476852416992188 2.0031490120461317\n",
            "i, sloss, closs, correct 615 0.21164552867412567 2.1317591667175293 0.25\n",
            "time: 1.8893623352050781 2.002965153811814\n",
            "i, sloss, closs, correct 616 0.21655318140983582 2.155061721801758 0.125\n",
            "time: 1.8750367164611816 2.002758590290187\n",
            "i, sloss, closs, correct 617 0.2712710499763489 2.214481830596924 0.28125\n",
            "time: 1.9247243404388428 2.002634272220451\n",
            "i, sloss, closs, correct 618 0.25622162222862244 2.194192886352539 0.296875\n",
            "time: 1.9397976398468018 2.002533571015267\n",
            "i, sloss, closs, correct 619 0.3196462392807007 2.184011936187744 0.296875\n",
            "time: 2.367029905319214 2.003122309330971\n",
            "i, sloss, closs, correct 620 0.2478797882795334 2.2237472534179688 0.296875\n",
            "time: 2.074565887451172 2.003244292140967\n",
            "i, sloss, closs, correct 621 0.25539734959602356 2.198800563812256 0.296875\n",
            "time: 1.8772761821746826 2.0030424893860648\n",
            "i, sloss, closs, correct 622 0.2547433376312256 2.1689248085021973 0.265625\n",
            "time: 1.8807964324951172 2.0028475425407937\n",
            "i, sloss, closs, correct 623 0.2390795350074768 2.156712055206299 0.328125\n",
            "time: 1.9057440757751465 2.0026927406971273\n",
            "i, sloss, closs, correct 624 0.32256433367729187 2.230463981628418 0.296875\n",
            "time: 1.922516107559204 2.00256534576416\n",
            "i, sloss, closs, correct 625 0.2693248391151428 2.194240093231201 0.25\n",
            "time: 2.173367977142334 2.00283898903539\n",
            "i, sloss, closs, correct 626 0.2533245086669922 2.1913204193115234 0.25\n",
            "time: 2.19779109954834 2.0031523305263246\n",
            "i, sloss, closs, correct 627 0.34309905767440796 2.181332588195801 0.21875\n",
            "time: 1.8953278064727783 2.0029814231927228\n",
            "i, sloss, closs, correct 628 0.36146366596221924 2.1737170219421387 0.296875\n",
            "time: 1.8918991088867188 2.002805687854324\n",
            "i, sloss, closs, correct 629 0.3946978449821472 2.1783852577209473 0.265625\n",
            "time: 1.878999948501587 2.0026099322334168\n",
            "i, sloss, closs, correct 630 0.2887261211872101 2.181356906890869 0.265625\n",
            "time: 1.8876674175262451 2.00243001127772\n",
            "i, sloss, closs, correct 631 0.3176972568035126 2.1672449111938477 0.203125\n",
            "time: 2.040675401687622 2.002491393044025\n",
            "i, sloss, closs, correct 632 0.2616756558418274 2.159022808074951 0.1875\n",
            "time: 2.38500714302063 2.0030968245736798\n",
            "i, sloss, closs, correct 633 0.3098691403865814 2.1545610427856445 0.234375\n",
            "time: 1.8842697143554688 2.002910322195348\n",
            "i, sloss, closs, correct 634 0.22077973186969757 2.1578550338745117 0.21875\n",
            "time: 1.9030802249908447 2.0027542643659695\n",
            "i, sloss, closs, correct 635 0.21225428581237793 2.1833996772766113 0.265625\n",
            "time: 1.883124828338623 2.0025669190118895\n",
            "i, sloss, closs, correct 636 0.29409104585647583 2.229168653488159 0.171875\n",
            "time: 1.8961155414581299 2.0024005640619778\n",
            "i, sloss, closs, correct 637 0.3721986711025238 2.2113451957702637 0.28125\n",
            "time: 1.9177775382995605 2.0022692388875356\n",
            "i, sloss, closs, correct 638 0.29569360613822937 2.1941580772399902 0.28125\n",
            "time: 2.4743590354919434 2.00300875889117\n",
            "i, sloss, closs, correct 639 0.2665870189666748 2.182629346847534 0.34375\n",
            "time: 1.9062483310699463 2.0028583981096744\n",
            "i, sloss, closs, correct 640 0.3493979573249817 2.14970064163208 0.296875\n",
            "time: 1.865912675857544 2.0026460575425125\n",
            "i, sloss, closs, correct 641 0.24199457466602325 2.2024006843566895 0.3125\n",
            "time: 1.8721776008605957 2.0024436106934353\n",
            "i, sloss, closs, correct 642 0.29934924840927124 2.2348573207855225 0.3125\n",
            "time: 1.9261527061462402 2.002326256750531\n",
            "i, sloss, closs, correct 643 0.3158016800880432 2.182652473449707 0.25\n",
            "time: 1.921191692352295 2.002201041079456\n",
            "i, sloss, closs, correct 644 0.230721116065979 2.2039613723754883 0.28125\n",
            "time: 2.362095594406128 2.0027597538260524\n",
            "i, sloss, closs, correct 645 0.2910092771053314 2.2049994468688965 0.3125\n",
            "time: 2.063901662826538 2.0028550849979507\n",
            "i, sloss, closs, correct 646 0.3645721673965454 2.2446277141571045 0.296875\n",
            "time: 1.8951175212860107 2.0026893214059207\n",
            "i, sloss, closs, correct 647 0.24036207795143127 2.2177278995513916 0.296875\n",
            "time: 1.905120611190796 2.0025400832111453\n",
            "i, sloss, closs, correct 648 0.38219979405403137 2.18367862701416 0.21875\n",
            "time: 1.9986321926116943 2.002534826290442\n",
            "i, sloss, closs, correct 649 0.37686362862586975 2.174496650695801 0.28125\n",
            "time: 1.9235084056854248 2.0024140284611627\n",
            "i, sloss, closs, correct 650 0.25847238302230835 2.1873393058776855 0.375\n",
            "time: 2.2987024784088135 2.0028704428636166\n",
            "i, sloss, closs, correct 651 0.24594567716121674 2.201657772064209 0.25\n",
            "time: 2.2012922763824463 2.003175373457692\n",
            "i, sloss, closs, correct 652 0.27052727341651917 2.169095039367676 0.25\n",
            "time: 1.9424667358398438 2.0030841151934125\n",
            "i, sloss, closs, correct 653 0.3204641044139862 2.2047362327575684 0.140625\n",
            "time: 1.911869764328003 2.002945430417309\n",
            "i, sloss, closs, correct 654 0.2779475152492523 2.211184501647949 0.296875\n",
            "time: 1.893455982208252 2.0027790349858408\n",
            "i, sloss, closs, correct 655 0.29280877113342285 2.191331148147583 0.296875\n",
            "time: 1.8995442390441895 2.002622446635874\n",
            "i, sloss, closs, correct 656 0.28916770219802856 2.208390712738037 0.234375\n",
            "time: 2.219404935836792 2.002953158121675\n",
            "i, sloss, closs, correct 657 0.25548920035362244 2.1703639030456543 0.296875\n",
            "time: 2.1992790699005127 2.0032522029789748\n",
            "i, sloss, closs, correct 658 0.2539125382900238 2.188990592956543 0.21875\n",
            "time: 1.9246962070465088 2.0031337600557504\n",
            "i, sloss, closs, correct 659 0.2845117449760437 2.2183837890625 0.234375\n",
            "time: 1.8739087581634521 2.0029387120044593\n",
            "i, sloss, closs, correct 660 0.3742713928222656 2.181443214416504 0.203125\n",
            "time: 1.8868329524993896 2.0027637972232974\n",
            "i, sloss, closs, correct 661 0.2495454102754593 2.1981019973754883 0.234375\n",
            "time: 1.8731942176818848 2.002568771831939\n",
            "i, sloss, closs, correct 662 0.4176155626773834 2.1894373893737793 0.25\n",
            "time: 2.0526938438415527 2.002645081524396\n",
            "i, sloss, closs, correct 663 0.44244638085365295 2.2074031829833984 0.21875\n",
            "time: 2.4282655715942383 2.003286669053227\n",
            "i, sloss, closs, correct 664 0.37398600578308105 2.1802899837493896 0.265625\n",
            "time: 1.9707727432250977 2.003238502301668\n",
            "i, sloss, closs, correct 665 0.30189865827560425 2.2156317234039307 0.234375\n",
            "time: 1.9378416538238525 2.003141012635675\n",
            "i, sloss, closs, correct 666 0.31299886107444763 2.1503138542175293 0.25\n",
            "time: 1.896700143814087 2.002983696993323\n",
            "i, sloss, closs, correct 667 0.2899843454360962 2.1634228229522705 0.203125\n",
            "time: 1.9245421886444092 2.0028670198189285\n",
            "i, sloss, closs, correct 668 0.3805058002471924 2.213350296020508 0.296875\n",
            "time: 2.0031557083129883 2.002868179070397\n",
            "i, sloss, closs, correct 669 0.3683558404445648 2.144597053527832 0.359375\n",
            "time: 2.4583120346069336 2.0035485538084115\n",
            "i, sloss, closs, correct 670 0.38498950004577637 2.1713976860046387 0.25\n",
            "time: 1.8654677867889404 2.0033434911918357\n",
            "i, sloss, closs, correct 671 0.37949681282043457 2.1582460403442383 0.1875\n",
            "time: 1.8784353733062744 2.0031583365939913\n",
            "i, sloss, closs, correct 672 0.2905423045158386 2.2221438884735107 0.3125\n",
            "time: 1.891082525253296 2.00299250884715\n",
            "i, sloss, closs, correct 673 0.38403254747390747 2.19899845123291 0.1875\n",
            "time: 1.8663668632507324 2.002790534177944\n",
            "i, sloss, closs, correct 674 0.34216004610061646 2.232360363006592 0.171875\n",
            "time: 1.8985536098480225 2.0026368561497443\n",
            "i, sloss, closs, correct 675 0.4123408794403076 2.2086737155914307 0.328125\n",
            "time: 2.3720715045928955 2.0031840959949605\n",
            "i, sloss, closs, correct 676 0.2531474828720093 2.177865505218506 0.296875\n",
            "time: 2.032235622406006 2.0032275909863335\n",
            "i, sloss, closs, correct 677 0.3269345760345459 2.158454179763794 0.375\n",
            "time: 1.8663287162780762 2.0030264214428475\n",
            "i, sloss, closs, correct 678 0.3216705620288849 2.179272174835205 0.203125\n",
            "time: 1.9436321258544922 2.0029400167479254\n",
            "i, sloss, closs, correct 679 0.30316558480262756 2.1593616008758545 0.265625\n",
            "time: 1.915517807006836 2.0028121871106763\n",
            "i, sloss, closs, correct 680 0.44235360622406006 2.202904224395752 0.265625\n",
            "time: 1.9063434600830078 2.0026721471206734\n",
            "i, sloss, closs, correct 681 0.3134341239929199 2.1883721351623535 0.25\n",
            "time: 2.246194839477539 2.0030299788695975\n",
            "i, sloss, closs, correct 682 0.31810277700424194 2.2015819549560547 0.25\n",
            "time: 2.163645029067993 2.003266422500722\n",
            "i, sloss, closs, correct 683 0.38586926460266113 2.2158265113830566 0.234375\n",
            "time: 1.905005693435669 2.0031235413244595\n",
            "i, sloss, closs, correct 684 0.28892678022384644 2.1922192573547363 0.359375\n",
            "time: 1.89542555809021 2.002967034639233\n",
            "i, sloss, closs, correct 685 0.27927640080451965 2.1967294216156006 0.21875\n",
            "time: 1.8813021183013916 2.002790851426194\n",
            "i, sloss, closs, correct 686 0.30327075719833374 2.223038911819458 0.296875\n",
            "time: 1.9069294929504395 2.002651988852666\n",
            "i, sloss, closs, correct 687 0.33430251479148865 2.2224254608154297 0.21875\n",
            "time: 2.1596593856811523 2.0028809208509535\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2025875566.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0msloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mijepa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0mcloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mijepa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mijepa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2025875566.py\u001b[0m in \u001b[0;36mstrain\u001b[0;34m(model, dataloader, optim, scheduler)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;31m# grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 5) # 0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# print('grad_norm', grad_norm.item())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# @title strain ctrain test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "def strain(model, dataloader, optim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.train()\n",
        "    for i, (x, _) in enumerate(dataloader):\n",
        "        x = x.to(device)#.to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "        # with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "        loss = model.loss(x)\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        # grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 5) # 0.5\n",
        "        # print('grad_norm', grad_norm.item())\n",
        "        optim.step()\n",
        "\n",
        "        # scaler.scale(loss).backward()\n",
        "        # scaler.unscale_(optim)\n",
        "        # scaler.step(optim)\n",
        "        # scaler.update()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            m=0.999 # 0.99 m = next(momentum_scheduler)\n",
        "            norms=[]\n",
        "            for param_q, param_k in zip(model.student.parameters(), model.teacher.parameters()):\n",
        "                param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        # if i%10==0: print(\"strain\",loss.item())\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=50: break\n",
        "    return loss.item()\n",
        "\n",
        "def ctrain(model, classifier, dataloader, coptim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.eval()\n",
        "    classifier.train()\n",
        "    # closs = 0\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        # with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "        with torch.no_grad(): sx = model(x).detach()\n",
        "        y_ = classifier(sx)\n",
        "        loss = F.cross_entropy(y_, y)\n",
        "        coptim.zero_grad()\n",
        "        loss.backward()\n",
        "        coptim.step()\n",
        "        # scaler.scale(loss).backward()\n",
        "        # scaler.step(coptim)\n",
        "        # scaler.update()\n",
        "        # print(\"classify\",loss.item())\n",
        "        # closs += loss.item()\n",
        "        try: wandb.log({\"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "    # return closs/\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        with torch.no_grad():\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        # print(correct/len(y))\n",
        "        try: wandb.log({\"correct\": correct/len(y)})\n",
        "        # try: wandb.log({\"correct\": correct/len(y), \"rankme\": rankme, \"lidar\": lidar})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "    return correct/len(y)\n",
        "\n",
        "import time\n",
        "start = begin = time.time()\n",
        "# for i in range(1):\n",
        "for i in range(1000): # 1000\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    sloss = strain(ijepa, train_loader, optim)\n",
        "    closs = ctrain(ijepa, classifier, train_loader, coptim)\n",
        "    correct = test(ijepa, classifier, test_loader)\n",
        "    print('i, sloss, closs, correct', i, sloss, closs, correct)\n",
        "    print('time:',time.time() - start, (time.time()-begin)/(i+1))\n",
        "    start = time.time()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(ijepa.student.transformer[0].in_proj.weight)"
      ],
      "metadata": {
        "id": "phs2ERy_RmMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jzo9DMDPcOxu"
      },
      "outputs": [],
      "source": [
        "# @title save/load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder='/content/drive/MyDrive/jepa/'\n",
        "\n",
        "# # modelsd, optimsd = torch.load(folder+'SeqJEPA.pkl', map_location=device).values()\n",
        "# modelsd, optimsd = torch.load('SeqJEPA.pkl', map_location=device).values()\n",
        "# seq_jepa.load_state_dict(modelsd, strict=False)\n",
        "# optim.load_state_dict(optimsd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "DNNPOuUmcSNf"
      },
      "outputs": [],
      "source": [
        "checkpoint = {'model': ijepa.state_dict(), 'optimizer': optim.state_dict()}\n",
        "torch.save(checkpoint, folder+'ijepa.pkl')\n",
        "# torch.save(checkpoint, 'IJEPA.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yl-xtHthFl0M"
      },
      "source": [
        "## store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "e7HYQxn6n6iD"
      },
      "outputs": [],
      "source": [
        "# @title GoldenGateRoPE2d\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class GoldenGateRoPE2d(nn.Module): # jerryxio.ng/posts/nd-rope\n",
        "    def __init__(self, image_size, n_heads, n_freqs, min_freq=.8, max_freq=10, n_zero_freqs=0):\n",
        "        super().__init__()\n",
        "        intv = math.pi * (math.sqrt(5)-1)/2 # mod pi instead of 2pi # pi*(sqrt5+-1)/2 ; + and - are equivalent bec mod pi\n",
        "        # intv = math.pi * (math.sqrt(5)-1) # https://en.wikipedia.org/wiki/Golden_angle\n",
        "        speed = torch.cat([torch.zeros(n_zero_freqs), min_freq * (max_freq/min_freq) ** torch.linspace(0,1,n_freqs-n_zero_freqs)]).unsqueeze(-1) # [n_freqs,1] # og\n",
        "        angle = torch.arange(n_heads*n_freqs).reshape(n_heads, n_freqs) * intv # [n_heads, n_freqs]\n",
        "        direction = torch.stack([torch.cos(angle), torch.sin(angle)], dim=-1) # [n_heads, n_freqs, 2]\n",
        "        h, w = image_size\n",
        "        xlim, ylim = w/h, h/w\n",
        "        y, x = torch.meshgrid(torch.linspace(-ylim, ylim, h), torch.linspace(-xlim, xlim, w), indexing=\"ij\") # [h,w], y:row_num, x:col_num\n",
        "        pos = torch.stack([x, y], dim=-1).reshape(-1,1,1,2) # [h*w,1,1,2] cartesian coords\n",
        "        theta = (speed*direction*pos).sum(dim=-1) # [t,n_heads,n_freqs,2]->[t,n_heads,d_head]\n",
        "        self.theta = theta\n",
        "        cos, sin = torch.cos(theta), torch.sin(theta)\n",
        "        self.affine = torch.stack([cos, -sin, sin, cos], dim=-1).transpose(0,1).reshape(1,n_heads,h*w,n_freqs,2,2).to(device) # [t,n_heads,n_freqs,4]->[1,n_heads,t,n_freqs,2,2]\n",
        "\n",
        "    def forward(self, x): # [b,h,t,d]\n",
        "        return (self.affine @ x.unflatten(-1, (-1,2)).unsqueeze(-1)).flatten(-3) # @ [b,h,t,d_head//2,2,1]\n",
        "\n",
        "# /2 better\n",
        "# speed [n_freqs,1] # og best\n",
        "# w/h best\n",
        "# rope < ggrope < learned\n",
        "\n",
        "# image_size=(8,8)\n",
        "image_size=(20,30)\n",
        "# image_size=(90,120)\n",
        "n_heads=4\n",
        "n_freqs=6\n",
        "ggrope = GoldenGateRoPE2d(image_size, n_heads, n_freqs)\n",
        "\n",
        "# x = torch.rand(2, *image_size, n_heads, n_freqs*2)\n",
        "x = torch.rand(2, n_heads, image_size[0]*image_size[1], n_freqs*2)\n",
        "out = ggrope(x)\n",
        "print(out.shape)\n",
        "# # print(out[0])\n",
        "# theta = ggrope.theta.flatten(-2).permute(2,0,1).unsqueeze(1) # [t,n_heads,d_head][b,1,h,w]\n",
        "theta = ggrope.theta.flatten(-2).T.reshape(n_heads*n_freqs, 1, *image_size) # [t,n_heads,d_head]->[d,1,h,w]\n",
        "cy, cx = image_size[0]//2, image_size[1]//2\n",
        "sim = torch.cos(theta-theta[...,cy,cx][...,None,None]) # [b,1,h,w]\n",
        "# sim = sim.unflatten(0, (n_heads, n_freqs)).mean(1)\n",
        "# print(sim.shape)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    print(npimg.shape)\n",
        "    plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "# bhwc\n",
        "\n",
        "import torchvision\n",
        "imshow(torchvision.utils.make_grid(sim, nrow=n_freqs))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "eA2R-JZ7G67P"
      },
      "outputs": [],
      "source": [
        "# @title RoPE pos\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class RoPE(nn.Module):\n",
        "    def __init__(self, dim, seq_len=512, top=torch.pi, base=10000):\n",
        "    # def __init__(self, dim, seq_len=512, min_freq=1, max_freq=400, n_zero_freqs=0):\n",
        "        super().__init__()\n",
        "        self.dim, self.top, self.base = dim, top, base\n",
        "        speed = top / (base ** (torch.arange(0, dim, step=2) / dim)) # [dim//2]\n",
        "        pos = torch.arange(seq_len).unsqueeze(-1) # [t,1]\n",
        "        # speed = torch.cat([torch.zeros(n_zero_freqs), min_freq * (max_freq/min_freq) ** torch.linspace(0,1,dim//2-n_zero_freqs)]) # [dim//2]\n",
        "        # pos = torch.linspace(0, 1, seq_len).unsqueeze(-1) # [t,1]\n",
        "        theta = (speed*pos) # [t,1]*[dim//2]=[t,d//2]\n",
        "        self.theta = theta\n",
        "        cos, sin = torch.cos(theta), torch.sin(theta)\n",
        "        self.affine = torch.stack([cos, -sin, sin, cos], dim=-1).unflatten(-1,(2,2)).to(device)#[None,None,...] # [t,d//2,4]->[t,d//2,2,2] # [1,1,t,d//2,2,2]\n",
        "\n",
        "\n",
        "        # angles = theta[None,...,None] # [seq_len, 1] * [dim//2] -> [1, 1, seq_len, dim//2, 1]\n",
        "        # self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [1, 1, seq_len, dim//2, 2] -> [1, 1, seq_len, dim]\n",
        "        # # self.rot_emb = torch.cat([sin, cos], dim=-1).flatten(-2).to(device) # [1, 1, seq_len, dim//2, 2] -> [1,1,seq_len,dim]\n",
        "        # print('self.rot_emb', self.rot_emb.shape)\n",
        "\n",
        "    def forward(self, x, ind=None): # [b,h,t,d], [b,t]\n",
        "        # seq_len = x.size(-2)\n",
        "        b,_,seq_len,_ = x.shape\n",
        "        if ind!=None: seq_len = max(seq_len, ind.max()+1)\n",
        "        if self.affine.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.top, self.base)\n",
        "        if ind!=None:\n",
        "            # print(\"if affine, x\",self.affine.shape, x.shape)\n",
        "            return (self.affine.unsqueeze(0).expand(b,-1,-1,-1,-1)[torch.arange(b, device=device).unsqueeze(-1), ind].unsqueeze(1) @ x.unflatten(-1, (-1,2)).unsqueeze(-1)).flatten(-3) # @ [b,h,t,d_head//2,2,1]\n",
        "        else:\n",
        "            # print(\"else affine, x\",self.affine.shape, x.shape) # [64, 4, 2, 2], [64, 8, 10, 8]\n",
        "            return (self.affine[None,None,...] @ x.unflatten(-1, (-1,2)).unsqueeze(-1)).flatten(-3) # @ [b,h,t,d_head//2,2,1]\n",
        "\n",
        "\n",
        "        # # if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.base)\n",
        "        # if ind==None:\n",
        "        #     # print(\"if rot, x\",self.rot_emb.shape, x.shape)\n",
        "        #     return x * self.rot_emb.unsqueeze(1)#[None,None,...]\n",
        "        # else:\n",
        "        #     return x * (self.rot_emb.expand(b,-1,-1)[torch.arange(b, device=device).unsqueeze(-1), ind].unsqueeze(1))\n",
        "\n",
        "\n",
        "\n",
        "dim=64\n",
        "seq_len=56\n",
        "rope = RoPE(dim, seq_len, top=torch.pi, base=100)\n",
        "# rope = RoPE(dim, seq_len, min_freq=1, max_freq=200, n_zero_freqs=0)\n",
        "\n",
        "batch=2\n",
        "t=50\n",
        "x = torch.rand(batch, 4, seq_len, dim, device=device)\n",
        "out = rope(x)\n",
        "print(\"out1\", out.shape)\n",
        "x = torch.rand(batch, 4, t, dim, device=device)\n",
        "pos = torch.randint(0,seq_len,(batch,t), device=device)\n",
        "out = rope(x, pos)\n",
        "print(\"out2\", out.shape)\n",
        "\n",
        "# theta = rope.theta # [t,d//2]\n",
        "# sim = torch.cos(theta-theta[0].unsqueeze(0)).T\n",
        "# # print(sim.shape)\n",
        "\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# def imshow(img):\n",
        "#     npimg = img.numpy()\n",
        "#     print(npimg.shape)\n",
        "#     plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "#     plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "#     plt.show()\n",
        "\n",
        "# import torchvision\n",
        "# imshow(torchvision.utils.make_grid(sim, nrow=dim//2))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "k3nfeLM4wtkJ"
      },
      "outputs": [],
      "source": [
        "# @title random_masking\n",
        "import torch\n",
        "\n",
        "def random_masking(length, mask_ratio, b=64):\n",
        "    noise = torch.rand(b, length)\n",
        "    len_mask = int(length * mask_ratio)\n",
        "    _, msk_ind = torch.topk(noise, k=len_mask, dim=-1, sorted=False) # val, ind -> [b,len_mask]\n",
        "    _, keep_ind = torch.topk(noise, k=length-len_mask, largest=False, dim=-1, sorted=False) # val, ind -> [b,len_keep]\n",
        "    return msk_ind, keep_ind\n",
        "\n",
        "# msk_ind, keep_ind = random_masking(10, .3, b=2)\n",
        "\n",
        "# x_ = torch.rand(4, 3, 2)\n",
        "# print(x_)\n",
        "# # ids = torch.tensor([0, 2, 1])[None,:,None]\n",
        "# # ids = torch.tensor([0, 2, 1])[None,:,None].repeat(4,1,2)\n",
        "# ids = torch.tensor([1, 2, 0])[None,:,None].repeat(4,1,2)\n",
        "# # o = torch.gather(x_, dim=1, index=ids)\n",
        "# o = torch.zeros_like(x_).scatter_(dim=1, index=ids, src=x_)\n",
        "# print(o)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "62UPGVucmGNe"
      },
      "outputs": [],
      "source": [
        "# @title AttentionBlock pos\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "# import inspect\n",
        "# class Seq(nn.Sequential):\n",
        "#     def __init__(self, *args):\n",
        "#         super().__init__(*args)\n",
        "#         for layer in self:\n",
        "#             params = inspect.signature(layer.forward).parameters.keys()\n",
        "#             layer._fwdparams = ','.join(params)\n",
        "\n",
        "#     def forward(self, x, pos=None, masks=None):\n",
        "#         arg_map = {'pos':pos, 'masks':masks}\n",
        "#         for layer in self:\n",
        "#             args = [x]\n",
        "#             # if 'masks' in layer._fwdparams: args.append(masks)\n",
        "#             args.extend(arg_map[p] for p in arg_map if p in layer._fwdparams)\n",
        "#             # print(layer._fwdparams, args)\n",
        "#             x = layer(*args)\n",
        "#         return x\n",
        "\n",
        "# import inspect\n",
        "# class Seq(nn.Sequential):\n",
        "#     def __init__(self, *args):\n",
        "#         super().__init__(*args)\n",
        "#         # for layer in self:\n",
        "#         #     params = inspect.signature(layer.forward).parameters.keys()\n",
        "#         #     layer._fwdparams = ','.join(params)\n",
        "\n",
        "#     # def forward(self, x, pos=None, masks=None):\n",
        "#     def forward(self, x, *args, **kwargs):\n",
        "#         # arg_map = {'pos':pos, 'masks':masks}\n",
        "#         for layer in self:\n",
        "#             # args = [x]\n",
        "#             # if 'masks' in layer._fwdparams: args.append(masks)\n",
        "#             # args.extend(arg_map[p] for p in arg_map if p in layer._fwdparams)\n",
        "#             # print(layer._fwdparams, args)\n",
        "#             x = layer(x, *args)\n",
        "#         return x\n",
        "\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        self._kwargs = [[name for name, p in inspect.signature(layer.forward).parameters.items()] for layer in self]\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        for layer, _kwargs in zip(self, self._kwargs):\n",
        "            x = layer(x, *args, **{k: v for k, v in kwargs.items() if k in _kwargs})\n",
        "        return x\n",
        "\n",
        "class SelfAttn(nn.Module):\n",
        "    def __init__(self, dim, n_heads):\n",
        "        super().__init__()\n",
        "        self.dim, self.n_heads = dim, n_heads\n",
        "        d_head = dim//n_heads\n",
        "        self.rope = RoPE(d_head, seq_len=64, top=torch.pi, base=1000)\n",
        "        # self.rope = RoPE(d_head, seq_len=512, base=1000)\n",
        "        # self.rope = RoPE2D(d_head, h=64, w=64, base=100)\n",
        "        # self.rope[0] = 1 # id for smry h\n",
        "        self.qkv = nn.Linear(dim, dim*3, bias=False)\n",
        "        # self.lin = nn.Linear(dim, dim)\n",
        "        self.lin = zero_module(nn.Linear(dim, dim))\n",
        "        self.scale = d_head**-.5\n",
        "\n",
        "    # def forward(self, x): # [b,t,d]\n",
        "    def forward(self, x, pos=None): # [b,t,d]\n",
        "        # print('SelfAttn', x.shape)\n",
        "        q,k,v = self.qkv(x).unflatten(-1, (self.n_heads,-1)).transpose(1,2).chunk(3, dim=-1) # [b, t, n_heads, d_head] -> [b, n_heads, t, d_head]\n",
        "        if pos==None: q, k = self.rope(q), self.rope(k)\n",
        "        else:\n",
        "            # print('SelfAttn fwd', x.shape, pos.shape)\n",
        "            q, k = self.rope(q, pos), self.rope(k, pos)\n",
        "\n",
        "        x = F.scaled_dot_product_attention(q,k,v, attn_mask=None) # https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html\n",
        "        # q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2)\n",
        "        # context = k.transpose(-2,-1) @ v # [batch, n_heads, d_head, d_head]\n",
        "        # x = q @ context # [b,n_heads,t,d_head]\n",
        "        x = x.transpose(1,2).flatten(2)\n",
        "        return self.lin(x)\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, mult=4, drop=0.):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        # self.norm = nn.RMSNorm(d_model, elementwise_affine=False) # LayerNorm RMSNorm\n",
        "        self.norm1, self.norm2 = nn.LayerNorm(d_model), nn.LayerNorm(d_model)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "        self.attn = SelfAttn(d_model, n_heads)\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        ff_dim=d_model*mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), act, nn.Dropout(drop), nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "            nn.RMSNorm(ff_dim), act, nn.Dropout(drop), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(drop), nn.Linear(d_model, ff_dim), act,\n",
        "            # nn.RMSNorm(ff_dim), nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "\n",
        "    # def forward(self, x): # [b,t,d]\n",
        "    def forward(self, x, pos=None): # [b,t,d]\n",
        "        # print('attnblk fwd',x.shape)\n",
        "        # x = x + self.drop(self.attn(self.norm1(x)))\n",
        "        x = x + self.drop(self.attn(self.norm1(x), pos))\n",
        "        x = x + self.ff(x)\n",
        "        # x = x + self.drop(self.ff(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, patch_size, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        patch_size=2\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Conv2d(in_dim, d_model, patch_size, patch_size), # like patch\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 1, 7//2, bias=False)\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.MaxPool2d(3,2,1)\n",
        "            nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.BatchNorm2d(d_model), nn.ReLU(),\n",
        "            nn.Conv2d(d_model, d_model, 3, 2, 3//2, bias=False)\n",
        "            )\n",
        "        # self.embed.requires_grad=False\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, 32*32, d_model)*.02)\n",
        "        # self.pos_emb = nn.Parameter(RoPE2D(dim=d_model, h=8, w=8, base=1000), requires_grad=False)\n",
        "\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=(32//patch_size)**2, base=10000), requires_grad=False)\n",
        "        # self.transformer = nn.Sequential(*[AttentionBlock(d_model, d_model, n_heads) for _ in range(nlayers)])\n",
        "        self.transformer = Seq(*[AttentionBlock(d_model, n_heads) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, cxt_inds=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x).flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c] # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        # x = self.pos_enc(x)\n",
        "        # print(\"TransformerModel\",x.shape, self.pos_emb.shape)\n",
        "        # print(\"TransformerModel\",x.shape)\n",
        "        # x = x + self.pos_emb[:,:x.shape[1]]\n",
        "        # if cxt_inds != None: x = x[torch.arange(x.shape[0]).unsqueeze(-1), cxt_inds] # [batch, num_context_toks, d_model]\n",
        "        # x = self.transformer(x)\n",
        "\n",
        "        if cxt_inds != None:\n",
        "            # print('vit transformer',x[torch.arange(x.shape[0]).unsqueeze(-1), cxt_inds].shape, cxt_inds.shape)\n",
        "            x = self.transformer(x[torch.arange(x.shape[0]).unsqueeze(-1), cxt_inds], cxt_inds)\n",
        "        else: x = self.transformer(x)\n",
        "\n",
        "        out = self.norm(x)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out\n",
        "\n",
        "d_model = 64\n",
        "in_dim = 3\n",
        "patch_size = 2\n",
        "# model = ViT(patch_size, in_dim, d_model, n_heads=4, nlayers=4, drop=0.).to(device)\n",
        "model = ViT(patch_size, in_dim, d_model, n_heads=4, nlayers=1, drop=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x = torch.rand((5, in_dim, 32, 32), device=device) # [b,c,h,w]\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "OgZ6d59vOQil"
      },
      "outputs": [],
      "source": [
        "# @title test seq\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        # for layer in self:\n",
        "        #     params = inspect.signature(layer.forward).parameters.keys()\n",
        "        #     layer._fwdparams = ','.join(params)\n",
        "\n",
        "    # def forward(self, x, pos=None, masks=None):\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        # arg_map = {'pos':pos, 'masks':masks}\n",
        "        for layer in self:\n",
        "            # args = [x]\n",
        "            # if 'masks' in layer._fwdparams: args.append(masks)\n",
        "            # args.extend(arg_map[p] for p in arg_map if p in layer._fwdparams)\n",
        "            # print(layer._fwdparams, args)\n",
        "            x = layer(x, *args, **kwargs)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        self._accepted_kwargs = []\n",
        "        for layer in self:\n",
        "            sig = inspect.signature(layer.forward)\n",
        "            print(sig.parameters.items())\n",
        "            self._accepted_kwargs.append(\n",
        "                [name for name, p in sig.parameters.items()]\n",
        "                # {name for name, p in sig.parameters.items()\n",
        "                #  if p.kind in (p.POSITIONAL_OR_KEYWORD, p.KEYWORD_ONLY)}\n",
        "            )\n",
        "\n",
        "            # if any(p.kind == p.VAR_KEYWORD for p in sig.parameters.values()):\n",
        "            #     accepted = None  # means \"pass everything\"\n",
        "\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        # for layer in self:\n",
        "        #     x = layer(x, *args, **kwargs)\n",
        "\n",
        "        # for layer in self:\n",
        "        #     sig = inspect.signature(layer.forward)\n",
        "        #     # keep only accepted keyword arguments\n",
        "        #     filtered_kwargs = {\n",
        "        #         k: v for k, v in kwargs.items()\n",
        "        #         if k in sig.parameters\n",
        "        #     }\n",
        "        #     x = layer(x, *args, **filtered_kwargs)\n",
        "\n",
        "        for layer, accepted in zip(self, self._accepted_kwargs):\n",
        "            filtered_kwargs = {k: v for k, v in kwargs.items() if k in accepted}\n",
        "            x = layer(x, *args, **filtered_kwargs)\n",
        "        # if accepted is None:\n",
        "        #     x = layer(x, *args, **kwargs)\n",
        "        # else:\n",
        "        #     filtered = {k: v for k, v in kwargs.items() if k in accepted}\n",
        "        #     x = layer(x, *args, **filtered)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        self._kwargs = [[name for name, p in inspect.signature(layer.forward).parameters.items()] for layer in self]\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        for layer, _kwargs in zip(self, self._kwargs):\n",
        "            x = layer(x, *args, **{k: v for k, v in kwargs.items() if k in _kwargs})\n",
        "        return x\n",
        "\n",
        "\n",
        "class SS(nn.Module):\n",
        "    def __init__(self, d):\n",
        "        super().__init__()\n",
        "        self.d = d\n",
        "    def forward(self, x, y, pos=None, bos=None):\n",
        "        # print('SS', x, pos, bos)\n",
        "        print('SS', x, y, pos, bos)\n",
        "        return x+1\n",
        "\n",
        "class PP(nn.Module):\n",
        "    def __init__(self, d):\n",
        "        super().__init__()\n",
        "        self.d = d\n",
        "    def forward(self, x, pos=None, dos=None):\n",
        "        print('PP', x, pos, dos)\n",
        "    # def forward(self, x, y, pos=None, dos=None):\n",
        "        # print('PP', x, y, pos, dos)\n",
        "        return x+1\n",
        "\n",
        "d=2\n",
        "# TT = Seq(*[SS(d) for _ in range(2)])\n",
        "TT = Seq(*[SS(d), PP(d)])\n",
        "# out = TT(3, a=2, pos='p')\n",
        "# out = TT(3, a=2, bos='b')\n",
        "# out = TT(3, a=2, dos='d')\n",
        "out = TT(3, 'q','w')\n",
        "# out = TT(3, 'q','w', pos=2, dos='d')\n",
        "# out = TT(3, 'q', pos=2, dos='d')\n",
        "print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4J2ahp3wmqcg"
      },
      "outputs": [],
      "source": [
        "# @title supervised train test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "# def strain(model, dataloader, optim, scheduler=None):\n",
        "def strain(model, classifier, dataloader, optim, coptim, scheduler=None):\n",
        "    model.train()\n",
        "    classifier.train()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x = x.to(device)#.to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "        y = y.to(device)\n",
        "\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        # print(loss)\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # for p in list(filter(lambda p: p.grad is not None, model.parameters())):\n",
        "        #     print(p.grad.data.norm(2).item())\n",
        "        # print(\"max grad norm\", max([p.grad.data.norm(2).item() for p in list(filter(lambda p: p.grad is not None, model.parameters()))]))\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 10) # 0.5\n",
        "\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        print(\"strain\",loss.item())\n",
        "        # for param in ijepa.student.cls: print(param.data)\n",
        "        # for param in ijepa.predicter.cls: print(param.data)\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=50: break\n",
        "\n",
        "\n",
        "# def test(model, dataloader):\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        # .to(torch.bfloat16)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "        loss = F.cross_entropy(y_, y)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        print(correct/len(y))\n",
        "        try: wandb.log({\"correct\": correct/len(y), \"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "for i in range(1000):\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # np.random.shuffle(train_indices); np.random.shuffle(val_indices)\n",
        "    # train_sampler, valid_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(val_indices)\n",
        "    # # batch_size = 64 #512\n",
        "    # train_loader = DataLoader(train_data, sampler=train_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "    # test_loader = DataLoader(train_data, sampler=valid_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True)\n",
        "\n",
        "    # strain(ijepa, train_loader, optim)\n",
        "    strain(ijepa, classifier, train_loader, optim, coptim)\n",
        "    test(ijepa, classifier, test_loader)\n",
        "\n",
        "    # strain(violet, train_loader, voptim)\n",
        "    # test(violet, test_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e1Yi9JrLPLd"
      },
      "source": [
        "## trash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KUvHrTi4LSEe"
      },
      "outputs": [],
      "source": [
        "# @title GoldenGateRoPE2d\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class GoldenGateRoPE2d(nn.Module): # jerryxio.ng/posts/nd-rope\n",
        "    def __init__(self, image_size: tuple[int, int], n_heads, n_freqs):\n",
        "        super().__init__()\n",
        "        n_zero_freqs = 1\n",
        "        min_freq, max_freq = .2, 20\n",
        "        speed = torch.cat([torch.zeros(n_zero_freqs), min_freq * (max_freq/min_freq) ** torch.linspace(0,1,n_freqs-n_zero_freqs)]) # [n_freqs]\n",
        "        direction_spacing = math.pi * (math.sqrt(5)-1)/2\n",
        "        phi = torch.arange(n_heads * n_freqs).reshape(n_heads, n_freqs) * direction_spacing # [n_heads, n_freqs]\n",
        "        directions = torch.stack((torch.cos(phi), torch.sin(phi)), dim=-1) # [n_heads, n_freqs, 2]\n",
        "        # print('directions', directions)\n",
        "        vel = speed.unsqueeze(-1) * directions # speed in direction[n_heads, n_freqs, 2]\n",
        "\n",
        "        H, W = image_size\n",
        "        xlim, ylim = math.sqrt(W / H), math.sqrt(H / W)\n",
        "        print(xlim, ylim)\n",
        "        y, x = torch.meshgrid(torch.linspace(-ylim, ylim, H), torch.linspace(-xlim, xlim, W), indexing=\"ij\") # [h,w], y:row_num, x:col_num\n",
        "        # y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\") # print(y,x) # [h,w], y:row_num, x:col_num\n",
        "\n",
        "        pos = torch.stack((x, y), dim=-1).reshape(H, W, 1, 1, 2) # cartesian coords\n",
        "\n",
        "        theta = (vel*pos).sum(dim=-1) # [h,w,n_heads,n_freqs,2]->[h,w,n_heads,d_head]\n",
        "        print('theta', theta.shape)\n",
        "        self.cos, self.sin = torch.cos(theta), torch.sin(theta)\n",
        "        print('self.cos', self.cos.shape)\n",
        "\n",
        "    def forward(self, input): # [b,h,w,n_head,d_head]\n",
        "        # x, y = input.float().chunk(2, dim=-1) # [b,h,w,n_head,n_freqs]\n",
        "        x, y = input.float().unflatten(-1, (-1,2)).chunk(2, dim=-1)\n",
        "        x, y = x.squeeze(-1), y.squeeze(-1)\n",
        "        x_out = x * self.cos - y * self.sin\n",
        "        y_out = x * self.sin + y * self.cos\n",
        "        # output = torch.cat((x_out, y_out), dim=-1)\n",
        "        output = torch.stack((x_out, y_out), dim=-1).flatten(-2)\n",
        "        return output.type_as(input)\n",
        "\n",
        "\n",
        "\n",
        "class GoldenGateRoPE2d2(nn.Module): # jerryxio.ng/posts/nd-rope\n",
        "    def __init__(self, image_size, n_heads, n_freqs):\n",
        "        super().__init__()\n",
        "        n_zero_freqs = 1\n",
        "        min_freq, max_freq = .2, 20\n",
        "        intv = math.pi * (math.sqrt(5)-1)/2\n",
        "        speed = torch.cat([torch.zeros(n_zero_freqs), min_freq * (max_freq/min_freq) ** torch.linspace(0,1,n_freqs-n_zero_freqs)]) # [n_freqs]\n",
        "        phi = torch.arange(n_heads*n_freqs).reshape(n_heads, n_freqs) * intv # [n_heads, n_freqs]\n",
        "        direction = torch.stack((torch.cos(phi), torch.sin(phi)), dim=-1) # [n_heads, n_freqs, 2]\n",
        "        vel = speed.unsqueeze(-1) * direction # speed in direction[n_heads, n_freqs, 2]\n",
        "        h, w = image_size\n",
        "        xlim, ylim = math.sqrt(w/h), math.sqrt(h/w)\n",
        "        y, x = torch.meshgrid(torch.linspace(-ylim, ylim, h), torch.linspace(-xlim, xlim, w), indexing=\"ij\") # [h,w], y:row_num, x:col_num\n",
        "        pos = torch.stack([x, y], dim=-1)[...,None,None,:] # [h,w,1,1,2] cartesian coords\n",
        "        theta = (vel*pos).sum(dim=-1) # [h,w,n_heads,n_freqs,2]->[h,w,n_heads,d_head]\n",
        "        cos, sin = torch.cos(theta), torch.sin(theta)\n",
        "        self.affine = torch.stack([cos, -sin, sin, cos], dim=-1).unflatten(-1, (2,2))\n",
        "\n",
        "    def forward(self, input): # [b,h,w,n_head,d_head]\n",
        "        return (self.affine @ input.unflatten(-1, (-1,2)).unsqueeze(-1)).flatten(-3)\n",
        "\n",
        "\n",
        "\n",
        "image_size=(5,7)\n",
        "n_heads=4\n",
        "d_head=16\n",
        "ggrope = GoldenGateRoPE2d(image_size, n_heads, d_head//2)\n",
        "ggrope2 = GoldenGateRoPE2d2(image_size, n_heads, d_head//2)\n",
        "\n",
        "# x = torch.rand(2, *image_size, n_heads, d_head)\n",
        "out = ggrope(x)\n",
        "out2 = ggrope2(x)\n",
        "print(out.shape)\n",
        "print(out2.shape)\n",
        "# print(out[0])\n",
        "# print(out2[0])\n",
        "# print((out==out2)[0])\n",
        "print((out==out2).all())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "K6saOuAX-FV8"
      },
      "outputs": [],
      "source": [
        "# @title test gather\n",
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "l=56\n",
        "b,t,d = 2,50,64\n",
        "src = torch.rand(b,l,d, device=device)\n",
        "idx = torch.randint(0,l,(b,t), device=device)\n",
        "out = src[torch.arange(b).unsqueeze(-1), idx]\n",
        "# out = torch.take_along_dim(src, idx.unsqueeze(-1).expand(-1,-1,d), dim=1)\n",
        "# out = src.index_select(0, idx).reshape(b, t, d) # == src[idx]\n",
        "# out = src.gather(1, idx.unsqueeze(-1).expand(-1, -1, d))\n",
        "print(out.shape)\n",
        "\n",
        "# %timeit out = src.expand(b,-1,-1).gather(1, idx.unsqueeze(-1).expand(-1, -1, src.size(-1))) # 2.53 ms\n",
        "# %timeit out = src.expand(b,-1,-1)[torch.arange(b, device=device).unsqueeze(-1), idx] # 1.39 ms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNt7SRjj9g_l"
      },
      "outputs": [],
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "1e1Yi9JrLPLd"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}