{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/I_JEPA_down.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LxACli7GdyGq"
      },
      "outputs": [],
      "source": [
        "# @title data\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "transform = transforms.Compose([transforms.ToTensor(),])\n",
        "# transform = transforms.Compose([transforms.RandomResizedCrop((32,32), scale=(.3,1.)), transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "# CIFAR10: 60000 32x32 color images in 10 classes, with 6000 images per class\n",
        "train_data = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
        "test_data = torchvision.datasets.CIFAR10(root='data', train=False, download=True, transform=transform)\n",
        "batch_size = 64 # 4\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# dataiter = iter(train_loader) # get some random training images\n",
        "# images, labels = next(dataiter)\n",
        "# print(images.shape) # [batch, 3, 32, 32]\n",
        "# imshow(torchvision.utils.make_grid(images))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3XSZZyUPY1i"
      },
      "source": [
        "## vit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "GD7ezZzmhTHU"
      },
      "outputs": [],
      "source": [
        "# @title RoPE\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def RoPE(dim, seq_len=512, top=torch.pi, base=1000):\n",
        "    theta = top / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "    pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "    angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim//2] -> [1, seq_len, dim//2, 1]\n",
        "    rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [1, seq_len, dim//2, 2] -> [1, seq_len, dim]\n",
        "    return rot_emb\n",
        "\n",
        "\n",
        "# class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "#     def __init__(self, dim, seq_len=512, top=torch.pi, base=1000):\n",
        "#         super().__init__()\n",
        "#         self.dim, self.base = dim, base\n",
        "#         theta = top / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "#         pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "#         angles = (pos * theta)[None,None,...,None] # [seq_len, 1] * [dim//2] -> [1, 1, seq_len, dim//2, 1]\n",
        "#         self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [1, 1, seq_len, dim//2, 2] -> [1, 1, seq_len, dim]\n",
        "\n",
        "#     def forward(self, x): # [b,h,t,d]\n",
        "#         seq_len = x.size(-2)\n",
        "#         if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.base)\n",
        "#         return x * self.rot_emb#[:,:,:seq_len]\n",
        "\n",
        "\n",
        "# @title proper rope\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class RoPE(nn.Module):\n",
        "    def __init__(self, dim, seq_len=512, top=torch.pi, base=1000):\n",
        "    # def __init__(self, dim, seq_len=512, min_freq=1, max_freq=400, n_zero_freqs=0):\n",
        "        super().__init__()\n",
        "        speed = top / (base ** (torch.arange(0, dim, step=2) / dim)) # [dim//2]\n",
        "        pos = torch.arange(seq_len).unsqueeze(-1) # [t,1]\n",
        "        # speed = torch.cat([torch.zeros(n_zero_freqs), min_freq * (max_freq/min_freq) ** torch.linspace(0,1,dim//2-n_zero_freqs)]) # [dim//2]\n",
        "        # pos = torch.linspace(0, 1, seq_len).unsqueeze(-1) # [t,1]\n",
        "\n",
        "        theta = (speed*pos) # [t,1]*[dim//2]=[t,d//2]\n",
        "        self.theta = theta\n",
        "        cos, sin = torch.cos(theta), torch.sin(theta)\n",
        "        self.affine = torch.stack([cos, -sin, sin, cos], dim=-1).unflatten(-1,(2,2)).to(device) # [t,d//2,4]-> [1,1,t,d//2,2,2]\n",
        "\n",
        "    def forward(self, x): # [b,h,t,d]\n",
        "        return (self.affine @ x.unflatten(-1, (-1,2)).unsqueeze(-1)).flatten(-3) # [1,1,t,d//2,2,2] @ [b,h,t,d//2,2,1] = [b,h,t,d]\n",
        "\n",
        "# dim=64\n",
        "# n_heads=4\n",
        "# seq_len=64\n",
        "# rope = RoPE(dim, seq_len, top=torch.pi, base=100)\n",
        "# # rope = RoPE(dim, seq_len, min_freq=1, max_freq=200, n_zero_freqs=0)\n",
        "\n",
        "# x = torch.rand(2, n_heads, seq_len, dim, device=device)\n",
        "# out = rope(x)\n",
        "# print(out.shape)\n",
        "\n",
        "# theta = rope.theta # [t,d//2]\n",
        "# sim = torch.cos(theta-theta[0].unsqueeze(0)).T\n",
        "# # print(sim.shape)\n",
        "\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# def imshow(img):\n",
        "#     npimg = img.numpy()\n",
        "#     print(npimg.shape)\n",
        "#     plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "#     plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "#     plt.show()\n",
        "\n",
        "# import torchvision\n",
        "# imshow(torchvision.utils.make_grid(sim, nrow=dim//2))\n",
        "\n",
        "\n",
        "\n",
        "class RotEmb(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, top=torch.pi, base=1000):\n",
        "        super().__init__()\n",
        "        self.theta = top / (base ** (torch.arange(0, dim, step=2, device=device) / dim))\n",
        "        # self.theta = top / (base ** torch.linspace(0, 1, dim//2, device=device))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (pos.unsqueeze(-1) * self.theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [seq_len, dim]\n",
        "\n",
        "# emb = RotEmb(dim, top=torch.pi, base=1000)\n",
        "\n",
        "# theta = emb.theta\n",
        "# print(theta)\n",
        "# pos = torch.arange(0,200,1)\n",
        "# angles = (pos.unsqueeze(-1) * theta).T # [b,t]\n",
        "# sim = torch.cos(angles-angles[:,0].unsqueeze(-1))\n",
        "# print(sim.shape)\n",
        "\n",
        "# imshow(torchvision.utils.make_grid(sim, nrow=dim//2))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "f6T4F651kmGh"
      },
      "outputs": [],
      "source": [
        "# @title AttentionBlock\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class SelfAttn(nn.Module):\n",
        "    def __init__(self, dim, n_heads):\n",
        "        super().__init__()\n",
        "        self.dim, self.n_heads = dim, n_heads\n",
        "        d_head = dim//n_heads\n",
        "        self.rope = RoPE(d_head, seq_len=64, top=torch.pi, base=1000)\n",
        "        # self.rope = RoPE(d_head, seq_len=512, base=1000)\n",
        "        # self.rope = RoPE2D(d_head, h=64, w=64, base=100)\n",
        "        # self.rope[0] = 1 # id for smry h\n",
        "        self.qkv = nn.Linear(dim, dim*3, bias=False)\n",
        "        # self.lin = nn.Linear(dim, dim)\n",
        "        self.lin = zero_module(nn.Linear(dim, dim))\n",
        "\n",
        "        self.scale = d_head**-.5\n",
        "\n",
        "    # def forward(self, x): # [b,t,d]\n",
        "    def forward(self, x, pos=None): # [b,t,d]\n",
        "        q,k,v = self.qkv(x).unflatten(-1, (self.n_heads,-1)).transpose(1,2).chunk(3, dim=-1) # [b, t, n_heads, d_head] -> [b, n_heads, t, d_head]\n",
        "        # q, k = self.rope(q), self.rope(k)\n",
        "        if pos != None:\n",
        "            # rope = self.rope[pos]\n",
        "            # q, k = q*rope, k*rope\n",
        "            q, k = self.rope(q, pos), self.rope(k, pos)\n",
        "\n",
        "        # x = F.scaled_dot_product_attention(q,k,v, attn_mask=None) # https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html\n",
        "        q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2)\n",
        "        context = k.transpose(-2,-1) @ v # [batch, n_heads, d_head, d_head]\n",
        "        x = q @ context # [b, n_heads, t, d_head]\n",
        "        # # print('SelfAttn', x.shape)\n",
        "        x = x.transpose(1,2).flatten(2)\n",
        "        return self.lin(x)\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, mult=4, drop=0.):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        # self.norm = nn.RMSNorm(d_model, elementwise_affine=False) # LayerNorm RMSNorm\n",
        "        self.norm1, self.norm2 = nn.LayerNorm(d_model), nn.LayerNorm(d_model)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "        self.attn = SelfAttn(d_model, n_heads)\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        ff_dim=d_model*mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), act, nn.Dropout(drop), nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "            nn.RMSNorm(ff_dim), act, nn.Dropout(drop), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(drop), nn.Linear(d_model, ff_dim), act,\n",
        "            # nn.RMSNorm(ff_dim), nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "\n",
        "    # def forward(self, x): # [b,t,d]\n",
        "    def forward(self, x, pos=None): # [b,t,d]\n",
        "        # print('attnblk fwd',x.shape)\n",
        "        # x = x + self.drop(self.attn(self.norm1(x)))\n",
        "        x = x + self.drop(self.attn(self.norm1(x), pos))\n",
        "        x = x + self.ff(x)\n",
        "        # x = x + self.drop(self.ff(self.norm2(x)))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gdtmiB_Zzn5u"
      },
      "outputs": [],
      "source": [
        "# @title ssd me\n",
        "# https://goombalab.github.io/blog/2024/mamba2-part4-systems/\n",
        "# https://tridao.me/blog/2024/mamba2-part3-algorithm/\n",
        "# # https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/triton/ssd_combined.py\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def segsum(x): # [...,c] # Naive segment sum calculation. exp(segsum(A)) produces a 1-SS matrix, which is equivalent to a scalar SSM.\n",
        "    T = x.size(-1)\n",
        "    x_cumsum = torch.cumsum(x, dim=-1)\n",
        "    x_segsum = x_cumsum.unsqueeze(-1) - x_cumsum.unsqueeze(-2) # [...,c,c] # vert-hori\n",
        "    mask = torch.triu(torch.ones(T, T, device=x.device, dtype=bool), diagonal=1)\n",
        "    return x_segsum.masked_fill(mask, -torch.inf) # [...,c,c]\n",
        "\n",
        "# def ssd(X, A, B, C, h0=None, b_ind=None, chunk=64): # X:[b,h,t,d], A:[b,h,t], B/C:[b,h,t,s], h0:[b,h,d,s], b_ind:[b]\n",
        "def ssd(X, A, B, C, h0=None, msk=None, b_ind=None, chunk=64): # X:[b,h,t,d], A:[b,h,t], B/C:[b,h,t,s], h0:[b,h,d,s], msk:[b,t], b_ind:[b]\n",
        "    # print('ssd', X.dtype, A.dtype, B.dtype)\n",
        "    # assert X.dtype == A.dtype == B.dtype == C.dtype\n",
        "    assert X.shape[:-1] == A.shape == B.shape[:-1]\n",
        "    assert h0==None or b_ind==None\n",
        "    # print('ssd', X.shape, A.shape, B.shape)\n",
        "    if b_ind!=None: A[:,:,b_ind[1:-1]] = 0 # at boundaries, A=0\n",
        "    if X.shape[2] % chunk != 0: X, A, B, C = [x.unsqueeze(2) for x in (X, A, B, C)]\n",
        "    else: X, A, B, C = [x.unflatten(2, (-1,chunk)) for x in (X, A, B, C)] # [b,h,t/c,c(,d/s)]\n",
        "\n",
        "    # 1. Compute the output for each intra-chunk (diagonal blocks)\n",
        "    L = torch.exp(segsum(A)) # [b,h,t/c,c,c]\n",
        "    # if msk!=None: L[msk] = 0 # L = L.masked_fill(msk, 0)\n",
        "    if msk!=None: # this only saves computation from ssd. full info leakage, full compute in in_proj\n",
        "        b,h,l,c,c = L.shape\n",
        "        assert msk.shape==(b,l*c)\n",
        "        msk = msk.reshape(b,1,l,c)\n",
        "        msk = msk.unsqueeeze(-2) | msk.unsqueeze(-1) # [b,1,t/c,c,c]\n",
        "        L[msk] = 0 # L = L.masked_fill(msk, 0)\n",
        "    Y_diag  = torch.einsum(\"...cs,...ks,...ck,...kd->...cd\", C, B, L, X) # bhlcs,bhlks,bhlck,bhlkd->bhlcd # full CA...ABX? for chunks\n",
        "\n",
        "    # 2. Compute the state for each intra-chunk # (right term of low-rank factorization of off-diagonal blocks; B terms)\n",
        "    A_cumsum = torch.cumsum(A, dim=-1) # [b,h,t/c,c]\n",
        "    decay_states = torch.exp((A_cumsum[...,-1:] - A_cumsum)) # [b,h,t/c,c] # Ai+1...T\n",
        "    states = torch.einsum(\"...cs,...c,...cd->...ds\", B, decay_states, X) # bhlcs,bhlc,bhlcd->bhlds # BiXiAi+1...T\n",
        "\n",
        "    # 3. Compute the inter-chunk SSM recurrence; produces correct SSM states at chunk boundaries # (middle term of factorization of off-diag blocks; A terms)\n",
        "    if h0==None: h0 = torch.zeros_like(states[:,:,0], device=states.device) # [b,h,d,s]\n",
        "    states = torch.cat([h0.unsqueeze(2), states], dim=2) # [b,1+t/c,h,d,s] # h0,hc,h2c,...,ht\n",
        "    decay_chunk = torch.exp(segsum(F.pad(A_cumsum[...,-1], (1,0)))) # [b,h,1+t/c]-> # [b,h,1+t/c,1+t/c] # 1,A1...1t/c,A1t/c+1...2t/c,...,A(c-1)t/c...At\n",
        "    new_states = torch.einsum(\"...tl,...lds->...tds\", decay_chunk, states) # bhtl,bhlds->bhtds # h0, BiXi/A1...i-1,\n",
        "\n",
        "    # 4. Compute state -> output conversion per chunk # (left term of low-rank factorization of off-diagonal blocks; C terms)\n",
        "    Y_off = torch.einsum('...cs,...ds,...c->...cd', C, new_states[:,:,:-1], torch.exp(A_cumsum)) # bhlcs,bhlds,bhlc->bhlcd # offset for each chunk # C1h0A1, Ci BiXi/A1...i-1,\n",
        "    Y = (Y_diag+Y_off).flatten(2,3)\n",
        "    return Y, new_states[:,:,-1] # [b,t,h,d], [b,h,d,s]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "E9ri_f5IGL5T"
      },
      "outputs": [],
      "source": [
        "# @title euler zoh bilinear diagonal A\n",
        "import torch\n",
        "\n",
        "# Euler: Abar = 1 + âˆ†A ; Bbar = âˆ†B\n",
        "def euler(A, B, dt): # bts, btsd, bt / bths, bthsd, bt\n",
        "    # return 1+dt.unsqueeze(-1)*A, dt[...,None,None]*B\n",
        "    return 1+torch.einsum('bt,bt...s->bt...s', dt, A), torch.einsum('bt,bt...sd->bt...sd', dt, B)\n",
        "\n",
        "# ZOH: Abar = exp(âˆ†A) ; Bbar = (exp(âˆ†A)-I)/A Â· B ~ (1+A/2)B # For numerical stability as A->0 # (e^x -1)/x ~ 1+(x/2) # from taylor expansion, e^x ~ 1 + x + x^2/2 + x^3/6 + x^4/24 + ...\n",
        "def zoh(A, B, dt): # btd, btds, bt\n",
        "    # A_bar = torch.exp(dt.unsqueeze(-1)*A)\n",
        "    A_bar = torch.exp(torch.einsum('bt,bt...s->bt...s', dt, A))\n",
        "    return A_bar, ((A_bar-1)/A).unsqueeze(-1) * B\n",
        "    # return torch.exp(dt.unsqueeze(-1)*A), (1+A/2).unsqueeze(-1) * B # For numerical stability\n",
        "\n",
        "# Bilinear: Abar = (1+âˆ†A/2) / (1-âˆ†A/2) ; Bbar = âˆ†B/(1-âˆ†A/2)\n",
        "def bilinear(A, B, dt): # btd, btds, bt\n",
        "    # dA_2 = dt.unsqueeze(-1)*A/2\n",
        "    dA_2 = torch.einsum('bt,bt...s->bt...s', dt, A/2)\n",
        "    # return (1+dA_2)/(1-dA_2), (dt.unsqueeze(-1)/(1-dA_2)).unsqueeze(-1) * B\n",
        "    return (1+dA_2)/(1-dA_2), torch.einsum('bt,bt...s->bt...s', dt, 1-dA_2).unsqueeze(-1) * B\n",
        "\n",
        "# b,t,d,s = 2,5,8,4\n",
        "# h=3\n",
        "# A = -torch.randn(b,t,d) # Negative for stability?\n",
        "# B = torch.randn(b,t,d,s)\n",
        "# dt = torch.rand(b,t)*.1\n",
        "# # A_bar, B_bar = euler(A, B, dt)\n",
        "# # A_bar, B_bar = zoh(A, B, dt)\n",
        "# A_bar, B_bar = bilinear(A, B, dt)\n",
        "# print(A_bar.shape, B_bar.shape)\n",
        "\n",
        "# A = -torch.randn(b,t,h,d) # Negative for stability?\n",
        "# B = torch.randn(b,t,h,d,s)\n",
        "\n",
        "# # A_bar, B_bar = euler(A, B, dt)\n",
        "# # print(A_bar.shape, B_bar.shape)\n",
        "# # A_bar, B_bar = zoh(A, B, dt)\n",
        "# # print(A_bar.shape, B_bar.shape) # [2, 5, 8, 8], [2, 5, 8, 4]\n",
        "# A_bar, B_bar = bilinear(A, B, dt)\n",
        "# print(A_bar.shape, B_bar.shape) # [2, 5, 8, 8], [2, 5, 8, 4]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "6eeu0m-bUSdh"
      },
      "outputs": [],
      "source": [
        "# @title hydra me\n",
        "# https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba2_simple.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "# @torch.compile()\n",
        "class Hydra(nn.Module):\n",
        "    def __init__(self, d_model, expand=3, n_heads=8, n_groups=8, d_state=8, d_conv=7):\n",
        "        super().__init__()\n",
        "        n_groups = min(n_heads, n_groups)\n",
        "        assert n_heads % n_groups == 0, \"nheads must be divisible by ngroups\"\n",
        "        self.d_model, self.n_groups, self.d_state, self.d_conv = d_model, n_groups, d_state, d_conv\n",
        "        self.d_inner = expand * self.d_model\n",
        "        self.n_heads, self.d_head = n_heads, self.d_inner//n_heads\n",
        "\n",
        "        # self.in_proj = nn.Linear(self.d_model, 2* self.d_inner + 2* self.n_groups*self.d_state + self.n_heads, bias=False) # z,x,B,C,A\n",
        "        self.in_proj = nn.Linear(self.d_model, 2* self.d_inner + 2* self.n_groups*self.d_state + 2* self.n_heads, bias=True) # z,x,B,C,A\n",
        "        # with torch.no_grad(): self.in_proj.weight[self.d_inner:] = .1*self.in_proj.weight[self.d_inner:]\n",
        "        # conv_dim = self.d_inner + 2*self.n_groups*self.d_state # for x,B,C\n",
        "        # self.conv1d = nn.Conv1d(conv_dim, conv_dim, kernel_size=d_conv, groups=conv_dim, padding=d_conv//2, bias=True)\n",
        "        self.conv1d = nn.Conv1d(d_model, d_model, kernel_size=d_conv, groups=d_model, padding=d_conv//2, bias=True)\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "        self.A_log = nn.Parameter(torch.log(torch.rand(self.n_heads)))\n",
        "        # self.h0 = nn.Parameter(torch.zeros(self.n_heads, self.d_head, self.d_state))\n",
        "        # self.D = nn.Parameter(torch.ones(self.n_heads)) # from mamba\n",
        "        self.D = nn.Linear(self.d_inner, self.n_heads) # og\n",
        "        self.norm = nn.RMSNorm(self.d_inner)\n",
        "        self.out = zero_module(nn.Linear(self.d_inner, self.d_model, bias=False))\n",
        "\n",
        "    # def forward(self, u, h=None, mask=None, b_ind=None): # [b,t,d], ([b,k-1,xbc], [b,h,d,s]), [b+1]\n",
        "    # def forward(self, u, h=None, ctx_idx= None, mask=None, b_ind=None): # [b,t,d], ([b,k-1,xbc], [b,h,d,s]), [b+1]\n",
        "    def forward(self, u, h=None, step= None, mask=None, b_ind=None): # [b,t,d], ([b,k-1,xbc], [b,h,d,s]), [b+1]\n",
        "        b, t = u.shape[:2]\n",
        "        if h==None: h_conv, h_ssm = None, None\n",
        "        else: h_conv, h_ssm = h\n",
        "        # ctx_idx = torch.cat([torch.zeros(b,1, device=u.device), ctx_idx], dim=-1) # [b,1+t]\n",
        "        # step = ctx_idx[:,1:] - ctx_idx[:,:-1] # [b,t]\n",
        "        # step = torch.ones(b,t, device=u.device) #\n",
        "        # step = torch.cat((step, torch.flip(step,(1,))), dim=0) # [b,t]->[2b,t]\n",
        "        if step==None: step = torch.ones(2*b,t, device=u.device) # [b,t]\n",
        "        else: step = torch.cat((step[:,:-1], torch.flip(step[:,1:],(1,))), dim=0).to(device) # [b,t+1]->[2b,t]\n",
        "\n",
        "        # u = self.act(self.conv1d(u.transpose(-2,-1)).transpose(-2,-1)) # [b,t,inr+2gs]\n",
        "        # z, xBC, A = self.in_proj(u).split([self.d_inner, self.d_inner + 2*self.n_groups*self.d_state, self.n_heads], dim=-1)\n",
        "        # z, xBC, A, dt = self.in_proj(u).split([self.d_inner, self.d_inner + 2*self.n_groups*self.d_state, self.n_heads, self.n_heads], dim=-1)\n",
        "        z, to_flip = self.in_proj(u).split([self.d_inner, self.d_inner + 2*self.n_groups*self.d_state + 2*self.n_heads], dim=-1)\n",
        "        # z, x, B, C, A, dt = self.in_proj(u).split([self.d_inner, self.d_inner, self.n_groups*self.d_state, self.n_groups*self.d_state, self.n_heads, self.n_heads], dim=-1)\n",
        "\n",
        "        to_flip = torch.cat((to_flip, torch.flip(to_flip,(1,))), dim=0) # [b,t,xbc]->[2b,t,xbc]\n",
        "        # x, B, C = to_flip.split([self.d_inner, self.n_groups*self.d_state, self.n_groups*self.d_state], dim=-1)\n",
        "        x, B, C, A, dt = to_flip.split([self.d_inner, self.n_groups*self.d_state, self.n_groups*self.d_state, self.n_heads, self.n_heads], dim=-1)\n",
        "\n",
        "        # dt = torch.cat((dt, torch.flip(dt,(1,))), dim=0) # [b,t,h]->[2b,t,h]\n",
        "        dt = F.softplus(dt)\n",
        "        # A = torch.cat((A, torch.flip(A,(1,))), dim=0) # [b,t,h]->[2b,t,h]\n",
        "        # A = -torch.exp(self.A_log) # mamba2\n",
        "        # A = -torch.exp(A) #\n",
        "        # A = -F.softplus(-A) # log(1+e^x)\n",
        "        A = -F.sigmoid(-A) # 1/(1+e^-x)\n",
        "        # print('mamba fwd0', A[-1,-3:,-5:])\n",
        "\n",
        "        # print('mamba fwd', xBC.shape, h_conv.shape)\n",
        "        # xBC = self.act(self.conv1d(xBC.transpose(-2,-1)).transpose(-2,-1)) # [b,t,inr+2gs]\n",
        "\n",
        "\n",
        "        x_og = x[:b] # [b,t,inr]\n",
        "        x, B, C = x.unflatten(-1, (self.n_heads, self.d_head)), B.unflatten(-1, (self.n_groups, self.d_state)), C.unflatten(-1, (self.n_groups, self.d_state)) # x:[2b,t,h,d], B/C:[2b,t,g,s]\n",
        "        # y_diag = (x[:b] * self.D.unsqueeze(-1)).flatten(2) # bthd*h1\n",
        "\n",
        "        h_g = self.n_heads//self.n_groups\n",
        "        if h_g>1: B, C = B.repeat_interleave(h_g, dim=-2), C.repeat_interleave(h_g, dim=-2) # [b,t,g,s]->[b,t,h,s]\n",
        "\n",
        "        # # A, x = (A*step[...,*[None]*(A.ndim-2)]).exp(), x*step[...,None,None] # bthd*bt11=bthd # sdd discretization\n",
        "        A, B = zoh(A, B, step) # euler zoh bilinear # bth,bths\n",
        "        # print('step', step.max())\n",
        "        # print('step', step)\n",
        "        # # print('mamba fwd', A.shape, B.shape, dt.shape)\n",
        "        # # A, B = zoh(A, B*dt.unsqueeze(-1), step) # euler zoh bilinear # bth,bths\n",
        "        # # A, B = zoh(A*dt, B*dt.unsqueeze(-1), step) # euler zoh bilinear # bth,bths\n",
        "        # # print('mamba fwd1', A[-1,-3:,-5:])\n",
        "        # dt = dt * step.unsqueeze(-1) # [2b,t,h]*[2b,t,1]\n",
        "\n",
        "# x:bthd, dt:bthd, A.exp:hds, B/C:bts, D:hd # mamba1\n",
        "# x:bhtd, dt:bht, A:h, B/C:bgts-repeat>bhts, D:h # mamba2\n",
        "# mamba(ssd):\n",
        "# h = Ah + Bx : A*h + x@B = 1/ds*ds + d1@1s = ds\n",
        "# y = Ch + Dx : h@C + D*x = ds@s1 + 1/d1*d1 = d1\n",
        "\n",
        "        # print('mamba fwd', x.shape, A.shape, B.shape)\n",
        "        x, A, B, C = [a.transpose(1,2) for a in (x, A, B, C)] # X:[b,h,t,d], A:[b,h,t], B/C:[b,h,t,s]\n",
        "        dt = dt.transpose(1,2) # [b,h,t]\n",
        "# x:bhtd, A:bht, B/C:bhts, 10.5s\n",
        "        y, h_ssm = ssd(x, A.log(), B, C, h_ssm, msk=mask, b_ind=b_ind, chunk=64) #\n",
        "        # y, h_ssm = ssd(x*dt.unsqueeze(-1), A*dt, B, C, h_ssm, msk=mask, b_ind=b_ind, chunk=64) # 256\n",
        "\n",
        "        y = y.transpose(1,2).flatten(2) # [b,t,d]/[1,b*t,d]\n",
        "\n",
        "        y = torch.roll(y, shifts=1, dims=1) # 123...l -> l12...l-1\n",
        "        y[:,0] = 0 # 012...l-1\n",
        "        y = y[:b] + torch.flip(y[b:], (1,)) + x_og * self.D(x_og).repeat(1,1,self.d_head) # [b,t,inr]\n",
        "        # y = y[:b] + torch.flip(y[b:], (1,)) +  y_diag # [b,t,inr]\n",
        "        y = self.norm(y * self.act(z)) # [b,t,inr] # norm(x)*silu(z) if norm_before_gate, else norm(x*silu(z)) # https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/triton/layernorm_gated.py#L18\n",
        "        out = self.out(y)\n",
        "        return out#, (h_conv, h_ssm) # [b,t,in], ([b,k-1,xbc], [b,h,d,s])\n",
        "\n",
        "\n",
        "b,t,d_model=5,256,32\n",
        "# b,t,d_model=5,7,32\n",
        "u = torch.randn(b,t,d_model, device=device)\n",
        "model = Hydra(d_model).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) #\n",
        "# out, h = model(u)\n",
        "out = model(u)\n",
        "# h0 = torch.randn(b, model.n_heads, model.d_head, model.d_state)\n",
        "# print(out.shape)\n",
        "# print(out.shape, h.shape)\n",
        "# print(out[0,-3:,:5], h[0,:2,:5,:5])\n",
        "# out, h = model(u, h)\n",
        "\n",
        "print(out.shape)\n",
        "print(out[0,-3:,:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pQfM2fYvcTh6"
      },
      "outputs": [],
      "source": [
        "# @title masks\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# def multiblock(seq, min_s, max_s, M=1):\n",
        "#     mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "#     mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "#     mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "#     indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "#     target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "#     return target_mask\n",
        "\n",
        "\n",
        "# def multiblock2d(hw=(8,8), scale=(.15,.2), aspect_ratio=(.75,1.5), M=1):\n",
        "#     mask_aspect = torch.rand(1) * (aspect_ratio[1] - aspect_ratio[0]) + aspect_ratio[0] # in (min_s, max_s) # all blocks same size\n",
        "#     mask_scale = torch.rand(1) * (scale[1] - scale[0]) + scale[0] # in (min_s, max_s) # all blocks same size\n",
        "#     h = (mask_scale/mask_aspect)**.5# h*(h*aspect) = scale\n",
        "#     w = h * mask_aspect\n",
        "#     h_pos, w_pos = torch.rand(M)*(1-w), torch.rand(M)*(1-h) # in (0, 1 - mask_len)\n",
        "#     h_len, h_pos = (h*hw[0]).int(), h_pos*hw[0]\n",
        "#     w_len, w_pos = (w*hw[1]).int(), w_pos*hw[1]\n",
        "#     h_ind, w_ind = torch.arange(hw[0]).unsqueeze(0), torch.arange(hw[1]).unsqueeze(0) # [1, seq]\n",
        "#     h_mask = (h_ind>=h_pos.unsqueeze(-1)) & (h_ind<(h_pos+h_len).unsqueeze(-1)) # [M, seq]\n",
        "#     w_mask = (w_ind>=w_pos.unsqueeze(-1)) & (w_ind<(w_pos+w_len).unsqueeze(-1)) # [M, seq]\n",
        "#     target_mask = h_mask.unsqueeze(-1) & w_mask.unsqueeze(-2) # [M, seq, seq]\n",
        "#     return target_mask\n",
        "\n",
        "# # https://arxiv.org/pdf/2210.07224\n",
        "# def randpatch(seq, mask_size=8, gamma=0.9): # num patches of seq, mask patch size, masking ratio\n",
        "#     # mask = torch.rand(seq//mask_size)<gamma\n",
        "#     length = seq//mask_size\n",
        "#     g = torch.normal(gamma, std=.1, size=(1,)).clamp(.5,.9)\n",
        "#     # g = gamma\n",
        "#     idx = torch.randperm(length)[:int(length*g)]\n",
        "#     mask = torch.zeros(length, dtype=bool)\n",
        "#     mask[idx] = True\n",
        "#     mask = mask.repeat_interleave(mask_size, dim=-1)\n",
        "#     return mask # [seq] , True -> mask\n",
        "\n",
        "\n",
        "# import torch\n",
        "# def apply_masks(x, mask): # [b,t,d], [mask_size] # https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "#     mask_keep = mask.unsqueeze(-1).repeat(x.size(0), 1, x.size(-1)) # [batch,T,dim]\n",
        "#     return torch.gather(x, dim=1, index=mask_keep) # [batch,mask_size,dim]\n",
        "\n",
        "\n",
        "# @title ijepa multiblock next\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "import math\n",
        "from multiprocessing import Value\n",
        "import torch\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(self, hw=(224, 224), enc_mask_scale=(.85,1), pred_mask_scale=(.15,.2), aspect_ratio=(.75,1.25),\n",
        "        nenc=1, npred=2, min_keep=4, allow_overlap=False):\n",
        "        super().__init__()\n",
        "        self.height, self.width = hw\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.aspect_ratio = aspect_ratio\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "\n",
        "    def _sample_block_size(self, scale, aspect_ratio_scale):\n",
        "        _rand = torch.rand(1).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.height * self.width * mask_scale) # num patches to keep\n",
        "        # -- Sample block aspect-ratio\n",
        "        min_ar, max_ar = aspect_ratio_scale\n",
        "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
        "        while h >= self.height: h -= 1 # crop mask to be smaller than img\n",
        "        while w >= self.width: w -= 1\n",
        "        return (h, w)\n",
        "\n",
        "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
        "        h, w = b_size\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            top = torch.randint(0, self.height - h, (1,))\n",
        "            left = torch.randint(0, self.width - w, (1,))\n",
        "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
        "            mask[top:top+h, left:left+w] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
        "        mask_complement[top:top+h, left:left+w] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, B):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        p_size = self._sample_block_size(scale=self.pred_mask_scale, aspect_ratio_scale=self.aspect_ratio)\n",
        "        e_size = self._sample_block_size(scale=self.enc_mask_scale, aspect_ratio_scale=(1., 1.))\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.height * self.width\n",
        "        min_keep_enc = self.height * self.width\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                print(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "        return collated_masks_enc, collated_masks_pred\n",
        "\n",
        "mask_collator = MaskCollator(hw=(32,32), enc_mask_scale=(.85, 1.), pred_mask_scale=(.15, .2), aspect_ratio=(.75, 1.5),\n",
        "        nenc=1, npred=4, min_keep=4,\n",
        "        # allow_overlap=True)\n",
        "        allow_overlap=False)\n",
        "\n",
        "b=16\n",
        "collated_masks_enc, collated_masks_pred = mask_collator(b)\n",
        "ctx_index, trg_index = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "# imshow(mask)\n",
        "\n",
        "mask = torch.zeros(b ,32*32)\n",
        "mask[torch.arange(b).unsqueeze(-1), trg_index] = 1\n",
        "mask[torch.arange(b).unsqueeze(-1), ctx_index] = .5\n",
        "mask = mask.reshape(b,1,32,32)\n",
        "print((mask==1).flatten(1).sum(-1))\n",
        "print((mask==.5).flatten(1).sum(-1))\n",
        "import torchvision\n",
        "imshow(torchvision.utils.make_grid(mask, nrow=8))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Nd1GqX3PYpIk"
      },
      "outputs": [],
      "source": [
        "# @title Random Fourier Features noise\n",
        "import math\n",
        "import torch\n",
        "\n",
        "# Random Fourier Features: âˆ‘_k] a_k * cos(Ï‰_k â‹… x + ðœ™_k)\n",
        "def rff_noise(x, out_dim, n_freqs=128, scale=1): # [...,n_Dim]\n",
        "    space, in_dim, device = x.shape[:-1], x.shape[-1], x.device\n",
        "    x = x.flatten(0,-2) # [N,in]\n",
        "    w = torch.randn(out_dim, n_freqs, in_dim, device=device) * scale\n",
        "    phi = torch.empty(out_dim, n_freqs, device=device).uniform_(0,2*math.pi)\n",
        "    a = torch.randn(out_dim, n_freqs, device=device) / math.sqrt(n_freqs)\n",
        "    y = torch.cos(torch.einsum(\"ofd,...d->...of\", w, x) + phi) # [N,out,freq]\n",
        "    return torch.einsum(\"of,...of->...o\", a, y).unflatten(0,space) # [...,out]\n",
        "\n",
        "def rffmask2d(bhw, ctx_scale=(.85,1.), trg_scale=(.6,.8), chaos=[1,.5]): # ctx_scale > trg_scale\n",
        "# def rffmask2d(bhw, ctx_scale=(.05,.5), trg_scale=(.2,.8), chaos=[1,.5],): # ctx_scale > trg_scale\n",
        "    b,h,w = bhw\n",
        "    ctx_mask_scale = torch.rand(1) * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    trg_mask_scale = torch.rand(1) * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "    seq = h*w\n",
        "    ctx_len, trg_len = int(seq*ctx_mask_scale), int(seq*trg_mask_scale)\n",
        "    # print(ctx_len, trg_len)\n",
        "    # ctx_len = ctx_len - trg_len\n",
        "    ctx_len = min(ctx_len, seq-trg_len)\n",
        "\n",
        "    ix, iy = torch.linspace(0, chaos[0], h), torch.linspace(0, chaos[0], w)\n",
        "    x = torch.stack(torch.meshgrid(ix, iy, indexing=\"ij\"), dim=-1) # [t,h,w,b] / [h,w,b]\n",
        "    noise = rff_noise(x, b).flatten(0,-2).T # [h,w,b]->[h*w,b]->[b,h*w]\n",
        "    _, trg_ind = torch.topk(noise, trg_len, dim=1, sorted=False)\n",
        "\n",
        "    ix, iy = torch.linspace(0, chaos[1], h), torch.linspace(0, chaos[1], w)\n",
        "    x = torch.stack(torch.meshgrid(ix, iy, indexing=\"ij\"), dim=-1) # [t,h,w,b] / [h,w,b]\n",
        "    noise = rff_noise(x, b).flatten(0,-2).T\n",
        "    noise.scatter_(1, trg_ind, -torch.inf).flatten()\n",
        "    _, ctx_ind = torch.topk(noise, ctx_len, dim=1, sorted=False)\n",
        "    return ctx_ind, trg_ind\n",
        "\n",
        "b=16\n",
        "# hw=(8,8)\n",
        "hw=(32,32)\n",
        "# cxt_inds, trg_inds = rffmask2d((b,)+hw, ctx_scale=(.85,1.), trg_scale=(.2,.8), chaos=[3,1])\n",
        "# cxt_inds, trg_inds = rffmask2d((b,)+hw, ctx_scale=(.85,1.), trg_scale=(.2,.8), chaos=[1,.5])\n",
        "cxt_inds, trg_inds = rffmask2d((b,)+hw, ctx_scale=(.05,.5), trg_scale=(.2,.8), chaos=[2,1])\n",
        "# print(cxt_inds, trg_inds)\n",
        "\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# def imshow(img):\n",
        "#     npimg = img.numpy()\n",
        "#     plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "#     plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "#     plt.show()\n",
        "# # imshow(mask)\n",
        "\n",
        "# mask = torch.zeros(b ,math.prod(hw))\n",
        "# mask[torch.arange(b).unsqueeze(-1), trg_inds] = 1\n",
        "# mask[torch.arange(b).unsqueeze(-1), cxt_inds] = .5\n",
        "# # print((mask==1).sum(1))\n",
        "# # print((mask==.5).sum(1))\n",
        "# mask = mask.reshape(b,1,*hw)\n",
        "# import torchvision\n",
        "# imshow(torchvision.utils.make_grid(mask, nrow=8))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivqYjBIVrB3e",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title ViT\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        self._kwargs = [[name for name, p in inspect.signature(layer.forward).parameters.items()] for layer in self]\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        for layer, _kwargs in zip(self, self._kwargs):\n",
        "            x = layer(x, *args, **{k: v for k, v in kwargs.items() if k in _kwargs})\n",
        "        return x\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, patch_size, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        patch_size=2\n",
        "        self.hw=(8,8) # (8,8) (32,32)\n",
        "        self.seq = self.hw[0]*self.hw[1]\n",
        "\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Conv2d(in_dim, d_model, patch_size, patch_size), # like patch\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 1, 7//2, bias=False)\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.MaxPool2d(3,2,1)\n",
        "            nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.BatchNorm2d(d_model), nn.ReLU(),\n",
        "            nn.Conv2d(d_model, d_model, 3, 2, 3//2, bias=False)\n",
        "            )\n",
        "        # self.embed.requires_grad=False\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, self.seq, d_model)*.02)\n",
        "        # self.pos_emb = nn.Parameter(RoPE2D(dim=d_model, h=8, w=8, base=1000), requires_grad=False)\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=(32//patch_size)**2, base=10000), requires_grad=False)\n",
        "\n",
        "        self.transformer = Seq(*[AttentionBlock(d_model, d_model, n_heads) for _ in range(nlayers)])\n",
        "        # self.transformer = Seq(*[Hydra(d_model, n_heads) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, cxt_inds=None): # [b,ctx,3], [b,ctx] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x).flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c] # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "\n",
        "        # x = self.pos_enc(x)\n",
        "        # print(\"TransformerModel\",x.shape, self.pos_emb.shape)\n",
        "        x = x + self.pos_emb[:,:x.shape[1]]\n",
        "        if cxt_inds != None: x = x[torch.arange(x.shape[0]).unsqueeze(-1), cxt_inds] # [batch, num_context_toks, d_model]\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        # if cxt_inds != None: # for pos\n",
        "        #     # x = self.transformer(x[torch.arange(x.shape[0]).unsqueeze(-1), cxt_inds], cxt_inds) # for pos attn\n",
        "        #     b = x.shape[0]\n",
        "        #     # print('vit fwd', cxt_inds)\n",
        "        #     ctx_idx = torch.cat([torch.full((b,1),-1), cxt_inds, torch.full((b,1),self.seq)], dim=-1) # [b,1+t+1]\n",
        "        #     step = ctx_idx[:,1:] - ctx_idx[:,:-1] # [b,t+1]\n",
        "        #     x = self.transformer(x[torch.arange(x.shape[0]).unsqueeze(-1), cxt_inds], step=step)\n",
        "        # else: x = self.transformer(x)\n",
        "\n",
        "        out = self.norm(x)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out\n",
        "\n",
        "d_model = 64\n",
        "in_dim = 3\n",
        "patch_size = 2\n",
        "# model = ViT(patch_size, in_dim, d_model, n_heads=4, nlayers=4, drop=0.).to(device)\n",
        "model = ViT(patch_size, in_dim, d_model, n_heads=4, nlayers=1, drop=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x = torch.rand((5, in_dim, 32, 32), device=device) # [b,c,h,w]\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-zdjdJixtOu",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Transformer Predictor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        self.hw=(8,8) # (8,8) (32,32)\n",
        "        self.seq = self.hw[0]*self.hw[1]\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, self.seq, d_model)*0.02)\n",
        "\n",
        "        self.transformer = Seq(*[AttentionBlock(d_model, n_heads) for _ in range(nlayers)])\n",
        "        # self.transformer = Seq(*[Hydra(d_model, n_heads) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.randn(1,1,d_model)*.02) # randn zeros\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim)# if out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, cxt_inds, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        b, seq, dim = x.shape\n",
        "\n",
        "        # # x = x * self.pos_enc(cxt_inds)\n",
        "        # # pred_tokens = self.cls * self.pos_enc(trg_indices) # [M, num_trg_toks, d_model]\n",
        "        x = x + self.pos_emb[0,cxt_inds]\n",
        "        pred_tokens = self.cls + self.pos_emb[0,trg_indices]\n",
        "        # # print(\"pred fwd\", x.shape, pred_tokens.shape)\n",
        "        x = torch.cat([x, pred_tokens], dim=1) # [batch, seq_len+num_trg_toks, d_model]\n",
        "        out = self.transformer(x)[:,seq:] # [b,trg,d]\n",
        "\n",
        "        # # for ssm\n",
        "        # x = torch.cat([x, self.cls.expand(*trg_indices.shape,-1)], dim=1) # [batch, num_ctx_toks+num_trg_toks, d_model]\n",
        "        # val, idx = torch.sort(torch.cat([cxt_inds, trg_indices], dim=-1)) # [b,ctx+trg]\n",
        "        # x = x[torch.arange(b).unsqueeze(-1), idx]\n",
        "        # indices = torch.cat([torch.full((b,1), -1), val, torch.full((b,1), self.seq)], dim=-1) # [b,1+t+1]\n",
        "        # step = indices[:,1:] - indices[:,:-1] # [b,t+1]\n",
        "        # out = self.transformer(x, step=step)\n",
        "        # back_idx = idx[idx>=cxt_inds.shape[-1]].reshape(b,-1) # [b,trg]\n",
        "        # out = out[torch.arange(b).unsqueeze(-1), back_idx] # [b,trg,d]\n",
        "\n",
        "        # # for pos attn\n",
        "        # x = torch.cat([x, self.cls.expand(*trg_indices.shape,-1)], dim=1) # [batch, num_ctx_toks+num_trg_toks, d_model]\n",
        "        # out = self.transformer(x, torch.cat([cxt_inds, trg_indices], dim=-1))[:,seq:] # [b,trg,d]\n",
        "\n",
        "        out = self.norm(out)\n",
        "        out = self.lin(out)\n",
        "        # print(\"pred fwd\", out.shape)\n",
        "        return out # [seq_len, batch_size, ntoken]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ardu1zJwdHM3",
        "outputId": "2b54855c-e2ea-4eb6-f2b2-4fc5388b590e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "153024\n",
            "torch.Size([])\n"
          ]
        }
      ],
      "source": [
        "# @title IJEPA\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "@torch.compile\n",
        "class IJEPA(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=64, out_dim=None, nlayers=1, n_heads=4):\n",
        "        super().__init__()\n",
        "        self.out_dim = out_dim = out_dim or d_model\n",
        "        self.patch_size = 4 # 4\n",
        "        self.student = ViT(self.patch_size, in_dim, d_model, out_dim=out_dim, n_heads=n_heads, nlayers=nlayers, drop=0.)\n",
        "        self.predicter = TransformerPredictor(out_dim, d_model//2, out_dim, n_heads=4, nlayers=1, drop=0.)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "        self.hw=(8,8) # (8,8) (32,32)\n",
        "        # self.mask_collator = MaskCollator(self.hw, enc_mask_scale=(.85, 1.), pred_mask_scale=(.15, .2), aspect_ratio=(.75, 1.5), nenc=1, npred=4, min_keep=4, allow_overlap=False) # og\n",
        "\n",
        "    def loss(self, x): #\n",
        "        b,c,h,w = x.shape\n",
        "        # collated_masks_enc, collated_masks_pred = self.mask_collator(b)\n",
        "        # cxt_inds, trg_inds = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "\n",
        "        # cxt_inds, trg_inds = rffmask2d((1,)+self.hw, ctx_scale=(.85,1.), trg_scale=(.2,.8), chaos=[3,1])\n",
        "        cxt_inds, trg_inds = rffmask2d((1,)+self.hw, ctx_scale=(.05,.5), trg_scale=(.2,.8), chaos=[3,1])\n",
        "        # cxt_inds, trg_inds = rffmask2d((1,)+self.hw, ctx_scale=(.05,.5), trg_scale=(.15,.2), chaos=[3,1])\n",
        "        # cxt_inds, trg_inds = simplexmask2d(hw, ctx_scale=(.85,1), trg_scale=(.7,.8), B=b, chaos=[3,.5])\n",
        "        cxt_inds, trg_inds = cxt_inds.repeat(b,1), trg_inds.repeat(b,1)\n",
        "        # cxt_inds = cxt_inds.sort(-1)[0]\n",
        "\n",
        "        # zero_mask = torch.zeros(b ,*self.hw, device=device).flatten(1)\n",
        "        # zero_mask[torch.arange(b).unsqueeze(-1), cxt_inds] = 1\n",
        "        # x_ = x * F.interpolate(zero_mask.reshape(b,1,*self.hw), size=x.shape[2:], mode='nearest-exact') # zero masked locations\n",
        "        # sx = self.student(x_, cxt_inds=cxt_inds) # [batch, num_context_toks, out_dim]\n",
        "\n",
        "        sx = self.student(x, cxt_inds=cxt_inds) # [batch, num_context_toks, out_dim]\n",
        "        # print('ijepa loss sx',sx.shape)\n",
        "        sy_ = self.predicter(sx, cxt_inds=cxt_inds, trg_indices=trg_inds) # [batch*M, num_trg_toks, out_dim]\n",
        "        sy_ = F.layer_norm(sy_, (sy_.size(-1),))\n",
        "        with torch.no_grad():\n",
        "            sy = self.teacher(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "            # print('ijepa loss sy',sy.shape)\n",
        "            sy = sy[torch.arange(sy.shape[0]).unsqueeze(-1), trg_inds] # [batch, num_context_toks, d_model] # nan bec len(trg_ind)==0 # print('loss sy',torch.isnan(sy).any())\n",
        "            sy = F.layer_norm(sy, (sy.size(-1),))\n",
        "        loss = F.mse_loss(sy, sy_)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x): # [b,t,3]\n",
        "        out = self.student(x).mean(1)\n",
        "        return out\n",
        "\n",
        "# min_s=0.15, max_s, M\n",
        "# trg.15.2M4 C.85 1\n",
        "# dont normalise data\n",
        "# randmsk < simplex 1msk < multiblk\n",
        "# teacher=trans ~? teacher=copy\n",
        "# learn convemb\n",
        "# lr pred,student: 3e-3/1e-2, 1e-3\n",
        "\n",
        "# lr 1e-4<<<1e-3 < 1e-2,1e-3? slower but dun plateau\n",
        "# zeromsk impt, else closs start increasing\n",
        "# l1rge trg helps delay increase?\n",
        "\n",
        "# vit 1lyr 15.4sec 15.6\n",
        "# hiera downsampling attn 20.5\n",
        "\n",
        "# ijepa = IJEPA(in_dim=3, d_model=64, out_dim=64, nlayers=4, n_heads=4).to(device)\n",
        "ijepa = IJEPA(in_dim=3, d_model=64, out_dim=64, nlayers=1, n_heads=8).to(device)\n",
        "# ijepa = IJEPA(in_dim=3, d_model=32, out_dim=32, nlayers=1, n_heads=4).to(device)\n",
        "# optim = torch.optim.AdamW(ijepa.parameters(), lr=1e-3)#, weight_decay=0) # 1e-3?\n",
        "optim = torch.optim.AdamW([{'params': ijepa.student.parameters()},\n",
        "#     {'params': ijepa.predicter.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # good 3e-3,1e-3 ; default 1e-2, 5e-2\n",
        "    {'params': ijepa.predicter.parameters(), 'lr': 1e-2}], lr=1e-3, weight_decay=1e-2)\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/configs/in1k_vith14_ep300.yaml\n",
        "# d_model 1024,384\n",
        "# depth 12,6/12\n",
        "# wd 5e-2 - 4e-1\n",
        "# adamw 1e-4 - 1e-3 - 1e-6\n",
        "# ema 0.996-1\n",
        "\n",
        "print(sum(p.numel() for p in ijepa.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in ijepa.parameters())) # 27584\n",
        "# print(sum(p.numel() for p in ijepa.predicter.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# d_model^2 * nlayers\n",
        "\n",
        "x = torch.rand((64,3,32,32), device=device)\n",
        "out = ijepa.loss(x)\n",
        "print(out.shape)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(ijepa.out_dim, 10).to(device)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n",
        "# optim = torch.optim.AdamW([{'params': ijepa.parameters()}, {'params': classifier.parameters(), 'lr': 1e-3}], lr=1e-3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "2Nd-sGe6Ku4S",
        "outputId": "adf95aee-ea7a-4f73-846d-0d4e45dcd56e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–‡â–‡â–‡â–‡â–…â–†â–…â–„â–…â–…â–…â–…â–…â–…â–„â–ƒâ–ƒâ–ƒâ–ƒâ–„â–‚â–‚â–ƒâ–ƒâ–ƒâ–â–‚â–‚â–â–‚â–‚â–‚â–ƒ</td></tr><tr><td>correct</td><td>â–‚â–‚â–‚â–â–â–ƒâ–‚â–â–ƒâ–…â–ƒâ–…â–â–ƒâ–ƒâ–…â–„â–…â–‡â–†â–‡â–…â–…â–…â–†â–…â–…â–…â–ˆâ–‡â–†â–…â–…â–…â–†â–‡â–…â–‡â–…â–†</td></tr><tr><td>loss</td><td>â–„â–‚â–‚â–‚â–‚â–‚â–â–ƒâ–†â–„â–…â–ƒâ–ƒâ–‚â–…â–ƒâ–â–„â–„â–‚â–‚â–ƒâ–‚â–‚â–‚â–ƒâ–‚â–ƒâ–‚â–‚â–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–ˆâ–„</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>1.74257</td></tr><tr><td>correct</td><td>0.34375</td></tr><tr><td>loss</td><td>0.34339</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">olive-oath-143</strong> at: <a href='https://wandb.ai/bobdole/ijepa/runs/b42sp19n' target=\"_blank\">https://wandb.ai/bobdole/ijepa/runs/b42sp19n</a><br> View project at: <a href='https://wandb.ai/bobdole/ijepa' target=\"_blank\">https://wandb.ai/bobdole/ijepa</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20260205_004301-b42sp19n/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.24.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20260205_012030-yfylwfxl</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/rff/runs/yfylwfxl' target=\"_blank\">lively-oath-1</a></strong> to <a href='https://wandb.ai/bobdole/rff' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/rff' target=\"_blank\">https://wandb.ai/bobdole/rff</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/rff/runs/yfylwfxl' target=\"_blank\">https://wandb.ai/bobdole/rff/runs/yfylwfxl</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "!pip install -q wandb\n",
        "import wandb # https://docs.wandb.ai/quickstart\n",
        "wandb.login(key='487a2109e55dce4e13fc70681781de9f50f27be7')\n",
        "try: run.finish()\n",
        "except NameError: pass\n",
        "run = wandb.init(project=\"rff\", config={\"model\": \"res18\",})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34de8800-fa4b-4460-803c-3cd8dbfb6c70",
        "outputId": "030494ed-dd54-4687-be11-9ecf20064a8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i, sloss, closs, correct 0 0.6461960673332214 2.3854360580444336 0.09375\n",
            "time: 1.7531650066375732 1.7531661987304688\n",
            "i, sloss, closs, correct 1 0.521209716796875 2.3058323860168457 0.09375\n",
            "time: 1.4409148693084717 1.5972832441329956\n",
            "i, sloss, closs, correct 2 0.3661462068557739 2.282358169555664 0.125\n",
            "time: 2.8038136959075928 1.9996182918548584\n",
            "i, sloss, closs, correct 3 0.4578378200531006 2.3301007747650146 0.109375\n",
            "time: 4.494295358657837 2.623396635055542\n",
            "i, sloss, closs, correct 4 0.3542252779006958 2.343141555786133 0.0625\n",
            "time: 2.896667718887329 2.6785419940948487\n",
            "i, sloss, closs, correct 5 0.3831673264503479 2.3484699726104736 0.078125\n",
            "time: 2.4616384506225586 2.6424816449483237\n",
            "i, sloss, closs, correct 6 0.3276422321796417 2.314404010772705 0.078125\n",
            "time: 1.495875597000122 2.4787496498652866\n",
            "i, sloss, closs, correct 7 0.371831476688385 2.298267126083374 0.15625\n",
            "time: 1.2762629985809326 2.328498214483261\n",
            "i, sloss, closs, correct 8 0.28698408603668213 2.371872901916504 0.09375\n",
            "time: 1.245943546295166 2.2082672913869223\n",
            "i, sloss, closs, correct 9 0.30057787895202637 2.3200643062591553 0.125\n",
            "time: 1.6250383853912354 2.149993419647217\n",
            "i, sloss, closs, correct 10 0.2549169659614563 2.280702590942383 0.109375\n",
            "time: 1.6675872802734375 2.1061725399710913\n",
            "i, sloss, closs, correct 11 0.26696860790252686 2.3356986045837402 0.171875\n",
            "time: 1.2463631629943848 2.0345618526140847\n",
            "i, sloss, closs, correct 12 0.29536476731300354 2.2634148597717285 0.140625\n",
            "time: 1.2390027046203613 1.9734026285318227\n",
            "i, sloss, closs, correct 13 0.27338239550590515 2.2839460372924805 0.09375\n",
            "time: 1.2581989765167236 1.922351326261248\n",
            "i, sloss, closs, correct 14 0.4623176157474518 2.2959351539611816 0.109375\n",
            "time: 1.240476131439209 1.8769248644510905\n",
            "i, sloss, closs, correct 15 0.28029629588127136 2.2447752952575684 0.09375\n",
            "time: 1.2732694149017334 1.839226707816124\n",
            "i, sloss, closs, correct 16 0.407609760761261 2.2895898818969727 0.140625\n",
            "time: 1.260787010192871 1.8052304352031034\n",
            "i, sloss, closs, correct 17 0.24468471109867096 2.2719080448150635 0.03125\n",
            "time: 1.2867298126220703 1.776451375749376\n",
            "i, sloss, closs, correct 18 0.25169309973716736 2.320467233657837 0.109375\n",
            "time: 1.3656730651855469 1.754856360586066\n",
            "i, sloss, closs, correct 19 0.3072957694530487 2.2510628700256348 0.171875\n",
            "time: 1.6562318801879883 1.7499438285827638\n",
            "i, sloss, closs, correct 20 0.3244034945964813 2.26405668258667 0.171875\n",
            "time: 1.4463117122650146 1.735508192153204\n",
            "i, sloss, closs, correct 21 0.2148144245147705 2.2653403282165527 0.125\n",
            "time: 1.262068510055542 1.714011929251931\n",
            "i, sloss, closs, correct 22 0.4139377176761627 2.2320799827575684 0.140625\n",
            "time: 1.2658162117004395 1.6945464611053467\n",
            "i, sloss, closs, correct 23 0.29891425371170044 2.2574620246887207 0.140625\n",
            "time: 1.2514195442199707 1.6761036813259125\n",
            "i, sloss, closs, correct 24 0.2416606843471527 2.296586751937866 0.15625\n",
            "time: 1.2459971904754639 1.6589194869995116\n",
            "i, sloss, closs, correct 25 0.4805331528186798 2.317382335662842 0.109375\n",
            "time: 1.2574541568756104 1.6434967059355516\n",
            "i, sloss, closs, correct 26 0.28427475690841675 2.2950429916381836 0.0625\n",
            "time: 1.2615251541137695 1.6293674574957953\n",
            "i, sloss, closs, correct 27 0.21896076202392578 2.287407398223877 0.171875\n",
            "time: 1.2269837856292725 1.615012833050319\n",
            "i, sloss, closs, correct 28 0.22394108772277832 2.2741599082946777 0.078125\n",
            "time: 1.5532934665679932 1.6129018520486766\n",
            "i, sloss, closs, correct 29 0.30440753698349 2.2442870140075684 0.09375\n",
            "time: 1.6784348487854004 1.6150989929835002\n",
            "i, sloss, closs, correct 30 0.27357998490333557 2.2311794757843018 0.171875\n",
            "time: 1.2349128723144531 1.6028542441706504\n",
            "i, sloss, closs, correct 31 0.3173413574695587 2.281796932220459 0.25\n",
            "time: 1.2248287200927734 1.591056540608406\n",
            "i, sloss, closs, correct 32 0.24223113059997559 2.2633931636810303 0.171875\n",
            "time: 1.2287647724151611 1.580092784130212\n",
            "i, sloss, closs, correct 33 0.33922526240348816 2.2283315658569336 0.140625\n",
            "time: 1.255542516708374 1.5705604763591992\n",
            "i, sloss, closs, correct 34 0.23820552229881287 2.2562508583068848 0.171875\n",
            "time: 1.2371065616607666 1.5610467433929442\n",
            "i, sloss, closs, correct 35 0.33451777696609497 2.2181761264801025 0.140625\n",
            "time: 1.2241055965423584 1.551701307296753\n",
            "i, sloss, closs, correct 36 0.3483019769191742 2.172656536102295 0.171875\n",
            "time: 1.2180016040802002 1.5426952130085714\n",
            "i, sloss, closs, correct 37 0.24394245445728302 2.227048873901367 0.1875\n",
            "time: 1.2136836051940918 1.5340490843120373\n",
            "i, sloss, closs, correct 38 0.3916647136211395 2.250521183013916 0.21875\n",
            "time: 1.6815500259399414 1.5378431784800994\n",
            "i, sloss, closs, correct 39 0.30190539360046387 2.2725589275360107 0.1875\n",
            "time: 1.5285985469818115 1.5376213192939758\n",
            "i, sloss, closs, correct 40 0.1862654983997345 2.200998544692993 0.15625\n",
            "time: 1.2415990829467773 1.530412232003561\n",
            "i, sloss, closs, correct 41 0.3129240870475769 2.3360917568206787 0.1875\n",
            "time: 1.216907024383545 1.5229608842304774\n",
            "i, sloss, closs, correct 42 0.32571348547935486 2.2157413959503174 0.171875\n",
            "time: 1.2189369201660156 1.515902269718259\n",
            "i, sloss, closs, correct 43 0.4379793405532837 2.1956841945648193 0.1875\n",
            "time: 1.2359585762023926 1.5095511024648494\n",
            "i, sloss, closs, correct 44 0.36246785521507263 2.2857208251953125 0.15625\n",
            "time: 1.2464168071746826 1.5037142859564887\n",
            "i, sloss, closs, correct 45 0.33248937129974365 2.2091176509857178 0.0625\n",
            "time: 1.2344391345977783 1.4978707251341448\n",
            "i, sloss, closs, correct 46 0.3368586003780365 2.263967275619507 0.15625\n",
            "time: 1.23673415184021 1.4923242203732754\n",
            "i, sloss, closs, correct 47 0.3001386523246765 2.2913219928741455 0.1875\n",
            "time: 1.393381118774414 1.4902726709842682\n",
            "i, sloss, closs, correct 48 0.2815488874912262 2.2271862030029297 0.15625\n",
            "time: 1.6373991966247559 1.4932827316984838\n",
            "i, sloss, closs, correct 49 0.37566420435905457 2.1689870357513428 0.171875\n",
            "time: 1.4138109683990479 1.4917069053649903\n",
            "i, sloss, closs, correct 50 0.23739545047283173 2.2625739574432373 0.15625\n",
            "time: 1.2273576259613037 1.4865327629388547\n",
            "i, sloss, closs, correct 51 0.25438910722732544 2.263991355895996 0.171875\n",
            "time: 1.2287442684173584 1.4815839116389935\n",
            "i, sloss, closs, correct 52 0.38863542675971985 2.2537364959716797 0.15625\n",
            "time: 1.2745449542999268 1.4776876737486642\n",
            "i, sloss, closs, correct 53 0.2951217591762543 2.1728949546813965 0.15625\n",
            "time: 1.244002342224121 1.4733693114033453\n",
            "i, sloss, closs, correct 54 0.2662144601345062 2.256744861602783 0.21875\n",
            "time: 1.2168400287628174 1.4687138253992253\n",
            "i, sloss, closs, correct 55 0.35994863510131836 2.1927828788757324 0.234375\n",
            "time: 1.2237188816070557 1.464347460440227\n",
            "i, sloss, closs, correct 56 0.36468756198883057 2.165590763092041 0.15625\n",
            "time: 1.2233271598815918 1.4601279350749232\n",
            "i, sloss, closs, correct 57 0.25933751463890076 2.206408739089966 0.25\n",
            "time: 1.5546178817749023 1.4617657620331337\n",
            "i, sloss, closs, correct 58 0.28092342615127563 2.1521871089935303 0.25\n",
            "time: 1.645282506942749 1.4648832305003021\n",
            "i, sloss, closs, correct 59 0.296397864818573 2.235950469970703 0.1875\n",
            "time: 1.226289987564087 1.4609145720799763\n",
            "i, sloss, closs, correct 60 0.22886480391025543 2.290693998336792 0.203125\n",
            "time: 1.2485861778259277 1.457441611368148\n",
            "i, sloss, closs, correct 61 0.2898657023906708 2.2173774242401123 0.140625\n",
            "time: 1.2216293811798096 1.4536460368863997\n",
            "i, sloss, closs, correct 62 0.35973235964775085 2.169435977935791 0.234375\n",
            "time: 1.216177225112915 1.4498841005658347\n",
            "i, sloss, closs, correct 63 0.2905696928501129 2.2541019916534424 0.203125\n",
            "time: 1.2214512825012207 1.4463224299252033\n",
            "i, sloss, closs, correct 64 0.23778484761714935 2.2152345180511475 0.203125\n",
            "time: 1.2310986518859863 1.443019074660081\n",
            "i, sloss, closs, correct 65 0.5463554263114929 2.195756196975708 0.1875\n",
            "time: 1.2367613315582275 1.4399012146574077\n",
            "i, sloss, closs, correct 66 0.21018028259277344 2.2594423294067383 0.09375\n",
            "time: 1.2348227500915527 1.4368477437033582\n",
            "i, sloss, closs, correct 67 0.20195627212524414 2.2753376960754395 0.25\n",
            "time: 1.6426022052764893 1.4398806656108183\n",
            "i, sloss, closs, correct 68 0.21980750560760498 2.1769089698791504 0.1875\n",
            "time: 1.5620262622833252 1.441656271616618\n",
            "i, sloss, closs, correct 69 0.28531908988952637 2.1712889671325684 0.15625\n",
            "time: 1.2343394756317139 1.4387016807283675\n",
            "i, sloss, closs, correct 70 0.4120698869228363 2.201287269592285 0.234375\n",
            "time: 1.231961727142334 1.435796408586099\n",
            "i, sloss, closs, correct 71 0.33725595474243164 2.226381778717041 0.125\n",
            "time: 1.228470802307129 1.432924234204822\n",
            "i, sloss, closs, correct 72 0.30320441722869873 2.2093732357025146 0.203125\n",
            "time: 1.2542672157287598 1.4304838572462943\n",
            "i, sloss, closs, correct 73 0.2653324007987976 2.0968518257141113 0.234375\n",
            "time: 1.2474241256713867 1.428016717369492\n",
            "i, sloss, closs, correct 74 0.4533430337905884 2.168201208114624 0.171875\n",
            "time: 1.2549967765808105 1.4257162539164225\n",
            "i, sloss, closs, correct 75 0.3079998195171356 2.1964073181152344 0.25\n",
            "time: 1.2810406684875488 1.423818798441636\n",
            "i, sloss, closs, correct 76 0.40374377369880676 2.1947827339172363 0.25\n",
            "time: 1.4453256130218506 1.4241051797742967\n",
            "i, sloss, closs, correct 77 0.5209933519363403 2.2551729679107666 0.171875\n",
            "time: 1.7088663578033447 1.4277606377234826\n",
            "i, sloss, closs, correct 78 0.2273726463317871 2.161931276321411 0.15625\n",
            "time: 1.557065725326538 1.4294241591344905\n",
            "i, sloss, closs, correct 79 0.315609335899353 2.1908106803894043 0.234375\n",
            "time: 1.439429521560669 1.4295548349618912\n",
            "i, sloss, closs, correct 80 0.18148891627788544 2.1866815090179443 0.234375\n",
            "time: 1.255300760269165 1.4274096901034132\n",
            "i, sloss, closs, correct 81 0.2659481465816498 2.2186741828918457 0.234375\n",
            "time: 1.278207778930664 1.4255959173528159\n",
            "i, sloss, closs, correct 82 0.4969916045665741 2.1527535915374756 0.203125\n",
            "time: 1.24163818359375 1.4233852696706013\n",
            "i, sloss, closs, correct 83 0.5443755984306335 2.2874526977539062 0.125\n",
            "time: 1.2391281127929688 1.4211973803383964\n",
            "i, sloss, closs, correct 84 0.17974263429641724 2.1691479682922363 0.203125\n",
            "time: 1.2643253803253174 1.4193580122555003\n",
            "i, sloss, closs, correct 85 0.30326730012893677 2.1700148582458496 0.09375\n",
            "time: 2.077126979827881 1.427012324333191\n",
            "i, sloss, closs, correct 86 0.4843546450138092 2.2615513801574707 0.171875\n",
            "time: 2.309293270111084 1.437157836453668\n",
            "i, sloss, closs, correct 87 0.3078557848930359 2.110682249069214 0.203125\n",
            "time: 1.2608861923217773 1.435162143273787\n",
            "i, sloss, closs, correct 88 0.3813943862915039 2.199056625366211 0.21875\n",
            "time: 1.252448558807373 1.4331151903345343\n",
            "i, sloss, closs, correct 89 0.2282128930091858 2.1212968826293945 0.1875\n",
            "time: 1.2653276920318604 1.4312563869688246\n",
            "i, sloss, closs, correct 90 0.1761421412229538 2.1196608543395996 0.171875\n",
            "time: 1.2674813270568848 1.4294618564647632\n",
            "i, sloss, closs, correct 91 0.3418668210506439 2.1587131023406982 0.3125\n",
            "time: 1.3095335960388184 1.4281647153522656\n",
            "i, sloss, closs, correct 92 0.4466899037361145 2.2066311836242676 0.203125\n",
            "time: 1.2461411952972412 1.4262123210455782\n",
            "i, sloss, closs, correct 93 0.3269324004650116 2.186492443084717 0.265625\n",
            "time: 1.2445244789123535 1.4242846433152543\n",
            "i, sloss, closs, correct 94 0.27666613459587097 2.13329815864563 0.25\n",
            "time: 1.4851250648498535 1.4249340509113513\n",
            "i, sloss, closs, correct 95 0.2838612496852875 2.159668445587158 0.203125\n",
            "time: 1.7545344829559326 1.4283714319268863\n",
            "i, sloss, closs, correct 96 0.1999460756778717 2.1733086109161377 0.296875\n",
            "time: 1.383852243423462 1.4279181932665639\n",
            "i, sloss, closs, correct 97 0.22435611486434937 2.172917604446411 0.28125\n",
            "time: 1.2680156230926514 1.426291482789176\n",
            "i, sloss, closs, correct 98 0.21533162891864777 2.1820833683013916 0.171875\n",
            "time: 1.289548397064209 1.4249154028266366\n",
            "i, sloss, closs, correct 99 0.23758232593536377 2.1673696041107178 0.203125\n",
            "time: 1.2422003746032715 1.4230940246582031\n",
            "i, sloss, closs, correct 100 0.23579199612140656 2.2226791381835938 0.21875\n",
            "time: 1.2592530250549316 1.4214768362517405\n",
            "i, sloss, closs, correct 101 0.2375326156616211 2.1620025634765625 0.203125\n",
            "time: 1.2571895122528076 1.4198707388896568\n",
            "i, sloss, closs, correct 102 0.46760880947113037 2.1861276626586914 0.234375\n",
            "time: 1.2787892818450928 1.4185066524061185\n",
            "i, sloss, closs, correct 103 0.24728012084960938 2.1887264251708984 0.3125\n",
            "time: 1.2749316692352295 1.41713078893148\n",
            "i, sloss, closs, correct 104 0.39725279808044434 2.168177604675293 0.21875\n",
            "time: 1.6365432739257812 1.4192301091693698\n",
            "i, sloss, closs, correct 105 0.43039217591285706 2.205512523651123 0.3125\n",
            "time: 1.5463156700134277 1.4204325833410587\n",
            "i, sloss, closs, correct 106 0.24187858402729034 2.1148972511291504 0.125\n",
            "time: 1.263258934020996 1.418967877592996\n",
            "i, sloss, closs, correct 107 0.2676296830177307 2.124190330505371 0.15625\n",
            "time: 1.2216384410858154 1.4171504819834675\n",
            "i, sloss, closs, correct 108 0.3976171314716339 2.159428596496582 0.1875\n",
            "time: 1.2515456676483154 1.4156355617243215\n",
            "i, sloss, closs, correct 109 0.29349035024642944 2.1437907218933105 0.21875\n",
            "time: 1.231396198272705 1.413964919610457\n",
            "i, sloss, closs, correct 110 0.25332650542259216 2.1736435890197754 0.25\n",
            "time: 1.2444744110107422 1.4124420183198947\n",
            "i, sloss, closs, correct 111 0.38017234206199646 2.137746810913086 0.25\n",
            "time: 1.2558915615081787 1.4110482356378011\n",
            "i, sloss, closs, correct 112 0.191585510969162 2.135744571685791 0.28125\n",
            "time: 1.2465739250183105 1.4095970276182732\n",
            "i, sloss, closs, correct 113 0.20389612019062042 2.1482491493225098 0.21875\n",
            "time: 1.5121915340423584 1.4105012354097868\n",
            "i, sloss, closs, correct 114 0.29309022426605225 2.1830825805664062 0.234375\n",
            "time: 1.6925780773162842 1.4129574402518894\n",
            "i, sloss, closs, correct 115 0.24553599953651428 2.0948281288146973 0.203125\n",
            "time: 1.2894806861877441 1.4118962719522674\n",
            "i, sloss, closs, correct 116 0.2099338173866272 2.160094738006592 0.1875\n",
            "time: 1.2421901226043701 1.410449781988421\n",
            "i, sloss, closs, correct 117 0.36208003759384155 2.0666046142578125 0.296875\n",
            "time: 1.2546329498291016 1.4091335393614688\n",
            "i, sloss, closs, correct 118 0.2574015259742737 2.1368649005889893 0.234375\n",
            "time: 1.2449555397033691 1.4077578632771468\n",
            "i, sloss, closs, correct 119 0.3826121985912323 2.155378818511963 0.203125\n",
            "time: 1.3040363788604736 1.4068968256314596\n",
            "i, sloss, closs, correct 120 0.38454627990722656 2.123863458633423 0.234375\n",
            "time: 1.271855115890503 1.4057869477705522\n",
            "i, sloss, closs, correct 121 0.2216174602508545 2.137932300567627 0.3125\n",
            "time: 1.2552642822265625 1.404557177277862\n",
            "i, sloss, closs, correct 122 0.32127323746681213 2.111729621887207 0.25\n",
            "time: 1.2495648860931396 1.4033013591921426\n",
            "i, sloss, closs, correct 123 0.4999486804008484 2.1560792922973633 0.34375\n",
            "time: 1.6708290576934814 1.405462641869822\n",
            "i, sloss, closs, correct 124 0.2110406458377838 2.133402109146118 0.25\n",
            "time: 1.5568232536315918 1.4066861495971679\n",
            "i, sloss, closs, correct 125 0.436624675989151 2.1357154846191406 0.25\n",
            "time: 1.2501270771026611 1.4054474698172674\n",
            "i, sloss, closs, correct 126 0.27570870518684387 2.0442605018615723 0.15625\n",
            "time: 1.2712745666503906 1.4044045031540038\n",
            "i, sloss, closs, correct 127 0.4040440618991852 2.1149396896362305 0.125\n",
            "time: 1.2629868984222412 1.4033036194741726\n",
            "i, sloss, closs, correct 128 0.25355157256126404 2.0802061557769775 0.15625\n",
            "time: 1.2482233047485352 1.4021052852157474\n",
            "i, sloss, closs, correct 129 0.2884557545185089 2.051281452178955 0.140625\n",
            "time: 1.2636973857879639 1.4010444255975576\n",
            "i, sloss, closs, correct 130 0.2815704047679901 2.117642879486084 0.234375\n",
            "time: 1.2651185989379883 1.4000104838655194\n",
            "i, sloss, closs, correct 131 0.40705645084381104 2.1340954303741455 0.21875\n",
            "time: 1.253302812576294 1.3989029436400442\n",
            "i, sloss, closs, correct 132 0.4506347179412842 2.203744888305664 0.1875\n",
            "time: 1.4756441116333008 1.3994833795647872\n",
            "i, sloss, closs, correct 133 0.23812350630760193 2.148829936981201 0.265625\n",
            "time: 1.803853988647461 1.4025039512719681\n",
            "i, sloss, closs, correct 134 0.2244393527507782 2.1365113258361816 0.296875\n",
            "time: 1.380234718322754 1.402343495686849\n",
            "i, sloss, closs, correct 135 0.27243366837501526 2.1949520111083984 0.21875\n",
            "time: 1.3478000164031982 1.4019552118637983\n",
            "i, sloss, closs, correct 136 0.2458706647157669 2.1979494094848633 0.140625\n",
            "time: 1.3320157527923584 1.4014482080501363\n",
            "i, sloss, closs, correct 137 0.30548304319381714 2.0158305168151855 0.234375\n",
            "time: 1.301302433013916 1.400726180145706\n",
            "i, sloss, closs, correct 138 0.2799511253833771 2.142427682876587 0.265625\n",
            "time: 1.3039028644561768 1.4000364104620844\n",
            "i, sloss, closs, correct 139 0.1940685659646988 2.0921969413757324 0.171875\n",
            "time: 1.2749619483947754 1.399146396773202\n",
            "i, sloss, closs, correct 140 0.21384212374687195 2.084904670715332 0.28125\n",
            "time: 1.2394990921020508 1.3980176702458809\n",
            "i, sloss, closs, correct 141 0.2594046890735626 2.129152774810791 0.21875\n",
            "time: 1.4512977600097656 1.3983965591645577\n",
            "i, sloss, closs, correct 142 0.3890904188156128 2.1657209396362305 0.25\n",
            "time: 1.6774611473083496 1.4003508307717063\n",
            "i, sloss, closs, correct 143 0.2203855961561203 2.01741623878479 0.1875\n",
            "time: 1.425872564315796 1.400530790289243\n",
            "i, sloss, closs, correct 144 0.2734161615371704 2.062870502471924 0.1875\n",
            "time: 1.310701608657837 1.3999147711129023\n",
            "i, sloss, closs, correct 145 0.30081042647361755 2.1388497352600098 0.203125\n",
            "time: 1.263983964920044 1.398987593716138\n",
            "i, sloss, closs, correct 146 0.4189637005329132 2.130166530609131 0.171875\n",
            "time: 1.261343240737915 1.3980545429956346\n",
            "i, sloss, closs, correct 147 0.27222010493278503 2.068835735321045 0.140625\n",
            "time: 1.2732598781585693 1.3972145141781986\n",
            "i, sloss, closs, correct 148 0.3515779674053192 2.032000780105591 0.296875\n",
            "time: 1.2456104755401611 1.3962002136563294\n",
            "i, sloss, closs, correct 149 0.4412834942340851 2.109253406524658 0.296875\n",
            "time: 1.269740104675293 1.3953601392110189\n",
            "i, sloss, closs, correct 150 0.36011677980422974 2.156045436859131 0.1875\n",
            "time: 1.263636589050293 1.394490930418305\n",
            "i, sloss, closs, correct 151 0.3556206226348877 2.0940632820129395 0.21875\n",
            "time: 1.6527674198150635 1.3961932016046423\n",
            "i, sloss, closs, correct 152 0.2910045385360718 2.1133813858032227 0.171875\n",
            "time: 1.6435141563415527 1.3978124163509194\n",
            "i, sloss, closs, correct 153 0.23855891823768616 2.0621094703674316 0.28125\n",
            "time: 1.3202452659606934 1.3973119754295844\n",
            "i, sloss, closs, correct 154 0.29694172739982605 2.109152317047119 0.265625\n",
            "time: 1.3098821640014648 1.3967509623496763\n",
            "i, sloss, closs, correct 155 0.18004575371742249 2.103118658065796 0.28125\n",
            "time: 1.2842884063720703 1.396033186178941\n",
            "i, sloss, closs, correct 156 0.23669570684432983 2.1077394485473633 0.234375\n",
            "time: 1.28727126121521 1.3953435132457952\n",
            "i, sloss, closs, correct 157 0.44577839970588684 2.107417345046997 0.21875\n",
            "time: 1.3022637367248535 1.3947578638414793\n",
            "i, sloss, closs, correct 158 0.18714387714862823 2.094860553741455 0.28125\n",
            "time: 1.3136842250823975 1.3942509702166672\n",
            "i, sloss, closs, correct 159 0.2242160141468048 2.206921339035034 0.265625\n",
            "time: 1.2546727657318115 1.3933816000819206\n",
            "i, sloss, closs, correct 160 0.2724415063858032 2.0673770904541016 0.140625\n",
            "time: 1.512939214706421 1.394127100891208\n",
            "i, sloss, closs, correct 161 0.30489930510520935 2.102224826812744 0.171875\n",
            "time: 1.7646594047546387 1.3964173396428425\n",
            "i, sloss, closs, correct 162 0.2839106321334839 2.076998233795166 0.25\n",
            "time: 1.3000996112823486 1.3958297053729098\n",
            "i, sloss, closs, correct 163 0.2831074297428131 2.101806879043579 0.25\n",
            "time: 1.3432819843292236 1.3955125038216754\n",
            "i, sloss, closs, correct 164 0.3470922112464905 2.0389552116394043 0.21875\n",
            "time: 1.3051931858062744 1.3949680082725755\n",
            "i, sloss, closs, correct 165 0.16158246994018555 2.0055787563323975 0.328125\n",
            "time: 1.2342007160186768 1.3940022882208767\n",
            "i, sloss, closs, correct 166 0.25393974781036377 2.1577415466308594 0.203125\n",
            "time: 1.2483537197113037 1.3931329821398157\n",
            "i, sloss, closs, correct 167 0.2848142385482788 2.050899028778076 0.203125\n",
            "time: 1.3075621128082275 1.3926276564598083\n",
            "i, sloss, closs, correct 168 0.4650929868221283 2.1173458099365234 0.234375\n",
            "time: 1.2863552570343018 1.392001679663122\n",
            "i, sloss, closs, correct 169 0.26034972071647644 2.079338550567627 0.25\n",
            "time: 1.3850553035736084 1.391963633368997\n",
            "i, sloss, closs, correct 170 0.21518972516059875 2.150956392288208 0.28125\n",
            "time: 1.630324125289917 1.3933597363923724\n",
            "i, sloss, closs, correct 171 0.18971310555934906 2.0321547985076904 0.1875\n",
            "time: 1.5237712860107422 1.3941252869228984\n",
            "i, sloss, closs, correct 172 0.45649683475494385 2.0668070316314697 0.25\n",
            "time: 1.2255685329437256 1.3931538796838308\n",
            "i, sloss, closs, correct 173 0.24984712898731232 2.0340638160705566 0.21875\n",
            "time: 1.2212183475494385 1.3921686835672664\n",
            "i, sloss, closs, correct 174 0.2556658983230591 2.0679006576538086 0.296875\n",
            "time: 1.2747836112976074 1.3915006242479597\n",
            "i, sloss, closs, correct 175 0.18934771418571472 2.0573832988739014 0.1875\n",
            "time: 1.2515161037445068 1.3907081471248106\n",
            "i, sloss, closs, correct 176 0.44694650173187256 2.0930986404418945 0.234375\n",
            "time: 1.2121431827545166 1.389702151724174\n",
            "i, sloss, closs, correct 177 0.23338185250759125 2.121734380722046 0.296875\n",
            "time: 1.2378554344177246 1.3888518140557107\n",
            "i, sloss, closs, correct 178 0.20064061880111694 2.1167287826538086 0.25\n",
            "time: 1.232297420501709 1.387980052212763\n",
            "i, sloss, closs, correct 179 0.265412837266922 2.121126651763916 0.3125\n",
            "time: 1.5768022537231445 1.3890320102373759\n",
            "i, sloss, closs, correct 180 0.22632162272930145 2.088268756866455 0.25\n",
            "time: 1.6495912075042725 1.3904736950911212\n",
            "i, sloss, closs, correct 181 0.2255832701921463 2.1808462142944336 0.390625\n",
            "time: 1.2457361221313477 1.3896810287957664\n",
            "i, sloss, closs, correct 182 0.20516087114810944 2.02828049659729 0.296875\n",
            "time: 1.2666640281677246 1.3890114351699911\n",
            "i, sloss, closs, correct 183 0.3811516761779785 2.0015952587127686 0.265625\n",
            "time: 1.2380592823028564 1.3881936462029167\n",
            "i, sloss, closs, correct 184 0.2141672819852829 2.083653450012207 0.203125\n",
            "time: 1.2383215427398682 1.387386247274038\n",
            "i, sloss, closs, correct 185 0.2536030411720276 2.0548949241638184 0.21875\n",
            "time: 1.2346272468566895 1.3865677746393348\n",
            "i, sloss, closs, correct 186 0.27571868896484375 2.0797624588012695 0.265625\n",
            "time: 1.2733376026153564 1.385969858118557\n",
            "i, sloss, closs, correct 187 0.27797362208366394 2.093449115753174 0.265625\n",
            "time: 1.2841379642486572 1.3854348608788023\n",
            "i, sloss, closs, correct 188 0.2586469352245331 2.105208396911621 0.203125\n",
            "time: 1.3113315105438232 1.385044637811247\n",
            "i, sloss, closs, correct 189 0.27082720398902893 2.066556930541992 0.3125\n",
            "time: 1.6652438640594482 1.3865247437828465\n",
            "i, sloss, closs, correct 190 0.18562039732933044 2.0646533966064453 0.28125\n",
            "time: 1.5275633335113525 1.3872657121788146\n",
            "i, sloss, closs, correct 191 0.2221831977367401 2.1058285236358643 0.34375\n",
            "time: 1.2265398502349854 1.3864310793578625\n",
            "i, sloss, closs, correct 192 0.5064776539802551 1.9985392093658447 0.40625\n",
            "time: 1.2170379161834717 1.3855558677040851\n",
            "i, sloss, closs, correct 193 0.45856180787086487 2.0510597229003906 0.28125\n",
            "time: 1.2495152950286865 1.3848572256638831\n",
            "i, sloss, closs, correct 194 0.369389146566391 2.0673437118530273 0.234375\n",
            "time: 1.2398033142089844 1.384115927035992\n",
            "i, sloss, closs, correct 195 0.44097644090652466 1.9999538660049438 0.21875\n",
            "time: 1.2339775562286377 1.383352362379736\n",
            "i, sloss, closs, correct 196 0.2830716371536255 2.132795810699463 0.21875\n",
            "time: 1.311621904373169 1.3829909339168955\n",
            "i, sloss, closs, correct 197 0.18502864241600037 2.0945799350738525 0.25\n",
            "time: 1.2303199768066406 1.3822223273190586\n",
            "i, sloss, closs, correct 198 0.41457775235176086 2.0511412620544434 0.21875\n",
            "time: 1.5062668323516846 1.382848035151036\n",
            "i, sloss, closs, correct 199 0.26322129368782043 2.157590627670288 0.34375\n",
            "time: 1.757138729095459 1.3847220647335052\n",
            "i, sloss, closs, correct 200 0.32728999853134155 2.083538770675659 0.265625\n",
            "time: 1.2210543155670166 1.3839102633556917\n",
            "i, sloss, closs, correct 201 0.2639681100845337 2.059983730316162 0.3125\n",
            "time: 1.2165310382843018 1.3830840764659467\n",
            "i, sloss, closs, correct 202 0.23941579461097717 1.9063622951507568 0.328125\n",
            "time: 1.235624074935913 1.3823600132477107\n",
            "i, sloss, closs, correct 203 0.2197372168302536 1.967736005783081 0.28125\n",
            "time: 1.2496037483215332 1.3817114105411605\n",
            "i, sloss, closs, correct 204 0.3419642448425293 1.917456865310669 0.28125\n",
            "time: 1.2672576904296875 1.3811555699604314\n",
            "i, sloss, closs, correct 205 0.2670154571533203 2.1388769149780273 0.234375\n",
            "time: 1.2362422943115234 1.3804543909517306\n",
            "i, sloss, closs, correct 206 0.20099210739135742 2.067068576812744 0.328125\n",
            "time: 1.238757848739624 1.3797722991537933\n",
            "i, sloss, closs, correct 207 0.34663915634155273 2.067949056625366 0.1875\n",
            "time: 1.3897597789764404 1.3798226381723697\n",
            "i, sloss, closs, correct 208 0.4338590204715729 1.97186279296875 0.28125\n",
            "time: 1.6813874244689941 1.3812673513969165\n",
            "i, sloss, closs, correct 209 0.2883349657058716 2.081186056137085 0.15625\n",
            "time: 1.4670064449310303 1.381677476565043\n",
            "i, sloss, closs, correct 210 0.27947884798049927 2.026215076446533 0.25\n",
            "time: 1.2221848964691162 1.3809248926515263\n",
            "i, sloss, closs, correct 211 0.2322085052728653 1.9236605167388916 0.171875\n",
            "time: 1.3072295188903809 1.3805794884573739\n",
            "i, sloss, closs, correct 212 0.20763182640075684 2.1267151832580566 0.28125\n",
            "time: 1.3040390014648438 1.3802224033875086\n",
            "i, sloss, closs, correct 213 0.38359493017196655 2.0440526008605957 0.265625\n",
            "time: 1.277493953704834 1.3797447035245807\n",
            "i, sloss, closs, correct 214 0.39945727586746216 2.065812349319458 0.21875\n",
            "time: 1.260347604751587 1.3791916680890461\n",
            "i, sloss, closs, correct 215 0.2249368578195572 2.0465216636657715 0.25\n",
            "time: 1.2608702182769775 1.3786472567805537\n",
            "i, sloss, closs, correct 216 0.33244720101356506 2.0355091094970703 0.265625\n",
            "time: 1.2854087352752686 1.3782197912717196\n",
            "i, sloss, closs, correct 217 0.16425827145576477 2.0708677768707275 0.234375\n",
            "time: 1.7113370895385742 1.3797502638003147\n",
            "i, sloss, closs, correct 218 0.30112117528915405 1.9544545412063599 0.3125\n",
            "time: 1.6221952438354492 1.3808653844545966\n",
            "i, sloss, closs, correct 219 0.3217564523220062 2.002356767654419 0.203125\n",
            "time: 1.2572028636932373 1.3803055546500467\n",
            "i, sloss, closs, correct 220 0.37815022468566895 2.061558246612549 0.234375\n",
            "time: 1.2400133609771729 1.3796728908745952\n",
            "i, sloss, closs, correct 221 0.16286954283714294 2.2046899795532227 0.359375\n",
            "time: 1.2417209148406982 1.3790535980516725\n",
            "i, sloss, closs, correct 222 0.26833856105804443 1.9713307619094849 0.328125\n",
            "time: 1.2930433750152588 1.3786741863986303\n",
            "i, sloss, closs, correct 223 0.2568082809448242 2.0386202335357666 0.265625\n",
            "time: 1.2538871765136719 1.3781192845531873\n",
            "i, sloss, closs, correct 224 0.20669953525066376 2.1352322101593018 0.125\n",
            "time: 1.2389044761657715 1.3775027476416695\n",
            "i, sloss, closs, correct 225 0.2781066596508026 2.0425710678100586 0.203125\n",
            "time: 1.2841930389404297 1.3770919552946512\n",
            "i, sloss, closs, correct 226 0.3581554889678955 2.0462427139282227 0.328125\n",
            "time: 1.4973063468933105 1.377623552792923\n",
            "i, sloss, closs, correct 227 0.1913212686777115 1.9551057815551758 0.34375\n",
            "time: 1.7273306846618652 1.3791591723759968\n",
            "i, sloss, closs, correct 228 0.26551303267478943 2.0303916931152344 0.328125\n",
            "time: 1.2446155548095703 1.3785736758636076\n",
            "i, sloss, closs, correct 229 0.20755383372306824 2.086615562438965 0.40625\n",
            "time: 1.2558050155639648 1.3780421619829923\n",
            "i, sloss, closs, correct 230 0.3412594497203827 1.9925906658172607 0.203125\n",
            "time: 1.226466178894043 1.3773879715890596\n",
            "i, sloss, closs, correct 231 0.21748049557209015 2.0127439498901367 0.234375\n",
            "time: 1.2429420948028564 1.3768106000176792\n",
            "i, sloss, closs, correct 232 0.19430817663669586 2.0151267051696777 0.28125\n",
            "time: 1.2886989116668701 1.3764343507300119\n",
            "i, sloss, closs, correct 233 0.4169260561466217 1.96721613407135 0.234375\n",
            "time: 1.2630283832550049 1.3759518219874456\n",
            "i, sloss, closs, correct 234 0.300258070230484 1.9942315816879272 0.296875\n",
            "time: 1.285376787185669 1.3755683624998052\n",
            "i, sloss, closs, correct 235 0.24399903416633606 2.0800929069519043 0.28125\n",
            "time: 1.340517282485962 1.3754219503725988\n",
            "i, sloss, closs, correct 236 0.3478962779045105 2.091264009475708 0.234375\n",
            "time: 1.636366844177246 1.3765394184659805\n",
            "i, sloss, closs, correct 237 0.26466962695121765 2.0922412872314453 0.234375\n",
            "time: 1.4810364246368408 1.3769804449642407\n",
            "i, sloss, closs, correct 238 0.23251450061798096 1.9825103282928467 0.3125\n",
            "time: 1.2369461059570312 1.3763965522893802\n",
            "i, sloss, closs, correct 239 0.2630417048931122 1.9788764715194702 0.328125\n",
            "time: 1.2169880867004395 1.3757343908150992\n",
            "i, sloss, closs, correct 240 0.3705040514469147 2.043529748916626 0.203125\n",
            "time: 1.2295782566070557 1.3751299509863635\n",
            "i, sloss, closs, correct 241 0.4790312647819519 2.0335841178894043 0.25\n",
            "time: 1.229942798614502 1.3745320365448628\n",
            "i, sloss, closs, correct 242 0.2568439841270447 2.0293850898742676 0.25\n",
            "time: 1.224123239517212 1.3739151120676425\n",
            "i, sloss, closs, correct 243 0.23039142787456512 1.97450852394104 0.296875\n",
            "time: 1.2452523708343506 1.3733898401260376\n",
            "i, sloss, closs, correct 244 0.2089221328496933 2.0805368423461914 0.21875\n",
            "time: 1.2248430252075195 1.3727853765293043\n",
            "i, sloss, closs, correct 245 0.30105671286582947 2.080288887023926 0.359375\n",
            "time: 1.5436410903930664 1.3734818958654635\n",
            "i, sloss, closs, correct 246 0.2641938030719757 1.9896941184997559 0.21875\n",
            "time: 1.649853229522705 1.374602476112273\n",
            "i, sloss, closs, correct 247 0.38574501872062683 2.048923969268799 0.3125\n",
            "time: 1.232361078262329 1.3740308659691964\n",
            "i, sloss, closs, correct 248 0.2871473431587219 2.038985252380371 0.296875\n",
            "time: 1.2228012084960938 1.3734255133862476\n",
            "i, sloss, closs, correct 249 0.27755358815193176 2.0786046981811523 0.28125\n",
            "time: 1.2352259159088135 1.3728746128082276\n",
            "i, sloss, closs, correct 250 0.22151194512844086 2.123168468475342 0.1875\n",
            "time: 1.254652738571167 1.3724056211600741\n",
            "i, sloss, closs, correct 251 0.2629309594631195 2.054996967315674 0.171875\n",
            "time: 1.2218759059906006 1.3718102385127355\n",
            "i, sloss, closs, correct 252 0.3828292489051819 1.989853858947754 0.3125\n",
            "time: 1.2250645160675049 1.3712321062804211\n",
            "i, sloss, closs, correct 253 0.3283079266548157 2.061638832092285 0.3125\n",
            "time: 1.222160816192627 1.3706471047063513\n",
            "i, sloss, closs, correct 254 0.43243086338043213 2.027172565460205 0.28125\n",
            "time: 1.318274974822998 1.3704437031465417\n",
            "i, sloss, closs, correct 255 0.26944664120674133 1.999566912651062 0.3125\n",
            "time: 1.6493542194366455 1.3715342227369547\n",
            "i, sloss, closs, correct 256 0.5199748277664185 1.9977695941925049 0.34375\n",
            "time: 1.5258865356445312 1.3721363229046535\n",
            "i, sloss, closs, correct 257 0.20841647684574127 1.994642972946167 0.296875\n",
            "time: 1.285069227218628 1.371800693430642\n",
            "i, sloss, closs, correct 258 0.30789288878440857 1.9484643936157227 0.265625\n",
            "time: 1.255599021911621 1.3713539631670506\n",
            "i, sloss, closs, correct 259 0.39822253584861755 1.984999656677246 0.28125\n",
            "time: 1.2599246501922607 1.3709273109069238\n",
            "i, sloss, closs, correct 260 0.3797120153903961 2.0902864933013916 0.21875\n",
            "time: 1.254920482635498 1.370484638031415\n",
            "i, sloss, closs, correct 261 0.22490175068378448 1.9935691356658936 0.234375\n",
            "time: 1.2748854160308838 1.3701228494862563\n",
            "i, sloss, closs, correct 262 0.3815876841545105 1.9913347959518433 0.359375\n",
            "time: 1.2787890434265137 1.369777407483003\n",
            "i, sloss, closs, correct 263 0.33010539412498474 1.9748542308807373 0.28125\n",
            "time: 1.2642807960510254 1.3693797254201137\n",
            "i, sloss, closs, correct 264 0.15979841351509094 1.9849982261657715 0.3125\n",
            "time: 1.6208178997039795 1.3703304155817573\n",
            "i, sloss, closs, correct 265 0.3617513179779053 2.0936641693115234 0.3125\n",
            "time: 1.5916790962219238 1.3711641623561543\n",
            "i, sloss, closs, correct 266 0.23985645174980164 1.9626009464263916 0.3125\n",
            "time: 1.2609524726867676 1.370753293626764\n",
            "i, sloss, closs, correct 267 0.28146860003471375 1.9362459182739258 0.21875\n",
            "time: 1.282275915145874 1.3704249342875694\n",
            "i, sloss, closs, correct 268 0.27943170070648193 1.9622154235839844 0.375\n",
            "time: 1.2855894565582275 1.3701112961680473\n",
            "i, sloss, closs, correct 269 0.442263662815094 2.02498197555542 0.328125\n",
            "time: 1.246417760848999 1.3696549141848529\n",
            "i, sloss, closs, correct 270 0.4629848897457123 2.177241563796997 0.3125\n",
            "time: 1.250429630279541 1.3692167729029356\n",
            "i, sloss, closs, correct 271 0.3146454095840454 2.101896286010742 0.15625\n",
            "time: 1.278362512588501 1.3688844547552221\n",
            "i, sloss, closs, correct 272 0.21526610851287842 1.961093544960022 0.328125\n",
            "time: 1.272327184677124 1.3685325903770251\n",
            "i, sloss, closs, correct 273 0.43275803327560425 2.0539355278015137 0.21875\n",
            "time: 1.4583799839019775 1.3688632789319448\n",
            "i, sloss, closs, correct 274 0.28312769532203674 1.9917773008346558 0.1875\n",
            "time: 1.751401424407959 1.3702594115517357\n",
            "i, sloss, closs, correct 275 0.5214626789093018 1.9795475006103516 0.140625\n",
            "time: 1.258019208908081 1.3698545668436133\n",
            "i, sloss, closs, correct 276 0.4059078097343445 2.0297181606292725 0.28125\n",
            "time: 1.284081220626831 1.3695466225758357\n",
            "i, sloss, closs, correct 277 0.40200889110565186 2.0422918796539307 0.34375\n",
            "time: 1.2785091400146484 1.369221175317284\n",
            "i, sloss, closs, correct 278 0.18791641294956207 1.9692171812057495 0.265625\n",
            "time: 1.2866158485412598 1.3689271044987503\n",
            "i, sloss, closs, correct 279 0.21746107935905457 2.114103078842163 0.296875\n",
            "time: 1.2667298316955566 1.3685639304774149\n",
            "i, sloss, closs, correct 280 0.3453577756881714 1.990892767906189 0.296875\n",
            "time: 1.2679083347320557 1.3682074326213145\n",
            "i, sloss, closs, correct 281 0.21441903710365295 1.9215987920761108 0.328125\n",
            "time: 1.2660987377166748 1.3678473031267206\n",
            "i, sloss, closs, correct 282 0.5087145566940308 1.927039384841919 0.3125\n",
            "time: 1.3785603046417236 1.3678868465625778\n",
            "i, sloss, closs, correct 283 0.44593462347984314 2.130074977874756 0.34375\n",
            "time: 1.703979253768921 1.369071601981848\n",
            "i, sloss, closs, correct 284 0.31593266129493713 2.0457258224487305 0.359375\n",
            "time: 1.423018217086792 1.36926224691826\n",
            "i, sloss, closs, correct 285 0.37615397572517395 2.0200281143188477 0.234375\n",
            "time: 1.2598295211791992 1.368881188906156\n",
            "i, sloss, closs, correct 286 0.2651965022087097 2.0263476371765137 0.375\n",
            "time: 1.247035026550293 1.3684581945997498\n",
            "i, sloss, closs, correct 287 0.32424530386924744 1.9034390449523926 0.34375\n",
            "time: 1.2562487125396729 1.3680703416466713\n",
            "i, sloss, closs, correct 288 0.3009890019893646 1.9809308052062988 0.296875\n",
            "time: 1.2550938129425049 1.3676810107841624\n",
            "i, sloss, closs, correct 289 0.49227699637413025 1.91493558883667 0.28125\n",
            "time: 1.2724964618682861 1.367354389716839\n",
            "i, sloss, closs, correct 290 0.2063581943511963 1.9769588708877563 0.234375\n",
            "time: 1.252781629562378 1.3669622255764466\n",
            "i, sloss, closs, correct 291 0.22893591225147247 1.9712846279144287 0.3125\n",
            "time: 1.2589056491851807 1.3665936842356643\n",
            "i, sloss, closs, correct 292 0.19810989499092102 1.9643019437789917 0.3125\n",
            "time: 1.6463265419006348 1.3675501330313828\n",
            "i, sloss, closs, correct 293 0.20966771245002747 1.9155769348144531 0.1875\n",
            "time: 1.5500319004058838 1.3681721062887282\n",
            "i, sloss, closs, correct 294 0.26301246881484985 1.9632797241210938 0.1875\n",
            "time: 1.2325575351715088 1.3677141076427395\n",
            "i, sloss, closs, correct 295 0.24643878638744354 1.931175708770752 0.296875\n",
            "time: 1.249814748764038 1.3673204417164262\n",
            "i, sloss, closs, correct 296 0.3510403037071228 2.1138463020324707 0.3125\n",
            "time: 1.269277811050415 1.3669920120175036\n",
            "i, sloss, closs, correct 297 0.3208647668361664 2.039686918258667 0.203125\n",
            "time: 1.2477457523345947 1.3665934849105426\n",
            "i, sloss, closs, correct 298 0.27770811319351196 2.0185229778289795 0.25\n",
            "time: 1.249211072921753 1.3662025322483535\n",
            "i, sloss, closs, correct 299 0.2739635407924652 1.9635592699050903 0.3125\n",
            "time: 1.2910866737365723 1.3659538332621257\n",
            "i, sloss, closs, correct 300 0.3692159354686737 1.9441044330596924 0.296875\n",
            "time: 1.242969036102295 1.3655468910635509\n",
            "i, sloss, closs, correct 301 0.20548959076404572 1.939418077468872 0.296875\n",
            "time: 1.432483196258545 1.3657702073356173\n",
            "i, sloss, closs, correct 302 0.3235149085521698 1.981877088546753 0.328125\n",
            "time: 1.7410569190979004 1.3670105863325666\n",
            "i, sloss, closs, correct 303 0.28289490938186646 1.987134337425232 0.296875\n",
            "time: 1.274977207183838 1.3667144838132357\n",
            "i, sloss, closs, correct 304 0.23166446387767792 2.0943093299865723 0.25\n",
            "time: 1.2962560653686523 1.3664850016109278\n",
            "i, sloss, closs, correct 305 0.18976642191410065 1.9610930681228638 0.328125\n",
            "time: 1.2544951438903809 1.3661205651713353\n",
            "i, sloss, closs, correct 306 0.22984875738620758 1.9222220182418823 0.25\n",
            "time: 1.2414274215698242 1.3657159929555092\n",
            "i, sloss, closs, correct 307 0.2912589907646179 1.9905809164047241 0.1875\n",
            "time: 1.2749018669128418 1.3654227055512465\n",
            "i, sloss, closs, correct 308 0.2966499328613281 1.9941449165344238 0.28125\n",
            "time: 1.244194746017456 1.3650319082451483\n",
            "i, sloss, closs, correct 309 0.40537330508232117 2.075216293334961 0.25\n",
            "time: 1.242112636566162 1.3646369695663452\n",
            "i, sloss, closs, correct 310 0.2651122510433197 2.0647664070129395 0.25\n",
            "time: 1.246551275253296 1.3642588964995848\n",
            "i, sloss, closs, correct 311 0.5735435485839844 2.018904447555542 0.3125\n",
            "time: 1.6780931949615479 1.3652705649534862\n",
            "i, sloss, closs, correct 312 0.44747769832611084 1.9368512630462646 0.265625\n",
            "time: 1.5048601627349854 1.365723879573444\n",
            "i, sloss, closs, correct 313 0.21486282348632812 1.8891551494598389 0.421875\n",
            "time: 1.244992733001709 1.3653413490125328\n",
            "i, sloss, closs, correct 314 0.44401291012763977 2.01741623878479 0.171875\n",
            "time: 1.2522988319396973 1.3649839333125524\n",
            "i, sloss, closs, correct 315 0.2946234941482544 1.996431589126587 0.203125\n",
            "time: 1.222440242767334 1.3645343539081043\n",
            "i, sloss, closs, correct 316 0.5006387829780579 1.9676456451416016 0.25\n",
            "time: 1.2722740173339844 1.364244822824039\n",
            "i, sloss, closs, correct 317 0.20017923414707184 2.035308837890625 0.359375\n",
            "time: 1.256330966949463 1.3639066811627563\n",
            "i, sloss, closs, correct 318 0.5032442212104797 2.0383472442626953 0.34375\n",
            "time: 1.2529840469360352 1.3635605016845895\n",
            "i, sloss, closs, correct 319 0.2553309500217438 1.9038829803466797 0.359375\n",
            "time: 1.2359282970428467 1.3631632059812546\n",
            "i, sloss, closs, correct 320 0.3422258794307709 2.013504981994629 0.375\n",
            "time: 1.5289549827575684 1.3636811685710681\n",
            "i, sloss, closs, correct 321 0.4411262571811676 1.9882439374923706 0.359375\n",
            "time: 1.6604244709014893 1.364603993315134\n",
            "i, sloss, closs, correct 322 0.35389459133148193 2.010439157485962 0.328125\n",
            "time: 1.2318203449249268 1.364194442613206\n",
            "i, sloss, closs, correct 323 0.16057822108268738 1.8899626731872559 0.296875\n",
            "time: 1.2285823822021484 1.363777325477129\n",
            "i, sloss, closs, correct 324 0.19539067149162292 1.9987986087799072 0.296875\n",
            "time: 1.2214381694793701 1.363340784219595\n",
            "i, sloss, closs, correct 325 0.3893619179725647 2.0027291774749756 0.25\n",
            "time: 1.2215025424957275 1.3629071705180442\n",
            "i, sloss, closs, correct 326 0.2802128791809082 1.984255075454712 0.265625\n",
            "time: 1.2341220378875732 1.3625148254067891\n",
            "i, sloss, closs, correct 327 0.2925979793071747 1.8810036182403564 0.171875\n",
            "time: 1.2333180904388428 1.3621223714293502\n",
            "i, sloss, closs, correct 328 0.21625255048274994 1.8999465703964233 0.265625\n",
            "time: 1.2364518642425537 1.3617417928295656\n",
            "i, sloss, closs, correct 329 0.22592560946941376 2.029306411743164 0.3125\n",
            "time: 1.255406379699707 1.3614209948164044\n",
            "i, sloss, closs, correct 330 0.19041426479816437 2.0317299365997314 0.34375\n",
            "time: 1.704742670059204 1.3624596084352707\n",
            "i, sloss, closs, correct 331 0.46394261717796326 1.9286794662475586 0.265625\n",
            "time: 1.475064992904663 1.3627998871975635\n",
            "i, sloss, closs, correct 332 0.17417946457862854 1.938019871711731 0.28125\n",
            "time: 1.2530865669250488 1.3624719298995651\n",
            "i, sloss, closs, correct 333 0.261811763048172 1.9911973476409912 0.234375\n",
            "time: 1.2505269050598145 1.3621381678267153\n",
            "i, sloss, closs, correct 334 0.24252568185329437 2.0009913444519043 0.28125\n",
            "time: 1.2809381484985352 1.3618971917166638\n",
            "i, sloss, closs, correct 335 0.3656929135322571 1.9684169292449951 0.34375\n",
            "time: 1.2523014545440674 1.361572483465785\n",
            "i, sloss, closs, correct 336 0.20997178554534912 2.011444568634033 0.359375\n",
            "time: 1.2561821937561035 1.3612612588469282\n",
            "i, sloss, closs, correct 337 0.38280948996543884 2.085655689239502 0.21875\n",
            "time: 1.245096206665039 1.3609189669761432\n",
            "i, sloss, closs, correct 338 0.17361442744731903 1.9596483707427979 0.4375\n",
            "time: 1.293964147567749 1.360722944096478\n",
            "i, sloss, closs, correct 339 0.2747829258441925 1.9737352132797241 0.296875\n",
            "time: 1.660252332687378 1.3616055130958558\n",
            "i, sloss, closs, correct 340 0.14041024446487427 1.932987928390503 0.34375\n",
            "time: 1.6181919574737549 1.3623590742038492\n",
            "i, sloss, closs, correct 341 0.39643794298171997 2.0340163707733154 0.328125\n",
            "time: 1.2640271186828613 1.3620730115656268\n",
            "i, sloss, closs, correct 342 0.4855234920978546 2.067351818084717 0.34375\n",
            "time: 1.2678821086883545 1.3617997808984694\n",
            "i, sloss, closs, correct 343 0.2712428569793701 1.9685009717941284 0.359375\n",
            "time: 1.2690954208374023 1.3615317109019258\n",
            "i, sloss, closs, correct 344 0.20799212157726288 1.90480375289917 0.234375\n",
            "time: 1.284268856048584 1.3613091316775998\n",
            "i, sloss, closs, correct 345 0.43870776891708374 1.9496248960494995 0.453125\n",
            "time: 1.2431113719940186 1.360969047325884\n",
            "i, sloss, closs, correct 346 0.2538594603538513 1.9177303314208984 0.390625\n",
            "time: 1.2413511276245117 1.3606257672268651\n",
            "i, sloss, closs, correct 347 0.22297005355358124 1.8930413722991943 0.359375\n",
            "time: 1.2511606216430664 1.3603125098107876\n",
            "i, sloss, closs, correct 348 0.3243696987628937 1.9422483444213867 0.3125\n",
            "time: 1.436072826385498 1.3605316334262618\n",
            "i, sloss, closs, correct 349 0.5637946724891663 1.9572956562042236 0.359375\n",
            "time: 1.6690657138824463 1.3614143889290946\n",
            "i, sloss, closs, correct 350 0.28330865502357483 1.9688255786895752 0.265625\n",
            "time: 1.2676217555999756 1.3611483675801856\n",
            "i, sloss, closs, correct 351 0.32050204277038574 1.85689377784729 0.296875\n",
            "time: 1.2175848484039307 1.360741782594811\n",
            "i, sloss, closs, correct 352 0.23393183946609497 1.9022847414016724 0.40625\n",
            "time: 1.2197473049163818 1.360343621067555\n",
            "i, sloss, closs, correct 353 0.3837582468986511 1.987205982208252 0.328125\n",
            "time: 1.2056095600128174 1.3599079638551184\n",
            "i, sloss, closs, correct 354 0.17303793132305145 1.9446091651916504 0.390625\n",
            "time: 1.2410407066345215 1.3595744737437074\n",
            "i, sloss, closs, correct 355 0.4630741775035858 2.04526686668396 0.25\n",
            "time: 1.2321691513061523 1.3592179504673132\n",
            "i, sloss, closs, correct 356 0.2263302206993103 1.913222312927246 0.359375\n",
            "time: 1.2287483215332031 1.358855164017664\n",
            "i, sloss, closs, correct 357 0.26023218035697937 1.9963641166687012 0.34375\n",
            "time: 1.2145867347717285 1.3584535994343252\n",
            "i, sloss, closs, correct 358 0.21326445043087006 1.9738969802856445 0.28125\n",
            "time: 1.5898411273956299 1.3590996119305283\n",
            "i, sloss, closs, correct 359 0.541338324546814 2.004241466522217 0.296875\n",
            "time: 1.5777547359466553 1.3597080641322665\n",
            "i, sloss, closs, correct 360 0.26095888018608093 2.0073413848876953 0.25\n",
            "time: 1.2382993698120117 1.3593730616107211\n",
            "i, sloss, closs, correct 361 0.3270033597946167 2.0176215171813965 0.28125\n",
            "time: 1.241440773010254 1.35904869848852\n",
            "i, sloss, closs, correct 362 0.2803642153739929 1.945832371711731 0.328125\n",
            "time: 1.2446575164794922 1.3587352395386079\n",
            "i, sloss, closs, correct 363 0.2945936918258667 2.003141403198242 0.359375\n",
            "time: 1.2275261878967285 1.3583759868537986\n",
            "i, sloss, closs, correct 364 0.18750987946987152 1.9401346445083618 0.34375\n",
            "time: 1.249988317489624 1.358080353148996\n",
            "i, sloss, closs, correct 365 0.20842710137367249 1.9489526748657227 0.234375\n",
            "time: 1.2573113441467285 1.357806370557983\n",
            "i, sloss, closs, correct 366 0.4252797067165375 1.9059330224990845 0.375\n",
            "time: 1.2343597412109375 1.357471309500754\n",
            "i, sloss, closs, correct 367 0.22188618779182434 1.9785873889923096 0.375\n",
            "time: 1.4527251720428467 1.3577314563419507\n",
            "i, sloss, closs, correct 368 0.23364783823490143 1.952275276184082 0.265625\n",
            "time: 1.7054123878479004 1.358674900318549\n",
            "i, sloss, closs, correct 369 0.1549057960510254 2.010211944580078 0.28125\n",
            "time: 1.3416147232055664 1.3586301623163997\n",
            "i, sloss, closs, correct 370 0.20043081045150757 1.9211657047271729 0.28125\n",
            "time: 1.258418083190918 1.3583613296724715\n",
            "i, sloss, closs, correct 371 0.3649183511734009 1.9324443340301514 0.265625\n",
            "time: 1.263512372970581 1.3581076136199377\n",
            "i, sloss, closs, correct 372 0.5027155876159668 1.9236350059509277 0.328125\n",
            "time: 1.2577414512634277 1.3578398208515894\n",
            "i, sloss, closs, correct 373 0.31814879179000854 1.9543401002883911 0.34375\n",
            "time: 1.2806134223937988 1.357634842714524\n",
            "i, sloss, closs, correct 374 0.4897766411304474 2.047466516494751 0.3125\n",
            "time: 1.2617778778076172 1.3573841031392415\n",
            "i, sloss, closs, correct 375 0.3197786509990692 1.9093217849731445 0.265625\n",
            "time: 1.265080451965332 1.357139896205131\n",
            "i, sloss, closs, correct 376 0.24465763568878174 1.9556574821472168 0.375\n",
            "time: 1.28493070602417 1.3569497644743174\n",
            "i, sloss, closs, correct 377 0.25754857063293457 1.9184491634368896 0.46875\n",
            "time: 1.6721596717834473 1.3577903887582203\n",
            "i, sloss, closs, correct 378 0.32341432571411133 1.9146567583084106 0.140625\n",
            "time: 1.487997055053711 1.35813498434102\n",
            "i, sloss, closs, correct 379 0.30008572340011597 1.8187897205352783 0.328125\n",
            "time: 1.2495007514953613 1.3578503778106288\n",
            "i, sloss, closs, correct 380 0.2567974328994751 1.956763744354248 0.40625\n",
            "time: 1.2820038795471191 1.3576525402820017\n",
            "i, sloss, closs, correct 381 0.5332888960838318 1.943359613418579 0.296875\n",
            "time: 1.2718515396118164 1.3574292653518198\n",
            "i, sloss, closs, correct 382 0.26727333664894104 1.8544878959655762 0.296875\n",
            "time: 1.264446496963501 1.3571877373727743\n",
            "i, sloss, closs, correct 383 0.46357741951942444 1.9035041332244873 0.328125\n",
            "time: 1.24546480178833 1.3568980228155851\n",
            "i, sloss, closs, correct 384 0.4678083658218384 1.8807926177978516 0.40625\n",
            "time: 1.269212007522583 1.3566716454245829\n",
            "i, sloss, closs, correct 385 0.20595140755176544 1.9579613208770752 0.296875\n",
            "time: 1.265939474105835 1.3564379258477008\n",
            "i, sloss, closs, correct 386 0.17157579958438873 1.8649300336837769 0.296875\n",
            "time: 1.5897300243377686 1.3570419774807085\n",
            "i, sloss, closs, correct 387 0.25660791993141174 1.9270485639572144 0.3125\n",
            "time: 1.6261496543884277 1.357736529763212\n",
            "i, sloss, closs, correct 388 0.2374986708164215 1.972574234008789 0.3125\n",
            "time: 1.2655649185180664 1.3575007927754852\n",
            "i, sloss, closs, correct 389 0.2534908652305603 1.9389026165008545 0.265625\n",
            "time: 1.2996273040771484 1.3573536609992003\n",
            "i, sloss, closs, correct 390 0.23193015158176422 1.9207854270935059 0.390625\n",
            "time: 1.3004469871520996 1.3572094580706429\n",
            "i, sloss, closs, correct 391 0.1796281486749649 1.9708402156829834 0.25\n",
            "time: 1.273308515548706 1.3569967734570405\n",
            "i, sloss, closs, correct 392 0.23822887241840363 1.9356355667114258 0.296875\n",
            "time: 1.281799077987671 1.3568067022862325\n",
            "i, sloss, closs, correct 393 0.22721385955810547 1.945454478263855 0.359375\n",
            "time: 1.2643296718597412 1.3565731381401798\n",
            "i, sloss, closs, correct 394 0.19850756227970123 1.8953015804290771 0.25\n",
            "time: 1.2633190155029297 1.3563382691974881\n",
            "i, sloss, closs, correct 395 0.25406795740127563 1.90370774269104 0.265625\n",
            "time: 1.5125348567962646 1.3567340163269428\n",
            "i, sloss, closs, correct 396 0.2790367901325226 1.9611120223999023 0.40625\n",
            "time: 1.6942129135131836 1.3575851629002569\n",
            "i, sloss, closs, correct 397 0.16518469154834747 1.7863686084747314 0.28125\n",
            "time: 1.2907836437225342 1.3574185778747252\n",
            "i, sloss, closs, correct 398 0.2916718125343323 1.933596134185791 0.296875\n",
            "time: 1.2758166790008545 1.3572153435613876\n",
            "i, sloss, closs, correct 399 0.3628486096858978 2.0086722373962402 0.34375\n",
            "time: 1.2628238201141357 1.3569805562496184\n",
            "i, sloss, closs, correct 400 0.2775646448135376 1.9938628673553467 0.375\n",
            "time: 1.2507367134094238 1.3567173504769952\n",
            "i, sloss, closs, correct 401 0.4031340479850769 2.0268096923828125 0.34375\n",
            "time: 1.2694675922393799 1.356501554375264\n",
            "i, sloss, closs, correct 402 0.2883288264274597 1.924309492111206 0.34375\n",
            "time: 1.2504417896270752 1.3562404446803016\n",
            "i, sloss, closs, correct 403 0.27544113993644714 1.8693771362304688 0.296875\n",
            "time: 1.2694313526153564 1.3560267509800372\n",
            "i, sloss, closs, correct 404 0.250868558883667 2.0860445499420166 0.3125\n",
            "time: 1.3782846927642822 1.3560826236819044\n",
            "i, sloss, closs, correct 405 0.2584783136844635 1.8050289154052734 0.375\n",
            "time: 1.6776385307312012 1.356875697967454\n",
            "i, sloss, closs, correct 406 0.3207642138004303 1.9531474113464355 0.34375\n",
            "time: 1.416865587234497 1.3570245196836879\n",
            "i, sloss, closs, correct 407 0.20815929770469666 1.9205601215362549 0.390625\n",
            "time: 1.2525756359100342 1.3567696786394305\n",
            "i, sloss, closs, correct 408 0.4316946566104889 1.9056289196014404 0.328125\n",
            "time: 1.2234702110290527 1.3564448642264368\n",
            "i, sloss, closs, correct 409 0.21557816863059998 1.8119993209838867 0.3125\n",
            "time: 1.2179725170135498 1.3561082118894996\n",
            "i, sloss, closs, correct 410 0.41753101348876953 1.946805477142334 0.421875\n",
            "time: 1.218125343322754 1.355773625872721\n",
            "i, sloss, closs, correct 411 0.18169422447681427 1.9878498315811157 0.234375\n",
            "time: 1.2191493511199951 1.3554430899110812\n",
            "i, sloss, closs, correct 412 0.281498521566391 1.7978748083114624 0.34375\n",
            "time: 1.2390635013580322 1.3551624473590251\n",
            "i, sloss, closs, correct 413 0.2939631938934326 1.807218074798584 0.3125\n",
            "time: 1.222701072692871 1.354843595177655\n",
            "i, sloss, closs, correct 414 0.3683178722858429 2.025355339050293 0.375\n",
            "time: 1.5279536247253418 1.3552627546241485\n",
            "i, sloss, closs, correct 415 0.25344711542129517 1.8142530918121338 0.4375\n",
            "time: 1.6018598079681396 1.3558566226409032\n",
            "i, sloss, closs, correct 416 0.29821696877479553 1.9069935083389282 0.34375\n",
            "time: 1.2261319160461426 1.3555467546129112\n",
            "i, sloss, closs, correct 417 0.23152466118335724 1.864868402481079 0.46875\n",
            "time: 1.2468008995056152 1.3552877292678687\n",
            "i, sloss, closs, correct 418 0.2145620733499527 2.0244803428649902 0.28125\n",
            "time: 1.2273602485656738 1.3549835755887634\n",
            "i, sloss, closs, correct 419 0.24970881640911102 2.000436544418335 0.296875\n",
            "time: 1.225776195526123 1.354677072593144\n",
            "i, sloss, closs, correct 420 0.25320765376091003 1.903047800064087 0.328125\n",
            "time: 1.2380938529968262 1.3544013420929535\n",
            "i, sloss, closs, correct 421 0.2766557037830353 1.927897572517395 0.390625\n",
            "time: 1.2261667251586914 1.3540986036237381\n",
            "i, sloss, closs, correct 422 0.3147846460342407 1.8702501058578491 0.421875\n",
            "time: 1.2207214832305908 1.3537844737934446\n",
            "i, sloss, closs, correct 423 0.3570050895214081 1.9199676513671875 0.21875\n",
            "time: 1.2949435710906982 1.3536468603700962\n",
            "i, sloss, closs, correct 424 0.21227994561195374 1.9397159814834595 0.3125\n",
            "time: 1.6532871723175049 1.3543564992792465\n",
            "i, sloss, closs, correct 425 0.23792728781700134 1.9067769050598145 0.328125\n",
            "time: 1.4440641403198242 1.3545680594556209\n",
            "i, sloss, closs, correct 426 0.2976701557636261 1.8536550998687744 0.40625\n",
            "time: 1.2365775108337402 1.3542928343913594\n",
            "i, sloss, closs, correct 427 0.3263512849807739 1.823395013809204 0.375\n",
            "time: 1.243835687637329 1.3540359312128798\n",
            "i, sloss, closs, correct 428 0.29852011799812317 1.8728761672973633 0.40625\n",
            "time: 1.2194042205810547 1.3537232720212782\n",
            "i, sloss, closs, correct 429 0.31762054562568665 1.9006515741348267 0.28125\n",
            "time: 1.2093768119812012 1.353388760256213\n",
            "i, sloss, closs, correct 430 0.2943636476993561 1.8844330310821533 0.34375\n",
            "time: 1.214348316192627 1.353067258671099\n",
            "i, sloss, closs, correct 431 0.3199072778224945 1.9787490367889404 0.390625\n",
            "time: 1.2345213890075684 1.352793938583798\n",
            "i, sloss, closs, correct 432 0.154407799243927 1.8261165618896484 0.296875\n",
            "time: 1.2271666526794434 1.352504949371479\n",
            "i, sloss, closs, correct 433 0.5260908007621765 1.8841978311538696 0.21875\n",
            "time: 1.4671063423156738 1.3527701670123684\n",
            "i, sloss, closs, correct 434 0.195176899433136 1.814163327217102 0.265625\n",
            "time: 1.6535255908966064 1.3534624149059427\n",
            "i, sloss, closs, correct 435 0.4701288640499115 1.89211106300354 0.3125\n",
            "time: 1.3036201000213623 1.3533491273538782\n",
            "i, sloss, closs, correct 436 0.39756470918655396 1.9274485111236572 0.4375\n",
            "time: 1.2358477115631104 1.3530813911141193\n",
            "i, sloss, closs, correct 437 0.34010204672813416 1.9694137573242188 0.421875\n",
            "time: 1.2378525733947754 1.352819397569247\n",
            "i, sloss, closs, correct 438 0.3298815190792084 1.8012781143188477 0.40625\n",
            "time: 1.2329418659210205 1.3525474114950262\n",
            "i, sloss, closs, correct 439 0.2562638521194458 1.9469902515411377 0.328125\n",
            "time: 1.233931303024292 1.352278928865086\n",
            "i, sloss, closs, correct 440 0.2124481201171875 1.8888640403747559 0.3125\n",
            "time: 1.2527360916137695 1.3520543580693183\n",
            "i, sloss, closs, correct 441 0.2671448588371277 1.8220748901367188 0.453125\n",
            "time: 1.244598627090454 1.3518123205970316\n",
            "i, sloss, closs, correct 442 0.30719828605651855 1.9738022089004517 0.3125\n",
            "time: 1.3333930969238281 1.3517718298978785\n",
            "i, sloss, closs, correct 443 0.3033650815486908 1.9783231019973755 0.359375\n",
            "time: 1.6601042747497559 1.352467270047815\n",
            "i, sloss, closs, correct 444 0.23969592154026031 1.92637300491333 0.3125\n",
            "time: 1.4811086654663086 1.352757282471389\n",
            "i, sloss, closs, correct 445 0.3209221065044403 1.910477876663208 0.40625\n",
            "time: 1.2496366500854492 1.3525271447784697\n",
            "i, sloss, closs, correct 446 0.2547883689403534 1.8558247089385986 0.328125\n",
            "time: 1.2669930458068848 1.352336913413916\n",
            "i, sloss, closs, correct 447 0.23366038501262665 1.962005376815796 0.328125\n",
            "time: 1.260206937789917 1.3521319734198707\n",
            "i, sloss, closs, correct 448 0.23689888417720795 1.9599990844726562 0.328125\n",
            "time: 1.2607347965240479 1.3519294734521539\n",
            "i, sloss, closs, correct 449 0.2672288715839386 1.8877224922180176 0.359375\n",
            "time: 1.267519474029541 1.3517442422442967\n",
            "i, sloss, closs, correct 450 0.33840808272361755 1.7975757122039795 0.453125\n",
            "time: 1.2781555652618408 1.3515821845991385\n",
            "i, sloss, closs, correct 451 0.2635713815689087 1.8331655263900757 0.265625\n",
            "time: 1.279505729675293 1.3514238242554453\n",
            "i, sloss, closs, correct 452 0.1913221925497055 1.8155699968338013 0.375\n",
            "time: 1.6409878730773926 1.35206418111098\n",
            "i, sloss, closs, correct 453 0.26203858852386475 1.8694602251052856 0.359375\n",
            "time: 1.5704009532928467 1.3525464177656803\n",
            "i, sloss, closs, correct 454 0.22080977261066437 1.779551386833191 0.375\n",
            "time: 1.2507848739624023 1.3523238197787777\n",
            "i, sloss, closs, correct 455 0.2907799184322357 1.9244263172149658 0.328125\n",
            "time: 1.2649850845336914 1.3521333964247453\n",
            "i, sloss, closs, correct 456 0.33782637119293213 1.9148693084716797 0.203125\n",
            "time: 1.2611069679260254 1.3519352765782395\n",
            "i, sloss, closs, correct 457 0.20648245513439178 1.8689519166946411 0.40625\n",
            "time: 1.2859740257263184 1.3517923344691247\n",
            "i, sloss, closs, correct 458 0.3189832866191864 1.9518083333969116 0.34375\n",
            "time: 1.3141379356384277 1.3517113521467887\n",
            "i, sloss, closs, correct 459 0.2511617839336395 1.9917627573013306 0.34375\n",
            "time: 1.2669003009796143 1.3515280853147091\n",
            "i, sloss, closs, correct 460 0.29024234414100647 1.9601969718933105 0.328125\n",
            "time: 1.216057538986206 1.3512352619667634\n",
            "i, sloss, closs, correct 461 0.4891974925994873 1.844236135482788 0.40625\n",
            "time: 1.4639153480529785 1.3514802584916483\n",
            "i, sloss, closs, correct 462 0.28262773156166077 1.8832426071166992 0.34375\n",
            "time: 1.6284894943237305 1.3520794996175087\n",
            "i, sloss, closs, correct 463 0.3778553903102875 1.9874953031539917 0.359375\n",
            "time: 1.2188701629638672 1.3517934459036793\n",
            "i, sloss, closs, correct 464 0.22459803521633148 1.8976686000823975 0.46875\n",
            "time: 1.2234573364257812 1.3515184633193478\n",
            "i, sloss, closs, correct 465 0.28063729405403137 1.94325590133667 0.375\n",
            "time: 1.229323387145996 1.351257493567569\n",
            "i, sloss, closs, correct 466 0.37878677248954773 1.8109705448150635 0.421875\n",
            "time: 1.2640817165374756 1.3510718263956925\n",
            "i, sloss, closs, correct 467 0.27221211791038513 1.9361469745635986 0.34375\n",
            "time: 1.2563385963439941 1.3508705359238844\n",
            "i, sloss, closs, correct 468 0.1951570361852646 1.8315476179122925 0.3125\n",
            "time: 1.2622311115264893 1.350683735377753\n",
            "i, sloss, closs, correct 469 0.22452419996261597 1.8830864429473877 0.3125\n",
            "time: 1.2708423137664795 1.3505149202143891\n",
            "i, sloss, closs, correct 470 0.32588067650794983 1.8696409463882446 0.328125\n",
            "time: 1.2876918315887451 1.3503824758428424\n",
            "i, sloss, closs, correct 471 0.23296022415161133 1.908447504043579 0.328125\n",
            "time: 1.696751356124878 1.3511172785597332\n",
            "i, sloss, closs, correct 472 0.1711008995771408 1.762904167175293 0.34375\n",
            "time: 1.4785022735595703 1.3513898058119083\n",
            "i, sloss, closs, correct 473 0.2555268108844757 1.895414113998413 0.265625\n",
            "time: 1.2513117790222168 1.351179685270736\n",
            "i, sloss, closs, correct 474 0.4785923659801483 2.037369966506958 0.421875\n",
            "time: 1.2645514011383057 1.350998054303621\n",
            "i, sloss, closs, correct 475 0.23541998863220215 1.9092848300933838 0.3125\n",
            "time: 1.2766060829162598 1.3508423211193885\n",
            "i, sloss, closs, correct 476 0.20332883298397064 2.0318949222564697 0.25\n",
            "time: 1.2434279918670654 1.3506180865209807\n",
            "i, sloss, closs, correct 477 0.2554692327976227 1.8526077270507812 0.34375\n",
            "time: 1.2292656898498535 1.350365208781414\n",
            "i, sloss, closs, correct 478 0.2139280140399933 1.855055570602417 0.34375\n",
            "time: 1.2178313732147217 1.3500895754031697\n",
            "i, sloss, closs, correct 479 0.30978843569755554 1.8413946628570557 0.34375\n",
            "time: 1.2189984321594238 1.3498174513379733\n",
            "i, sloss, closs, correct 480 0.19227498769760132 1.8308794498443604 0.296875\n",
            "time: 1.5071227550506592 1.350145516920982\n",
            "i, sloss, closs, correct 481 0.20583729445934296 2.0192675590515137 0.390625\n",
            "time: 1.6342456340789795 1.3507357113588896\n",
            "i, sloss, closs, correct 482 0.22795680165290833 1.9629929065704346 0.28125\n",
            "time: 1.240203857421875 1.3505079074922803\n",
            "i, sloss, closs, correct 483 0.19076493382453918 1.9060966968536377 0.40625\n",
            "time: 1.233534574508667 1.3502683127221982\n",
            "i, sloss, closs, correct 484 0.39376991987228394 1.915968656539917 0.328125\n",
            "time: 1.2304692268371582 1.350022299265124\n",
            "i, sloss, closs, correct 485 0.3375614881515503 1.8763349056243896 0.3125\n",
            "time: 1.2196896076202393 1.3497550987902982\n",
            "i, sloss, closs, correct 486 0.5472609400749207 1.9593112468719482 0.265625\n",
            "time: 1.2282495498657227 1.3495066263837245\n",
            "i, sloss, closs, correct 487 0.4331414997577667 1.80918288230896 0.34375\n",
            "time: 1.2219634056091309 1.3492462253961406\n",
            "i, sloss, closs, correct 488 0.21041297912597656 2.0048251152038574 0.28125\n",
            "time: 1.2282776832580566 1.349003300345017\n",
            "i, sloss, closs, correct 489 0.23233935236930847 1.9367318153381348 0.359375\n",
            "time: 1.237119436264038 1.3487759882090042\n",
            "i, sloss, closs, correct 490 0.3032473921775818 1.9363638162612915 0.40625\n",
            "time: 1.6422579288482666 1.349374654579551\n",
            "i, sloss, closs, correct 491 0.31389257311820984 1.880373239517212 0.25\n",
            "time: 1.4735956192016602 1.3496278815153169\n",
            "i, sloss, closs, correct 492 0.3580807149410248 1.7752819061279297 0.421875\n",
            "time: 1.2524447441101074 1.3494322822253555\n",
            "i, sloss, closs, correct 493 0.19284282624721527 2.0233683586120605 0.34375\n",
            "time: 1.246654748916626 1.3492251895217278\n",
            "i, sloss, closs, correct 494 0.2800759971141815 1.8159791231155396 0.390625\n",
            "time: 1.2511868476867676 1.3490281244721076\n",
            "i, sloss, closs, correct 495 0.2249402552843094 1.9185049533843994 0.34375\n",
            "time: 1.2458827495574951 1.3488211223194677\n",
            "i, sloss, closs, correct 496 0.28828778862953186 1.8760380744934082 0.359375\n",
            "time: 1.2218852043151855 1.3485666779685068\n",
            "i, sloss, closs, correct 497 0.6163889169692993 1.9004695415496826 0.375\n",
            "time: 1.21946382522583 1.348308405722959\n",
            "i, sloss, closs, correct 498 0.2712601125240326 1.8619781732559204 0.40625\n",
            "time: 1.2311112880706787 1.3480747854542399\n",
            "i, sloss, closs, correct 499 0.3335941433906555 1.922231912612915 0.34375\n",
            "time: 1.4786481857299805 1.3483369064331054\n",
            "i, sloss, closs, correct 500 0.4110793471336365 1.9093067646026611 0.390625\n",
            "time: 1.6364526748657227 1.348913148015797\n",
            "i, sloss, closs, correct 501 0.17211782932281494 1.9479477405548096 0.359375\n",
            "time: 1.2273075580596924 1.3486718894001022\n",
            "i, sloss, closs, correct 502 0.24264748394489288 1.9206583499908447 0.28125\n",
            "time: 1.2221455574035645 1.3484213337983573\n",
            "i, sloss, closs, correct 503 0.5407170653343201 1.8612371683120728 0.390625\n",
            "time: 1.2120203971862793 1.3481515806818765\n",
            "i, sloss, closs, correct 504 0.23092417418956757 1.910566806793213 0.34375\n",
            "time: 1.266733169555664 1.3479910916621143\n",
            "i, sloss, closs, correct 505 0.3966177999973297 1.7776095867156982 0.359375\n",
            "time: 1.2353429794311523 1.3477695063640007\n",
            "i, sloss, closs, correct 506 0.2267269790172577 1.9358491897583008 0.40625\n",
            "time: 1.2277421951293945 1.3475337282440367\n",
            "i, sloss, closs, correct 507 0.23749977350234985 1.843584656715393 0.375\n",
            "time: 1.2438750267028809 1.3473306866142694\n",
            "i, sloss, closs, correct 508 0.2505423128604889 1.8993840217590332 0.484375\n",
            "time: 1.240220308303833 1.3471211993624044\n",
            "i, sloss, closs, correct 509 0.22717100381851196 1.8419357538223267 0.34375\n",
            "time: 1.6434321403503418 1.3477031333773744\n",
            "i, sloss, closs, correct 510 0.26449528336524963 1.8923393487930298 0.21875\n",
            "time: 1.4808721542358398 1.347964929274617\n",
            "i, sloss, closs, correct 511 0.21230530738830566 1.8131401538848877 0.34375\n",
            "time: 1.2158546447753906 1.347707818262279\n",
            "i, sloss, closs, correct 512 0.2316933125257492 1.8152763843536377 0.28125\n",
            "time: 1.2200007438659668 1.3474598590858267\n",
            "i, sloss, closs, correct 513 0.28535282611846924 1.8732268810272217 0.296875\n",
            "time: 1.2141780853271484 1.3472014735181044\n",
            "i, sloss, closs, correct 514 0.3409825563430786 2.018805503845215 0.40625\n",
            "time: 1.226560115814209 1.3469681679623797\n",
            "i, sloss, closs, correct 515 0.5142266750335693 1.8469377756118774 0.28125\n",
            "time: 1.219165325164795 1.3467220266660054\n",
            "i, sloss, closs, correct 516 0.3005976378917694 1.8653817176818848 0.546875\n",
            "time: 1.2204012870788574 1.3464786397649886\n",
            "i, sloss, closs, correct 517 0.1947159320116043 1.849768877029419 0.328125\n",
            "time: 1.2274396419525146 1.346250286893955\n",
            "i, sloss, closs, correct 518 0.3480348587036133 1.847797155380249 0.34375\n",
            "time: 1.4164721965789795 1.3463865470334975\n",
            "i, sloss, closs, correct 519 0.36163130402565 1.9104437828063965 0.28125\n",
            "time: 1.7118608951568604 1.347093710532555\n",
            "i, sloss, closs, correct 520 0.23372036218643188 1.8165404796600342 0.390625\n",
            "time: 1.2347044944763184 1.3468789029258683\n",
            "i, sloss, closs, correct 521 0.46969544887542725 1.9180753231048584 0.4375\n",
            "time: 1.2086677551269531 1.346615051857813\n",
            "i, sloss, closs, correct 522 0.31202220916748047 1.938802719116211 0.25\n",
            "time: 1.3041234016418457 1.3465347103133485\n",
            "i, sloss, closs, correct 523 0.28704971075057983 1.84328031539917 0.328125\n",
            "time: 1.2520709037780762 1.3463553558779127\n",
            "i, sloss, closs, correct 524 0.31158319115638733 1.7266159057617188 0.40625\n",
            "time: 1.2489056587219238 1.3461706697373164\n",
            "i, sloss, closs, correct 525 0.1822039633989334 1.8982653617858887 0.40625\n",
            "time: 1.2206504344940186 1.3459329963183675\n",
            "i, sloss, closs, correct 526 0.3742096722126007 1.8379192352294922 0.375\n",
            "time: 1.2346172332763672 1.3457226888957015\n",
            "i, sloss, closs, correct 527 0.21506863832473755 1.874370813369751 0.390625\n",
            "time: 1.2723941802978516 1.345584712696798\n",
            "i, sloss, closs, correct 528 0.27946844696998596 2.0092577934265137 0.375\n",
            "time: 1.625166654586792 1.3461141365462306\n",
            "i, sloss, closs, correct 529 0.3075430691242218 1.8945846557617188 0.34375\n",
            "time: 1.4895398616790771 1.3463874056654157\n",
            "i, sloss, closs, correct 530 0.31725171208381653 1.9038910865783691 0.421875\n",
            "time: 1.2261743545532227 1.34616195639199\n",
            "i, sloss, closs, correct 531 0.3422582745552063 1.8425233364105225 0.328125\n",
            "time: 1.218639612197876 1.3459231414292987\n",
            "i, sloss, closs, correct 532 0.30157220363616943 2.0487866401672363 0.453125\n",
            "time: 1.21751070022583 1.3456830401357969\n",
            "i, sloss, closs, correct 533 0.1967887580394745 2.004505157470703 0.234375\n",
            "time: 1.2160611152648926 1.3454411373602764\n",
            "i, sloss, closs, correct 534 0.4821696877479553 1.765355110168457 0.328125\n",
            "time: 1.2314493656158447 1.3452291207892872\n",
            "i, sloss, closs, correct 535 0.33959904313087463 1.914367437362671 0.328125\n",
            "time: 1.2346911430358887 1.345023788178145\n",
            "i, sloss, closs, correct 536 0.22433702647686005 1.8771824836730957 0.421875\n",
            "time: 1.2538948059082031 1.3448549262638199\n",
            "i, sloss, closs, correct 537 0.37705186009407043 1.8737597465515137 0.375\n",
            "time: 1.4279003143310547 1.345010127276736\n",
            "i, sloss, closs, correct 538 0.21207979321479797 1.8383917808532715 0.375\n",
            "time: 1.7534642219543457 1.3457686640114863\n",
            "i, sloss, closs, correct 539 0.31250301003456116 1.9998412132263184 0.296875\n",
            "time: 1.266437292098999 1.3456255594889324\n",
            "i, sloss, closs, correct 540 0.21324197947978973 1.7750988006591797 0.421875\n",
            "time: 1.2632970809936523 1.3454742647583515\n",
            "i, sloss, closs, correct 541 0.2039499133825302 1.9878144264221191 0.359375\n",
            "time: 1.2661364078521729 1.3453288051914905\n",
            "i, sloss, closs, correct 542 0.39465343952178955 1.7365361452102661 0.25\n",
            "time: 1.2516658306121826 1.3451572540276178\n",
            "i, sloss, closs, correct 543 0.219945028424263 1.9782979488372803 0.328125\n",
            "time: 1.251488208770752 1.3449859833892654\n",
            "i, sloss, closs, correct 544 0.4034748673439026 1.9041156768798828 0.3125\n",
            "time: 1.279883623123169 1.3448674652554573\n",
            "i, sloss, closs, correct 545 0.26783281564712524 1.869836688041687 0.40625\n",
            "time: 1.2697367668151855 1.344730759278322\n",
            "i, sloss, closs, correct 546 0.33504557609558105 1.850814938545227 0.390625\n",
            "time: 1.3183703422546387 1.3446834479651024\n",
            "i, sloss, closs, correct 547 0.24154116213321686 1.868985652923584 0.28125\n",
            "time: 1.6404974460601807 1.3452239924103675\n",
            "i, sloss, closs, correct 548 0.3369012773036957 1.8772056102752686 0.421875\n",
            "time: 1.4619240760803223 1.3454372891528577\n",
            "i, sloss, closs, correct 549 0.34555283188819885 1.8237946033477783 0.3125\n",
            "time: 1.248859167098999 1.345262559110468\n",
            "i, sloss, closs, correct 550 0.19445762038230896 1.8766008615493774 0.375\n",
            "time: 1.295095443725586 1.3451724134642502\n",
            "i, sloss, closs, correct 551 0.2061091512441635 1.8499611616134644 0.390625\n",
            "time: 1.2549209594726562 1.3450097744015679\n",
            "i, sloss, closs, correct 552 0.2624812126159668 1.846609354019165 0.25\n",
            "time: 1.2849476337432861 1.3449020782387926\n",
            "i, sloss, closs, correct 553 0.27720165252685547 1.9449245929718018 0.359375\n",
            "time: 1.2777683734893799 1.3447817219723863\n",
            "i, sloss, closs, correct 554 0.21328158676624298 1.8767951726913452 0.28125\n",
            "time: 1.2709074020385742 1.3446495038969022\n",
            "i, sloss, closs, correct 555 0.192081481218338 1.9383504390716553 0.3125\n",
            "time: 1.2604732513427734 1.3444990069746114\n",
            "i, sloss, closs, correct 556 0.5497973561286926 1.8425778150558472 0.390625\n",
            "time: 1.608076810836792 1.344973115989507\n",
            "i, sloss, closs, correct 557 0.29554346203804016 1.790836215019226 0.421875\n",
            "time: 1.5867562294006348 1.3454070877430686\n",
            "i, sloss, closs, correct 558 0.17055650055408478 1.7393932342529297 0.34375\n",
            "time: 1.2710719108581543 1.3452749584996424\n",
            "i, sloss, closs, correct 559 0.38651135563850403 1.8350682258605957 0.265625\n",
            "time: 1.2543003559112549 1.3451133821691785\n",
            "i, sloss, closs, correct 560 0.23797808587551117 1.8748204708099365 0.296875\n",
            "time: 1.2689001560211182 1.3449784693658458\n",
            "i, sloss, closs, correct 561 0.23389124870300293 1.8625638484954834 0.3125\n",
            "time: 1.2694039344787598 1.3448448405995488\n",
            "i, sloss, closs, correct 562 0.3302988111972809 1.9345057010650635 0.28125\n",
            "time: 1.2820749282836914 1.3447342003641078\n",
            "i, sloss, closs, correct 563 0.36141327023506165 1.8447736501693726 0.359375\n",
            "time: 1.2660293579101562 1.344595584040838\n",
            "i, sloss, closs, correct 564 0.2206568717956543 1.832103967666626 0.390625\n",
            "time: 1.2594237327575684 1.3444461826729563\n",
            "i, sloss, closs, correct 565 0.3677223026752472 1.8950389623641968 0.21875\n",
            "time: 1.5069928169250488 1.344734209165135\n",
            "i, sloss, closs, correct 566 0.2340775430202484 1.9274433851242065 0.359375\n",
            "time: 1.6687169075012207 1.3453088702348175\n",
            "i, sloss, closs, correct 567 0.24084347486495972 1.7468130588531494 0.390625\n",
            "time: 1.2515466213226318 1.3451446450092424\n",
            "i, sloss, closs, correct 568 0.47550272941589355 1.8182919025421143 0.296875\n",
            "time: 1.2334885597229004 1.34494927562184\n",
            "i, sloss, closs, correct 569 0.32361286878585815 1.9079238176345825 0.296875\n",
            "time: 1.267686128616333 1.344814597932916\n",
            "i, sloss, closs, correct 570 0.2923579812049866 1.7729368209838867 0.375\n",
            "time: 1.2295091152191162 1.3446135362267704\n",
            "i, sloss, closs, correct 571 0.29117316007614136 1.8718245029449463 0.375\n",
            "time: 1.214378833770752 1.344386701817279\n",
            "i, sloss, closs, correct 572 0.500956654548645 1.9514747858047485 0.421875\n",
            "time: 1.2343368530273438 1.3441954749208886\n",
            "i, sloss, closs, correct 573 0.24245521426200867 1.8261804580688477 0.328125\n",
            "time: 1.2269954681396484 1.343992125281889\n",
            "i, sloss, closs, correct 574 0.2498251050710678 1.7042533159255981 0.421875\n",
            "time: 1.2212083339691162 1.3437794917562733\n",
            "i, sloss, closs, correct 575 0.25659704208374023 1.8418375253677368 0.375\n",
            "time: 1.6203253269195557 1.344260418580638\n",
            "i, sloss, closs, correct 576 0.3650708496570587 1.8843750953674316 0.34375\n",
            "time: 1.4805216789245605 1.3444972261399082\n",
            "i, sloss, closs, correct 577 0.23701228201389313 1.8139134645462036 0.375\n",
            "time: 1.2262001037597656 1.3442933712038614\n",
            "i, sloss, closs, correct 578 0.22773218154907227 1.7140979766845703 0.375\n",
            "time: 1.2567548751831055 1.3441430447632785\n",
            "i, sloss, closs, correct 579 0.28697335720062256 1.8567090034484863 0.296875\n",
            "time: 1.249650001525879 1.3439809495005115\n",
            "i, sloss, closs, correct 580 0.259607195854187 1.881136178970337 0.375\n",
            "time: 1.234550952911377 1.3437934135200644\n",
            "i, sloss, closs, correct 581 0.28282639384269714 1.8931405544281006 0.40625\n",
            "time: 1.2398812770843506 1.3436156658782172\n",
            "i, sloss, closs, correct 582 0.41341274976730347 1.903351068496704 0.34375\n",
            "time: 1.2261226177215576 1.3434149777296271\n",
            "i, sloss, closs, correct 583 0.29251232743263245 1.881321668624878 0.265625\n",
            "time: 1.2725248336791992 1.3432944123059103\n",
            "i, sloss, closs, correct 584 0.3047664761543274 1.8365439176559448 0.28125\n",
            "time: 1.4612956047058105 1.3434969999851325\n",
            "i, sloss, closs, correct 585 0.20529381930828094 1.985188364982605 0.28125\n",
            "time: 1.6771528720855713 1.3440673326876382\n",
            "i, sloss, closs, correct 586 0.29688555002212524 1.8007961511611938 0.40625\n",
            "time: 1.24104905128479 1.343892704689117\n",
            "i, sloss, closs, correct 587 0.26404672861099243 1.886579990386963 0.421875\n",
            "time: 1.2295465469360352 1.3436990372177695\n",
            "i, sloss, closs, correct 588 0.40964266657829285 1.7913343906402588 0.34375\n",
            "time: 1.2244408130645752 1.343497361715864\n",
            "i, sloss, closs, correct 589 0.4305661618709564 1.9712755680084229 0.28125\n",
            "time: 1.2251484394073486 1.3432975692264104\n",
            "i, sloss, closs, correct 590 0.5197030901908875 1.7190415859222412 0.296875\n",
            "time: 1.223829746246338 1.3430962578697656\n",
            "i, sloss, closs, correct 591 0.2797732949256897 1.8123764991760254 0.359375\n",
            "time: 1.2239797115325928 1.342895838859919\n",
            "i, sloss, closs, correct 592 0.4955213963985443 1.9250675439834595 0.28125\n",
            "time: 1.2285175323486328 1.3427037605748813\n",
            "i, sloss, closs, correct 593 0.20001310110092163 1.7592331171035767 0.359375\n",
            "time: 1.2265934944152832 1.342509075045987\n",
            "i, sloss, closs, correct 594 0.3442050814628601 1.7655000686645508 0.40625\n",
            "time: 1.6121315956115723 1.342963029957619\n",
            "i, sloss, closs, correct 595 0.2215103805065155 1.9732136726379395 0.4375\n",
            "time: 1.5320065021514893 1.3432808354396948\n",
            "i, sloss, closs, correct 596 0.3454400300979614 1.8489128351211548 0.359375\n",
            "time: 1.2316603660583496 1.343094680377187\n",
            "i, sloss, closs, correct 597 0.1733347624540329 1.7780808210372925 0.390625\n",
            "time: 1.2206354141235352 1.3428908474867958\n",
            "i, sloss, closs, correct 598 0.29982054233551025 1.8340964317321777 0.375\n",
            "time: 1.2199211120605469 1.3426863052610165\n",
            "i, sloss, closs, correct 599 0.3247099816799164 1.9327009916305542 0.40625\n",
            "time: 1.2254767417907715 1.3424917046229046\n",
            "i, sloss, closs, correct 600 0.3877578675746918 1.8565771579742432 0.375\n",
            "time: 1.2326276302337646 1.3423096883713506\n",
            "i, sloss, closs, correct 601 0.22573718428611755 1.8592472076416016 0.265625\n",
            "time: 1.223755121231079 1.3421135404181244\n",
            "i, sloss, closs, correct 602 0.29392239451408386 1.8547035455703735 0.359375\n",
            "time: 1.2156128883361816 1.3419045630972184\n",
            "i, sloss, closs, correct 603 0.23033972084522247 1.7795393466949463 0.390625\n",
            "time: 1.3825676441192627 1.3419727055442254\n",
            "i, sloss, closs, correct 604 0.22606845200061798 1.73878812789917 0.34375\n",
            "time: 1.6620397567749023 1.3425024072000804\n",
            "i, sloss, closs, correct 605 0.21556763350963593 1.813279151916504 0.40625\n",
            "time: 1.320610523223877 1.3424670405120347\n",
            "i, sloss, closs, correct 606 0.5152026414871216 1.76765775680542 0.296875\n",
            "time: 1.2239537239074707 1.3422725691630382\n",
            "i, sloss, closs, correct 607 0.21394658088684082 1.8873467445373535 0.5\n",
            "time: 1.2214691638946533 1.342074712248225\n",
            "i, sloss, closs, correct 608 0.24612335860729218 1.8425019979476929 0.421875\n",
            "time: 1.231048822402954 1.3418932693149461\n",
            "i, sloss, closs, correct 609 0.3143961429595947 1.8102047443389893 0.34375\n",
            "time: 1.2140309810638428 1.3416844063117856\n",
            "i, sloss, closs, correct 610 0.3090131878852844 1.8185865879058838 0.296875\n",
            "time: 1.2198810577392578 1.3414858487936168\n",
            "i, sloss, closs, correct 611 0.2481091320514679 1.8183327913284302 0.359375\n",
            "time: 1.2145588397979736 1.3412791965833677\n",
            "i, sloss, closs, correct 612 0.21029116213321686 1.8702974319458008 0.46875\n",
            "time: 1.215597152709961 1.3410752725834558\n",
            "i, sloss, closs, correct 613 0.23957042396068573 1.888427495956421 0.453125\n",
            "time: 1.5672597885131836 1.341444411961186\n",
            "i, sloss, closs, correct 614 0.5722560882568359 1.7549854516983032 0.4375\n",
            "time: 1.572474718093872 1.3418206846810938\n",
            "i, sloss, closs, correct 615 0.3177404999732971 1.8240740299224854 0.265625\n",
            "time: 1.2499027252197266 1.341672281940262\n",
            "i, sloss, closs, correct 616 0.44555962085723877 1.818605899810791 0.328125\n",
            "time: 1.2113010883331299 1.3414617427164384\n",
            "i, sloss, closs, correct 617 0.3326870799064636 1.9236962795257568 0.328125\n",
            "time: 1.2174556255340576 1.341261849048454\n",
            "i, sloss, closs, correct 618 0.3135736584663391 1.7874854803085327 0.375\n",
            "time: 1.2321128845214844 1.3410863032826315\n",
            "i, sloss, closs, correct 619 0.2346876859664917 1.7996690273284912 0.40625\n",
            "time: 1.255941390991211 1.3409497568684239\n",
            "i, sloss, closs, correct 620 0.2894257605075836 1.8933963775634766 0.390625\n",
            "time: 1.2345833778381348 1.340779218888705\n",
            "i, sloss, closs, correct 621 0.3373531401157379 1.8651518821716309 0.375\n",
            "time: 1.2314119338989258 1.3406041343112467\n",
            "i, sloss, closs, correct 622 0.24687595665454865 1.7851028442382812 0.453125\n",
            "time: 1.3323273658752441 1.3405917391157074\n",
            "i, sloss, closs, correct 623 0.2807285189628601 1.847729206085205 0.3125\n",
            "time: 1.635183334350586 1.341064494007673\n",
            "i, sloss, closs, correct 624 0.2552279531955719 1.729170322418213 0.4375\n",
            "time: 1.4007132053375244 1.341163265991211\n",
            "i, sloss, closs, correct 625 0.3509950637817383 1.842726707458496 0.390625\n",
            "time: 1.2218871116638184 1.3409734667299655\n",
            "i, sloss, closs, correct 626 0.24538102746009827 1.9082551002502441 0.390625\n",
            "time: 1.2208096981048584 1.3407829933379445\n",
            "i, sloss, closs, correct 627 0.24739322066307068 1.9064325094223022 0.375\n",
            "time: 1.2366044521331787 1.3406178579208956\n",
            "i, sloss, closs, correct 628 0.3924596905708313 1.8063130378723145 0.40625\n",
            "time: 1.214301586151123 1.340417907802782\n",
            "i, sloss, closs, correct 629 0.22729547321796417 1.795586347579956 0.234375\n",
            "time: 1.2057340145111084 1.3402048830002073\n",
            "i, sloss, closs, correct 630 0.2821063697338104 1.7397338151931763 0.3125\n",
            "time: 1.225346565246582 1.3400235776855904\n",
            "i, sloss, closs, correct 631 0.4715626537799835 1.774752140045166 0.375\n",
            "time: 1.2258639335632324 1.33984370888034\n",
            "i, sloss, closs, correct 632 0.453805536031723 1.8503270149230957 0.34375\n",
            "time: 1.5139083862304688 1.3401194808984067\n",
            "i, sloss, closs, correct 633 0.48381513357162476 1.7059054374694824 0.390625\n",
            "time: 1.6266920566558838 1.3405721341773915\n",
            "i, sloss, closs, correct 634 0.30922451615333557 1.9219176769256592 0.328125\n",
            "time: 1.2256085872650146 1.3403918499083032\n",
            "i, sloss, closs, correct 635 0.41894397139549255 1.8438876867294312 0.421875\n",
            "time: 1.2293455600738525 1.34021801993532\n",
            "i, sloss, closs, correct 636 0.2241380661725998 1.7765421867370605 0.453125\n",
            "time: 1.2527544498443604 1.3400814761546567\n",
            "i, sloss, closs, correct 637 0.2837164103984833 1.9223942756652832 0.296875\n",
            "time: 1.2162268161773682 1.339888091371351\n",
            "i, sloss, closs, correct 638 0.3803119957447052 1.7527440786361694 0.375\n",
            "time: 1.2216370105743408 1.3397038169496682\n",
            "i, sloss, closs, correct 639 0.182355597615242 1.8680638074874878 0.421875\n",
            "time: 1.2178187370300293 1.3395141202956438\n",
            "i, sloss, closs, correct 640 0.30590856075286865 1.8685660362243652 0.34375\n",
            "time: 1.2436301708221436 1.3393652781309464\n",
            "i, sloss, closs, correct 641 0.21341606974601746 1.9182653427124023 0.46875\n",
            "time: 1.2132935523986816 1.339169638179173\n",
            "i, sloss, closs, correct 642 0.3942682445049286 1.8069095611572266 0.40625\n",
            "time: 1.6747217178344727 1.3396922133017106\n",
            "i, sloss, closs, correct 643 0.45607441663742065 1.7863402366638184 0.375\n",
            "time: 1.5062997341156006 1.339951493355058\n",
            "i, sloss, closs, correct 644 0.5351362824440002 1.8817238807678223 0.359375\n",
            "time: 1.2301852703094482 1.3397820524467055\n",
            "i, sloss, closs, correct 645 0.40985965728759766 1.8271688222885132 0.421875\n",
            "time: 1.2836580276489258 1.3396959378623372\n",
            "i, sloss, closs, correct 646 0.2425268441438675 1.8156826496124268 0.3125\n",
            "time: 1.2487163543701172 1.3395562042961533\n",
            "i, sloss, closs, correct 647 0.25606128573417664 1.9153038263320923 0.421875\n",
            "time: 1.2431604862213135 1.3394081824355655\n",
            "i, sloss, closs, correct 648 0.23353278636932373 1.8164443969726562 0.265625\n",
            "time: 1.2323083877563477 1.339243959756037\n",
            "i, sloss, closs, correct 649 0.33074188232421875 1.7207415103912354 0.390625\n",
            "time: 1.2359082698822021 1.3390857388423039\n",
            "i, sloss, closs, correct 650 0.3278021514415741 1.9065768718719482 0.3125\n",
            "time: 1.220106840133667 1.338903705828384\n",
            "i, sloss, closs, correct 651 0.38611069321632385 1.7864959239959717 0.375\n",
            "time: 1.4474835395812988 1.3390709955268112\n",
            "i, sloss, closs, correct 652 0.3248348832130432 1.8442299365997314 0.453125\n",
            "time: 1.6589643955230713 1.3395626088561545\n",
            "i, sloss, closs, correct 653 0.24269211292266846 1.7037216424942017 0.234375\n",
            "time: 1.2280030250549316 1.3393927958398055\n",
            "i, sloss, closs, correct 654 0.44908326864242554 1.762049674987793 0.3125\n",
            "time: 1.228865385055542 1.3392247469370602\n",
            "i, sloss, closs, correct 655 0.21830087900161743 1.7686080932617188 0.421875\n",
            "time: 1.2074310779571533 1.3390243805763198\n",
            "i, sloss, closs, correct 656 0.48781347274780273 1.8699688911437988 0.4375\n",
            "time: 1.230527639389038 1.338860273361206\n",
            "i, sloss, closs, correct 657 0.3847484588623047 1.862451195716858 0.5\n",
            "time: 1.221827507019043 1.3386831011815636\n",
            "i, sloss, closs, correct 658 0.18925628066062927 1.8551385402679443 0.3125\n",
            "time: 1.2423269748687744 1.338537618495264\n",
            "i, sloss, closs, correct 659 0.3296974301338196 1.98525071144104 0.359375\n",
            "time: 1.2340762615203857 1.3383800654700309\n",
            "i, sloss, closs, correct 660 0.2108597457408905 1.8523067235946655 0.375\n",
            "time: 1.2434043884277344 1.3382370847075862\n",
            "i, sloss, closs, correct 661 0.29416409134864807 1.7896997928619385 0.328125\n",
            "time: 1.6619064807891846 1.3387267178999334\n",
            "i, sloss, closs, correct 662 0.24662677943706512 1.650773525238037 0.28125\n",
            "time: 1.5053660869598389 1.3389784908582363\n",
            "i, sloss, closs, correct 663 0.18683919310569763 1.8607743978500366 0.359375\n",
            "time: 1.2266809940338135 1.3388100919953312\n",
            "i, sloss, closs, correct 664 0.24769072234630585 1.798693060874939 0.390625\n",
            "time: 1.2189276218414307 1.338630539671819\n",
            "i, sloss, closs, correct 665 0.2588598430156708 1.8515057563781738 0.375\n",
            "time: 1.274705171585083 1.3385353091958765\n",
            "i, sloss, closs, correct 666 0.42428308725357056 1.7202839851379395 0.421875\n",
            "time: 1.2203600406646729 1.3383587782886968\n",
            "i, sloss, closs, correct 667 0.6066620349884033 1.7953611612319946 0.390625\n",
            "time: 1.2350177764892578 1.3382048192852272\n",
            "i, sloss, closs, correct 668 0.5115832686424255 1.8328386545181274 0.375\n",
            "time: 1.250145673751831 1.3380739086590183\n",
            "i, sloss, closs, correct 669 0.21316847205162048 1.7285104990005493 0.390625\n",
            "time: 1.2371633052825928 1.3379240957658682\n",
            "i, sloss, closs, correct 670 0.3126773238182068 1.8866691589355469 0.328125\n",
            "time: 1.4329626560211182 1.3380664308096009\n",
            "i, sloss, closs, correct 671 0.21896106004714966 1.7723164558410645 0.40625\n",
            "time: 1.6963303089141846 1.3386003201206524\n",
            "i, sloss, closs, correct 672 0.2927798330783844 1.8628578186035156 0.484375\n",
            "time: 1.2427337169647217 1.3384596830314195\n",
            "i, sloss, closs, correct 673 0.2569303512573242 1.8755731582641602 0.46875\n",
            "time: 1.2283329963684082 1.3382970158353404\n",
            "i, sloss, closs, correct 674 0.5249752998352051 1.8635284900665283 0.328125\n",
            "time: 1.2366340160369873 1.338147108996356\n",
            "i, sloss, closs, correct 675 0.48256996273994446 1.8370907306671143 0.4375\n",
            "time: 1.2224690914154053 1.337977076775929\n",
            "i, sloss, closs, correct 676 0.3767389953136444 1.8542428016662598 0.359375\n",
            "time: 1.2224392890930176 1.3378071742712836\n",
            "i, sloss, closs, correct 677 0.48054519295692444 1.8106071949005127 0.328125\n",
            "time: 1.222672462463379 1.337638101043251\n",
            "i, sloss, closs, correct 678 0.3175504207611084 1.7836294174194336 0.375\n",
            "time: 1.2326745986938477 1.337484232280321\n",
            "i, sloss, closs, correct 679 0.3812835216522217 1.7667049169540405 0.34375\n",
            "time: 1.2405071258544922 1.33734231801594\n",
            "i, sloss, closs, correct 680 0.33330801129341125 1.6972265243530273 0.421875\n",
            "time: 1.6339068412780762 1.3377785199539252\n",
            "i, sloss, closs, correct 681 0.22906267642974854 1.7953194379806519 0.421875\n",
            "time: 1.526818037033081 1.3380562434098588\n",
            "i, sloss, closs, correct 682 0.5487343072891235 1.764669418334961 0.375\n",
            "time: 1.2623233795166016 1.3379461178842436\n",
            "i, sloss, closs, correct 683 0.5151525139808655 1.845177173614502 0.375\n",
            "time: 1.2229080200195312 1.3377786209011635\n",
            "i, sloss, closs, correct 684 0.2175493687391281 1.6678235530853271 0.203125\n",
            "time: 1.2244627475738525 1.337613920921827\n",
            "i, sloss, closs, correct 685 0.23410286009311676 1.7492926120758057 0.40625\n",
            "time: 1.2298243045806885 1.3374575126275376\n",
            "i, sloss, closs, correct 686 0.3336562216281891 1.7390868663787842 0.359375\n",
            "time: 1.2286810874938965 1.3372998928225543\n",
            "i, sloss, closs, correct 687 0.5393668413162231 1.9080973863601685 0.328125\n",
            "time: 1.214158296585083 1.337121921569802\n",
            "i, sloss, closs, correct 688 0.4002222716808319 1.851595163345337 0.5\n",
            "time: 1.2472875118255615 1.3369922378412697\n",
            "i, sloss, closs, correct 689 0.2735788822174072 1.9175207614898682 0.390625\n",
            "time: 1.3668231964111328 1.3370361580364947\n",
            "i, sloss, closs, correct 690 0.39207789301872253 1.7834250926971436 0.40625\n",
            "time: 1.655074119567871 1.3374969534867063\n",
            "i, sloss, closs, correct 691 0.19156400859355927 1.7391146421432495 0.375\n",
            "time: 1.3292150497436523 1.3374869031024117\n",
            "i, sloss, closs, correct 692 0.2838451862335205 1.8803441524505615 0.359375\n",
            "time: 1.2165825366973877 1.3373131211873945\n",
            "i, sloss, closs, correct 693 0.1886352151632309 1.878002643585205 0.34375\n",
            "time: 1.2480723857879639 1.3371852145071332\n",
            "i, sloss, closs, correct 694 0.6261477470397949 1.9184789657592773 0.296875\n",
            "time: 1.221858024597168 1.3370216290727794\n",
            "i, sloss, closs, correct 695 0.272747665643692 1.8968886137008667 0.359375\n",
            "time: 1.2208259105682373 1.3368553656271134\n",
            "i, sloss, closs, correct 696 0.3634268045425415 1.739762544631958 0.34375\n",
            "time: 1.2192490100860596 1.3366874153679036\n",
            "i, sloss, closs, correct 697 0.25513362884521484 1.7758679389953613 0.4375\n",
            "time: 1.2076709270477295 1.3365032088790718\n",
            "i, sloss, closs, correct 698 0.22804220020771027 1.8249101638793945 0.359375\n",
            "time: 1.2121124267578125 1.3363259210436471\n",
            "i, sloss, closs, correct 699 0.21045230329036713 1.8756306171417236 0.375\n",
            "time: 1.5440900325775146 1.336623375415802\n",
            "i, sloss, closs, correct 700 0.1991988867521286 1.7273154258728027 0.390625\n",
            "time: 1.5755658149719238 1.336964811306027\n",
            "i, sloss, closs, correct 701 0.36626335978507996 1.789670705795288 0.328125\n",
            "time: 1.2447938919067383 1.336834218087699\n",
            "i, sloss, closs, correct 702 0.4738481044769287 1.8541746139526367 0.53125\n",
            "time: 1.2129120826721191 1.336659950348595\n",
            "i, sloss, closs, correct 703 0.3207401633262634 1.9032657146453857 0.359375\n",
            "time: 1.206822395324707 1.3364761966195973\n",
            "i, sloss, closs, correct 704 0.21582891047000885 1.8419992923736572 0.421875\n",
            "time: 1.2097985744476318 1.3362984636996655\n",
            "i, sloss, closs, correct 705 0.2477908879518509 1.9474869966506958 0.296875\n",
            "time: 1.2105693817138672 1.3361210667715533\n",
            "i, sloss, closs, correct 706 0.46389442682266235 1.7516522407531738 0.375\n",
            "time: 1.2220442295074463 1.3359604192083594\n",
            "i, sloss, closs, correct 707 0.23628531396389008 1.8205184936523438 0.34375\n",
            "time: 1.2310259342193604 1.3358128781372545\n",
            "i, sloss, closs, correct 708 0.24188227951526642 1.7733879089355469 0.5\n",
            "time: 1.3750677108764648 1.3358689746668375\n",
            "i, sloss, closs, correct 709 0.2369970977306366 1.7166283130645752 0.4375\n",
            "time: 1.6200830936431885 1.336269843746239\n",
            "i, sloss, closs, correct 710 0.33186593651771545 1.650174856185913 0.390625\n",
            "time: 1.4181740283966064 1.3363884665627352\n",
            "i, sloss, closs, correct 711 0.29760506749153137 1.685499906539917 0.4375\n",
            "time: 1.2484831809997559 1.3362656674358282\n",
            "i, sloss, closs, correct 712 0.20840634405612946 1.8642480373382568 0.34375\n",
            "time: 1.2245573997497559 1.336109658277386\n",
            "i, sloss, closs, correct 713 0.4819965362548828 1.7483437061309814 0.5\n",
            "time: 1.2151069641113281 1.3359408712520653\n",
            "i, sloss, closs, correct 714 0.43917372822761536 1.651750087738037 0.359375\n",
            "time: 1.2340099811553955 1.335798976471374\n",
            "i, sloss, closs, correct 715 0.40602734684944153 1.752077579498291 0.375\n",
            "time: 1.2211768627166748 1.3356395956524258\n",
            "i, sloss, closs, correct 716 0.28321975469589233 1.9049019813537598 0.4375\n",
            "time: 1.2357447147369385 1.335501071964514\n",
            "i, sloss, closs, correct 717 0.4241422116756439 1.770706057548523 0.484375\n",
            "time: 1.239922046661377 1.335368606041401\n",
            "i, sloss, closs, correct 718 0.29915300011634827 1.7753437757492065 0.40625\n",
            "time: 1.5080864429473877 1.3356094867695687\n",
            "i, sloss, closs, correct 719 0.4260973632335663 1.917712926864624 0.421875\n",
            "time: 1.6347362995147705 1.336025458243158\n",
            "i, sloss, closs, correct 720 0.28813886642456055 1.7582480907440186 0.34375\n",
            "time: 1.2181692123413086 1.3358626636817286\n",
            "i, sloss, closs, correct 721 0.2486383467912674 1.735229730606079 0.46875\n",
            "time: 1.2197821140289307 1.335702504147453\n",
            "i, sloss, closs, correct 722 0.3048902153968811 1.72197687625885 0.421875\n",
            "time: 1.2214317321777344 1.3355450748902633\n",
            "i, sloss, closs, correct 723 0.5721242427825928 1.7050930261611938 0.421875\n",
            "time: 1.230520486831665 1.3354006337197446\n",
            "i, sloss, closs, correct 724 0.5054104924201965 1.7861313819885254 0.421875\n",
            "time: 1.2366952896118164 1.3352651586203739\n",
            "i, sloss, closs, correct 725 0.3978787362575531 1.803308367729187 0.34375\n",
            "time: 1.2283656597137451 1.3351185912928305\n",
            "i, sloss, closs, correct 726 0.39624425768852234 1.7857718467712402 0.328125\n",
            "time: 1.2347204685211182 1.3349811283069595\n",
            "i, sloss, closs, correct 727 0.30555233359336853 1.824331283569336 0.390625\n",
            "time: 1.2226288318634033 1.3348274774603792\n",
            "i, sloss, closs, correct 728 0.4588454067707062 1.8341068029403687 0.421875\n",
            "time: 1.6643168926239014 1.3352801861913444\n",
            "i, sloss, closs, correct 729 0.5580692291259766 1.736792802810669 0.3125\n",
            "time: 1.4965276718139648 1.3355015826551881\n",
            "i, sloss, closs, correct 730 0.3006947934627533 1.8451675176620483 0.359375\n",
            "time: 1.211559772491455 1.3353326764217643\n",
            "i, sloss, closs, correct 731 0.2922571301460266 1.7985631227493286 0.421875\n",
            "time: 1.2137579917907715 1.3351672288498593\n",
            "i, sloss, closs, correct 732 0.513469398021698 1.816529393196106 0.34375\n",
            "time: 1.2256498336791992 1.3350185066420905\n",
            "i, sloss, closs, correct 733 0.47925713658332825 1.7911441326141357 0.359375\n",
            "time: 1.2167258262634277 1.3348580069373024\n",
            "i, sloss, closs, correct 734 0.3402978777885437 1.7724061012268066 0.40625\n",
            "time: 1.233771800994873 1.3347211169547775\n",
            "i, sloss, closs, correct 735 0.24185596406459808 1.6454545259475708 0.265625\n",
            "time: 1.2300233840942383 1.334579576616702\n",
            "i, sloss, closs, correct 736 0.34436702728271484 1.780328392982483 0.5\n",
            "time: 1.2096147537231445 1.3344106829473736\n",
            "i, sloss, closs, correct 737 0.25333425402641296 1.8896033763885498 0.328125\n",
            "time: 1.3920080661773682 1.3344895303410889\n",
            "i, sloss, closs, correct 738 0.2866511642932892 1.8441765308380127 0.40625\n",
            "time: 1.6849017143249512 1.334964202769877\n",
            "i, sloss, closs, correct 739 0.26277709007263184 1.7732353210449219 0.28125\n",
            "time: 1.2616603374481201 1.3348664728370874\n",
            "i, sloss, closs, correct 740 0.45737847685813904 1.8197118043899536 0.359375\n",
            "time: 1.2164196968078613 1.3347072247390643\n",
            "i, sloss, closs, correct 741 0.3153558671474457 1.8411250114440918 0.421875\n",
            "time: 1.2319471836090088 1.3345693648664778\n",
            "i, sloss, closs, correct 742 0.3159657120704651 1.7480823993682861 0.53125\n",
            "time: 1.25923490524292 1.33446864966268\n",
            "i, sloss, closs, correct 743 0.2747172713279724 1.6947669982910156 0.421875\n",
            "time: 1.223806381225586 1.3343205785238614\n",
            "i, sloss, closs, correct 744 0.4931338429450989 1.8214603662490845 0.421875\n",
            "time: 1.218944787979126 1.334166379582962\n",
            "i, sloss, closs, correct 745 0.27578577399253845 1.7667405605316162 0.40625\n",
            "time: 1.213071346282959 1.334004683405081\n",
            "i, sloss, closs, correct 746 0.18732763826847076 1.6933099031448364 0.421875\n",
            "time: 1.2230150699615479 1.3338567540348774\n",
            "i, sloss, closs, correct 747 0.4258986711502075 1.6980504989624023 0.328125\n",
            "time: 1.580003023147583 1.3341864494078937\n",
            "i, sloss, closs, correct 748 0.2868414521217346 1.8036584854125977 0.359375\n",
            "time: 1.5104317665100098 1.3344222840384266\n",
            "i, sloss, closs, correct 749 0.3147791028022766 1.7840920686721802 0.421875\n",
            "time: 1.2389848232269287 1.3342956787745157\n",
            "i, sloss, closs, correct 750 0.22221116721630096 1.6794441938400269 0.375\n",
            "time: 1.2092463970184326 1.3341298043013572\n",
            "i, sloss, closs, correct 751 0.22772417962551117 1.7771141529083252 0.28125\n",
            "time: 1.2323038578033447 1.3339950613518978\n",
            "i, sloss, closs, correct 752 0.2129848599433899 1.909277319908142 0.375\n",
            "time: 1.2143056392669678 1.3338367704692906\n",
            "i, sloss, closs, correct 753 0.1808113008737564 1.7105454206466675 0.34375\n",
            "time: 1.2263433933258057 1.3336948301811118\n",
            "i, sloss, closs, correct 754 0.22933553159236908 1.7081964015960693 0.375\n",
            "time: 1.2399780750274658 1.3335713212853235\n",
            "i, sloss, closs, correct 755 0.22074466943740845 1.7029755115509033 0.296875\n",
            "time: 1.2174029350280762 1.333418325141624\n",
            "i, sloss, closs, correct 756 0.29854315519332886 1.919927716255188 0.4375\n",
            "time: 1.3353416919708252 1.333421497710315\n",
            "i, sloss, closs, correct 757 0.6028624773025513 1.8426491022109985 0.3125\n",
            "time: 1.6408956050872803 1.333827628938378\n",
            "i, sloss, closs, correct 758 0.40088340640068054 1.7156294584274292 0.40625\n",
            "time: 1.4051885604858398 1.3339231840392503\n",
            "i, sloss, closs, correct 759 0.42620593309402466 1.7729215621948242 0.328125\n",
            "time: 1.2331817150115967 1.3337912694404\n",
            "i, sloss, closs, correct 760 0.5859790444374084 1.7968618869781494 0.40625\n",
            "time: 1.2287817001342773 1.333653928102551\n",
            "i, sloss, closs, correct 761 0.1801924854516983 1.7836320400238037 0.375\n",
            "time: 1.2287538051605225 1.333516950369507\n",
            "i, sloss, closs, correct 762 0.514162540435791 1.6650406122207642 0.34375\n",
            "time: 1.2317728996276855 1.3333842226280923\n",
            "i, sloss, closs, correct 763 0.4068663418292999 1.7256065607070923 0.484375\n",
            "time: 1.2281293869018555 1.333247069600989\n",
            "i, sloss, closs, correct 764 0.2535766363143921 1.829620122909546 0.453125\n",
            "time: 1.221752405166626 1.3331019554262846\n",
            "i, sloss, closs, correct 765 0.5867695212364197 1.747274398803711 0.328125\n",
            "time: 1.2162504196166992 1.3329500498410616\n",
            "i, sloss, closs, correct 766 0.43872255086898804 1.820385456085205 0.421875\n",
            "time: 1.5089972019195557 1.3331801944170658\n",
            "i, sloss, closs, correct 767 0.3109516501426697 1.7353851795196533 0.390625\n",
            "time: 1.614027738571167 1.3335463656112552\n",
            "i, sloss, closs, correct 768 0.30602192878723145 1.7689576148986816 0.359375\n",
            "time: 1.2327020168304443 1.3334158703936712\n",
            "i, sloss, closs, correct 769 0.4270193874835968 1.7939059734344482 0.40625\n",
            "time: 1.233067512512207 1.3332861730030605\n",
            "i, sloss, closs, correct 770 0.29585546255111694 1.8204288482666016 0.328125\n",
            "time: 1.2264409065246582 1.3331482302199387\n",
            "i, sloss, closs, correct 771 0.4848478138446808 1.706557273864746 0.34375\n",
            "time: 1.2294118404388428 1.3330144959410237\n",
            "i, sloss, closs, correct 772 0.21467845141887665 1.7237083911895752 0.40625\n",
            "time: 1.2231838703155518 1.3328730381475846\n",
            "i, sloss, closs, correct 773 0.3196891248226166 1.7591240406036377 0.328125\n",
            "time: 1.2185955047607422 1.332726027922421\n",
            "i, sloss, closs, correct 774 0.40634429454803467 1.8659213781356812 0.34375\n",
            "time: 1.232954502105713 1.3325979398912\n",
            "i, sloss, closs, correct 775 0.37789973616600037 1.776747226715088 0.53125\n",
            "time: 1.2696988582611084 1.3325175188251377\n",
            "i, sloss, closs, correct 776 0.4430263936519623 1.7326825857162476 0.484375\n",
            "time: 1.6243393421173096 1.3328943547134695\n",
            "i, sloss, closs, correct 777 0.28295257687568665 1.7998809814453125 0.390625\n",
            "time: 1.4835822582244873 1.3330885153503222\n",
            "i, sloss, closs, correct 778 0.48306965827941895 1.8144762516021729 0.34375\n",
            "time: 1.2295985221862793 1.332956329083718\n",
            "i, sloss, closs, correct 779 0.22196561098098755 1.9074029922485352 0.4375\n",
            "time: 1.2259023189544678 1.3328196495007245\n",
            "i, sloss, closs, correct 780 0.23554211854934692 1.788480520248413 0.46875\n",
            "time: 1.2292602062225342 1.3326876453492462\n",
            "i, sloss, closs, correct 781 0.298152893781662 1.7168077230453491 0.46875\n",
            "time: 1.2479536533355713 1.332579852065162\n",
            "i, sloss, closs, correct 782 0.3470373749732971 1.842818021774292 0.40625\n",
            "time: 1.2026054859161377 1.3324144806137181\n",
            "i, sloss, closs, correct 783 0.5613100528717041 1.7519601583480835 0.421875\n",
            "time: 1.2199876308441162 1.332271694224708\n",
            "i, sloss, closs, correct 784 0.2049102485179901 1.7670955657958984 0.3125\n",
            "time: 1.2175893783569336 1.332126176129481\n",
            "i, sloss, closs, correct 785 0.6188077330589294 1.8035447597503662 0.375\n",
            "time: 1.4318687915802002 1.3322536390246327\n",
            "i, sloss, closs, correct 786 0.31042036414146423 1.7608590126037598 0.390625\n",
            "time: 1.6928699016571045 1.332717031505541\n",
            "i, sloss, closs, correct 787 0.22571350634098053 1.744626522064209 0.453125\n",
            "time: 1.220447301864624 1.3325752010805352\n",
            "i, sloss, closs, correct 788 0.2764612138271332 1.837557315826416 0.4375\n",
            "time: 1.2165048122406006 1.332428717643438\n",
            "i, sloss, closs, correct 789 0.30422934889793396 1.6811497211456299 0.375\n",
            "time: 1.2187042236328125 1.3322853903227214\n",
            "i, sloss, closs, correct 790 0.2746504247188568 1.8211345672607422 0.4375\n",
            "time: 1.2198338508605957 1.33214385168893\n",
            "i, sloss, closs, correct 791 0.3051527738571167 1.7633967399597168 0.515625\n",
            "time: 1.218177318572998 1.3320005659503167\n",
            "i, sloss, closs, correct 792 0.28084686398506165 1.8153002262115479 0.328125\n",
            "time: 1.2122082710266113 1.331850167932306\n",
            "i, sloss, closs, correct 793 0.25517886877059937 1.6684470176696777 0.46875\n",
            "time: 1.2265958786010742 1.3317181937940776\n",
            "i, sloss, closs, correct 794 0.2960382401943207 1.8623911142349243 0.375\n",
            "time: 1.2461631298065186 1.3316111678597313\n",
            "i, sloss, closs, correct 795 0.25760072469711304 1.7932676076889038 0.390625\n",
            "time: 1.592790126800537 1.3319398778766842\n",
            "i, sloss, closs, correct 796 0.3930831849575043 1.6916472911834717 0.328125\n",
            "time: 1.5332365036010742 1.3321928980957758\n",
            "i, sloss, closs, correct 797 0.32259252667427063 2.038858652114868 0.375\n",
            "time: 1.23170804977417 1.3320675720845847\n",
            "i, sloss, closs, correct 798 0.4098755419254303 1.8126697540283203 0.4375\n",
            "time: 1.2269153594970703 1.3319368034190917\n",
            "i, sloss, closs, correct 799 0.3399868905544281 1.8739044666290283 0.34375\n",
            "time: 1.231602430343628 1.331812027990818\n",
            "i, sloss, closs, correct 800 0.4511798918247223 1.784804344177246 0.40625\n",
            "time: 1.2331583499908447 1.3316894982489158\n",
            "i, sloss, closs, correct 801 0.4377584755420685 1.8123412132263184 0.34375\n",
            "time: 1.2437520027160645 1.3315804735978047\n",
            "i, sloss, closs, correct 802 0.3522026240825653 1.8701499700546265 0.421875\n",
            "time: 1.2384862899780273 1.3314651890679878\n",
            "i, sloss, closs, correct 803 0.5135616660118103 1.8139411211013794 0.390625\n",
            "time: 1.235201358795166 1.3313464027139086\n",
            "i, sloss, closs, correct 804 0.24304810166358948 1.7406611442565918 0.390625\n",
            "time: 1.4488544464111328 1.3314930110244276\n",
            "i, sloss, closs, correct 805 0.40312299132347107 1.8077116012573242 0.359375\n",
            "time: 1.6889052391052246 1.3319369264631058\n",
            "i, sloss, closs, correct 806 0.2561515271663666 1.903991460800171 0.40625\n",
            "time: 1.3288824558258057 1.331933683031406\n",
            "i, sloss, closs, correct 807 0.1772964596748352 1.762526273727417 0.359375\n",
            "time: 1.2348525524139404 1.3318141533596681\n",
            "i, sloss, closs, correct 808 0.25304633378982544 1.9322338104248047 0.28125\n",
            "time: 1.2589521408081055 1.3317247011752888\n",
            "i, sloss, closs, correct 809 0.24440018832683563 1.7204549312591553 0.390625\n",
            "time: 1.2220485210418701 1.3315898995340607\n",
            "i, sloss, closs, correct 810 0.2350582629442215 1.7413158416748047 0.5\n",
            "time: 1.2281794548034668 1.3314629650586336\n",
            "i, sloss, closs, correct 811 0.28437843918800354 1.8353407382965088 0.296875\n",
            "time: 1.2358074188232422 1.331345781904136\n",
            "i, sloss, closs, correct 812 0.3388570547103882 1.7474966049194336 0.390625\n",
            "time: 1.2168810367584229 1.3312055683370592\n",
            "i, sloss, closs, correct 813 0.4318571984767914 1.8292560577392578 0.484375\n",
            "time: 1.237558364868164 1.3310910926111208\n",
            "i, sloss, closs, correct 814 0.19591335952281952 1.80030357837677 0.34375\n",
            "time: 1.5869526863098145 1.331405608054319\n",
            "i, sloss, closs, correct 815 0.41453585028648376 1.7641692161560059 0.359375\n",
            "time: 1.5630061626434326 1.3316899400715734\n",
            "i, sloss, closs, correct 816 0.3200342357158661 1.7993024587631226 0.3125\n",
            "time: 1.2388896942138672 1.3315769304114426\n",
            "i, sloss, closs, correct 817 0.17105020582675934 1.7664999961853027 0.484375\n",
            "time: 1.2209861278533936 1.3314423190935316\n",
            "i, sloss, closs, correct 818 0.3909021019935608 1.7388460636138916 0.234375\n",
            "time: 1.2415802478790283 1.3313331735003127\n",
            "i, sloss, closs, correct 819 0.4804314076900482 1.7045273780822754 0.3125\n",
            "time: 1.229395866394043 1.331209449942519\n",
            "i, sloss, closs, correct 820 0.28270190954208374 1.8018720149993896 0.28125\n",
            "time: 1.2325661182403564 1.3310898848776405\n",
            "i, sloss, closs, correct 821 0.25346851348876953 1.7686481475830078 0.40625\n",
            "time: 1.2347877025604248 1.330973314252793\n",
            "i, sloss, closs, correct 822 0.24779559671878815 1.7220065593719482 0.390625\n",
            "time: 1.2228491306304932 1.3308425036554348\n",
            "i, sloss, closs, correct 823 0.35936439037323 1.7758302688598633 0.453125\n",
            "time: 1.349626064300537 1.3308658926811032\n",
            "i, sloss, closs, correct 824 0.3911285996437073 1.783151626586914 0.4375\n",
            "time: 1.6415579319000244 1.3312429928057121\n",
            "i, sloss, closs, correct 825 0.19368025660514832 1.8334228992462158 0.34375\n",
            "time: 1.3724262714385986 1.331293334683841\n",
            "i, sloss, closs, correct 826 0.257435142993927 1.8073015213012695 0.375\n",
            "time: 1.2239696979522705 1.3311641596825263\n",
            "i, sloss, closs, correct 827 0.3736039996147156 1.7981665134429932 0.421875\n",
            "time: 1.2493023872375488 1.3310658721532223\n",
            "i, sloss, closs, correct 828 0.5929707288742065 1.6811461448669434 0.34375\n",
            "time: 1.2398598194122314 1.3309564567446277\n",
            "i, sloss, closs, correct 829 0.2660779356956482 1.6521066427230835 0.421875\n",
            "time: 1.225569248199463 1.3308300621538278\n",
            "i, sloss, closs, correct 830 0.4399089515209198 1.8705756664276123 0.390625\n",
            "time: 1.2236909866333008 1.3307017009037352\n",
            "i, sloss, closs, correct 831 0.1957029104232788 1.7686772346496582 0.375\n",
            "time: 1.2202866077423096 1.3305695460966\n",
            "i, sloss, closs, correct 832 0.2962337136268616 1.7973897457122803 0.453125\n",
            "time: 1.270599365234375 1.330498122939972\n",
            "i, sloss, closs, correct 833 0.3799973428249359 1.688305139541626 0.4375\n",
            "time: 1.560682773590088 1.3307746803160194\n",
            "i, sloss, closs, correct 834 0.2599601447582245 1.7991242408752441 0.40625\n",
            "time: 1.5702390670776367 1.331062045068798\n",
            "i, sloss, closs, correct 835 0.26484227180480957 1.7563250064849854 0.3125\n",
            "time: 1.2458763122558594 1.3309607360351599\n",
            "i, sloss, closs, correct 836 0.5099732279777527 1.7023489475250244 0.453125\n",
            "time: 1.2313873767852783 1.3308423328855417\n",
            "i, sloss, closs, correct 837 0.2772882580757141 1.7959516048431396 0.390625\n",
            "time: 1.230644941329956 1.330723319531625\n",
            "i, sloss, closs, correct 838 0.392400324344635 1.7397902011871338 0.328125\n",
            "time: 1.2305784225463867 1.3306052573957659\n",
            "i, sloss, closs, correct 839 0.3653380274772644 1.7623085975646973 0.40625\n",
            "time: 1.2383906841278076 1.3304960338842302\n",
            "i, sloss, closs, correct 840 0.22188007831573486 1.6069573163986206 0.40625\n",
            "time: 1.2317476272583008 1.3303792198829787\n",
            "i, sloss, closs, correct 841 0.23744185268878937 1.677769660949707 0.40625\n",
            "time: 1.216395616531372 1.3302444355504812\n",
            "i, sloss, closs, correct 842 0.3497897684574127 1.7288436889648438 0.40625\n",
            "time: 1.3003425598144531 1.3302095009904455\n",
            "i, sloss, closs, correct 843 0.1978277564048767 1.7334856986999512 0.34375\n",
            "time: 1.6059036254882812 1.3305380793544352\n",
            "i, sloss, closs, correct 844 0.2251061201095581 1.8219401836395264 0.421875\n",
            "time: 1.4365408420562744 1.3306639473819168\n",
            "i, sloss, closs, correct 845 0.2077343761920929 1.6907408237457275 0.40625\n",
            "time: 1.2542426586151123 1.3305741393537949\n",
            "i, sloss, closs, correct 846 0.35533055663108826 1.6761908531188965 0.359375\n",
            "time: 1.229780673980713 1.3304556719387013\n",
            "i, sloss, closs, correct 847 0.19124835729599 1.7219288349151611 0.421875\n",
            "time: 1.271862268447876 1.3303871106988978\n",
            "i, sloss, closs, correct 848 0.5245954394340515 1.7629642486572266 0.328125\n",
            "time: 1.2199678421020508 1.3302578347591405\n",
            "i, sloss, closs, correct 849 0.4210940897464752 1.8768504858016968 0.4375\n",
            "time: 1.2364087104797363 1.3301479777167826\n",
            "i, sloss, closs, correct 850 0.26076608896255493 1.7925277948379517 0.421875\n",
            "time: 1.2460951805114746 1.3300497643956006\n",
            "i, sloss, closs, correct 851 0.37711021304130554 1.7849314212799072 0.375\n",
            "time: 1.2263751029968262 1.3299285942959673\n",
            "i, sloss, closs, correct 852 0.27154141664505005 1.790532112121582 0.296875\n",
            "time: 1.5381295680999756 1.3301731950373052\n",
            "i, sloss, closs, correct 853 0.36223629117012024 1.6864616870880127 0.4375\n",
            "time: 1.5987167358398438 1.3304880985889838\n",
            "i, sloss, closs, correct 854 0.2981935143470764 1.7328139543533325 0.328125\n",
            "time: 1.25518798828125 1.330400588498478\n",
            "i, sloss, closs, correct 855 0.38468652963638306 1.784247636795044 0.34375\n",
            "time: 1.2261219024658203 1.3302793622573958\n",
            "i, sloss, closs, correct 856 0.3075513243675232 1.8114488124847412 0.375\n",
            "time: 1.2224042415618896 1.3301540442080593\n",
            "i, sloss, closs, correct 857 0.4496844410896301 1.8249285221099854 0.421875\n",
            "time: 1.2346110343933105 1.330043243639397\n",
            "i, sloss, closs, correct 858 0.22058236598968506 1.8975260257720947 0.421875\n",
            "time: 1.2353935241699219 1.3299336108672883\n",
            "i, sloss, closs, correct 859 0.19701159000396729 1.5688879489898682 0.390625\n",
            "time: 1.240035057067871 1.3298296063445336\n",
            "i, sloss, closs, correct 860 0.2727315127849579 1.8687565326690674 0.40625\n",
            "time: 1.2004146575927734 1.329679885392405\n",
            "i, sloss, closs, correct 861 0.24889488518238068 1.6732118129730225 0.421875\n",
            "time: 1.2264387607574463 1.3295606936213589\n",
            "i, sloss, closs, correct 862 0.2960663437843323 1.6908907890319824 0.265625\n",
            "time: 1.660022497177124 1.3299439094295922\n",
            "i, sloss, closs, correct 863 0.45369991660118103 1.6650466918945312 0.375\n",
            "time: 1.4836921691894531 1.3301233596823834\n",
            "i, sloss, closs, correct 864 0.3421745002269745 1.8025016784667969 0.484375\n",
            "time: 1.2322664260864258 1.330010830322442\n",
            "i, sloss, closs, correct 865 0.29892852902412415 1.679453730583191 0.421875\n",
            "time: 1.2158832550048828 1.3298802128014333\n",
            "i, sloss, closs, correct 866 0.41884133219718933 1.7386248111724854 0.484375\n",
            "time: 1.2180240154266357 1.3297519791085002\n",
            "i, sloss, closs, correct 867 0.2426852434873581 1.6817587614059448 0.390625\n",
            "time: 1.2453045845031738 1.3296552264745334\n",
            "i, sloss, closs, correct 868 0.3399718999862671 1.7174409627914429 0.421875\n",
            "time: 1.2012784481048584 1.3295080294131685\n",
            "i, sloss, closs, correct 869 0.3953213691711426 1.7580492496490479 0.421875\n",
            "time: 1.2384300231933594 1.3294039035665577\n",
            "i, sloss, closs, correct 870 0.33818721771240234 1.6971485614776611 0.40625\n",
            "time: 1.2331068515777588 1.3292939014193932\n",
            "i, sloss, closs, correct 871 0.5458104014396667 1.8202368021011353 0.375\n",
            "time: 1.4516870975494385 1.3294347981247334\n",
            "i, sloss, closs, correct 872 0.21892228722572327 1.7713661193847656 0.4375\n",
            "time: 1.6814994812011719 1.329838620838021\n",
            "i, sloss, closs, correct 873 0.2541915476322174 1.762235164642334 0.421875\n",
            "time: 1.261061429977417 1.3297618458418334\n",
            "i, sloss, closs, correct 874 0.3699983060359955 1.8882100582122803 0.328125\n",
            "time: 1.2267308235168457 1.3296446317945207\n",
            "i, sloss, closs, correct 875 0.23267783224582672 1.7929731607437134 0.5\n",
            "time: 1.2208073139190674 1.3295209263557712\n",
            "i, sloss, closs, correct 876 0.3787717819213867 1.8389315605163574 0.296875\n",
            "time: 1.2078170776367188 1.329382694977311\n",
            "i, sloss, closs, correct 877 0.27735236287117004 1.8014569282531738 0.34375\n",
            "time: 1.2115488052368164 1.3292495835071816\n",
            "i, sloss, closs, correct 878 0.1967567354440689 1.745870590209961 0.484375\n",
            "time: 1.2205581665039062 1.3291264483546235\n",
            "i, sloss, closs, correct 879 0.2730308175086975 1.6541551351547241 0.453125\n",
            "time: 1.2345328330993652 1.329019518602978\n",
            "i, sloss, closs, correct 880 0.4177287220954895 1.717876672744751 0.40625\n",
            "time: 1.2325773239135742 1.3289105892181396\n",
            "i, sloss, closs, correct 881 0.2391330897808075 1.8387516736984253 0.484375\n",
            "time: 1.5899832248687744 1.3292071378690586\n",
            "i, sloss, closs, correct 882 0.2588329017162323 1.6604899168014526 0.421875\n",
            "time: 1.5293726921081543 1.3294342695519177\n",
            "i, sloss, closs, correct 883 0.22259101271629333 1.786892294883728 0.40625\n",
            "time: 1.2313652038574219 1.3293238907917593\n",
            "i, sloss, closs, correct 884 0.3662073314189911 1.8344752788543701 0.4375\n",
            "time: 1.2343225479125977 1.3292170923308464\n",
            "i, sloss, closs, correct 885 0.42243528366088867 1.7635310888290405 0.40625\n",
            "time: 1.2258927822113037 1.329100978562848\n",
            "i, sloss, closs, correct 886 0.24349026381969452 1.7153918743133545 0.484375\n",
            "time: 1.2254090309143066 1.328984628804238\n",
            "i, sloss, closs, correct 887 0.20956289768218994 1.783830165863037 0.390625\n",
            "time: 1.2330560684204102 1.3288771566506978\n",
            "i, sloss, closs, correct 888 0.31363990902900696 1.641337275505066 0.40625\n",
            "time: 1.2308435440063477 1.3287674176545459\n",
            "i, sloss, closs, correct 889 0.2812926173210144 1.789102554321289 0.375\n",
            "time: 1.2302589416503906 1.3286572836758046\n",
            "i, sloss, closs, correct 890 0.2587525248527527 1.995252013206482 0.421875\n",
            "time: 1.3590617179870605 1.3286919259329302\n",
            "i, sloss, closs, correct 891 0.268891841173172 1.6565378904342651 0.28125\n",
            "time: 1.6469542980194092 1.3290491801740878\n",
            "i, sloss, closs, correct 892 0.21573033928871155 1.8642617464065552 0.34375\n",
            "time: 1.3557085990905762 1.329079564181047\n",
            "i, sloss, closs, correct 893 0.1783323884010315 1.7336773872375488 0.46875\n",
            "time: 1.242100715637207 1.3289827873776157\n",
            "i, sloss, closs, correct 894 0.2556738257408142 1.819385290145874 0.5\n",
            "time: 1.2894906997680664 1.3289403382626326\n",
            "i, sloss, closs, correct 895 0.22434143722057343 1.7772727012634277 0.484375\n",
            "time: 1.233339548110962 1.3288342029388462\n",
            "i, sloss, closs, correct 896 0.2097797691822052 1.8265479803085327 0.484375\n",
            "time: 1.238647222518921 1.3287342025284783\n",
            "i, sloss, closs, correct 897 0.5257129073143005 1.8276723623275757 0.40625\n",
            "time: 1.2607085704803467 1.3286589909768052\n",
            "i, sloss, closs, correct 898 0.28116491436958313 1.7040607929229736 0.328125\n",
            "time: 1.2235088348388672 1.3285425343158115\n",
            "i, sloss, closs, correct 899 0.35747429728507996 1.6331286430358887 0.375\n",
            "time: 1.2395789623260498 1.3284441939989726\n",
            "i, sloss, closs, correct 900 0.2212943583726883 1.6096103191375732 0.390625\n",
            "time: 1.595970630645752 1.3287416367102145\n",
            "i, sloss, closs, correct 901 0.38094156980514526 1.7730023860931396 0.46875\n",
            "time: 1.5647304058074951 1.3290036787214934\n",
            "i, sloss, closs, correct 902 0.39518409967422485 1.8294243812561035 0.328125\n",
            "time: 1.2312273979187012 1.328895927399628\n",
            "i, sloss, closs, correct 903 0.38589712977409363 1.668182611465454 0.3125\n",
            "time: 1.2203450202941895 1.3287763384591162\n",
            "i, sloss, closs, correct 904 0.2023446261882782 1.8258893489837646 0.34375\n",
            "time: 1.2106328010559082 1.3286462965591177\n",
            "i, sloss, closs, correct 905 0.4313669204711914 1.685878038406372 0.515625\n",
            "time: 1.2423951625823975 1.3285516660997674\n",
            "i, sloss, closs, correct 906 0.2035473734140396 1.7258059978485107 0.40625\n",
            "time: 1.231675386428833 1.3284453630710253\n",
            "i, sloss, closs, correct 907 0.20090381801128387 1.6154013872146606 0.328125\n",
            "time: 1.2189135551452637 1.328325776014034\n",
            "i, sloss, closs, correct 908 0.2732560634613037 1.7790721654891968 0.421875\n",
            "time: 1.231407880783081 1.3282196647656632\n",
            "i, sloss, closs, correct 909 0.2974071502685547 1.8158150911331177 0.3125\n",
            "time: 1.3894636631011963 1.3282875024355374\n",
            "i, sloss, closs, correct 910 0.2798064351081848 1.7396025657653809 0.3125\n",
            "time: 1.6756434440612793 1.3286709850889138\n",
            "i, sloss, closs, correct 911 0.2656335234642029 1.8761736154556274 0.359375\n",
            "time: 1.3467373847961426 1.3286914054239005\n",
            "i, sloss, closs, correct 912 0.20381061732769012 1.8086515665054321 0.546875\n",
            "time: 1.2111170291900635 1.3285632284943663\n",
            "i, sloss, closs, correct 913 0.2216995805501938 1.7292490005493164 0.34375\n",
            "time: 1.2533104419708252 1.328481430819572\n",
            "i, sloss, closs, correct 914 0.3353217840194702 1.8056447505950928 0.375\n",
            "time: 1.218306541442871 1.3283615391111114\n",
            "i, sloss, closs, correct 915 0.4960111081600189 1.6638238430023193 0.375\n",
            "time: 1.2317092418670654 1.3282565359985985\n",
            "i, sloss, closs, correct 916 0.4637910723686218 1.75508713722229 0.46875\n",
            "time: 1.227698564529419 1.3281474022464899\n",
            "i, sloss, closs, correct 917 0.27248314023017883 1.7558302879333496 0.375\n",
            "time: 1.2116470336914062 1.3280210172948235\n",
            "i, sloss, closs, correct 918 0.23037658631801605 1.7120535373687744 0.375\n",
            "time: 1.2261173725128174 1.3279106705180967\n",
            "i, sloss, closs, correct 919 0.25751134753227234 1.7905255556106567 0.390625\n",
            "time: 1.5705604553222656 1.3281749328841335\n",
            "i, sloss, closs, correct 920 0.19364884495735168 1.687583565711975 0.40625\n",
            "time: 1.626878261566162 1.3284996614652917\n",
            "i, sloss, closs, correct 921 0.4041208326816559 1.8273510932922363 0.375\n",
            "time: 1.2251207828521729 1.3283880540450586\n",
            "i, sloss, closs, correct 922 0.474213570356369 1.8081823587417603 0.34375\n",
            "time: 1.234442949295044 1.3282867697181577\n",
            "i, sloss, closs, correct 923 0.3903615474700928 1.8655768632888794 0.421875\n",
            "time: 1.2209279537200928 1.3281710808411304\n",
            "i, sloss, closs, correct 924 0.24685820937156677 1.6793886423110962 0.4375\n",
            "time: 1.215588092803955 1.3280498762388486\n",
            "i, sloss, closs, correct 925 0.23075467348098755 1.6719412803649902 0.328125\n",
            "time: 1.2154483795166016 1.3279287516427092\n",
            "i, sloss, closs, correct 926 0.4178296625614166 1.7660139799118042 0.359375\n",
            "time: 1.244795322418213 1.3278395761821133\n",
            "i, sloss, closs, correct 927 0.6013839840888977 1.8297741413116455 0.375\n",
            "time: 1.2163686752319336 1.3277199969723308\n",
            "i, sloss, closs, correct 928 0.24213264882564545 1.8776648044586182 0.421875\n",
            "time: 1.2792022228240967 1.3276683013842103\n",
            "i, sloss, closs, correct 929 0.1984422206878662 1.7975053787231445 0.375\n",
            "time: 1.599614143371582 1.3279611982325072\n",
            "i, sloss, closs, correct 930 0.2134530395269394 1.7894235849380493 0.328125\n",
            "time: 1.4389550685882568 1.328080818553232\n",
            "i, sloss, closs, correct 931 0.4998306632041931 1.6451747417449951 0.421875\n",
            "time: 1.2179036140441895 1.3279631235057192\n",
            "i, sloss, closs, correct 932 0.440405011177063 1.6165988445281982 0.4375\n",
            "time: 1.2247059345245361 1.3278529776893118\n",
            "i, sloss, closs, correct 933 0.3866829574108124 1.8564724922180176 0.359375\n",
            "time: 1.2381298542022705 1.327757426707086\n",
            "i, sloss, closs, correct 934 0.39141201972961426 1.7261003255844116 0.4375\n",
            "time: 1.2181406021118164 1.3276407221421838\n",
            "i, sloss, closs, correct 935 0.4399789869785309 1.7322739362716675 0.328125\n",
            "time: 1.227349042892456 1.3275340701270306\n",
            "i, sloss, closs, correct 936 0.2566303610801697 1.7588446140289307 0.328125\n",
            "time: 1.229552984237671 1.327430002462902\n",
            "i, sloss, closs, correct 937 0.5698761343955994 1.6873167753219604 0.359375\n",
            "time: 1.2069063186645508 1.3273020039743453\n",
            "i, sloss, closs, correct 938 0.286445289850235 1.7488672733306885 0.484375\n",
            "time: 1.4491305351257324 1.3274322680898876\n",
            "i, sloss, closs, correct 939 0.2550398111343384 1.8523201942443848 0.546875\n",
            "time: 1.6667072772979736 1.327793612125072\n",
            "i, sloss, closs, correct 940 0.36361807584762573 1.6507093906402588 0.40625\n",
            "time: 1.2291805744171143 1.3276892336724795\n",
            "i, sloss, closs, correct 941 0.18541279435157776 1.7303524017333984 0.421875\n",
            "time: 1.2338292598724365 1.3275901058929993\n",
            "i, sloss, closs, correct 942 0.19048209488391876 1.5649206638336182 0.4375\n",
            "time: 1.2109761238098145 1.3274669621933808\n",
            "i, sloss, closs, correct 943 0.365847110748291 1.6962693929672241 0.40625\n",
            "time: 1.2367832660675049 1.3273714025646954\n",
            "i, sloss, closs, correct 944 0.20052139461040497 1.6867166757583618 0.4375\n",
            "time: 1.2283611297607422 1.327267163392728\n",
            "i, sloss, closs, correct 945 0.2377147078514099 1.843428134918213 0.40625\n",
            "time: 1.2490270137786865 1.327184984598301\n",
            "i, sloss, closs, correct 946 0.31562355160713196 1.645074486732483 0.421875\n",
            "time: 1.2479469776153564 1.3271018121914728\n",
            "i, sloss, closs, correct 947 0.3515544831752777 1.5811054706573486 0.375\n",
            "time: 1.210665225982666 1.3269795370504323\n",
            "i, sloss, closs, correct 948 0.18820592761039734 1.646226167678833 0.484375\n",
            "time: 1.6110243797302246 1.3272793654772455\n",
            "i, sloss, closs, correct 949 0.3428439497947693 1.7294342517852783 0.375\n",
            "time: 1.5249733924865723 1.327487850189209\n",
            "i, sloss, closs, correct 950 0.3343871831893921 1.7683831453323364 0.34375\n",
            "time: 1.2206177711486816 1.3273759921141102\n",
            "i, sloss, closs, correct 951 0.24920658767223358 1.7502903938293457 0.40625\n",
            "time: 1.235884189605713 1.3272804830755507\n",
            "i, sloss, closs, correct 952 0.22269029915332794 1.6079767942428589 0.515625\n",
            "time: 1.2132439613342285 1.327161313354156\n",
            "i, sloss, closs, correct 953 0.5838988423347473 1.8264812231063843 0.40625\n",
            "time: 1.2251613140106201 1.3270548784507896\n",
            "i, sloss, closs, correct 954 0.24222791194915771 1.7501126527786255 0.3125\n",
            "time: 1.2493724822998047 1.3269740596491628\n",
            "i, sloss, closs, correct 955 0.2976973056793213 1.612560510635376 0.40625\n",
            "time: 1.2152996063232422 1.3268577334272313\n",
            "i, sloss, closs, correct 956 0.24516187608242035 1.7507457733154297 0.421875\n",
            "time: 1.2916247844696045 1.326821415152794\n",
            "i, sloss, closs, correct 957 0.3407091796398163 1.7237952947616577 0.359375\n",
            "time: 1.4114444255828857 1.3269102588327046\n",
            "i, sloss, closs, correct 958 0.3090760111808777 1.7493027448654175 0.484375\n",
            "time: 1.6791813373565674 1.327277990277542\n",
            "i, sloss, closs, correct 959 0.2532118558883667 1.7154088020324707 0.390625\n",
            "time: 1.3273003101348877 1.3272789393862088\n",
            "i, sloss, closs, correct 960 0.2902190089225769 1.7929611206054688 0.421875\n",
            "time: 1.2177443504333496 1.3271654594949331\n",
            "i, sloss, closs, correct 961 0.3022160530090332 1.8449325561523438 0.4375\n",
            "time: 1.2297394275665283 1.3270646722797546\n",
            "i, sloss, closs, correct 962 0.36149728298187256 1.737418532371521 0.546875\n",
            "time: 1.2117550373077393 1.3269454313834756\n",
            "i, sloss, closs, correct 963 0.262898325920105 1.7089747190475464 0.46875\n",
            "time: 1.2215397357940674 1.326836603805732\n",
            "i, sloss, closs, correct 964 0.22063970565795898 1.726925253868103 0.34375\n",
            "time: 1.2162694931030273 1.3267225517510133\n",
            "i, sloss, closs, correct 965 0.2467125505208969 1.7537165880203247 0.453125\n",
            "time: 1.2156963348388672 1.326608097331124\n",
            "i, sloss, closs, correct 966 0.5036029815673828 1.8045579195022583 0.34375\n",
            "time: 1.2598440647125244 1.3265395420906831\n",
            "i, sloss, closs, correct 967 0.6088575124740601 1.671861171722412 0.421875\n",
            "time: 1.5669889450073242 1.3267884146083484\n",
            "i, sloss, closs, correct 968 0.4470384120941162 1.740437626838684 0.4375\n",
            "time: 1.558941125869751 1.327028376891271\n",
            "i, sloss, closs, correct 969 0.22789010405540466 1.5630457401275635 0.265625\n",
            "time: 1.2218515872955322 1.326920427243734\n",
            "i, sloss, closs, correct 970 0.29500555992126465 1.74161958694458 0.46875\n",
            "time: 1.2227647304534912 1.3268136204700882\n",
            "i, sloss, closs, correct 971 0.25163134932518005 1.661787748336792 0.421875\n",
            "time: 1.2360312938690186 1.326720744248771\n",
            "i, sloss, closs, correct 972 0.2939407527446747 1.8126310110092163 0.375\n",
            "time: 1.2122118473052979 1.3266035640595015\n",
            "i, sloss, closs, correct 973 0.5887694954872131 1.590146541595459 0.34375\n",
            "time: 1.2185800075531006 1.3264931540714398\n",
            "i, sloss, closs, correct 974 0.347170889377594 1.9258445501327515 0.390625\n",
            "time: 1.2291936874389648 1.3263938568799925\n",
            "i, sloss, closs, correct 975 0.3160436153411865 1.67341148853302 0.375\n",
            "time: 1.218625545501709 1.3262839200066738\n",
            "i, sloss, closs, correct 976 0.30763691663742065 1.8528783321380615 0.453125\n",
            "time: 1.33748459815979 1.3262965537630549\n",
            "i, sloss, closs, correct 977 0.4215242862701416 1.8141670227050781 0.375\n",
            "time: 1.632840871810913 1.326610393563175\n",
            "i, sloss, closs, correct 978 0.34385916590690613 1.8700134754180908 0.375\n",
            "time: 1.392472267150879 1.3266781411448587\n",
            "i, sloss, closs, correct 979 0.23211663961410522 1.7545197010040283 0.390625\n",
            "time: 1.2206823825836182 1.3265704561252982\n",
            "i, sloss, closs, correct 980 0.5278635621070862 1.6727241277694702 0.390625\n",
            "time: 1.2713441848754883 1.3265146387704894\n",
            "i, sloss, closs, correct 981 0.2284671515226364 1.747976541519165 0.3125\n",
            "time: 1.2476451396942139 1.3264348045628813\n",
            "i, sloss, closs, correct 982 0.2036946415901184 1.622499704360962 0.515625\n",
            "time: 1.2174558639526367 1.326324418020394\n",
            "i, sloss, closs, correct 983 0.4264798164367676 1.7638678550720215 0.5625\n",
            "time: 1.212573766708374 1.3262092873332947\n",
            "i, sloss, closs, correct 984 0.2403283417224884 1.7460036277770996 0.375\n",
            "time: 1.2206077575683594 1.326102801627919\n",
            "i, sloss, closs, correct 985 0.23639324307441711 1.773538589477539 0.453125\n",
            "time: 1.2167465686798096 1.325992598011334\n",
            "i, sloss, closs, correct 986 0.17183926701545715 1.8335328102111816 0.484375\n",
            "time: 1.5199174880981445 1.3261895824710648\n",
            "i, sloss, closs, correct 987 0.1895107477903366 1.8921040296554565 0.390625\n",
            "time: 1.610360860824585 1.3264775795009938\n",
            "i, sloss, closs, correct 988 0.20229505002498627 1.8285527229309082 0.46875\n",
            "time: 1.2251276969909668 1.3263755984928296\n",
            "i, sloss, closs, correct 989 0.20180435478687286 1.7912846803665161 0.390625\n",
            "time: 1.2589313983917236 1.3263079744396788\n",
            "i, sloss, closs, correct 990 0.23284032940864563 1.855546236038208 0.375\n",
            "time: 1.2177057266235352 1.3261988636944537\n",
            "i, sloss, closs, correct 991 0.6124399900436401 1.7754220962524414 0.484375\n",
            "time: 1.234713077545166 1.326107458001183\n",
            "i, sloss, closs, correct 992 0.5463807582855225 1.8437374830245972 0.4375\n",
            "time: 1.2136304378509521 1.3259946954214321\n",
            "i, sloss, closs, correct 993 0.4681893289089203 1.735231637954712 0.40625\n",
            "time: 1.2071614265441895 1.325875635837885\n",
            "i, sloss, closs, correct 994 0.555489718914032 1.7969412803649902 0.390625\n",
            "time: 1.2326202392578125 1.3257824011184463\n",
            "i, sloss, closs, correct 995 0.5156320333480835 1.813259243965149 0.4375\n",
            "time: 1.2238259315490723 1.3256805462530818\n",
            "i, sloss, closs, correct 996 0.35919052362442017 1.7353264093399048 0.296875\n",
            "time: 1.6591441631317139 1.3260154877168127\n",
            "i, sloss, closs, correct 997 0.48187482357025146 1.7602415084838867 0.421875\n",
            "time: 1.4763963222503662 1.326167516574592\n",
            "i, sloss, closs, correct 998 0.40894821286201477 1.8443394899368286 0.421875\n",
            "time: 1.2188539505004883 1.3260605678902015\n",
            "i, sloss, closs, correct 999 0.24573853611946106 1.696406364440918 0.53125\n",
            "time: 1.2266294956207275 1.3259616434574126\n"
          ]
        }
      ],
      "source": [
        "# @title strain ctrain test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "# scaler = torch.GradScaler()\n",
        "\n",
        "def strain(model, dataloader, optim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.train()\n",
        "    for i, (x, _) in enumerate(dataloader):\n",
        "        x = x.to(device)\n",
        "        loss = model.loss(x)\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        # grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 5) # 0.5\n",
        "        # print('grad_norm', grad_norm.item())\n",
        "        optim.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            m=0.999 # 0.99 m = next(momentum_scheduler)\n",
        "            for param_q, param_k in zip(model.student.parameters(), model.teacher.parameters()):\n",
        "                param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        # if i%10==0: print(\"strain\",loss.item())\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=50: break\n",
        "    return loss.item()\n",
        "\n",
        "def ctrain(model, classifier, dataloader, coptim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.eval()\n",
        "    classifier.train()\n",
        "    # closs = 0\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        with torch.no_grad(): sx = model(x).detach()\n",
        "        y_ = classifier(sx)\n",
        "        loss = F.cross_entropy(y_, y)\n",
        "        coptim.zero_grad()\n",
        "        loss.backward()\n",
        "        coptim.step()\n",
        "        # print(\"classify\",loss.item())\n",
        "        # closs += loss.item()\n",
        "        try: wandb.log({\"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "    # return closs/\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        with torch.no_grad():\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        # print(correct/len(y))\n",
        "        try: wandb.log({\"correct\": correct/len(y)})\n",
        "        # try: wandb.log({\"correct\": correct/len(y), \"rankme\": rankme, \"lidar\": lidar})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "    return correct/len(y)\n",
        "\n",
        "import time\n",
        "start = begin = time.time()\n",
        "# for i in range(1):\n",
        "for i in range(1000): # 1000\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    sloss = strain(ijepa, train_loader, optim)\n",
        "    closs = ctrain(ijepa, classifier, train_loader, coptim)\n",
        "    correct = test(ijepa, classifier, test_loader)\n",
        "    print('i, sloss, closs, correct', i, sloss, closs, correct)\n",
        "    print('time:',time.time() - start, (time.time()-begin)/(i+1))\n",
        "    start = time.time()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phs2ERy_RmMh"
      },
      "outputs": [],
      "source": [
        "# print(ijepa.student.transformer[0].in_proj.weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jzo9DMDPcOxu"
      },
      "outputs": [],
      "source": [
        "# @title save/load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder='/content/drive/MyDrive/jepa/'\n",
        "\n",
        "# # modelsd, optimsd = torch.load(folder+'SeqJEPA.pkl', map_location=device).values()\n",
        "# modelsd, optimsd = torch.load('SeqJEPA.pkl', map_location=device).values()\n",
        "# seq_jepa.load_state_dict(modelsd, strict=False)\n",
        "# optim.load_state_dict(optimsd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "DNNPOuUmcSNf"
      },
      "outputs": [],
      "source": [
        "checkpoint = {'model': ijepa.state_dict(), 'optimizer': optim.state_dict()}\n",
        "torch.save(checkpoint, folder+'ijepa.pkl')\n",
        "# torch.save(checkpoint, 'IJEPA.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yl-xtHthFl0M"
      },
      "source": [
        "## store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "e7HYQxn6n6iD"
      },
      "outputs": [],
      "source": [
        "# @title GoldenGateRoPE2d\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class GoldenGateRoPE2d(nn.Module): # jerryxio.ng/posts/nd-rope\n",
        "    def __init__(self, image_size, n_heads, n_freqs, min_freq=.8, max_freq=10, n_zero_freqs=0):\n",
        "        super().__init__()\n",
        "        intv = math.pi * (math.sqrt(5)-1)/2 # mod pi instead of 2pi # pi*(sqrt5+-1)/2 ; + and - are equivalent bec mod pi\n",
        "        # intv = math.pi * (math.sqrt(5)-1) # https://en.wikipedia.org/wiki/Golden_angle\n",
        "        speed = torch.cat([torch.zeros(n_zero_freqs), min_freq * (max_freq/min_freq) ** torch.linspace(0,1,n_freqs-n_zero_freqs)]).unsqueeze(-1) # [n_freqs,1] # og\n",
        "        angle = torch.arange(n_heads*n_freqs).reshape(n_heads, n_freqs) * intv # [n_heads, n_freqs]\n",
        "        direction = torch.stack([torch.cos(angle), torch.sin(angle)], dim=-1) # [n_heads, n_freqs, 2]\n",
        "        h, w = image_size\n",
        "        xlim, ylim = w/h, h/w\n",
        "        y, x = torch.meshgrid(torch.linspace(-ylim, ylim, h), torch.linspace(-xlim, xlim, w), indexing=\"ij\") # [h,w], y:row_num, x:col_num\n",
        "        pos = torch.stack([x, y], dim=-1).reshape(-1,1,1,2) # [h*w,1,1,2] cartesian coords\n",
        "        theta = (speed*direction*pos).sum(dim=-1) # [t,n_heads,n_freqs,2]->[t,n_heads,d_head]\n",
        "        self.theta = theta\n",
        "        cos, sin = torch.cos(theta), torch.sin(theta)\n",
        "        self.affine = torch.stack([cos, -sin, sin, cos], dim=-1).transpose(0,1).reshape(1,n_heads,h*w,n_freqs,2,2).to(device) # [t,n_heads,n_freqs,4]->[1,n_heads,t,n_freqs,2,2]\n",
        "\n",
        "    def forward(self, x): # [b,h,t,d]\n",
        "        return (self.affine @ x.unflatten(-1, (-1,2)).unsqueeze(-1)).flatten(-3) # @ [b,h,t,d_head//2,2,1]\n",
        "\n",
        "# /2 better\n",
        "# speed [n_freqs,1] # og best\n",
        "# w/h best\n",
        "# rope < ggrope < learned\n",
        "\n",
        "# image_size=(8,8)\n",
        "image_size=(20,30)\n",
        "# image_size=(90,120)\n",
        "n_heads=4\n",
        "n_freqs=6\n",
        "ggrope = GoldenGateRoPE2d(image_size, n_heads, n_freqs)\n",
        "\n",
        "# x = torch.rand(2, *image_size, n_heads, n_freqs*2)\n",
        "x = torch.rand(2, n_heads, image_size[0]*image_size[1], n_freqs*2)\n",
        "out = ggrope(x)\n",
        "print(out.shape)\n",
        "# # print(out[0])\n",
        "# theta = ggrope.theta.flatten(-2).permute(2,0,1).unsqueeze(1) # [t,n_heads,d_head][b,1,h,w]\n",
        "theta = ggrope.theta.flatten(-2).T.reshape(n_heads*n_freqs, 1, *image_size) # [t,n_heads,d_head]->[d,1,h,w]\n",
        "cy, cx = image_size[0]//2, image_size[1]//2\n",
        "sim = torch.cos(theta-theta[...,cy,cx][...,None,None]) # [b,1,h,w]\n",
        "# sim = sim.unflatten(0, (n_heads, n_freqs)).mean(1)\n",
        "# print(sim.shape)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    print(npimg.shape)\n",
        "    plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "# bhwc\n",
        "\n",
        "import torchvision\n",
        "imshow(torchvision.utils.make_grid(sim, nrow=n_freqs))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "eA2R-JZ7G67P"
      },
      "outputs": [],
      "source": [
        "# @title RoPE pos\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class RoPE(nn.Module):\n",
        "    def __init__(self, dim, seq_len=512, top=torch.pi, base=10000):\n",
        "    # def __init__(self, dim, seq_len=512, min_freq=1, max_freq=400, n_zero_freqs=0):\n",
        "        super().__init__()\n",
        "        self.dim, self.top, self.base = dim, top, base\n",
        "        speed = top / (base ** (torch.arange(0, dim, step=2) / dim)) # [dim//2]\n",
        "        pos = torch.arange(seq_len).unsqueeze(-1) # [t,1]\n",
        "        # speed = torch.cat([torch.zeros(n_zero_freqs), min_freq * (max_freq/min_freq) ** torch.linspace(0,1,dim//2-n_zero_freqs)]) # [dim//2]\n",
        "        # pos = torch.linspace(0, 1, seq_len).unsqueeze(-1) # [t,1]\n",
        "        theta = (speed*pos) # [t,1]*[dim//2]=[t,d//2]\n",
        "        self.theta = theta\n",
        "        cos, sin = torch.cos(theta), torch.sin(theta)\n",
        "        self.affine = torch.stack([cos, -sin, sin, cos], dim=-1).unflatten(-1,(2,2)).to(device)#[None,None,...] # [t,d//2,4]->[t,d//2,2,2] # [1,1,t,d//2,2,2]\n",
        "\n",
        "\n",
        "        # angles = theta[None,...,None] # [seq_len, 1] * [dim//2] -> [1, 1, seq_len, dim//2, 1]\n",
        "        # self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [1, 1, seq_len, dim//2, 2] -> [1, 1, seq_len, dim]\n",
        "        # # self.rot_emb = torch.cat([sin, cos], dim=-1).flatten(-2).to(device) # [1, 1, seq_len, dim//2, 2] -> [1,1,seq_len,dim]\n",
        "        # print('self.rot_emb', self.rot_emb.shape)\n",
        "\n",
        "    def forward(self, x, ind=None): # [b,h,t,d], [b,t]\n",
        "        # seq_len = x.size(-2)\n",
        "        b,_,seq_len,_ = x.shape\n",
        "        if ind!=None: seq_len = max(seq_len, ind.max()+1)\n",
        "        if self.affine.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.top, self.base)\n",
        "        if ind!=None:\n",
        "            # print(\"if affine, x\",self.affine.shape, x.shape)\n",
        "            return (self.affine.unsqueeze(0).expand(b,-1,-1,-1,-1)[torch.arange(b, device=device).unsqueeze(-1), ind].unsqueeze(1) @ x.unflatten(-1, (-1,2)).unsqueeze(-1)).flatten(-3) # @ [b,h,t,d_head//2,2,1]\n",
        "        else:\n",
        "            # print(\"else affine, x\",self.affine.shape, x.shape) # [64, 4, 2, 2], [64, 8, 10, 8]\n",
        "            return (self.affine[None,None,...] @ x.unflatten(-1, (-1,2)).unsqueeze(-1)).flatten(-3) # @ [b,h,t,d_head//2,2,1]\n",
        "\n",
        "\n",
        "        # # if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.base)\n",
        "        # if ind==None:\n",
        "        #     # print(\"if rot, x\",self.rot_emb.shape, x.shape)\n",
        "        #     return x * self.rot_emb.unsqueeze(1)#[None,None,...]\n",
        "        # else:\n",
        "        #     return x * (self.rot_emb.expand(b,-1,-1)[torch.arange(b, device=device).unsqueeze(-1), ind].unsqueeze(1))\n",
        "\n",
        "\n",
        "\n",
        "dim=64\n",
        "seq_len=56\n",
        "rope = RoPE(dim, seq_len, top=torch.pi, base=100)\n",
        "# rope = RoPE(dim, seq_len, min_freq=1, max_freq=200, n_zero_freqs=0)\n",
        "\n",
        "batch=2\n",
        "t=50\n",
        "x = torch.rand(batch, 4, seq_len, dim, device=device)\n",
        "out = rope(x)\n",
        "print(\"out1\", out.shape)\n",
        "x = torch.rand(batch, 4, t, dim, device=device)\n",
        "pos = torch.randint(0,seq_len,(batch,t), device=device)\n",
        "out = rope(x, pos)\n",
        "print(\"out2\", out.shape)\n",
        "\n",
        "# theta = rope.theta # [t,d//2]\n",
        "# sim = torch.cos(theta-theta[0].unsqueeze(0)).T\n",
        "# # print(sim.shape)\n",
        "\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# def imshow(img):\n",
        "#     npimg = img.numpy()\n",
        "#     print(npimg.shape)\n",
        "#     plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "#     plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "#     plt.show()\n",
        "\n",
        "# import torchvision\n",
        "# imshow(torchvision.utils.make_grid(sim, nrow=dim//2))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "k3nfeLM4wtkJ"
      },
      "outputs": [],
      "source": [
        "# @title random_masking\n",
        "import torch\n",
        "\n",
        "def random_masking(length, mask_ratio, b=64):\n",
        "    noise = torch.rand(b, length)\n",
        "    len_mask = int(length * mask_ratio)\n",
        "    _, msk_ind = torch.topk(noise, k=len_mask, dim=-1, sorted=False) # val, ind -> [b,len_mask]\n",
        "    _, keep_ind = torch.topk(noise, k=length-len_mask, largest=False, dim=-1, sorted=False) # val, ind -> [b,len_keep]\n",
        "    return msk_ind, keep_ind\n",
        "\n",
        "# msk_ind, keep_ind = random_masking(10, .3, b=2)\n",
        "\n",
        "# x_ = torch.rand(4, 3, 2)\n",
        "# print(x_)\n",
        "# # ids = torch.tensor([0, 2, 1])[None,:,None]\n",
        "# # ids = torch.tensor([0, 2, 1])[None,:,None].repeat(4,1,2)\n",
        "# ids = torch.tensor([1, 2, 0])[None,:,None].repeat(4,1,2)\n",
        "# # o = torch.gather(x_, dim=1, index=ids)\n",
        "# o = torch.zeros_like(x_).scatter_(dim=1, index=ids, src=x_)\n",
        "# print(o)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "62UPGVucmGNe"
      },
      "outputs": [],
      "source": [
        "# @title AttentionBlock pos\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "# import inspect\n",
        "# class Seq(nn.Sequential):\n",
        "#     def __init__(self, *args):\n",
        "#         super().__init__(*args)\n",
        "#         for layer in self:\n",
        "#             params = inspect.signature(layer.forward).parameters.keys()\n",
        "#             layer._fwdparams = ','.join(params)\n",
        "\n",
        "#     def forward(self, x, pos=None, masks=None):\n",
        "#         arg_map = {'pos':pos, 'masks':masks}\n",
        "#         for layer in self:\n",
        "#             args = [x]\n",
        "#             # if 'masks' in layer._fwdparams: args.append(masks)\n",
        "#             args.extend(arg_map[p] for p in arg_map if p in layer._fwdparams)\n",
        "#             # print(layer._fwdparams, args)\n",
        "#             x = layer(*args)\n",
        "#         return x\n",
        "\n",
        "# import inspect\n",
        "# class Seq(nn.Sequential):\n",
        "#     def __init__(self, *args):\n",
        "#         super().__init__(*args)\n",
        "#         # for layer in self:\n",
        "#         #     params = inspect.signature(layer.forward).parameters.keys()\n",
        "#         #     layer._fwdparams = ','.join(params)\n",
        "\n",
        "#     # def forward(self, x, pos=None, masks=None):\n",
        "#     def forward(self, x, *args, **kwargs):\n",
        "#         # arg_map = {'pos':pos, 'masks':masks}\n",
        "#         for layer in self:\n",
        "#             # args = [x]\n",
        "#             # if 'masks' in layer._fwdparams: args.append(masks)\n",
        "#             # args.extend(arg_map[p] for p in arg_map if p in layer._fwdparams)\n",
        "#             # print(layer._fwdparams, args)\n",
        "#             x = layer(x, *args)\n",
        "#         return x\n",
        "\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        self._kwargs = [[name for name, p in inspect.signature(layer.forward).parameters.items()] for layer in self]\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        for layer, _kwargs in zip(self, self._kwargs):\n",
        "            x = layer(x, *args, **{k: v for k, v in kwargs.items() if k in _kwargs})\n",
        "        return x\n",
        "\n",
        "class SelfAttn(nn.Module):\n",
        "    def __init__(self, dim, n_heads):\n",
        "        super().__init__()\n",
        "        self.dim, self.n_heads = dim, n_heads\n",
        "        d_head = dim//n_heads\n",
        "        self.rope = RoPE(d_head, seq_len=64, top=torch.pi, base=1000)\n",
        "        # self.rope = RoPE(d_head, seq_len=512, base=1000)\n",
        "        # self.rope = RoPE2D(d_head, h=64, w=64, base=100)\n",
        "        # self.rope[0] = 1 # id for smry h\n",
        "        self.qkv = nn.Linear(dim, dim*3, bias=False)\n",
        "        # self.lin = nn.Linear(dim, dim)\n",
        "        self.lin = zero_module(nn.Linear(dim, dim))\n",
        "        self.scale = d_head**-.5\n",
        "\n",
        "    # def forward(self, x): # [b,t,d]\n",
        "    def forward(self, x, pos=None): # [b,t,d]\n",
        "        # print('SelfAttn', x.shape)\n",
        "        q,k,v = self.qkv(x).unflatten(-1, (self.n_heads,-1)).transpose(1,2).chunk(3, dim=-1) # [b, t, n_heads, d_head] -> [b, n_heads, t, d_head]\n",
        "        if pos==None: q, k = self.rope(q), self.rope(k)\n",
        "        else:\n",
        "            # print('SelfAttn fwd', x.shape, pos.shape)\n",
        "            q, k = self.rope(q, pos), self.rope(k, pos)\n",
        "\n",
        "        x = F.scaled_dot_product_attention(q,k,v, attn_mask=None) # https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html\n",
        "        # q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2)\n",
        "        # context = k.transpose(-2,-1) @ v # [batch, n_heads, d_head, d_head]\n",
        "        # x = q @ context # [b,n_heads,t,d_head]\n",
        "        x = x.transpose(1,2).flatten(2)\n",
        "        return self.lin(x)\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, mult=4, drop=0.):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        # self.norm = nn.RMSNorm(d_model, elementwise_affine=False) # LayerNorm RMSNorm\n",
        "        self.norm1, self.norm2 = nn.LayerNorm(d_model), nn.LayerNorm(d_model)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "        self.attn = SelfAttn(d_model, n_heads)\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        ff_dim=d_model*mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), act, nn.Dropout(drop), nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "            nn.RMSNorm(ff_dim), act, nn.Dropout(drop), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(drop), nn.Linear(d_model, ff_dim), act,\n",
        "            # nn.RMSNorm(ff_dim), nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "\n",
        "    # def forward(self, x): # [b,t,d]\n",
        "    def forward(self, x, pos=None): # [b,t,d]\n",
        "        # print('attnblk fwd',x.shape)\n",
        "        # x = x + self.drop(self.attn(self.norm1(x)))\n",
        "        x = x + self.drop(self.attn(self.norm1(x), pos))\n",
        "        x = x + self.ff(x)\n",
        "        # x = x + self.drop(self.ff(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, patch_size, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        patch_size=2\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Conv2d(in_dim, d_model, patch_size, patch_size), # like patch\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 1, 7//2, bias=False)\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.MaxPool2d(3,2,1)\n",
        "            nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.BatchNorm2d(d_model), nn.ReLU(),\n",
        "            nn.Conv2d(d_model, d_model, 3, 2, 3//2, bias=False)\n",
        "            )\n",
        "        # self.embed.requires_grad=False\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, 32*32, d_model)*.02)\n",
        "        # self.pos_emb = nn.Parameter(RoPE2D(dim=d_model, h=8, w=8, base=1000), requires_grad=False)\n",
        "\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=(32//patch_size)**2, base=10000), requires_grad=False)\n",
        "        # self.transformer = nn.Sequential(*[AttentionBlock(d_model, d_model, n_heads) for _ in range(nlayers)])\n",
        "        self.transformer = Seq(*[AttentionBlock(d_model, n_heads) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, cxt_inds=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x).flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c] # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        # x = self.pos_enc(x)\n",
        "        # print(\"TransformerModel\",x.shape, self.pos_emb.shape)\n",
        "        # print(\"TransformerModel\",x.shape)\n",
        "        # x = x + self.pos_emb[:,:x.shape[1]]\n",
        "        # if cxt_inds != None: x = x[torch.arange(x.shape[0]).unsqueeze(-1), cxt_inds] # [batch, num_context_toks, d_model]\n",
        "        # x = self.transformer(x)\n",
        "\n",
        "        if cxt_inds != None:\n",
        "            # print('vit transformer',x[torch.arange(x.shape[0]).unsqueeze(-1), cxt_inds].shape, cxt_inds.shape)\n",
        "            x = self.transformer(x[torch.arange(x.shape[0]).unsqueeze(-1), cxt_inds], cxt_inds)\n",
        "        else: x = self.transformer(x)\n",
        "\n",
        "        out = self.norm(x)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out\n",
        "\n",
        "d_model = 64\n",
        "in_dim = 3\n",
        "patch_size = 2\n",
        "# model = ViT(patch_size, in_dim, d_model, n_heads=4, nlayers=4, drop=0.).to(device)\n",
        "model = ViT(patch_size, in_dim, d_model, n_heads=4, nlayers=1, drop=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x = torch.rand((5, in_dim, 32, 32), device=device) # [b,c,h,w]\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "OgZ6d59vOQil"
      },
      "outputs": [],
      "source": [
        "# @title test seq\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        # for layer in self:\n",
        "        #     params = inspect.signature(layer.forward).parameters.keys()\n",
        "        #     layer._fwdparams = ','.join(params)\n",
        "\n",
        "    # def forward(self, x, pos=None, masks=None):\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        # arg_map = {'pos':pos, 'masks':masks}\n",
        "        for layer in self:\n",
        "            # args = [x]\n",
        "            # if 'masks' in layer._fwdparams: args.append(masks)\n",
        "            # args.extend(arg_map[p] for p in arg_map if p in layer._fwdparams)\n",
        "            # print(layer._fwdparams, args)\n",
        "            x = layer(x, *args, **kwargs)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        self._accepted_kwargs = []\n",
        "        for layer in self:\n",
        "            sig = inspect.signature(layer.forward)\n",
        "            print(sig.parameters.items())\n",
        "            self._accepted_kwargs.append(\n",
        "                [name for name, p in sig.parameters.items()]\n",
        "                # {name for name, p in sig.parameters.items()\n",
        "                #  if p.kind in (p.POSITIONAL_OR_KEYWORD, p.KEYWORD_ONLY)}\n",
        "            )\n",
        "\n",
        "            # if any(p.kind == p.VAR_KEYWORD for p in sig.parameters.values()):\n",
        "            #     accepted = None  # means \"pass everything\"\n",
        "\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        # for layer in self:\n",
        "        #     x = layer(x, *args, **kwargs)\n",
        "\n",
        "        # for layer in self:\n",
        "        #     sig = inspect.signature(layer.forward)\n",
        "        #     # keep only accepted keyword arguments\n",
        "        #     filtered_kwargs = {\n",
        "        #         k: v for k, v in kwargs.items()\n",
        "        #         if k in sig.parameters\n",
        "        #     }\n",
        "        #     x = layer(x, *args, **filtered_kwargs)\n",
        "\n",
        "        for layer, accepted in zip(self, self._accepted_kwargs):\n",
        "            filtered_kwargs = {k: v for k, v in kwargs.items() if k in accepted}\n",
        "            x = layer(x, *args, **filtered_kwargs)\n",
        "        # if accepted is None:\n",
        "        #     x = layer(x, *args, **kwargs)\n",
        "        # else:\n",
        "        #     filtered = {k: v for k, v in kwargs.items() if k in accepted}\n",
        "        #     x = layer(x, *args, **filtered)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        self._kwargs = [[name for name, p in inspect.signature(layer.forward).parameters.items()] for layer in self]\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        for layer, _kwargs in zip(self, self._kwargs):\n",
        "            x = layer(x, *args, **{k: v for k, v in kwargs.items() if k in _kwargs})\n",
        "        return x\n",
        "\n",
        "\n",
        "class SS(nn.Module):\n",
        "    def __init__(self, d):\n",
        "        super().__init__()\n",
        "        self.d = d\n",
        "    def forward(self, x, y, pos=None, bos=None):\n",
        "        # print('SS', x, pos, bos)\n",
        "        print('SS', x, y, pos, bos)\n",
        "        return x+1\n",
        "\n",
        "class PP(nn.Module):\n",
        "    def __init__(self, d):\n",
        "        super().__init__()\n",
        "        self.d = d\n",
        "    def forward(self, x, pos=None, dos=None):\n",
        "        print('PP', x, pos, dos)\n",
        "    # def forward(self, x, y, pos=None, dos=None):\n",
        "        # print('PP', x, y, pos, dos)\n",
        "        return x+1\n",
        "\n",
        "d=2\n",
        "# TT = Seq(*[SS(d) for _ in range(2)])\n",
        "TT = Seq(*[SS(d), PP(d)])\n",
        "# out = TT(3, a=2, pos='p')\n",
        "# out = TT(3, a=2, bos='b')\n",
        "# out = TT(3, a=2, dos='d')\n",
        "out = TT(3, 'q','w')\n",
        "# out = TT(3, 'q','w', pos=2, dos='d')\n",
        "# out = TT(3, 'q', pos=2, dos='d')\n",
        "print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4J2ahp3wmqcg"
      },
      "outputs": [],
      "source": [
        "# @title supervised train test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "# def strain(model, dataloader, optim, scheduler=None):\n",
        "def strain(model, classifier, dataloader, optim, coptim, scheduler=None):\n",
        "    model.train()\n",
        "    classifier.train()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x = x.to(device)#.to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "        y = y.to(device)\n",
        "\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        # print(loss)\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # for p in list(filter(lambda p: p.grad is not None, model.parameters())):\n",
        "        #     print(p.grad.data.norm(2).item())\n",
        "        # print(\"max grad norm\", max([p.grad.data.norm(2).item() for p in list(filter(lambda p: p.grad is not None, model.parameters()))]))\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 10) # 0.5\n",
        "\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        print(\"strain\",loss.item())\n",
        "        # for param in ijepa.student.cls: print(param.data)\n",
        "        # for param in ijepa.predicter.cls: print(param.data)\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=50: break\n",
        "\n",
        "\n",
        "# def test(model, dataloader):\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        # .to(torch.bfloat16)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "        loss = F.cross_entropy(y_, y)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        print(correct/len(y))\n",
        "        try: wandb.log({\"correct\": correct/len(y), \"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "for i in range(1000):\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # np.random.shuffle(train_indices); np.random.shuffle(val_indices)\n",
        "    # train_sampler, valid_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(val_indices)\n",
        "    # # batch_size = 64 #512\n",
        "    # train_loader = DataLoader(train_data, sampler=train_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "    # test_loader = DataLoader(train_data, sampler=valid_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True)\n",
        "\n",
        "    # strain(ijepa, train_loader, optim)\n",
        "    strain(ijepa, classifier, train_loader, optim, coptim)\n",
        "    test(ijepa, classifier, test_loader)\n",
        "\n",
        "    # strain(violet, train_loader, voptim)\n",
        "    # test(violet, test_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e1Yi9JrLPLd"
      },
      "source": [
        "## trash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KUvHrTi4LSEe"
      },
      "outputs": [],
      "source": [
        "# @title GoldenGateRoPE2d\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class GoldenGateRoPE2d(nn.Module): # jerryxio.ng/posts/nd-rope\n",
        "    def __init__(self, image_size: tuple[int, int], n_heads, n_freqs):\n",
        "        super().__init__()\n",
        "        n_zero_freqs = 1\n",
        "        min_freq, max_freq = .2, 20\n",
        "        speed = torch.cat([torch.zeros(n_zero_freqs), min_freq * (max_freq/min_freq) ** torch.linspace(0,1,n_freqs-n_zero_freqs)]) # [n_freqs]\n",
        "        direction_spacing = math.pi * (math.sqrt(5)-1)/2\n",
        "        phi = torch.arange(n_heads * n_freqs).reshape(n_heads, n_freqs) * direction_spacing # [n_heads, n_freqs]\n",
        "        directions = torch.stack((torch.cos(phi), torch.sin(phi)), dim=-1) # [n_heads, n_freqs, 2]\n",
        "        # print('directions', directions)\n",
        "        vel = speed.unsqueeze(-1) * directions # speed in direction[n_heads, n_freqs, 2]\n",
        "\n",
        "        H, W = image_size\n",
        "        xlim, ylim = math.sqrt(W / H), math.sqrt(H / W)\n",
        "        print(xlim, ylim)\n",
        "        y, x = torch.meshgrid(torch.linspace(-ylim, ylim, H), torch.linspace(-xlim, xlim, W), indexing=\"ij\") # [h,w], y:row_num, x:col_num\n",
        "        # y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\") # print(y,x) # [h,w], y:row_num, x:col_num\n",
        "\n",
        "        pos = torch.stack((x, y), dim=-1).reshape(H, W, 1, 1, 2) # cartesian coords\n",
        "\n",
        "        theta = (vel*pos).sum(dim=-1) # [h,w,n_heads,n_freqs,2]->[h,w,n_heads,d_head]\n",
        "        print('theta', theta.shape)\n",
        "        self.cos, self.sin = torch.cos(theta), torch.sin(theta)\n",
        "        print('self.cos', self.cos.shape)\n",
        "\n",
        "    def forward(self, input): # [b,h,w,n_head,d_head]\n",
        "        # x, y = input.float().chunk(2, dim=-1) # [b,h,w,n_head,n_freqs]\n",
        "        x, y = input.float().unflatten(-1, (-1,2)).chunk(2, dim=-1)\n",
        "        x, y = x.squeeze(-1), y.squeeze(-1)\n",
        "        x_out = x * self.cos - y * self.sin\n",
        "        y_out = x * self.sin + y * self.cos\n",
        "        # output = torch.cat((x_out, y_out), dim=-1)\n",
        "        output = torch.stack((x_out, y_out), dim=-1).flatten(-2)\n",
        "        return output.type_as(input)\n",
        "\n",
        "\n",
        "\n",
        "class GoldenGateRoPE2d2(nn.Module): # jerryxio.ng/posts/nd-rope\n",
        "    def __init__(self, image_size, n_heads, n_freqs):\n",
        "        super().__init__()\n",
        "        n_zero_freqs = 1\n",
        "        min_freq, max_freq = .2, 20\n",
        "        intv = math.pi * (math.sqrt(5)-1)/2\n",
        "        speed = torch.cat([torch.zeros(n_zero_freqs), min_freq * (max_freq/min_freq) ** torch.linspace(0,1,n_freqs-n_zero_freqs)]) # [n_freqs]\n",
        "        phi = torch.arange(n_heads*n_freqs).reshape(n_heads, n_freqs) * intv # [n_heads, n_freqs]\n",
        "        direction = torch.stack((torch.cos(phi), torch.sin(phi)), dim=-1) # [n_heads, n_freqs, 2]\n",
        "        vel = speed.unsqueeze(-1) * direction # speed in direction[n_heads, n_freqs, 2]\n",
        "        h, w = image_size\n",
        "        xlim, ylim = math.sqrt(w/h), math.sqrt(h/w)\n",
        "        y, x = torch.meshgrid(torch.linspace(-ylim, ylim, h), torch.linspace(-xlim, xlim, w), indexing=\"ij\") # [h,w], y:row_num, x:col_num\n",
        "        pos = torch.stack([x, y], dim=-1)[...,None,None,:] # [h,w,1,1,2] cartesian coords\n",
        "        theta = (vel*pos).sum(dim=-1) # [h,w,n_heads,n_freqs,2]->[h,w,n_heads,d_head]\n",
        "        cos, sin = torch.cos(theta), torch.sin(theta)\n",
        "        self.affine = torch.stack([cos, -sin, sin, cos], dim=-1).unflatten(-1, (2,2))\n",
        "\n",
        "    def forward(self, input): # [b,h,w,n_head,d_head]\n",
        "        return (self.affine @ input.unflatten(-1, (-1,2)).unsqueeze(-1)).flatten(-3)\n",
        "\n",
        "\n",
        "\n",
        "image_size=(5,7)\n",
        "n_heads=4\n",
        "d_head=16\n",
        "ggrope = GoldenGateRoPE2d(image_size, n_heads, d_head//2)\n",
        "ggrope2 = GoldenGateRoPE2d2(image_size, n_heads, d_head//2)\n",
        "\n",
        "# x = torch.rand(2, *image_size, n_heads, d_head)\n",
        "out = ggrope(x)\n",
        "out2 = ggrope2(x)\n",
        "print(out.shape)\n",
        "print(out2.shape)\n",
        "# print(out[0])\n",
        "# print(out2[0])\n",
        "# print((out==out2)[0])\n",
        "print((out==out2).all())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "K6saOuAX-FV8"
      },
      "outputs": [],
      "source": [
        "# @title test gather\n",
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "l=56\n",
        "b,t,d = 2,50,64\n",
        "src = torch.rand(b,l,d, device=device)\n",
        "idx = torch.randint(0,l,(b,t), device=device)\n",
        "out = src[torch.arange(b).unsqueeze(-1), idx]\n",
        "# out = torch.take_along_dim(src, idx.unsqueeze(-1).expand(-1,-1,d), dim=1)\n",
        "# out = src.index_select(0, idx).reshape(b, t, d) # == src[idx]\n",
        "# out = src.gather(1, idx.unsqueeze(-1).expand(-1, -1, d))\n",
        "print(out.shape)\n",
        "\n",
        "# %timeit out = src.expand(b,-1,-1).gather(1, idx.unsqueeze(-1).expand(-1, -1, src.size(-1))) # 2.53 ms\n",
        "# %timeit out = src.expand(b,-1,-1)[torch.arange(b, device=device).unsqueeze(-1), idx] # 1.39 ms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNt7SRjj9g_l"
      },
      "outputs": [],
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "1e1Yi9JrLPLd"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}