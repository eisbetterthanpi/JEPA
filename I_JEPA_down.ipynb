{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/I_JEPA_down.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LxACli7GdyGq"
      },
      "outputs": [],
      "source": [
        "# @title data\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "transform = transforms.Compose([transforms.ToTensor(),])\n",
        "# transform = transforms.Compose([transforms.RandomResizedCrop((32,32), scale=(.3,1.)), transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "# CIFAR10: 60000 32x32 color images in 10 classes, with 6000 images per class\n",
        "train_data = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
        "test_data = torchvision.datasets.CIFAR10(root='data', train=False, download=True, transform=transform)\n",
        "batch_size = 64 # 4\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# dataiter = iter(train_loader) # get some random training images\n",
        "# images, labels = next(dataiter)\n",
        "# print(images.shape) # [batch, 3, 32, 32]\n",
        "# imshow(torchvision.utils.make_grid(images))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3XSZZyUPY1i"
      },
      "source": [
        "## vit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "GD7ezZzmhTHU"
      },
      "outputs": [],
      "source": [
        "# @title RoPE\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def RoPE(dim, seq_len=512, top=torch.pi, base=1000):\n",
        "    theta = top / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "    pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "    angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim//2] -> [1, seq_len, dim//2, 1]\n",
        "    rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [1, seq_len, dim//2, 2] -> [1, seq_len, dim]\n",
        "    return rot_emb\n",
        "\n",
        "\n",
        "# class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "#     def __init__(self, dim, seq_len=512, top=torch.pi, base=1000):\n",
        "#         super().__init__()\n",
        "#         self.dim, self.base = dim, base\n",
        "#         theta = top / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "#         pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "#         angles = (pos * theta)[None,None,...,None] # [seq_len, 1] * [dim//2] -> [1, 1, seq_len, dim//2, 1]\n",
        "#         self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [1, 1, seq_len, dim//2, 2] -> [1, 1, seq_len, dim]\n",
        "\n",
        "#     def forward(self, x): # [b,h,t,d]\n",
        "#         seq_len = x.size(-2)\n",
        "#         if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.base)\n",
        "#         return x * self.rot_emb#[:,:,:seq_len]\n",
        "\n",
        "\n",
        "# @title proper rope\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class RoPE(nn.Module):\n",
        "    def __init__(self, dim, seq_len=512, top=torch.pi, base=1000):\n",
        "    # def __init__(self, dim, seq_len=512, min_freq=1, max_freq=400, n_zero_freqs=0):\n",
        "        super().__init__()\n",
        "        speed = top / (base ** (torch.arange(0, dim, step=2) / dim)) # [dim//2]\n",
        "        pos = torch.arange(seq_len).unsqueeze(-1) # [t,1]\n",
        "        # speed = torch.cat([torch.zeros(n_zero_freqs), min_freq * (max_freq/min_freq) ** torch.linspace(0,1,dim//2-n_zero_freqs)]) # [dim//2]\n",
        "        # pos = torch.linspace(0, 1, seq_len).unsqueeze(-1) # [t,1]\n",
        "\n",
        "        theta = (speed*pos) # [t,1]*[dim//2]=[t,d//2]\n",
        "        self.theta = theta\n",
        "        cos, sin = torch.cos(theta), torch.sin(theta)\n",
        "        self.affine = torch.stack([cos, -sin, sin, cos], dim=-1).unflatten(-1,(2,2)).to(device) # [t,d//2,4]-> [1,1,t,d//2,2,2]\n",
        "\n",
        "    def forward(self, x): # [b,h,t,d]\n",
        "        return (self.affine @ x.unflatten(-1, (-1,2)).unsqueeze(-1)).flatten(-3) # [1,1,t,d//2,2,2] @ [b,h,t,d//2,2,1] = [b,h,t,d]\n",
        "\n",
        "# dim=64\n",
        "# n_heads=4\n",
        "# seq_len=64\n",
        "# rope = RoPE(dim, seq_len, top=torch.pi, base=100)\n",
        "# # rope = RoPE(dim, seq_len, min_freq=1, max_freq=200, n_zero_freqs=0)\n",
        "\n",
        "# x = torch.rand(2, n_heads, seq_len, dim, device=device)\n",
        "# out = rope(x)\n",
        "# print(out.shape)\n",
        "\n",
        "# theta = rope.theta # [t,d//2]\n",
        "# sim = torch.cos(theta-theta[0].unsqueeze(0)).T\n",
        "# # print(sim.shape)\n",
        "\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# def imshow(img):\n",
        "#     npimg = img.numpy()\n",
        "#     print(npimg.shape)\n",
        "#     plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "#     plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "#     plt.show()\n",
        "\n",
        "# import torchvision\n",
        "# imshow(torchvision.utils.make_grid(sim, nrow=dim//2))\n",
        "\n",
        "\n",
        "\n",
        "class RotEmb(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, top=torch.pi, base=1000):\n",
        "        super().__init__()\n",
        "        self.theta = top / (base ** (torch.arange(0, dim, step=2, device=device) / dim))\n",
        "        # self.theta = top / (base ** torch.linspace(0, 1, dim//2, device=device))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (pos.unsqueeze(-1) * self.theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [seq_len, dim]\n",
        "\n",
        "# emb = RotEmb(dim, top=torch.pi, base=1000)\n",
        "\n",
        "# theta = emb.theta\n",
        "# print(theta)\n",
        "# pos = torch.arange(0,200,1)\n",
        "# angles = (pos.unsqueeze(-1) * theta).T # [b,t]\n",
        "# sim = torch.cos(angles-angles[:,0].unsqueeze(-1))\n",
        "# print(sim.shape)\n",
        "\n",
        "# imshow(torchvision.utils.make_grid(sim, nrow=dim//2))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "f6T4F651kmGh"
      },
      "outputs": [],
      "source": [
        "# @title AttentionBlock\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class SelfAttn(nn.Module):\n",
        "    def __init__(self, dim, n_heads):\n",
        "        super().__init__()\n",
        "        self.dim, self.n_heads = dim, n_heads\n",
        "        d_head = dim//n_heads\n",
        "        self.rope = RoPE(d_head, seq_len=64, top=torch.pi, base=1000)\n",
        "        # self.rope = RoPE(d_head, seq_len=512, base=1000)\n",
        "        # self.rope = RoPE2D(d_head, h=64, w=64, base=100)\n",
        "        # self.rope[0] = 1 # id for smry h\n",
        "        self.qkv = nn.Linear(dim, dim*3, bias=False)\n",
        "        # self.lin = nn.Linear(dim, dim)\n",
        "        self.lin = zero_module(nn.Linear(dim, dim))\n",
        "\n",
        "        self.scale = d_head**-.5\n",
        "\n",
        "    # def forward(self, x): # [b,t,d]\n",
        "    def forward(self, x, pos=None): # [b,t,d]\n",
        "        q,k,v = self.qkv(x).unflatten(-1, (self.n_heads,-1)).transpose(1,2).chunk(3, dim=-1) # [b, t, n_heads, d_head] -> [b, n_heads, t, d_head]\n",
        "        # q, k = self.rope(q), self.rope(k)\n",
        "        if pos != None:\n",
        "            # rope = self.rope[pos]\n",
        "            # q, k = q*rope, k*rope\n",
        "            q, k = self.rope(q, pos), self.rope(k, pos)\n",
        "\n",
        "        # x = F.scaled_dot_product_attention(q,k,v, attn_mask=None) # https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html\n",
        "        q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2)\n",
        "        context = k.transpose(-2,-1) @ v # [batch, n_heads, d_head, d_head]\n",
        "        x = q @ context # [b, n_heads, t, d_head]\n",
        "        # # print('SelfAttn', x.shape)\n",
        "        x = x.transpose(1,2).flatten(2)\n",
        "        return self.lin(x)\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, mult=4, drop=0.):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        # self.norm = nn.RMSNorm(d_model, elementwise_affine=False) # LayerNorm RMSNorm\n",
        "        self.norm1, self.norm2 = nn.LayerNorm(d_model), nn.LayerNorm(d_model)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "        self.attn = SelfAttn(d_model, n_heads)\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        ff_dim=d_model*mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), act, nn.Dropout(drop), nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "            nn.RMSNorm(ff_dim), act, nn.Dropout(drop), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(drop), nn.Linear(d_model, ff_dim), act,\n",
        "            # nn.RMSNorm(ff_dim), nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "\n",
        "    # def forward(self, x): # [b,t,d]\n",
        "    def forward(self, x, pos=None): # [b,t,d]\n",
        "        # print('attnblk fwd',x.shape)\n",
        "        # x = x + self.drop(self.attn(self.norm1(x)))\n",
        "        x = x + self.drop(self.attn(self.norm1(x), pos))\n",
        "        x = x + self.ff(x)\n",
        "        # x = x + self.drop(self.ff(self.norm2(x)))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gdtmiB_Zzn5u"
      },
      "outputs": [],
      "source": [
        "# @title ssd me\n",
        "# https://goombalab.github.io/blog/2024/mamba2-part4-systems/\n",
        "# https://tridao.me/blog/2024/mamba2-part3-algorithm/\n",
        "# # https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/triton/ssd_combined.py\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def segsum(x): # [...,c] # Naive segment sum calculation. exp(segsum(A)) produces a 1-SS matrix, which is equivalent to a scalar SSM.\n",
        "    T = x.size(-1)\n",
        "    x_cumsum = torch.cumsum(x, dim=-1)\n",
        "    x_segsum = x_cumsum.unsqueeze(-1) - x_cumsum.unsqueeze(-2) # [...,c,c] # vert-hori\n",
        "    mask = torch.triu(torch.ones(T, T, device=x.device, dtype=bool), diagonal=1)\n",
        "    return x_segsum.masked_fill(mask, -torch.inf) # [...,c,c]\n",
        "\n",
        "# def ssd(X, A, B, C, h0=None, b_ind=None, chunk=64): # X:[b,h,t,d], A:[b,h,t], B/C:[b,h,t,s], h0:[b,h,d,s], b_ind:[b]\n",
        "def ssd(X, A, B, C, h0=None, msk=None, b_ind=None, chunk=64): # X:[b,h,t,d], A:[b,h,t], B/C:[b,h,t,s], h0:[b,h,d,s], msk:[b,t], b_ind:[b]\n",
        "    # print('ssd', X.dtype, A.dtype, B.dtype)\n",
        "    # assert X.dtype == A.dtype == B.dtype == C.dtype\n",
        "    assert X.shape[:-1] == A.shape == B.shape[:-1]\n",
        "    assert h0==None or b_ind==None\n",
        "    # print('ssd', X.shape, A.shape, B.shape)\n",
        "    if b_ind!=None: A[:,:,b_ind[1:-1]] = 0 # at boundaries, A=0\n",
        "    if X.shape[2] % chunk != 0: X, A, B, C = [x.unsqueeze(2) for x in (X, A, B, C)]\n",
        "    else: X, A, B, C = [x.unflatten(2, (-1,chunk)) for x in (X, A, B, C)] # [b,h,t/c,c(,d/s)]\n",
        "\n",
        "    # 1. Compute the output for each intra-chunk (diagonal blocks)\n",
        "    L = torch.exp(segsum(A)) # [b,h,t/c,c,c]\n",
        "    # if msk!=None: L[msk] = 0 # L = L.masked_fill(msk, 0)\n",
        "    if msk!=None: # this only saves computation from ssd. full info leakage, full compute in in_proj\n",
        "        b,h,l,c,c = L.shape\n",
        "        assert msk.shape==(b,l*c)\n",
        "        msk = msk.reshape(b,1,l,c)\n",
        "        msk = msk.unsqueeeze(-2) | msk.unsqueeze(-1) # [b,1,t/c,c,c]\n",
        "        L[msk] = 0 # L = L.masked_fill(msk, 0)\n",
        "    Y_diag  = torch.einsum(\"...cs,...ks,...ck,...kd->...cd\", C, B, L, X) # bhlcs,bhlks,bhlck,bhlkd->bhlcd # full CA...ABX? for chunks\n",
        "\n",
        "    # 2. Compute the state for each intra-chunk # (right term of low-rank factorization of off-diagonal blocks; B terms)\n",
        "    A_cumsum = torch.cumsum(A, dim=-1) # [b,h,t/c,c]\n",
        "    decay_states = torch.exp((A_cumsum[...,-1:] - A_cumsum)) # [b,h,t/c,c] # Ai+1...T\n",
        "    states = torch.einsum(\"...cs,...c,...cd->...ds\", B, decay_states, X) # bhlcs,bhlc,bhlcd->bhlds # BiXiAi+1...T\n",
        "\n",
        "    # 3. Compute the inter-chunk SSM recurrence; produces correct SSM states at chunk boundaries # (middle term of factorization of off-diag blocks; A terms)\n",
        "    if h0==None: h0 = torch.zeros_like(states[:,:,0], device=states.device) # [b,h,d,s]\n",
        "    states = torch.cat([h0.unsqueeze(2), states], dim=2) # [b,1+t/c,h,d,s] # h0,hc,h2c,...,ht\n",
        "    decay_chunk = torch.exp(segsum(F.pad(A_cumsum[...,-1], (1,0)))) # [b,h,1+t/c]-> # [b,h,1+t/c,1+t/c] # 1,A1...1t/c,A1t/c+1...2t/c,...,A(c-1)t/c...At\n",
        "    new_states = torch.einsum(\"...tl,...lds->...tds\", decay_chunk, states) # bhtl,bhlds->bhtds # h0, BiXi/A1...i-1,\n",
        "\n",
        "    # 4. Compute state -> output conversion per chunk # (left term of low-rank factorization of off-diagonal blocks; C terms)\n",
        "    Y_off = torch.einsum('...cs,...ds,...c->...cd', C, new_states[:,:,:-1], torch.exp(A_cumsum)) # bhlcs,bhlds,bhlc->bhlcd # offset for each chunk # C1h0A1, Ci BiXi/A1...i-1,\n",
        "    Y = (Y_diag+Y_off).flatten(2,3)\n",
        "    return Y, new_states[:,:,-1] # [b,t,h,d], [b,h,d,s]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "E9ri_f5IGL5T"
      },
      "outputs": [],
      "source": [
        "# @title euler zoh bilinear diagonal A\n",
        "import torch\n",
        "\n",
        "# Euler: Abar = 1 + ∆A ; Bbar = ∆B\n",
        "def euler(A, B, dt): # bts, btsd, bt / bths, bthsd, bt\n",
        "    # return 1+dt.unsqueeze(-1)*A, dt[...,None,None]*B\n",
        "    return 1+torch.einsum('bt,bt...s->bt...s', dt, A), torch.einsum('bt,bt...sd->bt...sd', dt, B)\n",
        "\n",
        "# ZOH: Abar = exp(∆A) ; Bbar = (exp(∆A)-I)/A · B ~ (1+A/2)B # For numerical stability as A->0 # (e^x -1)/x ~ 1+(x/2) # from taylor expansion, e^x ~ 1 + x + x^2/2 + x^3/6 + x^4/24 + ...\n",
        "def zoh(A, B, dt): # btd, btds, bt\n",
        "    # A_bar = torch.exp(dt.unsqueeze(-1)*A)\n",
        "    A_bar = torch.exp(torch.einsum('bt,bt...s->bt...s', dt, A))\n",
        "    return A_bar, ((A_bar-1)/A).unsqueeze(-1) * B\n",
        "    # return torch.exp(dt.unsqueeze(-1)*A), (1+A/2).unsqueeze(-1) * B # For numerical stability\n",
        "\n",
        "# Bilinear: Abar = (1+∆A/2) / (1-∆A/2) ; Bbar = ∆B/(1-∆A/2)\n",
        "def bilinear(A, B, dt): # btd, btds, bt\n",
        "    # dA_2 = dt.unsqueeze(-1)*A/2\n",
        "    dA_2 = torch.einsum('bt,bt...s->bt...s', dt, A/2)\n",
        "    # return (1+dA_2)/(1-dA_2), (dt.unsqueeze(-1)/(1-dA_2)).unsqueeze(-1) * B\n",
        "    return (1+dA_2)/(1-dA_2), torch.einsum('bt,bt...s->bt...s', dt, 1-dA_2).unsqueeze(-1) * B\n",
        "\n",
        "# b,t,d,s = 2,5,8,4\n",
        "# h=3\n",
        "# A = -torch.randn(b,t,d) # Negative for stability?\n",
        "# B = torch.randn(b,t,d,s)\n",
        "# dt = torch.rand(b,t)*.1\n",
        "# # A_bar, B_bar = euler(A, B, dt)\n",
        "# # A_bar, B_bar = zoh(A, B, dt)\n",
        "# A_bar, B_bar = bilinear(A, B, dt)\n",
        "# print(A_bar.shape, B_bar.shape)\n",
        "\n",
        "# A = -torch.randn(b,t,h,d) # Negative for stability?\n",
        "# B = torch.randn(b,t,h,d,s)\n",
        "\n",
        "# # A_bar, B_bar = euler(A, B, dt)\n",
        "# # print(A_bar.shape, B_bar.shape)\n",
        "# # A_bar, B_bar = zoh(A, B, dt)\n",
        "# # print(A_bar.shape, B_bar.shape) # [2, 5, 8, 8], [2, 5, 8, 4]\n",
        "# A_bar, B_bar = bilinear(A, B, dt)\n",
        "# print(A_bar.shape, B_bar.shape) # [2, 5, 8, 8], [2, 5, 8, 4]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "6eeu0m-bUSdh"
      },
      "outputs": [],
      "source": [
        "# @title hydra me\n",
        "# https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba2_simple.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "# @torch.compile()\n",
        "class Hydra(nn.Module):\n",
        "    def __init__(self, d_model, expand=3, n_heads=8, n_groups=8, d_state=8, d_conv=7):\n",
        "        super().__init__()\n",
        "        n_groups = min(n_heads, n_groups)\n",
        "        assert n_heads % n_groups == 0, \"nheads must be divisible by ngroups\"\n",
        "        self.d_model, self.n_groups, self.d_state, self.d_conv = d_model, n_groups, d_state, d_conv\n",
        "        self.d_inner = expand * self.d_model\n",
        "        self.n_heads, self.d_head = n_heads, self.d_inner//n_heads\n",
        "\n",
        "        # self.in_proj = nn.Linear(self.d_model, 2* self.d_inner + 2* self.n_groups*self.d_state + self.n_heads, bias=False) # z,x,B,C,A\n",
        "        self.in_proj = nn.Linear(self.d_model, 2* self.d_inner + 2* self.n_groups*self.d_state + 2* self.n_heads, bias=True) # z,x,B,C,A\n",
        "        # with torch.no_grad(): self.in_proj.weight[self.d_inner:] = .1*self.in_proj.weight[self.d_inner:]\n",
        "        # conv_dim = self.d_inner + 2*self.n_groups*self.d_state # for x,B,C\n",
        "        # self.conv1d = nn.Conv1d(conv_dim, conv_dim, kernel_size=d_conv, groups=conv_dim, padding=d_conv//2, bias=True)\n",
        "        self.conv1d = nn.Conv1d(d_model, d_model, kernel_size=d_conv, groups=d_model, padding=d_conv//2, bias=True)\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "        self.A_log = nn.Parameter(torch.log(torch.rand(self.n_heads)))\n",
        "        # self.h0 = nn.Parameter(torch.zeros(self.n_heads, self.d_head, self.d_state))\n",
        "        # self.D = nn.Parameter(torch.ones(self.n_heads)) # from mamba\n",
        "        self.D = nn.Linear(self.d_inner, self.n_heads) # og\n",
        "        self.norm = nn.RMSNorm(self.d_inner)\n",
        "        self.out = zero_module(nn.Linear(self.d_inner, self.d_model, bias=False))\n",
        "\n",
        "    # def forward(self, u, h=None, mask=None, b_ind=None): # [b,t,d], ([b,k-1,xbc], [b,h,d,s]), [b+1]\n",
        "    # def forward(self, u, h=None, ctx_idx= None, mask=None, b_ind=None): # [b,t,d], ([b,k-1,xbc], [b,h,d,s]), [b+1]\n",
        "    def forward(self, u, h=None, step= None, mask=None, b_ind=None): # [b,t,d], ([b,k-1,xbc], [b,h,d,s]), [b+1]\n",
        "        b, t = u.shape[:2]\n",
        "        if h==None: h_conv, h_ssm = None, None\n",
        "        else: h_conv, h_ssm = h\n",
        "        # ctx_idx = torch.cat([torch.zeros(b,1, device=u.device), ctx_idx], dim=-1) # [b,1+t]\n",
        "        # step = ctx_idx[:,1:] - ctx_idx[:,:-1] # [b,t]\n",
        "        # step = torch.ones(b,t, device=u.device) #\n",
        "        # step = torch.cat((step, torch.flip(step,(1,))), dim=0) # [b,t]->[2b,t]\n",
        "        if step==None: step = torch.ones(2*b,t, device=u.device) # [b,t]\n",
        "        else: step = torch.cat((step[:,:-1], torch.flip(step[:,1:],(1,))), dim=0).to(device) # [b,t+1]->[2b,t]\n",
        "\n",
        "        # u = self.act(self.conv1d(u.transpose(-2,-1)).transpose(-2,-1)) # [b,t,inr+2gs]\n",
        "        # z, xBC, A = self.in_proj(u).split([self.d_inner, self.d_inner + 2*self.n_groups*self.d_state, self.n_heads], dim=-1)\n",
        "        # z, xBC, A, dt = self.in_proj(u).split([self.d_inner, self.d_inner + 2*self.n_groups*self.d_state, self.n_heads, self.n_heads], dim=-1)\n",
        "        z, to_flip = self.in_proj(u).split([self.d_inner, self.d_inner + 2*self.n_groups*self.d_state + 2*self.n_heads], dim=-1)\n",
        "        # z, x, B, C, A, dt = self.in_proj(u).split([self.d_inner, self.d_inner, self.n_groups*self.d_state, self.n_groups*self.d_state, self.n_heads, self.n_heads], dim=-1)\n",
        "\n",
        "        to_flip = torch.cat((to_flip, torch.flip(to_flip,(1,))), dim=0) # [b,t,xbc]->[2b,t,xbc]\n",
        "        # x, B, C = to_flip.split([self.d_inner, self.n_groups*self.d_state, self.n_groups*self.d_state], dim=-1)\n",
        "        x, B, C, A, dt = to_flip.split([self.d_inner, self.n_groups*self.d_state, self.n_groups*self.d_state, self.n_heads, self.n_heads], dim=-1)\n",
        "\n",
        "        # dt = torch.cat((dt, torch.flip(dt,(1,))), dim=0) # [b,t,h]->[2b,t,h]\n",
        "        dt = F.softplus(dt)\n",
        "        # A = torch.cat((A, torch.flip(A,(1,))), dim=0) # [b,t,h]->[2b,t,h]\n",
        "        # A = -torch.exp(self.A_log) # mamba2\n",
        "        # A = -torch.exp(A) #\n",
        "        # A = -F.softplus(-A) # log(1+e^x)\n",
        "        A = -F.sigmoid(-A) # 1/(1+e^-x)\n",
        "        # print('mamba fwd0', A[-1,-3:,-5:])\n",
        "\n",
        "        # print('mamba fwd', xBC.shape, h_conv.shape)\n",
        "        # xBC = self.act(self.conv1d(xBC.transpose(-2,-1)).transpose(-2,-1)) # [b,t,inr+2gs]\n",
        "\n",
        "\n",
        "        x_og = x[:b] # [b,t,inr]\n",
        "        x, B, C = x.unflatten(-1, (self.n_heads, self.d_head)), B.unflatten(-1, (self.n_groups, self.d_state)), C.unflatten(-1, (self.n_groups, self.d_state)) # x:[2b,t,h,d], B/C:[2b,t,g,s]\n",
        "        # y_diag = (x[:b] * self.D.unsqueeze(-1)).flatten(2) # bthd*h1\n",
        "\n",
        "        h_g = self.n_heads//self.n_groups\n",
        "        if h_g>1: B, C = B.repeat_interleave(h_g, dim=-2), C.repeat_interleave(h_g, dim=-2) # [b,t,g,s]->[b,t,h,s]\n",
        "\n",
        "        # # A, x = (A*step[...,*[None]*(A.ndim-2)]).exp(), x*step[...,None,None] # bthd*bt11=bthd # sdd discretization\n",
        "        A, B = zoh(A, B, step) # euler zoh bilinear # bth,bths\n",
        "        # print('step', step.max())\n",
        "        # print('step', step)\n",
        "        # # print('mamba fwd', A.shape, B.shape, dt.shape)\n",
        "        # # A, B = zoh(A, B*dt.unsqueeze(-1), step) # euler zoh bilinear # bth,bths\n",
        "        # # A, B = zoh(A*dt, B*dt.unsqueeze(-1), step) # euler zoh bilinear # bth,bths\n",
        "        # # print('mamba fwd1', A[-1,-3:,-5:])\n",
        "        # dt = dt * step.unsqueeze(-1) # [2b,t,h]*[2b,t,1]\n",
        "\n",
        "# x:bthd, dt:bthd, A.exp:hds, B/C:bts, D:hd # mamba1\n",
        "# x:bhtd, dt:bht, A:h, B/C:bgts-repeat>bhts, D:h # mamba2\n",
        "# mamba(ssd):\n",
        "# h = Ah + Bx : A*h + x@B = 1/ds*ds + d1@1s = ds\n",
        "# y = Ch + Dx : h@C + D*x = ds@s1 + 1/d1*d1 = d1\n",
        "\n",
        "        # print('mamba fwd', x.shape, A.shape, B.shape)\n",
        "        x, A, B, C = [a.transpose(1,2) for a in (x, A, B, C)] # X:[b,h,t,d], A:[b,h,t], B/C:[b,h,t,s]\n",
        "        dt = dt.transpose(1,2) # [b,h,t]\n",
        "# x:bhtd, A:bht, B/C:bhts, 10.5s\n",
        "        y, h_ssm = ssd(x, A.log(), B, C, h_ssm, msk=mask, b_ind=b_ind, chunk=64) #\n",
        "        # y, h_ssm = ssd(x*dt.unsqueeze(-1), A*dt, B, C, h_ssm, msk=mask, b_ind=b_ind, chunk=64) # 256\n",
        "\n",
        "        y = y.transpose(1,2).flatten(2) # [b,t,d]/[1,b*t,d]\n",
        "\n",
        "        y = torch.roll(y, shifts=1, dims=1) # 123...l -> l12...l-1\n",
        "        y[:,0] = 0 # 012...l-1\n",
        "        y = y[:b] + torch.flip(y[b:], (1,)) + x_og * self.D(x_og).repeat(1,1,self.d_head) # [b,t,inr]\n",
        "        # y = y[:b] + torch.flip(y[b:], (1,)) +  y_diag # [b,t,inr]\n",
        "        y = self.norm(y * self.act(z)) # [b,t,inr] # norm(x)*silu(z) if norm_before_gate, else norm(x*silu(z)) # https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/triton/layernorm_gated.py#L18\n",
        "        out = self.out(y)\n",
        "        return out#, (h_conv, h_ssm) # [b,t,in], ([b,k-1,xbc], [b,h,d,s])\n",
        "\n",
        "\n",
        "b,t,d_model=5,256,32\n",
        "# b,t,d_model=5,7,32\n",
        "u = torch.randn(b,t,d_model, device=device)\n",
        "model = Hydra(d_model).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) #\n",
        "# out, h = model(u)\n",
        "out = model(u)\n",
        "# h0 = torch.randn(b, model.n_heads, model.d_head, model.d_state)\n",
        "# print(out.shape)\n",
        "# print(out.shape, h.shape)\n",
        "# print(out[0,-3:,:5], h[0,:2,:5,:5])\n",
        "# out, h = model(u, h)\n",
        "\n",
        "print(out.shape)\n",
        "print(out[0,-3:,:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pQfM2fYvcTh6"
      },
      "outputs": [],
      "source": [
        "# @title masks\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# def multiblock(seq, min_s, max_s, M=1):\n",
        "#     mask_len = torch.rand(1) * (max_s - min_s) + min_s # in (min_s, max_s) # all blocks same size\n",
        "#     mask_pos = torch.rand(M) * (1 - mask_len) # in (0, 1 - mask_len)\n",
        "#     mask_len, mask_pos = (mask_len * seq).int(), mask_pos * seq\n",
        "#     indices = torch.arange(seq).unsqueeze(0) # [1, seq]\n",
        "#     target_mask = (indices >= mask_pos.unsqueeze(-1)) & (indices < (mask_pos + mask_len).unsqueeze(-1)) # [M, seq]\n",
        "#     return target_mask\n",
        "\n",
        "\n",
        "# def multiblock2d(hw=(8,8), scale=(.15,.2), aspect_ratio=(.75,1.5), M=1):\n",
        "#     mask_aspect = torch.rand(1) * (aspect_ratio[1] - aspect_ratio[0]) + aspect_ratio[0] # in (min_s, max_s) # all blocks same size\n",
        "#     mask_scale = torch.rand(1) * (scale[1] - scale[0]) + scale[0] # in (min_s, max_s) # all blocks same size\n",
        "#     h = (mask_scale/mask_aspect)**.5# h*(h*aspect) = scale\n",
        "#     w = h * mask_aspect\n",
        "#     h_pos, w_pos = torch.rand(M)*(1-w), torch.rand(M)*(1-h) # in (0, 1 - mask_len)\n",
        "#     h_len, h_pos = (h*hw[0]).int(), h_pos*hw[0]\n",
        "#     w_len, w_pos = (w*hw[1]).int(), w_pos*hw[1]\n",
        "#     h_ind, w_ind = torch.arange(hw[0]).unsqueeze(0), torch.arange(hw[1]).unsqueeze(0) # [1, seq]\n",
        "#     h_mask = (h_ind>=h_pos.unsqueeze(-1)) & (h_ind<(h_pos+h_len).unsqueeze(-1)) # [M, seq]\n",
        "#     w_mask = (w_ind>=w_pos.unsqueeze(-1)) & (w_ind<(w_pos+w_len).unsqueeze(-1)) # [M, seq]\n",
        "#     target_mask = h_mask.unsqueeze(-1) & w_mask.unsqueeze(-2) # [M, seq, seq]\n",
        "#     return target_mask\n",
        "\n",
        "# # https://arxiv.org/pdf/2210.07224\n",
        "# def randpatch(seq, mask_size=8, gamma=0.9): # num patches of seq, mask patch size, masking ratio\n",
        "#     # mask = torch.rand(seq//mask_size)<gamma\n",
        "#     length = seq//mask_size\n",
        "#     g = torch.normal(gamma, std=.1, size=(1,)).clamp(.5,.9)\n",
        "#     # g = gamma\n",
        "#     idx = torch.randperm(length)[:int(length*g)]\n",
        "#     mask = torch.zeros(length, dtype=bool)\n",
        "#     mask[idx] = True\n",
        "#     mask = mask.repeat_interleave(mask_size, dim=-1)\n",
        "#     return mask # [seq] , True -> mask\n",
        "\n",
        "\n",
        "# import torch\n",
        "# def apply_masks(x, mask): # [b,t,d], [mask_size] # https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py\n",
        "#     mask_keep = mask.unsqueeze(-1).repeat(x.size(0), 1, x.size(-1)) # [batch,T,dim]\n",
        "#     return torch.gather(x, dim=1, index=mask_keep) # [batch,mask_size,dim]\n",
        "\n",
        "\n",
        "# @title ijepa multiblock next\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/src/masks/multiblock.py\n",
        "import math\n",
        "from multiprocessing import Value\n",
        "import torch\n",
        "\n",
        "class MaskCollator(object):\n",
        "    def __init__(self, hw=(224, 224), enc_mask_scale=(.85,1), pred_mask_scale=(.15,.2), aspect_ratio=(.75,1.25),\n",
        "        nenc=1, npred=2, min_keep=4, allow_overlap=False):\n",
        "        super().__init__()\n",
        "        self.height, self.width = hw\n",
        "        self.enc_mask_scale = enc_mask_scale\n",
        "        self.pred_mask_scale = pred_mask_scale\n",
        "        self.aspect_ratio = aspect_ratio\n",
        "        self.nenc = nenc\n",
        "        self.npred = npred\n",
        "        self.min_keep = min_keep  # minimum number of patches to keep\n",
        "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
        "\n",
        "    def _sample_block_size(self, scale, aspect_ratio_scale):\n",
        "        _rand = torch.rand(1).item()\n",
        "        # -- Sample block scale\n",
        "        min_s, max_s = scale\n",
        "        mask_scale = min_s + _rand * (max_s - min_s)\n",
        "        max_keep = int(self.height * self.width * mask_scale) # num patches to keep\n",
        "        # -- Sample block aspect-ratio\n",
        "        min_ar, max_ar = aspect_ratio_scale\n",
        "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
        "        # -- Compute block height and width (given scale and aspect-ratio)\n",
        "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
        "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
        "        while h >= self.height: h -= 1 # crop mask to be smaller than img\n",
        "        while w >= self.width: w -= 1\n",
        "        return (h, w)\n",
        "\n",
        "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
        "        h, w = b_size\n",
        "        def constrain_mask(mask, tries=0):\n",
        "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
        "            N = max(int(len(acceptable_regions)-tries), 0)\n",
        "            for k in range(N):\n",
        "                mask *= acceptable_regions[k]\n",
        "        # -- Loop to sample masks until we find a valid one\n",
        "        tries = 0\n",
        "        timeout = og_timeout = 20\n",
        "        valid_mask = False\n",
        "        while not valid_mask:\n",
        "            # -- Sample block top-left corner\n",
        "            top = torch.randint(0, self.height - h, (1,))\n",
        "            left = torch.randint(0, self.width - w, (1,))\n",
        "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
        "            mask[top:top+h, left:left+w] = 1\n",
        "            # -- Constrain mask to a set of acceptable regions\n",
        "            if acceptable_regions is not None:\n",
        "                constrain_mask(mask, tries)\n",
        "            mask = torch.nonzero(mask.flatten())\n",
        "            # -- If mask too small try again\n",
        "            valid_mask = len(mask) > self.min_keep\n",
        "            if not valid_mask:\n",
        "                timeout -= 1\n",
        "                if timeout == 0:\n",
        "                    tries += 1\n",
        "                    timeout = og_timeout\n",
        "        mask = mask.squeeze()\n",
        "        # --\n",
        "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
        "        mask_complement[top:top+h, left:left+w] = 0\n",
        "        # --\n",
        "        return mask, mask_complement\n",
        "\n",
        "    def __call__(self, B):\n",
        "        '''\n",
        "        Create encoder and predictor masks when collating imgs into a batch\n",
        "        # 1. sample enc block (size + location) using seed\n",
        "        # 2. sample pred block (size) using seed\n",
        "        # 3. sample several enc block locations for each image (w/o seed)\n",
        "        # 4. sample several pred block locations for each image (w/o seed)\n",
        "        # 5. return enc mask and pred mask\n",
        "        '''\n",
        "        p_size = self._sample_block_size(scale=self.pred_mask_scale, aspect_ratio_scale=self.aspect_ratio)\n",
        "        e_size = self._sample_block_size(scale=self.enc_mask_scale, aspect_ratio_scale=(1., 1.))\n",
        "\n",
        "        collated_masks_pred, collated_masks_enc = [], []\n",
        "        min_keep_pred = self.height * self.width\n",
        "        min_keep_enc = self.height * self.width\n",
        "        for _ in range(B):\n",
        "\n",
        "            masks_p, masks_C = [], []\n",
        "            for _ in range(self.npred):\n",
        "                mask, mask_C = self._sample_block_mask(p_size)\n",
        "                masks_p.append(mask)\n",
        "                masks_C.append(mask_C)\n",
        "                min_keep_pred = min(min_keep_pred, len(mask))\n",
        "            collated_masks_pred.append(masks_p)\n",
        "\n",
        "            acceptable_regions = masks_C\n",
        "            try:\n",
        "                if self.allow_overlap:\n",
        "                    acceptable_regions= None\n",
        "            except Exception as e:\n",
        "                print(f'Encountered exception in mask-generator {e}')\n",
        "\n",
        "            masks_e = []\n",
        "            for _ in range(self.nenc):\n",
        "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
        "                masks_e.append(mask)\n",
        "                min_keep_enc = min(min_keep_enc, len(mask))\n",
        "            collated_masks_enc.append(masks_e)\n",
        "        collated_masks_pred = [[cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred]\n",
        "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
        "        # --\n",
        "        collated_masks_enc = [[cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc]\n",
        "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
        "        return collated_masks_enc, collated_masks_pred\n",
        "\n",
        "mask_collator = MaskCollator(hw=(32,32), enc_mask_scale=(.85, 1.), pred_mask_scale=(.15, .2), aspect_ratio=(.75, 1.5),\n",
        "        nenc=1, npred=4, min_keep=4,\n",
        "        # allow_overlap=True)\n",
        "        allow_overlap=False)\n",
        "\n",
        "b=16\n",
        "collated_masks_enc, collated_masks_pred = mask_collator(b)\n",
        "ctx_index, trg_index = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "# imshow(mask)\n",
        "\n",
        "mask = torch.zeros(b ,32*32)\n",
        "mask[torch.arange(b).unsqueeze(-1), trg_index] = 1\n",
        "mask[torch.arange(b).unsqueeze(-1), ctx_index] = .5\n",
        "mask = mask.reshape(b,1,32,32)\n",
        "print((mask==1).flatten(1).sum(-1))\n",
        "print((mask==.5).flatten(1).sum(-1))\n",
        "import torchvision\n",
        "imshow(torchvision.utils.make_grid(mask, nrow=8))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fh9o__m2-j7K",
        "cellView": "form",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# @title simplex\n",
        "!pip install -q opensimplex\n",
        "import opensimplex\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def simplexmask2d(hw=(32,32), ctx_scale=(.85,1.), trg_scale=(.6,.8), B=64, chaos=[1,.5]):\n",
        "    ix, iy = np.split(np.linspace(0, chaos[0], num=sum(hw)), [hw[0]])\n",
        "    noise = opensimplex.noise3array(ix, iy, np.random.randint(1e10, size=B)) # [b,h,w]\n",
        "    noise = torch.from_numpy(noise).flatten(1)\n",
        "    # trunc_normal = torch.fmod(torch.randn(2)*.3,1)/2 + .5\n",
        "    ctx_mask_scale = torch.rand(1) * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    trg_mask_scale = torch.rand(1) * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "    # ctx_mask_scale = trunc_normal[0] * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    # trg_mask_scale = trunc_normal[1] * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "    seq = hw[0]*hw[1]\n",
        "    ctx_len, trg_len = int(seq*ctx_mask_scale), int(seq*trg_mask_scale)\n",
        "    val, trg_index = torch.topk(noise, trg_len, dim=1, sorted=False)\n",
        "    # ctx_len = ctx_len - trg_len\n",
        "    ctx_len = min(ctx_len, seq-trg_len)\n",
        "\n",
        "    remove_mask = torch.ones((B,seq), dtype=bool) # [B, S]\n",
        "    remove_mask.scatter_(1, trg_index, False).flatten()\n",
        "    ind = torch.arange(seq).unsqueeze(0).repeat(B,1)[remove_mask].reshape(B, -1)\n",
        "\n",
        "    ix, iy = np.split(np.linspace(0, chaos[1], num=sum(hw)), [hw[0]])\n",
        "    noise = opensimplex.noise3array(ix, iy, np.random.randint(1e10, size=B)) # [b,h,w]\n",
        "    noise = torch.from_numpy(noise).flatten(1)[remove_mask].reshape(B, -1)\n",
        "    val, ctx_ind = torch.topk(noise, ctx_len, dim=1, sorted=False)\n",
        "    ctx_index = ind[torch.arange(B).unsqueeze(-1), ctx_ind]\n",
        "    return ctx_index, trg_index\n",
        "\n",
        "b=16\n",
        "# ctx_index, trg_index = simplexmask2d(hw=(32,32), ctx_scale=(.85,1), trg_scale=(.7,.8), B=b, chaos=[3,.5])\n",
        "# ctx_index, trg_index = simplexmask2d(hw=(32,32), ctx_scale=(.85,1), trg_scale=(.7,.8), B=b, chaos=[3,1])\n",
        "# ctx_index, trg_index = simplexmask2d(hw=(32,32), ctx_scale=(.85,1), trg_scale=(.2,.8), B=b, chaos=[3,1])\n",
        "ctx_index, trg_index = simplexmask2d(hw=(32,32), ctx_scale=(.05,.2), trg_scale=(.7,.8), B=b, chaos=[3,1])\n",
        "# ctx_index, trg_index = simplexmask2d(hw=(32,32), ctx_scale=(.85,1), trg_scale=(.5,.5), B=b, chaos=[.01,.5])\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "# imshow(mask)\n",
        "\n",
        "mask = torch.zeros(b ,32*32)\n",
        "# mask = torch.ones(b ,32*32)*.3\n",
        "mask[torch.arange(b).unsqueeze(-1), trg_index] = 1\n",
        "mask[torch.arange(b).unsqueeze(-1), ctx_index] = .5\n",
        "# print((mask==1).sum(1))\n",
        "# print((mask==.5).sum(1))\n",
        "mask = mask.reshape(b,1,32,32)\n",
        "import torchvision\n",
        "imshow(torchvision.utils.make_grid(mask, nrow=8))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Nd1GqX3PYpIk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "34913bcd-219f-4319-b404-cb4a17e0a3a5",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title Random Fourier Features noise\n",
        "import math\n",
        "import torch\n",
        "\n",
        "# Random Fourier Features: ∑_k] a_k * cos(ω_k ⋅ x + 𝜙_k)\n",
        "# def rff_noise(x, out_dim, n_freqs=128, scale=1): # [...,n_Dim]\n",
        "def rff_noise(x, out_dim, n_freqs=16, scale=1): # [...,n_Dim]\n",
        "    space, in_dim, device = x.shape[:-1], x.shape[-1], x.device\n",
        "    x = x.flatten(0,-2) # [N,in]\n",
        "    w = torch.randn(out_dim, n_freqs, in_dim, device=device) * scale\n",
        "    phi = torch.empty(out_dim, n_freqs, device=device).uniform_(0,2*math.pi)\n",
        "    a = torch.randn(out_dim, n_freqs, device=device) / math.sqrt(n_freqs)\n",
        "    y = torch.cos(torch.einsum(\"ofd,...d->...of\", w, x) + phi) # [N,out,freq]\n",
        "    return torch.einsum(\"of,...of->...o\", a, y).unflatten(0,space) # [...,out]\n",
        "\n",
        "\n",
        "def rffmask2d(bhw, ctx_scale=(.85,1.), trg_scale=(.6,.8), chaos=[1,.5]): # ctx_scale > trg_scale\n",
        "# def rffmask2d(bhw, ctx_scale=(.05,.5), trg_scale=(.2,.8), chaos=[1,.5],): # ctx_scale > trg_scale\n",
        "    b,h,w = bhw\n",
        "    ctx_mask_scale = torch.rand(1) * (ctx_scale[1] - ctx_scale[0]) + ctx_scale[0] # in (min_s, max_s) # all blocks same size\n",
        "    trg_mask_scale = torch.rand(1) * (trg_scale[1] - trg_scale[0]) + trg_scale[0]\n",
        "    seq = h*w\n",
        "    ctx_len, trg_len = int(seq*ctx_mask_scale), int(seq*trg_mask_scale)\n",
        "    # print(ctx_len, trg_len)\n",
        "    # ctx_len = ctx_len - trg_len\n",
        "    ctx_len = min(ctx_len, seq-trg_len)\n",
        "\n",
        "    ix, iy = torch.linspace(0, chaos[0], h), torch.linspace(0, chaos[0], w)\n",
        "    x = torch.stack(torch.meshgrid(ix, iy, indexing=\"ij\"), dim=-1) # [t,h,w,b] / [h,w,b]\n",
        "    noise = rff_noise(x, b).flatten(0,-2).T # [h,w,b]->[h*w,b]->[b,h*w]\n",
        "    _, trg_ind = torch.topk(noise, trg_len, dim=1, sorted=False)\n",
        "\n",
        "    ix, iy = torch.linspace(0, chaos[1], h), torch.linspace(0, chaos[1], w)\n",
        "    x = torch.stack(torch.meshgrid(ix, iy, indexing=\"ij\"), dim=-1) # [t,h,w,b] / [h,w,b]\n",
        "    noise = rff_noise(x, b).flatten(0,-2).T\n",
        "    noise.scatter_(1, trg_ind, -torch.inf).flatten()\n",
        "    _, ctx_ind = torch.topk(noise, ctx_len, dim=1, sorted=False)\n",
        "    return ctx_ind, trg_ind\n",
        "\n",
        "b=16\n",
        "hw=(8,8)\n",
        "# hw=(32,32)\n",
        "# cxt_inds, trg_inds = rffmask2d((b,)+hw, ctx_scale=(.85,1.), trg_scale=(.2,.8), chaos=[3,1])\n",
        "# cxt_inds, trg_inds = rffmask2d((b,)+hw, ctx_scale=(.85,1.), trg_scale=(.2,.8), chaos=[1,.5])\n",
        "cxt_inds, trg_inds = rffmask2d((b,)+hw, ctx_scale=(.05,.5), trg_scale=(.2,.8), chaos=[4,1])\n",
        "# print(cxt_inds, trg_inds)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "# imshow(mask)\n",
        "\n",
        "mask = torch.zeros(b ,math.prod(hw))\n",
        "mask[torch.arange(b).unsqueeze(-1), trg_inds] = 1\n",
        "mask[torch.arange(b).unsqueeze(-1), cxt_inds] = .5\n",
        "# print((mask==1).sum(1))\n",
        "# print((mask==.5).sum(1))\n",
        "mask = mask.reshape(b,1,*hw)\n",
        "import torchvision\n",
        "imshow(torchvision.utils.make_grid(mask, nrow=8))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivqYjBIVrB3e",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title ViT\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        self._kwargs = [[name for name, p in inspect.signature(layer.forward).parameters.items()] for layer in self]\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        for layer, _kwargs in zip(self, self._kwargs):\n",
        "            x = layer(x, *args, **{k: v for k, v in kwargs.items() if k in _kwargs})\n",
        "        return x\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, patch_size, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        patch_size=2\n",
        "        self.hw=(8,8) # (8,8) (32,32)\n",
        "        self.seq = self.hw[0]*self.hw[1]\n",
        "\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Conv2d(in_dim, d_model, patch_size, patch_size), # like patch\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 1, 7//2, bias=False)\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.MaxPool2d(3,2,1)\n",
        "            nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.BatchNorm2d(d_model), nn.ReLU(),\n",
        "            nn.Conv2d(d_model, d_model, 3, 2, 3//2, bias=False)\n",
        "            )\n",
        "        # self.embed.requires_grad=False\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, self.seq, d_model)*.02)\n",
        "        # self.pos_emb = nn.Parameter(RoPE2D(dim=d_model, h=8, w=8, base=1000), requires_grad=False)\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=(32//patch_size)**2, base=10000), requires_grad=False)\n",
        "\n",
        "        self.transformer = Seq(*[AttentionBlock(d_model, d_model, n_heads) for _ in range(nlayers)])\n",
        "        # self.transformer = Seq(*[Hydra(d_model, n_heads) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, cxt_inds=None): # [b,ctx,3], [b,ctx] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x).flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c] # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "\n",
        "        # x = self.pos_enc(x)\n",
        "        # print(\"TransformerModel\",x.shape, self.pos_emb.shape)\n",
        "        x = x + self.pos_emb[:,:x.shape[1]]\n",
        "        if cxt_inds != None: x = x[torch.arange(x.shape[0]).unsqueeze(-1), cxt_inds] # [batch, num_context_toks, d_model]\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        # if cxt_inds != None: # for pos\n",
        "        #     # x = self.transformer(x[torch.arange(x.shape[0]).unsqueeze(-1), cxt_inds], cxt_inds) # for pos attn\n",
        "        #     b = x.shape[0]\n",
        "        #     # print('vit fwd', cxt_inds)\n",
        "        #     ctx_idx = torch.cat([torch.full((b,1),-1), cxt_inds, torch.full((b,1),self.seq)], dim=-1) # [b,1+t+1]\n",
        "        #     step = ctx_idx[:,1:] - ctx_idx[:,:-1] # [b,t+1]\n",
        "        #     x = self.transformer(x[torch.arange(x.shape[0]).unsqueeze(-1), cxt_inds], step=step)\n",
        "        # else: x = self.transformer(x)\n",
        "\n",
        "        out = self.norm(x)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out\n",
        "\n",
        "d_model = 64\n",
        "in_dim = 3\n",
        "patch_size = 2\n",
        "# model = ViT(patch_size, in_dim, d_model, n_heads=4, nlayers=4, drop=0.).to(device)\n",
        "model = ViT(patch_size, in_dim, d_model, n_heads=4, nlayers=1, drop=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x = torch.rand((5, in_dim, 32, 32), device=device) # [b,c,h,w]\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-zdjdJixtOu",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Transformer Predictor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class TransformerPredictor(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        self.hw=(8,8) # (8,8) (32,32)\n",
        "        self.seq = self.hw[0]*self.hw[1]\n",
        "        self.embed = nn.Linear(in_dim, d_model)# if in_dim != d_model else None\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, self.seq, d_model)*0.02)\n",
        "\n",
        "        self.transformer = Seq(*[AttentionBlock(d_model, n_heads) for _ in range(nlayers)])\n",
        "        # self.transformer = Seq(*[Hydra(d_model, n_heads) for _ in range(nlayers)])\n",
        "\n",
        "        self.cls = nn.Parameter(torch.randn(1,1,d_model)*.02) # randn zeros\n",
        "        out_dim = out_dim or d_model\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim)# if out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, cxt_inds, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x) # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        b, seq, dim = x.shape\n",
        "\n",
        "        # # x = x * self.pos_enc(cxt_inds)\n",
        "        # # pred_tokens = self.cls * self.pos_enc(trg_indices) # [M, num_trg_toks, d_model]\n",
        "        x = x + self.pos_emb[0,cxt_inds]\n",
        "        pred_tokens = self.cls + self.pos_emb[0,trg_indices]\n",
        "        # # print(\"pred fwd\", x.shape, pred_tokens.shape)\n",
        "        x = torch.cat([x, pred_tokens], dim=1) # [batch, seq_len+num_trg_toks, d_model]\n",
        "        out = self.transformer(x)[:,seq:] # [b,trg,d]\n",
        "\n",
        "        # # for ssm\n",
        "        # x = torch.cat([x, self.cls.expand(*trg_indices.shape,-1)], dim=1) # [batch, num_ctx_toks+num_trg_toks, d_model]\n",
        "        # val, idx = torch.sort(torch.cat([cxt_inds, trg_indices], dim=-1)) # [b,ctx+trg]\n",
        "        # x = x[torch.arange(b).unsqueeze(-1), idx]\n",
        "        # indices = torch.cat([torch.full((b,1), -1), val, torch.full((b,1), self.seq)], dim=-1) # [b,1+t+1]\n",
        "        # step = indices[:,1:] - indices[:,:-1] # [b,t+1]\n",
        "        # out = self.transformer(x, step=step)\n",
        "        # back_idx = idx[idx>=cxt_inds.shape[-1]].reshape(b,-1) # [b,trg]\n",
        "        # out = out[torch.arange(b).unsqueeze(-1), back_idx] # [b,trg,d]\n",
        "\n",
        "        # # for pos attn\n",
        "        # x = torch.cat([x, self.cls.expand(*trg_indices.shape,-1)], dim=1) # [batch, num_ctx_toks+num_trg_toks, d_model]\n",
        "        # out = self.transformer(x, torch.cat([cxt_inds, trg_indices], dim=-1))[:,seq:] # [b,trg,d]\n",
        "\n",
        "        out = self.norm(out)\n",
        "        out = self.lin(out)\n",
        "        # print(\"pred fwd\", out.shape)\n",
        "        return out # [seq_len, batch_size, ntoken]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ardu1zJwdHM3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14f4be3f-48c0-4760-c7cf-2424d0d5957a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "153024\n",
            "torch.Size([])\n"
          ]
        }
      ],
      "source": [
        "# @title IJEPA\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "@torch.compile\n",
        "class IJEPA(nn.Module):\n",
        "    def __init__(self, in_dim=3, d_model=64, out_dim=None, nlayers=1, n_heads=4):\n",
        "        super().__init__()\n",
        "        self.out_dim = out_dim = out_dim or d_model\n",
        "        self.patch_size = 4 # 4\n",
        "        self.student = ViT(self.patch_size, in_dim, d_model, out_dim=out_dim, n_heads=n_heads, nlayers=nlayers, drop=0.)\n",
        "        self.predicter = TransformerPredictor(out_dim, d_model//2, out_dim, n_heads=4, nlayers=1, drop=0.)\n",
        "        import copy\n",
        "        self.teacher = copy.deepcopy(self.student)\n",
        "        self.teacher.requires_grad_(False)\n",
        "        self.hw=(8,8) # (8,8) (32,32)\n",
        "        # self.mask_collator = MaskCollator(self.hw, enc_mask_scale=(.85, 1.), pred_mask_scale=(.15, .2), aspect_ratio=(.75, 1.5), nenc=1, npred=4, min_keep=4, allow_overlap=False) # og\n",
        "\n",
        "    def loss(self, x): #\n",
        "        b,c,h,w = x.shape\n",
        "        # collated_masks_enc, collated_masks_pred = self.mask_collator(b)\n",
        "        # cxt_inds, trg_inds = torch.stack(collated_masks_enc).squeeze(0), torch.stack(collated_masks_pred).transpose(0,1).flatten(1).unique(dim=1) # [num_msk, b,num_tok]->[b,num_tok] # [64, 65], [64, 32]\n",
        "        # cxt_inds, trg_inds = rffmask2d((1,)+self.hw, ctx_scale=(.05,.2), trg_scale=(.2,.8), chaos=[4,1])\n",
        "        # cxt_inds, trg_inds = simplexmask2d(hw, ctx_scale=(.85,1), trg_scale=(.7,.8), B=b, chaos=[3,.5])\n",
        "        cxt_inds, trg_inds = simplexmask2d(hw, ctx_scale=(.05,.2), trg_scale=(.2,.8), B=1, chaos=[4,1])\n",
        "        cxt_inds, trg_inds = cxt_inds.repeat(b,1), trg_inds.repeat(b,1)\n",
        "        # cxt_inds = cxt_inds.sort(-1)[0]\n",
        "\n",
        "        # zero_mask = torch.zeros(b ,*self.hw, device=device).flatten(1)\n",
        "        # zero_mask[torch.arange(b).unsqueeze(-1), cxt_inds] = 1\n",
        "        # x_ = x * F.interpolate(zero_mask.reshape(b,1,*self.hw), size=x.shape[2:], mode='nearest-exact') # zero masked locations\n",
        "        # sx = self.student(x_, cxt_inds=cxt_inds) # [batch, num_context_toks, out_dim]\n",
        "\n",
        "        sx = self.student(x, cxt_inds=cxt_inds) # [batch, num_context_toks, out_dim]\n",
        "        # print('ijepa loss sx',sx.shape)\n",
        "        sy_ = self.predicter(sx, cxt_inds=cxt_inds, trg_indices=trg_inds) # [batch*M, num_trg_toks, out_dim]\n",
        "        sy_ = F.layer_norm(sy_, (sy_.size(-1),))\n",
        "        with torch.no_grad():\n",
        "            sy = self.teacher(x.detach()) # [batch, num_trg_toks, out_dim]\n",
        "            # print('ijepa loss sy',sy.shape)\n",
        "            sy = sy[torch.arange(sy.shape[0]).unsqueeze(-1), trg_inds] # [batch, num_context_toks, d_model] # nan bec len(trg_ind)==0 # print('loss sy',torch.isnan(sy).any())\n",
        "            sy = F.layer_norm(sy, (sy.size(-1),))\n",
        "        loss = F.mse_loss(sy, sy_)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x): # [b,t,3]\n",
        "        out = self.student(x).mean(1)\n",
        "        return out\n",
        "\n",
        "# min_s=0.15, max_s, M\n",
        "# trg.15.2M4 C.85 1\n",
        "# dont normalise data\n",
        "# randmsk < simplex 1msk < multiblk\n",
        "# teacher=trans ~? teacher=copy\n",
        "# learn convemb\n",
        "# lr pred,student: 3e-3/1e-2, 1e-3\n",
        "\n",
        "# lr 1e-4<<<1e-3 < 1e-2,1e-3? slower but dun plateau\n",
        "# zeromsk impt, else closs start increasing\n",
        "# l1rge trg helps delay increase?\n",
        "\n",
        "# vit 1lyr 15.4sec 15.6\n",
        "# hiera downsampling attn 20.5\n",
        "\n",
        "# ijepa = IJEPA(in_dim=3, d_model=64, out_dim=64, nlayers=4, n_heads=4).to(device)\n",
        "ijepa = IJEPA(in_dim=3, d_model=64, out_dim=64, nlayers=1, n_heads=8).to(device)\n",
        "# ijepa = IJEPA(in_dim=3, d_model=32, out_dim=32, nlayers=1, n_heads=4).to(device)\n",
        "# optim = torch.optim.AdamW(ijepa.parameters(), lr=1e-3)#, weight_decay=0) # 1e-3?\n",
        "optim = torch.optim.AdamW([{'params': ijepa.student.parameters()},\n",
        "#     {'params': ijepa.predicter.parameters(), 'lr': 3e-3}], lr=1e-3, weight_decay=1e-2) # good 3e-3,1e-3 ; default 1e-2, 5e-2\n",
        "    {'params': ijepa.predicter.parameters(), 'lr': 1e-2}], lr=1e-3, weight_decay=1e-2)\n",
        "\n",
        "# https://github.com/facebookresearch/ijepa/blob/main/configs/in1k_vith14_ep300.yaml\n",
        "# d_model 1024,384\n",
        "# depth 12,6/12\n",
        "# wd 5e-2 - 4e-1\n",
        "# adamw 1e-4 - 1e-3 - 1e-6\n",
        "# ema 0.996-1\n",
        "\n",
        "print(sum(p.numel() for p in ijepa.parameters() if p.requires_grad)) # 27584\n",
        "# print(sum(p.numel() for p in ijepa.parameters())) # 27584\n",
        "# print(sum(p.numel() for p in ijepa.predicter.transformer_encoder.parameters() if p.requires_grad)) # 27584\n",
        "# d_model^2 * nlayers\n",
        "\n",
        "x = torch.rand((64,3,32,32), device=device)\n",
        "out = ijepa.loss(x)\n",
        "print(out.shape)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(ijepa.out_dim, 10).to(device)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n",
        "# optim = torch.optim.AdamW([{'params': ijepa.parameters()}, {'params': classifier.parameters(), 'lr': 1e-3}], lr=1e-3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2Nd-sGe6Ku4S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "outputId": "d2dcfb7a-48a2-49bf-e35c-f4df62778e4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>██▇▆▆▆▅▅▅▅▄▄▃▄▄▄▄▁▂▃▃▂▂▁▄▃▂▂▃▃▁▂▁▂▂▄▃▂▂▂</td></tr><tr><td>correct</td><td>▂▄▃▂▁▄▄▃▃▅▅▄▄▃▅▆▅▄▆▅▄▃▆▆▆▆▆▅▅▆▅▆▆▆▆▆▆▆▇█</td></tr><tr><td>loss</td><td>▁▂▁▅▃▄▂▄▃▃▇▃▆▄▄▄▂▅▃▄█▅▄▆▄▂▂▃▄▇▆▃▇▃▂▃▃▄▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>1.64419</td></tr><tr><td>correct</td><td>0.375</td></tr><tr><td>loss</td><td>0.29544</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">olive-gorge-21</strong> at: <a href='https://wandb.ai/bobdole/rff/runs/yvigfan5' target=\"_blank\">https://wandb.ai/bobdole/rff/runs/yvigfan5</a><br> View project at: <a href='https://wandb.ai/bobdole/rff' target=\"_blank\">https://wandb.ai/bobdole/rff</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20260205_111638-yvigfan5/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.24.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20260205_115358-u500v8rv</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/rff/runs/u500v8rv' target=\"_blank\">revived-fire-22</a></strong> to <a href='https://wandb.ai/bobdole/rff' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/rff' target=\"_blank\">https://wandb.ai/bobdole/rff</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/rff/runs/u500v8rv' target=\"_blank\">https://wandb.ai/bobdole/rff/runs/u500v8rv</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "!pip install -q wandb\n",
        "import wandb # https://docs.wandb.ai/quickstart\n",
        "wandb.login(key='487a2109e55dce4e13fc70681781de9f50f27be7')\n",
        "try: run.finish()\n",
        "except NameError: pass\n",
        "run = wandb.init(project=\"rff\", config={\"model\": \"res18\",})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "34de8800-fa4b-4460-803c-3cd8dbfb6c70",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f481831a-aa56-4c33-c750-b89eb1f553a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 sloss, closs, correct 0.26478898525238037 2.303901195526123 0.140625\n",
            "time: 22.773778438568115 22.77377963066101\n",
            "1 sloss, closs, correct 0.21053919196128845 2.28214430809021 0.09375\n",
            "time: 13.310619115829468 18.04245436191559\n",
            "2 sloss, closs, correct 0.27040302753448486 2.254729747772217 0.265625\n",
            "time: 13.374930381774902 16.48678223292033\n",
            "3 sloss, closs, correct 0.2851162850856781 2.2034263610839844 0.203125\n",
            "time: 13.489120721817017 15.737502455711365\n",
            "4 sloss, closs, correct 0.20973274111747742 2.1726431846618652 0.171875\n",
            "time: 13.547840356826782 15.299733686447144\n",
            "5 sloss, closs, correct 0.25874871015548706 2.1147382259368896 0.171875\n",
            "time: 13.592289209365845 15.015252153078714\n",
            "6 sloss, closs, correct 0.20224101841449738 2.172114610671997 0.15625\n",
            "time: 13.407812356948853 14.785732337406703\n",
            "7 sloss, closs, correct 0.2978125214576721 2.1917877197265625 0.109375\n",
            "time: 14.080794095993042 14.697680801153183\n",
            "8 sloss, closs, correct 0.20515994727611542 2.19589900970459 0.140625\n",
            "time: 15.056450128555298 14.737592034869724\n",
            "9 sloss, closs, correct 0.3280084729194641 2.1550350189208984 0.203125\n",
            "time: 13.424245595932007 14.606348299980164\n",
            "10 sloss, closs, correct 0.19929111003875732 2.1915411949157715 0.140625\n",
            "time: 13.459519386291504 14.502159465443004\n",
            "11 sloss, closs, correct 0.2416440099477768 2.0987448692321777 0.1875\n",
            "time: 13.498182535171509 14.418537775675455\n",
            "12 sloss, closs, correct 0.3302016258239746 2.0520849227905273 0.125\n",
            "time: 13.531559705734253 14.350365015176626\n",
            "13 sloss, closs, correct 0.3158826529979706 2.148026704788208 0.234375\n",
            "time: 13.340779542922974 14.278286831719535\n",
            "14 sloss, closs, correct 0.3424939513206482 2.090149402618408 0.203125\n",
            "time: 13.496593952178955 14.226204983393352\n",
            "15 sloss, closs, correct 0.2651403844356537 2.189216136932373 0.1875\n",
            "time: 13.366169691085815 14.172496572136879\n",
            "16 sloss, closs, correct 0.4131348133087158 2.1139636039733887 0.28125\n",
            "time: 13.598490953445435 14.138761071597829\n",
            "17 sloss, closs, correct 0.2424549013376236 2.0416722297668457 0.203125\n",
            "time: 13.446351528167725 14.100323571099175\n",
            "18 sloss, closs, correct 0.45862340927124023 2.081094980239868 0.265625\n",
            "time: 13.384366273880005 14.062670030091939\n",
            "19 sloss, closs, correct 0.20008525252342224 2.0425803661346436 0.359375\n",
            "time: 13.438465595245361 14.031508088111877\n",
            "20 sloss, closs, correct 0.2477102279663086 2.066174030303955 0.15625\n",
            "time: 13.463005065917969 14.004459256217594\n"
          ]
        }
      ],
      "source": [
        "# @title strain ctrain test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "def strain(model, dataloader, optim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.train()\n",
        "    for i, (x, _) in enumerate(dataloader):\n",
        "        x = x.to(device)\n",
        "        loss = model.loss(x)\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        # grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 5) # 0.5\n",
        "        # print('grad_norm', grad_norm.item())\n",
        "        optim.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            m=0.999 # 0.99 m = next(momentum_scheduler)\n",
        "            for param_q, param_k in zip(model.student.parameters(), model.teacher.parameters()):\n",
        "                param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        # if i%10==0: print(\"strain\",loss.item())\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=500: break\n",
        "    return loss.item()\n",
        "\n",
        "def ctrain(model, classifier, dataloader, coptim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.eval()\n",
        "    classifier.train()\n",
        "    # closs = 0\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        with torch.no_grad(): sx = model(x).detach()\n",
        "        y_ = classifier(sx)\n",
        "        loss = F.cross_entropy(y_, y)\n",
        "        coptim.zero_grad()\n",
        "        loss.backward()\n",
        "        coptim.step()\n",
        "        # print(\"classify\",loss.item())\n",
        "        # closs += loss.item()\n",
        "        try: wandb.log({\"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=100: break\n",
        "    # return closs/\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        with torch.no_grad():\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        # print(correct/len(y))\n",
        "        try: wandb.log({\"correct\": correct/len(y)})\n",
        "        # try: wandb.log({\"correct\": correct/len(y), \"rankme\": rankme, \"lidar\": lidar})\n",
        "        except NameError: pass\n",
        "        if i>=100: break\n",
        "    return correct/len(y)\n",
        "\n",
        "import time\n",
        "start = begin = time.time()\n",
        "# for i in range(1):\n",
        "for i in range(100): # 1000\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    sloss = strain(ijepa, train_loader, optim)\n",
        "    closs = ctrain(ijepa, classifier, train_loader, coptim)\n",
        "    correct = test(ijepa, classifier, test_loader)\n",
        "    print(i, 'sloss, closs, correct', sloss, closs, correct)\n",
        "    print('time:',time.time() - start, (time.time()-begin)/(i+1))\n",
        "    start = time.time()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phs2ERy_RmMh"
      },
      "outputs": [],
      "source": [
        "# print(ijepa.student.transformer[0].in_proj.weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jzo9DMDPcOxu"
      },
      "outputs": [],
      "source": [
        "# @title save/load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder='/content/drive/MyDrive/jepa/'\n",
        "\n",
        "# # modelsd, optimsd = torch.load(folder+'SeqJEPA.pkl', map_location=device).values()\n",
        "# modelsd, optimsd = torch.load('SeqJEPA.pkl', map_location=device).values()\n",
        "# seq_jepa.load_state_dict(modelsd, strict=False)\n",
        "# optim.load_state_dict(optimsd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "DNNPOuUmcSNf"
      },
      "outputs": [],
      "source": [
        "checkpoint = {'model': ijepa.state_dict(), 'optimizer': optim.state_dict()}\n",
        "torch.save(checkpoint, folder+'ijepa.pkl')\n",
        "# torch.save(checkpoint, 'IJEPA.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yl-xtHthFl0M"
      },
      "source": [
        "## store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "e7HYQxn6n6iD"
      },
      "outputs": [],
      "source": [
        "# @title GoldenGateRoPE2d\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class GoldenGateRoPE2d(nn.Module): # jerryxio.ng/posts/nd-rope\n",
        "    def __init__(self, image_size, n_heads, n_freqs, min_freq=.8, max_freq=10, n_zero_freqs=0):\n",
        "        super().__init__()\n",
        "        intv = math.pi * (math.sqrt(5)-1)/2 # mod pi instead of 2pi # pi*(sqrt5+-1)/2 ; + and - are equivalent bec mod pi\n",
        "        # intv = math.pi * (math.sqrt(5)-1) # https://en.wikipedia.org/wiki/Golden_angle\n",
        "        speed = torch.cat([torch.zeros(n_zero_freqs), min_freq * (max_freq/min_freq) ** torch.linspace(0,1,n_freqs-n_zero_freqs)]).unsqueeze(-1) # [n_freqs,1] # og\n",
        "        angle = torch.arange(n_heads*n_freqs).reshape(n_heads, n_freqs) * intv # [n_heads, n_freqs]\n",
        "        direction = torch.stack([torch.cos(angle), torch.sin(angle)], dim=-1) # [n_heads, n_freqs, 2]\n",
        "        h, w = image_size\n",
        "        xlim, ylim = w/h, h/w\n",
        "        y, x = torch.meshgrid(torch.linspace(-ylim, ylim, h), torch.linspace(-xlim, xlim, w), indexing=\"ij\") # [h,w], y:row_num, x:col_num\n",
        "        pos = torch.stack([x, y], dim=-1).reshape(-1,1,1,2) # [h*w,1,1,2] cartesian coords\n",
        "        theta = (speed*direction*pos).sum(dim=-1) # [t,n_heads,n_freqs,2]->[t,n_heads,d_head]\n",
        "        self.theta = theta\n",
        "        cos, sin = torch.cos(theta), torch.sin(theta)\n",
        "        self.affine = torch.stack([cos, -sin, sin, cos], dim=-1).transpose(0,1).reshape(1,n_heads,h*w,n_freqs,2,2).to(device) # [t,n_heads,n_freqs,4]->[1,n_heads,t,n_freqs,2,2]\n",
        "\n",
        "    def forward(self, x): # [b,h,t,d]\n",
        "        return (self.affine @ x.unflatten(-1, (-1,2)).unsqueeze(-1)).flatten(-3) # @ [b,h,t,d_head//2,2,1]\n",
        "\n",
        "# /2 better\n",
        "# speed [n_freqs,1] # og best\n",
        "# w/h best\n",
        "# rope < ggrope < learned\n",
        "\n",
        "# image_size=(8,8)\n",
        "image_size=(20,30)\n",
        "# image_size=(90,120)\n",
        "n_heads=4\n",
        "n_freqs=6\n",
        "ggrope = GoldenGateRoPE2d(image_size, n_heads, n_freqs)\n",
        "\n",
        "# x = torch.rand(2, *image_size, n_heads, n_freqs*2)\n",
        "x = torch.rand(2, n_heads, image_size[0]*image_size[1], n_freqs*2)\n",
        "out = ggrope(x)\n",
        "print(out.shape)\n",
        "# # print(out[0])\n",
        "# theta = ggrope.theta.flatten(-2).permute(2,0,1).unsqueeze(1) # [t,n_heads,d_head][b,1,h,w]\n",
        "theta = ggrope.theta.flatten(-2).T.reshape(n_heads*n_freqs, 1, *image_size) # [t,n_heads,d_head]->[d,1,h,w]\n",
        "cy, cx = image_size[0]//2, image_size[1]//2\n",
        "sim = torch.cos(theta-theta[...,cy,cx][...,None,None]) # [b,1,h,w]\n",
        "# sim = sim.unflatten(0, (n_heads, n_freqs)).mean(1)\n",
        "# print(sim.shape)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    print(npimg.shape)\n",
        "    plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "# bhwc\n",
        "\n",
        "import torchvision\n",
        "imshow(torchvision.utils.make_grid(sim, nrow=n_freqs))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "eA2R-JZ7G67P"
      },
      "outputs": [],
      "source": [
        "# @title RoPE pos\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class RoPE(nn.Module):\n",
        "    def __init__(self, dim, seq_len=512, top=torch.pi, base=10000):\n",
        "    # def __init__(self, dim, seq_len=512, min_freq=1, max_freq=400, n_zero_freqs=0):\n",
        "        super().__init__()\n",
        "        self.dim, self.top, self.base = dim, top, base\n",
        "        speed = top / (base ** (torch.arange(0, dim, step=2) / dim)) # [dim//2]\n",
        "        pos = torch.arange(seq_len).unsqueeze(-1) # [t,1]\n",
        "        # speed = torch.cat([torch.zeros(n_zero_freqs), min_freq * (max_freq/min_freq) ** torch.linspace(0,1,dim//2-n_zero_freqs)]) # [dim//2]\n",
        "        # pos = torch.linspace(0, 1, seq_len).unsqueeze(-1) # [t,1]\n",
        "        theta = (speed*pos) # [t,1]*[dim//2]=[t,d//2]\n",
        "        self.theta = theta\n",
        "        cos, sin = torch.cos(theta), torch.sin(theta)\n",
        "        self.affine = torch.stack([cos, -sin, sin, cos], dim=-1).unflatten(-1,(2,2)).to(device)#[None,None,...] # [t,d//2,4]->[t,d//2,2,2] # [1,1,t,d//2,2,2]\n",
        "\n",
        "\n",
        "        # angles = theta[None,...,None] # [seq_len, 1] * [dim//2] -> [1, 1, seq_len, dim//2, 1]\n",
        "        # self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [1, 1, seq_len, dim//2, 2] -> [1, 1, seq_len, dim]\n",
        "        # # self.rot_emb = torch.cat([sin, cos], dim=-1).flatten(-2).to(device) # [1, 1, seq_len, dim//2, 2] -> [1,1,seq_len,dim]\n",
        "        # print('self.rot_emb', self.rot_emb.shape)\n",
        "\n",
        "    def forward(self, x, ind=None): # [b,h,t,d], [b,t]\n",
        "        # seq_len = x.size(-2)\n",
        "        b,_,seq_len,_ = x.shape\n",
        "        if ind!=None: seq_len = max(seq_len, ind.max()+1)\n",
        "        if self.affine.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.top, self.base)\n",
        "        if ind!=None:\n",
        "            # print(\"if affine, x\",self.affine.shape, x.shape)\n",
        "            return (self.affine.unsqueeze(0).expand(b,-1,-1,-1,-1)[torch.arange(b, device=device).unsqueeze(-1), ind].unsqueeze(1) @ x.unflatten(-1, (-1,2)).unsqueeze(-1)).flatten(-3) # @ [b,h,t,d_head//2,2,1]\n",
        "        else:\n",
        "            # print(\"else affine, x\",self.affine.shape, x.shape) # [64, 4, 2, 2], [64, 8, 10, 8]\n",
        "            return (self.affine[None,None,...] @ x.unflatten(-1, (-1,2)).unsqueeze(-1)).flatten(-3) # @ [b,h,t,d_head//2,2,1]\n",
        "\n",
        "\n",
        "        # # if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.base)\n",
        "        # if ind==None:\n",
        "        #     # print(\"if rot, x\",self.rot_emb.shape, x.shape)\n",
        "        #     return x * self.rot_emb.unsqueeze(1)#[None,None,...]\n",
        "        # else:\n",
        "        #     return x * (self.rot_emb.expand(b,-1,-1)[torch.arange(b, device=device).unsqueeze(-1), ind].unsqueeze(1))\n",
        "\n",
        "\n",
        "\n",
        "dim=64\n",
        "seq_len=56\n",
        "rope = RoPE(dim, seq_len, top=torch.pi, base=100)\n",
        "# rope = RoPE(dim, seq_len, min_freq=1, max_freq=200, n_zero_freqs=0)\n",
        "\n",
        "batch=2\n",
        "t=50\n",
        "x = torch.rand(batch, 4, seq_len, dim, device=device)\n",
        "out = rope(x)\n",
        "print(\"out1\", out.shape)\n",
        "x = torch.rand(batch, 4, t, dim, device=device)\n",
        "pos = torch.randint(0,seq_len,(batch,t), device=device)\n",
        "out = rope(x, pos)\n",
        "print(\"out2\", out.shape)\n",
        "\n",
        "# theta = rope.theta # [t,d//2]\n",
        "# sim = torch.cos(theta-theta[0].unsqueeze(0)).T\n",
        "# # print(sim.shape)\n",
        "\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# def imshow(img):\n",
        "#     npimg = img.numpy()\n",
        "#     print(npimg.shape)\n",
        "#     plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "#     plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "#     plt.show()\n",
        "\n",
        "# import torchvision\n",
        "# imshow(torchvision.utils.make_grid(sim, nrow=dim//2))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "k3nfeLM4wtkJ"
      },
      "outputs": [],
      "source": [
        "# @title random_masking\n",
        "import torch\n",
        "\n",
        "def random_masking(length, mask_ratio, b=64):\n",
        "    noise = torch.rand(b, length)\n",
        "    len_mask = int(length * mask_ratio)\n",
        "    _, msk_ind = torch.topk(noise, k=len_mask, dim=-1, sorted=False) # val, ind -> [b,len_mask]\n",
        "    _, keep_ind = torch.topk(noise, k=length-len_mask, largest=False, dim=-1, sorted=False) # val, ind -> [b,len_keep]\n",
        "    return msk_ind, keep_ind\n",
        "\n",
        "# msk_ind, keep_ind = random_masking(10, .3, b=2)\n",
        "\n",
        "# x_ = torch.rand(4, 3, 2)\n",
        "# print(x_)\n",
        "# # ids = torch.tensor([0, 2, 1])[None,:,None]\n",
        "# # ids = torch.tensor([0, 2, 1])[None,:,None].repeat(4,1,2)\n",
        "# ids = torch.tensor([1, 2, 0])[None,:,None].repeat(4,1,2)\n",
        "# # o = torch.gather(x_, dim=1, index=ids)\n",
        "# o = torch.zeros_like(x_).scatter_(dim=1, index=ids, src=x_)\n",
        "# print(o)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "62UPGVucmGNe"
      },
      "outputs": [],
      "source": [
        "# @title AttentionBlock pos\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "# import inspect\n",
        "# class Seq(nn.Sequential):\n",
        "#     def __init__(self, *args):\n",
        "#         super().__init__(*args)\n",
        "#         for layer in self:\n",
        "#             params = inspect.signature(layer.forward).parameters.keys()\n",
        "#             layer._fwdparams = ','.join(params)\n",
        "\n",
        "#     def forward(self, x, pos=None, masks=None):\n",
        "#         arg_map = {'pos':pos, 'masks':masks}\n",
        "#         for layer in self:\n",
        "#             args = [x]\n",
        "#             # if 'masks' in layer._fwdparams: args.append(masks)\n",
        "#             args.extend(arg_map[p] for p in arg_map if p in layer._fwdparams)\n",
        "#             # print(layer._fwdparams, args)\n",
        "#             x = layer(*args)\n",
        "#         return x\n",
        "\n",
        "# import inspect\n",
        "# class Seq(nn.Sequential):\n",
        "#     def __init__(self, *args):\n",
        "#         super().__init__(*args)\n",
        "#         # for layer in self:\n",
        "#         #     params = inspect.signature(layer.forward).parameters.keys()\n",
        "#         #     layer._fwdparams = ','.join(params)\n",
        "\n",
        "#     # def forward(self, x, pos=None, masks=None):\n",
        "#     def forward(self, x, *args, **kwargs):\n",
        "#         # arg_map = {'pos':pos, 'masks':masks}\n",
        "#         for layer in self:\n",
        "#             # args = [x]\n",
        "#             # if 'masks' in layer._fwdparams: args.append(masks)\n",
        "#             # args.extend(arg_map[p] for p in arg_map if p in layer._fwdparams)\n",
        "#             # print(layer._fwdparams, args)\n",
        "#             x = layer(x, *args)\n",
        "#         return x\n",
        "\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        self._kwargs = [[name for name, p in inspect.signature(layer.forward).parameters.items()] for layer in self]\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        for layer, _kwargs in zip(self, self._kwargs):\n",
        "            x = layer(x, *args, **{k: v for k, v in kwargs.items() if k in _kwargs})\n",
        "        return x\n",
        "\n",
        "class SelfAttn(nn.Module):\n",
        "    def __init__(self, dim, n_heads):\n",
        "        super().__init__()\n",
        "        self.dim, self.n_heads = dim, n_heads\n",
        "        d_head = dim//n_heads\n",
        "        self.rope = RoPE(d_head, seq_len=64, top=torch.pi, base=1000)\n",
        "        # self.rope = RoPE(d_head, seq_len=512, base=1000)\n",
        "        # self.rope = RoPE2D(d_head, h=64, w=64, base=100)\n",
        "        # self.rope[0] = 1 # id for smry h\n",
        "        self.qkv = nn.Linear(dim, dim*3, bias=False)\n",
        "        # self.lin = nn.Linear(dim, dim)\n",
        "        self.lin = zero_module(nn.Linear(dim, dim))\n",
        "        self.scale = d_head**-.5\n",
        "\n",
        "    # def forward(self, x): # [b,t,d]\n",
        "    def forward(self, x, pos=None): # [b,t,d]\n",
        "        # print('SelfAttn', x.shape)\n",
        "        q,k,v = self.qkv(x).unflatten(-1, (self.n_heads,-1)).transpose(1,2).chunk(3, dim=-1) # [b, t, n_heads, d_head] -> [b, n_heads, t, d_head]\n",
        "        if pos==None: q, k = self.rope(q), self.rope(k)\n",
        "        else:\n",
        "            # print('SelfAttn fwd', x.shape, pos.shape)\n",
        "            q, k = self.rope(q, pos), self.rope(k, pos)\n",
        "\n",
        "        x = F.scaled_dot_product_attention(q,k,v, attn_mask=None) # https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html\n",
        "        # q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2)\n",
        "        # context = k.transpose(-2,-1) @ v # [batch, n_heads, d_head, d_head]\n",
        "        # x = q @ context # [b,n_heads,t,d_head]\n",
        "        x = x.transpose(1,2).flatten(2)\n",
        "        return self.lin(x)\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, mult=4, drop=0.):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        # self.norm = nn.RMSNorm(d_model, elementwise_affine=False) # LayerNorm RMSNorm\n",
        "        self.norm1, self.norm2 = nn.LayerNorm(d_model), nn.LayerNorm(d_model)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "        self.attn = SelfAttn(d_model, n_heads)\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        ff_dim=d_model*mult\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), act, nn.Dropout(drop), nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "            nn.RMSNorm(ff_dim), act, nn.Dropout(drop), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            # nn.RMSNorm(d_model), nn.Dropout(drop), nn.Linear(d_model, ff_dim), act,\n",
        "            # nn.RMSNorm(ff_dim), nn.Dropout(drop), nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "\n",
        "    # def forward(self, x): # [b,t,d]\n",
        "    def forward(self, x, pos=None): # [b,t,d]\n",
        "        # print('attnblk fwd',x.shape)\n",
        "        # x = x + self.drop(self.attn(self.norm1(x)))\n",
        "        x = x + self.drop(self.attn(self.norm1(x), pos))\n",
        "        x = x + self.ff(x)\n",
        "        # x = x + self.drop(self.ff(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, nhead=8, d_hid=None, nlayers=1, dropout = 0.):\n",
        "    def __init__(self, patch_size, in_dim, d_model, out_dim=None, n_heads=4, nlayers=1, drop=0.):\n",
        "        super().__init__()\n",
        "        patch_size=2\n",
        "        self.embed = nn.Sequential(\n",
        "            # nn.Conv2d(in_dim, d_model, patch_size, patch_size), # like patch\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 1, 7//2, bias=False)\n",
        "            # nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.MaxPool2d(3,2,1)\n",
        "            nn.Conv2d(in_dim, d_model, 7, 2, 7//2, bias=False), nn.BatchNorm2d(d_model), nn.ReLU(),\n",
        "            nn.Conv2d(d_model, d_model, 3, 2, 3//2, bias=False)\n",
        "            )\n",
        "        # self.embed.requires_grad=False\n",
        "        # self.pos_enc = RotEmb(d_model, top=1, base=10000)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, 32*32, d_model)*.02)\n",
        "        # self.pos_emb = nn.Parameter(RoPE2D(dim=d_model, h=8, w=8, base=1000), requires_grad=False)\n",
        "\n",
        "        # self.pos_emb = nn.Parameter(RoPE(d_model, seq_len=(32//patch_size)**2, base=10000), requires_grad=False)\n",
        "        # self.transformer = nn.Sequential(*[AttentionBlock(d_model, d_model, n_heads) for _ in range(nlayers)])\n",
        "        self.transformer = Seq(*[AttentionBlock(d_model, n_heads) for _ in range(nlayers)])\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, cxt_inds=None): # [batch, num_context_toks, 3], [batch, num_context_toks] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        x = self.embed(x).flatten(2).transpose(1,2) # [b,c,h,w]->[b,h*w,c] # [batch, seq_len, d_model] or [batch, num_context_toks, d_model]\n",
        "        # x = self.pos_enc(x)\n",
        "        # print(\"TransformerModel\",x.shape, self.pos_emb.shape)\n",
        "        # print(\"TransformerModel\",x.shape)\n",
        "        # x = x + self.pos_emb[:,:x.shape[1]]\n",
        "        # if cxt_inds != None: x = x[torch.arange(x.shape[0]).unsqueeze(-1), cxt_inds] # [batch, num_context_toks, d_model]\n",
        "        # x = self.transformer(x)\n",
        "\n",
        "        if cxt_inds != None:\n",
        "            # print('vit transformer',x[torch.arange(x.shape[0]).unsqueeze(-1), cxt_inds].shape, cxt_inds.shape)\n",
        "            x = self.transformer(x[torch.arange(x.shape[0]).unsqueeze(-1), cxt_inds], cxt_inds)\n",
        "        else: x = self.transformer(x)\n",
        "\n",
        "        out = self.norm(x)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out\n",
        "\n",
        "d_model = 64\n",
        "in_dim = 3\n",
        "patch_size = 2\n",
        "# model = ViT(patch_size, in_dim, d_model, n_heads=4, nlayers=4, drop=0.).to(device)\n",
        "model = ViT(patch_size, in_dim, d_model, n_heads=4, nlayers=1, drop=0.).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 27584\n",
        "x = torch.rand((5, in_dim, 32, 32), device=device) # [b,c,h,w]\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "OgZ6d59vOQil"
      },
      "outputs": [],
      "source": [
        "# @title test seq\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        # for layer in self:\n",
        "        #     params = inspect.signature(layer.forward).parameters.keys()\n",
        "        #     layer._fwdparams = ','.join(params)\n",
        "\n",
        "    # def forward(self, x, pos=None, masks=None):\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        # arg_map = {'pos':pos, 'masks':masks}\n",
        "        for layer in self:\n",
        "            # args = [x]\n",
        "            # if 'masks' in layer._fwdparams: args.append(masks)\n",
        "            # args.extend(arg_map[p] for p in arg_map if p in layer._fwdparams)\n",
        "            # print(layer._fwdparams, args)\n",
        "            x = layer(x, *args, **kwargs)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        self._accepted_kwargs = []\n",
        "        for layer in self:\n",
        "            sig = inspect.signature(layer.forward)\n",
        "            print(sig.parameters.items())\n",
        "            self._accepted_kwargs.append(\n",
        "                [name for name, p in sig.parameters.items()]\n",
        "                # {name for name, p in sig.parameters.items()\n",
        "                #  if p.kind in (p.POSITIONAL_OR_KEYWORD, p.KEYWORD_ONLY)}\n",
        "            )\n",
        "\n",
        "            # if any(p.kind == p.VAR_KEYWORD for p in sig.parameters.values()):\n",
        "            #     accepted = None  # means \"pass everything\"\n",
        "\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        # for layer in self:\n",
        "        #     x = layer(x, *args, **kwargs)\n",
        "\n",
        "        # for layer in self:\n",
        "        #     sig = inspect.signature(layer.forward)\n",
        "        #     # keep only accepted keyword arguments\n",
        "        #     filtered_kwargs = {\n",
        "        #         k: v for k, v in kwargs.items()\n",
        "        #         if k in sig.parameters\n",
        "        #     }\n",
        "        #     x = layer(x, *args, **filtered_kwargs)\n",
        "\n",
        "        for layer, accepted in zip(self, self._accepted_kwargs):\n",
        "            filtered_kwargs = {k: v for k, v in kwargs.items() if k in accepted}\n",
        "            x = layer(x, *args, **filtered_kwargs)\n",
        "        # if accepted is None:\n",
        "        #     x = layer(x, *args, **kwargs)\n",
        "        # else:\n",
        "        #     filtered = {k: v for k, v in kwargs.items() if k in accepted}\n",
        "        #     x = layer(x, *args, **filtered)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        self._kwargs = [[name for name, p in inspect.signature(layer.forward).parameters.items()] for layer in self]\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        for layer, _kwargs in zip(self, self._kwargs):\n",
        "            x = layer(x, *args, **{k: v for k, v in kwargs.items() if k in _kwargs})\n",
        "        return x\n",
        "\n",
        "\n",
        "class SS(nn.Module):\n",
        "    def __init__(self, d):\n",
        "        super().__init__()\n",
        "        self.d = d\n",
        "    def forward(self, x, y, pos=None, bos=None):\n",
        "        # print('SS', x, pos, bos)\n",
        "        print('SS', x, y, pos, bos)\n",
        "        return x+1\n",
        "\n",
        "class PP(nn.Module):\n",
        "    def __init__(self, d):\n",
        "        super().__init__()\n",
        "        self.d = d\n",
        "    def forward(self, x, pos=None, dos=None):\n",
        "        print('PP', x, pos, dos)\n",
        "    # def forward(self, x, y, pos=None, dos=None):\n",
        "        # print('PP', x, y, pos, dos)\n",
        "        return x+1\n",
        "\n",
        "d=2\n",
        "# TT = Seq(*[SS(d) for _ in range(2)])\n",
        "TT = Seq(*[SS(d), PP(d)])\n",
        "# out = TT(3, a=2, pos='p')\n",
        "# out = TT(3, a=2, bos='b')\n",
        "# out = TT(3, a=2, dos='d')\n",
        "out = TT(3, 'q','w')\n",
        "# out = TT(3, 'q','w', pos=2, dos='d')\n",
        "# out = TT(3, 'q', pos=2, dos='d')\n",
        "print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4J2ahp3wmqcg"
      },
      "outputs": [],
      "source": [
        "# @title supervised train test\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "# def strain(model, dataloader, optim, scheduler=None):\n",
        "def strain(model, classifier, dataloader, optim, coptim, scheduler=None):\n",
        "    model.train()\n",
        "    classifier.train()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x = x.to(device)#.to(torch.bfloat16) # [b,c,h,w] -> [b,h*w,c]\n",
        "        y = y.to(device)\n",
        "\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "            loss = F.cross_entropy(y_, y)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        # print(loss)\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # for p in list(filter(lambda p: p.grad is not None, model.parameters())):\n",
        "        #     print(p.grad.data.norm(2).item())\n",
        "        # print(\"max grad norm\", max([p.grad.data.norm(2).item() for p in list(filter(lambda p: p.grad is not None, model.parameters()))]))\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 10) # 0.5\n",
        "\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        print(\"strain\",loss.item())\n",
        "        # for param in ijepa.student.cls: print(param.data)\n",
        "        # for param in ijepa.predicter.cls: print(param.data)\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=50: break\n",
        "\n",
        "\n",
        "# def test(model, dataloader):\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        # .to(torch.bfloat16)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            sx = model(x)\n",
        "            y_ = classifier(sx)\n",
        "        loss = F.cross_entropy(y_, y)\n",
        "        correct = (y==y_.argmax(dim=1)).sum().item()\n",
        "        print(correct/len(y))\n",
        "        try: wandb.log({\"correct\": correct/len(y), \"closs\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i>=10: break\n",
        "\n",
        "\n",
        "for i in range(1000):\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # np.random.shuffle(train_indices); np.random.shuffle(val_indices)\n",
        "    # train_sampler, valid_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(val_indices)\n",
        "    # # batch_size = 64 #512\n",
        "    # train_loader = DataLoader(train_data, sampler=train_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "    # test_loader = DataLoader(train_data, sampler=valid_sampler, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True)\n",
        "\n",
        "    # strain(ijepa, train_loader, optim)\n",
        "    strain(ijepa, classifier, train_loader, optim, coptim)\n",
        "    test(ijepa, classifier, test_loader)\n",
        "\n",
        "    # strain(violet, train_loader, voptim)\n",
        "    # test(violet, test_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e1Yi9JrLPLd"
      },
      "source": [
        "## trash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KUvHrTi4LSEe"
      },
      "outputs": [],
      "source": [
        "# @title GoldenGateRoPE2d\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class GoldenGateRoPE2d(nn.Module): # jerryxio.ng/posts/nd-rope\n",
        "    def __init__(self, image_size: tuple[int, int], n_heads, n_freqs):\n",
        "        super().__init__()\n",
        "        n_zero_freqs = 1\n",
        "        min_freq, max_freq = .2, 20\n",
        "        speed = torch.cat([torch.zeros(n_zero_freqs), min_freq * (max_freq/min_freq) ** torch.linspace(0,1,n_freqs-n_zero_freqs)]) # [n_freqs]\n",
        "        direction_spacing = math.pi * (math.sqrt(5)-1)/2\n",
        "        phi = torch.arange(n_heads * n_freqs).reshape(n_heads, n_freqs) * direction_spacing # [n_heads, n_freqs]\n",
        "        directions = torch.stack((torch.cos(phi), torch.sin(phi)), dim=-1) # [n_heads, n_freqs, 2]\n",
        "        # print('directions', directions)\n",
        "        vel = speed.unsqueeze(-1) * directions # speed in direction[n_heads, n_freqs, 2]\n",
        "\n",
        "        H, W = image_size\n",
        "        xlim, ylim = math.sqrt(W / H), math.sqrt(H / W)\n",
        "        print(xlim, ylim)\n",
        "        y, x = torch.meshgrid(torch.linspace(-ylim, ylim, H), torch.linspace(-xlim, xlim, W), indexing=\"ij\") # [h,w], y:row_num, x:col_num\n",
        "        # y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\") # print(y,x) # [h,w], y:row_num, x:col_num\n",
        "\n",
        "        pos = torch.stack((x, y), dim=-1).reshape(H, W, 1, 1, 2) # cartesian coords\n",
        "\n",
        "        theta = (vel*pos).sum(dim=-1) # [h,w,n_heads,n_freqs,2]->[h,w,n_heads,d_head]\n",
        "        print('theta', theta.shape)\n",
        "        self.cos, self.sin = torch.cos(theta), torch.sin(theta)\n",
        "        print('self.cos', self.cos.shape)\n",
        "\n",
        "    def forward(self, input): # [b,h,w,n_head,d_head]\n",
        "        # x, y = input.float().chunk(2, dim=-1) # [b,h,w,n_head,n_freqs]\n",
        "        x, y = input.float().unflatten(-1, (-1,2)).chunk(2, dim=-1)\n",
        "        x, y = x.squeeze(-1), y.squeeze(-1)\n",
        "        x_out = x * self.cos - y * self.sin\n",
        "        y_out = x * self.sin + y * self.cos\n",
        "        # output = torch.cat((x_out, y_out), dim=-1)\n",
        "        output = torch.stack((x_out, y_out), dim=-1).flatten(-2)\n",
        "        return output.type_as(input)\n",
        "\n",
        "\n",
        "\n",
        "class GoldenGateRoPE2d2(nn.Module): # jerryxio.ng/posts/nd-rope\n",
        "    def __init__(self, image_size, n_heads, n_freqs):\n",
        "        super().__init__()\n",
        "        n_zero_freqs = 1\n",
        "        min_freq, max_freq = .2, 20\n",
        "        intv = math.pi * (math.sqrt(5)-1)/2\n",
        "        speed = torch.cat([torch.zeros(n_zero_freqs), min_freq * (max_freq/min_freq) ** torch.linspace(0,1,n_freqs-n_zero_freqs)]) # [n_freqs]\n",
        "        phi = torch.arange(n_heads*n_freqs).reshape(n_heads, n_freqs) * intv # [n_heads, n_freqs]\n",
        "        direction = torch.stack((torch.cos(phi), torch.sin(phi)), dim=-1) # [n_heads, n_freqs, 2]\n",
        "        vel = speed.unsqueeze(-1) * direction # speed in direction[n_heads, n_freqs, 2]\n",
        "        h, w = image_size\n",
        "        xlim, ylim = math.sqrt(w/h), math.sqrt(h/w)\n",
        "        y, x = torch.meshgrid(torch.linspace(-ylim, ylim, h), torch.linspace(-xlim, xlim, w), indexing=\"ij\") # [h,w], y:row_num, x:col_num\n",
        "        pos = torch.stack([x, y], dim=-1)[...,None,None,:] # [h,w,1,1,2] cartesian coords\n",
        "        theta = (vel*pos).sum(dim=-1) # [h,w,n_heads,n_freqs,2]->[h,w,n_heads,d_head]\n",
        "        cos, sin = torch.cos(theta), torch.sin(theta)\n",
        "        self.affine = torch.stack([cos, -sin, sin, cos], dim=-1).unflatten(-1, (2,2))\n",
        "\n",
        "    def forward(self, input): # [b,h,w,n_head,d_head]\n",
        "        return (self.affine @ input.unflatten(-1, (-1,2)).unsqueeze(-1)).flatten(-3)\n",
        "\n",
        "\n",
        "\n",
        "image_size=(5,7)\n",
        "n_heads=4\n",
        "d_head=16\n",
        "ggrope = GoldenGateRoPE2d(image_size, n_heads, d_head//2)\n",
        "ggrope2 = GoldenGateRoPE2d2(image_size, n_heads, d_head//2)\n",
        "\n",
        "# x = torch.rand(2, *image_size, n_heads, d_head)\n",
        "out = ggrope(x)\n",
        "out2 = ggrope2(x)\n",
        "print(out.shape)\n",
        "print(out2.shape)\n",
        "# print(out[0])\n",
        "# print(out2[0])\n",
        "# print((out==out2)[0])\n",
        "print((out==out2).all())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "K6saOuAX-FV8"
      },
      "outputs": [],
      "source": [
        "# @title test gather\n",
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "l=56\n",
        "b,t,d = 2,50,64\n",
        "src = torch.rand(b,l,d, device=device)\n",
        "idx = torch.randint(0,l,(b,t), device=device)\n",
        "out = src[torch.arange(b).unsqueeze(-1), idx]\n",
        "# out = torch.take_along_dim(src, idx.unsqueeze(-1).expand(-1,-1,d), dim=1)\n",
        "# out = src.index_select(0, idx).reshape(b, t, d) # == src[idx]\n",
        "# out = src.gather(1, idx.unsqueeze(-1).expand(-1, -1, d))\n",
        "print(out.shape)\n",
        "\n",
        "# %timeit out = src.expand(b,-1,-1).gather(1, idx.unsqueeze(-1).expand(-1, -1, src.size(-1))) # 2.53 ms\n",
        "# %timeit out = src.expand(b,-1,-1)[torch.arange(b, device=device).unsqueeze(-1), idx] # 1.39 ms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNt7SRjj9g_l"
      },
      "outputs": [],
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "1e1Yi9JrLPLd"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}