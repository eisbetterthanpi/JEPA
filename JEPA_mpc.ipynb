{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDVz4+DIr1aoqvMxYpFi1o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/JEPA_mpc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsXCi49JNzX-"
      },
      "source": [
        "## mpc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxrfE91Cy820"
      },
      "source": [
        "### locuslab_mpc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jf-o1_yAjBsN"
      },
      "outputs": [],
      "source": [
        "# https://github.com/locuslab/mpc.pytorch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhvqvtdaj8E-",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title util\n",
        "# https://github.com/locuslab/mpc.pytorch/blob/master/mpc/util.py\n",
        "import torch\n",
        "from torch.autograd import Function, Variable\n",
        "from torch.nn import Module\n",
        "from torch.nn.parameter import Parameter\n",
        "import operator\n",
        "\n",
        "def jacobian(f, x, eps):\n",
        "    if x.ndimension() == 2:\n",
        "        assert x.size(0) == 1\n",
        "        x = x.squeeze()\n",
        "    e = Variable(torch.eye(len(x)).type_as(get_data_maybe(x)))\n",
        "    J = []\n",
        "    for i in range(len(x)):\n",
        "        J.append((f(x + eps*e[i]) - f(x - eps*e[i]))/(2.*eps))\n",
        "    J = torch.stack(J).transpose(0,1)\n",
        "    return J\n",
        "\n",
        "def expandParam(X, n_batch, nDim):\n",
        "    if X.ndimension() in (0, nDim):\n",
        "        return X, False\n",
        "    elif X.ndimension() == nDim - 1:\n",
        "        return X.unsqueeze(0).expand(*([n_batch] + list(X.size()))), True\n",
        "    else:\n",
        "        raise RuntimeError(\"Unexpected number of dimensions.\")\n",
        "\n",
        "def bdiag(d):\n",
        "    assert d.ndimension() == 2\n",
        "    nBatch, sz = d.size()\n",
        "    dtype = d.type() if not isinstance(d, Variable) else d.data.type()\n",
        "    D = torch.zeros(nBatch, sz, sz).type(dtype)\n",
        "    I = torch.eye(sz).repeat(nBatch, 1, 1).type(dtype).byte()\n",
        "    D[I] = d.view(-1)\n",
        "    return D\n",
        "\n",
        "def bger(x, y):\n",
        "    return x.unsqueeze(2).bmm(y.unsqueeze(1))\n",
        "\n",
        "def bmv(X, y):\n",
        "    return X.bmm(y.unsqueeze(2)).squeeze(2)\n",
        "\n",
        "def bquad(x, Q):\n",
        "    return x.unsqueeze(1).bmm(Q).bmm(x.unsqueeze(2)).squeeze(1).squeeze(1)\n",
        "\n",
        "def bdot(x, y):\n",
        "    return torch.bmm(x.unsqueeze(1), y.unsqueeze(2)).squeeze(1).squeeze(1)\n",
        "\n",
        "def eclamp(x, lower, upper):\n",
        "    # In-place!!\n",
        "    if type(lower) == type(x):\n",
        "        assert x.size() == lower.size()\n",
        "    if type(upper) == type(x):\n",
        "        assert x.size() == upper.size()\n",
        "    I = x < lower\n",
        "    x[I] = lower[I] if not isinstance(lower, float) else lower\n",
        "    I = x > upper\n",
        "    x[I] = upper[I] if not isinstance(upper, float) else upper\n",
        "    return x\n",
        "\n",
        "def get_data_maybe(x):\n",
        "    return x if not isinstance(x, Variable) else x.data\n",
        "\n",
        "_seen_tables = []\n",
        "def table_log(tag, d):\n",
        "    # TODO: There's probably a better way to handle formatting here, or a better way altogether to replace this quick hack.\n",
        "    global _seen_tables\n",
        "    def print_row(r):\n",
        "        print('| ' + ' | '.join(r) + ' |')\n",
        "    if tag not in _seen_tables:\n",
        "        print_row(map(operator.itemgetter(0), d))\n",
        "        _seen_tables.append(tag)\n",
        "    s = []\n",
        "    for di in d:\n",
        "        assert len(di) in [2,3]\n",
        "        if len(di) == 3:\n",
        "            e, fmt = di[1:]\n",
        "            s.append(fmt.format(e))\n",
        "        else:\n",
        "            e = di[1]\n",
        "            s.append(str(e))\n",
        "    print_row(s)\n",
        "\n",
        "def get_traj(T, u, x_init, dynamics):\n",
        "    if isinstance(dynamics, LinDx):\n",
        "        F = get_data_maybe(dynamics.F)\n",
        "        f = get_data_maybe(dynamics.f)\n",
        "        if f is not None:\n",
        "            assert f.shape == F.shape[:3]\n",
        "    x = [get_data_maybe(x_init)]\n",
        "    for t in range(T):\n",
        "        xt = x[t]\n",
        "        ut = get_data_maybe(u[t])\n",
        "        if t < T-1:\n",
        "            # new_x = f(Variable(xt), Variable(ut)).data\n",
        "            if isinstance(dynamics, LinDx):\n",
        "                xut = torch.cat((xt, ut), 1)\n",
        "                new_x = bmv(F[t], xut)\n",
        "                if f is not None:\n",
        "                    new_x += f[t]\n",
        "            else:\n",
        "                new_x = dynamics(Variable(xt), Variable(ut)).data\n",
        "            x.append(new_x)\n",
        "    x = torch.stack(x, dim=0)\n",
        "    return x\n",
        "\n",
        "def get_cost(T, u, cost, dynamics=None, x_init=None, x=None):\n",
        "    assert x_init is not None or x is not None\n",
        "    if isinstance(cost, QuadCost):\n",
        "        C = get_data_maybe(cost.C)\n",
        "        c = get_data_maybe(cost.c)\n",
        "    if x is None:\n",
        "        x = get_traj(T, u, x_init, dynamics)\n",
        "    objs = []\n",
        "    for t in range(T):\n",
        "        xt = x[t]\n",
        "        ut = u[t]\n",
        "        xut = torch.cat((xt, ut), 1)\n",
        "        if isinstance(cost, QuadCost):\n",
        "            obj = 0.5*bquad(xut, C[t]) + bdot(xut, c[t])\n",
        "        else:\n",
        "            obj = cost(xut)\n",
        "        objs.append(obj)\n",
        "    objs = torch.stack(objs, dim=0)\n",
        "    total_obj = torch.sum(objs, dim=0)\n",
        "    return total_obj\n",
        "\n",
        "def detach_maybe(x):\n",
        "    if x is None:\n",
        "        return None\n",
        "    return x if not x.requires_grad else x.detach()\n",
        "\n",
        "def data_maybe(x):\n",
        "    if x is None:\n",
        "        return None\n",
        "    return x.data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDr01NA8jxQ8",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title pnqp\n",
        "# https://github.com/locuslab/mpc.pytorch/blob/master/mpc/pnqp.py\n",
        "import torch\n",
        "\n",
        "# \\@profile\n",
        "def pnqp(H, q, lower, upper, x_init=None, n_iter=20):\n",
        "    GAMMA = 0.1\n",
        "    n_batch, n, _ = H.size()\n",
        "    pnqp_I = 1e-11*torch.eye(n).type_as(H).expand_as(H)\n",
        "    def obj(x):\n",
        "        return 0.5*bquad(x, H) + bdot(q, x)\n",
        "    if x_init is None:\n",
        "        if n == 1:\n",
        "            x_init = -(1./H.squeeze(2))*q\n",
        "        else:\n",
        "            H_lu = H.lu()\n",
        "            x_init = -q.unsqueeze(2).lu_solve(*H_lu).squeeze(2) # Clamped in the x assignment.\n",
        "    else:\n",
        "        x_init = x_init.clone() # Don't over-write the original x_init.\n",
        "    x = eclamp(x_init, lower, upper)\n",
        "    # Active examples in the batch.\n",
        "    J = torch.ones(n_batch).type_as(x).byte()\n",
        "    for i in range(n_iter):\n",
        "        g = bmv(H, x) + q\n",
        "        # TODO: Could clean up the types here.\n",
        "        Ic = (((x == lower) & (g > 0)) | ((x == upper) & (g < 0))).float()\n",
        "        If = 1-Ic\n",
        "        if If.is_cuda:\n",
        "            Hff_I = bger(If.float(), If.float()).type_as(If)\n",
        "            not_Hff_I = 1-Hff_I\n",
        "            Hfc_I = bger(If.float(), Ic.float()).type_as(If)\n",
        "        else:\n",
        "            Hff_I = bger(If, If)\n",
        "            not_Hff_I = 1-Hff_I\n",
        "            Hfc_I = bger(If, Ic)\n",
        "        g_ = g.clone()\n",
        "        g_[Ic.bool()] = 0.\n",
        "        H_ = H.clone()\n",
        "        H_[not_Hff_I.bool()] = 0.0\n",
        "        H_ += pnqp_I\n",
        "        if n == 1:\n",
        "            dx = -(1./H_.squeeze(2))*g_\n",
        "        else:\n",
        "            H_lu_ = H_.lu()\n",
        "            dx = -g_.unsqueeze(2).lu_solve(*H_lu_).squeeze(2)\n",
        "        J = torch.norm(dx, 2, 1) >= 1e-4\n",
        "        m = J.sum().item() # Number of active examples in the batch.\n",
        "        if m == 0:\n",
        "            return x, H_ if n == 1 else H_lu_, If, i\n",
        "        alpha = torch.ones(n_batch).type_as(x)\n",
        "        decay = 0.1\n",
        "        max_armijo = GAMMA\n",
        "        count = 0\n",
        "        while max_armijo <= GAMMA and count < 10:\n",
        "            # Crude way of making sure too much time isn't being spent doing the line search.\n",
        "            # assert count < 10\n",
        "            maybe_x = eclamp(x+torch.diag(alpha).mm(dx), lower, upper)\n",
        "            armijos = (GAMMA+1e-6)*torch.ones(n_batch).type_as(x)\n",
        "            armijos[J] = (obj(x)-obj(maybe_x))[J]/bdot(g, x-maybe_x)[J]\n",
        "            I = armijos <= GAMMA\n",
        "            alpha[I] *= decay\n",
        "            max_armijo = torch.max(armijos)\n",
        "            count += 1\n",
        "        x = maybe_x\n",
        "    # TODO: Maybe change this to a warning.\n",
        "    print(\"[WARNING] pnqp warning: Did not converge\")\n",
        "    return x, H_ if n == 1 else H_lu_, If, i\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0_0b9x8jmpa",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title dynamics\n",
        "# https://github.com/locuslab/mpc.pytorch/blob/master/mpc/dynamics.py\n",
        "import torch\n",
        "from torch.autograd import Function, Variable\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "ACTS = {\n",
        "    'sigmoid': torch.sigmoid,\n",
        "    'relu': F.relu,\n",
        "    'elu': F.elu,\n",
        "}\n",
        "\n",
        "class NNDynamics(nn.Module):\n",
        "    def __init__(self, n_state, n_ctrl, hidden_sizes=[100], activation='sigmoid', passthrough=True):\n",
        "        super().__init__()\n",
        "        self.passthrough = passthrough\n",
        "        self.fcs = []\n",
        "        in_sz = n_state+n_ctrl\n",
        "        for out_sz in hidden_sizes + [n_state]:\n",
        "            fc = nn.Linear(in_sz, out_sz)\n",
        "            self.fcs.append(fc)\n",
        "            in_sz = out_sz\n",
        "        self.fcs = nn.ModuleList(self.fcs)\n",
        "        assert activation in ACTS.keys()\n",
        "        act_f = ACTS[activation]\n",
        "        self.activation = activation\n",
        "        self.acts = [act_f]*(len(self.fcs)-1)+[lambda x:x] # Activation functions.\n",
        "        self.Ws = [y.weight for y in self.fcs]\n",
        "        self.zs = [] # Activations.\n",
        "\n",
        "    def __getstate__(self):\n",
        "        return (self.fcs, self.activation, self.passthrough)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super().__init__()\n",
        "        if len(state) == 2:\n",
        "            # TODO: Remove this soon, keeping for some old models.\n",
        "            self.fcs, self.activation = state\n",
        "            self.passthrough = True\n",
        "        else:\n",
        "            self.fcs, self.activation, self.passthrough = state\n",
        "        act_f = ACTS[self.activation]\n",
        "        self.acts = [act_f]*(len(self.fcs)-1)+[lambda x:x] # Activation functions.\n",
        "        self.Ws = [y.weight for y in self.fcs]\n",
        "\n",
        "    def forward(self, x, u):\n",
        "        x_dim, u_dim = x.ndimension(), u.ndimension()\n",
        "        if x_dim == 1:\n",
        "            x = x.unsqueeze(0)\n",
        "        if u_dim == 1:\n",
        "            u = u.unsqueeze(0)\n",
        "        self.zs = []\n",
        "        z = torch.cat((x, u), 1)\n",
        "        for act, fc in zip(self.acts, self.fcs):\n",
        "            z = act(fc(z))\n",
        "            self.zs.append(z)\n",
        "        # Hack: Don't include the output.\n",
        "        self.zs = self.zs[:-1]\n",
        "        if self.passthrough:\n",
        "            z += x\n",
        "        if x_dim == 1:\n",
        "            z = z.squeeze(0)\n",
        "        return z\n",
        "\n",
        "    def grad_input(self, x, u):\n",
        "        assert isinstance(x, Variable) == isinstance(u, Variable)\n",
        "        diff = isinstance(x, Variable)\n",
        "        x_dim, u_dim = x.ndimension(), u.ndimension()\n",
        "        n_batch, n_state = x.size()\n",
        "        _, n_ctrl = u.size()\n",
        "        if not diff:\n",
        "            Ws = [W.data for W in self.Ws]\n",
        "            zs = [z.data for z in self.zs]\n",
        "        else:\n",
        "            Ws = self.Ws\n",
        "            zs = self.zs\n",
        "        assert len(zs) == len(Ws)-1\n",
        "        grad = Ws[-1].repeat(n_batch,1,1)\n",
        "        for i in range(len(zs)-1, 0-1, -1):\n",
        "            n_out, n_in = Ws[i].size()\n",
        "            if self.activation == 'relu':\n",
        "                I = get_data_maybe(zs[i] <= 0.).unsqueeze(2).repeat(1,1,n_in)\n",
        "                Wi_grad = Ws[i].repeat(n_batch,1,1)\n",
        "                Wi_grad[I] = 0.\n",
        "            elif self.activation == 'sigmoid':\n",
        "                d = zs[i]*(1.-zs[i])\n",
        "                d = d.unsqueeze(2).expand(n_batch, n_out, n_in)\n",
        "                Wi_grad = Ws[i].repeat(n_batch,1,1)*d\n",
        "            else:\n",
        "                assert False\n",
        "            grad = grad.bmm(Wi_grad)\n",
        "        R = grad[:,:,:n_state]\n",
        "        S = grad[:,:,n_state:]\n",
        "        if self.passthrough:\n",
        "            I = torch.eye(n_state).type_as(get_data_maybe(R)).unsqueeze(0).repeat(n_batch, 1, 1)\n",
        "            if diff:\n",
        "                I = Variable(I)\n",
        "            R = R + I\n",
        "        if x_dim == 1:\n",
        "            R = R.squeeze(0)\n",
        "            S = S.squeeze(0)\n",
        "        return R, S\n",
        "\n",
        "class CtrlPassthroughDynamics(nn.Module):\n",
        "    def __init__(self, dynamics):\n",
        "        super().__init__()\n",
        "        self.dynamics = dynamics\n",
        "\n",
        "    def forward(self, tilde_x, u):\n",
        "        tilde_x_dim, u_dim = tilde_x.ndimension(), u.ndimension()\n",
        "        if tilde_x_dim == 1:\n",
        "            tilde_x = tilde_x.unsqueeze(0)\n",
        "        if u_dim == 1:\n",
        "            u = u.unsqueeze(0)\n",
        "        n_ctrl = u.size(1)\n",
        "        x = tilde_x[:,n_ctrl:]\n",
        "        xtp1 = self.dynamics(x, u)\n",
        "        tilde_xtp1 = torch.cat((u, xtp1), dim=1)\n",
        "        if tilde_x_dim == 1:\n",
        "            tilde_xtp1 = tilde_xtp1.squeeze()\n",
        "        return tilde_xtp1\n",
        "\n",
        "    def grad_input(self, x, u):\n",
        "        assert False, \"Unimplemented\"\n",
        "\n",
        "class AffineDynamics(nn.Module):\n",
        "    def __init__(self, A, B, c=None):\n",
        "        super(AffineDynamics, self).__init__()\n",
        "        assert A.ndimension() == 2\n",
        "        assert B.ndimension() == 2\n",
        "        if c is not None:\n",
        "            assert c.ndimension() == 1\n",
        "        self.A = A\n",
        "        self.B = B\n",
        "        self.c = c\n",
        "\n",
        "    def forward(self, x, u):\n",
        "        if not isinstance(x, Variable) and isinstance(self.A, Variable):\n",
        "            A = self.A.data\n",
        "            B = self.B.data\n",
        "            c = self.c.data if self.c is not None else 0.\n",
        "        else:\n",
        "            A = self.A\n",
        "            B = self.B\n",
        "            c = self.c if self.c is not None else 0.\n",
        "        x_dim, u_dim = x.ndimension(), u.ndimension()\n",
        "        if x_dim == 1:\n",
        "            x = x.unsqueeze(0)\n",
        "        if u_dim == 1:\n",
        "            u = u.unsqueeze(0)\n",
        "        z = x.mm(A.t()) + u.mm(B.t()) + c\n",
        "        if x_dim == 1:\n",
        "            z = z.squeeze(0)\n",
        "        return z\n",
        "\n",
        "    def grad_input(self, x, u):\n",
        "        n_batch = x.size(0)\n",
        "        A, B = self.A, self.B\n",
        "        A = A.unsqueeze(0).repeat(n_batch, 1, 1)\n",
        "        B = B.unsqueeze(0).repeat(n_batch, 1, 1)\n",
        "        if not isinstance(x, Variable) and isinstance(A, Variable):\n",
        "            A, B = A.data, B.data\n",
        "        return A, B\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dtt2mx2qjKdy",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title lqr_step\n",
        "# https://github.com/locuslab/mpc.pytorch/blob/master/mpc/lqr_step.py\n",
        "# time-varying linear control (LQR) problem\n",
        "import torch\n",
        "from torch.autograd import Function, Variable\n",
        "from torch.nn import Module\n",
        "from torch.nn.parameter import Parameter\n",
        "import numpy as np\n",
        "import numpy.random as npr\n",
        "from collections import namedtuple\n",
        "import time\n",
        "\n",
        "LqrBackOut = namedtuple('lqrBackOut', 'n_total_qp_iter')\n",
        "LqrForOut = namedtuple('lqrForOut', 'objs full_du_norm alpha_du_norm mean_alphas costs')\n",
        "\n",
        "def LQRStep(n_state, n_ctrl, T,\n",
        "            u_lower=None, u_upper=None,\n",
        "            u_zero_I=None,\n",
        "            delta_u=None,\n",
        "            linesearch_decay=0.2,\n",
        "            max_linesearch_iter=10,\n",
        "            true_cost=None,\n",
        "            true_dynamics=None,\n",
        "            delta_space=True,\n",
        "            current_x=None, current_u=None,\n",
        "            verbose=0,\n",
        "            back_eps=1e-3,\n",
        "            no_op_forward=False):\n",
        "    \"\"\"A single step of the box-constrained iLQR solver.\n",
        "        Required Args:\n",
        "            n_state, n_ctrl, T\n",
        "            x_init: The initial state [n_batch, n_state]\n",
        "        Optional Args:\n",
        "            u_lower, u_upper: The lower- and upper-bounds on the controls.\n",
        "                These can either be floats or shaped as [T, n_batch, n_ctrl]\n",
        "                TODO: Better support automatic expansion of these.\n",
        "            TODO\"\"\"\n",
        "    # \\@profile\n",
        "    def lqr_backward(ctx, C, c, F, f):\n",
        "        n_batch = C.size(1)\n",
        "        u = ctx.current_u\n",
        "        Ks = []\n",
        "        ks = []\n",
        "        prev_kt = None\n",
        "        n_total_qp_iter = 0\n",
        "        Vtp1 = vtp1 = None\n",
        "        for t in range(T-1, -1, -1):\n",
        "            if t == T-1:\n",
        "                Qt = C[t]\n",
        "                qt = c[t]\n",
        "            else:\n",
        "                Ft = F[t]\n",
        "                Ft_T = Ft.transpose(1,2)\n",
        "                Qt = C[t] + Ft_T.bmm(Vtp1).bmm(Ft)\n",
        "                if f is None or f.nelement() == 0:\n",
        "                    qt = c[t] + Ft_T.bmm(vtp1.unsqueeze(2)).squeeze(2)\n",
        "                else:\n",
        "                    ft = f[t]\n",
        "                    qt = c[t] + Ft_T.bmm(Vtp1).bmm(ft.unsqueeze(2)).squeeze(2) + \\\n",
        "                        Ft_T.bmm(vtp1.unsqueeze(2)).squeeze(2)\n",
        "            Qt_xx = Qt[:, :n_state, :n_state]\n",
        "            Qt_xu = Qt[:, :n_state, n_state:]\n",
        "            Qt_ux = Qt[:, n_state:, :n_state]\n",
        "            Qt_uu = Qt[:, n_state:, n_state:]\n",
        "            qt_x = qt[:, :n_state]\n",
        "            qt_u = qt[:, n_state:]\n",
        "            if u_lower is None:\n",
        "                if n_ctrl == 1 and u_zero_I is None:\n",
        "                    Kt = -(1./Qt_uu)*Qt_ux\n",
        "                    kt = -(1./Qt_uu.squeeze(2))*qt_u\n",
        "                else:\n",
        "                    if u_zero_I is None:\n",
        "                        Qt_uu_inv = [torch.pinverse(Qt_uu[i]) for i in range(Qt_uu.shape[0])]\n",
        "                        Qt_uu_inv = torch.stack(Qt_uu_inv)\n",
        "                        Kt = -Qt_uu_inv.bmm(Qt_ux)\n",
        "                        kt = bmv(-Qt_uu_inv, qt_u)\n",
        "                        # Qt_uu_LU = Qt_uu.lu()\n",
        "                        # Kt = -Qt_ux.lu_solve(*Qt_uu_LU)\n",
        "                        # kt = -qt_u.lu_solve(*Qt_uu_LU)\n",
        "                    else:\n",
        "                        # Solve with zero constraints on the active controls.\n",
        "                        I = u_zero_I[t].float()\n",
        "                        notI = 1-I\n",
        "                        qt_u_ = qt_u.clone()\n",
        "                        qt_u_[I.bool()] = 0\n",
        "                        Qt_uu_ = Qt_uu.clone()\n",
        "                        if I.is_cuda:\n",
        "                            notI_ = notI.float()\n",
        "                            Qt_uu_I = (1-bger(notI_, notI_)).type_as(I)\n",
        "                        else:\n",
        "                            Qt_uu_I = 1-bger(notI, notI)\n",
        "                        Qt_uu_[Qt_uu_I.bool()] = 0.\n",
        "                        Qt_uu_[bdiag(I).bool()] += 1e-8\n",
        "                        Qt_ux_ = Qt_ux.clone()\n",
        "                        Qt_ux_[I.unsqueeze(2).repeat(1,1,Qt_ux.size(2)).bool()] = 0.\n",
        "                        if n_ctrl == 1:\n",
        "                            Kt = -(1./Qt_uu_)*Qt_ux_\n",
        "                            kt = -(1./Qt_uu.squeeze(2))*qt_u_\n",
        "                        else:\n",
        "                            Qt_uu_LU_ = Qt_uu_.lu()\n",
        "                            Kt = -Qt_ux_.lu_solve(*Qt_uu_LU_)\n",
        "                            kt = -qt_u_.unsqueeze(2).lu_solve(*Qt_uu_LU_).squeeze(2)\n",
        "            else:\n",
        "                assert delta_space\n",
        "                lb = get_bound('lower', t) - u[t]\n",
        "                ub = get_bound('upper', t) - u[t]\n",
        "                if delta_u is not None:\n",
        "                    lb[lb < -delta_u] = -delta_u\n",
        "                    ub[ub > delta_u] = delta_u\n",
        "                kt, Qt_uu_free_LU, If, n_qp_iter = pnqp(\n",
        "                    Qt_uu, qt_u, lb, ub,\n",
        "                    x_init=prev_kt, n_iter=20)\n",
        "                if verbose > 1:\n",
        "                    print('  + n_qp_iter: ', n_qp_iter+1)\n",
        "                n_total_qp_iter += 1+n_qp_iter\n",
        "                prev_kt = kt\n",
        "                Qt_ux_ = Qt_ux.clone()\n",
        "                Qt_ux_[(1-If).unsqueeze(2).repeat(1,1,Qt_ux.size(2)).bool()] = 0\n",
        "                if n_ctrl == 1:\n",
        "                    # Bad naming, Qt_uu_free_LU isn't the LU in this case.\n",
        "                    Kt = -((1./Qt_uu_free_LU)*Qt_ux_)\n",
        "                else:\n",
        "                    Kt = -Qt_ux_.lu_solve(*Qt_uu_free_LU)\n",
        "            Kt_T = Kt.transpose(1,2)\n",
        "            Ks.append(Kt)\n",
        "            ks.append(kt)\n",
        "            Vtp1 = Qt_xx + Qt_xu.bmm(Kt) + Kt_T.bmm(Qt_ux) + Kt_T.bmm(Qt_uu).bmm(Kt)\n",
        "            vtp1 = qt_x + Qt_xu.bmm(kt.unsqueeze(2)).squeeze(2) + \\\n",
        "                Kt_T.bmm(qt_u.unsqueeze(2)).squeeze(2) + \\\n",
        "                Kt_T.bmm(Qt_uu).bmm(kt.unsqueeze(2)).squeeze(2)\n",
        "        return Ks, ks, n_total_qp_iter\n",
        "\n",
        "\n",
        "    # \\@profile\n",
        "    def lqr_forward(ctx, x_init, C, c, F, f, Ks, ks):\n",
        "        x = ctx.current_x\n",
        "        u = ctx.current_u\n",
        "        n_batch = C.size(1)\n",
        "        old_cost = get_cost(T, u, true_cost, true_dynamics, x=x)\n",
        "        current_cost = None\n",
        "        alphas = torch.ones(n_batch).type_as(C)\n",
        "        full_du_norm = None\n",
        "        i = 0\n",
        "        while (current_cost is None or \\\n",
        "            (old_cost is not None and \\\n",
        "                torch.any((current_cost > old_cost)).cpu().item() == 1)) and \\\n",
        "            i < max_linesearch_iter:\n",
        "            new_u = []\n",
        "            new_x = [x_init]\n",
        "            dx = [torch.zeros_like(x_init)]\n",
        "            objs = []\n",
        "            for t in range(T):\n",
        "                t_rev = T-1-t\n",
        "                Kt = Ks[t_rev]\n",
        "                kt = ks[t_rev]\n",
        "                new_xt = new_x[t]\n",
        "                xt = x[t]\n",
        "                ut = u[t]\n",
        "                dxt = dx[t]\n",
        "                new_ut = bmv(Kt, dxt) + ut + torch.diag(alphas).mm(kt)\n",
        "                # Currently unimplemented:\n",
        "                assert not ((delta_u is not None) and (u_lower is None))\n",
        "                if u_zero_I is not None:\n",
        "                    new_ut[u_zero_I[t]] = 0.\n",
        "                if u_lower is not None:\n",
        "                    lb = get_bound('lower', t)\n",
        "                    ub = get_bound('upper', t)\n",
        "                    if delta_u is not None:\n",
        "                        lb_limit, ub_limit = lb, ub\n",
        "                        lb = u[t] - delta_u\n",
        "                        ub = u[t] + delta_u\n",
        "                        I = lb < lb_limit\n",
        "                        lb[I] = lb_limit if isinstance(lb_limit, float) else lb_limit[I]\n",
        "                        I = ub > ub_limit\n",
        "                        ub[I] = ub_limit if isinstance(lb_limit, float) else ub_limit[I]\n",
        "                    # TODO(eugenevinitsky) why do we need to do this here?\n",
        "                    new_ut = eclamp(new_ut, lb, ub)\n",
        "                new_u.append(new_ut)\n",
        "                new_xut = torch.cat((new_xt, new_ut), dim=1)\n",
        "                if t < T-1:\n",
        "                    if isinstance(true_dynamics, LinDx):\n",
        "                        F, f = true_dynamics.F, true_dynamics.f\n",
        "                        new_xtp1 = bmv(F[t], new_xut)\n",
        "                        if f is not None and f.nelement() > 0:\n",
        "                            new_xtp1 += f[t]\n",
        "                    else:\n",
        "                        new_xtp1 = true_dynamics(Variable(new_xt), Variable(new_ut)).data\n",
        "                    new_x.append(new_xtp1)\n",
        "                    dx.append(new_xtp1 - x[t+1])\n",
        "                if isinstance(true_cost, QuadCost):\n",
        "                    C, c = true_cost.C, true_cost.c\n",
        "                    obj = 0.5*bquad(new_xut, C[t]) + bdot(new_xut, c[t])\n",
        "                else:\n",
        "                    obj = true_cost(new_xut)\n",
        "                objs.append(obj)\n",
        "            objs = torch.stack(objs)\n",
        "            current_cost = torch.sum(objs, dim=0)\n",
        "            new_u = torch.stack(new_u)\n",
        "            new_x = torch.stack(new_x)\n",
        "            if full_du_norm is None:\n",
        "                full_du_norm = (u-new_u).transpose(1,2).contiguous().view(n_batch, -1).norm(2, 1)\n",
        "            alphas[current_cost > old_cost] *= linesearch_decay\n",
        "            i += 1\n",
        "        # If the iteration limit is hit, some alphas are one step too small.\n",
        "        alphas[current_cost > old_cost] /= linesearch_decay\n",
        "        alpha_du_norm = (u-new_u).transpose(1,2).contiguous().view(\n",
        "            n_batch, -1).norm(2, 1)\n",
        "        return new_x, new_u, LqrForOut(\n",
        "            objs, full_du_norm,\n",
        "            alpha_du_norm,\n",
        "            torch.mean(alphas),\n",
        "            current_cost\n",
        "        )\n",
        "\n",
        "    def get_bound(side, t):\n",
        "        if side == 'lower':\n",
        "            v = u_lower\n",
        "        if side == 'upper':\n",
        "            v = u_upper\n",
        "        if isinstance(v, float):\n",
        "            return v\n",
        "        else:\n",
        "            return v[t]\n",
        "\n",
        "    class LQRStepFn(Function):\n",
        "        # \\@profile\n",
        "        @staticmethod\n",
        "        def forward(ctx, x_init, C, c, F, f=None):\n",
        "            if no_op_forward:\n",
        "                ctx.save_for_backward(x_init, C, c, F, f, current_x, current_u)\n",
        "                ctx.current_x, ctx.current_u = current_x, current_u\n",
        "                return current_x, current_u\n",
        "            if delta_space:\n",
        "                # Taylor-expand the objective to do the backward pass in the delta space.\n",
        "                assert current_x is not None\n",
        "                assert current_u is not None\n",
        "                c_back = []\n",
        "                for t in range(T):\n",
        "                    xt = current_x[t]\n",
        "                    ut = current_u[t]\n",
        "                    xut = torch.cat((xt, ut), 1)\n",
        "                    c_back.append(bmv(C[t], xut) + c[t])\n",
        "                c_back = torch.stack(c_back)\n",
        "                f_back = None\n",
        "            else:\n",
        "                assert False\n",
        "            ctx.current_x = current_x\n",
        "            ctx.current_u = current_u\n",
        "            Ks, ks, n_total_qp_iter = lqr_backward(ctx, C, c_back, F, f_back)\n",
        "            new_x, new_u, for_out = lqr_forward(ctx,\n",
        "                x_init, C, c, F, f, Ks, ks)\n",
        "            ctx.save_for_backward(x_init, C, c, F, f, new_x, new_u)\n",
        "            return new_x, new_u, torch.Tensor([n_total_qp_iter]), \\\n",
        "              for_out.costs, for_out.full_du_norm, for_out.mean_alphas\n",
        "\n",
        "        @staticmethod\n",
        "        def backward(ctx, dl_dx, dl_du, temp=None, temp2=None):\n",
        "            start = time.time()\n",
        "            x_init, C, c, F, f, new_x, new_u = ctx.saved_tensors\n",
        "            r = []\n",
        "            for t in range(T):\n",
        "                rt = torch.cat((dl_dx[t], dl_du[t]), 1)\n",
        "                r.append(rt)\n",
        "            r = torch.stack(r)\n",
        "            if u_lower is None:\n",
        "                I = None\n",
        "            else:\n",
        "                I = (torch.abs(new_u - u_lower) <= 1e-8) | \\\n",
        "                    (torch.abs(new_u - u_upper) <= 1e-8)\n",
        "            dx_init = Variable(torch.zeros_like(x_init))\n",
        "            _mpc = MPC(n_state, n_ctrl, T,\n",
        "                u_zero_I=I, u_init=None,\n",
        "                lqr_iter=1,\n",
        "                verbose=-1,\n",
        "                n_batch=C.size(1),\n",
        "                delta_u=None,\n",
        "                exit_unconverged=False, # It's really bad if this doesn't converge.\n",
        "                eps=back_eps,\n",
        "            )\n",
        "            dx, du, _ = _mpc(dx_init, QuadCost(C, -r), LinDx(F, None))\n",
        "            dx, du = dx.data, du.data\n",
        "            dxu = torch.cat((dx, du), 2)\n",
        "            xu = torch.cat((new_x, new_u), 2)\n",
        "            dC = torch.zeros_like(C)\n",
        "            for t in range(T):\n",
        "                xut = torch.cat((new_x[t], new_u[t]), 1)\n",
        "                dxut = dxu[t]\n",
        "                dCt = -0.5*(bger(dxut, xut) + bger(xut, dxut))\n",
        "                dC[t] = dCt\n",
        "            dc = -dxu\n",
        "            lams = []\n",
        "            prev_lam = None\n",
        "            for t in range(T-1, -1, -1):\n",
        "                Ct_xx = C[t,:,:n_state,:n_state]\n",
        "                Ct_xu = C[t,:,:n_state,n_state:]\n",
        "                ct_x = c[t,:,:n_state]\n",
        "                xt = new_x[t]\n",
        "                ut = new_u[t]\n",
        "                lamt = bmv(Ct_xx, xt) + bmv(Ct_xu, ut) + ct_x\n",
        "                if prev_lam is not None:\n",
        "                    Fxt = F[t,:,:,:n_state].transpose(1, 2)\n",
        "                    lamt += bmv(Fxt, prev_lam)\n",
        "                lams.append(lamt)\n",
        "                prev_lam = lamt\n",
        "            lams = list(reversed(lams))\n",
        "            dlams = []\n",
        "            prev_dlam = None\n",
        "            for t in range(T-1, -1, -1):\n",
        "                dCt_xx = C[t,:,:n_state,:n_state]\n",
        "                dCt_xu = C[t,:,:n_state,n_state:]\n",
        "                drt_x = -r[t,:,:n_state]\n",
        "                dxt = dx[t]\n",
        "                dut = du[t]\n",
        "                dlamt = bmv(dCt_xx, dxt) + bmv(dCt_xu, dut) + drt_x\n",
        "                if prev_dlam is not None:\n",
        "                    Fxt = F[t,:,:,:n_state].transpose(1, 2)\n",
        "                    dlamt += bmv(Fxt, prev_dlam)\n",
        "                dlams.append(dlamt)\n",
        "                prev_dlam = dlamt\n",
        "            dlams = torch.stack(list(reversed(dlams)))\n",
        "            dF = torch.zeros_like(F)\n",
        "            for t in range(T-1):\n",
        "                xut = xu[t]\n",
        "                lamt = lams[t+1]\n",
        "                dxut = dxu[t]\n",
        "                dlamt = dlams[t+1]\n",
        "                dF[t] = -(bger(dlamt, xut) + bger(lamt, dxut))\n",
        "            if f.nelement() > 0:\n",
        "                _dlams = dlams[1:]\n",
        "                assert _dlams.shape == f.shape\n",
        "                df = -_dlams\n",
        "            else:\n",
        "                df = torch.Tensor()\n",
        "            dx_init = -dlams[0]\n",
        "            backward_time = time.time()-start\n",
        "            return dx_init, dC, dc, dF, df\n",
        "    return LQRStepFn.apply\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjGT2NCFiPqs",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title mpc\n",
        "# https://github.com/locuslab/mpc.pytorch/blob/master/mpc/mpc.py\n",
        "import torch\n",
        "from torch.autograd import Function, Variable\n",
        "from torch.nn import Module\n",
        "from torch.nn.parameter import Parameter\n",
        "import numpy as np\n",
        "import numpy.random as npr\n",
        "from collections import namedtuple\n",
        "from enum import Enum\n",
        "import sys\n",
        "\n",
        "QuadCost = namedtuple('QuadCost', 'C c')\n",
        "LinDx = namedtuple('LinDx', 'F f')\n",
        "\n",
        "# https://stackoverflow.com/questions/11351032\n",
        "QuadCost.__new__.__defaults__ = (None,) * len(QuadCost._fields)\n",
        "LinDx.__new__.__defaults__ = (None,) * len(LinDx._fields)\n",
        "\n",
        "class GradMethods(Enum):\n",
        "    AUTO_DIFF = 1\n",
        "    FINITE_DIFF = 2\n",
        "    ANALYTIC = 3\n",
        "    ANALYTIC_CHECK = 4\n",
        "\n",
        "class SlewRateCost(Module):\n",
        "    \"\"\"Hacky way of adding the slew rate penalty to costs.\"\"\"\n",
        "    # TODO: It would be cleaner to update this to just use the slew rate penalty instead of # slew_C\n",
        "    def __init__(self, cost, slew_C, n_state, n_ctrl):\n",
        "        super().__init__()\n",
        "        self.cost = cost\n",
        "        self.slew_C = slew_C\n",
        "        self.n_state = n_state\n",
        "        self.n_ctrl = n_ctrl\n",
        "\n",
        "    def forward(self, tau):\n",
        "        true_tau = tau[:, self.n_ctrl:]\n",
        "        true_cost = self.cost(true_tau)\n",
        "        # The slew constraints are time-invariant.\n",
        "        slew_cost = 0.5 * bquad(tau, self.slew_C[0])\n",
        "        return true_cost + slew_cost\n",
        "\n",
        "    def grad_input(self, x, u):\n",
        "        raise NotImplementedError(\"Implement grad_input\")\n",
        "\n",
        "\n",
        "class MPC(Module):\n",
        "    \"\"\"A differentiable box-constrained iLQR solver.\n",
        "    This provides a differentiable solver for the following box-constrained\n",
        "    control problem with a quadratic cost (defined by C and c) and\n",
        "    non-linear dynamics (defined by f):\n",
        "        min_{tau={x,u}} sum_t 0.5 tau_t^T C_t tau_t + c_t^T tau_t\n",
        "                        s.t. x_{t+1} = f(x_t, u_t)\n",
        "                            x_0 = x_init\n",
        "                            u_lower <= u <= u_upper\n",
        "    This implements the Control-Limited Differential Dynamic Programming\n",
        "    paper with a first-order approximation to the non-linear dynamics:\n",
        "    https://homes.cs.washington.edu/~todorov/papers/TassaICRA14.pdf\n",
        "    Some of the notation here is from Sergey Levine's notes:\n",
        "    http://rll.berkeley.edu/deeprlcourse/f17docs/lecture_8_model_based_planning.pdf\n",
        "    Required Args:\n",
        "        n_state, n_ctrl, T\n",
        "    Optional Args:\n",
        "        u_lower, u_upper: The lower- and upper-bounds on the controls.\n",
        "            These can either be floats or shaped as [T, n_batch, n_ctrl]\n",
        "        u_init: The initial control sequence, useful for warm-starting:\n",
        "            [T, n_batch, n_ctrl]\n",
        "        lqr_iter: The number of LQR iterations to perform.\n",
        "        grad_method: The method to compute the Jacobian of the dynamics.\n",
        "            GradMethods.ANALYTIC: Use a manually-defined Jacobian.\n",
        "                + Fast and accurate, use this if possible\n",
        "            GradMethods.AUTO_DIFF: Use PyTorch's autograd.\n",
        "                + Slow\n",
        "            GradMethods.FINITE_DIFF: Use naive finite differences\n",
        "                + Inaccurate\n",
        "        delta_u (float): The amount each component of the controls\n",
        "            is allowed to change in each LQR iteration.\n",
        "        verbose (int):\n",
        "            -1: No output or warnings\n",
        "             0: Warnings\n",
        "            1+: Detailed iteration info\n",
        "        eps: Termination threshold, on the norm of the full control\n",
        "             step (without line search)\n",
        "        back_eps: `eps` value to use in the backwards pass.\n",
        "        n_batch: May be necessary for now if it can't be inferred.\n",
        "                 TODO: Infer, potentially remove this.\n",
        "        linesearch_decay (float): Multiplicative decay factor for the\n",
        "            line search.\n",
        "        max_linesearch_iter (int): Can be used to disable the line search\n",
        "            if 1 is used for some problems the line search can\n",
        "            be harmful.\n",
        "        exit_unconverged: Assert False if a fixed point is not reached.\n",
        "        detach_unconverged: Detach examples from the graph that do\n",
        "            not hit a fixed point so they are not differentiated through.\n",
        "        backprop: Allow the solver to be differentiated through.\n",
        "        slew_rate_penalty (float): Penalty term applied to\n",
        "            ||u_t - u_{t+1}||_2^2 in the objective.\n",
        "        prev_ctrl: The previous nominal control sequence to initialize\n",
        "            the solver with.\n",
        "        not_improved_lim: The number of iterations to allow that don't\n",
        "            improve the objective before returning early.\n",
        "        best_cost_eps: Absolute threshold for the best cost\n",
        "            to be updated.\"\"\"\n",
        "    def __init__(self, n_state, n_ctrl, T,\n",
        "            u_lower=None, u_upper=None,\n",
        "            u_zero_I=None, u_init=None,\n",
        "            lqr_iter=10,\n",
        "            grad_method=GradMethods.ANALYTIC,\n",
        "            delta_u=None,\n",
        "            verbose=0,\n",
        "            eps=1e-7,\n",
        "            back_eps=1e-7,\n",
        "            n_batch=None,\n",
        "            linesearch_decay=0.2,\n",
        "            max_linesearch_iter=10,\n",
        "            exit_unconverged=True,\n",
        "            detach_unconverged=True,\n",
        "            backprop=True,\n",
        "            slew_rate_penalty=None,\n",
        "            prev_ctrl=None,\n",
        "            not_improved_lim=5,\n",
        "            best_cost_eps=1e-4\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert (u_lower is None) == (u_upper is None)\n",
        "        assert max_linesearch_iter > 0\n",
        "        self.n_state = n_state\n",
        "        self.n_ctrl = n_ctrl\n",
        "        self.T = T\n",
        "        self.u_lower = u_lower\n",
        "        self.u_upper = u_upper\n",
        "        if not isinstance(u_lower, float):\n",
        "            self.u_lower = detach_maybe(self.u_lower)\n",
        "        if not isinstance(u_upper, float):\n",
        "            self.u_upper = detach_maybe(self.u_upper)\n",
        "        self.u_zero_I = detach_maybe(u_zero_I)\n",
        "        self.u_init = detach_maybe(u_init)\n",
        "        self.lqr_iter = lqr_iter\n",
        "        self.grad_method = grad_method\n",
        "        self.delta_u = delta_u\n",
        "        self.verbose = verbose\n",
        "        self.eps = eps\n",
        "        self.back_eps = back_eps\n",
        "        self.n_batch = n_batch\n",
        "        self.linesearch_decay = linesearch_decay\n",
        "        self.max_linesearch_iter = max_linesearch_iter\n",
        "        self.exit_unconverged = exit_unconverged\n",
        "        self.detach_unconverged = detach_unconverged\n",
        "        self.backprop = backprop\n",
        "        self.not_improved_lim = not_improved_lim\n",
        "        self.best_cost_eps = best_cost_eps\n",
        "        self.slew_rate_penalty = slew_rate_penalty\n",
        "        self.prev_ctrl = prev_ctrl\n",
        "\n",
        "    # \\@profile\n",
        "    def forward(self, x_init, cost, dx):\n",
        "        # QuadCost.C: [T, n_batch, n_tau, n_tau]\n",
        "        # QuadCost.c: [T, n_batch, n_tau]\n",
        "        assert isinstance(cost, QuadCost) or isinstance(cost, Module) or isinstance(cost, Function)\n",
        "        assert isinstance(dx, LinDx) or isinstance(dx, Module) or isinstance(dx, Function)\n",
        "        # TODO: Clean up inferences, expansions, and assumptions made here.\n",
        "        if self.n_batch is not None:\n",
        "            n_batch = self.n_batch\n",
        "        elif isinstance(cost, QuadCost) and cost.C.ndimension() == 4:\n",
        "            n_batch = cost.C.size(1)\n",
        "        else:\n",
        "            print('MPC Error: Could not infer batch size, pass in as n_batch')\n",
        "            sys.exit(-1)\n",
        "        # if c.ndimension() == 2:\n",
        "        #     c = c.unsqueeze(1).expand(self.T, n_batch, -1)\n",
        "        if isinstance(cost, QuadCost):\n",
        "            C, c = cost\n",
        "            if C.ndimension() == 2:\n",
        "                # Add the time and batch dimensions.\n",
        "                C = C.unsqueeze(0).unsqueeze(0).expand(\n",
        "                    self.T, n_batch, self.n_state+self.n_ctrl, -1)\n",
        "            elif C.ndimension() == 3:\n",
        "                # Add the batch dimension.\n",
        "                C = C.unsqueeze(1).expand(\n",
        "                    self.T, n_batch, self.n_state+self.n_ctrl, -1)\n",
        "            if c.ndimension() == 1:\n",
        "                # Add the time and batch dimensions.\n",
        "                c = c.unsqueeze(0).unsqueeze(0).expand(self.T, n_batch, -1)\n",
        "            elif c.ndimension() == 2:\n",
        "                # Add the batch dimension.\n",
        "                c = c.unsqueeze(1).expand(self.T, n_batch, -1)\n",
        "            if C.ndimension() != 4 or c.ndimension() != 3:\n",
        "                print('MPC Error: Unexpected QuadCost shape.')\n",
        "                sys.exit(-1)\n",
        "            cost = QuadCost(C, c)\n",
        "        assert x_init.ndimension() == 2 and x_init.size(0) == n_batch\n",
        "        if self.u_init is None:\n",
        "            u = torch.zeros(self.T, n_batch, self.n_ctrl).type_as(x_init.data)\n",
        "        else:\n",
        "            u = self.u_init\n",
        "            if u.ndimension() == 2:\n",
        "                u = u.unsqueeze(1).expand(self.T, n_batch, -1).clone()\n",
        "        u = u.type_as(x_init.data)\n",
        "        if self.verbose > 0:\n",
        "            print('Initial mean(cost): {:.4e}'.format(torch.mean(get_cost(self.T, u, cost, dx, x_init=x_init)).item()))\n",
        "        best = None\n",
        "        n_not_improved = 0\n",
        "        for i in range(self.lqr_iter):\n",
        "            u = Variable(detach_maybe(u), requires_grad=True)\n",
        "            # Linearize the dynamics around the current trajectory.\n",
        "            x = get_traj(self.T, u, x_init=x_init, dynamics=dx)\n",
        "            if isinstance(dx, LinDx):\n",
        "                F, f = dx.F, dx.f\n",
        "            else:\n",
        "                F, f = self.linearize_dynamics(x, detach_maybe(u), dx, diff=False)\n",
        "            if isinstance(cost, QuadCost):\n",
        "                C, c = cost.C, cost.c\n",
        "            else:\n",
        "                C, c, _ = self.approximate_cost(x, detach_maybe(u), cost, diff=False)\n",
        "            x, u, n_total_qp_iter, costs, full_du_norm, mean_alphas = self.solve_lqr_subproblem(x_init, C, c, F, f, cost, dx, x, u)\n",
        "            n_not_improved += 1\n",
        "            assert x.ndimension() == 3\n",
        "            assert u.ndimension() == 3\n",
        "            if best is None:\n",
        "                best = {\n",
        "                    'x': list(torch.split(x, split_size_or_sections=1, dim=1)),\n",
        "                    'u': list(torch.split(u, split_size_or_sections=1, dim=1)),\n",
        "                    'costs': costs,\n",
        "                    'full_du_norm': full_du_norm,\n",
        "                }\n",
        "            else:\n",
        "                for j in range(n_batch):\n",
        "                    if costs[j] <= best['costs'][j] + self.best_cost_eps:\n",
        "                        n_not_improved = 0\n",
        "                        best['x'][j] = x[:,j].unsqueeze(1)\n",
        "                        best['u'][j] = u[:,j].unsqueeze(1)\n",
        "                        best['costs'][j] = costs[j]\n",
        "                        best['full_du_norm'][j] = full_du_norm[j]\n",
        "            if self.verbose > 0:\n",
        "                table_log('lqr', (\n",
        "                    ('iter', i),\n",
        "                    ('mean(cost)', torch.mean(best['costs']).item(), '{:.4e}'),\n",
        "                    ('||full_du||_max', max(full_du_norm).item(), '{:.2e}'),\n",
        "                    # ('||alpha_du||_max', max(alpha_du_norm), '{:.2e}'),\n",
        "                    # TODO: alphas, total_qp_iters here is for the current iterate, not the best\n",
        "                    ('mean(alphas)', mean_alphas.item(), '{:.2e}'),\n",
        "                    ('total_qp_iters', n_total_qp_iter),\n",
        "                ))\n",
        "            if max(full_du_norm) < self.eps or \\\n",
        "               n_not_improved > self.not_improved_lim:\n",
        "                break\n",
        "        x = torch.cat(best['x'], dim=1)\n",
        "        u = torch.cat(best['u'], dim=1)\n",
        "        full_du_norm = best['full_du_norm']\n",
        "        if isinstance(dx, LinDx):\n",
        "            F, f = dx.F, dx.f\n",
        "        else:\n",
        "            F, f = self.linearize_dynamics(x, u, dx, diff=True)\n",
        "        if isinstance(cost, QuadCost):\n",
        "            C, c = cost.C, cost.c\n",
        "        else:\n",
        "            C, c, _ = self.approximate_cost(x, u, cost, diff=True)\n",
        "        x, u = self.solve_lqr_subproblem(x_init, C, c, F, f, cost, dx, x, u, no_op_forward=True)\n",
        "        if self.detach_unconverged:\n",
        "            if max(best['full_du_norm']) > self.eps:\n",
        "                if self.exit_unconverged:\n",
        "                    assert False\n",
        "                if self.verbose >= 0:\n",
        "                    print(\"LQR Warning: All examples did not converge to a fixed point.\")\n",
        "                    print(\"Detaching and *not* backpropping through the bad examples.\")\n",
        "                I = full_du_norm < self.eps\n",
        "                Ix = Variable(I.unsqueeze(0).unsqueeze(2).expand_as(x)).type_as(x.data)\n",
        "                Iu = Variable(I.unsqueeze(0).unsqueeze(2).expand_as(u)).type_as(u.data)\n",
        "                x = x*Ix + x.clone().detach()*(1.-Ix)\n",
        "                u = u*Iu + u.clone().detach()*(1.-Iu)\n",
        "        costs = best['costs']\n",
        "        return (x, u, costs)\n",
        "\n",
        "    def solve_lqr_subproblem(self, x_init, C, c, F, f, cost, dynamics, x, u, no_op_forward=False):\n",
        "        if self.slew_rate_penalty is None or isinstance(cost, Module):\n",
        "            _lqr = LQRStep(n_state=self.n_state, n_ctrl=self.n_ctrl, T=self.T,\n",
        "                u_lower=self.u_lower, u_upper=self.u_upper,\n",
        "                u_zero_I=self.u_zero_I,\n",
        "                true_cost=cost,\n",
        "                true_dynamics=dynamics,\n",
        "                delta_u=self.delta_u,\n",
        "                linesearch_decay=self.linesearch_decay,\n",
        "                max_linesearch_iter=self.max_linesearch_iter,\n",
        "                delta_space=True,\n",
        "                current_x=x, current_u=u,\n",
        "                back_eps=self.back_eps,\n",
        "                no_op_forward=no_op_forward,\n",
        "            )\n",
        "            e = Variable(torch.Tensor())\n",
        "            return _lqr(x_init, C, c, F, f if f is not None else e)\n",
        "        else:\n",
        "            nsc = self.n_state + self.n_ctrl\n",
        "            _n_state = nsc\n",
        "            _nsc = _n_state + self.n_ctrl\n",
        "            n_batch = C.size(1)\n",
        "            _C = torch.zeros(self.T, n_batch, _nsc, _nsc).type_as(C)\n",
        "            half_gamI = self.slew_rate_penalty*torch.eye(\n",
        "                self.n_ctrl).unsqueeze(0).unsqueeze(0).repeat(self.T, n_batch, 1, 1)\n",
        "            _C[:,:,:self.n_ctrl,:self.n_ctrl] = half_gamI\n",
        "            _C[:,:,-self.n_ctrl:,:self.n_ctrl] = -half_gamI\n",
        "            _C[:,:,:self.n_ctrl,-self.n_ctrl:] = -half_gamI\n",
        "            _C[:,:,-self.n_ctrl:,-self.n_ctrl:] = half_gamI\n",
        "            slew_C = _C.clone()\n",
        "            _C = _C + torch.nn.ZeroPad2d((self.n_ctrl, 0, self.n_ctrl, 0))(C)\n",
        "            _c = torch.cat((torch.zeros(self.T, n_batch, self.n_ctrl).type_as(c),c), 2)\n",
        "            _F0 = torch.cat((torch.zeros(self.n_ctrl, self.n_state+self.n_ctrl), torch.eye(self.n_ctrl),), 1).type_as(F).unsqueeze(0).unsqueeze(0).repeat(self.T-1, n_batch, 1, 1)\n",
        "            _F1 = torch.cat((torch.zeros(self.T-1, n_batch, self.n_state, self.n_ctrl).type_as(F),F), 3)\n",
        "            _F = torch.cat((_F0, _F1), 2)\n",
        "            if f is not None:\n",
        "                _f = torch.cat((torch.zeros(self.T-1, n_batch, self.n_ctrl).type_as(f),f), 2)\n",
        "            else:\n",
        "                _f = Variable(torch.Tensor())\n",
        "            u_data = detach_maybe(u)\n",
        "            if self.prev_ctrl is not None:\n",
        "                prev_u = self.prev_ctrl\n",
        "                if prev_u.ndimension() == 1:\n",
        "                    prev_u = prev_u.unsqueeze(0)\n",
        "                if prev_u.ndimension() == 2:\n",
        "                    prev_u = prev_u.unsqueeze(0)\n",
        "                prev_u = prev_u.data\n",
        "            else:\n",
        "                prev_u = torch.zeros(1, n_batch, self.n_ctrl).type_as(u)\n",
        "            utm1s = torch.cat((prev_u, u_data[:-1])).clone()\n",
        "            _x = torch.cat((utm1s, x), 2)\n",
        "            _x_init = torch.cat((Variable(prev_u[0]), x_init), 1)\n",
        "            if not isinstance(dynamics, LinDx):\n",
        "                _dynamics = CtrlPassthroughDynamics(dynamics)\n",
        "            else:\n",
        "                _dynamics = None\n",
        "            if isinstance(cost, QuadCost):\n",
        "                _true_cost = QuadCost(_C, _c)\n",
        "            else:\n",
        "                _true_cost = SlewRateCost(cost, slew_C, self.n_state, self.n_ctrl)\n",
        "            _lqr = LQRStep(n_state=_n_state, n_ctrl=self.n_ctrl, T=self.T,\n",
        "                u_lower=self.u_lower, u_upper=self.u_upper,\n",
        "                u_zero_I=self.u_zero_I,\n",
        "                true_cost=_true_cost,\n",
        "                true_dynamics=_dynamics,\n",
        "                delta_u=self.delta_u,\n",
        "                linesearch_decay=self.linesearch_decay,\n",
        "                max_linesearch_iter=self.max_linesearch_iter,\n",
        "                delta_space=True,\n",
        "                current_x=_x, current_u=u,\n",
        "                back_eps=self.back_eps,\n",
        "                no_op_forward=no_op_forward,\n",
        "            )\n",
        "            x, *rest = _lqr(_x_init, _C, _c, _F, _f)\n",
        "            x = x[:,:,self.n_ctrl:]\n",
        "            return [x] + rest\n",
        "\n",
        "    def approximate_cost(self, x, u, Cf, diff=True):\n",
        "        with torch.enable_grad():\n",
        "            tau = torch.cat((x, u), dim=2).data\n",
        "            tau = Variable(tau, requires_grad=True)\n",
        "            if self.slew_rate_penalty is not None:\n",
        "                print(\"\"\"\n",
        "MPC Error: Using a non-convex cost with a slew rate penalty is not yet implemented.\n",
        "The current implementation does not correctly do a line search.\n",
        "More details: https://github.com/locuslab/mpc.pytorch/issues/12\n",
        "\"\"\")\n",
        "                sys.exit(-1)\n",
        "                differences = tau[1:, :, -self.n_ctrl:] - tau[:-1, :, -self.n_ctrl:]\n",
        "                slew_penalty = (self.slew_rate_penalty * differences.pow(2)).sum(-1)\n",
        "            costs = list()\n",
        "            hessians = list()\n",
        "            grads = list()\n",
        "            for t in range(self.T):\n",
        "                tau_t = tau[t]\n",
        "                if self.slew_rate_penalty is not None:\n",
        "                    cost = Cf(tau_t) + (slew_penalty[t-1] if t > 0 else 0)\n",
        "                else:\n",
        "                    cost = Cf(tau_t)\n",
        "                grad = torch.autograd.grad(cost.sum(), tau_t, create_graph=True, retain_graph=True)[0]\n",
        "                hessian = list()\n",
        "                for v_i in range(tau.shape[2]):\n",
        "                    hessian.append(torch.autograd.grad(grad[:, v_i].sum(), tau_t, retain_graph=True)[0])\n",
        "                hessian = torch.stack(hessian, dim=-1)\n",
        "                costs.append(cost)\n",
        "                grads.append(grad - bmv(hessian, tau_t))\n",
        "                hessians.append(hessian)\n",
        "            costs = torch.stack(costs, dim=0)\n",
        "            grads = torch.stack(grads, dim=0)\n",
        "            hessians = torch.stack(hessians, dim=0)\n",
        "            if not diff:\n",
        "                return hessians.data, grads.data, costs.data\n",
        "            return hessians, grads, costs\n",
        "\n",
        "    # \\@profile\n",
        "    def linearize_dynamics(self, x, u, dynamics, diff):\n",
        "        # TODO: Cleanup variable usage.\n",
        "        n_batch = x[0].size(0)\n",
        "        if self.grad_method == GradMethods.ANALYTIC:\n",
        "            _u = Variable(u[:-1].view(-1, self.n_ctrl), requires_grad=True)\n",
        "            _x = Variable(x[:-1].contiguous().view(-1, self.n_state), requires_grad=True)\n",
        "            # This inefficiently calls dynamics again, but is worth it because\n",
        "            # we can efficiently compute grad_input for every time step at once.\n",
        "            _new_x = dynamics(_x, _u)\n",
        "            # This check is a little expensive and should only be done if modifying this code.\n",
        "            # assert torch.abs(_new_x.data - torch.cat(x[1:])).max() <= 1e-6\n",
        "            if not diff:\n",
        "                _new_x = _new_x.data\n",
        "                _x = _x.data\n",
        "                _u = _u.data\n",
        "            R, S = dynamics.grad_input(_x, _u)\n",
        "            f = _new_x - bmv(R, _x) - bmv(S, _u)\n",
        "            f = f.view(self.T-1, n_batch, self.n_state)\n",
        "            R = R.contiguous().view(self.T-1, n_batch, self.n_state, self.n_state)\n",
        "            S = S.contiguous().view(self.T-1, n_batch, self.n_state, self.n_ctrl)\n",
        "            F = torch.cat((R, S), 3)\n",
        "            if not diff:\n",
        "                F, f = list(map(Variable, [F, f]))\n",
        "            return F, f\n",
        "        else:\n",
        "            # TODO: This is inefficient and confusing.\n",
        "            x_init = x[0]\n",
        "            x = [x_init]\n",
        "            F, f = [], []\n",
        "            for t in range(self.T):\n",
        "                if t < self.T-1:\n",
        "                    xt = Variable(x[t], requires_grad=True)\n",
        "                    ut = Variable(u[t], requires_grad=True)\n",
        "                    xut = torch.cat((xt, ut), 1)\n",
        "                    new_x = dynamics(xt, ut)\n",
        "                    # Linear dynamics approximation.\n",
        "                    if self.grad_method in [GradMethods.AUTO_DIFF, GradMethods.ANALYTIC_CHECK]:\n",
        "                        Rt, St = [], []\n",
        "                        for j in range(self.n_state):\n",
        "                            Rj, Sj = torch.autograd.grad(\n",
        "                                new_x[:,j].sum(), [xt, ut],\n",
        "                                retain_graph=True)\n",
        "                            if not diff:\n",
        "                                Rj, Sj = Rj.data, Sj.data\n",
        "                            Rt.append(Rj)\n",
        "                            St.append(Sj)\n",
        "                        Rt = torch.stack(Rt, dim=1)\n",
        "                        St = torch.stack(St, dim=1)\n",
        "                        if self.grad_method == GradMethods.ANALYTIC_CHECK:\n",
        "                            assert False # Not updated\n",
        "                            Rt_autograd, St_autograd = Rt, St\n",
        "                            Rt, St = dynamics.grad_input(xt, ut)\n",
        "                            eps = 1e-8\n",
        "                            if torch.max(torch.abs(Rt-Rt_autograd)).data[0] > eps or \\\n",
        "                            torch.max(torch.abs(St-St_autograd)).data[0] > eps:\n",
        "                                print('''nmpc.ANALYTIC_CHECK error: The analytic derivative of the dynamics function may be off.''')\n",
        "                            else:\n",
        "                                print('''nmpc.ANALYTIC_CHECK: The analytic derivative of the dynamics function seems correct.\n",
        "        Re-run with GradMethods.ANALYTIC to continue.''')\n",
        "                            sys.exit(0)\n",
        "                    elif self.grad_method == GradMethods.FINITE_DIFF:\n",
        "                        Rt, St = [], []\n",
        "                        for i in range(n_batch):\n",
        "                            Ri = jacobian(lambda s: dynamics(s, ut[i]), xt[i], 1e-4)\n",
        "                            Si = jacobian(lambda a : dynamics(xt[i], a), ut[i], 1e-4)\n",
        "                            if not diff:\n",
        "                                Ri, Si = Ri.data, Si.data\n",
        "                            Rt.append(Ri)\n",
        "                            St.append(Si)\n",
        "                        Rt = torch.stack(Rt)\n",
        "                        St = torch.stack(St)\n",
        "                    else:\n",
        "                        assert False\n",
        "                    Ft = torch.cat((Rt, St), 2)\n",
        "                    F.append(Ft)\n",
        "                    if not diff:\n",
        "                        xt, ut, new_x = xt.data, ut.data, new_x.data\n",
        "                    ft = new_x - bmv(Rt, xt) - bmv(St, ut)\n",
        "                    f.append(ft)\n",
        "                if t < self.T-1:\n",
        "                    x.append(detach_maybe(new_x))\n",
        "            F = torch.stack(F, 0)\n",
        "            f = torch.stack(f, 0)\n",
        "            if not diff:\n",
        "                F, f = list(map(Variable, [F, f]))\n",
        "            return F, f\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y67R8bVQkXDW",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title pendulum\n",
        "# https://github.com/locuslab/mpc.pytorch/blob/master/mpc/env_dx/pendulum.py\n",
        "import torch\n",
        "from torch.autograd import Function, Variable\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.nn.parameter import Parameter\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('bmh')\n",
        "\n",
        "class PendulumDx(nn.Module):\n",
        "    def __init__(self, params=None, simple=True):\n",
        "        super().__init__()\n",
        "        self.simple = simple\n",
        "        self.max_torque = 2.0\n",
        "        self.dt = 0.05\n",
        "        self.n_state = 3\n",
        "        self.n_ctrl = 1\n",
        "        if params is None:\n",
        "            if simple:\n",
        "                # gravity (g), mass (m), length (l)\n",
        "                self.params = Variable(torch.Tensor((10., 1., 1.)))\n",
        "            else:\n",
        "                # gravity (g), mass (m), length (l), damping (d), gravity bias (b)\n",
        "                self.params = Variable(torch.Tensor((10., 1., 1., 0., 0.)))\n",
        "        else:\n",
        "            self.params = params\n",
        "        assert len(self.params) == 3 if simple else 5\n",
        "        self.goal_state = torch.Tensor([1., 0., 0.])\n",
        "        self.goal_weights = torch.Tensor([1., 1., 0.1])\n",
        "        self.ctrl_penalty = 0.001\n",
        "        # self.lower, self.upper = -2., 2.\n",
        "        # self.mpc_eps = 1e-3\n",
        "        # self.linesearch_decay = 0.2\n",
        "        # self.max_linesearch_iter = 5\n",
        "\n",
        "    def forward(self, x, u):\n",
        "        squeeze = x.ndimension() == 1\n",
        "        if squeeze:\n",
        "            x = x.unsqueeze(0)\n",
        "            u = u.unsqueeze(0)\n",
        "        assert x.ndimension() == 2\n",
        "        assert x.shape[0] == u.shape[0]\n",
        "        assert x.shape[1] == 3\n",
        "        assert u.shape[1] == 1\n",
        "        assert u.ndimension() == 2\n",
        "        if x.is_cuda and not self.params.is_cuda:\n",
        "            self.params = self.params.cuda()\n",
        "        if not hasattr(self, 'simple') or self.simple:\n",
        "            g, m, l = torch.unbind(self.params)\n",
        "        else:\n",
        "            g, m, l, d, b = torch.unbind(self.params)\n",
        "        u = torch.clamp(u, -self.max_torque, self.max_torque)[:,0]\n",
        "        cos_th, sin_th, dth = torch.unbind(x, dim=1)\n",
        "        th = torch.atan2(sin_th, cos_th)\n",
        "        if not hasattr(self, 'simple') or self.simple:\n",
        "            newdth = dth + self.dt*(-3.*g/(2.*l) * (-sin_th) + 3. * u / (m*l**2))\n",
        "        else:\n",
        "            sin_th_bias = torch.sin(th + b)\n",
        "            newdth = dth + self.dt*(-3.*g/(2.*l)*(-sin_th_bias) + 3.*u/(m*l**2) - d*th)\n",
        "        newth = th + newdth*self.dt\n",
        "        state = torch.stack((torch.cos(newth), torch.sin(newth), newdth), dim=1)\n",
        "        if squeeze:\n",
        "            state = state.squeeze(0)\n",
        "        return state\n",
        "\n",
        "    def get_frame(self, x, ax=None):\n",
        "        x = get_data_maybe(x.view(-1))\n",
        "        assert len(x) == 3\n",
        "        l = self.params[2].item()\n",
        "        cos_th, sin_th, dth = torch.unbind(x)\n",
        "        th = np.arctan2(sin_th, cos_th)\n",
        "        x = sin_th*l\n",
        "        y = cos_th*l\n",
        "        if ax is None:\n",
        "            fig, ax = plt.subplots(figsize=(6,6))\n",
        "        else:\n",
        "            fig = ax.get_figure()\n",
        "        ax.plot((0,x), (0, y), color='k')\n",
        "        ax.set_xlim((-l*1.2, l*1.2))\n",
        "        ax.set_ylim((-l*1.2, l*1.2))\n",
        "        return fig, ax\n",
        "\n",
        "    # def get_true_obj(self): #cost terms for the swingup\n",
        "    #     q = torch.cat((self.goal_weights, self.ctrl_penalty*torch.ones(self.n_ctrl)))\n",
        "    #     assert not hasattr(self, 'mpc_lin')\n",
        "    #     px = -torch.sqrt(self.goal_weights)*self.goal_state #+ self.mpc_lin\n",
        "    #     p = torch.cat((px, torch.zeros(self.n_ctrl)))\n",
        "    #     return Variable(q), Variable(p)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "872dRoSSkGe9"
      },
      "source": [
        "### wwwwwww"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODWgA7GsewY7",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title setup\n",
        "# !pip install mpc\n",
        "# https://locuslab.github.io/mpc.pytorch/\n",
        "# https://github.com/locuslab/mpc.pytorch/tree/master/examples\n",
        "# https://colab.research.google.com/github/locuslab/mpc.pytorch/blob/master/examples/Pendulum%20Control.ipynb\n",
        "import torch\n",
        "import numpy as np\n",
        "import numpy.random as npr\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import io\n",
        "import base64\n",
        "import tempfile\n",
        "from IPython.display import HTML\n",
        "from tqdm import tqdm\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYOJaMvCczrj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b61b88f7-5bdf-44cd-c0fc-eca012780e07",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WARNING] pnqp warning: Did not converge\n",
            "[WARNING] pnqp warning: Did not converge\n",
            "0 [[-0.2 -0.2]] [[-0.17509563 -0.28452164  1.         -1.        ]]\n",
            "1 [[-0.23185    -0.23529498]] [[-0.18100911 -0.30179918  1.         -1.        ]]\n",
            "2 [[-0.26399294 -0.27027354]] [[-0.18096578 -0.31398958  1.         -1.        ]]\n",
            "3 [[-0.29629573 -0.3050775 ]] [[-0.1974833 -0.336871   1.        -1.       ]]\n",
            "4 [[-0.32907286 -0.33935136]] [[-0.20583177 -0.35086885  1.         -1.        ]]\n",
            "5 [[-0.36211836 -0.37331837]] [[-0.22839351 -0.3674021   1.         -1.        ]]\n",
            "6 [[-0.3956101 -0.4067604]] [[-0.2702965  -0.39168274  1.         -1.        ]]\n",
            "7 [[-0.42983505 -0.43929994]] [[-0.31932482 -0.41390368  1.         -1.        ]]\n",
            "8 [[-0.46482164 -0.47084403]] [[-0.47291186 -0.5129598   0.93203855 -0.65716136]]\n",
            "9 [[-0.49130416 -0.49320322]] [[-0.63244087 -0.61670333  0.5832975  -0.34502947]]\n",
            "10 [[-0.49831346 -0.49866238]] [[-0.6639701  -0.6327156   0.49365735 -0.26996472]]\n",
            "11 [[-0.4996941  -0.49974266]] [[-0.6700189  -0.6355926   0.47605738 -0.2552817 ]]\n",
            "12 [[-0.49994403 -0.49995053]] [[-0.6713223  -0.6363251   0.47276387 -0.25257173]]\n",
            "13 [[-0.49998975 -0.49998719]] [[-0.6714821  -0.6363861   0.47219604 -0.2521078 ]]\n",
            "14 [[-0.49999788 -0.4999972 ]] [[-0.67157716 -0.63643783  0.47210926 -0.25197977]]\n",
            "15 [[-0.5000006  -0.49999958]] [[-0.67160684 -0.6364595   0.4721159  -0.2520097 ]]\n",
            "16 [[-0.50000554 -0.50000155]] [[-0.6715668  -0.6364546   0.47200945 -0.25200135]]\n",
            "17 [[-0.5000043 -0.5000019]] [[-0.6715767  -0.6364521   0.4720017  -0.25196052]]\n",
            "18 [[-0.50000155 -0.5000012 ]] [[-0.6715547  -0.6364376   0.47200665 -0.25194407]]\n",
            "19 [[-0.49999782 -0.5000011 ]] [[-0.67162097 -0.6364471   0.4721333  -0.25195736]]\n",
            "20 [[-0.50000197 -0.5000024 ]] [[-0.67161566 -0.6364789   0.4720079  -0.2519359 ]]\n",
            "21 [[-0.49999997 -0.49999988]] [[-0.6715784  -0.63646114  0.4720379  -0.2519875 ]]\n",
            "22 [[-0.49999997 -0.50000024]] [[-0.6715944  -0.63650966  0.4719792  -0.25199938]]\n",
            "23 [[-0.49999878 -0.49999815]] [[-0.67157406 -0.6364516   0.47207066 -0.2520012 ]]\n",
            "24 [[-0.50000054 -0.49999982]] [[-0.67158985 -0.6364665   0.4720519  -0.25199106]]\n",
            "25 [[-0.50000167 -0.50000024]] [[-0.6715654  -0.6364495   0.47202116 -0.25198507]]\n",
            "26 [[-0.50000036 -0.5000007 ]] [[-0.67159516 -0.6364681   0.47203583 -0.25197527]]\n",
            "27 [[-0.50000036 -0.5000003 ]] [[-0.6716407  -0.6365206   0.47204387 -0.25199658]]\n",
            "28 [[-0.5000032  -0.49999833]] [[-0.6715508  -0.63643116  0.47201005 -0.2519648 ]]\n",
            "29 [[-0.5000001 -0.4999989]] [[-0.671569   -0.63644695  0.47204134 -0.25198042]]\n",
            "30 [[-0.49999964 -0.49999973]] [[-0.6715856  -0.63645816  0.47205126 -0.25198323]]\n",
            "31 [[-0.50000024 -0.50000024]] [[-0.6715335  -0.6364434   0.47196966 -0.25198177]]\n",
            "32 [[-0.49999565 -0.50000036]] [[-0.6715985  -0.63645285  0.47210044 -0.25198808]]\n",
            "33 [[-0.4999989 -0.5000019]] [[-0.6715776  -0.636451    0.4720346  -0.25197858]]\n",
            "34 [[-0.49999827 -0.50000226]] [[-0.6716528  -0.63651216  0.47207272 -0.25198308]]\n",
            "35 [[-0.50000215 -0.5000006 ]] [[-0.671598   -0.6364663   0.47204718 -0.25198084]]\n",
            "36 [[-0.50000286 -0.5000005 ]] [[-0.67157996 -0.63644856  0.47201568 -0.25194648]]\n",
            "37 [[-0.5000003 -0.4999998]] [[-0.6716061  -0.636467    0.47207797 -0.2519839 ]]\n",
            "38 [[-0.50000274 -0.50000024]] [[-0.6715887  -0.6364182   0.47206032 -0.25192273]]\n",
            "39 [[-0.50000113 -0.50000054]] [[-0.67161804 -0.63648295  0.47204393 -0.25197396]]\n",
            "40 [[-0.50000215 -0.49999946]] [[-0.67160124 -0.6364465   0.47206467 -0.2519428 ]]\n",
            "41 [[-0.5000022  -0.49999937]] [[-0.6715851  -0.63645226  0.4720292  -0.25195798]]\n",
            "42 [[-0.50000083 -0.49999902]] [[-0.6715959  -0.6364856   0.4720114  -0.25198746]]\n",
            "43 [[-0.50000036 -0.49999788]] [[-0.6715798  -0.6364538   0.4720621  -0.25200135]]\n",
            "44 [[-0.50000185 -0.4999992 ]] [[-0.671567   -0.6364456   0.47202167 -0.25197276]]\n",
            "45 [[-0.5000001  -0.49999952]] [[-0.67158794 -0.6364676   0.47206327 -0.25201455]]\n",
            "46 [[-0.5000025  -0.50000066]] [[-0.67163825 -0.636482    0.47206843 -0.25197384]]\n",
            "47 [[-0.50000507 -0.49999967]] [[-0.67152536 -0.6364279   0.4719622  -0.25197548]]\n",
            "48 [[-0.49999946 -0.5       ]] [[-0.6715627 -0.636462   0.4719964 -0.2519949]]\n",
            "49 [[-0.49999747 -0.4999999 ]] [[-0.6715658  -0.6364767   0.47198492 -0.25200546]]\n"
          ]
        }
      ],
      "source": [
        "# @title pytorch dx\n",
        "# again\n",
        "\n",
        "n_state = 2\n",
        "n_ctrl = 4#3\n",
        "torch.manual_seed(1)\n",
        "class model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(model, self).__init__()\n",
        "        self.param = nn.Parameter(torch.rand(1,2), requires_grad=True)\n",
        "        self.lst=[7,10,7]\n",
        "        self.lin = nn.Sequential(\n",
        "            # nn.Linear(n_ctrl + n_state, 5), nn.ReLU(),\n",
        "            # nn.Linear(n_state, 5), nn.ReLU(), nn.Sigmoid(),\n",
        "            nn.Linear(n_ctrl, self.lst[0]), nn.Tanh(),\n",
        "            # nn.Linear(self.lst[0], self.lst[1]), nn.Tanh(),\n",
        "            nn.Linear(self.lst[0], self.lst[-1]), nn.Tanh(), #nn.ReLU(),\n",
        "            nn.Linear(self.lst[-1], n_state)\n",
        "        )\n",
        "        self.n_state = n_state\n",
        "        self.n_ctrl = n_ctrl\n",
        "    def forward(self, x, u=None): # state, control\n",
        "        if u==None: u=self.param\n",
        "        # else:\n",
        "        #     with torch.no_grad():\n",
        "        #         self.param=nn.Parameter(u)\n",
        "        # print(\"xu\",x.shape, u.shape) # [1, 2] [1, n_ctrl]\n",
        "        # sx = torch.cat([u, x], dim=-1)\n",
        "        # x1 = self.lin(sx)\n",
        "        x1 = self.lin(u)\n",
        "        x1=x+0.5*x1\n",
        "        # sx = torch.cat([u, x], dim=-1)\n",
        "        return x1\n",
        "\n",
        "dx=model()\n",
        "\n",
        "n_batch, T, mpc_T = 1, 50, 45 # 16, 100, 20 ; batch size, epochs, ? larger solves\n",
        "xinit = torch.tensor([[-0.20]*n_state]) # assert x_init.ndimension() == 2 and x_init.size(0) == n_batch\n",
        "x = xinit # init state\n",
        "u_init = None # initial control?\n",
        "\n",
        "# goal_weights = torch.Tensor((0.9, 0.8, 0.7)) # 1., 1., 0.1\n",
        "goal_weights = torch.Tensor([1.0]*n_state)\n",
        "# goal_weights = torch.linspace(1, 0.1, n_state)\n",
        "\n",
        "# goal_state = torch.Tensor((1., 0. ,0.))\n",
        "# goal_state = torch.Tensor((-0.5, 0.1))\n",
        "goal_state = torch.Tensor([-0.50]*n_state)\n",
        "\n",
        "ctrl_penalty = 0.001\n",
        "q = torch.cat((goal_weights, ctrl_penalty*torch.ones(n_ctrl))) # goal_weights.shape + n_ctrl.shape\n",
        "Q = torch.diag(q).unsqueeze(0).unsqueeze(0).repeat(mpc_T, n_batch, 1, 1) # [50, 1, 5, 5]\n",
        "px = -torch.sqrt(goal_weights)*goal_state\n",
        "# px = -goal_weights*goal_state\n",
        "# px = -goal_state\n",
        "p = torch.cat((px, torch.zeros(n_ctrl)))\n",
        "p = p.unsqueeze(0).repeat(mpc_T, n_batch, 1)\n",
        "\n",
        "\n",
        "for t in range(T):\n",
        "    nominal_states, nominal_actions, nominal_objs = MPC(\n",
        "        n_state, n_ctrl, mpc_T, # state dim, action dim, \n",
        "        u_init=u_init,\n",
        "        u_lower=-1., u_upper=1., #\n",
        "        # u_lower=-0.8, u_upper=0.8, #\n",
        "        lqr_iter=50, # 50 num LQR iterations to perform\n",
        "        verbose=0, #0\n",
        "        exit_unconverged=False,\n",
        "        detach_unconverged=False,\n",
        "        linesearch_decay=0.2, #dx.linesearch_decay,\n",
        "        max_linesearch_iter=5,#dx.max_linesearch_iter,\n",
        "        grad_method=GradMethods.AUTO_DIFF,\n",
        "        eps=1e-2,\n",
        "    )(x, QuadCost(Q, p), dx)\n",
        "    \n",
        "    next_action = nominal_actions[0]\n",
        "    u_init = torch.cat((nominal_actions[1:], torch.zeros(1, n_batch, n_ctrl)), dim=0)\n",
        "    # print('u_init',u_init.shape) #[100, 1, 1]\n",
        "    # u_init[-2] = u_init[-3]\n",
        "    print(t, x.detach().numpy(), next_action.detach().numpy())\n",
        "    x = dx(x, next_action)\n",
        "    error = nn.MSELoss()(x,goal_state)\n",
        "    print(\"error\",error)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test controlability?\n",
        "torch.manual_seed(1)\n",
        "dx=model()\n",
        "# x = torch.randn(n_state).uniform_(-1, 1)\n",
        "x = torch.zeros(n_state)\n",
        "u = torch.ones(n_ctrl)\n",
        "for t in range(50):\n",
        "    # next_action = torch.randn(n_ctrl)\n",
        "    st = dx(x, u)\n",
        "    print(x.detach().numpy(), u.detach().numpy())\n",
        "    u = torch.zeros(n_ctrl)\n",
        "    x=st\n"
      ],
      "metadata": {
        "id": "Jj5FK8ojSU3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def optimal_control(dx, init_st, goal_st):\n"
      ],
      "metadata": {
        "id": "VQQdzbgBW1kU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyFOZfSG6Bpk"
      },
      "outputs": [],
      "source": [
        "# @title sentencepiece tokenizer\n",
        "!pip install sentencepiece\n",
        "# https://github.com/kutvonenaki/cc100-sentencepiece\n",
        "!git clone https://github.com/kutvonenaki/cc100-sentencepiece.git\n",
        "\n",
        "import sentencepiece as spm\n",
        "\n",
        "# cc100_en_vocab_8000.model # cc100_en_vocab_8000.vocab\n",
        "modelpath=\"/content/cc100-sentencepiece/trained_tokenizers/cc100_en_vocab_8000.model\"\n",
        "sp = spm.SentencePieceProcessor(model_file=modelpath)\n",
        "# text=\"yes tokenizers would be peferable, over traditional cat! dog?\"\n",
        "# encoded = sp.encode(text)\n",
        "# print(encoded)\n",
        "# decoded = sp.decode(encoded)\n",
        "# print(decoded)\n",
        "\n",
        "# enpieces = sp.encode_as_pieces(text)\n",
        "# print(enpieces)\n",
        "# decpieces = sp.decode_pieces(enpieces)\n",
        "# print(decpieces)\n",
        "# print(sp.get_piece_size())\n",
        "\n",
        "# https://pytorch.org/text/stable/data_functional.html#torchtext.data.functional.sentencepiece_tokenizer\n",
        "from torchtext.transforms import SentencePieceTokenizer\n",
        "sp_tokenizer = SentencePieceTokenizer(modelpath)\n",
        "sp_tokenizer([\"hello world\", \"attention is all you need!\"])\n",
        "\n",
        "for x in range(50):\n",
        "    char=next(loader)\n",
        "    print(char, end =\" \")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## JEPA"
      ],
      "metadata": {
        "id": "Dqmvyz15lCci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title setup\n",
        "# https://openreview.net/pdf?id=BZ5a1r-kVsf\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim\n",
        "import collections\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
      ],
      "metadata": {
        "id": "btQjTpnUd95O",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title data\n",
        "\n",
        "# data = open('input.txt', 'r').read()\n",
        "# data = list(data)\n",
        "\n",
        "# https://edisciplinas.usp.br/pluginfile.php/3403095/mod_resource/content/1/56ViktorFrankl_Mans%20Search.pdf\n",
        "text='''\n",
        "Only slowly could these men be guided back to the commonplace truth that no one has the right to do wrong, not\n",
        "even if wrong has been done to them. We had to strive to\n",
        "lead them back to this truth, or the consequences would\n",
        "have been much worse than the loss of a few thousand stalks\n",
        "of oats. I can still see the prisoner who rolled up his shirt\n",
        "sleeves, thrust his right hand under my nose and shouted,\n",
        "\"May this hand be cut off if I don't stain it with blood\n",
        "on the day when I get home!\" I want to emphasize that the\n",
        "man who said these words was not a bad fellow. He had\n",
        "been the best of comrades in camp and afterwards.\n",
        "Apart from the moral deformity resulting from the sudden release of mental pressure, there were two other\n",
        "fundamental experiences which threatened to damage the\n",
        "character of the liberated prisoner: bitterness and disillusionment when he returned to his former life.\n",
        "Bitterness was caused by a number of things he came up\n",
        "against in his former home town. When, on his return, a\n",
        "man found that in many places he was met only with a\n",
        "shrug of the shoulders and with hackneyed phrases, he\n",
        "tended to become bitter and to ask himself why he had\n",
        "gone through all that he had. When he heard the same\n",
        "phrases nearly everywhere\"We did not know about it,\"\n",
        "and \"We, too, have suffered,\" then he asked himself, have\n",
        "they really nothing better to say to me?\n",
        "The experience of disillusionment is different. Here it\n",
        "was not one's fellow man (whose superficiality and lack of\n",
        "feeling was so disgusting that one finally felt like creeping\n",
        "into a hole and neither hearing nor seeing human beings\n",
        "any more) but fate itself which seemed so cruel. A man who\n",
        "Experiences in a Concentration Camp 99\n",
        "for years had thought he had reached the absolute limit of\n",
        "all possible suffering now found that suffering has no limits,\n",
        "and that he could suffer still more, and still more intensely.\n",
        "When we spoke about attempts to give a man in camp\n",
        "mental courage, we said that he had to be shown something\n",
        "to look forward to in the future. He had to be reminded\n",
        "that life still waited for him, that a human being waited for\n",
        "his return. But after liberation? There were some men who\n",
        "found that no one awaited them. Woe to him who found\n",
        "that the person whose memory alone had given him courage\n",
        "in camp did not exist any more! Woe to him who, when the\n",
        "day of his dreams finally came, found it so different from all\n",
        "he had longed for! Perhaps he boarded a trolley, traveled\n",
        "out to the home which he had seen for years in his mind,\n",
        "and only in his mind, and pressed the bell, just as he has\n",
        "longed to do in thousands of dreams, only to find that the\n",
        "person who should open the door was not there, and would\n",
        "never be there again.\n",
        "'''\n",
        "\n",
        "# # make dataset\n",
        "# text=text.replace('\\n','')\n",
        "# data=list(text)\n",
        "# # print(data)\n",
        "# # data=sorted([ord(x) for x in set(data)])\n",
        "# dataset=[ord(x)-31 for x in data]\n",
        "# # dataset = map(lambda x:x.strip(\"8\"), lst)\n",
        "# # [f(x) if condition else g(x) for x in sequence]\n",
        "# dataset=[0 if x>91 else x for x in dataset] # 0 unk, 1-91\n",
        "# vocab_size=92\n",
        "# print(dataset)\n",
        "# 32space;A65-Z90;a97-z122\n",
        "\n",
        "\n",
        "encoded = sp.encode(text)\n",
        "print(encoded)\n",
        "decoded = sp.decode(encoded)\n",
        "print(decoded)\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "class Datasetme(Dataset): #https://www.kaggle.com/code/pinocookie/pytorch-dataset-and-dataloader/notebook\n",
        "    def __init__(self, text, embed_size=2):\n",
        "        super().__init__()\n",
        "        text=text.replace('\\n','')\n",
        "        data=list(text)\n",
        "        self.dataset=[self.encode(x) for x in data] # 0 unk, 1-91\n",
        "        self.vocab_size=vocab_size=92\n",
        "        self.embed_size=embed_size\n",
        "        self.batch_size = 1\n",
        "        self.embed=nn.Embedding(vocab_size, embed_size)\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)-1\n",
        "    def __getitem__(self, index):\n",
        "        # return self.dataset[index], self.dataset[index+1]\n",
        "        # for i in range(30):\n",
        "        #     print(self.dataset[i])\n",
        "        return self.embeder(self.dataset[index])\n",
        "        # return self.embeder(char0), self.embeder(char1)\n",
        "    def embeder(self, x):\n",
        "        # return self.embed(torch.tensor(self.encode(char)))\n",
        "        return self.embed(torch.tensor(x))\n",
        "    def encode(self, char):\n",
        "        x = ord(char)-31\n",
        "        if x>91: x=0\n",
        "        return x\n",
        "    def decode(self, x): return chr(x+31)\n",
        "dataset=Datasetme(text)\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "class Datasetme(Dataset):\n",
        "    def __init__(self, text, embed_size=2):\n",
        "        super().__init__()\n",
        "        text=text.replace('\\n',' ')\n",
        "        data=list(text)\n",
        "        self.dataset=self.tokenize(data)\n",
        "        self.vocab_size=vocab_size=92\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)-1\n",
        "    def __getitem__(self, index):\n",
        "        # print(\"dat: \",self.dataset[index])\n",
        "        return self.dataset[index]\n",
        "    def tokenize(self, string):\n",
        "        data=list(string)\n",
        "        return [self.token(x) for x in data] # 0 unk, 1-91\n",
        "    def token(self, char):\n",
        "        x = ord(char)-31\n",
        "        if x>91: x=0\n",
        "        return x\n",
        "    def detoken(self, x): return chr(x+31)\n",
        "    def detokenize(self, string):\n",
        "        data=list(string)\n",
        "        return [self.detoken(x) for x in data]\n",
        "dataset=Datasetme(text)\n",
        "\n",
        "# print(dataset.__getitem__(30))\n",
        "# print(dataset.embeder(91))\n",
        "\n",
        "loader=iter(dataset)\n",
        "\n",
        "text=\"yes tokenizers would be peferable over traditional cat dog\"\n",
        "encoded = dataset.tokenize(text)\n",
        "print(encoded)\n",
        "decoded = dataset.detokenize(encoded)\n",
        "print(decoded)\n",
        "\n"
      ],
      "metadata": {
        "id": "0Hz09uxCn-Eb",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_Xjga__gdeY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0ccc903-07db-473a-a6a7-d87943b904a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.0326, 0.0571, 0.0159, 0.0333, 0.0406], device='cuda:0',\n",
            "       grad_fn=<VarBackward0>) tensor([0.0272, 0.0178, 0.0244, 0.0065, 0.0253], device='cuda:0',\n",
            "       grad_fn=<VarBackward0>)\n",
            "tensor([0.1809, 0.2391, 0.1265, 0.1828, 0.2017], device='cuda:0',\n",
            "       grad_fn=<SqrtBackward0>) tensor([0.1652, 0.1339, 0.1564, 0.0812, 0.1592], device='cuda:0',\n",
            "       grad_fn=<SqrtBackward0>)\n",
            "in vicreg tensor(0.0853, device='cuda:0', grad_fn=<MseLossBackward0>) tensor(0.8373, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0596, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# @title jepa\n",
        "def off_diagonal(x):\n",
        "    # print(\"off_diagonal\",x.shape)\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "\n",
        "class JEPA(nn.Module):\n",
        "    def __init__(self, in_dimx, in_dimy, dim_sx, dim_sy, dim_z, dim_v):\n",
        "        super(JEPA, self).__init__()\n",
        "        self.enc_x = nn.Sequential(\n",
        "            nn.Linear(in_dimx, dim_sx),\n",
        "            nn.Tanh(),\n",
        "            )\n",
        "        self.enc_y = nn.Sequential(\n",
        "            nn.Linear(in_dimy, dim_sx),\n",
        "            nn.Tanh(),\n",
        "            )\n",
        "        self.pred = nn.Sequential(\n",
        "            nn.Linear(dim_sx + dim_z, dim_sy),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(dim_sy, dim_sy),\n",
        "            )\n",
        "        self.exp_x = nn.Sequential(\n",
        "            nn.Linear(dim_sx, dim_v),\n",
        "            # nn.ReLU(),\n",
        "            )\n",
        "        self.exp_y = nn.Sequential(\n",
        "            nn.Linear(dim_sy, dim_v),\n",
        "            # nn.ReLU(),\n",
        "            )\n",
        "        self.dim_z = dim_z\n",
        "        self.g_st = nn.Parameter(torch.rand(1,in_dimx), requires_grad=True)\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size=x.size(dim=0)\n",
        "        num_features=32\n",
        "        sim_coeff=25.0 # \n",
        "        std_coeff=25.0 # \n",
        "        cov_coeff=1.0 # \n",
        "\n",
        "        if x.dim() == 1:\n",
        "            x = x.view(-1, 1)\n",
        "        if y.dim() == 1:\n",
        "            y = y.view(-1, 1)\n",
        "        x=x.T\n",
        "        y=y.T\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = (sim_coeff * repr_loss + std_coeff * std_loss + cov_coeff * cov_loss)\n",
        "        print(\"in vicreg \",sim_coeff * repr_loss , std_coeff * std_loss , cov_coeff * cov_loss)\n",
        "        return loss\n",
        "\n",
        "    # https://stackoverflow.com/questions/67328098/how-to-find-input-that-maximizes-output-of-a-neural-network-using-pytorch\n",
        "    def argm(self, sx, sy, dim_z):\n",
        "        batch=sx.size(dim=0)\n",
        "        z = nn.Parameter(torch.rand(batch,dim_z), requires_grad=True)\n",
        "        # z = nn.Parameter(torch.rand(1,dim_z), requires_grad=True)\n",
        "        self.pred.requires_grad_(False)\n",
        "        optim = torch.optim.SGD([z], lr=1e-1)\n",
        "        lossfn = torch.nn.MSELoss()\n",
        "        sx=sx.detach()\n",
        "        sy=sy.detach()\n",
        "        z=z.to(device)\n",
        "        num_steps = 100\n",
        "        # print(\"argm\",sx.shape,z.shape)\n",
        "        for _ in range(num_steps):\n",
        "            sxz = torch.cat([sx, z], dim=-1)#.to(device)\n",
        "            sy_ = self.pred(sxz)\n",
        "            loss = lossfn(sy_, sy)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "        # z=z.detach()\n",
        "        return z\n",
        "\n",
        "    def loss(self, x, y):\n",
        "        if x.dim()==2: batch_size,_=x.shape\n",
        "        # print(\"loss\",x,x.shape,x.dtype)\n",
        "        # sx = self.enc_x(x)\n",
        "        # sy = self.enc_y(y)\n",
        "        sx,sy=x,y\n",
        "        # sx=sx.flatten(start_dim=1)\n",
        "        # sy=sy.flatten(start_dim=1)\n",
        "        # print(\"sx, sy\",sx.shape, sy.shape) #10000\n",
        "        z = self.argm(sx, sy, self.dim_z).to(device)\n",
        "        # z=torch.tensor(z).view(1,-1)\n",
        "        # z=z.clone().view(1,-1)\n",
        "        sxz = torch.cat([sx, z], dim=-1)\n",
        "        # self.pred.requires_grad_(True)\n",
        "        sy_ = self.pred(sxz)\n",
        "        mseloss = nn.MSELoss()(sy, sy_)\n",
        "        vx = self.exp_x(sx)\n",
        "        vy = self.exp_y(sy)\n",
        "        vicloss = self.vicreg(vx, vy)\n",
        "        return mseloss + vicloss\n",
        "\n",
        "    def forward(self, sx, a):\n",
        "        # sx = self.enc_x(x)\n",
        "        # sx=sx.flatten()\n",
        "        print(\"in fwd\",sx.shape,a.shape)\n",
        "        sxz = torch.cat([sx, a], dim=-1)\n",
        "        sy_ = self.pred(sxz)\n",
        "        return sy_\n",
        "    def encode(self, x): return self.enc_x(x)\n",
        "\n",
        "# vocab_size=dataset.vocab_size #92\n",
        "# embed_size=dataset.embed_size #4\n",
        "vocab_size=92\n",
        "embed_size=2\n",
        "\n",
        "percept_size=64 # dim of raw data from perceiving world\n",
        "encode_size=64 # dim of inner model of world state\n",
        "in_dimx=percept_size\n",
        "in_dimy=percept_size\n",
        "dim_sx=encode_size\n",
        "dim_sy=encode_size\n",
        "dim_z=embed_size # dim of action\n",
        "dim_v=5 # expanded dim for vicreg regularisation\n",
        "\n",
        "model = JEPA(in_dimx, in_dimy, dim_sx, dim_sy, dim_z, dim_v).to(device)\n",
        "\n",
        "# x=torch.rand(1, in_dimx).to(device)\n",
        "# y=torch.rand(1, in_dimy).to(device)\n",
        "\n",
        "batch=10\n",
        "x=torch.rand(batch, in_dimx).to(device)\n",
        "y=torch.rand(batch, in_dimy).to(device)\n",
        "\n",
        "loss = model.loss(x,y)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### wwwwwwwwwww"
      ],
      "metadata": {
        "id": "nsKk_OKSqfTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr = lr, betas = betas)\n",
        "# optimizer = torch.optim.AdamW(model.parameters, lr = lr, betas = betas)\n",
        "\n",
        "def loss_fn(logits, y):\n",
        "    return nn.CrossEntropyLoss()(logits.view(-1, logits.size(-1)), y.view(-1))\n",
        "\n"
      ],
      "metadata": {
        "id": "Yu4L88zkjA79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=10\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "# train_loader = DataLoader(train_dataset, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 4)\n",
        "# test_loader = DataLoader(test_dataset, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 0)\n",
        "loader = DataLoader(dataset, shuffle = False, batch_size = batch_size, num_workers = 0)\n",
        "\n",
        "epochs=1\n",
        "for epoch in range(epochs):\n",
        "    # run_epoch(train_loader)\n",
        "    # train(loader, model, loss_fn, optimizer)\n",
        "    test_loss = eval(loader, model, loss_fn)\n",
        "    print('Test Loss:', test_loss)\n"
      ],
      "metadata": {
        "id": "WNxuQST8jGeu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710
        },
        "outputId": "de8da9b7-aae8-41d5-ffca-413923dbebb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "in eval torch.Size([1, 64]) torch.Size([10, 2])\n",
            "in fwd torch.Size([1, 64]) torch.Size([10, 2])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-a2063d97acde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# run_epoch(train_loader)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# train(loader, model, loss_fn, optimizer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test Loss:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-92-c5c396ae1a04>\u001b[0m in \u001b[0;36meval\u001b[0;34m(loader, model, loss_fn)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m# print(\"x eval\",x,a)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"in eval\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0;31m# # loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;31m# loss = loss_fn(logits, y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-86-34d2f8899d22>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sx, a)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;31m# sx=sx.flatten()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"in fwd\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0msxz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0msy_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msxz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msy_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 1 but got size 10 for tensor number 1 in the list."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dembed(embedding):\n",
        "# embedding=torch.tensor([ 0.2544, -1.5085])\n",
        "    dist = torch.norm(dataset.embed.weight.data - embedding, dim=1)\n",
        "    nearest = torch.argmin(dist)\n",
        "    # print(nearest)\n",
        "    out = dataset.decode(nearest)\n",
        "    # print(out)\n",
        "    return out\n",
        "\n",
        "pbar = enumerate(loader)\n",
        "x = model.g_st.to(device)\n",
        "for it, a in pbar:\n",
        "    # print(it, a)\n",
        "    print(it, [dembed(x) for x in a])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gopZRij1suH",
        "outputId": "a4ea17c1-46bc-4077-c786-c2db4c5eb486"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 ['O', 'n', 'l', 'y', ' ', 's', 'l', 'o', 'w', 'l']\n",
            "1 ['y', ' ', 'c', 'o', 'u', 'l', 'd', ' ', 't', 'h']\n",
            "2 ['e', 's', 'e', ' ', 'm', 'e', 'n', ' ', 'b', 'e']\n",
            "3 [' ', 'g', 'u', 'i', 'd', 'e', 'd', ' ', 'b', 'a']\n",
            "4 ['c', 'k', ' ', 't', 'o', ' ', 't', 'h', 'e', ' ']\n",
            "5 ['c', 'o', 'm', 'm', 'o', 'n', 'p', 'l', 'a', 'c']\n",
            "6 ['e', ' ', 't', 'r', 'u', 't', 'h', ' ', 't', 'h']\n",
            "7 ['a', 't', ' ', 'n', 'o', ' ', 'o', 'n', 'e', ' ']\n",
            "8 ['h', 'a', 's', ' ', 't', 'h', 'e', ' ', 'r', 'i']\n",
            "9 ['g', 'h', 't', ' ', 't', 'o', ' ', 'd', 'o', ' ']\n",
            "10 ['w', 'r', 'o', 'n', 'g', ',', ' ', 'n', 'o', 't']\n",
            "11 ['e', 'v', 'e', 'n', ' ', 'i', 'f', ' ', 'w', 'r']\n",
            "12 ['o', 'n', 'g', ' ', 'h', 'a', 's', ' ', 'b', 'e']\n",
            "13 ['e', 'n', ' ', 'd', 'o', 'n', 'e', ' ', 't', 'o']\n",
            "14 [' ', 't', 'h', 'e', 'm', '.', ' ', 'W', 'e', ' ']\n",
            "15 ['h', 'a', 'd', ' ', 't', 'o', ' ', 's', 't', 'r']\n",
            "16 ['i', 'v', 'e', ' ', 't', 'o', 'l', 'e', 'a', 'd']\n",
            "17 [' ', 't', 'h', 'e', 'm', ' ', 'b', 'a', 'c', 'k']\n",
            "18 [' ', 't', 'o', ' ', 't', 'h', 'i', 's', ' ', 't']\n",
            "19 ['r', 'u', 't', 'h', ',', ' ', 'o', 'r', ' ', 't']\n",
            "20 ['h', 'e', ' ', 'c', 'o', 'n', 's', 'e', 'q', 'u']\n",
            "21 ['e', 'n', 'c', 'e', 's', ' ', 'w', 'o', 'u', 'l']\n",
            "22 ['d', 'h', 'a', 'v', 'e', ' ', 'b', 'e', 'e', 'n']\n",
            "23 [' ', 'm', 'u', 'c', 'h', ' ', 'w', 'o', 'r', 's']\n",
            "24 ['e', ' ', 't', 'h', 'a', 'n', ' ', 't', 'h', 'e']\n",
            "25 [' ', 'l', 'o', 's', 's', ' ', 'o', 'f', ' ', 'a']\n",
            "26 [' ', 'f', 'e', 'w', ' ', 't', 'h', 'o', 'u', 's']\n",
            "27 ['a', 'n', 'd', ' ', 's', 't', 'a', 'l', 'k', 's']\n",
            "28 ['o', 'f', ' ', 'o', 'a', 't', 's', '.', ' ', 'I']\n",
            "29 [' ', 'c', 'a', 'n', ' ', 's', 't', 'i', 'l', 'l']\n",
            "30 [' ', 's', 'e', 'e', ' ', 't', 'h', 'e', ' ', 'p']\n",
            "31 ['r', 'i', 's', 'o', 'n', 'e', 'r', ' ', 'w', 'h']\n",
            "32 ['o', ' ', 'r', 'o', 'l', 'l', 'e', 'd', ' ', 'u']\n",
            "33 ['p', ' ', 'h', 'i', 's', ' ', 's', 'h', 'i', 'r']\n",
            "34 ['t', 's', 'l', 'e', 'e', 'v', 'e', 's', ',', ' ']\n",
            "35 ['t', 'h', 'r', 'u', 's', 't', ' ', 'h', 'i', 's']\n",
            "36 [' ', 'r', 'i', 'g', 'h', 't', ' ', 'h', 'a', 'n']\n",
            "37 ['d', ' ', 'u', 'n', 'd', 'e', 'r', ' ', 'm', 'y']\n",
            "38 [' ', 'n', 'o', 's', 'e', ' ', 'a', 'n', 'd', ' ']\n",
            "39 ['s', 'h', 'o', 'u', 't', 'e', 'd', ',', '\"', 'M']\n",
            "40 ['a', 'y', ' ', 't', 'h', 'i', 's', ' ', 'h', 'a']\n",
            "41 ['n', 'd', ' ', 'b', 'e', ' ', 'c', 'u', 't', ' ']\n",
            "42 ['o', 'f', 'f', ' ', 'i', 'f', ' ', 'I', ' ', 'd']\n",
            "43 ['o', 'n', \"'\", 't', ' ', 's', 't', 'a', 'i', 'n']\n",
            "44 [' ', 'i', 't', ' ', 'w', 'i', 't', 'h', ' ', 'b']\n",
            "45 ['l', 'o', 'o', 'd', 'o', 'n', ' ', 't', 'h', 'e']\n",
            "46 [' ', 'd', 'a', 'y', ' ', 'w', 'h', 'e', 'n', ' ']\n",
            "47 ['I', ' ', 'g', 'e', 't', ' ', 'h', 'o', 'm', 'e']\n",
            "48 ['!', '\"', ' ', 'I', ' ', 'w', 'a', 'n', 't', ' ']\n",
            "49 ['t', 'o', ' ', 'e', 'm', 'p', 'h', 'a', 's', 'i']\n",
            "50 ['z', 'e', ' ', 't', 'h', 'a', 't', ' ', 't', 'h']\n",
            "51 ['e', 'm', 'a', 'n', ' ', 'w', 'h', 'o', ' ', 's']\n",
            "52 ['a', 'i', 'd', ' ', 't', 'h', 'e', 's', 'e', ' ']\n",
            "53 ['w', 'o', 'r', 'd', 's', ' ', 'w', 'a', 's', ' ']\n",
            "54 ['n', 'o', 't', ' ', 'a', ' ', 'b', 'a', 'd', ' ']\n",
            "55 ['f', 'e', 'l', 'l', 'o', 'w', '.', ' ', 'H', 'e']\n",
            "56 [' ', 'h', 'a', 'd', 'b', 'e', 'e', 'n', ' ', 't']\n",
            "57 ['h', 'e', ' ', 'b', 'e', 's', 't', ' ', 'o', 'f']\n",
            "58 [' ', 'c', 'o', 'm', 'r', 'a', 'd', 'e', 's', ' ']\n",
            "59 ['i', 'n', ' ', 'c', 'a', 'm', 'p', ' ', 'a', 'n']\n",
            "60 ['d', ' ', 'a', 'f', 't', 'e', 'r', 'w', 'a', 'r']\n",
            "61 ['d', 's', '.', 'A', 'p', 'a', 'r', 't', ' ', 'f']\n",
            "62 ['r', 'o', 'm', ' ', 't', 'h', 'e', ' ', 'm', 'o']\n",
            "63 ['r', 'a', 'l', ' ', 'd', 'e', 'f', 'o', 'r', 'm']\n",
            "64 ['i', 't', 'y', ' ', 'r', 'e', 's', 'u', 'l', 't']\n",
            "65 ['i', 'n', 'g', ' ', 'f', 'r', 'o', 'm', ' ', 't']\n",
            "66 ['h', 'e', ' ', 's', 'u', 'd', 'd', 'e', 'n', ' ']\n",
            "67 ['r', 'e', 'l', 'e', 'a', 's', 'e', ' ', 'o', 'f']\n",
            "68 [' ', 'm', 'e', 'n', 't', 'a', 'l', ' ', 'p', 'r']\n",
            "69 ['e', 's', 's', 'u', 'r', 'e', ',', ' ', 't', 'h']\n",
            "70 ['e', 'r', 'e', ' ', 'w', 'e', 'r', 'e', ' ', 't']\n",
            "71 ['w', 'o', ' ', 'o', 't', 'h', 'e', 'r', 'f', 'u']\n",
            "72 ['n', 'd', 'a', 'm', 'e', 'n', 't', 'a', 'l', ' ']\n",
            "73 ['e', 'x', 'p', 'e', 'r', 'i', 'e', 'n', 'c', 'e']\n",
            "74 ['s', ' ', 'w', 'h', 'i', 'c', 'h', ' ', 't', 'h']\n",
            "75 ['r', 'e', 'a', 't', 'e', 'n', 'e', 'd', ' ', 't']\n",
            "76 ['o', ' ', 'd', 'a', 'm', 'a', 'g', 'e', ' ', 't']\n",
            "77 ['h', 'e', 'c', 'h', 'a', 'r', 'a', 'c', 't', 'e']\n",
            "78 ['r', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'l']\n",
            "79 ['i', 'b', 'e', 'r', 'a', 't', 'e', 'd', ' ', 'p']\n",
            "80 ['r', 'i', 's', 'o', 'n', 'e', 'r', ':', ' ', 'b']\n",
            "81 ['i', 't', 't', 'e', 'r', 'n', 'e', 's', 's', ' ']\n",
            "82 ['a', 'n', 'd', ' ', 'd', 'i', 's', 'i', 'l', 'l']\n",
            "83 ['u', 's', 'i', 'o', 'n', 'm', 'e', 'n', 't', ' ']\n",
            "84 ['w', 'h', 'e', 'n', ' ', 'h', 'e', ' ', 'r', 'e']\n",
            "85 ['t', 'u', 'r', 'n', 'e', 'd', ' ', 't', 'o', ' ']\n",
            "86 ['h', 'i', 's', ' ', 'f', 'o', 'r', 'm', 'e', 'r']\n",
            "87 [' ', 'l', 'i', 'f', 'e', '.', 'B', 'i', 't', 't']\n",
            "88 ['e', 'r', 'n', 'e', 's', 's', ' ', 'w', 'a', 's']\n",
            "89 [' ', 'c', 'a', 'u', 's', 'e', 'd', ' ', 'b', 'y']\n",
            "90 [' ', 'a', ' ', 'n', 'u', 'm', 'b', 'e', 'r', ' ']\n",
            "91 ['o', 'f', ' ', 't', 'h', 'i', 'n', 'g', 's', ' ']\n",
            "92 ['h', 'e', ' ', 'c', 'a', 'm', 'e', ' ', 'u', 'p']\n",
            "93 ['a', 'g', 'a', 'i', 'n', 's', 't', ' ', 'i', 'n']\n",
            "94 [' ', 'h', 'i', 's', ' ', 'f', 'o', 'r', 'm', 'e']\n",
            "95 ['r', ' ', 'h', 'o', 'm', 'e', ' ', 't', 'o', 'w']\n",
            "96 ['n', '.', ' ', 'W', 'h', 'e', 'n', ',', ' ', 'o']\n",
            "97 ['n', ' ', 'h', 'i', 's', ' ', 'r', 'e', 't', 'u']\n",
            "98 ['r', 'n', ',', ' ', 'a', 'm', 'a', 'n', ' ', 'f']\n",
            "99 ['o', 'u', 'n', 'd', ' ', 't', 'h', 'a', 't', ' ']\n",
            "100 ['i', 'n', ' ', 'm', 'a', 'n', 'y', ' ', 'p', 'l']\n",
            "101 ['a', 'c', 'e', 's', ' ', 'h', 'e', ' ', 'w', 'a']\n",
            "102 ['s', ' ', 'm', 'e', 't', ' ', 'o', 'n', 'l', 'y']\n",
            "103 [' ', 'w', 'i', 't', 'h', ' ', 'a', 's', 'h', 'r']\n",
            "104 ['u', 'g', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ']\n",
            "105 ['s', 'h', 'o', 'u', 'l', 'd', 'e', 'r', 's', ' ']\n",
            "106 ['a', 'n', 'd', ' ', 'w', 'i', 't', 'h', ' ', 'h']\n",
            "107 ['a', 'c', 'k', 'n', 'e', 'y', 'e', 'd', ' ', 'p']\n",
            "108 ['h', 'r', 'a', 's', 'e', 's', ',', ' ', 'h', 'e']\n",
            "109 ['t', 'e', 'n', 'd', 'e', 'd', ' ', 't', 'o', ' ']\n",
            "110 ['b', 'e', 'c', 'o', 'm', 'e', ' ', 'b', 'i', 't']\n",
            "111 ['t', 'e', 'r', ' ', 'a', 'n', 'd', ' ', 't', 'o']\n",
            "112 [' ', 'a', 's', 'k', ' ', 'h', 'i', 'm', 's', 'e']\n",
            "113 ['l', 'f', ' ', 'w', 'h', 'y', ' ', 'h', 'e', ' ']\n",
            "114 ['h', 'a', 'd', 'g', 'o', 'n', 'e', ' ', 't', 'h']\n",
            "115 ['r', 'o', 'u', 'g', 'h', ' ', 'a', 'l', 'l', ' ']\n",
            "116 ['t', 'h', 'a', 't', ' ', 'h', 'e', ' ', 'h', 'a']\n",
            "117 ['d', '.', ' ', 'W', 'h', 'e', 'n', ' ', 'h', 'e']\n",
            "118 [' ', 'h', 'e', 'a', 'r', 'd', ' ', 't', 'h', 'e']\n",
            "119 [' ', 's', 'a', 'm', 'e', 'p', 'h', 'r', 'a', 's']\n",
            "120 ['e', 's', ' ', 'n', 'e', 'a', 'r', 'l', 'y', ' ']\n",
            "121 ['e', 'v', 'e', 'r', 'y', 'w', 'h', 'e', 'r', 'e']\n",
            "122 ['\\x1f', '\"', 'W', 'e', ' ', 'd', 'i', 'd', ' ', 'n']\n",
            "123 ['o', 't', ' ', 'k', 'n', 'o', 'w', ' ', 'a', 'b']\n",
            "124 ['o', 'u', 't', ' ', 'i', 't', ',', '\"', 'a', 'n']\n",
            "125 ['d', ' ', '\"', 'W', 'e', ',', ' ', 't', 'o', 'o']\n",
            "126 [',', ' ', 'h', 'a', 'v', 'e', ' ', 's', 'u', 'f']\n",
            "127 ['f', 'e', 'r', 'e', 'd', ',', '\"', ' ', 't', 'h']\n",
            "128 ['e', 'n', ' ', 'h', 'e', ' ', 'a', 's', 'k', 'e']\n",
            "129 ['d', ' ', 'h', 'i', 'm', 's', 'e', 'l', 'f', ',']\n",
            "130 [' ', 'h', 'a', 'v', 'e', 't', 'h', 'e', 'y', ' ']\n",
            "131 ['r', 'e', 'a', 'l', 'l', 'y', ' ', 'n', 'o', 't']\n",
            "132 ['h', 'i', 'n', 'g', ' ', 'b', 'e', 't', 't', 'e']\n",
            "133 ['r', ' ', 't', 'o', ' ', 's', 'a', 'y', ' ', 't']\n",
            "134 ['o', ' ', 'm', 'e', '?', 'T', 'h', 'e', ' ', 'e']\n",
            "135 ['x', 'p', 'e', 'r', 'i', 'e', 'n', 'c', 'e', ' ']\n",
            "136 ['o', 'f', ' ', 'd', 'i', 's', 'i', 'l', 'l', 'u']\n",
            "137 ['s', 'i', 'o', 'n', 'm', 'e', 'n', 't', ' ', 'i']\n",
            "138 ['s', ' ', 'd', 'i', 'f', 'f', 'e', 'r', 'e', 'n']\n",
            "139 ['t', '.', ' ', 'H', 'e', 'r', 'e', ' ', 'i', 't']\n",
            "140 ['w', 'a', 's', ' ', 'n', 'o', 't', ' ', 'o', 'n']\n",
            "141 ['e', \"'\", 's', ' ', 'f', 'e', 'l', 'l', 'o', 'w']\n",
            "142 [' ', 'm', 'a', 'n', ' ', '(', 'w', 'h', 'o', 's']\n",
            "143 ['e', ' ', 's', 'u', 'p', 'e', 'r', 'f', 'i', 'c']\n",
            "144 ['i', 'a', 'l', 'i', 't', 'y', ' ', 'a', 'n', 'd']\n",
            "145 [' ', 'l', 'a', 'c', 'k', ' ', 'o', 'f', 'f', 'e']\n",
            "146 ['e', 'l', 'i', 'n', 'g', ' ', 'w', 'a', 's', ' ']\n",
            "147 ['s', 'o', ' ', 'd', 'i', 's', 'g', 'u', 's', 't']\n",
            "148 ['i', 'n', 'g', ' ', 't', 'h', 'a', 't', ' ', 'o']\n",
            "149 ['n', 'e', ' ', 'f', 'i', 'n', 'a', 'l', 'l', 'y']\n",
            "150 [' ', 'f', 'e', 'l', 't', ' ', 'l', 'i', 'k', 'e']\n",
            "151 [' ', 'c', 'r', 'e', 'e', 'p', 'i', 'n', 'g', 'i']\n",
            "152 ['n', 't', 'o', ' ', 'a', ' ', 'h', 'o', 'l', 'e']\n",
            "153 [' ', 'a', 'n', 'd', ' ', 'n', 'e', 'i', 't', 'h']\n",
            "154 ['e', 'r', ' ', 'h', 'e', 'a', 'r', 'i', 'n', 'g']\n",
            "155 [' ', 'n', 'o', 'r', ' ', 's', 'e', 'e', 'i', 'n']\n",
            "156 ['g', ' ', 'h', 'u', 'm', 'a', 'n', ' ', 'b', 'e']\n",
            "157 ['i', 'n', 'g', 's', 'a', 'n', 'y', ' ', 'm', 'o']\n",
            "158 ['r', 'e', ')', ' ', 'b', 'u', 't', ' ', 'f', 'a']\n",
            "159 ['t', 'e', ' ', 'i', 't', 's', 'e', 'l', 'f', ' ']\n",
            "160 ['w', 'h', 'i', 'c', 'h', ' ', 's', 'e', 'e', 'm']\n",
            "161 ['e', 'd', ' ', 's', 'o', ' ', 'c', 'r', 'u', 'e']\n",
            "162 ['l', '.', ' ', 'A', ' ', 'm', 'a', 'n', ' ', 'w']\n",
            "163 ['h', 'o', 'E', 'x', 'p', 'e', 'r', 'i', 'e', 'n']\n",
            "164 ['c', 'e', 's', ' ', 'i', 'n', ' ', 'a', ' ', 'C']\n",
            "165 ['o', 'n', 'c', 'e', 'n', 't', 'r', 'a', 't', 'i']\n",
            "166 ['o', 'n', ' ', 'C', 'a', 'm', 'p', ' ', '9', '9']\n",
            "167 ['f', 'o', 'r', ' ', 'y', 'e', 'a', 'r', 's', ' ']\n",
            "168 ['h', 'a', 'd', ' ', 't', 'h', 'o', 'u', 'g', 'h']\n",
            "169 ['t', ' ', 'h', 'e', ' ', 'h', 'a', 'd', ' ', 'r']\n",
            "170 ['e', 'a', 'c', 'h', 'e', 'd', ' ', 't', 'h', 'e']\n",
            "171 [' ', 'a', 'b', 's', 'o', 'l', 'u', 't', 'e', ' ']\n",
            "172 ['l', 'i', 'm', 'i', 't', ' ', 'o', 'f', 'a', 'l']\n",
            "173 ['l', ' ', 'p', 'o', 's', 's', 'i', 'b', 'l', 'e']\n",
            "174 [' ', 's', 'u', 'f', 'f', 'e', 'r', 'i', 'n', 'g']\n",
            "175 [' ', 'n', 'o', 'w', ' ', 'f', 'o', 'u', 'n', 'd']\n",
            "176 [' ', 't', 'h', 'a', 't', ' ', 's', 'u', 'f', 'f']\n",
            "177 ['e', 'r', 'i', 'n', 'g', ' ', 'h', 'a', 's', ' ']\n",
            "178 ['n', 'o', ' ', 'l', 'i', 'm', 'i', 't', 's', ',']\n",
            "179 ['a', 'n', 'd', ' ', 't', 'h', 'a', 't', ' ', 'h']\n",
            "180 ['e', ' ', 'c', 'o', 'u', 'l', 'd', ' ', 's', 'u']\n",
            "181 ['f', 'f', 'e', 'r', ' ', 's', 't', 'i', 'l', 'l']\n",
            "182 [' ', 'm', 'o', 'r', 'e', ',', ' ', 'a', 'n', 'd']\n",
            "183 [' ', 's', 't', 'i', 'l', 'l', ' ', 'm', 'o', 'r']\n",
            "184 ['e', ' ', 'i', 'n', 't', 'e', 'n', 's', 'e', 'l']\n",
            "185 ['y', '.', 'W', 'h', 'e', 'n', ' ', 'w', 'e', ' ']\n",
            "186 ['s', 'p', 'o', 'k', 'e', ' ', 'a', 'b', 'o', 'u']\n",
            "187 ['t', ' ', 'a', 't', 't', 'e', 'm', 'p', 't', 's']\n",
            "188 [' ', 't', 'o', ' ', 'g', 'i', 'v', 'e', ' ', 'a']\n",
            "189 [' ', 'm', 'a', 'n', ' ', 'i', 'n', ' ', 'c', 'a']\n",
            "190 ['m', 'p', 'm', 'e', 'n', 't', 'a', 'l', ' ', 'c']\n",
            "191 ['o', 'u', 'r', 'a', 'g', 'e', ',', ' ', 'w', 'e']\n",
            "192 [' ', 's', 'a', 'i', 'd', ' ', 't', 'h', 'a', 't']\n",
            "193 [' ', 'h', 'e', ' ', 'h', 'a', 'd', ' ', 't', 'o']\n",
            "194 [' ', 'b', 'e', ' ', 's', 'h', 'o', 'w', 'n', ' ']\n",
            "195 ['s', 'o', 'm', 'e', 't', 'h', 'i', 'n', 'g', 't']\n",
            "196 ['o', ' ', 'l', 'o', 'o', 'k', ' ', 'f', 'o', 'r']\n",
            "197 ['w', 'a', 'r', 'd', ' ', 't', 'o', ' ', 'i', 'n']\n",
            "198 [' ', 't', 'h', 'e', ' ', 'f', 'u', 't', 'u', 'r']\n",
            "199 ['e', '.', ' ', 'H', 'e', ' ', 'h', 'a', 'd', ' ']\n",
            "200 ['t', 'o', ' ', 'b', 'e', ' ', 'r', 'e', 'm', 'i']\n",
            "201 ['n', 'd', 'e', 'd', 't', 'h', 'a', 't', ' ', 'l']\n",
            "202 ['i', 'f', 'e', ' ', 's', 't', 'i', 'l', 'l', ' ']\n",
            "203 ['w', 'a', 'i', 't', 'e', 'd', ' ', 'f', 'o', 'r']\n",
            "204 [' ', 'h', 'i', 'm', ',', ' ', 't', 'h', 'a', 't']\n",
            "205 [' ', 'a', ' ', 'h', 'u', 'm', 'a', 'n', ' ', 'b']\n",
            "206 ['e', 'i', 'n', 'g', ' ', 'w', 'a', 'i', 't', 'e']\n",
            "207 ['d', ' ', 'f', 'o', 'r', 'h', 'i', 's', ' ', 'r']\n",
            "208 ['e', 't', 'u', 'r', 'n', '.', ' ', 'B', 'u', 't']\n",
            "209 [' ', 'a', 'f', 't', 'e', 'r', ' ', 'l', 'i', 'b']\n",
            "210 ['e', 'r', 'a', 't', 'i', 'o', 'n', '?', ' ', 'T']\n",
            "211 ['h', 'e', 'r', 'e', ' ', 'w', 'e', 'r', 'e', ' ']\n",
            "212 ['s', 'o', 'm', 'e', ' ', 'm', 'e', 'n', ' ', 'w']\n",
            "213 ['h', 'o', 'f', 'o', 'u', 'n', 'd', ' ', 't', 'h']\n",
            "214 ['a', 't', ' ', 'n', 'o', ' ', 'o', 'n', 'e', ' ']\n",
            "215 ['a', 'w', 'a', 'i', 't', 'e', 'd', ' ', 't', 'h']\n",
            "216 ['e', 'm', '.', ' ', 'W', 'o', 'e', ' ', 't', 'o']\n",
            "217 [' ', 'h', 'i', 'm', ' ', 'w', 'h', 'o', ' ', 'f']\n",
            "218 ['o', 'u', 'n', 'd', 't', 'h', 'a', 't', ' ', 't']\n",
            "219 ['h', 'e', ' ', 'p', 'e', 'r', 's', 'o', 'n', ' ']\n",
            "220 ['w', 'h', 'o', 's', 'e', ' ', 'm', 'e', 'm', 'o']\n",
            "221 ['r', 'y', ' ', 'a', 'l', 'o', 'n', 'e', ' ', 'h']\n",
            "222 ['a', 'd', ' ', 'g', 'i', 'v', 'e', 'n', ' ', 'h']\n",
            "223 ['i', 'm', ' ', 'c', 'o', 'u', 'r', 'a', 'g', 'e']\n",
            "224 ['i', 'n', ' ', 'c', 'a', 'm', 'p', ' ', 'd', 'i']\n",
            "225 ['d', ' ', 'n', 'o', 't', ' ', 'e', 'x', 'i', 's']\n",
            "226 ['t', ' ', 'a', 'n', 'y', ' ', 'm', 'o', 'r', 'e']\n",
            "227 ['!', ' ', 'W', 'o', 'e', ' ', 't', 'o', ' ', 'h']\n",
            "228 ['i', 'm', ' ', 'w', 'h', 'o', ',', ' ', 'w', 'h']\n",
            "229 ['e', 'n', ' ', 't', 'h', 'e', 'd', 'a', 'y', ' ']\n",
            "230 ['o', 'f', ' ', 'h', 'i', 's', ' ', 'd', 'r', 'e']\n",
            "231 ['a', 'm', 's', ' ', 'f', 'i', 'n', 'a', 'l', 'l']\n",
            "232 ['y', ' ', 'c', 'a', 'm', 'e', ',', ' ', 'f', 'o']\n",
            "233 ['u', 'n', 'd', ' ', 'i', 't', ' ', 's', 'o', ' ']\n",
            "234 ['d', 'i', 'f', 'f', 'e', 'r', 'e', 'n', 't', ' ']\n",
            "235 ['f', 'r', 'o', 'm', ' ', 'a', 'l', 'l', 'h', 'e']\n",
            "236 [' ', 'h', 'a', 'd', ' ', 'l', 'o', 'n', 'g', 'e']\n",
            "237 ['d', ' ', 'f', 'o', 'r', '!', ' ', 'P', 'e', 'r']\n",
            "238 ['h', 'a', 'p', 's', ' ', 'h', 'e', ' ', 'b', 'o']\n",
            "239 ['a', 'r', 'd', 'e', 'd', ' ', 'a', ' ', 't', 'r']\n",
            "240 ['o', 'l', 'l', 'e', 'y', ',', ' ', 't', 'r', 'a']\n",
            "241 ['v', 'e', 'l', 'e', 'd', 'o', 'u', 't', ' ', 't']\n",
            "242 ['o', ' ', 't', 'h', 'e', ' ', 'h', 'o', 'm', 'e']\n",
            "243 [' ', 'w', 'h', 'i', 'c', 'h', ' ', 'h', 'e', ' ']\n",
            "244 ['h', 'a', 'd', ' ', 's', 'e', 'e', 'n', ' ', 'f']\n",
            "245 ['o', 'r', ' ', 'y', 'e', 'a', 'r', 's', ' ', 'i']\n",
            "246 ['n', ' ', 'h', 'i', 's', ' ', 'm', 'i', 'n', 'd']\n",
            "247 [',', 'a', 'n', 'd', ' ', 'o', 'n', 'l', 'y', ' ']\n",
            "248 ['i', 'n', ' ', 'h', 'i', 's', ' ', 'm', 'i', 'n']\n",
            "249 ['d', ',', ' ', 'a', 'n', 'd', ' ', 'p', 'r', 'e']\n",
            "250 ['s', 's', 'e', 'd', ' ', 't', 'h', 'e', ' ', 'b']\n",
            "251 ['e', 'l', 'l', ',', ' ', 'j', 'u', 's', 't', ' ']\n",
            "252 ['a', 's', ' ', 'h', 'e', ' ', 'h', 'a', 's', 'l']\n",
            "253 ['o', 'n', 'g', 'e', 'd', ' ', 't', 'o', ' ', 'd']\n",
            "254 ['o', ' ', 'i', 'n', ' ', 't', 'h', 'o', 'u', 's']\n",
            "255 ['a', 'n', 'd', 's', ' ', 'o', 'f', ' ', 'd', 'r']\n",
            "256 ['e', 'a', 'm', 's', ',', ' ', 'o', 'n', 'l', 'y']\n",
            "257 [' ', 't', 'o', ' ', 'f', 'i', 'n', 'd', ' ', 't']\n",
            "258 ['h', 'a', 't', ' ', 't', 'h', 'e', 'p', 'e', 'r']\n",
            "259 ['s', 'o', 'n', ' ', 'w', 'h', 'o', ' ', 's', 'h']\n",
            "260 ['o', 'u', 'l', 'd', ' ', 'o', 'p', 'e', 'n', ' ']\n",
            "261 ['t', 'h', 'e', ' ', 'd', 'o', 'o', 'r', ' ', 'w']\n",
            "262 ['a', 's', ' ', 'n', 'o', 't', ' ', 't', 'h', 'e']\n",
            "263 ['r', 'e', ',', ' ', 'a', 'n', 'd', ' ', 'w', 'o']\n",
            "264 ['u', 'l', 'd', 'n', 'e', 'v', 'e', 'r', ' ', 'b']\n",
            "265 ['e', ' ', 't', 'h', 'e', 'r', 'e', ' ', 'a', 'g']\n",
            "266 ['a', 'i', 'n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title save\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "PATH=\"/content/gdrive/MyDrive/torch_save/\" # for saving to google drive\n",
        "name='jepa_mpc.pth'\n",
        "# PATH=\"/content/\" # for saving on colab only\n",
        "# name='model.pth'\n",
        "\n",
        "torch.save(model.state_dict(), PATH+name)\n",
        "\n",
        "# model.load_state_dict(torch.load(PATH+name))\n"
      ],
      "metadata": {
        "id": "UMg8i6HfjPBX",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### inference"
      ],
      "metadata": {
        "id": "2gLKjKZej5L_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "char='l'\n",
        "a=dataset.encode(char)\n",
        "a=dataset.embeder(a)\n",
        "print(a)\n",
        "# a=torch.tensor(a).view(1,-1).to(device)\n",
        "# a=torch.tensor(a.clone()).view(1,-1).to(device)\n",
        "a=a.clone().view(1,-1).to(device)\n",
        "sx=model.encode(x)\n",
        "# print(\"sx a\",sx,a,sx.shape, a.shape)\n",
        "sy = model(sx, a)\n",
        "print(sy)\n"
      ],
      "metadata": {
        "id": "GLmVXWyimQBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "goal_st=torch.tensor([-0.0426, -0.2183, -0.0351,  0.0014, -0.0591, -0.0012, -0.0198,  0.0319,\n",
        "          0.1887,  0.1562,  0.0134,  0.1240, -0.1527, -0.0601,  0.0372,  0.0323,\n",
        "          0.0605, -0.0699,  0.0766, -0.0679, -0.0489, -0.1938, -0.1501, -0.0716,\n",
        "          0.2145,  0.0125, -0.1071, -0.0086,  0.2173,  0.0805, -0.1742,  0.0086,\n",
        "         -0.0442,  0.2411, -0.0668, -0.1390,  0.0308,  0.1623, -0.2207,  0.1363,\n",
        "          0.0945,  0.1521, -0.1333,  0.0900, -0.0801,  0.1209,  0.0082,  0.0654,\n",
        "          0.0111,  0.1649,  0.1243, -0.2940, -0.0775, -0.0036,  0.1020,  0.0426,\n",
        "         -0.2507,  0.1013, -0.0405,  0.0273,  0.0664, -0.0432,  0.2488,  0.0915]).to(device)\n",
        "init_st=model.g_st\n",
        "dx=model\n",
        "ctrl=optimal_control(dx, init_st, goal_st)\n",
        "print(ctrl)\n"
      ],
      "metadata": {
        "id": "YElHNc_nnNGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.g_st)"
      ],
      "metadata": {
        "id": "rsN2llQCK2wL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "context = \"This is what \"\n",
        "\n",
        "# def encode(char):\n",
        "#     x = ord(char)-31\n",
        "#     if x>91: x=0\n",
        "#     return x\n",
        "# def decode(x): return chr(x+31)\n",
        "\n",
        "out=[]\n",
        "x = model.g_st\n",
        "for i in range(13):\n",
        "    # a=torch.zeros(1,1).to(device)\n",
        "    char=context[i]\n",
        "    a=dataset.encode(char)\n",
        "    a=dataset.embeder(a)\n",
        "    # print(a)\n",
        "    # a=torch.tensor(a).view(1,-1).to(device)\n",
        "    a=a.clone().view(1,-1).to(device)\n",
        "    sx=model.encode(x)\n",
        "    # print(\"sx a\",sx,a,sx.shape, a.shape)\n",
        "    sy = model(sx, a)\n",
        "    # out.append(sy)\n",
        "    sx=sy\n",
        "# print(out)\n",
        "print(''.join(out))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0H2ER22wNahP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### archive"
      ],
      "metadata": {
        "id": "6l_mHBAY4H_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### train eval"
      ],
      "metadata": {
        "id": "oEr6soVkqbQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from tqdm import tqdm\n",
        "\n",
        "# grad_clip_norm=1\n",
        "# lr=1e-4\n",
        "# betas=(0.9, 0.95)\n",
        "# batch_size=1\n",
        "# def train(loader, model, loss_fn, optimizer):\n",
        "#     model.train()\n",
        "#     losses = []\n",
        "#     pbar = tqdm(enumerate(loader), total = len(loader))\n",
        "#     for it, (x, y) in pbar:\n",
        "#         # print(\"x,y\",x.dtype,y.dtype) #torch.float32 [1,4]\n",
        "#         # print(\"x,y\",x.shape,y.shape)\n",
        "#         x = x.to(device)\n",
        "#         y = y.to(device)\n",
        "#         # print(\"x\",x)\n",
        "#         # with torch.set_grad_enabled(True):\n",
        "\n",
        "#         # logits = model(x)\n",
        "#         # # loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
        "#         # loss = loss_fn(logits, y)\n",
        "#         loss = model.loss(x,y)\n",
        "#         losses.append(loss.item())\n",
        "\n",
        "#         model.zero_grad()\n",
        "#         loss.backward()\n",
        "#         torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)\n",
        "#         optimizer.step()\n",
        "#         # lr = lr\n",
        "#         # pbar.set_description(f\"epoch {epoch + 1} iter {it}: train loss {loss.item():.5f}. lr {lr:e}\")\n",
        "#         pbar.set_description(f\"epoch {epoch + 1} iter {it}: train loss {loss.item():.5f}\")\n",
        "\n",
        "\n",
        "# def eval(loader, model, loss_fn):\n",
        "#     model.eval()\n",
        "#     losses = []\n",
        "#     pbar = enumerate(loader)\n",
        "#     for it, (x, y) in pbar:\n",
        "#         x = x.to(device)\n",
        "#         y = y.to(device)\n",
        "#         # with torch.set_grad_enabled(False):\n",
        "#         with torch.no_grad():\n",
        "#             # logits = model(x)\n",
        "#             # # loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
        "#             # loss = loss_fn(logits, y)\n",
        "#             loss = model.loss(x,y)\n",
        "#             losses.append(loss.item())\n",
        "#     test_loss = float(np.mean(losses))\n",
        "#     # logger.info(\"test loss: %f\", test_loss)\n",
        "#     return test_loss\n"
      ],
      "metadata": {
        "id": "V_EUHNNBgxuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # def loss(self, x, y):\n",
        "    #     sx = self.enc_x(x)\n",
        "    #     sy = self.enc_y(y)\n",
        "    #     sx=sx.flatten()\n",
        "    #     sy=sy.flatten()\n",
        "    #     z = self.argm(sx, sy)\n",
        "    #     sxz = torch.cat([sx, z], dim=-1)\n",
        "    #     sy_ = self.pred(sxz)\n",
        "    #     # loss(sy, sy_)\n",
        "    #     mseloss = nn.MSELoss()(sy, sy_)\n",
        "\n",
        "    #     vx = self.exp_x(sx)\n",
        "    #     vy = self.exp_y(sy)\n",
        "    #     # print(\"vx\",vx.shape) #[40]\n",
        "    #     vicloss = self.vicreg(vx, vy)\n",
        "    #     return mseloss + vicloss\n"
      ],
      "metadata": {
        "id": "wOJ7QpgfSh4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title archive optuna argm\n",
        "# !pip install optuna\n",
        "# import optuna\n",
        "\n",
        "    # def argm(self, sx, sy):\n",
        "    #     optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "    #     sampler = optuna.samplers.NSGAIISampler()\n",
        "    #     # sampler = optuna.samplers.MOTPESampler()\n",
        "    #     study = optuna.create_study(direction=\"maximize\", sampler=sampler, pruner=optuna.pruners.MedianPruner())\n",
        "    #     # study = optuna.create_study()\n",
        "    #     # print(\"sx\",sx.shape)\n",
        "    #     # sx=sx.flatten()\n",
        "    #     def objective(trial):\n",
        "    #         z = trial.suggest_uniform('z', -1, 1)\n",
        "    #         # print(\"z trail\",sx,z)\n",
        "    #         z=torch.tensor([z])\n",
        "    #         sxz = torch.cat([sx, z], dim=-1)\n",
        "    #         sy_ = self.pred(sxz)\n",
        "    #         mseloss = nn.MSELoss()(sy, sy_)\n",
        "    #         return mseloss\n",
        "    #     study.optimize(objective, n_trials=100)\n",
        "    #     st=study.best_params\n",
        "    #     # print(\"st\",st['z'])\n",
        "    #     st=torch.tensor([st['z']])\n",
        "    #     return st\n",
        "\n",
        "\n",
        "    # def argm(self, SX, SY, dim_z):\n",
        "    #     optuna.logging.set_verbosity(70)\n",
        "    #     sampler = optuna.samplers.NSGAIISampler()\n",
        "    #     # sampler = optuna.samplers.MOTPESampler()\n",
        "    #     pruner = optuna.pruners.MedianPruner()\n",
        "    #     batch_size=1\n",
        "    #     # if sx.dim() == 2: batch_size,_=sx.shape\n",
        "    #     if SX.dim() == 2: batch_size,_=SX.shape\n",
        "    #     s=[]\n",
        "    #     for i in range(batch_size):\n",
        "    #         sx, sy = SX[i],SY[i]\n",
        "    #         study = optuna.create_study(direction=\"minimize\", sampler=sampler, pruner=pruner)\n",
        "    #         def objective(trial):\n",
        "    #             if dim_z<0:\n",
        "    #                 z = [trial.suggest_categorical(\"z\", np.linspace(-1,1,-dim_z,dtype=np.float32))]\n",
        "    #             elif dim_z>0:\n",
        "    #                 z=[]\n",
        "    #                 for d in range(dim_z):\n",
        "    #                     z.append(trial.suggest_uniform(chr(d), -1, 1))\n",
        "    #             # z = trial.suggest_uniform('z', -1, 1)\n",
        "    #             # print(\"z trail\",z)\n",
        "    #             z=torch.tensor(z).to(device)\n",
        "    #             # print(\"sx, z\",sx.dtype, z.dtype) #[500, 20] [1]\n",
        "    #             sxz = torch.cat([sx, z], dim=-1)\n",
        "    #             # print(\"sxz\",sxz,sxz.shape)\n",
        "    #             sy_ = self.pred(sxz)\n",
        "    #             mseloss = nn.MSELoss()(sy, sy_)\n",
        "    #             return mseloss\n",
        "    #         study.optimize(objective, n_trials=10)\n",
        "    #         st=list(study.best_params.values())\n",
        "    #         # print(s)\n",
        "    #         # st=torch.tensor([st['z']])\n",
        "    #         s.append(st)\n",
        "    #     return torch.tensor(s)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Qny_0zfTeT6m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}