{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOzjaphWvdis6Wy8lse1YpW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/procgen_JEPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7WkwnVjJTrW1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3e3c0fe-dd77-435b-c12e-1c22207d00f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.7/203.7 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.7/267.7 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qq procgen faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title procgen\n",
        "# https://github.com/openai/procgen\n",
        "import gym\n",
        "# env = gym.make(\"procgen:procgen-coinrun-v0\")\n",
        "# env = gym.make(\"procgen:procgen-coinrun-v0\", start_level=0, num_levels=1)\n",
        "\n",
        "# from procgen import ProcgenGym3Env\n",
        "# env = ProcgenGym3Env(num=1, env_name=\"coinrun\")\n",
        "\n",
        "env_name=\"procgen:procgen-{}-v0\".format(\"bigfish\")\n",
        "env = gym.make(env_name, use_sequential_levels=True, render_mode=\"rgb_array\")\n",
        "\n",
        "ENV_NAMES = [\"bigfish\", \"bossfight\", \"caveflyer\", \"chaser\", \"climber\", \"coinrun\", \"dodgeball\", \"fruitbot\", \"heist\", \"jumper\", \"leaper\", \"maze\", \"miner\", \"ninja\", \"plunder\", \"starpilot\",]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5-_pfGZTsip",
        "outputId": "f1b369d7-e5ae-4af5-95e0-2f6aa5a8bfa0",
        "cellView": "form"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title faiss\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# d = 256 # dimension\n",
        "# res = faiss.StandardGpuResources()  # use a single GPU\n",
        "# nlist = 100\n",
        "# m = 8\n",
        "# index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "# index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "# # index = faiss.IndexIVFPQ(index, d, nlist, m, 8) # each sub-vector is encoded as 8 bits # 3-IVFPQ.py\n",
        "# # index = faiss.index_cpu_to_gpu(res, 0, index) # 4-GPU.py\n",
        "# # index = faiss.index_cpu_to_all_gpus(index) # 5-Multiple-GPUs.py\n",
        "\n",
        "\n",
        "# import torch\n",
        "# ltmk = torch.rand(1000,d)\n",
        "# ltmv = torch.rand(1000,d)\n",
        "\n",
        "def makefaissindex(vert_store):\n",
        "    d = vert_store.shape[-1]\n",
        "    nlist = 100\n",
        "    index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "    index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "    if not index.is_trained: index.train(vert_store)\n",
        "    index.add(vert_store)\n",
        "    return index\n",
        "# index = makefaissindex(ltmk)\n",
        "\n",
        "\n",
        "def vecsearch(query, index, k=5, treshold=36): # k nearest neighbors\n",
        "    # index.nprobe = 5 # 1\n",
        "    D, I = index.search(query, k) # dist, idx\n",
        "    D, I = D[0], I[0]\n",
        "    mask = I[D<treshold]\n",
        "    return mask\n",
        "\n",
        "# import torch\n",
        "# query = torch.rand(1,d)\n",
        "\n",
        "# mask = vecsearch(query, index, k=5, treshold=37)\n",
        "# print(mask)\n",
        "# rag = ltmk[mask]\n",
        "# print(rag)\n",
        "\n",
        "\n",
        "# removing = torch.tensor([998, 769, 643])\n",
        "# index.remove_ids(removing)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "SKlOoBh8yHXA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ltm\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "class Ltm():\n",
        "    def __init__(self, ltmk=None, ltmv=None):\n",
        "        self.index = None\n",
        "        if ltmk is None:\n",
        "            self.ltmk = torch.tensor([])\n",
        "            self.ltmv = torch.tensor([])\n",
        "        else:\n",
        "            self.ltmk = ltmk # [len_ltm, d_model]\n",
        "            self.ltmv = ltmv\n",
        "        if len(self.ltmk)>=100:\n",
        "            self.index = makefaissindex(ltmk)\n",
        "\n",
        "    def add(self, k, v):\n",
        "        # self.ltmk.append(k)\n",
        "        # self.ltmv.append(v)\n",
        "        if k.ndim==1:\n",
        "            k=k.unsqueeze(0)\n",
        "            v=v.unsqueeze(0)\n",
        "        self.ltmk = torch.cat([self.ltmk, k], dim=0)\n",
        "        self.ltmv = torch.cat([self.ltmv, v], dim=0)\n",
        "        if self.index:\n",
        "            self.index.add(k)\n",
        "            if torch.rand(1)<0.1:\n",
        "                self.index.train(self.ltmk)\n",
        "\n",
        "    def makefaissindex(self, vert_store):\n",
        "        d = vert_store.shape[-1]\n",
        "        nlist = 100\n",
        "        index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "        index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "        if not index.is_trained: index.train(vert_store)\n",
        "        index.add(vert_store)\n",
        "        return index\n",
        "\n",
        "    def vecsearch(self, query, k=5, treshold=36): # k nearest neighbors\n",
        "        # index.nprobe = 5 # 1\n",
        "        D, I = self.index.search(query, k) # dist, idx\n",
        "        D, I = D[0], I[0]\n",
        "        mask = I[D<treshold]\n",
        "        return mask\n",
        "\n",
        "    def __call__(self, query, k=5, treshold=36): # [batch_size, d_model]\n",
        "        if self.index!=None and len(self.ltmk)>=100:\n",
        "            mask = self.vecsearch(query, k, treshold)\n",
        "            rag = self.ltmk[mask] # [len_rag, d_model]\n",
        "        else:\n",
        "            rag = self.ltmk\n",
        "        # print(\"ltm call\", query.shape, rag.shape)\n",
        "        if len(rag)==0: return 0\n",
        "        attn = query @ rag.T # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, len_ltm]\n",
        "        x = attention @ self.ltmv\n",
        "        return x # [batch_size, d_model]\n",
        "\n",
        "    def remove_ids(self, removing): # torch.tensor indexes\n",
        "        mask = torch.ones(len(self.ltmk), dtype=torch.bool)\n",
        "        mask[removing] = False\n",
        "        self.ltmk, self.ltmv = self.ltmk[mask], self.ltmv[mask]\n",
        "        if self.index: self.index = makefaissindex(ltmk)\n",
        "\n",
        "    def save(file='ltm.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.ltmk, self.ltmv), f)\n",
        "\n",
        "    def load(file='ltm.pkl'):\n",
        "        with open(file, 'rb') as f: self.ltmk, self.ltmv = pickle.load(f)\n",
        "\n",
        "ltm = Ltm()\n"
      ],
      "metadata": {
        "id": "5pscE7mtaPAq"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title pickle save/load\n",
        "import pickle\n",
        "# # save\n",
        "# with open('semantic_wiki.pkl', 'wb') as f: pickle.dump(semantic_wiki, f)\n",
        "# # load\n",
        "# with open('semantic_wiki.pkl', 'rb') as f: semantic_wiki = pickle.load(f)\n",
        "\n",
        "# def save_mem():\n",
        "#     for name in [semantic_wiki, ]\n",
        "#     with open(name+'.pkl', 'wb') as f: pickle.dump(name, f)\n"
      ],
      "metadata": {
        "id": "4-4GHdQZM_eK",
        "cellView": "form"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title mha\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None): # [batch_size, seq_len, d_model]\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.q(query).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) # [batch_size, n_heads, seq_len, head_dim]\n",
        "        K = self.k(key).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v(value).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        # attn = torch.matmul(Q, K.transpose(2, 3)) / self.scale\n",
        "        attn = Q @ K.transpose(2, 3) / self.scale # [batch_size, n_heads, seq_len, seq_len]\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        # x = torch.matmul(self.drop(attention), V)\n",
        "        x = self.drop(attention) @ V # [batch_size, n_heads, seq_len, head_dim]\n",
        "        x = x.transpose(1, 2).reshape(batch_size, -1, self.d_model) # [batch_size, seq_len, d_model]\n",
        "        x = self.lin(x)\n",
        "        return x, attention\n",
        "\n",
        "\n",
        "class MHAme(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None): # [batch_size, seq_len, d_model]\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.q(query).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) # [batch_size, n_heads, seq_len, head_dim]\n",
        "        K = self.k(key).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v(value).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        # attn = torch.matmul(Q, K.transpose(2, 3)) / self.scale\n",
        "        attn = Q @ K.transpose(2, 3) / self.scale # [batch_size, n_heads, seq_len, seq_len]\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        # x = torch.matmul(self.drop(attention), V)\n",
        "        x = self.drop(attention) @ V # [batch_size, n_heads, seq_len, head_dim]\n",
        "        x = x.transpose(1, 2).reshape(batch_size, -1, self.d_model) # [batch_size, seq_len, d_model]\n",
        "        x = self.lin(x)\n",
        "        return x, attention\n"
      ],
      "metadata": {
        "id": "9OFjAK232GNp",
        "cellView": "form"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test mha\n",
        "import torch\n",
        "batch_size=3\n",
        "L=5\n",
        "d_model=8\n",
        "n_heads=2\n",
        "\n",
        "\n",
        "\n",
        "trg = torch.rand(batch_size,L, d_model)\n",
        "src = torch.rand(batch_size,L, d_model)\n",
        "\n",
        "mha = MultiHeadAttention(d_model, n_heads)\n",
        "x, attn = mha(trg,src,src)\n",
        "\n",
        "head_dim = d_model // n_heads\n",
        "\n",
        "# # trg1=trg.view(batch_size, -1, n_heads, head_dim).transpose(1, 2)\n",
        "# trg=trg.view(batch_size, n_heads, -1, head_dim)\n",
        "# src=src.view(batch_size, n_heads, -1, head_dim)\n",
        "# # print(trg1)\n",
        "# # print(\"##########\")\n",
        "# # print(trg2)\n",
        "# attn = trg @ src.transpose(2, 3)\n",
        "# x=attn@trg\n",
        "print(x.shape)\n",
        "print(attn.shape)\n",
        "\n",
        "# # trg1=trg1.view(batch_size,L, d_model)\n",
        "# trg1=trg1.reshape(batch_size,L, d_model)\n",
        "# trg2=trg2.view(batch_size,L, d_model)\n",
        "# print(trg1)\n",
        "# print(\"##########\")\n",
        "# print(trg2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S87jVbpM21X7",
        "outputId": "0c45bb84-0cd7-4f78-fd8d-0a79ba8f84ba",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 5, 8])\n",
            "torch.Size([3, 2, 5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title augmentations\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/augmentations.py\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import InterpolationMode\n",
        "\n",
        "class TrainTransform(object):\n",
        "    def __init__(self):\n",
        "        self.transform = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(32, interpolation=InterpolationMode.BICUBIC),#224\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,),\n",
        "                transforms.RandomGrayscale(p=0.2),\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=1.0),\n",
        "                # transforms.RandomSolarize(threshold=130, p=0.0)\n",
        "                # transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "        self.transform_prime = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(32, interpolation=InterpolationMode.BICUBIC),#224\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,),\n",
        "                transforms.RandomGrayscale(p=0.2),\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=0.1),\n",
        "                # transforms.RandomSolarize(threshold=130/255, p=0.2) # og threshold=130, /255 bec after normalising\n",
        "                transforms.RandomSolarize(threshold=.9, p=0.2),\n",
        "                # transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "                # transforms.RandomPerspective(distortion_scale=0.3, p=0.5), # me\n",
        "                # transforms.RandomErasing(p=0.5, scale=(0.1, 0.11), ratio=(1,1), value=0, inplace=True), # default p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False\n",
        "        # dims = len(sample.shape)\n",
        "        # if dims==3: x1 = self.transform(sample) # same transforms per minibatch\n",
        "        # elif dims==4: x1 = transforms.Lambda(lambda x: torch.stack([self.transform(x_) for x_ in x]))(sample) # diff transforms per img in minibatch\n",
        "    def __call__(self, sample):\n",
        "        x1 = self.transform(sample)\n",
        "        x2 = self.transform_prime(sample)\n",
        "        return x1, x2\n",
        "\n",
        "trs=TrainTransform()\n"
      ],
      "metadata": {
        "id": "hEUffQ24mkRY",
        "cellView": "form"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "cellView": "form",
        "id": "r0mXVAUnVYX-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b64b686-8f2f-4505-b0ef-362278b5ea15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 256])\n"
          ]
        }
      ],
      "source": [
        "# @title resnet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models # https://pytorch.org/vision/0.12/models.html#id10\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def get_res(dim_embd):\n",
        "    model = models.resnet18(weights='DEFAULT') # 18 34 50 101 152\n",
        "    num_ftrs = model.fc.in_features # 1000\n",
        "    model.fc = nn.Sequential( # og\n",
        "        nn.Linear(num_ftrs, dim_embd, bias=None),\n",
        "        # nn.Softmax(dim=1),\n",
        "        )\n",
        "    return model\n",
        "# model = get_res(256).to(device)\n",
        "# input = torch.rand(1,3,64,64)\n",
        "# out = model(input)\n",
        "# print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title vicreg next\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/resnet.py\n",
        "class VICReg(nn.Module):\n",
        "    def __init__(self, dim_embd=256, ema=False):\n",
        "        super().__init__()\n",
        "        self.conv = get_res(dim_embd=dim_embd)\n",
        "\n",
        "        # f=[dim_embd,1024,1024,1024]\n",
        "        # f=[dim_embd,512,512,512]\n",
        "        f=[dim_embd,256,256,256]\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(f[0], f[1]), nn.BatchNorm1d(f[1]), nn.ReLU(),\n",
        "            nn.Linear(f[1], f[2]), nn.BatchNorm1d(f[2]), nn.ReLU(),\n",
        "            nn.Linear(f[-2], f[-1], bias=False)\n",
        "            )\n",
        "        self.ema = ema\n",
        "        if ema:\n",
        "            self.conv_ema = AveragedModel(self.conv, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "            self.exp_ema = AveragedModel(self.exp, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size=x.size(dim=0)\n",
        "        num_features=32\n",
        "        sim_coeff=10.0 # 25.0 # λ\n",
        "        std_coeff=10.0 # 25.0 # µ\n",
        "        cov_coeff=1.0 # 1.0 # ν\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = (sim_coeff * repr_loss + std_coeff * std_loss + cov_coeff * cov_loss)\n",
        "        print(\"in vicreg \",(sim_coeff * repr_loss).item() , (std_coeff * std_loss).item() , (cov_coeff * cov_loss).item())\n",
        "        return loss\n",
        "\n",
        "    def loss(self, sx, sy):\n",
        "        sx = self.forward(sx)\n",
        "        sy = self.forward(sy)\n",
        "        with torch.no_grad(): # target encoder is ema\n",
        "            sy = self.conv_ema(sy)\n",
        "            vy = self.exp_ema(sy)\n",
        "        vx = self.exp(sx)\n",
        "        vy = self.exp(sy)\n",
        "        loss = self.vicreg(vx,vy)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "model = VICReg().to(device) # create an instance and move it to device (cache?)\n"
      ],
      "metadata": {
        "id": "V15LtR8myLL9",
        "cellView": "form"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title jepa\n",
        "# https://openreview.net/pdf?id=BZ5a1r-kVsf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class JEPA(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, dim_a, dim_z, dim_v):\n",
        "        super(JEPA, self).__init__()\n",
        "        self.enc = nn.Sequential(nn.Linear(in_dim, d_model),)\n",
        "        self.pred = nn.Sequential(\n",
        "            nn.Linear(d_model+dim_a+dim_z, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model),\n",
        "            )\n",
        "        # self.pred = gru(emb_dim, rnn_units, num_layers)\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_v),# nn.ReLU(True),\n",
        "            )\n",
        "        self.d_model = d_model\n",
        "        self.dim_z = dim_z\n",
        "        self.sim_coeff=10.0 # 25.0 # λ\n",
        "        self.std_coeff=10.0 # 25.0 # µ\n",
        "        self.cov_coeff=1.0 # 1.0 # ν\n",
        "        self.z=torch.zeros((1,dim_z),device=device)\n",
        "\n",
        "    def v_creg(self, x): # vx [batch_size, d_model]\n",
        "        x = x - x.mean(dim=0)\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2\n",
        "        batch_size, num_features = x.shape\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\n",
        "        loss = self.std_coeff * std_loss + self.cov_coeff * cov_loss\n",
        "        return loss\n",
        "\n",
        "    def argm(self, sx, a, sy):\n",
        "        batch=sx.size(dim=0)\n",
        "        z = nn.Parameter(torch.rand((batch,self.dim_z),device=device), requires_grad=True)\n",
        "        optim = torch.optim.SGD([z], lr=1e-1, momentum=0.9)\n",
        "        lossfn = torch.nn.MSELoss()\n",
        "        sx, a, sy = sx.detach(), a.detach(), sy.detach()\n",
        "        # x, y = x.detach(), y.detach().unsqueeze(-1).float()\n",
        "        num_steps = 10\n",
        "        for i in range(num_steps):\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            sy_ = self.pred(sxaz)\n",
        "            # print(\"y_, y\",y_.shape, y.shape)\n",
        "            loss = lossfn(sy_, sy)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "        return z#.detach()\n",
        "\n",
        "    def loss(self, x, y, a, z=None):\n",
        "        sx, sy = self.enc(x), self.enc(y)\n",
        "        z = self.argm(sx, a, sy)\n",
        "        sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "        sy_ = self.pred(sxaz)\n",
        "        repr_loss = self.sim_coeff * F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "        # v_c_loss = self.v_creg(self.exp(sx))\n",
        "        vx, vy = self.exp(sx), self.exp(sy)\n",
        "        v_c_loss = self.v_creg(vx) + self.v_creg(vy)\n",
        "        return repr_loss + v_c_loss\n",
        "\n",
        "    def forward(self, sx, a): # state, ctrl\n",
        "        batch=sx.size(dim=0)\n",
        "        z=torch.zeros((batch,self.dim_z),device=device)\n",
        "        sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "        sy_ = self.pred(sxaz)\n",
        "        return sy_ # state1\n",
        "\n",
        "\n",
        "d_model=16\n",
        "dim_z= 1#-5\n",
        "dim_v=32\n",
        "dim_a=4\n",
        "# model = JEPA(in_dim, d_model, dim_a, dim_z, dim_v).to(device)\n",
        "# x=torch.rand(1, in_dimx)\n",
        "# y=torch.rand(1, in_dimy)\n",
        "# loss = model.loss(x,y)\n",
        "# distance = torch.norm(embeddings.weight.data - my_sample, dim=-1)\n",
        "# nearest = torch.argmin(distance)\n"
      ],
      "metadata": {
        "id": "FuA25qQknUAX",
        "cellView": "form"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title torch gru\n",
        "# text_generation.ipynb https://colab.research.google.com/drive/1SguQZQYZBaalRuElJcxGdgF3YxhiwkAM\n",
        "# RNNs.ipynb https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR\n",
        "\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class gru(nn.Module):\n",
        "    def __init__(self, emb_dim, rnn_units, num_layers):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(emb_dim, rnn_units, num_layers=num_layers, dropout=0.0, batch_first=True)\n",
        "        self.dense = nn.Linear(rnn_units, vocab_size)\n",
        "        self.rnn_units = rnn_units\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(self.num_layers, batch_size, self.rnn_units, device=device) # https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embedding(x)\n",
        "        if hidden is None: hidden = self.init_hidden(x.shape[0])\n",
        "        # print('fwd',x.shape, hidden.shape) # fwd [batch_size, bptt, emb_dim], [num_layers, batch_size, rnn_units]\n",
        "        output, hidden = self.gru(x, hidden)\n",
        "        output = self.dense(output)\n",
        "        return output, hidden\n",
        "\n",
        "\n",
        "emb_dim = 256#256\n",
        "rnn_units = 1024#1024\n",
        "num_layers = 1\n",
        "# model = gru(emb_dim, rnn_units, num_layers).to(device)\n",
        "# model.summary()\n",
        "\n"
      ],
      "metadata": {
        "id": "ZDtHEU4tCo5z",
        "cellView": "form"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q vector-quantize-pytorch"
      ],
      "metadata": {
        "id": "6CPBGb-k_aXl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44e5982f-8e5e-4ddd-a04b-db26d13fabc0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.0/103.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title vector quantize\n",
        "# https://github.com/lucidrains/vector-quantize-pytorch?tab=readme-ov-file#finite-scalar-quantization\n",
        "import torch\n",
        "from vector_quantize_pytorch import FSQ\n",
        "\n",
        "quantizer = FSQ(levels = [3,3,2]) # https://github.com/lucidrains/vector-quantize-pytorch/blob/master/vector_quantize_pytorch/finite_scalar_quantization.py\n",
        "\n",
        "# x = torch.randn(1, 1024, 3) # last dim is num levels\n",
        "x = torch.randn(4, 256, 3)*3 -1.5 # [batch_size, T, num_levels]\n",
        "xhat, indices = quantizer(x) # [batch_size, T, num_levels], [batch_size, T]\n",
        "# print(xhat[0])\n",
        "# print(indices[0])\n",
        "\n",
        "# assert torch.all(xhat == quantizer.indices_to_codes(indices))\n"
      ],
      "metadata": {
        "id": "ko5qJO7Et09L",
        "cellView": "form"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title agent\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=32):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.sense = VICReg(d_model)\n",
        "        self.ltm = Ltm(ltmk=torch.rand(5, d_model), ltmv=torch.rand(5, d_model))\n",
        "        # self.ltm.load()\n",
        "        self.stm = torch.rand(d_model, d_model)\n",
        "        self.world_state = torch.rand(d_model, d_model) # Sum i] vi kiT\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        # self.critic = GRU(\n",
        "        # self.critic = nn.Sequential(\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            # nn.Linear(d_model, d_model),\n",
        "            # )\n",
        "        self.actor = nn.Sequential( # -> goal sx/ssx/sssx/...\n",
        "            nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model),\n",
        "            )\n",
        "        self.lcost = nn.Sequential(\n",
        "            nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.quantizer = FSQ(levels = [3,3,2])\n",
        "\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        state = self.transform(state).unsqueeze(0).to(device)\n",
        "        current = self.sense(state) # [1, 256]\n",
        "\n",
        "        ltm = self.ltm(current)\n",
        "        stm = self.stm @ current.T\n",
        "        # print(\"agent fwd\", current.shape, self.stm.shape)\n",
        "        # print(\"agent fwd1\", ltm.shape, stm.shape)\n",
        "        # ltm, stm = F.normalize(ltm, dim=-1), F.normalize(stm, dim=-1)\n",
        "        ltm, stm = F.normalize(ltm, dim=-1), F.normalize(stm)\n",
        "        # a=0.5\n",
        "        # obs = torch.cat([current, a*stm + (1-a)*ltm], dim=-1)\n",
        "        obs = current + stm + ltm\n",
        "        # Q = self.q(obs)\n",
        "        K, V = self.k(obs), self.v(obs)\n",
        "        V_ = self.world_state @ K\n",
        "        self.world_state = self.world_state + (V - V_) @ K.T # -V_.K^T, + V.K^T # update world state\n",
        "\n",
        "        sx = self.jepa.enc(self.world_state.flatten()).unsqueeze(0)\n",
        "\n",
        "        lact = self.search(sx, T=256)\n",
        "        act = lact[0][0]\n",
        "        self.ltm.add(K, V)\n",
        "\n",
        "        return act\n",
        "\n",
        "    # def plan(self, ): # mpc\n",
        "    #     # xs, us = locuslab_mpc(x_init, goal_state, self.jepa)\n",
        "    #     xs, us = locuslab_mpc(x_init, goal_state, self.jepa)\n",
        "\n",
        "    def search(self, sx, T=256): # 256\n",
        "\n",
        "        # x = torch.randn(4, T, 3)*3 -1.5 # [batch_size, T, num_levels]\n",
        "        # xhat, indices = quantizer(x) # [batch_size, T, num_levels], [batch_size, T]\n",
        "        # z = nn.Parameter(torch.rand((batch,self.dim_z),device=device), requires_grad=True)\n",
        "        x = nn.Parameter(torch.rand((1, T, 3),device=device))\n",
        "        optim = torch.optim.SGD([x], lr=1e-1, momentum=0.9)\n",
        "        # print(\"search x\",x.requires_grad)\n",
        "        sx = sx.detach()\n",
        "        # model.train()\n",
        "        for _ in range(5):\n",
        "            la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "            # print(\"search la\",la.requires_grad)\n",
        "            # with torch.no_grad():\n",
        "            #     x = model(x)\n",
        "            # print(\"search la sx\", la.shape, sx.shape) # [4, 256, 3], [1, 256, 3]\n",
        "            loss = self.rnn_fwd(sx, la)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "        return lact # [batch_size, T]\n",
        "\n",
        "\n",
        "    def loss(self, lx): # offline\n",
        "        self.sense.loss(lx) #\n",
        "        self.jepa.loss(lx, ly, la)\n",
        "\n",
        "    def cost(self, sx, a): # critic+intrinsic cost\n",
        "        # self.critic(sxa)\n",
        "        sxa = torch.cat([sx, a], dim=-1)\n",
        "        return self.lcost(sxa)\n",
        "\n",
        "    def rnn_fwd(self, sx, la): # state, ctrl\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        z=torch.zeros((batch,self.dim_z),device=device)\n",
        "        # out=[]\n",
        "        cost = 0\n",
        "        for t in range(seq_len): # simple single layer\n",
        "            a = la[:,t]\n",
        "            cost += self.cost(sx, a)\n",
        "            # print(\"rnn fwd\", t,sx.shape, a.shape, z.shape)\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            sx = self.jepa.pred(sxaz)\n",
        "            # out.append(sx)\n",
        "        # out=torch.cat(out)\n",
        "        # out = out[:, -1, :] # out: (n, 128)\n",
        "        # return out, h0\n",
        "        return cost\n",
        "\n",
        "\n",
        "# train cost\n",
        "\n",
        "agent = Agent()\n"
      ],
      "metadata": {
        "id": "RCD647ZpPrGf"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "\n",
        "# print(env.action_space)\n",
        "\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    # action = env.action_space.sample() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "    action = agent(state)\n",
        "    state, reward, done, info = env.step(action)\n",
        "    # print(state.shape) # 0-255 (64, 64, 3)\n",
        "    print(reward, done)\n",
        "    out.write(state)\n",
        "\n",
        "    # break\n",
        "    if done:\n",
        "        break\n",
        "env.close()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "TA_rcOQQTxan",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df079fe6-feff-4305-e16d-5932c3bf4f00"
      },
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n",
            "lact torch.Size([1, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# s = env.reset()\n",
        "# print(s.shape)\n",
        "\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# s= transform(s)\n",
        "# print(s.shape)\n",
        "\n",
        "action = agent(state)\n",
        "print(action)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MY73kQHLYN-e",
        "outputId": "a727edfa-e34d-4557-a472-03da6fda6e99"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([13, 17, 13, 14, 14, 13, 16, 14, 16, 13, 17, 14, 13, 14, 16, 14, 13, 16,\n",
            "        14, 17, 17, 13, 14, 13, 16, 14, 16, 13, 17, 14, 17, 16, 13, 14, 16, 17,\n",
            "        16, 14, 13, 13, 16, 14, 13, 13, 17, 14, 13, 17, 14, 14, 17, 17, 16, 16,\n",
            "        16, 17, 16, 17, 13, 17, 16, 17, 17, 13, 14, 14, 14, 14, 14, 13, 13, 17,\n",
            "        17, 16, 13, 16, 14, 13, 13, 14, 17, 16, 14, 16, 14, 13, 17, 13, 16, 14,\n",
            "        17, 17, 13, 16, 13, 17, 17, 17, 14, 16, 16, 13, 14, 13, 16, 14, 17, 13,\n",
            "        13, 16, 13, 17, 16, 16, 14, 13, 14, 17, 17, 16, 14, 14, 14, 17, 14, 14,\n",
            "        14, 16, 14, 16, 13, 13, 16, 13, 17, 14, 13, 13, 16, 13, 13, 16, 13, 13,\n",
            "        16, 17, 17, 14, 14, 16, 17, 13, 16, 16, 13, 13, 16, 16, 13, 13, 17, 16,\n",
            "        17, 13, 14, 16, 16, 14, 16, 13, 14, 14, 16, 17, 17, 14, 13, 16, 16, 16,\n",
            "        17, 13, 13, 14, 17, 13, 16, 13, 16, 17, 17, 17, 17, 16, 16, 13, 14, 17,\n",
            "        16, 16, 16, 16, 14, 13, 17, 17, 17, 13, 17, 14, 16, 13, 16, 17, 14, 13,\n",
            "        17, 14, 17, 14, 13, 13, 13, 17, 13, 13, 16, 14, 17, 16, 16, 17, 17, 14,\n",
            "        16, 16, 13, 14, 13, 13, 14, 17, 13, 17, 13, 16, 17, 16, 16, 17, 13, 14,\n",
            "        13, 17, 13, 17], dtype=torch.int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ffmpeg -hide_banner -loglevel error -i video.avi video.mp4 -y\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open('video.mp4', \"rb\").read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"<video width=400 controls autoplay><source src=\"{data_url}\" type=\"video/mp4\"></video>\"\"\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "b8zxYU9jpE8K",
        "outputId": "ff480fbb-a2b5-4446-aee1-b06a227f8999"
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<video width=400 controls autoplay><source src=\"data:video/mp4;base64,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\" type=\"video/mp4\"></video>"
            ]
          },
          "metadata": {},
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uT9m-J1BUWyz",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title wandb\n",
        "# https://docs.wandb.ai/quickstart\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login() # 487a2109e55dce4e13fc70681781de9f50f27be7\n",
        "run = wandb.init(\n",
        "    project=\"vicreg\",\n",
        "    config={\n",
        "        \"model\": \"res18\",\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title rnn train\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer, bptt=32):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        hid = model.init_hidden(X.shape[0])\n",
        "        X, y = X.to(device), y.to(device) # [batch_size, seq_len]\n",
        "        Xs, ys = torch.split(X, bptt, dim=1), torch.split(y, bptt, dim=1)\n",
        "        for (X, y) in zip(Xs, ys): # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            optimizer.zero_grad()\n",
        "            # print(\"X.shape:\",X.shape) # [batch_size, bptt]\n",
        "            pred, hid = model(X, hid)\n",
        "            loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.flatten())\n",
        "            # loss = loss_fn(pred.flatten(0,1), y.flatten())\n",
        "            # loss = loss_fn(pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            hid = hid.detach()\n",
        "\n",
        "        train_loss = loss.item()/ len(X)\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        # if batch % 100 == 0:\n",
        "        #     loss, current = loss.item(), batch * len(X)\n",
        "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n"
      ],
      "metadata": {
        "id": "jw28GX1HzTeb",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title train test function\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def strain(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            x1, x2 = trs(x)\n",
        "            loss = model.loss(x1,x2)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        optimizer.zero_grad()\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "        # model.exp_ema.update_parameters(model.exp)\n",
        "\n",
        "        scaler.update()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "        train_loss = loss.item()/len(y)\n",
        "        loss_list.append(loss.item())\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "def train(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        x1, x2 = trs(x)\n",
        "        loss = model.loss(x1,x2)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "\n",
        "        loss_list.append(loss.item())\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "\n",
        "# def ctrain(dataloader, model, loss_fn, optimizer, scheduler=None, verbose=True):\n",
        "def ctrain(dataloader, model, loss_fn, optimizer, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.no_grad():\n",
        "            x = model(x)\n",
        "        pred = model.classify(x)\n",
        "        loss = loss_fn(pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if batch % (size//10) == 0:\n",
        "        # if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(x)\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            x = model(x)\n",
        "            pred = model.classify(x)\n",
        "            loss = loss_fn(pred, y)\n",
        "            # predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= len(dataloader)\n",
        "    correct /= len(dataloader.dataset)\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return test_loss\n"
      ],
      "metadata": {
        "id": "fsealXK3OPQa",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title rnn train, gen\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        pred,_ = model(X)\n",
        "        loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        train_loss = loss.item()/ len(X)\n",
        "\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        # if batch % 100 == 0:\n",
        "        #     loss, current = loss.item(), batch * len(X)\n",
        "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer, bptt=32):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # hid = model.init_hidden(bptt)\n",
        "        hid = model.init_hidden(X.shape[0])\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        # print(\"X.shape:\",X.shape) # [batch_size, seq_len]\n",
        "        Xs, ys = torch.split(X, bptt, dim=1), torch.split(y, bptt, dim=1)\n",
        "        for (X, y) in zip(Xs, ys): # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            optimizer.zero_grad()\n",
        "            # print(\"X.shape:\",X.shape) # [batch_size, bptt]\n",
        "            pred, hid = model(X, hid)\n",
        "            loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.flatten())\n",
        "            # loss = loss_fn(pred.flatten(0,1), y.flatten())\n",
        "            # loss = loss_fn(pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            hid = hid.detach()\n",
        "\n",
        "        train_loss = loss.item()/ len(X)\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        # if batch % 100 == 0:\n",
        "        #     loss, current = loss.item(), batch * len(X)\n",
        "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "def generate(model, context, max_steps = 64, temperature=1):\n",
        "    # x = torch.tensor([train_dataset.stoi.get(c) for c in context], device=device).unsqueeze(0)\n",
        "    x=ix = torch.tensor([train_dataset.stoi.get(c) for c in context], device=device).unsqueeze(0)\n",
        "    model.eval()\n",
        "    hidden=None\n",
        "    with torch.no_grad():\n",
        "        for n in range(max_steps):\n",
        "            # output, hidden = model(x, hidden)\n",
        "            output, hidden = model(ix, hidden)\n",
        "            hidden=hidden[:, -1, :].unsqueeze(1)\n",
        "            output = output[:, -1, :] # get logit for last character\n",
        "            output = output/temperature\n",
        "            output = F.softmax(output, dim = -1) # vocab_size to char\n",
        "            ix = torch.multinomial(output, num_samples = 1) # rand sample by output distribution\n",
        "            x = torch.cat((x, ix),1)\n",
        "        completion = ''.join([train_dataset.itos[int(i)] for i in x.flatten()])\n",
        "        return completion\n",
        "\n",
        "# out=generate(model, \"A wi\")\n",
        "# print(out)\n"
      ],
      "metadata": {
        "id": "zOB1Kh3jL6YV",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title straight through estimator\n",
        "class STEFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        return (input > 0).float()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return F.hardtanh(grad_output)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "y4vBx6CBgoTG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}